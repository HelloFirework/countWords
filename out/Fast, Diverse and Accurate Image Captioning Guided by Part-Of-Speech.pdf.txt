Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech

Aditya Deshpande1 ∗, Jyoti Aneja1 ∗, Liwei Wang2, Alexander Schwing1 and David Forsyth1

ardeshp2,janeja2,aschwing,daf@illinois.edu
1University of Illinois at Urbana Champaign

liweiwang@tencent.com
2Tencent AI Lab, Seattle

Abstract

Image captioning is an ambiguous problem, with many
suitable captions for an image. To address ambiguity, beam
search is the de facto method for sampling multiple cap-
tions. However, beam search is computationally expensive
and known to produce generic captions [8, 10]. To address
this concern, some variational auto-encoder (VAE) [32]
and generative adversarial net (GAN) [5, 25] based meth-
ods have been proposed. Though diverse, GAN and VAE are
less accurate. In this paper, we ﬁrst predict a meaningful
summary of the image, then generate the caption based on
that summary. We use part-of-speech as summaries, since
our summary should drive caption generation. We achieve
the trifecta: (1) High accuracy for the diverse captions as
evaluated by standard captioning metrics and user stud-
ies; (2) Faster computation of diverse captions compared
to beam search and diverse beam search [28]; and (3) High
diversity as evaluated by counting novel sentences, distinct
n-grams and mutual overlap (i.e., mBleu-4) scores.

1. Introduction

In this paper we show how to force an image caption-
ing system to generate diverse captions by conditioning on
different high-level summaries of the image. Our sum-
maries are quantized part-of-speech (POS) tag sequences.
Our system generates captions by (a) predicting different
summaries from the image then (b) predicting captions con-
ditioned on each summary. This approach leads to captions
that are accurate, quick to obtain, and diverse. Our system
is accurate, because it is able to steer a number of narrow
beam searches to explore the space of caption sequences
more efﬁciently. It is fast because each beam is narrow. And
the captions are diverse, because depending on the sum-
mary (i.e., part-of-speech) the system is forced to produce
captions that contain (for example) more or less adjectives.
This means we can avoid the tendency to produce minimal
or generic captions that is common in systems that try to

∗ Denotes equal contribution.

Method

Fast Diverse Accurate

Beam search
Diverse beam search [28]
AG-CVAE [32]
Ours (POS)

×
×
X

X

×
×
X

X

X

X
×
X

Table 1: We show that our part-of-speech (POS) based
method achieves the trifecta of high accuracy, fast com-
putation and more diversity. Beam search and diverse
beam search are slow. They also produce captions with high
mutual overlap and lower distinct n-grams than POS (see
mBleu-4, div-1 and div-2 in Tab. 5). POS and AG-CVAE
are fast, however POS does better on captioning metrics in
Fig. 3 and is therefore more accurate.

optimize likelihood without awareness of language priors
(like part-of-speech).

A large body of literature has focused on developing pre-
dictive image captioning techniques, often using recurrent
neural nets (RNN) [20, 29, 34, 12, 2]. More recently [3, 33],
demonstrate predictive captioning with accuracy similar to
RNNs while using convolutional networks. An essential
feature of captioning is that it is ambiguous – many captions
can describe the same image. This creates a problem, as
image captioning programs trained to maximize some score
may do so by producing strongly non-committal captions. It
also creates an opportunity for research – how can one pro-
duce multiple, diverse captions that still properly describe
the image? Our method offers a procedure to do so.

Tractable image captioning involves factoring the se-
quence model for the caption. Inference then requires beam
search, which investigates a set of captions determined by
local criteria to ﬁnd the caption with highest posterior prob-
ability. Finding very good captions requires a wide beam,
which is slow. Moreover, beam search is also known to
generate generic captions that lack diversity [8, 10]. Vari-
ational auto-encoder (VAE) [32] and generative adversar-
ial net (GAN) [5, 25, 14] formulations outperform beam
search on diversity metrics. VAE and GAN-based methods
sample latent vectors from some distribution, then generate
captions conditioned on these samples. The latent variables
have no exposed semantics, and captions tend not to score
as well (on captioning metrics) as those produced by beam

110695

search (e.g., Tab. 1 of [25]).

This paper offers an alternative. First predict a meaning-
ful summary of the image, then generate the caption based
on that summary. For this to work, the summary needs to
be able to drive language generation (for the caption gen-
erator), and must be predictable. We ﬁnd quantized part of
speech tag sequences to be very effective summaries. These
sequences can clearly drive language generation (e.g., forc-
ing a captioner to produce adjectives in particular loca-
tions). More surprisingly, one can predict quantized tag
sequences from images rather well, likely because such se-
quences do summarize the main action of the image. For
example, compare determiner-noun-verb with determiner-
adjective-noun-verb-adjective-noun. In the ﬁrst case, some-
thing appears in the image, in the second, a subject with a
noteworthy attribute is doing something to an object with a
noteworthy attribute. Consequently, the two images appear
quite different.

Contributions: We show that image captioning with
POS tag sequences is fast, diverse and accurate (Tab. 1).
Our POS methods sample captions faster and with more di-
versity than techniques based on beam search and its vari-
ant diverse beam search [28] (Tab. 5). Our diverse cap-
tions are more accurate than their counterparts produced by
GANs [25] (Tab. 4) and VAEs [32] (Tab. 3, Fig. 3).

2. Related Work

In the following, we ﬁrst review works that generate a
single (or best-1) caption before discussing diverse image
captioning methods which produce k different (or a set of
best-k) captions.

2.1. Image Captioning

Most image captioning approaches [12, 29, 34] use a
convolutional neural net pre-trained on classiﬁcation [26]
to represent image features.
Image features are fed into
a recurrent net (often based on long-short-term-memory
(LSTM) units) to model the language word-by-word. These
networks are trained with maximum likelihood. To obtain
high performance on standard image captioning metrics,
Yao et al. [35] use a network trained on COCO-attributes
in addition to image features. Anderson et al. [2] develop
an attention-based network architecture. Aneja et al. [3]
change the language decoder from an LSTM-net to a con-
voluational network and show that they obtain more diver-
sity. Similarly, Wang et al. [33] also use a convolutional
language decoder. Since diversity is of interest to us, we use
the convolutional language decoder similar to [3, 33]. We
leave incorporation of techniques such as attribute vectors
speciﬁc to the COCO dataset, and a sophisticated attention
mechanism from [35, 2] for further performance gains to
future work.

Apart from exploring different network architectures,
some prior works focus on using different training losses.
Reinforcement learning has been used in [19, 24, 17], to di-
rectly train for non-differentiable evaluation metrics such as
BLEU, CIDEr and SPICE. In this paper, we use maximum
likelihood training for our methods and baselines to ensure
a fair comparison. Training our POS captioning network in
a reinforcement learning setup can be investigated as part of
future work.

Notable advances have been made in conditioning image
captioning on semantic priors of objects by using object de-
tectors [18, 30]. This conditioning is only limited to the
objects (or nouns) in the caption and ignores the remain-
der, while our POS approach achieves coordination for the
entire sentence.

2.2. Diverse Image Captioning

Four main techniques have been proposed to generate
multiple captions and rank them to obtain a set of best-k
captions.
Beam search. Beam search is the classical method to sam-
ple multiple solutions given sequence models for neural
machine translation and image captioning. We compare
to beam search on the same base captioning network as
POS, but without part-of-speech conditioning. We ﬁnd that
though beam search is accurate, it is slow (Tab. 3) and lacks
diversity (Tab. 5). Our base captioning network uses a con-
volutional neural net (CNN) [3] and is equivalent to the
standard LSTM based captioning network of Karpathy et
al. [12] in terms of accuracy.
Diverse beam search. Vijayakumar et al. [28] augment
beam search with an additional diversity function to gen-
erate diverse outputs. They propose a hamming diversity
function that penalizes expanding a beam with the same
word used in an earlier beam. In our results, we compare
to this diverse beam search (Div-BS). Note, beam search
and diverse beam search are local search procedures which
explore the output captioning space word-by-word. While,
POS tag sequences act as global probes that permit to sam-
ple captions in many different parts of the captioning space.
GAN. More recent work on diverse image captioning fo-
cuses on using GANs. Adversarial training has been em-
ployed by [5, 14, 25] to generate diverse captions. [5, 14]
train a conditional GAN for diverse caption generation. [5]
uses a trainable loss which differentiates human annotations
from generated captions. Ranking based techniques, which
attempt to score human annotated captions higher than gen-
erated ones, are demonstrated in [14]. Shetty et al. [25]
use adversarial training in combination with an approximate
Gumbel sampler to match the generated captions to the hu-
man annotations.

Generally, GAN based methods improve on diversity,
but suffer on accuracy. For example, in Tab. 1 of [25], ME-

10696

TEOR and SPICE scores drop drastically compared to an
LSTM baseline. In Tab. 4, we compare GAN [25] and our
POS-based method which is more accurate.

VAE. Wang et al. [32] propose to generate diverse captions
using a conditional variational auto-encoder with an addi-
tive Gaussian latent space (AG-CVAE) instead of a GAN.
The diversity obtained with their approach is due to sam-
pling from the learned latent space. They demonstrate im-
provements in accuracy over the conventional LSTM base-
line. Due to the computational complexity of beam search
they used fewer beams for the LSTM baseline compared to
the number of captions sampled from the VAE, i.e., they
ensured equal computational time. We compare to AG-
CVAE [32] and show that we obtain higher best-1 caption
accuracy (Tab. 3) and our best-kth caption accuracy (k = 1
to 10) outperforms AG-CVAE (Fig. 3). Note, best-k scores
in Tab. 3 and Fig. 3 denote the score of the kth ranked cap-
tion given the same number of sampled captions (20 or 100)
for all methods. For fairness, we use the same ranking pro-
cedure (i.e., consensus re-ranking proposed by [7] and used
in [32]) to rank the sampled captions for all methods.

3. Background

Problem Setup and Notation. The goal of diverse cap-
tioning is to generate k sequences y1, y2, . . . , yk, given
an image. For readability we drop the super-script and
focus on a single sequence y. The methods we discuss
and develop will sample many such sequences y and rank
them to obtain the best-k – y1, y2, . . . , yk. A single cap-
tion y = (y1, . . . , yN ) consists of a sequence of words yi,
i ∈ {1, . . . , N } which accurately describe the given image
I. For each caption y, the words yi, i ∈ {1, . . . , N } are
obtained from a ﬁxed vocabulary Y, i.e., yi ∈ Y. Addi-
tionally, we assume availability of a part-of-speech (POS)
tagger for the sentence y. More speciﬁcally, the POS tagger
provides a tag sequence t = (t1, . . . , tN ) for a given sen-
tence, where ti ∈ T is the POS tag for word yi. The set
T encompasses 12 universal POS tags – verb (VERB), noun
(NOUN), pronoun (PRON), etc.1

To train our models we use a dataset D = {(I, y, t)}
which contains tuples (I, y, t) composed of an image I,
a sentence y, and the corresponding POS tag sequence t.
Since it is not feasible to annotate the ∼ .5M captions of
MSCOCO with POS tags, we use an automatic part-of-
speech tagger.1

Classical Image Captioning. Classical techniques factor
the joint probabilistic model pθ(y|I) over all words into a
product of conditionals. They learn model parameters θ∗ by

Beam Search

Our POS

Expand
Top-k

Expand
Top-k

Expand
Top-k

.
.
.

k
-
p
o
t
 

w
e
n
o
t
 

 

s
y
a
r
r
a

 

e
z
i

s
-
k

 

x

 

k

 

e
g
r
e
M

.
.
.

.
.
.

Expand
Top-1

Expand
Top-1

Expand
Top-1

.
.
.

Figure 1: Illustration of beam search and POS-sampling to
expand the best-k captions (y1
i ) from word posi-
tion i to i + 1. See Sec. 3 for notation and other details.

i , . . . yk

i , y2

maximizing the likelihood over the training set D, i.e.,

X

log pθ(y|I), where pθ(y|I)=

Y

p(yi|y<i, I).

N

max

θ

(I,y)∈D

i=1

(1)
The factorization of the joint probability distribution en-
forces a temporal ordering of words. Hence, word yi at the
ith time-step (or word position) depends only on all previ-
ous words y<i. This probability model is represented using
a recurrent neural network or a feed-forward network with
temporal (or masked) convolutions. Particularly the latter,
i.e., temporal convolutions, have been used recently for dif-
ferent vision and language tasks in place of classical recur-
rent neural nets, e.g., [3, 9, 4].

During training, we learn the optimal parameters θ∗.
Then for test image I, conditional word-wise posterior
probabilities pθ∗ (yi|y<i, I) are generated sequentially from
i = 1 to N . Given these posteriors, beam search is applica-
ble and forms our baseline. Fig. 1 illustrates beam search
with a beam width of k from word position yi to yi+1.
Here, beam search maintains best-k (incomplete) captions
ordered by likelihood. It expands the best-k captions at ev-
ery word greedily from start to end of the sentence.

θ∗ (yi+1|yj

More speciﬁcally, for beam search from word position i,
we ﬁrst generate posteriors pj
<(i+1), I) based on
the current top-k list containing yj
<(i+1), j ∈ {1, . . . , k}.
We then obtain new top-k captions by expanding each of
the k entries yj
<(i+1) in the list using the computed posterior
pj
θ∗ (yi+1|yj
<(i+1), I). We call this ‘expand top-k.’ The time
complexity for a single expand top-k operation is identical
to obtaining the sorted top-k values from an array of size
|Y|.2 The time complexity of all expand top-k operations is
O(k2 + |Y|k log k).

1 See https://www.nltk.org/book/ch05.html for POS tag

2https://www.geeksforgeeks.org/

and automatic POS tagger details

k-largestor-smallest-elements-in-an-array/

10697

PART OF SPEECH CLASSIFICATION

CAPTIONING NETWORK

VGG­16

Distribution over 
 1024 exemplar  
POS templates

(Sampled Part of

Speech Tag Sequence)

LSTM

DET ADV ADJ NOUN ADP ADJ

NOUN  NOUN  

CONJ NOUN NOUN

1. Separate

Training (POS) 

 or 

    2. Gumbel     
     Softmax       
(POS+Joint) 

IMAGE REPRESENTATION

VGG­16

FC­7 Features

Object Vectors

Faster
RCNN

y< i  = a  very nice kitchen with stainless steel appliances and wood 

PREVIOUS WORDS

 
 

CNN  
(Temporal
Conv) 

yi  = cabinets

Figure 2: An illustration of our POS captioning method on a test image. For the image representation, fc7 features are
extracted from VGG-16 and embedded into 512 dimensional vectors. For object vectors, we use the 80 dimensional class
vector from faster rcnn [22] (same as [32]). For part-of-speech classiﬁcation, we use VGG-16 with two linear layers and a
1024-way softmax. Then, we encode sampled POS via an LSTM-net to a 512 dimensional vector. Our captioning network
uses temporal convolutions and operates on image representation, part-of-speech vector, object vector and previous words in
the caption (y<i) to produce the next word (yi). The network is trained for 20 epochs using the ADAM optimizer [13] (initial
learning rate of 5e−5 and a decay factor of .1 after 15 epochs). The part of speech classiﬁcation step can be trained separately
(POS) or jointly using a gumbel softmax (POS+Joint). Note, image representation is same for our method and baselines.

We merge all the expanded top-k captions to the ﬁnal
top-k captions using the log sum of the posterior probability
at word position i + 1. We call this operation merge. The
merge operation has a complexity of O(k + k log k), which
is identical to merging k sorted arrays.3 In Sec. 4, we show
that our inference with POS has better time complexity.

4. Image Captioning with Part-of-Speech

In our approach for image captioning, we introduce a
POS tag sequence t, to condition the recurrent model given
in Eq. (1). More formally, we use the distribution

pθ(y|t, I) =

N

Y

i=1

pθ(yi|t, y<i, I).

(2)

Following classical
techniques, we train our POS-
conditioned approach by maximizing the likelihood (similar
to Eq. (1)), i.e., we want to ﬁnd the parameters

θ∗ = argmax

θ

X

(I,t,y)∈D

log pθ(y|t, I).

(3)

Importantly, note that we use the entire POS tag sequence in
the conditional above, because it allows global control over
the entire sentence structure.

Training involves learning the parameters θ∗ for our
conditional captioning model (Eq. (3)). During test time,
conditioning on POS tags provides a mechanism for di-
verse image captioning, i.e., given a test image I, we ob-
tain k diverse captions by sampling k POS tag sequences

3https://www.geeksforgeeks.org/

merge-k-sorted-arrays/

t1, t2, . . . , tk. Note that every sequence is a tuple of POS
tags, i.e., ti = (ti

2, . . .), i ∈ {1, . . . , k}.

1, ti

Since a large number of possible POS tag sequences ex-
ists, in Sec. 4.1, we discuss how we obtain quantized POS
tag sequences q1, q2, . . . , qk given the input image. These
quantized sequences approximate the actual POS tag se-
quences t1, t2, . . . , tk.

Concretely, during inference we sample k quantized
POS tag sequences given the image. This is shown as the
part-of-speech classiﬁcation step in Fig. 2. Then, we encode
each sampled POS tag sequence q using an LSTM model.
The encoded POS tag sequence, along with object vector,
image features (fc7 of VGG-16) and previous words (y<i)
forms the input to the temporal convolutions-based caption-
ing network. This captioning network implements our pos-
terior probability pθ(yi|y<i, q, I), which is used to predict
the next word y∗

i = argmaxyi pθ(yi|y<i, q, I).

Fast inference with POS. For every sampled tag sequence
qj, j ∈ {1, 2, · · · , k} (i.e. quantization of tag sequence tj ),
we maximize the learned probabilistic model, i.e., yj
i =
argmaxy pθ∗ (yi|y<i, qj, I) greedily. As just discussed, we
simply use the maximum probability word at every word
position. Fig. 1 compares this computationally much more
effective method, which has a time complexity of O(k|Y|),
to the breadth ﬁrst approach employed by beam search.

Note that POS-based sampling requires only a single
max-operation at every step during inference (our effec-
tive beam size is 1), making it faster than beam search with
wide beams. It is also faster than diverse beam search (with
group size parameter set to 1 as in our results) which per-
forms the k ‘expand top-k’ operations sequentially using an
augmented diversity function.

10698

Method

Beam size
or #samples

Beam search
Div-BS [28]
AG-CVAE [32]
POS
POS+Joint

Beam Search
Div-BS [28]
AG-CVAE [32]
POS
POS+Joint

20

100

S

R

C

M

B3

B2

B1

Best-1 Oracle Accuracy

0.538×
0.573
0.593
0.581

Speed
(s/img)
B4
0.489X 0.626X 0.752X 0.875X 1.595X 0.698X 0.402X 0.284X 3.74×
0.383×
3.42
-
0.471
0.21
0.449
0.20X
0.431
0.641X 0.742X 0.835X 0.931X 1.904X 0.772X 0.482X 0.332X 20.33
0.402×
19.05
-
0.557
1.29
0.578
1.27X
0.550

0.666×
0.690
0.739
0.725

1.448×
1.517
1.710
1.661

0.687×
0.698
0.737
0.721

0.846×
0.883
0.921
0.909

0.698×
0.767
0.802
0.787

0.555×
0.654
0.689
0.672

0.269
0.244×
0.277
0.271

0.357
0.309×
0.365
0.357

0.290
0.277×
0.322
0.311

0.653
0.638×
0.678
0.670

0.372
0.345×
0.423
0.409

1.405
1.308×
1.468
1.448

0.837
0.834×
0.874
0.865

Speed Accuracy

×
×
-
X

X

×
×
-
X

X

X
×
×
X

X

X
×
×
X

X

Table 2: Best-1 accuracy by oracle re-ranking. Our POS methods are faster at sampling than beam search and they also
generate a higher scoring best-1 caption than AG-CVAE [32] and Div-BS [28]. Beam search obtains the best scores, however
it is slow. From all sampled captions (#samples = 20 or 100), we use oracle to pick the best-1 caption for every metric. This
gives an estimate of the upper bound on captioning accuracy for each method. We use standard captioning metrics, BLEU
(B1-B4) [21], CIDEr (C) [27], ROUGE (R) [15], METEOR (M) [6] and SPICE (S) [1]. Note, Xindicates good performance
on the metric for the corresponding column and × indicates bad performance.

4.1. Image to Part of Speech Classiﬁcation

Because our model conditions sentence probabilities on
a POS tag sequence, we need to compute it before perform-
ing inference. Several ways exist to obtain the POS tag se-
quence. E.g., choosing a POS tag sequence by hand, sam-
pling from a distribution of POS tag sequences seen in the
dataset D, or predicting POS tag sequences conditioned on
the observed image I. The ﬁrst one is not scalable. The sec-
ond approach of sampling from D without considering the
provided image is easy, but generates inaccurate captions.
We found the third approach to yield most accurate results.
While this seems like an odd task at ﬁrst, our experiments
suggest very strongly that image based prediction of POS
tag sequences works rather well. Indeed, intuitively, infer-
ring a POS tag sequence from an image is similar to pre-
dicting a situation template [36] – one must predict a rough
template sketching what is worth to be said about an image.

To capture multi-modality, we use a classiﬁcation model
to compute our POS predictions for a given image I. How-
ever, we ﬁnd that there are > 210K POS tag sequences in
our training dataset D of |D| > 500K captions. To main-
tain efﬁciency, we therefore quantize the space of POS tag
sequences to 1024 exemplars as discussed subsequently.

Quantizing POS tag sequences. We perform a hamming
distance based k-medoids clustering to obtain 1024-cluster
centers. We use concatenated 1-hot encodings (of POS tags)
to encode the POS tag sequence. We observe our clusters
to be tight, i.e., more than 75% of the clusters have an av-
erage hamming distance less than 3. We use the cluster
medoids as the quantized POS tag sequences for our clas-
siﬁer. Given an input tag sequence t we represent it using
its nearest neighbor in quantized space, which we denote by
q = Q(t). Note, in our notation the quantization function
Q(t), reduces t to its quantized tag sequence q.

Our image to part-of-speech classiﬁer (shown in Fig. 2)
learns to predict over quantized POS sequence space by

maximizing the likelihood, pφ(q|I). Formally, we look for
its optimal parameters φ∗ via

φ∗ = argmax

X

(I,t)∈D

log pφ(q|I),

(4)

where log pφ(q|I) =

φ

1024

P

i=1

δ[qi = Q(t)] log pφ(qi|I).

4.2. Separate vs. Joint Training

Training involves learning the parameters θ of the cap-
tioning network (Eq. (3)) and the parameters φ of the POS
classiﬁcation network (Eq. (4)). We can trivially train these
two networks separately and we call this method POS.

We also experiment with joint training by sampling from
the predicted POS posterior pφ(t|I) using a Gumbel soft-
max [11] before subsequently using its output in the cap-
tioning network. Inconsistencies between sampled POS se-
quence and corresponding caption y will introduce noise
since the ground-truth caption y may be incompatible with
the sampled sequence q. Therefore, during every training it-
eration, we sample 50 POS tag sequences from the Gumbel
soft-max and only pick the one q with the best alignment
to POS tagging of caption y. We refer to this form of joint
training via POS+Joint. In Sec. 5.1 and Sec. 5.2, we show
that POS+Joint (i.e., jointly learning θ and φ) is useful and
produces more accurate captions.

5. Results

In the following, we compare our developed approach
for diverse captioning with POS tags to competing baselines
for diverse captioning. We ﬁrst provide information about
the dataset, the baselines and the evaluation metrics before
presenting our results.
Dataset. We use the MS COCO dataset [16] for our ex-
periments. For the train/val/test splits we follow: (1) M-
RNN [20] using 118,287 images for training, 4,000 images

10699

Method

Beam size
or #samples

B4

B3

Best-1 Consensus Re-ranking Accuracy
B2

B1

C

R

M

S

Speed
(s/img)

Speed Accuracy

Beam search
(w. Likelihood)
Beam search
Div-BS [28]
AG-CVAE [32]
POS
POS+Joint

Beam search
(w. Likelihood)
Beam search
Div-BS [28]
AG-CVAE [32]
POS
POS+Joint

20

100

0.305

0.402×

0.538×

0.709×

0.947×

0.523

0.248

0.175

3.19

0.423

1.018
0.319
0.564
1.032X 0.536
0.320X 0.424X 0.562
0.299×
0.963
0.544
0.570X 0.744X 1.014
0.306
0.305
0.563
1.020

0.402×
0.419
0.415

0.733
0.729
0.716

0.518×
0.531
0.531

0.737

0.185
0.255X 0.184
0.237×
0.252
0.251

0.173×
0.188X 1.13X
1.13X
0.185

7.41
7.60×
-

0.537X 0.255

0.300×

0.397×

0.532×

0.703×

0.937×

0.519×

0.246

0.174×

18.24

0.558

0.419

0.729
0.317
0.325X 0.430X 0.569X 0.734
0.732
0.311
0.311
0.737
0.316

0.186
0.532
0.538X 0.255X 0.187
0.245×
0.179
0.528
0.559
0.188X 7.54
0.253
0.567
0.530
0.569X 0.739X 1.045X 0.532
0.255X 0.188X 7.32

1.020
1.034
1.001
1.036

0.417
0.421
0.425

0.253

40.39×
39.71
-

×

×
×
-
X

X

×

×
×
-
X

X

×

X

X
×
X

X

×

X

X
×
X

X

Table 3: Best-1 accuracy by consensus re-ranking. Our POS methods obtain higher scores on captioning metrics than
AG-CVAE [32]. This demonstrates our POS natural language prior is more useful than the abstract latent vector used by
VAE-based methods. POS methods obtain comparable accuracy to Beam Search and Div-BS [28], and they are more com-
putationally efﬁcient at sampling (i.e., high speed). Note, we also outperform the standard beam search that uses likelihood
based ranking. For these results, consensus re-ranking [7] is used to pick the best-1 caption from all sampled captions (unless
‘w. Likelihood’ is speciﬁed). For fair comparison, each method uses the same 80-dimensional object vector from faster
rccn [23] and the same image features/parameters for consensus re-ranking. The captioning metrics are the same as in Tab. 2.
Note, Xindicates good performance on the metric for the corresponding column and × indicates bad performance.

Method

#samples Meteor Spice

5
5
5

.175
.166
.180

.247
.236
.247

Beam Search (with VGG-16)
GAN (with Resnet-152)
POS+Joint (with VGG-16) [25]
Table 4: Comparison to GAN-based method. To com-
pare to GAN, we train our POS+Joint on another split
of MSCOCO by Karpathy et al. [12]. Our POS+Joint
method samples more accurate best-1 captions than the
GAN method. POS+Joint also obtains better SPICE score
on this split compared to beam search. Our accuracy may
improve with the use of Resnet-152 features. For fair com-
parison, we use the same 80-dimensional object vectors
from faster rcnn [23] and rank the generated captions with
likelihood for all methods.

for validation, and 1,000 images for testing; and (2) Karpa-
thy et al. [12] using 113,287 images for training, 5,000 im-
ages for validation and 5,000 images for testing. The latter
split is used to compare to GAN-based results in Tab. 4.
Methods. In the results, we denote our approach by POS,
and our approach with joint training by POS+Joint (see
Sec. 4.2 for the differences). We compare to the addi-
tive Gaussian conditional VAE-based diverse captioning
method of Wang et al. [32], denoted by AG-CVAE. Our
captioning network is based on [3]. For a fair compari-
son to beam search we also compare to convolutional cap-
tioning [3] with beam search. This is referred to as beam
search. We compare to diverse beam search denoted de-
noted by Div-BS. The abbreviation GAN is used to denote
the GAN-based method in [25].
Evaluation criteria. We compare all methods using four
criteria – accuracy, diversity, speed, human perception:

• Accuracy.

In Sec. 5.1 (Best-1 Accuracy) we com-

pare the accuracy using the standard image captioning
task of generating a single caption. Subsequently, in
Sec. 5.2 (Best-kth Accuracy), we assess the accuracy
of k captions on different image captioning metrics.

• Diversity. We evaluate the performance of each

method on different diversity metrics in Sec. 5.3.

• Speed. In addition to accuracy, in Sec. 5.4, we also
measure the computational efﬁciency of each method
for sampling multiple captions.

• Human perception. We do a user study in Sec. 5.5.

5.1. Best-1 Accuracy

We use two ranking methods – oracle and consensus re-
ranking – on the set of generated captions and pick the best-
1 caption. Our results for oracle re-ranking in Tab. 2 and
for consensus re-ranking in Tab. 3 show that, beam search
and diverse beam search are accurate however slow. POS
is both fast and accurate. POS outperforms the accuracy of
AG-CVAE.
Oracle re-ranking. The reference captions of the test set
are used and the generated caption with the maximum score
for each metric is chosen as best-1 (as also used in [32]).
This metric permits to assess the best caption for each met-
ric and the score provides an upper bound on the achievable
best-1 accuracy. Higher oracle scores are also indicative of
the method being a good search method in the space of cap-
tions. Results in Tab. 2 show that beam search obtains the
best oracle scores. However, it is painfully slow (∼ 20s
per image to sample 100 captions). POS, POS+Joint obtain
higher accuracy than AG-CVAE and comparable accuracy
to beam search with faster runtime.
Consensus re-ranking scores. In a practical test setting,

10700

(a) Best-10 CIDEr from 20 samples (b) Best-10 SPICE from 20 samples (c) Best-10 CIDEr from 100 samples (d) Best-10 SPICE from 100 samples

Figure 3: Best-10 CIDEr and SPICE accuracy. Our POS and POS+Joint achieve best-k accuracy comparable to Beam
Search and Div-BS [28] with faster computation time. We outperform the best-k scores of AG-CVAE [32], demonstrating
part-of-speech conditioning is better than abstract latent variables of a VAE. Note, this ﬁgure is best viewed in high-resolution.

Beam size Distinct # Novel sentences mBleu-4 n-gram Diversity (Best-5) Overall Diversity
or #samples Captions

(Best-5) Div-1

(Best-5)

Div-2

Method

Beam search
Div-BS [28]
AG-CVAE [32]
POS
POS+Joint

Beam search
Div-BS [28]
AG-CVAE [32]
POS
POS+Joint

20

100

Human

5

100%
100%
69.8%
96.3%
77.9%

100%
100%
47.4%
91.5%
58.1%

99.8%

2317
3106
3189
3394
3409

2299
3421
3069
3446
3427

-

0.777
0.813
0.666
0.639
0.662

0.781
0.824
0.706
0.673
0.703

0.510

0.21
0.20
0.24
0.24
0.23

0.21
0.20
0.23
0.23
0.22

0.34

0.29
0.26
0.34
0.35
0.33

0.28
0.25
0.32
0.33
0.31

0.48

×
×
X

X

X

×
×
X

X

X

Table 5: Diversity statistics. For each method, we report the number of novel sentences (i.e., sentences not seen in the
training set) out of at most best-5 sentences after consensus re-ranking. Though Beam Search showed high accuracy in
Tab. 2, 3 and Fig. 3, here, we see that it produces less number of novel sentences than our POS methods. Therefore, beam
search is more prone to regurgitating training data. Low mBleu-4 indicates lower 4-gram overlap between generated captions
and more diversity in generated captions. POS has the lowest mBleu-4 and therefore high diversity in generated captions.
For details on other metrics see Sec. 5.3.

reference captions of the test set won’t be available to rank
the best k captions and obtain best-1. Therefore, in con-
sensus re-ranking, the reference captions of training images
similar to the test image are retrieved. The generated cap-
tions are ranked via the CIDEr score computed with respect
to the retrieved reference set [7].

We use the same image features [31] and parameters
for consensus re-ranking as [32]. Tab. 3 shows that our
methods POS and POS+Joint outperform the AG-CVAE
baseline on all metrics. Moreover, our methods are faster
than beam search and diverse beam search. They pro-
duce higher CIDEr, Bleu-1,2, METEOR and SPICE scores.
Other scores are comparable and differ in the 3rd decimal.
Note, our POS+Joint achieves better scores than POS, espe-
cially for 100 samples. This demonstrates that joint training
is useful.

We also train our POS+Joint method on the train/test
split of Karpathy et al. [12] used by the GAN method [25].
In Tab. 4, we show that we obtain higher METEOR and
SPICE scores than those reported in [25].

Baseline Method

POS Wins Baseline Method Wins

57.7%
45.3%
64.8%

42.2%
54.6%
35.1%

Beam search
Diverse beam search [28]
AG-CVAE [32]
Table 6: We show the user captions sampled from best-k
(same kth ranked, k = 1 to 5) for baseline methods and our
POS. The user is allowed to pick the caption that best de-
scribes the image. Note, user is not aware of the method
that generated the caption. Here, we observe that our POS
method outperforms Beam search and AG-CVAE on our
user study. Our user study has 123 participants with on av-
erage 23.3 caption pairs annotated by each user.

5.2. Best kth Accuracy

Our captioning method can be conditioned on different
part-of-speech tags to generate diverse captions. For diverse
image captioning, in addition to best-1 accuracy, best-kth
accuracy should also be measured. Best-kth accuracy is the
score of the kth ranked caption, therefore it is lower than the
best-1 score. All k generated captions should be accurate
and therefore it is desirable to have high best-kth scores.
This metric has not been reported previously [25, 32].

10701

123456789kth ranked caption0.60.70.80.91.0CIDEr scoreCIDEr score of consensus re-ranked best-10 captionsPOSPOS+JointAG-CVAEBeam SearchDiv-BS123456789kth ranked caption0.130.140.150.160.170.180.19SPICE scoreSPICE score of consensus re-ranked best-10 captionsPOSPOS+JointAG-CVAEBeam SearchDiv-BS123456789kth ranked caption0.800.850.900.951.001.05CIDEr scoreCIDEr score of consensus re-ranked best-10 captionsPOSPOS+JointAG-CVAEBeam SearchDiv-BS123456789kth ranked caption0.1550.1600.1650.1700.1750.1800.1850.190SPICE scoreSPICE score of consensus re-ranked best-10 captionsPOSPOS+JointAG-CVAEBeam SearchDiv-BSPOS:

Beam Search:

- two people are standing  on the back  of an elephant.
- a man and  a woman are on the back  of an elephant.
- two people  standing  on top of an elephant.
- a group of people riding an elephant  in a park.
- two people  are riding  an elephant  on a dirt road.

Diverse Beam Search:

- two people are standing  next to an  elephant.
- a couple  of people standing  next  to an elephant.
- a couple  of people  standing  next  to an elephant  on
a dirt road.
- a couple  of people  that are standing  next to an 
elephant.
- a couple  of people  standing  next  to an elephant  on 
top of a.

- a couple  of people  standing  on top of an elephant.
- a couple  of people  are standing  on  top of an 
elephant.
- a man and  a woman standing  next  to an elephant.
- a man and  woman  standing  next  to an elephant.
- a group of people  standing  next to an elephant.

AG-CVAE:

- a group of people riding on top of an elephant.
- a man and  a man riding on top  of an  elephant.
- a large group  of people  riding  on top of an 
elephant.
- a man riding on the  back  of a elephant.
- a group of people  standing  on top of an 
elephant.

POS:

Beam Search:

- a rear view mirror on the  side of a car window.
- a side view  mirror of a car with a bird on the
window.
- a rear view mirror hanging  on  the side of a car.
- a side view  of a car with  birds in the side  mirror.
- a view of a mirror of a car looking  at a mirror.

Diverse Beam Search:

- a close  up of a bird on a car mirror. 
- a bird is sticking  its head out of a car window. 
- a close  up of a bird on a car. 
- a close  up of a bird on the back  of a car. 
- a bird that is sitting in the back  of a car.

- a reflection  of a bird on the  back  of a truck. 
- a close  up of a bird on the back  of a vehicle. 
- a bird is perched on the  back of a car. 
- a bird is sitting  in the  seat  of a car. 
- a bird that is sitting  in the back  seat of a car.

AG-CVAE:

- a dog is looking  out the window  of a car.
- a dog is sitting in the window  of a car.
- a small bird sitting  on the  side of a car.
- a dog sitting on the side  of a car.
- a bird sitting on the  back  of a car.

(a) Qualitative Comparison

(b) Diversity (or Overlap) Comparison

Figure 4: In ﬁgure on left, notice POS captions contain things like rear/side view mirror, dirt road, the quantiﬁer ‘two’ which
is less common in other methods. The inaccuracies are highlighted in red and the novel parts in green. In ﬁgure on right, we
compare the diversity (or overlap) of captions. The mBleu-4 score measures 4-gram overlap between one generated caption
and the rest. Lower is better, e.g., 0 means caption has no 4-gram overlap to other sentences. POS is better than BS and
Div-BS in the plots above (lower mBleu-4 scores). Note, ground-truth 5 captions all have 0 overlap to each other for this
example. On our 1000 image test set with 10 captions generated per image, POS generates 10.94% sentences with 0 overlap;
in contrast Div-BS generates 1.02% and Beam Search 2.4%. Figure best viewed in high-resolution.

In Fig. 3, we compare best-kth (k = 1 to 10) scores for
all methods. Note, the accuracy of AG-CVAE drops dras-
tically on both CIDEr and Spice, while our POS methods
maintain accuracy comparable to beam search. This proves
that our POS image summaries are better at sampling accu-
rate captions than the abstract latent variables of a VAE.

5.3. Evaluation of Diversity

In Tab. 5 we compare methods on diversity metrics.

(1) Uniqueness. The number of unique sentences gener-
ated after sampling. Beam search and diverse beam search
always sample a unique sentence. Note, our POS also sam-
ples a high number of unique sentences 19.26 (96.3%) out
of 20, 91.55 out of 100. The uniqueness reduces for joint
training. This is because, generation of a caption while
training POS+Joint is based on a noisy POS tag sequence
sampled from the Gumbel softmax. Therefore, the caption
may not be compatible with this noisy POS tag sequence
which leads to an overly smooth latent representation for
the POS tag. Therefore, different POS tags may produce
the same latent code and hence the same caption.
(2) Novel sentences. We measure the number of novel sen-
tences (not seen in train), and ﬁnd that our POS-based meth-
ods produce more novel sentences than all other methods.
Beam search produces the least number of novel sentences.
(3) Mutual overlap. We also measure the mutual overlap
between generated captions. This is done by taking one cap-
tion out of k generated captions and evaluating the average
Bleu-4 with respect to all other k − 1 captions. Lower value
indicates higher diversity. POS is the most diverse. Note,
the average score is computed by picking every caption vs.

the remaining k − 1 captions.
(4) n-gram diversity (div-n). We measure the ratio of dis-
tinct n-grams per caption to the total number of words gen-
erated per image. POS outperforms other methods.

5.4. Speed

In Fig. 1 we showed that our POS based methods have
better time complexity than beam search and diverse beam
search. The time complexity of our POS-based approach
is the same as sampling from a VAE or GAN, provided the
max probability word is chosen at each word position (as
we do). The empirical results in Tab. 2 and Tab. 3 show that
POS methods are 5× faster than beam search methods.

5.5. User Study

Fig. 4 compares the captions generated by different
methods and in Tab. 6, we provide the results of a user study.
A user is shown two captions sampled from two different
methods. The user is asked to pick the more appropriate
image caption. Tab. 6 summarizes our results. We observe
POS outperforms AG-CVAE and Beam search.

6. Conclusion

The developed diverse image captioning approach con-
ditions on part-of-speech. It obtains higher accuracy (best-
1 and best-10) than GAN and VAE-based methods and
is computationally more efﬁcient than the classical beam
search.
It performs better on different diversity metrics
compared to other methods.
Acknowledgments: Supported by NSF Grant No. 1718221 and
ONR MURI Award N00014-16-1-2007.

10702

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In ECCV,
2016. 5

[2] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down attention
for image captioning and visual question answering. arXiv
preprint arXiv:1707.07998, 2017. 1, 2

[3] J. Aneja, A. Deshpande, and A. Schwing. Convolutional im-
age captioning. In Computer Vision and Pattern Recognition,
2018. 1, 2, 3, 6

[4] S. Bai, J. Z. Kolter, and V. Koltun. An empirical evaluation
of generic convolutional and recurrent networks for sequence
modeling. CoRR, abs/1803.01271, 2018. 3

[5] B. Dai, S. Fidler, R. Urtasun, and D. Lin. Towards diverse
In

and natural image descriptions via a conditional gan.
ICCV, 2017. 1, 2

[6] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language.
In
Proceedings of the EACL 2014 Workshop on Statistical Ma-
chine Translation, 2014. 5

[7] J. Devlin, S. Gupta, R. B. Girshick, M. Mitchell, and C. L.
Zitnick. Exploring nearest neighbor approaches for image
captioning. CoRR, abs/1505.04467, 2015. 3, 6, 7

[8] J. R. Finkel, C. D. Manning, and A. Y. Ng. Solving the prob-
lem of cascading errors: Approximate bayesian inference for
linguistic annotation pipelines. In Proceedings of the 2006
Conference on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’06, 2006. 1

[9] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N.
Dauphin. Convolutional sequence to sequence learning.
CoRR, abs/1705.03122, 2017. 3

[10] K. Gimpel, D. Batra, C. Dyer, and G. Shakhnarovich. A
systematic exploration of diversity in machine translation. In
In Proc. of EMNLP, 2013. 1

[11] E. Jang, S. Gu, and B. Poole. Categorical reparameterization

with gumbel-softmax. 2017. 5

[12] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions.
In 2015 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3128–3137, June 2015. 1, 2, 6, 7

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. abs/1412.6980, 2014. 4

[14] D. Li, X. He, Q. Huang, M.-T. Sun, and L. Zhang. Generat-
ing diverse and accurate visual captions by comparative ad-
versarial learning. arXiv preprint arXiv:1804.00861, 2018.
1, 2

[15] C.-Y. Lin. Rouge: a package for automatic evaluation of

summaries. July 2004. 5

[16] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon Objects in Context, pages 740–755. Springer Interna-
tional Publishing, Cham, 2014. 5

[17] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy. Im-
proved image captioning via policy gradient optimization of
spider. arXiv preprint arXiv:1612.00370, 2016. 2

[18] J. Lu, J. Yang, D. Batra, and D. Parikh. Neural baby talk,

2018. 2

[19] R. Luo, B. Price, S. Cohen, and G. Shakhnarovich. Discrim-
inability objective for training descriptive captions. arXiv
preprint arXiv:1803.04376, 2018. 2

[20] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). ICLR, 2015. 1, 5

[21] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu:
A method for automatic evaluation of machine translation.
In Proceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA, 2002. Association for Computational
Linguistics. 5

[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works.
In Neural Information Processing Systems (NIPS),
2015. 4

[23] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Proceedings of the 28th International Conference on Neural
Information Processing Systems - Volume 1, NIPS’15, pages
91–99, Cambridge, MA, USA, 2015. MIT Press. 6

[24] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
In

Self-critical sequence training for image captioning.
CVPR, 2017. 2

[25] R. Shetty, M. Rohrbach, L. A. Hendricks, M. Fritz, and
B. Schiele. Speaking the same language: Matching machine
to human captions by adversarial training.
In Proceedings
of the IEEE International Conference on Computer Vision
(ICCV), 2017. 1, 2, 3, 6, 7

[26] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 2

[27] R. Vedantam, C. L. Zitnick, and D. Parikh.

Consensus-based image description evaluation.
pages 4566–4575. IEEE Computer Society, 2015. 5

Cider:
In CVPR,

[28] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun,
S. Lee, D. J. Crandall, and D. Batra. Diverse beam search
for improved description of complex scenes. In Proceedings
of the Thirty-Second AAAI Conference on Artiﬁcial Intelli-
gence, New Orleans, Louisiana, USA, February 2-7, 2018,
2018. 1, 2, 5, 6, 7

[29] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: Lessons learned from the 2015 mscoco image caption-
ing challenge. IEEE Trans. Pattern Anal. Mach. Intell., Apr.
2017. 1, 2

[30] J. Wang, P. S. Madhyastha, and L. Specia. Object counts!
bringing explicit detections back into image captioning. In
Proceedings of the North American Chapter of the Associa-
tion of Computational Linguistics: Human Language Tech-
nologies (NAACL HLT). Association for Computational Lin-
guistics, 2018. 2

[31] L. Wang, Y. Li, and S. Lazebnik. Learning two-branch
neural networks for image-text matching tasks. CoRR,
abs/1704.03470, 2017. 7

[32] L. Wang, A. G. Schwing, and S. Lazebnik. Diverse and accu-
rate image description using a variational auto-encoder with

10703

an additive gaussian encoding space. In Advances in Neural
Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pages 5758–5768, 2017. 1, 2,
3, 4, 5, 6, 7

[33] Q. Wang and A. B. Chan. Cnn+cnn: Convolutional decoders

for image captioning. CoRR, abs/1805.09019, 2018. 1, 2

[34] K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention.
In
Proceedings of the 32Nd International Conference on In-
ternational Conference on Machine Learning - Volume 37,
ICML’15, pages 2048–2057. JMLR.org, 2015. 1, 2

[35] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei. Boosting image
captioning with attributes. In IEEE International Conference
on Computer Vision, ICCV 2017, Venice, Italy, October 22-
29, 2017, pages 4904–4912, 2017. 2

[36] M. Yatskar, L. Zettlemoyer, and A. Farhadi. Situation recog-
nition: Visual semantic role labeling for image understand-
ing. In Conference on Computer Vision and Pattern Recog-
nition, 2016. 5

10704

