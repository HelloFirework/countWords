Wide-Context Semantic Image Extrapolation

Yi Wang1

,

2 Xin Tao2 Xiaoyong Shen2

Jiaya Jia1

2

,

1The Chinese University of Hong Kong

2YouTu Lab, Tencent

yiwang@cse.cuhk.edu.hk

{xintao, dylanshen, jiayajia}@tencent.com

Abstract

This paper studies the fundamental problem of extrapo-
lating visual context using deep generative models, i.e., ex-
tending image borders with plausible structure and details.
This seemingly easy task actually faces many crucial techni-
cal challenges and has its unique properties. The two major
issues are size expansion and one-side constraints. We pro-
pose a semantic regeneration network with several special
contributions and use multiple spatial related losses to ad-
dress these issues. Our results contain consistent structures
and high-quality textures. Extensive experiments are con-
ducted on various possible alternatives and related meth-
ods. We also explore the potential of our method for var-
ious interesting applications that can beneﬁt research in a
variety of ﬁelds.

1. Introduction

Humans have the natural ability to perceive unseen sur-
roundings based on limited visual content. For computer
vision, accomplishing this task requires generating seman-
tically meaningful and consistent structure and texture. In
this paper, we focus on the special task to infer unseen con-
tent outside image boundaries.

This task ﬁnds several related methods and topics in
image processing and graphics.
It was treated as an in-
triguing application in view expansion [35, 43, 49], im-
age editing [2], texture synthesis [10, 11, 41], to name a
few. These methods exploit information from either exter-
nal images or internal statistics. For example, algorithms
of [35, 43, 49] enlarge the view by matching and stitch-
ing similar candidates. Another line [15] uses retarget-
ing. It is also a natural choice to use inpainting methods
[1, 5, 7, 20, 23, 25, 37] for extrapolating images. We note
that these methods are not specially designed for our task
and thus have their respective limitations when applied to
content generation. External-image-based algorithms re-
quire a large amount of or structurally very similar reference
images while internal pixels/patches-based methods mostly
produce apparently similar or repeated patterns.

Figure 1. Illustration of our pursuit with examples of face, dog,
bird, and human body, which are all highly semantically sensitive
and representative.

Different from the results shown in previous work, the
illustration in Figure 1 indicates that our method has its
unique and strong capability. It can generate the full portrait
with hair and background even from a small part of faces
(top row of Figure 1), create bird head and tail based on
body shape (bottom left of Figure 1), or produce a full hu-
man body given only upper body information (bottom right
of Figure 1). Note that in all these examples, the algorithm
needs to suitably take vastly different context of each in-
complete image into account and predict up to 3 times more
unknown pixels than known ones.

In regard to technical strategies, deep learning becomes
popular and effective in low-level vision [8, 26, 39, 46, 48].
Applying it to this context generation task, however, still
needs to consider the following two issues.

Image Size Change
Image expansion extends image size
beyond boundaries. A similar task is super-resolution
[8, 24, 36, 38], which produces high-res (HR) results from
low-res (LR) input. Current SR frameworks either upsam-
ple input before fed into networks [8], or use spatial expan-
sion modules [24, 36, 38] within the network. So the ﬁrst
issue to conquer in our framework is to properly increase
size with structure and detail generation.

One-sided Constraints The boundary condition in con-
text generation has only one side, as illustrated in Figure 1
where black arrows show inference direction. This con-
ﬁguration is different from that of general image-to-image

1399

translation (e.g. image synthesis, deblur), where the latter
has a one-to-one spatial correspondence between the pre-
diction and input. The unknown pixels away from image
border are less constrained than those near border, poten-
tially accumulating errors or repeated patterns. To deal with
it, we design the relative spatial variant loss, context adver-
sarial loss, and context normalization to regularize the gen-
eration procedure.

Our Contribution To address these key issues, we pro-
pose a Semantic Regeneration Network (SRN) to regen-
erate the full object from a small portion of visual clues.
SRN can generate arbitrary-size semantic structure beyond
image boundary without training multiple models.
It di-
rectly learns semantic features from small-size input, which
is both effective and efﬁcient by avoiding bias in common
padding and upsampling procedures [33, 40, 26].

In the structure level, SRN contains two components of
Feature Expansion Network (FEN) and Context Prediction
Network (CPN). FEN takes small-size images as input and
extracts features. Such features and extrapolation indicator
are fed to CPN for reconstructing ﬁnal expansion results.
With the separation of feature extraction and image recon-
struction, learning and inference of our network becomes
appropriate and efﬁcient. Further, the designed losses and
other processing modules adapt our network to one-sided
constraints, generating semantically meaningful structure
and natural texture. Our major contribution is twofold.

• We propose an effective deep generative model SRN
for image extrapolation. Practical context normaliza-
tion (CN) module and relative spatial variant (RSV)
loss are proposed. They are evaluated along with sev-
eral other alternatives.

• We apply our solution to various intriguing and impor-

tant applications.

2. Related Work

2.1. Image Extrapolation

Prior extrapolation solutions [35, 43, 49] usually turn to
an external library for solutions in a data-driven manner.
This type of methods formulates the problem into match-
ing and stitching, where the new content is retrieved from a
pre-constructed dataset. For example, Wang et al. [43] ex-
ploited this method on the graph representation of images.
They retrieve candidate images by subgraph matching, and
stitch these wrapped images into the input. Shan et al. for-
mulated the image composition into a MRF problem, able to
process a large library with high robustness regarding view-
point, appearance, and layout variation [35]. Zhang et al.
[49], with the retrieved large image candidate, aligned the
small input and candidate. The relative position between
similar patches in the known and unknown regions of the

candidate is applied to the input in a copy-and-paste man-
ner. As a non-parametric method, data-driven image extrap-
olation is limited by the used dataset. Moreover, sophisti-
cated or ﬁne textures along expanding boundary hinder the
application of this type of methods.

2.2. Conditional Image Generation

Image extrapolation belongs to conditional image gen-
eration in deep learning. The most related problem is
inpainting. Recent advance in inpainting lies in apply-
ing deep generative models to repair large missing pieces
[47, 46, 31, 48, 44]. Pathak et al. [31] ﬁrst applied adver-
sarial loss to learn an encoder-decoder network. To create
realistic textures based on given context, MRF-based style
transfer via patch matching in the deep feature space was
employed as post-processing [46]. Further, Yu et al. [48]
proposed the contextual attention layer, which replaces deep
features with its neighborhood weighted average and im-
proves both texture quality and inference efﬁciency. The
other related topic is image retargeting [34, 3].
In [3], a
CNN was designed to learn the shift map for each pixel.
Salient objects are preserved while background is seam-
lessly modiﬁed. Retargeting has no intention to extend sur-
rounding content.

2.3. Spatial Expansion Operators

Spatial expansion operators are indispensable compo-
nents in various tasks, when output is with a larger size.
Prevalent spatial expansion operators include padding, in-
terpolation, deconvolution [30, 9], sub-pixel convolution
[36], and a warping-based SPMC module [38]. We discuss
and experiment with these operators except SPMC in Sec-
tion 4 since SPMC only works with sequential input.

3. Our Method

×w′

Given an input image X ∈ Rh×w×c and ﬁlling margin
m = (top, lef t, bottom, right), semantic image expansion
(or extrapolation) intends to generate a visually convincing
image ˆY ∈ Rh′
×c, where h′ = h + top + bottom, w′ =
w+lef t+right, and X is a sub-image of ˆY. Contrary to the
inpainting process, which ﬁlls interior holes of an image,
image extrapolation is meant to expand image borders. For
convenience, we denote h′ = r1h and w′ = r2w (where
r1 ≥ 1, r2 ≥ 1, and r1r2 > 1).

3.1. Framework Design

Our model G consists of two sub-networks of feature
expansion network (FEN) and context prediction network
(CPN), as shown in Figure 2. FEN extracts deep features
from the given image, and CPN decodes these features into
images considering ﬁlling margin and size. The input to our
network contains an image X and a margin variable m =
(top, lef t, bottom, right) indicating extension.

1400

Figure 2. Our framework illustration.

3.1.1 Feature Expansion

This module employs an encoder-decoder-like structure,
where input is only X of size h × w × c, and output is its
feature maps f (X) of size r1h × r2w × c′. Increasing the
feature size is realized by nearest-neighbor upsampling fol-
lowed by convolution, except the last layer, which is other-
wise achieved by a sub-pixel convolution [36] variant. It is
a vanilla convolution followed by reshufﬂing feature chan-
nels. Given a feature map F ∈ Rh×w×r1r2c′
, such reshuf-
ﬂing operation s(·) is deﬁned as

s(F )i,j,k = F⌊i/r1⌋,⌊j/r2⌋,c′r2·mod(i,r1)+c′·mod(j,r2)+k,

(1)

.

where s(F ) ∈ Rr1h×r2w×c′
i, j, and k denote index
height, width, and channel, respectively. Compared with
the original sub-pixel convolution [36], the presented vari-
ant relaxes the constraint that r1 = r2. It handles scenarios
when r1 6= r2 while the method of [36] cannot. This ability
is useful in human body generation (r1 = 4 and r2 = 1
in Figure 1) and view expansion (r1 = 1 and r2 = 2 in
Section 4).

We discuss and compare alternative trainable opera-
tors, i.e., deconvolution layer and convolution after padding
(termed as unfold operator in the following) or interpola-
tion. Here deconvolution is not considered since it causes
visual artifacts in generation due to the overlap problem
[30, 9]. Interpolation or padding methods have their respec-
tive properties. Speciﬁcally, interpolation assumes that the
ﬁlling region is similar to that in the corresponding loca-
tion of the input; zero padding assumes a constant value for
missing part; symmetric/mirror padding makes the context
feature the mirror version along the image border. Compar-
ing with deconvolution and unfold, sub-pixel convolution
expands features with less bias. This is experimentally val-
idated in Session 4.3.

Feature Expansion Network (FEN) is to learn latent con-
text features. Experimental results show that ﬁlled pixels in
early batches serve as a kind of prior for later generation.
Computation directly conditioned on available pixels could

yield better performance in terms of both ﬁdelity and visual
naturalness [33, 40, 26]. Thus, our model directly infers
upon the given visual data without predeﬁned priors.

3.1.2 Context Prediction

We also use encoder-decoder-like network for this compo-
nent. The input is the concatenation of f (X) and ﬁlling
indicator, i.e. a binary mask, where 0 is for known pixels
and 1 for unknown ones, denoted by M. The output is ˆY
of size r1h × r2w × c. A context normalization module is
developed for coordinating the feature distribution between
ﬁlling and known regions.

Rather than a simple reﬁnement stage commonly used
in the coarse-to-ﬁne framework, the rationale behind Con-
text Prediction Network (CPN) is twofold. First, it incorpo-
rates ﬁlling margin, which is excluded in FEN, to indicate
where to predict. Second, besides the ﬁlling margin, in-
put to the network also includes context features learned by
FEN instead of coarse prediction. These features are prop-
erly handled by compression via an encoder-decoder and
our designed context normalization module.

Context Normalization To improve style consistency of
the generated image, a context normalization (CN) module
is proposed. Recent study shows that image style is char-
acterized by its feature statistics. Various image statistical
losses [12, 14] and normalization operations [18, 42, 16]
were explored to capture such statistics implicitly or ex-
plicitly. Inspired by instance normalization [42] and AdaIN
[16], our proposed CN function (t(·)) is deﬁned as

t(f (X), ρ) =[ρ · n(f (XΩ), f (X ¯Ω))

+ (1 − ρ)f (XΩ)] ⊙ M ↓ +

(2)

f (X ¯Ω) ⊙ (1 − M ↓),

n(x1, x2) =

x1 − µ(x1)

σ(x1)

· σ(x2) + µ(x2),

(3)

where X ¯Ω and XΩ indicate known and unknown image re-
gions respectively, f (·) extracts bottleneck features based

1401

on the input-expanded feature maps, and ρ ∈ [0, 1]. ↓ is
the nearest-neighbor downsampling operator. M ↓ shares
the same height and width with f (X). µ(·) and σ(·) com-
pute the mean and standard deviation. Essentially, it trans-
fers mean and variance from known features to unknown
area, which regularizes the generated content beyond one-
side constraints and enhances the color/texture consistency
between input and predicted regions.

Note that CN and AdaIN [16] are fundamentally dif-
ferent. AdaIN replaces the feature statistics of an image
with those from another image. For CN, feature statistics
in known/unknown regions of the same image are consid-
ered. Moreover, a blending step is incorporated in CN. Be-
cause the feature statistics from known and unknown re-
gions could be different for semantically sensitive targets
like face and body, blending these feature statistics is cru-
cial for our system. Detailed comparisons are given in the
supplementary material.

3.2. Loss Design

The optimization target comprises the reconstruction
texture consistency loss, and the adversarial loss,

loss,
which are detailed as follows.

Relative Spatial Variant Loss Reconstruction loss sta-
bilizes the training procedure by providing pixel-wise su-
pervision. Due to the one-sided property of content ex-
trapolation, spatial variant supervision [48, 44] is needed.
We design a relative spatial variant (RSV) reconstruction
loss for incorporating such spatial regularization. For the
conﬁdence-driven (CD) loss [44], it is formulated as

(a)

(b)

(c)

(d)

Figure 3. Visualization of weight mask Mw used in Eq. (5). (a)
Input mask (0 and 1 for known and unknown color), (b) use M as
Mw, (c) Mw in CD [44], (d) Mw in RSV. (b)-(d) are shown in
the jet colormap.

w

and Mc

w to describe the conﬁdence.

results Mc−1
Intu-
itively, unknown pixels close to existing regions have high-
conﬁdence neighboring pixels. So their relative increase is
quicker than that of unknown pixels away from it. As shown
in Figure 3, CD does not constrain distant areas while RSV
assigns meaningful weight. More comparisons are given in
Section 4.3.

Implicit Diversiﬁed MRF Loss Along with pixel-wise
reconstruction loss, implicit diversiﬁed MRF regularization
[29, 44] is introduced as part of the optimization goal for
creating crisp texture by bringing close feature distributions
of G(X, m) and Y.

We use ˆYL

Ω and YL to denote features extracted from
the Lth feature layer of a pretrained network, where ˆYΩ
indicates the prediction of the regions to be ﬁlled. The ID-
MRF loss [29, 44] between ˆYL

Ω and YL is deﬁned as

LM (L) = − log(

1
Z X

s∈YL

max
v∈ ˆYL

Ω

RS(v, s)),

(7)

Mi

w = (g ∗ M

i

) ⊙ M,

(4)

with respect to

where g is a normalized Gaussian ﬁlter, M
Mi−1
Eq. (4) is repeated c times to generate Mc
w.

= 1 − M +
w = 0. ⊙ is the Hadamard product operator.

w , and M0

i

In RSV, our used weight matrix is

RS(v, s) = RS(v, s)/ X

RS(v, r),

(8)

r∈ρs(YL)

β(v, s)

maxr∈ρs(YL) β(v, r) + ǫ

)/h),

(9)

Mw = Mc−1

w /max(Mc

w, ǫ).

(5)

RS(v, s) = exp((

The ﬁnal reconstruction loss is

Ls = ||(Y − G(X, m; θ)) ⊙ Mw||1,

(6)

where G(X, m; θ) is the output of our generative model G,
Y is the corresponding ground truth, and θ denotes param-
eters that can be learned.

The repetitive convolution of g over M

propagates the
conﬁdence of known pixels to unknown ones. However,
since existing pixels are fewer than unknown ones, and they
are almost separated (only a handful of unknown pixels
have neighboring known pixels), the conﬁdence propaga-
tion is hindered by its scarce neighborhood support. To
remedy it, we apply the ratio of two adjacent convolutional

i

where Z is a normalization factor. Eq. (8) is a normalized
version of Eq. (9), which deﬁnes the similarity between two
extracted patches v and s from ˆYL
Ω and YL respectively.
β(·, ·) is the cosine similarity. r ∈ ρs(YL) means r belong-
ing to YL excluding s. h and ǫ are two positive constants. If
v is like s more than other neural patches in YL, RS(v, s)
turns large.

In our experiments, we compute the sum of LM between
G(X, m; θ) and Y on conv3 2 and conv4 2 extracted from
pre-trained VGG19 network as Lmrf .

Compared with other losses, e.g., style loss and its vari-
ants, focusing on restoring texture or style, ID-MRF loss
reinforces local image details by referring their most rela-
tively similar patches in ground truth.

1402

Final Learning Objective With relative spatial variant
reconstruction loss, ID-MRF loss, and adversarial loss, the
model objective of our network is expressed as

L = λsLs + λmrf Lmrf + λadvLadv,

(12)

where λs, λadv, and λmrf are coefﬁcients used to balance
the effect among regression, local structure regularization,
and adversarial training.

3.3. Learning Scheme

To better stabilize the adversarial training, our model is
pre-trained ﬁrst with only reconstruction loss (λs = 5).
Afterwards, we let λmrf = 0.05 and λadv = 0.001 for
ﬁne-tuning SRN until convergence. During training, Adam
solver [22] with learning rate 1e − 4 is adopted where
β1 = 0.5 and β2 = 0.9. Training batch size is 16. The
input and output are linearly scaled within range [−1, 1].

Figure 4. Context discriminator illustration.

4. Experiments

Contextual Adversarial Loss Various generation tasks
using generative adversarial networks have validated the
effectiveness of adversarial training in image creation and
synthesis. The adversarial loss, which is an optimization
measure that can be learned, is an indispensable ingredient
in producing convincing details.
In our work, the global
and local discriminators [17] with improved Wasserstein
distance [13] are employed.

It is noteworthy of the specialty in our design. Unlike
restoring a local rectangle region in inpainting tasks where
local information can be easily extracted, the contextual
region (to be predicted) surrounds the given input region,
leading to the difﬁculty of aggregating local regions into
a single probability. To tackle this issue, a masked patch
discriminator is adopted as the context discriminator (Fig-
ure 4). The output Dcontext( ˆY) of context discriminator for
the input prediction ˆY is deﬁned as

Dcontext( ˆY) = Pp∈P ( ˆY) p
Pq∈M↓ q

,

w.r.t. P ( ˆY) = dcontext( ˆY) ⊙ M ↓,

(10)

where dcontext( ˆY) denotes the feature maps of ˆY, and ↓
is the max pooling operator. For SRN, the global/context
adversarial loss is deﬁned as

Ln

adv = − EX∼PX [Dn(G(X; θ))]+

λgpE ˆX∼P ˆX

[(||∇ ˆXDn( ˆX) ⊙ Mw||2 − 1)2],

(11)

where ˆX = tG(X, m; θ) + (1 − t)Y, t ∈ [0, 1],
Y is the ground truth corresponding to X, and n ∈
{context, global}.
the employed Ladv =
+ Lglobal
(Lcontext

Thus,

)/2.

adv

adv

Our models are implemented with TensorFlow v1.4 and
trained on a PC with Intel Xeon E5 (2.60GHz) CPU and an
NVidia TITAN X GPU. We evaluated our method on a va-
riety of datasets, including CelebA-HQ [21], CUB200 [45],
DeepFashion [27, 28], ETHZ Synthesizability [6], Paris
street view [31], Places2 [50], and Cityscapes [4]. For each
dataset, models are trained on the training set and tested
on the validation set. Exceptions are CUB200 and ETHZ
Synthesizability, which we split as described in the supple-
mentary material.

We train our models on three different resolution set-
tings. 1) 128 × 128 → 256 × 256 (used for CelebA-HQ,
ETHZ Synthesizability, and CUB200). 2) 64 × 128 →
256 × 128 (used for DeepFashion); 3) 256 × 256 → 256 ×
512 (on Paris street view, Places2, and Cityscapes). We use
input image size to indicate setting names in the following.
For visual and quantitative evaluation. We choose 3
models for comparison. Model CA is current state-of-the-
art inpainting method using contextual attention layer [48].
We feed a zero-value padded full size image as input, and
retrain this model using publicly available codes but with
context adversarial loss instead of global and local adver-
sarial loss for fairness. Besides, we compare with baseline
model ED and SRN-HR, which have different network ar-
chitectures, which will be detailed in Section 4.3.

4.1. Quantitative Evaluation

As indicated in previous image generation papers [46,
48], the peak signal-to-noise ratio (PSNR) and structural
similarity index measure (SSIM) are not optimal metrics
for evaluating conditional image generation tasks. Thus we
only provide these values for reference in Table 1. It is no-
table that our method yields competitive PSNR and SSIM.

1403

Method

ED

SRN-HR
CA [48]

SRN

CelebA-HQ-2K
SSIM
0.5859
0.6183
0.6010
0.6171

PSNR
13.88
13.88
13.56
14.01

CUB200-1.7K

PSNR
14.90
15.70
15.56
15.59

SSIM
0.5744
0.6035
0.6467
0.6473

DeepFashion-3K
SSIM
0.5677
0.5686
0.5769
0.5686

PSNR
12.50
12.72
12.58
12.58

Table 1. Quantitative results on the validation data.

CelebA-HQ CUB200 DeepFashion

SRN > CA [48]

SRN > ED

SRN > SRN-HR

97.54%
96.02%
77.69%

96.42%
92.69%
69.63%

93.68%
91.13%
62.25%

Table 2. User study statistics. Each entry gives the percentage of
cases where results by our approach are judged more realistic than
another solution.

Method

64 × 128

128 × 128

256 × 256

CA
ED

SRN-HR

SRN

17.35
18.92
17.73
11.07

30.56
26.66
28.95
18.15

60.44
41.81
52.50
36.75

Table 3. Running time for different structures (ms/image).

More convincing blind user studies of pairwise A/B tests
are conducted. Each questionnaire includes 40 pairwise
comparisons, regarding results from two different methods
on the same input. There are 40 participants invited to user
study. They are required to select the more realistic image
in each pair. The images are all shown at the same resolu-
tion (256×128, 256×256, or 256×512). The comparisons
are randomized across different methods, as well as in the
left-right order. Participants have unlimited time to decide.
In all conditions given in Table 2, our method outperforms
the baselines.

Regarding efﬁciency, Table 3 presents the evaluation
time on images of various resolutions. Note that SRN only
takes up to 60% ∼ 65% testing time of CA, with similar
network depth, width, and capacity (17.14M vs. 20.62M).

4.2. Qualitative Evaluation

As shown in Figures 5 and 6, our method produces more
convincing objects, portraits, and scene layouts with ﬁne
details, inferred from a limited-view input. Compared with
the baseline CA, our method performs better with regard
to quality of semantic structure, texture and border consis-
tency. Moreover, since the ﬁlling margin of our model is
arbitrary, SRN can infer visual context from different loca-
tions as shown in Figure 7. More results are presented in
the supplementary material.

4.3. Ablation Studies

(a)

(b)

(c)

(a)

(b)

(c)

(a)

(b)

(c)

Figure 5. Visual comparisons on CelebA-HQ (top), CUB200 (mid-
dle), and DeepFashion (down). (a) Input images. (b) Results of
CA [48]. (c) Our results.

Feature expansion operator

PSNR
SSIM

deconv
14.95
0.6409

unfold
15.06
0.6412

sub-pixel

15.02
0.6452

Table 4. Quantitative results of different feature expansion opera-
tors in SRN on CelebA-HQ dataset in the pre-training phase.

cesses the input like SRN. Large-to-large frameworks com-
promise vanilla encoder-decoder, SRN-HR, and coarse-to-
ﬁne networks, which are formed by two sequential encoder-
decoder. Here we directly employ CA [48] as the coarse-to-
ﬁne network. The SRN-HR is an variant of SRN, which
replaces the feature expansion operator in FEN with com-
mon convolution and preserves all the remaining compo-
nents. Small-to-large design is SRN. The network depth
and parameters are set to similar values for fairness.

Figure 8 shows comparison between the given architec-
tures. Note SRN and SRN-HR give better predictions than
CA and ED on creating more natural hair and face shape
with fewer visual artifacts, which validates the effectiveness
of SRN design. Compared with SRN-HR, SRN produces
more realistic hair texture with less inference time (Table
3), which indicates pre-ﬁlling padding for the input harms
ﬁnal ﬁlling performance as well as efﬁciency.

Network architectures We analyze multiple possible
network designs. The compared network architectures
cover three large-to-large designs and one small-to-large de-
sign. Large-to-large means the input is padded into the same
size as the output ﬁrst, while small-to-large directly pro-

Feature Expansion Operator
In our experiments, three
feature expansion operators, including deconv, unfold (sym-
metric padding plus conv.), and sub-pixel conv., are evalu-
ated in SRN structure. Except for these operators, other
components in three SRNs are identical. We evaluate the

1404

(a)

(b)

(c)

Figure 6. Visual comparison on Paris street view (top) and Cityscapes (down). (a) Input image. (b) Results of CA [48]. (c) Our results.

(a)

(b)
Figure 7. Extrapolation on CelebA-HQ (top) and CUB200 (down) with arbitrary ﬁlling margin. (a) Input images. (b) Our results.

(b)

(b)

(b)

(a)

(a)

(a)

(a)

(b)

(c)

(d)

(e)

Figure 8. Visual comparison of different network structures on
CelebA-HQ. (a) Input image.
(c) Naive
encoder-decoder. (d) SRN-HR. (e) SRN.

(b) Coarse-to-ﬁne.

(a)

(b)

(c)

(d)

(e)

Figure 10. Visual comparison of using CN (or not) on CelebA-HQ.
(a) Input image. (b) SRN w/o CN in pre-training. (c) SRN w/ CN
in pre-training. (d) SRN w/o CN. (e) SRN w/ CN.

(a)

(b)

(c)

(d)

Figure 9. Visual comparison of different feature expansion opera-
tors on CelebA-HQ. (a) Input image. (b) Deconv. (c) Unfold. (d)
Sub-pixel conv.

ﬁdelity of the three SRNs on CelebA-HQ with their pre-

trained models. The corresponding quantitative results of
pre-trained models are given in Table 4 and the example
images of full models are shown in Figure 9. Notably, the
PSNR and SSIM of these three SRNs are close to each
other. Results using SRN in sub-pixel level are more vi-
sual pleasing compared with that with deconv and unfold.
Figure 9 shows details of facial structure and texture.

W/O Context Normalization Two SRNs are evaluated
on CelebA-HQ. One of them is with context normalization
(CN) module, while the other is not. Their ﬁdelity tests

1405

(a)

(b)

(c)

Figure 11. Visual comparison of different adversarial losses on
CelebA-HQ. (a) Input image. (b) Vanilla global adversarial loss.
(c) Context adversarial loss.

(a)

(b)

(c)

Figure 13. Visual comparison of texture synthesis on ETHZ Syn-
thesizability. (a) Input image. (b) CA [48]. (c) Our result.

(a)

(b)

(c)

(d)

(a)

(b)

(a)

(b)

Figure 12. Visual comparisons of different reconstruction losses on
CelebA-HQ. (a) Input image. (b) Vanilla l1 loss. (c) Conﬁdence-
driven loss. (d) Relative spatial variant loss.

Figure 14. Morphing of dog images with SRN model trained on
CelebA-HQ. (a) Input images. (b) Our results.

Using CN

PSNR
SSIM

Pre-training
NO
14.48
0.6084

YES
15.02
0.6452

Full-training
NO
13.92
0.5961

YES
14.01
0.6171

aries (hairline and face shape in Figure 12) than that with
conﬁdence-driven loss (where Mw = Mc
w) [44] and com-
mon l1 loss.

Table 5. Quantitative results of using context normalization (CN)
(or not) in SRN on CelebA-HQ dataset.

4.4. Other Applications and Limitations

RSV loss CD loss

vanilla l1 loss

PSNR
SSIM

15.02
0.6452

14.41
0.6229

15.06
0.6478

Table 6. Quantitative results of only using different reconstruction
losses in SRN on CelebA-HQ dataset (RSV loss: relative spatial
variant loss, CD loss: conﬁdence-driven loss).

are given in Table 5 and the resulting visual prediction is
shown in Figure 10. Clearly, CN improves the SRN quan-
titatively and qualitatively.
In Figure 10, CN harmonizes
color and border consistency both in pre-training and full-
training phases.

Contextual Adversarial Loss vs. Vanilla Impr. WGAN
Loss We give qualitative evaluation (Figure 11) on
CelebA-HQ of these two types of GAN losses since PSNR,
SSIM, and other metrics may not reﬂect true visual qual-
ity. The base model is SRN where relative spatial variant
loss and ID-MRF loss are also employed.
In Figure 11,
SRN with context adversarial loss predicts clearer hair de-
tails than that with only global adversarial loss.

Relative Spatial Variant Loss vs. Conﬁdence-driven Loss
vs. Vanilla l1 Loss Compared with common l1 loss
(where Mw = M), SRN pre-training with relative spa-
tial variant loss (Eq. (5)) gives comparable ﬁdelity (Table
6). However, it produces more distinctive semantic bound-

Other than content extrapolation for uncropping pictures,
SRN also ﬁnds applications of texture synthesis (Figure 13)
and morphing (Figure 14).

About limitations, each trained model now is with spe-
ciﬁc expanding ratios (e.g., a model trained for predicting
three times more pixels based on the input only produces re-
sults in the same setting). Moreover, a gigantic dataset with
more than thousands of scene types like Places2 is difﬁcult
to ﬁt by a generative model. This problem may be lessened
with new research breakthrough for the GAN model.

5. Concluding Remarks

We have explored a deep learning model to conduct im-
age extrapolation for semantically sensitive objects. We
summarize that the challenge lies in size expansion and
one-sided constraints, and tackle them via proposing new
network modules and loss design. Our method achieves
promising semantic expansion effect. In future work, semi-
parametric approaches will be studied when efﬁciency is
not an issue. As shown in recent work [32, 19], this line
of methods use retrieved object segments matched by input
to ﬁll the unknown region in advance, and regress raw ma-
terial. Further, it is interesting to apply image expansion
to videos with temporal consistency and redundant spatial
information.

1406

References

[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspon-
dence algorithm for structural image editing. ACM Trans.
Graph., 28(3):24, 2009.

[2] Tao Chen, Ming-Ming Cheng, Ping Tan, Ariel Shamir, and
Shi-Min Hu. Sketch2photo: Internet image montage. ACM
Trans. Graph., 28(5):124, 2009.

[3] Donghyeon Cho, Jinsun Park, Tae-Hyun Oh, Yu-Wing Tai,
and In So Kweon. Weakly-and self-supervised learning for
content-aware deep image retargeting. In ICCV, 2017.

[4] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016.

[5] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-
ject removal by exemplar-based inpainting. In CVPR, 2003.
[6] Dengxin Dai, Hayko Riemenschneider, and Luc Van Gool.

The synthesizability of texture examples. In CVPR, 2014.

[7] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B
Goldman, and Pradeep Sen. Image melding: Combining in-
consistent images using patch-based synthesis. ACM Trans.
Graph., 31(4):82, 2012.

[8] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. TPAMI, 38(2):295–307, 2016.

[9] Vincent Dumoulin and Francesco Visin. A guide to con-
arXiv preprint

volution arithmetic for deep learning.
arXiv:1603.07285, 2016.

[10] Alexei A Efros and Thomas K Leung. Texture synthesis by

non-parametric sampling. In ICCV, 1999.

[11] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Tex-
In

ture synthesis using convolutional neural networks.
NeurIPS, 2015.

[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In

age style transfer using convolutional neural networks.
CVPR, 2016.

[13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Improved training of

Dumoulin, and Aaron C Courville.
wasserstein gans. In NeurIPS, 2017.

[14] Agrim Gupta, Justin Johnson, Alexandre Alahi, and Li Fei-
Fei. Characterizing and improving stability in neural style
transfer. In CVPR, 2017.

[15] Kaiming He, Huiwen Chang, and Jian Sun.

Rectan-
gling panoramic images via warping. ACM Trans. Graph.,
32(4):79, 2013.

[16] Xun Huang and Serge J Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. In ICCV,
2017.

[17] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.
Globally and locally consistent image completion. ACM
Trans. Graph., 36(4):107, 2017.

[19] Karim Iskakov. Semi-parametric image inpainting. arXiv

preprint arXiv:1807.02855, 2018.

[20] Jiaya Jia and Chi-Keung Tang. Image repairing: Robust im-
age synthesis by adaptive nd tensor voting. In CVPR, 2003.

[21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196, 2017.

[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[23] Johannes Kopf, Wolf Kienzle, Steven Drucker,

and
Sing Bing Kang. Quality prediction for image completion.
ACM Trans. Graph., 31(6):131, 2012.

[24] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, 2017.

[25] Anat Levin, Assaf Zomet, and Yair Weiss. Learning how to

inpaint from global image statistics. In ICCV, 2003.

[26] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Andrew Tao, and Bryan Catanzaro.
Image inpainting for
irregular holes using partial convolutions. arXiv preprint
arXiv:1804.07723, 2018.

[27] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition and
retrieval with rich annotations. In CVPR, 2016.

[28] Ziwei Liu, Sijie Yan, Ping Luo, Xiaogang Wang, and Xiaoou
In ECCV,

Tang. Fashion landmark detection in the wild.
2016.

[29] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The
contextual loss for image transformation with non-aligned
data. arXiv preprint arXiv:1803.02077, 2018.

[30] Augustus Odena, Vincent Dumoulin, and Chris Olah. De-
convolution and checkerboard artifacts. Distill, 1(10):e3,
2016.

[31] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR, 2016.

[32] Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun.

Semi-parametric image synthesis. In CVPR, 2018.

[33] Jimmy SJ Ren, Li Xu, Qiong Yan, and Wenxiu Sun. Shepard

convolutional neural networks. In NeurIPS, 2015.

[34] Michael Rubinstein, Diego Gutierrez, Olga Sorkine, and
Ariel Shamir. A comparative study of image retargeting.
ACM Trans. Graph., 29(6):160, 2010.

[35] Qi Shan, Brian Curless, Yasutaka Furukawa, Carlos Hernan-

dez, and Steven M Seitz. Photo uncrop. In ECCV, 2014.

[36] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016.

[18] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015.

[37] Jian Sun, Lu Yuan, Jiaya Jia, and Heung-Yeung Shum. Image
completion with structure propagation. ACM Trans. Graph.,
24(3):861–868, 2005.

1407

[38] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya
Jia. Detail-revealing deep video super-resolution. In ICCV,
2017.

[39] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-
aya Jia. Scale-recurrent network for deep image deblurring.
In CVPR, 2018.

[40] Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke,
Thomas Brox, and Andreas Geiger. Sparsity invariant cnns.
arXiv preprint arXiv:1708.06500, 2017.

[41] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor S Lempitsky. Texture networks: Feed-forward synthesis
of textures and stylized images. In ICML, 2016.

[42] D Ulyanov, A Vedaldi, and V Lempitsky.

Instance nor-
malization: the missing ingredient for fast stylization. cscv.
arXiv preprint arXiv:1607.08022, 2017.

[43] Miao Wang, Yukun Lai, Yuan Liang, Ralph Robert Martin,
and Shi-Min Hu. Biggerpicture: data-driven image extrap-
olation using graph matching. ACM Trans. Graph., 33(6),
2014.

[44] Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya
Jia. Image inpainting via generative multi-column convolu-
tional neural networks. In NeurIPS, 2018.

[45] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-ucsd birds 200. Technical
Report CNS-TR-2010-001, California Institute of Technol-
ogy, 2010.

[46] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang,
and Hao Li. High-resolution image inpainting using multi-
scale neural patch synthesis. In CVPR, 2017.

[47] Raymond A Yeh, Chen Chen, Teck-Yian Lim, Alexander G
Schwing, Mark Hasegawa-Johnson, and Minh N Do. Seman-
tic image inpainting with deep generative models. In CVPR,
2017.

[48] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with contex-
tual attention. arXiv preprint, 2018.

[49] Yinda Zhang, Jianxiong Xiao, James Hays, and Ping Tan.
Framebreak: Dramatic image extrapolation by guided shift-
maps. In CVPR, 2013.

[50] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. TPAMI, 40(6):1452–1464, 2018.

1408

