Deep Rigid Instance Scene Flow

Wei-Chiu Ma1

2

,

Shenlong Wang1

,

3 Rui Hu1 Yuwen Xiong1

,

3 Raquel Urtasun1

3

,

1Uber Advanced Technologies Group
2Massachusetts Institute of Technology

3University of Toronto

Abstract

In this paper we tackle the problem of scene ﬂow estima-
tion in the context of self-driving. We leverage deep learn-
ing techniques as well as strong priors as in our application
domain the motion of the scene can be composed by the
motion of the robot and the 3D motion of the actors in the
scene. We formulate the problem as energy minimization in
a deep structured model, which can be solved efﬁciently in
the GPU by unrolling a Gaussian-Newton solver. Our ex-
periments in the challenging KITTI scene ﬂow dataset show
that we outperform the state-of-the-art by a very large mar-
gin, while being 800 times faster.

1. Introduction

Scene ﬂow refers to the problem of estimating a three-
dimenional motion ﬁeld from a set of two consecutive (in
time) stereo pairs. It was ﬁrst introduced in [40] to describe
the 3D motion of each point in the scene. Through scene
ﬂow, we can gain insights into the geometry as well as the
overall composition and motion of the scene. It is of par-
ticular importance for robotics systems, such as self-driving
cars, as knowing the 3D motion of other objects in the scene
can not only help the autonomous systems avoid collision
while planing its own future movements, but also improve
the understanding of the scene and predict the intent of oth-
ers. In this work, we focus on estimating the 3D scene ﬂow
in autonomous driving scenarios.

In the world of self-driving, the motion of the scene can
be mostly explained by the motion of the ego-car. The pres-
ence of dynamic objects which typically move rigidly can
also be utilized as strong priors. Previous structure predic-
tion approaches often exploit these facts and ﬁt a piece-wise
rigid representations of motion [41, 44, 27, 3]. While these
methods achieve impressive results on scene ﬂow estima-
tion, they require minutes to process each frame, and thus
cannot be employed in real-world robotics systems.

On the other hand, deep learning based methods have
achieved state-of-the-art performance in real time on a va-

Figure 1: Performance vs runtime on KITTI SceneFlow
dataset: Our approach is much faster and more accurate.

riety of low level tasks, such as optical ﬂow prediction
[11, 32, 38] and stereo estimation [46, 26, 24]. While they
produce ‘accurate’ results, their output is not structured and
cannot capture the relationships between estimated vari-
ables. For instance, they lack the ability to guarantee that
pixels on a given object produce consistent estimates. While
this phenomenon may have little impact in photography
editing applications, this can cathastrophic in the context
of self-driving cars, where the motion of the full object is
more important than the motion of each individual pixel.

With these problems in mind, we develop a novel deep
rigid instance scene ﬂow (DRISF) model that takes the best
of both worlds. The idea behind is that the motion of the
scene can be composed by estimating the 3D rigid motion
of each actor. The static background can also be modeled as
a rigidly moving object, as its 3D motion can be described
by the ‘ego-car’ motion. The problem is thus reduced to es-
timating the 3D motion of each trafﬁc participant. Towards
this gaol, we ﬁrst capitalize on deep neural networks to esti-
mate optical ﬂow, disparity and instance segmentation. We
then exploit multiple geometry based energy functions to
encode the structural geometric relationship between these
visual cues. Through optimizing the energy function, we
can effectively reason about the 3D motion of each trafﬁc
participant. As the energy takes the form of weighted sum
of squares, it can be efﬁciently minimized via Gaussian-
Newton (GN) algorithm [5]. We implement the GN solver

3614

100101102103Runtime (sec)6810121416SF-All Outliers (%)ISFOSFPRSMCSFSceneFFieldsSSFOSF 2018FSF+MSOursKITTI Scene Flow (Test)Left

t

t+1

Flow

Seg.

Stereo

Mask

Right

Extract 
visual cue

RGB

Flow

D1

D2

GN 

Solver

3D 

Motion

Structure optimization 

Figure 2: Overview of our approach: Given two consecutive stereo images, we ﬁrst estimate the ﬂow, stereo, and seg-
mentation (Sec. 3.1). The visual cues of each instance are then encoded as energy functions (Sec. 3.2) and passed into the
Gaussian-Newton (GN) solver to ﬁnd the best 3D rigid motion (Sec. 3.3). The GN solver is unrolled as a recurrent network.

as layers in neural networks, thus all operations can be com-
puted efﬁciently on the GPU in an end-to-end fashion.

We demonstrate the effectiveness of our approach on the
KITTI scene ﬂow dataset [27]. As shown in Fig. 1, our deep
rigid instance scene ﬂow model outperforms all previous
methods by a signiﬁcant margin in both runtime and accu-
racy. Importantly, it achieves state-of-the-art performance
on almost every entry. Comparing to prior art, DRISF re-
duces the D1 outliers ratio by 43%, the D2 outliers ratio by
32%, and the ﬂow outliers ratio by 24%. Comparing to the
existing best scene ﬂow model [3], our scene ﬂow error is
22% lower and our runtime is 800 times faster.

2. Related Work
Optical ﬂow: Optical ﬂow is traditionally posed as an en-
ergy minimization task. It dates back to Horn and Schunck
[17] where they deﬁne the energy as a combination of a
data term and a smoothness term, and adopt variational in-
ference to solve it. Since then, a variety of improvements
have been proposed [6, 4, 30]. Recently, deep learning has
replaced the variational approaches. Employing deep fea-
tures for matching [1, 43] improved performance by a large
margin. However, as the matching results are not dense,
post-processing steps are required [35]. This not only re-
duces the speed, but also limits the overall performance.

Pioneered by Flownet [11], various end-to-end deep re-
gression based methods have been proposed [21]. Flownet2
[20] stacks multiple networks to iteratively reﬁne the esti-
mated ﬂow and introduces a differentiable warping opera-
tion to compensate for large displacements. As the result-
ing network is very large, SpyNet [32] propose to use spatial
pyramid network to handle large motions. They reduce the
model size greatly, yet at the cost of degrading performance.
Lite-Flownet [19] and PWC-Net [38, 37] extend this idea
and incorporate the traditional pyramid processing and cost
volume concepts into the network. Comparing to previous
approach, the resulting model is smaller and faster. In this

work, we adapt the latest PWC-Net as our ﬂow module.

Stereo: Traditional stereo methods [16, 22] follow three
steps: compute patch-wise feature, construct cost volumes,
and ﬁnal post-processing. The representation of the patch
plays an important role. Modern approaches leverage CNNs
to predict whether two patches are a match [45, 46]. While
they showed great performance in challenging benchmarks,
they are computationally expensive. To speed up the match-
ing process, Luo et al. [24] propose a siamese matching
network which exploits a correlation layer [9] to extract
marginal distributions over all possible disparities. While
the usage of the correlation layer signiﬁcantly improves efﬁ-
ciency, they still require post-processing techniques [15, 47]
to smooth their estimation, which largely limits their speed.
In light of this, networks that directly regress sub-pixel dis-
parities from the given stereo image pair have been pro-
posed. DispNet [26] exploits a 1D correlation layer to ap-
proximate the stereo cost volumes and rely on later layers
for implicit aggregation. Kendall et al. [23] incorporate
3D conv for further regularization and propose a differen-
tiable soft argmin to enable sub-pixel disparity from cost
volumes. PSM-Net [8] later extend [23] by incorporating
stacked hourglass [29] and Pyramid spatial pooling [48, 14].
In this work, we exploit PSM-Net as our stereo module.

Scene ﬂow: Scene ﬂow [40] characterizes the 3D motion
of a point. Similar to optical ﬂow estimation, the task is
traditionally formulated as a variational inference problem
[39, 31, 18, 2]. However, the performance is rather lim-
ited in real world scenarios due to errors caused by large
motions. To improve the robustness, slanted-plane based
methods [44, 27, 41, 25] propose to decompose the scene
into small rigidly moving planes and solve the discrete-
continuous optimization problem. Behl et al. [3] build upon
[27], and incorporate recognition cues. With the help of
ﬁne-grained instance and geometric feature, they are able to
establish correspondences across various challenging sce-
narios. Similar to our work, Ren et al. [34] exploit multiple

3615

B
G
R

B
G
R

1
D

2
D

w
o
l
F

r
o
r
r
E
F
S

Figure 3: Qualitative results on val set: Our model can estimate the background motion very accurately. It is also able to
estimate the 3D motion of foreground objects in most scenarios. It fails in challenging cases as show in last column.

visual cues for scene ﬂow estimation. They encode the fea-
tures via a cascade of conditional random ﬁelds and itera-
tively reﬁne them. While these methods have achieved im-
pressive performance, they are computationally expensive
for practical usage. Most methods require minutes to com-
pute one scene ﬂow. This is largely due to the complicated
optimization task. In contrast, our deep structured motion
estimation model is able to compute scene ﬂow in less than
a second, which is two to three orders of magnitude faster.

3. Deep Rigid Instance Scene Flow

In this paper we are interested in estimating scene ﬂow
in the context of self-driving cars. We build our model on
the intuition that in this scenario the motion of the scene can
be formed by estimating the 3D motion of each actor. The
static background can be also modeled as a rigidly moving
object, as its 3D motion can be described by the ‘ego-car’
motion. Towards this goal, we proposed a novel deep struc-
tured model that exploits optical ﬂow, stereo, as well as in-
stance segmentation as visual cues. We start by describing
how we employ deep learning to effectively estimate the ge-
ometric and semantic features. We then formulate the scene
ﬂow task as an energy minimization problem and discuss
each energy term in details. Finally, we describe how to
perform efﬁcient inference and learning.

3.1. Visual Cues

We exploit three types of visual cues: instance segmen-

tation, optical ﬂow and stereo.

Instance Segmentation: We utilize Mask R-CNN [13] as
our instance segmentation network, as it produces state-of-
the-art results in autonomous driving benchmarks such as

KITTI [12] and Cityscapes [10]. Mask R-CNN is a proposal
based two stage network built upon Faster R-CNN [33]. For
each object proposal, it predicts the object class, regresses
its 2D box, and infers the bg/fg segmentation mask.
Stereo: We exploit the pyramid stereo matching network
(PSM-Net) [8] to compute our stereo estimates. It consists
of three main modules: fully convolutional feature module,
spatial pyramid pooling [14, 48] and 3D cost volume pro-
cessing. The feature module computes a high-dimensional
feature map in a fully convolutional manner; the spatial
pyramid pooling aggregates context in different scales and
locations to construct the cost volume; the 3D cost volume
module then performs implicit cost volume aggregation and
regularizes it using stacked hourglass networks. Compared
to previous disparity regression networks, PSM-Net learns
to reﬁne and produce sharp disparity images that respect
object boundaries better. This is of crucial importance as
over-smoothed results often deteriorates motion estimation.

Optical Flow: Our ﬂow module is akin to PWC-Net [38],
which is a state-of-the-art ﬂow network designed based on
three classical principles (similar to stereo networks): pyra-
midal feature processing, warping, and cost volume reason-
ing. Pyramidal feature processing encode visual features
with large context; the progressive warping reduces the cost
of building cost-volume through a coarse-to-ﬁne scheme.
Cost volume reasoning further boost performance by sharp-
ening the boundaries. We implement PWC-net with one
modiﬁcation: during the warping operation, we use the fea-
ture of the nearest boundary pixel to pad if the sampling
point falls outside the image, rather than 0. Empirically we
found this to improve performance.

3616

Dispairty 1

Dispairty 2

Optical Flow

Scene Flow

Methods
CSF [25]
OSF [27]
SSF [34]
OSF-TC* [28]
PRSM* [42]
ISF [3]
Our DRISF

Runtime
1.3 mins
50 mins
5 mins
50 mins
5 mins
10 mins
0.75 sec

bg
4.57
4.54
3.55
4.11
3.02
4.12
2.16

fg

13.04
12.03
8.75
9.64
10.52
6.17
4.49

all
5.98
5.79
4.42
5.03
4.27
4.46
2.55

bg
7.92
5.45
4.94
5.18
5.13
4.88
2.90

fg

20.76
19.41
17.48
15.12
15.11
11.34
9.73

all

10.06
7.77
7.02
6.84
6.79
5.95
4.04

bg

10.40
5.62
5.63
5.76
5.33
5.40
3.59

fg

25.78
18.92
14.71
13.31
13.40
10.29
10.40

all

12.96
7.83
7.14
7.02
6.68
6.22
4.73

bg

12.21
7.01
7.18
7.08
6.61
6.58
4.39

fg

33.21
26.34
24.58
20.03
20.79
15.63
15.94

all

15.71
10.23
10.07
9.23
8.97
8.08
6.31

Table 1: Comparison against top 6 published approaches: Our method acheives state-of-the-art performance on almost
every entry while being two to three orders of magnitude faster. (*: Method uses more than two temporally adjacent images.)
3.2. Energy Formulation

followed by a rigid transform ξ. Speciﬁcally,

We now describe the energy formulation of our deep
structured model. Let L0, R0, L1, R1 be the input stereo
pairs captured from two consecutive time steps. Let D0, D1
be the estimated stereo, and FL, FR be the inferred ﬂow.
Denote S 0
L as the instance segmentation computed on the
left image L0. Assume all cameras are pre-calibrated with
known intrinsics. We parametrize the 3D rigid motion with
ξ ∈ se(3), the Lie-algebra associated with SE(3). We
use this parametrization as it is a minimal representation
for 3D motion. For each instance i ∈ S 0
L, we aim to ﬁnd
the rigid 3D motion that minimizes the weighted combina-
tion of photometric error, rigid ﬁtting and ﬂow consistency,
where the weights are denoted as λ·,i. For simplicity, let
I = {L0, R0, L1, R1, D0, D1, FL, FR} be input images
and visual cues. We denote the set of pixels belonging to
instance i as Pi = {p|S0
L(p) = i}. Note that background
can be considered as an ‘instance’ since all the pixels in it
undergo the same rigid transform. We obtain the 3D motion
of each instance by minimizing

min

ξ

{λphoto,iEphoto,i(ξ; I) + λrigid,iErigid,i(ξ; I)

(1)

+ λﬂow,iEﬂow,i(ξ; I)}

The three energy terms are complementary. They capture
the geometry and appearance agreement between the obser-
vations and inferred rigid motion. Next, we describe the
energy terms in more details.

Photometric Error: This energy encodes the fact that
correspondences should have similar appearance across all
images. In particular, for each pixel p ∈ Pi in the reference
image, we compare its photometric value with that of the
corresponding pixel in the target image:

Ephoto,i(ξ; I) = X

p∈Pi

αpρ(L0(p) − L1(p′))

(2)

p′ = πK(ξ ◦ π−1

K (p, D(p)))

(3)

where πK(·) : R3 → R2 is the perspective projection func-
tion given known intrinsic K and π−1
K (·, ·) : R2 × R → R3
is the inverse projection that convert a pixel and its asso-
ciated disparity into a 3D point; ξ ◦ x transforms a 3D
point xrigidly with transformation exp(ξ)x. ρ is a robust
error function that improves the overall robustness by reduc-
ing the inﬂuence of outliers on the non-linear least squares
problems. Following Sun et al. [36], we adopt the general-
ized Charbonnier function ρ(x) = (x2 + ǫ2)α as our robust
function and set α = 0.45 and ǫ = 10−5. Similar to [36],
we observe the slightly non-convex penalty improves the
performance in practice.

Rigid Fitting: This term encourages the estimated 3D
rigid motion to be similar to the point-wise 3D motion ob-
tained from the stereo and ﬂow networks. Formally, given
correspondences {(p, q = p + FL(p))|p ∈ Pi} deﬁned by
the output of optical ﬂow network and the disparity maps
D0, D1, the energy measures rigid ﬁtting error of ξ:
Erigid,i(ξ; I) = X

K (cid:0)p, D0(p)(cid:1)−π−1

αpρ(ξ◦π−1

K (cid:0)q, D1(q)(cid:1)),

(p,q)

where q = p + FL(p) and π−1
tion function, and ρ is the same robust error function.

K denotes the inverse projec-

Flow Consistency: This term encourages the projection
of the 3D rigid motion to be close to the original ﬂow es-
timation. This is achieved by measuring the difference be-
tween our optical ﬂow net, and the structured rigid ﬂow,
which is computed by warping each pixel using D0 and the
rigid motion ξ.

Eﬂow,i(ξ; I) = X

p∈Pi

ρ( (p′ − p)
| {z }

2D Rigid ﬂow

)

(4)

− FL(p)
| {z }

optical ﬂow

where αp ∈ {0, 1} is an indicator function representing
which pixel is an outlier. We refer the reader to section 3.3
for a discussion on how to estimate αp. p is a pixel in the
reference image and p′ stands for the projected image co-
ordinate on another image, given by inverse depth warping

where p′ is the rigid warping function deﬁned in Eq. (3),
and ρ is the same robust error function.
3.3. Inference
Uncertain Pixel Removal: Due to viewpoint change,
ﬂow/stereo prediction errors, etc, the visual cues of some

3617

B
G
R

B
G
R

F
S
O

M
S
R
P

F
S
I

F
S
I
R
D

Figure 4: Qualitative comparison on test sest: Our method can effecitvely handle occlusion and texture-less regions. It is
more robust to the illumination change as well as large displacement. Please refer to the supp. material for more results.

pixels are not reliable. For instance, pixels in one image
may be occluded in another image due to viewpoint change.
This motivates us to assign αp to each pixel p as an indi-
cation of outlier or not. Towards this goal, we ﬁrst exclude
pixels which are likely to be occluded in the next frame.
Speciﬁcally, pixels are labeled as occluded if the warped 3D
disparity of the second frame signiﬁcantly differs from the
disparity of the ﬁrst frame. The intuition is that the disparity
of a pixel cannot change drastically in real world due to the
speed limit. We empirically set threshold to 30. Next, we
employ the RANSAC scheme to ﬁt a rigid motion for each
instance. We only keep the inlier points and prune out the
rest. Despite simple, we found this strategy very effective.

Initialization: Due to the highly non-convex structure of
the energy model, a good initialization is critical to achieve
good performance. As previous step already prune out most
unreliable points, we directly exploit the rigid motion ob-
tained by RANSAC as our robust initial guess.

Gaussian Newton Solver: The energy function is non-
convex but differentiable w.r.t. ξ deﬁned over continuous
space. In order to handle the robust function, we adopt an
iterative reweighted least square algorithm [7]. For each
iteration, we can rewrite the original energy minimization
problem of each instance i as a weighted sum of squares:

ξ(n+1) = arg min

ξ

Etotal,i(ξ) = arg min

ξ

wi(ξ(n))r2

i (ξ(n)),

X

Eng

where r denotes the residual function, w reweights each
sample based on the robust function ρ, and Eng refers to
summing over the energy terms. We employ Gaussian-

Newton algorithm to minimize the function. Thus we have

ξ(n+1) = ξ(n) ◦ (JT WJ)(−1)JT Wr(ξ(n))

(5)

δǫ

where ◦ is a pose composition operator and J =
δr(ǫ◦ξ(n))
|ǫ=0. In practice, we unroll the inference steps as
a recurrent neural network and deﬁne its computation graph
as in Eq. (5). The full pipeline including the matrix inverse
is differentiable. Please refer to the supp. material for the
derivation of the Jacobian matrix of each term and more de-
tails on the Gaussian-Newton solver.

Final Scene Flow Prediction: Given the ﬁnal rigid mo-
tion estimation for each instance ξ∗
i , we are able to com-
pute the dense instance-wise rigid scene ﬂow. Our scene
ﬂow consists of three component, namely the ﬁrst frame’s
stereo D0, warped stereo to second frame Dwarp as well as
the instance-wise rigid ﬂow estimation F rigid. Speciﬁcally,
for each point p we have:

D0(p) = D0(p)
Dwarp(p) = zK(ξ∗
S 0

L(p) ◦ π−1

K (p, D0(p)))

(6)

F rigid(p) = p′ − p = πK(ξ ◦ π−1

K (cid:0)p, D0(p)(cid:1)) − p
where zK(·) computes the disparity of the 3D point; π−1
K is
the inverse projection function; and ξ ◦ x transforms a 3D
point x using the rigid motion ξ.

3.4. Learning

The whole deep structured network can be trained end-
In practice, we train our instance segmentation,

to-end.

3618

Figure 5: 3D rigid motion analysis: Over 80% of the estimated 3D rigid motion has an error less than 1m and 1.3◦. Large
errors often happen at farther distances where the vehicles are small and less points are observable.

Figure 6: Odometry from background motion: On aver-
age, our ego-car drifts 0.9cm and 0.024◦ every 1m of drive.

ﬂow estimation, and stereo estimation module respectively
through back-propagation. To be more speciﬁc, Mask R-
CNN model is pre-trained on Cityscapes and ﬁne-tuned on
KITTI. The loss function includes ROI classiﬁcation loss,
box regression loss as well as the mask segmentation loss.
PSM-Net is pre-trained on Scene Flow [26] and ﬁne-tuned
on KITTI with L1 regression loss. PWC-Net is pre-trained
on FlyingChairs [11] and FlyingThings [26] then ﬁne-tuned
over KITTI, with weighted L1 regression loss.

4. Experiments

In this section we ﬁrst describe the experimental setup.
Next we evaluate our model based on pixel-level scene ﬂow
metric and instance-level rigid motion metric. Finally we
comprehensively study the characteristic of our model.

4.1. Dataset and Implementation Details

Data: We validate our approach on the KITTI scene ﬂow
dataset [27]. The dataset consists of 200 sets of training
images and 200 sets of test images, captured on real world
driving scenarios. Following [8], we divide the training data
into train, val splits based on the 4:1 ratio.

Implementation details: For foreground objects, we use
all energy terms. The weights are set to 1. For background,
we only use photometric term (see ablation study). We run
RANSAC 5 times and use the one with lowest mean en-
ergy as initialization. We unroll the GN solver for 50 steps.
The solver terminates early if the energy reaches plateau. In
practice, best energy are often reached within 10 iterations.

4.2. Scene Flow Estimation

Comparison to the state-of-the-art: We compare our
approach against the leading methods on the benchmark1:
ISF [3], PRSM [42], OSF+TC [28], SSF [34], OSF [27],
and CSF [25]. Note that in addition to the standard two ad-
jacent frames, PRSM and OSF+TC rely on extra temporal
frames. As shown in Tab. 1, our approach (DRISF) out-
performs all previous methods by a signiﬁcant margin in
both runtime and outliers ratio. It achieves state-of-the-art
performance on almost every entry. DRISF reduces the D1
outliers ratio by 43%, the D2 outliers ratio by 32%, and the
ﬂow outliers ratio by 24%. Comparing to ISF model [3],
our scene ﬂow error is 22% lower and our runtime is 800
times faster. Fig. 1 compares the performance and runtime
of all methods.

Qualitative results: To better understand the pros and
cons of our approach, we visualize a few scene ﬂow results
on test set in Fig. 4. Scene ﬂow estimation is challenging
in these scenarios due to large vehicle motions, texture-less
regions, occlusion, and illumination variation. For the left-
most image, prior methods fail to estimate the vehicle’s mo-
tion and adjacent area due to the sun reﬂection and occlu-
sion. The saturated, high intensity pixels hinder photomet-
ric based approaches [27] from matching accurately. With
the help of detection and segmentation, ISF [3] is able to im-
prove the foreground estimation. Yet it still fails at the oc-
cluded background. In comparison, our approach is robust
to illumination changes and is able to handle the occlusion
by effectively separating the vehicle from the background.
It can also accurately estimate the motion of the small car
far away, as well as those of the trafﬁc sticks aside. As we
only train our Mask R-CNN on vehicles, it fails to segment
the train and hence the failure of our model. For the middle
image, the texture-less car has a large displacement and is
occluded in the second frame. While previous approaches
failed substantially, our method is able to produce accurate
motion estimation through the inferred ﬂow and disparity of

1As the validation performance of our PWC-Net (ﬁne-tuned on 160
images) performs slightly worse than the ofﬁcial one (ﬁne-tuned on all 200
images), we use their weights instead when submitting to the benchmark.
All other settings remain intact. We thank Deqing Sun for his help.

3619

Employed energy

Epho Ef low Erigid

X

X

X

X

X

X

Fl

Background outliers (%)
SF
D1
4.30
1.92
5.28
1.92
1.92
5.21

D2
2.69
2.56
2.56

3.71
4.72
4.63

Employed energy

Epho Ef low Erigid

X

X

X

X

X

X

Fl

Foreground outliers (%)
D1
SF
9.00
1.70
8.67
1.70
8.39
1.70

D2
4.25
4.58
4.56

7.57
6.98
6.73

Table 2: Contributions of each energy: As foreground objects sometimes are texture-less and have large displacement,
simple photometric term is not enough. In contrast, background is full of disriminative cues. Simple photometric error would
sufﬁce. Adding extra terms will introduce noises and hurt the performance. Please refer to the supp. material for full table.

Methods
PSM + PWC
Deep+RANSAC
Our Full DRISF

D1-all D2-all
1.89
(47.0)
2.75
1.89
1.89
2.89

Fl-all
11.0
7.65
4.10

SF-all
(50.8)
8.26
4.84

Table 3: Improvement over original ﬂow/stereo estima-
tion on validation set: The numbers in parenthis are ob-
tained by simply warping the disparity output with optical
ﬂow, without interpolation, occlusion handling, etc.

Module
Inference time
Module
Inference time

Stereo

409 ms / pair

RANSAC

Optical Flow
30 ms / pair
GN Solver

Segmentation
251 ms / pair

Total

93 ms / instance

244 ms / instance

746 ms / pair

Table 4: Runtime analysis. Modules within each building
block can be executed in parallel (see text for more details).

the remaining non-occluded part. The middle failure mode
is again due to the inaccurate segmentation.

4.3. 3D Rigid Motion Estimation

We now evaluate how good our DRISF model is at esti-
mating the 3D rigid motion. Towards this goal, we exploit
the ground truth optical ﬂow, disparity, and instance seg-
mentation provided in the KITTI scene ﬂow dataset to ﬁt a
least square rigid motion for each object instance in order
to create the ground truth rigid motion.

Curating KITTI scene ﬂow:
During ﬁtting, we dis-
cover two critical issues with KITTI: ﬁrst, there are mis-
alignments between GT ﬂow/disparity and GT segmen-
tation. Second, the scale ﬁtting of the same 3D CAD
model employed to compute ground truth changes some-
times across frames. The ﬁrst issue is due to the fact that
the GT are collected via different means and thus not con-
sistent. While the GT ﬂow and GT disparity are obtained
from the ﬁtted 3D CAD models, the GT segmentation are
based on human annotation. To address this, we ﬁrst use
the GT segmentation mask to deﬁne each object instance.
We then ﬁt a rigid motion using the GT ﬂow and GT dispar-
ity of each instance via least squares. Since some boundary
pixels may be mis-labeled by the annotators, for each pixel
around the boundary we search if there are other instances
in the surrounding area, and if there are, we transform the
pixel with their rigid motion.
If their rigid motion better
explains the pixel’s 3D movement, i.e.
the 3D distance is
closer, then we assign the pixel to that instance. At the end,

we perform the least square ﬁtting again with the new pixel
assignment. Unfortunately, even after re-labeling, there are
still a few vehicle instances where the rigid motion cannot
be explained. After careful diagnose, we notice that this is
because the scale of the CAD model changes across frames.
To verify our hypothesis, we compute the eigen decompo-
sition for the same instance across frames.
Ideally if the
scale of the instance does not change much, the eigen value
should be roughly the same. Yet we discover a few exam-
ples where the largest eigen value changes by 7%. We sim-
ply prune those instances as the GT is not accurate.

3D Motion evaluation: Most scene ﬂow methods are
pixel-based or adopted a piece-wise rigid setting. It is un-
clear how to aggregate their estimation into instance-based
motion model without affecting their performance. In light
of this, we exploit the motion initialization of our GN Solver
as baseline. We take the output of the deep nets and ap-
ply RANSAC to ﬁnd the best rigid motion. We denote
it as Deep+RANSAC. As shown in Tab. 3, this baseline
is very competitive. Its performance is comparable to, or
even better than prior state-of-the-art. We evaluate our mo-
tion model based on translation error and angular error. As
shown in Fig. 5, over 80% of the vehicles have translation
error less than 1m and angular error less than 1.3◦. Further-
more, most vehicles with translation error larger than 1m is
at least 20m away. In general, both error slightly increase
with distance. This is expected as the farther the vehicle
is, the less observations we have. The translation error and
angular error are also strongly correlated.

Visual odometry: The odometry of the ‘ego-car’ can be
computed by estimating the background movement. As a
proof-of-concept, we compute the per frame odometry er-
ror on the validation images. On average our motion model
drifts 0.09m and 0.24◦ every 10m. Fig. 6 shows the de-
tailed odometry error w.r.t. the travel distance. We note that
the current result is without any pose ﬁlter, loop closure, etc.
We plan to exploit this direction further in the future.

4.4. Analysis

Ablation study: To understand the effectiveness of each
energy term on background and foreground objects, we
evaluate our model with different energy combinations. As
shown in Tab. 2, best performance is achieved for fore-

3620

Before (PWC)

After (DRISF)

Before (PSM+Warp)

After (DRISF)

Figure 7: Improvement over original ﬂow/stereo: DRISF improves the overall performance. It is especially effective on
texture-less regions (e.g. window of the black car on the left) and occluded areas (right).

ground objects when using all energy terms, while for back-
ground the error is lowest when employing only photomet-
ric term. This can be explained by the fact that vehicles
are often texture-less, and sometimes have large displace-
ments. If we only employ photometric term, it will be very
difﬁcult to establish correspondences and handle drastic ap-
pearances changes. With the help of ﬂow and rigid term, we
can guide the motion and reduce such effect, and deal with
occlusions. In contrast, background is full of discriminative
textures and has relatively small motion, which is ideal for
photometric term. Adding other terms may introduce extra
noise and degrade the performance.

Comparison against original ﬂow/disparity: Through
exploiting the structure between visual cues and occlusion
handling, our model is able to improve the performance
both quantitatively (Tab. 3) and qualitatively (Fig. 7).
The object motion estimation is better, the boundaries are
sharper, and the occlusion error is greatly reduced, sug-
gesting that incorporating prior knowledge, such as pixels
of same instance should have same rigid motion, into the
model is crucial for the task.

Potential improvement To understand the potential gain
we may enjoy when improving each module, we sequen-
tially replace the input to our solver with ground truth, one
by one, and evaluate our model. Replacing D1 and ﬂow
with GT reduce the scene ﬂow error rate by 8% and 21%
respectively, while substituting GT for segmentation does
not improve the results. This suggests that there are still
space for ﬂow and stereo modules to improve.

Runtime analysis We benchmark the runtime of each
component in the model during inference in Tab. 4. The
whole inference pipeline can be decomposed into three se-
quential stages: visual cues extraction, occlusion reason-
ing, and optimization. As modules within the same stage
are independent, they can be executed in parallel. Further-
more, modern self-driving vehicles are equipped with mul-
tiple GPUs. The runtime for each stage is thus the max
over all parallel modules. In practice, we exploit two Nvidia
1080Ti GPUs to extract the visual cues: one for PSM-Net,

and the other for Mask R-CNN and PWC-Net. Currently,
the stereo module takes more than 50% of the overall time.
This is largely due to the 3D CNN cost aggregation and
the stacked hourglass reﬁnement. In the future, we plan to
investigate other faster yet reliable stereo networks. The
runtime of the GN solver depends highly on the number
of steps we unroll and the number of points we consider.
Please refer to the supp. material for detailed analysis.

Limitations: DRISF has two main limitations: ﬁrst, it
heavily depends on the performance of the segmentation
network. If the segmentation module fails to detect a vehi-
cle, the vehicle will be treated as background and assigned
an inverse ego-car motion.
In this case, the 3D motion
might be completely wrong, even if the optical ﬂow net-
work accurately predicts its ﬂow. In the future we plan to
address this by jointly reasoning about instance segmenta-
tion and scene ﬂow. Second, the current energy functions
are highly ﬂow centric. Only the photometric term is in-
dependent of ﬂow. If the optical ﬂow network completely
failed, it would be difﬁcult for the solver to recover the cor-
rect motion. One possible solution is thus adding more
ﬂow-invariant energy terms, such as instance association
between adjacent frames.

5. Conclusion

In this paper we develop a novel deep structured model
for 3D scene ﬂow estimation. We focus on the self-driving
scenario where the motion of the scene can be composed by
estimating the 3D rigid motion of each actor. We ﬁrst ex-
ploit deep learning to extract visual cues for each instance.
Then we employ multiple geometry based energy functions
to encode the structural geometric relationship between
them. Through optimizing the energy function, we can rea-
son the 3D motion of each trafﬁc participant, and thus scene
ﬂow. All operations, including the Gassian-Newton solver,
are done in GPU. Our method acheives state-of-the-art per-
formance on the KITTI scene ﬂow dataset. It outperforms
all previous methods by a huge margin in both runtime and
accuracy. Comparing to prior art, DRISF is 22% better
while being two to three orders of magnitude faster.

3621

References

[1] Min Bai, Wenjie Luo, Kaustav Kundu, and Raquel Urtasun. Ex-
ploiting semantic information and deep matching for optical ﬂow. In
ECCV, 2016. 2

[2] Tali Basha, Yael Moses, and Nahum Kiryati. Multi-view scene ﬂow

estimation: A view centered variational approach. IJCV, 2013. 2

[3] Aseem Behl, Omid Hosseini Jafari, Siva Karthik Mustikovela, Has-
san Abu Alhaija, Carsten Rother, and Andreas Geiger. Bound-
ing boxes, segmentations and object coordinates: How important is
recognition for 3d scene ﬂow estimation in autonomous driving sce-
narios? In ICCV, 2017. 1, 2, 4, 6

[4] Michael Black and T. Anandan. The robust estimation of multiple
motions: Parametric and piecewise-smooth ﬂow ﬁelds. CVIU, 1996.
2

[5] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cam-

bridge university press, 2004. 1

[6] Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim Weick-
ert. High accuracy optical ﬂow estimation based on a theory for
warping. In ECCV, 2004. 2

[7] Emmanuel J Candes, Michael B Wakin, and Stephen P Boyd. En-
hancing sparsity by reweighted l1 minimization. Journal of Fourier
analysis and applications, 2008. 5

[23] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry,
Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end
learning of geometry and context for deep stereo regression. 2017. 2
[24] Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Efﬁcient

deep learning for stereo matching. In CVPR, 2016. 1, 2

[25] Zhaoyang Lv, Chris Beall, Pablo F Alcantarilla, Fuxin Li, Zsolt Kira,
and Frank Dellaert. A continuous optimization approach for efﬁcient
and accurate scene ﬂow. In ECCV, 2016. 2, 4, 6

[26] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel
Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset
to train convolutional networks for disparity, optical ﬂow, and scene
ﬂow estimation. In CVPR, 2016. 1, 2, 6

[27] Moritz Menze and Andreas Geiger. Object scene ﬂow for au-

tonomous vehicles. In CVPR, 2015. 1, 2, 4, 6

[28] Michal Neoral and Jan ˇSochman. Object scene ﬂow with temporal

consistency. In CVWW, 2017. 4, 6

[29] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass

networks for human pose estimation. In ECCV, 2016. 2

[30] Nils Papenberg, Andr´es Bruhn, Thomas Brox, Stephan Didas, and
Joachim Weickert. Highly accurate optic ﬂow computation with the-
oretically justiﬁed warping. IJCV, 2006. 2

[31] Jean-Philippe Pons, Renaud Keriven, and Olivier Faugeras. Multi-
view stereo reconstruction and scene ﬂow estimation with a global
image-based matching score. IJCV, 2007. 2

[8] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching net-

[32] Anurag Ranjan and Michael J Black. Optical ﬂow estimation using

work. In CVPR, 2018. 2, 3, 6

[9] Zhuoyuan Chen, Xun Sun, Liang Wang, Yinan Yu, and Chang
Huang. A deep visual correspondence embedding model for stereo
matching costs. In ICCV, 2015. 2

[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,
Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,
and Bernt Schiele. The cityscapes dataset for semantic urban scene
understanding. In CVPR, 2016. 3

[11] Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H¨ausser, Caner
Hazırbas¸, Vladimir Golkov, Patrick Van der Smagt, Daniel Cremers,
and Thomas Brox. Flownet: Learning optical ﬂow with convolu-
tional networks. arXiv, 2015. 1, 2, 6

[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for
autonomous driving? the kitti vision benchmark suite. In Conference
on Computer Vision and Pattern Recognition (CVPR), 2012. 3

[13] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick.

Mask r-cnn. In ICCV, 2017. 3

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial
pyramid pooling in deep convolutional networks for visual recogni-
tion. In ECCV, 2014. 2, 3

[15] Heiko Hirschmuller. Stereo processing by semiglobal matching and

mutual information. TPAMI, 2008. 2

[16] William Hoff and Narendra Ahuja. Surfaces from stereo: Integrat-
ing feature matching, disparity estimation, and contour detection.
TPAMI, 1989. 2

[17] Berthold KP Horn and Brian G Schunck. Determining optical ﬂow.

Artiﬁcial intelligence, 1981. 2

[18] Fr´ed´eric Huguet and Fr´ed´eric Devernay. A variational method for

scene ﬂow estimation from stereo sequences. In ICCV, 2007. 2

[19] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Liteﬂownet: A
lightweight convolutional neural network for optical ﬂow estimation.
In CVPR, 2018. 2

[20] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey
Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical
ﬂow estimation with deep networks. In CVPR, 2017. 2

[21] Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas Brox. Oc-
clusions, motion and depth boundaries with a generic network for
disparity, optical ﬂow or scene ﬂow estimation. 2018. 2

[22] Takeo Kanade and Masatoshi Okutomi. A stereo matching algorithm
with an adaptive window: Theory and experiment. In ICRA, 1991. 2

a spatial pyramid network. In CVPR, 2017. 1, 2

[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
r-cnn: Towards real-time object detection with region proposal net-
works. In NIPS, 2015. 3

[34] Zhile Ren, Deqing Sun, Jan Kautz, and Erik Sudderth. Cascaded
scene ﬂow prediction using semantic segmentation. In 3DV, 2017. 2,
4, 6

[35] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia
Schmid. Epicﬂow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. In CVPR, 2015. 2

[36] Deqing Sun, Stefan Roth, and Michael J Black. Secrets of optical

ﬂow estimation and their principles. In CVPR, 2010. 4

[37] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Models
matter, so does training: An empirical study of cnns for optical ﬂow
estimation. arXiv, 2018. 2

[38] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net:
Cnns for optical ﬂow using pyramid, warping, and cost volume. In
CVPR, 2018. 1, 2, 3

[39] Levi Valgaerts, Andr´es Bruhn, Henning Zimmer, Joachim Weickert,
Carsten Stoll, and Christian Theobalt. Joint estimation of motion,
structure and geometry from stereo sequences. In ECCV, 2010. 2

[40] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins, and
Takeo Kanade. Three-dimensional scene ﬂow. In ICCV, 1999. 1,
2

[41] Christoph Vogel, Konrad Schindler, and Stefan Roth. Piecewise rigid

scene ﬂow. In ICCV, 2013. 1, 2

[42] Christoph Vogel, Konrad Schindler, and Stefan Roth. 3d scene ﬂow

estimation with a piecewise rigid scene model. IJCV, 2015. 4, 6

[43] Shenlong Wang, Linjie Luo, Ning Zhang, and Jia Li. Autoscaler:
Scale-attention networks for visual correspondence. arXiv, 2016. 2
[44] Koichiro Yamaguchi, David McAllester, and Raquel Urtasun. Efﬁ-
cient joint segmentation, occlusion labeling, stereo and ﬂow estima-
tion. In ECCV, 2014. 1, 2

[45] Sergey Zagoruyko and Nikos Komodakis. Learning to compare im-

age patches via convolutional neural networks. In CVPR, 2015. 2

[46] Jure Zbontar and Yann LeCun. Computing the stereo matching cost

with a convolutional neural network. In CVPR, 2015. 1, 2

[47] Jure Zbontar and Yann LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. JMLR, 2016.
2

[48] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and

Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 2, 3

3622

