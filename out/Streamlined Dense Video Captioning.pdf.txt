Streamlined Dense Video Captioning

Jonghwan Mun1

5 ∗

,

Linjie Yang2

Zhou Ren3

Ning Xu4

Bohyung Han5

1POSTECH 2ByteDance AI Lab 3Wormpex AI Research 4Amazon Go 5Seoul National University

1

jonghwan.mun@postech.ac.kr

2

linjie.yang@bytedance.com

3

zhou.ren@bianlifeng.com

4

ninxu@amazon.com

5

bhhan@snu.ac.kr

Abstract

Dense video captioning is an extremely challenging task
since accurate and coherent description of events in a video
requires holistic understanding of video contents as well as
contextual reasoning of individual events. Most existing ap-
proaches handle this problem by ﬁrst detecting event pro-
posals from a video and then captioning on a subset of the
proposals. As a result, the generated sentences are prone
to be redundant or inconsistent since they fail to consider
temporal dependency between events. To tackle this chal-
lenge, we propose a novel dense video captioning frame-
work, which models temporal dependency across events in
a video explicitly and leverages visual and linguistic context
from prior events for coherent storytelling. This objective
is achieved by 1) integrating an event sequence generation
network to select a sequence of event proposals adaptively,
and 2) feeding the sequence of event proposals to our se-
quential video captioning network, which is trained by re-
inforcement learning with two-level rewards—at both event
and episode levels—for better context modeling. The pro-
posed technique achieves outstanding performances on Ac-
tivityNet Captions dataset in most metrics.

1. Introduction

Understanding video contents is an important topic in
computer vision. Through the introduction of large-scale
datasets [9, 31] and the recent advances of deep learning
technology, research towards video content understanding
is no longer limited to activity classiﬁcation or detection
and addresses more complex tasks including video caption
generation [1, 4, 13, 14, 15, 22, 23, 26, 28, 30, 33, 35, 36].
Video captions are effective for holistic video descrip-
tion. However, since videos usually contain multiple in-
terdependent events in context of a video-level story (i.e.
episode), a single sentence may not be sufﬁcient to describe
videos. Consequently, dense video captioning task [8] has

∗This work was done during the internship program at Snap Research.

Figure 1. An example of dense video captioning about a busking
episode, which is composed of four interdependent events.

been introduced and getting more popular recently. This
task is conceptually more complex than simple video cap-
tioning since it requires detecting individual events in a
video and understanding their context. Fig. 1 presents an
example of dense video captioning for a busking episode,
which is composed of four ordered events. Despite the com-
plexity of the problem, most existing methods [8, 10, 27,
37] are limited to describing an event using two subtasks—
event detection and event description—in which an event
proposal network is in charge of detecting events and a cap-
tioning network generates captions for the selected propos-
als independently.

We propose a novel framework for dense video caption-
ing, which considers the temporal dependency of the events.
Contrary to existing approaches shown in Fig. 2(a), our al-
gorithm detects event sequences from videos and generates
captions sequentially, where each caption is conditioned on
prior events and captions as illustrated in Fig. 2(b). Our al-
gorithm has the following procedure. First, given a video,
we obtain a set of candidate event proposals from an event
proposal network. Then, an event sequence generation net-
work selects a series of ordered events adaptively from the
event proposal candidates. Finally, we generate captions for
the selected event proposals using a sequential captioning
network. The captioning network is trained via reinforce-
ment learning using both event and episode-level rewards;

16588

𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑐𝑐1𝑐𝑐2𝑐𝑐3𝑐𝑐4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑐𝑐1𝑐𝑐2𝑐𝑐3𝑐𝑐4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑐𝑐1𝑐𝑐2𝑐𝑐3𝑐𝑐4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑐𝑐1𝑐𝑐2𝑐𝑐3𝑐𝑐4time𝑒𝑒1: an elderly man is playing the piano in front of a crowd𝑒𝑒2: a woman walks to the piano and briefly talks to the elderly man𝑒𝑒3: the woman starts singing along with the pianist𝑒𝑒4: eventually the elderly man finishes playing and hugs the woman, and the crowd applaudEpisode: busking𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑐𝑐1𝑐𝑐2𝑐𝑐3𝑐𝑐4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑐𝑐1𝑐𝑐2𝑐𝑐3𝑐𝑐4→→→joint embedding space of sentences and videos [14], hier-
archical recurrent encoder [1, 13], attribute-augmented de-
coder [4, 15, 36], multimodal memory [28], and reconstruc-
tion loss [26]. Despite their impressive performances, they
are limited to describing a video using a single sentence
and can be applied only to a short video containing a single
event. Thus, Yu et al. [35] propose a hierarchical recurrent
neural network to generate a paragraph for a long video,
while Xiong et al. [30] introduce a paragraph generation
method based on event proposals, where an event selection
module determines which proposals need to be utilized for
caption generation in a progressive way. Contrary to these
tasks, which simply generate a sentence or paragraph for an
input video, dense video captioning requires localizing and
describing events at the same time.

2.2. Dense Video Captioning

Recent dense video captioning techniques typically at-
tempt to solve the problem using two subtasks—event de-
tection and caption generation [8, 10, 27, 37]; an event pro-
posal network ﬁnds a set of candidate proposals and a cap-
tioning network is employed to generate a caption for each
proposal independently. The performance of the methods is
affected by the manual thresholding strategies to select the
ﬁnal event proposals for caption generation.

Based on the framework, Krishna et al. [8] adopt a multi-
scale action proposal network [3], and introduce a caption-
ing network that exploits visual context from past and future
events with an attention mechanism. In [27], a bidirectional
RNN is employed to improve the quality of event propos-
als and a context gating mechanism in caption generation
is proposed to adaptively control the contribution of sur-
rounding events. Li et al. [10] incorporate temporal coor-
dinate and descriptiveness regressions for precise localiza-
tion of event proposals, and adopt the attribute-augmented
captioning network [34]. Rennie et al. [37] utilize a self-
attention [20] for event proposal and captioning networks,
and propose a masking network for conversion of the event
proposals to differentiable masks and end-to-end learning
of the two networks.

In contrast to the prior works, our algorithm identiﬁes a
small set of representative event proposals (i.e., event se-
quences) for sequential caption generation, which enables
us to generate coherent and comprehensive captions by ex-
ploiting both visual and linguistic context across selected
events. Note that the existing works fail to take advantage
of linguistic context since the captioning network is applied
to event proposals independently.

3. Our Framework

This section describes our main idea and the deep neural

network architecture for our algorithm in detail.

6589

Figure 2. Comparison between the existing approaches and ours
for dense video captioning. Our algorithm generates captions for
events sequentially conditioned on the prior ones by detecting an
event sequence in a video.

the event-level reward allows to capture speciﬁc content in
each event precisely while the episode-level reward drives
all generated captions to make a coherent story.

The main contributions of the proposed approach are

summarized as follows:

• We propose a novel framework of detecting event se-
quences for dense video captioning. The proposed
event sequence generation network allows the caption-
ing network to model temporal dependency between
events and generate a set of coherent captions to de-
scribe an episode in a video.

• We present reinforcement learning with two-level re-
wards, episode and event levels, which drives the cap-
tioning model to boost coherence across generated
captions and quality of description for each event.

• The proposed algorithm achieves the state-of-the-art
performance on the ActivityNet Captions dataset with
large margins compared to the methods based on the
existing framework.

The rest of the paper is organized as follows. We ﬁrst
discuss related works for our work in Section 2. The pro-
posed method and its training scheme are described in Sec-
tion 3 and 4 in detail, respectively. We present experimental
results in Section 5, and conclude this paper in Section 6.

2. Related Work

2.1. Video Captioning

Recent video captioning techniques often incorporate
the encoder-decoder framework inspired by success in im-
age captioning [11, 16, 17, 25, 32]. Basic algorithms [22,
23] encode a video using Convolutional Neural Networks
(CNNs) or Recurrent Neural Networks (RNNs), and decode
the representation into a natural sentence using RNNs. Then
various techniques are proposed to enhance the quality of
generated captions by integrating temporal attention [33],

𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4(a) Conventional approach𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑑𝑑1𝑑𝑑2𝑑𝑑3𝑑𝑑4Event Detection(b) Our approachEvent Sequence Detection𝑒𝑒1𝑒𝑒2𝑒𝑒3𝑒𝑒4𝑑𝑑1𝑑𝑑2𝑑𝑑3𝑑𝑑4→→→Figure 3. Overall framework of the proposed algorithm. Given an input video, our algorithm ﬁrst extracts a set of candidate event proposals
(p1, p2, p3, p4, p5) using the Event Proposal Network (Section 3.2). From the candidate set, the Event Sequence Generation Network
detects an event sequence (ˆe1 → ˆe2 → ˆe3) by selecting one out of the candidate event proposals (Section 3.3). Finally, the Sequential
Captioning Network takes the detected event sequence and sequentially generates captions ( ˆd1, ˆd2, ˆd3) conditioned on preceding events
(Section 3.4). The three models are trained in a supervised manner (Section 4.1) and then the Sequential Captioning Network is optimized
additionally with reinforcement learning using two-level rewards (Section 4.2).

3.1. Overview

Let a video V contain a set of events E = {e1, . . . , eN}
with corresponding descriptions D = {d1, . . . , dN}, where
N events are temporally localized using their starting and
ending time stamps. Existing methods [8, 10, 27, 37] typ-
ically divide the whole problem into two steps: event de-
tection followed by description of detected events. These
algorithms train models by minimizing the sum of negative
log-likelihoods of event and caption pairs as follows:

N

L =

X
n=1− log p(dn, en|V )

N

=

X
n=1− log p(en|V )p(dn|en, V ).

(1)

However, events in a video have temporal dependency
and should be on a story about a single topic. Therefore, it
is critical to identify an ordered list of events to describe a
coherent story corresponding to the episode, the composi-
tion of the events. With this in consideration, we formulate
dense video captioning as detection of an event sequence
followed by sequential caption generation as follows:

L = − log p(E,D|V )
Y

= − log p(E|V )

N

n=1

p(dn|d1, . . . , dn−1,E, V ).

(2)

The overall framework of our proposed algorithm is il-
lustrated in Fig. 3. For a given video, a set of candidate

event proposals is generated by the event proposal network.
Then, our event sequence generation network provides a se-
ries of events by selecting one of candidate event propos-
als sequentially, where the selected proposals correspond
to events comprising an episode in the video. Finally, we
generate captions from the selected proposals using the pro-
posed sequential captioning network, where each caption is
generated conditioned on preceding proposals and their cap-
tions. The captioning network is trained via reinforcement
learning using event and episode-level rewards.

3.2. Event Proposal Network (EPN)

EPN plays a key role in selecting event candidates. We
adopt Single-Stream Temporal action proposals (SST) [2]
due to its good performance and efﬁciency in ﬁnding se-
mantically meaningful temporal regions via a single scan
of videos. SST divides an input video into a set of non-
overlapping segments with a ﬁxed length (e.g., 16 frames),
where the representation of each segment is given by a 3D
convolution (C3D) network [19]. By treating each segment
as an ending point of an event proposal, SST identiﬁes its
matching starting points from the k preceding segments,
which are represented by k-dimensional output vector from
a Gated Recurrent Unit (GRU) at each time step. After ex-
tracting the top 1,000 event proposals, we obtain M candi-
date proposals, P = {p1, . . . , pM}, by eliminating highly
overlapping ones using non-maximum suppression. Note
that EPN provides a representation of each proposal p ∈ P,
which is a concatenated vector of two hidden states at start-
ing and ending segments in SST. This visual representation,
denoted by Vis(p), is utilized for the other two networks.

6590

Sequential Captioning NetworkRNNε�e1RNNe�d1RNNε�e2RNNe�d2RNNε�e3RNNe�d3Event EvaluatorGround-Truth CaptionsEpisode EvaluatorEvent Proposal Networkp1p2p3p4p5C3DC3DC3DC3DC3DGRUGRUGRUGRUGRUEvent Sequence GenerationNetworkPointer Network (PtrNet)�e𝑒𝑒𝑒𝑒𝑒𝑒�e𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠locationvisualp4p5p3p2END�e1�e2�e3p5p3p2p1pend𝑝𝑝𝑒𝑒𝑒𝑒𝑒𝑒𝑝𝑝𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠visual�e3(p5)�e2(p3)�e1(p2)100�e1�e2�e3�e4RNNencRNNencRNNencRNNencRNNencRNNptrRNNptrRNNptrRNNptr3.3. Event Sequence Generation Network (ESGN)

3.4. Sequential Captioning Network (SCN)

Given a set of candidate event proposals, ESGN selects
a series of events that are highly correlated and make up an
episode for a video. To this ends, we employ a Pointer Net-
work (PtrNet) [24] that is designed to produce a distribution
over the input set using a recurrent neural network by adopt-
ing an attention module. PtrNet is well-suited for selecting
an ordered subset of proposals and generating coherent cap-
tions with consideration of their temporal dependency.

As shown in Fig. 3, we ﬁrst encode a set of candidate
proposals, P, by feeding proposals to an encoder RNN in
an increasing order of their starting times, and initialize the
ﬁrst hidden state of PtrNet with the encoded representations
to guide proposal selection. At each time step in PtrNet, we
compute likelihoods at over the candidate event proposals
and select a proposal with the highest likelihood out of all
available proposals. The procedure is repeated until PtrNet
happens to select the END event proposal, pend, which is a
special proposal to indicate the end of an event sequence.

The whole process is summarized as follows:

hptr
0 = RNNenc(Vis(p1), . . . , Vis(pM )),
hptr
t = RNNptr(u(ˆet−1), hptr
at = ATT(hptr

t , u(p0), . . . , u(pM )),

t−1),

(3)

(4)

(5)

where hptr is a hidden state in PtrNet, ATT() is an atten-
tion function computing conﬁdence scores over proposals,
and the representation of proposal p in PtrNet, u(p) =
[Loc(p); Vis(p)], is given by visual information Vis(p) as
well as the location information Loc(p). Also, ˆet is a se-
lected event proposal at time step t, which is given by

ˆet = pj ∗ and j∗ = arg max
j∈{0,...,M }

aj
t ,

(6)

where p0 corresponds to pend. Note that the location feature,
Loc(p), is a binary mask vector, where the elements corre-
sponding to temporal intervals of an event are set to 1s and
0s otherwise. This is useful to identifying and disregarding
proposals that overlap strongly with previous selections.

Our ESGN has clear beneﬁts for dense video captioning.
Speciﬁcally, it determines the number and order of events
adaptively, which facilitates compact, comprehensive and
context-aware caption generation. Noticeably, there are too
many detected events in existing approaches (e.g., ≥ 50)
given by manual thresholding. On the contrary, ESGN de-
tects only 2.85 on average, which is comparable to the av-
erage number of events per video in ActivityNet Caption
dataset, 3.65. Although sorting event proposals is an ill-
deﬁned problem, due to their two time stamps (starting and
ending points), ESGN naturally learns the number and order
of proposals based on semantics and contexts in individual
videos in a data-driven manner.

SCN employs a hierarchical recurrent neural network to
generate coherent captions based on the detected event se-

quence ˆE = {ˆe1, . . . , ˆeNs}, where Ns(≤ M ) is the number

of selected events. As shown in Fig. 3, SCN consists of
two RNNs—an episode RNN and an event RNN—denoted
by RNNE and RNNe, respectively. The episode RNN takes
the proposals in a detected event sequence one by one and
models the state of an episode implicitly, while the event
RNN generates words in caption sequentially for each event
proposal conditioned on the implicit representation of the
episode, i.e., based on the current context of the episode.

Formally, the caption generation process for the tth event

proposal ˆet in the detected event sequence is given by

rt = RNNE (Vis(ˆet), gt−1, rt−1),
gt = RNN∗
e(C3D(ˆet), Vis(ˆet), rt),

(7)

(8)

where rt is an episodic feature from the tth event proposal,
and gt is a generated caption feature given by the last hidden
state of the unrolled (denoted by ∗) event RNN. C3D(ˆet)
denotes a set of C3D features for all segments lying in tem-
poral intervals of tth event proposal. The episode RNN pro-
vides the current episodic feature so that the event RNN
generates context-aware captions, which are given back to
the episode RNN.

Although both networks can be implemented with any
RNNs conceptually, we adopt a single-layer Long Short-
Term Memory (LSTM) with a 512 dimensional hidden state
as the episode RNN, and a captioning network with tem-
poral dynamic attention and context gating (TDA-CG) pre-
sented in [27] as the event RNN. TDA-CG generates words
from a feature computed by gating a visual feature Vis(e)
and an attended feature obtained from segment feature de-
scriptors C3D(e).

Note that sequential captioning generation scheme en-
ables to exploit both visual context (i.e. how other events
look) and linguistic context (i.e. how other events are de-
scribed) across events, and allows us to generate captions in
an explicit context. Although existing methods [8, 27] also
utilize context for caption generation, they are limited to vi-
sual context and model with no linguistic dependency due
to their architectural constraints from independent caption
generation scheme, which would result in inconsistent and
redundant caption generation.

4. Training

We ﬁrst learn the event proposal network and ﬁx its pa-
rameters during training of the other two networks. We
train the event sequence generation network and the sequen-
tial captioning network in a supervised manner, and further
optimize the captioning network based on reinforcement
learning with two-level rewards—event and episode levels.

6591

4.1. Supervised Learning

4.2. Reinforcement Learning

Event Proposal Network Let ck
t be the conﬁdence of the
kth event proposal at time step t in EPN, which is SST [2]
in our algorithm. Denote the ground-truth label of the pro-
posal by yk
t , which is set to 1 if the event proposal has a
temporal Intersection-over-Union (tIoU) with ground-truth
events larger than 0.5, and 0 otherwise. Then, for a given
video V and ground-truth labels y, we train EPN by mini-
mizing a following weighted binary cross entropy loss:

LEPN(V,Y) =
X

X

Tc

K

−

t=1

k=1

yk
t log ck

t + (1 − yk

t ) log(1 − ck
t ),

(9)

where Y = {yk
t |1 ≤ t ≤ Tc, 1 ≤ k ≤ K}, K is the number
of proposals containing each segment at the end and Tc is
the number of segments in the video.

Event Sequence Generation Network For a video with
ground-truth event sequence E = {e1, . . . , eN} and a set of
candidate event proposals P = {p1, . . . , pM}, the goal of
ESGN is to select a proposal p highly overlapping with the
ground-truth event e, which is achieved by minimizing the
following sum of binary cross entropy loss:

LESGN(V,P,E) = −

N

M

X

n=1

X

m=1

tIoU(pm, en) log am
n

(10)

+ (1 − tIoU(pm, en)) log(1 − am
n ),

where tIoU(·,·) is a temporal Intersection-over-Union value
between two proposals, and am
n is the likelihood that the mth
event proposal is selected as the nth event.

Sequential Captioning Network We utilize the ground-
truth event sequence E and its descriptions D to learn our
SCN via the teacher forcing technique [29]. Speciﬁcally,
to learn two RNNs in SCN, we provide episode RNN and
event RNN with ground-truth events and captions as their
inputs, respectively. Then, the captioning network is trained
by minimizing negative log-likelihood over words of the
ground-truth captions as follows:

LSCN(V,E,D) = −

N

X

n=1

log p(dn|en)

(11)

N

X

n=1

Tdn

X

t=1

= −

log p(wt

n|w1

n, . . . , wt−1

n , en),

where p(·) denotes a predictive distribution over word vo-
cabulary from the event RNN, and wt
n and Tdn mean the tth
ground-truth word and the length of ground-truth descrip-
tion for the nth event.

Inspired by the success in image captioning task [16, 17],
we further employ reinforcement learning to optimize SCN.
While similar to the self-critical sequence training [17] ap-
proach, the objective of learning our captioning network is
revised to minimize the negative expected rewards for sam-
pled captions. The loss is formally given by

SCN(V, ˆE, ˆD) = −
LRL

Ns

X

n=1

E ˆdn hR( ˆdn)i ,

(12)

where ˆD = { ˆd1, . . . , ˆdNS} is a set of sampled descriptions
from the detected event sequence ˆE with Ns events from
ESGN, and R( ˆd) is a reward value for the individual sam-
pled description ˆd. Then, the expected gradient on the sam-
ple set ˆD is given by

∇LRL

SCN(V, ˆE, ˆD) = −

≈ −

Ns

X

n=1

Ns

X

n=1

E ˆdn hR( ˆdn)∇ log p( ˆdn)i

R( ˆdn)∇ log p( ˆdn).

(13)

We adopt a reward function with two levels: episode and
event levels. This encourages models to generate coherent
captions by reﬂecting the overall context of videos, while
facilitating the choices of better word candidates in describ-
ing individual events depending on the context. Also, mo-
tivated by [6, 16, 17], we use the rewards obtained from
the captions generated with ground-truth proposals as base-
lines, which is helpful to reduce the variance of the gradient
estimate. This drives models to generate captions at least as
competitive as the ones generated from ground-truth pro-
posals, although the intervals of event proposals are not ex-
actly aligned with those of ground-truth proposals. Specif-

ically, for a sampled event sequence ˆE, we ﬁnd a refer-
ence event sequence ˜E = {˜e1, . . . , ˜eNs} and its descriptions
˜D = { ˜d1, . . . , ˜dNs}, where the reference event ˜e is given by
one of the ground-truth proposals with the highest overlap-
ping ratio with sampled event ˆe. Then, the reward for the
nth sampled description ˆdn is given by

R( ˆdn) =
hf ( ˆdn, ˜dn) − f ( ˇdn, ˜dn)i + hf ( ˆD, ˜D) − f ( ˇD, ˜D)i ,

(14)

where f (·,·) returns a similarity score between two cap-
tions or two set of captions, and ˇD = { ˇd1, . . . , ˇdNs} de-
note the generated descriptions from the reference event se-
quence. Both terms in Eq. (14) encourage our model to in-
crease the probability of sampled descriptions whose scores
are higher than the results of generated captions from the
ground-truth event proposals. Note that the ﬁrst and sec-
ond terms are computed on the current event and episode,

6592

Table 1. Event detection performances including recall and precision at four thresholds of temporal intersection of unions (@tIoU) on the
ActivityNet Captions validation set. The bold-faced numbers mean the best performance for each metric.

Method

Recall (@tIoU)

Precision (@tIoU)

@0.3 @0.5 @0.7 @0.9 Average @0.3 @0.5 @0.7 @0.9 Average

MFT [30]

ESGN (ours)

46.18
93.41

29.76
76.40

15.54
42.40

5.77
10.10

24.31
55.58

86.34
96.71

68.79
77.73

38.30
44.84

12.19
10.99

51.41
57.57

Table 2. Dense video captioning results including Bleu@N (B@N), CIDEr (C) and METEOR (M) for our model and other state-of-the-art
methods on ActivityNet Captions validation set. We report performances obtained from both ground-truth (GT) proposals and learned
proposals. Asterisk (∗) stands for the methods re-evaluated using the newer evaluation tool and star (⋆) indicates the methods exploiting
additional modalities (e.g. optical ﬂow and attribute) for video representation. The bold-faced numbers mean the best for each metric.

Method

DCE [8]
DVC [10]⋆

Masked Transformer [37]∗⋆

TDA-CG [27]∗

MFT [30]

SDVC (ours)

with GT proposals

with learned proposals

B@1

B@2 B@3 B@4

C

M

B@1 B@2 B@3 B@4

C

18.13
19.57
23.93

8.43
9.90
12.16

-
-

-
-

4.09
4.55
5.76

-
-

1.60
1.62
2.71

-
-

25.12
25.24
47.71

-
-

8.88
10.33
11.16
10.89

-

28.02

12.05

4.41

1.28

43.48

13.07

10.81
12.22
9.96
10.75
13.31
17.92

4.57
5.72
4.81
5.06
6.13
7.99

1.90
2.27
2.42
2.55
2.82
2.94

0.71
0.73
1.15
1.31
1.24
0.93

12.43
12.61
9.25
7.99
21.00
30.68

M

5.69
6.93
4.98
5.86
7.08
8.82

respectively. We use two famous captioning metrics, ME-
TEOR and CIDEr, to deﬁne f (·,·).

5. Experiments

5.1. Dataset

We evaluate the proposed algorithm on the ActivityNet
Captions dataset [8], which contains 20k YouTube videos
with an average length of 120 seconds. The dataset consists
of 10,024, 4,926 and 5,044 videos for training, validation
and test splits, respectively. The videos have 3.65 tempo-
rally localized events and descriptions on average, where
the average length of the descriptions is 13.48 words.

5.2. Metrics

We use the performance evaluation tool1 provided by the
2018 ActivityNet Captions Challenge, which measures the
capability to localize and describe events2. For evaluation,
we measure recall and precision of event proposal detec-
tion, and METEOR, CIDEr and BLEU of dense video cap-
tioning. The scores of the metrics are summarized via their
averages based on tIoU thresholds of 0.3, 0.5, 0.7 and 0.9
given identiﬁed proposals and generated captions. We use
METEOR as the primary metric for comparison, since it is
known to be more correlated to human judgments than oth-
ers when only a small number of reference descriptions are
available [21].

Table 3. Results on ActivityNet Captions evaluation server.

RUC+CMU

YH Technologies
Shandong Univ.

SDVC (ours)

Audio Flow Visual Ensemble METEOR
√

√
√
√

√
√
√
√

yes
no
yes
no

8.53
8.13
8.11
8.19

5.3. Implementation Details

For EPN, we use a two-layer GRU with 512 dimensional
hidden states and generate 128 proposals at each ending
segment, which makes the dimensionality of ct in Eq. (9)
128.
In our implementation, EPN based on SST takes a
whole span of video for training as an input to the network,
this allows the network to consider all ground-truth propos-
als, while the original SST [2] is trained with densely sam-
pled clips given by the sliding window method.

For ESGN, we adopt a single-layer GRU and a single-
layer LSTM as EncoderRNN and RNNptr, respectively,
where the dimensions of hidden states are both 512. We rep-
resent the location feature, denoted by Loc(·), of proposals
with a 100 dimensional vector. When learning SGN with
reinforcement learning, we sample 100 event sequences for
each video and generate one caption for each event in the
event sequence with a greedy decoding. In all experiments,
we use Adam [7] to learn models with the mini-batch size 1
video and the learning rate 0.0005.

5.4. Comparison with Other Methods

1https://github.com/ranjaykrishna/densevid_eval
2On 11/02/2017, the ofﬁcial evaluation tool ﬁxed a critical issue; only
one out of multiple incorrect predictions for each video was counted. This
leads to performance overestimation of [27, 37]. Thus, we received raw re-
sults from the authors and reported the scores measured by the new metric.

We compare the proposed Streamlined Dense Video
Captioning (SDVC) algorithm with several existing state-
of-the-art methods including DCE [8], DVC [10], Masked
Transformer [37] and TDA-CG [27]. We additionally report

6593

Table 4. Ablation results of mean averaged recall, precision and METEOR over four tIoU thresholds of 0.3, 0.5, 0.7 and 0.9 on the Activ-
ityNet Captions validation set. We also present the number of proposals in average. The bold-faced number means the best performance.

Method

EPN-Ind
ESGN-Ind
ESGN-SCN
ESGN-SCN-RL (SDVC)

Proposal modules
EPN
√

ESGN

√
√
√

√
√
√
√

Captioning modules

eventRNN episodeRNN RL

Number of
proposals

Recall

Precision METEOR

77.99
2.85
2.85
2.85

84.97
55.58
55.58
55.58

28.10
57.57
57.57
57.57

4.58
6.73
6.92
8.82

√
√

√

the results of MFT [30], which is originally proposed for
video paragraph generation but its event selection module is
also able to generate an event sequence from the candidate
event proposals; it makes a choice between selecting each
proposal for caption generation and skipping it, and con-
structs an event sequence implicitly. For MFT, we compare
performances in both event detection and dense captioning.
Table 1 presents the event detection performances of
ESGN and MFT in ActivityNet Captions validation set.
ESGN outperforms the progressive event selection module
in MFT on most tIoUs with large margins, especially in re-
call. This validates the effectiveness of our proposed event
sequence selection algorithm.

Table 2 illustrates performances of dense video caption-
ing algorithms evaluated on ActivityNet Captions validation
set. We measure the scores with both ground-truth propos-
als and learned ones, where the number of the predicted
proposals in individual algorithms may be different; DCE,
DVC, Masked Transformer and TDA-CG uses 1,000, 1,000,
226.78 and 97.61 proposals in average, respectively, while
the average number of proposals in SDVC is only 2.85.
According to Table 2, SDVC improves the quality of cap-
tions signiﬁcantly compared to all other methods. Masked
Transformer achieves comparable performance to ours us-
ing ground-truth proposals, but does not work well with
learned proposals. Note that it uses optical ﬂow features
in addition to visual features, while SDVC is only trained
on visual features. Since the motion information from opti-
cal ﬂow features consistently improves the performances in
other video understanding tasks [12, 18], incorporating mo-
tion information to our model may lead to additional perfor-
mance gain. MFT has the highest METEOR score among
existing methods, which is partly because MFT considers
temporal dependency across captions.

Table 3 shows the test split results from the evaluation
server. SDVC achieves competitive performance based only
on basic visual features while other methods exploit addi-
tional modalities (e.g., audio and optical ﬂow) to represent
videos and/or ensemble models to boost accuracy as de-
scribed in [5].

5.5. Ablation Studies

We perform several ablation studies on ActivityNet Cap-
tions validation set to investigate the contributions of indi-

Table 5. Performance comparison varying reward levels in rein-
forcement learning on the ActivityNet Captions dataset.

Event-level reward Episode-level reward METEOR

√
√

√
√

8.73
8.29
8.82

In this experiment,
vidual components in our algorithm.
we train the following four variants of our model: 1) EPN-
Ind: generating captions independently from all candidate
event proposals, which is a baseline similar to most ex-
isting frameworks, 2) ESGN-Ind: generating captions in-
dependently using eventRNN only from the events within
the event sequence identiﬁed by our ESGN, 3) ESGN-
SCN: generating captions sequentially using our hierarchi-
cal RNN from the detected event sequence, and 4) ESGN-
SCN-RL: our full model (SDVC) that uses reinforcement
learning to further optimize the captioning network.

Table 4 summarizes the results from this ablation study,
and we have the following observations. First, the approach
based on ESGN (ESGN-Ind) is more effective than the base-
line that simply relies on all event proposals (EPN-Ind).
Also, ESGN reduces the number of candidate proposals sig-
niﬁcantly, from 77.99 to 2.85 in average, with substantial in-
crease in METEOR score, which indicates that ESGN suc-
cessfully identiﬁes event sequences from candidate event
proposals. Second, context modeling through hierarchical
structure (i.e., event RNN + episode RNN) in a caption-
ing network (ESGN-SCN) enhances performance compared
to the method with independent caption generation without
considering context (ESGN-Ind). Finally, ESGN-SCN-RL
successfully integrates reinforcement learning to effectively
improve the quality of generated captions.

We also analyze the impact of two reward levels—event
and episode—used for reinforcement learning. The results
are presented in Table 5, which clearly demonstrates the ef-
fectiveness of training with rewards from both levels.

5.6. Qualitative Results

Fig. 4 illustrates qualitative results, where the detected
event sequences and generated captions are presented to-
gether. We compare the generated captions by our model
(SDVC), which sequentially generates captions, with the
model (ESGN-Ind) that generates descriptions indepen-

6594

time

𝑒𝑒2
𝑒𝑒2

SDVC

ESGN-Ind

Predicted

Ground-truth

Ground-truth𝑒𝑒1: a man is seen standing in a room with a tennis racket and begins hitting the ball around the room

𝑒𝑒1
𝑒𝑒3
𝑒𝑒1
𝑒𝑒3
𝑒𝑒1: two men are shown in playing racket ball
𝑒𝑒2: they then take a brief and the man begins hitting the ball on the ground
𝑒𝑒3: the other man back from his break and they begin playing again
𝑒𝑒2: the man then begins to play squash with the camera and leads into him hitting the ball
𝑒𝑒3: the man then begins to play with the racket and the man walks around the room
𝑒𝑒1: two men are playing racquetball on a court
𝑒𝑒2: they are playing a game of racquetball
𝑒𝑒3: they continue to play the game
𝑒𝑒2
𝑒𝑒2

Predicted 𝑒𝑒1
𝑒𝑒3
𝑒𝑒1
𝑒𝑒3
𝑒𝑒1: a man is seen speaking to the camera that leads into several clips of a gym
𝑒𝑒2: many people are seen performing gymnastics on a mat while the camera follows close behind
𝑒𝑒3: people continue flipping around the gym while also stopping to speak to the camera
Ground-truth𝑒𝑒1: a man is doing a gymnastics routine on a blue mat
𝑒𝑒2: a man is doing gymnastics on a beam
𝑒𝑒3: the man then does a gymnastics routine on the mat
𝑒𝑒1: a man is seen speaking to the camera while holding a pole and speaking to the camera
𝑒𝑒2: the man then jumps onto a mat and begins performing a routine
𝑒𝑒3: the man continues to perform several more tricks and ends with him jumping down

Ground-truth

ESGN-Ind

SDVC

time

Figure 4. Qualitative results on ActivityNet Captions dataset. The arrows represent ground-truth events (red) and events in the predicted
event sequence from our event sequence generation network (blue) for input videos. Note that the events in the event sequence are selected
in the order of its index. For the predicted events, we show the captions generated independently (ESGN-Ind) and sequentially (SDVC).
More consistent captions are obtained by our sequential captioning network, where words for comparison are marked in bold-faced black.

dently from the detected event sequences. Note that the pro-
posed ESGN effectively identiﬁes event sequences for input
videos and our sequential caption generation strategy facil-
itates to describe events more coherently by exploiting both
visual and linguistic contexts. For instance, in the ﬁrst ex-
ample in Fig. 4, SDVC captures the linguistic context (‘two
men’ in e1 is represented by ‘they’ in both e2 and e3) as
well as temporal dependency between events (an expres-
sion of ‘continue’ in e3), while ESGN-Ind just recognizes
and describes e2 and e3 as independently occurring events.

6. Conclusion

We presented a novel framework for dense video cap-
tioning, which considers visual and linguistic contexts for

coherent caption generation by modeling temporal depen-
dency across events in a video explicitly. Speciﬁcally, we
introduced the event sequence generation network to detect
a series of event proposals adaptively. Given the detected
event sequence, a sequence of captions is generated by con-
ditioning on preceding events in our sequential captioning
network. We trained the captioning network in a supervised
manner while further optimizing via reinforcement learn-
ing with two-level rewards for better context modeling. Our
algorithm achieved the state-of-the-art accuracy on the Ac-
tivityNet Captions dataset in terms of METEOR.

Acknowledgments This work was partly supported by
Snap Inc., Korean ICT R&D program of the MSIP/IITP
grant [2016-0-00563, 2017-0-01780], and SNU ASRI.

6595

References

[1] Lorenzo Baraldi, Costantino Grana, and Rita Cuc-
chiara. Hierarchical Boundary-Aware Neural Encoder
for Video Captioning. In CVPR, 2017.

[2] Shyamal Buch, Victor Escorcia, Chuanqi Shen,
Bernard Ghanem, and Juan Carlos Niebles. SST:
Single-Stream Temporal Action Proposals. In CVPR,
2017.

[3] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos
Niebles, and Bernard Ghanem. DAPs: Deep Action
Proposals for Action Understanding. In ECCV, 2016.

[4] Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu,
Kenneth Tran, Jianfeng Gao, Lawrence Carin, and Li
Deng. Semantic Compositional Networks for Visual
Captioning. In CVPR, 2017.

[5] Bernard Ghanem, Juan Carlos Niebles, Cees Snoek,
Fabian Caba Heilbron, Humam Alwassel, Vic-
tor Escorcia, Ranjay Khrisna, Shyamal Buch, and
Cuong Duc Dao. The ActivityNet Large-Scale Ac-
tivity Recognition Challenge 2018 Summary. arXiv
preprint arXiv:1808.03766, 2018.

[6] Jiuxiang Gu, Jianfei Cai, Gang Wang, and Tsuhan
Chen. Stack-Captioning: Coarse-to-Fine Learning for
Image Captioning. In AAAI, 2018.

[7] Diederik P Kingma and Jimmy Ba. Adam: A Method

for Stochastic Optimization. In ICLR, 2015.

[8] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
and Juan Carlos Niebles. Dense-Captioning Events in
Videos. In ICCV, 2017.

[9] Yuncheng Li, Yale Song, Liangliang Cao,

Joel
Tetreault, Larry Goldberg, Alejandro Jaimes, and
Jiebo Luo. TGIF: A New Dataset and Benchmark on
Animated GIF Description. In CVPR, 2016.

[10] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao,
Jointly Localizing and Describing
and Tao Mei.
Events for Dense Video Captioning. In CVPR, 2018.

[11] Jonghwan Mun, Minsu Cho, and Bohyung Han. Text-
In

Guided Attention Model for Image Captioning.
AAAI, 2017.

[12] Phuc Nguyen, Ting Liu, Gautam Prasad, and Bo-
hyung Han. Weakly Supervised Action Localization
by Sparse Temporal Pooling Network. In CVPR, 2018.

[13] Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and
Yueting Zhuang. Hierarchical Recurrent Neural En-
coder for Video Representation with Application to
Captioning. In CVPR, 2016.

[14] Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and
Yong Rui. Jointly Modeling Embedding and Transla-
tion to Bridge Video and Language. In CVPR, 2016.

[15] Yingwei Pan, Ting Yao, Houqiang Li, and Tao
Mei. Video Captioning with Transferred Semantic At-
tributes. In CVPR, 2017.

[16] Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. Deep Reinforcement Learning-Based Image
Captioning with Embedding Reward. In CVPR, 2017.

[17] Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. Self-Critical Se-
quence Training for Image Captioning.
In CVPR,
2017.

[18] Karen Simonyan and Andrew Zisserman. Two-Stream
Convolutional Networks for Action Recognition in
Videos. In NIPS, 2014.

[19] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-
resani, and Manohar Paluri. Learning Spatiotemporal
Features with 3D Convolutional Networks. In ICCV,
2015.

[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention Is All You
Need. In NIPS, 2017.

[21] Ramakrishna Vedantam, C Lawrence Zitnick, and
Devi Parikh. CIDEr: Consensus-Based Image De-
scription Evaluation. In CVPR, 2015.

[22] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey
Donahue, Raymond Mooney, Trevor Darrell, and Kate
Saenko. Sequence to Sequence-Video to Text.
In
ICCV, 2015.

[23] Subhashini Venugopalan, Huijuan Xu, Jeff Don-
ahue, Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. Translating Videos to Natural Language using
Deep Recurrent Neural Networks.
In NAACL-HLT,
2015.

[24] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.

Pointer Networks. In NIPS, 2015.

[25] Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. Show and Tell: A Neural Image Cap-
tion Generator. In CVPR, 2015.

[26] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Re-
construction Network for Video Captioning. In CVPR,
2018.

[27] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and
Yong Xu. Bidirectional Attentive Fusion with Context
Gating for Dense Video Captioning. In CVPR, 2018.

[28] Junbo Wang, Wei Wang, Yan Huang, Liang Wang, and
Tieniu Tan. M3: Multimodal Memory Modelling for
Video Captioning. In CVPR, 2018.

[29] Ronald J Williams and David Zipser. A Learning
Algorithm for Continually Running Fully Recurrent
Neural Networks. Neural computation, 1(2):270–280,
1989.

6596

[30] Yilei Xiong, Bo Dai, and Dahua Lin. Move Forward
and Tell: A Progressive Generator of Video Descrip-
tions. In ECCV, 2018.

[31] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-
VTT: A Large Video Description Dataset for Bridging
Video and Language. In CVPR, 2016.

[32] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhutdinov, Richard
Zemel, and Yoshua Bengio. Show, Attend and Tell:
Neural Image Caption Generation with Visual Atten-
tion. In ICML, 2015.

[33] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Bal-
las, Christopher Pal, Hugo Larochelle, and Aaron
Courville. Describing Videos by Exploiting Tempo-
ral Structure. In ICCV, 2015.

[34] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and
Tao Mei. Boosting Image Captioning with Attributes.
In ICCV, 2017.

[35] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang,
and Wei Xu. Video Paragraph Captioning using Hier-
archical Recurrent Neural Networks. In CVPR, 2016.

[36] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gun-
hee Kim. End-to-End Concept Word Detection for
Video Captioning, Retrieval, and Question Answer-
ing. In CVPR, 2017.

[37] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard
Socher, and Caiming Xiong. End-to-End Dense Video
Captioning with Masked Transformer.
In CVPR,
2018.

6597

