Constrained Generative Adversarial Networks for Interactive Image Generation

Eric Heim

Air Force Research Laboratory

Information Directorate

Rome, NY USA

eheim602@gmail.com

Abstract

Generative Adversarial Networks (GANs) have received
a great deal of attention due in part to recent success in gen-
erating original, high-quality samples from visual domains.
However, most current methods only allow for users to guide
this image generation process through limited interactions.
In this work we develop a novel GAN framework that allows
humans to be “in-the-loop” of the image generation process.
Our technique iteratively accepts relative constraints of the
form “Generate an image more like image A than image B”.
After each constraint is given, the user is presented with new
outputs from the GAN, informing the next round of feedback.
This feedback is used to constrain the output of the GAN
with respect to an underlying semantic space that can be
designed to model a variety of different notions of similarity
(e.g. classes, attributes, object relationships, color, etc.). In
our experiments, we show that our GAN framework is able to
generate images that are of comparable quality to equivalent
unsupervised GANs while satisfying a large number of the
constraints provided by users, effectively changing a GAN
into one that allows users interactive control over image
generation without sacriﬁcing image quality.

1. Introduction

Learning a generative model from data is a task that has
gotten recent attention due to a number of breakthroughs
in complex data domains [15, 30, 13]. Some of the most
striking successes have been in creating novel imagery us-
ing Generative Adversarial Networks (GANs) [6]. While
GANs show promise in having machines effectively “draw”
realistic pictures, the mechanisms for allowing humans to
guide the image generation process have been largely limited
to conditioning on class labels [21] (e.g. “Draw a zero.”)
or domain-speciﬁc attributes [35] (e.g. “Draw a coat with
stripes.”). Such feedback, though powerful, limits the user
to expressing feedback through a pre-deﬁned set of labels.
If the user is unable to accurately express the characteristics

than

M ore like

User

M
a
p
 
t
o
 
S
e
m
a
n
t
i
c
 
S
p
a
c
e

Constraint

+

Constraint

Set

Generator

Image

I
n
t
e
r
a
c
t
i
o
n
w

 

 

i
t
h
u
s
e
r

Figure 1: Interaction with the CONGAN generator: A user
provides a relative constraint in the form of two images mean-
ing “Generate an image more like image A than image B.”
The constraint is combined to previously given constraints
to form a set, which is input to the generator to produce an
image. This image is shown to the user to drive further itera-
tions of feedback. The goal for the generator is to “satisfy”
the constraints with respect to a mapping to an underlying
semantic space. The generator satisﬁes a constraint (A, B)
by producing an image that is mapped to a coordinate closer
to where A is mapped than to where B is mapped.

that they desire using this label set, then they cannot guide
the model to produce acceptable images.

In this work, we seek a more natural and powerful way for
humans to interact with a generative model. To this end, we
propose a novel GAN technique we call CONstrained GAN
(CONGAN). Our model is designed to accept human feed-
back iteratively, effectively putting users “in-the-loop” of the
generation process. Figure 1 illustrates how a user interacts
with the CONGAN generator. The generator accepts relative
constraints of the form “More like image A than image B.”

10753

These constraints are used to deﬁne a feasible region within
a given semantic space that models an underlying notion of
similarity between images. The goal of the generator is to
accept relative constraints as input, and output an image that
is within the corresponding feasible region.

Modeling interaction in this way has two primary beneﬁts.
First, such relative pair-wise assessments have been shown
to be an easy medium for humans to articulate similarity [14,
26]. As such, CONGAN allows users to reﬁne its output
in a natural way. Relative constraints can also be used to
allow for different interactions besides providing pair-wise
comparisons. If the output from the generator is then input
as the B (“less similar”) image in the next iteration, the user
need only provide the A (“more similar”) image. In this
way, the user can provide single examples, meaning “More
like image A than what was previously generated”, to reﬁne
the output. Second, within the CONGAN framework, the
semantic space deﬁnes the characteristics that users guide
the image generation process. This allows for the option of
a variety of notions of similarity, such as class and attribute
information, but also more continuous or complex notions
such as color spaces, or size of objects within the image.

To achieve this form of interaction, our model must have
multiple interrelated components. The generator must be
able to accept a variable number of constraints as a set,
i.e. the output should be invariant to the order of the input
constraints. For this, we leverage recent work in Memory
Networks [7, 34, 27, 31] within the CONGAN generator to
learn a ﬁxed length vector representation over the constraint
set as a whole. In addition, the generator must not only be
able to generate realistic looking images, but images that
are within the feasible region of a given semantic space.
During training, the CONGAN generator is trained against
a constraint critic that enforces the output to satisfy given
constraints. The result is a generator that is able to produce
imagery guided by iterative relative feedback.

The remainder of the paper will proceed as follows. First,
we discuss prior related work. Then, we describe our method
beginning with a formal deﬁnition of the constrained gen-
eration problem, continuing to an outline of the CONGAN
training algorithm, and ending with a description of the
CONGAN generator. Next, we perform an evaluation where
we compare our method to an unsupervised GAN, showing
qualitative and quantitative results. Finally, we conclude.

2. Related Work

Our proposed CONGAN method follows from a long line
of work in neural network image generation. Speciﬁcally,
autoencoders [15], autoregressive models [30], and genera-
tive adversarial networks [6] (GANs) have all shown recent
success. We chose to learn a model using the GAN frame-
work, as GANs are arguably the best performing generative
models in terms of qualitative image quality.

Much of the fundamental work in GANs have focused
on unsupervised learning settings [6, 39, 1]. The output of
these models can be controlled by manipulating the latent
space used as input [23, 22]. However, such manipulation is
limited in that the latent space often has no obvious human
understandable interpretation. Thus ﬁnding ways to manipu-
late it requires either trial and error or interpolating between
two points in the latent space. Other works learn conditional
GAN models [21], where generation is guided by side in-
formation, such as class labels [21], visual attributes [35],
text [24], and images [28]. In this work, we aim to develop
a method that allows more intuitive manipulation of a GANs
output that generalizes to many different forms of similarity.
The GAN method most similar to ours is the one intro-
duced in [40]. This method ﬁrst maps an image to a manifold
of natural images using a GAN. Then, they provide a series
of image editing operations that users can use to move the
image along that manifold. We see our work as related but
orthogonal to this work as both the means for manipulation,
as well as the goals of the methods differ.

Another line of research that motivates this work is inter-
active learning over imagery. Much of the work in this ﬁeld
has focused on classiﬁcation problems [3, 32, 17, 33], but
also others such as learning localized attributes [4]. Most no-
tably, in [16] the authors propose an interactive image search
method that allows users to provide iterative reﬁnements
to their query, based on visual attributes. This is similar in
principle to our method in that their method searches images
through interactive comparisons to other images in the do-
main of interest. However, our method does not necessarily
require predeﬁned attributes and generates novel imagery
instead of retrieving relevant images from a database.

3. A Model for Constrained Image Generation

The goal of this work is to learn an image generation
model in the form of a mapping from a set of pair-wise
relative constraints to a realistic looking image. Let X be a
domain of images. We wish to learn the mapping:

gΘ : n(X ×X )i | i ≥ 1o × Z 7→ X

This generator maps a set of constraints C = {C1, C2, ...}
and a random noise vector z ∈ Z to an image, where a
constraint C = (X+, X−) ∈ X × X is a pair of images
meaning “Generate an image more like X+ than X−.” Intu-
itively, z represents the variation of imagery allowed within
the constraints, and different z will produce different images.
Practically, z provides the noise component necessary for
our generator to be trained within the GAN framework.

For training our generator, we require a mechanism that
determines whether the output of gΘ satisﬁes input con-
straints. To this end, we assume the existence of a mapping
φ : X 7→ S that maps images to a semantic space. The only

10754

Algorithm 1 CONGAN Training Procedure

Input: Gradient penalty coefﬁcient λ, constraint penalty co-
efﬁcient γ, discriminator iterations per generator iteration
ndisc, batch size m, Adam optimizer parameters α, β1, β2
repeat
for t = 1, ...ndisc do
for i = 1, ..., m do
Sample X ∼ PD, C ∼ PC, z ∼ Z, ǫ ∼ U (0, 1)
ˆX ← gΘ (C, z)
˜X ← ǫX + (1 − ǫ) ˆX
Li ← dW ( ˆX) − dW (X) + λ(||∇ ˜XdW ( ˜X)||2 − 1)2
end for

1

end for

i=1 Li, α, β1, β2(cid:1)
i=1 ∼ PC

m Pm
i=1 ∼ Z,(cid:8)Ci(cid:9)m
i=1 −dW ( ˆXi) + γlφ,S ( ˆXi)

W ← Adam(cid:0)∇W
Sample batches (cid:8)zi(cid:9)m
i=1 ← (cid:8)gΘ (cid:0)Ci, zi(cid:1)(cid:9)m
{ ˆXi}m
m Pm
L ← 1
Θ ← Adam (∇ΘL, α, β1, β2)
until Θ converged

i=1

requirements are that φ be differentiable, and that there exists
a distance metric dS over elements of S. For instance, if one
wanted to have users manipulate generated images by their
attributes (i.e. the dimensions of S correspond to attributes),
φ could be a learned attribute classiﬁer (for binary attributes)
or regressor (for continuous attributes). We say a generated
image ˆX satisﬁes a given constraint C = (X+, X−) with
respect to S if the following holds:

dS (cid:16)φ( ˆX), φ(X+)(cid:17) < dS (cid:16)φ( ˆX), φ(X−)(cid:17)

(1)

Given a set of constraints C, the goal of gΘ is to produce an
ˆX that satisﬁes all constraints in the set. In doing so, the
generator produces images that are closer in the semantic
space to “positive” images X+ than “negative” images X−.
Put another way, C deﬁnes a feasible region in S for which
ˆX must lie in. How we use this idea of relative constraints
to train gΘ is discussed in the following section.

3.1. Adversarial Training with Relative Constraints

To train the generator gΘ, we utilize the GAN framework
that pits a generator gΘ against a discriminator dW , where
both g and d and neural networks parameterized by Θ and
W , respectively. The discriminator is trained to distinguish
outputs of the generator from real image samples. The gen-
erator is trained to produce images that the discriminator
cannot differentiate from real samples. The two are trained
against one another; at convergence, the generator is often
able to produce instances that are similar to real samples.

While dW ensures output images look realistic, we use
another model to enforce constraint satisfaction. For this,

we introduce the idea of a constraint critic that informs the
training procedure in a similar manner as dW . We deﬁne the
constraint critic loss as the average loss over each constraint
after mapping images into the semantic space:

lφ,S ( ˆX, C) = −

1
|C| X

(X+,X−)∈C

pS (φ( ˆX), φ (X+) , φ (X−))

Loss over each constraint pS is inspired by the loss used in
t-Distributed Stochastic Triplet Embedding (STE) [29]:

pS (a, b, c) =

2

(cid:16)1 + dS (a,b)
α (cid:17)− α+1

α (cid:17)− α+1
+ (cid:16)1 + dS (a,c)

2

α (cid:17)− α+1

2

(cid:16)1 + dS (a,b)

This loss compares pairs of objects according to a t-Student
kernel and is motivated by successes in dimensionality reduc-
tion techniques that use heavy tailed similarity kernels [19].
By minimizing the negation of pS for each constraint, ˆX
is “pulled” closer to images X+ and “pushed” farther from
images X− in S. As a result, using this loss during training
will produce images more likely to satisfy constraints.

We leverage the constraint critic loss in tandem with
the discriminator to train the CONGAN generator. More
speciﬁcally, our training algorithm is an extension of the
Wasserstein GAN [1, 8]. We aim to optimize the following:

min

Θ

max

W

X∼PD

E

[dW (X)] − E

ˆX∼Pg

hdW ( ˆX) − γlφ,S ( ˆX, C)i

Here, PD is a data distribution (i.e. X is a sample from
a training set), Pg is the generator distribution (i.e. ˆX =
gΘ (C, z) for a given z ∼ Z and a given C drawn from a
training set of constraint sets). Finally, dW is constrained to
be 1-Lipschitz. This objective is optimized by alternating
between updating discriminator parameters W and generator
parameters Θ using stochastic gradient descent, sampling
from the training set and generator where necessary. Intu-
itively, the discriminator’s output can be interpreted as a
score of how likely the input is from the data distribution.
When the discriminator updates, it attempts to increase its
score for real samples and decrease its score for generated
samples. Conversely, when the generator updates, it attempts
to increase the discriminator’s score for generated images.
In addition, generator updates decrease the constraint loss
by a factor of the hyperparameter γ. As a result, generator
updates encourage gΘ to produce images similar to those in
the image training set, while also satisfying samples from a
constraint training set. To enforce the 1-Lipschitz constraint
on dW we use the gradient penalty term proposed in [8].

The CONGAN training procedure is outlined in Alg. 1.
This algorithm is very similar to the WGAN training algo-
rithm (Algorithm 1 in [8]) with a few key additions. First,
when updating both the discriminator and generator, batches

10755

(
(

,

,

.
.
.

)
)

!"

!#

Read CNN

Read CNN

.
.
.

∗
'(

L
S
T
M

'"

A

t
t
e
n
t
i
o
n

∗ ',-+
…
'+

∗

L
S
T
M

'.

A

t
t
e
n
t
i
o
n

$

N
e
t
w
o
r
k

W

r
i
t
e
 

% ~Z

Figure 2: The CONGAN generator. Purple is the read network, orange is the process network, and green is the write network

of constraint sets are selected from a training set. In practice,
we use φ to construct ground truth constraint sets of variable
length from images in the image train set, ensuring that our
generator is trained on constraint sets that are feasible in S.
Second, the generator update has an additional term: the
constraint critic term that encourages constraint satisfaction.

3.2. A Constrained Generator Network

While Alg. 1 outlines how to train gΘ, we have yet to for-
mally deﬁne gΘ. In order for gΘ to accept C as a set it must
1) accept a variable number of constraints, and 2) output
the same image regardless of the order in which constraints
are given. For this we leverage the work of [31] that intro-
duces a neural network framework capable of considering
order-invariant inputs, such as sets. An illustration of the
CONGAN generator is depicted in Fig. 2. Our generator
has three components: 1) A read network used to learn a
representation of each constraint 2) a process network that
combines all constraints in a set into a single set representa-
tion, and 3) a write network that maps the set representation
to an image. Below we describe each of these components.
The read network puts images within a constraint set
through a Convolutional Neural Network (CNN) to extract
visual features. Feature vectors of images from a common
constraint pair are concatenated and input to a fully con-
nected layer. The result is a single vector ci for each con-
straint, which are collectively input to the process network.
The process network consists of a “processing unit” that
is repeated p times. Let {c1, ..., cn} be the output of the
read network for a size n set of constraints. For each of the
t repetitions of the processing unit, an iteration through an
LSTM cell with “content-based” attention is performed:

First, z (as “input”) and the hidden state from previous repe-
tition are put through LSTM unit. The resultant hidden state
output of the LSTM qt is then combined with each ci via
dot product to create a scalar value ei,t for each constraint.
These are used in a softmax function to obtain scalars ai,t,
which in turn are used in a weighted sum. This sum is the
key operation that combines the constraints. Because addi-
tion is commutative, the result of (5), and thus the output
of the processing network, is invariant to the order that the
constraints were given. The result rt is concatenated with
qt and is used as the input in the next processing iteration.
After p steps, q∗
p is put through a fully connected layer to
produce s, which is input to the write network.

One way of interpreting this network is that each process-
ing unit iteration reﬁnes the representation of the constraint
set produced by the previous iteration. The output of the
processing unit has two parts. First, rt is a learned weighted
average of the constraints, ideally emphasizing constraints
with stronger signal. Second, qt is the output of the LSTM
which combines the noise vector and the output from the pre-
vious iteration, using various gates to retain certain features
while removing others. These two components are sent back
through the processing unit for further rounds of reﬁnement.
Similar to the generator in the unconditional GAN frame-
work, the write network maps a noise vector to image
space. Motivated by this, we use the transpose convolu-
tions [5, 25] utilized in Deep Convolutional GANs (DC-
GANs) [23]. Transpose convolutions effectively learn an
upsampling transformation. By building a network from
transpose convolutional layers, our write network is able to
learn how to map from a lower dimensional representation
of constraint set to a higher dimensional image.

qt = LST M (cid:0)z, q∗
ei,t = ci · qt

t−1(cid:1)

ai,t =

rt =

exp (ei,t)
Pn
j exp (ej,t)
X

ai,tci

n

i

q∗

t = [qt, rt]

(2)

(3)

(4)

(5)

(6)

4. Empirical Evaluation

In order to evaluate CONGAN we aim to show its ability
to satisfy constraints while achieving the image quality of
similar WGAN models. Further, we wish to highlight some
examples of how a user can interact with a CONGAN gen-
erator. To this end, we perform experiments with three data
sets: MNIST [18], CelebA [36], and Zappos50k [37, 38].

In all experiments, we use the hyperparameters suggested

10756

+

-

+

-

+

-

Figure 3: Example illustrating the order invariance property
of CONGAN. On the left are relative constraints (top is
positive image, bottom is negative) in the order they are
input to the CONGAN generator. On the right are the images
produced for two different z vectors. The output remains the
same even when the constraints are given in different orders.

in [8]: (λ = 10, ndisc = 5, α = 0.0001, β = 0, β = 0.9),
follow Algorithm 1 from the same work to train WGAN,
and set the batch size m = 32. We seed WGANs with noise
vectors z drawn from a standard normal (Z = N (0, I)), and
CONGANs with a uniform distribution (Z = U (−1, 1)).
We opt to use the uniform distribution as it allows both inputs
into the processing network to be in the same range. The
noise vectors are of size 64 for the MNIST experiments and
of size 128 for the CelebA and Zappos50k experiments. We
set p (number of “processing” steps) to 5 in both experiments,
but have observed that CONGAN is robust to this setting.

In [1] the authors observe that the Wasserstein Distance
can be used to determine convergence. In our experiments,
the Wasserstein Distance stopped improving by 100,000
generator update iterations for all models and use that as
the iteration limit. We chose values for γ that were able to
reduce the t-STE train error signiﬁcantly while maintaining
Wasserstein Distance close to what was achieved by the
WGAN. To strike a good balance we set γ = 10 on MNIST,
γ = 250 on CelebA, and γ = 100 on Zappos50k.

WGAN models were trained on the designated trained
sets for MNIST and CelebA. For Zappos50k, we randomly
chose 90% of the images as the train set, leaving the rest
as test. Similarly, CONGAN model constraint sets C in the
training set of constraint sets are created by ﬁrst randomly
choosing an image of the train set to be a reference image.
Then, anywhere between 1 and 10 pairs of images are ran-
domly chosen to be constraints. Next, φ is applied to the
reference image and each pair. The resultant representations
in S are used to determine which elements of the pairs are
considered X+ (positive examples) and X− (negative exam-

Figure 4: Examples from WGAN (left) and CONGAN
(right) generators trained on the MNIST data set.

ples) according to (1). Test sets are constructed similarly.

The CONGAN network architectures used in these exper-
iments are as follows 1. For MNIST: The discriminator and
read networks are ﬁve layer CNNs. The write network is a
ﬁve layer transpose convolutional network. For CelebA and
Zappos50k: The discriminator and read networks are resid-
ual networks [10] with four residual CNN blocks. The write
network has four transpose convolutional residual blocks. To
maintain some regularity between models in the interest of
fair comparison, we use the same discriminator architectures
for both WGAN and CONGAN and use the WGAN genera-
tor architecture as the CONGAN write network architecture.
Other than a few special cases, we use rectiﬁed linear units
as activation functions and perform layer normalization [2].

4.1. MNIST

MNIST is a well known data set containing 28x28 im-
ages of hand-written digits. For preprocessing we zero pad
the images to 32x32 and scale them to [-1,1]. For φ we
train a “mirrored” autoencoder on the MNIST train set using
squared Euclidean loss. The encoder portion consists of
four convolutional layers and a fully connected layer with
no activation to a two-dimensional encoding. We use the
encoder as φ. The decoder has a similar structure but uses
transpose convolutions to reverse the mapping. Simply au-
toencoding MNIST digits reveals a loose class structure in
the embedding space (S in this experiment). As such, this
experiment shows how class relationships can be retrieved
even if φ does not precisely map to classes.

We seek to evaluate the CONGAN’s ability to satisfy
given constraints. To this end, we constructed ten differ-
ent test sets, each containing constraint sets of a ﬁxed size.
For example, each constraint set in the “2” test set has two
constraints. We call an evaluation over a different test set
an “experiment”. In each experiment, we performed ten
different trials where the generator was given different noise
vectors per constraint set. With these experiments we can

1A more rigorous description can be found in the supplement.

10757

!"

!#

!$

+

-

Figure 5: Example of CONGAN generator outputs when trained on the CelebA data set. The bottom two rows of images are
constraints, where the positive and negative images only differ by a single attribute. The ﬁrst three constraints differ by only
the “Male” attribute, the second three by only the “Beard” attribute, and the third three by only the “Eyeglasses” attribute. The
top three rows are images produced from three different seeds when the constraints are provided to the CONGAN generator
from left to right. For example, the third image in the ﬁrst row is generated when z1 and the ﬁrst three constraints are given.

!"

!#

!$

+

-

Figure 6: Another example of CONGAN generator outputs when trained on the CelebA data set. This is the same experiment
as in Fig. 5, but with the attributes “Pale Skin”, “Brown Hair”, and “Female” from left to right.

observe the effect constraint set size has on the generator.

Results: Table 1 shows the mean constraint satisfaction
error (i.e one minus the prevalence of (1)) of the CONGAN
generator for each MNIST experiment. Overall, it was able
to satisfy over 90% of given constraints. Note that the gen-
erator performs slightly better when more constraints are
given. This is somewhat counter-intuitive. We believe that

in this case the generator is using constraints to determine
what class of digit to produce. If given few constraints, it is
more difﬁcult for the generator to determine the class of the
output. Figures 3 and 4 show example outputs of CONGAN
when trained on MNIST: One showing the order invariance
property of CONGAN and the other showing CONGAN gen-
erated images next to ones produced by a similar WGAN.

10758

1

2

3

4

5

6

7

8

9

10

0.0931

0.0895

0.0860

0.0831

0.0808

0.0784

0.0775

0.0756

0.0743

0.0733

# input constraints

Table 1: Mean constraint satisfaction errors of CONGAN on MNIST constraints per input set size (10 trials).

-WGAN
-CONGAN
MCSE

WGAN
20.31
481.12

1

18.32
479.27
0.0885

2

18.90
480.08
0.1047

3

19.34
480.74
0.1154

WGAN
CONGAN
MCSE

WGAN
60.31
5.43

1

44.85
-27.04
0.0950

2

46.03
-18.64
0.0967

3

46.99
-11.39
0.0974

CONGAN (# input constraints)
4
7

5

6

19.64
481.29
0.1202

19.82
481.74
0.1257

19.93
482.07
0.1279

20.00
482.36
0.1296

CONGAN (# input constraints)
7
4

5

6

47.45
-5.53
0.1001

47.30
-1.37
0.1009

46.76
1.09
0.1019

45.84
2.06
0.1052

8

19.99
482.57
0.1307

9

19.96
482.71
0.1318

10
19.90
482.80
0.1325

8

44.86
1.69
0.1065

9

43.81
0.10
0.1065

10
42.78
-2.41
0.1066

Table 2: Evaluation results on CelebA (top table) and Zappos50K (bottom table) data sets (10 trials). Rows 1-2 of each table:
Mean discriminator scores for WGAN and CONGAN discriminators at convergence on WGAN and CONGAN generators
(negative scores for CelebA). Row 3 of each table: Mean constraint satisfaction error of CONGAN models per input set size.

4.2. CelebA

The CelebA data set contains 202,599 color images of
celebrity faces. For our experiments, we resize each image
to 64x64 and scale to [-1,1]. Associated with each image are
40 binary attributes ranging from “Blond Hair” to “Smiling”.
We chose twelve of these attributes to be S. More specif-
ically, an image’s representation in S is a binary vector of
attributes, which differs from the MNIST experiment. In the
previous experiment, S was both lower dimensional and con-
tinuous. As such, this experiment will evaluate CONGAN’s
ability to adapt to different semantic spaces.

For φ we construct a simple multi-task CNN (MCNN) [9]
2 that consists of one base network and multiple specialized
networks, trained end-to-end. The base network accepts
the image as input and extracts features for detecting all
attributes. The specialized networks split from the base
network and learn to detect to their predetermined subset.
Our φ base network consists of two convolutional layers.
The specialized networks (one for each of twelve attributes)
consists of three convolutional layers followed by a fully
connected layer that maps to a scalar attribute identiﬁer.

For this experiment we sought to more objectively com-
pare the WGAN generated images with those produced by
CONGAN. To this end we ﬁrst train a WGAN on the CelebA
train set. Then, we initialize the CONGAN write network
and discriminator to the trained WGAN generator and dis-
criminator, respectively, before training the CONGAN gen-
erator. By doing this, we can observe how image quality is

2Details and an evaluation of the MCNN can be found in the supplement.

affected by adding the CONGAN components to a WGAN.

Results: Rows one and two of Table 2 (top table) show
the mean negative discriminator scores for both the WGAN
and CONGAN generators against the WGAN and CONGAN
discriminators at convergence over ten trials. We can see
that for both discriminators, WGAN generated images are
scored very similarly to those generated by CONGAN. This
is especially true when considering the standard deviation
for the WGAN generator against the WGAN and CONGAN
discriminators is 8.75 and 16.22, respectively, and slightly
higher on both for the CONGAN generator. We believe this
result shows evidence that adding the CONGAN framework
to the WGAN training did not drastically alter image quality.

The last row of Table 2 shows the mean constraint satis-
faction error on the test set for each experiment. Here, the
CONGAN generator is able to satisfy around 87% or more
of the constraints. Figures 5 and 6 show images generated by
CONGAN. As constraints are provided, the image produced
from different seeds take on the attributes indicated by the
constraints. In Fig. 5, the ﬁrst three constraints indicate the
“Male” attribute, the next three indicate “Beard”, and the last
“Eyeglasses”. In Fig. 6, “Pale Skin”, “Brown Hair”, and
“Female” are indicated. These examples show that a user can
iteratively reﬁne the images to have desired characteristics,
and still be given a variety of realistic, novel images.

4.3. Zappos50K

The Zappos50K data set contains 50,025 color images
of shoes. We resize each image to 64x64 and scale to [-

10759

Initial

Constraint

Target

Generated Images

+

-

+

-

+

-

!"

!#

!"

!#

!"

!#

Figure 7: Three sets of two examples from the CONGAN
generator trained on the Zappos data set. The generator was
ﬁrst provided the initial constraint of the left, generating the
ﬁrst (left-most) image in the generated images column. To
generate each of the next three images, the generator was fed
a constraint where the positive image was the target image,
and the negative image was the previously generated image.

1,1]. For this experiment, we chose S to be a color space.
To accomplish this, we computed a 64 bin color histogram
over each image and trained a nine-layer CNN to embed
the images in 2-dimensions using a triplet network [11] 3,
and used this as φ. We opted to use the T-STE loss in the
objective as it produced a clear separation of colors.

There is inherent bias in the Zappos50K data set when
it comes to color, as most shoes tend to be black, brown,
or white. This poses a challenge in that if constraint sets
used for training are formed by uniformly sampling over the
train set, the model will tend to favor few colors, making
it difﬁcult to guide generation to other colors. To combat
this, we constructed constraints to include a more uniform
sampling over colors. When constructing the train set of
constraint sets, with probability 0.5 we uniformly sampled
over training images as in the other experiments. When not
sampling uniformly, we focused on a single color bin by
ﬁrst selecting a bin and choosing all positive images in the
constraint set to be images where the highest histogram value
corresponded to that bin (e.g. all positive examples would be
“light blue”). Negative examples would be chose uniformly
from the other bins. We found this allowed the CONGAN
generator to more easily learn to produce a variety of colors.
Results: Table 2 (bottom table) shows the discrimina-
tor scores and mean constraint satisfaction errors for each
Zappos50K experiment. Here, the CONGAN generator pro-

3A visualization of this embedding can be found in the supplement.

duced lower scores than the WGAN for both discriminators,
though within one standard deviation. We believe this is due
to training the generator to produce a wider variety of colors.
If training data contains many brown, black, and white shoes,
then training the generator to produce blue, red and yellow
shoes will force it to produce images that differ than those
provided to the discriminator. Nevertheless, we believe that
image quality was only slightly degraded as a result.

Figure 7 shows examples of the images produced by the
CONGAN generator. Here, we wanted to test the use case
of providing single images, instead of pair-wise constraints,
to guide the generator to a result. An initial constraint is
provided to produce a starting images. After that, a single
target image is used repeatedly as the positive example to
generate shoes more similarly colored to the target.

5. Conclusion and Future Work

In this work, we introduce a Generative Adversarial Net-
work framework that is able to generate imagery guided by
iterative human feedback. Our model relies on two novel
components. First, we develop a generator, based on recent
work in memory networks, that maps variable-sized sets
of constraints to image space using order-invariant opera-
tions. Second, this generator is informed during training by
a critic that determines whether generated imagery satisﬁes
given constraints. The result is a generator that can can be
guided interactively by humans through relative constraints.
Empirically our model is able to generate images that are
of comparable quality to those produced by similar GAN
models, while satisfying a up to 90% of given constraints.

There are multiple avenues of future work that we believe
are worthy of further study. First, it may not be feasible for
users of CONGAN to search through large image databases
to ﬁnd the exact constraints they desire. We will apply pair-
wise active ranking techniques [12] to suggest constraint
queries in order to quickly constrain the semantic space
without requiring users to search through images themselves.
Second, we will investigate the output of the process network
more closely seeing if constraint representations have prop-
erties that match intuition about how sets of constraints are
classically reasoned about, similar to word embeddings [20].
Acknowledgments: This work was supported by the
AFOSR Science of Information, Computation, Learning,
and Fusion program lead by Dr. Doug Riecken. Eric would
like to thank Davis Gilton (UW-M) and Timothy Van Slyke
(NEU) for early exploratory experiments that made this
work possible. Eric would also like to thank Ritwik Gupta
(SEI/CMU) for reviewing a draft of the paper. Finally, Eric
would like to thank his colleagues at AFRL/RI: Dr. Lee
Seversky, Dr. Walter Bennette, Dr. Matthew Klawonn, and
Dylan Elliot for insightful feedback as this work progressed.

10760

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein genera-

tive adversarial networks. In ICML, 2017.

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.

NIPS Deep Learning Symposium, 2016.

[3] S. Branson, C. Wah, F. Schroff, B. Babenko, P. Welinder,
P. Perona, and S. Belongie. Visual recognition with humans
in the loop. ECCV, 2010.

[4] K. Duan, D. Parikh, D. Crandall, and K. Grauman. Discover-
ing localized attributes for ﬁne-grained recognition. In CVPR,
2012.

[5] V. Dumoulin and F. Visin. A guide to convolution arithmetic

for deep learning. arXiv preprint arXiv:1603.07285, 2016.

[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio. Generative
adversarial nets. In NIPS, 2014.

[7] A. Graves, G. Wayne, and I. Danihelka. Neural turing ma-

chines. arXiv preprint arXiv:1410.5401, 2014.

[8] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. Courville. Improved training of wasserstein gans. arXiv
preprint arXiv:1704.00028, 2017.

[9] E. M. Hand and R. Chellappa. Attributes for improved at-
tributes: A multi-task network utilizing implicit and explicit
relationships for facial attribute classiﬁcation. In AAAI, 2017.
[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016.

[11] E. Hoffer and N. Ailon. Deep metric learning using triplet
In International Workshop on Similarity-Based

network.
Pattern Recognition, 2015.

[23] A. Radford, L. Metz, and S. Chintala. Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks. ICLR, 2016.

[24] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee. Generative adversarial text to image synthesis. In
ICML, 2016.

[25] W. Shi, J. Caballero, L. Theis, F. Huszar, A. Aitken, C. Ledig,
and Z. Wang. Is the deconvolution layer the same as a convo-
lutional layer? arXiv preprint arXiv:1609.07009, 2016.

[26] N. Stewart, G. D. Brown, and N. Chater. Absolute identiﬁca-
tion by relative judgment. Psychological review, 112(4):881,
2005.

[27] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory

networks. In NIPS, 2015.

[28] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,
A. Graves, et al. Conditional image generation with pixelcnn
decoders. In NIPS, 2016.

[29] L. Van Der Maaten and K. Weinberger. Stochastic triplet

embedding. In MLSP, 2012.

[30] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel

recurrent neural networks. In ICML, 2016.

[31] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Se-

quence to sequence for sets. In ICLR, 2016.

[32] C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass
recognition and part localization with humans in the loop. In
ICCV, 2011.

[33] C. Wah, G. Van Horn, S. Branson, S. Maji, P. Perona, and
S. Belongie. Similarity comparisons for interactive ﬁne-
grained categorization. In CVPR, 2014.

[12] K. G. Jamieson and R. Nowak. Active ranking using pairwise

[34] J. Weston, S. Chopra, and A. Bordes. Memory networks. In

comparisons. In NIPS, 2011.

ICLR, 2015.

[13] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive grow-
ing of GANs for improved quality, stability, and variation. In
ICLR, 2018.

[35] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image:
Conditional image generation from visual attributes. In ECCV,
2016.

[36] S. Yang, P. Luo, C.-C. Loy, and X. Tang. From facial parts
responses to face detection: A deep learning approach. In
ICCV, 2015.

[37] A. Yu and K. Grauman. Fine-grained visual comparisons with

local learning. In CVPR, 2014.

[38] A. Yu and K. Grauman. Semantic jitter: Dense supervision
for visual comparisons via synthetic images. In ICCV. IEEE,
2017.

[39] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative

adversarial network. ICLR, 2017.

[40] J.-Y. Zhu, P. Kr¨ahenb¨uhl, E. Shechtman, and A. A. Efros.
Generative visual manipulation on the natural image manifold.
In ECCV. Springer, 2016.

[14] M. G. Kendall and J. D. Gibbons. Rank correlation methods.

1990.

[15] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. 2014.

[16] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch:
Image search with relative attribute feedback. In CVPR, 2012.
[17] N. Kumar, P. N. Belhumeur, A. Biswas, D. W. Jacobs, W. J.
Kress, I. C. Lopez, and J. V. Soares. Leafsnap: A computer
vision system for automatic plant species identiﬁcation. In
ECCV. 2012.

[18] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceedings
of the IEEE, 1998.

[19] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.

JMLR, 2008.

[20] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.
Distributed representations of words and phrases and their
compositionality. In NIPS, 2013.

[21] M. Mirza and S. Osindero. Conditional generative adversarial

nets. arXiv preprint arXiv:1411.1784, 2014.

[22] G. Perarnau, J. van de Weijer, B. Raducanu, and J. M. ´Alvarez.
Invertible conditional gans for image editing. arXiv preprint
arXiv:1611.06355, 2016.

10761

