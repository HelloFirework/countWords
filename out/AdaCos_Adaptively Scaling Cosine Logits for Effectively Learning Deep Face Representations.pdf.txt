AdaCos: Adaptively Scaling Cosine Logits for Effectively

Learning Deep Face Representations

Xiao Zhang1 Rui Zhao2 Yu Qiao3 Xiaogang Wang1 Hongsheng Li1

1CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong

2SenseTime Research

3SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
zhangx9411@gmail.com zhaorui@sensetime.com yu.qiao@siat.ac.cn {xgwang, hsli}@ee.cuhk.edu.hk

Abstract

The cosine-based softmax losses [21, 28, 39, 8] and their
variants [40, 38, 7] achieve great success in deep learning
based face recognition. However, hyperparameter settings
in these losses have signiﬁcant inﬂuences on the optimiza-
tion path as well as the ﬁnal recognition performance. Man-
ually tuning those hyperparameters heavily relies on user
experience and requires many training tricks.

In this paper, we investigate in depth the effects of two
important hyperparameters of cosine-based softmax losses,
the scale parameter and angular margin parameter, by ana-
lyzing how they modulate the predicted classiﬁcation prob-
ability. Based on these analysis, we propose a novel cosine-
based softmax loss, AdaCos, which is hyperparameter-free
and leverages an adaptive scale parameter to automati-
cally strengthen the training supervisions during the train-
ing process. We apply the proposed AdaCos loss to large-
scale face veriﬁcation and identiﬁcation datasets, includ-
ing LFW [13], MegaFace [16], and IJB-C [23] 1:1 Veri-
ﬁcation. Our results show that training deep neural net-
works with the AdaCos loss is stable and able to achieve
high face recognition accuracy. Our method outperforms
state-of-the-art softmax losses [28, 40, 7] on all the three
datasets.

1. Introduction

Recent years witnessed the breakthrough of deep Con-
volutional Neural Networks (CNNs) [17, 12, 25, 35] on sig-
niﬁcantly improving the performance of one-to-one (1 : 1)
face veriﬁcation and one-to-many (1 : N ) face identiﬁ-
cation tasks. The successes of deep face CNNs can be
mainly credited to three factors: enormous training data
[9], deep neural network architectures [10, 33] and ef-
fective loss functions [28, 21, 7]. Modern face datasets,
such as LFW [13], CASIA-WebFace [43], MS1M [9] and
MegaFace [24, 16], contain huge number of identities
which enable the training of deep networks. A number
of recent studies, such as DeepFace [36], DeepID2 [31],
DeepID3 [32], VGGFace [25] and FaceNet [29], demon-

strated that properly designed network architectures also
lead to improved performance.

Apart from the large-scale training data and deep struc-
tures, training losses also play key roles in learning accu-
rate face recognition models [41, 6, 11]. Unlike image clas-
siﬁcation tasks, face recognition is essentially an open set
recognition problem, where the testing categories (identi-
ties) are generally different from those used in training. To
handle this challenge, most deep learning based face recog-
nition approaches [31, 32, 36] utilize CNNs to extract fea-
ture representations from facial images, and adopt a metric
(usually the cosine distance) to estimate the similarities be-
tween pairs of faces during inference.

However, such inference evaluation metric is not well
considered in the methods with softmax cross-entropy loss
function1, which train the networks with the softmax loss
but perform inference using cosine-similarities. To miti-
gate the gap between training and testing, recent works [21,
28, 39, 8] directly optimized cosine-based softmax losses.
Moreover, angular margin-based terms [19, 18, 40, 38, 7]
are usually integrated into cosine-based losses to maximize
the angular margins between different identities. These
methods improve the face recognition performance in the
open-set setup.
In spite of their successes, the training
processes of cosine-based losses (and their variants intro-
ducing margins) are usually tricky and unstable. The con-
vergence and performance highly depend on the hyperpa-
rameter settings of loss, which are determined empirically
through large amount of trials. In addition, subtle changes
of these hyperparameters may fail the entire training pro-
cess.

In this paper, we investigate state-of-the-art cosine-based
softmax losses [28, 40, 7], especially those aiming at max-
imizing angular margins, to understand how they provide
supervisions for training deep neural networks. Each of the
functions generally includes several hyperprameters, which
have substantial impact on the ﬁnal performance and are
usually difﬁcult to tune. One has to repeat training with dif-

1We denote it as “softmax loss” for short in the remaining sections.

10823

ferent settings for multiple times to achieve optimal perfor-
mance. Our analysis shows that different hyperparameters
in those cosine-based losses actually have similar effects on
controlling the samples’ predicted class probabilities. Im-
proper hyperparameter settings cause the loss functions to
provide insufﬁcient supervisions for optimizing networks.

Based on the above observation, we propose an adap-
tive cosine-based loss function, AdaCos, which automati-
cally tunes hyperparameters and generates more effective
supervisions during training. The proposed AdaCos dy-
namically scales the cosine similarities between training
samples and corresponding class center vectors (the fully-
connection vector before softmax), making their predicted
class probability meets the semantic meaning of these co-
sine similarities. Furthermore, AdaCos can be easily imple-
mented using built-in functions from prevailing deep learn-
ing libraries [26, 1, 5, 15]. The proposed AdaCos loss leads
to faster and more stable convergence for training without
introducing additional computational overhead.

To demonstrate the effectiveness of the proposed Ada-
Cos loss function, we evaluated it on several face bench-
marks, including LFW face veriﬁcation [13], MegaFace
one-million identiﬁcation [24] and IJB-C [23]. Our method
outperforms state-of-the-art cosine-based losses on all these
benchmarks.

2. Related Works

Cosine similarities for inference. For learning deep
face representations, feature-normalized losses are com-
monly adopted to enhance the recognition accuracy. Coco
loss [20, 21] and NormFace [39] studied the effect of nor-
malization and proposed two strategies by reformulating
softmax loss and metric learning. Similarly, Ranjan et al. in
[28] also discussed this problem and applied normalization
on learned feature vectors to restrict them lying on a hyper-
sphere. Movrever, compared with these hard normalization,
ring loss [45] came up with a soft feature normalization ap-
proach with convex formulations.

Margin-based softmax loss. Earlier, most face recog-
nition approaches utilized metric-targeted loss functions,
such as triplet [41] and contrastive loss [6], which utilize
Euclidean distances to measure similarities between fea-
tures. Taking advantages of these works, center loss [42]
and range loss [44] were proposed to reduce intra-class
variations via minimizing distances within each class [2].
Following this, researchers found that constraining mar-
gin in Euclidean space is insufﬁcient to achieve optimal
generalization. Then angular-margin based loss functions
were proposed to tackle the problem. Angular constraints
were integrated into the softmax loss function to improve
the learned face representation by L-softmax [19] and A-
softmax [18]. CosFace [40], AM-softmax [38] and ArcFace
[7] directly maximized angular margins and employed sim-
pler and more intuitive loss functions compared with afore-

mentioned methods.

Automatic hyperparameter tuning. The performance
of an algorithm highly depends on hyperparameter settings.
Grid and random search [3] are the most widely used strate-
gies. For more automatic tuning, sequential model-based
global optimization [14] is the mainstream choice. Typ-
ically, it performs inference with several hyperparameters
settings, and chooses setting for the next round of testing
based on the inference results. Bayesian optimization [30]
and tree-structured parzen estimator approach [4] are two
famous sequential model-based methods. However, these
algorithms essentially run multiple trials to predict the opti-
mized hyperparameter settings.

3. Investigation of hyperparameters in cosine-

based softmax losses

In recent years, state-of-the-art cosine-based softmax
losses,
including L2-softmax [28], CosFace [40], Arc-
Face [7], signiﬁcantly improve the performance of deep
face recognition. However, the ﬁnal performances of those
losses are substantially affected by their hyperparameters
settings, which are generally difﬁcult to tune and require
multiple trials in practice. We analyze two most important
hyperparameters, the scaling parameter s and the margin
parameter m, in cosine-based losses. Specially, we deeply
study their effects on the prediction probabilities after soft-
max, which serves as supervision signals for updating entire
neural network.

Let ~xi denote the deep representation (feature) of the i-
th face image of the current mini-batch with size N , and
yi be the corresponding label. The predicted classiﬁcation
probability Pi,j of all N samples in the mini-batch can be
estimated by the softmax function as

Pi,j =

,

(1)

efi,j
k=1 efi,k

PC

where fi,j is logit used as the input of softmax, Pi,j repre-
sents its softmax-normalized probability of assigning ~xi to
class j, and C is the number of classes. The cross-entropy
loss associated with current mini-batch is

LCE = −

1
N

log Pi,yi = −

1
N

N

Xi=1

N

Xi=1

log

efi,yi
k=1 efi,k

.

PC

(2)
Conventional softmax loss and state-of-the-art cosine-
based softmax losses [28, 40, 7] calculate the logits fi,j in
different ways. In conventional softmax loss, logits fi,j are
obtained as the inner product between feature ~xi and the j-
th class weights ~Wj as fi,j = ~W T
j ~xi. In the cosine-based
softmax losses [28, 40, 7], cosine similarity is calculated by

cos θi,j = h~xi, ~Wji/k~xikk ~Wjk. The logits fi,j are calcu-
lated as fi,j = s·cos θi,j , where s is a scale hyperparameter.
To enforce angular margin on the representations, ArcFace
[7] modiﬁed the loss to the form

10824

j
,
i
θ

f
o

e
e
r
g
e
D

π
2

7π
16

3π
8

5π
16

π
4

Average θi,yi
Median θi,yi
Average θi,j, j 6= yi

Num. of Iteration

Figure 1: Changing process of angles in each mini-batch
when training on WebFace. (Red) average angles in each
mini-batch for non-corresponding classes, θi,j for j 6= yi.
(Blue) median angles in each mini-batch for corresponding
classes, θi,yi . (Brown) average angles in each mini-batch
for corresponding classes, θi,yi .

fi,j = s · cos (θi,j + ✶{j = yi} · m),

while CosFace [40] uses

fi,j = s · (cos θi,j − ✶{j = yi} · m),

(3)

(4)

where m is the margin. The indicator function ✶{j = yi}
returns 1 when j = yi and 0 otherwise. All margin-based
variants decrease fi,yi associate with the correct class by
subtracting margin m. Compared with the losses without
margin, margin-based variants require fi,yi to be greater
than other fi,j for j 6= yi, by a speciﬁed m.
Intuitively, on one hand, the parameter s scales up the
narrow range of cosine distances, making the logits more
discriminative. On the other hand, the parameter m enlarges
the margin between different classes to enhance classiﬁca-
tion ability. These hyperparameters eventually affect Pi,yi .
Empirically, an ideal hyperparameter setting should help
Pi,j to satisfy the following two properties: (1) Predicted
probabilities Pi,yi of each class (identity) should span to
the range [0, 1]: the lower boundary of Pi,yi should be near
0 while the upper boundary near 1; (2) Changing curve of
Pi,yi should have large absolute gradients around θi,yi to
make training effective.

3.1. Effects of the scale parameter s

The scale parameter s can signiﬁcantly affect Pi,yi . In-
tuitively, Pi,yi should gradually increase from 0 to 1 as the
angle θi,yi decreases from π
2 to 02, i.e., the smaller the angle
between ~xi and its corresponding class weight ~Wyi is, the
larger the probability should be. Both improper probability
range and probability curves w.r.t. θi,yi would negatively
affect the training process and thus the recognition perfor-
mance.

We ﬁrst study the range of classiﬁcation probability Pi,j .
Given scale parameter s, the range of probabilities in all
cosine-based softmax losses is

2Mathematically, θ can be any value in [0, π]. We empirically found,
2 . See the red curve in Fig. 1

however, the maximum θ is always around π
for examples.

1

1 + (C − 1) · es ≤ Pi,j ≤

es

es + (C − 1)

,

(5)

where the lower boundary is achieved when fi,j = s· 0 = 0
and fi,k = s · 1 = s for all k 6= j in Eq. (1). Similarly, the
upper bound is achieved when fi,j = s and fi,k = 0 for all
k 6= j. The range of Pi,j approaches 1 when s → ∞, i.e.,

s→+∞(cid:18)

lim

es

es + (C − 1) −

1

1 + (C − 1) · es(cid:19) = 1,

(6)

which means that the requirement of the range spanning
[0, 1] could be satisﬁed with a large s. However it does not
mean that the larger the scale parameter, the better the se-
lection is. In fact the probability range can easily approach
a high value, such as 0.94 when class number C = 10 and
scale parameter s = 5.0. But an oversized scale would lead
to poor probability distribution, as will be discussed in the
following paragraphs.

We investigate the inﬂuences of parameter s by taking
Pi,yi as a function of s and angle θi,yi where yi denotes the
label of ~xi. Formally, we have

Pi,yi =

efi,yi

efi,yi + Bi

=

es·cos θi,yi

es·cos θi,yi + Bi

,

(7)

during training (see red curve in Fig. 1).

efi,k = Pk6=yi

where Bi = Pk6=yi

es·cos θi,k are the log-
its summation of all non-corresponding classes for feature
~xi. We observe that the values of Bi are almost unchanged
during the training process. This is because the angles θi,k
for non-corresponding classes k 6= yi always stay around π
Therefore, we can assume Bi is constant, i.e., Bi ≈
es·cos(π/2) = C − 1. We then plot curves of prob-
abilities Pi,yi w.r.t. θi,yi under different setting of param-
eter s in Fig. 2(a). It is obvious that when s is too small
(e.g., s = 10 for class/identity number C = 2, 000 and
C = 20, 000), the maximal value of Pi,yi could not reach
1. This is undesirable because even when the network is
very conﬁdent on a sample ~xi’s corresponding class label
yi, e.g. θi,yi = 0, the loss function would still penalize the
classiﬁcation results and update the network.

Pk6=yi

2

On the other hand, when s is too large (e.g., s = 64),
the probability curve Pi,yi w.r.t. θi,yi is also problematic.
It would output a very high probability even when θi,yi is
close to π/2, which means that the loss function with large
s may fail to penalize mis-classiﬁed samples and cannot ef-
fectively update the networks to correct mistakes.

In summary, the scaling parameter s has substantial in-
ﬂuences to the range as well as the curves of the probabili-
ties Pi,yi , which are crucial for effectively training the deep
network.

3.2. Effects of the margin parameter m

In this section, we investigate the effect of margin pa-
rameters m in cosine-based softmax losses (Eqs. (3) & (4)),

10825

2k-way classiﬁcation

20k-way classiﬁcation

1.0
1.0

0.8
0.8

0.6
0.6

0.4
0.4

0.2
0.2

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

S = 10

S = 32

S = 40

S = 48

S = 64

1.0

0.8

0.6

0.4

0.2

0.0
0.0
0

0.0

Fixed AdaCos

π
16

π
8

3π
16

0.2

π
4

5π
16

3π
8

7π
0.4
16

π
2

0.0
0

θi,yi ∈ (0, π
2)

S = 10

S = 32

S = 40

S = 48

S = 64

Fixed AdaCos

π
0.6
16

π
8

3π
16

π
4

0.8

5π
16

3π
8

7π
16

π
1.0
2

1.0
1.0

0.8
0.8

0.6
0.6

0.4
0.4

0.2
0.2

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

0.0
0.0
0

0.0

2k-way classiﬁcation

20k-way classiﬁcation

m = 0.2

m = 0.4

m = 0.6

m = 0.8

m = 1.0

Fixed AdaCos

1.0

0.8

0.6

0.4

0.2

m = 0.2

m = 0.4

m = 0.6

m = 0.8

m = 1.0

Fixed AdaCos

π
16

π
8

3π
16

0.2

π
4

5π
16

3π
8

7π
0.4
16

π
2

0.0
0

θi,yi ∈ (0, π
2)

π
0.6
16

π
8

3π
16

π
4

0.8

5π
16

3π
8

7π
16

π
1.0
2

(a) Pi,yi w.r.t. θi,yi .

(b) Pi,yi w.r.t. θi,yi .

Figure 2: Curves of Pi,yi w.r.t. θi,yi by choosing different sclae and margin parameters. (Left) C = 2000. (Right) C = 20000. Fig. 2(a)
is for choosing different scale parameters and Fig. 2(b) is for ﬁxing s = 30 and choosing different margin parameters.

and their effects on feature ~xi’s predicted class probability
Pi,yi . For simplicity, we here study the margin parameter
m for ArcFace (Eq. 3); while the similar conclusions also
apply to the parameter m in CosFace (Eq. (4)).

We ﬁrst re-write classiﬁcation probability Pi,yi follow-

ing Eq. (7) as

Pi,yi =

efi,yi

efi,yi + Bi

=

es·cos (θi,yi +m)

es·cos (θi,yi +m) + Bi

.

(8)

To study the inﬂuence of parameter m on the probability
Pi,yi , we assume both s and Bi are ﬁxed. Following the
discussion in Section 3.1, we set Bi ≈ C − 1, and ﬁx s =
30. The probability curves Pi,yi w.r.t. θi,yi under different
m are shown in Fig. 2(b).

According to Fig. 2(b), increasing the margin parame-
ter shifts probability Pi,yi curves to the left. Thus, with
the same θi,yi , larger margin parameters lead to lower prob-
abilities Pi,yi and thus larger loss even with small angles
θi,yi . In other words, the angles θi,yi between the feature
~xi and its corresponding class’s weights ~Wyi have to be
very small for sample i being correctly classiﬁed. This is
the reason why margin-based losses provide stronger super-
visions for the same θi,yi than conventional cosine-based
losses. Proper margin settings have shown to boost the ﬁnal
recognition performance in [40, 7].

Although larger margin m provides stronger supervi-
sions, it should not be too large either. When m is over-
sized (e.g., m = 1.0), the probabilities Pi,yi becomes un-
reliable. It would output probabilities around 0 even θi,yi
is very small. This lead to large loss for almost all samples
even with very small sample-to-class angles, which makes
the training difﬁcult to converge. In previous methods, the
margin parameter selection is an ad-hoc procedure and has
no theoretical guidance for most cases.

3.3. Summary of the hyparameter study

According to our analysis, we can draw the following

conclusions:

(1) Hyperparameters scale s and margin m can substan-
tially inﬂuence the prediction probability Pi,yi of feature ~xi
with ground-truth identity/category yi. For the scale param-
eter s, too small s would limit the maximal value of Pi,yi .
On the other hand, too large s would make most predicted
probabilities Pi,yi to be 1, which makes the training loss in-

sensitive to the correctness of θi,yi . For the margin parame-
ter m, a too small margin is not strong enough to regularize
the ﬁnal angular margin, while an oversized margin makes
the training difﬁcult to converge.

(2) The effect of scale s and margin m can be uniﬁed
to modulate the mapping from cosine distances cos θi,yi to
the prediction probability Pi,yi . As shown in Fig. 2(a) and
Fig. 2(b), both small scales and large margins have simi-
lar effect on θi,yi for strengthening the supervisions, while
both large scales and small margins weaken the supervi-
sions. Therefore it is feasible and promising to control the
probability Pi,yi using one single hyperparameter, either s
or m. Considering the fact that s is more related to the range
of Pi,yi that required to span [0, 1], we will focus on auto-
matically tuning the scale parameter s in the reminder of
this paper.

4. The cosine-based softmax loss with adaptive

scaling

Based on our previous studies on the hyperparameters of
the cosine-based softmax loss functions, in this section, we
propose a novel loss with a self-adaptive scaling scheme,
namely AdaCos, which does not require the ad-hoc and
time-consuming manual parameter tuning. Training with
the proposed loss does not only facilitate convergence but
also results in higher recognition accuracy.

Our previous studies on Fig. 1 show that during the train-
ing process, the angles θi,k for k 6= yi between the feature
~xi and its non-corresponding weights ~Wk6=yi are almost al-
ways close to π
2 , In other words, we could safely assume
es·cos(π/2) = C−1 in Eq. (7). Obviously,
it is the probability Pi,yi of feature xi belonging to its cor-
responding class yi that has the most inﬂuence on supervi-
sion for network training. Therefore, we focus on designing
an adaptive scale parameter for controling the probabilities
Pi,yi .

that Bi ≈ Pk6=yi

From the curves of Pi,yi w.r.t. θi,yi (Fig. 2(a)), we ob-
serve that the scale parameter s does not only simply affect
Pi,yi ’s boundary of of determining correct/incorrect but also
squeezes/stretches the Pi,yi curvature; In contrast to scale
s, margin parameter m only shifts the curve in phase. We
therefore propose to automatically tune the scale parameter
s and eliminate the margin parameter m from our loss func-

10826

tion, which makes our proposed AdaCos loss different from
state-of-the-art softmax loss variants with angular margin.
With softmax function, the predicted probability can be de-
ﬁned by

Pi,j =

,

(9)

e˜s·cos θi,j
PC
k=1 e˜s·cos θi,k

where ˜s is the automatically tuned scale parameter to be
discussed below.

Let us ﬁrst re-consider the Pi,yi (Eq. (7)) as a function
of θi,yi . Note that θi,yi represents the angle between sam-
ple ~xi and the weight vector of its ground truth category
yi. For network training, we hope to minimize θi,yi with
the supervision from the loss function LCE. Our objective is
choose a suitable scale ˜s which makes predicted probability
Pi,yi change signiﬁcantly with respect to θi,yi . Mathemat-
ically, we ﬁnd the point where the absolute gradient value
k reaches its maximum, when the second-order

k ∂Pi,yi (θ)
derivative of Pi,yi at θ0 equals 0, i.e.,

∂θ

∂2Pi,yi (θ0)

2

∂θ0

= 0,

(10)

where θ0 ∈ [0, π
2 ]. Combining Eqs. (7) and (10), we ob-
tain the relation between the scale parameter s and the point
(θ0, P (θ0)) as

s0 =

log Bi
cos θ0

,

(11)

where Bi

can be well

approximated as Bi

=
es·cos θi,k ≈ C − 1 since the angles θi,k dis-
Pk6=yi
tribute around π/2 during training (see Eq. (7) and Fig. 1).
Then the task of automatically determining ˜s would reduce
to select an reasonable central angle ˜θ in [0, π/2].

4.1. Automatically choosing a ﬁxed scale parameter

Since π

4 is in the center of [0, π

2 ], it is natural to regard
π/4 as the point, i.e. setting θ0 = π/4 for ﬁguring out an
effective mapping from angle θi,yi to the probability Pi,yi .
Then the supervisions determined by Pi,yi would be back-
propagated to update θi,yi and further to update network
parameters. According to Eq.
(11), we can estimate the
corresponding scale parameter sf as

˜sf =

log Bi
cos π
4

=

≈

es·cos θi,k

logPk6=yi
√2 · log (C − 1)

cos π
4

(12)

where Bi is approximated by C − 1.
For such an automatically-chosen ﬁxed scale parameter
˜sf (see Figs. 2(a) and 2(b)), it depends on the number of
classes C in the training set and also provides a good guide-
line for existing cosine distance based softmax losses to
choose their scale parameters. In contrast, the scaling pa-
rameters in existing methods was manually set according to
human experience. It acts as a good baseline method for our
dynamically tuned scale parameter ˜sd in the next section.

4.2. Dynamically adaptive scale parameter

As Fig. 1 shows, the angles θi,yi between features ~xi and
their ground-truth class weights ~Wyi gradually decrease as
the training iterations increase; while the angles between
features ~xi and non-corresponding classes ~Wj6=yi become
stabilize around π

2 , as shown in Fig. 1.

Although our previously ﬁxed scale parameter ˜sf be-
haves properly as θi,yi changes over [0, π
2 ], it does not
take into account the fact that θi,yi gradually decrease dur-
ing training. Since smaller θi,yi gains higher probability
Pi,yi and thus gradually receives weaker supervisions as
the training proceeds, we therefore propose a dynamically
adaptive scale parameter ˜sd to gradually apply stricter re-
quirement on the position of θ0 which can progressively en-
hance the supervisions throughout the training process.

Formally we introduce a modulating indicator variable
θ(t)
med, which is the median of all corresponding classes’ an-
gles, θ(t)
, from the mini-batch of size N at the t-th itera-
i,yi
tion. θ(t)
med roughly represents the current network’s degree
of optimization on the mini-batch. When the median angle
is large, it denotes that the network parameters are far from
optimum and less strict supervisions should be applied to
make the training converge more stably; when the median
angle θ(t)
med is small, it denotes that the network is close to op-
timum and stricter supervisions should be applied to make
the intra-class angles θi,yi become even smaller. Based on
this observation, we set the central angle ˜θ(t)
0 = θ(t)
med. We
also introduce B(t)
as

avg as the average of B(t)

i

B(t)

avg =

1

N Xi∈N (t)

B(t)

i =

1

N Xi∈N (t) Xk6=yi

e˜s(t−1)

d

·cos θi,k ,

(13)

i

d

using the scale parameter ˜s(t−1)

where N (t) denotes the face identity indices in the mini-
batch at the t-th iteration. Unlike approximating Bi ≈ C−1
for the ﬁxed adaptive scale parameter ˜sf , here we estimate
B(t)
of previous iteration,
which provides us a more accurate approximation. Be re-
minded that B(t)
d . We
can obtain it by solving the nonlinear function given by the
above equation. In practice, we notice that ˜s(t)
d changes very
little following iterations. So, we just use ˜s(t−1)
to calculate
B(t)
i with Eq. (7). Then we can obtain dynamic scale ˜s(t)
directly with Eq. (11). So we have:

also includes dynamic scale ˜s(t)

d

d

i

˜s(t)
d =

avg

log B(t)
cos θ(t)

med

,

(14)

where B(t)
estimate it using the scale parameter ˜s(t−1)
iteration.

avg is related to the dynamic scale parameter. We
of the previous

d

10827

At the begin of the training process, the median an-
gle θ(t)
med of each mini-batch might be too large to impose
enough supervisions for training. We therefore force the
central angle θ(t)
4 . Our dynamic scale
parameter for the t-th iteration could then be formulated as

med to be less than π
√2 · log (C − 1)
log B(t)
cos(cid:16)min( π
4 , θ(t)

avg




med)(cid:17)

˜s(t)
d =

t = 0,

t ≥ 1,

(15)

where ˜s(0)
d
when t = 0.

is initialized as our ﬁxed scale parameter ˜sf

Substituting ˜s(t)

d into fi,j = ˜s(t)

ing gradients can be calculated as follows

d ·cos θi,j , the correspond-

C

=

Xj=1
= (P (t)

∂L(~xi)
∂~xi
∂L( ~Wj)
∂ ~Wj

(P (t)

i,j − ✶(yi = j) · ˜s(t)

d

∂ cos θi,j

∂~xi

,

(16)

i,j − ✶(yi = j)) · ˜s(t)

d

∂ cos θi,j

∂ ~Wj

,

where ✶ is the indicator function and

P (t)
i,j =

d ·cos θi,j

e˜s(t)
k=1 e˜s(t)
PC

d ·cos θi,k

.

(17)

Eq. (17) shows that the dynamically adaptive scale param-
eter ˜s(t)
inﬂuences classiﬁcation probabilities differently
d
at each iteration and also effectively affects the gradients
(Eq. (16)) for updating network parameters. The beneﬁt of
dynamic AdaCos is that it can produce reasonable scale pa-
rameter by sensing the training convergence of the model in
the current iteration.

5. Experiments

We examine the proposed AdaCos loss function on sev-
eral public face recognition benchmarks and compare it
with state-of-the-art cosine-based softmax losses. The com-
pared losses include l2-softmax [28], CosFace [40], and
ArcFace [7]. We present evaluation results on LFW [13],
MegaFace 1-million Challenge [24], and IJB-C [23] data.
We also present results on some exploratory experiments
to show the convergence speed and robustness against low-
resolution images.

Preprocessing. We use two public training datasets,
CASIA-WebFace [43] and MS1M [9], to train CNN mod-
els with our proposed loss functions. We carefully clean
the noisy and low-quality images from the datasets. The
cleaned WebFace [43] and MS1M [9] contain about 0.45M
and 2.35M facial images, respectively. All models are
trained based on these training data and directly tested on
the test splits of the three datasets. RSA [22] is applied to
the images to extract facial areas. Then, according to de-
tected facial landmarks, the faces are aligned through simi-
larity transformation and resized to the size 144 × 144. All

Method

Softmax
l2-softmax [28]
CosFace [40]
ArcFace [7]
Fixed AdaCos
Dyna. AdaCos

1st

93.05
98.22
99.37
99.55
99.63
99.73

2nd

92.92
98.27
99.35
99.37
99.62
99.72

3rd

93.27
98.08
99.42
99.43
99.55
99.68

Average Acc.

93.08
98.19
99.38
99.45
99.60
99.71

Table 1: Recognition accuracy on LFW by ResNet-50 trained
with different compared losses. All the methods are trained on the
cleaned WebFace [43] training data and tested on LFW for three
times to obtain the average accuracy.

14.0

13.5

13.0

12.5

12.0

11.5

11.0

10.5

10.0

s

f
o

e
u
l
a
V

Dynamic AdaCos
Fixed AdaCos

Num. of Iteration

Figure 3: The change of the ﬁxed adaptive scale parameter ˜sf
and dynamic adaptive scale parameter ˜s(t)
d when training on the
cleaned WebFace dataset. The dynamic scale parameter ˜s(t)
d grad-
ually and automatically decreases to strengthen training supervi-
sions for feature angles θi,yi , which validates our assumption on
the adaptive scale parameter in our proposed AdaCos loss. Best
viewed in color.

image pixel values are subtracted with the mean 127.5 and
dividing by 128.

5.1. Results on LFW

The LFW [13] dataset collected thousands of identi-
ties from the inertnet.
Its testing protocol contains about
13, 000 images for about 1, 680 identities with a total of
6, 000 ground-truth matches. Half of the matches are posi-
tive while the other half are negative ones. LFW’s primary
difﬁculties lie in face pose variations, color jittering, illu-
mination variations and aging of persons. Note portion of
the pose variations can be eliminated by the RSA [22] fa-
cial landmark detection and alignment algorithm, but there
still exist some non-frontal facial images which can not be
aligned by RSA [22] and then aligned manually.

5.1.1 Comparison on LFW

For all experiments on LFW [13], we train ResNet-50 mod-
els [10] with batch size of 512 on the cleaned WebFace [43]
dataset. The input size of facial image is 144 × 144 and the
feature dimension input into the loss function is 512. Differ-
ent loss functions are compared with our proposed AdaCos
losses.

Results in Table 1 show the recognition accuracies of
models trained with different softmax loss functions. Our
proposed AdaCos losses with ﬁxed and dynamic scale pa-
rameters (denoted as Fixed AdaCos and Dyna. AdaCos)

10828

j
,
i
θ

f
o

e
e
r
g
e
D
e
g
a
r
e
v
A

π
2

7π
16

3π
8

5π
16

π
4

3π
16

π
8

π
16

θi,yi of l2-softmax
θi,yi of CosFace
θi,yi of ArcFace
θi,yi of Dynamic AdaCos
θi,j, j 6= yi of Dynamic AdaCos

Method

Softmax
l2-softmax [28]
CosFace [40]
ArcFace [7]
Fixed AdaCos
Dyna. AdaCos

Num. of Iteration

25k

70.15
79.08
78.17
82.43
85.10
88.52

50k

85.33
88.52
90.87
92.37
94.38
95.78

75k

89.50
93.38
98.52
98.78
99.05
99.30

100k

93.05
98.22
99.37
99.55
99.63
99.73

Table 2: Convergence rates of different softmax losses. At the
same iterations, training with our proposed dynamic AdaCos loss
leads to the best recognition accuracy.

0k

20k

40k
Num. of Iteration

60k

80k

100k

Figure 4: The change of θi,yi when training on the cleaned Web-
Face dataset. θi,yi represents the angle between the feature vector
of i-th sample and the weight vector of its ground truth category yi.
Curves calculated by proposed dynamic AdaCos loss, l2-softmax
loss [28], CosFace [40] and ArcFace [7] are shown. Best viewed
in color.

surpass the state-of-the-art cosine-based softmax losses un-
der the same training conﬁguration. For the hyperparam-
eter settings of the compared losses, the scaling parame-
ter is set as 30 for l2-softmax [28], CosFace [40] and Arc-
Face [7]; the margin parameters are set as 0.25 and 0.5 for
CosFace [40], and ArcFace [7], respectively. Since LFW is
a relatively easy evaluation set, we train and test all losses
for three times. The average accuracy of our proposed dy-
namic AdaCos is 0.26% higher than state-of-the-art Arc-
Face [7] and 1.52% than l2-softmax [28].

5.1.2 Exploratory Experiments

The change of scale parameters and feature angles
during training. In this part, we will show the change of
scale parameter ˜s(t)
d and feature angles θi,j during training
with our proposed AdaCos loss. The scale parameter ˜s(t)
d
changes along with the current recognition performance of
the model, which continuously strengthens the supervisions
by gradually reducing θi,yi and thus shrinking ˜s(t)
d . Fig. 3
shows the change of the scale parameter s with our pro-
posed ﬁxed AdaCos and dynamic AdaCos losses. For the
dynamic AdaCos loss, the scale parameter ˜s(t)
adaptively
d
decreases as the training iterations increase, which indicates
that the loss function provides stricter supervisions to up-
date network parameters. Fig. 4 illustrates the change of
θi,j by our proposed dynamic AdaCos and l2-softmax. The
average (orange curve) and median (green curve) of θi,yi ,
which indicating the angle between a sample and its ground-
truth category, gradually reduce while the average (maroon
curve) of θi,j where j 6= yi remains nearly π
2 . Compared
with l2-softmax loss, our proposed loss could achieve much
smaller sample feature to category angles on the ground-
truth classes and leads to higher recognition accuracies.

Convergence rates. Convergence rate is an important

100

99

98

97

96

95

94

93

92

l2-softmax s = 45
CosFace s = 45, m = 0.15
ArcFace s = 45, m = 0.3
Fixed AdaCos

Dynamic AdaCos

)

%

(

y
c
a
r
u
c
c
A

101

102

103
Size of Distractor

104

105

106

Figure 5: Recognition accuracy curves on MegaFace dataset
by Inception-ResNet [34] models trained with different softmax
losses and on the same cleaned WebFace [43] and MS1M [9] train-
ing data. Best viewed in color.

indicator of efﬁciency of loss functions. We examine the
convergence rates of several cosine-based losses at different
training iterations. The training conﬁgurations are same as
Table 1. Results in Table 2 reveal that the convergence rates
when training with the AdaCos losses are much higher.

5.2. Results on MegaFace

We then evaluate the performance of proposed AdaCos
on the MegaFace Challenge [16], which is a publicly avail-
able identiﬁcation benchmark, widely used to test the per-
formance of facial recognition algorithms. The gallery set
of MegaFace incorporates over 1 million images from 690K
identities collected from Flickr photos [37]. We follow Ar-
cFace [7]’s testing protocol, which cleaned the dataset to
make the results more reliable. We train the same Inception-
ResNet [33] models with CASIA-WebFace [43] and MS1M
[9] training data, where overlapped subjects are removed.

Table 3 and Fig. 5 summarize the results of models
trained on both WebFace and MS1M datasets and tested on
the cleaned MegaFace dataset. The proposed AdaCos and
state-of-the-art softmax losses are compared, where the dy-
namic AdaCos loss outperforms all compared losses on the
MegaFace.

5.3. Results on IJB C 1:1 veriﬁcation protocol

The IJB-C dataset [23] contains about 3, 500 identities
with a total of 31, 334 still facial images and 117, 542 un-
constrained video frames. In the 1:1 veriﬁcation, there are

10829

Method

l2-softmax
CosFace
ArcFace

Fixed AdaCos

Dynamic AdaCos

101

102

103

104

105

106

Size of MegaFace Distractor

99.73% 99.49% 99.03% 97.85% 95.56% 92.05%
99.82% 99.68% 99.46% 98.57% 97.58% 95.50%
99.78% 99.65% 99.48% 98.87% 98.03% 96.88%
99.85% 99.70% 99.47% 98.80% 97.92% 96.85%
99.88% 99.72% 99.51% 99.02% 98.54% 97.41%

Table 3: Recognition accuracy on MegaFace by Inception-ResNet [34] models trained with different compared softmax loss and the same
cleaned WebFace [43] and MS1M [9] training data.

Method

FaceNet [29]
VGGFace [25]

Crystal Loss [27]

l2-softmax
CosFace [40]
ArcFace [7]

Fixed AdaCos

Dynamic AdaCos

10−1

10−2

10−3

10−4

10−5

10−6

10−7

True Accept Rate @ False Accept Rate

92.45% 81.71% 66.45% 48.69% 33.30% 20.95%
95.64% 87.13% 74.79% 59.75% 43.69% 32.20%
99.06% 97.66% 95.63% 92.29% 87.35% 81.15% 71.37%
98.40% 96.45% 92.78% 86.33% 77.25% 62.61% 26.67%
99.01% 97.55% 95.37% 91.82% 86.94% 76.25% 61.72%
99.07% 97.75% 95.55% 92.13% 87.28% 82.15% 72.28%
99.05% 97.70% 95.48% 92.35% 87.87% 82.38% 72.66%
99.06% 97.72% 95.65% 92.40% 88.03% 83.28% 74.07%

-
-

Table 4: True accept rates by different compared softmax losses on the IJB-C 1:1 veriﬁcation task. The same training data (WebFace [43]
and MS1M [9]) and Inception-ResNet [33] networks are used. The results of FaceNet [29], VGGFace [25], and Crystal Loss [27] are from
[27].

100

80

60

40

20

FaceNet

VggFace

Crystal Loss

l2-softmax s = 45

CosFace s = 45, m = 0.15

ArcFace s = 45, m = 0.3

Fixed AdaCos

Dynamic AdaCos

)

%

(

e
t
a
R
t
p
e
c
c
A
e
u
r
T

0
10−1

10−2

10−3

10−4

10−5

10−6

10−7

False Accept Rate

Figure 6: TARs by different compared softmax losses on the IJB-
C 1:1 veriﬁcation task. The same training data (WebFace [43] and
MS1M [9]) and Inception-ResNet [33] are used. The results of
FaceNet [29], VGGFace [25] are reported in Crystal Loss [27].
19, 557 positive matches and 15, 638, 932 negative matches,
which allow us to evaluate TARs at various FARs (e.g.,
10−7).

We compare the softmax loss functoins, including the
proposed AdaCos, l2-softmax [28], CosFace [40], and Ar-
cFace [7] with the same training data (WebFace [43] and
MS1M [9]) and network architecture (Inception-ResNet
[33]). We also report the results of FaceNet [29], VGGFace
[36] listed in Crystal loss [27]. Table 4 and Fig. 6 exhibit
their performances on the IJB-C 1:1 veriﬁcation. Our pro-
posed dynamic AdaCos achieves the best performance.

6. Conclusions

In this work, we argue that the bottleneck of existing
cosine-based softmax losses may primarily comes from the
mis-match between cosine distance cos θi,yi and the clas-

siﬁcation probability Pi,yi , which limits the ﬁnal recogni-
tion performance. To address this issue, we ﬁrst deeply an-
alyze the effects of hyperparameters in cosine-based soft-
max losses from the perspective of probability. Based on
these analysis, we propose the AdaCos which automati-
cally adjusts an adaptive parameter ˜s(t)
d in order to reformu-
late the mapping between cosine distance and classiﬁcation
probability. Our proposed AdaCos loss is simple yet ef-
fective. We demonstrate its effectiveness and efﬁciency by
exploratory experiments and report its state-of-the-art per-
formances on several public benchmarks.

Acknowledgements. This work is supported in part by
SenseTime Group Limited, in part by the General Research
Fund through the Research Grants Council of Hong
Kong under Grants CUHK14202217, CUHK14203118,
CUHK14205615, CUHK14207814, CUHK14213616,
CUHK14208417, CUHK14239816,
in part by CUHK
Direct Grant, and in part by National Natural Science
Foundation of China (61472410) and the Joint Lab of
CAS-HK.

References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning.
In OSDI, vol-
ume 16, pages 265–283, 2016.

[2] Peter N. Belhumeur, Jo˜ao P Hespanha, and David J. Krieg-
man. Eigenfaces vs. ﬁsherfaces: Recognition using class
speciﬁc linear projection.
IEEE Transactions on pattern
analysis and machine intelligence, 19(7):711–720, 1997.

10830

[3] James Bergstra and Yoshua Bengio. Random search for
hyper-parameter optimization. Journal of Machine Learn-
ing Research, 13(Feb):281–305, 2012.

[4] James S Bergstra, R´emi Bardenet, Yoshua Bengio, and
Bal´azs K´egl. Algorithms for hyper-parameter optimization.
In Advances in neural information processing systems, pages
2546–2554, 2011.

[5] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang,
Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and
Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine
learning library for heterogeneous distributed systems. arXiv
preprint arXiv:1512.01274, 2015.

[6] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation.
In Computer Vision and Pattern Recognition,
2005. CVPR 2005. IEEE Computer Society Conference on,
volume 1, pages 539–546. IEEE, 2005.

[7] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface:
Additive angular margin loss for deep face recognition. arXiv
preprint arXiv:1801.07698, 2018.

[8] Siddharth Gopal and Yiming Yang. Von mises-ﬁsher cluster-
ing models. In International Conference on Machine Learn-
ing, pages 154–162, 2014.

[9] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for
large-scale face recognition.
In European Conference on
Computer Vision, pages 87–102. Springer, 2016.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[11] Elad Hoffer and Nir Ailon. Deep metric learning using triplet
In International Workshop on Similarity-Based

network.
Pattern Recognition, pages 84–92. Springer, 2015.

[12] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017.

[13] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik
Learned-Miller. Labeled faces in the wild: A database for
studying face recognition in unconstrained environments.
Technical report, Technical Report 07-49, University of Mas-
sachusetts, Amherst, 2007.

[14] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown.
Sequential model-based optimization for general algorithm
conﬁguration. In International Conference on Learning and
Intelligent Optimization, pages 507–523. Springer, 2011.

[15] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In Proceedings of the 22nd ACM inter-
national conference on Multimedia, pages 675–678. ACM,
2014.

[16] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel
Miller, and Evan Brossard. The megaface benchmark: 1
million faces for recognition at scale. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4873–4882, 2016.

In Advances in neural information processing sys-

works.
tems, pages 1097–1105, 2012.

[18] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. Sphereface: Deep hypersphere embedding
for face recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 1, 2017.

[19] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
Large-margin softmax loss for convolutional neural net-
works. In ICML, pages 507–516, 2016.

[20] Yu Liu, Hongyang Li, and Xiaogang Wang. Learning deep
features via congenerous cosine loss for person recognition.
arXiv preprint arXiv:1702.06890, 2017.

[21] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking fea-
ture discrimination and polymerization for large-scale recog-
nition. arXiv preprint arXiv:1710.00870, 2017.

[22] Yu Liu, Hongyang Li, Junjie Yan, Fangyin Wei, Xiaogang
Wang, and Xiaoou Tang. Recurrent scale approximation for
object detection in cnn. In IEEE International Conference
on Computer Vision, 2017.

[23] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan
Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler
Niggel, Janet Anderson, Jordan Cheney, et al.
Iarpa janus
benchmark–c: Face dataset and protocol. In 11th IAPR In-
ternational Conference on Biometrics, 2018.

[24] Aaron Nech and Ira Kemelmacher-Shlizerman. Level play-
ing ﬁeld for million scale face recognition.
In 2017 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3406–3415. IEEE, 2017.

[25] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman.

Deep face recognition.

[26] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS-W, 2017.

[27] Rajeev Ranjan, Ankan Bansal, Hongyu Xu, Swami Sankara-
narayanan, Jun-Cheng Chen, Carlos D Castillo, and Rama
Chellappa. Crystal loss and quality pooling for uncon-
strained face veriﬁcation and recognition. arXiv preprint
arXiv:1804.01159, 2018.

[28] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-
constrained softmax loss for discriminative face veriﬁcation.
arXiv preprint arXiv:1703.09507, 2017.

[29] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 815–823, 2015.

[30] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Prac-
tical bayesian optimization of machine learning algorithms.
In Advances in neural information processing systems, pages
2951–2959, 2012.

[31] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.
Deep learning face representation by joint identiﬁcation-
veriﬁcation. In Advances in neural information processing
systems, pages 1988–1996, 2014.

[32] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang.
Deepid3: Face recognition with very deep neural networks.
arXiv preprint arXiv:1502.00873, 2015.

[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-

[33] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the

10831

impact of residual connections on learning.
ume 4, page 12, 2017.

In AAAI, vol-

[34] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In AAAI, vol-
ume 4, page 12, 2017.

[35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2015.

[36] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior
Wolf. Deepface: Closing the gap to human-level perfor-
mance in face veriﬁcation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1701–1708, 2014.

[37] Bart Thomee, David A Shamma, Gerald Friedland, Ben-
jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and
Li-Jia Li. Yfcc100m: The new data in multimedia research.
arXiv preprint arXiv:1503.01817, 2015.

[38] Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Ad-
ditive margin softmax for face veriﬁcation. arXiv preprint
arXiv:1801.05599, 2018.

[39] Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille.
Normface: l 2 hypersphere embedding for face veriﬁcation.
arXiv preprint arXiv:1704.06369, 2017.

[40] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li,
Dihong Gong, Jingchao Zhou, and Wei Liu. Cosface: Large
margin cosine loss for deep face recognition. arXiv preprint
arXiv:1801.09414, 2018.

[41] Kilian Q Weinberger and Lawrence K Saul. Distance met-
ric learning for large margin nearest neighbor classiﬁcation.
Journal of Machine Learning Research, 10(Feb):207–244,
2009.

[42] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A
discriminative feature learning approach for deep face recog-
nition. In European Conference on Computer Vision, pages
499–515. Springer, 2016.

[43] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-
arXiv preprint

ing face representation from scratch.
arXiv:1411.7923, 2014.

[44] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and
Yu Qiao. Range loss for deep face recognition with long-
tailed training data. In The IEEE International Conference
on Computer Vision (ICCV), Oct 2017.

[45] Yutong Zheng, Dipan K Pal, and Marios Savvides. Ring
loss: Convex feature normalization for face recognition. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 5089–5097, 2018.

10832

