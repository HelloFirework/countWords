Joint Representation and Estimator Learning for Facial

Action Unit Intensity Estimation

Yong Zhang1, Baoyuan Wu1 ∗, Weiming Dong2, Zhifeng Li1, Wei Liu1, Bao-Gang Hu2, and Qiang Ji3

1Tencent AI Lab, 2National Laboratory of Pattern Recognition, CASIA, 3Rensselaer Polytechnic Institute

{zha6.5ngyong201303,wubaoyuan1987}@gmail.com, weiming.dong@ia.ac.cn

michaelzfli@tencent.com, wl2223@columbia.edu, hubg@nlpr.ia.ac.cn, qji@ecse.rpi.edu

Abstract

Facial action unit (AU) intensity is an index to charac-
terize human expressions. Accurate AU intensity estima-
tion depends on three major elements: image representa-
tion, intensity estimator, and supervisory information. Most
existing methods learn intensity estimator with ﬁxed image
representation, and rely on the availability of fully annotat-
ed supervisory information. In this paper, a novel general
framework for AU intensity estimation is presented, which
differs from traditional estimation methods in two aspect-
s. First, rather than keeping image representation ﬁxed, it
simultaneously learns representation and intensity estima-
tor to achieve an optimal solution. Second, it allows in-
corporating weak supervisory training signal from human
knowledge (e.g. feature smoothness, label smoothness, la-
bel ranking, and positive label), which makes our model
trainable even fully annotated information is not available.
More speciﬁcally, human knowledge is represented as ei-
ther soft or hard constraints which are encoded as regular-
ization terms or equality/inequality constraints, respective-
ly. On top of our novel framework, we additionally propose
an efﬁcient algorithm for optimization based on Alternat-
ing Direction Method of Multipliers (ADMM). Evaluations
on two benchmark databases show that our method outper-
forms competing methods under different ratios of AU in-
tensity annotations, especially for small ratios.

1. Introduction

Facial Action Coding System (FACS) [7] deﬁnes AUs to
depict facial muscle movements. It quantiﬁes AU intensi-
ties into 6 ordinal levels and provides instructions for anno-
tation. AU intensity estimation is challenging partially due
to the lack of annotations. Reliable AU intensity labels can
be annotated by trained human coders. It is convenient to
capture a large set of expression sequences through digital

∗Corresponding author.

Figure 1: The diagram of joint learning for AU intensity estima-
tion. It simultaneously learns image representation and intensity
estimator with limited annotations. Human knowledge is incorpo-
rated as hard or soft constraints to provide weak supervision.

cameras. However, it requires great effort to annotate every
frame, which makes it difﬁcult to construct a large database.
The performance of AU intensity estimation is deter-
mined by several factors, including image representation,
intensity estimator, and supervisory information. Most ex-
isting methods [16, 10] focus on estimator learning, regard-
less of image representation and unlabeled frames. Repre-
sentation is ﬁrstly learned by an unsupervised or supervised
method. Then, the estimator is trained in the pre-learned
feature space. However, the pre-learned feature space is
not guaranteed to ﬁt well the estimator learning since they
are learned separately without considering their connection-
s. Recently, deep learning provides an end-to-end strategy
to learn a mapping from the input to the output [8]. It can
be treated as a joint representation and estimator learning
method. But deep models require a large amount of labeled
samples to avoid overﬁtting. Though [41] learns a weakly
supervised deep model, it still requires thousands of images.
Besides, few works focus on intensity estimation with
limited annotations except for [26, 43, 42, 41]. Human
knowledge such as label smoothness or label ranking are
exploited to compensate the lack of annotations. [26] en-
codes label smoothness through the structure of dynamic
models while [42] emphasizes the smoothness on relevance

3457

through regularization. Label ranking has been studied in
ordinal regression [9] and applied to intensity estimation
in [43, 42, 41]. Other types of general knowledge such as
feature smoothness and positive intensity are rarely studied.
The smoothness refers to soft constraint while label ranking
and positive intensity refer to hard constraint. Though there
exist various types of human knowledge, existing methods
always leverage one of them and there is no general frame-
work to incorporate different types of knowledge.

Joint representation and estimator learning and human
knowledge have been studied individually, but there lacks
a framework to simultaneously incorporate all of them. To
alleviate this issue, we propose a general framework for AU
intensity estimation (see Fig. 1), which can not only joint-
ly learn representation and estimator with weakly labeled
sequences, but also ﬂexibly incorporate different types of
knowledge on AU intensity and image representation. Weak
annotation refers that locations of peak and valley frames
are ﬁrstly identiﬁed in training sequences (qualitative an-
notation) and then intensities of few selected frames are la-
beled (quantitative annotation). Identiﬁcation of peak and
valley frames can be performed according to their deﬁni-
tions in [17], which is much easier to obtain than quanti-
tatively annotating every frame. The proposed framework
has several advantages. Firstly, representation and estima-
tor are jointly optimized to make accurate intensity predic-
tion. The joint learning can obtain a better solution than
individual learning. Secondly, human knowledge involves
both representation and estimator. It can provide weak su-
pervision for joint learning and make it feasible to exploit
unlabeled frames more efﬁciently by using relationships a-
mong labeled and unlabeled frames. Thirdly, this frame-
work requires scarce intensity annotations to obtain satis-
ﬁed performance, while deep models need much more.

Our main contributions are three-fold. (1) We propose
a general framework for AU intensity estimation to joint-
ly learn representation and estimator with very limited in-
tensity annotations and to encode different types of human
(2) We develop an efﬁcient algorithm based
knowledge.
on the ADMM framework to optimize the formulated prob-
lem. (3) Evaluations under different annotation ratios are
performed on two benchmark databases to demonstrate the
superior performance of the proposed method.

2. Related Work

AU intensity estimation. Most existing approaches
use supervised techniques for AU intensity estimation such
as [16, 10, 38, 22, 32, 21, 12, 36]. They leverage only la-
beled images. Several methods exploit spatial relationships
among the intensities of multiple AUs through probabilistic
graphical models, such as tree-structured Markov random
ﬁeld [27], copula conditional random ﬁeld [35]. Kaltwang
et al. [11] propose a latent tree model by learning graph

structure from features and labels. The co-occurrence of
AU pairs is spatial label smoothness which is implicitly en-
coded in potential functions. Besides, several works con-
sider temporal label smoothness by using dynamic model-
s, such as hidden Markov model [18], dynamic Bayesian
networks [14], context-sensitive conditional ordinal random
ﬁeld [24]. Temporal label smoothness is implicitly encod-
ed in the model structure. However, these supervised mod-
els require frame-level intensity annotations and they on-
ly focus on estimator learning, regardless of representation
learning. Deep models have made astonishing progress in
different ﬁelds due to their large model complexity. Super-
vised deep models have been used to estimate AU intensi-
ty [8, 34, 31]. However, they contain millions of parameters
and require a large amount of annotated images for training.

Few works focus on using unlabeled images for AU
intensity estimation. Multi-instance learning (MIL) has
been used for event detection such as MS-MIL [29], RMC-
MIL [25], and LOMo [30]. Event detection is a binary
classiﬁcation problem, but the AU intensity has multiple
levels. They can not be directly applied. Only method-
s [26, 43, 42, 41] exploit unlabeled images for AU intensity
estimation. Ruiz et al. [26] propose Multi-instance Dynam-
ic Ordinal Random Fields (MI-DORFs) by exploiting multi-
instance learning to treat each sequence as a bag. Zhang et
al. [42] propose a bilateral ordinal multi-instance regression
model (BORMIR). Both use temporal label smoothness to
exploit unlabeled images. Zhao et al. [43] estimate expres-
sion intensity estimation by combining ordinal regression
and SVR (OSVR). They use label ranking to exploit unla-
beled images. These methods learn only intensity estima-
tor or leverage only one type of domain knowledge, regard-
less of image representation. Differently, our method joint-
ly learns estimator and representation. We simultaneously
incorporate all types of knowledge such as features smooth-
ness, label smoothness, label ranking, and positive intensity.
Besides, knowledge is applied in the subspace rather than in
the original feature space. A weakly supervised deep mod-
el is used in [41], but it still requires thousands of labeled
images. Differently, our method performs the joint learning
with only few hundreds of labeled images and is applicable
to a small database.

Semi-supervised learning. Semi-supervised learning
methods learn models with both labeled and unlabeled im-
ages. Different assumptions are made on the correlation
between sample representation and target label, including
smoothness assumption [15], cluster assumption [4], and
manifold assumption [1, 20]. Kim et al. [13] consider the
second-order Hessian energy for semi-supervised regres-
sion (HSSR) under the manifold assumption. These meth-
ods leverage unlabeled images, but they do not learn the rep-
resentation. Zhang et al. [40] proposed a convex subspace
learning (CSL) approach by combining unsupervised sub-

3458

space learning and supervised classiﬁer learning. Compared
to [40], our method incorporates various types of human
knowledge to leverage unlabeled images more efﬁciently.

denotes the parameters of the estimator. For training, giv-
en Df and Ds, we jointly learn the representation Φl and
Φu, the subspace span(B), and the intensity estimator w.

3. The Proposed Approach

3.1. Weak annotation

The smoothness of muscle movements leads to the s-
mooth evolution of facial appearance. AU intensity also e-
volves smoothly in sequences if the frame rate of camera is
high enough to capture subtle changes of facial appearance.
Weak annotation consists of two parts, i.e., qualitative an-
notation and quantitative annotation. Qualitative annotation
refers to identifying the locations of key frames, i.e., peak
and valley frames. Quantitative annotation refers to anno-
tating AU intensities of a small set of frames in sequences.
Though multiple peaks and valleys exist in a sequence, they
occupy a small portion in the whole database. Weak an-
notation contains the locations of key frames and intensity
annotations of few frames, which is easier to achieve than
labeling every frame. Given the weak annotation, training
sequences can be split into segments according to the loca-
tions of key frames. AU intensity monotonically evolves in
each training segment and it has three types, i.e., increasing,
decreasing, and keeping the same. Following [42], to avoid
using an extra variable to specify the trend, we reverse the
frame ordering of segments that evolve from a peak to a val-
ley. Finally, AU intensity increases or keeps the same in all
training segments.

3.2. Problem statement

u }M

u }Tm

u; ...; SM

u = {Sm,t

m=1, where Sm

Given unlabeled expression sequences, the locations of
key frames are ﬁrstly identiﬁed. Then, intensities of par-
tial randomly selected frames are labeled. The training
set consists of two parts. One part is qualitatively labeled
segments, Ds = {Sm
t=1 ∈
RTm×d denotes the features of frames in the m-th segment
and Tm is the number of frames. Su = [S1
u ] is the
concatenated features of all segments. This part has only
the trend of intensity. Intensity in Sm
u increases or keep-
s the same. The other part is a small set of quantitatively
labeled frames, Df = {xn, yn}N
n=1, where xn ∈ Rd is
the raw feature vector of the n-th frame and yn ∈ R is its
intensity. Xl ∈ RN ×d is the concatenated features of all la-
beled frames and Yl ∈ RN is their AU intensities. Frames
in Xl are randomly selected frames rather than neighbor
frames. Each row of Xl and Su represents the features of a
frame. Note that Xl is a subset of Su. For convenience, we
denote them separately to avoid extra frame indexes. Let
B ∈ RK×d denote the basis vectors. K is the number of
basis vectors. Φl denotes the coefﬁcients of labeled frames
Xl in latent space, and Φu = [Φ1
u ] denotes the co-
u ∈ RTm×K . w ∈ RK
efﬁcients of segments Su, where Φm

u; ...; ΦM

3.3. Hard constraints from human knowledge

Limited AU intensity annotations
In training sequences,
only few frames are labeled with AU intensities. The inten-
sity labels provide strong supervision for joint learning. The
representation and estimator are encouraged to satisfy

Φlw = Yl.

(1)

The annotations of few frames are encoded as equality con-
straints. It is equivalent to put the loss in the objective, i.e.,

Ll(w, Φl, Df ) =

λ0
2

kΦlw − Ylk2.

(2)

Temporal label ranking During a facial action, AU in-
tensity evolves smoothly over time. As mentioned in Sec-
tion 3.2, training sequences are split into segments accord-
ing to key frames. AU intensity monotonically increases
or keeps the same in each training segment. Though AU
intensities of frames in a segment are unknown, the tempo-
ral relationships among multiple frames can provide weak
supervision for joint learning. Instead of constraining the
original representation Su, we emphasize domain knowl-
edge on the new representation Φu. In a training segment,
the representation and estimator are encouraged to satisfy
that the intensity of the current frame should be larger than
or equal to its previous frames,

Φm,1

u

w ≤ ... ≤ Φm,i

u

w ≤ Φm,i+1

u

w ≤ ... ≤ Φm,Tm

u

w,

where label ranking is encoded as inequality constraints. It
is equivalent to

ΓmΦm
u

w ≤ 0,

(3)

where Γm ∈ R(Tm−1)×Tm is a matrix with Γm
Γm

i,i = 1,
i,i+1 = −1, and other elements being 0’s. For all qual-

itatively labeled segments, we have a set of constrains,

ΓΦuw ≤ 0,

(4)

where Γ = diag([Γ1, Γ2, ..., ΓM ]) and 0 is a vector with all
elements being 0’s. Different from [43, 42], we emphasize
ranking constrains on both representation and estimator.

Positive intensity AU intensity is a non-negative value.
According to this prior knowledge, the prediction of AU in-
tensity is encouraged to be not less than 0. Such knowledge
is encoded as constraints which provide weak supervision
for joint learning. The constraints are deﬁned as

Φuw ≥ 0, Φlw ≥ 0.

(5)

The positive intensity is encoded as inequality constraints.

3459

3.4. Soft constraints from human knowledge

Temporal label smoothness AU intensity is labeled ac-
cording to corresponding local appearance. Since muscle-
s move smoothly, facial appearance also changes smooth-
ly over time.
In a training segment, the intensity of a
frame is close to the intensities of its neighbor frames. The
representation and estimator are encouraged to satisfy that
the intensities of neighbor frames should be similar, i.e.,
wk2 is supposed to be small for two neigh-
kΦm,i
bor frames i and j. Considering all qualitatively labeled
segments, we have the following regularization, i.e.,

w − Φm,j

u

u

RI (w, Φu, Ds) =

1
2

M

Xm=1

Tm

Xi,j

Cm

i,j(Φm,i

u

w − Φm,j

u

w)2

= (Φuw)T L(Φuw),

(6)

where Lm = Dm−Cm and L = diag([L1, L2, ..., LM ]). L
is a positive semi-deﬁnite matrix. Cm is an adjacent matrix,
where Cm
i,j = 1 if the j-th and i-th frames are neighbors.
Otherwise Cm
i,i =
i,j . Since frames in Xl are not neighbor frames, label

i,j = 0. Dm is a diagonal matrix with Dm

Cm

smoothness can not be applied to Φlw.

Pj

Temporal feature smoothness As facial muscles move s-
moothly, neighbor frames in sequences have similar facial
appearance. The learned representation should keep such
property that neighbor frames should have similar represen-
tations. The distance between representations of neighbor
frames should small. Such knowledge can be encoded as a
regularization term, i.e.,

RF (Φu, Ds) =

1
2

M

Xm=1

Tm

Xi,j

Cm

i,j||Φm,i

u − Φm,j

u ||2

= tr(ΦT
u

LΦu),

(7)

where L is the sames as Eq.( 6). tr(·) represents the trace.
Representation and estimator are coupled in Eqs. (6) and (4)
while Eq. (7) involves only the representation.

3.5. Formulation

Given qualitatively labeled segments and limited quan-
titatively labeled frames, we formulate the problem as fol-
lows. For representation learning, learned coefﬁcient ma-
trix and basis vectors should be able to reconstruct raw fea-
tures [23]. The reconstruction loss is deﬁned as

Lu(Φl, Φu, B, Df , Ds)

=

1

2(cid:13)(cid:13)(cid:13)

(cid:20)Xl
Su(cid:21) −(cid:20) Φl

2

F

Φu(cid:21) B(cid:13)(cid:13)(cid:13)

+ λ1(cid:13)(cid:13)(cid:13)

Φu(cid:21)T
(cid:20) Φl

(cid:13)(cid:13)(cid:13)2,1

where k · k2,1 encourages to learn features through the w-
hole dataset rather than regularizing features of individu-
al samples. To avoid degeneracy, the convex set for B is
B = {b : kbk2 ≤ 1}.

Considering unlabeled samples and human knowledge,
the joint learning of image representation and intensity es-
timator can be formulated as

min
B∈B

min

w

min
Φl,Φu

Lu(Φl, Φu, B, Df , Ds) + Ll(w, Φl, Df )

+ λ2RI (w, Φu, Ds) + λ3RF (Φu, Ds)
s.t. ΓΦuw ≤ 0, Φlw ≥ 0, Φuw ≥ 0, (9)

where λ0, λ1, λ2 and λ3 are hyperparameters. The ﬁrst term
is the reconstruction error of all samples. The second is the
loss of labeled samples. The third is the regularization of
temporal label smoothness. The fourth is the regularization
of temporal feature smoothness. The constraints represent
the temporal label ranking and positive intensity.

Intensities of few frames provide strong supervision
while domain knowledge provides weak supervision. A-
mong types of knowledge, intensity and feature smooth-
ness encourage smooth predictions. Label ranking encour-
ages predictions in training segments to satisfy ordinal con-
straints, and positive intensity ensures the nonnegative pre-
diction. We jointly learn the subspace Φ and regressor w,
which are coupled through the knowledge. The soft and
hard constraints involve both Φ and w. During optimiza-
tion, the constraints and regularizations cooperate with each
other to ﬁnd the optimal solution of Φ and w.

3.6. Alternating optimization

Problem (9) is not jointly convex in all variables, but it
is convex in each of them. Since Eq.( 7) contains only Φu,
we can not just optimize ΦB and Φw by treating them as
new variables. We propose an algorithm to solve the prob-
lem based on ADMM [3]. The scaled form of augmented
Lagrangian function is

Lρ(Φ, B, w, C·, Λ·, Z·, V·)

(10)

=

+

2

2

F

1

ρ1

ρ1

Cu(cid:21)T
(cid:20) Cl
Φu(cid:21) B(cid:13)(cid:13)(cid:13)
Su(cid:21) −(cid:20) Φl
(cid:20)Xl
(cid:13)(cid:13)(cid:13)2,1
2(cid:13)(cid:13)(cid:13)
+ λ1(cid:13)(cid:13)(cid:13)
(cid:20) Λl
(cid:20) Φl
Φu(cid:21) −(cid:20) Cl
Λu(cid:21)(cid:13)(cid:13)(cid:13)
Cu(cid:21) +(cid:20) Λl
Λu(cid:21)(cid:13)(cid:13)(cid:13)
2 (cid:13)(cid:13)(cid:13)
2 (cid:13)(cid:13)(cid:13)
||V0||2
||ΓΦuw − Z0 + V0||2 −
ρ3
2
ρ3
2

||Φuw − Z2 + V2||2 −

||Φlw − Z1 + V1||2 −

ρ2
2
||V1||2

ρ2
2
ρ3
2
ρ3
2

||V2||2

−

F

F

2

+ I+(Z1) +

+ I+(Z2) +

+ I−(Z0) +

,

(8)

+

λ0
2

||Φlw − Yl||2 + λ2wT ΦuLT Φuw + λ3tr(ΦuLT Φu),

where Φ· = {Φl, Φu}, C· = {Cl, Cu}, Z· =
{Z1, Z2, Z3}, and V· = {V0, V1, V2}. C· and Z· are
introduced variables while Λ· and V· are the multipliers.
They are introduced to handle the L21 norm and inequality

3460

constraints. ρ = {ρ1, ρ2, ρ3} are penalty parameters to em-
phasize the importance of different knowledge. I−(·) and
I−(·) are projection functions, i.e., I−(·) = min(·, 0) and
I+(·) = max(·, 0). We optimize each variable alternatively
as follows (Algo. 1). PCA [37] is used to initialize B, Φl,
and Φu. Cl = Φl and Cu = Φu while other variables
are randomly initialized. Note that currently updated vari-
able will be used to update other variables. Following the
conventional procedures of ADMM [3], the updates of the
above variables are as follows:

B(k+1) ← arg min

B

Lρ(· · · ),

Φ(k+1)

l

← arg min
Φl

Lρ(· · · ),

Φ(k+1)

u ← arg min
Φu
w(k+1) ← arg min

w

← arg min

C

·

← arg min

Z

·

Lρ(· · · ),

Lρ(· · · ),

Lρ(· · · ),

Lρ(· · · ),

C(k+1)

·

Z(k+1)

·

Λ(k+1)

·

V(k+1)

0

V(k+1)

1

V(k+1)

2

·

← Λ(k)
← V(k)
← V(k)
← V(k)

· + Φ(k+1)
0 + ΓΦ(k+1)
1 + Φ(k+1)
2 + Φ(k+1)

u

u

l

·

,

− C(k+1)
w(k+1) − Z(k+1)

0

w(k+1) − Z(k+1)
w(k+1) − Z(k+1)

1

,

.

2

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

,

Algorithm 1 Joint Representation and Estimator Learning.

Input: Labeled frames Df and weakly labeled sequences

Ds. Penalty parameters {λi}3

i=0 and {ρi}3

i=1.

Output: Representation Φl and Φu, basis vectors B, and

the estimator w.

1: Initi: use PCA to obtain B, Φl, and Φu. Cl = Φl and

Cu = Φu. Randomly initialize Z·, Λ·, and V·.

2: while not converging do
3:

Update variables by solving Problem (11) ∼ (16)
Update Lagrangian multipliers by Eqs. (17) ∼ (20)

4:
5: end while
6: return Φl , Φu, B, and w.

Let C = [Cl; Cu], Φ = [Φl; Φu], and Λ = [Λl; Λu]. The
problem can be decomposed into small problems, i.e.,

C·i = arg min

C

·i

λ1||C·i||2 +

ρ1
2

||Φ·i − C·i + Λ·i||2
F ,

where C·i is the i-th column of C, Φ·i is the i-th column of
Φ, and Λ·i is the i-th column of Λ. The solution is

C·i = Sλ1/ρ1 (Φ·i + Λ·i),

(23)

where Sk(a) = [1 − k
max(·, 0). ⊙ represents pairwise product.

||a||2

]+ ⊙ a and Sk(0) = 0. [·]+ =

For Problem (16), the solutions for Z0, Z1 and Z2 are

For Problem (11), we ﬁrstly obtain the closed-form so-

lution of ˜B by taking the gradient and setting it to 0, i.e.,

˜B = [ΦT
l

Φl + ΦT
u

Φu]−1[ΦT
l

Xl + ΦT
u

Xu].

(21)

Z0 = min{0, ΓΦuw + V0},
Z1 = max{0, Φlw + V1},
Z2 = max{0, Φuw + V2}.

(24)

(25)

(26)

We project ˜B into B = {b : ||b||2 = 1} by normalizing
, where ˜Bi· is the i-th row
each row of ˜B, i.e., Bi· =
of ˜B.

|| ˜Bi·||2

˜Bi·

For Problem (12) and (14), we can get the closed-form
solutions for Φl and w by computing the gradient and set-
ting the gradient to 0.

For Problem (13), though we can get the closed-form
solution by taking the gradient of Φu, the computation is
inefﬁcient since it involves the inverse of a large matrix.
Instead we use a gradient-based method to update Φu, i.e.,

Φu ← Φu − α∇u,

(22)

where ∇u is the gradient of Φu and the step size α is ob-
tained by exact line search. Detailed representation of α is
in the supplementary material.

For Problem (15), the subproblem with respect to Cl

and Cu is

min
Cl,Cu

λ1(cid:13)(cid:13)(cid:13)

Cu(cid:21)T
(cid:20) Cl

+

(cid:13)(cid:13)(cid:13)2,1

ρ1

2 (cid:13)(cid:13)(cid:13)

(cid:20) Φl
Φu(cid:21) −(cid:20) Cl

Cu(cid:21) +(cid:20) Λl
Λu(cid:21)(cid:13)(cid:13)(cid:13)

2

F

.

The optimization details of each subproblem are present-

ed in the supplementary material.

For testing. We estimate intensities of testing samples in a
transductive manner. Let Xt denote testing samples and Φt
denote their coefﬁcients. We jointly learn the model and es-
timate intensities of testing samples by simply augmenting
Su and Φu, i.e., S = [Su; Xt] and Φ = [Φu; Φt]. L =
diag(L1, ..., LM , Lt) and Γ = diag(Γ1, ..., ΓM , Γt), where
Lt and Γt are matrices with all elements being 0’s because
we have no information about testing samples and knowl-
edge is only applied to training segments. During testing,
we perform frame-level prediction by using Yt = Φtw.

4. Experiments

4.1. Settings

Data. BP4D-spontaneous database [39] was used as
the Train/Development splits of the FERA 2015 Chal-
lenge [32]. AU intensity is qualiﬁed into 6 discrete level-
s. Following the protocol of FERA 2015, we use the Train
split for training and the Development split for evaluation.

3461

700

600

500

400

300

primal objective
dual objective

4

2

0

0

5

10

15

20

25

0

20

GT

40

Prediction

60

80

4

2

0

4

2

0

4

2

0

0

20

40

60

80

100

0

20

40

60

80

100

0

20

40

60

80

100

Figure 2: The learning curve of KJRE on AU12 under the scenario that 6% of training samples are annotated. The last four show the
intensity prediction on a testing sequence at different iterations.

Note that FERA 2017 [33] is a challenge for AU intensi-
ty estimation under different poses. Since our goal is to
learn an estimator with limited annotations, we use FERA
2015. DISFA [19] consists of 27 sequences from 27 sub-
jects. We perform 5-fold subject independent cross valida-
tion. For feature extraction, we follow the same procedures
in [32, 33, 42] to extract 218D features.

Annotation.

Instead of using the intensity label of
each frame, our method needs only weak annotation (see
Sec. 3.1), i.e., identifying locations of key frames and label-
ing the intensities of few frames. We follow [17] to identify
key frames. Knowledge can be applied to all training seg-
ments even though no frame has intensity annotation. Since
sequences in both databases are captured with a high frame
rate, faces in consecutive frames have minor changes. Se-
quences are downsampled by selecting one frame every ﬁve
frames. Segment length varies between 10 and 80. Distri-
butions of AU intensity are shown in Fig. 3 . For evaluation,
we vary the proportion of labeled frames in the training set,
including 2%, 4%, 6%, 8%, 10%, 30%, 50%, 70%, 90%,
and 100%. Labeled frames are randomly selected and key
frames have the priority to be selected. We perform each
experiment 5 times and report the average performance.

i=0 and {ρi}3

Evaluation metrics. Pearson Correlation Coefﬁcien-
t (PCC), Intra-Class Correlation (ICC(3,1) [28]), and Mean
Absolute Error (MAE) are adopted as the measures for eval-
uation. K, {λi}3
i=1 are the hyperparameters of
our model. For parameter selection, the training set is di-
vided into two parts with 60% segments for training and
40% for validation. We use grid search strategy to ﬁnd the
best hyperparameters from K ∈ {60, 80, 100, 120, 140},
{λi}3

i=1 ∈ {10−1, 10−2, 10−3, 10−4}.

i=0, {ρi}3

Models. We incorporate four types of human knowl-
edge to jointly learn representation and estimator (KJRE).
To verify the effectiveness of each type of knowledge, we
compare the performance of not using knowledge (JRE)
with the performance of using only one type of knowl-
edge, including label ranking (KJRE-O), label smoothness
(KJRE-I), feature smoothness (KJRE-F), and positive inten-
sity (KJRE-P). KE-PCA ﬁrst uses PCA to get the represen-
tation and then uses knowledge for estimator learning. We
then compare with the state-of-the-art supervised methods
(SVR [32], RVR [10], SOVRIM [5], LT [11], COR [35],
DSRVM [12]), semi-supervised methods (CSL [40], HSS-
R [13]), and weakly supervised methods (OSVR [43],

0

0
1
2
3
4
5

0

1

2

4

5

6

9

0
1
2
3
4
5

Figure 3: Intensity distribution.

BORMIR [42] ). Supervised methods use only labeled
samples while weakly and semi-supervised methods use
both labeled and unlabeled samples. For weakly supervised
methods, OSVR, BORMIR, and our method require prepro-
cessing by splitting sequences into segments. We also com-
pare to supervised deep models (CCNN [34], 2CD [31]) and
a weakly supervised deep model (KBSS [41]).

Complexity and convergence. The computational com-
plexity is O(d3 + T (K 3 + (2N + d)K 2 + N Kd)). The
space complexity is O(N d + d2). d denotes the dimen-
sion of input space. T denotes the iterations of ADMM.
When d and K are large, the complexity can be reduced
to O(d2.373 + T (K 2.373 + (2N + d)K 2 + N Kd)) by us-
ing [6] to compute matrix inversion. Fig. 2 illustrates the
learning curve of AU12 with KJRE and also the prediction
on a testing sequence at different iterations. The primal ob-
jective decreases while the dual objective increases. When
they get close, the algorithm converges. Our method has the
same complexity as BORMIR [42] (O(N d2T )) and is more
efﬁcient than OSVR [43] (O(N 2dT )) when N ≫ d.

4.2. Results

Comparison with baseline methods. The results are
shown in Table 1. Methods are valuated under the scenari-
o that 6% of training frames have intensity labels. Each
method achieves better performance on FERA than on D-
ISFA because DISFA is a more challenge database due to
the low-quality images, large head poses, complex illumina-
tions, and imbalanced intensity distribution. Detailed anal-
yses are as follows. Firstly, methods that use one type of hu-
man knowledge, including KJRE-O, KJRE-I, KJRE-F, and
KJRE-P, achieve better results than JRE which does not use
any type of knowledge. It demonstrates the effectiveness of
each type of knowledge. Label ranking and label smooth-
ness are relatively more important than feature smoothness
and positive intensity. Secondly, KJRE combines all type-
s of knowledge and achieves better performance than JRE
as well as methods that use partial knowledge. It further

3462

Table 1: Comparison with baseline methods. The performance is evaluated under the scenario that 6% of training frames are annotated.
Bold numbers with gray background indicate the best performance; bold numbers indicate the second best.

Database

AU

KE-PCA

JRE

KJRE-O
KJRE-I
KJRE-F
KJRE-P
KJRE

KE-PCA

JRE

KJRE-O
KJRE-I
KJRE-F
KJRE-P
KJRE

KE-PCA

JRE

KJRE-O
KJRE-I
KJRE-F
KJRE-P
KJRE

C
C
P

C
C

I

E
A
M

FERA 2015
12
14

.64
.81
.85
.83
.82
.82
.87

.63
.81
.85
.83
.81
.82
.87

.18
.30
.39
.36
.32
.33
.40

.15
.30
.37
.35
.32
.32
.39

17

.45
.36
.37
.36
.37
.35
.43

.39
.36
.36
.36
.37
.34
.42

10

.49
.56
.60
.60
.57
.60
.65

.45
.56
.60
.59
.57
.58
.61

6

.63
.66
.70
.68
.67
.69
.72

.61
.65
.70
.68
.66
.69
.71

1.56
1.07
.91
.98
1.09
.99
.82

2.02
1.09
1.00
1.00
1.06
1.01
.95

2.37
.87
.71
.78
.85
.78
.64

1.78
1.23
1.10
1.14
1.21
1.15
1.08

1.09
1.04
.92
.97
1.04
.94
.85

Avg

.48
.54
.58
.57
.55
.56
.62

.45
.54
.58
.56
.55
.55
.60

1.76
1.06
.93
.97
1.05
.98
.87

1

.18
.07
.14
.15
.11
.21
.28

.05
.07
.14
.15
.11
.20
.27

2

.29
.26
.30
.30
.30
.38
.38

.07
.22
.27
.29
.28
.33
.35

4

.15
.25
.31
.27
.26
.28
.26

.03
.17
.26
.27
.23
.26
.25

5

.34
.29
.39
.36
.35
.28
.34

.13
.22
.37
.35
.33
.26
.33

6

.25
.34
.43
.43
.39
.52
.54

.06
.31
.42
.41
.37
.48
.51

DISFA

12

.54
.44
.62
.60
.49
.59
.69

.22
.33
.61
.60
.41
.57
.67

15

.10
.05
.12
.10
.07
.04
.18

.03
.06
.11
.09
.08
.04
.14

9

.32
.29
.35
.34
.31
.33
.33

.09
.28
.34
.31
.29
.30
.31

17

.15
.17
.22
.20
.19
.10
.18

.04
.13
.21
.20
.18
.11
.17

20

.30
.17
.19
.21
.19
.20
.22

.07
.16
.18
.18
.18
.18
.20

25

.47
.58
.70
.71
.64
.69
.75

.17
.52
.70
.70
.61
.68
.74

26

.27
.22
.28
.29
.25
.25
.25

.06
.19
.27
.28
.23
.24
.25

.81
1.75
1.38
1.08
1.43
1.21
1.02

.64
1.59
1.31
.96
1.21
.99
.92

1.51
2.97
2.28
1.90
2.36
1.86
1.86

.41
1.15
.74
.63
.81
.90
.70

1.03
1.38
.96
.90
1.16
.89
.79

.61
1.39
1.09
.97
1.25
1.10
.87

1.21
2.05
1.01
.95
1.68
1.13
.77

.43
1.00
.71
.66
.85
.71
.60

.77
1.33
.92
.86
1.03
1.06
.80

.44
1.00
.90
.65
.79
.83
.72

1.92
1.49
.97
.91
1.22
1.07
.96

.94
1.57
1.28
1.03
1.38
1.11
.94

Avg

.28
.26
.34
.33
.30
.32
.37

.08
.22
.32
.32
.27
.30
.35

.89
1.56
1.13
.96
1.26
1.07
.91

Figure 4: Comparison with the state-of-the-art methods under different annotation ratios.
different annotation ratios. ‘The percentage of annotated frames’ refers to #quantitatively annotated frames
RVR [10], SOVRIM [5], LT [11], COR [35], DSRVM [12], CSL [40], HSSR [13], OSVR [43], and BORMIR [42].

It presents the average performance under
. Competitive methods are SVR [32],

#all frames

demonstrates human knowledge helps improve both rep-
resentation and estimator learning. Thirdly, our method
achieves much better results than KE-PCA which uses PCA
to learn representation in an unsupervised manner and then
incorporates knowledge for estimator learning. On DISFA,
KE-PCA achieves slightly better MAE than our method, but
it gets much worse PCC and ICC. Since intensity levels are
imbalanced and the majority intensity is 0 in DISFA, the
representation learning will be dominated by samples with
the intensity of 0. It makes KE-PCA tend to predict the in-
tensity of 0 for all testing samples. As the majority intensity
is 0, KE-PCA can get good MAE, but poor performance in
ICC. The comparison to KE-PCA shows that our method is
more robust on the learning of representation and estimator

when the database is imbalanced.

Comparison with the state-of-the-art methods. Fig. 4
presents the average performance of methods under differ-
ent annotation ratios. Table 2 shows the results when the
annotation ratios is 6%. The state-of-the-art methods are e-
valuated by using the code provided by the authors. Note
that BORMIR [42] can not use segments that have no in-
tensity annotations of peak and valley frames. It can use
at most about 10% labeled frames. As shown in Fig. 4
and Table 2, on FERA 2015, our method achieves better
performance than other methods under all evaluation met-
rics, especially when the annotation ratio is small such as
2% ∼ 10%. On DISFA, our method achieves better per-
formance than other methods under PCC and ICC. Its MAE

3463

2%4%6%8%10%30%50%70%90%100%0.350.400.450.500.550.60PCCFERA 2015SVRSOVRIMRVRLTCORDSRVMHSSRCSLOSVRBORMIRKJRE2%4%6%8%10%30%50%70%90%100%0.10.20.30.40.50.6ICCFERA 20152%4%6%8%10%30%50%70%90%100%0.81.01.21.41.61.82.0MAEFERA 20152%4%6%8%10%30%50%70%90%100%The percentage of annotated frames in the training set0.050.100.150.200.250.300.350.40PCCDISFA2%4%6%8%10%30%50%70%90%100%The percentage of annotated frames in the training set0.000.050.100.150.200.250.300.35ICCDISFA2%4%6%8%10%30%50%70%90%100%The percentage of annotated frames in the training set1.01.52.02.53.03.5MAEDISFATable 2: Comparison with the state-of-the-art methods. The performance is evaluated under the scenario that 6% of training frames are
annotated. Bold numbers with gray background indicate the best performance; bold numbers indicate the second best.

Database

AU

SVR [32]

SOVRIM [5]

RVR [10]
LT [11]

COR [35]

DSRVM [12]

HSSR [13]
CSL [40]
OSVR [43]
BORMIR [42]

KJRE

SVR [32]

SOVRIM [5]

RVR [10]
LT [11]

COR [35]

DSRVM [12]

HSSR [13]
CSL [40]
OSVR [43]
BORMIR [42]

KJRE

SVR [32]

SOVRIM [5]

RVR [10]
LT [11]

COR [35]

DSRVM [12]

HSSR [13]
CSL [40]
OSVR [43]
BORMIR [42]

KJRE

6

.45
.50
.67
.59
.49
.63
.53
.66
.63
.71
.72

.40
.47
.65
.58
.48
.60
.28
.65
.63
.71
.71

10

.42
.43
.60
.61
.54
.64
.55
.52
.54
.63
.65

.41
.43
.59
.58
.54
.62
.24
.49
.53
.62
.61

1.71
1.41
1.16
.94
1.21
.89
1.16
.97
.99
.83
.82

1.44
1.46
1.13
.99
1.15
.96
1.24
1.37
1.13
.99
.95

FERA 2015
12
14

.74
.76
.82
.71
.69
.81
.63
.76
.84
.82
.87

.73
.76
.82
.69
.69
.80
.28
.76
.84
.82
.87

1.06
.97
.84
.91
.88
.78
1.25
1.22
.77
.92
.64

.25
.24
.27
.35
.16
.36
.26
.28
.35
.32
.40

.23
.23
.27
.32
.16
.31
.09
.27
.35
.31
.39

1.76
1.54
1.35
1.09
1.63
1.09
1.21
1.32
1.15
1.14
1.08

C
C
P

C
C

I

E
A
M

17

.28
.30
.41
.09
.18
.22
.20
.42
.37
.42
.43

.26
.29
.41
.08
.18
.16
.07
.42
.36
.42
.42

1.38
1.21
1.03
1.00
.92
.87
.92
.94
1.06
0.90
.85

Avg

.43
.45
.55
.47
.42
.53
.43
.53
.55
.58
.62

.41
.44
.55
.45
.41
.50
.19
.52
.54
.58
.60

1.47
1.32
1.10
.99
1.16
.92
1.16
1.16
1.02
0.96
.87

1

.06
.06
.25
.22
.18
.12
.05
.16
.16
.24
.28

.04
.05
.24
.18
.14
.02
.00
.15
.16
.19
.27

2.42
2.43
1.22
1.00
1.16
.89
.92
1.20
1.44
.90
1.02

2

.30
.29
.35
.07
.18
.02
.01
.27
.23
.36
.38

.24
.22
.31
.05
.12
.00
-.02
.26
.23
.27
.35

1.94
1.91
1.09
.93
1.24
.82
.84
1.16
1.14
.77
.92

4

.21
.24
.04
.04
.29
.12
-.02
.28
.20
0.37
.26

.16
.19
.04
.01
.24
.01
-.03
.27
.17
.33
.25

3.41
3.52
2.66
1.43
1.18
1.31
1.30
1.40
3.09
1.48
1.86

5

.29
.27
.31
.24
.04
.06
.04
.23
.12
.21
.34

.18
.16
.29
.19
.01
.00
.03
.22
.11
.14
.33

2.07
2.31
.88
.61
2.88
.64
.44
1.11
.90
.58
.70

6

.33
.36
.40
.42
.32
.37
.11
.34
.29
.47
.54

.27
.30
.35
.39
.29
.07
.04
.31
.26
.41
.51

1.83
1.90
1.03
.80
.98
.88
.92
.98
1.61
.78
.79

DISFA

12

.44
.41
.61
.45
.39
.40
.16
.37
.47
.60
.69

.32
.28
.59
.42
.38
.08
.11
.36
.37
.58
.67

2.31
2.60
1.31
.91
1.11
.98
1.08
1.03
1.94
0.91
.77

9

.15
.10
.27
.13
.24
.20
.14
.22
.29
.24
.33

.12
.08
.23
.09
.19
.04
.06
.21
.28
.18
.31

2.28
2.22
1.20
.84
.91
.78
.66
1.19
.94
.78
.87

15

.15
.23
.11
.04
.09
.07
.00
.18
.14
.31
.18

.11
.16
.08
.02
.05
.02
-.01
.18
.14
.16
.14

1.54
1.60
.72
.56
.91
.56
.53
.98
.78
0.56
.60

17

.01
.06
.22
.13
.12
.07
.00
.06
.07
.20
.18

.00
.04
.18
.10
.10
.02
.01
.05
.05
.16
.17

2.45
2.43
1.22
.73
.68
.77
.73
1.17
1.75
.75
.80

20

.16
.16
.06
.00
.01
-.04
.09
.15
.14
.20
.22

.11
.10
.05
.00
.01
-.02
.05
.15
.14
.09
.20

1.76
1.80
.79
.67
1.15
.58
.59
1.05
.85
0.64
.72

25

.54
.57
.82
.52
.66
.52
.25
.53
.59
.70
.75

.47
.49
.81
.51
.65
.16
.14
.49
.56
.69
.74

26

.14
.14
.27
.28
.13
.06
.10
.19
.17
.13
.25

.07
.07
.27
.21
.11
.00
.08
.18
.12
.13
.25

1.95
1.86
.78
1.13
.77
1.27
1.31
1.08
1.36
.95
.96

2.47
2.85
1.49
.85
.80
.82
.80
1.12
1.78
.98
.94

Avg

.23
.24
.31
.21
.22
.16
.08
.25
.24
.34
.37

.17
.18
.29
.18
.19
.03
.04
.24
.22
.28
.35

2.20
2.29
1.20
.87
1.15
.86
.84
1.12
1.46
.84
.91

Table 3: Comparison with deep models

FERA 2015

DISFA

PCC

ICC

MAE

PCC

ICC

MAE

# annotated frames: more than 75,000

-
-

.50
.58

.60
.62

.63
.63

.63
.66

1.26

-

-
-

# annotated frames: 120

.49
.57

1.17
.90

.23
.29

# annotated frames: 360

.58
.60

.94
.87

.31
.37

# annotated frames: 1000

.63
.61

.88
.85

.37
.38

.38
.50

.21
.28

.30
.35

.35
.36

.66

-

.68
1.10

.62
.91

.46
.82

Database
Method

CCNN [34]*
2DC [31]*

KBSS [41]

KJRE

KBSS [41]

KJRE

KBSS [41]

KJRE

is slightly worse than HSSR, LT, DSRVM, and BORMIR
when annotation ratio is less than 10%. HSSR, LT, and D-
SRVM have good performance in MAE because they are
sensitive to the imbalanced AU intensity distribution during
learning and tend to predict intensity level 0 for all testing
samples. Since the majority intensity is 0, this makes them
have good MAE but poor ICC and MAE.

Table 3 shows the comparison with deep models when
using limited annotated frames. The average performance is
presented. Results of CCNN and 2DC are adapted from the

corresponding papers. On FERA 2015, our method outper-
forms KBSS when using only 120 or 360 annotated frames.
When using 1000 frames, our method is comparative to KB-
SS. On DISFA, our method achieves better PCC and ICC.
The results show that our method is applicable to databas-
es with scarce annotated frames, even with few hundreds of
annotated frames where our method can outperform KBSS.

5. Conclusion

We propose a general framework for AU intensity es-
timation which jointly learns representation and estimator
with limited annotations. Besides, it can ﬂexibly incorpo-
rate various types of human knowledge. Human knowledge
is used to provided weak supervision for the joint learning
and to efﬁciently exploit unlabeled images. We also propose
an algorithm for optimization based on the framework of
ADMM. Evaluations on two benchmark databases demon-
strate the effectiveness of the proposed method, especially
when the ratio of intensity annotations is small.

Acknowledgments: This work is partially supported by the Na-
tional Key R&D Program of China (Grant No. 2018YFC0807500)
and by NSFC Nos. 61832016 and 61720106006. Qiang Ji’s in-
volvement in this work is supported in part by the US National
Science Foundation award CNS No. 1629856.

3464

References

[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regular-
ization: A geometric framework for learning from labeled
and unlabeled examples. JMLR, 2006. 2

[2] B. Bentsianov and A. Blitzer. Facial anatomy. Clinics in

dermatology, 2004.

[3] S. Boyd. Alternating direction method of multipliers. 4, 5

[4] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by

low density separation. In AISTATS, 2005. 2

[5] W. Chu and S. S. Keerthi. New approaches to support vector

ordinal regression. In ICML, 2005. 6, 7, 8

[6] D. Coppersmith and S. Winograd. Matrix multiplication via
arithmetic progressions.
In Proceedings of the nineteenth
annual ACM symposium on Theory of computing, pages 1–
6. ACM, 1987. 6

[7] P. Ekman and W. V. Friesen. Manual for the facial action

coding system. Consulting Psychologists Press, 1978. 1

[8] A. Gudi, H. E. Tasli, T. M. den Uyl, and A. Maroulis. Deep
learning based facs action unit occurrence and intensity esti-
mation. In FG workshop, 2015. 1, 2

[9] R. Herbrich, T. Graepel, and K. Obermayer. Large margin

rank boundaries for ordinal regression. 2000. 2

[10] S. Kaltwang, O. Rudovic, and M. Pantic. Continuous pain
intensity estimation from facial expressions. In ISVC, 2012.
1, 2, 6, 7, 8

[11] S. Kaltwang, S. Todorovic, and M. Pantic. Latent trees for
estimating intensity of facial action units. In CVPR, 2015. 2,
6, 7, 8

[12] S. Kaltwang, S. Todorovic, and M. Pantic. Doubly sparse
relevance vector machine for continuous facial behavior es-
timation. TPAMI, 2016. 2, 6, 7, 8

[13] K. I. Kim, F. Steinke, and M. Hein. Semi-supervised re-
gression using hessian energy with an application to semi-
supervised dimensionality reduction. In NIPS, 2009. 2, 6, 7,
8

[14] Y. Li, S. M. Mavadati, M. H. Mahoor, Y. Zhao, and Q. Ji.
Measuring the intensity of spontaneous facial action units
with dynamic bayesian network. PR, 2015. 2

[15] W. Liu, J. Wang, and S.-F. Chang. Robust and scalable
graph-based semisupervised learning. Proceedings of the
IEEE, 2012. 2

[21] Z. Ming, A. Bugeau, J.-L. Rouas, and T. Shochi. Facial ac-
tion units intensity estimation by the fusion of features with
multi-kernel support vector machine. In FG Workshop, vol-
ume 6, pages 1–6. IEEE, 2015. 2

[22] J. Nicolle, K. Bailly, and M. Chetouani. Facial action unit
intensity prediction via hard multi-task metric learning for
kernel regression.
In FG Workshop, volume 6, pages 1–6.
IEEE, 2015. 2

[23] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: A strategy employed by v1? Vision re-
search, 37(23):3311–3325, 1997. 4

[24] O. Rudovic, V. Pavlovic, and M. Pantic. Context-sensitive
dynamic ordinal regression for intensity estimation of facial
action units. TPAMI, 2015. 2

[25] A. Ruiz, J. Van de Weijer, and X. Binefa. Regularized multi-
concept mil for weakly-supervised facial behavior catego-
rization. In BMVC, 2014. 2

[26] O. R. Ruiz, Adria, X. binefa, and M. Pantic. Multi-instance
dynamic ordinal random ﬁelds for weakly-supervised facial
behavior analysis. arXiv preprint arXiv:1803.00907. 1, 2

[27] G. Sandbach, S. Zafeiriou, and M. Pantic. Markov random
ﬁeld structures for facial action unit intensity estimation. In
ICCVW, 2013. 2

[28] P. E. Shrout and J. L. Fleiss. Intraclass correlations: uses in

assessing rater reliability. Psychological bulletin, 1979. 6

[29] K. Sikka, A. Dhall, and M. Bartlett. Weakly supervised pain
localization using multiple instance learning. In FG work-
shop, 2013. 2

[30] K. Sikka, G. Sharma, and M. Bartlett. Lomo: Latent ordinal

model for facial analysis in videos. In CVPR, 2016. 2

[31] D. L. Tran, R. Walecki, S. Eleftheriadis, B. Schuller, M. Pan-
tic, et al. Deepcoder: Semi-parametric variational autoen-
coders for facial action unit intensity estimation. In ICCV,
2017. 2, 6, 8

[32] M. F. Valstar, T. Almaev, J. M. Girard, G. McKeown,
M. Mehu, L. Yin, M. Pantic, and J. F. Cohn. Fera 2015-
second facial expression recognition and analysis challenge.
In FG workshop, 2015. 2, 5, 6, 7, 8

[33] M. F. Valstar, E. S´anchez-Lozano, J. F. Cohn, L. A. Jeni,
J. M. Girard, Z. Zhang, L. Yin, and M. Pantic. Fera 2017-
addressing head pose in the third facial expression recogni-
tion and analysis challenge. In FG, 2017. 6

[16] M. H. Mahoor, S. Cadavid, D. S. Messinger, and J. F. Cohn.
A framework for automated measurement of the intensity of
non-posed facial action units. In CVPRW, 2009. 1, 2

[34] R. Walecki, V. Pavlovic, B. Schuller, M. Pantic, et al. Deep
structured learning for facial action unit intensity estimation.
In CVPR, 2017. 2, 6, 8

[17] M. Mavadati, P. Sanger, and M. H. Mahoor. Extended disfa
dataset: Investigating posed and spontaneous facial expres-
sions. In CVPRW, 2016. 2, 6

[35] R. Walecki, O. Rudovic, M. Pantic, and V. Pavlovic. Copula
ordinal regression for joint estimation of facial action unit
intensity. In CVPR, 2016. 2, 6, 7, 8

[18] S. M. Mavadati and M. H. Mahoor. Temporal facial expres-
sion modeling for automated action unit intensity measure-
ment. In ICPR, 2014. 2

[36] S. Wang, J. Yang, Z. Gao, and Q. Ji. Feature and label re-
lation modeling for multiple-facial action unit classiﬁcation
and intensity estimation. PR, 2017. 2

[19] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and J. F.
Cohn. Disfa: A spontaneous facial action intensity database.
IEEE Transactions on Affective Computing, 2013. 6

[37] S. Wold, K. Esbensen, and P. Geladi. Principal component
analysis. Chemometrics and intelligent laboratory systems,
2(1-3):37–52, 1987. 5

[20] S. Melacci and M. Belkin. Laplacian support vector ma-

[38] Z. Zafar and N. A. Khan. Pain intensity evaluation through

chines trained in the primal. JMLR, 2011. 2

facial action units. In ICPR, 2014. 2

3465

[39] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale,
A. Horowitz, P. Liu, and J. M. Girard. Bp4d-spontaneous:
a high-resolution spontaneous 3d dynamic facial expression
database. IVC, 2014. 5

[40] X. Zhang, Y. Yu, M. White, R. Huang, and D. Schuur-
mans. Convex sparse coding, subspace learning, and semi-
supervised extensions. In AAAI, 2011. 2, 3, 6, 7, 8

[41] Y. Zhang, W. Dong, B.-G. Hu, and Q. Ji. Weakly-supervised
deep convolutional neural network learning for facial action
unit intensity estimation. In CVPR, 2018. 1, 2, 6, 8

[42] Y. Zhang, R. Zhao, W. Dong, B.-G. Hu, and Q. Ji. Bilateral
ordinal relevance multi-instance regression for facial action
unit intensity estimation. In CVPR, 2018. 1, 2, 3, 6, 7, 8

[43] R. Zhao, Q. Gan, S. Wang, and Q. Ji. Facial expression in-
tensity estimation using ordinal information. In CVPR, 2016.
1, 2, 3, 6, 7, 8

3466

