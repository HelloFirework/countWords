IRLAS: Inverse Reinforcement Learning for Architecture Search

Minghao Guo

SenseTime Research

Zhao Zhong

NLPR, CASIA

guominghao@sensetime.com

University of Chinese Academy of Sciences

zhao.zhong@nlpr.ia.ac.cn

Wei Wu

Dahua Lin

Junjie Yan

SenseTime Research

The Chinese University of Hong Kong

SenseTime Research

wuwei@sensetime.com

dhlin@ie.cuhk.edu.hk

yanjunjie@sensetime.com

Abstract

In this paper, we propose an inverse reinforcement learn-
ing method for architecture search (IRLAS), which trains an
agent to learn to search network structures that are topolog-
ically inspired by human-designed network. Most existing
architecture search approaches totally neglect the topolog-
ical characteristics of architectures, which results in com-
plicated architecture with a high inference latency. Moti-
vated by the fact that human-designed networks are elegant
in topology with a fast inference speed, we propose a mir-
ror stimuli function inspired by biological cognition theory
to extract the abstract topological knowledge of an expert
human-design network (ResNet). To avoid raising a too
strong prior over the search space, we introduce inverse
reinforcement learning to train the mirror stimuli func-
tion and exploit it as a heuristic guidance for architecture
search, easily generalized to different architecture search
algorithms. On CIFAR-10, the best architecture searched
by our proposed IRLAS achieves 2.60% error rate. For Im-
ageNet mobile setting, our model achieves a state-of-the-art
top-1 accuracy 75.28%, while being 2∼4× faster than most
auto-generated architectures. A fast version of this model
achieves 10% faster than MobileNetV2, while maintaining
a higher accuracy.

1. Introduction

The past several years have witnessed the remark-
able success of convolutional neural networks in com-
puter vision applications. Thanks to the advances in net-
work architectures, e.g. ResNet [11], Inception [28] and
DenseNet [15], the performances on a number of key tasks,
such as image classiﬁcation, object detection, and semantic
segmentation, have been taken to an amazing level. How-
ever, every step along the way of network design improve-

Figure 1. Topologies of different architectures. Human-designed
architectures have a more simple and elegant topology than ex-
isting auto-generated architectures. Our IRLAS aims to search
topologically elegant architectures guided by human-designed net-
works. (a) ResNeXt [32]; (b) NASNet [37]; (c) Best performed
architecture found by our IRLAS.

ment requires extensive efforts from experienced experts
and takes a long period of time. This already constitutes
a signiﬁcant obstacle to further progress.

Naturally, automatically ﬁnding suitable network archi-
tectures for a given task becomes an alternative option and
is gaining ground in recent years. Along this direction,
a number of network search methods have been devel-
oped, including evolution [26, 31], surrogate model based
search [18, 21], and reinforcement learning [36, 37, 34, 4].
Whereas these methods have shown promising results and
found new architectures that surpass those crafted by ex-
perts, they are still subject to a serious limitation – auto-
generated networks usually have a rather high inference la-
tency, making them difﬁcult to be deployed on practical sys-
tem with limited computational capabilities. An important
cause to this issue is that auto-generated structures are often
excessively complicated, which, as observed in [20], tends
to adversely inﬂuence the run-time efﬁciency. While there
have been attempts [29] to incorporate latency information

19021

to guide the search, the problem has not been effectively
solved – the search algorithms themselves still follow a pre-
deﬁned way for network motif construction, e.g. recursively
expanding a tree structure as in NASNet [37], without en-
forcing any explicit guidance to the network topology.

In this work, we aim to explore a new approach that ex-
plicitly takes the topological structure into account. Our ef-
forts are motivated by the observation that human-designed
networks are usually topologically simple, as shown in Fig-
ure 1, especially when compared to auto-generated ones,
and often stride a better balance between accuracy and efﬁ-
ciency. These designs are often grounded on the rich expe-
riences obtained through many years of joint efforts by the
community, which are valuable resources and deserve to be
leveraged during the searching process.

Speciﬁcally, we propose an inverse reinforcement learn-
ing method for architecture search (IRLAS). At the heart of
this method is a mirror stimuli function learned by inverse
reinforcement learning. This function is expected to reward
those architectures that are topologically similar to the net-
works designed by experts. During the searching process,
an agent resorts to this function to provide structural guid-
ance, so as to generate networks with desirable architec-
tures, similar to those crafted by experts. This method has
two beneﬁts: (1) While the search receives guidance from
the mirror stimuli function, it is not restricted. The agent
is allowed to explore instead of just copying the experts.
(2) The mirror stimuli function is generic and is orthogonal
to the design of search space and strategy. Hence, it can
be readily generalized to different search settings. On both
CIFAR-10 [16] and ImageNet [8], IRLAS is able to ﬁnd
new architectures that yield high accuracies while maintain-
ing low inference latency.

Our contributions are summarized as follows: 1) We pro-
pose a mirror stimuli function that can provide topological
guidance to architecture search, based on the knowledge
learned from the expert-designed networks. This function
can be easily generalized to different architecture search al-
gorithms. 2) We introduce inverse reinforcement learning
algorithm to train the mirror stimuli function, which helps
the agent to efﬁciently explore the large search space with-
out being overly restricted. 3) The network searched by our
IRLAS is topologically similar to the given expert network
and shows competitive accuracy and high inference speed,
compared to both state-of-the-art human-designed and auto-
searched networks. On CIFAR-10, the best architecture
searched by our proposed IRLAS achieves 2.60% error rate.
For ImageNet mobile setting, our model achieves a state-
of-the-art top-1 accuracy 75.28%, while being 2∼4× faster
than most auto-generated architectures. A fast version of
this model achieves 10% faster than MobileNetV2, while
maintaining a higher accuracy.

2. Related Work

2.1. Neural Architecture Search

Neural architecture search focuses on automatically
searching effective neural topologies in a given architec-
ture space. Existing architecture search methods can be
mainly classiﬁed into three categories: evolutionary, surro-
gate model based search and reinforcement learning. Evo-
lutionary methods [10, 31, 24] aim to simultaneously evolve
the topology of a neural network along with its weights and
hyperparameters to evolve a population of networks. Early
evolutionary approaches utilized genetic algorithms to op-
timize both the architecture and its weights, while recent
studies used gradient-based methods and evolutionary algo-
rithms to optimize the weights and architecture respectively.
Surrogate model based search methods [18, 5, 21] utilize
sequential model-based optimization as a technique for pa-
rameter optimization. Typical methods like PNAS [18] per-
formed a progressive scan of the neural architecture search
space, which was constrained according to the state-of-the-
art of previous iterations. EPNAS in [21] further increased
the search efﬁciency by sharing weights among sampled ar-
chitectures. However, these methods generate architectures
greedily by picking the top K at each iteration, which may
result in a sub-optimum over the search space.

Reinforcement learning (RL) methods [4, 36, 34, 37, 24]
formulate the generation of a neural architecture as an
agent’s action, whose space is identical to the architecture
search space. The agent’s reward is the performance of
the trained architecture on unseen data. Differences be-
tween different RL-based approaches lie in the representa-
tion of agent’s policy and how to optimize it. For exam-
ple, [36] used a recurrent neural network (RNN) to sam-
ple a sequence of string which encoded the neural archi-
tecture. Policy gradient algorithms including REINFORCE
and Proximal Policy Optimization (PPO) were used to train
the agent.
[4] and [34] used Q-learning to train a policy
that sequentially chose a layer’s type and its corresponding
hyperparameters. There are some other RL-based methods
that transform existing architectures incrementally to avoid
generating entire networks from scratch, such as [6]. How-
ever, these approaches could not visit the same architec-
ture twice so that strong generalization over the architec-
ture space was required from the policy. Instead of directly
using an existing architecture as an initialization, our IR-
LAS aims to learn a mirror stimuli function, and utilizes it
in the searching process as a heuristic guidance without any
restraints for the search space.

There also exists recent efforts [19] introducing a real-
valued architecture parameter, which was jointly trained
with weight parameters. Different from other methods, this
kind of algorithm does not involve architecture sampling
during searching process. Our mirror stimuli function can

29022

Figure 2. The pipeline of our IRLAS. We propose a mirror stimuli function to extract the abstract representation for topological charac-
teristic of the expert. Topology structures of networks are converted to state feature code as the input of mirror stimuli function. During
the agent’s searching process, the mirror stimuli function is utilized as a heuristic guidance to generate desirable human-designed-like
networks. Inverse reinforcement learning is utilized to train the mirror stimuli function, which helps the agent to efﬁciently explore the
large search space without being overly restricted.

also be generalized to this brunch of methods.

2.2. Imitation Learning

As our proposed IRLAS attempts to generate architec-
tures that are topologically similar to human-designed net-
works, the learning for the agent involves imitation learning
problem. Imitation Learning (IL) enables an agent to learn
from demonstrations of an expert, independent of any spe-
ciﬁc knowledge in the proposed task. There exists two dif-
ferent areas for IL: policy imitation and inverse reinforce-
ment learning. Policy imitation, which is also known as
behavioral cloning, targets directly learning the policy map-
ping from perceived environment or preprocessed features
to the agent’s actions. For the settings of this paper, since
the number of human-designed networks is limited, it is
hard to obtain sufﬁcient number of expert’s state-action tu-
ples for supervised learning. As a result, the direct policy
imitation cannot be used for our purpose.

Inverse reinforcement learning (IRL) refers to the prob-
lem of deriving a reward function from observed behavior.
As it is a common presupposition that reward function is a
succinct, robust and transferable deﬁnition of a task, IRL
provides a more effective form of IL than policy imitation.
Early studies in IRL [3, 35, 23] assumed that the expert was
trying to optimize an unknown reward function that could
be expressed as a linear combination of pre-determined fea-
tures. [7] extended this approach to a limited set of non-
linear rewards and learned to build composites of logical
conjunctions for atomic features. Other ﬂexible non-linear
function approximators such as Gaussian Processes further
extended the modeling capacity of IRL models [17]. In this
paper, we assume the reward function of the expert network

as a linear parametrization of state features. Experiments
show that this simple assumption is effective enough to ex-
tract the topological knowledge of the human-designed ar-
chitectures.

3. Approach

In this section, we ﬁrst present the problem formulation
of architecture search. Then we propose the mirror stim-
uli function inspired by biological cognition and its training
procedure via inverse reinforcement learning. Finally we
detail the search space and the searching algorithm. The
pipeline of our IRLAS is shown in Figure 2.

3.1. Problem Formulation

Like modern CNNs, our automatic neural network pro-
cess designs the topological structure of block instead of
the entire network. This block-wise design is more ﬂexible
for different datasets and tasks with powerful generalization
ability. The task of the agent is to sequentially sample layers
from the pool of layer candidates to form the block. Then
the block structure is stacked sequentially to form the com-
plete network. For different datasets, we manually choose
different number of down-sampling operations due to dif-
ferent input image size and choose different repeat times of
the block to meet the demand for limitation of parameters
or FLOPs.

In this paper, we consider the design process of net-
work topology as a variable-length decision sequence for
the choice of operation. And this sequential process can be
formulated as a Markov decision process (MDP). The pol-
icy π : S → A, where S is the state space and A is the
action space, determines the agent’s behavioral preference

39023

of generating architectures. The state s ∈ S is the status of
current layer. The action a ∈ A is the decision for the sub-
sequent layer. Thus, an architecture m sampled by the agent
can be determined by a state-action trajectory according to
the policy π, i.e. m = {(st, at)}t=1...T .

The training of the agent is to maximize the expected

reward over all possible architectures,

Jπ = Eπ[R(m)],

(1)

where R(·) is the reward function. A common deﬁnition of
R(m) is the validation accuracy of the corresponding net-
work. This formulation of the reward function is based on
an assumption that the evaluation for an architecture is only
determined by its validation performance, while totally ne-
glect the topology information.

where γ denotes a discounted scalar. Thus, the sequential
order is also included by the discounted γ over layer index.
The feature count is utilized as an appropriate encoding for
the topological knowledge of a given network.

As for the other question of how the agent uses the
topological knowledge as a guidance, this encompasses the
classical exploration-exploitation trade-off. We attempt the
agent to search architectures that are topologically similar
to the expert network, while efﬁciently explore the archi-
tecture search space. This requires the searching algorithm
exhibiting no preferences on a speciﬁc architecture as we do
not aim the agent to reproduce human-designed networks.
Direct policy imitating between the feature counts of sam-
pled architecture and expert network will raise a strong prior
on the search space and force the agent to ‘mimic’ the ex-
pert [3, 2], which does not meet our expectation.

3.2. Topological Knowledge

3.3. Mirror Stimuli Function

As the human-designed architectures are demonstrated
to be effective in practice, we attempt to utilize such existing
abundant topological knowledge as efﬁcacious guidance for
architecture search. However, it is a challenging problem to
ﬁnd an effective method to formalize the abstract topologi-
cal knowledge and design an appropriate way to further ex-
ploit it in the search process. For example, shortcut connec-
tion of the block in ResNet is a quotable structure for archi-
tecture generating. Human can easily understand the topo-
logical structure simply by visualization, while the agent
cannot. It remains harder for the agent to learn to search
ResNet-like architectures if it even cannot understand the
topology. This naturally raises two basic problems: 1) How
to encode network architecture to extract the abstract topo-
logical knowledge as an available input for the agent? 2)
How to utilize this knowledge to guide the agent to design
desirable architectures?

For the ﬁrst problem, we need to deﬁne a feature embed-
ding for network architectures. To encode the architecture,
we carefully choose a state feature function φ : S → Rk×1,
which consists of: operation type, kernel size, and the
indexes of two predecessor of the current layer (for layer
with only one predecessor, one of the indexes is set to zero).
Despite the simplicity, this state feature function provides a
complete characterization of the network architecture, in-
cluding the information about the computation carried out
by individual layers as well as how the layers are connected.
We further exploit feature count to unify the information
of each state feature to get the feature embedding for the
whole architecture. Given an architecture’s sequential tra-
jectory m = {(st, at)}t=1...T , the feature count is deﬁned
as:

µ =

T

X

t=1

γtφ(st),

(2)

To address this problem, we design a mirror stim-
uli function, denoting as Ftopology, which aims to sof tly
guide the agent while preventing a hard and strong constrain
on the search space. The design of the mirror stimuli func-
tion is inspired by the mirror neuron system in primate’s
premotor cortex. This system is responsible for the linkage
of self-generated and observed demonstrations. The mirror
neuron ﬁres both when an animal acts and when the animal
observes the same action performed by another, which is an
important scheme for learning new skills by imitation. In
our problem, the mirror stimuli function has a similar func-
tionality as the mirror neuron. Given the architecture sam-
pled by the agent as the self-generated demonstration, the
expert network as the observed demonstration, our mirror
stimuli function will output a signal to judge the topological
similarity between these two networks. The higher output
represents higher similarity, where the highest for the exact
expert network.

The mirror stimuli function is deﬁned as a linear function

of feature count:

Ftopology(m) = wT · µ,

(3)

where w ∈ Rk×1. Such a linear parametric form is easy to
optimize, while effective enough to use as the evaluation of
topology, as further shown in our experiment.

By substituting Equation 2 to Equation 3, we can get

Ftopology(m) =

T

X

t=1

γt · wT · φ(st).

(4)

Thus, the problem of solving the parameter w could be re-
garded as the problem of ﬁnding a time-step reward func-
tion r(st) = wT · φ(st), whose corresponding policy has a
maximum value at the sequence of expert network (i.e., the
value of Ftopology(m∗), m∗ = {(s∗
t )}t=1...T denotes

t , a∗

49024

Algorithm 1 Max-Margin Optimization for Inverse Reinforce-
ment Learning

set i = 1, randomly pick policy ˆπ0, compute ˆM0;
repeat

Compute δ(i) in optimization problem of Equation 8 with
{ ˆM } = { ˆMj , j = 0...i − 1}, get w(i), δ(i);
Using standard RL algorithm, ﬁnd the optimal policy as ˆπi
with reward function r(i)(s) = (w(i))T · φ(s);
Compute ˆMi;
i = i + 1;
until δ(i) ≤ ǫ
return w;

the expert network). This refers to the standard inverse re-
inforcement learning problem.

To ﬁnd such an reward function, we use the feature
match algorithm proposed in [3]. For the expert network,
the architecture is generated following an expert policy π∗,
which has a maximum value for the following expression:

Jπ∗ = Eπ∗ [

T

X

t=1

γtr(st)] = wT · Eπ∗ [

T

X

t=1

γtφ(st)]

(5)

= wT · Eπ∗ [µ] = wT · Mπ∗ .

As we have one expert network, Mπ∗ is estimated as
Mπ∗ = Eπ∗ [µ] ≈ µ∗ = PT

t=1 γtφ(s∗

To get the weight parameter w of the unknown reward
function r(st), we need to ﬁnd a policy ˆπ whose perfor-
mance is close to that of the expert’s:

t ).

|Jˆπ − Jπ∗ | = |wT · Mˆπ − wT · Mπ∗ | ≤ ǫ.

(6)

This process could be regarded as ‘imitating’ the ob-
served behavior in the mirror neuron system, which makes
the self-generated demonstration (regarded as Jˆπ) similar
to the observed demonstration (regarded as Jπ∗ ). So the
problem is reduced to ﬁnding a policy ˆπ that induces the
expectation of feature count Mˆπ close to Mπ∗ . This fea-
ture matching problem could be solved by max-margin op-
timization, derived as,

where Faccuracy(m) denotes model m’s accuracy percent-
age on target task, λ denotes a balance scalar.

By optimizing this multi-objective search problem, the
agent is guided by both the topological similarity and the
accuracy. Thus, the agent can efﬁciently explore the search
space to generate high-speed, topologically elegant archi-
tectures along with high accuracy.

3.4. Search Space and Training Strategy

In this section we introduce the search space and train-
ing strategy of our IRLAS. We will further discuss the gen-
eralization of our mirror stimuli function to other typical
architecture search approaches in Section 3.5. In our IR-
LAS, the search space consists of operations based on their
prevalence in the CNN literature. The considered oper-
ations are: Depthwise convolution with kernel size 1×1,
3×3, 5×5; Max pooling with kernel size 3×3, 5×5; Aver-
age pooling with kernel size 3×3, 5×5; Identity; Elemental
add with two input layers; and Concat with two input lay-
ers. Note that the depthwise convolution operation refers
to pre-activation convolution containing ReLU, convolution
and batch normalization. All the layers without successor
in the searched block are concatenated together as the ﬁnal
output.

For the searching stage, we utilize Q-learning method to
train the agent to take actions that maximize the cumulative
reward, which is formulated as Equation 9. Q-learning it-
eratively updates the action-selection policy following the
Bellman Equation:

Q(st, at) = rt + γ max

a′

Q(st+1, a

′

),

(10)

where rt denotes the intermediate reward observed for the
current state st. Since rt could not be explicitly measured,
reward shaping method is used, derived as rt = R(m)/T ,
where T denotes the state length referring to the number
of layers. The Bellman Equation is achieved following
Temporal-Difference control algorithm:

max

w:kwk2≤1

min
∀ ˆµ

wT · Mπ∗ − wT · Mˆπ.

(7)

Q(st, at) =(1 − η)Q(st, at)

Thus the weight parameter w is optimized following:

max
δ,w

s.t.

δ

wT · Mπ∗ ≥ wT · Mˆπ + δ,
kwk2 ≤ 1.

∀ ˆπ

(8)

The detailed algorithm is illustrated in Algorithm 1.

During the agent’s training stage, we add the output of
mirror stimuli function as an additional reward term. The
complete reward function in Section 3.1 is calculated as:

R(m) = Faccuracy(m) + λFtopology(m),

(9)

+ η[rt+1 + γ max

a′

Q(st+1, a

′

)],

(11)

where η denotes the learning rate.

The whole learning procedure is summarized as follows:
The agent ﬁrst samples a network architecture, which is
taken as input of the mirror stimuli function. Then the gen-
erated network is trained on a certain task to get the valida-
tion accuracy. The reward, which is the combination of the
accuracy and the output value of the mirror stimuli function,
is used to update the Q-value. The above process circulates
for iterations and the agent learns to sample block structure
with higher accuracy and more elegant topology iteratively.

59025

3.5. Generalization of Mirror Stimuli Function

It is worthy to point out that our mirror stimuli function
can be easily generalized to different architecture search al-
gorithms. For algorithms that involve architecture sampling
and performance evaluation for the sampled architecture,
including reinforcement learning based methods and evolu-
tionary methods, we can simply utilize the output of Equa-
tion 9 as an alternative of evaluation, while the other search-
ing steps remain the same to the original algorithm. The
only difference lies in the expression of state feature func-
tion φ(s), which need to be modiﬁed due to different candi-
date operations in the search space of different algorithms.
Thus, the topological information is considered during the
searching process.

For differentiable architecture search algorithm, typi-
cally DARTS [19], the architecture is encoded by a set of
continuous variables α = {α{i,j}} ((i, j) denotes a pair of
nodes, i.e. a path in the architecture). Thus, the weight
parameters and architecture parameters could be trained
jointly via standard gradient descent. To introduce topolog-
ical information to the training procedure in differentiable
architecture search algorithms, we add an additional loss
term Ltopology calculated by mirror stimuli function to the
original cross entropy loss. To convert the continuous α
to discreted architectures, we consider the sof tmax out-
put of α as a probabilistic distribution of all possible ar-
chitectures, denoted as {pk}, and sample according to the
distribution to get state feature φ(s). Since the conversion
from architecture parameters α to state feature φ(s) is non-
differentiable, the output of mirror stimuli function cannot
be backpropagated. Here, we consider the solution based
on REINFORCE algorithm [30], so the loss term Ltopology
is calculated and updated as:

Ltopology =

K

X

k=1

pkFtopology(mk)

∇Ltopology ≈

1
K

K

X

k=1

Ftopology(mk)∇log(pk),

where K is the number of sampled architectures.

(12)

4. Experiments and Results

4.1. Implementation Details

In this section, we introduce the implementation details
of our IRLAS. We use a distributed asynchronous frame-
work as proposed in [34], which enables efﬁcient network
generation on multiple machines with multiple GPUs. With
this framework, our IRLAS can sample and train networks
in parallel to speed up the whole training process. For the
inverse reinforcement learning procedure, ResNet, whose
convolution operation is modiﬁed to depthwise convolution,

Table 1. IRLAS’s results compared with state-of-the-art methods
on CIFAR-10 dataset. “Error” is the top-1 misclassiﬁcation rate
on the CIFAR-10 test set, “Param” is the number of model param-
eters.

Method

Resnet [11]

Resnet (pre-activation) [12]

Wide ResNet [33]

DenseNet (k=12) [15]
DenseNet (k=12) [15]
DenseNet (k=24) [15]

DenseNet-BC (k=40) [15]

MetaQNN (top model) [4]

NAS v1 [36]

EAS [6]

Block-QNN-A, N=4 [34]
Block-QNN-S, N=2 [34]
NASNet-A (6 @ 768) [37]
NASNet-B (4 @ 1152) [37]
NASNet-C (4 @ 640) [37]

PNASNet-5 [18]

ENAS [22]

AmoebaNet-A [24]

DARTS [19]

IRLAS

IRLAS-differential

Param Error(%)

1.7M
10.2M
36.5M
1.0M
7.0M
27.2M
25.6M

11.2M
4.2M
23.4M

-

6.1M
3.3M
2.6M
3.1M
3.2M
4.6M
3.2M
3.4M

3.91M
3.43M

6.61
4.62
4.17
5.24
4.10
3.74
3.46

6.92
5.50
4.23
3.60
3.30
2.65
3.73
3.59
3.41
2.89
3.34
2.83

2.60
2.71

is chosen as the expert network to calculate the weight w in
the mirror stimuli function. The training procedure is about
3 hours on CPU.

For our IRLAS, we choose Q-value table as the agent.
We use Q-learning with epsilon-greedy and experience re-
play buffer. At each training iteration, the agent samples 64
structures with their corresponding rewards from the mem-
ory to update Q-values following Equation 11. For the hy-
perparameters of Q-learning process, the learning rate η is
set to 0.01, the discount factor γ is 0.9 and the balance scalar
λ is 30. The mini-batch size is set to 64 and the maximum
layer index for a block is set to 24. The agent is trained for
180 iterations, which totally samples 11,500 blocks. Each
sampled architecture is trained with ﬁxed 12 epochs with
Adam optimizer to get evaluation of Faccuracy.

We also generalize our mirror stimuli function to the dif-
ferent architecture search algorithm. We choose DARTS
[19] as the basic algorithm. The additional loss term
Ltopology is scaled by 0.5 and added to the original cross-
entropy loss. The number of the sampled architectures K
is set to 5. All the other training details and hyperparam-
eters follow the original paper. For both of the conditions,
the architecture searching processes are proposed on dataset
CIFAR-10 [16].

4.2. Results

Results on CIFAR-10 After the searching process, we se-
lect the searched optimal block structure and train the net-

69026

In this phase, the
work on CIFAR-10 until convergence.
training data is augmented with randomly cropping size of
32 × 32, horizontal ﬂipping and Cutout [9]. The cosine
learning rate scheme is utilized with the initial learning rate
of 0.2. The momentum rate is set to 0.9 and weight decay
is set to 0.0005. All the networks are trained for 600 epochs
with 256 batch size.

For the task of image classiﬁcation on CIFAR-10, we set
the total number of stacked blocks as 10. The results are re-
ported in Table 1 along with other models. We see that our
proposed IRLAS achieves a 2.60% test error, which shows a
state-of-the-art performance over both human-designed net-
works and auto-generated networks. For the differential set-
ting, the result is reported in Table 1 as IRLAS-differential.
Compared to the result reported in original paper (2.83% er-
ror rate), the searched architecture facilitated by our mirror
stimuli function achieves a higher accuracy.

Results on ImageNet For the ImageNet task, we trans-
fer the model searched on CIFAR-10 by increasing the total
number of stacked blocks and the ﬁlter channel size. We
consider the mobile setting to compare inference speed. The
training is conducted with a mini-batch size of 256 with in-
put image size 224 × 224. Randomly cropping and ﬂipping
are used to augment data. We choose SGD strategy for opti-
mization with cosine learning rate scheme. The accuracy on
test images is evaluated with center crop. We use the true in-
ference latency for fair comparison, which is validated for
16 batch size on TensorRT [1] framework with one Titan
Xp.

The results are illustrated in Table 2. Our IRLAS-mobile
achieves a state-of-the-art accuracy over both the human-
designed and auto-generated architectures. As for the infer-
ence latency, our IRLAS-mobile can achieve 2∼4× fewer
inference latency compared with most auto-generated ar-
chitectures beneﬁting from the elegant topology facilitated
by our mirror stimuli function. We also further squeeze
the number of stacked blocks of IRLAS-mobile and in-
crease conduct a IRLAS-mobile-fast model with a infer-
ence speed of 9ms, making our model even faster than
human-designed network MobileNetV2. Note that Mnas-
Net [29] was searched directly on ImageNet dataset and
need to validate time latency during searching, which is a
very resource-exhausted process due to the high training
cost on such a large scale dataset. As the shufﬂe operation,
channel split operation and inverted block backbone used
in ShufﬂeNetV2 and MobileNet-224 are not adopted in our
search space, we believe our inference speed can be further
boosted by introducing them to our searching process.

4.3. Analysis of Inverse Reinforcement Learning

In this section, we conduct an analysis of inverse rein-
forcement learning algorithm. As we introduce inverse re-

Table 2. ImageNet classiﬁcation results in the mobile setting. The
input image size is 224×224. The inference latency is validated
with 16 batch size on TensorRT framework.

Method

Latency Acc (%)

Inception V1 [27]

MobileNet-224 [14]

ShufﬂeNet [13]

MobileNetV2 1.4 [25]
ShufﬂeNetV2 2× [20]

NASNet-A(4 @ 1056) [37]

AmoebaNet-A [24]

PNASNet [18]
DARTS [19]
MnasNet [29]

IRLAS-mobile

IRLAS-mobile-fast

-

6ms
10ms
10ms
6ms

23ms
33ms
25ms
55ms
11ms

12ms
9ms

69.8
70.6
70.9
74.7
74.9

74.0
74.5
74.2
73.1
74.79

75.28
75.15

inforcement learning to avoid the agent to exhibt preference
on the expert network, we compare the output value changes
of our mirror stimuli function with those of the feature count
µ by modifying a speciﬁc architecture. Here we choose
the expert architecture ResNet, and modify it in three ways:
M odif y1, adding a conv3×3 operation before the residual
function; M odif y2, adding a conv3 × 3 operation after the
residual function; M odif y3, removing the short-cut con-
nection. The results are illustrated in Figure 6 (a). Since
M odif y1 and M odif y2 have a minor change in topology
than M odif y3, our mirror stimuli function is able to out-
put relative value change, where the feature count is very
sensitive to tiny changes. As a result, comparing to direct
feature count, our mirror stimuli function is a more reason-
able guidance to avoid the agent to just mimic the expert
network, which helps the agent to explore the search space
without being overly restricted.

4.4. Search Efﬁency

In this section, we perform an analysis on search
efﬁency. Note that the overall searching cost is largely de-
pends on the design of search strategy, which is orthogo-
nal to the design of our mirror stimuli function. To illus-
trate the efﬁciency improvement introduced by our mirror
stimuli function, we conducted two experiments based on
two search algorithms of different kinds: one is BlockQNN
[34], the other is DARTS [19]. For each experiment, the
baseline followed the searching process proposed in orig-
inal paper, compared with the searching facilitated by our
mirror stimuli function. We evaluate the efﬁency of by mir-
ror stimuli function by comparing the relative improvement
of convergence speed, instead of the absolute search time.
Convergence curves are reported in Figure 5. For both of the
conditions, our methods converge faster, beneﬁting from the
guidance provided by the expert network’s topology. The
results further demonstrate that our mirror stimuli function
is able to be generalized to different search algorithms and
improve the search efﬁency.

79027

Figure 3. Topologies of top-4 block architectures searched without mirror stimuli function.

Figure 4. Topologies of top-4 block architectures searched with mirror stimuli function.

Figure 5. Convergence curves for the searching processes compar-
ing with searching without mirror stimuli function.

4.5. Ablation Study

In this section, we perform analysis to illustrate how mir-
ror stimuli function affects the topology of ﬁnal searched
architecture. We ﬁrst illustrate topologies of top-4 block ar-
chitectures searched without and with mirror stimuli func-
tion in Figure 3 and Figure 4 (b). It is obvious that archi-
tectures searched without mirror stimuli function are com-
plicated, including numerous operations and connections,
while our searched models are much more simple and ele-
gant. Furthermore, our searched models are more topolog-
ically similar to ResNet, each containing a shortcut follow-
ing add operation to form the residual function.

We further conduct IRLAS with three different λ: 0, 30,
60. All three searching experiments followed the same pro-
cedure described in Section 3.4. For each experiment, top-4
models were chosen and transfered to meet the ImageNet
mobile setting, with about 5M parameters. These models
were then trained from scratch on ImageNet, following set-
tings in Section 4.2. The ﬁnal inference latency and accu-
racy of these models are illustrated in Figure 6. It can be no-

Figure 6. (a) Comparison the output value changes of mirror stim-
uli function and feature count for three modiﬁed models. (b) Re-
sults of inference latency and accuracy on ImageNet of 4 top mod-
els from each experiment with different λ.

ticed that the inference speed of searched architectures can
be drastically improved by utilizing mirror stimuli function,
about 1× faster. For λ = 60, the prior topological knowl-
edge of expert network is too strong for searching, which
results in accuracy drop. λ = 30 is regarded as a choice to
balance the trade-off between accuracy and speed.

5. Conclusion

In this paper, we have proposed an inverse reinforce-
ment learning method for architecture search. Based on the
knowledge learned from the expert-designed networks, our
mirror stimuli function can provide topological guidance to
architecture search, which can be easily generalized to dif-
ferent architecture search algorithms. Inverse reinforcement
learning method has been introduced to train this function.
Experiment results have shown that our IRLAS achieves to
search high-speed architectures with high accuracy. How
to extract representation of multiple networks to further im-
prove the performance will be our future work.

89028

References

[19] Hanxiao Liu, Karen Simonyan,

[1] https://developer.nvidia.com/tensorrt.
[2] Pieter Abbeel, Dmitri Dolgov, Andrew Y Ng, and Sebastian
Thrun. Apprenticeship learning for motion planning with
application to parking lot navigation. In IROS, 2008.

[3] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning

via inverse reinforcement learning. In ICML, 2004.

[4] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh
Raskar. Designing neural network architectures using rein-
forcement learning. arXiv preprint arXiv:1611.02167, 2016.
[5] Andrew Brock, Theodore Lim, James M Ritchie, and Nick
Weston. Smash: one-shot model architecture search through
hypernetworks. arXiv preprint arXiv:1708.05344, 2017.

[6] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun
Wang. Efﬁcient architecture search by network transforma-
tion. AAAI, 2018.

[7] Jaedeug Choi and Kee-Eung Kim. Bayesian nonparametric
In

feature construction for inverse reinforcement learning.
IJCAI, 2013.

[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009.

[9] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552, 2017.

[10] Tobias Domhan, Jost Tobias Springenberg, and Frank Hut-
ter. Speeding up automatic hyperparameter optimization of
deep neural networks by extrapolation of learning curves. In
IJCAI, volume 15, 2015.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In ECCV,

Identity mappings in deep residual networks.
2016.

[13] Michael G Hluchyj and Mark J Karol. Shufﬂe net: An appli-
cation of generalized perfect shufﬂes to multihop lightwave
networks. Journal of Lightwave Technology, 9(10):1386–
1397, 1991.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[15] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR, 2017.

[16] Alex Krizhevsky, Vinod Nair,

online: http://www. cs.

and Geoffrey Hinton.
toronto.

The cifar-10 dataset.
edu/kriz/cifar. html, 2014.

[17] Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlin-
ear inverse reinforcement learning with gaussian processes.
In NIPS, 2011.

[18] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon
Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan
Huang, and Kevin Murphy. Progressive neural architecture
search. In ECCV, 2018.

Darts: Differentiable architecture search.
arXiv:1806.09055, 2018.

and Yiming Yang.
arXiv preprint

[20] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn architec-
ture design. arXiv preprint arXiv:1807.11164, 2018.

[21] Juan-Manuel Perez-Rua, Moez Baccouche, and Stephane
Pateux. Efﬁcient progressive neural architecture search.
arXiv preprint arXiv:1808.00391, 2018.

[22] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and
Jeff Dean. Efﬁcient neural architecture search via parameter
sharing. arXiv preprint arXiv:1802.03268, 2018.

[23] Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinke-

vich. Maximum margin planning. In ICML, 2006.

[24] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classiﬁer architecture
search. arXiv preprint arXiv:1802.01548, 2018.

[25] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR, 2018.

[26] Shreyas Saxena and Jakob Verbeek. Convolutional neural

fabrics. In NIPS, 2016.

[27] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, 2015.

[28] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, 2016.

[29] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,
and Quoc V Le. Mnasnet: Platform-aware neural architec-
ture search for mobile. arXiv preprint arXiv:1807.11626,
2018.

[30] Ronald J Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.

[31] Lingxi Xie and Alan L Yuille. Genetic cnn. In ICCV, 2017.
[32] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR, 2017.

[33] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-

works. arXiv preprint arXiv:1605.07146, 2016.

[34] Zhao Zhong, Zichen Yang, Boyang Deng, Junjie Yan, Wei
Wu, Jing Shao, and Cheng-Lin Liu. Blockqnn: Efﬁcient
block-wise neural network architecture generation. arXiv
preprint arXiv:1808.05584, 2018.

[35] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and
Anind K Dey. Maximum entropy inverse reinforcement
learning. In AAAI, volume 8, 2008.

[36] Barret Zoph and Quoc V Le. Neural architecture search with
reinforcement learning. arXiv preprint arXiv:1611.01578,
2016.

[37] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
Le. Learning transferable architectures for scalable image
recognition. arXiv preprint arXiv:1707.07012, 2(6), 2017.

99029

