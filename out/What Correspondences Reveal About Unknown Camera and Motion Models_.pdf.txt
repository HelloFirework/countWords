What Correspondences Reveal about Unknown Camera and Motion Models?

Thomas Probst1, Ajad Chhatkuli1, Danda Pani Paudel1, Luc Van Gool1,2

1Computer Vision Laboratory, ETH Zurich, Switzerland

2VISICS, ESAT/PSI, KU Leuven, Belgium

Abstract

In two-view geometry, camera models and motion types
are used as key knowledge along with the image point cor-
respondences in order to solve several key problems of 3D
vision. Problems such as Structure-from-Motion (SfM) and
camera self-calibration are tackled under the assumptions
of a speciﬁc camera projection model and motion type.
However, these key assumptions may not be always justi-
ﬁed, i.e., we may often know neither the camera model nor
the motion type beforehand.
In that context, one can ex-
tract only the point correspondences between images. From
such correspondences, recovering two-view relationship –
expressed by the unknown camera model and motion type–
remains to be an unsolved problem. In this paper, we tackle
this problem in two steps. First, we propose a method that
computes the correct two-view relationship in the presence
of noise and outliers. Later, we study different possibili-
ties to disambiguate the obtained relationships into camera
model and motion type. By extensive experiments on both
synthetic and real data, we verify our theory and assump-
tions in practical settings.

1. Introduction

Structure-from-Motion (SfM) [14, 17, 21] and a vast ma-
jority of 3D vision applications rely on feature point corre-
spondences between images, while assuming a known cam-
era model. A key component of SfM and other 3D vision
methods is consensus maximization [10, 7, 28, 2, 13, 3, 15,
31] where the correct set of inliers is computed by searching
for speciﬁc relationships between image correspondences.
Such approaches have made possible the use of millions of
images for reconstructing 3D, as unreliable point correspon-
dences are weeded out through maximizing the consensus.
However, this is only true when one is certain about the
most suitable two-view relationship. In fact, methods devel-
oped to tackle many 3D vision problems can be used only

for certain camera and motion types. For example, current
state-of-the-art SfM methods assume that the camera obeys
a perspective projection model and that the camera motion
involves at least some translation [24, 21, 14]. Similarly, ex-
isting camera calibration methods (self-calibration or with
known patterns) assume a known camera model [30, 6], and
sometimes also a known motion type [12]. In such cases,
experiments using images from an afﬁne camera when the
method assumes a perspective model are doomed to fail. It
is therefore essential to know the camera model and the mo-
tion type beforehand, apart from the two-view relationship.
In fact, one requires to know the motion (if not its type) to
reason about the camera model, and vice versa. Therefore,
jointly recovering the camera model and motion becomes
very difﬁcult, leading to the causality dilemma.

In that context, the problem can be broken down into two
important sub-problems. The ﬁrst is that of choosing the
correct two-view relationship without knowing the camera
type and motion beforehand. Fitting an unknown model to
point correspondences is a challenging problem and is in
general NP hard. On the one hand, one can only hope to
know the correct two-view relationship after trying out all
possible models. This is exactly what is done in the context
of SfM for the Fundamental matrix and the 2D projective
homography as geometric veriﬁcation [27, 25, 16]. How-
ever, it is not always clear how one should select among
the several models for a given problem even after trying
out all of them. For example, we can always ﬁt a rela-
tionship less constrained than the actual model in order to
obtain a higher inlier count. Therefore, simply choosing
the model with the largest inlier set may lead to an unde-
sired outcome. Additionally, there exists a natural conﬂict
between the desired camera motion baseline and the match-
ing performance in real images [18, 1]. In such cases, one
can beneﬁt by constraining the motion to be sparse [23, 11].
However, it may not be known which motions are absent
beforehand. The second problem is that of obtaining the
camera model and the motion type from the correctly re-
covered two-view relation. Although many key results are

8730

already known [9, 17, 14, 26], we seek to answer and sum-
marize a different question, i.e., what is the correct camera
model and motion type given a two-view relationship?

In this paper, we provide contributions to the two prob-
lems discussed above: i) computing the two-view polyno-
mial relationship with point correspondences when the ex-
act type of relationship is not known and ii) disambiguating
the camera model and motion type from the two-view rela-
tionship. In order to tackle the ﬁrst problem, we present a
uniﬁed approach to ﬁtting polynomials to image point cor-
respondences despite the unknown model type and outliers.
To that end, we deﬁne the model search as that of ﬁnding
the sparsest set of polynomials which agree with more than
half the point correspondences up to some small ﬁxed error.
We constrain the polynomials by using a basis of mono-
mials where the solution is known to exist. This is done
using the so-called Vandermonde matrix [4, 5].
In doing
so we solve for both the model parameters as well as the
inlier correspondences similar to that of Random Sampling
and Consensus (RANSAC) [10]. Unlike in RANSAC, we
can recover multiple polynomials that describe a given set
of measurements (correspondences) by iteratively searching
for the orthogonal set of sparse bases of the monomial co-
efﬁcients. Additionally, we also encourage the motion to be
sparse, constraining the model search better for small base-
lines. Our approach only requires one to know beforehand
the maximum degree of the polynomial and not the actual
number of the polynomials. We express the problem as a
Mixed Integer Program (MIP) and solve it using the Branch
and Bound (BnB) approach. Our second contribution is the
analysis of the camera model and the motion type using the
computed two-view relationship. We consider each camera
and motion type and analyze the resulting two-view rela-
tionship. We provide conditions when such camera model
and motion recovery are ambiguous and why.

In order to quantify the model ﬁtting accuracy, we eval-
uate the proposed method on both synthetic and real data.
In the synthetic case, we simulate image point correspon-
dences with outliers for various camera models and mo-
tion types. On average our method performs better than
RANSAC for unknown model ﬁtting as well as inlier-outlier
classiﬁcation. We also show that our method performs simi-
larly to RANSAC with known camera and motion model on
the same tasks. We use the real data in order to show motion
disambiguation on driving sequences demonstrating the im-
portance of such disambiguation in a practical scenario.

2. Preliminaries

Notations. We denote matrices with upper case letters
and their elements with double-indexed lower case letters:
A = (aij). Similarly, we write vectors and index them as:
a = (ai). We use special uppercase Latin or uppercase
Greek letters for sets such as P. We use lowercase Latin

letters for scalars as in a. Finally, we use σi(.) for a func-
tion which gives the i-th largest singular value. We write
the ring of polynomials parameterized by variables x ∈ Rn
as R[x]. A polynomial p(x) ∈ R[x] is represented using
the basis of coefﬁcients B. We use vp to denote the ℓ−p
norm of any vector v.

2.1. Problem Formulation

We consider two cameras related by a motion. Let there
be m point correspondences {ui, vi}m
i=1 between the images
of the two cameras. Then we are interested to solve the
following problem.

Problem 2.1 What are the camera model M and mo-
tion parameters θ for the image point correspondences
{ui, vi}m

i=1?

This problem is NP-hard and difﬁcult to solve in its cur-
rent form. Therefore, we make the following assumption to
search the camera model and motion parameters.

Assumption 2.2 The optimal answer to Problem 2.1, i.e.,
(M, θ) respects the point correspondences {ui, vi}m
i=1 and
minimizes the joint degrees of freedom of M and θ.

We represent both M and θ using polynomials, as com-
monly done in the literature [9, 14, 22]. In this regard, we
express Problem 2.1 under the Assumption 2.2 as an alge-
braic problem of ﬁnding a low dimensional variety, whose
samples are the point correspondences.

i , v⊺

Consider the ring R[x] ∶= R[x1, ..., xn] of multivari-
ate polynomials and an algebraic variety V ⊆ Rn deﬁned
such that V ∶= {x ∈ Rn ∶ pj(x) = 0, for j = 1, . . . , r}. Let
ωi = (u⊺
i )⊺ be a measurement sample representing a
pair of corresponding points. For a given set of samples
Ω = {ωi}m
i=1, we wish to ﬁnd the variety V of the low-
est dimension. There is an extensive literature on comput-
ing an intrinsic dimension of the samples Ω from a variety
V [4, 5]. However, the existing methods do not explicitly
consider the noisy and outlier samples present in Ω. There-
fore, they are not suitable for our task.
In this work, we
develop a tractable method for estimating the variety given
the sample measurements which may contain noise and out-
liers. Primarily, we are interested in recovering polynomials
pj(x) representing V, from a corrupted sample set Ω. The
topological space deﬁned by V can be thought of as a semi-
algebraic set, a differential manifold, a metric space, a Lie
group, a category, a hypergraph, and many more.

Let I(V)∶ = {∑j gj(x)pj(x) ∶ gj(x) ∈ R[x]} be the ideal
of V. Every polynomial in the ideal I(V) of the unknown
variety V vanishes on samples Ω. Unfortunately, the con-
verse is not true, i.e., not all polynomials in the ideal I(Ω)
vanish on the variety V. Therefore, recovering pj(x) to de-
ﬁne V exactly is not only NP-hard, but also a non-decidable

8731

problem. In this work, we limit the degree of the polynomi-
als in I(Ω) and assume that V can be recovered from I(Ω)
of low degree polynomials. Note that the ideal I(Ω) of the
ﬁnite set Ω can be computed using linear algebra, with the
help of the so-called Vandermonde Matrix [4, 5].

Deﬁnition 2.3 (Vandermonde Matrix) The Vandermonde
matrix Md(x) is a matrix with a geometric progression of
monomials in each row, such that the entries mij are the
monomials xe = xe1

n of degree at most d.

2 . . . xen

1 xe2

For example, if n = 1, d = 3, and Ω = {u, v, w} then M3(Ω)
is the Vandermonde matrix of the form,

M3(Ω) =

⎡⎢⎢⎢⎢⎢

u2
v2

⎤
u3
u 1
⎥
⎥
v3
1
⎥
v
⎥
⎥
w3 w2 w 1
⎦
⎣

.

When n ≥ 2, Md(Ω) is a multivariate Vandermonde matrix
with the following property:

Property 2.4 Let B be a set representing a linearly inde-
pendent basis of R[x] and RB be the vector space spanned
by B . Then, the nullspace of Md(Ω) is the vector space of
I(Ω) ∩ RB.

We hope to learn the variety V by learning the ideal I(V).
The ideal I(V) is learned for samples Ω in the form of
I(Ω), using the Vandermonde Matrix Md(Ω). In this pro-
cess, we rely on the property of basis B. The two desirable
properties of B are:

Property 2.5 The ideal I(V) is generated by I(V) ∩ RB.

Property 2.6 I(V)∩RB ⊆ I(Ω)∩RB holds with equality.

There is a fundamental tension between Properties 2.5
and 2.6. For small B, Property 2.5 may not be satisﬁed.
Similarly, Property 2.6 may be violated for large B. Fortu-
nately, the following theorem ensures the existence of B.

Theorem 2.7 (Hilbert’s Basis Theorem) Every ideal
the polynomial ring R[x] is ﬁnitely generated.

in

The desired property 2.6 sets a lower bound on the sample
size m. In fact, by construction m is the upper bound on the
rank of Md(Ω). This implies the following lemma.

Lemma 2.8 If Property 2.6 holds true,
m ≥ dim(B) − dim(I(V) ∩ RB) must also be true.

the inequality

For given sample set Ω, the upper bound m on the rank is
ﬁxed. Therefore, one of the issues is choosing a suitable set
B. It is known that a suitable choice of B can dramatically
improve the numerical accuracy. We will discuss our choice
of B, for our applications, later in this paper.

Another pending issue is the representation of I(Ω) ∩
RB. It is obvious from Property 2.4 that I(Ω) ∩ RB is rep-
resented by the nullspace of Md(Ω). However, it is still
unclear how to choose and compute the basis representing
the null space. For example, we can obtain the orthonor-
mal basis of I(Ω) ∩ RB in the least-square sense by Singu-
lar Value Decomposition (SVD) of Md(Ω). Unfortunately,
such basis are not favored as under noise this may result in
a less sparse basis. Furthermore the samples Ω may con-
tain outliers, in which case the orthonormal basis given by
the SVD of Md(Ω) will be entirely wrong. In many appli-
cations, it is desirable to compute ideals I(V) with sparse
generators. In particular, our Assumption 2.2 implicitly de-
mands the basis to be sparse. Therefore, we wish to solve
the following problem for the sparse basis Y of I(Ω) ∩ RB.

Problem 2.9 Given the noise tolerance ǫ, ﬁnd the or-
thonormal sparse basis Y from the nullspace of Md(Ω) by
solving,

argmin

Y

subject to

yi0,

Q
yi∈Y
Md(Ω)yi∞ ≤ ǫ,
yi ~= 0, y⊺

i yj = 0, ∀i, j, i ~= j.

(1)

(1) involves ℓ0 minimization, which is the holy grail of
sparse approximation. Unfortunately, ℓ0 minimization is
NP-hard. Additionally, the non-linear objective for orthog-
onality and the search for non-trivial solutions make the
problem even more difﬁcult.

3. Sparse Basis Estimation

In this section, we develop a method to search for the
polynomial constraint y ∈ Y as a solution to the Prob-
lem 2.9. Our approach iteratively estimates the individual
sparse orthonormal basis by solving the following Problem.

Problem 3.1 For a given sparse basis w ∈ W ⊆ Y, estimate
a new sparse basis by solving,

argmin

y

subject to

y0,

Md(Ω)y∞ ≤ ǫ,
y∞ = 1, y⊺w = 0, ∀w ∈ W.

(2)

A common approximation of (2) is to replace ℓ0 by a con-
vex ℓ1 objective. Here, we are rather interested to solve the
exact problem of (2) using MIP.

3.1. Mixed Integer Programming (MIP)

Proposition 3.2 If z ∈ {0, 1}n represents the sparsity of the
basis vector y, Problem 3.1 is then equivalent to solving the

8732

following MIP.

min

y∈Rn,z∈{0,1}n Q
subject to

zi,

i
Md(Ω)y∞ ≤ ǫ,
 yi  ≤ zi, y⊺w = 0, ∀i, ∀w ∈ W,
y∞ = 1,
zi ≥ 1.
Q
i

Proof Proof is provided in the supplementary document.

Although equivalent to (2), (3) is tractable and can be solved
by BnB. Here we avoid the trivial zero solution by con-
straining the sum over the components of z to be greater
than 1. However, (3) only optimizes for the sparse poly-
nomial basis and fails if the measurements contain outliers.
We therefore propose the following to handle outliers.

3.2. Sparse Basis in the Presence of Outliers

Proposition 3.3 For M = Md(Ω), z ∈ {0, 1}n, s ∈ {0, 1}m,
and y ∈ Rn, the following MIP ensures that at least half of
the correspondences respect the sparse basis obtained by
solving,

min
y,z,s

n
Q
i=1
subject to m⊺
j y ≤ ǫ + sjm,

zi,

 yi  ≤ zi, y⊺w = 0,
y∞ = 1,
Q
i

zi ≥ 1, Q
j

sj ≤ m~2.

∀j = 1, . . . , m,
∀i, ∀w ∈ W,

(4)

Proof Proof is provided in the supplementary document.

In (4), we introduce the binary variable s ∈ {0, 1}m to clas-
sify the polynomial from a correspondence pair (uj, vj) as
an outlier if sj = 1 or inlier if sj = 0. We express the bi-
nary constraint for every measurement row mj of the Van-
dermonde matrix M using the big-M formulation [20]. In
order to make the problem tractable, we assume that at least
half of the points are inliers. However, in practice, the con-
straint may be adjusted to suit the outlier statistics. In the
following section, we discuss our basis selection method in
the context of the two-view camera geometry problem.

4. Two-view Geometry Applications

The sparse basis computation discussed in section 3 al-
lows one to compute the polynomials that relate the image
point correspondences accurately in the presence of out-
liers. The basis computation directly gives the correct two-
view relation, whether they are the Essential matrix, the
Fundamental matrix or the 2D projective homography. On

the other hand, such two-view relations may or may not say
anything about the camera and motion types, and ﬁnally the
actual camera motion knowing the camera type. We now
discuss problem 2.1 of obtaining the camera model M and
the motion parameters θ from the point correspondences.
For that purpose, we consider various camera projection and
camera motion types and analyze each condition further.

(3)

4.1. Projection and Motion Types

Considering camera projections, we analyze ﬁve differ-
ent camera models:
i) calibrated perspective, ii) uncali-
brated perspective, iii) orthographic, iv) weak-perspective
and v) afﬁne camera. For each camera model we divide the
motion into seven types: i) full motion with rotation and
translation, ii) rotation, iii) translation, iv) rotation about x
or y, v) rotation about z, vi) translation about x or y and vii)
translation about z. Given only images, the orthographic
camera and the weak-perspective camera projections differ
only by a single scale factor. Therefore, we treat them as
equivalent cameras in this analysis and use the term ortho-
graphic camera to discuss both types. Below we ﬁrst deﬁne
the transformations and the relevant two-view relationships
before presenting the analysis.

Transformations and model. We consider the camera
being transformed by a rotation R ∈ SO3 and translation
t ∈ R3. Let the rotation in Euler angles be r ∈ R3. We
consider the camera translation t, also represented as a
transformation matrix T ∈ T where T is the space of all
translations. The Essential matrix [17] for the calibrated
perspective camera is E ∈ P ⊂ R3×3. Let P represent
the space of matrices that satisfy the property of having
two equal non-zero singular values σ1(E) = σ2(E) and
σ3(E) = 0. In the orthographic camera, the Essential matrix
is EO ∈ O ⊂ R3×3. It has the following properties [14].

⎡
⎢
⎢
⎢
⎢
⎢
⎣

0
0
a b

0
c
0 d
e

⎤
⎥
⎥
⎥
⎥
⎥
⎦

EO =

,

a2 + b2 − c2 − d2 = 0,

a, b, c, d, e ∈ R.

(5)
The next two-view relation model is the Fundamental
matrix [9], F ∈ U ⊂ R3×3. We use U for the space of the
Fundamental matrices obtained from uncalibrated perspec-
tive cameras. The perspective fundamental matrix has two
non-zero singular values and the third singular value 0. Un-
like the Essential matrix from calibrated perspective cam-
eras, the two non-zero singular values of the Fundamental
matrix are in general not equal. In case of afﬁne cameras,
the Fundamental matrix is FA ∈ A ⊂ R3×3. FA has the
following property:

⎡
⎢
⎢
⎢
⎢
⎢
⎣

0
0
a b

0
c
0 d
e

⎤
⎥
⎥
⎥
⎥
⎥
⎦

,

a, b, c, d, e ∈ R.

(6)

FA =

8733

Table 1. Summary of the sparse basis for various camera and
motion types. The sparse basis is summarized as follows. The
two-view relationship is G, number/dimension of basis r, actual
degrees of freedom under known camera and motion type d and
the number of non-zero two-view model parameters p.

Cal. Perspective

Uncal. Perspective

G(r, d, p)

Orthographic
G(r, d, p)

Afﬁne

G(r, d, p)

Full motion

Rotation

Translation

Rotation x

Rotation y

Rotation z

G(r, d, p)

E(1, 5, 9)

H(3, 3, 9)

E(1, 3, 6)

H(3, 1, 5)

H(3, 1, 5)

H(3, 1, 5)

F(1, 7, 9)

EO(1, 4, 5)

FA(1, 5, 5)

H(3, 8, 9)

EO(1, 3, 4)

FA(1, 4, 4)

F(1, 3, 6)

H(1, 2, 5)

H(1, 2, 5)

H(3, 6, 7)

EO(1, 1, 2)

FA(1, 4, 4)

H(3, 6, 9)

EO(1, 1, 2)

FA(1, 4, 4)

H(3, 5, 7)

H(3, 1, 5)

H(3, 5, 7)

Translation x/y

E(1, 1, 2)

F(1, 3, 6)

EO(3, 1, 4)

H(3, 2, 5)

Translation z

E(1, 1, 2)

F(1, 3, 6)

EO(3, 0, 3)

H(3, 0, 3)

From the relations, we have, P ⊂ U and O ⊂ A, while none
of the sets P, U , O and A are disjoint.

The projective 2d homography H ∈ PGL(2, R) is a full
rank transformation unlike the Essential and Fundamental
matrices. The homography is a point to point relation and
the space of homographies PGL(2, R) is disjoint to the four
spaces of the Essential and Fundamental matrices. In spe-
cial cases, the homography can be represented by an afﬁne
transform A ∈ Aff(2, R), a rotation R ∈ SO3, a translation
T ∈ T or an identity I. All of the two-view relationships
are described by models which are homogeneous quantities
and therefore they are equivalent at different scales.

Sparsity of two-view relationship for different condi-
tions. We summarize the sparsity of various two-view re-
lationships for different camera models and motion types in
Table 1. In each case, the point correspondences {ui, vi}m
i=1
are related either by the Fundamental matrix, the Essential
matrix or the 2d projective homography. The properties of
these two-view relations vary according to both the camera
types and the motion types. Despite that and the varying
number of model parameters, all these relations can be ex-
pressed as polynomials of degree 2 in the image point cor-
respondences. Therefore the corresponding Vandermonde
matrix used in (4) is M2(Ω) of degree 2.

One interesting problem exists in the combinations of
camera and motion, where the approach of using RANSAC
with 8 points results in an incorrect model. This happens
when the image point correspondences are related by a ho-
mography rather than the Fundamental matrix or the Es-
sential matrix. Solving problem (4) with the Vandermonde
matrix M2(Ω) results in the correct relationship between
the image point correspondences in either case. When the
correspondences are related by a homography, the iterative
application of problem (4) will ﬁnd three independent bases
corresponding to either of the following system of equa-

tions.

[u⊺
i

1] × H[v⊺
i

1]⊺ = 0

or

[v⊺
i

1]⊺ × H−1[u⊺
i

1]⊺ = 0 (7)

We recover the homography H by using a change of basis
for the resulting system of equations so that the sparsity of
the ﬁnal system corresponds to the left equation of eq. (7).

4.2. Camera and Motion Type Recovery

Recovering the camera model M and the motion param-
eters θ from the computed two-view model is not trivial
and in fact, as we show in this paper, in most cases it is
not possible with only point correspondences. Knowing the
camera and motion type is crucial in many 3D vision prob-
lems [17, 14, 6]. We theoretically analyze the types of the
two-view models and justify when the camera type and the
motion type can be disambiguated and when the ambigui-
ties lie otherwise. We provide the summary of the proper-
ties of each two-view relationship for various camera model
and motion types in Table 2. We discuss the ambiguities for
each camera model below.

Calibrated perspective. The calibrated perspective cam-
era images are either related by the Essential matrix E ∈ P,
when there exists non-zero translation, or the homography
H ∈ PGL(2, R), when there is no translation between two
cameras. For a purely rotating camera, the induced rela-
tionship is the homography [12], which is in fact the cor-
responding relative rotation R ∈ SO(3). There are two
important cases when the camera model cannot be disam-
biguated from the model. The ﬁrst is that of pure transla-
tion, in which case, the essential matrix E = R[t]× is a skew-
symmetric matrix, similar as in the case of an uncalibrated
camera. A congruent transformation K−⊺EK−1 of a skew-
symmetric matrix E results in a skew symmetric matrix with
exactly two equal non-zero singular values. Therefore, the
calibration cannot be veriﬁed from the image point corre-
spondences of purely translating cameras. The second case
when the camera model cannot be ascertained is when there
is a pure rotation around the Z-axis. It is straight-forward to
verify that an orthographic camera gives the same rotational
homography in such a case. The same is true for an afﬁne
camera without the skew component. The disambiguation
of motion is also not possible for pure translation without
assuming a calibrated perspective camera beforehand. For
example, the essential matrix for an orthographic camera ro-
tating about X or Y axis is the same as the essential matrix
for the calibrated perspective camera with pure translation
on X or Y axis, respectively. The motion computation, as-
suming a calibrated perspective camera is always possible
even when the relationship is a homography, where we do
not have the usual 4-fold ambiguity of the planar homogra-
phy decomposition due to camera translation.

8734

Table 2. Properties of the two view relationship for each camera and motion type. Each two-view relationship is followed by three
boxes showing unique disambiguation of the camera type, the motion type and the metric motion given known camera type, resp. A check
mark indicates uniqueness and a cross mark indicates ambiguity.

Cal. Perspective

Uncal. Perspective

Orthographic

Afﬁne

Full motion

σ1(E) = σ2(E)

✓ ✓ ✓

F ∈ U

✓ ✓ ✖

EO ∈ O

✓ ✓ ✖

FA ∈ A

Rotation

Translation

H = R
E = [t]× = −E⊺

Rotation x

H = Rx

Rotation y

H = Ry

Rotation z

Translation x/y

Translation z

H = Rz
E = [tx~y]× = −E⊺
E = [tz]× = −E⊺

✓ ✓ ✓

✖ ✖ ✓

✓ ✓ ✓

✓ ✓ ✓

✖ ✓ ✓

✖ ✖ ✓

✓ ✓ ✓

HH⊺ ≠ I, H = KRK−1
F = K[t]×K⊺ = −F⊺
HH⊺ ≠ I, H = KRxK−1
H2,1 = H3,1 = 0

✓ ✓ ✓

EO ∈ O, EO 3,3 = 0

✖ ✖ ✖

FA ∈ A, FA 3,3 = 0

✖ ✖ ✖

H ∈ T

✖ ✓ ✓

H ∈ T

✓ ✓ ✖

✓ ✓ ✖

✖ ✓ ✖

✓ ✓ ✖

EO = [rx]× = −E⊺
O

✖ ✖ ✖

FA ∈ O, FA 3,3 = 0

✖ ✖ ✖

HH⊺ ≠ I, H = KRy K−1

✓ ✖ ✖

EO = [ry]× = −E⊺
O

✖ ✖ ✖

FA = [ry]× = −F⊺
A

H ∈ Aff(2, R)
F = K[tx~y]×K⊺ = −F⊺
F = K[tz]×K⊺ = −F⊺

✖ ✓ ✖

H = Rz

✖ ✖ ✖

H ∈ Tx~y

✖ ✖ ✖

H = I

✖ ✓ ✓

H ∈ Aff(2, R)

✓ ✓ ✓

✖ ✖ ✖

H ∈ T

H = I

✖ ✖ ✖

✖ ✓ ✖

✖ ✖ ✖

✖ ✖ ✖

Uncalibrated perspective. An uncalibrated perspective
camera in general has more ambiguities from projections.
For full motion and rotation, the camera model can be dis-
ambiguated by the singular values of F or the singular val-
ues of H. As described above, pure translational motion
results in F = K−⊺[t]×K−1 = −F⊺, with F ∈ P, meaning that
the uncalibrated camera cannot be disambiguated from the
calibrated camera. Particularly interesting is the asymmetry
of rotation about X and Y axis. The rotation about the X-
axis results in the homography H with two zeros on the ﬁrst
column but nothing can be said for the case with pure ro-
tation about the Y-axis. This apparent asymmetry is simply
due to the choice of axes for the skew that results in the con-
vention of the upper triangular intrinsics. The uncalibrated
camera type can be disambiguated only for full motion, ro-
tation and rotation about the X or Y -axis. For rotation of
the uncalibrated camera we have the following proposition.

Proposition 4.1 For the rotation-induced 2d homography
H, with intrinsics K ≠ I and translation t = 0, we have,

HH⊺ ≠ I,

(8)

where, r ≠ ±[0 0 π~2]⊺.

Proof Proof is provided in the supplementary document.

Proposition 4.1 claims that the homography in an uncali-
brated perspective camera is never orthogonal unless the
rotation is the identity or a rotation of π~2 about the Z-
axis. It can be shown that when the rotation is ±π~2 and
the two focals of K are equal, H becomes orthogonal using
the spectral decomposition of HH⊺. The motion in case of
an uncalibrated perspective camera can be disambiguated
into full motion, translation, rotation and rotation around
the X-axis or Z-axis. However, even after assuming an un-
calibrated perspective camera, the metric motion is always
ambiguous without knowing the camera intrinsics [9]. One
notable exception is pure rotation discussed in [12], where
one can reason about the metric rotation.

Orthographic camera. The orthographic camera images
are related by either the Essential matrix EO ∈ O or the
afﬁne Homography A ∈ Aff(2, R). The orthographic cam-
era can be identiﬁed from the images if the motion is full.
A pure rotation will result in the Essential matrix EO with
the last diagonal element 0, the same as in the afﬁne cam-
era with rotation around the X axis.
In the case of pure
rotation around only the X or Y axis, EO is the same as E
for the pure translation of a calibrated perspective camera.
Similarly, a pure rotation around the Z-axis results in a rota-
tional homography as for the calibrated perspective camera.
Motions with pure translation results in an afﬁne transform
similar to that in afﬁne camera and thus the camera model
cannot be disambiguated completely. As to the motion type
disambiguation, the full motion, rotation and translation can
be identiﬁed as such for the orthographic camera. The rest
of the motion shows ambiguity either with the afﬁne cam-
era or the calibrated perspective camera as evident from the
relations detailed in table 2. Finally, even with a known
camera type, it is not possible to exactly decompose camera
motion from two images of an orthographic camera due to
the bas-relief ambiguity [14, 26]. This means that one of
the rotational components can never be disambiguated, but
other motions can be decomposed.

Afﬁne camera. The afﬁne camera images are related ei-
ther by the Fundamental matrix FA or the afﬁne Homogra-
phy H ∈ Aff(2, R). The afﬁne camera model can be disam-
biguated for full motion and pure rotation. In full motion,
the afﬁne Fundamental matrix satisﬁes eq. (6) but in gen-
eral not the constraint of the orthographic Essential matrix
eq. (5). Since the afﬁne camera is a generalization of the
orthographic camera, it has camera model ambiguity with
the orthographic camera in all other motion types. The only
exception is in the case of pure rotation around the Y -axis,
where the ambiguity is with both the orthographic camera
rotating around the Y -axis and the uncalibrated perspective

8735

Inlier Detection Rate

r

r

t

t

t

Outlier Rate

Figure 1. Inlier-outlier on the synthetic data. The top row shows
the inlier and outlier % detection rates when varying noise magni-
tude while the bottom row shows the same for varying % outlier
rate. Our method gives expected results in all camera type and
motion type, thus producing identical results to ransac-M.

camera translating along the Y -axis. One more ambiguity
with the uncalibrated perspective camera is for pure rotation
around the Z-axis where the two-view relation is an afﬁne
transform for both camera types. The motion in an afﬁne
camera can be identiﬁed as full motion, rotation and transla-
tion. Further disambiguation is also possible in case of pure
rotation around the Z-axis. Given the afﬁne camera type,
one cannot compute the metric motion due to the afﬁne
ambiguity and furthermore certain motion components [14]
cannot be resolved due to the bas-relief ambiguity.

5. Experimental Results

We use MATLAB to implement our method written as
sparse-basis. We use RANSAC [10] for the Fundamental
matrix as the compared method for unknown model. We
write it as ransac. We write RANSAC with known model
as ransac-M, where a speciﬁc implementation of RANSAC
is used according to the ground-truth two-view relationship
type. We verify the theoretical discussions and its applica-
tions using both the synthetic and real data. We choose the
threshold ǫ = 1.5 px for our method and equivalently for
ransac. We further test a globally optimal method of outlier
rejection [8] as global.

5.1. Synthetic Data Experiments

We use synthetic data to validate the theoretical results in
various conditions of noise and outliers for the camera and
motion types discussed in section 4.1. We use a camera res-
olution of 512 pixels and generate matches with outlier ratio
varying from 0 to 40%. We also test the compared methods
with noise, by varying uniform noise from 0 to 4 pixels.
We add 0 to 0.5 pixel noise of uniform distribution and also

Figure 2. Two-view poses for a sequence in the Oxford Robot
Car dataset [19]. In the sequence, the car does not stop and hence
we do not get any homographies. We observe that the relative
camera poses obtained with sparse-basis is very close to the results
obtained from multi-view SfM.

t

t

t

t

t

t

t

Figure 3. Results zoomed to show differences from global method
for the ﬁrst part of Oxford sequence [19] on translation estimation.

add 5% outliers to all the projections. For each condition of
motion type, camera type and noise/outlier, we use 20 sim-
ulations each with 50 points to generate the experimental
results. We then average out the detection results for dif-
ferent motion and camera types. Figure 1 shows the results
of the inlier/outlier classiﬁcation obtained during the model
computation with our method and the compared methods.
More importantly, we consistently meet the two-view rela-
tionship property of table 2 in the experiment.

While we expect the same performance of ransac-M and
sparse-basis in all cases, small gap can be particularly noted
in the inlier detection rate with varying noise. Our method

8736

r

r

r

t

t

t

t

Figure 4. Two-view poses for TUM. We are able to capture the
turns and the degenerate motions better despite the short baseline.
We introduce r′
z to show the respective motions estimated
from the homography instead of the Essential matrix.

y and t′

sparse-basis performs slightly better on average simply due
to the threshold and the way we normalize each polynomial.
Another issue is the behavior of ransac and global for the
increase in outlier detection rate with the outlier rate. When
the correct two-view relationship between image correspon-
dences is a homography, a model can still ﬁnd outliers by
ﬁtting the fundamental matrix. However, the fundamental
matrix is a weaker constraint and particularly for the ﬁrst
few outliers, ransac and global can ﬁnd ﬁt them as inliers
in the fundamental matrix model. However, as the outlier
rate increases more outliers are correctly rejected. In con-
trast, sparse-basis always ﬁts the correct model.

5.2. Real Data Experiments

We conduct experiments on real datasets to show how
our method performs in practical settings. In the ﬁrst ex-
periment we evaluate motion disambiguation on the ﬁrst
2400 frames of a sequence from the Oxford Robot car
dataset [19] and a sequence in the TUM RGBD [29] dataset
in ﬁgure 2 and 4 respectively, using all consecutive frames.
The Oxford Robot car dataset consists of high quality im-
ages where a good set of the feature matches can be ex-
pected. In order to compute poses, we match each video
frame with the one after the next frame. We keep the num-
ber of points m = 100 for both sparse-basis and ransac by
randomly choosing the matches. Same set of matches are
used for both the methods. The odometry ground-truth pro-
vided in the dataset contains drift and inaccuracies. There-
fore, we reconstruct the sequence of about 2400 frames us-
ing multi-view SfM COLMAP [24]. The relative ground-
truth poses are then obtained from the SfM reconstruction
of COLMAP. We refer to the ground-truth poses as multi-
view SfM. We plot the comparison between sparse-basis
and ransac in ﬁgures 2 and 4 for the two datasets. We fur-

ther provide a comparison between sparse-basis and global
on a subsequence of ﬁgure 2.

Both sequences pose challenging condition for comput-
ing motion due to the short baseline. Nonetheless, sparse-
basis is able to better condition the motion computation
as we look for sparse basis for the two-view relationship,
whether it is the Essential matrix or the homography. In par-
ticular, we capture the smooth turning of the vehicle (rota-
tion about Y -axis), the consistent forward motion of the car
(Z-translation) and no motion (middle of the sub-sequence)
which is not possible without searching for the sparse bases
as shown in ﬁgure 4 for ransac. We also note a smooth
transition between ry and r′

y as seen in ﬁgure 4.

Figure 5. Inlier detected in real images. The top row shows the
inliers for images with change in resolution and bottom row for
perspective image of a printed picture. In both cases sparse-basis
correctly ﬁts a homography as the two-view relationship.

We further show the qualitative results of matching
points using our method in two interesting examples in ﬁg-
ure 5. We ﬁrst match points using SIFT [18] descriptors in
each of the three cases. In ﬁgure 5 our method sparse-basis
is able to match identical lower and higher resolution im-
ages while rejecting outliers. At the same time, we can rea-
son that the correspondences are from identical images as
we obtain an identity homography. We provide additional
experiments in the supplementary material.

6. Conclusions

We proposed a method that computes the correct two-
view relationship in the presence of noise and outliers, even
when the camera and the motion types are unknown by
looking for sparse polynomials. In this scenario, we dis-
cussed the possibilities of disambiguating camera and mo-
tion case-by-case. The experiments verify our theory and
give practical applications of our method.

Acknowledgements. This research was funded by the
EU’s Horizon 2020 programme under grant No. 687757
– REPLICATE and by the Swiss Commission for Technol-
ogy and Innovation (CTI), Grant No. 26253.1 PFES-ES –
EXASOLVED.

8737

[19] W. Maddern, G. Pascoe, C. Linegar, and P. Newman. 1 Year,
1000km: The Oxford RobotCar Dataset. The International
Journal of Robotics Research (IJRR), 36(1):3–15, 2017. 7, 8
[20] G. P. McCormick. Computability of global solutions to
factorable nonconvex programs: Part iconvex underestimat-
ing problems. Mathematical programming, 10(1):147–175,
1976. 4

[21] D. Nist´er. An efﬁcient solution to the ﬁve-point relative
IEEE Trans. Pattern Anal. Mach. Intell.,

pose problem.
26(6):756–777, 2004. 1

[22] D. Nister, O. Naroditsky, and J. Bergen. Visual odometry. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 652–659, 2004. 2

[23] D. Scaramuzza. 1-point-ransac structure from motion for
vehicle-mounted cameras by exploiting non-holonomic con-
straints. Int. J. Comp. Vision, 95(1):74–85, 2011. 1

[24] J. L. Sch¨onberger and J.-M. Frahm. Structure-from-motion
In Conference on Computer Vision and Pattern

revisited.
Recognition (CVPR), 2016. 1, 8

[25] J. L. Schonberger and J.-M. Frahm. Structure-from-motion

revisited. In CVPR, 2016. 1

[26] L. S. Shapiro, A. Zisserman, and M. Brady. 3d motion re-
covery via afﬁne epipolar geometry. International Journal of
Computer Vision, 16(2):147–182, 1995. 2, 6

[27] N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the
world from internet photo collections. Int. J. Comput. Vision,
80(2):189–210, 2007. 1

[28] P. Speciale, D. P. Paudel, M. R. Oswald, T. Kroeger, L. V.
Gool, and M. Pollefeys. Consensus maximization with linear
matrix inequality constraints. In CVPR, 2017. 1

[29] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of rgb-d slam systems.
In IROS, 2012. 8

[30] Z. Zhang and A. R. Hanson. 3d reconstruction based on ho-
mography mapping. In ARPA Image Understanding Work-
shop, 1996. 1

[31] Y. Zheng, S. Sugimoto, and M. Okutomi. Deterministically
maximizing feasible subsystem for robust model ﬁtting with
unit norm constraint. In CVPR, 2011. 1

References

[1] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool. Speeded-
up robust features (surf). Comput. Vis. Image Underst.,
110(3):346–359, 2008. 1

[2] J.-C. Bazin, H. Li, I. S. Kweon, C. Demonceaux, P. Vasseur,
and K. Ikeuchi. A branch-and-bound approach to correspon-
dence and grouping problems. IEEE transactions on pattern
analysis and machine intelligence, 35(7):1565–1576, 2013.
1

[3] J. C. Bazin, Y. Seo, R. I. Hartley, and M. Pollefeys. Globally
optimal inlier set maximization with unknown rotation and
focal length. In ECCV, 2014. 1

[4] . Bj¨orck and V. Pereyra. Solution of vandermonde systems of
equations. Mathematics of Computation, 24(112):893–903,
1970. 2, 3

[5] P. Breiding, S. K. Verovsek, B. Sturmfels, and M. Weinstein.
Learning algebraic varieties from samples. arXiv preprint
arXiv:1802.09436, 2018. 2, 3

[6] M. Chandraker, S. Agarwal, F. Kahl, D. Nister, and D. Krieg-
man. Autocalibration via rank-constrained estimation of the
absolute quadric. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2007. 1, 5

[7] T. J. Chin, Y. H. Kee, A. Eriksson, and F. Neumann. Guaran-
teed outlier removal with mixed integer linear programs. In
CVPR, 2016. 1

[8] T.-J. Chin, P. Purkait, A. Eriksson, and D. Suter. Efﬁcient
globally optimal consensus maximisation with tree search.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2413–2421, 2015. 7

[9] O. D. Faugeras. What can be seen in three dimensions with
an uncalibrated stereo rig. In ECCV, pages 563–578, 1992.
2, 4, 6

[10] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: A paradigm for model ﬁtting with applications to im-
age analysis and automated cartography. Commun. ACM,
24(6):381–395, 1981. 1, 2, 7

[11] F. Fraundorfer, P. Tanskanen, and M. Pollefeys. A minimal
case solution to the calibrated relative pose problem for the
case of two known orientation angles. In ECCV, 2010. 1

[12] R. I. Hartley. Self-calibration from multiple views with a

rotating camera. In ECCV, pages 471–478, 1994. 1, 5, 6

[13] R. I. Hartley and F. Kahl. Global optimization through rota-

tion space search. IJCV, 82(1):64–79, 2009. 1

[14] R. I. Hartley and A. Zisserman. Multiple View Geometry
in Computer Vision. Cambridge University Press, second
edition, 2004. 1, 2, 4, 5, 6, 7

[15] H. Li. Consensus set maximization with guaranteed global
optimality for robust geometry estimation. In ICCV, 2009. 1
[16] A. Locher, M. Havlena, and L. Van Gool. Progressive struc-

ture from motion. In ECCV, 2018. 1

[17] H. Longuet-Higgins. A computer algorithm for reconstruct-
ing a scene from two projections. Nature, 293:133–135,
1981. 1, 2, 4, 5

[18] D. G. Lowe. Distinctive image features from scale-invariant
International Journal of Computer Vision,

keypoints.
60(2):91–110, 2004. 1, 8

8738

