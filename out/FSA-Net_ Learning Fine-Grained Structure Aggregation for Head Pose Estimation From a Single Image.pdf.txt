FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose

Estimation from a Single Image

Tsun-Yi Yang1

2

,

Yi-Ting Chen1

Yen-Yu Lin1

Yung-Yu Chuang1

2

,

1Academia Sinica, Taiwan

2National Taiwan University, Taiwan

shamangary@citi.sinica.edu.tw

jamie@media.ee.ntu.edu.tw

yylin@citi.sinica.edu.tw

cyy@csie.ntu.edu.tw

Abstract

This paper proposes a method for head pose estimation
from a single image. Previous methods often predict head
poses through landmark or depth estimation and would re-
quire more computation than necessary. Our method is
based on regression and feature aggregation. For having
a compact model, we employ the soft stagewise regression
scheme. Existing feature aggregation methods treat inputs
as a bag of features and thus ignore their spatial relation-
ship in a feature map. We propose to learn a ﬁne-grained
structure mapping for spatially grouping features before ag-
gregation. The ﬁne-grained structure provides part-based
information and pooled values. By utilizing learnable and
non-learnable importance over the spatial location, differ-
ent model variants can be generated and form a comple-
mentary ensemble. Experiments show that our method out-
performs the state-of-the-art methods including both the
landmark-free ones and the ones based on landmark or
depth estimation. With only a single RGB frame as in-
put, our method even outperforms methods utilizing multi-
modality information (RGB-D, RGB-Time) on estimating
the yaw angle. Furthermore, the memory overhead of our
model is 100× smaller than those of previous methods.

1. Introduction

Facial modeling and analysis have long been an active
research topic in computer vision [2, 3, 4, 5, 6, 7, 21, 22, 24,
25]. Large facial datasets [16, 37, 48] and efﬁcient methods
for different facial analysis problems have been proposed
for years, such as face recognition [4, 6] or identiﬁcation,
facial age estimation [45], landmark detection [3], and head
pose estimation [35]. This paper addresses the head pose
estimation problem which has many applications such as
driver behavior monitoring and human attention modelling.
It could also be used to improve or provide extra informa-
tion for other problems such as identity recognition [39],
expression recognition [46], or attention detection [8].

Figure 1. Sample results of pose estimation using the proposed
method. Our method only takes as input a single RGB frame.
Results for two sequences of head motion are shown. The blue
line indicates the direction the subject is facing; the green line for
the downward direction while the red one for the side.

Head pose estimation from a single image is a challeng-
ing problem. Head pose is a 3D vector containing the angles
of yaw, pitch and roll. Estimating the head pose from an im-
age essentially requires to learn a mapping between 2D and
3D spaces. Some methods utilize more modalities such as
3D information in depth images [28, 25, 14, 27] or tem-
poral information in video sequences [16]. The depth im-
ages provide 3D information that is missing in 2D images.
Videos capture continuous movement of human heads and
provide extra information to help the pose estimation. How-
ever, learning the temporal information is usually achieved
by recurrent structures with high computation costs while
capturing depth information often requires special cameras
which are not always available. Most single-frame pose es-
timation methods utilize facial landmark detection for esti-
mating head poses [20, 3]. However, it would incur more
computation and leads to bigger models. Hence, all these
models are not suitable to be adopted on platforms with lim-
ited memory and computation resource.

This paper proposes FSA-Net, a compact model for pose
estimation from a single image using direct regression with-
out landmarks. For having a compact model, the proposed
model is built on the soft stagewise regression scheme [45].
To harvest multi-scale information, like many regression
methods [45, 3], our method combines feature maps from
different layers/stages. For having more accurate predic-

1087

tions, it requires to learn meaningful intermediate features
for performing regression. The state-of-the-art differen-
tiable aggregation/pooling methods such as capsule net-
works [36] and NetVLAD [1] can be adopted for distilling
representative features from candidate features. However,
these methods often treat the inputs as a bag of features and
neglect their spatial relationship in the feature map. The key
idea of the proposed method is to spatially group pixel-level
features of the feature map together into a set of features
encoded with spatial information. These features are then
used as the candidate features for aggregation. That is, the
proposed method learns to ﬁnd the ﬁne-grained structure
mapping for spatially grouping pixel-level features together
to form more power region-level features.

The proposed ﬁne-grained structure mapping can be in-
terpreted as a more ﬂexible and versatile tool for pooling.
Conventional pooling takes a set of features at ﬁxed loca-
tions within a local window. A pre-deﬁned operation is
applied to them without taking data content into account
while our method pools features from a wider area with a
more versatile operation. For harvesting more versatile spa-
tial information, we adopt both learnable and non-learnable
importance measures, and complementary model variants
can be generated for making a powerful and robust ensem-
ble. Experiments show that our model outperforms other
single-frame pose estimation methods with the model size
of only 5MB, around 100× smaller than that of the previ-
ous state-of-the-art method. For yaw angle prediction, the
proposed method is even favorable against heavy models
utilizing multiple modalities such as RGB-D or RGB-Time.
Figure 1 shows sample results of the proposed method. It is
clear that the pose estimation is rather accurate.

2. Related work

Landmark-based methods. They ﬁnd facial landmarks
ﬁrst and then use them to estimate the head pose. Given a
set of 2D face landmarks, the head pose can be determined
by 3D computer vision techniques such as POSIT [11].
Regression-based methods [5, 43, 12, 23, 42] sketch initial
faces, and incrementally align the drawn faces to real ones
by regression. Model-based methods [26, 24, 10] model hu-
man faces with several key points, and then locate the key
points on real faces via trained appearance models. Deep-
learning-based methods [48, 3, 38] estimate 3D face models
using convolutional neural networks (CNNs) and gain supe-
rior performance compared to previous methods. Although
effective, landmark detection is not required for pose esti-
mation and often incurs unnecessary computation.
Methods with different modalities.
Landmark-based
methods require manually annotated labels as ground truth.
However, acquiring annotated landmarks is labor-intensive.
In some cases with low-resolution images, even experts can-
not accurately pinpoint facial landmark locations. Consid-

ering the cost and accuracy, some proposed face alignment
algorithms without face landmarks [6, 35]. On the other
hand, it is also very popular to adopt different modalities to
compensate for the loss of information [14, 27, 9, 29].

RGB. Several approaches only utilize a single RGB im-
age for pose estimation [6, 35, 32, 22, 31]. FacePoseNet [6]
employed a CNN for 3D head pose regression, which im-
proves face recognition accuracy. Nataniel et al. [35] com-
bined ResNet50 with a multi-loss architecture. Each loss
contains a binned pose classiﬁcation and regression, corre-
sponding to yaw, pitch, and roll individually. With binned
classiﬁcation, their method obtained robust neighborhood
prediction of the pose.

Depth. Intensity-based head pose estimation algorithms
fail to produce accurate head poses in conditions such as
poor illumination during night time or large illumination
variations during day time. Fanelli et al. [14] exploited dis-
criminative random regression forests for head pose esti-
mation with depth images. Meyer et al. [27] proposed to
register 3D morphable models to depth images and incre-
mentally reﬁne the registration over time.

RGB+Time. Methods for facial video analysis take a se-
quence of RGB images as inputs and utilize temporal infor-
mation. Previous facial analysis methods on videos [9, 29]
cope with temporal coherence by Bayesian ﬁlters or parti-
cle ﬁlters. Inspired by the similarity between Bayesian ﬁl-
ters and recurrent neural networks (RNNs), Gu et al. [16]
proposed to track facial features by RNNs over time.
Multi-task methods. Head pose estimation is closely re-
lated to other facial analysis problems. Recent work [7,
31, 49] demonstrates that learning related tasks jointly
achieves better results than performing individual tasks in-
dependently. Several methods [31, 32] propose to perform
various related facial analysis tasks simultaneously using
CNNs. Hyperface [31] learns common features by CNNs,
for simultaneously performing face detection, facial land-
mark localization, head pose estimation and gender recog-
nition. KEPLER [22] learns global and local features by a
Heatmap-CNN to explore structural dependencies.
Attention. Our method provides attention for pose estima-
tion. Our attention can be optimized in an end-to-end man-
ner along with the pose estimation without complex addi-
tional techniques [18, 19, 40, 30, 17]. Compared with other
pooling methods using attention such as CBAM [41] and
Attentional Pooling [15], our method has the following dif-
ferences with them. First, they focus on categorical clas-
siﬁcation problems (image classiﬁcation and action recog-
nition) while our method is for a regression problem. Sec-
ond, they only generate one or two spatial heatmaps while
our model is capable of generating multiple spatial attention
proposals which are more ﬂexible for reﬁning regression
values. Finally, our method takes into account multi-scale
information, and it’s useful to other applications.

1088

3. Method

In this section, we ﬁrst formulate the pose estima-
tion problem (Section 3.1). Next, we introduce the soft
stagewise regression and apply it to pose estimation (Sec-
tion 3.2). We then give an overview of the proposed FSA-
Net (Section 3.3). Two important ingredients of the FSA-
Net,
the scoring function and the ﬁne-grained structure
mapping, are then described in Section 3.4 and Section 3.5
respectively. Finally, we explain details of the architecture
(Section 3.6).

3.1. Problem formulation

For the problem of image-based head pose estima-
tion, we are given a set of training face images X =
{xn | n = 1, ..., N } and the pose vector yn for each image
xn, where N is the number of images. Each pose vector yn
is a 3D vector whose components respectively correspond
to the angles of yaw, pitch, and roll. The goal is to ﬁnd a
function F so that it predicts ˜y = F (x) that matches the
real head pose y for the given image x as much as possible.
We ﬁnd F by minimizing the mean absolute error (MAE)
between the predicted and the ground truth poses,

J(X) =

1
N

N

X

n=1

k˜yn − ynk1 ,

(1)

where ˜yn=F (xn) is the predicted pose for the training im-
age xn. It is a regression problem by nature.

3.2. SSR Net MD

Our proposed solution is built on the SSR-Net [45],
which provides a compact model for age estimation from a
single image. Inspired by DEX [33], SSR-Net casts the re-
gression problem of age estimation as a classiﬁcation prob-
lem by dividing into the age domain into several age classes
(bins). A network performs the classiﬁcation task and out-
puts a probability distribution for age classes. Given the
probability distribution, the age is estimated as the expected
value. For having a compact model, SSR-Net adopts a
coarse-to-ﬁne strategy for classiﬁcation. Each stage only
performs intermediate classiﬁcation with a small number of
classes, say “relatively younger”, “about right” and “rela-
tively older” within the current age group. The next stage
reﬁnes the decision within the age group assigned by the
previous stage [45].
In sum, SSR-Net performs a hierar-
chical classiﬁcation and uses the following soft stage-wise
regression for estimating the age ˜y:

˜y =

K

X

k=1

~p(k) · ~µ(k),

(2)

where K is the number of stages; ~p(k) is the probability
distribution for the k-th stage; and ~µ(k) is a vector consist-
ing of the representative values of age groups at the k-th

stage. To accommodate quantization errors and class am-
biguity, a shift vector ~η(k) adjusts the center for each bin
and a scale factor ∆k scales the widths of all bins at the k-
th stage, thus modifying the representative ages ~µ(k). Like
~p(k), both ~η(k) and ∆k are found by a neural network. Given
an input image, SSR-Net outputs K sets of stage parameters
{~p(k), ~η(k), ∆k}K
k=1 and uses the soft stage-wise regression
for estimating the age.

The soft stagewise regression formulation can be ap-
plied to any regression problem.
For a given re-
gression problem, the soft stagewise regression function
SSR({~p(k), ~η(k), ∆k}K
k=1) accepts K sets of stage param-
eters and outputs the expected value as the regression value
according to Equation (2). In this paper, we apply the soft
stagewise regression to the problem of pose estimation from
a single image. Different from the age estimation problem,
the pose estimation problem estimates a vector, rather than a
scalar. We denote the SSR-Net for multiple dimensional re-
gression by SSR-Net-MD, and revise the two-stream struc-
ture of the SSR-Net as described in Section 3.6. Although
SSR-Net-MD gives fairly good performance, we propose to
use feature aggregation to further improve it.

3.3. Overview of FSA Net

Figure 2(a) depicts the architecture of the proposed FSA-
Net. The input image goes through two streams. There are
K stages (K = 3 in Figure 2(a)). Each stream extracts a
feature map at a stage. For the k-th stage, the extracted
feature maps are fused together by the stage fusion module
(the green boxes between two streams in Figure 2(a)). The
stage fusion module ﬁrst combines the two feature maps by
element-wise multiplication. It then applies c 1×1 convo-
lutions to transform the combined feature map into c chan-
nels. Finally, average pooling is used to reduce the size of
the feature map to w×h. Thus, we obtain a w×h ×c feature
map Uk for the k-th stage. The feature map Uk is a spa-
tial grid, in which each cell contains a c-dimensional fea-
ture representation of a particular spatial location. These K
feature maps are then fed into the mapping module for ob-
taining K c′-d vectors, each of which will be used to obtain
the stage outputs {~p(k), ~η(k), ∆k} for the SSR function.

Given K feature maps of size w ×h ×c, the task of the
aggregation module is to aggregate them into a small num-
ber of more representative features, in our case, K c′-d fea-
tures, one for each stage. Through the aggregation process,
a more meaningful representation can be distilled from a
bag of features. Existing feature aggregation methods, such
as capsule [36] and NetVLAD [1], can be employed for the
task. However, as mentioned in Section 1, these methods
treat the input features as a bag of features and completely
ignore the spatial information exhibited within the feature
map. To overcome the problem, we propose to perform
spatial grouping of features before feeding them into the

1089

U

1

U

K

c

n

c

s
t
r
e
a
m
o
n
e

 



Stage	
fusion

k=3

k=2

k=1

s
t
r
e
a
m

 
t

w
o



Fine-grained	

structure	feature	

aggregation

SSR	Module

Regression	
prediction

…

…



U

…



(w	x	h	x	c)

Fine-grained	

structure	mapping

A
k

Scoring	
function

!U

…

(n’K	x	c)

Feature	

aggregation

V

(K	x	c’)

B
k

c

m

(1xc)

m

(1xn)

n

(1xc)

U

single	stage	

mapping

M

k

m

c

Reshape

n	=	(w	x	h)	x	K

n’

(1xc)

!U

k

(1xm)

m

(1xc)

n’

C

B
k

cross	stage		
mapping

h

K

w

fine-grained	

structure	

visualization

(a) FSA-Net

(b) Fine-grained structure mapping

Figure 2. Overview of the proposed FSA-Net. Source code available at https://github.com/shamangary/FSA-Net

aggregation process. Thus, the inputs to the feature aggre-
gation module would be more powerful features encoded
with global spatial information, rather than the pixel-level
features1 in the feature maps.

For the purpose of spatial grouping, for each feature map
Uk, we ﬁrst compute its attention map Ak through a scor-
ing function (Section 3.4). Next, the feature maps Uk and
the attention maps Ak are fed into the ﬁne-grained structure
mapping module. The module learns to extract n′ c-d repre-
sentative features by spatially weighting the pixel-level fea-
tures in the feature maps. These vectors are then fed into
a feature aggregation method for generating the ﬁnal set
of representative features for regression, V , containing K
c′-d features. The vector Vk is used to generate the stage
outputs {~p(k), ~η(k), ∆k} for the k-th stage through a fully-
connected layer. These outputs are then substituted into the
SSR function for obtaining the pose estimation.

3.4. Scoring function

For better grouping features, it is useful to measure the
signiﬁcance of the pixel-level features. Given a pixel-level
feature u = (u1, . . . , uc), we design a scoring function
Φ(u) to measure its importance to facilitate spatial group-
ing. Thus, for each feature map Uk, we obtain an impor-
tance or attention map Ak, where Ak(i, j) = Φ(Uk(i, j)).

We explore three options as the scoring functions. (1)
1 × 1 convolution, (2) Variance and (3) Uniform. The ﬁrst
option takes an extra 1 × 1 convolution layer as a learn-
able scoring function, i.e., Φ(u) = σ(w · u), where σ is the
sigmoid function and w is the learnable convolution kernel.
Although the use of 1×1 convolution as the scoring function
allows us to learn how to weight features from the training

1We refer the feature associated with a cell of the feature map as a pixel-
level feature. Notice that a “pixel” of the feature map actually occupies a
local patch in the input image.

c Pc

data, it could suffer from the potential overﬁtting problem
when there is signiﬁcant discrepancy between the training
and testing data. Inspired by ORB [34] in which features
are selected using variances, the second option explores the
use of variance for importance, i.e., Φ(u) = Pc
i=1(ui −µ)2
where µ = 1
i=1 ui. Note that variance is differentiable
although not learnable. The ﬁnal option, uniform, is to treat
all features equally, i.e., Φ(u) = 1. In this case, ˜U = U and
the ﬁne-grained structure mapping is not performed. Note
that these three options explore learnable, non-learnable and
constant alternatives. They could provide complementary
information. In Section 4, we will compare the performance
of these options. We found that they capture different as-
pects and the best practice is to form an ensemble model
by averaging their predictions together. This way, the pose
estimation is more robust.

3.5. Fine grained structure mapping

With the feature maps Uk and their attention maps Ak,
the next step is to perform ﬁne-grained structure mapping to
extract a set of representative features ˜U . Figure 2(b) illus-
trates the process. We ﬁrst ﬂatten all feature maps Uk into a
matrix U whose ﬁrst dimension is n = w×h×K, U ∈ Rn×c.
In other words, U is a 2D matrix containing all c-d pixel-
level features in all feature maps across all stages. For the
k-th stage, we would like to ﬁnd a mapping Sk which se-
lects and groups features in U into a set of n′ representative
features ˜Uk by

˜Uk = SkU,

(3)

×n and ˜Uk ∈ Rn′

where Sk ∈ Rn′
×c. That is, we assem-
ble n′ representative features from n pixel-level features by
their linear combinations. The map Sk is the linear transfor-
mation which performs the linear dimensionality reduction
by taking weighted averages over all pixel-level features.

1090

We write the map Sk as the product of two learnable

maps, C and Mk:

Sk = CMk,

(4)
where C ∈ Rn′
×m, Mk ∈ Rm×n and m is a parameter.
The map Mk is speciﬁc for the k-th stage while the map C
is shared across all stages. The maps Mk and C are formed
as follows:

Mk = σ(fM (Ak)),

C = σ(fC(A)),

(5)

(6)

where σ is the sigmoid function; fM and fC are two differ-
ent functions deﬁned by fully-connected layers; and A =
[A1, A2, . . . , AK] is the concatenation of all attention maps.
Both fM and fC are parts of the end-to-end trainable FSA-
Net and they are discovered through learning from training
data. The use of a separable map for Sk not only reduces
the number of parameters, but also stabilizes the training.
Furthermore, L1 normalization is performed for each row
of Sk for more stable training.

Each row of the map Mk can be folded into K maps
of the size w × h, each of which represents how the pixel-
level features spatially contribute to the representative fea-
ture corresponding to the speciﬁc row. Thus, each row of
Mk can be taken as a ﬁne-grained structure that is salient to
pose estimation. Figure 5 visualizes some maps.

Finally, we concatenate all ˜Uk together to form the ﬁnal
set of representative features, ˜U = [ ˜U1, ˜U2, . . . , ˜UK], where
˜U ∈ R(n′
·K)×c. The set of representative features ˜Uk is
then fed into a feature aggregation method for obtaining the
ﬁnal set of features, V ∈ RK×c′
, for stage-wise regression.

3.6. Details of the architecture

Similar to the DeepCD [44] and SSR-Net, the FSA-Net
has two streams. They are built with two basic building
blocks BR and BT as:

BR(c) ≡ {SepConv2D(3×3, c)-BN-ReLU},
BT(c) ≡ {SepConv2D(3×3, c)-BN-Tanh},

where SepConv2D is separable 2D convolution; BN de-
notes batch normalization and c is a parameter. The struc-
ture of the ﬁrst stream is {BR(16)-AvgPool(2 × 2)-BR(32)-
BR(32)-AvgPool(2×2)} - {BR(64)-BR(64)-AvgPool(2×2)}
- {BR(128)-BR(128)}. The layers between each pair of
brackets form a stage. The feature map at the end of a stage
is the output of the stage. The structure of the second stream
is {BT(16)-MaxPool(2×2)-BT(32)-BT(32)-MaxPool(2×2)}
- {BT(64)-BT(64)-MaxPool(2 × 2)} - {BT(128)-BT(128)}.
Since there are three stages, the parameter K is equal to 3
in our FSA-Net.

As for other parameters, in our current implementation,
we set w = 8, h = 8 and c = 64 for the feature maps.
We set m = 5 and n′ = 7 for the ﬁne-grained structure
mapping, and c′ = 16 for the feature aggregation module
throughout all experiments.

Figure 3. Examples from the datasets. The ﬁrst row is from the
300W-LP synthetic dataset. In this dataset, the images at different
poses are rendered, instead of being taken in the real world. The
second row is from the AFLW2000 dataset which contains many
different real-world backgrounds and light conditions. The third
row is from the BIWI dataset which was collected under the con-
trolled environment.

4. Experiments

This section describes implementation, the datasets for
training and testing, evaluation protocols, results, compar-
isons with other methods, and the ablation study.

4.1. Implementation

We used Keras with Tensorﬂow backend for imple-
menting the proposed FSA-Net. For data augmentation in
training, we applied random cropping and random scaling
(0.8 ∼ 1.2) to training images. We used 90 epochs to train
the network with the Adam optimizer with the initial learn-
ing rate 0.001. The learning rate was reduced by a factor of
0.1 every 30 epochs. The experiments were performed on
a computer with an Intel i7 CPU and an GTX1080Ti GPU.
The inference time of our model is around 1ms per image.

4.2. Datasets and evaluation protocols

Datasets. Three popular datasets for head pose estima-
tion were adopted in the experiments: the 300W-LP [48],
AFLW2000 [48], and BIWI [13] datasets, The 300W-LP
dataset [48] was derived from the 300W dataset [37] which
uniﬁes several datasets for face alignment with 68 land-
marks. Zhu et al. used face proﬁling with 3D image mesh-
ing to generate 61, 225 samples across large poses and fur-
ther expanded to 122, 450 samples with ﬂipping [48]. The
synthesized dataset is named as the 300W across Large
Poses (300W-LP). The AFLW2000 dataset [48] provides
ground-truth 3D faces and the corresponding 68 landmarks
for the ﬁrst 2, 000 images of the AFLW dataset [21]. The
faces in the dataset have large pose variations with var-
ious illumination conditions and expressions. The BIWI
dataset [13] contains 24 videos of 20 subjects in the con-
trolled environment. There are a total of roughly 15, 000

1091

frames in the dataset. In addition to RGB frames, the dataset
also provides the depth image for each frame. Figure 3
shows examples from these three datasets. For training and
evaluation on these datasets, we follow the following two
common protocols.
Protocol 1. For this protocol, we follow the setting of
Hopenet [35] whose goal is also landmark-free head pose
estimation: training on the synthetic 300W-LP dataset while
testing on the two real-world datasets, the AFLW2000 and
BIWI datasets. Notice that, the same as the setting of
Hopenet, when evaluating on the BIWI dataset, we do
not use tracking and only considers samples whose ro-
tation angles are within the range of [−99◦, +99◦] with
MTCNN [47] face detection. We compare several state-of-
the-art landmark-based pose estimation methods using this
protocol. The batch size we used for this protocol is 16.
Protocol 2. In this protocol, we used 70% of videos (16
videos) in the BIWI dataset for training, and the others (8
videos) for testing. The faces in the BIWI dataset are de-
tected by MTCNN with the empirical tracking technique to
avoid failure of face detection. Notice that this protocol was
adopted by several pose estimation methods with different
modalities such as RGB, depth, and time while our method
only utilizes a single RGB frame. We used the batch size 8
for training in this protocol.

4.3. Competing methods

We compare our method with the following state-of-the-
art methods for pose estimation. The ﬁrst group of methods
is landmark-based. KEPLER [22] predicts facial keypoints
and pose at the same time with a modiﬁed GoogLeNet ar-
chitecture. The coarse pose supervision is used for im-
proving landmark detection. FAN [3] is a state-of-the-art
landmark detection method. It is robust against occlusions
and head poses. The method acquires multi-scale informa-
tion by merging block features multiple times across lay-
ers. Dlib [20] is a standard face library which contains
landmark detection, face detection, and several other tech-
niques. 3DDFA [48] uses CNNs to ﬁt a 3D model to an
RGB image. The dense 3D model allows alignment of
the landmarks even with occlusions. Hopenet [35] is a
landmark-free regression method. It employs ResNet and
trains it using both MSE and cross-entropy losses.

There are also some head pose estimation methods
which utilize multiple modalities. VGG16 (RGB) and
VGG16+RNN (RGB+Time) were proposed by Gu et
al. [16]. They analyzed multiple possibilities of combin-
ing the CNN and RNN based on analysis of Bayesian ﬁl-
ters. Martin [25] estimates head pose from depth images
from a consumer depth camera by building and registering
a 3D head model. DeepHeadPose [28] focuses on low-
resolution RGB-D images. It uses both classiﬁcation and
regression to predict the estimation conﬁdence.

MB

Yaw

Pitch

Roll MAE

Dlib (68 points) [20]

FAN (12 points) [3]

Landmarks [35]

3DDFA [48]

Hopenet (α = 2) [35]
Hopenet (α = 1) [35]

SSR-Net-MD [45]

FSA-Caps (w/o)
FSA-Caps (1×1)
FSA-Caps (var.)

FSA-Caps-Fusion

-

183

-

-

95.9

95.9

1.1

2.9

1.1

1.1

5.1

23.1

6.36

5.92

5.40

6.47

6.92

5.14

5.27

4.82

4.96

4.50

13.6

12.3

11.86

8.53

6.56

6.64

7.09

6.71

6.19

6.34

6.08

10.5

8.71

8.27

8.25

5.44

5.67

5.89

5.28

4.76

4.78

4.64

15.8

9.12

8.65

7.39

6.16

6.41

6.01

5.75

5.25

5.36

5.07

Table 1. Comparisons with the state-of-the-art methods on the
AFLW2000 dataset. All are trained on the 300W-LP dataset.

MB

Yaw Pitch

Roll MAE

3DDFA [48]

KEPLER [22]

Dlib (68 points) [20]

FAN (12 points) [3]
Hopenet (α = 2) [35]
Hopenet (α = 1) [35]
SSR-Net-MD [45]

FSA-Caps (w/o)
FSA-Caps (1×1)
FSA-Caps (var.)

FSA-Caps-Fusion

-

-

-

183

95.9

95.9

1.1

2.9

1.1

1.1

5.1

36.2

8.80

16.8

8.53

5.17

4.81

4.49

4.56

4.78

4.56

4.27

12.3

17.3

13.8

7.48

6.98

6.61

6.31

5.15

6.24

5.21

4.96

8.78

16.2

6.19

7.63

3.39

3.27

3.61

2.94

3.31

3.07

2.76

19.1

13.9

12.2

7.89

5.18

4.90

4.65

4.22

4.31

4.28

4.00

Table 2. Comparisons with the state-of-the-art methods on the
BIWI dataset. All are trained on the 300W-LP dataset.

4.4. Results with protocol 1

In this scenario, pose estimation methods are trained
with the 300W-LP dataset. Table 1 and Table 2 com-
pare our FSA-Net with the state-of-the-art methods on the
AFLW2000 and BIWI datasets, respectively. The mean ab-
solute error (MAE) is used as the evaluation metric.
In
this protocol, the characteristics of the training and test-
ing datasets are quite different. The training dataset is syn-
thetic while the testing datasets are real. The landmark-free
approaches can better accommodate the domain discrep-
ancy between training and testing. Thus, the landmark-free
methods (Hopenet, SSR-Net-MD and FSA-Net) perform
better than landmark-based ones on both AFLW2000 and
BIWI datasets. Figure 4 compares our model with Hopenet
by showing a few examples. Both SSR-Net-MD and FSA-
Net are more compact than Hopenet. All FSA-Net variants
perform better than SSR-Net-MD. FSA-Caps denotes FSA-
Net that uses capsule [36] for feature aggregation. There are
three options for the scoring function: w/o for not applying
ﬁne-grained feature mapping, 1×1 for 1×1 convolution and

1092

Method

MB

Yaw Pitch

Roll MAE

RGB-based

DeepHeadPose [28]

SSR-Net-MD [45]

VGG16 [16]

FSA-Caps-Fusion

-

1.1

500

5.1

5.67

4.24

3.91

2.89

5.18

4.35

4.03

4.29

-

4.19

3.03

3.60

-

4.26

3.66

3.60

RGB+Depth

DeepHeadPose [28]

Martin [25]

-

-

5.32

3.6

4.76

2.5

-

2.6

-

2.9

RGB+Time

VGG16+RNN [16] >500

3.14

3.48

2.60

3.07

Table 3. Comparisons with the-state-of-art methods on the BIWI
dataset. 70% of videos are used for training (16 videos) and 30%
for testing (8 videos). There are three groups of methods that use
information from different modalities: RGB-based, RGB+Depth
and RGB+Time.

(a)

(b)

(c)

(d)

(e)

Figure 5. Visualizations of the discovered ﬁne-grained spatial
structures. The model is the FSA-Caps (1×1) trained on the 300W-
LP dataset. The ﬁrst column shows the estimated head poses.
The other four columns display four spatial structures by heatmaps
which visualize the folded versions of some rows of Mk discov-
ered by the model. They show how pixels are aggregated for a
speciﬁc representative feature. The examples of the ﬁrst two rows
are from the AFLW2000 dataset, and those of the last two rows
come from the BIWI dataset.

Section 3.5. For example, the heatmaps in Figure 5(c) show
that the forehead and the region of eyes are aggregated for
the speciﬁc feature. The detected regions are similar across
images but slightly different due to the head poses. As an-
other example, Figure 5(e) focuses on the right cheek.

4.7. Ablation Study

We have conducted the ablation study for understanding
the inﬂuence of individual components, including different

1093

Figure 4. Pose estimation on the AFLW2000 dataset (protocol
1). From top to bottom, they are ground truth, results of Hopenet
and our results. The blue line indicates the direction the subject
is facing; the green line for the downward direction while the red
one for the side. Best view in color.

var. for variance. Although 1×1 convolution has the poten-
tial for learning a better mapping from data, it could suffer
from overﬁtting. Thus, from the experiments, it does not
always lead to the best performance. We found that their
fusion with simple average, denoted as FSA-Caps-Fusion,
produces the most robust results. KEPLER [22] also in-
tends to ﬁnd the structure relation between keypoints, but
our scheme of learning ﬁne-grained structure mapping is
much more effective than their iterative method.

4.5. Results with protocol 2

The BIWI dataset contains information from multiple
modalities. Other than using color information within a
single frame, it is possible to leverage depth or temporal
information for improving performance. Table 3 reports
performance of methods using different modalities. The
RGB-based group only uses a single RGB frame while
RGB+Depth and RGB+Time respectively utilize depth and
temporal
information in addition to color information.
Our method (FSA-Caps-Fusion) only uses a single RGB
frame and outperforms all other methods in its peer group.
VGG16 is close but its model size is much bigger. Our
model does not perform as well as methods using multi-
modality information, but not too far from them. In addi-
tion, our method is the best on predicting the yaw angle,
even outperforming those with multi-modality information.

4.6. Visualization

Figure 5 visualizes the ﬁne-grained structures captured
by our method. The model is the FSA-Caps (1×1) model
trained on the 300W-LP dataset. The ﬁrst column shows the
estimated poses. The rests are visualizations for how some
representative features are aggregated from pixel-level fea-
tures, one column for one feature. The heatmaps are the
reshaped versions of the row vectors of Mk recovered in

testing set

method

aggregation

pixelwise scoring
model size (MB)

MAE

SSR

-
-

1.1
6.01

w/o
0.5
5.54

MAE (late fusion)

-

AFLW2000 (protocol 1)

FSA-Net

-

1×1
0.8
5.48
5.14

Capsule [36]

var.
0.8
5.41

w/o
2.9
5.75

var.
1.1
5.36

1×1
1.1
5.25
5.07

SSR

-
-

1.1
4.65

-

BIWI (protocol 1)
FSA-Net

w/o
0.5
4.61

-

1×1
0.8
4.53
4.19

Capsule [36]

var.
0.8
4.16

w/o
2.9
4.22

var.
1.1
4.28

1×1
1.1
4.31
4.00

Table 4. Ablation study for different aggregation methods (no aggregation and Capsule) and the different pixelwise scoring functions for
protocol 1. The results are the MAEs of the yaw, pitch, and roll angles. SSR denotes SSR-Net-MD [45].

testing set

method

aggregation

pixelwise scoring
model size (MB)

MAE

MAE (late fusion)

SSR-Net-MD [45]

-
-

1.1
4.26

-

BIWI (protocol 2)

w/o
0.5
3.95

-

1×1
0.8
4.01
3.75

FSA-Net

Capsule [36]

NetVLAD [1]

var.
0.8
3.83

w/o
2.9
3.84

1×1
1.1
3.77
3.60

var.
1.1
3.92

w/o
0.6
3.97

var.
0.8
3.88

1×1
0.8
3.88
3.68

Table 5. Ablation study over different aggregation methods (no aggregation, Capsule and NetVLAD) and the different pixelwise scoring
functions under protocol 2. The results are the MAEs of the yaw, pitch, and roll angles.

(a) AFLW2000 (protocol 1)

(b) BIWI (protocol 1)

(c) BIWI (protocol 2)

Figure 6. Comparisons over each angle for different testing datasets and corresponding protocols. We divide the components of FSA-
Caps-Fusion into three parts, 1×1, var., and w/o variants. The legend “mix” represents the fusion model.

aggregation methods (none, capsule, NetVLAD), and dif-
ferent pixelwise scoring functions (none, 1 × 1 convlution,
or variance). Table 4 and Table 5 report the results. Since
our method is based on SSR-Net-MD, its performance is
also listed as a reference. The results are improved by us-
ing capsule or NetVLAD as the feature aggregation. This
means that state-of-the-art aggregation methods can be nat-
urally combined with our method. Figure 6 shows detailed
comparison over the yaw, pitch and roll angles for several
settings. While a single scoring function model does not al-
ways achieve good results, the fusion ensemble model guar-
antees the best result in every case, showing that comple-
mentary information is learned in different model variants.

functions of the pixel-level features, we are able to learn
complementary model variants. Experiments show that
the ensemble of these variants outperforms the state-of-the-
art methods (both landmark-based and landmark-free ones)
while its model size is around 100× smaller than those of
previous methods. Furthermore, its estimation on the yaw
angle is even more accurate than those methods with multi-
modality information such as the RGB-D or RGB-Time re-
current model. We show that it is possible to improve re-
gression results by learning meaningful intermediate fea-
tures. Although we only demonstrate on the pose estima-
tion problem, we believe that the idea can be extended to
other regression problems as well.

5. Conclusion

In this paper, we propose a new way to acquire more
meaningful aggregated features with the ﬁne-grained spatial
structures. By deﬁning learnable and non-learnable scoring

Acknowledgement This work was supported in part by
Ministry of Science and Technology (MOST) under grants
107-2628-E-001-005-MY3 and 108-2634-F-007-009, and
MOST Joint Research Center for AI Technology and All
Vista Healthcare under grant 108-2634-F-002-004.

1094

yawpitchrollAngles4.55.05.56.06.5Mean Absolute Error (MAE)1x1var.w/omixyawpitchrollAngles3.03.54.04.55.05.5Mean Absolute Error (MAE)1x1var.w/omixyawpitchrollAngles3.003.253.503.754.004.254.50Mean Absolute Error (MAE)1x1var.w/omixReferences

[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. NetVLAD: CNN architecture for
weakly supervised place recognition.
In Proceedings of
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5297–5307, 2016.

[2] Relja Arandjelovic and Andrew Zisserman. All about
VLAD. In Proceedings of Conference on Computer Vision
and Pattern Recognition (CVPR), 2013.

[3] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2D & 3D face alignment problem? (and a
dataset of 230,000 3D facial landmarks). In Proceedings of
International Conference on Computer Vision (ICCV), 2017.
[4] Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, and
Chen Change Loy. Pose-robust face recognition via deep
residual equivariant mapping. In Proceedings of Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
[5] Xudong Cao, Yichen Wei, Fang Wen, and Jian Sun. Face
alignment by explicit shape regression. International Jour-
nal of Computer Vision (IJCV), 107(2):177–190, 2014.

[6] Feng Ju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi,
Ram Nevatia, and Gerard Medioni. FacePoseNet: Making
a case for landmark-free face alignment.
In Proceedings
of Conference on Computer Vision and Pattern Recognition
Workshops (CVPR Workshops), 2017.

[7] Dong Chen, Shaoqing Ren, Yichen Wei, Xudong Cao, and
Jian Sun. Joint cascade face detection and alignment.
In
Proceedings of European Conference on Computer Vision
(ECCV), 2014.

[8] Eunji Chong, Nataniel Ruiz, Yongxin Wang, Yun Zhang,
Agata Rozga, and James M. Rehg. Connecting gaze, scene,
and attention: Generalized attention estimation via joint
modeling of gaze and scene saliency. In Proceedings of Eu-
ropean Conference on Computer Vision (ECCV), 2018.

[9] Grigorios G. Chrysos, Epameinondas Antonakos, Patrick
Snape, Akshay Asthana, and Stefanos Zafeiriou. A com-
prehensive performance evaluation of deformable face track-
ing “in-the-wild”. International Journal of Computer Vision
(IJCV), 126(2-4):198–232, 2018.

[10] Timothy F. Cootes, Christopher J. Taylor, David H. Cooper,
and Jim Graham. Active shape models-their training and
application. Computer Vision and Image Understanding,
61(1):38–59, 1995.

[11] Daniel F. DeMenthon and Larry S. Davis. Model-based ob-
ject pose in 25 lines of code. In Proceedings of European
Conference on Computer Vision (ECCV), 1992.

[12] Piotr Doll´ar, Peter Welinder, and Pietro Perona. Cascaded
pose regression. In Proceedings of Conference on Computer
Vision and Pattern Recognition (CVPR), 2010.

[13] Gabriele Fanelli, Matthias Dantone, Juergen Gall, Andrea
Fossati, and Luc Van Gool. Random forests for real time
3D face analysis. International Journal of Computer Vision
(IJCV), 101(3):437–458, 2013.

[14] Gabriele Fanelli, Thibaut Weise, Juergen Gall, and Luc
Van Gool. Real time head pose estimation from consumer
depth cameras.
In Joint Pattern Recognition Symposium,
pages 101–110. Springer, 2011.

[15] Rohit Girdhar and Deva Ramanan. Attentional pooling for
In Proceedings of Neural Information

action recognition.
Processing Systems Conference (NIPS), 2017.

[16] Jinwei Gu, Xiaodong Yang, Shalini De Mello, and Jan
Kautz. Dynamic facial analysis: From Bayesian ﬁltering to
recurrent neural network. In Proceedings of Conference on
Computer Vision and Pattern Recognition (CVPR), 2017.

[17] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuan. Deepco3:
Deep instance co-segmentation by co-peak search and co-
saliency detection. In Proceedings of Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2019.

[18] Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Xiaoning
Qian, and Yung-Yu Chuang. Unsupervised CNN-based co-
saliency detection with graphical optimization. In Proceed-
ings of European Conference on Computer Vision (ECCV),
2018.

[19] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing.
Unsupervised video object segmentation using motion
saliency-guided spatio-temporal propagation.
In Proceed-
ings of European Conference on Computer Vision (ECCV),
2018.

[20] Vahid Kazemi and Josephine Sullivan. One millisecond face
alignment with an ensemble of regression trees. In Proceed-
ings of Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2014.

[21] Martin Koestinger, Paul Wohlhart, Peter M. Roth, and Horst
Bischof. Annotated facial landmarks in the wild: A large-
scale, real-world database for facial landmark localization.
In Proceedings of International Conference on Computer Vi-
sion Woskshops, 2011.

[22] Amit Kumar, Azadeh Alavi, and Rama Chellappa. KE-
PLER: Keypoint and pose estimation of unconstrained faces
by learning efﬁcient H-CNN regressors. In Proceedings of
the IEEE International Conference on Automatic Face and
Gesture Recognition, 2017.

[23] Donghoon Lee, Hyunsin Park, and Chang D. Yoo. Face
alignment using cascade Gaussian process regression trees.
In Proceedings of Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2015.

[24] Lin Liang, Rong Xiao, Fang Wen, and Jian Sun. Face
alignment via component-based discriminative search.
In
Proceedings of European Conference on Computer Vision
(ECCV), pages 72–85. Springer, 2008.

[25] Manuel Martin, Florian Van De Camp, and Rainer Stiefelha-
gen. Real time head model creation and head pose estimation
on consumer depth cameras. In Proceedings of The 2nd In-
ternational Conference on 3D Vision (3DV), 2014.

[26] Iain Matthews and Simon Baker. Active appearance models
revisited. International Journal of Computer Vision (IJCV),
60(2):135–164, 2004.

[27] Gregory P. Meyer, Shalini Gupta, Iuri Frosio, Dikpal Reddy,
and Jan Kautz. Robust model-based 3D head pose estima-
tion. In Proceedings of International Conference on Com-
puter Vision (ICCV), 2015.

[28] Sankha S Mukherjee and Neil Martin Robertson. Deep
head pose: Gaze-direction estimation in multimodal video.
IEEE Transactions on Multimedia (TMM), 17(11):2094–
2107, 2015.

1095

[42] Xuehan Xiong and Fernando De la Torre. Supervised descent
method and its applications to face alignment. In Proceed-
ings of Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2013.

[43] Xuehan Xiong and Fernando De la Torre. Global supervised
descent method. In Proceedings of Conference on Computer
Vision and Pattern Recognition (CVPR), pages 2664–2673,
2015.

[44] Tsun-Yi Yang, Jo-Han Hsu, Yen-Yu Lin, and Yung-Yu
Chuang. Deepcd: Learning deep complementary descriptors
for patch representations.
In Proceedings of International
Conference on Computer Vision (ICCV), 2017.

[45] Tsun-Yi Yang, Yi-Hsuan Huang, Yen-Yu Lin, Pi-Cheng
Hsiu, and Yung-Yu Chuang. SSR-Net: A compact soft stage-
wise regression network for age estimation. In Proceedings
of the International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI), 2018.

[46] Feifei Zhang, Tianzhu Zhang, Qirong Mao, and Changsheng
Xu. Joint pose and expression modeling for facial expression
recognition. In Proceedings of Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2018.

[47] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks.
IEEE Signal Processing Letters,
23(10):1499–1503, 2016.

[48] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and
Stan Z. Li. Face alignment across large poses: A 3D so-
lution.
In Proceedings of Conference on Computer Vision
and Pattern Recognition (CVPR), pages 146–155, 2016.

[49] Xiangxin Zhu and Deva Ramanan. Face detection, pose es-
timation, and landmark localization in the wild. In Proceed-
ings of Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2012.

[29] Erik Murphy-Chutorian and Mohan Manubhai Trivedi. Head
pose estimation in computer vision: A survey.
IEEE
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 31(4):607–626, 2009.

[30] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion
of visual and language representations by dense symmetric
co-attention for visual question answering. In Proceedings
of Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.

[31] Rajeev Ranjan, Vishal M. Patel, and Rama Chellappa. Hy-
perFace: A deep multi-task learning framework for face de-
tection, landmark localization, pose estimation, and gender
recognition. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 2017.

[32] Rajeev Ranjan, Swami Sankaranarayanan, Carlos D.
Castillo, and Rama Chellappa. An all-in-one convolutional
neural network for face analysis. In Proceedings of the IEEE
International Conference on Automatic Face and Gesture
Recognition, 2017.

[33] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep ex-
pectation of real and apparent age from a single image with-
out facial landmarks. International Journal of Computer Vi-
sion (IJCV), 126(2-4):144–157, 2016.

[34] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. ORB: An efﬁcient alternative to SIFT or SURF.
In Proceedings of International Conference on Computer Vi-
sion (ICCV), 2011.

[35] Nataniel Ruiz, Eunji Chong, and James M. Rehg. Fine-
grained head pose estimation without keypoints.
In Pro-
ceedings of International Conference on Computer Vision
Woskshops, 2017.

[36] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dy-
namic routing between capsules. In Proceedings of Neural
Information Processing Systems Conference (NIPS), pages
3856–3866, 2017.

[37] Christos Sagonas, Georgios Tzimiropoulos, Stefanos
Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge:
The ﬁrst facial landmark localization challenge.
In Pro-
ceedings of International Conference on Computer Vision
Woskshops, 2013.

[38] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep convolu-
tional network cascade for facial point detection. In Proceed-
ings of Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2013.

[39] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre-
sentation learning GAN for pose-invariant face recognition.
In Proceedings of Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2017.

[40] Chung-Chi Tsai, Weizhi Li, Kuang-Jui Hsu, Xiaoning Qian,
Image co-saliency detection and co-
IEEE

and Yen-Yu Lin.
segmentation via progressive joint optimization.
Transactions on Image Processing (TIP), 2019.

[41] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In
So Kweon. CBAM: Convolutional block attention module.
In Proceedings of European Conference on Computer Vision
(ECCV), 2018.

1096

