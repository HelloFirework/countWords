Representation Flow for Action Recognition

AJ Piergiovanni and Michael S. Ryoo

Department of Computer Science, Indiana University, Bloomington, IN 47408

{ajpiergi,mryoo}@indiana.edu

Abstract

In this paper, we propose a convolutional layer inspired
by optical ﬂow algorithms to learn motion representations.
Our representation ﬂow layer is a fully-differentiable layer
designed to capture the ‘ﬂow’ of any representation channel
within a convolutional neural network for action recognition.
Its parameters for iterative ﬂow optimization are learned in
an end-to-end fashion together with the other CNN model
parameters, maximizing the action recognition performance.
Furthermore, we newly introduce the concept of learning
‘ﬂow of ﬂow’ representations by stacking multiple representa-
tion ﬂow layers. We conducted extensive experimental evalu-
ations, conﬁrming its advantages over previous recognition
models using traditional optical ﬂows in both computational
speed and performance. The code is publicly available. 1

1. Introduction

Activity recognition is an important problem in computer
vision with many societal applications including surveillance,
robot perception, smart environment/city, and more. Use of
video convolutional neural networks (CNNs) have become
the standard method for this task, as they can learn more op-
timal representations for the problem. Two-stream networks
[20], taking both RGB frames and optical ﬂow as input, pro-
vide state-of-the-art results and have been extremely popular.
3-D spatio-temporal CNN models, e.g., I3D [3], with XYT
convolutions also found that such two-stream design (RGB
+ optical ﬂow) increases their accuracy. Abstracting both
appearance information and explicit motion ﬂow beneﬁts the
recognition.

However, optical ﬂow is expensive to compute. It often
requires hundreds of optimization iterations every frame, and
causes learning of two separate CNN streams (i.e., RGB-
stream and ﬂow-stream). This requires signiﬁcant computa-
tion cost and a great increase in the number of model param-
eters to learn. Further, this means that the model needs to
compute optical ﬂow every frame even during inference and

1Code/models available here: https://piergiaj.github.io/rep-ﬂow-site/

Figure 1: Comparing the results of (b) TVL-1 and (c) our
learned ﬂow when applied to RGB images. Our layer is able
to capture similar motion information to TVL-1. However,
compared to TVL-1, our layer is faster, is learnable, and
can be applied directly to any intermediate CNN feature
maps. With the representation ﬂow layer, optical ﬂow pre-
extraction is no longer needed and a single-stream CNN
design becomes possible.

run two parallel CNNs, limiting its real-time applications.

There were previous works to learn representations captur-
ing motion information without using optical ﬂow as input,
such as motion feature networks [15] and ActionFlowNet
[16]. However, although they were more advantageous in
terms of the number of model parameters and computation
speed, they suffered from inferior performance compared
to two-stream models on public datasets such as Kinetics
[13] and HMDB [14]. We hypothesize that the iterative op-
timization performed by optical ﬂow methods produces an
important feature that other methods fail to capture.

In this paper, we propose a CNN layer inspired by optical
ﬂow algorithms to learn motion representations for action
recognition without having to compute optical ﬂow. Our rep-
resentation ﬂow layer is a fully-differentiable layer designed
to capture ‘ﬂow’ of any representation channels within the
model. Its parameters for iterative ﬂow optimization are
learned together with other model parameters, maximizing
the action recognition performance. This is also done with-
out having/training multiple network streams, reducing the
number of parameters in the model. Further, we newly intro-
duce the concept of learning ‘ﬂow of ﬂow’ representations
by stacking multiple representation ﬂow layers. We conduct

19945

extensive action classiﬁcation experimental evaluation of
where to compute optical ﬂow and various hyperparameters,
learning parameters, and fusion techniques.

Our contribution is the introduction of a new differen-
tiable CNN layer that unrolls the iterations of the TV-L1
optical ﬂow method. This allows for learning of the optical
ﬂow parameters, application to any CNN feature maps (i.e.,
intermediate representations), and lower computational cost
while maintaining performance.

2. Related Works

Capturing motion and temporal information has been stud-
ied for activity recognition. Early, hand-crafted approaches
such as dense trajectories [24] captured motion information
by tracking points through time. Many algorithms have been
developed to compute optical ﬂow as a way to capture mo-
tion in video [8]. Other works have explored learning the
ordering of frames to summarize a video in a single ‘dynamic
image’ used for activity recognition [1].

Convolutional neural networks (CNNs) have been applied
to activity recognition. Initial approaches explored methods
to combine temporal information based on pooling or tem-
poral convolution [12, 17]. Other works have explored using
attention to capture sub-events of activities [18]. Two-stream
networks have been very popular: they take input of a single
RGB frame (captures appearance information) and a stack
of optical ﬂow frames (captures motion information). Often,
the two network streams of the model are separately trained
and the ﬁnal predictions are averaged together [20]. There
were other two-stream CNN works exploring different ways
to ‘fuse’ or combine the motion CNN with the appearance
CNN [7, 6]. There were also large 3D XYT CNNs learn-
ing spatio-temporal patterns [26, 3], enabled by large video
datasets such as Kinetics [13]. However, these approaches
still rely on optical ﬂow input to maximize their accuracies.
While optical ﬂow is known to be an important feature,
ﬂows optimized for activity recognition are often different
from the true optical ﬂow [19], suggesting that end-to-end
learning of motion representations is beneﬁcial. Recently,
there have been works on learning such motion represen-
tations using convolutional models. Fan et al. [5] imple-
mented the TV-L1 method using deep learning libraries to
increase its computational speed and allow for learning some
parameters. The result was fed to a two-stream CNN for
the recognition. Several works explored learning a CNN
to predict optical ﬂow, which also can be used for action
recognition [4, 9, 11, 16, 21]. Lee et al. [15] shifted features
from sequential frames to capture motion in a non-iterative
fashion. Sun et al. [21] proposed an optical ﬂow guided
feature (OFF) by computing the gradients of representations
and temporal differences, but it lacked the iterative optimiza-
tion necessary for accurate ﬂow computation. Further, it
requires a three-stream model taking RGB, optical ﬂow, and

RGB differences to achieve state-of-the-art performance.

Unlike prior works, our proposed model with representa-
tion ﬂow layers relies only on RGB input, learning far fewer
parameters while correctly representing motion with the it-
erative optimization. It is signiﬁcantly faster than the video
CNNs requiring optical ﬂow input, while still performing as
good as or even better than the two-stream models. It clearly
outperforms existing motion representation methods includ-
ing TVNet [5] and OFF [21] in both speed and accuracy,
which we experimentally conﬁrm.

3. Approach

Our method is a fully-differentiable convolutional layer
inspired by optical ﬂow algorithms. Unlike traditional opti-
cal ﬂow methods, all the parameters of our method can be
learned end-to-end, maximizing action recognition perfor-
mance. Furthermore, our layer is designed to compute the
‘ﬂow’ of any representation channels, instead of limiting its
input to be traditional RGB frames.

3.1. Review of Optical Flow Methods

Before describing our layer, we brieﬂy review how optical
ﬂow is computed. Optical ﬂow methods are based on the
brightness consistency assumption. That is, given sequential
images I1, I2, a point x, y in I1 is located at x + ∆x, y + ∆y
in I2, or I1(x, y) = I2(x + ∆x, y + ∆y). These methods
assume small movements between frames, so this can be
approximated with a Taylor series: I2 = I1 + δI
δy ∆y,
where u = [∆x, ∆y]. These equations are solved for u to
obtain the ﬂow, but can only be approximated due to the two
unknowns.

δx ∆x+ δI

The standard, variational methods for approximating op-
tical ﬂow (e.g., Brox [2] and TV-L1 [27] methods) take
sequential images I1, I2 as input. Variational optical ﬂow
methods estimate the ﬂow ﬁeld, u, using an iterative opti-
mization method. The tensor u ∈ R2×W ×H is the x and y
directional ﬂow for every location in the image. Taking two
sequential images as input, I1, I2, the methods ﬁrst compute
the gradient in both x and y directions: ∇I2. The initial ﬂow
is set to 0, u = 0. Then ρ, which captures the motion resid-
ual between two frames, based on the current ﬂow estimate
u, can be computed. For efﬁciency, the constant part of ρ,
ρc is pre-computed:

ρc = I2 − ∇xI2 · ux − ∇yI2 · uy − I1

(1)

The iterative optimization is then performed, each updat-

9946

ing u:

ρ = ρc + ∇xI2 · ux + ∇yI2 · uy

u + λθ∇I2
u − λθ∇I2
u − ρ ∇I2
|I2|2

ρ < −λθ|∇I2|2
ρ > λθ|∇I2|2
otherwise

v = 


u = v + θ · divergence(p)

p =

p + τ
1 + τ

θ ∇u
θ |∇u|

(2)

(3)

(4)

(5)

Here θ controls the weight of the TV-L1 regularization
term, λ controls the smoothness of the output and τ controls
the time-step. These hyperparameters are manually set. p is
the dual vector ﬁelds, which are used to minimize the energy.
The divergence of p, or backward difference, is computed
as:

divergence(p) = px,i,j − px,i−1,j + py,i,j − py,i,j−1 (6)

where px is the x direction and py is the y direction, and p
contains all the spatial locations in the image.

The goal is to minimize the total variational energy:

E = |∇u| + λ|∇I1 ∗ u + I1 − I2|

(7)

Approaches run this iterative optimization for multiple
input scales, from small to large, and use the previous ﬂow
estimate u to warp I2 at the larger scale, providing a coarse-
to-ﬁne optical ﬂow estimation. These standard approaches
require multiple scales and warpings to obtain a good ﬂow
estimate, taking thousands of iterations.

3.2. Representation Flow Layer

Inspired by the optical ﬂow algorithm, we design a fully-
differentiable, learnable, convolutional representation ﬂow
layer by extending the general algorithm outlined above.
The main differences are that (i) we allow the layer to cap-
ture ﬂow of any CNN feature map, and that (ii) we learn
its parameters including θ, λ, and τ as well as the diver-
gence weights. We also make several key changes to reduce
computation time: (1) we only use a single scale, (2) we
do not perform any warping, and (3) we compute the ﬂow
on a CNN tensor with a smaller spatial size. Multiple scale
and warping are computationally expensive, each requiring
many iterations. By learning the ﬂow parameters, we can
eliminate the need for these additional steps. Our method is
applied on lower resolution CNN feature maps, instead of
the RGB input, and is trained in an end-to-end fashion. This
not only beneﬁts its speed, but also allows the model to learn
a motion representation optimized for activity recognition.
We note that the brightness consistency assumption can
similarly be applied to CNN feature maps. Instead of cap-
turing pixel brightness, we capture feature value consistency.

Figure 2: Illustration of our ﬂow layer. It unrolls the it-
erations of the TV-L1 algorithm as a sequence of tensor
operations, while sharing parameters across the iterations.

This same assumption holds as CNNs are designed to be spa-
tially invariant; i.e., they produce roughly the same feature
value for the same object as it moves.

Given the input F1, F2, a single channel from sequential
CNN feature maps (or input image), we compute the feature-
map-gradient by convolving the input feature maps with the
Sobel ﬁlter:

∗F2

∇F2x = 


1 0 −1
2 0 −2
1 0 −1




∗F2, ∇F2y = 


2
0

1
1
0
0
−1 −2 −1




(8)
We set u = 0, p = 0 initially, each having width and
height matching the input, then we can compute ρc = F2 −
F1. Next, following Algorithm 1, we repeatedly apply the
operations in Eqs. 2-5 for a ﬁxed number of iterations to
enable the iterative optimization. To compute the divergence,
we zero-pad p on the ﬁrst column (x-direction) or row (y-
direction) then convolve it with weights, wx, wy to compute
Eq. 6:

divergence(p) = px ∗ wx + py ∗ wy

(9)

where initially wx = (cid:2)−1 1(cid:3) and wy = (cid:20)−1

1 (cid:21). Note that

these parameters are also differentiable and can be learned
with backpropagation. We compute ∇u as

∇ux = 


1 0 −1
2 0 −2
1 0 −1


∗ux, ∇uy = 


2
0

1
1
0
0
−1 −2 −1


∗uy

(10)

Representation Flow within a CNN Algorithm 1 and
Fig. 2 describe the process of our representation ﬂow layer.
Our ﬂow layer with multiple iterations could also be inter-
preted as having a sequence of convolutional layers sharing
parameters (i.e., each blue box in Fig. 2), with each layer’s
behavior dependent on its previous layer. As a result of this
formulation, the layer becomes fully differentiable and al-
lows for the learning of all parameters, including (τ, λ, θ) and
the divergence weights (wx, wy). This enables our learned
representation ﬂow layer to be optimized for its task (i.e.,
action recognition).

9947

i=0Eq. 2Eq. 3Eq. 4Eq. 5u1u0p1p0v0⍴0i=1u2p2v1⍴1Eq. 2Eq. 3Eq. 4Eq. 5F1F2...i=n-1unpnvi⍴iEq. 2Eq. 3Eq. 4Eq. 5Figure 3: Illustration of a video-CNN with our representation ﬂow layer. The CNN computes intermediate feature maps, and
sequential feature maps are used as input to the ﬂow layer. The outputs of the ﬂow layer are used for prediction.

Algorithm 1 Method for the representation ﬂow layer

function REPRESENTATIONFLOW(F1, F2)

u = 0, p = 0
Compute image/feature map gradients (Eq. 8)
ρc = F2 − F1
for n iterations do

ρ = ρc + ∇xF2 · ux + ∇yF2 · uy

ρ < −λθ|∇F2|2
ρ > λθ|∇F2|2
otherwise

u + λθ∇F2
u − λθ∇F2
u − ρ ∇F2
|F2|2

v = 

u = v + θ · divergence(p)
p = p+ τ

θ ∇u
θ |∇u|

1+ τ

end for
return u
end function

Computing Flow-of-Flow Standard optical ﬂow algo-
rithms compute the ﬂow for two sequential images. An
optical ﬂow image contains information about the direction
and magnitude of the motion. Applying the ﬂow algorithm
directly on two ﬂow images means that we are tracking
pixels/locations showing similar motion in two consecutive
frames. In practice, this typically leads to a worse perfor-
mance due to inconsistent optical ﬂow results and non-rigid
motion. On the other hand, our representation ﬂow layer
is ‘learned’ from the data, and is able to suppress such in-
consistency and better abstract/represent motion by having

multiple regular convolutional layers between the ﬂow layers.
Fig. 6 illustrates such design, which we conﬁrm its beneﬁts
in the experiment section. By stacking multiple representa-
tion ﬂow layers, our model is able to capture longer temporal
intervals and consider locations with motion consistency.

CNN feature maps may have hundreds or thousands of
channels and our representation ﬂow layer computes the ﬂow
for each channel, which can take signiﬁcant time and mem-
ory. To address this, we apply a convolutional layer to reduce
the number of channels from C to C ′ before the ﬂow layer
(note that C ′ is still signiﬁcantly more than traditional optical
ﬂow algorithms, which were only applied to single-channel,
greyscale images). For numerical stability, we normalize
this feature map to be in [0, 255], matching standard image
values. We found that the CNN features were quite small on
average (< 0.5) and the TVL-1 algorithm default hyperpa-
rameters are designed for standard images values in [0, 255],
thus we found this normalization step important. Using the
normalized feature, we compute the ﬂow and stack the x
and y ﬂows, resulting in 2C ′ channels. Finally, we apply
another convolutional layer to convert from 2C ′ channels to
C channels. This is passed to the remaining CNN layers for
the prediction. We average predictions from many frames to
classify each video, as shown in Fig. 3.

3.3. Activity Recognition Model

We place the representation ﬂow layer inside a standard
activity recognition model taking a T × C × W × H tensor
as input to a CNN. Here, C is 3 as our model uses direct

9948

Initial Per-Frame CNNReduce ChannelsVideo framesNormalize FeaturesRemaining CNN + ClassificationDxHxWD’xHxWFlow LayerRGB frames as an input. T is the number of frames the
model processes, and W and H are the spatial dimensions.
The CNN outputs a prediction per-timestep and these are
temporally averaged to produce a probability for each class.
The model is trained to minimize cross-entropy:

Table 1: Computing the optical ﬂow representation after
various number of CNN layers. Results are video classiﬁca-
tion accuracy on our Tiny-Kinetics and LowRes-HMDB51
datasets using 100 iterations to compute the ﬂow representa-
tion.

L(v, c) = −

K

Xi

(c == i) log(pi)

(11)

where p = M (v), v is the video, the function M is the
classiﬁcation CNN and c represents which of the K classes
v belongs. That is, the parameters in our ﬂow layers are
trained together with the other layers, so that it maximizes
the ﬁnal classiﬁcation accuracy.

4. Experiments

Implementation details We implemented our represen-
tation ﬂow layer in PyTorch and our code and models are
available. As training CNNs on videos is computationally
expensive, we used a subset of the Kinetics dataset [13] with
100k videos from 150 classes: Tiny-Kinetics. This allowed
testing many models more quickly, while still having sufﬁ-
cient data to train large CNNs. For most experiments, we
used ResNet-34 [10] with input of size 16 × 112 × 112 (i.e.,
16 frames with spatial size of 112). To further reduce the
computation time for many studies, we used this smaller
input, which reduces performance, but allowed us to use
larger batch sizes and run many experiments more quickly.
Our ﬁnal models are trained on standard 224 × 224 images.
Check Appendix for speciﬁc training details.

Where to compute ﬂow? To determine where in the net-
work to compute the ﬂow, we compare applying our ﬂow
layer on the RGB input, after the ﬁrst conv. layer, and after
the each of the 5 residual blocks. The results are shown in Ta-
ble 1. We ﬁnd that computing the ﬂow on the input provides
poor performance, similar to the performance of the ﬂow-
only networks, but there is a signiﬁcant jump after even 1
layer, suggesting that computing the ﬂow of a feature is bene-
ﬁcial, capturing both the appearance and motion information.
However, after 4 layers, the performance begins to decline as
the spatial information is too abstracted/compressed (due to
pooling and large spatial receptive ﬁeld size), and sequential
features become very similar, containing less motion infor-
mation. Note that our HMDB performance in this table is
quite low compared to state-of-the-art methods due to being
trained from scratch using few frames and low spatial reso-
lution (112 × 112). For the following experiments, unless
otherwise noted, we apply the layer after the 3rd residual
block. In Fig. 7, we visualize the learned motion representa-
tions computer after block 3.

What to learn? As our method is fully differentiable, we
can learn any of the parameters, such as the kernels used

Tiny-Kinetics

LowRes-HMDB

RGB CNN
Flow CNN
Two-Stream CNN
Flow Layer on RGB Input
After Block 1
After Block 2
After Block 3
After Block 4
After Block 5

55.2
35.4
57.6
37.4
52.4
57.4
59.4
52.1
50.3

35.5
37.5
41.5
40.5
42.6
44.5
45.4
43.5
42.2

Table 2: Comparison of learning different parameters. The
ﬂow was computed after Block 3 using 100 iterations.

Tiny-Kinetics LowRes-HMDB

None (all ﬁxed)
Sobel kernels
Divergence (wx, wy)
τ, λ, θ
All
Divergence + τ, λ, θ

59.4
58.5
60.2
59.9
59.2
60.7

45.4
43.5
46.4
46.2
46.2
46.8

to compute image gradients, the kernels for the divergence
computation and even τ, λ, θ. In Table 2, we compare the
effects of learning different parameters. We ﬁnd that learning
the Sobel kernel values reduces performance due to noisy
gradients particularly when the batch size is limited, but
learning the divergence and τ, λ, θ is beneﬁcial.

How many iterations for ﬂow? To conﬁrm that the it-
erations are important and determine how many we need,
we experiment with various numbers of iterations. We
compare the number of iterations needed for both learning
(divergence+τ, λ, θ) and not learning parameters. The ﬂow
is computed after 3 residual blocks. The results are shown in
Table 3. We ﬁnd that learning provides better performance
with fewer iterations (similar to the ﬁnding in [5]), and that
iteratively computing the feature is important. We use 10 or
20 iterations in the remaining experiments as they provide
good performance and are fast.

Two-stream fusion? Two-stream CNNs fusing both RGB
and optical ﬂow features has been heavily studied [20, 7].
Based on these works, we compare various ways of fusing
RGB and our ﬂow representation, shown in Fig. 4. We
compare no fusion, late fusion (i.e., separate RGB and ﬂow

9949

Table 3: Effect of the number of iterations on our Tiny-
Kinetics dataset for learning and not learning.

Not learned Learned

1 iteration
5 iterations
10 iterations
20 iterations
50 iterations
100 iterations

46.7
51.3
52.4
53.6
59.2
59.4

49.5
55.4
59.4
60.7
60.9
60.7

Figure 5: Example (a) RGB image, (b) TVL-1 ﬂow image
and (c) TVL-1 applied twice (i.e., Flow-of-Flow). Directly
computing ﬂow-of-ﬂow results in poor input, as the inputs
of magnitude and directions do not follow the brightness
consistency assumption.

Figure 4: Different approaches to fusing RGB and ﬂow in-
formation. (a) No fusion (b) Late fusion (c) The circle repre-
sents elementwise addition/multiplication or concatenation.
We experimentally ﬁnd that no fusion performs comparably,
when applied to after 3rd residual block.

CNNs) and addition/multiplication/concatenation fusion. In
Table 4, we compare different fusion methods for different
locations in the network. We ﬁnd that fusing RGB informa-
tion is very important “when computing ﬂow directly from
RGB input”. However, it is not as beneﬁcial when com-
puting the ﬂow of representations as the CNN has already
abstracted much appearance information away. We found
that concatenation of the RGB and ﬂow features perform
poorly compared to the others. We do not use two-stream
fusion in any other experiments, as we found that computing
the representation ﬂow after the 3rd residual block provides
sufﬁcient performance even without any fusion.

Flow-of-ﬂow We can stack our layer multiple times, com-
puting the ﬂow-of-ﬂow (FoF). This has the advantage of
combining more temporal information into a single feature.
Our results are shown in Table 5. Applying the TV-L1 al-
gorithm twice gives quite poor performance, as optical ﬂow
features do not really satisfy the brightness consistency as-
sumption, as they capture magnitude and direction of motion
(shown in Fig. 5). Applying our representation ﬂow layer
twice performs signiﬁcantly better than TV-L1 twice, but still

Figure 6: Illustration of how our model computes the FoF.
Adding the intermediate conv layer allows for the smoothing
of ﬂow and conversion from magnitude+direction to feature
values. This allows a second ﬂow layer to further reﬁne the
motion feature.

Table 4: Different fusion methods for ﬂow computed at dif-
ferent locations in the network on our Tiny-Kinetics dataset
using 10 iterations with ﬂow parameter learning.

RGB 1 Block

3 Blocks

None
Late
Add
Multiply
Layer + Multiply
Concat

37.4
61.3
59.7
58.3
60.1
42.4

52.4
60.4
57.2
58.1
61.7
48.5

59.4
61.5
56.5
57.8
61.7
47.6

worse than our baseline of not doing so. However, we can
add a convolutional layer between the ﬁrst and second ﬂow
layer, ﬂow-conv-ﬂow (FcF), (Fig. 6), allowing the model to
better learn longer-term ﬂow representations. We ﬁnd this
performs best, as this intermediate layer is able to smooth
the ﬂow and produce a better input for the representation
ﬂow layer. However, we ﬁnd adding a third ﬂow layer re-
duces performance as the motion representation becomes
unreliable, due to the large spatial receptive ﬁeld size. In Fig.
7, we visualize the learned ﬂow-of-ﬂow, which is a smoother,
acceleration-like feature with abstract motion patterns.

Flow of 3D CNN Feature Since 3D convolutions capture
some temporal information, we test computing our ﬂow rep-

9950

(a)(b)(c)Table 5: Computing the FoF representation. TV-L1 twice
provides poor performance, using two ﬂow layers with a
conv. in between provides the best performance. Experi-
ments used 10 iterations and learning ﬂow parameters.

Table 6: Computing representation ﬂow using 3D ResNet-18.
We ﬁnd that even though 3D CNNs capture some tempo-
ral information, the use of the iterative representation ﬂow
further improves performance.

Tiny-Kinetics

Tiny-Kinetics

TVL-1 twice
Single Flow Layer
Flow-of-Flow
Flow-Conv-Flow (FcF)
Flow-Conv-Flow-Conv-Flow

12.2
59.4
47.2
62.3
56.5

Figure 7: Visualization of learned representation ﬂows. Note
these are after the 3rd residual block and are low-resolution
(28x28). (a) Examples of rep. ﬂow for various activities.
(b) Example of different channels capturing different mo-
tions: (left) hands (right) other motion. (c) Flow-of-ﬂow
is an acceleration-like feature with smoother, more abstract
motion patterns.

resentation on features from a 3D CNN. As 3D CNNs are
expensive to train, we follow the method of I3D [3] to in-
ﬂate a ResNet-18 pretrained on ImageNet to a 3D CNN for
videos. We also compare to the (2+1)D method of spatial
conv. followed by temporal conv from [26], which produces
a similar feature combining spatial and temporal information.
We ﬁnd our ﬂow layer increases performance even with 3D
and (2+1)D CNNs already capturing some temporal informa-
tion: Tables 6 and 7. These experiments used 10 iterations
and learning the ﬂow parameters. In these experiments, FcF
was not used.

We also compared to the OFF [21] using (2+1)D and
3D CNNs. We observe that this method does not result in
meaningful performance increases using CNNs that capture
temporal information, while our approach does.

Comparison to other motion representations We com-
pare to existing CNN-based motion representation methods
to conﬁrm the usefulness of our representation ﬂow. For
these experiments, when available, we used code provided
by the authors and otherwise implemented the methods our-
selves. To better compare to existing works, we used (16×)
224 × 224 images. Table 8 shows the results. MFNet [15]
captures motion by spatially shifting CNN feature maps,
then summing the results, TVNet [5] applies a convolutional
optical ﬂow method to RGB inputs, and ActionFlowNet

RGB 3D ResNet-18
TVL-1 3D ResNet-18
Two-Stream 3D ResNet
RGB-Only OFF [21]
Input (RGB)
After Block 1
After Block 3

54.6
37.6
57.5
54.8
38.5
58.4
59.7

Table 7: Computing representation ﬂow using (2+1)D
ResNet-18. We ﬁnd that the representation ﬂow layer is
beneﬁcial with this base network, conﬁrming it captures
features standard spatio-temporal convolution does not.

Tiny-Kinetics

RGB (2+1)D ResNet-18
TVL-1 (2+1)D ResNet-18
Two-Stream (2+1)D ResNet
RGB-Only OFF [21]
Input (RGB)
After Block 1
After Block 3

53.4
36.3
55.6
53.7
39.2
57.3
60.7

Table 8: Comparisons to other CNN-based motion repre-
sentations, using 10 iterations and learning ﬂow parameters.
This is without FcF and two-stream fusion.

Tiny-Kinetics HMDB

ActionFlownet [16]
MFNet [15]
TVNet [5]
RGB-OFF [21]
Ours

51.8
52.5
39.4
55.6
61.1

56.2
56.8
57.5
56.9
65.4

[16] trains a CNN to jointly predict optical ﬂow and activity
classes. We also compare to OFF [21] using only RGB in-
puts. Note that the HMDB performance in [21] was reported
using their three-stream model (i.e., RGB + RGB-diff + op-
tical ﬂow inputs), and here we compare to the version only
using RGB. Our method, which applies the iterative ﬂow
computation on CNN feature maps, performs the best.

Computation time We compare our representation ﬂow
to state-of-the-art two-stream approaches in terms of run-
time and number of parameters. All timings were measured

9951

(a)(b)(c)Table 9: Comparison to the state-of-the-art action classiﬁcations. ‘HMDB(+Kin)’ means that the model was pre-trained on
Kinetics before training/testing with HMDB. Missing results are due to those papers not reporting that setting. We marked the
best performances (per dataset) with bold texts. Note that all our models have a single-stream design.

Kinetics HMDB HMDB(+Kin) Run-time (ms)

2D CNNs
RGB
Flow
Two-stream
TVNet (+RGB) [5]
OFF (RGB Only) [21]
OFF (RGB + Flow + RGB Diff) [21]
Ours (2D CNN + Rep. Flow)
Ours (2D CNN + FcF)

(2+1)D CNNs
RGB R(2+1)D [23]
Two-Stream R(2+1)D [23]
Ours ((2+1)D CNN + Rep. Flow)
Ours ((2+1)D CNN + FcF)
Ours ((2+1)D CNN + FcF) + Non-local

3D CNNs
RGB S3D [26]
Two-Stream S3D [26]
I3D (RGB) [3]
I3D (Flow)
I3D (Two-Stream)
ResNet-101 + Non-local [25]

61.3
48.2
64.5

-
-
-

68.5
69.4

74.3
75.4
75.5
77.1
77.9

74.7
77.2
71.1
63.4
74.2
77.7

53.4
57.3
62.4
71.0
57.1
74.2
73.5
74.4

-
-
-
-
-

-
-

49.8
61.9
66.4

-

59.4
61.2
66.6

-
-
-

76.4
77.3

74.5
78.7
77.1
81.1
81.1

75.9

-

74.3
77.3
80.7

-

225 ±15
8039 ±140
8546 ±147
785 ±21
365 ±26
9520 ±156
524 ±24
576 ±22

471 ±18
8623 ±152
622 ±23
654 ±21
865 ±21

525 ±22
8886 ±162
594 ±23
8845 ±148
9354 ±154
3750 ±125

using a single Pascal Titan X GPU, for a batch of videos
with size 32 × 224 × 224. The ﬂow/two-stream CNNs in-
clude the time to run the TV-L1 algorithm (OpenCV GPU
version) to compute the optical ﬂow. All CNNs were based
on the ResNet-34 architecture. As also shown in Table 9,
our method is signiﬁcantly faster than two-stream models
relying on TV-L1 or other optical ﬂow methods, while per-
forming similarly or better. The number of parameters our
model has is half of its two-stream competitors (e.g., 21M
vs. 42M, in the case of 2D CNNs).

Comparison to state-of-the-arts We also compared our
action recognition accuracies with the state-of-the-arts on
Kinetics and HMDB. For this, we train our models using
32 × 224 × 224 inputs with the full kinetics dataset, using
8 V100s. We used the 2D ResNet-50 as the architecture.
Based on our experiments, we applied our representation
ﬂow layer after the 3rd residual block, learned the hyper-
parameters and divergence kernels, and used 20 iterations.
We also compare our ﬂow-of-ﬂow model. Following [22],
the evaluation is performed using a running average of the
parameters over time. Our results, shown in Table 9, conﬁrm
that this approach clearly outperforms existing models using

RGB only inputs, and is competitive against expensive two-
stream networks. Our model performs the best among those
not using optical ﬂow inputs (i.e., among the models only
taking ∼600ms per video). The models requiring optical
ﬂow were more than 10 times slower, including two-stream
versions of [3, 25, 26]

5. Conclusion

We introduced a learnable representation ﬂow layer in-
spired by optical ﬂow algorithms. We experimentally com-
pared various forms of our layer to conﬁrm that the iterative
optimization and learnable parameters are important. Our
model clearly outperformed existing methods in both speed
and accuracy on standard datasets. We also introduced the
concept of ‘ﬂow of ﬂow’ to compute longer-term motion
representations and showed it beneﬁts performance.

Acknowledgement This work was supported in part by
the National Science Foundation (IIS-1812943 and CNS-
1814985).

9952

References

[1] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould.
Dynamic image networks for action recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016. 2

[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In Proceedings of European Conference on Computer Vision
(ECCV), 2004. 2

[3] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 7, 8

[4] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), 2015. 2

[5] L. Fan, W. Huang, S. E. Chuang Gan, B. Gong, and J. Huang.
End-to-end learning of motion representation for video under-
standing. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 2, 5, 7,
8

[6] C. Feichtenhofer, A. Pinz, and R. Wildes. Spatiotemporal
residual networks for video action recognition. In Advances
in Neural Information Processing Systems (NIPS), 2016. 2

[7] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional
two-stream network fusion for video action recognition. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1933–1941, 2016. 2, 5

[8] D. Fortun, P. Bouthemy, and C. Kervrann. Optical ﬂow mod-
eling and computation: a survey. Computer Vision and Image
Understanding, 134:1–21, 2015. 2

[9] R. Gao, B. Xiong, and K. Grauman. Im2ﬂow: Motion halluci-
nation from static images for action recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 2

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
5

[11] T.-W. Hui, X. Tang, and C. C. Loy. Liteﬂownet: A lightweight
convolutional neural network for optical ﬂow estimation. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 2

[12] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convolu-
tional neural networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 1725–1732, 2014. 2

[13] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vi-
jayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al.
The kinetics human action video dataset. arXiv preprint
arXiv:1705.06950, 2017. 1, 2, 5

[14] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
HMDB: a large video database for human motion recogni-

tion. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), 2011. 1

[15] M. Lee, S. Eui Lee, S. Joon Son, G. Park, and N. Kwak.
Motion feature network: Fixed motion ﬁlter for action recog-
nition. In Proceedings of European Conference on Computer
Vision (ECCV), 2018. 1, 2, 7

[16] J. Y.-H. Ng, J. Choi, J. Neumann, and L. S. Davis. Action-
ﬂownet: Learning motion representation for action recogni-
tion. In IEEE Winter Conference on Applications of Computer
Vision (WACV). IEEE, 2018. 1, 2, 7

[17] J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici. Beyond short snippets: Deep
networks for video classiﬁcation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 4694–4702. IEEE, 2015. 2

[18] A. Piergiovanni, C. Fan, and M. S. Ryoo. Learning latent
sub-events in activity videos using temporal attention ﬁlters.
In Proceedings of the American Association for Artiﬁcial
Intelligence (AAAI), 2017. 2

[19] L. Sevilla-Lara, Y. Liao, F. Güney, V. Jampani, A. Geiger, and
M. J. Black. On the integration of optical ﬂow and action
recognition. In German Conference on Pattern Recognition,
pages 281–297. Springer, 2018. 2

[20] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In Advances in
Neural Information Processing Systems (NIPS), pages 568–
576, 2014. 1, 2, 5

[21] S. Sun, Z. Kuang, L. Sheng, W. Ouyang, and W. Zhang.
Optical ﬂow guided feature: A fast and robust motion repre-
sentation for video action recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 2, 7, 8

[22] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2818–2826, 2016. 8

[23] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and
M. Paluri. A closer look at spatiotemporal convolutions for
action recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
8

[24] H. Wang, A. Kläser, C. Schmid, and C.-L. Liu. Action recog-
nition by dense trajectories. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR),
pages 3169–3176. IEEE, 2011. 2

[25] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local
neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 8

[26] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy. Rethink-
ing spatiotemporal feature learning for video understanding.
arXiv preprint arXiv:1712.04851, 2017. 2, 7, 8

[27] C. Zach, T. Pock, and H. Bischof. A duality based approach
for realtime tv-l 1 optical ﬂow. In Joint Pattern Recognition
Symposium, pages 214–223. Springer, 2007. 2

9953

