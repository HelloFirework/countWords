Mapping, Localization and Path Planning for

Image-based Navigation using Visual Features and Map

Janine Thoma1, Danda Pani Paudel1, Ajad Chhatkuli1, Thomas Probst1, Luc Van Gool1,2

1Computer Vision Laboratory, ETH Zurich, Switzerland

2VISICS, ESAT/PSI, KU Leuven, Belgium

Abstract

Building on progress in feature representations for im-
age retrieval, image-based localization has seen a surge of
research interest. Image-based localization has the advan-
tage of being inexpensive and efﬁcient, often avoiding the
use of 3D metric maps altogether. That said, the need to
maintain a large amount of reference images as an effec-
tive support of localization in a scene, nonetheless calls for
them to be organized in a map structure of some kind.

The problem of localization often arises as part of a navi-
gation process. We are, therefore, interested in summarizing
the reference images as a set of landmarks, which meet the
requirements for image-based navigation. A contribution
of this paper is to formulate such a set of requirements for
the two sub-tasks involved: compact map construction and
accurate self localization. These requirements are then ex-
ploited for compact map representation and accurate self-
localization, using the framework of a network ﬂow prob-
lem. During this process, we formulate the map construc-
tion and self-localization problems as convex quadratic and
second-order cone programs, respectively. We evaluate our
methods on publicly available indoor and outdoor datasets,
where they outperform existing methods signiﬁcantly1.

1. Introduction

Vision-based navigation is one of the key components
of robotics, self-driving cars and many mobile applica-
tions.
It is tackled either by using a 3D map representa-
tion such as in Structure-from-Motion (SfM) based meth-
ods [11, 17, 23, 22, 29, 7] and Simultaneous Localiza-
tion and Mapping (SLAM) methods [20, 8, 6, 5, 10] or
by using a map purely represented with geo-tagged im-
ages [3, 24, 4, 12]. In contrast to SfM and SLAM-based

1Code: https://github.com/janinethoma/

methods, localization by image retrieval (or simply image-
based localization) is inexpensive, with a simple map repre-
sentation, which also scales better in larger spaces [3, 23].
The problem of image-based localization is posed as the
matching of one or more query images taken at unknown
locations to a set of reference images captured at known
locations in a map. Recent developments in learning im-
age feature representations for object and place recogni-
tion [16, 24, 4, 3] have made image retrieval a viable method
for localization. Despite the increased interest, image-based
navigation methods are largely error-prone due to match-
ing inaccuracies [1]. Some existing methods address this
by learning better feature representations for place recog-
nition [3, 24, 4, 12]. Nonetheless, errors in matches cannot
be avoided in realistic settings with changes in illumination,
camera pose and dynamic objects [1]. Methods that directly
regress poses [26, 14, 13] naturally run into similar prob-
lems. We argue that, in addition to feature representation,
the success of localization in navigation is determined by
several other key factors. In particular, current methods do
not adequately address the problem of map representation.

Many methods use a large (or even complete) reference
image set in order to localize a given query image [4, 12, 1].
Although a large reference set has a higher chance of a sim-
ilar (in pose and illumination) reference and query image
pair to exist, it not only leads to higher memory require-
ments but may also become sub-optimal for the matching
process. Another important neglected aspect in image based
localization is the order of query image sequences, which is
the key to the success of visual SLAM methods. Unlike
SLAM, localization by retrieval often works with a much
sparser sequence of query images. Exploiting information
from such interleaved image sequences is very challenging.
In this context, [19] localizes a sequence of query images
by assuming a linear change of features over time. How-
ever, this assumption is rather naive, since it fails as soon as
some objects appear (or disappear) in images. As a conse-

7383

quence, we are interested in answering the question of what
are the desired criteria of a good map representation for im-
age retrieval-based navigation? And how can we beneﬁt
from such a representation during image localization?

In this paper, we address the task of navigation on a map
where there exist geometric relationships between images
or landmarks. Given visual features of images and image
locations of the reference set, we identify three key prob-
lems: map construction by image selection, path planning,
and localization using a history of image matches for multi-
ple images. In particular, we provide new methods for map
construction and matching multiple images to the reference
images of the map. We present the construction and repre-
sentation of the map as image landmark selection from a se-
quence of images using the principles of optimal transport.
For that purpose we introduce rules that direct how images
should be selected for the map representation and derive the
costs accordingly. We model the rules as a problem of com-
puting ﬂow from source images to target images given the
image geometric locations and the visual features and solve
it using Quadratic Programming (QP). Our second contri-
bution is about the localization of multiple query images on
the map, where we model the problem as bipartite graph
matching. We solve the localization by computing a ﬂow
between the landmark images as the sources and the query
images as the targets in the bipartite graph, using Second
Order Cone Programming (SOCP). We evaluate both land-
mark image selection and localization on publicly available
indoor and outdoor datasets, and show that we signiﬁcantly
outperform the state-of-the-art.

We brieﬂy describe some relevant works on map-based
localization by image retrieval. We do not discuss the al-
ternative approach of navigation based on a pre-built 3D
map using SfM or SLAM [11, 17, 23, 7, 22, 29]. Although
both map-building and navigation play important roles in
localization by image retrieval, most research interests have
been directed towards learning better features [3, 24, 4, 12].
Nonetheless, some progress has been made on modeling
the map and matching.
In particular, [15] perform map-
building by uniformly sampling video streams for images
and improve matching by interpreting the map as a Hidden
Markov Model. However, they do not model the temporal
relation of input images in the matching process. Moreover
the uniform sampling for map-building may not be the opti-
mal approach. [28] model the query image sequence to map
sequence matching as a directed graph problem where the
temporal continuity is exploited. A similar strategy is also
pursued by [19]. However, previous works do not fully con-
sider the context of the navigation process when tackling the
problems of map-building, localization and path planning.
In the following sections, we deﬁne the preliminaries for

modeling these problems using the theory of optimal trans-
port and solve the problems based on the rules we develop.

3. Preliminaries

i

Let us consider a graph G = (V, E) with a set of vertices
V = {vi} and a set of directed edges E = {eij}i~=j . For the
edge eij ∈ E, we deﬁne the ﬂow capacity and the ﬂow cost
rate uij , cij ∈ R+, respectively. Let yij ∈ [0, uij] be the ﬂow
for eij ∈ E, such that the ﬂow of an edge is non-negative and
cannot exceed its capacity. For each vertex vi ∈ V, we deﬁne
= ∑eij yij and the total incoming
the total outgoing ﬂow y
ﬂow yi = ∑eji yji, such that the net ﬂow is yi = y
− yi
and the absolute ﬂow is ˆyi = y
+ yi. We consider two sets
S, T ⊂ V for source and target vertices respectively, such
that S ∩ T = ∅. For each source vertex vi ∈ S, we are given
the net outgoing ﬂow si ∈ R+. Similarly, ti ∈ R+ is the given
net incoming ﬂow of target vertex vi ∈ T . For the remaining
vertices, we apply the rule of conservation of ﬂows: the sum
of the ﬂows entering a vertex must equal the sum of the
ﬂows exiting a vertex. We also ensure that the ﬂow between
the sources and targets are conserved by imposing the ﬂow
constraint ∑vi∈S si = ∑vi∈T ti. Now, we wish to transport
the source ﬂows {si} to the target ﬂows {ti}, with minimal
transportation cost, by solving the following optimization
problem.

i

i

min
yij

s.t.

cijyij,

Q
eij ∈E
0 ≤ yij ≤ uij ,

∀eij ∈ E

(1)

⎧⎪⎪⎪⎪
∀vi ∈ S
si
−ti ∀vi ∈ T
⎪⎪⎪⎪⎩
0

⎨

∀vi ∈ V ∖ (S ∪ T )

The problem of (1) is convex and can be solved using
Linear Programming (LP). There exist various off-the-self
solvers [2, 25] that offer very efﬁcient LP solutions.

4. Image-based Navigation

We rely only on images and the scene topology for
all three sub-tasks of navigation—map representation, path
planning, and self localization. During these processes, vi-
sual features of images and their locations on the topolog-
ical map are considered. In the following, we provide the
exact problem setup addressed in this paper, followed by
our solutions for each of the three sub-tasks.

4.1. Problem Setup

We consider a map M ⊂ R2 and a set of images I =
{Ii}n
i=1 with their location coordinates X = {xi ∈ M}n
i=1
and visual features F = {fi}n
i=1. Using this information,
we construct a graph G = (V, E) where the set of vertices
V = {vi} represent images Ii, i = 1, . . . , n and the set of

7384

2. Related Work

yi =

directed edges E = {eij}i~=j represent pairwise relations be-
tween images Ii and Ij . Efﬁcient navigation demands a
compact representation of G, supporting path planning and
the self-localization of image sequences.

4.2. Map Representation

For a given set of vertices V = {vi}n

marize them as a set of landmarks V ′ = {v′
V ′ ⊂ V. To do so, we ﬁrst deﬁne the following measure,

i=1, we wish to sum-
i=1 such that

i}m

4.3. Path Planning

The task of path planning is to choose an ordered set of
landmarks that help to travel from a given source to a target
location along the shortest path using only the landmark im-
ages. Since the rules of map representation already ensure
a good set of landmarks, the task of path planning simply
becomes a problem of ﬁnding the shortest path along the
selected landmarks. Such a path can be found using exist-
ing methods such as Dijkstra’s algorithm.

kx = argmin
i=1,...,m

d(x, v′

i),

(2)

4.4. Self Localization

where d(x, vi) is the distance measure between x and xi of
the vertex vi. Here, kx is the index of the vertex in V ′ which
is geometrically closest to the point x. While summarizing
the landmarks, we consider the following four rules.

Rule 4.1 (Geometric Representation) Landmarks must
be well distributed geometrically, i.e. the selected land-
marks must minimize the following,

min
V ′

max
x∈M

d(x, v′

kx).

(3)

Rule 4.2 (Visual Representation) Landmarks must
be
useful for localizing images using their visual features.
More precisely, all
feature
distance to the geometrically closest landmark, i.e. for the
feature distance d(fi, fj), landmarks must also respect,

images must have a small

min
V ′

max
{x,f}∈V

d(f, f′

kx).

(4)

Rule 4.3 (Navigation Assurance) Landmarks must sup-
port navigation from any source to to any target location,
using only visual features. In other words, the next land-
mark along the path must not only be close, it must also
be distinct from the current one, to avoid confusion, i.e. if
P = {v′
l=1 ⊂ V ′ is the ordered sequence of landmarks
along a path, two consecutive landmarks must be within the
distance α such that,

l}q

d(x′

l, x′

l+1) ≤ α, ∀x′

l ∈ P,

and their visual features must be distinct such that,

max

V ′

min
l+1∈P ′

f ′
l,f ′

d(f′

l , f′

l+1).

(5)

(6)

This ensures that the navigation process can ﬁnd the next
landmark without getting confused with the previous one.

Rule 4.4 (Map Compactness) The number of landmarks
must be small, i.e.  V ′  ≤ N , for maximally N landmarks.

Landmark summarization for image-based navigation is a
multi-objective problem which favours the above four rules.

Given a sequence of images and landmarks along a path,
the task of self-localization is equivalent to ﬁnding the most
consistent match. We assume that an ordered sequence of
images Ip = {Il}q
l=1, captured along a path are given. We
wish to localize these images by matching them to land-
marks V ′. We formulate self-localization as a graph match-
ing problem between P = {pl}q
l=1 representing Ip and
i=1. Let B ∶ l → l′ be a map that generates the
V ′ = {v′
desired matching pairs {pl, v′
l=1 of sequence images and
landmark images. For the purpose of self-localization, we
want the matching process to favor the following two rules.

i}m

l′}q

Rule 4.5 (Visual Matching) The visual distance between
matched pairs must be minimized, i.e. if {fl, f′
l′} are the
visual features coming from the pair {pl, v′
l′}, the mapping
corresponding to the best matching is found as

min

B

Q

d(fl, f′

l′).

l

(7)

Rule 4.6 (Geometric Matching) Neighbours of pl must be
matched to the neighbours of v′

l′ ∈ V ′ itself. i.e.

l′ or v′

l′−1, v′
v′

l′+1 ∈ N (v′

l′) ∪ v′

l′ , ∀pl ∈ P.

(8)

5. Map Construction using Network Flow

To represent the map using only images, we deﬁne the
graph G = {V, E}, as discussed in the previous section and
as visualized in Fig. 1. Any edge eij ∈ E represents the
relationship between Ii and Ij , using the ﬂow capacity uij
and cost rate cij deﬁned as,

uij = λxd(xi, xj) and cij = λf~d(fi, fj),

(9)

for weights λx and λf associated to geometric and visual
measures. Recall that d(xi, xj) and d(fi, fj) respectively
are the geometric and visual distances between images Ii
and Ij . Here, we ﬁrst deﬁne the landmark selection process
for map representation, in the context of network ﬂow.

Deﬁnition 5.1 (Landmarks) Graph vertices with absolute
ﬂow greater than a given ﬂow threshold τ are the desired
landmarks. i.e. the landmarks are, V ′ = {vi ∶ ˆyi ≥ τ}.

7385

t1

v1

v2

v3

v4

v5

t2

s2

v6

v14

v15

v16

a1

v7

s1

v8

v10

v11

v13

a2

v9

v12

t3

v17

v19

v18

neighbourhood must have signiﬁcant absolute ﬂow. We en-
sure such ﬂow by introducing the ﬂow sensitivity ρij for
every edge eij ∈ E. The ﬂow sensitivity controls the cost
rate as ﬂow approaches capacity, such that the new cost rate
is given by,

bij = cij +

yijρij

1 − yij~uij

,

(12)

for base cost rate cij and sensitivity ρij . We deﬁne the sen-
sitivity using the feature distribution around a vertex as fol-
lows,

ρij = 1 −

d(fi, fj)

∑k∈N (xi)

d(fi, fk)

.

(13)

Figure 1. Visualization of Graph G = {V, E} for map construction
with sources si ∈ S, targets ti ∈ T , anchor points ai ∈ A, and
remaining image vertices vi ∈ V.

In the following, we make use of (9) within the formulation
of (1), with additional constraints, in order to obtain land-
marks that favour rules 4.1–4.4. We also provide the reason
behind our choice of cost rate and capacity expressed in (9).

5.1. Geometric Representation

The geometric representation Rule 4.1 is indeed the well
known k-Center Problem, which in itself is NP-hard. How-
ever, there exist simple greedy approximation algorithms of
O(n) complexity that solve the k-Center Problem with an
approximation factor of 2. We use a similar approach to
choose a set of anchor points A ⊂ M by solving,

The sensitivity encourages the ﬂow to spread before the
maximum capacity of the cheapest edge is used. This is
particularly important when a diverse set of visual features
is clustered together geometrically. In such cases, the risk is
that the ﬂow primarily passes though only one vertex thus
selecting only one landmark, since both incoming and out-
going edges offer low cost and sufﬁcient capacity. This vi-
olates the visual representation rule. In such circumstances,
sensitivity encourages the ﬂow to spread around such that
more than one landmark is selected favouring Rule 4.2.
Note that the sensitivity is high for higher feature diversity.
On the other hand, if there is only one distinct visual fea-
ture in a neighbourhood, the ﬂow sensitivity of the edge to
that vertex is very low. Using (12) and (13), the new cost
corresponding to any edge eij ∈ E can be expressed as,

yijbij = yijcij + zij, with

zij ≥

y2
ijρij

1 − yij~uij

,

(14)

min

A

 A 

s.t. d(x, A) ≤ r~2, ∀x ∈ M (10)

where the inequality is a rotated cone constraint,

for radius r and point-to-set distance d(x, A). Note that the
distance is constrained by r~2 to compensate the approxi-
mation factor of 2. Using the obtained set of anchor points,
we impose the following constraint on the absolute ﬂow to
favour the geometric representation Rule 4.1,

Q

ˆyi ≥ tg, ∀a ∈ A.

vi∈N (a)

(11)

for neighbourhood ﬂow threshold tg and neighbouring ver-
tices N (a) ⊂ V of the anchor point a within a radius r. The
constraint in (11) ensures ﬂow around every anchor point,
thus encouraging the landmarks to be well distributed. In
fact, one can alternatively maximize tg to guarantee the
feasibility of the network ﬂow problem, by adding a term
−λgtg to the original cost of (1) for a constant weight λg.

5.2. Visual Representation

The rule of visual representation demands no image to
be visually too far from its geometrically closest landmark.
Therefore, all nodes with distinct visual features in a local

uij − yijzij~(ρijuij) ≥ y2
ij.

(15)

5.3. Navigation Assurance

The formulation of network ﬂow ensures that all ﬂow
must transfer from source to sink vertices. Therefore, the
network ﬂow problem is already tuned for the navigation
task. While encouraging the ﬂow to make bigger geomet-
ric jumps, by keeping the capacity directly proportional to
the geometric distance (using (9)), we ensure that all jumps
are smaller than the navigation radius α by constructing the
graph such that,

d(xi, xj) ≤ α, ∀eij ∈ E.

(16)

Furthermore, we minimize the ﬂow between two vertices
with similar features, by keeping the cost rate inversely pro-
portional to the feature distance (using (9)). This encour-
ages the selection of distinct consecutive features along the
ﬂow path, thus favouring the objective of (6). The construc-
tion of a locally connected graph and our choice of cost rate
and capacity support the navigation assurance Rule 4.3.

7386

5.4. Map Compactness

Algorithm 1 V ′ = selectLandmarks(I, F , X , M, S, T )

Given a threshold τ on the absolute ﬂow, for a ver-
tex to qualify as a landmark, we determine a set of land-
marks V ′ = {vi ∶ ˆyi ≥ τ} by controlling source and target
ﬂows {si} and {ti}. Starting from the input/output ﬂow
YG = ∑∈S si = ∑∈T ti, we gradually increase YG to gen-
erate new landmarks as long as the ﬂow problem remains
feasible and  V ′  ≤ N , for a given upper bound on the num-
ber of landmarks N .
In this process, the most important
landmarks are generated in the beginning. Therefore, one
can further control the compactness by choosing the desired
number of initial landmarks.

5.5. Map Construction Algorithm

In the following, we present the ﬂow formulation which
builds the core of our landmark selection method and sum-
marize our graph representation to map construction pro-
cess in the form of a landmark selection algorithm.

Result 5.2 Given a graph G = {V, E} with cost rate, ca-
pacity, and sensitivity {cij, uij, ρij} for each edge eij ∈ E,
a set of anchor points A, source and target vertices S and
T , and a neighbourhood ﬂow threshold tg, the ﬂows {yij}
required for map reconstruction can be obtained by solving
the following network ﬂow problem.

min
yij ,zij

s.t.

(cij yij + zij),

Q
eij ∈E
uij − yijzij~(ρijuij) ≥ y2
ij ,
0 ≤ yij ≤ uij,

Q

ˆyi ≥ tg,

vi∈N (a)

yi =

⎧⎪⎪⎪⎪
∀vi ∈ S,
si,
−ti, ∀vi ∈ T ,
⎨
⎪⎪⎪⎪⎩
0,

∀vi ∈ V ∖ (S ∪ T ).

∀eij ∈ E,
∀eij ∈ E,
∀a ∈ A,

(17)

The ﬂow problem of (17) is convex and can be solved
optimally using Quadratic Programming (QP). In Algo-
rithm 1, we summarize the complete process of obtaining
landmarks, starting from images with features and loca-
tions. Note that the ﬂow problem needs to be solved multi-
ple times to obtain the desired compactness, as discussed in
Section 5.4. This can be done either by gradually increas-
ing the input/output ﬂow YG (as discussed earlier), or by
performing a bisection search on the parameter YG .

6. Network Flow for Self Localization

We formulate the self localization of an ordered se-
quence of images P with respect to landmarks V ′ as a bi-
partite graph matching problem. For this task, we construct

1. Construct G = {V, E} using I, F and X (ref. Sec 4.1/(16)).
2. Compute capacity uij and rate cij for all eij ∈ E, using (9).
3. Select anchor points A ⊂ M, by solving (10) for k-centers.
4. Compute the ﬂow sensitivity ρij for all eij ∈ E, using (13).
5. Solve the ﬂow problem (17) for sources S and targets T .
6. Derive ˆyi for all vi ∈ V from yij . Return, V ′ = {vi ∶ ˆyi ≥ τ }.

x′
4
v′

4

eil∈ Ep

∈E v
e si

x′
3
v′

3

x′
2
v′

2

s

x′
1
v′

1

∑i yi3x′

i

p3

e
l

t∈

∑i yi2x′

i

E

t

p2

t

∑i yi1x′

i

p1

r

Figure 2. Bipartite graph Gb with source s and target t.

i ∈ V ′ to pl ∈ P.

a complete bipartite graph Gb = {V ′, P, Ep} with directed
edges eil ∈ Ep from v′
In addition, we
introduce auxiliary source and target vertices s and t, re-
spectively. The source s is connected to all the vertices
v′
i ∈ V with directed edges esi ∈ Ev. Similarly, directed
edges elt ∈ Et connect pl ∈ P to t. Using Gb, s and t, we
represent the ﬂow network using the graph G = {V, E} with
V = s∪V ′ ∪P ∪ t and E = Ev ∪Ep ∪Et, as shown in Fig. 2. In
this section, we solve the bipartite graph matching problem
using the network ﬂow formulation of (1), with additional
constraints to obtain matches that respect Rules 4.5 and 4.6.

6.1. Visual Matching

To obtain visually similar matches, we deﬁne the ﬂow
cost rate between any landmark and a query image using
the visual distance between them. On the other hand, no
cost is added for the ﬂow from source to landmarks and
from query images to target. Furthermore, we introduce a
robust loss for feature matching such that the cost rate is
deﬁned as,

cij = h (d(fi, fj)) , ∀eij ∈ Ep; cij = 0, ∀eij ∈ E ∖ Ep,

(18)

where h(.) is the Huber loss function. To ensure that an im-
age cannot be matched to more than one landmark, we limit
the maximum absolute ﬂow at every query image to one.
This translates to the the following capacity constraints,

uij = q, ∀eij ∈ Ev; uij = 1, ∀eij ∈ E ∖ Ev.

(19)

We allow many query images to be matched to one land-
mark, by setting the source to landmark capacity higher than
one. Additionally, (19) also ensures the matching of every
query image.

7387

6.2. Geometric Matching

Algorithm 2 L = selfLocalization(V ′, Ip)

Recall that we are given only the visual features of the
query images, along with the visual features and geometric
locations of the landmarks. In this regard, our task is to infer
the geometric location of the query images. To do so, for a
given ﬂow between the landmarks and the query images, we
ﬁrst deﬁne the location of the query images as follows,

xl = Q
vi∈V ′

xiyil for pl ∈ P.

(20)

Note that
the absolute ﬂow of every query image is
∑i yil = 1. Therefore, (20) guarantees that the query im-
age lies within the convex polytope deﬁned by landmark
locations. Now, the geometric matching rule of 4.6 for nav-
igation radius r and sequential query image pair {pl, pl+1}
can be expressed as the following quadratic constraint,

XXXXXXXXXXX

Q
vi∈V ′

xiyi(l+1) − Q
vi∈V ′

xiyil

XXXXXXXXXXX

≤ r, ∀pl, pl+1 ∈ P.

(21)

6.3. Self Localization Algorithm

We perform self localization by performing bipartite
graph matching, using network ﬂow. In the following, we
ﬁrst present the proposed network ﬂow formulation for self
localization, as one of our results. Subsequently, we sum-
marize our self localization method as an algorithm.
Result 6.1 Consider a graph G = {V, E} constructed us-
ing vertices P = {pl}q
l=1 (representing a sequence of im-
ages Ip) and landmarks V ′ = {v′
i=1 at locations {xi}m
i=1
(as shown in Fig. 2), with cost rate cij and capacity uij de-
ﬁned using (18)–(19). Given navigation radius r and source
and target vertices {s, t}, the ﬂows {yij} required for self
localization can be obtained by solving the following ﬂow
problem.

i}m

min
yij

cijyij ,

Q
eij ∈E
0 ≤ yij ≤ uij,
ys = q, yt = −q, yi = 0,

∀eij ∈ E,

XXXXXXXXXXX

Q
vi∈V ′

xiyi(l+1) − Q
vi∈V ′

xiyil

∀vi ∈ V ∖ (s ∪ t),

XXXXXXXXXXX

≤ r, ∀pl, pl+1 ∈ P.

(22)

The ﬂow problem of (22) is convex and can be solved opti-
mally using Second Order Cone Programming (SOCP). We
use the solution of SOCP in (20) to obtain the location xl of
query image Il represented by vertex pl ∈ P. The proposed
localization method is summarized in Algorithm 2.

7. Experiments

We conduct experiments on two publicly available real
world datasets, COLD-Freiburg [21] and the Oxford Robot-
car [18] database. The following paragraphs describe how

1. Construct G = {V, E} using V ′ and Ip (ref. Fig. 2).
2. Compute rates {cij} and capacities {uij}, using (18)–(19).
3. Obtain ﬂows {yij} by solving the ﬂow problem (22).
4. Compute the location xl using (20). Return, L = {xl}q

l=1.

we obtain the location coordinates X , visual features F and
the edges E, as introduced in Section 4.1.
Location Coordinates. The COLD-Freiburg sequences
directly provide location coordinates X . For the Oxford
Robotcar dataset we use UTM coordinates, i.e. northing and
easting. We exclude any sequences with inaccurate or in-
complete GPS and INS trajectories. Given the large size of
the Oxford Robotcar dataset and the limitation in download
speed for public users, we limit ourselves to a randomly se-
lected subset of sequences and only look at roughly the ﬁrst
1250m of each run.
Visual Features. We use two different types of image fea-
tures F = {fi}n
i=1. The ﬁrst type are VGG16 [24] based
off-the-shelf NetVLAD [4] features with PCA and whiten-
ing, where the VGG16 layers were pretrained on ImageNet
[9] and the NetVLAD weights are computed using 30’000
images from Pittsburgh 250k [27]. The second type of fea-
tures are simply the output of the last VGG16 fully con-
nected layer using the weights from [24]. The resulting fea-
ture vectors have length 4096 for NetVLAD and 1000 for
VGG16 FC3.
Edges. For the COLD-Freiburg dataset, we look at any con-
nection between images that are less than 2m apart. If the
connection does not intersect with any walls on the given
ﬂoor plan, we add it to E. For the Oxford Robotcar dataset,
we add a connection between two images to E if the distance
between the images is smaller than a threshold of 12m. To
avoid edges that cut corners, we use the geodesic distance.

7.1. Landmark Selection

We validate our map construction approach, introduced
in Section 5, on real world data. Starting from 4853 images
from the ﬁrst 1.25km of a rainy Oxford Robotcar sequence
(2015-10-29 12:18:17), we build a reference summary V ′
with  V ′  = 250. For a total of ﬁve different setups, Fig. 3
shows the distribution of feature distance d(f, v′
kx) and ge-
ometric distance d(x, v′
kx) of the points in the original set V
to the geometrically nearest neighbour v′
(2) in the sum-
kx
marized set V ′. First we study the baseline setup obtained
by sampling uniformly along the path of the captured ref-
erence sequence. Then we analyze our approach. To il-
lustrate the impact of anchors (Section 5.1) and sensitivity
(Section 5.2) on geometric and visual representation, we se-
lectively switch off these two constraints.

The results in Fig. 3 clearly show that a reference sum-
marized using network ﬂow has better geometric and visual

7388

Figure 3. Distribution of normalized feature distance and geometric distance of points in the full reference set V to the geometric nearest
neighbour in the summarized reference set V ′, for the following settings from left to right: 1) Reference images sampled uniformly along
sequence path. 2) Network ﬂow, without imposing geometric representation (anchors) and visual representation (sensitivity). 3) Network
ﬂow without sensitivity. 4) Network ﬂow without anchors. 5) Network ﬂow imposing geometric and visual representation (our method).

Figure 4. Left to right: Initial top-1 matches between a rainy uniformly sampled reference and an overcast query sequence of the Oxford
Robotcar dataset. Top-1 matches reﬁned by applying our self localization algorithm. Matches obtained with our method and with SeqS-
LAM shown on the visual distance matrix. Localization accuracy for a given distance threshold for top-1 & top-5 localization without
sequence information, SeqSLAM and our localization method.

representation than a uniformly sampled baseline represen-
tation with the same number of images. We observe that
introducing anchors reduces the number of points with high
geometric distance and introducing sensitivity reduces the
number of points with high feature distance.

7.2. Self Localization

In this section, we illustrate the feasibility of our self lo-
calization algorithm. As a reference set, we take 600 images
from the same rainy Oxford Robotcar sequence (2015-10-
29 12:18:17) as in Section 7.1. We use uniform sampling as
the baseline for summarizing the reference set. As a query
sequence, we uniformly sample 125 images from an over-
cast Oxford Robotcar sequence (2015-02-13 09:16:26) us-
ing a step size of 20 images. The leftmost subplot in Fig. 4
illustrates the unreﬁned top-1 feature matches between the
query sequence images and the reference images. The sec-
ond subplot in Fig. 4 shows the top-1 feature matches after
applying our self localization algorithm. It is evident that
our approach greatly improves the localization for this ex-
ample by removing inconsistent matches. The third subplot
in Fig. 4 shows the visual distance matrix between query
sequence and reference set (ordered according to topology,
i.e. the originally driven route). It shows that matches do
not occur independently and that the neighbours of match-
ing images also have low feature distance. The true matches
(i.e. the matches with smallest geometric distance) are indi-

cated in black. In red, we plot the reﬁned matches of our
self-localization algorithm. As a comparison, in green we
show the matches produced by SeqSLAM [19], the state of
the art for capitalizing on sequential information to improve
image matches. Finally, the rightmost subplot in Fig. 4 in-
dicates the percentage of correctly localized images for any
given distance threshold. As an example, for a tolerance of
80m, our method has an accuracy of 68.7%, while SeqS-
LAM reaches 60.9%.

7.3. Quantitative Evaluation

We provide quantitative evaluation on the COLD-
Freiburg and the Oxford Robotcar dataset. For both datasets
we randomly choose one reference and three query se-
quences. For the Oxford Robotcar dataset, the reference
is the rainy sequence from 2015-10-29 12:18:17. The query
sequences are taken in three different conditions: Sun and
clouds (2014-11-18 13:20:12), snow (2015-02-03 08:45:10)
and overcast (2015-02-13 09:16:26). From the COLD-
Freiburg dataset we use the second sunny sequence of the
extended part A as a reference. As query sequences, we
use the ﬁrst sunny, cloudy and night sequences taken on the
extended part A.

Fig. 5 plots the percentage of correctly localized query
images for a given distance threshold for each of the six dif-
ferent query sequences. The number of images in the ref-
erence set are 415 for Oxford Robotcar and 50 for COLD-

7389

050.10.20.30.40.5Feature distance1)050.10.20.30.40.52)05Geometric distance [m]0.10.20.30.40.53)050.10.20.30.40.54)050.10.20.30.40.55)020406080Number of images-200-1000100200300400-400-300-200-1000100200300-40-200100150200250Query image locationsReference image locationsTop-1 matched imagesx-location [m]y-location [m]-200-1000100200300400-400-300-200-1000100200300-40-200100150200250Query image locationsReference image locationsTop-1 matched imagesx-location [m]y-location [m]10020030040050060020406080100120Top-1 refinedTop-1 SeqSLAMGround truthReference indexQuery index0100200300400500600020406080100Top-5 initialTop-1 refinedTop-1 SeqSLAMTop-1 initialAccuracy [%]Distance [m]D
A
L
V
t
e
N

]

%

[
 
y
c
a
r
u
c
c
A

3
C
F
 
6
1
G
G
V

]

%

[
 
y
c
a
r
u
c
c
A

100

80

60

40

20

0

0

100

80

60

40

20

0

0

sun, clouds

50

100

150

50

100

150

100

80

60

40

20

0

0

100

80

60

40

20

0

0

overcast

50

100

150

Oxford Robotcar

snow

50

100

150

100

80

60

40

20

0

0

100

80

60

40

20

50

100
Distance [m]

150

0

0

50

100

150

100

80

60

40

20

0

0

100

80

60

40

20

0

0

cloudy

10

20

10

20

100

80

60

40

20

0

0

100

80

60

40

20

0

0

COLD

night

10

20

10

20

Distance [m]

100

80

60

40

20

0

0

100

80

60

40

20

0

0

sunny

10

20

10

20

Top-10 initial

Top-1 our map + our matching

Top-1 uniform map + SeqSLAM

Top-1 our map + SeqSlam

Top-1 uniform map + our matching

Top-1 uniform map + SeqSLAM

Top-1 initial

Figure 5. Accuracy vs. distance plot for three partial sequences from the Oxford Robotcar dataset and three full sequences from the COLD-
Freiburg dataset. Red and dark blue: Unreﬁned top-1 and top-10 matches on a uniformly summarized reference set. Black: SeqSLAM on
a uniformly summarized reference set. Purple: Our self-localization on a uniformly summarized reference set. Light blue: SeqSLAM on
our network ﬂow based map. Green: Our method, with network ﬂow based map construction and self localization.

Freiburg. The results in 5 show that, by incorporating se-
quential information, SeqSLAM clearly outperforms the
unreﬁned top-1 localization on a uniformly summarized ref-
erence set. However, the top-1 accuracy achieved by our
map building algorithm in combination with our self local-
ization is even higher. For some distance thresholds, the
top-1 accuracy of our method even beats the unreﬁned top-
10 reference. The beneﬁt of our method is especially pro-
nounced for the more challenging Oxford Robotcar dataset.
While our method shows signiﬁcant improvement on all
sequences presented in Fig. 5, it fails on sequences with
non-distinctive image features, such as the outdoor night
sequences in the Oxford Robotcar dataset. This is shown
in Fig. 6. It can be observed, that for these sequences, the
baseline method using SeqSLAM also fails. Fig. 7 shows
three examples of query images which are correctly local-
ized by our method while SeqSLAM fails. The reference
sets and query sequences are the same as the ones used for
Fig. 5. The images in Fig. 7 were matched using NetVLAD
features.

8. Conclusion

In this paper we have formulated a set of requirements
for map building and self localization in the context of
image-based navigation. Based on these requirements, we
proposed a method to perform map building by selecting the
most suitable images for navigation. To improve self local-
ization we proposed a method that can use multiple query
images. We modeled both the methods using network ﬂow
and solved them using convex quadratic and second-order
cone programs, respectively. Our experiments on challeng-
ing real world datasets show that our approach signiﬁcantly
outperforms existing methods.

Figure 6. Failure cases on Oxford Robotcar night (2014-12-16
18:44:24) and night, rain (2014-12-17 18:18:43) sequences. For
legend and description see Fig. 5.

Figure 7. Visual examples of our method correctly localizing,
while SeqSLAM baseline fails on Oxford Robotcar dataset.

Acknowledgements. This research was funded by the
EU’s Horizon 2020 programme under grant No. 687757
– REPLICATE and by the Swiss Commission for Technol-
ogy and Innovation (CTI), Grant No. 26253.1 PFES-ES –
EXASOLVED.

7390

050100150Distance [m]020406080100night050100150020406080100night, rain050100150Distance [m]020406080100night050100150020406080100night, rainNetVLADVGG16FC3Accuracy [%]QueryOurssnowsun, cloudsovercastSeqSLAMReferences

[1] Asha Anoosheh, Torsten Sattler, Radu Timofte, Marc Polle-
Night-to-day image trans-
arXiv preprint

retrieval-based localization.

feys, and Luc Van Gool.
lation for
arXiv:1809.09767, 2018. 1

[2] MOSEK ApS. The MOSEK optimization toolbox for MAT-

LAB manual. Version 7.1 (Revision 28)., 2015. 2

[3] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. Netvlad: Cnn architecture for weakly
supervised place recognition. In CVPR, pages 5297–5307,
2016. 1, 2

[4] Relja Arandjelovi´c and Andrew Zisserman. Dislocation:
Scalable descriptor distinctiveness for location recognition.
In ACCV, pages 188–204, 2014. 1, 2, 6

[5] Guillaume Bresson, Zayed Alsayed, Li Yu, and S´ebastien
Glaser. Simultaneous localization and mapping: A survey of
current trends in autonomous driving. IEEE Transactions on
Intelligent Vehicles, pages 194–220, 2017. 1

[6] Robert Castle, Georg Klein, and David W Murray. Video-
rate localization in multiple maps for wearable augmented
reality.
In 2008 12th IEEE International Symposium on
Wearable Computers, pages 15–22. IEEE, 2008. 1

[7] Siddharth Choudhary and PJ Narayanan. Visibility probabil-
ity structure from sfm datasets and applications. In ECCV,
pages 130–143. Springer, 2012. 1, 2

[8] Andrew J Davison, Ian D Reid, Nicholas D Molton, and
Olivier Stasse. Monoslam: Real-time single camera slam.
PAMI, pages 1052–1067, 2007. 1

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248–255, 2009. 6

[10] Ethan Eade and Tom Drummond. Scalable monocular slam.

In CVPR, pages 469–476, 2006. 1

[11] Arnold Irschara, Christopher Zach, Jan-Michael Frahm, and
Horst Bischof. From structure-from-motion point clouds to
fast location recognition. In CVPR, pages 2599–2606, 2009.
1, 2

[12] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.
Learned contextual feature reweighting for image geo-
localization. In CVPR, pages 2136–2145, 2017. 1, 2

[13] Alex Kendall and Roberto Cipolla. Geometric loss functions
In CVPR,

for camera pose regression with deep learning.
pages 5974–5983, 2017. 1

[14] Alex Kendall, Matthew Grimes, and Roberto Cipolla.
Posenet: A convolutional network for real-time 6-dof camera
relocalization. In CVPR, pages 2938–2946, 2015. 1

[15] Jana Kosecka and Fayin Li. Vision based topological markov

localization. In ICRA, pages 1481–1486, 2004. 2

[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012. 1

[17] Yunpeng Li, Noah Snavely, and Daniel P Huttenlocher. Lo-
In

cation recognition using prioritized feature matching.
ECCV, pages 791–804, 2010. 1, 2

[18] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul
Newman. 1 year, 1000 km: The oxford robotcar dataset.
The International Journal of Robotics Research, pages 3–15,
2017. 6

[19] Michael J Milford and Gordon F Wyeth. Seqslam: Visual
route-based navigation for sunny summer days and stormy
winter nights. In ICRA, pages 1643–1649, 2012. 1, 2, 7

[20] Etienne Mouragnon, Maxime Lhuillier, Michel Dhome, Fa-
bien Dekeyser, and Patrick Sayd. Real time localization and
3d reconstruction. In CVPR, pages 363–370, 2006. 1

[21] Andrzej Pronobis and Barbara Caputo. COLD: COsy Lo-
calization Database. The International Journal of Robotics
Research, pages 588–594, May 2009. 6

[22] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efﬁcient
& effective prioritized matching for large-scale image-based
localization. PAMI, pages 1744–1756, 2017. 1, 2

[23] Torsten Sattler, Akihiko Torii, Josef Sivic, Marc Pollefeys,
Hajime Taira, Masatoshi Okutomi, and Tomas Pajdla. Are
large-scale 3d models really necessary for accurate visual lo-
calization? In CVPR, pages 1637–1646, 2017. 1, 2

[24] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 1, 2, 6

[25] Jos F Sturm. Using sedumi 1.02, a matlab toolbox for opti-

mization over symmetric cones, 1999. 2

[26] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea
Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-
ihiko Torii.
Indoor visual localization with dense
matching and view synthesis. In CVPR, pages 7199–7209,
2018. 1

Inloc:

[27] Akihiko Torii, Josef Sivic, Masatoshi Okutomi, and Tomas
Pajdla. Visual place recognition with repetitive structures.
PAMI, pages 1–14, 2015. 6

[28] Olga Vysotska and Cyrill Stachniss. Lazy data association
for image sequences matching under substantial appearance
changes. IEEE Robotics and Automation Letters, pages 213–
220, 2016. 2

[29] Bernhard Zeisl, Torsten Sattler, and Marc Pollefeys. Cam-
era pose voting for large-scale image-based localization. In
ICCV, pages 2704–2712, 2015. 1, 2

7391

