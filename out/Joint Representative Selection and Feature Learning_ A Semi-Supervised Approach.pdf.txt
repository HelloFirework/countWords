Joint Representative Selection and Feature Learning:

A Semi-Supervised Approach

Suchen Wang1

Jingjing Meng2

Junsong Yuan2

Yap-Peng Tan1

1Nanyang Technological University

2State University of New York at Buffalo

{wang.sc, eyptan}@ntu.edu.sg, {jmeng2, jsyuan}@buffalo.edu

Abstract

In this paper, we propose a semi-supervised approach for
representative selection, which ﬁnds a small set of repre-
sentatives that can well summarize a large data collection.
Given labeled source data and big unlabeled target data,
we aim to ﬁnd representatives in the target data, which can
not only represent and associate data points belonging to
each labeled category, but also discover novel categories in
the target data, if any. To leverage labeled source data, we
guide representative selection from labeled source to unla-
beled target. We propose a joint optimization framework
which alternately optimizes (1) representative selection in
the target data and (2) discriminative feature learning from
both the source and the target for better representative se-
lection. Experiments on image and video datasets demon-
strate that our proposed approach not only ﬁnds better rep-
resentatives, but also can discover novel categories in the
target data that are not in the source.

1. Introduction

Representative selection aims to ﬁnd a small subset of
data points that can well represent a big data collection. It
has attracted much interest in recent years due to the in-
creasing need of analyzing massive visual data and the lim-
ited capacity of our computing and storage resources.

Although the problem of representative selection has
been well studied in the literature [12, 38, 20, 9, 13, 29, 28,
7], most previous works apply unsupervised approaches.
That is, ﬁnding a set of items from the given target data
without supervision. However, in many applications, we
are not only interested in ﬁnding representative items (i.e.,
which data points are exemplars), but also interested in
knowing what they are (i.e., recognizing their categories).
In other words, although we can ﬁnd representatives and
associate remaining data samples with them based on the
similarity, we do not know the exact category of each rep-
resentative unless labels are provided.

Figure 1. Framework overview. We leverage labeled source data to
ﬁnd representatives from unlabeled target data. Once representa-
tives are found, labels can be naturally transferred from the source
to the target. Then we update features for better representative
selection. These two steps will alternate until termination.

Recently, Elhamifar et al. [8, 7] introduce an additional
source set of known items and propose to select source
items to represent the target. In this way, target items can
be easily recognized by passing category labels of source
representatives. Unfortunately, this scheme operates in a
closed world assumption, i.e., the source set knows every
category that may appear in the target. However, it may not
be the case in many real world applications.

In this paper, we take a semi-supervised approach by
likewise introducing a source set, but leverage it to ﬁnd rep-
resentative items from the target set rather than the source
set. More importantly, we do not assume that the source set
has covered all categories in the target data. We formulate
the representative selection as the facility location problem.
We incorporate labeled source data into the objective func-
tion such that they can guide the selection for new target
data. In the end, connections between the source and the
target can be formed via selected representatives, so that
we can transfer the labels from the source to the target. As
shown in Figure 1(c), we can transfer the labels of “cat”,
“dog” and “horse” to corresponding groups.

As feature representation plays a critical role in represen-
tative selection, we devise a joint optimization framework
which alternates between two steps: (1) representative se-

6005

lection and (2) discriminative feature learning. After repre-
sentatives are found, we leverage both source data and target
data to update the features based on their associations with
representatives. Subsequently, we reselect representatives
and further update the features. This procedure continues
until the termination condition is met. The entire process is
shown in Figure 1.

The proposed work has the following beneﬁts:

• It leverages the labeled source data to ﬁnd better rep-

resentatives for the target.

• The proposed formulation can discover novel cate-

gories in the target data.

• The joint representative selection and feature learning

can iteratively improve the performance.

Extensive comparisons with state-of-the-arts on two image
and two video datasets validate the above beneﬁts.

2. Related Work

In the literature, the problem of representative selec-
tion or subset selection has been well studied in many
speciﬁc applications, such as ﬁnding a subset of data to
reduce the requirement of computation and memory cost
[9, 10, 8], highlighting important shots or events in videos
[25, 13, 6, 32, 27, 39, 40, 14] and summarizing a big collec-
tion of images [34, 37, 35]. Depending on what information
needs to be preserved, the notion of representativeness dif-
fers from tasks to tasks. For instance, when selecting a sub-
set of training data to reduce the computations of the train-
ing process, the statistical property of data points should be
preserved [10, 9]. For the task of the image or video sum-
marization, the diversity and coverage of representatives are
taken into account [14, 13, 37, 26].

Representative selection methods may have different ob-
jective functions. There are several popular directions in the
literature, such as maximum spanning volume [22, 21, 39],
sparse coding [10, 25], and facility location [8]. To select a
subset of maximum volume, one common way is to apply
the determinantal point processes (DPPs) [22]. Recently,
many variants of DPPs have been proposed for various ap-
plications. For instance, k-DPPs [21] was proposed to han-
dle the ﬁxed number of representatives, Affandi et al. [1]
combine the DPPs with Markov random ﬁeld to model the
temporal dynamics, and Gong et al. [13] propose a learn-
able scheme for DPPs to select key items from video se-
quences. For sparse coding [10, 25, 40, 6], the underlying
data structures are typically assumed to be linear or sub-
spaces. Elhamifar et al. [10] propose to formulate the repre-
sentative selection as a sparse dictionary selection problem.
Meng et al. [25] propose to incorporate the locality prior

Schemes
Unsupervised
Semi-supervised [8, 7]
Ours

Source set Discover novel class Label passing

✗

✓

✓

✓

✗

✓

✗

✓

✓

Table 1. Comparison of three schemes on representative selection.

with the dictionary selection to suppress outliers. For facil-
ity location [9, 16, 6], data points with the minimum encod-
ing cost (serving cost) are selected as representatives based
on the given pairwise similarity or dissimilarity. Its objec-
tive is closely related to clustering algorithms [12, 29, 11],
which can also be applied for representative selection.

Although there are various criteria proposed for the sub-
set selection, most of them follow the property of submod-
ularity [35] and, in general, the optimization is NP-hard.
To address this issue, many efﬁcient solutions have been
studied in the literature. One feasible way is to relax the
non-convex objective function to convex and obtain the so-
lution via convex optimization [8]. Another direction is the
constant-factor approximation, such as greedy search algo-
rithms [33, 5, 24, 3].

3. Approach

3.1. Problem Statement

1, xt

i , ys

2, . . . , xt

Let T = {xt

n} be a target set of n unla-
i )}m
beled items and S = {(xs
i=1 be a source set of m
labeled items, where each item xs
i has a corresponding cat-
egory ys
i ∈ Y s. Our goal is to ﬁnd a small subset Z ⊆ T
to well represent the collection of target items, with each
representative either represents a known category from S
or a novel category in T . Let {ztt
ij} be a set of indicators,
where ztt
i and
xt
j which is 1 if xt
j and is 0 otherwise.
We also aim to ﬁnd {ztt
ij} such that each target item can be
represented by a representative of the same category.

ij ∈ {0, 1} denotes the association between xt

i is represented by xt

In this work, we do not assume that the source set has
covered all possible categories and want to discover novel
categories in the target set. If the category space of T is
Y t, we allow Y s ⊂ Y t. The comparison of our proposed
problem and previous problems is summarized in Table 1.

3.2. Preliminary

Representative selection can be formulated as the facil-
ity location problem [8]. The objective is to select known
source items to represent the target, i.e., Z ⊆ S. However,
items of unseen categories may appear in T . In this case,
no source item can well represent them. To address this
limitation, we reformulate it to ﬁnd representatives from T
instead of S by reversing the roles of the source and target
data in terms of facilities and clients.

Let dst

ij ∈ R be the cost of source item xs

i served by
target item xt
j . We quantify the serving cost as the distance
between items in the feature space. Let Fθ(x) be the feature

6006

representation of item x, where θ denote all parameters for
the representation. We can compute the serving cost dst
ij as

dst
ij = kFθ(xs

i ) − Fθ(xt

j)k2

(1)

ij ∈ {0, 1} be a binary indicator of the association
ij = 1 if xs
i
ij = 0 otherwise. The facility

Let zst
between source item xs
is represented by xt
location formulation [8] can be rewritten as

i and target xt

j , where zst

j and zst

min
{zst
ij }

m

Xi=1

zst
ij dst

ij + λ

n

Xj=1

I(kz

st
·j kp)

(2)

subject to

zst
ij = 1, ∀i,

n

Xj=1
Xj=1

n

·j = [zst

1j, . . . , zst

mj]T , k · kp is lp-norm, I(·) is the
where zst
indicator function, and λ balances the inﬂuence of serving
cost (ﬁrst term) and opening cost (second term). The con-
straint ensures every source item can be associated with one
representative. It is worth noting that Z can be directly de-
rived from non-zero zst

·j , i.e., Z = {xt

j ∈ T | zst

·j 6= 0}.

In order to well describe target set, Z should cover all
categories in T . However, problem (2) is still limited to
existing categories, since it acts to ﬁnd target items to rep-
resent S. Hence, we add target items into the client list such
that target items can also be served. Let dtt
ij be the cost of
j . We quantify dtt
target item xt
ij as

i served by xt

dtt
ij = kFθ(xt

i) − Fθ(xt

j)k2

We can then rewrite the problem as

zst
ij dst

ij +

ztt
ijdtt

ij + λLopen

min
ij },{ztt

ij }

{zst

m

Xi=1

subject to

n

Xj=1
Xj=1

n

n

n

Xi=1
Xj=1
Xj=1

n

zst
ij = 1, ∀i;

ztt
ij = 1, ∀i

Here the opening cost is

Lopen =

n

Xj=1

I(k(cid:20)zst

·j
ztt

·j(cid:21) kp)

(3)

(4)

(5)

Hence, the problem can be alternatively formulated as

min
ij },{ztt

ij }

{zst

m

n

Xi=1

Xj=1

zst
ij dst

ij +

n

n

Xi=1

Xj=1

ztt
ijdtt

ij

s.t.

n

Xj=1

zst
ij = 1, ∀i;

n

Xj=1

ztt
ij = 1, ∀i; Lopen = K

(6)

3.3. Joint Representative Selection and Feature

Learning

3.3.1 Representative Selection

Initially, we reformulate representative selection as problem
(6). Although our primary objective is to ﬁnd {ztt
ij}, the op-
timization of {zst
ij } actually beneﬁts the representative se-
lection for the target data. On one hand, source items can
provide prior knowledge about known categories. On the
other hand, labels can be transferred from the source to the
target based on {zst
ij }. However, problem (6) does not fully
exploit the given source labels. Here we incorporate the
source labels into the objective function. As we will show in
the experiments, the label information can further improve
the selection. Suppose the source set S consists of c cate-
gories. We ﬁrst partition S into c groups based on their cate-
gories, i.e., S = {Ck}c
i = k}.
Then we ﬁnd one target item to represent one entire source
group. Let dct
j . We
can compute dct

kj be the cost of group Ck served by xt
kj by

k=1, where Ck = {xs

i ∈ S | ys

dct

kj = Xi∈Ck

dst

ij = Xi∈Ck

kFθ(xs

i ) − Fθ(xt

j)k2

(7)

k=1 zct

j and zct

kj = 1 if group Ck is represented by xt

Let zct
kj ∈ {0, 1} be the association between Ck and xt
j ,
where zct
kj = 0
otherwise. Notice that different source groups are in dif-
ferent categories. Basically, no target item can represent
two source categories. To avoid this, we add a constraint
kj ≤ 1, such that each representative can only rep-
resent at most one source group. It is to be noted that this
constraint plays a crucial role in the feature learning process
(in Section 3.3.2). If two source groups are allowed to be
represented by one representative, the feature learning will
treat them as one category and then mix them up. Then we
can rewrite the problem as

Pc

c

n

n

n

·j = [ztt

where ztt
be derived as Z = {xt

1j, . . . , ztt

nj]T . The set of representatives can

j ∈ T | [zst

·j , ztt

·j]T 6= 0}.

Note that the opening loss is actually the cardinality of
the selected subset, i.e., Lopen = |Z|. In many applications,
the desired number of representatives K could be given
in advance. Then the objective of representative selection
will turn into minimize the serving cost within the budget.

n

Xk=1
Xj=1
Xk=1

c

min
kj },{ztt
ij }

{zct

subject to

6007

zct
kjdct

kj +

Xj=1

ztt
ijdtt

ij

Xj=1

zct
kj = 1, ∀k;

ztt
ij = 1, ∀i;

(8)

Xi=1
Xj=1

n

zct
kj ≤ 1, ∀j; Lopen = K,

xt
i<

∆<

ijdtt
Pj φtt
ij

(b)

(a)

(c)

(d)

Figure 2. Illustration of the discriminative term. (a) Center points.
(b) Discriminative term. (c) Discriminative points. (d) Updated
feature. Representatives are highlighted by the red circles. Com-
pared with the representatives in (a), the representatives in (c) are
more likely to differentiate two categories, and the neighbors of
representatives are more likely to be in the same category.

min
kj },{ztt
ij }

{zct

+

subject to

c

n

(zct

kjdct

kj + φct

kj[∆k − dct

kj]+)

(ztt

ijdtt

ij + φtt

ij[∆ − dtt

ij]+)

Xj=1
Xj=1

n

zct
kj = 1, ∀k;

n

Xj=1

ztt
ij = 1, ∀i;

zct
kj ≤ 1, ∀j; Lopen = K,

(11)

n

Xk=1
Xi=1
Xj=1
Xk=1

n

c

The opening cost is

n

Lopen =

Xj=1
cj]T .

where zct

·j = [zct

1j, . . . , zct

I(k(cid:20)zct

·j
ztt

·j(cid:21) kp)

Basically, when clusters are mixed up, discriminative terms
penalize center points and promote discriminative points.
But if clusters are well separated, the discriminative terms
will be zero. Then problem (11) will degrade to problem
(8).

(9)

3.3.2 Discriminative Feature Learning

In general, facility location formulation tends to select
center items in the clusters. However, cluster centers are
not the best choice when items of two categories are mixed
up. Compared with center points, there are two advantages
to select discriminative points as representatives (as shown
in Figure 2(a) and 2(c)). First, the discriminative points
are more likely to differentiate two categories. Second, the
neighbors of discriminative points are more likely to belong
to the same category. As we will show in section 3.3.2, it is
crucial for the neighbors to be in the same category as the
representative, since we will use them to update the feature
representation.

ij = 1 if xt

In order to promote the discriminative points when clus-
ters are not well separated, we include a discriminative
term. To give an intuitive understanding, we illustrate how
it works in Figure 2(b). Speciﬁcally, let φtt
ij ∈ {0, 1} be a
binary indicator, where φtt
j is the second near-
est representative of xt
ij = 0 otherwise. For a target
item xt
i, the distance to its second nearest representative can
ij . We expect this distance can be
greater than a margin ∆. Otherwise, there will be a penalty.
Likewise, let φct
j is the second near-
est representative of Ck. Our designed discriminative term
for the target data and source data can be written as

ijdtt
be expressed as Pj φtt

kj ∈ {0, 1} indicate if xt

i and φtt

n

n

[∆ −

φtt
ijdtt

ij]+ =

Xi=1
Xk=1

c

Xj=1
Xj=1

n

[∆k −

φct
kjdct

kj]+ =

n

n

Xi=1
Xk=1

c

Xj=1
Xj=1

n

φtt
ij[∆ − dtt

ij]+

(10)

φct
kj[∆k − dct

kj]+

where ∆k = |Ck|∆ and [·]+ is the hinge function. Then our
ﬁnal formulation with the discriminative terms becomes

In previous sections, we ﬁx θ to optimize the representative
selection. Here we will optimize the feature representation
Fθ(·) based on the built associations {zct

kj} and {ztt

ij}.

Most previous representative selection approaches apply
a two-stage strategy, i.e., ﬁrst extract the feature of data
points and then ﬁnd representatives. Before extracting the
feature, labeled source data can be used to ﬁne-tune θ for
better representation. However, this two-stage strategy is
not optimal since the feature learning process is indepen-
dent of the target data which may consist of novel cate-
gories.

To take the target data into consideration, we devise a
framework which can alternately ﬁnd representatives and
optimize the representation. By doing this, we can utilize
target items to update the representation, including items of
novel categories.

We optimize the feature representation by minimizing
the triplet loss [31]. Speciﬁcally, we reduce the intra-class
distances and enlarge the inter-class distances (see Figure
2(d)) so that better {ztt
ij} can be found in the next selection.
Suppose {(xa
i , xn
i )} is a set of training triplets. We up-
date θ by minimizing

i , xp

Xi (cid:2)∆ + kFθ(xa

i ) − Fθ(xp

i )k2 − kFθ(xa

i ) − Fθ(xn

i )k2(cid:3)+

(12)
The key step is how to construct training triplets. Specif-
ically, the training set consists of three parts. The ﬁrst part
is based on S by using their labels. We use the same tech-
niques as [31] to create training triplets.

The second part is constructed based on {zct

For each source item, we see it as the anchor xa
its assigned representative as the positive xp

kj} and {φct

kj}
i and see
i . Because the

6008

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
n
K
m
5
l
4
I
N
M
0
k
5
T
p
a
J
t
x
n
8
x
a
5
g
X
8
=
"
>
A
A
A
B
7
3
i
c
b
Z
A
9
T
w
J
B
E
I
b
3
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
L
S
w
x
k
Y
8
E
L
m
R
v
m
Y
M
N
e
3
v
n
7
p
w
J
u
f
A
n
b
C
w
0
x
t
a
/
Y
+
e
/
c
Y
E
r
F
H
y
T
T
Z
6
8
M
5
O
d
e
Y
N
E
C
o
O
u
+
+
0
U
N
j
a
3
t
n
e
K
u
6
W
9
/
Y
P
D
o
/
L
x
S
d
v
E
q
e
b
Q
4
r
G
M
d
T
d
g
B
q
R
Q
0
E
K
B
E
r
q
J
B
h
Y
F
E
j
r
B
5
G
Z
e
7
z
y
B
N
i
J
W
D
z
h
N
w
I
/
Y
S
I
l
Q
c
I
b
W
6
l
b
7
t
y
C
R
V
Q
f
l
i
l
t
z
F
6
L
r
4
O
V
Q
I
b
m
a
g
/
J
X
f
x
j
z
N
A
K
F
X
D
J
j
e
p
6
b
o
J
8
x
j
Y
J
L
m
J
X
6
q
Y
G
E
8
Q
k
b
Q
c
+
i
Y
h
E
Y
P
1
v
s
O
6
M
X
1
h
n
S
M
N
b
2
K
a
Q
L
9
/
d
E
x
i
J
j
p
l
F
g
O
y
O
G
Y
7
N
a
m
5
v
/
1
X
o
p
h
t
d
+
J
l
S
S
I
i
i
+
/
C
h
M
J
c
W
Y
z
o
+
n
Q
6
G
B
o
5
x
a
Y
F
w
L
u
y
v
l
Y
6
Y
Z
R
x
t
R
y
Y
b
g
r
Z
6
8
D
u
1
6
z
b
N
8
X
6
8
0
G
n
k
c
R
X
J
G
z
s
k
l
8
c
g
V
a
Z
A
7
0
i
Q
t
w
o
k
k
z
+
S
V
v
D
m
P
z
o
v
z
7
n
w
s
W
w
t
O
P
n
N
K
/
s
j
5
/
A
E
Z
o
4
9
U
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
n
K
m
5
l
4
I
N
M
0
k
5
T
p
a
J
t
x
n
8
x
a
5
g
X
8
=
"
>
A
A
A
B
7
3
i
c
b
Z
A
9
T
w
J
B
E
I
b
3
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
L
S
w
x
k
Y
8
E
L
m
R
v
m
Y
M
N
e
3
v
n
7
p
w
J
u
f
A
n
b
C
w
0
x
t
a
/
Y
+
e
/
c
Y
E
r
F
H
y
T
T
Z
6
8
M
5
O
d
e
Y
N
E
C
o
O
u
+
+
0
U
N
j
a
3
t
n
e
K
u
6
W
9
/
Y
P
D
o
/
L
x
S
d
v
E
q
e
b
Q
4
r
G
M
d
T
d
g
B
q
R
Q
0
E
K
B
E
r
q
J
B
h
Y
F
E
j
r
B
5
G
Z
e
7
z
y
B
N
i
J
W
D
z
h
N
w
I
/
Y
S
I
l
Q
c
I
b
W
6
l
b
7
t
y
C
R
V
Q
f
l
i
l
t
z
F
6
L
r
4
O
V
Q
I
b
m
a
g
/
J
X
f
x
j
z
N
A
K
F
X
D
J
j
e
p
6
b
o
J
8
x
j
Y
J
L
m
J
X
6
q
Y
G
E
8
Q
k
b
Q
c
+
i
Y
h
E
Y
P
1
v
s
O
6
M
X
1
h
n
S
M
N
b
2
K
a
Q
L
9
/
d
E
x
i
J
j
p
l
F
g
O
y
O
G
Y
7
N
a
m
5
v
/
1
X
o
p
h
t
d
+
J
l
S
S
I
i
i
+
/
C
h
M
J
c
W
Y
z
o
+
n
Q
6
G
B
o
5
x
a
Y
F
w
L
u
y
v
l
Y
6
Y
Z
R
x
t
R
y
Y
b
g
r
Z
6
8
D
u
1
6
z
b
N
8
X
6
8
0
G
n
k
c
R
X
J
G
z
s
k
l
8
c
g
V
a
Z
A
7
0
i
Q
t
w
o
k
k
z
+
S
V
v
D
m
P
z
o
v
z
7
n
w
s
W
w
t
O
P
n
N
K
/
s
j
5
/
A
E
Z
o
4
9
U
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
n
K
m
5
l
4
I
N
M
0
k
5
T
p
a
J
t
x
n
8
x
a
5
g
X
8
=
"
>
A
A
A
B
7
3
i
c
b
Z
A
9
T
w
J
B
E
I
b
3
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
L
S
w
x
k
Y
8
E
L
m
R
v
m
Y
M
N
e
3
v
n
7
p
w
J
u
f
A
n
b
C
w
0
x
t
a
/
Y
+
e
/
c
Y
E
r
F
H
y
T
T
Z
6
8
M
5
O
d
e
Y
N
E
C
o
O
u
+
+
0
U
N
j
a
3
t
n
e
K
u
6
W
9
/
Y
P
D
o
/
L
x
S
d
v
E
q
e
b
Q
4
r
G
M
d
T
d
g
B
q
R
Q
0
E
K
B
E
r
q
J
B
h
Y
F
E
j
r
B
5
G
Z
e
7
z
y
B
N
i
J
W
D
z
h
N
w
I
/
Y
S
I
l
Q
c
I
b
W
6
l
b
7
t
y
C
R
V
Q
f
l
i
l
t
z
F
6
L
r
4
O
V
Q
I
b
m
a
g
/
J
X
f
x
j
z
N
A
K
F
X
D
J
j
e
p
6
b
o
J
8
x
j
Y
J
L
m
J
X
6
q
Y
G
E
8
Q
k
b
Q
c
+
i
Y
h
E
Y
P
1
v
s
O
6
M
X
1
h
n
S
M
N
b
2
K
a
Q
L
9
/
d
E
x
i
J
j
p
l
F
g
O
y
O
G
Y
7
N
a
m
5
v
/
1
X
o
p
h
t
d
+
J
l
S
S
I
i
i
+
/
C
h
M
J
c
W
Y
z
o
+
n
Q
6
G
B
o
5
x
a
Y
F
w
L
u
y
v
l
Y
6
Y
Z
R
x
t
R
y
Y
b
g
r
Z
6
8
D
u
1
6
z
b
N
8
X
6
8
0
G
n
k
c
R
X
J
G
z
s
k
l
8
c
g
V
a
Z
A
7
0
i
Q
t
w
o
k
k
z
+
S
V
v
D
m
P
z
o
v
z
7
n
w
s
W
w
t
O
P
n
N
K
/
s
j
5
/
A
E
Z
o
4
9
U
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
n
K
m
5
l
4
I
N
M
0
k
5
T
p
a
J
t
x
n
8
x
a
5
g
X
8
=
"
>
A
A
A
B
7
3
i
c
b
Z
A
9
T
w
J
B
E
I
b
3
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
L
S
w
x
k
Y
8
E
L
m
R
v
m
Y
M
N
e
3
v
n
7
p
w
J
u
f
A
n
b
C
w
0
x
t
a
/
Y
+
e
/
c
Y
E
r
F
H
y
T
T
Z
6
8
M
5
O
d
e
Y
N
E
C
o
O
u
+
+
0
U
N
j
a
3
t
n
e
K
u
6
W
9
/
Y
P
D
o
/
L
x
S
d
v
E
q
e
b
Q
4
r
G
M
d
T
d
g
B
q
R
Q
0
E
K
B
E
r
q
J
B
h
Y
F
E
j
r
B
5
G
Z
e
7
z
y
B
N
i
J
W
D
z
h
N
w
I
/
Y
S
I
l
Q
c
I
b
W
6
l
b
7
t
y
C
R
V
Q
f
l
i
l
t
z
F
6
L
r
4
O
V
Q
I
b
m
a
g
/
J
X
f
x
j
z
N
A
K
F
X
D
J
j
e
p
6
b
o
J
8
x
j
Y
J
L
m
J
X
6
q
Y
G
E
8
Q
k
b
Q
c
+
i
Y
h
E
Y
P
1
v
s
O
6
M
X
1
h
n
S
M
N
b
2
K
a
Q
L
9
/
d
E
x
i
J
j
p
l
F
g
O
y
O
G
Y
7
N
a
m
5
v
/
1
X
o
p
h
t
d
+
J
l
S
S
I
i
i
+
/
C
h
M
J
c
W
Y
z
o
+
n
Q
6
G
B
o
5
x
a
Y
F
w
L
u
y
v
l
Y
6
Y
Z
R
x
t
R
y
Y
b
g
r
Z
6
8
D
u
1
6
z
b
N
8
X
6
8
0
G
n
k
c
R
X
J
G
z
s
k
l
8
c
g
V
a
Z
A
7
0
i
Q
t
w
o
k
k
z
+
S
V
v
D
m
P
z
o
v
z
7
n
w
s
W
w
t
O
P
n
N
K
/
s
j
5
/
A
E
Z
o
4
9
U
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
B
p
n
x
d
S
O
3
o
5
V
b
i
a
V
W
1
K
A
q
b
s
A
D
U
Y
=
"
>
A
A
A
C
D
H
i
c
b
Z
D
L
S
s
N
A
F
I
Y
n
9
V
b
r
r
e
r
S
z
W
A
r
u
C
p
J
N
7
o
s
u
H
F
Z
w
V
6
g
i
W
E
y
m
b
T
T
T
i
7
M
n
A
g
l
5
A
H
c
+
C
p
u
X
C
j
i
1
g
d
w
5
9
s
4
b
Q
N
q
6
w
8
D
H
/
8
5
h
z
P
n
9
x
L
B
F
Z
j
m
l
1
F
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
X
R
W
n
k
r
I
O
j
U
U
s
+
x
5
R
T
P
C
I
d
Y
C
D
Y
P
1
E
M
h
J
6
g
v
W
8
y
d
W
s
3
r
t
n
U
v
E
4
u
o
V
p
w
p
y
Q
D
C
M
e
c
E
p
A
W
2
6
1
V
r
d
V
G
r
p
j
b
C
c
j
7
m
Z
8
n
N
9
l
A
D
n
2
f
7
i
u
u
8
y
G
O
R
d
e
B
a
u
A
G
i
r
U
d
q
u
f
t
h
/
T
N
G
Q
R
U
E
G
U
G
l
h
m
A
k
5
G
J
H
A
q
W
F
6
x
U
8
U
S
Q
i
d
k
y
A
Y
a
I
x
I
y
5
W
T
z
Y
3
J
8
p
h
0
f
B
7
H
U
L
w
I
8
d
3
9
P
Z
C
R
U
a
h
p
6
u
j
M
k
M
F
L
L
t
Z
n
5
X
2
2
Q
Q
n
D
p
Z
D
x
K
U
m
A
R
X
S
w
K
U
o
E
h
x
r
N
k
s
M
8
l
o
y
C
m
G
g
i
V
X
P
8
V
0
x
G
R
h
I
L
O
r
6
J
D
s
J
Z
P
X
o
V
u
s
2
F
p
v
m
n
W
W
q
0
i
j
j
I
6
Q
a
f
o
H
F
n
o
A
r
X
Q
N
W
q
j
D
q
L
o
A
T
2
h
F
/
R
q
P
B
r
P
x
p
v
x
v
m
g
t
G
c
X
M
M
f
o
j
4
+
M
b
b
n
C
b
2
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
B
p
n
x
d
S
O
3
o
5
V
b
i
a
V
W
1
K
A
q
b
s
A
D
U
Y
=
"
>
A
A
A
C
D
H
i
c
b
Z
D
L
S
s
N
A
F
I
Y
n
9
V
b
r
r
e
r
S
z
W
A
r
u
C
p
J
N
7
o
s
u
H
F
Z
w
V
6
g
i
W
E
y
m
b
T
T
T
i
7
M
n
A
g
l
5
A
H
c
+
C
p
u
X
C
j
i
1
g
d
w
5
9
s
4
b
Q
N
q
6
w
8
D
H
/
8
5
h
z
P
n
9
x
L
B
F
Z
j
m
l
1
F
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
X
R
W
n
k
r
I
O
j
U
U
s
+
x
5
R
T
P
C
I
d
Y
C
D
Y
P
1
E
M
h
J
6
g
v
W
8
y
d
W
s
3
r
t
n
U
v
E
4
u
o
V
p
w
p
y
Q
D
C
M
e
c
E
p
A
W
2
6
1
V
r
d
V
G
r
p
j
b
C
c
j
7
m
Z
8
n
N
9
l
A
D
n
2
f
7
i
u
u
8
y
G
O
R
d
e
B
a
u
A
G
i
r
U
d
q
u
f
t
h
/
T
N
G
Q
R
U
E
G
U
G
l
h
m
A
k
5
G
J
H
A
q
W
F
6
x
U
8
U
S
Q
i
d
k
y
A
Y
a
I
x
I
y
5
W
T
z
Y
3
J
8
p
h
0
f
B
7
H
U
L
w
I
8
d
3
9
P
Z
C
R
U
a
h
p
6
u
j
M
k
M
F
L
L
t
Z
n
5
X
2
2
Q
Q
n
D
p
Z
D
x
K
U
m
A
R
X
S
w
K
U
o
E
h
x
r
N
k
s
M
8
l
o
y
C
m
G
g
i
V
X
P
8
V
0
x
G
R
h
I
L
O
r
6
J
D
s
J
Z
P
X
o
V
u
s
2
F
p
v
m
n
W
W
q
0
i
j
j
I
6
Q
a
f
o
H
F
n
o
A
r
X
Q
N
W
q
j
D
q
L
o
A
T
2
h
F
/
R
q
P
B
r
P
x
p
v
x
v
m
g
t
G
c
X
M
M
f
o
j
4
+
M
b
b
n
C
b
2
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
B
p
n
x
d
S
O
3
o
5
V
b
i
a
V
W
1
K
A
q
b
s
A
D
U
Y
=
"
>
A
A
A
C
D
H
i
c
b
Z
D
L
S
s
N
A
F
I
Y
n
9
V
b
r
r
e
r
S
z
W
A
r
u
C
p
J
N
7
o
s
u
H
F
Z
w
V
6
g
i
W
E
y
m
b
T
T
T
i
7
M
n
A
g
l
5
A
H
c
+
C
p
u
X
C
j
i
1
g
d
w
5
9
s
4
b
Q
N
q
6
w
8
D
H
/
8
5
h
z
P
n
9
x
L
B
F
Z
j
m
l
1
F
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
X
R
W
n
k
r
I
O
j
U
U
s
+
x
5
R
T
P
C
I
d
Y
C
D
Y
P
1
E
M
h
J
6
g
v
W
8
y
d
W
s
3
r
t
n
U
v
E
4
u
o
V
p
w
p
y
Q
D
C
M
e
c
E
p
A
W
2
6
1
V
r
d
V
G
r
p
j
b
C
c
j
7
m
Z
8
n
N
9
l
A
D
n
2
f
7
i
u
u
8
y
G
O
R
d
e
B
a
u
A
G
i
r
U
d
q
u
f
t
h
/
T
N
G
Q
R
U
E
G
U
G
l
h
m
A
k
5
G
J
H
A
q
W
F
6
x
U
8
U
S
Q
i
d
k
y
A
Y
a
I
x
I
y
5
W
T
z
Y
3
J
8
p
h
0
f
B
7
H
U
L
w
I
8
d
3
9
P
Z
C
R
U
a
h
p
6
u
j
M
k
M
F
L
L
t
Z
n
5
X
2
2
Q
Q
n
D
p
Z
D
x
K
U
m
A
R
X
S
w
K
U
o
E
h
x
r
N
k
s
M
8
l
o
y
C
m
G
g
i
V
X
P
8
V
0
x
G
R
h
I
L
O
r
6
J
D
s
J
Z
P
X
o
V
u
s
2
F
p
v
m
n
W
W
q
0
i
j
j
I
6
Q
a
f
o
H
F
n
o
A
r
X
Q
N
W
q
j
D
q
L
o
A
T
2
h
F
/
R
q
P
B
r
P
x
p
v
x
v
m
g
t
G
c
X
M
M
f
o
j
4
+
M
b
b
n
C
b
2
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
B
p
n
x
d
S
O
3
o
5
V
b
i
a
V
W
1
K
A
q
b
s
A
D
U
Y
=
"
>
A
A
A
C
D
H
i
c
b
Z
D
L
S
s
N
A
F
I
Y
n
9
V
b
r
r
e
r
S
z
W
A
r
u
C
p
J
N
7
o
s
u
H
F
Z
w
V
6
g
i
W
E
y
m
b
T
T
T
i
7
M
n
A
g
l
5
A
H
c
+
C
p
u
X
C
j
i
1
g
d
w
5
9
s
4
b
Q
N
q
6
w
8
D
H
/
8
5
h
z
P
n
9
x
L
B
F
Z
j
m
l
1
F
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
X
R
W
n
k
r
I
O
j
U
U
s
+
x
5
R
T
P
C
I
d
Y
C
D
Y
P
1
E
M
h
J
6
g
v
W
8
y
d
W
s
3
r
t
n
U
v
E
4
u
o
V
p
w
p
y
Q
D
C
M
e
c
E
p
A
W
2
6
1
V
r
d
V
G
r
p
j
b
C
c
j
7
m
Z
8
n
N
9
l
A
D
n
2
f
7
i
u
u
8
y
G
O
R
d
e
B
a
u
A
G
i
r
U
d
q
u
f
t
h
/
T
N
G
Q
R
U
E
G
U
G
l
h
m
A
k
5
G
J
H
A
q
W
F
6
x
U
8
U
S
Q
i
d
k
y
A
Y
a
I
x
I
y
5
W
T
z
Y
3
J
8
p
h
0
f
B
7
H
U
L
w
I
8
d
3
9
P
Z
C
R
U
a
h
p
6
u
j
M
k
M
F
L
L
t
Z
n
5
X
2
2
Q
Q
n
D
p
Z
D
x
K
U
m
A
R
X
S
w
K
U
o
E
h
x
r
N
k
s
M
8
l
o
y
C
m
G
g
i
V
X
P
8
V
0
x
G
R
h
I
L
O
r
6
J
D
s
J
Z
P
X
o
V
u
s
2
F
p
v
m
n
W
W
q
0
i
j
j
I
6
Q
a
f
o
H
F
n
o
A
r
X
Q
N
W
q
j
D
q
L
o
A
T
2
h
F
/
R
q
P
B
r
P
x
p
v
x
v
m
g
t
G
c
X
M
M
f
o
j
4
+
M
b
b
n
C
b
2
A
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
S
k
9
P
t
h
k
h
o
1
3
d
5
u
z
b
c
5
b
7
z
j
Y
H
F
z
Q
=
"
>
A
A
A
B
7
n
i
c
b
Z
A
9
T
w
J
B
E
I
b
n
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
G
0
t
M
5
C
O
B
k
+
w
t
e
7
B
h
b
+
+
y
O
2
c
k
F
3
6
E
j
Y
X
G
2
P
p
7
7
P
w
3
L
n
C
F
g
m
+
y
y
Z
N
3
Z
r
I
z
b
5
B
I
Y
d
B
1
v
5
3
C
x
u
b
W
9
k
5
x
t
7
S
3
f
3
B
4
V
D
4
+
a
Z
s
4
1
Y
y
3
W
C
x
j
3
Q
2
o
4
V
I
o
3
k
K
B
k
n
c
T
z
W
k
U
S
N
4
J
J
j
f
z
e
u
e
R
a
y
N
i
d
Y
/
T
h
P
s
R
H
S
k
R
C
k
b
R
W
p
3
q
0
0
A
8
Y
H
V
Q
r
r
g
1
d
y
G
y
D
l
4
O
F
c
j
V
H
J
S
/
+
s
O
Y
p
R
F
X
y
C
Q
1
p
u
e
5
C
f
o
Z
1
S
i
Y
5
L
N
S
P
z
U
8
o
W
x
C
R
7
x
n
U
d
G
I
G
z
9
b
r
D
s
j
F
9
Y
Z
k
j
D
W
9
i
k
k
C
/
f
3
R
E
Y
j
Y
6
Z
R
Y
D
s
j
i
m
O
z
W
p
u
b
/
9
V
6
K
Y
b
X
f
i
Z
U
k
i
J
X
b
P
l
R
m
E
q
C
M
Z
n
f
T
o
Z
C
c
4
Z
y
a
o
E
y
L
e
y
u
h
I
2
p
p
g
x
t
Q
i
U
b
g
r
d
6
8
j
q
0
6
z
X
P
8
l
2
9
0
m
j
k
c
R
T
h
D
M
7
h
E
j
y
4
g
g
b
c
Q
h
N
a
w
G
A
C
z
/
A
K
b
0
7
i
v
D
j
v
z
s
e
y
t
e
D
k
M
6
f
w
R
8
7
n
D
6
R
S
j
x
Y
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
S
k
9
P
t
h
k
h
o
1
3
d
5
u
z
b
c
5
b
7
z
j
Y
H
F
z
Q
=
"
>
A
A
A
B
7
n
i
c
b
Z
A
9
T
w
J
B
E
I
b
n
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
G
0
t
M
5
C
O
B
k
+
w
t
e
7
B
h
b
+
+
y
O
2
c
k
F
3
6
E
j
Y
X
G
2
P
p
7
7
P
w
3
L
n
C
F
g
m
+
y
y
Z
N
3
Z
r
I
z
b
5
B
I
Y
d
B
1
v
5
3
C
x
u
b
W
9
k
5
x
t
7
S
3
f
3
B
4
V
D
4
+
a
Z
s
4
1
Y
y
3
W
C
x
j
3
Q
2
o
4
V
I
o
3
k
K
B
k
n
c
T
z
W
k
U
S
N
4
J
J
j
f
z
e
u
e
R
a
y
N
i
d
Y
/
T
h
P
s
R
H
S
k
R
C
k
b
R
W
p
3
q
0
0
A
8
Y
H
V
Q
r
r
g
1
d
y
G
y
D
l
4
O
F
c
j
V
H
J
S
/
+
s
O
Y
p
R
F
X
y
C
Q
1
p
u
e
5
C
f
o
Z
1
S
i
Y
5
L
N
S
P
z
U
8
o
W
x
C
R
7
x
n
U
d
G
I
G
z
9
b
r
D
s
j
F
9
Y
Z
k
j
D
W
9
i
k
k
C
/
f
3
R
E
Y
j
Y
6
Z
R
Y
D
s
j
i
m
O
z
W
p
u
b
/
9
V
6
K
Y
b
X
f
i
Z
U
k
i
J
X
b
P
l
R
m
E
q
C
M
Z
n
f
T
o
Z
C
c
4
Z
y
a
o
E
y
L
e
y
u
h
I
2
p
p
g
x
t
Q
i
U
b
g
r
d
6
8
j
q
0
6
z
X
P
8
l
2
9
0
m
j
k
c
R
T
h
D
M
7
h
E
j
y
4
g
g
b
c
Q
h
N
a
w
G
A
C
z
/
A
K
b
0
7
i
v
D
j
v
z
s
e
y
t
e
D
k
M
6
f
w
R
8
7
n
D
6
R
S
j
x
Y
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
S
k
9
P
t
h
k
h
o
1
3
d
5
u
z
b
c
5
b
7
z
j
Y
H
F
z
Q
=
"
>
A
A
A
B
7
n
i
c
b
Z
A
9
T
w
J
B
E
I
b
n
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
G
0
t
M
5
C
O
B
k
+
w
t
e
7
B
h
b
+
+
y
O
2
c
k
F
3
6
E
j
Y
X
G
2
P
p
7
7
P
w
3
L
n
C
F
g
m
+
y
y
Z
N
3
Z
r
I
z
b
5
B
I
Y
d
B
1
v
5
3
C
x
u
b
W
9
k
5
x
t
7
S
3
f
3
B
4
V
D
4
+
a
Z
s
4
1
Y
y
3
W
C
x
j
3
Q
2
o
4
V
I
o
3
k
K
B
k
n
c
T
z
W
k
U
S
N
4
J
J
j
f
z
e
u
e
R
a
y
N
i
d
Y
/
T
h
P
s
R
H
S
k
R
C
k
b
R
W
p
3
q
0
0
A
8
Y
H
V
Q
r
r
g
1
d
y
G
y
D
l
4
O
F
c
j
V
H
J
S
/
+
s
O
Y
p
R
F
X
y
C
Q
1
p
u
e
5
C
f
o
Z
1
S
i
Y
5
L
N
S
P
z
U
8
o
W
x
C
R
7
x
n
U
d
G
I
G
z
9
b
r
D
s
j
F
9
Y
Z
k
j
D
W
9
i
k
k
C
/
f
3
R
E
Y
j
Y
6
Z
R
Y
D
s
j
i
m
O
z
W
p
u
b
/
9
V
6
K
Y
b
X
f
i
Z
U
k
i
J
X
b
P
l
R
m
E
q
C
M
Z
n
f
T
o
Z
C
c
4
Z
y
a
o
E
y
L
e
y
u
h
I
2
p
p
g
x
t
Q
i
U
b
g
r
d
6
8
j
q
0
6
z
X
P
8
l
2
9
0
m
j
k
c
R
T
h
D
M
7
h
E
j
y
4
g
g
b
c
Q
h
N
a
w
G
A
C
z
/
A
K
b
0
7
i
v
D
j
v
z
s
e
y
t
e
D
k
M
6
f
w
R
8
7
n
D
6
R
S
j
x
Y
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
S
k
9
P
t
h
k
h
o
1
3
d
5
u
z
b
c
5
b
7
z
j
Y
H
F
z
Q
=
"
>
A
A
A
B
7
n
i
c
b
Z
A
9
T
w
J
B
E
I
b
n
8
A
v
x
C
7
W
0
2
Q
g
m
V
u
S
O
R
k
s
S
G
0
t
M
5
C
O
B
k
+
w
t
e
7
B
h
b
+
+
y
O
2
c
k
F
3
6
E
j
Y
X
G
2
P
p
7
7
P
w
3
L
n
C
F
g
m
+
y
y
Z
N
3
Z
r
I
z
b
5
B
I
Y
d
B
1
v
5
3
C
x
u
b
W
9
k
5
x
t
7
S
3
f
3
B
4
V
D
4
+
a
Z
s
4
1
Y
y
3
W
C
x
j
3
Q
2
o
4
V
I
o
3
k
K
B
k
n
c
T
z
W
k
U
S
N
4
J
J
j
f
z
e
u
e
R
a
y
N
i
d
Y
/
T
h
P
s
R
H
S
k
R
C
k
b
R
W
p
3
q
0
0
A
8
Y
H
V
Q
r
r
g
1
d
y
G
y
D
l
4
O
F
c
j
V
H
J
S
/
+
s
O
Y
p
R
F
X
y
C
Q
1
p
u
e
5
C
f
o
Z
1
S
i
Y
5
L
N
S
P
z
U
8
o
W
x
C
R
7
x
n
U
d
G
I
G
z
9
b
r
D
s
j
F
9
Y
Z
k
j
D
W
9
i
k
k
C
/
f
3
R
E
Y
j
Y
6
Z
R
Y
D
s
j
i
m
O
z
W
p
u
b
/
9
V
6
K
Y
b
X
f
i
Z
U
k
i
J
X
b
P
l
R
m
E
q
C
M
Z
n
f
T
o
Z
C
c
4
Z
y
a
o
E
y
L
e
y
u
h
I
2
p
p
g
x
t
Q
i
U
b
g
r
d
6
8
j
q
0
6
z
X
P
8
l
2
9
0
m
j
k
c
R
T
h
D
M
7
h
E
j
y
4
g
g
b
c
Q
h
N
a
w
G
A
C
z
/
A
K
b
0
7
i
v
D
j
v
z
s
e
y
t
e
D
k
M
6
f
w
R
8
7
n
D
6
R
S
j
x
Y
=
<
/
l
a
t
e
x
i
t
>
Algorithm 1: Local Search for Solving Problem (11)

Algorithm 2: Joint Optimization

: S, T , K
Input
Output: Z, {ztt

ij}, {zct

kj}

1 Initialize Z by an arbitrary solution with |Z| = K;
2 repeat

3

4

5

6

7

8

9

10

for xt

i ∈ Z do
ﬁnd cost(Z), {ztt
for xt
j ∈ T \ Z do
Zj = Z \ {xt
ﬁnd cost(Zj );

ij}, {zct

kj} ;

i} ∪ {xt

j};

j∗ = arg maxj cost(Z) - cost(Zj ) ;
if cost(Zj ∗ ) < cost(Z) then

Z ← Zj ∗ ;

11 until convergence;

second best representative is from a different group, we see
it as the negative xn
i . For this part, anchors are source items,
while positives and negatives are both target items.

ij} and {φtt

The third part is constructed based on {ztt

ij}.
Anchors, positives and negatives are all unlabeled target
items. To obtain reliable training triplets, we only use tar-
get items near the representatives since they are more likely
to belong to the same category as the representatives. It is
to be noted that the discriminative term is necessary since
it can provide more reliable triplets for feature learning. In
this part, we treat target items as the anchor xa
i , associated
representatives as the positive xp
i , and second nearest repre-
sentative as the negative xn
i .

3.3.3 Optimization

Here we present the algorithm to solve problem (11) and the
entire joint optimization.

kj} and {ztt

In general, the optimization problem (11) is NP-hard [5].
In order to solve it efﬁciently, we adopt the local search al-
gorithm [17, 3], which is similar to the PAM algorithm [16]
in clustering. Let cost(·) denote the serving cost in the prob-
lem (11). The algorithm begins with an arbitrary feasible
subset with K items. Given the initial subset Z, we need to
ﬁnd the optimal {zct
ij} such that the serving cost
is minimal. The optimization of {zct
kj} is equivalent to the
minimum weight matching problem in the bipartite graph,
which can be solved by the Hungarian algorithm [19]. The
optimization of {ztt
ij} can be solved by ﬁnding the nearest
representative. Then for each current representative, we ﬁnd
a new candidate to replace it such that the decrease of serv-
ing cost is the largest. If no candidate can reduce the cost,
the algorithm will be terminated. We summarize the local
search algorithm in Algorithm 1. The proposed algorithm
guarantees convergence since the serving cost is monotoni-
cally reduced.

In the feature learning step, the number of triplets whose

: S, T , K, τ

Input
Output: Z, {ztt

ij}, {zct

kj}

1 repeat

2

3

ij}, {zct

kj} by using Algorithm 1;

ﬁnd Z, {ztt
create training triplets based on S, {zct
update θ by minimizing loss (12)

4
5 until # hard triplets < τ or reach maximal epoch;

kj}, {ztt

ij};

loss is greater than 0, is a direct clue to terminate our joint
optimization. We call those triplets as hard triplets. When
there are few hard triplets, further feature learning will not
bring better performance but cause oscillation (as shown in
Figure 6(b)). We denote τ as the minimal number of hard
samples controlling the termination. We will terminate the
joint optimization if the number of hard triplets is less than τ
or the maximal training epoch is reached. The entire frame-
work can be summarized in Algorithm 2.

4. Experiments

In this section, we evaluate the performance of our pro-
posed method on the task of representative selection. We
ﬁrst conduct the ablation studies to thoroughly investigate
each proposed component. For speciﬁc applications, we fo-
cus on ﬁnding key actions from video sequences.

Baselines: We compare our proposed method with the
K-medoids clustering (KM) [16], the ﬁxed-size determi-
nantal point processes (kDPP) [21] and two subspace-based
methods, sparse modeling representative selection (SMRS)
[10] and locally linear reconstruction induced sparse dic-
tionary selection (LLR-SDS) [25]. We also compare the
preliminary problem (6) (DS3A) which is modiﬁed based
on the dissimilarity-based sparse subset selection [8]. For
those baselines, we follow the two-stage strategy. That is,
we ﬁrst use the labeled source data to learn the feature rep-
resentation with the cross-entropy loss (CE) and triplet loss
(T) respectively. After the learning process, we apply those
approaches to ﬁnd representatives.

Evaluation Metrics: To evaluate the performance, we
consider two factors: (1) how many categories in the target
are found by the representatives; (2) whether data points
are accurately represented by the representatives belonging
to the same category. Correspondingly, we calculate the re-
call of categories in Z and the accuracy of the associations
between representatives and target items. If there are nc out
of n target items are correctly associated to its category, the
accuracy is computed by nc/n.

4.1. Ablation Studies

Here we perform some proof-of-concept experiments.
The experiments are conducted on two image datasets,
MNIST [23] and SCENE15 [30].

6009

(a) MNIST, Recall

(b) MNIST, Accuracy

(a) MNIST, Recall

(b) MNIST, Accuracy

(c) SCENE15, Recall

(d) SCENE15, Accuracy

(c) SCENE15, Recall

(d) SCENE15, Accuracy

Figure 3. Experimental results of our reformulations on the selec-
tion with varying number of source categories.

Figure 4. Experimental results of different optimization strategies.

Problem (6)

Problem (8)

Problem (11)

✗

✓

✓

Source labels
Discriminative term

✓
Table 2. Comparison of three proposed problems.

✗

✗

For a fair comparison, we run 25 different experiments
and report the averaged results. At each run, we construct
the source and target set by random sampling. Speciﬁ-
cally, we build the target set with 2,000 randomly selected
data points and all categories are included in it (i.e., 10 for
MNIST and 15 for SCENE15). For the source set, we ran-
domly select 100 data samples per category and vary the
number of categories c′ within {4, 5, 6, 7, 8} for MNIST
and {6, 8, 10, 12, 14} for SCENE15. Here we mainly in-
vestigate the performance with respect to source categories,
the number of representatives is simply set to 10 for MNIST
and 15 for SCENE15. We compute the feature of data
points from scattering convolution network [4] for MNIST
and Resnet18 [15] (pre-trained on ImageNet) for SCENE.
We stack two more fully-connected layers and one ℓ2 nor-
malization layer on the top to learn a new feature represen-
tation. We freeze all previous layers and only update newly
added layers. Stochastic gradient descent is used to opti-
mize the network. The learning rate and momentum are set
to 0.001 and 0.9 respectively. The margin ∆ in problem
(11) and triplet loss is set to 0.5. The parameter τ is set to
64 and the maximum epoch is 32.

4.1.1 Effectiveness of Proposed Reformulation

We ﬁrst evaluate our proposed reformulations on the selec-
tion stage. Here no feature learning procedure is performed.
We use the feature extracted from the original pre-trained
network.

Figure 3 shows the performance of problem (6), (8) and

(11). In Table 2, we summarize their differences. All prob-
lems are solved by the local search algorithm. From the
plots, we have two observations. (a) By comparing problem
(8) with problem (6), we can see an remarkable improve-
ment as we incorporate the source labels into the objective
function. (b) By comparing problem (11) with problem (8),
we can see that the discriminative terms can offer further
performance gains.

Qualitative results Figure 5 visualizes one example of
selected representatives on the SCENE15 dataset. The
source set includes 14 categories. Problem (11) success-
fully ﬁnds all target categories and achieves a higher accu-
racy, while problem (8) misses 3 categories. It shows the
discriminative term can help to cover more categories and
achieve higher accuracy when the feature is not discrimina-
tive.

4.1.2

Iterative Optimization Strategy

In this section, we examine the effectiveness of our pro-
posed iterative optimization strategy on feature learning.

(a) Recall= 0.8, Accr.= 0.453

(b) Recall= 1.0, Accr.= 0.560

Figure 5. T-SNE visualization [36] of selected representatives
(highlighted by blue triangles) on SCENE15 dataset.
(a) Prob-
lem (8) misses 3 categories in the target. (b) Problem (11) is able
to cover all categories.

6010

45678Number of source categories0.800.850.900.951.001.05RecallProblem (11)Problem (8)Problem (6)45678Number of source categories0.650.700.750.80AccuracyProblem (11)Problem (8)Problem (6)67891011121314Number of source categories0.650.700.750.800.850.900.951.00RecallProblem (11)Problem (8)Problem (6)67891011121314Number of source categories0.4000.4250.4500.4750.5000.5250.550AccuracyProblem (11)Problem (8)Problem (6)45678Number of source categories0.8500.8750.9000.9250.9500.9751.000RecallIterativeTwo-stage (CE)Two-stage (T)45678Number of source categories0.650.700.750.800.850.90AccuracyIterativeTwo-stage (CE)Two-stage (T)67891011121314Number of source categories0.700.750.800.850.900.951.00RecallIterativeTwo-stage (CE)Two-stage (T)67891011121314Number of source categories0.400.450.500.550.600.650.70AccuracyIterativeTwo-stage (CE)Two-satge (T)Recall / Accr.
changing tire
coffee
cpr
jump car
repot
Average

CE+KM

0.625 / 0.581
0.573 / 0.749
0.700 / 0.699
0.441 / 0.798
0.570 / 0.653
0.582 / 0.696

CE+kDPPs
0.567 / 0.529
0.527 / 0.715
0.638 / 0.669
0.419 / 0.771
0.550 / 0.618
0.540 / 0.660

CE+SMRS
0.455 / 0.452
0.453 / 0.642
0.583 / 0.606
0.283 / 0.725
0.437 / 0.578
0.442 / 0.601

CE+LLRSDS
0.467 / 0.460
0.445 / 0.611
0.555 / 0.541
0.296 / 0.705
0.427 / 0.571
0.438 / 0.578

CE + DS3A
0.613 / 0.592
0.559 / 0.715
0.672 / 0.673
0.451 / 0.774
0.565 / 0.620
0.572 / 0.675

T+KM

T+kDPPs

T+SMRS

0.670 / 0.671
0.638 / 0.806
0.773 / 0.777
0.481 / 0.830
0.624 / 0.712
0.637 / 0.759

0.567 / 0.570
0.523 / 0.755
0.623 / 0.702
0.419 / 0.802
0.579 / 0.658
0.542 / 0.697

0.651 / 0.595
0.596 / 0.755
0.722 / 0.700
0.487 / 0.800
0.593 / 0.667
0.610 / 0.703

T+LLRSDS
0.656 / 0.605
0.622 / 0.747
0.736 / 0.682
0.491 / 0.801
0.588 / 0.666
0.619 / 0.700

T+DS3A

0.686 / 0.669
0.626 / 0.817
0.733 / 0.760
0.421 / 0.817
0.580 / 0.722
0.609 / 0.757

Ours

0.716 / 0.729
0.663 / 0.842
0.771 / 0.806
0.523 / 0.833
0.640 / 0.762
0.663 / 0.794

Table 3. Experimental results on Narrated instructional dataset with 2.5c representatives. c is the number of categories in the source set.

(a)

(b)

Figure 6. (a) The average number of swappings of the local search
algorithm. (b) The number of hard triplets and accuracy during the
joint optimization (best viewed in color).

We compare it with the conventional two-stage strategy.
Both are applied to problem (11) to ﬁnd representatives.

Figure 4 shows the superior performance of our iterative
optimization strategy. One obvious limitation of the two-
stage strategy is that the feature learning procedure is inde-
pendent of the target data. In contrast, our iterative strategy
can leverage both source data and target data to learn fea-
ture representation. Even though target data are unlabeled,
the results show that they can still boost the performance
signiﬁcantly. The beneﬁts become more obvious as more
unseen categories appear in the target.

4.1.3 Efﬁciency

Figure 6(a) shows the averaged number of swappings of lo-
cal search on MNIST dataset. As seen, the proposed local
search algorithm can efﬁciently solve the problem within 20
swappings. For SCENE15 dataset, it can be ﬁnished within
25 swappings. Figure 6(b) shows the change of hard triplets
and accuracy during the iterative optimization on SCENE15
dataset with 10 categories in the source set. When there are
few hard training triplets, further feature learning will cause
the oscillation of accuracy. Therefore, the parameter τ can
be used to terminate the entire process.

4.1.4 Baseline Comparison

Figure 7 reports the experimental results of our approach
and baselines. For the particularly easy MNIST dataset,
despite baselines already present strong performances, our
method is still able to outperform them. For more com-
plex SCENE15 dataset, our approach can achieve an obvi-
ous improvement. On both datasets, the improvement of
our proposed method becomes more obvious as more novel
categories appear in target data.

(a) MNIST, Recall

(b) MNIST, Accuracy

(c) SCENE15, Recall

(d) SCENE15, Accuracy

Figure 7. Experimental results of our approach and baselines.

4.2. Key Action Discovery

In this section, we evaluate our approach on the task
of ﬁnding key actions from videos. Two video datasets
are used to evaluate the performance, Breakfast [18] and
narrated instructional videos [2]. For Breakfast dataset,
there are 1,712 videos with 10 coarse activities (e.g., mak-
ing coffee, sandwich and pancake) and 48 ﬁne-grained ac-
tions. Narrated instructional video dataset includes 5 ac-
tivities. For each activity, there are 30 videos with 8 to 13
ﬁne-grained key steps.

Experiments are performed on each activity separately
and use cross-validation to evaluate the performance: 4-fold
for Breakfast based on the provided splits and 5-fold for
Narrated dataset. For Breakfast, we use the reduced 64D
ﬁsher vector of dense trajectories. For Narrated dataset, we
use the provided 3000D bag-of-word feature vectors of the
motion and the appearance. In the experiments, we learn
a linear transform for better features, i.e., Fθ(x) = W x,
where W ∈ R64×64 for Breakfast and W ∈ R256×3000 for
Narrated dataset. For both datasets, the source set is con-
structed by randomly selecting 50% categories. Since Nar-
rated dataset only has 150 videos, in order to mitigate sam-
ple bias of randomly sampling, we conduct 5 different runs
and report the average results. As the number of key actions
in target videos is unknown in advance, we compare the
performance as the number of representatives varies within
{2c, 2.5c, 3c} 1, where c is the number of categories in the
source set.

1The results of 2c and 3c can be seen in the supplementary material.

6011

45678Number of source categories05101520Number of swappings03691215182124Iterations100150200250300Hard Triplets0.450.500.550.60Accuracy45678Number of source categories0.700.750.800.850.900.951.00RecallOursCE+KMCE+KDPPsCE+SMRSCE+LLRSDSCE+DS3AT+KMT+KDPPsT+SMRST+LLRSDST+DS3A45678Number of source categories0.40.50.60.70.80.9AccuracyOursCE+KMCE+KDPPsCE+SMRSCE+LLRSDSCE+DS3AT+KMT+KDPPsT+SMRST+LLRSDST+DS3A67891011121314Number of source categories0.60.70.80.91.0RecallOursCE+KMCE+KDPPsCE+SMRSCE+LLRSDSCE+DS3AT+KMT+KDPPsT+SMRST+LLRSDST+DS3A67891011121314Number of source categories0.30.40.50.60.7AccuracyOursCE+KMCE+KDPPsCE+SMRSCE+LLRSDSCE+DS3AT+KMT+KDPPsT+SMRST+LLRSDST+DS3ARecall/Accr.
cereals
coffee
friedegg
juice
milk
pancake
salat
sandwich
scrambledegg
tea
Average

CE+KM

0.869 / 0.623
0.912 / 0.744
0.701 / 0.585
0.803 / 0.718
0.863 / 0.598
0.660 / 0.535
0.680 / 0.642
0.867 / 0.636
0.763 / 0.574
0.927 / 0.710
0.805 / 0.637

CE+KDPPs
0.816 / 0.562
0.881 / 0.654
0.682 / 0.555
0.709 / 0.652
0.773 / 0.521
0.659 / 0.501
0.644 / 0.594
0.805 / 0.567
0.729 / 0.524
0.874 / 0.621
0.757 / 0.575

CE+SMRS
0.837 / 0.559
0.905 / 0.676
0.702 / 0.542
0.812 / 0.654
0.870 / 0.526
0.688 / 0.488
0.707 / 0.567
0.876 / 0.573
0.799 / 0.521
0.914 / 0.647
0.811 / 0.575

CE+LLRSD
0.854 / 0.503
0.910 / 0.585
0.646 / 0.551
0.844 / 0.617
0.852 / 0.472
0.630 / 0.500
0.738 / 0.557
0.844 / 0.551
0.737 / 0.514
0.906 / 0.576
0.796 / 0.543

CE+DS3A

0.798 / 0.571
0.903 / 0.698
0.648 / 0.550
0.669 / 0.651
0.777 / 0.530
0.645 / 0.507
0.608 / 0.598
0.794 / 0.560
0.696 / 0.512
0.889 / 0.653
0.743 / 0.583

T+KM

T+KDPPs

T+SMRS

T+LLRSD

T+DS3A

Ours

0.836 / 0.547
0.899 / 0.642
0.691 / 0.559
0.825 / 0.697
0.845 / 0.489
0.629 / 0.514
0.751 / 0.618
0.866 / 0.568
0.731 / 0.530
0.905 / 0.600
0.798 / 0.576

0.817 / 0.490
0.863 / 0.588
0.666 / 0.544
0.722 / 0.636
0.808 / 0.442
0.643 / 0.484
0.646 / 0.583
0.818 / 0.505
0.733 / 0.500
0.892 / 0.544
0.761 / 0.532

0.834 / 0.472
0.903 / 0.565
0.661 / 0.539
0.804 / 0.563
0.815 / 0.427
0.628 / 0.490
0.704 / 0.566
0.834 / 0.504
0.732 / 0.490
0.908 / 0.519
0.782 / 0.514

0.854 / 0.503
0.910 / 0.585
0.646 / 0.551
0.844 / 0.617
0.852 / 0.472
0.630 / 0.500
0.738 / 0.557
0.844 / 0.551
0.737 / 0.514
0.906 / 0.576
0.796 / 0.543

0.837 / 0.504
0.884 / 0.603
0.719 / 0.515
0.757 / 0.660
0.821 / 0.449
0.687 / 0.476
0.732 / 0.577
0.830 / 0.514
0.727 / 0.484
0.883 / 0.550
0.788 / 0.533

0.877 / 0.647
0.935 / 0.767
0.750 / 0.627
0.896 / 0.781
0.890 / 0.635
0.688 / 0.591
0.796 / 0.685
0.890 / 0.693
0.834 / 0.629
0.928 / 0.749
0.848 / 0.680

Table 4. Experimental results on Breakfast video dataset with 2.5c representatives. c is the number of categories in the source set.

SIL

Take knife

Cut orange

Squeeze orange

Take glass

Pour juice

SIL

Take knife

Cut orange

Squeeze orange

Take glass

Take glass

Pour juice

Pour juice

SIL

SIL

(a) Ours. Recall = 1.000, Accuracy = 0.742

Cut orange

Squeeze orange

Squeeze orange

Squeeze orange

Take glass

Take glass

Pour juice

Pour juice

SIL

SIL

(b) CE+SMRS. Recall = 0.833, Accuracy = 0.390

SIL

Take knife

Squeeze orange

Squeeze orange

Squeeze orange

Squeeze orange

Squeeze orange

Squeeze orange

Take glass

SIL

Figure 8. Qualitative illustration of representatives in a “juice” video. The location of representatives is highlighted by red lines in the
video sequence. The source set only includes four actions, i.e., “SIL”, “take knife”, “squeeze orange” and “pour juice”, while the target set
also includes “take glass” and “cut orange”.

(c) CE+DS3A. Recall = 0.667, Accuracy = 0.621

Quantitive results Table 4 and Table 3 present the experi-
mental results of 2.5c representatives on Breakfast and Nar-
rated instructional video dataset respectively. The results
show that our approach is able to ﬁnd the most activities
appear in the target videos. Besides, more frames can be
represented by the representatives of the same category.

Qualitative results Figure 8 displays picked representa-
tives by our approach, CE+SMRS, and CE+DS3A. Good
representatives should achieve both high recall and high ac-
curacy. Figure 8(b) shows that CE+SMRS achieves high
recall but low accuracy. The representatives include al-
most all key actions except for action ”take knife”. But we
can ﬁnd that many representatives are at the boundaries of
adjacent actions. Because action boundaries are typically
ambiguous, transition frames between two actions actually
cannot well represent either action. Figure 8(c) shows that
CE+DS3A achieves low recall but high accuracy. It selects
many representatives from the long action ”squeeze orange”
and ignores some short actions. Because of the uneven
length of actions, ignoring short actions may still achieve
a good accuracy but the low recall shows these representa-

tives do not provide good coverage of various categories.
Figure 8(a) shows the result of our proposed method, which
not only ﬁnds all actions but also achieves higher accuracy.

5. Conclusion

In this paper, we design a semi-supervised approach to
address the problem of joint representative selection and
discriminative feature learning. We leverage the labeled
source data to ﬁnd representatives in the new target data
and can discover new categories. Our formulation is based
on the facility location problem. We show that the guidance
of labeled source data and our proposed discriminative term
can effectively improve the performance of both representa-
tive selection and feature learning. By iteratively updating
the feature representation based on the selected represen-
tatives, we show that our strategy can learn better feature
for the representative selection than conventional two-stage
strategy. Experiments on two image datasets and two video
datasets show that our approach not only ﬁnds more cate-
gories in the target data but also learn better discriminative
feature representation for representative selection.

6012

References

[1] R. H. Affandi, A. Kulesza, and E. B. Fox. Markov determi-

nantal point processes. In UAI, pages 26–35, 2012. 2

[2] J.-B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev,
and S. Lacoste-Julien. Unsupervised learning from narrated
instruction videos. In CVPR, June 2016. 7

[3] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Muna-
gala, and V. Pandit. Local search heuristic for k-median and
facility location problems. In STOC, 2001. 2, 5

[4] J. Bruna and S. Mallat. Invariant scattering convolution net-

works. IEEE Trans. Pattern Anal. Mach. Intell., 2013. 6

[5] M. Charikar, S. Guha, E. Tardos, and D. B. Shmoys. A
constant-factor approximation algorithm for the k-median
problem. In STOC, 1999. 2, 5

[6] E. Elhamifar and M. Clara De Paolis Kaluza. Online summa-
rization via submodular and convex optimization. In CVPR,
July 2017. 2

[7] E. Elhamifar and M. C. De Paolis Kaluza. Subset selection

and summarization in sequential data. In NIPS, 2017. 1, 2

[8] E. Elhamifar, G. Sapiro, and S. S. Sastry. Dissimilarity-based
IEEE Trans. Pattern Anal. Mach.

sparse subset selection.
Intell., 2016. 1, 2, 3, 5

[9] E. Elhamifar, G. Sapiro, and R. Vidal. Finding exemplars
from pairwise dissimilarities via simultaneous sparse recov-
ery. In NIPS. 2012. 1, 2

[10] E. Elhamifar, G. Sapiro, and R. Vidal. See all by looking at
a few: Sparse modeling for ﬁnding representative objects. In
CVPR, 2012. 2, 5

[22] A. Kulesza and B. Taskar. Determinantal Point Processes
for Machine Learning. Now Publishers Inc., Hanover, MA,
USA, 2012. 2

[23] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. In Proceed-
ings of the IEEE, 1998. 5

[24] S. Li and O. Svensson. Approximating k-median via pseudo-

approximation. In STOC, 2013. 2

[25] J. Meng, H. Wang, J. Yuan, and Y.-P. Tan. From keyframes
to key objects: Video summarization by representative object
proposal selection. In CVPR, June 2016. 2, 5

[26] J. Meng, S. Wang, H. Wang, J. Yuan, and Y. Tan. Video
summarization via multiview representative selection. IEEE
Transactions on Image Processing, 2018. 2

[27] R. Panda, A. Das, Z. Wu, J. Ernst, and A. K. Roy-
Chowdhury. Weakly supervised summarization of web
videos. In ICCV, Oct 2017. 2

[28] A. Prasad, S. Jegelka, and D. Batra. Submodular meets struc-
tured: Finding diverse subsets in exponentially-large struc-
tured item sets. In NIPS, 2014. 1

[29] A. Rodriguez and A. Laio. Clustering by fast search and ﬁnd

of density peaks. Science, 2014. 1, 2

[30] C. Schmid. Beyond bags of features: Spatial pyramid match-
ing for recognizing natural scene categories. In CVPR, pages
2169–2178, 2006. 5

[31] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
June 2015. 4

[32] A. Sharghi, B. Gong, and M. Shah. Query-focused extractive

[11] B. J. Frey and D. Dueck. Mixture modeling by afﬁnity prop-

video summarization. In ECCV, pages 3–19, 2016. 2

agation. In NIPS, 2005. 2

[12] B. J. Frey and D. Dueck. Clustering by passing messages

between data points. Science, 2007. 1, 2

[13] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse
sequential subset selection for supervised video summariza-
tion. In NIPS, 2014. 1, 2

[14] M. Gygli and H. G. L. Van Gool. Video summarization by
learning submodular mixtures of objectives. In CVPR, pages
3090–3098, 2015. 2

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, June 2016. 6

[16] L. Kaufman and P. Rousseeuw. Clustering by means of
medoids. In Statistical Data Analysis Based on the L1 Norm
and Related Methods. 1987. 2, 5

[17] M. R. Korupolu, C. Plaxton, and R. Rajaraman. Analysis of a
local search heuristic for facility location problems. Journal
of Algorithms, 2000. 5

[18] H. Kuehne, A. B. Arslan, and T. Serre. The language of ac-
tions: Recovering the syntax and semantics of goal-directed
human activities. In CVPR, 2014. 7

[19] H. W. Kuhn and B. Yaw. The hungarian method for the as-

signment problem. Naval Res. Logist. Quart, 1955. 5

[20] A. Kulesza and B. Taskar. Structured determinantal point

processes. In NIPS, 2010. 1

[21] A. Kulesza and B. Taskar. k-dpps: Fixed-size determinantal

point processes. In ICML, 2011. 2, 5

[33] D. B. Shmoys, E. Tardos, and K. Aardal. Approximation
algorithms for facility location problems. In STOC, 1997. 2
[34] I. Simon, N. Snavely, and S. M. Seitz. Scene summarization

for online image collections. In ICCV, 2007. 2

[35] S. Tschiatschek, R. K. Iyer, H. Wei, and J. A. Bilmes. Learn-
ing mixtures of submodular functions for image collection
summarization. In NIPS, pages 1413–1421. 2014. 2

[36] L. van der Maaten and G. Hinton. Visualizing high-
dimensional data using t-sne. Journal of Machine Learning
Research, 2008. 6

[37] C. Yang, J. Peng, and J. Fan. Image collection summarization
via dictionary learning for sparse representation. In CVPR,
2012. 2

[38] Y. Yue and T. Joachims. Predicting diverse subsets using

structural svms. In ICML, 2008. 1

[39] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Video sum-
marization with long short-term memory. In ECCV, 2016.
2

[40] B. Zhao and E. P. Xing. Quasi real-time summarization for

consumer videos. In CVPR, June 2014. 2

6013

