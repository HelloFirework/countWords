Social-IQ: A Question Answering Benchmark

for Artiﬁcial Social Intelligence

Amir Zadeh1, Michael Chan1, Paul Pu Liang2, Edmund Tong1, Louis-Philippe Morency1

1 Language Technologies Institute, 2 Machine Learning Department

School of Computer Science, Carnegie Mellon University

multicomp.cs.cmu.edu/social-iq

{abagherz,mkchan,pliang,edtong,morency}@cs.cmu.edu

00:29          00:37

00:37          00:40

00:40          00:42

(trying to 

speak)

Steven went, got the keys and we are 

gonna have them back. That easy. 

I couldn’t …

(Interrupts) But this was 

(silenced)

Friday Matt! This was Friday.  

(serious face)

You said you were going to do it and 

you are not doing it!

(serious face)

Q2: How is the man who is not being blamed responding to the situation? <advanced> 
A1. He thinks the other man is slacking even if he is not saying it. <advanced>
A2. He is showing support for the woman by taking her side. <intermediate>
A3. He thinks he is better than both of the people arguing. <easy>
A4. He doesn’t want to pick a side. <advanced>

Q1: How is the discussion between the woman and the man in the white shirt ? <intermediate> 
A1. The woman is blaming the man in the white shirt who seems to be in the fault. <easy>
A2. She is blaming her in a tense voice and not letting him defend himself. <advanced>
A3. They are having a romantic conversation. <easy>
A4. An active argument that both are blaming each other. <advanced>

Q3: Why is the woman seem so overwhelmed? <advanced> 
A1. Because a small problem became a huge problem. <intermediate>
A2. She has too much on her plate, and this new problem  overwhelms her. <advanced>
A3. The woman is upset because the men are insulting her. <easy>
A4. Because both of them men seem to be ignoring her. <intermediate>

Figure 1: Best viewed zoomed in and in color. An overview of the Social-IQ dataset questions and videos. Social-IQ evaluates
artiﬁcial social intelligence through question answering. The dataset contains 1, 250 videos, 7500 questions, 30, 000 correct
answers and 22, 500 incorrect answers. Questions and answers are annotated for complexity levels: easy, intermediate and
advanced. Q indicates questions and A indicates answers. Answers in green are correct and answers in red are incorrect.

Abstract

As intelligent systems increasingly blend into our every-
day life, artiﬁcial social intelligence becomes a prominent
area of research. Intelligent systems must be socially intel-
ligent in order to comprehend human intents and maintain
a rich level of interaction with humans. Human language
offers a unique unconstrained approach to probe through
questions and reason through answers about social situ-
ations. This unconstrained approach extends previous at-
tempts to model social intelligence through numeric super-
vision (e.g. sentiment and emotions labels). In this paper,
we introduce the Social-IQ, an unconstrained benchmark
speciﬁcally designed to train and evaluate socially intelli-

gent technologies. By providing a rich source of open-ended
questions and answers, Social-IQ opens the door to explain-
able social intelligence. The dataset contains rigorously
annotated and validated videos, questions and answers, as
well as annotations for the complexity level of each ques-
tion and answer. Social-IQ contains 1, 250 natural in-the-
wild social situations, 7, 500 questions and 52, 500 correct
and incorrect answers. Although humans can reason about
social situations with very high accuracy (95.08%), exist-
ing state-of-the-art computational models struggle on this
task. As a result, Social-IQ brings novel challenges that will
spark future research in social intelligence modeling, visual
reasoning, and multimodal question answering (QA).

8807

1. Introduction

2. Related Works

Deﬁnition and studies of social intelligence have a rich
history in psychology, sociology and psycholinguistics [44,
47]. These studies aim to evaluate the cognitive process
behind understanding social situations; a hidden cognitive
process which often goes beyond explicit understanding of
meanings and structures [23]. As intelligent systems in-
creasingly become a reality in our every day lives, social
intelligence becomes a key part of future artiﬁcial intelli-
gence (AI) systems.

Unlike traditional AI systems that can measure a phe-
nomena based on numerical labels, psychometric evalua-
tion of social intelligence requires probes that go beyond
numeric labels. To this end, we present the Social-IQ (So-
cial Intelligence Queries) dataset. Social-IQ opens the door
to unconstrained and explainable social evaluation and un-
derstanding for AI. It contains a rigorously annotated and
manually validated set of 7, 500 questions, 52, 500 answers
(30, 000 correct and 22, 500 incorrect) over a broad range
of 1, 250 social in-the-wild videos.

Question answering is an effective way of probing the
level of understanding of an underlying phenomena [27, 6].
In machine learning, this form of probing has a well estab-
lished precedence in multiple different areas ranging from
understanding books and text [25], to understanding events
in the movies [26]. To build a suitable question answer-
ing resource for social understanding, Social-IQ strives to
analyze social situations as they happen in the wild. Natu-
ralistic interactions are captured by cameras and uploaded
to social media on a daily basis from different aspects of
life; such as birthday parties or a basketball game. Us-
ing an extensive set of YouTube videos, Social-IQ covers
a broad range of social and behavioral situations. Fur-
thermore, Social-IQ is diverse in question types and how
each question probes social intelligence. The questions also
cover a broad range of complexity (advanced, intermediate
and easy).

Our contributions in this paper are as follows: 1) We
formalize an open-ended question answering task for mea-
suring social intelligence for current and future AI systems.
2) We present the ﬁrst dataset in this area, called Social-IQ,
that focuses on psychometric measurement of social intelli-
gence and operationalizes this measurement through ques-
tion answering. 3) We analyze the performance of state
of the art in multimodal QA over the Social-IQ dataset.
Through our experiments, we observe that Social-IQ is a
challenging dataset; Humans can achieve very high level of
accuracy (95.08%) while state of the art in machine learn-
ing (64.82%) trails by a large margin (on a task with 50%
random performance). This gap highlights the value of a
resource such as Social-IQ; a dataset which enables uncon-
strained probing of social intelligence.

The dataset and experiments in this paper are connected

to the following areas:

2.1. Question Answering

Intelligent question answering, one of the most ambi-
tious goals of AI, has roots in decades of research in ar-
tiﬁcial intelligence [17, 54].
In the past few years, there
has been a surge of interest in using neural models for in-
telligent question answering. Recently, question answer-
ing has evolved into a multimodal framework. Datasets in
this domain started with DAQUAR [33], where image and
questions were paired together. Subsequently, four other
successful and inﬂuential datasets followed which are as
follows: COCO-QA [37], VQA [6], FM-IQA [15], Vi-
sual7w [63]. In all aforementioned datasets, questions are
asked about a single image. More recently, the idea of visual
question answering has extended to videos. MovieQA [43]
focused on understanding the events in a movie as well as
their ordering from movie frames, scripts and plot. Close
to this idea, TVQA [26] presented an alternative dataset
for the task of understanding movies and plots. In general,
compared to visual question answering and textual question
answering [59, 11], there is a lack of resources speciﬁcally
designed to benchmark social intelligence in current and fu-
ture AI systems.

Social-IQ builds upon lessons learned from previous
multimodal datasets and includes some key components:
1) unconstrained and unscripted environment: Social-IQ
videos come from a diverse set of in-the-wild videos on
YouTube. There are diverse sets of distinct characters
across these videos. Social situations in these videos are
rarely scripted and events are more volatile than movies. 2)
multimodal stimuli: all questions directly relate to events in
the videos and require information from multiple modal-
ities to correctly answer. The question are grounded in
variety of manners across video, dialogues, and audio. 3)
annotator bias: unlike famous movies, arbitrary social on-
line videos are less likely to be seen by annotators prior to
the annotation. Furthermore, multiple validation stages are
devised for Social-IQ to remove annotation bias and make
sure the quality of videos, questions, and answers remains
high. 4) explainability: annotators of Social-IQ accompany
their answers with sufﬁcient reasoning, going beyond short
answers consisting of only a few words. Social-IQ answers
are longer than previous datasets by nearly a factor of 100%
in average length.

2.2. Multimodal Machine Learning

Multimodal machine learning has been among the most
successful recent trends in machine learning [7]. Pow-
ered by advances in deep learning, multimodal models are

8808

creatively used by research communities centered around
tasks such as multimodal language analysis [58], sentiment
[46, 31], emotion recognition [29], personality
analysis
traits recognition [56], image captioning [5, 4, 34], mul-
timedia description [50, 51, 62], and video comprehen-
sion [14, 32, 19, 60].

3. Measurement of Social Intelligence

Inspired by past psychological and sociological studies
in measuring social intelligence [21, 45, 35, 48, 39, 53],
we design the guidelines of Social-IQ according to the fol-
lowing four criteria: 1) Judgment in Social Situations, 2)
Processing Human Intelligent Behavior, 3) Understanding
Mental State, Trait, Attitude, and Attributes 4) Memory for
Referencing and Grounding. Questions in Social-IQ relate
to at least one or more of the above social intelligence cri-
teria. What follows is the detailed deﬁnition of each of the
above criteria with examples:

Judgment in Social Situations: Aligned with sociological
deﬁnitions developed by Piotr Sztompka [42] and Max We-
ber [52], we deﬁne a social situation as a social exchange
or behavior involving two (dyadic) or more individuals.
More formally, a social situation involves human physical
movement,
intentions, and a set of unique interactions
in response to one another. Social situations can occur
through communications from both verbal and nonverbal
channels. Intelligence in this areas includes understanding
the causes and intentions behind a social situation. Example
acceptable questions for this criteria are: “Are the people
in this group getting along?” (yes, the group seems to
be laughing together), or “How is the atmosphere of the
room?” (it is tense since the people involved seem to argue
and disagree). In both cases, questions target the core of a
particular social interaction.

Processing Human Intelligent Behavior: This criteria
refers to both how and why humans act or react in a certain
manner [40]. Example questions to probe human behavior
include: “How did the two men demonstrate that they for-
give each other?” (by hugging for a long time), or “Why
does the woman pretend to not hear the man?” (she is act-
ing this way because her feelings were hurt by him). It is
noteworthy that direct action questions are not acceptable
based on this criteria. Examples such as “Is the man lifting
weights?” (yes, he is doing so in a gym) are not acceptable
as questions for Social-IQ, because they do not probe social
intelligence.

Understanding Mental State, Trait, Attitude and At-
tributes: We deﬁne traits as stable characteristics of per-
sonality, while states are temporary behaviors or feelings
that depend on a persons situation and motives at a particu-
lar time [8]. Both traits and states are manifest through com-

munication and inferable by humans [16]. Furthermore, we
deﬁne attitude as a person’s (or a group’s) opinion towards
a speciﬁc topic [18]. Example acceptable questions for this
criteria include: “Does the man in the black robe seem like
he can manage high stress?” (no because a simple problem
with his laptop made him panic more than he should), “Why
did the woman in the purple skirt call the man in the suit a
psychopath?” (she thinks he has no remorse for what he did
to her). We deﬁne human attributes as demonstrating cer-
tain manners or consistent behaviors (for example bravery,
justness). Example questions for attributes include: “How
did the man in blue shirt show his bravery?” (by standing
up to the crowd who were bullying the silent man).

Memory for Referencing and Grounding: Aside the
above criteria, social intelligence includes comprehending
a variety of references through multimodal grounding. This
form of grounding goes beyond simple references from one
modality (i.e. individual names or appearances). In social
situations, even if the identity of a character is not known,
humans establish a common grounding to point to enti-
ties. For example “the man with a tense voice”, or “the
woman who was sad when coming into the house”. Social-
IQ strives to diversify references. Since humans understand
these references (as long as the references are determinis-
tic), we encourage a broad range of referencing methods
for entities. It is noteworthy that references should be con-
tained within the respective video. As an example, indi-
vidual names that cannot be inferred from videos are not
acceptable.

Beyond the above criteria, it is required that all questions
in Social-IQ focus on humans. Therefore, questions focus-
ing on inanimate entities, objects and animals are rejected.
For example: “What is the man picking up?” (a big wooden
box) is not acceptable. However, a question such as “Is the
man lifting the box under pressure?” (Yes, the box is too
heavy for him) is accepted.

Understanding and answering questions in Social-IQ
may require different levels of social intelligence. We add a
complexity measure for the questions and answers as a sub-
jective approximation of the level of intelligence and rea-
soning required to process them. Each question or answer
(correct or incorrect) is assigned a complexity level. The
complexity level is deﬁned as a Likert scale based on 3 lev-
els (easy, intermediate, and advanced) outlining a vote for
the level of social intelligence deemed required for answer-
ing the question, accepting (as correct), or rejecting (as in-
correct) the answer. The easy complexity level is assigned
to questions and answers which require simple social intel-
ligence and understanding of the video. For example “Who
is the dominant person in the group?” (the woman in red
dress), which may require simple understanding of who is
speaking and their tone of voice. For the advanced com-

8809

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0

Other (3%)
Who (1%)

Why 
(34%)

Does
(8%)

Is (10%)

Describe 

(2%)

What
(15%)

How 
(26%)

Advanced

(41%)

Easy
(19%)

Intermediate

(40%)

≤3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ≥25

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

(a) Distribution of question lengths

(c) Distribution of question types

(e) Distribution of question complexity

0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

1

1

2
2

3

3

4
4

≥5
5

≤3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ≥25
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

(b) Distribution of correct (green) and incorrect (red) answer lengths

(d) Distribution of # of active characters in videos

(f) Video categories

Figure 2: Best viewed zoomed in and in color. Social-IQ dataset statistics. (a) demonstrates the distribution of question length
in terms of number of words. The average number of words in questions is 10.87. (b) demonstrates the distribution of answer
length in terms of number of words with an average of 10.46 words per answer. Both correct (green) and incorrect (red)
follow the same distribution. (c) various question types in Social-IQ dataset. (d) distribution of number of active characters
in videos. (e) distribution of question complexity in Social-IQ with majority of questions being intermediate and advanced.
(f) variety of topics in Social-IQ dataset.

plexity level, questions and answers require in-depth un-
derstanding and analysis of the video, characters and their
interactions, as well as potential multi-hop inferences and
reference resolution. For example: “What strategy did the
woman who disagrees the most with the man choose to con-
front him?” (she decided to ﬁrst blame him for being naive,
after which she conjectured he is immoral). This question
answer pair requires understanding the interactions between
the characters in the video, as well as how the interaction
develops over time.

4. Social-IQ Dataset

In this section we present the details of the Social-
IQ (Social Intelligence Queries) dataset which follows the
guidelines for measuring social intelligence outlined in
the previous section (Section 3). Social-IQ is a ques-
tion answering (QA) benchmark for assessment of so-
cial intelligence in naturalistic social situations. Social-IQ
presents 1, 250 videos, 7, 500 questions, and 52, 500 an-
swers (30, 000 correct and 22, 500 incorrect).

We ﬁrst comprehensively outline the statistics of the
Social-IQ dataset. Afterwards, we discuss the rigorous an-
notation procedure and multiple validation stages.

4.1. Dataset Statistics

In this subsection we present the main statistics of the
Social-IQ dataset. We split the statistics into three parts: a)

questions statistics, b) answers statistics, and c) multimedia
statistics.

Question Statistics: The Social-IQ dataset contains a to-
tal of 7500 questions (6 per video). Figure 2 (a) demon-
strates the distribution of question length in terms of number
of words. The average length of questions in Social-IQ is
10.87 words. Figure 2 (c) shows the different question types
in the Social-IQ dataset. Questions starting with why and
how, which often require causal reasoning, are the largest
group of questions in Social-IQ. This is a unique feature of
the Social-IQ dataset and a distinguishing factor of Social-
IQ from other multimodal QA datasets (which commonly
have what (object) and who questions as the most common
[26, 43]).

Figure 2 (e) demonstrates the distribution of complexity
across questions of the Social-IQ. Majority of the dataset
consists of advanced and intermediate questions (with al-
most equal share between the two) while easy questions
share a small portion of the dataset. The distribution of
question types and complexity levels in Social-IQ demon-
strates the challenging nature of the dataset.

Answer Statistics: Social-IQ contains a total of 30, 000
correct (4 per question) and 22, 500 (3 per question) incor-
rect answers. Figure 2 (b) demonstrates the distribution of
word length for answers in the Social-IQ dataset. Both the
correct (green) and incorrect (red) answers follow similar
distribution. On average, there are a total of 10.46 words

8810

per answer in Social-IQ. This is also a unique characteris-
tic of the Social-IQ dataset since the average answer length
is longer than other multimodal QA datasets (with aver-
age length between 1.24 to 5.3 words [6, 38, 33, 26, 43]).
The long average length demonstrates the level of detail in-
cluded in Social-IQ answers.

Presence of multiple correct answers in the Social-IQ
dataset allows for modeling diversity and subjectivity across
annotators in cases where multiple explanations are correct
for a certain question. Furthermore, having multiple cor-
rect answers enables answer generation tasks (which of-
ten require multiple correct answers for successful evalu-
ation [36, 30]).

Multimedia Statistics: Social-IQ dataset consists of a to-
tal of 1, 250 videos from YouTube. Figure 2 (f) demon-
strates an overview of categories of the videos in Social-IQ.
There is a total of 1, 239 minutes of annotated video content
(across 10, 529 minutes of full videos). Figure 2 (d) shows
the distribution of number of characters in videos. All the
videos in the Social-IQ dataset contain manual transcrip-
tions with detailed timestamps.

4.2. Annotation Procedure

Annotation of the Social-IQ dataset is carried out in 6
distinct stages (Figure 3). A total of 50 annotators 1 worked
across these multiple stages over a period of 14 months
(across three annotation seasons). Before annotating, the
annotators went through several training sessions (discussed
in Subsection 4.3) for this task to build a proper understand-
ing of measuring social intelligence as deﬁned in Section 3.
The details of these 6 stages is as follows:

Video Acquisition Stage: Online social media platforms,
including YouTube, contain a large cache of in-the-wild
videos with variety of social situations. As a ﬁrst step, a
set of 2, 000 videos were harvested from YouTube 2 using a
broad set of search terms. The choice of these search terms
followed precedence previously established by the CMU-
MOSEI dataset [58] which contains 250 diverse search
terms. We required all the videos to maintain at least one
face (detected using MTCNN [61]) in 80% of the frames.
A set of 2, 000 videos were acquired using this strategy.

Video Validation Stage: After acquiring the initial set of
videos, for each video, two trained annotators inspected the
video to make sure a social situation exists. Speciﬁcally,
annotators looked at presence of social interactions, opinion
sharing and communication. A total of 1, 250 videos were
selected this way.

Question Creation Stage: During this stage, expert anno-

1Hired and trained undergraduate students from Carnegie Mellon Uni-

versity.

2Videos followed creative commons license.

Video Acquisition Stage

Video Validation Stage

1,250 videos

Question Creation Stage

Question Validation Stage

7,500 questions

Answering Stage

Answer Validation Stage

52,000 answers

Figure 3: The 6 stages used to create the Social-IQ dataset.
Video Acquisition and Validation Stage produce 1, 250
videos with social situations in them. Question Creation
and Validation Stages produce 7, 500 questions. Answer-
ing and Answer Validation Stage produce 52, 500 answers
(30, 000 correct and 22, 500 incorrect).

tators were tasked to ask questions that probe social intelli-
gence as deﬁned by Section 3. Given a video, two trained
annotators were instructed to each ask 3 questions. The an-
notators were also instructed to keep their questions diverse
with high level of complexity. They proposed one correct
and one incorrect answer for each question. Furthermore,
they labelled their questions and answers with complexity
labels. After this stage, each video consists of 6 questions,
6 correct answers and 6 incorrect answers.

Question Validation Stage: Given the set of 6 questions,
we ask a separate set of 2 annotators to validate the ques-
tions (whether or not they comply with deﬁnitions in Sec-
tion 3). If an annotator disputes the validity of a question,
the question is removed and passed to Question Creation
Stage for re-annotation. A similar procedure is performed
on the answer. Furthermore, the two annotators label each
question and answer with complexity labels.

Answering Stage: A set of two annotators answer the 6
questions for each video (3 for each annotator). These anno-
tators are different than the annotators who asked the ques-
tions in Question Creation Phase and validated the ques-
tion in Question Validation Stage. Each annotator creates 3
correct and 2 incorrect answers for each question (without
knowledge of any prior correct or incorrect answers from
Question Creation Stage). Similar to Question Creation
Stage, the annotators are encouraged to keep their answers
diverse. Annotators also label each answer with complexity
levels. After this stage, each question contains 4 correct an-

8811

Q1. How do the men in the room feel about each other?
Q2. How was the man in the blue cap made fun of?
Q3. Why did the man in black hoodie at the right shake 
his head when his friend started talking?

Q1. Do the people in this video feel comfortable about
the clown being there?
Q2. Who seems to be the most excited person about the
clown?
Q3. How did the clown start bugging the woman in
striped shirt and wearing boots?

Q1. How do the blonde woman and the red-haired
woman feel around each other?
Q2. Why doesn't the man step in when the two women
are arguing?
Q3. Does the man think the woman with the straight
red hair is completely innocent?

Q1. Are the men having a serious conversation?
Q2. Does the man sitting on the red chair seem excited
to talk to the man sitting in front of him?
Q3. Are the two men mostly agreeing with each other?

Q1. Does the man want to make the woman laugh?
Q2. How does the man feel about water being poured in
his face?
Q3. Did the woman want to offend the man by spitting
water in his face?

Q1. Why is the man in checkers shirt in front row
making a weird face?
Q2. After being called out by others, how did the man
in navy jacket in the right respond?
Q3. Are the people friendly towards each other?

Figure 4: Example videos and questions in Social-IQ dataset, a benchmark for assessment of social intelligence in naturalistic
social situations. In-the-wild online videos exhibit various social situations which form the basis of the Social-IQ dataset.
Social-IQ presents 1250 videos, 7500 questions, and 52, 500 answers (30, 000 correct and 22, 500 incorrect).

swers and 3 incorrect answers (including the 1 correct and
1 incorrect from Question Creation Stage).

Answer Validation Stage: Similar to Question Validation
Phase, a set of 2 annotators (different than annotators in
Question Creation and Answering Stages) validate each an-
swer. Answers are validated for diversity and whether or
not they are correctly labelled (if correct/incorrect answers
are indeed correct/incorrect). Furthermore, they label the
answers with complexity level.

After the above stages, the set of 1, 250 videos, 7, 500
questions, and 52, 500 correct and incorrect answers shape
the Social-IQ dataset. Figure 4 shows examples of some
videos in the Social-IQ dataset along with the annotations
for questions, correct answers and incorrect answers.

4.3. Annotator Training

Due to the rigorous nature of the guidelines in Section 3,
a detailed annotator selection and training process is re-
quired to achieve high quality annotations. The training
process was split into 3 stages:

Initial Training Stage: The ﬁrst stage of training involves
in-depth understanding of the crtieria in Section 3. Anno-
tators were trained during a single training session where

the Social-IQ criteria were deﬁned. Annotators also learned
how to annotate the data through a designated online anno-
tation system built for Social-IQ. A generic implementation
of this annotation system called CMU-Crowd: https://
github.com/A2Zadeh/CMU-Crowd is available for
academic use.

Secondary Training Stage: Before using the online an-
notation system, annotators were given training videos for
each of the annotation stages in Subsection 4.2. After
watching the videos, annotators ﬁnished a set of 10 training
examples from Question Creation Stage before beginning
Social-IQ annotations.

Continuous Supervision Stage: The performance of an-
notators was continuously monitored on a weekly basis by
the authors. A set of 8 annotation workshops were held
throughout a period of a year. Annotators with low quality
of work were asked to attend individual meetings for re-
training. It is noteworthy that the training and supervision
throughout the annotation timeline was designed to encour-
age creativity and diversity of questions and answers. None
of our measures stopped annotators from exploring new di-
rections of asking and answering questions. In fact, anno-
tators were incentivized through monetary gifts based on

8812

the creativity of their annotations and their ability to bring
questions and answers from areas that were unexplored pre-
viously.

5. Experiments

The ﬁrst goal of our experiments is to analyze the perfor-
mance of state of the art on Social-IQ. We conduct extensive
evaluation of top performing models across scoreboards of
MovieQA [43]3, TVQA [26] 4, and CMU-MOSEI [58]. We
compare the performance of these models with each other
and human level performance in binary and multiple choice
setups. In binary case, models are given an answer and are
expected to predict whether or not the answer is correct or
incorrect. In multiple choice case, models should pick the
correct answer from a set of 4 answers (3 of which are in-
correct).

The second goal of our experiments is to identify any
potential biases in the Social-IQ. Models that target biases
are by design simple models that demonstrate whether or
not there is any trivial yet frequently occurring pattern in
the data that can be exploited during training. The follow-
ing baselines aim to explore these biases. In all these base-
lines, a LSTM model is used to encode sequential informa-
tion for each input modality, and answers are conditioned
on the concatenation of input encodings similar to [26]. We
ﬁrst outline the models for exploiting biases, followed by
state-of-the-art performing models on relevant tasks.

Q+A: We study the predictability of correct and incorrect
answers given only question and answers (no video, audio
or transcript). This baseline demonstrates whether or not
there exists a pattern across correct or incorrect answers
which can lead to identifying the correctness without any
context from videos. We use BERT embeddings [11] as
contextual distributed word representations for language.
BERT embeddings have shown to be suitable representa-
tions for both common sense reasoning and question an-
swering.

Q+A+T: This bias demonstrates the usefulness of tran-
scripts (T) in predicting the correct and incorrect answers.
Similar to Q+A baselines, distributional features of T are
also extracted using BERT embeddings 5. The sequence of
embeddings for T are then encoded using an LSTM.

Q+A+V: This bias demonstrates the usefulness of visual
modality (V) through using holistic visual embeddings in
predicting the correct and incorrect answers. We use rep-
resentations extracted from DenseNet161 [20] (last mean

3http://movieqa.cs.toronto.edu/leaderboard/
4http://tvqa.cs.unc.edu/leaderboard.html
5Since BERT tokenizes the input words, we modify the code for BERT
embeddings to keep mappings between tokens and words so the times-
tamps can be correctly calculated for the duration of the video.

Baseline
Metric
Random
Q+A (BERT) [11]
Q+A+T (BERT) [11]
Q+A+Ac (BERT+COVAREP)
Q+A+V (BERT+DenseNet161)

LMN [49]
FVTA [28]
E2EMemNet [41]
MDAM [24]
MSM [26]
TFN [55]
MFN [56]
Tensor-MFN
Human

Accuracy

A2 ↑
50.00
57.02
57.87
57.22
63.91

61.12
60.88
62.58
60.23
59.96
63.15
62.78
64.82
95.08

A4 ↑
25.00
28.61
29.36
29.58
32.62

31.81
31.01
31.46
30.71
29.89
29.82
30.86
34.14

-

Table 1: Performance of various models including state of
the art in MovieQA [43], TVQA [26], and CMU-MOSEI
[57]. A2 ↑ demonstrates the binary accuracy and A4
↑ demonstrates multiple (four) choice (higher is better).
There is a large discrepancy between human level perfor-
mance and neural state of the art, a gap of 30.26% in binary
question answering task.

pooling layer, 2208 dimensions) for each frame. While
videos are originally in 30fps sampling rate, we only use
1fps for baseline experiments.

Q+A+Ac: This bias demonstrates the usefulness of acous-
tic (Ac) modality in predicting the correct and incorrect an-
swers. We use low and high level acoustic representations
from COVAREP [10] including 12 Mel-frequency cepstral
coefﬁcients, pitch tracking and voiced/unvoiced segmenting
features [12], glottal source parameters [9, 13, 1, 3, 2], peak
slope parameters and maxima dispersion quotients [22].

The following baselines are among the state of the art for
TVQA [26], MovieQA [43] and CMU-MOSEI [58] dataset.
We pick these baselines based on their performance and
structural diversity.

Memory

Multimodal

Network
End2End
(E2EMMemNet):
This baseline has shown promis-
ing performance on MovieQA dataset. We implement this
baseline based on the original implementation [41] and
multimodal extensions using DenseNet161 features and
COVAREP.

Multimodal Dual Attention Memory (MDAM) [24]:
This baseline is also among the top performing for
MovieQA dataset. MDAM uses two attentions: 1) self-
attention (temporal) based on visual frames and 2) cross-
attention based on question. Afterwards, the answering is
done using a deep recurrent neural network.

8813

Layered Memory Network (LMN) [49]: This baseline is
the winner of “The Joint Video and Language Understand-
ing Workshop” in ICCV 2017 6 and still a strong performing
model for MovieQA. The baseline has two main modules:
Static Word Memory Module, which builds a representation
of the transcription words based on the visual frames, and
Dynamic Subtitle Memory Module, which builds a repre-
sentation of the transcription sentences based on the high
level descriptors of the frames.

Focal Visual-Text Attention (FVTA) [28]: FVTA is a
strong baseline for MovieQA. This baseline proposed a new
form of attention called Focal Visual-Text (FVT); an exten-
sion of attention which uses outer-product to build a joint
multimodal space.

Multi-stream Memory (MSM) [26]: This baseline is the
top performing baseline of TVQA dataset. Multiple streams
of data from visual, acoustic and language are fused to-
gether to answer questions. All the modalities are embed-
ded using recurrent networks and fused together in subse-
quent stages to answer questions.

Tensor Fusion Network (TFN) [55]: Originally proposed
for multimodal sentiment analysis, we extend this model for
question answering by conditioning the answer based on an
outer tensor-product of embeddings of transcript, visual and
acoustic modalities. A strong aspect of TFN is perform-
ing fusion on unimodal, bimodal and trimodal components
of the data. Before fusion, the modalities are summarized
using three LSTMs. The output of fusion is added to the
question and answer to make a ﬁnal prediction.

Memory Fusion Network (MFN) [56]: This models is
used for the tasks of sentiment analysis, emotion recog-
nition and personality traits recognition.
It uses a delta-
memory attention which stores the sequntial changes of
memory across multiple LSTMs. Afterwards, it performs
multimodal fusion over the changes in modalities and stores
the information in a separate memory. MFN model uses
alignment information between transcript, audio and video
which is an important component speciﬁcally in under-
standing multimodal language [58].

Tensor-MFN is a baseline created by performing architec-
ture and hyperparameter search on TFN and MFN mod-
els and combining them into a joint model.
In simple
terms, Tensor-MFN uses DenseNet161 scene embeddings
and Tensor Fusion for multimodal fusion in the recurrent
stages of MFN.

Human Performance demonstrates the human perfor-
mance (annotators did not see the question and the video
prior) in picking correct answer for question-answer pair in

6http://movieqa.cs.toronto.edu/workshops/iccv2017/

binary format, similar to the setup used for all the baselines.

6. Results and Discussion

Table 1 demonstrates the performance of the baselines
in Subsection 5. At a ﬁrst glance, our bias analysis exper-
iments demonstrate minimal bias in the Social-IQ dataset
coming from Q+A. BERT embeddings, commonly known
for their success in common-sense reasoning, show slightly
higher performance than random. This essentially demon-
strates that common sense reasoning purely by looking at
question and answers is not enough for answering the ques-
tions in Social-IQ. Answering questions in the Social-IQ
dataset requires both common sense and context. Contex-
tual information from T, Ac, and V are able to improve the
answering performance. Speciﬁcally, the improvement is
highest by adding visual information from DenseNet161.

Aside the performance of bias analysis models, results of
state of the art models from MovieQA, TVQA, and CMU-
MOSEI are reported in Table 1. Human performance (cal-
culated during the validation stage) is reported 95.08% for
the binary task. The gap between state of the art model
and human performance remains large. This signiﬁes the
challenging nature of Social-IQ dataset and the necessity of
further research in this direction.

7. Conclusion

To conclude, this paper introduced Social-IQ (Social
Intelligence Queries), a pioneer real-world unconstrained
dataset designed to evaluate the social intelligence and ca-
pabilities of existing and future AI technologies. Social-IQ
also focuses on the explainability of models by using open-
ended answers to model the rationale behind the model’s
comprehension of social intelligence. The rigorously an-
notated dataset contains 7, 500 questions with 52, 500 an-
swers spanning across 1, 250 natural social situations. Our
experimental results show that although humans can rea-
son about open-ended social intelligence with high accuracy
(95.08%), existing QA models struggle on this task. As a
result, Social-IQ is a challenging dataset that we hope will
instigate future research in social intelligence modeling, vi-
sual reasoning, and multimodal QA. The dataset is made
publicly available for research purposes alongside provided
features.

Acknowledgment

We would like to thank our annotators for their dedica-
tion in annotating the Social-IQ dataset. Furthermore, we
would like to thank Amy Lee and Helen Li for preparation
of tutorials as well as their management and guidance of an-
notators. This work was funded by National Science Fouda-
tion (NSF) grant 1750439 and Oculus Research (Facebook
Reality Labs).

8814

References

[1] Paavo Alku. Glottal wave analysis with pitch synchronous
iterative adaptive inverse ﬁltering. Speech communication,
11(2-3):109–118, 1992.

[2] Paavo Alku, Tom B¨ackstr¨om, and Erkki Vilkman. Nor-
malized amplitude quotient for parametrization of the glot-
tal ﬂow.
the Journal of the Acoustical Society of America,
112(2):701–710, 2002.

[3] Paavo Alku, Helmer Strik, and Erkki Vilkman. Parabolic
spectral parametera new method for quantiﬁcation of the
glottal ﬂow. Speech Communication, 22(1):67–79, 1997.

[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.

[5] Jyoti Aneja, Aditya Deshpande, and Alexander G. Schwing.
In The IEEE Conference
Convolutional image captioning.
on Computer Vision and Pattern Recognition (CVPR), June
2018.

[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
Vqa: Visual question answering. In Proceedings of the IEEE
International Conference on Computer Vision, pages 2425–
2433, 2015.

[7] Tadas Baltruˇsaitis, Chaitanya Ahuja, and Louis-Philippe
Morency. Multimodal machine learning: A survey and tax-
onomy. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2018.

[8] William F Chaplin, Oliver P John, and Lewis R Goldberg.
Conceptions of states and traits: dimensional attributes with
ideals as prototypes. Journal of personality and social psy-
chology, 54(4):541, 1988.

[9] Donald G Childers and CK Lee. Vocal quality factors: Anal-
ysis, synthesis, and perception. the Journal of the Acoustical
Society of America, 90(5):2394–2410, 1991.

[10] Gilles Degottex, John Kane, Thomas Drugman, Tuomo
Raitio, and Stefan Scherer. Covarepa collaborative voice
analysis repository for speech technologies.
In Acoustics,
Speech and Signal Processing (ICASSP), 2014 IEEE Inter-
national Conference on, pages 960–964. IEEE, 2014.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Pre-training of deep bidirectional
arXiv preprint

Toutanova.
transformers for language understanding.
arXiv:1810.04805, 2018.

Bert:

[12] Thomas Drugman and Abeer Alwan. Joint robust voicing
detection and pitch estimation based on residual harmonics.
In Interspeech, pages 1973–1976, 2011.

[13] Thomas Drugman, Mark Thomas, Jon Gudnason, Patrick
Naylor, and Thierry Dutoit. Detection of glottal closure in-
stants from speech signals: A quantitative review.
IEEE
Transactions on Audio, Speech, and Language Processing,
20(3):994–1006, 2012.

[14] Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon,
Boqing Gong, and Junzhou Huang. End-to-end learning
of motion representation for video understanding.
In The

IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.

[15] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei
Wang, and Wei Xu. Are you talking to a machine? dataset
and methods for multilingual image question. In Advances
in neural information processing systems, pages 2296–2304,
2015.

[16] Jennifer M George. State or trait: Effects of positive mood on
prosocial behaviors at work. Journal of applied Psychology,
76(2):299, 1991.

[17] Bert F Green Jr, Alice K Wolf, Carol Chomsky, and Kenneth
Laughery. Baseball: an automatic question-answerer. In Pa-
pers presented at the May 9-11, 1961, western joint IRE-
AIEE-ACM computer conference, pages 219–224. ACM,
1961.

[18] Eduard H Hovy. What are sentiment, affect, and emotion?
applying the methodology of michael zock to sentiment anal-
ysis. In Language production, cognition, and the Lexicon,
pages 13–24. Springer, 2015.

[19] De-An Huang, Vignesh Ramanathan, Dhruv Mahajan,
Lorenzo Torresani, Manohar Paluri, Li Fei-Fei, and Juan
Carlos Niebles. What makes a video a video: Analyz-
ing temporal information in video understanding models and
datasets. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.

[20] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely
connected convolutional networks. CoRR, abs/1608.06993,
2016.

[21] Thelma Hunt. The measurement of social intelligence. Jour-

nal of Applied Psychology, 12(3):317, 1928.

[22] John Kane and Christer Gobl. Wavelet maxima dispersion
for breathy to tense voice discrimination. IEEE Transactions
on Audio, Speech, and Language Processing, 21(6):1170–
1179, 2013.

[23] John F Kihlstrom and Nancy Cantor. Social intelligence.

Handbook of intelligence, 2:359–379, 2000.

[24] Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and
Byoung-Tak Zhang. Multimodal dual attention mem-
ory for video story question answering.
arXiv preprint
arXiv:1809.07999, 2018.

[25] Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris
Dyer, Karl Moritz Hermann, G´aabor Melis, and Edward
Grefenstette. The narrativeqa reading comprehension chal-
lenge. Transactions of the Association of Computational Lin-
guistics, 6:317–328, 2018.

[26] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
Tvqa: Localized, compositional video question answering.
In EMNLP, 2018.

[27] Willem J.M Levelt and Stephanie Kelter.

Surface form
and memory in question answering. Cognitive Psychology,
14(1):78 – 106, 1982.

[28] Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, and
Alexander Hauptmann. Focal visual-text attention for visual
question answering. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 6135–
6143, 2018.

8815

[29] Paul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-Philippe
Morency. Multimodal language analysis with recurrent mul-
tistage fusion.
In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing, EMNLP,
2018.

[30] Chin-Yew Lin. Rouge: A package for automatic evaluation
of summaries. In Proc. ACL workshop on Text Summariza-
tion Branches Out, page 10, 2004.

[31] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-
narasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and
Louis-Philippe Morency. Efﬁcient low-rank multimodal fu-
sion with modality-speciﬁc factors.
In Proceedings of the
56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 2018.

[32] Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan
AlRegib, and Hans Peter Graf. Attend and interact: Higher-
order object interactions for video understanding.
In The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2018.

[33] Mateusz Malinowski and Mario Fritz. A multi-world ap-
proach to question answering about real-world scenes based
on uncertain input. In Advances in neural information pro-
cessing systems, pages 1682–1690, 2014.

[34] Alexander Mathews, Lexing Xie, and Xuming He. Semstyle:
Learning to generate stylised image captions using unaligned
text. In The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), June 2018.

[35] Maureen O’sullivan et al. Measurement of social intelli-

gence. 1965.

[36] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: A method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, ACL ’02,
pages 311–318, Stroudsburg, PA, USA, 2002. Association
for Computational Linguistics.

[37] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring
models and data for image question answering. In Advances
in neural information processing systems, pages 2953–2961,
2015.

[38] Matthew Richardson, Christopher J.C. Burges, and Erin
Renshaw. Mctest: A challenge dataset for the open-domain
machine comprehension of text. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 193–203. Association for Computational Lin-
guistics, 2013.

[39] David M Romney and Michael C Pyryt. Guilford’s con-
cept of social intelligence revisited. High Ability Studies,
10(2):137–142, 1999.

[40] Constanze Rossmann. Theory of reasoned action-theory of
planned behavior. Nomos Verlagsgesellschaft mbH & Co.
KG, 2010.

[41] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-
to-end memory networks. In Advances in neural information
processing systems, pages 2440–2448, 2015.

[42] Piotr Sztompka. Socjologia znak. 2002.
[43] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,
Antonio Torralba, Raquel Urtasun, and Sanja Fidler.

through
MovieQA: Understanding Stories
Question-Answering. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2016.

in Movies

[44] Edward L Thorndike.

Intelligence and its uses. Harper’s

magazine, 1920.

[45] Robert L Thorndike and Saul Stein. An evaluation of the
attempts to measure social intelligence. Psychological Bul-
letin, 34(5):275, 1937.

[46] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-
Learn-
Philippe Morency, and Ruslan Salakhutdinov.
ing factorized multimodal representations. arXiv preprint
arXiv:1806.06176, 2018.

[47] Philip E Vernon. Some characteristics of the good judge of
personality. The Journal of Social Psychology, 4(1):42–57,
1933.

[48] Ronald E Walker and Jeanne M Foley.

Social intelli-
gence: Its history and measurement. Psychological Reports,
33(3):839–864, 1973.

[49] Bo Wang, Youjiang Xu, Yahong Han, and Richang Hong.
Movie question answering: Remembering the textual cues
for layered visual contents. In AAAI, 2018.

[50] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong
Xu. Bidirectional attentive fusion with context gating for
dense video captioning. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.

[51] Junbo Wang, Wei Wang, Yan Huang, Liang Wang, and Tie-
niu Tan. M3: Multimodal memory modelling for video cap-
tioning. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.

[52] Max Weber. The nature of social action. Weber: Selections

in translation, pages 7–32, 1978.

[53] Susanne Weis and Heinz-Martin S¨uß. Reviving the search
for social intelligence–a multitrait-multimethod study of its
structure and construct validity. Personality and individual
differences, 42(1):3–14, 2007.

[54] William A Woods. Semantics and quantiﬁcation in natural
In Advances in computers,

language question answering.
volume 17, pages 1–87. Elsevier, 1978.

[55] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria,
and Louis-Philippe Morency. Tensor fusion network for mul-
timodal sentiment analysis. In Empirical Methods in Natural
Language Processing, EMNLP, 2017.

[56] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya
Poria, Erik Cambria, and Louis-Philippe Morency. Memory
fusion network for multi-view sequential learning. Proceed-
ings of the Thirty-Second AAAI Conference on Artiﬁcial In-
telligence, 2018.

[57] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe
Morency. Multimodal sentiment intensity analysis in videos:
Facial gestures and verbal messages. IEEE Intelligent Sys-
tems, 31(6):82–88, 2016.

[58] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. Multimodal lan-
guage analysis in the wild: Cmu-mosei dataset and inter-
pretable dynamic fusion graph. In Proceedings of the 56th
Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages 2236–
2246, 2018.

8816

[59] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. Swag: A large-scale adversarial dataset for grounded
commonsense inference. arXiv preprint arXiv:1808.05326,
2018.

[60] Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang,
Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. Lever-
aging video descriptions to learn video question answering.
CoRR, abs/1611.04021, 2016.

[61] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks.
IEEE Signal Processing Letters,
23(10):1499–1503, 2016.

[62] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,
and Caiming Xiong. End-to-end dense video captioning with
masked transformer. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.

[63] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.
Visual7w: Grounded question answering in images. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4995–5004, 2016.

8817

