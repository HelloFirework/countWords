A Content Transformation Block For Image Style Transfer

Dmytro Kotovenko

Artsiom Sanakoyeu

Pingchuan Ma

Sabine Lang

Bj¨orn Ommer

Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University

Abstract

Style transfer has recently received a lot of attention,
since it allows to study fundamental challenges in image
understanding and synthesis. Recent work has signiﬁcantly
improved the representation of color and texture and com-
putational speed and image resolution. The explicit trans-
formation of image content has, however, been mostly ne-
glected: while artistic style affects formal characteristics of
an image, such as color, shape or texture, it also deforms,
adds or removes content details. This paper explicitly fo-
cuses on a content-and style-aware stylization of a content
image. Therefore, we introduce a content transformation
module between the encoder and decoder. Moreover, we
utilize similar content appearing in photographs and style
samples to learn how style alters content details and we
generalize this to other class details. Additionally, this work
presents a novel normalization layer critical for high reso-
lution image synthesis. The robustness and speed of our
model enables a video stylization in real-time and high def-
inition. We perform extensive qualitative and quantitative
evaluations to demonstrate the validity of our approach.

1. Introduction

Style transfer renders the content of a real photograph
in the style of an artist using either a single style sam-
ple [11] or a set of images [28].
Initial work on style
transfer by Gatys et al. [9] proposed a method which ex-
ploits a deep CNN (Convolutional Neural Network) pre-
trained on a large dataset of natural images. Their costly
computational optimization process has been replaced by
an efﬁcient encoder-decoder architecture in recent works
[18, 28, 7, 2, 13] that efﬁciently generate the stylized output
in a single feed-forward pass. While [18] has proven that
an encoder-decoder architecture is both fast and effective
for transferring style, it acts as a black-box model, lacking
interpretability and accurate control of style injection: con-
tent transformation is performed indirectly, meaning there
is no explicit control which part of the network carries out
the stylization of photos and to what extend. To address this

2compvis.github.io/content-targeted-style-transfer/

Figure 1. Examples of generated images using our approach in the
style of (from top) Vincent van Gogh, Pablo Picasso, Paul Cezanne
and Wassily Kandinsky. More stylization examples of images and
videos can be found on the project page2.

10032

Figure 2. Illustrates the role of the content transformation block. Real input images and painting examples displaying similar content are
matched in the feature space. We utilize content similarity to learn how style explicitly alters content details.

issue, [28] introduced a ﬁxpoint loss that ensures stylization
has converged and reached a ﬁxpoint after one feed-forward
pass. This style-aware content loss forces the stylization to
take place in the decoder. However, the main issue remains:
The decoder alters style, synthesizes the stylized image, and
upsamples it. All these individual tasks cannot be learned
and controlled individually.

As a remedy, we introduce a novel content transforma-
tion block between encoder and decoder allowing control
over stylization and achieving a style-aware editing of con-
tent images. We force the encoder to explicitly extract con-
tent information; the content transformation block T then
modiﬁes the content information in a manner appropriate to
the artist’s style. Eventually the decoder superimposes the
style on the altered content representation. Our approach
measures the content similarity between the content target
image and stylized image before and after the transforma-
tion.

In contrast to previous work, stylization should be object
speciﬁc and depending on the underlying object, the style
transformation needs to adapt. The Cubist style of Picasso,
for example, tends to reduce the human nose to a simple tri-
angle or distorts the location of the eyes. Therefore, we fur-
ther investigate, if we can achieve an object-speciﬁc alter-
ation. We utilize similar content appearing in photographs
and style samples to learn how style alters content details.
We show that by using a prominent, complex, and diverse
object class, i.e., persons, our model can learn how details
are to be altered in a content-and style-aware manner. More-
over, the model learns to generalize beyond this one partic-
ular object class to diverse content. This is crucial to styl-
ize also modern objects like computers which an artist like
Monet never painted. In addition, we propose a local feature
normalization layer to reduce the number of artifacts in styl-
ized images, signiﬁcantly improving results when moving
to other image collections (i.e. from Places365 [34] to Ima-
geNet [27]) and increasing the image resolution. To validate

the performance of our approach, we perform various qual-
itative and quantitative evaluations of stylized images and
also demonstrate the applicability of our method to videos.
Additional results can be found on the project page.

2. Related Work

Texture synthesis Neural networks were long used for
texture synthesize [10]; feed-forward networks then enable
a fast synthesis, however these methods often display a lack
of diversity and quality [18, 30]. To circumvent this is-
sue, [21] propose a deep generative feed-forward network,
which allows to synthesize multiple textures within one
[12] has demonstrated how control over
single network.
spatial location, color and across spatial scale leads to en-
hanced stylized images, where regions are altered by differ-
ent styles; control over style transfer has been extended to
stroke sizes [17]. [24] used a multiscale synthesis pipeline
for spatial control and to improve texture quality and stabil-
ity.
Separating content and style The integration of localized
style losses improved the separation of content and style.
In order to separate and recombine style and content in an
image, works have utilized low-level features for texture
transfer and high-level information to represent content us-
ing neural networks [11]. [6, 3, 8, 33] focused on distin-
guishing between different contents, styles and techniques
in the latent space; to translate an image to another image
is a vision problem, where the mapping between input and
output image relies on aligned pairs. To avoid the need for
paired examples, [35] presented an adversarial loss coupled
with a cycle consistency loss to effectively assign two im-
ages. On the basis of [35], [28] has proposed an approach,
where a style-aware content loss helps to focus on those
content details relevant for a style. A combination of gen-
erative Markov random ﬁeld (MRF) models and deep con-
volutional neural networks have been used for the task of

10033

synthesizing content of photographs and artworks [20].
Real-time and super-resolution The processing time of
style transfer and the resolution of images have been fur-
ther addressed. Scholars aimed to achieve stylization in real
time and in super-resolution using an unsupervised training
approach, where either neural network features and statis-
tics compute the acquired loss function [18] or a multi-
scale network is employed [30]. To achieve a better qual-
ity for stylized images in high resolution, [32] propose a
multimodal convolutional network, which performs a hier-
archical stylization by utilizing multiple losses of increasing
scales.
Stylizing videos While these works have approached the
task of style transfer for input photographs, others concen-
trated on transferring artistic style to videos [25, 15, 28, 26],
using feed-forward style transfer networks [4] or networks,
which do not rely on optical ﬂow at test time [15] to improve
the consistency of stylization.

3. Approach

Let Y be a collection of images that deﬁnes a style. We
extract the very essence of an artistic style presented in Y
and learn to transfer it onto images from a different dataset
X, such as photos. This formulation resembles a typical
unsupervised image translation problem, which requires a
generator G (usually consisting of the encoder E and de-
coder D) and a discriminator D trained against each other:
one mimics the target distribution Y, the other one distin-
guishes between the authentic sample y ∈ Y and the styl-
ized sample D(E(x)) for x ∈ X. Hence, we can extract
the style by solving the min-max optimization task for the
standard adversarial loss:

Ladv := E

y∼Y

[log(D(y))]+

E

x∼X

[log (1 − Ds (D(E(x))))]

(1)

Let s be additional content information that is easily avail-
able, i.e., we utilize a simple coarse scene label of the image
x. Now the discriminator should not only discern real from
synthesized art. It should also enforce that the scene infor-
mation is retained in D(E(x)) by the stylization process,

Lcadv := E

y∼Y

[log(D(y))]+

E

(x,s)∼X

[log (1 − D (D(E(x))|s))]

(2)

In contrast to a GAN framework that generates an image
from a random vector z, style transfer not only requires to
stylize a real input photograph x but also to retain the con-
tent of the input image after stylization. The simplest solu-
tion would be to enforce a per-pixel similarity between the
input x ∼ X and stylized image G(x):

Lpxl := E

x∼X

[kD(E(x)) − xk2

2].

(3)

However, this loss alone would counter the task of styliza-
tion, since the image should not be the same afterwards on
a per-pixel basis. Previous work [18, 9] has utilized a pre-
trained perceptual loss [29]. Since this loss is pretrained on
an image dataset unrelated to any speciﬁc style, it cannot
account for the characteristic way in which an artist alters
content. Rather, we enforce the stylization to have reached
a ﬁxpoint, meaning that another round of stylization should
not further alter the content. The resulting ﬁxpoint loss
measures the residual in the style-speciﬁc encoding space
E(·),

LF P := E

x∼X

[kE(D(E(x))) − E(x)k2

2].

(4)

3.1. Content Transformation Block

While a painting of an artist is associated with one style,
it is noticeable that style affects image regions differently:
to emphasize the importance of an individual object, artists
would use a more expressive brushstroke or deform it to a
higher degree. Therefore, we do not only want to learn a
simple stylization but a content-speciﬁc stylization. Thus
each content detail must be stylized in a manner speciﬁc to
this particular content category. This means that a stylized
human ﬁgure should resemble how an artist has painted the
ﬁgure in a speciﬁc style and not an arbitrary object, such as
a vase or a chair. We enforce this capability by pulling im-
ages of similar content – but from different domains(art and
photograph) – closer to each other in the latent space, while
keeping dissimilar content images apart from each other. To
be more speciﬁc, we force the content representation of an
input photograph belonging to a speciﬁc class c to become
more similar to the input painting’s content representation
of the same class. To achieve this, we introduce a content
transformation block T transforming the output representa-
tion of the encoder E. We train this block in the adversarial
fashion: the discriminator Dc has to distinguish between the
representation of the real artworks’ content and the trans-
formed representation of the input photographs. But since
we strive to obtain a content speciﬁc stylization, the dis-
criminator Dc also has to classify the content class c of the
artwork y and the content class of the input photograph x.
Supplied with the content information c discriminator be-
comes more sensitive to content speciﬁc visual clues and
enforces the content transformation block to mimic them in
an artistic way.

Ladv−cont := E

(y,c)∼Y

[log(Dc(E(y)|c)))]+

E

(x,c)∼X

[log (1 − Dc (T (E(x))|c))]

(5)

In terms of neural architecture the T represents a con-
catenation of nine “residual blocks”. Each block consists
of six consecutive blocks with a skip connection: conv-

10034

Figure 3. The two ﬁgures describe the two alternating training steps. The ﬁrst step (top) is designated to obtain an artistic stylization while
retaining the content information of the input photograph. The second step (bottom) trains the content transformation block T to alter the
image content in a style-speciﬁc style. The lock sign indicates that the weights are ﬁxed. See Approach section for further details.

layer, LFN-layer, lrelu-activation, conv-layer, LFN-
layer, lrelu-activation.

3.2. Local Feature Normalization Layer

Many approaches using convolutional networks for im-
age synthesis suffer from domain change (i.e. from photos
of landscapes to faces) or synthesis resolution change. As
a result, the inference size is often identical to the training
size or the visual quality of the results deteriorates when
switching to another domain. Reason being that instance
normalization layers overﬁt to image statistics and the layer
is not able to generalize to another image. We can improve
the ability to generalize by enforcing stronger normalization
through our local feature normalization layer. This layer
normalizes the input tensor across a group of channels and
also acts locally, not seeing the whole tensor but only the
vicinity of the spatial location. Formally, for an input tensor
T ∈ RB×H×W ×C , where B stands for the samples num-
ber, height H, width W and having C channels, we can
deﬁne a Local Feature Normalization Layer(LFN) with pa-
rameters W S denoting spatial resolution of the normaliza-
tion window and G - number of channels across which we
normalize:

LF N (·|W S, G) : RB×H×W ×C −→ RB×H×W ×C.

To simplify the notation, we ﬁrst deﬁne a subset of the
tensor T around (b, h, w, c) with a spatial window of size

W S × W S and across a group of G neighbouring channels:

BW S,G(T, b, h, w, c) :=




T (b, x, y, z)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

h − W S/2 ≤ x ≤ h + W S/2
w − W S/2 ≤ y ≤ w + W S/2
G(cid:5)G + G

≤ z ≤ (cid:4) c

(cid:4) c
G(cid:5)G




.

(6)

Finally, we can write out the expression for the Local Fea-
ture Normalization Layer applied to tensor T as:

LF N (T |W S, G)(b, h, w, c) :=
T (b, h, w, c) − mean[BW S,G(T, b, h, w, c)]

γc

std[BW S,G(T, b, h, w, c)]

+ βc.

(7)

In this equation, similar to the Instance Normalization
Layer [31], parameters γ, β ∈ RC denote vectors of train-
able parameters and represent how to scale and shift each
channel; those are learned jointly with other weights of the
network via back-propagation. However, in practice the
computation of mean and std of a large tensor could be
a laborious task, so we compute these values only at the
selected locations (b, h, w, c) and interpolate for others.

3.3. Training Details

The training dataset X is the union of the Places365
dataset [34] and the COCO dataset [23], such that for a
tuple (x, c, s) ∈ X where x is a photograph, s is a scene
class if x is from the Places dataset and c is a content class
if x is from the COCO dataset. The second dataset Y con-
tains tuples (y, c) where y is the artwork and c is the content

10035

class. We focus on the content classes “person” and a nega-
tive class “non-person”. The generator network consists of
encoder E, transformer block T and decoder D. We utilize
two conditional discriminators Ds and Dc - the former is
applied to the input images and stylized outputs. The latter
is applied to the content representation obtained by encoder
E. Given this notation the losses become

Ladv−style := E

(y,c)∼Y

[log(Ds(y))]+

E

(x,c,s)∼X

[log (1 − Ds (D(T (E(x)))|s))]

Ladv−cont := E

(y,c)∼Y

[log(Dc(E(y)|c)))]+

E

(x,c,s)∼X

[log (1 − Dc (T (E(x))|c))]

(8)

(9)

LF P :=

E

(x,c,s)∼X

[kE(D(T (E(x)))) − E(x)k2

2].

(10)

Training procedure For variables θE, θD, θT , θDc , θDs
denoting parameters of the blocks E, D, T , Dc, Ds. Train-
ing is performed in two alternating optimization steps.

The ﬁrst step designated to obtain an accurate content
extraction in encoder E and to learn a convincing style in-
jection by decoder D.

min
θE ,θD

max
θDs

λLpxl Lpxl+

λLF P LF P + λLadv−styleLadv−style

(11)

The second step is aimed to learn style-speciﬁc content

editing by the block T .

min
θT

λLadv−cont Ladv−cont+

max
θDc
λLadv−styleLadv−style

(12)

Please see Figure 3 illustrating the alternating steps of

the training.

4. Experiments and Discussion

4.1. Stylization Assessment

To measure the quality of the generated stylizations we
provide qualitative results of our approach and perform sev-
eral quantitative experiments which we describe below.
Deception rate. This metric was introduced in [28] to asses
how good the target style characteristics are preserved in the
generated stylizations. A network pre-trained for artist clas-
siﬁcation should predict the artist which was used to gener-
ate the stylization. The deception rate is then calculated as
the fraction of times the network predicted the correct artist.
We report the deception rate for our and competing methods
in Tab. 1 in the ﬁrst column, where we can see that our ap-
proach outperforms other methods by a signiﬁcant margin.

Figure 4. Can you guess which patches are real and which were
generated by our approach? Each row contains three patches gen-
erated by our model and two real patches. Artists: (from the top)
Cezanne, van Gogh, Claude Monet, Ernst Ludwig Kirchner and
Berthe Morisot. The solution is provided on the last page.

Expert and non-expert score. We also perform human
evaluation studies to highlight the quality of our styliza-
tion results. Given a content image patch, we stylize it
with different methods and show results alongside a patch
from a real painting to experts and non-experts. Both are
asked to guess which one of the shown patches is real. The
score is the fraction of times the stylization generated by
this method was selected as the real patch. This experiment
is performed with experts from art history and people with-
out art education. Results are reported in Tab. 1.
Expert preference score.
In addition, we asked art his-
torians to choose which of the stylized images resemble the
style of the target artist the most. Then the expert preference
score (see Tab. 1) is calculated as the fraction of times the
stylizations of the method was selected as the best among
the others. The quantitative results in Tab. 1 show that both
experts and non-experts prefer our stylizations in compari-
son to images obtained by other methods.
Content retention evaluation. To quantify how well the
content of the original image is preserved, we stylize the Im-
ageNet [27] validation dataset with different methods and
compute the accuracy using pretrained VGG-16 [29] and
ResNet-152 [14] networks averaged across 8 artists. Results
presented in Tab. 2 show that the best classiﬁcation score
is achieved on stylizations by CycleGAN [36] and Gatys
et al. [11], since both methods barely alter the content of
the image. However, our main contribution is that we sig-

10036

Method

Deception
rate [28]

Non-Expert
deception

Expert

Expert

deception

preference

AdaIn [16]

WCT [22]
PatchBased[5]
Johnson et al. [18]
CycleGan [36]
Gatys et al. [11]
AST [28]
Ours

Wikiart test

Photos

0.067

0.030
0.061
0.087
0.140
0.221
0.459
0.582

0.6156

0.002

score

0.033

0.033
0.118
0.013
0.026
0.088
0.056
0.178

0.454

-

score

0.016

0.001
0.011
0.001
0.031
0.068
0.131
0.220

0.528

-

score

0.019

0.009
0.038
0.010
0.010
0.118
0.341
0.456

-

-

Table 1. A higher score indicates better stylization results. All
scores are averaged over 8 different styles. The row ”Wikiart
test” [19] shows accuracy on real artworks from the test set. The
deception rate for ”Photos” shows how often photos were miss-
classiﬁed by the network as real paintings of the target artist.

Network

Original
photos

VGG-16

ResNet-152

0.710

0.783

Ours

0.016

0.057

AST
[28]

0.009

0.032

Gatys
[11]

0.271

0.389

CycleGAN

[36]

0.198

0.341

Table 2. Top-1 classiﬁcation accuracy on stylized images from val-
idations set of ImageNet [27] using the networks pretrained on
ImageNet. Note that the classiﬁcation accuracy of our model is
higher then the state-of-the art model [28]. We include results for
[11] and [36], but they are not directly comparable to our method
since they barely alter the content of the input image. In the second
column we present the classiﬁcation accuracy on the input photos.

Feature
extractor

VGG-16

VGG-19

Photographs

Ours

1.108

1.025

0.756

0.724

Ours
w/o T

0.882

0.838

AST [28]

0.812

0.808

Table 3. The table summarizes RSSCD computed using differ-
ent classiﬁcation networks for different stylization methods. The
score characterizes the content dissimilarity between real artworks
and stylized images, relative to the average content dissimilarity
between the artworks. The lower the better.

niﬁcantly outperform the state-of-the-art AST [28] model
on the content preservation task, while still providing more
convincing stylization results, measured by the deception
rate in Tab. 1.
Qualitative comparison. We compare our method qualita-
tively with existing approaches in Fig. 5. The reader may
also try to guess between real and fake patches generated
by our model in Fig.4. More qualitative comparisons be-
tween our approach and other methods are available in the
supplementary material.

4.2. Ablation Study

4.2.1 Content Transformation

Figure 5. Qualitative comparison: the ﬁrst column shows the entire
image stylized by our approach (the real content image is provided
on top), a detailed view is presented in the second. The third col-
umn shows the real photo detail respectively, while the last two
display results obtained by other methods. Zoom in for a better
view and details.

Photographs

Ours

Accuracy

Recall

Precision
F1-Score

0.953

0.978
0.927
0.954

0.659

0.375
0.864
0.524

Ours
w/o T

0.598

0.209
0.937
0.343

AST [28]

0.548

0.109
0.893
0.195

Table 4. Results of the person detection on the stylized images
from COCO dataset [23] using Mask-RCNN [1]. Columns from
left to right: Person detection on photos; on stylized images by our
method; on stylized images by our method without transformation
block; on stylized images by AST [28].

Relative style-speciﬁc content distance. To verify that
the image content is transformed in a style-speciﬁc manner,

we introduce a quantitative measure, called relative style-
speciﬁc content distance (RSSCD). It measures the ratio

10037

between the average distance of the generated image styl-
izations to the closest artworks and the average distance be-
tween all the artworks. Distances are computed using the
features φ(·) of the classiﬁcation CNN pretrained on Ima-
geNet. Then, RSSCD is deﬁned as

RSSCD :=

1

|Zp| Pz∈Zp

min
y∈Yp

kφ(z) − φ(y)k2

1

|Yp||Yn|

Pyp∈Yp,yn∈Yn

kφ(yp) − φ(yn)k2

,

Zp denotes the set of stylizations of the positive content
class (e.g., person), Yp denotes the set artworks of the pos-
itive content class, and Yn denotes all other artworks (see
Fig. 6 for an illustration).

We report the RSSCD for our model with and without
T . For comparison we also evaluate the state-of-the-art ap-
proach AST [28]. Here, we use class ”person” as the pos-
itive content class and two pretrained networks as content
feature extractors φ(·), namely VGG-16 and VGG-19 [29].
As can be seen in Tab. 3, the content transformation block
signiﬁcantly decreases the distance between the stylized im-
ages and original van Gogh paintings, proving its effective-
ness.

We measure how well our model retains the information
present in the selected “person” class and compare it to both
the model not using T and to the AST [28]. We run the
Mask-RCNN detector [1] on images from the COCO [23]
dataset stylized by different methods and compute the accu-
racy, precision, recall and F1-score. From results ins Tab. 4
we conclude that the proposed block T helps to retain visual
details relevant for the “person” class.

In Fig. 7 we show stylizations of our method with and
without content transformation block. We recognize that
applying the content transformation block alters the shape
of the human ﬁgures in a manner appropriate to van Gogh’s
style resulting in curved forms (cf. the crop-outs from orig-
inal paintings by van Gogh provided in the 4th column of
Fig. 7). For small persons, the artist preferred to paint ho-
mogeneous regions with very little texture. This is appar-
ent, for example, in the stylized patches in row one and six.
Lastly, while van Gogh’s self-portraits display detailed fa-
cial features, in small human ﬁgures he tended to remove
them (see our stylizations in 3rd and 4th rows of Fig. 7).
This might be due to his abstract style, which included a
fast-applied and coarse brushstroke.

4.2.2 Generalization Ability

The transformer block T learns to transform content rep-
resentation of the photographs of class “person” in such a
way that it becomes indistinguishable of the content rep-
resentation of artworks of the same class. Though trans-
formation has been learned for one only class “person” it

Figure 6. Illustration of the RSSCD measure. Real positive, real
negative and stylized positive images are mapped into the feature
space using deep CNN. We then compute the average distance
from a stylized positive to the closest real positive and divide it
by the average distance between real positives and negatives. Pos-
itive images correspond to the class “person”.

Method

Deception
rate [28]
“person”

Deception

Non-Expert

rate

deception score

non-“person”

“person”

Non-Expert

deception score
non-“person”

AST
Ours w/o T
Ours

0.398
0.521
0.618

0.485
0.541
0.563

0.016
0.127
0.210

0.086
0.143
0.165

Table 5. Stylization quality for different content classes. Our
model has signiﬁcantly improved stylization quality compared to
the state-of-the-art AST[28] model. The T block improves the de-
ception rate and preference score on both classes. The higher the
better.

can still generalize to other classes. To measure this gen-
eralization ability we compute the deception rate[28] and
non-expert deception scores on stylized patches for classes
“person” and non-“person” separately. The evaluation re-
sults are provided in Tab. 5 and indicate improvement of
the stylization quality for unseen content.

4.2.3 Artifacts Removal

To verify the effectiveness of the local feature normalization
layer (LFN layer), we perform a visual inspection of learned
models and notice prominent artifacts illustrated in Fig. 8.
We can observe that especially for plain regions with lit-
tle structure, the model without a LFN layer often produces
unwanted artifacts. In comparison, results obtained with an
LFN layer show no artifacts in the same regions. Notably,
for a model without an LFN layer the number of artifacts
increases proportionally to the resolution of the stylized im-
age.

10038

Figure 8. From left to right: stylized image with LFN layer, detail
of the image, same region for the model without LFN layer. Styles
from top to bottom: Cezanne, Kirchner, Cezanne, Paul Gauguin.
Local feature normalization signiﬁcantly reduces the number of
artifacts by normalizing the network activation statistics.

in a content-and style-speciﬁc manner. We utilize objects
from the same class in content and style target images to
learn how content details need to be transformed. Exper-
iments show that from only one complex object category,
our model learns how to stylize details of content in general
and thus improves the stylization quality for other objects as
well. In addition, we proposed a local feature normalization
layer, which signiﬁcantly reduces the number of artifacts in
stylized images, especially when increasing the image res-
olution or applying our model to previously unseen image
types (photos of faces, road scenes etc.). The experimental
evaluation showed that both art experts and persons with-
out speciﬁc art education preferred our method to others.
Our model outperforms existing state-of-the-art methods in
terms of stylization quality in both objective and subjective
evaluations, also enabling a real-time and high deﬁnition
stylization of videos.

Acknowledgements

This work has been supported by a hardware donation

from NVIDIA Corporation.

Figure 7. Shows the impact of the content transformation block:
column two shows results without using a content transformation
block. The third displays identical details with a content transfor-
mation block, emphasizing outlines of ﬁgures in a manner appro-
priate to van Gogh’s style. The last column provides details from
the artist’s paintings to highlight the validity of our approach. Best
seen on screen and zoomed in.

5. Conclusion

We introduced a novel content-transformation block de-
signed as a dedicated part of the network to alter an object

Solution to Figure 4:

Cezanne: fake, real, fake, fake, real
van Gogh: real, fake, real, fake, fake
Monet: fake, real, fake, real, fake
Kirchner: real, fake, fake, fake, real
Morisot: fake, real, fake, real, fake.

10039

References

[1] W. Abdulla. Mask r-cnn for object detection and in-
stance segmentation on keras and tensorﬂow. https://
github.com/matterport/Mask_RCNN, 2017. 6, 7

[2] M. Babaeizadeh and G. Ghiasi. Adjustable real-time style

transfer. CoRR, abs/1811.08560, 2018. 1

[3] M. A. Bautista, A. Sanakoyeu, E. Tikhoncheva, and B. Om-
mer. Cliquecnn: Deep unsupervised exemplar learning. In
Advances in Neural Information Processing Systems, pages
3846–3854, 2016. 2

[4] D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua. Stylebank:
An explicit representation for neural image style transfer. In
Proc. CVPR, volume 1, page 4, 2017. 3

[5] T. Q. Chen and M. Schmidt. Fast patch-based style transfer
of arbitrary style. arXiv preprint arXiv:1612.04337, 2016. 6
[6] J. Collomosse, T. Bui, M. J. Wilber, C. Fang, and H. Jin.
Sketching with style: Visual search with sketches and aes-
thetic context. In ICCV, pages 2679–2687, 2017. 2

[7] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style. Proc. of ICLR, 2017. 1

[8] P. Esser, E. Sutter, and B. Ommer. A variational u-net
for conditional appearance and shape generation. CoRR,
abs/1804.04694, 2018. 2

[9] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm
of artistic style. arXiv preprint arXiv:1508.06576, 2015. 1,
3

[10] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
and the controlled generation of natural stimuli using convo-
lutional neural networks. arXiv preprint arXiv:1505.07376,
12, 2015. 2

[11] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks.
In Computer Vision
and Pattern Recognition (CVPR), 2016 IEEE Conference on,
pages 2414–2423. IEEE, 2016. 1, 2, 5, 6

[12] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2017. 2

[13] A. Gupta, J. Johnson, A. Alahi, and L. Fei-Fei. Character-
izing and improving stability in neural style transfer. 2017
IEEE International Conference on Computer Vision (ICCV),
pages 4087–4096, 2017. 1

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. CoRR, abs/1512.03385, 2015. 5

[15] H. Huang, H. Wang, W. Luo, L. Ma, W. Jiang, X. Zhu,
Z. Li, and W. Liu. Real-time neural style transfer for videos.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 7044–7052. IEEE, 2017. 3

[16] X. Huang and S. Belongie. Arbitrary style transfer in real-
time with adaptive instance normalization. In ICCV, 2017.
6

[17] Y. Jing, Y. Liu, Y. Yang, Z. Feng, Y. Yu, D. Tao, and M. Song.
Stroke controllable fast style transfer with adaptive receptive
ﬁelds. arXiv preprint arXiv:1802.07101, 9, 2018. 2

[18] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.

Conference on Computer Vision, pages 694–711. Springer,
2016. 1, 2, 3, 6

[19] S. Karayev, M. Trentacoste, H. Han, A. Agarwala, T. Darrell,
A. Hertzmann, and H. Winnemoeller. Recognizing image
style. arXiv preprint arXiv:1311.3715, 2013. 6

[20] C. Li and M. Wand. Combining markov random ﬁelds and
convolutional neural networks for image synthesis. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2479–2486, 2016. 3

[21] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
Diversiﬁed texture synthesis with feed-forward networks. In
Proc. CVPR, 2017. 2

[22] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
Universal style transfer via feature transforms. In Advances
in Neural Information Processing Systems, pages 385–395,
2017. 6

[23] T.-Y. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Gir-
shick, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L.
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 4, 6, 7

[24] E. Risser, P. Wilmot, and C. Barnes. Stable and controllable
neural texture synthesis and style transfer using histogram
losses. arXiv preprint arXiv:1701.08893, 2017. 2

[25] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer
for videos. In German Conference on Pattern Recognition,
pages 26–36. Springer, 2016. 3

[26] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer
International Journal of

for videos and spherical images.
Computer Vision, pages 1–21, 2018. 3

[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision,
115:211–252, 2015. 2, 5, 6

[28] A. Sanakoyeu, D. Kotovenko, S. Lang, and B. Ommer. A
In
style-aware content loss for real-time hd style transfer.
Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2018. 1, 2, 3, 5, 6, 7

[29] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3, 5, 7

[30] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
Texture networks: Feed-forward synthesis of textures and
stylized images. In ICML, pages 1349–1357, 2016. 2, 3

[31] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky.

Instance
normalization: The missing ingredient for fast stylization.
CoRR, abs/1607.08022, 2016. 4

[32] X. Wang, G. Oxholm, D. Zhang, and Y.-F. Wang. Mul-
timodal transfer: A hierarchical deep convolutional neural
network for fast artistic style transfer. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, volume 2, page 7, 2017. 3

[33] M. J. Wilber, C. Fang, H. Jin, A. Hertzmann, J. Collomosse,
and S. J. Belongie. Bam! the behance artistic media dataset
for recognition beyond photography. In ICCV, pages 1211–
1220, 2017. 2

10040

[34] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
Places: A 10 million image database for scene recognition.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2017. 2, 4

[35] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. arXiv preprint, 2017. 2

[36] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In IEEE International Conference on Computer Vi-
sion, 2017. 5, 6

10041

