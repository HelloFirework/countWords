HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for

Scene Flow Estimation on Large-scale Point Clouds

Xiuye Gu1,3, Yijie Wang2, Chongruo Wu3, Yong Jae Lee3, and Panqu Wang2

1Stanford University, 2TuSimple, 3University of California, Davis

Abstract

We present a novel deep neural network architecture for
end-to-end scene ﬂow estimation that directly operates on
large-scale 3D point clouds. Inspired by Bilateral Convo-
lutional Layers (BCL), we propose novel DownBCL, Up-
BCL, and CorrBCL operations that restore structural in-
formation from unstructured point clouds, and fuse infor-
mation from two consecutive point clouds. Operating on
discrete and sparse permutohedral lattice points, our archi-
tectural design is parsimonious in computational cost. Our
model can efﬁciently process a pair of point cloud frames
at once with a maximum of 86K points per frame. Our
approach achieves state-of-the-art performance on the Fly-
ingThings3D and KITTI Scene Flow 2015 datasets. More-
over, trained on synthetic data, our approach shows great
generalization ability on real-world data and on different
point densities without ﬁne-tuning.

1. Introduction

Scene ﬂow is the dense 3D motion ﬁeld of points. It is
the 3D counterpart of optical ﬂow, and is a more fundamen-
tal and unambiguous representation – optical ﬂow is sim-
ply the projection of scene ﬂow onto the image plane of a
camera [42]. Scene ﬂow can be useful in various ﬁelds, in-
cluding robotics, autonomous driving, human-computer in-
teraction, and can also be used to complement and improve
visual odometry and SLAM algorithms [15, 30].

Estimating scene ﬂow in 3D space directly with point
cloud inputs is appealing, as approaches that use stereo in-
puts require 3D motion reconstruction from optical ﬂow and
disparities, and thus the optimization is indirect.
In this
work, we focus on efﬁcient large-scale scene ﬂow estima-
tion directly on 3D point clouds.

The problem statement for scene ﬂow estimation is as
follows: The inputs are two point clouds (PC) at two con-
secutive frames: P C1 at time t and P C2 at time t +
1. Generally, each point has an associated feature fi =
(xi, yi, zi, . . .) ∈ Rdf , where (xi, yi, zi) are the 3D coor-

t
e
N
w
o
l
F
L
P
H

PC1

PC2

Scene Flow

Figure 1: Our end-to-end trainable HPLFlowNet takes two successive
frames of point cloud (PC) as input, and outputs dense estimation of the
3D motion ﬁeld for every point in the ﬁrst PC frame. The color for scene
ﬂow encodes magnitude/velocity from blue to red (small to large).

dinates for each point. Other low-level features, such as
color and normal vectors, can also be included.1 The out-
put is the predicted scene ﬂow for each point i in P C1:

csf i = (dxi, dyi, dzi). We use the world coordinate system

as the reference system; the goal is to estimate the scene
ﬂow of both ego-motion and motion of dynamic objects;
see Fig. 1.

Many existing deep learning approaches for 3D point
cloud processing [33, 35, 24, 46] focus on accuracy but put
less emphasis on minimizing computational cost. Conse-
quently, these networks can only deal with a limited num-
ber of points at once due to limited GPU memory, which
is unfavorable for large-scale scene analysis. The reason
is twofold: 1) these methods frequently resort to dividing

1In our experiments, we only use point coordinates to demonstrate the
effectiveness of our approach with the bare minimum geometry informa-
tion.

3254

the point cloud into chunks, which can cause global infor-
mation loss and inaccurate prediction of boundary points
due to information loss from the local neighborhood; and 2)
these methods also sometimes resort to point subsampling,
which impacts performance signiﬁcantly for regions with
sparse point density.
(1) How can we process the entire
point cloud of the scene at once while avoiding the above
problems?

Moreover, in [33, 35], information across multiple points
can only be aggregated through max-pooling either glob-
ally or hierarchically, and [35] uses linear search to lo-
cate the neighborhood each time. (2) How can we better
restore structural information from unstructured and un-
ordered point clouds? Also, in most 3D sensors, the point
density is uneven, e.g., nearby objects have larger density
while faraway objects have much less density. (3) How can
we make the approach robust under different point densi-
ties? Finally, scene ﬂow estimation requires combining in-
formation from both point clouds. (4) How can we best fuse
such information?

We propose a novel deep network architecture for scene
ﬂow estimation that tackles the above four problems. In-
spired by Bilateral Convolutional Layers (BCL) [23, 21]
and the permutohedral lattice [2], we propose three new
layer designs: DownBCL, UpBCL, and CorrBCL, which
process general unstructured data efﬁciently (even beyond
scene ﬂow estimation). Our network ﬁrst interpolates sig-
nals from the input points onto a permutohedral lattice. It
then performs sparse convolutions on the lattice, and inter-
polates the ﬁltered signals to coarser lattice points. This
process is repeated across several DownBCL layers. In this
way, we form a hierarchical downsampling network. Simi-
larly, our network interpolates the ﬁltered signals from the
coarsest lattice points to ﬁner lattice points, and performs
sparse convolutions on the ﬁner lattice points. Again, this
process is repeated across several UpBCL layers (a hier-
archical upsampling network). Finally, the ﬁltered signals
from the ﬁnest lattice points are interpolated to each point
in the ﬁrst input point cloud. Through the downsampling
process, we also fuse signals from both point clouds to the
same lattices and perform our correlation operation (Cor-
rBCL). Overall, we form an hourglass-like model that op-
erates on a structured lattice space (except the ﬁrst and last
operation) for unstructured points.

We conduct experiments on two datasets: FlyingTh-
ings3D [29], which contains synthetic data, and KITTI
Scene Flow 2015 [32, 31], which contains real-world data
from LiDAR scans. Our method outperforms state-of-the-
art approaches. Furthermore, by training on synthetic data
only, our model generalizes to real-world data that have
different patterns. With a novel normalization scheme for
BCLs, our approach also generalizes well under different
point densities. Finally, we show that our network is efﬁ-
cient in terms of computational cost, and it can process a

whole pair of KITTI frames at one time with a maximum
of 86K points per frame. Code and model are available at
https://github.com/laoreja/HPLFlowNet.

2. Related work

3D deep learning. Multi-view CNNs [39, 4, 22, 9, 17]
and volumetric networks [44, 13, 28, 34] leverage stan-
dard CNNs with grid-structured inputs, but suffer from dis-
cretization error on viewpoint selection and on volumetric
representations respectively. PointNet [33, 35] is the ﬁrst
deep learning approach to work on point clouds directly.
Qi et al. [33] propose to use a symmetry function for un-
ordered inputs and use max-pooling to globally aggregate
information. PointNet++ [35] is a follow-up with a hierar-
chical architecture that aggregates information within local
neighborhoods. Klokov and Lempitsky [24] use kd-trees to
divide the point clouds and build architectures based on the
divisions. Another branch of work [8, 14, 10, 6, 46] repre-
sent the 3D surface as a graph, and perform convolution on
its spectral representation. Su et al. [38] propose an archi-
tecture for point cloud segmentation based on BCL [23, 21]
and achieve joint 2D-3D reasoning.

Our work is inspired by [38], but with a different focus:
[38] focuses on BCL’s property of allowing different inputs
and outputs to fuse 2D and 3D information in a new way,
while we focus on processing large-scale point clouds ef-
ﬁciently without sacriﬁcing accuracy – which is different
from all the above approaches.
In addition, scene ﬂow
estimation requires combining information from two point
clouds whereas [38] operates on a single point cloud.

Scene ﬂow estimation. Scene ﬂow estimation with point
cloud inputs is underexplored. Dewan et al. [11] formulate
an energy minimization problem with assumptions on local
geometric constancy and regularization for smooth motion
ﬁelds. Ushani et al. [41] present a real-time four-step al-
gorithm, which constructs occupancy grids, ﬁlters the back-
ground, solves an energy minimization problem, and reﬁnes
with a ﬁltering framework. Unlike [11, 41], our approach is
end-to-end. We also learn directly from data using deep
networks and have no explicit assumptions, e.g., we do not
assume rigid motions.

Wang et al. [43] propose a parametric continuous convo-
lution layer that operates on non-grid structured data and
apply this layer to point cloud segmentation and LiDAR
motion estimation. However, its novel operator is deﬁned
on each point and pooling is the only proposed way for ag-
gregating information. FlowNet3D [25] builds on Point-
Net++ [35] and uses a ﬂow embedding layer to mix two
point clouds, so it shares the aforementioned drawbacks of
[35]. Work on scene ﬂow estimation with other input for-
mats (stereo [19], RGBD [20], light ﬁeld [27]) is less re-
lated, and we refer to Yan and Xiang [45] for a survey.

3255

(a) DownBCLs

(b) UpBCLs

Figure 2: Hierarchical DownBCLs and UpBCLs on permutohedral lattice. DownBCLs are for downsampling and use the Splat-Conv pipeline. During
downsampling, the non-empty lattice points (see the blue squares for example) at the previous layer serve as the input points for the next layer on coarser
permutohedral lattice, and are splatted onto coarser lattice points (green triangles); vice versa for UpBCLs with Conv-Slice pipeline.

3. BCL on permutohedral lattice

Bilateral Convolutional Layer (BCL). BCL [23, 21] is
the basic building block we use. Similar to how a standard
CNN endows the traditional convolution operation with
learning ability, BCL extends the fast high-dimensional
Gaussian ﬁltering algorithm [2] with learnable weights.

BCL takes general inputs. The convolution is operated
on a d-dimensional space, and each input point has a po-
sition vector pin,i ∈ Rd and signal value vi ∈ Rdf . The
position vectors are for locating the points in the deﬁned
space on which convolution operates. In our case, d = 3
and vi = pin,i.

The convolution step of BCL operates on a discrete do-
main but the input points locate in a continuous domain
(for now, without loss of generality, think of the convolu-
tion operating on the most commonly used integer lattice
Zd, i.e. the regular grid, whose lattice points are d-tuples
of integers), so BCL: 1) Gathers signals from each input
point pin,i ∈ Rd onto its enclosing lattice points via inter-
polation (splat), and then 2) Performs sparse convolution
on the lattice; since not every lattice point has gathered sig-
nals, a hash table is used so that convolution is only per-
formed on non-empty lattice points for efﬁciency. 3) Re-
turns the ﬁltered signals from each lattice point to the out-
put points inside the lattice point’s nearest grids, via inter-
polation (slice); the use of interpolation makes it possible
that the output points can locate at different positions from
the input points. The above procedure forms the three-step
pipeline of BCL: Splat-Conv-Slice.

Permutohedral lattice. The integer lattice works ﬁne in
low-dimensional spaces. However, the number of lattice
points each input point interpolates to (i.e., vertices of the
Delaunay cell containing each input point) is 2d, which
makes the splatting and slicing step have a complexity that
is exponential in d. Hence, we use the permutohedral
lattice2 A∗
the d-dimensional permuto-

d [2, 1, 3] instead:

2A lattice is a discrete additive subgroup of a Euclidean space [3]. Both

regular grid Zd and permutohedral lattice A∗

d are speciﬁc lattices.

hedral lattice is the projection of the scaled regular grid
(d + 1)Zd+1 along the vector ~1 = [1, ...1] onto the hy-
perplane Hd : ~x · ~1 = 0, which is the subspace of Rd+1
in which coordinates sum to zero. The Delaunay cells of
the permutohedral lattice are d-simplices and the uniform
simplices of the lattice tessellates Hd. By replacing regular
grids with uniform simplices and using barycentric inter-
polation, the BCL can perform on the permutohedral lattice
with the same scheme as on the integer lattice. Special prop-
erties of permutohedral lattice make it efﬁcient to compute
the vertices of the simplex enclosing any query position and
the barycentric weights in O(d2) time.

Multiplying the position vectors by a scaling factor s, we
can adjust the lattice resolution, i.e., larger s corresponds
to ﬁner resolution where each simplex contains less points.
This effect is the same as scaling the lattice. For better ex-
planation, we interchange the two, and use the term ﬁner
lattice points and coarser lattice points.

4. Approach: HPLFlowNet

BCL restores structural information from unstructured
point clouds, which makes it possible to perform convolu-
tions with kernel size greater than 1. Previous work [38, 21]
use the same set of input points on the continuous domain
for all the BCLs in their network. However, both the time
and space cost of splatting and slicing in BCL are linear
in the number of input points. Is there a way to more efﬁ-
ciently stack BCLs to form a deep architecture? How can
we combine information from both point clouds for scene
ﬂow estimation? In this section, we address these problems
and introduce our HPLFlowNet architecture.

4.1. DownBCL and UpBCL

We ﬁrst introduce the downsampling and upsampling op-
erators, DownBCL and UpBCL. Compared with the three-
step operation in the original BCL, DownBCL only has
two steps: Splat-Conv. The non-empty lattice points at
the previous DownBCL become the input points to the next

3256

layer, thus saving the slicing step. DownBCL is for down-
sampling: we stack DownBCLs with gradually decreasing
scales, so signals from ﬁner lattice points are splatted to
coarser lattice points iteratively, with coarser and coarser
resolution and fewer and fewer input points. Similarly, Up-
BCL, with a two-step pipeline Conv-Slice, is used for up-
sampling with gradually increasing scales. Signals from
coarser lattice points are sliced to ﬁner lattice points di-
rectly, thus saving the splatting step. See Fig. 2.

There are several advantages of DownBCL and UpBCL

over the original BCL:

(1) We reduce the three-step pipeline to a two-step
pipeline without introducing any new computation, which
saves computational cost.

(2) Usually there are much fewer non-empty lattice
points than in the input point cloud, especially on the
coarser lattice. So we reduce the input size for each Down-
BCL, except the ﬁrst one. Similarly, in UpBCL, slicing to
the next layer’s lattice points instead of to the input point
cloud saves computational cost of slicing. In this way, after
the ﬁrst DownBCL and before the last UpBCL, the data size
that DownBCLs and UpBCLs have to deal with has nothing
to do with the size of the input point cloud, but is instead
linear in the number of non-empty lattice points at different
scales; i.e., it is only related to the actual volume the point
cloud occupies. This is the key advantage of DownBCL and
UpBCL that makes computation efﬁcient.

(3) The saved time and memory allow deeper architec-
tures. We use multiple convolution layers with nonlinear ac-
tivations in between for the convolution step in each Down-
BCL and UpBCL, instead of the single convolution in the
original BCL.

(4) Barycentric interpolation is a heuristic to gather and
return signals. The splatting and slicing steps are not sym-
metric: for input point i, let D(i) denote the vertices of its
enclosing simplex; for lattice point j, let V(j) denote the set
of input points that lie in a simplex with vertex j, bij denote
the barycentric weight used when splatting i to j, which is
the same weight for slicing j to i, and let g(·) denote con-
volution. Then in the original BCL, the ﬁltered signals for i
can be expressed as:

i = X

v′

j∈D(i)

bij · g( X

k∈V(j)

bkj · vk)

(1)

Even when g(·) is an identity map, we can see that the in-
put signals are changed after the “identity” BCL. Also, be-
cause of barycentric interpolation, the output signals inside
each simplex are always smooth – this is ﬁne in image ﬁl-
tering [2] where blurring is the expected effect, while it is
not ideal for per-point regression, where points within one
simplex may have drastically different ground truth. Hence,
by removing the slicing step for DownBCL and the splat-
ting step for UpBCL, we reduce such errors caused by the
heuristic and asymmetric operations.

Displacement Filtering

Patch Correlation

Patch Correlation:

(Cin, p)    (Cin, p)

⨁

(2Cin, p)

ConvNet

(Ccorr_out,1 )

Displacement Filtering:
ConvNet

(Ccorr_out, q)

(Cfilter_out, 1)

Figure 3: Proposed CorrBCL for combining information from two point
clouds, which is crucial for scene ﬂow estimation. The correlation layer
consists of two steps: patch correlation and displacement ﬁltering.

4.2. CorrBCL

Because of the interpolation design of BCLs, informa-
tion from two consecutive point clouds can be splatted onto
the same permutohedral lattice. In order to fuse informa-
tion from both point clouds, we propose a novel bilateral
convolutional correlation layer (CorrBCL), inspired by the
matching cost computation and cost aggregation for stereo
algorithms [47]. Our CorrBCL consists of two steps, patch
correlation and displacement ﬁltering.

Patch correlation. Similar to cost matching, patch corre-
lation mixes information from a patch (local neighborhood)
at P C1 and another patch at P C2, but in a more general and
learnable manner.

Let F1 and F2 denote hash tables storing signals for the
two point clouds indexed by lattice positions, p the corre-
lation neighborhood size, and Oc ∈ Zp×d the offset matrix
such that ith neighbor of lattice point at coordinate x is lo-
cated at x + Oc[i]. Then the patch correlation for lattice
point in P C1 located at x and lattice point in P C2 located
at y is

c(x, y) = g(cid:16)γ(cid:0)F1(x+Oc[i]), F2(y+Oc[i])(cid:1) | i = 1, ..., p(cid:17)

(2)
where γ(·, ·) is a bivariate function that combines signals
from the two point clouds, and g is a p-variate function
that aggregates the combined information within each patch
neighborhood.

In traditional vision algorithms, γ is usually element-
wise multiplication, and g is the average function. Our g
is instead a convnet, and γ is the concatenation function. In
this way, we can combine signals of different channel num-
bers for the two point clouds (element-wise multiplication is
unable to do so): we concatenate CorrBCL’s output signals
and P C1’s signals as input for P C1 and use P C2’s signals
only as input for P C2 for the next CorrBCL, see Fig. 4.

Displacement ﬁltering. Bruteforce aggregation of all
possible patch correlation results is computationally pro-
hibitive. Since we are considering point clouds from two

3257

consecutive time instances and the l2 norm of the motion
is limited, given a lattice point x in P C1, we can move it
within a local neighborhood, and match it with the lattice
points in P C2 at the moved positions, and then aggregate all
such pair matching information for x in a sliding-window
manner. This is similar to warping and residual ﬂow in opti-
cal ﬂow [7, 36], but we are warping at every position within
the neighborhood. Let q denote the displacement ﬁltering
neighborhood size and Of ∈ Zq×d denote the offset ma-
trix. For lattice points in P C1 located at x, the displacement
ﬁltering is deﬁned as:

f (x) = h(cid:0)c(x, x + Of [j]) | j = 1, ..., q(cid:1)

(3)

where c(·, ·) is the patch correlation in Eq. 2, and h is a
q-variate aggregating convnet.

Note that the whole CorrBCL can be represented as the

following general pq-variate function:

ψ(x) = φ(cid:0)γ(F1(x + Oc[i]), F2(x + Of [j] + Oc[i]))

| i = 1, ..., p, j = 1..., q(cid:1) (4)

We use the factorization technique to save the number of
parameters from O(pq) to O(p + q), which is similar to
[40, 16], and each of our steps has a physical meaning.
Fig. 3 shows an example of CorrBCL, where d = 2 and the
correlation and displacement ﬁltering have the same neigh-
borhood size p = q = 7.

4.3. Density normalization

Since point clouds are usually sampled with non-uniform
densities and sparse, the lattice points can gather uneven
signals. Thus, a normalization scheme is needed to make
BCLs more robust. All previous work on BCL [23, 21, 38]
use the following normalization scheme following the non-
learnable ﬁltering algorithm [2]: input signals are ﬁltered
in a second round with their values replaced by 1s with a
Gaussian kernel, and the ﬁltered values serve as the normal-
ization weights. However, this scheme does not work well
for our task (see ablation studies). Unlike image ﬁltering,
our ﬁltering weights are learned, and thus it’s not suitable
to continue using Gaussian ﬁltering for normalization.

We instead propose to add a density normalization term

to the splatted signals:

uj = Pk∈V(j) bkj · vk
Pk∈V(j) bkj

(5)

where uj denotes the splatted signals for lattice point j, and
other notations are the same as Eq. 1.

The advantages of this design are: 1) Normalization is
performed during splatting. Compared with the original
scheme where the normalization goes through the three-step
pipeline, the new scheme saves computational cost.
It is
worth noticing that [35] proposes schemes for non-uniform

PC1 (n1 x df)

PC2 (n2 x df)

1x1 ConvNet1

1x1 ConvNet1

DownBCL1, s=3

DownBCL1, s=3

DownBCL2, s=2

DownBCL2, s=2

DownBCL3, s=1

DownBCL3, s=1

CorrBCL1, s=1

DownBCL4, s=0.5

DownBCL4, s=0.5

CorrBCL2, s=0.5

DownBCL5, s=0.25

DownBCL5, s=0.25

CorrBCL3, s=0.25

DownBCL6, s=0.125

DownBCL6, s=0.125

CorrBCL4, s=0.125

DownBCL7, s=0.0625

DownBCL7, s=0.0625

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

Rel. pos.

CorrBCL5, s=0.0625

UpBCL7, s=0.0625

UpBCL6, s=0.125

UpBCL5, s=0.25

UpBCL4, s=0.5

UpBCL3, s=1

UpBCL2, s=2

UpBCL1, s=3

1x1 ConvNet2

Figure 4: HPLFlowNet architecture. The layers with the same name
share weights. s is scaling factor. Rel. pos. is explained in Sec. 4.4.

OUTPUT

… denote input

sampling density as well, but their scheme increases compu-
tational cost greatly. 2) It applies directly to CorrBCL; and
3) Experiments show that this scheme makes our approach
generalize well under different point densities without ﬁne-
tuning.

4.4. Network architecture

The network architecture for HPLFlowNet is shown in
Fig. 4. We use an hourglass-like model due to its good
performance in applications of 2D images [26, 37]. It has
a Siamese-like downsampling stage with information fu-
sion and an upsampling stage. In the downsampling stage,
DownBCLs with gradually decreasing scales are stacked,
so that lattice points in higher layers have larger receptive
ﬁelds and information within a larger volume is gathered to
each lattice point. Since P C2 is important for making scene
ﬂow predictions, it goes through all the same layers as P C1
with shared weights. Unlike previous work [25, 12] that
fuse signals from P C1 and P C2 only once, we use multi-
ple CorrBCLs at different scales for better signal fusion. In
the upsampling stage, we gradually reﬁne the predictions by
stacking UpBCLs of gradually increasing scale, and ﬁnally,
slicing back to the points in P C1. For each UpBCL, we use
skip links from the outputs of their corresponding Down-
BCL and CorrBCL – information from different stages can

3258

be merged at reﬁning time because layers with the same
scaling factor have the same set of non-empty lattice points,
At each BCL, we concatenate the input signals with its
relative positions w.r.t. its enclosing simplex (its position
vector minus the lattice coordinates of its “ﬁrst” enclosing
simplex vertex). In Fig. 4, we use Rel. pos. to denote the
relative positions. By providing the network with relative
positions directly, it can achieve better translational invari-
ance. The CNN we use is translational invariant under cer-
tain quantization errors, but unlike standard CNNs, we are
interpolating signals from the continuous domain onto the
discrete domain, which leads to some positional informa-
tion loss. By incorporating Rel. pos. into the input signals,
such loss can be compensated.

Since most layers of our model always operate on sparse
lattice points, their computational cost is unrelated to the
size of point clouds, but only relates to the actual volume
that the point cloud occupies. To train HPLFlowNet, we

use the End Point Error (EPE3D) loss: kcsf − sf k2 aver-
aged over each point, where csf denotes the predicted scene

ﬂow vector and sf denotes the ground truth. EPE3D is the
counterpart of EPE for 2D optical ﬂow estimation.

5. Experiments

We show results for the following experiments: 1) We
train and evaluate our model on the synthetic FlyingTh-
ings3D dataset, and 2) also test it directly on the real-world
KITTI Scene Flow dataset without ﬁne-tuning. 3) We test
the model on inputs with different point densities, 4) com-
pare computational cost at both architecture and single-
layer level, and 5) conduct ablation studies to analyze the
contribution of each component.

Evaluation metrics. EPE3D (m): our main metric, kcsf −

sf k2 averaged over each point. Acc3D Strict: a strict ver-
sion of accuracy, the percentage of points whose EPE3D
< 0.05m or relative error < 5%. Acc3D Relax: a relaxed
version of accuracy, the percentage of points whose EPE3D
< 0.1m or relative error < 10%. Outliers3D: the per-
centage of outliers whose EPE3D > 0.3m or relative error
> 10%. By projecting the point clouds back to the image
plane, we obtain 2D optical ﬂow. In this way, we measure
how well our approach works for optical ﬂow estimation.
EPE2D (px): 2D End Point Error, which is a common met-
ric for optical ﬂow. Acc2D: the percentage of points whose
EPE2D < 3px or relative error < 5%.

5.1. Results on FlyingThings3D

FlyingThings3D [29] is the ﬁrst large-scale synthetic
dataset that enables training deep neural networks for scene
ﬂow estimation. To our knowledge, it is the only scene ﬂow
dataset that has more than 10,000 training samples. We re-
construct the 3D point clouds and ground truth scene ﬂow
using the provided camera parameters.

Training and evaluation details. Following [29, 18, 19],
we use the dataset version where some extremely hard sam-
ples are removed3. To simulate real-world point clouds,
we remove points whose disparity and optical ﬂow are oc-
cluded. Following [25], we train on points with depth
less than 35 meters. Most foreground moving objects are
within this depth range. We randomly sample n points from
each frame in a non-corresponding manner: corresponding
points for the ﬁrst frame may not necessarily be found in the
sampled points of the second frame. We use n = 8, 192 for
training. To reduce training time, we use one quarter of the
training set (4910 pairs), which already yields good gener-
alization ability. The model ﬁnetuned on whole training set
achieves 0.0696/0.1113 EPE3D on FlyingThings3D/KITTI.
We evaluate on the whole test set (3824 pairs).

Baselines. We compare to the following methods:

Iterative Closest Point [5]: a common baseline for
scene ﬂow estimation, the algorithm iteratively revises the
rigid transformation needed to minimize the error metric.

FlowNet3D [25]: the state-of-the-art for scene ﬂow esti-
mation with point cloud inputs. Since code is unavailable,
we use our own implementation.

SPLATFlowNet: a strong baseline based on SPLAT-
Net [38]; architecture is the Siamese network of SPLATNet
with CorrBCLs that is about the same depth as our model.
It does not use the hourglass architecture, but concatenates
all outputs from the BCLs and CorrBCLs of different scales
to make the prediction.

Original BCL: We replace DownBCL and UpBCL with
the original BCL used in previous work [23, 21, 38] while
keeping everything else the same as our model.

We also list results of FlowNet3 [19] for reference pur-
poses, since the inputs are in different modalities. It’s the
state-of-the-art with stereo inputs. We remove points with
extremely wrong predictions (e.g., disparity with opposite
signs) – the extremes will induce too much error.

Results. Quantitative results are shown in Table 1. Our
method outperforms all baselines on all metrics by a large
margin, and is the only method with EPE3D below 10cm.
FlowNet3 has the best Acc2D because its optical ﬂow net-
work is optimized on 2D metrics; but it has worse EPE2D
since we mainly evaluate on foreground objects, which can
have large motions in 2D due to projection and is thus hard
to predict. The fact that it is easily affected by extremes
(worse EPE3D and EPE2D) also shows that using stereo
inputs is more sensitive to prediction errors due to its in-
direct 3D representation. The reason that our method out-
performs FlowNet3D is likely that we better restore struc-
tural information and design a better architecture for com-
bining information from both point clouds. Our method and

3https://lmb.informatik.uni-freiburg.de/data/

FlyingThings3D_subset/FlyingThings3D_subset_all_
download_paths.txt

3259

Table 1: Evaluation results on FlyingThings3D and KITTI Scene Flow 2015. Our method outperforms all baseline methods on all metrics (FlowNet3 is not
directly comparable). The good performance on KITTI shows our method’s generalization ability.

Dataset

Method

EPE3D

Acc3D Strict

Acc3D Relax

Outliers3D

EPE2D

Acc2D

FlyingThings3D

KITTI

FlowNet3 [19]
ICP [5]
FlowNet3D [25]
SPLATFlowNet [38]
original BCL
Ours

FlowNet3 [19]
ICP [5]
FlowNet3D [25]
SPLATFlowNet [38]
original BCL
Ours

0.4570
0.4062
0.1136
0.1205
0.1111
0.0804

0.9111
0.5181
0.1767
0.1988
0.1729
0.1169

0.4179
0.1614
0.4125
0.4197
0.4279
0.6144

0.2039
0.0669
0.3738
0.2174
0.2516
0.4783

0.6168
0.3038
0.7706
0.7180
0.7551
0.8555

0.3587
0.1667
0.6677
0.5391
0.6011
0.7776

0.6050
0.8796
0.6016
0.6187
0.6054
0.4287

0.7463
0.8712
0.5271
0.6575
0.6215
0.4103

5.1348
23.2280
5.9740
6.9759
6.3027
4.6723

5.1023
27.6752
7.2141
8.2306
7.3476
4.8055

0.8125
0.2913
0.5692
0.5512
0.5669
0.6764

0.7803
0.1056
0.5093
0.4189
0.4411
0.5938

Figure 5: Qualitative results on FlyingThings3D (top) and KITTI (bottom). Blue points are P C1, green points are correctly predicted (measured by
csf , and red points are ground-truth ﬂowed points P C1 + sf which are not correctly predicted. Note that the objects in
Acc3D Relax) ﬂowed points P C1 +
the two datasets have very different motion patterns, which shows our method’s generalization ability. The third ﬁgure of the second row shows that some
failures are on the ground, which suggests the performance on KITTI may be further improved by better ground removal algorithms.

SPLATFlowNet have similar depth and use the same build-
ing blocks, so our performance gain can be credited to our
hourglass-like model and the skip links that combine ﬁl-
tered signals in the downsampling and upsampling stages.
Comparison with the original BCL shows that we improve
performance by reduction and veriﬁes that the heuristic and
asymmetric nature of the barycentric interpolation makes it
better to avoid unnecessary operations. Fig. 5 shows qual-
itative results. Our model performs well for complicated
shapes, large motions, and also the hard case where multi-
ple neighboring objects have different motions.

5.2. Generalization results on real world data

Next, to study our model’s generalization ability to un-
seen real-world data, we take our model which was trained
on FlyingThings3D, and without any ﬁne-tuning evaluate
on KITTI Scene Flow 2015 [32, 31].

Evaluation details. KITTI Scene Flow 2015 is obtained
by annotating dynamic scenes from the KITTI raw data col-
lection using detailed 3D CAD models for all vehicles in
motion. Since disparity is not given for the test set, we

evaluate on all 142 scenes in the training set with publicly
available raw 3D data, following [25]. Since in autonomous
driving, the motion of the ground is not useful and remov-
ing ground is a common step [11, 41, 25], we remove the
ground by height (< 0.3m). We use similar preprocessing
as in Sec. 5.1 except that we do not remove occluded points.

Results. Our method again outperforms all other methods
in all metrics by a large margin; see Table 1. This demon-
strates our method’s generalization ability to new real-world
data. Without ground removal, Ours/FlowNet3D EPE3D is
0.2366/0.3331, so ours is still better. Qualitative results are
shown in Fig. 5. Even though our approach is trained on
a dataset with very different patterns and different objects,
it makes precise estimations in driving scenes where ego-
motion is large and multiple dynamic objects have differ-
ent motions. It also correctly predicts the trees and bushes
which are never seen by the network during training.

5.3. Empirical efﬁciency

Our architecture is optimized for performance. To show
how efﬁcient our proposed novel BCL variants can be,

3260

Table 2: Efﬁciency comparison: average runtime (ms) on FlyingThings3D
measured on a single Titan V. Ours and Ours-shallow are more efﬁcient.

Table 4: Ablation studies (EPE3D) on FlyingThings3D. Results show that
each component is important.

Method

8,192

16,384

32,768

NoSkips

OneCorr

OriNorm

EM

No Rel. P os.

Full

FlowNet3D [25]
Ours
Ours-shallow

130.8
98.4
50.5

279.2
115.5
55.1

770.0
142.8
63.7

Table 3: Results (EPE3D) under different point densities on FlyingTh-
ings3D and KITTI. Some results for FlowNet3D are missing since mem-
ory runs out without signiﬁcant sacriﬁce in speed and/or optimization for
memory. Our density normalization scheme works well and achieves supe-
rior performance for all testing densities different from the training density.

Dataset

# points Ours No Norm Ours-shallow FlowNet3D

FlyingThings3D

KITTI

8,192
0.0804
16,384 0.0782
32,768 0.0774
65,536 0.0772

0.1169
8,192
16,384 0.1114
32,768 0.1087
65,536 0.1087
0.1087
All

0.0790
0.0779
0.0874
0.1267

0.1187
0.1305
0.1663
0.1842
0.1853

0.0957
0.0932
0.0925
0.0925

0.1630
0.1646
0.1671
0.1674
0.1674

0.1136
0.1085
0.1327

-

0.1767
0.2095
0.3110

-
-

we make a shallower version Ours-shallow by removing
Down/UpBCL6/7 and CorrBCL4/5, and cutting down con-
volutions (see supp. for details). Table 2 shows the efﬁ-
ciency comparison results among different models. Ours is
faster than FlowNet3D. Ours-shallow is very fast and also
outperforms all other methods (Table. 3). And our run-
time does not linearly scale with the number of input points,
which empirically validates our architectural design.

We also compare with the original BCL w.r.t. layer ef-
ﬁciency. We measure runtime of each BCL variant in our
architecture, averaged on FlyingThings3D. We then replace
them with original BCLs and do the same. Runtime ratio
of ours to original BCL averaged over all layers: 56%. We
include a more detailed analysis in supp.

5.4. Generalization results on point density

We next evaluate how our model generalizes to different
point densities. During training, we sample 8,192 points
for each frame. Without any ﬁne-tuning, we evaluate on
16,384, 32,768, 65,536 sampled points. For KITTI, we also
evaluate on all points.

Because of our architectural design, we have the advan-
tage of being able to process large-scale point clouds at one
time, and thus do not need to divide the scene and feed the
parts one by one into the network like [33, 35]. For all our
experiments, we feed the two whole point clouds into the
network in one pass. The maximum number of points for
one frame in KITTI is around 86K.

Table 3 shows the performance of various point densities
on both datasets, where we also compare with an identical
architecture without our normalization scheme (No Norm).
Results show that the normalization scheme has slight infor-
mation loss. No Norm has best performance on the training

0.3149

0.3698

0.6583

0.0948

0.0989

0.0804

density, but our architecture with normalization is the most
robust under different densities – EPE3D does not increase
even though we evaluate on totally different point densities
from the density used during training.

5.5. Ablation studies

To study the contribution of each component, we conduct
a series of ablation studies, where each time we only change
one component:
• NoSkips: We remove all skip links.
• OneCorr: To validate that using multiple CorrBCLs of
different scales improves performance, we only keep the
last CorrBCL.

• OriNorm: We replace the normalization scheme for each
BCL with the original normalization scheme used in pre-
vious work [23, 21, 38].

• Elementwise Multiplication (EM): We use elementwise
multiplication in patch correlation. Since elementwise
multiplication does not support input features of differ-
ent lengths for the two point clouds, we remove the links
from previous CorrBCLs to the next CorrBCLs.

• No Rel. P os.: We remove all the relative positions that

are concatenated with input signals.

We see from Table 4 that the original normalization
scheme does not work well for scene ﬂow estimation. Both
skip links and multiple CorrBCLs contribute signiﬁcantly.
We see that by using concatenation instead of elementwise
multiplication, we are able to link previous CorrBCLs to the
next CorrBCLs, and thus boost the performance. By taking
both global and local positional information, our model ob-
tains improved performance.

6. Conclusion

We presented HPLFlowNet, a novel deep network for
scene ﬂow estimation on large-scale point clouds. We pro-
posed the novel DownBCL, UpBCL and CorrBCL and a
density normalization scheme, which allow the bulk of our
network to robustly perform on permutohedral lattices of
different scales. This greatly saves computational cost with-
out sacriﬁcing performance. Through extensive experi-
ments, we demonstrated its advantages over various com-
parison methods.

Acknowledgments. This work was supported in part
by NSF IIS-1748387, TuSimple and GPUs donated by
NVIDIA.

3261

References

[1] A.B. Adams. High-dimensional gaussian ﬁltering for com-
putational photography. PhD thesis, Stanford University,
2011. 3

[2] A. Adams, J. Baek, and M.A. Davis. Fast high-dimensional
ﬁltering using the permutohedral
In Computer
Graphics Forum, volume 29, pages 753–762, 2010. 2, 3,
4, 5

lattice.

[3] J. Baek and A.B. Adams. Some useful properties of the per-
mutohedral lattice for gaussian ﬁltering. Technical report,
Stanford University, 2009. 3

[4] S. Bai, X. Bai, Z. Zhou, Z. Zhang, and L. Jan Latecki. Gift:
A real-time and scalable 3d shape search engine. In CVPR,
2016. 2

[19] E. Ilg, T. Saikia, M. Keuper, and T. Brox. Occlusions, motion
and depth boundaries with a generic network for disparity,
optical ﬂow or scene ﬂow estimation. In ECCV, 2018. 2, 6,
7

[20] M. Jaimez, C. Kerl, J. Gonzalez-Jimenez, and D. Cremers.
Fast odometry and scene ﬂow from rgb-d cameras based on
geometric clustering. In ICRA, 2017. 2

[21] V. Jampani, M. Kiefel, and P.V. Gehler. Learning sparse high
dimensional ﬁlters: Image ﬁltering, dense crfs and bilateral
neural networks. In CVPR, 2016. 2, 3, 5, 6, 8

[22] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri. 3d
shape segmentation with projective convolutional networks.
In CVPR, 2017. 2

[23] M. Kiefel, V. Jampani, and P.V. Gehler. Permutohedral lat-

tice cnns. In ICLR Workshop Track, 2015. 2, 3, 5, 6, 8

[5] P.J. Besl and N.D. McKay. Method for registration of 3-d
shapes. In Sensor Fusion IV: Control Paradigms and Data
Structures, 1992. 6, 7

[24] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-
In

networks for the recognition of 3d point cloud models.
ICCV, 2017. 1, 2

[6] D. Boscaini, J. Masci, S. Melzi, M.M. Bronstein, U. Castel-
lani, and P. Vandergheynst. Learning class-speciﬁc descrip-
tors for deformable shapes using localized spectral convolu-
tional networks. In Computer Graphics Forum, 2015. 2

[7] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, 2004. 5

[8] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral net-
works and locally connected networks on graphs. In ICLR,
2014. 2

[9] Z. Cao, Q. Huang, and R. Karthik. 3d object classiﬁcation

via spherical projections. In 3DV, 2017. 2

[10] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-
tional neural networks on graphs with fast localized spectral
ﬁltering. In NIPS, 2016. 2

[11] A. Dewan, T. Caselitz, G.D. Tipaldi, and W. Burgard. Rigid

scene ﬂow for 3d lidar scans. In IROS, 2016. 2, 7

[12] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In ICCV, 2015. 5

[13] B. Graham, M. Engelcke, and L. van der Maaten. 3d se-
mantic segmentation with submanifold sparse convolutional
networks. In CVPR, 2018. 2

[14] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional net-
works on graph-structured data. arXiv:1506.05163, 2015. 2

[15] E. Herbst, X. Ren, and D. Fox. Rgb-d ﬂow: Dense 3-d mo-

tion estimation using color and depth. In ICRA, 2013. 1

[16] A.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv:1704.04861, 2017. 5

[17] H. Huang, E. Kalogerakis, S. Chaudhuri, D. Ceylan, V.G.
Kim, and E. Yumer. Learning local shape descriptors from
part correspondences with multi-view convolutional net-
works. ACM Transactions on Graphics (TOG), 2017. 2

[25] X. Liu, C.R. Qi, and L.J. Guibas. Learning scene ﬂow in 3d

point clouds. arXiv:1806.01411v1, 2018. 2, 5, 6, 7, 8

[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. In CVPR, 2015. 5

[27] S. Ma, B.M. Smith, and M. Gupta. 3d scene ﬂow from 4d

light ﬁeld gradients. In ECCV, 2018. 2

[28] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
In IROS,

neural network for real-time object recognition.
2015. 2

[29] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A.
Dosovitskiy, and T. Brox. A large dataset to train convo-
lutional networks for disparity, optical ﬂow, and scene ﬂow
estimation. In CVPR, 2016. 2, 6

[30] M. Menze and A. Geiger. Object scene ﬂow for autonomous

vehicles. In CVPR, 2015. 1

[31] M. Menze, C. Heipke, and A. Geiger. Joint 3d estimation of
vehicles and scene ﬂow. In ISPRS Annals of Photogramme-
try, Remote Sensing & Spatial Information Sciences, 2015.
2, 7

[32] M. Menze, C. Heipke, and A. Geiger. Object scene ﬂow.
ISPRS Journal of Photogrammetry and Remote Sensing
(JPRS), 2018. 2, 7

[33] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
CVPR, 2017. 1, 2, 8

[34] C.R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L.J. Guibas.
Volumetric and multi-view cnns for object classiﬁcation on
3d data. In CVPR, 2016. 2

[35] C.R. Qi, L. Yi, H. Su, and L.J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, 2017. 1, 2, 5, 8

[36] A. Ranjan and M.J. Black. Optical ﬂow estimation using a

spatial pyramid network. In CVPR, 2017. 5

[37] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, 2015. 5

[18] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. In CVPR, 2017. 6

[38] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.H.
Yang, and J. Kautz. Splatnet: Sparse lattice networks for
point cloud processing. In CVPR, 2018. 2, 3, 5, 6, 7, 8

3262

[39] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-
view convolutional neural networks for 3d shape recognition.
In ICCV, 2015. 2

[40] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M.
Paluri. A closer look at spatiotemporal convolutions for ac-
tion recognition. In CVPR, 2018. 5

[41] A.K. Ushani, R.W. Wolcott, J.M. Walls, and R.M. Eustice.
A learning approach for real-time temporal scene ﬂow esti-
mation from lidar data. In ICRA, 2017. 2, 7

[42] S. Vedula, S. Baker, P. Rander, R. Collins, and T. Kanade.

Three-dimensional scene ﬂow. In ICCV, 1999. 1

[43] S. Wang, S. Suo, W.C. Ma, A. Pokrovsky, and R. Urtasun.
Deep parametric continuous convolutional neural networks.
In CVPR, 2018. 2

[44] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In CVPR, 2015. 2

[45] Z. Yan and X. Xiang. Scene ﬂow estimation: A survey.

arXiv:1612.02590, 2016. 2

[46] L. Yi, H. Su, X. Guo, and L.J. Guibas. Syncspeccnn: Syn-
chronized spectral cnn for 3d shape segmentation. In CVPR,
2017. 1, 2

[47] J. Zbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. Jour-
nal of Machine Learning Research, 17(1-32):2, 2016. 4

3263

