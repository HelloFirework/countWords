Toward Convolutional Blind Denoising of Real Photographs

Shi Guo1,3,4, Zifei Yan(☞) 1, Kai Zhang1,3, Wangmeng Zuo1,2, Lei Zhang3,4
1Harbin Institute of Technology, Harbin; 2Peng Cheng Laboratory, Shenzhen;

3 The Hong Kong Polytechnic University, Hong Kong; 4DAMO Academy, Alibaba Group

guoshi28@outlook.com, {wmzuo,yanzifei}@hit.edu.cn

cskaizhang@gmail.com, cslzhang@comp.polyu.edu.hk

Abstract

While deep convolutional neural networks (CNNs) have
achieved impressive success in image denoising with addi-
tive white Gaussian noise (AWGN), their performance re-
mains limited on real-world noisy photographs. The main
reason is that their learned models are easy to overﬁt on
the simpliﬁed AWGN model which deviates severely from
the complicated real-world noise model.
In order to im-
prove the generalization ability of deep CNN denoisers, we
suggest training a convolutional blind denoising network
(CBDNet) with more realistic noise model and real-world
noisy-clean image pairs. On the one hand, both signal-
dependent noise and in-camera signal processing pipeline
is considered to synthesize realistic noisy images. On the
other hand, real-world noisy photographs and their nearly
noise-free counterparts are also included to train our CBD-
Net. To further provide an interactive strategy to rectify de-
noising result conveniently, a noise estimation subnetwork
with asymmetric learning to suppress under-estimation of
noise level is embedded into CBDNet. Extensive experi-
mental results on three datasets of real-world noisy pho-
tographs clearly demonstrate the superior performance of
CBDNet over state-of-the-arts in terms of quantitative met-
rics and visual quality. The code has been made available
at https://github.com/GuoShi28/CBDNet.

1. Introduction

Image denoising is an essential and fundamental prob-
lem in low-level vision and image processing. With decades
of studies, numerous promising approaches [3, 12, 17,
53, 11, 61] have been developed and near-optimal per-
formance [8, 31, 50] has been achieved for the removal
of additive white Gaussian noise (AWGN). However, in
real camera system,
image noise comes from multiple
sources (e.g., dark current noise, short noise, and thermal
noise) and is further affected by in-camera processing (ISP)
pipeline (e.g., demosaicing, Gamma correction, and com-
pression). All these make real noise much more different

(b) Noisy

(a) “0002 02” from DND [45]

(c) BM3D [12]

(d) DnCNN [61]

(e) FFDNet+ [62]

(f) CBDNet

Figure 1: Denoising results of different methods on real-
world noisy image “0002 02” from DND [45].

from AWGN, and blind denoising of real-world noisy pho-
tographs remains a challenging issue.

In the recent past, Gaussian denoising performance has
been signiﬁcantly advanced by the development of deep
CNNs [61, 38, 62]. However, deep denoisers for blind
AWGN removal degrades dramatically when applied to real
photographs (see Fig. 1(d)). On the other hand, deep de-
noisers for non-blind AWGN removal would smooth out
the details while removing the noise (see Fig. 1(e)). Such
an phenomenon may be explained from the characteristic of
deep CNNs [39], where their generalization largely depends
on the ability of memorizing large scale training data. In
other words, existing CNN denoisers tend to be over-ﬁtted
to Gaussian noise and generalize poorly to real-world noisy
images with more sophisticated noise.

1712

In this paper, we tackle this issue by developing a convo-
lutional blind denoising network (CBDNet) for real-world
photographs. As indicated by [39], the success of CNN de-
noisers are signiﬁcantly dependent on whether the distribu-
tions of synthetic and real noises are well matched. There-
fore, realistic noise model is the foremost issue for blind de-
noising of real photographs. According to [14, 45], Poisson-
Gaussian distribution which can be approximated as het-
eroscedastic Gaussian of a signal-dependent and a station-
ary noise components has been considered as a more appro-
priate alternative than AWGN for real raw noise modeling.
Moreover, in-camera processing would further makes the
noise spatially and chromatically correlated which increases
the complexity of noise. As such, we take into account both
Poisson-Gaussian model and in-camera processing pipeline
(e.g., demosaicing, Gamma correction, and JPEG compres-
sion) in our noise model. Experiments show that in-camera
processing pipeline plays a pivot role in realistic noise mod-
eling, and achieves notably performance gain (i.e., > 5 dB
by PSNR) over AWGN on DND [45].

Our CBDNet is comprised of two subnetworks,

We further incorporate both synthetic and real noisy im-
ages to train CBDNet. On one hand, it is easy to access
massive synthetic noisy images. However, the noise in real
photographs cannot be fully characterized by our model,
thereby giving some leeway for improving denoising per-
formance. On the other hand, several approaches [43, 1]
have suggested to get noise-free image by averaging hun-
dreds of noisy images at the same scene. Such solution,
however, is expensive in cost, and suffers from the over-
smoothing effect of noise-free image. Beneﬁted from the
incorporation of synthetic and real noisy images, 0.3 ∼ 0.5
dB gain on PSNR can be attained by CBDNet on DND [45].
i.e.,
noise estimation and non-blind denoising. With the intro-
duction of noise estimation subnetwork, we adopt an asym-
metric loss by imposing more penalty on under-estimation
error of noise level, making our CBDNet perform robustly
when the noise model is not well matched with real-world
noise. Besides, it also allows the user to interactively rectify
the denoising result by tuning the estimated noise level map.
Extensive experiments are conducted on three real noisy im-
age datasets, i.e., NC12 [29], DND [45] and Nam [43]. In
terms of both quantitative metrics and perceptual quality,
our CBDNet performs favorably in comparison to state-of-
the-arts. As shown in Fig. 1, both non-blind BM3D [12] and
DnCNN for blind AWGN [61] fail to denoise the real-world
noisy photograph. In contrast, our CBDNet achieves very
pleasing denoising results by retaining most structure and
details while removing the sophisticated real-world noise.

To sum up, the contribution of this work is four-fold:

• A realistic noise model is presented by considering
both heteroscedastic Gaussian noise and in-camera
processing pipeline, greatly beneﬁting the denoising

performance.

• Synthetic noisy images and real noisy photographs are
incorporated for better characterizing real-world image
noise and improving denoising performance.

• Beneﬁted from the introduction of noise estimation
subnetwork, asymmetric loss is suggested to improve
the generalization ability to real noise, and interactive
denoising is allowed by adjusting the noise level map.

• Experiments on three real-world noisy image datasets
show that our CBDNet achieves state-of-the-art results
in terms of both quantitative metrics and visual quality.

2. Related Work

2.1. Deep CNN Denoisers

The advent of deep neural networks (DNNs) has led to
great improvement on Gaussian denoising. Until Burger
et al. [6], most early deep models cannot achieve state-of-
the-art denoising performance [22, 49, 57]. Subsequently,
CSF [53] and TNRD [11] unroll the optimization algo-
rithms for solving the ﬁelds of experts model to learn stage-
wise inference procedure. By incorporating residual learn-
ing [19] and batch normalization [21], Zhang et al. [61]
suggest a denoising CNN (DnCNN) which can outperform
traditional non-CNN based methods. Without using clean
data, Noise2Noise [30] also achieves state-of-the-art. Most
recently, other CNN methods, such as RED30 [38], Mem-
Net [55], BM3D-Net [60], MWCNN [33] and FFDNet [62],
are also developed with promising denoising performance.
Beneﬁted from the modeling capability of CNNs, the
studies [61, 38, 55] show that it is feasible to learn a single
model for blind Gaussian denoising. However, these blind
models may be over-ﬁtted to AWGN and fail to handle real
noise. In contrast, non-blind CNN denoisiers, e.g., FFD-
Net [62], can achieve satisfying results on most real noisy
images by manually setting proper or relatively higher noise
level. To exploit this characteristic, our CBDNet includes a
noise estimation subnetwork as well as an asymmetric loss
to suppress under-estimation error of noise level.

2.2. Image Noise Modeling

Most denoising methods are developed for non-blind
Gaussian denoising. However, the noise in real images
comes from various sources (dark current noise, short noise,
thermal noise, etc.), and is much more sophisticated [44].
By modeling photon sensing with Poisson and remaining
stationary disturbances with Gaussian, Poisson-Gaussian
noise model [14] has been adopted for the raw data of imag-
ing sensors. In [14, 32], camera response function (CRF)
and quantization noise are also considered for more practi-
cal noise modeling. Instead of Poisson-Gaussian, Hwang et

1713

Lasymm : Asymmetric loss

LT V : TV regularizer

Lrec : Reconstruction loss

CNNE : Noise Estimation Subnetwork

256

128

64

CNND : Non-blind Denoising Subnetwork

λasymmLasymm + λT V LT V

Lrec

CBDNet : Convolutional Blind Denoising Network

32

Figure 2: Illustration of our CBDNet for blind denoising of real-world noisy photograph.

al. [20] present a Skellam distribution for Poisson photon
noise modeling. Moreover, when taking in-camera image
processing pipeline into account, the channel-independent
noise assumption may not hold true, and several approaches
[25, 43] are proposed for cross-channel noise modeling.
In this work, we show that realistic noise model plays a
pivot role in CNN-based denoising of real photographs, and
both Poisson-Gaussian noise and in-camera image process-
ing pipeline beneﬁt denoising performance.

2.3. Blind Denoising of Real Images

Blind denoising of real noisy images generally is more
challenging and can involve two stages, i.e., noise estima-
tion and non-blind denoising. For AWGN, several PCA-
based [48, 34, 9] methods have been developed for estimat-
ing noise standard deviation (SD.). Rabie [49] models the
noisy pixels as outliers and exploits Lorentzian robust esti-
mator for AWGN estimation. For Poisson-Gaussian model,
Foi et al. [14] suggest a two-stage scheme, i.e., local estima-
tion of multiple expectation/standard-deviation pairs, and
global parametric model ﬁtting.

In most blind denoising methods, noise estimation is
closely coupled with non-blind denoising. Portilla [46, 47]
adopts a Gaussian scale mixture for modeling wavelet
patches of each scale, and utilizes Bayesian least square
to estimate clean wavelet patches. Based on the piece-
wise smooth image model, Liu et al. [32] propose a uni-
ﬁed framework for the estimation and removal of color
noise. Gong et al. [15] model the data ﬁtting term as the
weighted sum of the L1 and L2 norms, and utilize a spar-
sity regularizer in wavelet domain for handling mixed or un-
known noises. Lebrun et al. [28, 29] propose an extension
of non-local Bayes approach [27] by modeling the noise
of each patch group to be zero-mean correlated Gaussian
distributed. Zhu et al. [63] suggest a Bayesian nonpara-
metric technique to remove the noise via the low-rank mix-

ture of Gaussians (LR-MoG) model. Nam et al. [43] model
the cross-channel noise as a multivariate Gaussian and per-
form denoising by the Bayesian nonlocal means ﬁlter [24].
Xu et al. [59] suggest a multi-channel weighted nuclear
norm minimization (MCWNNM) model to exploit chan-
nel redundancy. They further present a trilateral weighted
sparse coding (TWSC) method for better modeling noise
and image priors [58]. Except noise clinic (NC) [28, 29],
MCWNNM [59], and TWSC [58], the codes of most blind
denoisers are not available. Our experiments show that they
are still limited for removing noise from real images.

3. Proposed Method

This section presents our CBDNet consisting of a noise
estimation subnetwork and a non-blind denoising subnet-
work. To begin with, we introduce the noise model to gener-
ate synthetic noisy images. Then, the network architecture
and asymmetric loss. Finally, we explain the incorporation
of synthetic and real noisy images for training CBDNet.

3.1. Realistic Noise Model

As noted in [39], the generalization of CNN largely de-
pends on the ability in memorizing training data. Exist-
ing CNN denoisers, e.g., DnCNN [61], generally does not
work well on real noisy images, mainly due to that they may
be over-ﬁtted to AWGN while the real noise distribution is
much different from Gaussian. On the other hand, when
trained with a realistic noise model, the memorization abil-
ity of CNN will be helpful to make the learned model gen-
eralize well to real photographs. Thus, noise model plays a
critical role in guaranteeing performance of CNN denoiser.
Different from AWGN, real image noise generally is
more sophisticated and signal-dependent [35, 14]. Practi-
cally, the noise produced by photon sensing can be mod-
eled as Poisson, while the remaining stationary disturbances

1714

can be modeled as Gaussian. Poisson-Gaussian thus pro-
vides a reasonable noise model for the raw data of imaging
sensors [14], and can be further approximated with a het-
eroscedastic Gaussian n(L) ∼ N (0, σ2(L)) deﬁned as,

σ2(L) = L · σ2

s + σ2
c .

(1)

where L is the irradiance image of raw pixels. n(L) =
ns(L) + nc involves two components, i.e., a stationary
noise component nc with noise variance σ2
c and a signal-
dependent noise component ns with spatially variant noise
variance L · σ2
s .

Real photographs, however, are usually obtained after in-
camera processing (ISP), which further increases the com-
plexity of noise and makes it spatially and chromatically
correlated. Thus, we take two main steps of ISP pipeline,
i.e., demosaicing and Gamma correction, into considera-
tion, resulting in the realistic noise model as,

y = f (DM(L + n(L))),

(2)

where y denotes the synthetic noisy image, f (·) stands
for the camera response function (CRF) uniformly sampled
from the 201 CRFs provided in [16]. And L = Mf −1(x) is
adopted to generate irradiance image from a clean image x.
M(·) represents the function that converts sRGB image to
Bayer image and DM(·) represents the demosaicing func-
tion [37]. Note that the interpolation in DM(·) involves
pixels of different channels and spatial locations. The syn-
thetic noise in Eqn. (2) is thus channel and space dependent.
to extend CBDNet for handling com-
pressed image, we can include JPEG compression in gen-
erating synthetic noisy image,

Furthermore,

y = J P EG(f (DM(L + n(L)))).

(3)

For noisy uncompressed image, we adopt the model in
Eqn. (2) to generate synthetic noisy images. For noisy com-
pressed image, we exploit the model in Eqn. (3). Speciﬁ-
cally, σs and σc are uniformly sampled from the ranges of
[0, 0.16] and [0, 0.06], respectively. In JPEG compression,
the quality factor is sampled from the range [60, 100]. We
note that the quantization noise is not considered because it
is minimal and can be ignored without any obvious effect
on denoising result [62].

3.2. Network Architecture

As illustrated in Fig. 2, the proposed CBDNet includes a
noise estimation subnetwork CNNE and a non-blind denos-
ing subnetwork CNND. First, CNNE takes a noisy obser-
vation y to produce the estimated noise level map ˆσ(y) =
FE(y; WE), where WE denotes the network parameters
of CNNE. We let the output of CNNE be the noise level
map due to that it is of the same size with the input y and
can be estimated with a fully convolutional network. Then,
CNND takes both y and ˆσ(y) as input to obtain the ﬁnal de-
noising result ˆx = FD(y, ˆσ(y); WD), where WD denotes

the network parameters of CNND. Moreover, the introduc-
tion of CNNE also allows us to adjust the estimated noise
level map ˆσ(y) before putting it to the the non-blind denos-
ing subnetwork CNND. In this work, we present a simple
strategy by letting ˆ̺(y) = γ · ˆσ(y) for interactive denoising.
We further explain the network structures of CNNE and
CNND. CNNE adopts a plain ﬁve-layer fully convolu-
tional network without pooling and batch normalization op-
erations. In each convolution (Conv) layer, the number of
feature channels is set as 32, and the ﬁlter size is 3 × 3.
The ReLU nonlinearity [42] is deployed after each Conv
layer. As for CNND, we adopt an U-Net [51] architec-
ture which takes both y and ˆσ(y) as input to give a pre-
diction ˆx of the noise-free clean image. Following [61],
the residual learning is adopted by ﬁrst learning the resid-
ual mapping R(y, ˆσ(y); WD) and then predicting ˆx =
y + R(y, ˆσ(y); WD). The 16-layer U-Net architecture
of CNNE is also given in Fig. 2, where symmetric skip
connections, strided convolutions and transpose convolu-
tions are introduced for exploiting multi-scale information
as well as enlarging receptive ﬁeld. All the ﬁlter size is 3×3,
and the ReLU nonlinearity [42] is applied after every Conv
layer except the last one. Moreover, we empirically ﬁnd that
batch normalization helps little for the noise removal of real
photographs, partially due to that the real noise distribution
is fundamentally different from Gaussian.

Finally, we note that it is also possible to train a sin-
gle blind CNN denoiser by learning a direct mapping from
noisy observation to clean image. However, as noted in
[62, 41], taking both noisy image and noise level map as
input is helpful in generalizing the learned model to images
beyond the noise model and thus beneﬁts blind denoising.
We empirically ﬁnd that single blind CNN denoiser per-
forms on par with CBDNet for images with lower noise
level, and is inferior to CBDNet for images with heavy
noise. Furthermore, the introduction of noise estimation
subnetwork also makes interactive denoising and asymmet-
ric learning allowable. Therefore, we suggest to include the
noise estimation subnetwork in our CBDNet.

3.3. Asymmetric Loss and Model Objective

Both CNN and traditional non-blind denoisers perform
robustly when the input noise SD. is higher than the
ground-truth one (i.e., over-estimation error), which encour-
ages us to adopt asymmetric loss for improving general-
ization ability of CBDNet. As illustrates in FFDNet [62],
BM3D/FFDNet achieve the best result when the input noise
SD. and ground-truth noise SD. are matched. When the
input noise SD. is lower than the ground-truth one, the re-
sults of BM3D/FFDNet contain perceptible noises. When
the input noise SD. is higher than the ground-truth one,
BM3D/FFDNet can still achieve satisfying results by grad-
ually wiping out some low contrast structure along with

1715

the increase of input noise SD. Thus, non-blind denois-
ers are sensitive to under-estimation error of noise SD.,
but are robust to over-estimation error. With such property,
BM3D/FFDNnet can be used to denoise real photographs
by setting relatively higher input noise SD., and this might
explain the reasonable performance of BM3D on the DND
benchmark [45] in the non-blind setting.

To exploit the asymmetric sensitivity in blind denoising,
we present an asymmetric loss on noise estimation to avoid
the occurrence of under-estimation error on the noise level
map. Given the estimated noise level ˆσ(yi) at pixel i and the
ground-truth σ(yi), more penalty should be imposed to their
MSE when ˆσ(yi) < σ(yi). Thus, we deﬁne the asymmetric
loss on the noise estimation subnetwork as,

Lasymm = X

|α − I(ˆσ(yi)−σ(yi))<0| · (ˆσ(yi) − σ(yi))2 , (4)

i

where Ie = 1 for e < 0 and 0 otherwise. By setting 0 <
α < 0.5, we can impose more penalty to under-estimation
error to make the model generalize well to real noise.

Furthermore, we introduce a total variation (TV) regu-

larizer to constrain the smoothness of ˆσ(y),

LT V = k∇h ˆσ(y)k2

2 + k∇v ˆσ(y)k2
2 ,

(5)

where ∇h (∇v) denotes the gradient operator along the hor-
izontal (vertical) direction. For the output ˆx of non-blind
denoising, we deﬁne the reconstruction loss as,

Lrec = kˆx − xk2
2 .

(6)

To sum up, the overall objective of our CBDNet is,

L = Lrec + λasymmLasymm + λT V LT V ,

(7)

where λasymm and λT V denote the tradeoff parameters for
the asymmetric loss and TV regularizer, respectively.
In
our experiments, the PSNR/SSIM results of CBDNet are
reported by minimizing the above objective. As for qualita-
tive evaluation of visual quality, we train CBDNet by further
adding perceptual loss [23] on relu3 3 of VGG-16 [54] to
the objective in Eqn. (7).

3.4. Training with Synthetic and Real Noisy Images

The noise model in Sec. 3.1 can be used to synthesize
any amount of noisy images. And we can also guarantee the
high quality of the clean images. Even though, the noise in
real photographs cannot be fully characterized by the noise
model. Fortunately, according to [43, 45, 1], nearly noise-
free image can be obtained by averaging hundreds of noisy
images from the same scene, and several datasets have been
built in literatures. In this case, the scenes are constrained
to be static, and it is generally expensive to acquire hun-
dreds of noisy images. Moreover, the nearly noise-free im-
age tends to be over-smoothing due to the averaging effect.
Therefore, synthetic and real noisy images can be combined
to improve the generalization ability to real photographs.

In this work, we use the noise model in Sec. 3.1 to gen-
erate the synthetic noisy images, and use 400 images from
BSD500 [40], 1600 images from Waterloo [36], and 1600
images from MIT-Adobe FiveK dataset [7] as the training
data. Speciﬁcally, we use the RGB image x to synthesize
clean raw image L = Mf −1(x) as a reverse ISP process
and use the same f to generate noisy image as Eqns. (2) or
(3), where f is a CRF randomly sampled from those in [16].
As for real noisy images, we utilize the 120 images from the
RENOIR dataset [4]. In particular, we alternatingly use the
batches of synthetic and real noisy images during training.
For a batch of synthetic images, all the losses in Eqn. (7) are
minimized to update CBDNet. For a batch of real images,
due to the unavailability of ground-truth noise level map,
only Lrec and LT V are considered in training. We empiri-
cally ﬁnd that such training scheme is effective in improving
the visual quality for denoising real photographs.

4. Experimental Results

4.1. Test Datasets

Three datasets of

real-world noisy images,

i.e.,

NC12 [29], DND [45] and Nam [43], are adopted:

NC12 includes 12 noisy images. The ground-truth clean
images are unavailable, and we only report the denoising
results for qualitative evaluation.

DND contains 50 pairs of real noisy images and the cor-
responding nearly noise-free images. Analogous to [4],
the nearly noise-free images are obtained by carefully post-
processing of the low-ISO images. PSNR/SSIM results are
obtained through the online submission system.

Nam contains 11 static scenes and for each scene the
nearly noise-free image is the mean image of 500 JPEG
noisy images. We crop these images into 512 × 512 patches
and randomly select 25 patches for evaluation.

4.2. Implementation Details

The model parameters in Eqn. (7) are given by α = 0.3,
λ1 = 0.5, and λ2 = 0.05. Note that the noisy images from
Nam [43] are JPEG compressed, while the noisy images
from DND [45] are uncompressed. Thus we adopt the noise
model in Eqn. (2) to train CBDNet for DND and NC12, and
the model in Eqn. (3) to train CBDNet(JPEG) for Nam.

To train our CBDNet, we adopt the ADAM [26] algo-
rithm with β1 = 0.9. The method in [18] is adopted for
model initialization. The size of mini-batch is 32 and the
size of each patch is 128 × 128. All the models are trained
with 40 epochs, where the learning rate for the ﬁrst 20
epochs is 10−3, and then the learning rate 5 × 10−4 is used
to further ﬁne-tune the model. It takes about three days to
train our CBDNet with the MatConvNet package [56] on a
Nvidia GeForce GTX 1080 Ti GPU.

1716

Table 1: The quantitative results on the DND benchmark.

Table 2: The quantitative results on the Nam dataset [43].

Method

Blind/Non-blind

Denoising on

PSNR

SSIM

Method

Blind/Non-blind

PSNR

CDnCNN-B [61]
EPLL [64]
TNRD [11]
NCSR [13]
MLP [6]
FFDNet [62]
BM3D [12]
FoE [52]
WNNM [17]
GCBD [10]
CIMM [5]
KSVD [3]
MCWNNM [59].
TWSC [58]

Blind
Non-blind
Non-blind
Non-blind
Non-blind
Non-blind
Non-blind
Non-blind
Non-blind
Blind
Non-blind
Non-blind
Blind
Blind

CBDNet(Syn)
CBDNet(Real)
CBDNet(All)

Blind
Blind
Blind

sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB
sRGB

sRGB
sRGB
sRGB

32.43
33.51
33.65
34.05
34.23
34.40
34.51
34.62
34.67
35.58
36.04
36.49
37.38
37.94

37.57
37.72
38.06

0.7900
0.8244
0.8306
0.8351
0.8331
0.8474
0.8507
0.8845
0.8646
0.9217
0.9136
0.8978
0.9294
0.9403

0.9360
0.9408
0.9421

4.3. Comparison with State of the arts

We consider four blind denoising approaches,

i.e.,
NC [29, 28], NI [2], MCWNNM [59] and TWSC [58] in our
comparison. NI [2] is a commercial software and has been
included into Photoshop and Corel PaintShop. Besides,
we also include a blind Gaussian denoising method (i.e.,
CDnCNN-B [61]), and three non-blind denoising methods
(i.e., CBM3D [12], WNNM [17], FFDNet [62]). When ap-
ply non-blind denoiser to real photographs, we exploit [9]
to estimate the noise SD..

NC12. Fig. 3 shows the results of an NC12 images. All
the competing methods are limited in removing noise in the
dark region. In comparison, CBDNet performs favorably in
removing noise while preserving salient image structures.

DND. Table 1 lists the PSNR/SSIM results released on
the DND benchmark website. Undoubtedly, CDnCNN-
B [61] cannot be generalized to real noisy photographs
and performs very poorly. Although the noise SD. is pro-
vided, non-blind Gaussian denoisers, e.g., WNNM [17],
BM3D [12] and FoE [52], only achieve limited perfor-
mance, mainly due to that the real noise is much different
from AWGN. MCWNNM [59] and TWSC [58] are spe-
cially designed for blind denoising of real photographs, and
also achieve promising results. Beneﬁted from the realis-
tic noise model and incorporation with real noisy images,
our CBDNet achieves the highest PSNR/SSIM results, and
slightly better than MCWNNM [59] and TWSC [58]. CBD-
Net also signiﬁcantly outperforms another CNN-based de-
noiser, i.e., CIMM [5]. As for running time, CBDNet takes
about 0.4s to process an 512 × 512 image. Fig. 4 pro-
vides the denoising results of an DND image. BM3D and
CDnCNN-B fail to remove most noise from real photo-
graph, NC, NI, MCWNNM and TWSC still cannot remove
all noise, and NI also suffers from the over-smoothing ef-
fect.
In comparison, our CBDNet performs favorably in
balancing noise removal and structure preservation.

NI [2]
CDnCNN-B [61]
TWSC [58]
MCWNNM [59]
BM3D [12]
NC [29]
WNNM [17]

Blind
Blind
Blind
Blind
Non-blind
Blind
Non-blind

CBDNet
CBDNet(JPEG)

Blind
Blind

31.52
37.49
37.52
37.91
39.84
40.41
41.04

40.02
41.31

SSIM

0.9466
0.9272
0.9292
0.9322
0.9657
0.9731
0.9768

0.9687
0.9784

Table 3: PSNR/SSIM results by different noise models.

Method

CBDNet(G)
CBDNet(HG)
CBDNet(G+ISP)
CBDNet(HG+ISP)
CBDNet(JPEG)

DND [45]

Nam [43]

32.52 / 0.79

33.70 / 0.9084
37.41 / 0.9353
37.57 / 0.9360

—

37.62 / 0.9290
38.40 / 0.9453
39.03 / 0.9563
39.20 / 0.9579
40.51 / 0.9745

Nam. The quantitative and qualitative results are given in
Table 2 and Fig. 5. CBDNet(JPEG) performs much better
than CBDNet (i.e., ∼ 1.3 dB by PSNR) and achieves the
best performance in comparison to state-of-the-arts.

4.4. Ablation Studies

Effect of noise model.
Instead of AWGN, we consider
heterogeneous Gaussian (HG) and in-camera processing
(ISP) pipeline for modeling image noise. On DND and
Nam, we implement four variants of noise models:
(i)
Gaussian noise (CBDNet(G)), (ii) heterogeneous Gaus-
sian (CBDNet(HG)), (iii) Gaussian noise and ISP (CBD-
Net(G+ISP)), and (iv) heterogeneous Gaussian and ISP
(CBDNet(HG+ISP), i.e., full CBDNet. For Nam, CBD-
Net(JPEG) is also included. Table 3 shows the PSNR/SSIM
results of different noise models.

G vs HG. Without ISP, CBDNet(HG) achieves about
0.8 ∼ 1 dB gain over CBDNet(G). When ISP is included,
the gain by HG is moderate, i.e., CBDNet(HG+ISP) only
outperforms CBDNet(G+ISP) about 0.15 dB.

w/o ISP.
In comparison, ISP is observed to be more crit-
ical for modeling real image noise.
In particular, CBD-
Net(G+ISP) outperforms CBDNet(G) by 4.88 dB, while
CBDNet(HG+ISP) outperforms CBDNet(HG) by 3.87 dB
on DND. For Nam, the inclusion of JPEG compression in
ISP further brings a gain of 1.31 dB.

Incorporation of synthetic and real images. We imple-
ment two baselines: (i) CBDNet(Syn) trained only on syn-
thetic images, and (ii) CBDNet(Real) trained only on real
images, and rename our full CBDNet as CBDNet(All). Fig.
7 shows the denoising results of these three methods on a
NC12 image. Even trained on large scale synthetic image
dataset, CBDNet(Syn) still cannot remove all real noise,
partially due to that real noise cannot be fully character-
ized by the noise model. CBDNet(Real) may produce over-
smoothing results, partially due to the effect of imperfect

1717

(a) Noisy image

(b) WNNM [17]

(c) FFDNet [62]

(d) NC [29]

(e) NI [2]

(f) MCWNNM [59]

(g) TWSC [58]

(h) CBDNet

Figure 3: Denoising results of another NC12 image by different methods.

(a) Noisy image

(b) BM3D [12]

(c) CDnCNN-B [61]

(d) NC [29]

(e) NI [2]

(f) MCWNNM [59]

(g) TWSC [58]

(h) CBDNet

Figure 4: Denoising results of a DND image by different methods.

(a) Noisy image

(b) WNNM [17]

(c) CDnCNN-B [61]

(d) NC [29]

(e) NI [2]

(f) MCWNNM [59]

(g) TWSC [58]

(h) CBDNet

Figure 5: Denoising results of a Nam image by different methods.

1718

(a) Noisy

(b) γ = 0.4

(c) γ = 0.7

(d) γ = 1.0

(e) γ = 1.3

(f) γ = 1.6

Figure 6: Results by interactive image denoising on two DND images.

(a) Noisy image

(b) CBDNet(Syn)

(c) CBDNet(Real)

(d) CBDNet(All)

Figure 7: Denoising results of CBDNet trained by different data.

noise-free images. In comparison, CBDNet(All) is effec-
tive in removing real noise while preserving sharp edges.
Also quantitative results of the three models on DND are
shown in Table 1. CBDNet(All) obtains better PSNR/SSIM
results than CBDNet(Syn) and CBDNet(Real).

Asymmetric loss. Fig. 8 compares the denoising results
of CBDNet with different α values, i.e., α = 0.5, 0.4 and
0.3. CBDNet imposes equal penalty to under-estimation
and over-estimation errors when α = 0.5, and more penalty
is imposed on under-estimation error when α < 0.5. It can
be seen that smaller α (i.e., 0.3) is helpful in improving the
generalization ability of CBDNet to unknown real noise.

4.5. Interactive Image Denoising

Given the estimated noise level map ˆσ(y), we introduce
a coefﬁcient γ (> 0) to interactively modify ˆσ(y) to ˆ̺ =
γ · ˆσ(y). By allowing the user to adjust γ, the non-blind
denoising subnetwork takes ˆ̺ and the noisy image as input
to obtain denoising result. Fig. 6 presents two real noisy
DND images as well as the results obtained using different
γ values. By specifying γ = 0.7 to the ﬁrst image and

noisy

α = 0.5

(a) Noisy image

(b) Denoised patches

α = 0.4

α = 0.3

Figure 8: Denoising results of CBDNet with different α values

γ = 1.3 to the second, CBDNet can achieve the results
with better visual quality in preserving detailed textures and
removing sophisticated noise, respectively. Such interactive
scheme can thus provide a convenient means for adjusting
the denosing results in practical scenario.

5. Conclusion

We presented a CBDNet for blind denoising of real-
world noisy photographs. The main ﬁndings of this work
are two-fold. First, realistic noise model, including het-
erogenous Gaussian and ISP pipeline, is critical in making
the learned model from synthetic images be applicable to
real-world noisy photographs. Second, the denoising per-
formance of a network can be boosted by incorporating both
synthetic and real noisy images in training. Moreover, by
introducing a noise estimation subnetwork into CBDNet,
we were able to utilize asymmetric loss to improve its gen-
eralization ability to real-world noise, and perform interac-
tive denoising conveniently.

6. Acknowledgements

This work is supported by NSFC (grant no. 61671182,
61872118, 61672446) and HK RGC General Research
Fund (PolyU 152216/18E).

1719

References

[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S
Brown. A high-quality denoising dataset for smartphone
cameras. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1692–1700,
2018. 2, 5

[2] Neatlab ABSoft.

Neat

image.

https://ni.

neatvideo.com/home. 6, 7

[3] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-
svd: An algorithm for designing overcomplete dictionaries
for sparse representation. 2006. 1, 6

[4] Josue Anaya and Adrian Barbu. Renoir - a dataset for real

low-light noise image reduction. 2014. 5

[5] Saeed Anwar, Cong Phuoc Huynh, and Fatih Murat Porikli.
Chaining identity mapping modules for image denoising.
CoRR, abs/1712.02933, 2017. 6

[6] Harold Christopher Burger, Christian J. Schuler, and Stefan
Harmeling.
Image denoising: Can plain neural networks
compete with bm3d? 2012 IEEE Conference on Computer
Vision and Pattern Recognition, pages 2392–2399, 2012. 2,
6

[7] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr´edo
Durand. Learning photographic global tonal adjustment with
a database of input / output image pairs.
In The Twenty-
Fourth IEEE Conference on Computer Vision and Pattern
Recognition, 2011. 5

[8] Priyam Chatterjee and Peyman Milanfar. Is denoising dead?
IEEE Transactions on Image Processing, 19:895–911, 2010.
1

[9] Guangyong Chen, Fengyuan Zhu, and Pheng-Ann Heng. An
efﬁcient statistical method for image noise level estimation.
2015 IEEE International Conference on Computer Vision
(ICCV), pages 477–485, 2015. 3, 6

[10] Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming
Yang. Image blind denoising with generative adversarial net-
work based noise modeling.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3155–3164, 2018. 6

[11] Yunjin Chen and Thomas Pock. Trainable nonlinear reaction
diffusion: A ﬂexible framework for fast and effective image
restoration. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 39:1256–1272, 2017. 1, 2, 6

[12] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and
Karen O. Egiazarian. Color image denoising via sparse 3d
collaborative ﬁltering with grouping constraint in luminance-
chrominance space. 2007 IEEE International Conference on
Image Processing, 1:I – 313–I – 316, 2007. 1, 2, 6, 7

[13] Weisheng Dong, Lei Zhang, Guangming Shi, and Xin
Li. Nonlocally centralized sparse representation for im-
age restoration.
IEEE Transactions on Image Processing,
22:1620–1630, 2013. 6

[14] Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and
Karen O. Egiazarian. Practical poissonian-gaussian noise
modeling and ﬁtting for single-image raw-data. IEEE Trans-
actions on Image Processing, 17:1737–1754, 2008. 2, 3, 4

[15] Zheng Gong, Zuowei Shen, and Kim-Chuan Toh.

Image
restoration with mixed or unknown noises. Multiscale Mod-
eling and Simulation, 12:458–487, 2014. 3

[16] Michael D. Grossberg and Shree K. Nayar. Modeling the
space of camera response functions. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 26:1272–1282,
2004. 4, 5

[17] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu
Feng. Weighted nuclear norm minimization with application
to image denoising. 2014 IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 2862–2869, 2014. 1, 6,
7

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. 2015 IEEE International
Conference on Computer Vision (ICCV), pages 1026–1034,
2015. 5

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, 2016. 2

[20] Youngbae Hwang,

Jun-Sik Kim,

and In-So Kweon.
Difference-based image noise modeling using skellam distri-
bution. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 34:1329–1341, 2012. 3

[21] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015. 2

[22] Viren Jain and H. Sebastian Seung. Natural image denoising

with convolutional networks. In NIPS, 2008. 2

[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In
losses for real-time style transfer and super-resolution.
European Conference on Computer Vision, pages 694–711.
Springer, 2016. 5

[24] Charles Kervrann, J´erˆome Boulanger, and Pierrick Coup´e.
image redundancy and

Bayesian non-local means ﬁlter,
adaptive dictionaries for noise removal. In SSVM, 2007. 3

[25] Seon Joo Kim, Hai Ting Lin, Zheng Lu, Sabine S¨usstrunk,
Stephen Lin, and Michael S. Brown. A new in-camera
imaging model for color computer vision and its application.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 34:2289–2302, 2012. 3

[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014. 5

[27] Marc Lebrun, Antoni Buades, and Jean-Michel Morel. A
nonlocal bayesian image denoising algorithm. SIAM Journal
on Imaging Sciences, 6(3):1665–1688, 2013. 3

[28] Marc Lebrun, Miguel Colom, and Jean-Michel Morel. Mul-
tiscale image blind denoising. IEEE Transactions on Image
Processing, 24:3149–3161, 2015. 3, 6

[29] Marc Lebrun, Miguel Colom, and Jean-Michel Morel. The
noise clinic: a blind image denoising algorithm. IPOL Jour-
nal, 5:1–54, 2015. 2, 3, 5, 6, 7

[30] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli
Laine, Tero Karras, Miika Aittala,
and Timo Aila.
Noise2noise: Learning image restoration without clean data.
arXiv preprint arXiv:1803.04189, 2018. 2

1720

[31] Anat Levin, Boaz Nadler, Fr´edo Durand, and William T.
Freeman. Patch complexity, ﬁnite pixel correlations and op-
timal denoising. In ECCV, 2012. 1

[32] Ce Liu, Richard Szeliski, Sing Bing Kang, C. Lawrence Zit-
nick, and William T. Freeman. Automatic estimation and
removal of noise from a single image.
IEEE Transactions
on Pattern Analysis and Machine Intelligence, 30:299–314,
2008. 2, 3

[33] Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and
Wangmeng Zuo. Multi-level wavelet-cnn for image restora-
tion. CoRR, abs/1805.07071, 2018. 2

[34] Xinhao Liu, Masayuki Tanaka, and Masatoshi Okutomi.
Single-image noise level estimation for blind denoising.
IEEE Transactions on Image Processing, 22:5226–5237,
2013. 3

[35] Xinhao Liu, Masayuki Tanaka, and Masatoshi Okutomi.
Practical signal-dependent noise parameter estimation from
a single noisy image. IEEE Transactions on Image Process-
ing, 23:4361–4371, 2014. 3

[36] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang,
Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo
exploration database: New challenges for image quality as-
sessment models. IEEE Transactions on Image Processing,
26:1004–1016, 2016. 5

[37] Henrique S Malvar, Li-wei He, and Ross Cutler. High-
interpolation for demosaicing of bayer-
quality linear
patterned color images.
In Acoustics, Speech, and Signal
Processing, 2004. Proceedings.(ICASSP’04). IEEE Interna-
tional Conference on, volume 3, pages iii–485. IEEE, 2004.
4

[38] Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang.

Image
restoration using very deep convolutional encoder-decoder
networks with symmetric skip connections. In NIPS, 2016.
1, 2

[39] Charles H. Martin and Michael W. Mahoney. Rethinking
generalization requires revisiting old ideas: statistical me-
chanics approaches and complex learning behavior. CoRR,
abs/1710.09553, 2017. 1, 2, 3

[40] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-
cal statistics. In Proc. 8th Int’l Conf. Computer Vision, vol-
ume 2, pages 416–423, July 2001. 5

[41] Ben Mildenhall, Jonathan T Barron, Jiawen Chen, Dillon
Sharlet, Ren Ng, and Robert Carroll. Burst denoising with
kernel prediction networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2502–2510, 2018. 4

[42] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units

improve restricted boltzmann machines. In ICML, 2010. 4

[43] Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita,
and Seon Joo Kim. A holistic approach to cross-channel
image noise modeling and its application to image denois-
ing. 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1683–1691, 2016. 2, 3, 5, 6

[44] Alberto Ortiz and Gabriel Oliver. Radiometric calibration
of ccd sensors: dark current and ﬁxed pattern noise estima-

tion.
IEEE International Conference on Robotics and Au-
tomation, 2004. Proceedings. ICRA ’04. 2004, 5:4730–4735
Vol.5, 2004. 2

[45] Tobias Plotz and Stefan Roth. Benchmarking denoising al-
gorithms with real photographs. 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
2750–2759, 2017. 1, 2, 5, 6

[46] Javier Portilla. Blind non-white noise removal in images
In

using gaussian scale mixtures in the wavelet domain.
Benelux Signal Processing Symposium, 2004. 3

[47] Javier Portilla. Full blind denoising through noise covariance
estimation using gaussian scale mixtures in the wavelet do-
main. 2004 International Conference on Image Processing,
2004. ICIP ’04., 2:1217–1220 Vol.2, 2004. 3

[48] Stanislav Pyatykh, J¨urgen Hesser, and Lei Zheng.

Im-
age noise level estimation by principal component analysis.
IEEE Transactions on Image Processing, 22:687–699, 2013.
3

[49] Tamer F. Rabie. Robust estimation approach for blind de-
noising. IEEE Transactions on Image Processing, 14:1755–
1765, 2005. 2, 3

[50] Yaniv Romano, Michael Elad, and Peyman Milanfar. The
little engine that could: Regularization by denoising (red).
CoRR, abs/1611.02862, 2016. 1

[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical Image Com-
puting and Computer-Assisted Intervention, pages 234–241.
Springer, 2015. 4

[52] Stefan Roth and Michael J. Black. Fields of experts: a frame-
work for learning image priors. 2005 IEEE Computer Soci-
ety Conference on Computer Vision and Pattern Recognition
(CVPR’05), 2:860–867 vol. 2, 2005. 6

[53] Uwe Schmidt and Stefan Roth. Shrinkage ﬁelds for effec-
tive image restoration. 2014 IEEE Conference on Computer
Vision and Pattern Recognition, pages 2774–2781, 2014. 1,
2

[54] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 5

[55] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration.
2017 IEEE International Conference on Computer Vision
(ICCV), pages 4549–4557, 2017. 2

[56] Andrea Vedaldi and Karel Lenc. Matconvnet - convolutional

neural networks for matlab. In ACM Multimedia, 2015. 5

[57] Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising
and inpainting with deep neural networks. In NIPS, 2012. 2
[58] Jun Xu, Lei Zhang, and David Zhang. A trilateral weighted
In

sparse coding scheme for real-world image denoising.
European Conference on Computer Vision, 2018. 3, 6, 7

[59] Jun Xu, Lei Zhang, David Zhang, and Xiangchu Feng.
Multi-channel weighted nuclear norm minimization for real
color image denoising. 2017 IEEE International Conference
on Computer Vision (ICCV), pages 1105–1113, 2017. 3, 6, 7
[60] Dong Yang and Jian Sun. Bm3d-net: A convolutional neural
network for transform-domain collaborative ﬁltering. IEEE
Signal Processing Letters, 25:55–59, 2018. 2

1721

[61] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE Transactions on Image
Processing, 26:3142–3155, 2017. 1, 2, 3, 4, 6, 7

[62] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward
a fast and ﬂexible solution for cnn based image denoising.
CoRR, abs/1710.04026, 2017. 1, 2, 4, 6, 7

[63] Fengyuan Zhu, Guangyong Chen, and Pheng-Ann Heng.
From noise modeling to blind image denoising. 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 420–429, 2016. 3

[64] Daniel Zoran and Yair Weiss. From learning models of natu-
ral image patches to whole image restoration. 2011 Inter-
national Conference on Computer Vision, pages 479–486,
2011. 6

1722

