Adversarial Structure Matching for Structured Prediction Tasks

Jyh-Jing Hwang1,2,∗

Tsung-Wei Ke1,∗

Jianbo Shi2

Stella X. Yu1

1UC Berkeley / ICSI
Berkeley, CA, USA

2University of Pennsylvania

Philadelphia, PA, USA

{jyh,twke,stellayu}@berkeley.edu

{jyh,jshi}@seas.upenn.edu

Abstract

Pixel-wise losses, e.g., cross-entropy or L2, have been
widely used in structured prediction tasks as a spatial ex-
tension of generic image classiﬁcation or regression. How-
ever, its i.i.d. assumption neglects the structural regular-
ity present in natural images. Various attempts have been
made to incorporate structural reasoning mostly through
structure priors in a cooperative way where co-occurring
patterns are encouraged.

We, on the other hand, approach this problem from an
opposing angle and propose a new framework, Adversar-
ial Structure Matching (ASM), for training such structured
prediction networks via an adversarial process, in which
we train a structure analyzer that provides the supervisory
signals, the ASM loss. The structure analyzer is trained to
maximize the ASM loss, or to emphasize recurring multi-
scale hard negative structural mistakes among co-occurring
patterns. On the contrary, the structured prediction net-
work is trained to reduce those mistakes and is thus enabled
to distinguish ﬁne-grained structures. As a result, training
structured prediction networks using ASM reduces contex-
tual confusion among objects and improves boundary local-
ization. We demonstrate that our ASM outperforms pixel-
wise IID loss or structural prior GAN loss on three different
structured prediction tasks: semantic segmentation, monoc-
ular depth estimation, and surface normal prediction.

1. Introduction

Pixel-wise losses, e.g. cross-entropy or L2, are widely
used in structured prediction tasks such as semantic seg-
mentation, monocular depth estimation, and surface normal
prediction [21, 40, 13, 31], as a spatial extension of generic
image recognition [29, 20]. However, the disadvantage of
such pixel-wise losses is also obvious due to its additive na-
ture and i.i.d. assumption of predictions: IID losses would

* Equal contributions.

Figure 1: Experimental results on semantic segmentation on
PASCAL VOC 2012 [14], monocular depth estimation, and
surface normal prediction on Stanford 2D-3D-S [2]. Net-
works trained using IID loss and shape prior based (GAN)
loss mostly fail at confusing contexts (top row) and ambigu-
ous boundaries (bottom 2 rows) whereas our ASM approach
improves upon these aspects.

yield the same overall error for different spatial distributions
of prediction mistakes. Ideally, some mistakes such as im-
plausible and incomplete round wheels should incur more
penalty than slightly thinner wheels. Structural reasoning is
thus highly desirable for structured prediction tasks.

Various attempts have been made to incorporate struc-
tural reasoning into structured prediction in a coopera-
tive way, including two mainstreams, bottom-up Condi-
tional Random Fields (CRFs) [28, 61] and top-down shape
priors [53, 19, 24] or Generative Adversarial Networks
(GANs) [17, 41, 22]: (1) CRF enforces label consistency
between pixels and is commonly employed as a post-
processing step [28, 7], or as a plug-in module inside deep
neural networks [61, 39] that coordinate bottom-up infor-
mation. Effective as it is, CRF is usually sensitive to in-
put appearance changes and needs expensive iterative infer-
ence. (2) As an example of learning top-down shape pri-
ors, GAN emerges as an alternative to enforce structural

4056

regularity in the structured prediction space. Speciﬁcally,
the discriminator network is trained to distinguish the pre-
dicted mask from the ground truth mask. Promising as it
is, GAN suffers from inaccurate boundary localization as
a consequence of generic shape modeling. More recently,
Ke et al. [24] propose adaptive afﬁnity ﬁelds that capture
structural information with adaptive receptive ﬁelds. How-
ever, it is designed speciﬁcally for classiﬁcation and cannot
be extended straightforward to regression.

As a result, top-down cooperative approaches prefer an
additional loss (together with IID) that penalizes more on
the anomaly structures that are deemed undesirable. Such
trained networks are thus aware of intra-category shape in-
variance and inter-category object co-occurrences. How-
ever, we notice that in real examples as in Fig. 1, complex
and deformable shapes and confusing co-occurrences are
the most common mistakes in structured prediction espe-
cially when the visual cues are ambiguous. As a result,
training with shape priors sometimes deteriorates the pre-
diction as shown in the bicycle example. We are thus in-
spired to tackle this problem from an opposing angle: top-
down approaches should adapt the focus to confusing co-
occurring context or ambiguous boundaries so as to make
the structured prediction network learn harder.

We propose a new framework called Adversarial Struc-
ture Matching (ASM), which replaces IID losses, for train-
ing structured prediction networks via an adversarial pro-
cess, in which we train a structure analyzer to provide su-
pervisory signals, the adversarial structure matching (ASM)
loss. By maximizing ASM loss, or learning to exaggerate
structural mistakes from the structured prediction networks,
the structure analyzer not only becomes aware of complex
shapes of objects but adaptively emphasize those multi-
scale hard negative structural mistakes. As a result, train-
ing structured prediction networks by minimizing ASM
loss reduces contextual confusion among co-occurring ob-
jects and improves boundary localization. To improve the
stability of training, we append a structure regularizer on
the structure analyzer to compose a structure autoencoder.
By training the autoencoder to reconstruct ground truth,
which contains complete structures, we ensure the ﬁlters
in the structure analyzer form a good structure basis. We
demonstrate that structured prediction networks trained us-
ing ASM outperforms its pixel-wise counterpart and GAN
on the ﬁgure-ground segmentation task on Weizmann horse
dataset [5] and semantic segmentation task on PASCAL
VOC 2012 dataset [14] with various base architectures, such
as FCN [40], U-Net [47], DeepLab [7], and PSPNet [60].
Besides the structured classiﬁcation tasks, we also verify
the efﬁcacy of ASM on structured regression tasks, such as
monocular depth estimation and surface normal prediction,
with U-Net [47] on Stanford 2D-3D-S dataset [2].

2. Related Works

Semantic Segmentation. The ﬁeld of semantic segmenta-
tion has progressed fast in the last few years since the in-
troduction of fully convolutional networks [40, 7]. Both
deeper [60, 34] and wider [44, 47, 56] network architec-
tures have been proposed and have dramatically boosted
the performance on standard benchmarks like PASCAL
VOC 2012 [14]. Notably, multi-scale context informa-
tion emerges to remedy limited receptive ﬁelds, e.g., spa-
tial pyramid pooling [60] and atrous spatial pyramid pool-
ing [8]. Though these methods yield impressive perfor-
mance w.r.t. mIoU, they fail to capture rich structure in-
formation present in natural scenes.

Structure Modeling. To overcome the aforementioned
drawback, researchers have explored several ways to incor-
porate structure information [28, 9, 61, 39, 35, 4, 53, 19,
24]. For example, Chen et al. [7] utilized denseCRF [28]
as post-processing to reﬁne the ﬁnal segmentation results.
Zheng et al. [61] and Liu et al. [39] further made the CRF
module differentiable within the deep neural network. Be-
sides, low-level cues, such as afﬁnity [49, 42, 38, 4] and
contour [3, 6] have also been leveraged to encode image
structures. However, these methods either are sensitive to
appearance changes or require expensive iterative inference.

Monocular Depth Estimation and Normal Prediction.
With large-scale RGB-D data available [50, 2, 57], data-
driven approaches [23, 25, 30, 31, 33, 37, 48, 13, 15]
based on deep neural networks make remarkable progress
on depth estimation and surface normal prediction.
Just
like in semantic segmentation, some incorporate structural
regularization such as CRFs into the system and demon-
strate notable improvements [26, 32, 36, 51, 52, 55, 63, 10].
Recently, leveraging depth estimation and semantic seg-
mentation for each other has emerged to be a promising
direction to improve structured predictions of both tasks
[11, 12, 18, 45, 50]. Others propose to jointly train a net-
work for both tasks [43, 46, 27, 59, 54]. Our work is also
along this direction as the ASM framework can train differ-
ent structured prediction tasks in a consistent manner.

3. Method

We provide an overview of the framework in Fig. 2 and

summarize the training procedure in Alg. 1.

3.1. Adversarial Structure Matching

We consider structured prediction tasks, in which a struc-
tured prediction network (structured predictor) S : x 7→ ˆy,
which usually is a deep CNN, is trained to map an input
image x ∈ Rn to a per-pixel label mask ˆy ∈ Rn. We
propose to train such a structured predictor with another
network, structure analyzer. The analyzer A : Rn 7→ Rk

4057

Figure 2: Framework overview: A structure analyzer extracts structure features (red arrows) from structured outputs. The
analyzer is trained to maximize an adversarial structure matching (ASM) loss, or discrepancy between multi-scale structure
features extracted from ground truth and from predictions of a structured predictor. The analyzer thus learns to exaggerate
the hard negative structural mistakes and to distinguish ﬁne-grained structures. The structured predictor on the contrary is
trained to minimize the ASM loss. To make sure the ﬁlters in the analyzer form a good structure basis, we introduce a
structure regularizer, which together with the analyzer, constitutes an autoencoder. The autoencoder is trained to reconstruct
ground truth with a structure regularization (SR) loss. Dotted lines denote computations during training only.

extracts k-dimensional multi-layer structure features from
either ground truth masks, denoted as A(y), or predictions,
denoted as A(S(x)). We train the analyzer to maximize the
distance between the structure features from either inputs,
so that it learns to exaggerate structural mistakes, or hard
negative structural examples, made by the structured pre-
dictor. On the contrary, we simultaneously train the struc-
tured predictor to minimize the same distance.
In other
words, structured predictor S and structure analyzer A play
the following two-player minimax game with value func-
tion V (S, A):

min

S

max

A

V (S, A) = E

x,y(cid:20) 1

2

kA (S(x)) − A(y)k2

2(cid:21) ,

(1)
that is, we prefer the optimal structured predictor as the one
that learns to predict true structures to satisfy the analyzer.
Note that the analyzer will bias its discriminative power to-
wards similar but subtly different structures as they occur
more frequently through the course of training.

One might relate this framework to GAN [17]. A critical
distinction is that GAN tries to minimize the data distribu-
tions between real and fake examples and thus accepts a
set of solutions. Here, structured prediction tasks require a
speciﬁc one-to-one mapping of each pixel between ground
truth masks and predictions. Therefore, the discrimina-
tion of structures should take place for every patch between
corresponding masks, hence the name adversarial structure
matching (ASM).

It is also related to perceptual loss [16, 58] for style trans-

fer, which uses pretrained CNN features to capture image
statistics. We, on the other hand, generalize this concept by
adapting the CNN and accepting any dimensional inputs.

3.2. Global Optimality of S(x) = y and Convergence

We would like the structured predictor to converge to a
good mapping of y given x, if given enough capacity and
training time. To simplify the dynamic of convergence, we
consider both structured predictor S and analyzer A as mod-
els with inﬁnite capacity in a non-parametric setting.

6= y,

if S(x)

Proposition 1. For a ﬁxed S,
then
kA∗ (S(x)) − A∗(y)k2
2 is inﬁnitely large for an optimal A.
Proof. If S(x) 6= y, there exists an index i such that
S(x)[i] − y[i] = ǫ, where ǫ ∈ R \ {0}. Without loss
of generality, we assume S(x)[j] = y[j] if j 6= i and let
S(x)[i] = c + 1

2 ǫ and y[i] = c − 1

2 ǫ.

We consider a special case where Al on the i-th dimen-
sion of the input is a linear mapping, i.e., Al(x[i]) = wix[i].
As A is with inﬁnite capacity, we know there exists A such
that

kA (S(x)) − A(y)k2

wi(cid:18)c +

1
2

(cid:13)(cid:13)(cid:13)(cid:13)

2 ≥ kAl (S(x)) − Al(y)k2
ǫ(cid:19) − wi(cid:18)c −

1
2

2

2 =

= |wiǫ|

(2)

ǫ(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)

2

2 → ∞ as |wi| → ∞. Thus

Note that kAl (S(x)) − Al(y)k2
kA∗ (S(x)) − A∗(y)k2

2 → ∞.

4058

In

practice,

parameters

within certain range under weight
kA∗ (S(x)) − A∗(y)k2

2 would not go to inﬁnity.

of A are

restricted
regularization so

Corollary 1. For an optimal A, S(x) = y if and only if
A∗(S(x)) = A∗(y).
Proof. ⇒ If S(x) = y, kA (S(x)) − A(y)k2
kA(y) − A(y)k2
=
0,
2
kA∗ (S(x)) − A∗(y)k2
2 = 0.
⇐ If A∗(S(x)) = A∗(y) or kA∗ (S(x)) − A∗(y)k2
S(x) 6= y contradicts Proposition 1. Hence S(x) = y.

2 =
Hence

any A.

2 = 0,

for

Theorem 1. If (S∗, A∗) is a Nash equilibrium of the system,
then S∗(x) = y and V (S∗, A∗) = 0
Proof. From Proposition 1, we proved V (S, A∗) → ∞ if
S(x) 6= y. From Corollary 1, we proved V (S, A∗) = 0 if
and only if S(x) = y. Since V (S, A) ≥ 0 for any S and
A, the Nash equilibrium only exists when S∗(x) = y, or
V (S∗, A∗) = 0.

From the proofs, we recognize imbalanced powers be-
tween the structured predictor and structure analyzer where
the analyzer can arbitrarily enlarge the value function if the
structured predictor is not optimal. In practice, we should
limit the training of the analyzers or apply regularization,
such as weight regularization or gradient capping to prevent
gradient exploding. Therefore, we train the analyzer only
once per iteration with a learning rate that is not larger than
the one for structured predictor. Another trick for semantic
segmentation is to binarize the predictions S(x) (winner-
take-all across channels for every pixel) before calculating
ASM loss for analyzer. In this way, the analyzers will fo-
cus on learning to distinguish the structures instead of the
conﬁdence levels of predictions.

3.3. Reconstructing y as Structure Regularization

Although theoretically structure analyzers would dis-
cover any structural difference between predictions and
ground truth, randomly initialized analyzers suffer from
missing certain structures in the early stage. For example,
if ﬁlter responses for a sharp curve are initially very low,
ASM loss for the sharp curve will be as small, resulting in
inefﬁcient learning. This problem will emerge when train-
ing both structured predictors and structure analyzers from
scratch. To alleviate this problem, we propose a regulariza-
tion method to stabilize the learning of analyzers.

One way to ensure the ﬁlters in the analyzer form a
good structure basis is through reconstructing ground truth,
which contains complete structures. If ﬁlters in the analyzer
fail to capture certain structures, the ground truth mask can-
not be reconstructed. Hence, we append a structure regu-
larizer on top of structure analyzer to constitute an autoen-
coder. we denote the structure regularizer R : At(y) 7→ y,
where At(·) denotes features from the structure analyzer,

Algorithm 1: Algorithm for training structured predic-
tion networks using ASM.

for number of training iterations do

/* Train structure analyzer */
(optional) Binarize structured predictions S(x(i)).
Update the structure analyzer A and regularizer R
by ascending its stochastic gradient:

∇θA,R

1
m

m

Xi=1h 1

2

2 (cid:13)(cid:13)(cid:13)A(cid:16)S(x(i))(cid:17) − A(y(i))(cid:13)(cid:13)(cid:13)
+ λIID (y, R (At(y)))i

2

/* Train structured predictor */
Sample a minibatch with m images
{x(1), . . . , x(m)} and ground truth masks
{y(1), . . . , y(m)}.
Update the structured predictor S by descending
its stochastic gradient:

∇θS

1
2m

m

Xi=1(cid:13)(cid:13)(cid:13)A(cid:16)S(x(i))(cid:17) − A(y(i))(cid:13)(cid:13)(cid:13)

2

2

end
The gradient-based updates can use any standard
gradient-based learning rule. Structure analyzer A
should use a learning rate that is not larger than
structured predictor S.

which are not necessarily the same set of features for ASM;
hence the reconstruction mapping: R (At(y)) 7→ y. As a
result, the ﬁnal objective function is as follows

2

x,y(cid:20) 1
{z

S∗ = argmin

S

E

max

A

kA (S(x)) − A(y)k2

+ min
A,R

|
|

adversarial structure matching loss

λE

y [IID (y, R (At(y)))]

,

(3)

structure regularization loss

{z

}

2(cid:21)
}

where IID (y, R (At(y))) is deﬁned for target tasks as:

Semantic segmentation: − y · log R (At(y))
Depth estimation: ky − R (At(y))k2
2

Surface normal Prediction:(cid:13)(cid:13)(cid:13)(cid:13)

y

kyk2

−

R (At(y))

kR (At(y))k2(cid:13)(cid:13)(cid:13)(cid:13)

2

2

Note that structure regularization loss is independent to S.

4059

Figure 3: Error maps of predictions and reconstructions by the structure autoencoder on VOC PASCAL 2012. The autoen-
coder network is able to reconstruct the missing part of certain structures, for example, it completes the round wheel in the
front. Note that neither the structure analyzer nor regularizer has access to input images.

Figure 4: Visualization of loss maps from different layers in the structure analyzer. Examples are from monocular depth
estimation on 2D-3D-Semantics [2]. We observe earlier layers capture lower-level structural mistakes as opposed to later
layers. It is worth to note that conv9, with skip connection from conv1, provides low-level structure errors guided by global
information. We observe that multi-scale structure supervision is achieved by the multi-layer structure analyzer.

4. Experiments

We demonstrate the effectiveness of our proposed meth-
ods on three structure modeling tasks: semantic segmenta-
tion, monocular depth estimation, and surface normal pre-
diction. We conduct extensive experiments on all three
tasks to compare the same structured prediction networks
trained using ASM, GAN, or IID losses. (Note that GAN
means training with IID loss and adversarial loss.)

We ﬁrst give an overview of the datasets and implemen-
tation details in Sec. 4.1. Then we analyze what is learned
in the structure analyzer to understand the mechanics of
ASM in Sec. 4.2. Finally, we present our main results and
analyses on segmentation in Sec. 4.3 and on depth estima-
tion and surface normal prediction in Sec. 4.4.

4.1. Experimental Setup

Tasks and datasets. We compare our proposed ASM
against GAN and IID losses on the Weizmann horse [5],
PASCAL VOC 2012 [14], and Stanford 2D-3D-Semantics
[2] datasets. The Weizmann horse [5] is a relatively small
dataset for ﬁgure-ground segmentation that contains 328
side-view horse images, which are split into 192 training
and 136 validation images. The VOC 2012 [14] dataset is a
well-known benchmark for generic semantic segmentation
which includes 20 object classes and a ‘background’ class,
containing 10, 582 and 1, 449 images for training and vali-
dation, respectively. We also conduct experiments on 2D-

3D-Semantics [2] dataset for monocular depth and surface
normal prediction. The 2D-3D-S dataset is a large-scale in-
door scene benchmark, which consists of 70, 496 images,
along with depth, surface normal, instance- and semantic-
level segmentation annotations. The results are reported
over fold-1 data splits–Area 1,2,3,4,6 (52, 903 images) for
training and Area 5 (17, 593 images) for testing.

Architectures. For all the structure autoencoders (i.e., ana-
lyzer and regularizer), we use U-Net [47] with either 7 conv
layers for ﬁgure-ground segmentation, 5 conv layers for se-
mantic segmentation and 9 conv layers for depth and surface
normal estimation. We conduct experiments on different
structured prediction architectures. On horse dataset [5],
we use U-Net [47] (with 7 convolutional layers) as the
base architecture. On VOC [14] dataset, we carry out ex-
periments and thorough analyses over 3 different architec-
tures with ResNet-101 [20] backbone, including FCN [40],
DeepLab [7], and PSPNet [60], which is a highly competi-
tive segmentation model. On 2D-3D-S [2] dataset, we use
U-Net [47] (with 18 convolutional layers) as the base archi-
tecture for both depth and surface normal prediction. Aside
from base architectures, neither extra parameters nor post-
processing are required at inference time.

Implementation details on Weizmann horse. We use the
poly learning rate policy where the current learning rate
max iter )0.9 with max
equals the base one multiplied by (1 − iter
iterations as 100 epochs. We set the base learning rate as

4060

0.0005 with Adam optimizer for both S and A. Momentum
and weight decay are set to 0.9 and 0.00001, respectively.
We set the batch size as 1 and use only random mirroring.
For ASM, We set λ = 2 for structure regularization.

Implementation details on VOC dataset. Our implemen-
tation follows the implementation details depicted in [8].
We adopt the same poly learning rate policy and set the
base learning rate with SGD optimizer as 0.001 for S and
0.0005 for A. The training iterations for all experiments
on all datasets are 30K. Momentum and weight decay are
set to 0.9 and 0.0005, respectively. For data augmentation,
we adopt random mirroring and random resizing between
0.5 and 2 for all datasets. We do not use random rotation
and random Gaussian blur. We do not upscale the logits
(prediction map) back to the input image resolution, in-
stead, we follow [7]’s setting by downsampling the ground-
truth labels for training (output stride = 8). The crop
size is set to 336 × 336 and batch size is set to 8. We
update BatchNorm parameters with decay = 0.9997 for
ImageNet-pretrained layers and decay = 0.99 for untrained
layers. For ASM, we set λ = 10 for structure regularization.

Implementation details on 2D-3D-S. We implement pixel-
wise L2 loss and normalized L2 for depth and surface nor-
mal prediction as a counterpart to cross-entropy loss for
segmentation. We adopt the same poly learning rate pol-
icy, and set the base learning rate to 0.01 for both S and
A. While the original image resolution is 1080 × 1080,
we down-sample image to half resolution, set “cropsize” to
512 × 512 for training, and keep 540 × 540 for testing. We
use random cropping and mirroring for data augmentation.
The “batchsize” is set to 8, weight decay is set to 0.0005,
and models are trained for 120K iterations from scratch.

4.2. Analyses of Structure Analyzer

Before getting to the main results, we verify the structure
analyzer actually captures certain structures and visualize
what kind of supervisory signals it provides for training the
structured predictor.

For segmentation, some objects are rigid and certain
shapes are important structures, e.g., rectangles for buses,
circles for wheels, and parallel lines for poles. Here, we vi-
sualize the prediction, reconstruction, and their error maps
in Fig. 3. We observe that that the structure autoencoder
network is able to reconstruct the missing part of certain
structures, for example, it completes round wheels of bikes.
Note that neither the structure analyzer nor regularizer has
access to input images.

We further analyze the losses from each layer in the
structure analyzer for monocular depth estimation, shown
in Fig. 4. We observe that in the early layers where in-
formation is local, the errors are near object boundaries.
In the middle layers, e.g., layer 2 and 3, the structure an-

Figure 5: Top 10 input stimuli (in a row) for a ﬁlter in the
analyzer with maximal activations for semantic segmenta-
tion (top 3 rows), depth estimation (middle 3 rows), and
surface normal prediction (bottom 3 rows).

alyzer attends to broader context, such as the orientation of
walls. In the ﬁnal layer (conv9, with skip connection from
conv1), low-level structure errors are guided by global in-
formation. We observe that multi-scale structure supervi-
sion is achieved by the multi-layer structure analyzer.

To understand what’s learned in the analyzer, we are in-
spired by Zhou et al. [62] to visualize features in a CNN by
ﬁnding the patches that maximally activate a targeted ﬁlter.
We show in Fig. 5 the top 10 input stimuli for three ﬁlters
in different layers for each task. We observe that each ﬁlter
in the analyzer attends to a speciﬁc structure, e.g., chairs,
bottles, hallways, bookcases, etc..

4.3. Main Results on Segmentation

We evaluate both ﬁgure-ground and semantic segmenta-
tion tasks via mean pixel-wise intersection-over-union (de-
noted as mIoU) [40]. We ﬁrst conduct ablation studies
on both datasets to thoroughly analyze the effectiveness of
using different layers of structure features in the structure
analyzer. As summarized in Table 1, using low- to mid-
level features (from conv1 to conv2) of structure analyz-
ers yields the highest performance, 79.62% and 71.60%
mIoU on Weizmann horse dataset and VOC dataset, respec-
tively). We also report mIoU on VOC dataset using differ-
ent base architectures as shown in Table 2. Our proposed
method achieves consistent improvements across all three
base architectures, boosting mIoU by 3.23% with FCN,
0.51% with DeepLab and 1.31% with PSPNet. ASM is also
0.71% higher than GAN (incorporated with IID loss) on

4061

Loss
IID

ASM (Conv1)
ASM (Conv2)
ASM (Conv1-2)
ASM (Conv3)
ASM (Conv1-3)

ASM w/o rec.
ASM w/o adv.

IID+ASM

Horse mIoU (%) VOC mIoU (%)

77.28

78.14
78.15
79.62
77.79
78.11

77.83
76.70

78.34

68.91

70.00
70.70
71.60
70.85
69.81

72.14
71.26

68.49

Table 1: Ablation studies of ASM on Weizmann horse
dataset with U-Net and on PASCAL VOC dataset with
FCN. Generally, using low- to mid-level features (conv1
and conv2) of structure analyzers yield the best perfor-
mance.
It also shows that reconstruction is not always
needed if base networks are pre-trained. DeepLab or PSP-
Net (not shown here) has the same trend as U-Net.

Base / Loss
FCN / IID
FCN / ASM

DeepLab / IID
DeepLab / ASM

PSPNet / IID
PSPNet / cGAN
PSPNet / GAN
PSPNet / ASM

mIoU (%)

68.91
72.14

77.54
78.05

80.12
80.67
80.74
81.43

Table 2: Experimental results on PASVAL VOC with sev-
eral base models, FCN [40], DeepLab [7], and PSPNet [60].
The improvements by replacing pixel-wise losses with
ASM are consistent across different base models. cGAN
[22] denotes the discriminator is conditioned on the input.

VOC dataset. We show some visual comparison in Fig. 8.

Boundary Localization Improvement. We argue that our
proposed method is more sensitive to complex shapes of
objects. We thus evaluate boundary localization using stan-
dard contour detection metrics [1]. The contour detection
metrics compute the correspondences between prediction
boundaries and ground-truth boundaries, and summarize
the results with precision, recall, and f-measure. We com-
pare the results with different loss functions: IID, GAN and
ASM on VOC validation set. As shown in Figure 6, ASM
outperforms both IID and GAN among most categories and
overall in boundary precision. (Detailed numbers including
recall and f-measure can be found in the supplementary.)
The boundaries of thin-structured objects, such as ‘bike’
and ‘chair’, are much better captured by ASM.

Figure 6: Improvements (%) of per-class boundary preci-
sion of PSPNet [60] on PASCAL VOC 2012 [14] validation
set. ASM outperforms IID and GAN in most categories.

rel

log10

rms

δ < 1.25 δ < 1.252 δ < 1.253

0.267 0.393 1.174
IID
cGAN 0.266 0.371 1.123
ASM 0.252 0.405 1.079

0.466
0.509
0.540

0.790
0.816
0.834

0.922
0.932
0.929

Table 3: Depth estimation measurements on 2D-3D-S [2].
Note that lower is better for the ﬁrst three columns, and
higher is better for the last three columns. IID is L2.

4.4. Main Results on Depth and Surface Normal

We compare ASM with pixel-wise L2 loss (IID) and
cGAN [22] for monocular depth estimation. The perfor-
mance is evaluated by the metrics proposed in [13], which
are formulated as follows:

Abs Relative difference (rel) : 1

RMSE (rms) : q 1
log10 : q 1

|T | Py∈T ky − ˆyk2
|T | Py∈T k log y − log ˆyk2

|T | Py∈T |y − ˆy|/ˆy

, ˆyi
yi

) = δ < thr

Threshold : % of yi s.t max( yi
ˆyi
As summarized in Table 3, our proposed method consis-
tently improves among most metrics, including ‘rel’, ‘rms’,
and accuracy with different thresholds. Note that ‘rel’,
‘log10’, and ‘rms’ metrics reﬂect the mean value of error
distributions, which penalizes more on larger errors, which
are usually incurred by mislabeling of the ground truth. On
the other hand, accuracy with thresholds evaluates the me-
dian of errors, which reﬂects more precisely the visual qual-
ity of predictions. We notice that our method outperforms
others even more signiﬁcantly in median errors with smaller
thresholds, which demonstrates the ﬁne-grained structure
discriminator power of ASM. The conclusion is consistent
with our observations from visual comparisons in Fig. 8.

We compare ASM with pixel-wise normalized L2 loss
(IID) and cGAN [22] for surface normal prediction. To
evaluate the prediction quality, we follow [15] and report
the results on several metrics, including the mean and me-
dian of error angles between the ground truth and predic-
tion, and the percentage of error angles within 2.82◦, 5.63◦,

4062

Angle Distance
Mean Median 2.82◦ 5.63◦ 11.25◦ 22.5◦

Within t◦ Deg.

30◦

16.84
IID
cGAN 17.12
ASM 16.98

9.20
9.04
8.28

12.90 31.12
12.28 31.52
17.58 35.48

58.01
58.52
63.29

77.37 82.76
77.00 82.32
78.50 82.47

Table 4: Surface normal estimation measurements on 2D-
3D-S [2]. Note that lower is better for the ﬁrst two columns,
and higher is better for the last three columns. IID is nor-
malized L2.

Figure 7: Improvements (%) of instance-average surface
normal metric on 2D-3D-S [2]. The categories with most
improvements are ‘column’, ‘beam’, ‘table’, and ‘wall’.

11.25◦, 22.5◦, 30◦. The results are presented in Table 4,
and ASM improves metrics including median angles and
percentage of angles within 2.82◦, 5.63◦, 11.25◦ by large
margin. Similar to accuracy with thresholds for depth es-
timation, metrics with smaller angles are more consistent
with visual quality. We conclude that ASM captures most
details and outperforms baselines in most cases. We ob-
serve in Fig. 8 prominent visual improvements on thin struc-
tures, such as bookcase shelves. The surface normal predic-
tion of larger objects, including wall and ceiling, are more
uniform. Also, the contrast between adjacent surfaces are
sharper with ASM.

Instance- and Semantic-level Analysis. Just as mIoU met-
ric in semantic segmentation, the metrics proposed by [13]
for depth and by [15] for surface normal are biased toward
larger objects. We thus follow [24] to evaluate instance-
wise metrics to attenuate the bias and fairly evaluate the per-
formance on smaller objects. We collect semantic instance
masks on 2D-3D-S dataset and formulate the instance-wise
, where Ic denotes the set of instances
metric as
in class c and Mi,c is the metric for depth or surface nor-
mal of instance i in class c. (We do not use instance- or
semantic-level information during training.) As shown in
Fig 7, we demonstrate that ASM improves instance-wise
percentage of angles within 11.25◦ consistently among all
categories. The instance-wise analysis for depth estimation
can be found in the supplementary.

Pi∈Ic
|Ic|

Mi,c

Figure 8: Visual quality comparison for semantic segmen-
tation (top two rows) on VOC [14] validation set, monocu-
lar depth estimation (middle two rows) and surface normal
prediction (bottom two rows) on [2]. Left to right: Images,
predictions from training with IID, IID+(c)GAN, ASM, and
ground truth masks.

5. Conclusion

We proposed a novel framework, Adversarial Structure
Matching, for training structured prediction networks. We
provided theoretical analyses and extensive experiments
to demonstrate the efﬁcacy of ASM. We concluded that
multi-scale hard negative structural errors provide better
supervision than the conventional pixel-wise IID losses (or
incorporated with structure priors) in different structured
prediction tasks, namely, semantic segmentation, monocu-
lar depth estimation, and surface normal prediction.

Acknowledgements. This research was supported, in part,
by Berkeley Deep Drive, NSF (IIS-1651389), DARPA,
and US Government fund through Etegent Technologies
on Low-Shot Detection in Remote Sensing Imagery. The
views, opinions and/or ﬁndings expressed should not be in-
terpreted as representing the ofﬁcial views or policies of
NSF, DARPA, or the U.S. Government.

4063

References

[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour de-
tection and hierarchical image segmentation. TPAMI, 2011.
7

[2] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2d-3d-
semantic data for indoor scene understanding. arXiv preprint
arXiv:1702.01105, 2017. 1, 2, 5, 7, 8

[3] G. Bertasius, J. Shi, and L. Torresani. Semantic segmentation

with boundary neural ﬁelds. In CVPR, 2016. 2

[4] G. Bertasius, L. Torresani, S. X. Yu, and J. Shi. Convolu-
tional random walk networks for semantic image segmenta-
tion. In CVPR, 2017. 2

[5] E. Borenstein and S. Ullman. Class-speciﬁc, top-down seg-

mentation. In ECCV, 2002. 2, 5

[6] L.-C. Chen, J. T. Barron, G. Papandreou, K. Murphy, and
A. L. Yuille. Semantic image segmentation with task-speciﬁc
edge detection using cnns and a discriminatively trained do-
main transform. In CVPR, 2016. 2

[7] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint arXiv:1606.00915, 2016. 1, 2, 5,
6, 7

[8] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-
thinking atrous convolution for semantic image segmenta-
tion. arXiv preprint arXiv:1706.05587, 2017. 2, 6

[9] L.-C. Chen, A. Schwing, A. Yuille, and R. Urtasun. Learning

deep structured models. In ICML, 2015. 2

[10] X. Cheng, P. Wang, and R. Yang. Depth estimation via afﬁn-
ity learned with convolutional spatial propagation network.
In ECCV, 2018. 2

[11] C. Couprie, C. Farabet, L. Najman, and Y. LeCun. Indoor se-
mantic segmentation using depth information. arXiv preprint
arXiv:1301.3572, 2013. 2

[12] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In ICCV, 2015. 2

[13] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
In

from a single image using a multi-scale deep network.
NIPS, 2014. 1, 2, 7, 8

[14] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. IJCV, 2010. 1, 2, 5, 7, 8

[15] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3d prim-
itives for single image understanding. In ICCV, 2013. 2, 7,
8

[16] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm

of artistic style. arXiv preprint arXiv:1508.06576, 2015. 3

[17] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 3

[18] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learning
rich features from rgb-d images for object detection and seg-
mentation. In ECCV, 2014. 2

[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 5

[21] J.-J. Hwang and T.-L. Liu. Pixel-wise deep learning for con-

tour detection. In ICLR Workshop, 2015. 1

[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. CVPR,
2017. 1, 7

[23] K. Karsch, C. Liu, and S. B. Kang. Depth transfer: Depth ex-
traction from video using non-parametric sampling. TPAMI,
2014. 2

[24] T.-W. Ke, J.-J. Hwang, Z. Liu, and S. X. Yu. Adaptive afﬁnity

ﬁeld for semantic segmentation. 2018. 1, 2, 8

[25] A. Kendall and Y. Gal. What uncertainties do we need in
bayesian deep learning for computer vision? In NIPS, 2017.
2

[26] S. Kim, K. Park, K. Sohn, and S. Lin. Uniﬁed depth predic-
tion and intrinsic image decomposition from a single image
via joint convolutional neural ﬁelds. In ECCV, 2016. 2

[27] I. Kokkinos. Ubernet: Training a universal convolutional
neural network for low-, mid-, and high-level vision using
diverse datasets and limited memory. In CVPR, 2017. 2

[28] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS, 2011.
1, 2

[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

Imagenet
In

[30] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of

perspective. In CVPR, 2014. 2

[31] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and
N. Navab. Deeper depth prediction with fully convolutional
residual networks. In 3DV, 2016. 1, 2

[32] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, and M. He. Depth
and surface normal estimation from monocular images using
regression on deep features and hierarchical crfs. In CVPR,
2015. 2

[33] J. Li, R. Klein, and A. Yao. A two-streamed network for
estimating ﬁne-scaled depth maps from single rgb images.
In ICCV, 2017. 2

[34] X. Li, Z. Liu, P. Luo, C. C. Loy, and X. Tang. Not all pixels
are equal: Difﬁculty-aware semantic segmentation via deep
layer cascade. In CVPR, 2017. 2

[35] G. Lin, C. Shen, A. van den Hengel, and I. Reid. Efﬁcient
piecewise training of deep structured models for semantic
segmentation. In CVPR, 2016. 2

[36] F. Liu, C. Shen, and G. Lin. Deep convolutional neural ﬁelds
for depth estimation from a single image. In CVPR, 2015. 2

[37] F. Liu, C. Shen, G. Lin, and I. D. Reid. Learning depth
from single monocular images using deep convolutional neu-
ral ﬁelds. TPAMI, 2016. 2

[38] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and
J. Kautz. Learning afﬁnity via spatial propagation networks.
In NIPS, 2017. 2

[19] M. Gygli, M. Norouzi, and A. Angelova. Deep value net-
works learn to evaluate and iteratively reﬁne structured out-
puts. In ICML, 2017. 1, 2

[39] Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang. Semantic
In CVPR,

image segmentation via deep parsing network.
2015. 1, 2

4064

[61] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional ran-
dom ﬁelds as recurrent neural networks. In ICCV, 2015. 1,
2

[62] B. Zhou, D. Bau, A. Oliva, and A. Torralba.

Interpreting
deep visual representations via network dissection. PAMI,
2018. 6

[63] W. Zhuo, M. Salzmann, X. He, and M. Liu. Indoor scene
In

structure analysis for single image depth estimation.
CVPR, 2015. 2

[40] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1, 2,
5, 6, 7

[41] P. Luc, C. Couprie, S. Chintala, and J. Verbeek. Semantic
segmentation using adversarial networks. NIPS Workshop,
2016. 1

[42] M. Maire, T. Narihira, and S. X. Yu. Afﬁnity cnn: Learning
pixel-centric pairwise relations for ﬁgure/ground embedding.
In CVPR, 2016. 2

[43] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-

stitch networks for multi-task learning. In CVPR, 2016. 2

[44] H. Noh, S. Hong, and B. Han. Learning deconvolution net-

work for semantic segmentation. In CVPR, 2015. 2

[45] X. Ren, L. Bo, and D. Fox. Rgb-(d) scene labeling: Features

and algorithms. In CVPR, 2012. 2

[46] S. R. Richter, Z. Hayder, and V. Koltun. Playing for bench-

marks. In ICCV, volume 2, 2017. 2

[47] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, 2015. 2, 5

[48] A. Roy and S. Todorovic. Monocular depth estimation using

neural regression forest. In CVPR, 2016. 2

[49] J. Shi and J. Malik. Normalized cuts and image segmenta-

tion. TPAMI, 2000. 2

[50] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.

segmentation and support inference from rgbd images.
ECCV, 2012. 2

Indoor
In

[51] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L.
Yuille. Towards uniﬁed depth and semantic prediction from
a single image. In CVPR, 2015. 2

[52] P. Wang, X. Shen, B. Russell, S. Cohen, B. Price, and A. L.
Yuille. Surge: Surface regularized geometry estimation from
a single image. In NIPS, 2016. 2

[53] S. Xie, X. Huang, and Z. Tu. Top-down learning for struc-
In ECCV,

tured labeling with convolutional pseudoprior.
2016. 1, 2

[54] D. Xu, W. Ouyang, X. Wang, and N. Sebe. Pad-net: Multi-
tasks guided prediction-and-distillation network for simulta-
neous depth estimation and scene parsing. arXiv preprint
arXiv:1805.04409, 2018. 2

[55] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe. Multi-
scale continuous crfs as sequential deep networks for monoc-
ular depth estimation. In CVPR, volume 1, 2017. 2

[56] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 2

[57] A. R. Zamir, A. Sax, W. Shen, L. Guibas, J. Malik, and
S. Savarese. Taskonomy: Disentangling task transfer learn-
ing. In CVPR, 2018. 2

[58] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.
The unreasonable effectiveness of deep features as a percep-
tual metric. In CVPR, 2018. 3

[59] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang. Joint
task-recursive learning for semantic segmentation and depth
estimation. In ECCV, 2018. 2

[60] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, 2017. 2, 5, 7

4065

