PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation

Sida Peng1∗

Yuan Liu1∗

Qixing Huang2

Xiaowei Zhou1†

Hujun Bao1†

1Zhejiang University

2University of Texas at Austin

Abstract

This paper addresses the challenge of 6DoF pose esti-
mation from a single RGB image under severe occlusion or
truncation. Many recent works have shown that a two-stage
approach, which ﬁrst detects keypoints and then solves
a Perspective-n-Point (PnP) problem for pose estimation,
achieves remarkable performance. However, most of these
methods only localize a set of sparse keypoints by regressing
their image coordinates or heatmaps, which are sensitive to
occlusion and truncation.
Instead, we introduce a Pixel-
wise Voting Network (PVNet) to regress pixel-wise vectors
pointing to the keypoints and use these vectors to vote for
keypoint locations. This creates a ﬂexible representation for
localizing occluded or truncated keypoints. Another impor-
tant feature of this representation is that it provides uncer-
tainties of keypoint locations that can be further leveraged
by the PnP solver. Experiments show that the proposed ap-
proach outperforms the state of the art on the LINEMOD,
Occlusion LINEMOD and YCB-Video datasets by a large
margin, while being efﬁcient for real-time pose estimation.
We further create a Truncation LINEMOD dataset to vali-
date the robustness of our approach against truncation. The
code is available at https://zju3dv.github.io/pvnet/.

1. Introduction

Object pose estimation aims to detect objects and esti-
mate their orientations and translations relative to a canon-
ical frame [39]. Accurate pose estimation is essential for
a variety of applications such as augmented reality, au-
tonomous driving and robotic manipulation. For instance,
fast and robust pose estimation is crucial in Amazon Pick-
ing Challenge [6], where a robot needs to pick objects from
a warehouse shelf. This paper focuses on the speciﬁc setting
of recovering the 6DoF pose of an object, i.e., rotation and
translation in 3D, from a single RGB image of that object.
This problem is quite challenging from many perspectives,

∗The ﬁrst two authors contributed equally. The authors from Zhejiang
University are afﬁliated with the State Key Lab of CAD&CG and the ZJU-
SenseTime Joint Lab of 3D Vision.

†Corresponding authors: Hujun Bao and Xiaowei Zhou.

Figure 1. The 6D pose estimation problem is formulated as a
Perspective-n-Point (PnP) problem in this paper, which requires
correspondences between 2D and 3D keypoints, as illustrated in
(d) and (e). We predict vectors pointing to keypoints for each pixel,
as shown in (b), and localize 2D keypoints in a RANSAC-based
voting scheme, as shown in (c). The proposed method is robust to
occlusion (g) and truncation (h), where the green bounding boxes
represent the ground truth poses and the blue bounding boxes rep-
resent our predictions.

including object detection under severe occlusions, varia-
tions in lighting and appearance, and cluttered background
objects.

Traditional methods [24, 20, 15] have shown that pose
estimation can be achieved by establishing the correspon-
dences between an object image and the object model. They
rely on hand-crafted features, which are not robust to image
variations and background clutters. Deep learning based
methods [33, 17, 40, 4] train end-to-end neural networks
that take an image as input and output its corresponding
pose. However, generalization remains as an issue, as it
is unclear that such end-to-end methods learn sufﬁcient fea-
ture representations for pose estimation.

Some recent methods [29, 30, 36] use CNNs to ﬁrst
regress 2D keypoints and then compute 6D pose parame-
ters using the Perspective-n-Point (PnP) algorithm. In other
words, the detected keypoints serve as an intermediate rep-
resentation for pose estimation. Such two-stage approaches
achieve state-of-the-art performance, thanks to robust de-
tection of keypoints. However, these methods have difﬁ-
culty in tackling occluded and truncated objects, since part
of their keypoints are invisible. Although CNNs may pre-
dict these unseen keypoints by memorizing similar patterns,

14561

(b) Vectors(c) Voting(a) Input image(d) 2D keypoints(e) 3D keypoints(f) Aligned model(h) Truncation(g) Occlusiongeneralization remains difﬁcult.

We argue that addressing occlusion and truncation re-
quires dense predictions, namely pixel-wise or patch-wise
estimates for the ﬁnal output or intermediate representa-
tions. To this end, we propose a novel framework for
6D pose estimation using a Pixel-wise Voting Network
(PVNet). The basic idea is illustrated in Figure 1. Instead of
directly regressing image coordinates of keypoints, PVNet
predicts vectors that represent directions from each pixel of
the object towards the keypoints. These directions then vote
for the keypoint locations based on RANSAC [9]. This vot-
ing scheme is motivated from a property of rigid objects
that once we see some local parts, we are able to infer the
relative directions to other parts.

Our approach essentially creates a vector-ﬁeld represen-
tation for keypoint localization. In contrast to coordinate or
heatmap based representations, learning such a representa-
tion enforces the network to focus on local features of ob-
jects and spatial relations between object parts. As a result,
the location of an invisible part can be inferred from the vis-
ible parts. In addition, this vector-ﬁeld representation is able
to represent object keypoints that are even outside the input
image. All these advantages make it an ideal representation
for occluded or truncated objects. Xiang et al. [40] proposed
a similar idea to detect objects and here we use it to localize
keypoints.

Another advantage of the proposed approach is that
the dense outputs provide rich information for the PnP
solver to deal with inaccurate keypoint predictions. Speciﬁ-
cally, RANSAC-based voting prunes outlier predictions and
also gives a spatial probability distribution for each key-
point. Such uncertainties of keypoint locations give the PnP
solver more freedom to identify consistent correspondences
for predicting the ﬁnal pose. Experiments show that the
uncertainty-driven PnP algorithm improves the accuracy of
pose estimation.

We evaluate our approach on LINEMOD [15], Occlu-
sion LINEMOD [2] and YCB-Video [40] datasets, which
are widely-used benchmark datasets for 6D pose estimation.
Across all datasets, PVNet exhibits state-of-the-art perfor-
mances. We also demonstrate the capability of our approach
to handle truncated objects on a new dataset called Trunca-
tion LINEMOD which is created by randomly cropping im-
ages of LINEMOD. Furthermore, our approach is efﬁcient,
which runs 25 fps on a GTX 1080ti GPU, to be used for
real-time pose estimation.

In summary, this work has the following contributions:

• We propose a novel framework for 6D pose estima-
tion using a pixel-wise voting network (PVNet), which
learns a vector-ﬁeld representation for robust 2D key-
point localization and naturally deals with occlusion
and truncation.

• We propose to utilize an uncertainty-driven PnP algo-
rithm to account for uncertainties in 2D keypoint loca-
tions, based on the dense predictions from PVNet.

• We demonstrate signiﬁcant performance improve-
ments of our approach compared to the state of the
art on benchmark datasets (ADD: 86.3% vs. 79% and
40.8% vs. 30.4% on LINEMOD and OCCLUSION,
respectively). We also create a new dataset for evalua-
tion on truncated objects.

2. Related work

Holistic methods. Given an image, some methods aim to
estimate the 3D location and orientation of the object in a
single shot. Traditional methods mainly rely on template
matching techniques [16, 12, 14, 42], which are sensitive to
cluttered environments and appearance changes. Recently,
CNNs have shown signiﬁcant robustness to environment
variations. As a pioneer, PoseNet [19] introduces a CNN
architecture to directly regress a 6D camera pose from a
single RGB image, a task similar to object pose estima-
tion. However, directly localizing objects in 3D is difﬁcult
due to a lack of depth information and the +large search
space. To overcome this problem, PoseCNN [40] localizes
objects in the 2D image and predicts their depths to obtain
the 3D location. However, directly estimating the 3D rota-
tion is also difﬁcult, since the non-linearity of the rotation
space makes CNNs less generalizable. To avoid this prob-
lem, [38, 33, 23, 35] discretize the rotation space and cast
the 3D rotation estimation into a classiﬁcation task. Such
discretization produces a coarse result and a post-reﬁnement
is essential to get an accurate 6DoF pose.

Keypoint-based methods.
Instead of directly obtaining
the pose from an image, keypoint-based methods adopt a
two-stage pipeline: they ﬁrst predict 2D keypoints of the ob-
ject and then compute the pose through 2D-3D correspon-
dences with a PnP algorithm. 2D keypoint detection is rel-
atively easier than 3D localization and rotation estimation.
For objects of rich textures, traditional methods [24, 32, 1]
detect local keypoints robustly, so the object pose is esti-
mated both efﬁciently and accurately, even under cluttered
scenes and severe occlusions. However, traditional methods
have difﬁculty in handling texture-less objects and process-
ing low-resolution images [20]. To solve this problem, re-
cent works deﬁne a set of semantic keypoints and use CNNs
as keypoint detectors. [30] uses segmentation to identify
image regions that contain objects and regresses keypoints
from the detected image regions. [36] employs the YOLO
architecture [31] to estimate the object keypoints. Their net-
works make predictions based on a low-resolution feature
map. When global distractions occur, such as occlusions,

4562

Figure 2. Overview of the keypoint localization: (a) An image of the Occlusion LINEMOD dataset. (b) The architecture of PVNet. (c)
Pixel-wise vectors pointing to the object keypoints. (d) Semantic labels. (e) Hypotheses of the keypoint locations generated by voting. The
hypotheses with higher voting scores are brighter. (f) Probability distributions of the keypoint locations estimated from hypotheses. The
mean of a distribution is represented by a red star and the covariance matrix is shown by ellipses.

the feature map is interfered [27] and the pose estimation
accuracy drops. Motivated by the success of 2D human
pose estimation [26], another category of methods [29, 27]
outputs pixel-wise heatmaps of keypoints to address the is-
sue of occlusion. However, since heatmaps are ﬁx-sized,
these methods have difﬁculty in handling truncated objects,
whose keypoints may be outside the input image. In con-
trast, our method makes pixel-wise predictions for 2D key-
points using a more ﬂexible representation, i.e., vector ﬁeld.
The keypoint locations are determined by voting from the
directions, which is applicable to truncated objects.

Dense methods.
In these methods, every pixel or patch
produces a prediction for the desired output, and then casts
a vote for the ﬁnal result in a generalized Hough voting
scheme [22, 34, 11]. [2, 25] use a random forest to predict
3D object coordinates for each pixel and produce 2D-3D
correspondence hypotheses using geometric constraints. To
utilize the powerful CNNs, [18, 7] densely sample image
patches and use networks to extract features for the latter
voting. However, these methods require RGB-D data. In
the presence of RGB data alone, [3] uses an auto-context
regression framework [37] to produce pixel-wise distribu-
tions of 3D object coordinates. Compared with sparse key-

points, object coordinates provide dense 2D-3D correspon-
dences for pose estimation, which is more robust to occlu-
sion. But regressing object coordinates is more difﬁcult
than keypoint detection due to the larger output space. Our
approach makes dense predictions for keypoint localization.
It can be regarded as a hybrid of keypoint-based and dense
methods, which combines advantages of both methods.

3. Proposed approach

In this paper, we propose a novel framework for 6DoF
object pose estimation. Given an image, the task of pose es-
timation is to detect objects and estimate their orientations
and translations in the 3D space. Speciﬁcally, 6D pose is
represented by a rigid transformation (R; t) from the object
coordinate system to the camera coordinate system, where
R represents the 3D rotation and t represents the 3D trans-
lation.

Inspired by recent methods [29, 30, 36], we estimate
the object pose using a two-stage pipeline: we ﬁrst de-
tect 2D object keypoints using CNNs and then compute 6D
pose parameters using the PnP algorithm. Our innovation
is in a new representation for 2D object keypoints as well
as a modiﬁed PnP algorithm for pose estimation. Specif-
ically, our method uses PVNet to detect 2D keypoints in

4563

(a) Input image(b) PVNet(c) Unit vectors(d) Semantic labels(e) Hypotheses(f) Keypoints distributionsVotingDistribution Estimation(e) Hypotheses(f) Keypoint distributionsConv-BN-ReLUMax poolResidual blockBilinear upsamplingResidual block with strided convResidual block with dilated convSkip connectionVector-fieldPredictionSemantic Segmentationa RANSAC-like fashion, which robustly handles occluded
and truncated objects. The RANSAC-based voting also
gives a spatial probability distribution of each keypoint,
allowing us to estimate the 6D pose with an uncertainty-
driven PnP.

3.1. Voting based keypoint localization

(a)

(b)

(c)

Figure 2 overviews the proposed pipeline for keypoint
localization. Given an RGB image, PVNet predicts pixel-
wise object labels and vectors that represent the direction
from every pixel to every keypoint. Given the directions
to a certain object keypoint from all pixels belonging to that
object, we generate hypotheses of 2D locations for that key-
point as well as the conﬁdence scores through RANSAC-
based voting. Based on these hypotheses, we estimate the
mean and covariance of the spatial probability distribution
for each keypoint.

In contrast to directly regressing keypoint locations from
an image window [30, 36], the task of predicting pixel-
wise directions enforces the network to focus more on lo-
cal features of objects and alleviates the inﬂuence of clut-
tered background. Another advantage of this approach is
the ability to represent keypoints that are occluded or out-
side the image. Even if a keypoint is invisible, it can be
correctly located according to the directions estimated from
other visible parts of the object.

More speciﬁcally, PVNet performs two tasks: seman-
tic segmentation and vector-ﬁeld prediction. For a pixel p,
PVNet outputs the semantic label that associates it with a
speciﬁc object and the vector vk(p) that represents the di-
rection from the pixel p to a 2D keypoint xk of the object.
A vector vk(p) could be the offset between pixel p and key-
point xk, namely xk − p. Using semantic labels and offsets,
we obtain the target object pixels and add the offsets to gen-
erate a set of keypoint hypotheses.

However, these offsets are sensitive to the scale changes
of object, which limits the generalization ability of PVNet.
Therefore, we further propose a scale-invariant vector

vk(p) =

xk − p

kxk − pk2

,

(1)

which only cares the relative direction between object parts.
We will compare the performances of the two types of vec-
tors in Section 5.3.

Given target object pixels and unit vectors, we generate
keypoint hypotheses in a RANSAC-based voting scheme.
First, we randomly choose two pixels and take the intersec-
tion of their vectors as a hypothesis hk,i for the keypoint xk.
This step is repeated N times to generate a set of hypothe-
ses {hk,i|i = 1, 2, ..., N } that represent possible keypoint
locations. Then, all pixels of the object vote for these hy-
potheses. Speciﬁcally, the voting score wk,i of a hypothesis
hk,i is deﬁned as

Figure 3. (a) A 3D object model and its 3D bounding box. (b)
Hypotheses produced by PVNet for a bounding box corner. (c)
Hypotheses produced by PVNet for a keypoint selected on the ob-
ject surface. The smaller variance of the surface keypoint shows
that it is easier to localize the surface keypoint than the bounding
box corner in our approach.

wk,i = Xp∈O

I(cid:18) (hk,i − p)T

khk,i − pk2

vk(p) ≥ θ(cid:19) ,

(2)

where I represents the indicator function, θ is a threshold
(0.99 in all experiments), and p ∈ O means that the pixel
p belongs to the object O. Intuitively, a higher voting score
means that a hypothesis is more conﬁdent as it coincides
with more predicted directions.

The resulting hypotheses characterize the spatial proba-
bility distribution of a keypoint in the image. Figure 2(e)
shows an example. Finally, the mean µk and the covariance
Σk for a keypoint xk are estimated by:

µk = PN
PN

i=1 wk,ihk,i

i=1 wk,i

,

(3)

Σk = PN

i=1 wk,i(hk,i − µk)(hk,i − µk)T

,

(4)

i=1 wk,i

PN

which are used latter for uncertainty-driven PnP described
in Section 3.2.

Keypoint selection. The keypoints need to be deﬁned
based on the 3D object model. Many recent methods [30,
36, 27] use the eight corners of the 3D bounding box of the
object as the keypoints. An example is shown in Figure 3(a).
These bounding box corners may be far from the object pix-
els in the image. Longer distance to the object pixels results
in a larger localization error, since the keypoint hypotheses
are generated using the vectors that start at the object pixels.
Figure 3(b) and (c) show the hypotheses of a bounding box
corner and a keypoint selected on the object surface, respec-
tively, which are generated by our PVNet. The keypoint on
the object surface usually has a much smaller variance in
localization.

Therefore, the keypoints should be selected on the ob-
ject surface in our approach. Meanwhile, these keypoints
should spread out on the object to make the PnP algorithm
more stable. Considering the two requirements, we select K

4564

cat

duck

iron

driller

Figure 4. Keypoints of four objects in the LINEMOD dataset.

kepoints using the farthest point sampling (FPS) algorithm.
First, we initialize the keypoint set by adding the object cen-
ter. Then, we repeatedly ﬁnd a point on the object surface,
which is farthest to the current keypoint set, and add it to the
set until the size of the set reaches K. The empirical results
in Section 5.3 show that this strategy produces better results
than using the bounding box corners. We also compare the
results using different numbers of keypoints. Considering
both accuracy and efﬁciency, we suggest K = 8 according
to the experiment results. Figure 4 visualizes the selected
keypoints of some objects.

Multiple instances. Our method can handle multiple in-
stances based on the strategy proposed in [40, 28]. For
each object class, we generate the hypotheses of the object
centers and their voting scores using our proposed voting
scheme. Then, we ﬁnd the modes among the hypotheses
and mark these modes as centers of different instances. Fi-
nally, the instance masks are obtained by assigning pixels
to the nearest instance center they vote for.

3.2. Uncertainty driven PnP

Given 2D keypoint locations for each object, its 6D pose
can be computed by solving the PnP problem using an off-
the-shelf PnP solver, e.g., the EPnP [21] used in many pre-
vious methods [36, 30]. However, most of them ignore the
fact that different keypoints may have different conﬁdences
and uncertainty patterns, which should be considered when
solving the PnP problem.

As introduced in Section 3.1, our voting-based method
estimates a spatial probability distribution for each key-
point. Given the estimated mean µk and covariance matrix
Σk for k = 1, · · · , K, we compute the 6D pose (R, t) by
minimizing the Mahalanobis distance:

Sampson errors. In our method, we directly minimize the
reprojection errors.

4. Implementation details

Assuming there are C classes of objects and K keypoints
for each class, PVNet takes as input the H × W × 3 image,
processes it with a fully convolutional architecture, and out-
puts an H × W × (K × 2 × C) tensor representing vectors
and an H × W × (C + 1) tensor representing class probabil-
ities. We use a pretrained ResNet-18 [13] as the backbone
network, and we make three modiﬁcations on it. First, when
the feature map of the network has a size of H/8×W/8, we
do not downsample the feature map anymore by discarding
the subsequent pooling layers. Second, to keep the receptive
ﬁelds unchanged, the subsequent convolutions are replaced
with suitable dilated convolutions [41]. Third, the fully con-
nected layers in the original ResNet-18 are replaced with
convolution layers. Then, we repeatedly perform skip con-
nection, convolution and upsampling on the feature map,
until its size reaches H × W , as shown in Figure 2(b). By
applying a 1 × 1 convolution on the ﬁnal feature map, we
obtain the unit vectors and class probabilities.

4.1. Training strategy

We use the smooth ℓ1 loss proposed in [10] for learning
unit vectors. The corresponding loss function is deﬁned as

ℓ(w) =

K

Xk=1 Xp∈O

ℓ1(∆vk(p; w)|x) + ℓ1(∆vk(p; w)|y),

∆vk(p; w) = ˜vk(p; w) − vk(p),

(6)

where w represents the parameters of PVNet, ˜vk is the pre-
dicted vector, vk is the ground truth unit vector, and ∆vk|x
and ∆vk|y represent the two elements of ∆vk, respec-
tively. Note that during testing, we do not need the pre-
dicted vectors to be unit because the subsequent processing
uses only the directions of the vectors.

To prevent overﬁtting, we add 20,000 synthetic images
to the training set for each object. More details are given in
the supplementary materials.

minimize

R,t

K

Xk=1

(˜xk − µk)T Σ−1

k (˜xk − µk),

5.1. Datasets

(5)

5. Experiments

˜xk = π(RXk + t),

where Xk is the 3D coordinate of the keypoint, ˜xk is the 2D
projection of Xk, and π is the perspective projection func-
tion. The parameters R and t are initialized by EPnP [21]
based on four keypoints, whose covariance matrices have
the smallest traces. Then, we solve (5) using the Levenberg-
In [8], the authors also consider
Marquardt algorithm.
the feature uncertainties by minimizing the approximated

LINEMOD [15]
is a standard benchmark for 6D object
pose estimation. This dataset exhibits many challenges for
pose estimation: cluttered scenes, texture-less objects, and
lighting condition variations.

Occlusion LINEMOD [2] was created by additionally
annotating a subset of the LINEMOD images. Each im-
age contains multiple annotated objects, and these objects
are heavily occluded.

4565

Truncation LINEMOD To fully evaluate our method on
truncated objects, we create this dataset by randomly crop-
ping images in the LINEMOD dataset. After cropping, only
40% to 60% of the area of a target object remains in the im-
age. Some examples are shown in Figure 6.

On LINEMOD, we use exactly the same training-test
split as in [36], while the Occlusion LINEMOD and Trun-
cation LINEMOD are used for testing only.

methods

ape
can
cat
duck
driller
eggbox

glue

holepuncher

average

8

8

Tekin BBox Offset
[36]
2.48
17.48
0.67
1.14
7.66

6.50
65.04
15.00
15.95
55.60
35.23
42.64
35.06
33.88

12.99
69.10
26.12
14.55
65.24
41.62
55.48
32.22
39.66

-

10.08
5.45
6.42

FPS

4

5.31
18.81
16.01
13.85
12.19
36.77
24.81
15.98
17.96

FPS

8

17.44
63.21
17.35
26.12
62.19
44.96
47.32
39.50
39.76

FPS
12
15.1
64.87
16.68
24.89
64.17
41.53
51.94
40.16
39.92

FPS 8
+ Un
15.81
63.30
16.68
25.24
65.65
50.17
49.62
39.67
40.77

YCB-Video [40]
is a recently proposed dataset. The im-
ages are collected from the YCB object set [5]. This dataset
is challenging due to the varying lighting conditions, signif-
icant image noise and occlusions.

5.2. Evaluation metrics

We evaluate our method using two standard metrics: 2D
projection metric [3] and average 3D distance of model
points (ADD) metric [15].

2D Projection metric. This metric computes the mean
distance between the projections of 3D model points given
the estimated pose and the ground-truth pose. A pose is
considered as correct if the distance is less than 5 pixels.

ADD metric. We compute the mean distance between
two transformed model points using the estimated pose and
the ground-truth pose. When the distance is less than 10%
of the model’s diameter, it is claimed that the estimated pose
is correct. For symmetric objects, we use the ADD-S met-
ric [40], where the mean distance is computed based on the
closest point distance. When evaluating on the YCB-Video
dataset, we compute the ADD(-S) AUC proposed in [40].

5.3. Ablation studies

We conduct ablation studies to evaluate the effect of vec-
tor types, different keypoint detection methods, keypoint se-
lection schemes, numbers of keypoints and PnP algorithms,
on the Occlusion LINEMOD dataset. Table 1 summarizes
the results of ablation studies.

Our results are based on the unit vectors, except Col-
umn “Offset 8” that uses the offset vectors. “Offset 8” and
“FPS 8” predict the same set of keypoints and obtain sim-
ilar performances. We attribute this to our sufﬁcient data
augmentation on object scale changes.

To compare PVNet with [36], we re-implement the same
pipeline as [36] but use PVNet to detect the keypoints which
include 8 bounding box corners and the object center. The
result is listed in the column “BBox 8” in Table 1. The
column “Tekin” shows the original result of [36], which di-
rectly regresses coordinates of keypoints via a CNN. Com-
paring the two columns demonstrates that pixel-wise voting
is more robust to occlusion.

Table 1. Ablation studies on different conﬁgurations for pose es-
timation on the Occlusion LINEMOD dataset. These results are
accuracies in terms of the ADD(-S) metric, where glue and eggbox
are considered as symmetric objects. Tekin [36] detects the key-
points by regression, while other conﬁgurations use the proposed
voting-based keypoint localization. BBox 8 shows the result of
our method using the keypoints deﬁned in [36]. Offset 8 shows
the result of predicting offsets to keypoints. FPS K means that we
detect K surface keypoints generated by the FPS algorithm. Un
means that we use the uncertainty-driven PnP. In conﬁgurations
without Un, the pose is estimated using the EPnP [21].

To analyze the keypoint selection schemes discussed in
Section 3.1, we compare the pose estimation results based
on different keypoint sets: “BBox 8” that includes 8 bound-
ing box corners plus the center and “FPS 8” that includes 8
surface points selected by the FPS algorithm plus the center.
Comparing “BBox 8” with “FPS 8” in Table 1 shows that
the proposed FPS scheme results in better pose estimation.
When exploring the inﬂuence of the keypoint number on
pose estimation, we train PVNet to detect 4, 8 and 12 sur-
face keypoints plus the object center, respectively. All the
three sets of keypoints are selected by the FPS algorithm
as described in Section 3.1. Comparing columns “FPS 4”,
“FPS 8” and “FPS 12” shows that the accuracy of pose es-
timation increases with the keypoint number. But the gap
between “FPS 8” and “FPS 12” is negligible. Considering
efﬁciency, we use “FPS 8” in all the other experiments.

To validate the beneﬁt of considering the uncertainties
in solving the PnP problem, we replace the EPnP [21] used
in “FPS 8” with the uncertainty-driven PnP. The results are
shown in the last column “FPS 8 + Un” in Table 1, which
demonstrate that considering uncertainties of keypoint lo-
cations improves the accuracy of pose estimation.

The conﬁguration “FPS 8 + Un” is the ﬁnal conﬁgura-
tion for our approach, which is denoted by “OURS” in the
following experiments.

5.4. Comparison with the state of the art methods

We compare with other state-of-the-art methods which

take RGB images as input and output 6D object poses.

Performance on the LINEMOD dataset.
In Table 2,
we compare our method with [30, 36] on the LINEMOD

4566

w/o reﬁnement

w/ reﬁnement

methods

ape

benchwise

cam
can
cat

driller
duck

eggbox

glue

holepuncher

iron
lamp
phone
average

BB8 Tekin OURS
[30]
95.3
80.0
80.9
84.1
97.0
74.1
81.2
87.9
89.0
90.5
78.9
74.4
77.6
83.9

[36]
92.10
95.06
93.24
97.44
97.41
79.41
94.65
90.33
96.53
92.86
82.94
76.87
86.07
90.37

99.23
99.81
99.21
99.90
99.30
96.92
98.02
99.34
98.45
100.0
99.18
98.27
99.42
99.00

BB8
[30]
96.6
90.1
86.0
91.2
98.8
80.9
92.2
91.0
92.3
95.3
84.8
75.8
85.3
89.3

Table 2. The accuracies of our method and the baseline methods
on the LINEMOD dataset in terms of the 2D projection metric.

ape

methods

w/o reﬁnement

cam
can
cat

w/ reﬁnement
BB8 SSD-6D Tekin OURS BB8 SSD-6D
[30]
27.9
benchwise 62.0
40.1
48.1
45.2
58.6
32.8
40.0
27.0
holepuncher 42.4
67.0
39.9
35.2
43.6

[36]
[30]
21.62 43.62 40.4
81.80 99.90 91.8
36.57 86.86 55.7
68.80 95.47 64.1
41.82 79.34 62.6
63.51 96.43 74.4
27.23 52.58 44.30
69.58 99.15 57.8
80.02 95.66 41.2
42.63 81.92 67.20
74.97 98.88 84.7
71.11 99.33 76.5
47.74 92.41 54.0
55.95 86.27 62.7

[17]
0.00
0.18
0.41
1.35
0.51
2.58
0.00
8.90
0.00
0.30
8.86
8.20
0.18
2.42

[17]
65
80
78
86
70
73
66
100
100
49
78
73
79
79

iron
lamp
phone
average

driller
duck

eggbox

glue

Table 3. The accuracies of our method and the baseline methods
on the LINEMOD dataset in terms of the ADD(-S) metric, where
glue and eggbox are considered as symmetric objects.

[30, 36] de-
dataset in terms of the 2D projection metric.
tect keypoints by regression, while our method uses the pro-
posed voting-based keypoint localization. BB8 [30] trains
another CNN to reﬁne the predicted pose and the reﬁned re-
sults are shown in a separate column. Our method achieves
the best performance on all objects without reﬁnement.

Table 3 shows the comparison of our methods with [30,
17, 36] in terms of the ADD(-S) metric. Note that we com-
pute the ADD-S metric for the eggbox and the glue, which
are symmetric, as suggested in [40]. Comparing to these
methods without using reﬁnement, our method outperforms
them by a large margin of at least 30.32%. SSD-6D [17] sig-
niﬁcantly improves its own performance using edge align-
ment to reﬁne the estimated pose. Nevertheless, our method
still outperforms it by 7.27%.

Robustness to occlusion. We use the model
trained
on the LINEMOD dataset for testing on the Occlusion
LINEMOD dataset. Table 4 and Table 5 summarize the

methods

ape
can
cat
duck
driller
eggbox

glue

holepuncher

average

Tekin
[36]
7.01
11.20
3.62
5.07
1.40

-

4.70
8.26
6.16

PoseCNN Oberweger OURS

[40]
34.6
15.1
10.4
31.8
7.4
1.9
13.8
23.1
17.2

[27]
69.6
82.6
65.1
61.4
73.8
13.1
54.9
66.4
60.9

69.14
86.09
65.12
61.44
73.06
8.43
55.37
69.84
61.06

Table 4. The accuracies of our method and the baseline methods
on the Occlusion LINEMOD dataset in terms of 2D projection.

methods

ape
can
cat
duck
driller
eggbox

glue

holepuncher

average

Tekin
[36]
2.48
17.48
0.67
1.14
7.66

-

10.08
5.45
6.42

PoseCNN Oberweger OURS

[40]
9.6
45.2
0.93
19.6
41.4
22
38.5
22.1
24.9

[27]
17.6
53.9
3.31
19.2
62.4
25.9
39.6
21.3
30.4

15.81
63.30
16.68
25.24
65.65
50.17
49.62
39.67
40.77

Table 5. The accuracies of our method and the baseline methods on
the Occlusion LINEMOD dataset in terms of the ADD(-S) met-
ric, where glue and eggbox are considered as symmetric objects.

objects

2D Projection

ADD(-S)

ape

52.59
12.78

benc-
hvise
58.19
42.80

objects

eggbox

glue

2D Projection

ADD(-S)

87.23
44.13

86.64
38.11

cam

can

cat

driller

duck

54.87
27.73
holep-
uncher
53.84
22.39

57.44
32.94

61.66
25.19

43.27
37.04

54.23
12.36

iron

lamp

phone

avg

46.53
42.01

46.94
40.91

51.35
30.86

58.06
31.48

Table 6. Our results on the Truncation LINEMOD dataset in
terms of the 2D projection and the ADD(-S) metrics.

comparison with [36, 40, 27] on the Occlusion LINEMOD
dataset in terms of the 2D projection metric and the ADD(-
S) metric, respectively.
For both metrics, our method
achieves the best performance among all methods. In partic-
ular, our method outperforms other methods by a margin of
10.37% in terms of the ADD(-S) metric. Some qualitative
results are shown in Figure 5. The improved performance
demonstrates that the proposed vector-ﬁeld representation
enables PVNet to learn the relationship between parts of
the object, so that the occluded keypoints can be robustly
recovered by the visible parts.

Robustness to truncation. We evaluate our method on
the Truncation LINEMOD dataset. Note that, the model
used for testing is only trained on the LINEMOD dataset.
Table 6 shows quantitative results in terms of the 2D projec-
tion and ADD(-S) metrics. We also test the released model
from [36], but it does not obtain reasonable results as it is
not designed for this case.

Figure 6 shows some qualitative results. Even the ob-

4567

Figure 5. Visualization of results on the Occlusion LINEMOD dataset. Green bounding boxes represent the ground-truth poses while blue
bounding boxes represent our predictions.

Figure 6. Visualization of results on the Truncation LINEMOD dataset are shown. The images of the last column are the failure cases,
where the visible parts are too ambiguous to provide enough information for the pose estimation.

methods

2D Projection
ADD(-S) AUC

PoseCNN Oberweger OURS

[40]
3.72
61.0

[27]
39.4
72.8

47.4
73.4

the uncertainty-driven PnP.

6. Conclusion

Table 7. The accuracies of our and baseline methods on the YCB-
Video dataset in terms of 2D projection and ADD(-S) AUC.

jects are partially visible, our method robustly recovers their
poses. We show two failure cases in the last column of Fig-
ure 6, where the visible parts do not provide enough infor-
mation to infer the poses.

Performance on the YCB-Video dataset.
In Table 7,
we compare our method with [40, 27] on the YCB-Video
dataset in terms of the 2D projection and the ADD(-S) AUC
metrics. Our method again achieves the state-of-the-art
performance and surpasses Oberweger [27] which is spe-
cially designed for dealing with occlusion. The results of
PoseCNN were obtained from Oberweger [27].

5.5. Running time

For 480 × 640 input images, our algorithm runs at 25
fps on a desktop with an Intel i7 3.7GHz CPU and a GTX
1080 Ti GPU, which is efﬁcient for real-time pose estima-
tion. Speciﬁcally, our implementation takes 10.9 ms for
data loading, 3.3 ms for network forward propagation, 22.8
ms for the RANSAC-based voting scheme, and 3.1 ms for

We introduced a novel framework for 6DoF object pose
estimation, which consists of the pixel-wise voting net-
work (PVNet) for keypoint localization and the uncertainty-
driven PnP for ﬁnal pose estimation. We showed that pre-
dicting the vector ﬁelds followed by RANSAC-based vot-
ing for keypoint localization gained a superior performance
than direct regression of keypoint coordinates, especially
for occluded or truncated objects. We also showed that con-
sidering the uncertainties of predicted keypoint locations in
solving the PnP problem further improved pose estimation.
We reported the state-of-the-art performances on all three
widely-used benchmark datasets and demonstrated the ro-
bustness of the proposed approach on a new dataset of trun-
cated objects.

Acknowledgements: The authors from Zhejiang Univer-
sity would like to acknowledge support from 973 Program
of China (No. 2015CB352503), NSFC (No. 61806176),
Fundamental Research Funds for the Central Universi-
ties and ZJU-SenseTime Joint Lab of 3D Vision. Qixing
Huang would like to acknowledge support from NSF DMS-
1700234, NSF CIP-1729486, NSF IIS-1618648, a gift from
Snap Research and a GPU donation from Nvidia Inc.

4568

References

[1] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up

robust features. In ECCV, 2006. 2

[2] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton,
and C. Rother. Learning 6d object pose estimation using 3d
object coordinates. In ECCV, 2014. 2, 3, 5

[3] E. Brachmann, F. Michel, A. Krull, M. Ying Yang,
S. Gumhold, et al. Uncertainty-driven 6d pose estimation
of objects and scenes from a single rgb image.
In CVPR,
2016. 3, 6

[4] M. Bui, S. Zakharov, S. Albarqouni, S. Ilic, and N. Navab.
When regression meets manifold learning for object recog-
nition and pose estimation. In ICRA, 2018. 1

[5] B. Calli, A. Singh, A. Walsman, S. Srinivasa, P. Abbeel, and
A. M. Dollar. The ycb object and model set: Towards com-
mon benchmarks for manipulation research. In ICRA, 2015.
6

[6] N. Correll, K. E. Bekris, D. Berenson, O. Brock, A. Causo,
K. Hauser, K. Okada, A. Rodriguez, J. M. Romano, and P. R.
Wurman. Analysis and observations from the ﬁrst amazon
picking challenge.
IEEE Transactions on Automation Sci-
ence and Engineering, 15(1):172–188, 2018. 1

[7] A. Doumanoglou, R. Kouskouridas, S. Malassiotis, and T.-
K. Kim. Recovering 6d object pose and predicting next-best-
view in the crowd. In CVPR, 2016. 3

[8] L. Ferraz, X. Binefa, and F. Moreno-Noguer. Leveraging

feature uncertainty in the pnp problem. In BMVC, 2014. 5

[9] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 2

[10] R. Girshick. Fast r-cnn. In ICCV, 2015. 5
[11] D. Glasner, M. Galun, S. Alpert, R. Basri,

and
G. Shakhnarovich. Viewpoint-aware object detection and
pose estimation. In ICCV, 2011. 3

[12] C. Gu and X. Ren. Discriminative mixture-of-templates for

viewpoint classiﬁcation. In ECCV, 2010. 2

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 5

[14] S. Hinterstoisser, C. Cagniart, S. Ilic, P. Sturm, N. Navab,
P. Fua, and V. Lepetit. Gradient response maps for real-
time detection of textureless objects. T-PAMI, 34(5):876–
888, 2012. 2

[15] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski,
K. Konolige, and N. Navab. Model based training, detec-
tion and pose estimation of texture-less 3d objects in heavily
cluttered scenes. In ACCV, 2012. 1, 2, 5, 6

[16] D. P. Huttenlocher, G. A. Klanderman, and W. J. Rucklidge.
Comparing images using the hausdorff distance. T-PAMI,
15(9):850–863, 1993. 2

[19] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
ICCV, 2015. 2

[20] V. Lepetit, P. Fua, et al. Monocular model-based 3d track-
ing of rigid objects: A survey. Foundations and Trends in
Computer Graphics and Vision, 1(1):1–89, 2005. 1, 2

[21] V. Lepetit, F. Moreno-Noguer, and P. Fua. Epnp: An accurate
o (n) solution to the pnp problem. IJCV, 81(2):155, 2009. 5,
6

[22] J. Liebelt, C. Schmid, and K. Schertler. independent object

class detection using 3d feature maps. In CVPR, 2008. 3

[23] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. Ssd: Single shot multibox detector. In
ECCV, 2016. 2

[24] D. G. Lowe. Object recognition from local scale-invariant

features. In ICCV, 1999. 1, 2

[25] F. Michel, A. Kirillov, E. Brachmann, A. Krull, S. Gumhold,
B. Savchynskyy, and C. Rother. Global hypothesis genera-
tion for 6d object pose estimation. In CVPR, 2017. 3

[26] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-

works for human pose estimation. In ECCV, 2016. 3

[27] M. Oberweger, M. Rad, and V. Lepetit. Making deep
heatmaps robust to partial occlusions for 3d object pose esti-
mation. In ECCV, 2018. 3, 4, 7, 8

[28] G. Papandreou, T. Zhu, L.-C. Chen, S. Gidaris, J. Tompson,
and K. Murphy. Personlab: Person pose estimation and in-
stance segmentation with a bottom-up, part-based, geometric
embedding model. In ECCV, 2018. 5

[29] G. Pavlakos, X. Zhou, A. Chan, K. G. Derpanis, and K. Dani-
ilidis. 6-dof object pose from semantic keypoints. In ICRA,
2017. 1, 3

[30] M. Rad and V. Lepetit. Bb8: A scalable, accurate, robust
to partial occlusion method for predicting the 3d poses of
challenging objects without using depth. In ICCV, 2017. 1,
2, 3, 4, 5, 6, 7

[31] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

In CVPR, 2017. 2

[32] F. Rothganger, S. Lazebnik, C. Schmid, and J. Ponce. 3d
object modeling and recognition using local afﬁne-invariant
image descriptors and multi-view spatial constraints. IJCV,
66(3):231–259, 2006. 2

[33] H. Su, C. R. Qi, Y. Li, and L. J. Guibas. Render for cnn:
Viewpoint estimation in images using cnns trained with ren-
dered 3d model views. In ICCV, 2015. 1, 2

[34] M. Sun, G. Bradski, B.-X. Xu, and S. Savarese. Depth-
encoded hough voting for joint object detection and shape
recovery. In ECCV, 2010. 3

[35] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and
R. Triebel. Implicit 3d orientation learning for 6d object de-
tection from rgb images. In ECCV, 2018. 2

[17] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab. Ssd-
6d: Making rgb-based 3d detection and 6d pose estimation
great again. In ICCV, 2017. 1, 7

[36] B. Tekin, S. N. Sinha, and P. Fua. Real-time seamless single
shot 6d object pose prediction. In CVPR, 2018. 1, 2, 3, 4, 5,
6, 7

[18] W. Kehl, F. Milletari, F. Tombari, S. Ilic, and N. Navab. Deep
learning of local rgb-d patches for 3d object detection and 6d
pose estimation. In ECCV, 2016. 3

[37] Z. Tu and X. Bai. Auto-context and its application to high-
level vision tasks and 3d brain image segmentation. T-PAMI,
32(10):1744–1757, 2010. 3

4569

[38] S. Tulsiani and J. Malik. Viewpoints and keypoints.

In

CVPR, 2015. 2

[39] Y. Xiang, R. Mottaghi, and S. Savarese. Beyond pascal: A
In WACV,

benchmark for 3d object detection in the wild.
2014. 1

[40] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox. Posecnn:
A convolutional neural network for 6d object pose estimation
in cluttered scenes. In Robotics: Science and Systems (RSS),
2018. 1, 2, 5, 6, 7, 8

[41] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 5

[42] M. Zhu, K. G. Derpanis, Y. Yang, S. Brahmbhatt, M. Zhang,
C. Phillips, M. Lecce, and K. Daniilidis. Single image 3d
object detection and pose estimation for grasping. In ICRA,
2014. 2

4570

