Residual Networks for Light Field Image Super-Resolution

Shuo Zhang1,2 , Youfang Lin1,2, Hao Sheng3,4

1Beijing Key Lab of Trafﬁc Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University

3State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University

2Key Laboratory of Intelligent Passenger Service of Civil Aviation, CAAC

4Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University

{zhangshuo, yflin}@bjtu.edu.cn, shenghao@buaa.edu.cn

Abstract

Light ﬁeld cameras are considered to have many po-
tential applications since angular and spatial information
is captured simultaneously. However, the limited spatial
resolution has brought lots of difﬁculties in developing re-
lated applications and becomes the main bottleneck of light
ﬁeld cameras. In this paper, a learning-based method us-
ing residual convolutional networks is proposed to recon-
struct light ﬁelds with higher spatial resolution. The view
images in one light ﬁeld are ﬁrst grouped into different im-
age stacks with consistent sub-pixel offsets and fed into dif-
ferent network branches to implicitly learn inherent corre-
sponding relations. The residual information in different
spatial directions is then calculated from each branch and
further integrated to supplement high-frequency details for
the view image. Finally, a ﬂexible solution is proposed to
super-resolve entire light ﬁeld images with various angu-
lar resolutions. Experimental results on synthetic and real-
world datasets demonstrate that the proposed method out-
performs other state-of-the-art methods by a large margin
in both visual and numerical evaluations. Furthermore, the
proposed method shows good performances in preserving
the inherent epipolar property in light ﬁeld images.

1. Introduction

With recent advances in camera devices, light ﬁeld (LF)
imaging technology is commonly used in the market for 3D
reconstruction and virtual reality applications [12, 28, 14].
For large-scale applications, the camera array is often used
to capture high-resolution LF images with a large base-
line. By inserting the micro-lens array between the main
lens and the imaging plane [1], the handheld plenoptic cam-
era [12, 14] is developed and is able to capture LF images
with a small baseline by one shot, which has more broad
applications such as image refocusing [13]. However, the

View-GT: PSNR/SSIM MMSR[29]: 31.41/0.9370 GBSQ[16]: 31.51/0.9228

LFNet[21]: 29.15/0.9373

EDSR[10]: 31.03/0.9263

Ours: 34.16/0.9728

EPI-GT: PSNR/SSIM

Bicubic: 27.98/0.8516

GBSQ[16]: 31.41/0.9047

Ours: 34.04/0.9661

LFNet[21]: 28.84/0.8727 EDSR[10]: 31.04/0.9101
Figure 1. The spatially super-resolution results of
image
Horses [25] with ×2 magniﬁcation factor. Our results of the cen-
tral view images and epipolar plane images (EPIs) outperform the
other state-of-the-art methods with signiﬁcant higher PSNR and
SSIM. The background letters along the occlusion boundary are
clearly recovered with sharp edges both in view and epipolar plane
images using the proposed method, while the others exhibit strong
artifacts or ambiguous textures.

development of plenoptic cameras is severely limited due
to their lower spatial resolutions compared with traditional
cameras, which has brought a lot of difﬁculties in many
practical vision applications.

Since light ﬁelds record scenes with multiple view im-
ages, disparity information in these view images provides
multiple sampling with sub-pixel offsets to enhance the spa-
tial resolution. Traditional methods register the sub-pixel
information by explicitly warping other view images based
on prior disparity information [16, 23, 29]. However, exist-
ing disparity estimation methods for LF images suffer from
occlusions, noises and textureless regions [7], which lead to
signiﬁcant artifacts in reconstructed LF images. Recently,
deep-learning-based methods have been proposed for light
ﬁeld super-resolution (LFSR), in which disparity informa-
tion is implicitly learned during training processes [21, 27].
However, these methods are quite limited in exploring ac-

11046

curate sub-pixel information and preserving the inherent
epipolar property in LF images.

Taking advantage of the residual structure in super-
resolution networks [8, 10, 30], we design a novel resid-
ual network (resLF) to enhance the spatial resolution of LF
images. In the proposed method, the view images in one
LF are ﬁrst separated into four groups according to their
angular directions and fed into different network branches
to learn high-frequency details in the speciﬁc spatial direc-
tions. Different from other LFSR methods, inherent cor-
responding relations in view images, which reﬂect dispar-
ity information, are implicitly explored and sub-pixel map-
pings from various directions are learned in the proposed
method. Residual information from different spatial direc-
tions is then combined to generate complete residual details
for ﬁnal super-resolved central view images. The LF is di-
vided into different parts and the entire view images are ﬁ-
nally super-resolved based on a ﬂexible solution.

The experiments are conducted on different LF images
and various challenging scenes, which include noises, oc-
clusions and non-Lambertian surfaces. The resLF networks
can be used for both synthetical and real-world LF im-
ages with different angular resolutions. Experimental re-
sults show that the proposed framework signiﬁcantly out-
performs the other state-of-the-art methods in terms of nu-
merical and visual evaluations, where the PSNR results
are improved by 1.5 dB on average in ×2 and ×4 super-
resolution tasks. Moreover, the comparison of epipolar
plane images (EPIs) shows that the proposed method is able
to preserve the corresponding relations in super-resolved
view images.

2. Related Work

As LFs capture scenes with multiple view images from
different angles, texture information lost in spatial domain
actually remains in angular domain according to scene
structures. Most LFSR methods can be divided into two cat-
egories, disparity-based and learning-based methods, based
on learning structure information directly or implicitly.

Given estimated structure information as priors, many
researches focus on how to warp multiple view images ac-
curately and ﬁnd sub-pixel information to improve the spa-
tial resolution. Based on EPIs, Wanner et al. [23, 24] ex-
tracted depth information using structure tensor and inter-
polated lines in EPIs accordingly to super-resolve view im-
ages. By modeling LF patches based on a Gaussian Mix-
ture Model, Mitra et al. [11] proposed to reconstruct patches
with higher resolutions based on estimated disparities. The
other methods [2, 3] focused on recovering view images
by explicitly warping pixels from other view images. Re-
cently, Zhang et al. [29] proposed to estimate matching re-
lationships between micro-lens images and view images,
and used micro-lens images with richer textures to recover

Figure 2. Sub-pixel information from surrounding view images in
LF. Different images contain sub-pixel shifts in different directions
according to the disparity information, e.g., the horizontal sub-
pixel position (labeled as yellow) can be found in the horizontal
adjacent images. In our network, we propose to learn the mapping
from sub-pixel shifts in surrounding image to a high-resolution
central view image.

related view images. Based on a graph-based regularizer,
Rossi et al. [16] designed a global optimization problem
to augment the resolution of all LF view images together.
The disparity of each view is roughly estimated to calcu-
late the warping matrix and the geometric structure between
each view is considered to optimize the super-resolution re-
sults. However, although lots of disparity estimation meth-
ods have been proposed [7], reconstructed view images are
still easily affected by estimation errors, which cause sig-
niﬁcant artifacts along occlusion boundaries.

Recently, deep Convolutional Neural Networks (CNN)
have been developed for single image super-resolution
(SISR)
[4] and achieved remarkable performances by in-
troducing residual learning, recursive layers or deeper CNN
models [8, 20, 30]. For LF images, several frameworks are
designed to implicitly learn geometric structures and aug-
ment the spatial resolution. Cho et al. [3] proposed to train
a dictionary for high and low quality LF image pairs and
improved the image quality based on sparse coding. Yoon
et al. [26, 27] developed spatial and angular networks to up-
sample angular and spatial resolution simultaneously. The
different view images are augmented separately and then
combined into different types of image pairs to create novel
views. Wang et al. [21] built a bidirectional recurrent CNN
to super-resolve horizontal and vertical image stacks sepa-
rately and then combined them using stacked generalization
technique to obtain complete view images. In their method,
the spatial relations are iteratively investigated between two
adjacent view images. However, as most learning-based
frameworks chose to super-resolve each view image pair or
stack separately, corresponding relations in different views
are not fully considered so that accurate high-frequency de-
tails cannot be well recovered.

Since view images in LF capture scenes from various di-
rections, high-frequency details in spatial domain are actu-

11047

sub-pixel informationpixel informationsub-pixel informationpixel informationFigure 3. The overall structure of the proposed resLF network. The different image stacks I lr
θ are fed into different network branches
including feature extraction layer (HF Eθ ) and S residual blocks (HRBθ ,s). The output of each branch is then concatenated for D global
residual blocks (HGRBθ ,d). The central image I lr
c after feature extraction (HF Ec ) is then added to the global residual output. Finally, the
up-sampling network (HU P ) is introduced to obtain ﬁnal super-resolution results.

ally kept in view images from different angular directions,
as shown in Fig 2. This special architecture provides the
possibilities of ﬁnding sub-pixel information in spatial do-
main from angular domain. Different from the above meth-
ods, we propose a speciﬁcally designed super-resolution
framework to ﬁnd accurate sub-pixel information from dif-
ferent angular directions and preserve the epipolar property
at the same time.

3. Methodology

The objective of the proposed resLF network is to re-
construct a super-resolution (SR) LF image Lsr(x, y, u, v)
from a low-resolution (LR) image Llr(x, y, u, v), where
(x, y) is in the spatial domain and (u, v) is in the angular do-
main [9]. Assuming that the resolution of Llr is described
with (X, Y, U, V ), Lsr with higher spatial resolution can
be denoted with (rX, rY, U, V ), where r represents the up-
sampling factor in spatial resolution and U = V in most LF
images. We convert images to YCbCr color space and only
deal with Y channel images.

3.1. Framework Overview

The view images in one LF, i.e. sub-aperture images,
capture scenes in different directions, which can be ex-
tracted by ﬁxing (u, v) and changing (x, y) coordinates.
Different from traditional multi-view images, viewpoints in
LFs have various angles. As shown in Fig.2, the surround-
ing view image in one angular direction contains sub-pixel
offsets in the speciﬁc spatial direction. The shifted pixels in

different directions can be combined according to disparity
information to yield the high-resolution view image. There-
fore, we propose to explore the detail information from sur-
rounding view images which have horizontal, vertical or di-
agonal sub-pixel shifts.

If angular direction tan θ = v/u, we extract image
stacks Iθ=0, Iθ=90, Iθ=45, Iθ=135 around one view image,
whose viewpoints change along horizontal, vertical, left and
right diagonal directions. Inspired by the great performance
of the recently residual learning in SISR [30], we speciﬁ-
cally design a residual network structure for LF images. As
shown in Fig. 3, the resLF network contains four branches
and a global part with several residual blocks. Compared
with other view images, the central view image in one LF
has more available sub-pixel information from the related
image stacks. Therefore, we ﬁrst design the network to im-
prove the spatial resolution of the central view image (see
Sec. 3.2), where the number of available view images in
each image stack is the same. The network is then trained
using LF images with different angular resolutions and dif-
ferent image stack inputs. Finally, the entire LF images are
recovered according to a ﬂexible solution (see Sec. 3.3).

3.2. Network Design

Suppose that the training data {Llr, I hr

c } is given. I hr
c
is the ground truth, which represents the central view image
with high resolution. The four image stacks from different
directions {I lr
135} around the central view im-
age I lr
c can be calculated. The objective of our network is to
learn a model HU ×V that can predict a high-resolution cen-

90, I lr

45, I lr

0 , I lr

11048

...............ResBlockResBlockResBlockResBlockResBlockResBlockResBlockResBlockResBlockResBlockUpSampleConvConvConvConvConvConvConvReluConvResBlockReluShuﬄeConvConv(a) H7×7

(b) H5×5

(c) H3×3

(d) H3×3(2×3,3×2,2×2)

(e) Final LF images

(f) PSNR values

Figure 4. The ﬂexible solution for super-resolving complete LF images. As shown in (a–d), the LF is divided into different parts and each
central view image in the LF part is super-resolved using corresponding networks. (e) shows all the views in one LF where each color
represents the corresponding network. In (f), the color of each grid represents the PSNR of the reconstructed view image in Mona.

45, I lr
tral view image I sr
given input with U × V angular resolution.

c = HU ×V (I lr

0 , I lr

90, I lr

135, I lr

c ) from

Considering the narrow baseline in adjacent view im-
ages, the image stack from one direction is directly con-
catenated as the input of each branch. Similar with [18], the
network is constructed with four branches and each image
stack is encoded individually to learn the residual part in the
speciﬁc direction. The disparity information is calculated
implicitly using convolutional networks to ﬁnd out accurate
sub-pixel shifts between each view image. In each branch,
the ﬁrst convolutional layer HF Eθ (·) extracts features from
each image stack:

FF Eθ = HF Eθ (I lr

θ ),

(1)

where I lr
θ denotes the image stack in each direction θ and
FF Eθ is given as the input of the following residual blocks.
We also deﬁne a similar convolution operation HF Ec (·) for
the central view image I lr
c to extract corresponding features
for residual learning:

FF Ec = HF Ec (I lr

c ).

(2)

Suppose that we have S residual blocks (RB) in each
branch, the output FRBθ ,s of the s-th RB can be calculated
as:

FRBθ ,s = HRBθ ,s(FRBθ ,s−1),

(3)

where HRBθ ,s(·) denotes the s-th RB. The structure of RB
is deﬁned similar with [10], which contains a convolution
layer, rectiﬁed linear units (ReLU) and a convolution layer
in order. In each block, the input is directly added to the
output as a residual part.

After extracting features from RBs with different direc-
tions, we integrate all the features in a global way and fur-
ther feed them into more residual learning blocks:

FGRB,d = HGRB,d(FGRB,d−1)

= HGRB,d(· · · HGRB,1(FGRB,0) · · · ),

where FGRB,0 = [FRB0,S, FRB90,S, FRB45,S, FRB135,S]
refers to the concatenation of features from S residual
blocks in each branch. Since the image stack is grouped

according to different angular directions, the output of each
group is corresponding to sub-pixels in the speciﬁc spatial
direction. We deﬁne D global residual block HGRB,d(·)
to further exploit the sub-pixel residual information from
different directions. HGRB,d(·) has the similar structure
with residual block HRB,s(·) in each branch but has 4 times
larger number of ﬁlters to extract more features.

The global features are then fed into a convolution layer
HGF (·) and combined with the features FF Ec from the cen-
tral view image. After extracting local and global features
for the central image in the low-resolution space, we intro-
duce an up-sampling net HU P (·) to obtain the ﬁnal image
in the high-resolution space. Inspired by the work in [17],
one convolution layer and one shufﬂe layer followed by one
convolutional layer is used to construct the up-sampling net.
The ﬁnal super-resolved view image can be denoted as:

I sr
c = HU P (HGF (FGRB,D) + FF Ec ),

(4)

where FGRB,D refers to the output of D global residual
blocks. The super-resolved view image combines the resid-
ual information learning from surrounding view images
with different directions.

3.3. Light Field Super Resolution

As different view images of one LF capture scenes in
their speciﬁc directions and have their speciﬁc features, it
is difﬁcult to super-resolve all view images in one network
simultaneously. State-of-the-art learning-based LFSR al-
gorithms built complex networks to reconstruct complete
LF images by either applying the super-resolution process
for each image individually [27] or calculating view images
based on already super-resolved view images [21]. In this
way, the view images in one LF are super-resolved with un-
balanced information and related results show big differ-
ences. At the same time, corresponding relations in view
images are also hard to preserve. Moreover, for LF images
with different angular resolutions, these networks should be
trained from the beginning, whose performances decrease
sharply for LFs with small angular resolution. Different
from these complicated networks, we choose to deal with
each view image individually by combining surrounding

11049

43.5642.88H3×3(3×2)H3×3(2×3)H3×3(2×2)H3×3H5×5H7×7Table 1. Quantitative comparisons using different network structures for ×2 super-resolution results.

Network

Incomplete Images Stacks

Different Angular Resolutions

Different Image Stacks

H3×3(3×2) H3×3(2×3) H3×3(2×2) H3×3

H5×5

H7×7

H9×9 H1×9,h H9×1,v H9×9,c

Avg. PSNR
Avg. SSIM

37.19

0.9756

37.09

0.9748

37.02

37.24

37.60

37.65

0.9742

0.9756

0.9777

0.9782

37.77
0.9788

36.27

36.32

37.35

0.9687

0.9701

0.9768

view images to keep geometric structures, which also pro-
vides a more ﬂexible solution to obtain entire LF images
with various angular resolutions.

In order to super-resolve complete LF images, we train
different resLF networks with various angular resolutions.
The LF image is divided into different parts, where the other
view images are treated as the central view image in the
corresponding LF part. As in Fig. 4, we show the super-
resolution process for the LF image with 7 × 7 angular res-
olution. For the central view image, each stack has 7 im-
ages and network H7×7, is used to generate the result. The
adjacent view images are recovered using H5×5 and H3×3
network, respectively. As for border view images, we lack
the image information from one or more directions in the
image stacks. One solution is to change the network struc-
ture with different branches for border views using avail-
able surrounding views.
In the proposed framework, we
choose another solution to pad lacking views with 0 in each
image stack and use the same network structure. Specif-
ically, we train network H3×3(3×2) for left and right bor-
der images, H3×3(2×3) for up and down border images and
H3×3(2×2) for four corner images, separately. The resLF
networks with various angular resolutions are then used for
super-resolving all view images in LFs.

We evaluate the different resLF networks on a part of our
test dataset and the results are shown in Table 1. The net-
work with more view images in each image stack achieves
better performances in general. For LF images with 3 × 3
angular resolution, even for border images with incomplete
image stacks, the proposed network also produces compara-
ble super-resolution results after training accordingly. The
numerical results for the example image Mona are shown in
Fig. 4. For this image, the center view image achieves the
highest accuracy and the border view images also obtain
similar performances.

3.4. Implementation Detail

Due to the small baseline of LF images, we set 3 × 3 as
the kernel size and pad zero in all convolutional layers. The
convolutional layers in the individual branch have 32 ﬁlters
and the layers in global residual blocks after concatenation
have 32 × 4 ﬁlters. The global feature extraction layer has
32 ﬁlters to combine the residual details with the original
image. The ﬁnal convolutional layer has 1 channel to out-
put the desired SR image. We set S = 4 in each branch and

Ground Truth

(a) 26.66, 30.72

(b) 29.18, 34.90

(c) 30.23, 34.67

(d) 32.26, 36.56

(e) 32.81, 37.87

Figure 5. Super-resolution results using different network struc-
tures, where the PSNR value of each image is shown accord-
ingly. (a) Bicubic Interpolation (b)H9×1,v (c)H1×9,h(d) H9×9,c
(e) H9×9. The network combined with image stacks from more
directions obtains sharper edges and textures than the others.

D = 4 in the global part after several experimental tests.
The L1 loss function is used as it provides better perfor-
mances than L2 loss function in the proposed network.

In each training batch, we randomly extract 64 LF
patches with the spatial size of 35 × 35 as inputs. We ran-
domly augment the dataset by ﬂipping the images horizon-
tally or vertically, or rotating 180 degree. As pixel shifts
are related to disparity information, the image order in each
stack should be changed accordingly to preserve the epipo-
lar property in LF images. We train our model with Adam
optimizer and the weights in each layer are initialized us-
ing Xaviers algorithm [5]. The learning rate is initialized
to 10−3 for all weights and decreases by a factor of 0.1 for
every 100 epoch. We implement the resLF network with
Torch7 framework and the training process roughly takes
1 day with a Titan GPU. Using the proposed model, each
view image can be spatially ×2 super-resolved within 7 ms.

4. Experiment

The synthetic images from HCI1 [25] and HCI2 [6],
and real-world images [19, 15, 21] from Lytro Illum cam-
eras [12] are used in the experiment. The training, val-
idation and test datasets are chosen from the above im-

11050

Table 2. Quantitative evaluations (PSNR / SSIM) of ×2 super-resolution results on synthetic light ﬁeld image Buddha and Mona .

Methods

Mitra [11]
Wanner [23]

Yoon [27]
MMSR [29]
GBSQ [16]
LFNet [21]
EDSR [10]

Min

28.83 / 0.8665
24.43 / 0.7662
36.25 / 0.9579

- / -

39.29 / 0.9678
38.09 / 0.9709
39.72 / 0.9680

Buddha

Avg

Max

Min

Mona

Avg

29.91 / 0.8994
29.69 / 0.8691
36.95 / 0.9623
39.83 / 0.9745
40.00 / 0.9730
38.42 / 0.9731
39.93 / 0.9703

31.17 / 0.9343
36.97 / 0.9470
37.35 / 0.9657

28.54 / 0.8541
25.40 / 0.8542
37.03 / 0.9833

- / -

- / -

40.37 / 0.9754
38.77 / 0.9760
40.43 / 0.9726

39.88 / 0.9795
38.38 / 0.9884
42.21 / 0.9800

29.28 / 0.8911
30.76 / 0.9324
37.99 / 0.9863
34.44 / 0.9702
40.41 / 0.9812
38.73 / 0.9891
42.35 / 0.9803

Max

30.07 / 0.9319
37.60 / 0.9862
38.53 / 0.9878

- / -

40.73 / 0.9821
38.80 / 0.9895
42.46 / 0.9806

Proposed

41.09 / 0.9881

41.62 / 0.9897

42.24 / 0.9910

42.88 / 0.9929

43.13 / 0.9934

43.56 / 0.9941

ages, which include 250, 50 and 50 LF images, respec-
tively. All the dataset are preprocessed with 9 × 9 angu-
lar resolution and are cropped with smaller angular resolu-
tions to train different networks. The images are spatially
×2 and ×4 downsampled using bicubic interpolation and
super-resolved using the proposed method. We train the
synthetic and real-world images all together to obtain ro-
bust super-resolution results. The super-resolution results
are evaluated with PSNR and SSIM [22]. The results are
compared with state-of-the-art LFSR algorithms, including
disparity-based method [23, 11, 29, 16] and learning-based
methods [27, 21]. The state-of-the-art SISR method [10] is
also used for further comparisons.

4.1. Ablation Investigation

In this subsection, we investigate the performances of
networks with different branches. We design network
H1×9,h and H9×1,v using only horizontal or vertical image
stacks as input, and H9×9,c using both horizontal and verti-
cal image stacks. The number of parameters and the archi-
tectures of the networks are kept the same with the original
network for a fair comparison.

The quantitive comparisons on the part of our test dataset
are illustrated in Table 1. The network with image stacks
from more angular directions achieves better performances
than single angular direction. Moreover, compared with
H9×9,c, H5×5 network with input images from more direc-
tions obtains better results, where the number of input view
images in each network is almost the same. We also show
some examples in Fig. 5, which contain complex textures
along with a lot of occlusions. It is obvious that the results
from H9×9 network achieve better visual effects than the
others.

4.2. EPIs Comparison

As we deal with each view using different networks, it
is important to verify whether the method is able to keep
the inherent geometric structure. We integrate the super-
resolved views into EPIs and compare the epipolar property
in Fig. 1, Fig. 6 and Table 3, where the SSIM values of

EPIs are provided. As shown, EPIs from GBSQ [16] have
blurry results. Although the image stacks are simultane-
ously super-resolved in LFNet [21], the oblique lines are
still distorted since the epipolar constraint in each image
stack is not fully considered in the super-resolution process.
As EDSR [10] only focuses on one single view image, high-
frequency textures are super-resolved individually in each
view image so that the corresponding cues in EPIs are dis-
ordered. By contrast, our method uses surrounding view
images to take the epipolar constraint into consideration for
each view image so that the geometric structure is well pre-
served in ﬁnal reconstructed LF images.

4.3. Sythetic Images

We compare the proposed method with state-of-the-art
methods on synthetic LF images (HCI1 [25] and HCI2 [6])
in this subsection.
The LF images are cropped with
5 × 5 angular resolution and super-resolved with ×2
spatial resolution. As explained in Sec. 3.3,
the net-
works H5×5, H3×3, H3×3(3×2), H3×3(2×3) and H3×3(2×2)
are used to super-resolve different view images in one LF.
The detailed comparisons for image Buddha and Mona are
listed in Table 2, where the minimum, average and maxi-
mum PSNR and SSIM is shown. Results from disparity-
based methods [23, 11, 29] vary greatly in different view
images due to the uncertain disparity information. As most
of the textures in Mona are regular, the EDSR [10] achieves
the second best scores compared with the other LF speciﬁc
methods. Our method outperforms the other methods with
more than 1.62 dB (PSNR) in Buddha and 0.78 dB (PSNR)
in Mona. The differences in our super-resolved view images
are small for one LF and the minimum PSNR and SSIM val-
ues of our results are still higher than the others.

We provide the average results on the synthetic datasets
in Table 3. The proposed resLF exceeds second best re-
sults by 1.62 dB in PSNR and 0.02 in SSIM on average.
The qualitative comparisons are shown in Fig. 1 and Fig. 6.
MMSR [29] obtains clear textures in ﬂat regions but fails
in occlusion edges due to the wrong estimated disparity
information. EDSR [10] cannot predict complex textures

11051

Ground Truth

MMSR [29] / Bicubic

GBSQ [16]

LFNet [21]

EDSR [10]

Proposed

PSNR / SSIM

39.83 / 0.9745

40.37/ 0.9754

38.77 / 0.9760

40.43 / 0.9726

42.24 / 0.9910

EPI-PSNR / EPI-SSIM

37.53 / 0.9530

40.26 / 0.9728

37.82 / 0.9576

40.08 / 0.9697

41.64 / 0.9899

PSNR / SSIM

31.43 / 0.9285

33.22 / 0.9392

32.38 / 0.9403

31.80 / 0.9378

34.35 / 0.9764

EPI-PSNR / EPI-SSIM

29.95 / 0.8886

32.30 / 0.9203

31.00 / 0.9075

32.21 / 0.9254

34.50 / 0.9727

PSNR / SSIM

33.13 / 0.9447

35.00 / 0.9544

32.05 / 0.9293

36.58 / 0.9668

38.99 / 0.9865

31.81 / 0.9170

The detailed ×2 super-resolution results for synthetic image Buddha [25] and real-world image occlusion 4

EPI-PSNR / EPI-SSIM
Figure 6.
[19],
ISO Chart 12 [15]. The super-resolved central view images and EPIs are shown, where the corresponding PSNR and SSIM values are
illustrated below. As MMSR [29] is only effective for central view in LF, we show the EPIs using Bicubic methods. Our method is able to
recover more accurate details in view images and preserve the epipolar features in EPIs than the others.

37.75 / 0.9820

36.44 / 0.9597

34.47 / 0.9412

32.51 / 0.9200

Table 3. Avg. PSNR/SSIM of the ×2 super-resolved view images in each LF dataset and Avg. SSIM of the related EPIs in all datasets.

Methods

HCI1 [25]

HCI2 [6]

Lytro [15]

Bikes [19]

Occlusions [19]

Reﬂective [19]

Overall EPIs

Bicubic

MMSR [29]
GBSQ [16]
LFNet [21]
EDSR [10]
Proposed

35.23 / 0.9303
35.44 / 0.9621
38.04 / 0.9635
36.46 / 0.9645
39.24 / 0.9657
41.09 / 0.9882

31.67 / 0.8816
31.46 / 0.9189
34.61 / 0.9423
33.63 / 0.9317
35.07 / 0.9489
36.45 / 0.9786

31.23 / 0.8856
29.83 / 0.9284
32.46 / 0.9295
32.70 / 0.9348
33.94 / 0.9473
35.48 / 0.9727

29.76 / 0.9014
29.83 / 0.9284
31.69 / 0.9445
31.92 / 0.9499
33.86 / 0.9638
35.21 / 0.9806

33.60 / 0.9273
33.38 / 0.9440
36.23 / 0.9596
35.92 / 0.9630
37.61 / 0.9692
39.71 / 0.9876

36.94 / 0.9495
36.13 / 0.9571
38.29 / 0.9649
38.80 / 0.9706
40.64 / 0.9758
42.32 / 0.9904

0.9210

-

0.9411

0.9367

0.9560

0.9778

based on one single view image and produces blurry de-
tails. LFNet [21] is trained with horizontal and vertical im-
age stacks and results are combined in the stacked gener-
alization. As they analyzed in [21], their ﬁnal results after
the combination only achieve tiny improvements in PSNR,
which means sub-pixel information from different direc-
tions is not well integrated. Therefore, the related results
are recovered with artifacts. In our method, the proposed
network combines surrounding view images from different
directions in a global way so that it is able to deal with com-
plex textures. The results show signiﬁcantly better image
qualities where the textures are recovered accurately in both
ﬂat and occlusion regions.

We also train a set of 4× networks for a harder super-
resolution task. The quantitative results are illustrated in
Table 4 and the qualitative results are shown in Fig. 7. Our

method also outperforms the others with more than 1 dB in
most of the evaluated images. The results from EDSR [10]
are over-smoothed with ambiguous details and LFNet [21]
produces blurry and noisy results. By contrast, our results
preserve the boundaries and textures well and show superior
performances in visual effects.

4.4. Real World Images

As the proposed networks are designed and trained for
all kinds of LF images, the models can be directly used
for Lytro images. The ×2 results are shown in Table 3
and Fig. 6, where different categories in [19] are evaluated
separately. The ×4 results are compared in Fig. 7 and Ta-
ble 4. As these LF images are captured using plenoptic cam-
eras, original view images contain noticeable artifacts and
noises, which make it more difﬁcult to estimate disparity in-

11052

Proposed

Ground Truth

Bicubic

LFNet [21]

EDSR [10]

Proposed

Bicycle ×4

PSNR / SSIM

25.08 / 0.6567

26.06 / 0.7492

27.36 / 0.7953

28.02 / 0.8918

Flower ×4

PSNR / SSIM

29.38 / 0.8334

31.39 / 0.8720

32.56 / 0.9158

34.08 / 0.9675

Stone ×4

PSNR / SSIM

27.61 / 0.6713

28.54 / 0.7193

28.92 / 0.7940

30.26 / 0.9016

Figure 7. Image comparisons of ×4 super-resolution results, where the super-resolved central view image is shown. The average PSNR
and SSIM values are illustrated below. Our results show fewer artifacts and superior image quality compared with other methods.

Table 4. Quantitative evaluations (Avg. PSNR / Avg. SSIM) of ×4 super-resolution results on different light ﬁeld datasets.

Methods

Budda [25]

Sideboard [6]

ISO Chart 12 [15]

Reeds [15]

Stone [21]

Flower [21]

Bicubic

LFNet [21]
EDSR [10]
Proposed

32.30 / 0.8470
33.51 / 0.8827
34.98 / 0.9059
36.06 / 0.9623

23.41 / 0.5886
24.67 / 0.7273
26.10 / 0.7968
27.35 / 0.8840

26.60 / 0.7755
27.59 / 0.8776
30.96 / 0.9148
32.57 / 0.9551

37.09 / 0.8853
37.71 / 0.9624
38.02 / 0.9071
39.25 / 0.9731

27.61 / 0.6713
28.54 / 0.7193
28.92 / 0.7940
30.26 / 0.9016

29.38 / 0.8334
31.39 / 0.8720
32.56 / 0.9158
34.07 / 0.9675

formation and reconstruct LF images, especially for the 4×
task. The LFNet in [21], which trains real-world images
especially, produces obvious artifacts in the super-resolved
images. The results from EDSR [10] are ambiguous and
over-smoothed since the information from other view im-
ages is not considered. By contrast, our results achieve sig-
niﬁcantly higher PSNR and SSIM in different kinds of real-
world images for both tasks, which shows that the proposed
network can not only handle the noisy input but also recover
more high-frequency details.

5. Conclusions

ages is explored in different network branches from differ-
ent angular directions and used to infer sub-pixel informa-
tion in high-resolution view images through the network.
The entire light ﬁeld images with different angular resolu-
tions can be super-resolved based on different trained mod-
els. Experimental results show that our method outperforms
the state-of-the-art methods by a large margin in PSNR and
SSIM and exhibits signiﬁcantly better visual effects. The
proposed network can preserve the epipolar property of the
images well and can be used for different kinds of light ﬁeld
images with different angular resolutions.

In this paper, a residual convolutional network has been
proposed to augment the spatial resolution of light ﬁeld im-
ages. The inherent structure information in light ﬁeld im-

Acknowledgment: This work is supported by the Fun-
damental Research Funds for the Central Universities
2019RC013.

11053

References

[1] Edward H Adelson and John YA Wang. Single lens stereo
with a plenoptic camera. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 14(2):99–106, 1992.

[2] Tom E Bishop, Sara Zanetti, and Paolo Favaro. Light ﬁeld
superresolution.
In Proceedings of the IEEE International
Conference on Computational Photography (ICCP), pages
1–9, 2009.

[3] Donghyeon Cho, Minhaeng Lee, Sunyeong Kim, and Yu-
Wing Tai. Modeling the calibration pipeline of the lytro cam-
era for high quality light-ﬁeld image reconstruction. In Pro-
ceedings of the IEEE International Conference on Computer
Vision (ICCV), pages 3280–3287, 2013.

[4] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(2):295–307, 2016.

[5] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
culty of training deep feedforward neural networks. In Pro-
ceedings of the International Conference on Artiﬁcial Intel-
ligence and Statistics, pages 249–256, 2010.

[6] Katrin Honauer, Ole Johannsen, Daniel Kondermann, and
Bastian Goldluecke. A dataset and evaluation methodology
for depth estimation on 4d light ﬁelds. In Proceedings of the
Asian Conference on Computer Vision (ACCV), pages 19–
34, 2016.

[7] Ole Johannsen, Katrin Honauer, Bastian Goldluecke, Anna
Alperovich, Federica Battisti, Yunsu Bok, Michele Brizzi,
Marco Carli, Gyeongmin Choe, Maximilian Diebold, et al.
A taxonomy and evaluation of dense light ﬁeld depth es-
timation algorithms.
In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition Work-
shops (CVPRW), pages 1795–1812, 2017.

[8] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1646–1654,
2016.

[9] Marc Levoy and Pat Hanrahan.

Light ﬁeld rendering.
In ACM Proceedings of Annual Conference on Computer
Graphics and Interactive Techniques, pages 31–42, 1996.

[10] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for sin-
gle image super-resolution. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops (CVPRW), page 4, 2017.

[11] Kaushik Mitra and Ashok Veeraraghavan. Light ﬁeld de-
noising, light ﬁeld superresolution and stereo camera based
refocussing using a gmm light ﬁeld patch prior. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), pages 22–28, 2012.

[12] Ren Ng. Lytro redeﬁnes photography with light ﬁeld cam-
eras. http://www.lytro.com. Accessed: Oct. 22,
2018.

a hand-held plenoptic camera. Computer Science Technical
Report CSTR, 2(11), 2005.

[14] Christian Perwa and Lennart Wietzke. Raytrix: Light ﬁled
technology. http://www.raytrix.de. Accessed: Oct.
22, 2018.

[15] Martin Rerabek and Touradj Ebrahimi. New light ﬁeld im-
age dataset. In 8th International Conference on Quality of
Multimedia Experience, 2016.

[16] Mattia Rossi and Pascal Frossard. Geometry-consistent light
ﬁeld super-resolution via graph-based regularization. IEEE
Transactions on Image Processing, 27(9):4207–4218, 2018.
[17] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 1874–1883, 2016.

[18] Changha Shin, Hae-Gon Jeon, Youngjin Yoon, In So Kweon,
and Seon Joo Kim. Epinet: A fully-convolutional neural net-
work using epipolar geometry for depth from light ﬁeld im-
ages. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 4748–4757,
2018.

[19] Abhilash Sunder Raj, Michael Lowney, Raj Shah, and
The stanford lytro light ﬁeld
http://lightfields.stanford.edu/

Gordon Wetzstein.
archive.
LF2016.html, 2016. Accessed: Oct. 22, 2018.

[20] Ying Tai, Jian Yang, and Xiaoming Liu.

Image super-
resolution via deep recursive residual network. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), page 5, 2017.

[21] Yunlong Wang, Fei Liu, Kunbo Zhang, Guangqi Hou,
Zhenan Sun, and Tieniu Tan. Lfnet: A novel bidirectional
recurrent convolutional neural network for light-ﬁeld image
super-resolution. IEEE Transactions on Image Processing,
27(9):4274–4286, 2018.

[22] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600–612, 2004.

[23] Sven Wanner and Bastian Goldluecke. Spatial and angular
variational super-resolution of 4d light ﬁelds. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 608–621. Springer, 2012.

[24] Sven Wanner and Bastian Goldluecke. Variational light ﬁeld
analysis for disparity estimation and super-resolution. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
36(3):606–619, 2014.

[25] Sven Wanner, Stephan Meister, and Bastian Goldluecke.
Datasets and benchmarks for densely sampled 4d light ﬁelds.
In Vision, Modeling & Visualization, pages 225–226, 2013.

[26] Youngjin Yoon, Haegon Jeon, Donggeun Yoo, Joonyoung
Lee, and In So Kweon. Learning a deep convolutional net-
work for light-ﬁeld image super-resolution. In Proceedings
of the IEEE International Conference on Computer Vision
Workshop (ICCVW), pages 57–65, 2015.

[13] Ren Ng, Marc Levoy, Mathieu Br´edif, Gene Duval, Mark
Horowitz, and Pat Hanrahan. Light ﬁeld photography with

[27] Youngjin Yoon, Hae-Gon Jeon, Donggeun Yoo, Joon-Young
Lee, and In So Kweon. Light-ﬁeld image super-resolution

11054

using convolutional neural network. IEEE Signal Processing
Letters, 24(6):848–852, 2017.

[28] Jingyi Yu, Xu Hong, Jason Yang, and Yi Ma. Dgene:
The light of science, the light of future. http://www.
plex-vr.com/product/model/. Accessed: Oct. 22,
2018.

[29] Shuo Zhang, Hao Sheng, Da Yang, Jun Zhang, and Zhang
Xiong. Micro-lens-based matching for scene recovery in
lenslet cameras.
IEEE Transactions on Image Processing,
27(3):1060–1075, 2018.

[30] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.

11055

