Double Nuclear Norm based Low Rank Representation on Grassmann

Manifolds for Clustering

Xinglin Piao1, Yongli Hu2

,

∗

, Junbin Gao3, Yanfeng Sun2 and Baocai Yin1

2

,

,

4

1Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, China

2Beijing Key Laboratory of Multimedia and Intelligent Software Technology,

Faculty of Information Technology, Beijing University of Technology, China
3Discipline of Business Analytics, The University of Sydney Business School,

The University of Sydney, Camperdown NSW 2006, Australia

4Peng Cheng Laboratory, China

piaoxinglin@dlut.edu.cn; huyongli@bjut.edu.cn; junbin.gao@sydney.edu.au;

yfsun@bjut.edu.cn; ybc@bjut.edu.cn

Abstract

Unsupervised clustering for high-dimension data (such
as imageset or video) is a hard issue in data processing and
data mining area since these data always lie on a manifold
(such as Grassmann manifold). Inspired of Low Rank repre-
sentation theory, researchers proposed a series of effective
clustering methods for high-dimension data with non-linear
metric. However, most of these methods adopt the tradition-
al single nuclear norm as the relaxation of the rank func-
tion, which would lead to suboptimal solution deviated from
the original one. In this paper, we propose a new low rank
model for high-dimension data clustering task on Grass-
mann manifold based on the Double Nuclear norm which
is used to better approximate the rank minimization of ma-
trix. Further, to consider the inner geometry or structure of
data space, we integrated the adaptive Laplacian regular-
ization to construct the local relationship of data samples.
The proposed models have been assessed on several public
datasets for imageset clustering. The experimental results
show that the proposed models outperform the state-of-the-
art clustering ones.

1. Introduction

As an active topic in data processing and data mining,
data clustering has attracted great interests [2, 13]. A large
number of clustering methods [10, 20] have been proposed

∗Corresponding author: Yongli Hu (huyongli@bjut.edu.cn)

and successfully used in many applications [41, 17].
In
all the clustering methods, the spectral clustering methods
[41, 16] based on subspace assumption are considered state-
of-the-art methods with promising performance. It is gen-
erally assumed that data have intrinsic subspace structures
[32] or the data are generated from multiple subspaces.
Thus, the datum in a subspace could be linearly represent-
ed by a smaller number of other data samples from the
same subspace. The key problem of these methods is to ob-
tain a good afﬁnity matrix which usually describes the data
similarity determined by the underlying subspace structure.
To this end, various subspace clustering methods are pro-
posed. The most representative methods are Sparse Sub-
space Clustering (SSC) [7] and Low-Rank Representation
(LRR) [16], which use sparse and low rank constraints to
construct afﬁnity matrix, respectively. Later, researchers u-
tilize the local geometry or structure of the raw data as reg-
ularizers such as the Laplacian regularizer, and propose the
Non-negative Sparse Laplacian regularized Low Rank Rep-
resentation (NSLLRR) [40]. From the representation ma-
trix, clustering results can be obtained by using a spectral
clustering algorithm such as Normalized Cuts (NCut)[28].

In the aforementioned methods, the data is usually for-
mulated as vectors with Euclidean distance. However, one
often faces high dimensional data following nonlinear con-
straints, especially for imagesets and videos data [39, 34].
Since these high-dimension data are always lie on non-
linear manifold space, the traditional linear methods are no
longer valid. For example, an imageset or a video is actual-
ly modeled as a data sample on Grassmann manifold. Thus,

12075

researchers extended the traditional LRR based methods on
the Grassmann manifold for high-dimension data cluster-
ing based on the non-Euclidean geometry. The representa-
tive methods are Low Rank Representation on Grassmann
manifold method (G-LRR)[33], Partial Sum Minimization
of Singular Values Representation on Grassmann manifold
method (G-PSSVR)[37] and Cascaded Low Rank and S-
parse Representation on Grassmann manifold method (G-
CLRSR)[35]. Further, combined with the Laplacian regu-
larizer, researchers proposed Laplacian Low-Rank Repre-
sentation on Grassmann manifold method (G-LLRR)[34,
36] and Laplacian Partial Sum Minimization of Singular
Values Representation on Grassmann manifold method (G-
LPSSVR)[37] in which authors construct the Laplacian ma-
trix by original data samples.

Although the above methods have achieved good perfor-
mance in Grassmann manifold clustering, all these methods
adopt the traditional nuclear norm as the low rank constraint
which would lead to a suboptimal solution [15, 8, 5, 38].
That is because the traditional nuclear norm based low-
rank subproblem would tend to over-relaxations of rank
components from the representation matrix [42]. In recen-
t works, researchers usually adopt Schatten-p quasi-norm
(0 < p < 1) instead of the traditional nuclear norm for low
rank based problems[22, 27]. Schatten-p norm has decom-
posable approach and it could construct a more accurate low
rank matrix than the traditional nuclear norm [21]. In this
paper, we adopt double nuclear norm, a kind of Schatten-p
quasi-norm, instead of the traditional single nuclear norm to
formulate a new clustering model on Grassmann manifold
with non-linear metric for imageset clustering task. We call
this model as Double Nuclear norm based Low Rank model
on Grassmann manifold (G-DNLR). Further, to better ex-
ploit the the local geometrical structure of data space, we in-
troduce the adaptive Laplacian regularizer into the G-DNLR
and formulate an adaptive Laplacian regularized G-DNLR
which is called adaptive Laplacian Double Nuclear based
Low Rank model on Grassmann manifold (G-ALDNLR).
The contributions of this paper are following:

• Proposing a new low rank based clustering model on
Grassmann manifold for imageset clustering task by
utilizing double nuclear norm with non-linear metric;
• Adaptive Laplacian regularizer is introduced into the
G-DNLR to formulate G-ALDNLR model for exploit-
ing the local geometrical structure of the data samples;
• An algorithm is proposed to solve the complicated op-
timization problems involved in the proposed models.

The paper is organized as follows. We introduce the no-
tations and deﬁnition of Grassmann manifold in Section 2.
Section 3 review the related works. We will introduce the
formulation and optimization of the propose G-DNLR and
G-ALDNLR models in Section 4 and 5 respectively. Sec-
tion 6 assesses the clustering performance of the proposed

method on several datasets. Finally, conclusions are dis-
cussed in Section 7.

2. Notation and Deﬁnition of Grasssmann

Manifold

2.1. Notation

We use bold lowercase letters for vectors, e.g. x, y, a,
bold uppercase for matrices, e.g. X, Y, A, calligraphy let-
ters for tensors e.g. X , Y, A, lowercase letters for scalars
such as dimension and class numbers, e.g. m, n, c. xi rep-
resents the i-th column of matrix X. xij represents the i-th
element in j-th column from matrix X. Rm×n represents
the space of real numbers.

2.2. Deﬁnition of Grassmann Manifold

According to [1], the Grassmann manifold consists of
all linear p-dimension subspaces in m-dimension Euclidean
space Rm(0 ≤ p ≤ m) which is denoted as G(p, m). Thus,
we could construct a Grassmann manifold as below:

G(p, m) = {Y ∈ Rm×p : YT Y = Ip}/O(p),

(1)

where O(p) represents the p-order orthogonal group. For
two Grassmann manifold data samples Y1 and Y2, the dis-
tance of them could be deﬁned as below [12]:

distg(Y1, Y2) =

1
2

kΠ(Y1) − Π(Y2)kF ,

(2)

where Π(·) is a mapping function deﬁned as below:

Π : G(p, m) −→ Sym(m), Π(Y) = YYT .

(3)

where Sym(m) represents the space of m-dimension sym-
metric matrices. With the function Π(·), Grassmann mani-
fold could be embedded into the symmetric matrices.

3. Related Works

We ﬁrst introduce related clustering methods on the
Euclidean Space. Given a set of sample vectors Y =
[y1, y2, ..., yn] ∈ Rm×n drawn from a union of c subspaces
{Si}c
i=1, where m denotes the dimension of each sample yi
and n represents the number of samples Y. The task of sub-
space clustering is to segment the sample set Y according
to the underlying subspaces. By introducing a hypothesized
representation matrix X, the data could be self-represented
by a linear combination as Y = YX. To avoid the trivial
solution, some matrix constraints are adopted on X. In the
past decade, sparse and low rank theories have been applied
to subspace clustering successfully. Elhamifar and Vidal [7]
proposed Sparse Subspace Clustering (SSC) method, which
aims to ﬁnd the sparsest representation matrix X by using
ℓ1 norm k · k1. The SSC model is formulated as follows,

min
X

λkXk1 + kY − YXk2
F ,

(4)

12076

where λ is balance parameter and kXk1 =Pn

i=1,j=1 |xij|.
Instead of adopting the sparse constraint, Liu et al. [16] pro-
posed Low Rank Representation (LRR) method for cluster-
ing by using low rank constraint or nuclear norm k · k∗ on
X, which is formulated as follows,

min
X

λkXk∗ + kY − YXk2
F ,

(5)

i σi(X) and σi(X) represents the i-th

where kXk∗ = Pr

singular value of X, r represents the rank of X.

Later, many researchers develop some Laplacian regu-
larizer based subspace clustering methods [19, 24]. The
idea of Laplacian regularizer is induced from the graph the-
ory [6]. According to this theory, an undirected local k-
connected graph is constructed for Y. This graph could be
encoded by a symmetric afﬁnity matrix W ∈ Rn×n, where
0 ≤ wij ≤ 1 reﬂects the probability that the data points yi
and yj are connected, i.e., wij > 0 means yi and yj are
closer in certain metric or in a local neighbourhood. The lo-
cal geometry of these data points should be correspondingly
reﬂected in the data representation matrix X, which can be
formulated by minimizing the following error,

n

n

Xi=1

Xj=1

wijkxi − xjk2

2 = 2tr(XLWXT ),

(6)

where tr(·) represents the trace function of matrix, LW =
D − W represents the Laplacian matrix, D ∈ Rn×n
is a diagonal matrix D with diagonal elements dii =
j=1 wij, i = 1, ..., n. Thus, Yin et al. [40] proposed a
general Laplacian regularized low-rank representation mod-
el by using a graph regularizer called hypergraph as below:

Pn

min
X

kXk∗ + λkXk1 + αtr(XLWXT ) + βkEk1,

(7)

s.t. X ≥ 0, Y = YX + E,

where α, β are balance parameters.

The above related works all construct the representation
matrix of data samples by employing Euclidean distance
which is not suitable for the high-dimension Grassmann
manifold data. Therefore, researchers proposed a series of
clustering methods for Grassmann manifold based on the
non-distance deﬁned in (2). For a set of Grassmann sam-
ples Y = {Y1, Y2, ..., Yn} where Yi ∈ G(p, m). By gen-
erating the (5) on Grassmann, Wang et al. [33] proposed
a Low Rank model with non-linear metric for Grassmann
(G-LRR):

min
X

λkXk∗ +

kYi ⊖

n

Xi=1

n

]j=1

Yj ⊛ xjikG,

(8)

where kYi⊖Un
error of the sample Yi on Grassmann manifold,Un

j=1 Yj ⊛xjikG represents the reconstruction
j=1 Yj ⊛

j=1 with the coefﬁ-

xji denotes the “combination” of {Yj}n
cients {xji}n
bols which are used to simulated the “linear” operations on
Grassmann manifold. Combined with Laplacian regularizer
deﬁned in (6), Wang et al.[34, 36] proposed Laplacian Low
Rank model on Grassmann manifold (G-LLRR) model:

i=1,j=1, the symbol ⊖,U, ⊛ are abstract sym-

min
X

λkXk∗ + αtr(XLWXT )

+

n

Xi=1

kYi ⊖

n

]j=1

Yj ⊛ xjikG,

(9)

where afﬁnity matrix W is constructed based on non-
linear distance metric deﬁned in (2) based on the raw data
{Yi}n
i=1. They also proposed a cascaded Low Rank and
Sparse model on Grassmann manifold (G-CLRSR) model:

min
X,C

λkXk∗ + αkZk1 + βkX − XZk2
F

+

n

Xi=1

kYi ⊖

n

]j=1

Yj ⊛ xjikG.

(10)

Further, to achieve better low rank representation matrix
for clustering, Wang et al.
[37] adopt Partial Sum Min-
imization of Singular Values (PSSV) norm to instead the
nuclear norm for formulating PSSV Low Rank model on
Grassmann manifold (G-PSSVLR) model:

min
X

λkXk>r +

kYi ⊖

n

Xi=1

n

]j=1

Yj ⊛ xjikG,

(11)

where k · k>r represents the PSSV norm deﬁned as below
[25]:

n

kXk>r =

σi(X),

(12)

Xi=r+1

where σi(X) represents the i-th largest singular value of
X, r represents the expected rank of X. Similar to G-
LLRR, Wang et al. also proposed Laplacian G-PSSVLR
(G-LPSSVLR) as below [37]:

min
X

λkXk>r + αtr(XLWXT )

+

n

Xi=1

kYi ⊖

n

]j=1

Yj ⊛ xjikG.

(13)

Although the above methods achieve great performance in
Grassmann manifold clustering problem, all these methods
adopt nuclear norm or PSSV norm which would cause sub-
optimal solution of the low rank based problem. Further, the
afﬁnity matrices in G-LLRR and G-LPSSVLR are all con-
structed based on the raw data. However there may exist
noise and outlier in the raw data, e.g. face images varia-
tions caused by illumination, color and pose. These factors
would reduce the ability to represent the correlation among
data.

12077

4. Double Nuclear norm based Low Rank mod-

el on Grassmann manifold

solve it. We ﬁrst introduce two auxiliary variables ˆA = A,
ˆB = B and rewrite (17) as below:

In this section, we will introduce the formulation and op-

timization of the proposed G-DNLR model in detail.

min

X,A,B, ˆA, ˆB

λ(k ˆAk∗ + k ˆBk∗) + tr(XT GX) − 2tr(GX),

4.1. Models Formulation

For a set of Grassmann samples Y = {Y1, Y2, ..., Yn}
where Yi ∈ G(p, m), we could formulate a double nuclear
norm based low rank representation model on Grassmann
manifold as below:

min
X,A,B

λ(kAk∗ + kBk∗) +

s.t. X = AB,

kYi ⊖

n

Xi=1

n

]j=1

Yj ⊛ xjikG,

(14)
where A ∈ Rn×r, B ∈ Rr×n, r < n represents the expect
rank of X. kAk∗ +kBk∗ represents the double nuclear nor-
m for X. We call this model the Double Nuclear norm based
Low Rank model on Grassmann manifold (G-DNLR). The
objective function is hard to solve owing to the non-linear
metric on Grassmann manifold. According to the proper-
ty and deﬁnition in Section 2, we could use the embedding
distance to replace the construction error in (14) as below:

kYi ⊖

n

]j=1

Yj ⊛ xjikG = dist2

g(Yi,

Yj ⊛ xji)

= kYiYT

i −

xjiYj YT

j k2
F .

n

n

]j=1
Xj=1

(15)
With this measurement, the function in (14) could be rewrit-
ten as below:

s.t. X = AB, ˆA = A, ˆB = B.

(18)
Then we remove the linear equality constraints in (18) by
using the augmented Lagrangian method and have

min

X,A,B, ˆA, ˆB

λ(k ˆAk∗ + k ˆBk∗) + tr(XT GX) − 2tr(GX)

+ tr(FT

1 ( ˆA − A)) + tr(FT

2 ( ˆB − B))

+ tr(FT

3 X − AB) +

γ
2

(k ˆA − Ak2
F

+ k ˆB − Bk2

F + kX − ABk2

F ),

(19)
where F1, F2, F3 are Lagrangian multipliers. γ > 0 is a
penalty parameter. For this problem, X, A, B, ˆA, ˆB and
other parameters can be solved by the following alternative
iterations, in which superscript t denotes the current itera-
tion step.

4.2.1 Update ˆA with ﬁxing others

When other variables are ﬁxed, (19) degenerates into a func-
tion with respect to ˆA as below:

λk ˆAk∗ +

min
ˆA

γ
2

k ˆA − (A −

F1
γ

)k2
F .

(20)

We can update ˆA based on the closed-form solution [4]:

ˆA(t+1) = U(t)

1 max{Σ(t)

1 −

λ

γ(t) , 0}V(t)T

1

,

(21)

is the singular value decomposition

min
X,A,B

λ(kAk∗ + kBk∗) + kYiYT

i −

s.t. X = AB.

xjiYj YT

j k2
F .

n

Xj=1

1 Σ(t)
where U(t)
(SVD) of A(t) −

1 V(t)T
1
F(t)
γ(t) .

1

Donating gij = tr((YT
could rewrite (16) as below:

j Yi)(YT

(16)
i Yj)) according to [33], we

min
X,A,B

λ(kAk∗ + kBk∗) + tr(XT GX) − 2tr(GX),

s.t. X = AB,

(17)
where matrix G = {gij}n×n ∈ Rn×n is a symmetric
matrix. With these transformation, the original non-linear
function in (14) could be converted into a linear one.

4.2. Optimization of G DNLR

G-DNLR model is a complicated optimization problem
which is difﬁcult to solve directly. Here, we adopt the al-
ternating direction method of multipliers (ADMM) [3] to

4.2.2 Update ˆB with ﬁxing others

When other variables are ﬁxed, (19) degenerates into a func-
tion with respect to ˆB as below:

λk ˆBk∗ +

min

ˆB

γ
2

k ˆB − (B −

F2
γ

)k2
F .

(22)

We can update ˆB based on the closed-form solution [4]:

ˆB(t+1) = U(t)

2 max{Σ(t)

2 −

λ

γ(t) , 0}V(t)T

2

,

(23)

2 V(t)T

2

where U(t)
of B(t) −

2 Σ(t)
F(t)
γ(t) .

2

is the singular value decomposition

12078

4.2.3 Update A with ﬁxing others

5. Adaptive Laplacian regularized G-DNLR

When other variables are ﬁxed, (19) degenerates into a func-
tion with respect to A as below:

min
A

kA − ( ˆA +

F1
γ

)k2

F + kAB − (X +

F3
γ

)k2
F .

(24)

The closed-form solution to the problem(24) is given by

A(t+1) = (P(t)

2 B(t))(I1 + B(t)B(t)T
γ(t) , P(t)
where P(t)
represents the identify matrix.

1 + P(t)
F(t)

1 = ˆA(t) +

2 = X(t) +

1

)−1,

(25)

F(t)
γ(t) , I1 ∈ Rr×r

3

4.2.4 Update B with ﬁxing others

When other variables are ﬁxed, (19) degenerates into a func-
tion with respect to A as below:

min

B

kB − ( ˆB +

F2
γ

)k2

F + kAB − (X +

F3
γ

)k2
F .

(26)

The closed-form solution to the problem(26) is given by

B(t+1) = (A(t)T

A(t) + I1)−1(A(t)T

P(t)

2 + P(t)
3 ),

(27)

where P(t)

3 = ˆB(t) +

F(t)
γ(t) ,

2

4.2.5 Update X with ﬁxing others

When other variables are ﬁxed, (19) degenerates into a func-
tion with respect to X as below:

min
X

tr(XT GX) − 2tr(GX) +

γ
2

kX − AB +

F3
γ

k2
F .

(28)

The closed-form solution to the problem(28) is given by
F(t)
3
γ(t) )).
(29)

X(t+1) = (2G + γ(t)I2)−1(2G + γ(t)(A(t)B(t) −

4.2.6 Update F1, F2, F3 and γ

The Lagrangian multipliers F1, F2, F3 and penalty param-
eter γ could be updated as follows:

F(t+1)

1

F(t+1)

2
= F(t)

= F(t)
1 + γ(t)( ˆA(t+1) − A(t+1)).
= F(t)
2 + γ(t)( ˆB(t+1) − B(t+1)).
3 + γ(t)(X(t+1) − A(t+1)B(t+1)).
γ(t+1) = min(ργ(t), γmax),

F(t+1)

3

(33)
where ρ > 1 is a constant and γmax is the upper bound of γ.
In our algorithm, the stopping criterion is measured by the
following condition:

(30)

(31)

(32)

k ˆA(t+1) − A(t+1)k∞,
k ˆB(t+1) − B(t+1)k∞,

kX(t+1) − A(t+1)B(t+1)k∞

max


≤ ε.

(34)




model

5.1. Model formulation

Combined the Laplacian regularizer, we could construct
an afﬁnity matrix W based on the raw data samples and
formulate a Laplacian double nuclear norm based low rank
model for Grassmann manifold:

min
X,A,B

λ(kAk∗ + kBk∗) + αtr(XLWXT )

+ tr(XT GX) − 2tr(GX),

(35)

s.t. X = AB.

However, as we discussed in Section 3, the afﬁnity matrix
constructed by raw data would be biased by the noise or
outliers among the data. Therefore, we adopt the similar
approach in [11] to construct an adaptive Laplacian double
nuclear norm base low rank model for Grassmann manifold
(G-ALDNLR) as follows:

min

X,A,B,W

λ(kAk∗ + kBk∗) + αtr(XLWXT ) + βkWk2
F

+ tr(XT GX) − 2tr(GX),

s.t. X = AB, WT 1n = 1n, W = WT ,

wij ≥ 0, ∀i, j,

(36)
where kWk2
F is a regularisation term on W to prevent triv-
ial solution, 1n represents an n-dimension vector of all 1s.
In this formulation, the afﬁnity matrix W is no longer con-
structed from the raw data directly. It can be regarded as a
latent afﬁnity matrix to reﬂect the geometry property of the
the element wij of W represents
original data space, i.e.
the probability of the i-th and j-th data points belonging to
the same class and can be adaptively adjusted in the above
optimal procedure.

5.2. Optimization of G ALDNLR

To solve G-ALDNLR model in (36), we introduce three
auxiliary variables ˆA = A, ˆB = B, Z = X and remove the
linear equality constraints rewrite (36) by using the aug-
mented Lagrangian method as below:

min

Θ

λ(k ˆAk∗ + k ˆBk∗) + αtr(ZLWZT ) + βkWk2
F

+ tr(XT GX) − 2tr(GX) + tr(FT
+ tr(FT

2 ( ˆB − B)) + tr(FT

3 (X − AB))

1 ( ˆA − A))

+ tr(FT

4 (X − Z)) +

γ
2

(k ˆA − Ak2

F + k ˆB − Bk2

F

+ kX − Zk2

F + kX − ABk2

F ),

s.t. WT 1n = 1n, W = WT , wij ≥ 0, ∀i, j,

(37)
where Θ = {X, A, B, W, ˆA, ˆB, Z}. We could adopt (20)
to (27)to solve A, B, ˆA, ˆB, and solve Z, X, W as below:

12079

5.2.1 Update X with ﬁxing others

When other variables are ﬁxed, (37) degenerates into a func-
tion with respect to X as below:

min
X

tr(XT GX) − 2tr(GX) +

γ
2

(kX − AB +

F3
γ

k2
F

+ kX − Z +

F4
γ

k2
F ).

The closed-form solution to the problem(38) is given by

(38)

X(t+1) = (2G + γ(t)I2)−1(2G + γ(t)(P(t)

4 + P(t)
5 ),

(39)

where P(t)

4 = A(t)B(t) −

F(t)

3

γ(t) , P(t)

5 = X(t) −

F(t)
γ(t) .

4

5.2.2 Update Z with ﬁxing others

When other variables are ﬁxed, (37) degenerates into a func-
tion with respect to Z as below:

min

Z

αtr(ZLWZT ) +

γ
2

kZ − (X +

F4
γ

)k2
F .

(40)

The closed-form solution to the problem(38) is given by:

Z(t+1) = γ(t)(Z(t) +

F(t)
4
γ(t) )(2αLW

(t) + γ(t)I2)−1. (41)

5.2.3 Update W with ﬁxing others

When other variables are ﬁxed, (37) degenerates into a

function with respect to W as below:

min
W

αtr(ZLWZT ) + βkWk2
F ,

s.t. WT 1n = 1n, W = WT , wij ≥ 0, ∀i, j.

(42)

This problem can be separated into a set of independent sub-
problems, i.e.:

Algorithm 1 The solution to G-DNLR and G-ALDNLR
Require: The Grassmann

sample

set
the expect rank r,

Y

=
the number

{Y1, Y2, ..., Yn},
of neighbours k, the parameters λ, α, β

1: Initialize : X(0) = Z(0) = 0 ∈ Rn×n, A(0) =
ˆA(0) = 0 ∈ Rn×r, B(0) = ˆB(0) = 0 ∈ Rr×n,
F(0)
1 = 1 ∈ Rn×r, F(0)
4 =
1 ∈ Rn×n, initialize W by solving (49), γ(0) = 10−4,
ρ > 1, γmax = 1010, ε = 10−7, the number of maxi-
mum iteration M axIter = 1000

2 = 1 ∈ Rr×n, F(0)

3 = F(0)

2: Calculate matrix G by gij = tr((YT
3: t = 0;
4: while not converged and t ≤ M axIter do
5:

j Yi)(YT

i Yj));

Update ˆA, ˆB, A, B by (20) to (27) respectively;
Update X by (28) and (29) for G-DNLR, or update
X by (38) and (39) for G-ALDNLR;
Update Z and W by (40) to (46) respectively for G-
ALDNLR ;
Update F1, F2, F3, F4 and γ by (30), (31), (32), (47)
and (33) respectively;
Check the convergence condition deﬁned as (34) for
G-DNLR, or check the condition deﬁned as (48) for
G-ALDNLR;
t = t + 1.

6:

7:

8:

9:

10:
11: end while
Ensure:

The matrices ˆA, ˆB, A, B, X, Z, W.

i

ji

are same as those of q(t)

is the j-th element of column vector ˆq(t)

where ˆq(t)
elements of ˆq(t)
in ascending or-
der. For any column u, the operator (u)+ turns negative
elements in u to 0 while keeping the rest. The proof of the
closed-form solution to (45) could be found in [23]. Final-
ly, we could adjust A(t+1) which is generally an unbalanced
digraph obtained by (45) as follows:

, the

i

i

W(t+1) =

1
2

(W(t+1) + W(t+1)T

).

(46)

W(t+1) = arg min

W

kW + Q(t)k2
F ,

s.t. WT 1n = 1n, wij ≥ 0, ∀i, j,

(43)

5.2.4 Update F1, F2, F3, F4 and γ

F1, F2, F3 and γ could be updated by (30) to (33). Then
we could update F4 as below:

where each element q(t)
lows:

ij of matrix Q(t) is deﬁned as fol-

q(t)
ij =

kz(t)

i − z(t)

j k2
2,

(44)

F(t+1)

4

= F(t)

4 + γ(t)(X(t+1) − Z(t+1)).

(47)

α
4β

i

where z(t)
closed-form solution for the i-th column w(t+1)
by its k nearest neighbours as below:

represents the i-th column of Z(t). We have the
of W(t+1)

i

w(t+1)

i

=  1 +Pk

j=1 ˆq(t)
k

ji

1n − q(t)

i !+

,

(45)

In this algorithm, the stopping criterion of G-ALDNLR is
measured by the following condition:

k ˆA(t+1) − A(t+1)k∞,
k ˆB(t+1) − B(t+1)k∞,
kX(t+1) − Z(t+1)k∞,

kX(t+1) − A(t+1)B(t+1)k∞

max





≤ ε.

(48)

12080

8 x 105

6

4

2

0
0

4 x 106

3

2

1

4 x 106

3

2

1

8 x 105

6

4

2

4 x 106

3

2

1

8 x 105

6

4

2

8 x 105

6

4

2

50

100

150

200

0
0

50

100

150

200

0
0

50

100

150

200

0
0

50

100

150

200

0
0

50

100

150

200

0
0

50

100

150

200

0
0

50

100

150

200

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 1. The convergence curves of G-DNLR and G-ALDNLR on Extended Yale B face dataset. In each subﬁgure, the x-axis represents
the iterations and the y-axis represents the value of function: (a) k ˆA(t+1)−A(t+1)k∞ for G-DNLR; (b) k ˆB(t+1)−B(t+1)k∞ for G-DNLR;
(c) kX(t+1) − A(t+1)B(t+1)k∞ for G-DNLR. (d) k ˆA(t+1) − A(t+1)k∞ for G-ALDNLR; (e) k ˆB(t+1) − B(t+1)k∞ for G-ALDNLR; (f)
kX(t+1) − Z(t+1)k∞ for G-ALDNLR; (g) kX(t+1) − A(t+1)B(t+1)k∞ for G-ALDNLR.

We summarized all update steps in Algorithm 1 for both G-
DNLR and G-ALDNLR. For G-ALDNLR, we initialize W
based on the distance of original Grassmann data sample as
below:

min
W

wijkYiYT

i − Yj YT

j k2
F ,

s.t. WT 1n = 1n, W = WT , wij ≥ 0, ∀i, j.

The solution to this problem is similar to (42).

5.3. Converge and Complexity Analysis

(49)

For Algorithm 1, by splitting variables in the proposed
objective functions, we have a set of subproblems for each
variable with closed-form solution. Therefore, all the con-
vergence analysis and proof in [27, 42] could be applicable
for our proposed methods. Figure 1 shows an example of
the convergence of the proposed methods on Extended Yale
B face dataset[14]. It is indicated that the curves decrease
fast and almost tend to be stable within 200 iterations, which
veriﬁes the good convergence of our algorithm.

Further, we discuss the complexity of the proposed mod-
els. Calculating G has a complexity of O(n2(p2m + p3)).
In each iteration step, the complexity of updating ˆA, ˆB,
A and B are all O(nr2). The complexity of updating Z
and X are both O(n3). The complexity of updating W is
O(n2). Therefore, the total complexity of G-DNLR and
G-ALDNLR are O(n2(p2m + p3) + t(4nr2 + n3)) and
O(n2(p2m + p3) + t(4nr2 + 2n3 + n2)) respectively. We
also list the complexities of other methods in Table 1. Be-
sides, we test all the methods on Extended Yale B dataset
and show the wall-clock time in Table 1. It demonstrates
that our proposed methods (G-DNLR & G-ALDNLR) have
acceptable executive time. All methods are coded in Mat-
lab R2014a and implemented on an Intel Core i7-7700
3.60GHz CPU machine with 16G RAM.

6. Empirical Comparison

We test our methods on four datasets, including the Ex-
tended Yale B face dataset[14], CMU-PIE face dataset[30],
Ballet action dataset [9] and SKIG gesture dataset [18].
The performance of the proposed methods is compared

Method

SSC

LRR

LS3C

SCGSM

G-KM

G-CLRSR

G-LRR

G-LLRR

G-PSSVLR

G-LPSSVLR

G-DNLR

G-ALDNLR

Complextiy

O(tn2 (1 + n))

O(2tn3 )

O(tn2 (sn2 + 1))

O(m3 p3 d2 t(n + d))
O(n2 p2 (m + p) + 3n3

O(n2 p2 (m + p) + tn2 (3n + 1))

O(n2 p2 (m + p) + 2tn3 )
O(n2 p2 (m + p) + 2tn3 )
O(n2 p2 (m + p) + 2tn3 )
O(n2 p2 (m + p) + 2tn3 )

O(n2 p2 (m + p) + tn(4r2 + n2 ))

O(n2 p2 (m + p) + tn(4r2 + 2n2 + n))

Running Time

1.61

21.41

30.65

1050.67

0.11

90.56

17.76

18.46

15.17

15.74

17.28

32.25

Table 1. The complexity and running time (second) on Extended
Yale B dataset of various methods.

with some state-of-the-art clustering algorithms, such as
SSC [7], LRR [16], LS3C [26], SCGSM [31], G-KM
[29], G-LRR [33], G-LLRR [34, 36], G-PSSVLR &G-
LPSSVLR[37] and G-CLRSR [35]. In our methods, after
learning the representation X, we use the NCut method [28]
to obtain the ﬁnal clustering results. The clustering results
are measured by the clustering Accuracy (ACC) and Nor-
malized Mutual Information (NMI). The details of data set-
ting and results analysis are given below.

6.1. Data and parameters setting

The Extended Yale B dataset contains 2,414 frontal face
images of 38 subjects. Each subject has about 64 images.
In our experiments, we resize images into 20 × 20 and ran-
domly select 8 images from the same subject to form an
imageset sample. The CMU-PIE dataset is composed of 68
subjects and each subject has 21 front face images with no
expression. Every 4 images from the same subject are se-
lected to form an imageset sample. Ballet action contains
44 video clips collected from an instructional Ballet DVD.
We resize each image into 30 × 30 and every 12 images
are chosen for an imageset from each clip. SKIG gesture
dataset consists of 1080 RGB-D video collected from 6 sub-
jects with 10 gesture types. Each image is resized as 24×32
and each clip is regarded as an imageset.

In our experiments, we ﬁrst transform each image into a
m-dimension vector. For vector based LRR, SSC and LS3C
methods, we stack all image vectors from the same image-
set as a long vector and adopt PCA to reduce the dimension
which equals to the dimension of PCA components retain-
ing 95% of its variance energy. For other Grassmann man-

12081

Method

Extended Yale B

CMU-PIE

Ballet
SKIG

SSC

0.4032
0.5231
0.2962
0.3892

LRR
0.4659
0.4034
0.2923
0.2537

SCGSM G-KM G-CLRSR G-LRR G-LLRR G-PSSVLR G-LPSSVLR G-DNLR G-ALDNLR
0.7946
0.5732
0.5613
0.3716

LS3C
0.2461
0.2761
0.4262
0.2941
Table 2. The accuracy results of various methods on four datasets.

0.8135
0.6153
0.5912
0.5022

0.7921
0.5862
0.6076
0.5176

0.9035
0.6213
0.6013
0.5502

0.9118
0.6373
0.6243
0.5712

0.8194
0.6289
0.5931
0.5083

0.8365
0.6025
0.5699
0.5308

0.9855
0.7244
0.6885
0.6667

0.9749
0.6618
0.6768
0.6244

Method

Extended Yale B

CMU-PIE

Ballet
SKIG

SSC

0.6231
0.7865
0.2813
0.4762

LRR
0.6813
0.7321
0.2910
0.3343

LS3C
0.4992
0.6313
0.4370
0.3421

SCGSM G-KM G-CLRSR G-LRR G-LLRR G-PSSVLR G-LPSSVLR G-DNLR G-ALDNLR
0.9326
0.5736
0.5646
0.5367

0.9461
0.8080
0.6102
0.5873

0.9262
0.7926
0.5837
0.5692

0.8903
0.8103
0.5762
0.5450

0.8923
0.7942
0.5983
0.5571

0.9103
0.8132
0.5862
0.5679

0.9341
0.5976
0.5779
0.5671

0.9891
0.8774
0.6692
0.6484

0.9874
0.8510
0.6310
0.6482

Table 3. The normalized mutual information results of various methods on four datasets.

ifold based methods, we form all image vectors from the
same imageset as a matrix. Then SVD is applied on the ma-
trix and we pick up the ﬁrst p columns of the left singular
matrix as a sample data on Grassmann manifold G(p, m).

To get a suitable setting for these parameters, we s-
tudy each parameter’s inﬂuence on the clustering accura-
cy by some pre-experiments.
In these experiments, each
parameter is tested with ﬁxing the other parameters for
both G-DNLR and G-ALDNLR. We tune the rank val-
ue r within {1, 2, · · · , n}, the neighbour number k with-
in {1, 2, · · · , 10}, other parameters λ, α, β are tuned within
{10−10, 10−9, · · · , 1, 10, 102}. Figure 2 shows the inﬂu-
ence of λ, α, β, r and k on Extended Yale B dataset as an
example. The parameters setting are: λ = 1, r = 100 for
G-DNLR and λ = α = β = 10−2, r = 100, k = 7 for
G-ALDNLR on Extended Yale B dataset; λ = 1, r = 142
for G-DNLR and λ = α = β = 10−2, r = 110, k = 6
for G-ALDNLR on CMU-PIE dataset; λ = 102, r = 6 for
G-DNLR and λ = α = β = 10−7, r = 10, k = 8 for
G-ALDNLR on Ballet dataset; λ = 10−2, r = 27 for G-
DNLR and λ = 1, α = 102, β = 10−5, r = 27, k = 8 for
G-ALDNLR on SKIG dataset.

6.2. Results analysis

All clustering results are shown in Tables 2 and 3. For
each experiment, the clustering are repeated 20 times and
the average results are reported. The best results are bold,
the second ones are in italic script and the third ones are
underlined. From the results, Grassmann manifold repre-
sentation based methods always have better performances
than the vectors based ones (SSC, LRR, LS3C), which is ex-
plained that the manifold representation have the advantage
of revealing the complicated relationship within the image-
set data effectively. In all the methods, the low rank based
models always obtain the top results, which shows the ben-
eﬁt of low rank representation. From the results, our pro-
posed G-DNLR and G-ALDNLR obtain the top 2 best re-
sults and outperform the third best ones with about 4 and 8
percentage points gap in terms of ACC on avenges respec-
tively. The signiﬁcant improvement of our method is ana-
lyzed and own to the superiority that the proposed method

1

0.8

0.6

0.4

0.2

0

C
C
A

1

0.8

0.6

0.4

0.2

0

C
C
A

C
C
A

1

0.8

0.6

0.4

0.2

0

50

100

150

200

r

250

297

2

0

−2

−4

−6

log(λ)

−8

−10

50

100

150

200

r

250

297

2

0

−2

−4

−6

log(λ)

−8

−10

−10

−8

−6

−4

−2

log(α)

0

2

−8

−10

2

0

−2

−4

−6

log(β)

1

0.8

0.6

0.4

0.2

C
C
A

0
1

2

3

4

5

6

7

8

9

10

k

(a)

(b)

(c)

(d)

Figure 2. The clustering accuracy of the proposed methods on Ex-
tended Yale B dataset with different parameters setting: (a) G-
DNLR with different λ and r; (b) G-ALDNLR with different λ
and r; (c) G-ALDNLR with different α and β; (d) G-ALDNLR
with different k.

not only adopts the double nuclear norm but also constructs
adaptive afﬁnity matrix based on the local structure.

7. Conclusion

In this paper, we propose two new low rank model
on Grassmann manifold for high-dimension data cluster-
ing task. Instead of the traditional single nuclear norm, we
adopt a kind of Schatten-p quasi-norm named Double Nu-
clear norm to formulate novel clustering models on Grass-
mann manifold with non-linear metric, which is called G-
DNLR. Further, to exploit the local geometrical structure
of the data samples, we integrated the adaptive Laplacian
regularization with G-DNLR as G-ALDNLR. The proposed
models has been evaluated on four public datasets. The ex-
perimental results show that our proposed models outper-
forms state-of-the-art ones.

Acknowledgement

The research project is supported by National Natural
Science Foundation of China under Grant No. 61632006,
61672071, U1811463, 61772048, 61806014, Beijing Natu-
ral Science Foundation No. 4172003, 4184082, the Innova-
tion Foundation of Science and Technology of Dalian No.
2018J11CY010.

References

[1] Pierre-Antoine Absil, Robert E. Mahony, and Rodolphe
Sepulchre. Optimization Algorithms on Matrix Manifolds.
Princeton University Press, 2009.

12082

[2] Arijit Biswas and David Jacobs. Active image clustering:
Seeking constraints from humans to complement algorithm-
s. In Procceedings of IEEE Conference on Computer Vision
and Pattern Recognition, pages 2152–2159, 2012.

[3] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and
Jonathan Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122,
2011.

[4] Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A
singular value thresholding algorithm for matrix completion.
SIAM Journal on Optimization, 20(4):1956–1982, 2008.

[5] Emmanuel J. Cand`es, Michael B. Wakin, and Stephen P.
Boyd. Enhancing sparsity by reweighted ℓ1 minimization.
Journal of Fourier analysis and applications, 14(5–6):877–
905, 2008.

[6] Fan R. K. Chung. Spectral graph theory, volume 92. Amer-

ican Mathematical Soc., Providence, RI, USA, 1997.

[7] Ehsan Elhamifar and Ren´e Vidal. Sparse subspace cluster-
ing: Algorithm, theory, and applications. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(1):2765–
2781, 2013.

[8] Jianqing Fan and Runze Li. Variable selection via noncon-
cave penalized likelihood and its oracle properties. Journal
of the American statistical Association, 96(456):1348–1360,
2001.

[9] Alireza Fathi and Greg Mori. Action recognition by learning
In Procceedings of IEEE Con-
mid-level motion features.
ference on Computer Vision and Pattern Recognition, pages
1–8, 2008.

[10] Amit Gruber and Yair Weiss. Multibody factorization with
In
uncertainty and missing data using the em algorithm.
Procceedings of IEEE Conference on Computer Vision and
Pattern Recognition, pages 707–714, 2004.

[11] Xiaojie Guo. Robust subspace segmentation by simultane-
ously learning data representations and their afﬁnity matrix.
In Proceedings of International Joint Conference on Artiﬁ-
cial Intelligence, pages 3547–3553, 2015.

[12] Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, and
Brian C. Lovell. Dictionary learning and sparse coding on
grassmann manifolds: An extrinsic solution.
In Procceed-
ings of International Conference on Computer Vision, pages
3120–3127, 2013.

[13] Stelios Krinidis and Vassilios Chatzis. A robust fuzzy local
IEEE Transac-

information c-means clustering algorithm.
tions on Image Processing, 19(5):1328–1337, 2010.

[14] Kuang-Chih Lee, Jeffrey Ho, and David J Kriegman. Ac-
quiring linear subspaces for face recognition under variable
lighting.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(5):684–698, 2005.

[15] Gilad Lerman and Teng Zhang. Robust recovery of multi-
ple subspaces by geometric lp minimization. The Annals of
Statistics, 39(5):2686–2715, 2011.

[16] Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust sub-
space segmentation by low-rank representation.
In Pro-
ceedings of International Conference on Machine Learning,
pages 663–670, 2010.

[17] Hongfu Liu, Ming Shao, Sheng Li, and Yun Fu. Inﬁnite en-
semble for image clustering. In Proceedings of ACM SIGKD-
D International Conference on Knowledge Discovery and
Data Mining, pages 1745–1754, 2016.

[18] Li Liu and Ling Shao. Learning discriminative represen-
tations from RGB-D video data. In Procceedings of Inter-
national Joint Conference on Artiﬁcial Intelligence, pages
1493–1500, 2013.

[19] Xiaoqiang Lu, Yulong Wang, and Yuan Yuan. Graph-
regularized low-rank representation for destriping of hyper-
spectral images. IEEE Transactions on Geoscience and Re-
mote Sensing, 51(7):4009–4018, 2013.

[20] Yi Ma, Allen Y. Yang, Harm Derksen, and Robert Fossum.
Estimation of subspace arrangements with applications in
modeling and segmenting mixed data. Statistics and com-
puting, 50(3):413–458, 2008.

[21] Mohammadreza Malek-Mohammadi, Massoud Babaie-
Zadeh, and Mikael Skoglund. Performance guarantees for
schatten-p quasi-norm minimization in recovery of low-rank
matrices. Signal Processing, 114:225–230, 2015.

[22] Feiping Nie, Hua Wang, Heng Huang, and Chris Ding. Join-
t schatten p-norm and ℓp-norm robust matrix completion for
missing value recovery. Knowledge and Information System-
s, 42(3):525–544, 2015.

[23] Feiping Nie, Xiaoqian Wang, and Heng Huang. Clustering
and projected clustering with adaptive neighbors.
In Pro-
ceedings of ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, pages 977–986, 2014.

[24] Feiping Nie, Xiaoqian Wang, Michael I. Jordan, and Heng
Huang. The constrained laplacian rank algorithm for graph-
based clustering. In Proceedings of AAAI Conference on Ar-
tiﬁcial Intelligence, pages 1969–1976, 2016.

[25] Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeong-
woo Kim, and In So Kweon. Partial sum minimization of
singular values in robust pca: Algorithm and applications.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 38(4):744–758, 2015.

[26] Vishal M. Patel, Hien Van Nguyen, and Ren´e Vidal. Latent
space sparse subspace clustering. In Procceedings of IEEE
International Conference on Computer Vision, pages 225–
232, 2013.

[27] Fanhua Shang, James Cheng, Yuanyuan Liu, Zhi-Quan Lu-
o, and Zhouchen Lin. Bilinear factor matrix norm mini-
mization for robust pca: Algorithms and applications. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
40(9):2066–2080, 2018.

[28] Jianbo Shi and Jitendra Malik. Normalized cuts and image
IEEE Transactions on Pattern Analysis and

segmentation.
Machine Intelligence, 22(1):888–905, 2010.

[29] Sareh Shirazi, Mehrtash T. Harandi, Conrad Sanderson,
Azadeh Alavi, and Brian C. Lovell. Clustering on grassman-
n manifolds via kernel embedding with application to action
analysis. In Proceedings of IEEE International Conference
on Image Processing, pages 781–784, 2012.

[30] Terence Sim, Simon Baker, and Maan Bsat. The cmu pose,
illumination, and expression database.
IEEE Transactions
on Pattern Analysis and Machine Intelligence, 25(12):1615–
1618, 2003.

12083

[31] Pavan Turaga, Ashok Veeraraghavan, Anuj Srivastava, and
Rama Chellappa. Statistical computations on grassmann
and stiefel manifolds for image and video-based recognition.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 33(11):2273–2286, 2011.

[32] Ren´e Vidal. A tutorial on subspace clustering. IEEE Signal

Processing Magazine, 25(2):52–68, 2011.

[33] Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, and
Baocai Yin. Low rank representation on Grassmann man-
ifolds.
In Proceedings of Asian Conference on Computer
Vision, pages 81–96, 2014.

[34] Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, and
Baocai Yin. Laplacian lrr on product grassmann manifolds
for human activity clustering in multicamera video surveil-
lance. IEEE Transactions on Circuits and Systems for Video
Technology, 27(3):554–566, 2017.

[35] Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, and
Baocai Yin. Cascaded low rank and sparse representation
on grassmann manifolds.
In Proceedings of International
Joint Conference on Artiﬁcial Intellignce, pages 2755–2761,
2018.

[36] Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, and
Baocai Yin. Localized lrr on grassmann manifold: An ex-
trinsic view. IEEE Transactions on Circuits and Systems for
Video Technology, 28(10):2524–2536, 2018.

[37] Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, and
Baocai Yin. Partial sum minimization of singular values rep-
resentation on grassmann manifolds. ACM Transactions on
Knowledge Discovery from Data, 12(1):Article 13, 2018.

[38] Shusen Wang, Dehua Liu, and Zhihua Zhang. Nonconvex
relaxation approaches to robust matrix recovery.
In Proc-
ceedings of International Joint Conference on Artiﬁcial In-
telligence, pages 1764–1770, 2013.

[39] Tong Wu and Waheed U. Bajwa. Learning the nonlinear
geometry of high-dimensional data: Models and algorithm-
s.
IEEE Transactions on Signal Processing, 63(23):6229–
6244, 2015.

[40] Ming Yin, Junbin Gao, and Zhouchen Lin. Laplacian reg-
ularized low-rank representation and its applications. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
38(3):504–517, 2016.

[41] Chong You, Daniel Robinson, and Ren´e Vidal. Scalable s-
parse subspace clustering by orthogonal matching pursuit. In
Procceedings of IEEE Conference on Computer Vision and
Pattern Recognition, pages 3918–3927, 2016.

[42] Hengmin Zhang, Jian Yang, Fanhua Shang, Chen Gong,
and Zhenyu Zhang. LRR for subspace segmentation vi-
a tractable schatten-p norm minimization and factorization.
IEEE Transactions on Cybernetics, 49(5):172–1734, 2019.

12084

