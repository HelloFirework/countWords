High-level Semantic Feature Detection:

A New Perspective for Pedestrian Detection

Wei Liu1

,

2∗, Shengcai Liao3†, Weiqiang Ren4, Weidong Hu1, Yinan Yu4

1 ATR, College of Electronic Science, National University of Defense Technology, Changsha, China

2 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing ,China

3 Inception Institute of Artiﬁcial Intelligence (IIAI), Abu Dhabi, UAE

4 Horizon Robotics Inc., Beijing ,China

{liuwei16, wdhu}@nudt.edu.cn, scliao@ieee.org, {weiqiang.ren, yinan.yu}@hobot.cc

Abstract

Object detection generally requires sliding-window clas-
siﬁers in tradition or anchor-based predictions in modern
deep learning approaches. However, either of these ap-
proaches requires tedious conﬁgurations in windows or an-
chors. In this paper, taking pedestrian detection as an ex-
ample, we provide a new perspective where detecting ob-
jects is motivated as a high-level semantic feature detec-
tion task. Like edges, corners, blobs and other feature de-
tectors, the proposed detector scans for feature points all
over the image, for which the convolution is naturally suit-
ed. However, unlike these traditional low-level features, the
proposed detector goes for a higher-level abstraction, that
is, we are looking for central points where there are pedes-
trians, and modern deep models are already capable of such
a high-level semantic abstraction. Besides, like blob de-
tection, we also predict the scales of the pedestrian points,
which is also a straightforward convolution. Therefore, in
this paper, pedestrian detection is simpliﬁed as a straight-
forward center and scale prediction task through convolu-
tions. This way, the proposed method enjoys an anchor-
free setting. Though structurally simple, it presents compet-
itive accuracy and good speed on challenging pedestrian
detection benchmarks, and hence leading to a new attrac-
tive pedestrian detector. Code and models will be available
at https://github.com/liuwei16/CSP.

1. Introduction

Feature detection is one of the most fundamental prob-
lems in computer vision. It is usually viewed as a low-level
technique, with typical tasks including edge detection (e.g.

∗Wei Liu ﬁnished his part of work during his visit in CASIA.
†Shengcai Liao is the corresponding author. He was previously in CA-

SIA.

Figure 1. The overall pipeline of the proposed CSP detector. The
ﬁnal convolutions have two channels, one is a heatmap indicat-
ing the locations of the centers (red dots), and the other serves to
predict the scales (yellow dotted lines) for each detected center.

Canny [4], Sobel [41]), corner (or interest point) detection
(e.g. SUSAN [40], FAST [37]), and blob (or region of inter-
est point) detection (e.g. LoG [25], DoG [31], MSER [33]).
Feature detection is of vital importance to a variety of com-
puter vision tasks ranging from image representation, image
matching to 3D scene reconstruction, to name a few.

Generally speaking, a feature is deﬁned as an ”interest-
ing” part of an image, and so feature detection aims to com-
pute abstractions of image information and make local deci-
sions at every image point whether there is an image feature
of a given type at that point or not [1]. Regarding abstrac-
tion of image information, with the rapid development for
computer vision tasks, deep convolutional neural networks
(CNN) are believed to be of very good capability to learn
high-level image abstractions. Therefore, it has also been
applied for feature detection, and demonstrates attractive
successes even in low-level feature detections. For example,
there is a recent trend of using CNN to perform edge detec-
tion [39, 47, 2, 29], which has substantially advanced this
ﬁeld. It shows that clean and continuous edges can be ob-
tained by deep convolutions, which indicates that CNN has
a stronger capability to learn higher-level abstraction of nat-
ural images than traditional methods. This capability may

15187

not be limited to low-level feature detection; it may open up
many other possibilities of high-level feature detection.

Therefore, in this paper, taking pedestrian detection as an
example, we provide a new perspective where detecting ob-
jects is motivated as a high-level semantic feature detection
task. Like edges, corners, blobs and other feature detectors,
the proposed detector scans for feature points all over the
image, for which the convolution is naturally suited. How-
ever, unlike these traditional low-level feature detectors, the
proposed detector goes for a higher-level abstraction, that
is, we are looking for central points where there are pedes-
trians. Besides, similar to the blob detection, we also pre-
dict the scales of the pedestrian points. However, instead of
processing an image pyramid to determine the scale as in
traditional blob detection, we predict object scale with also
a straightforward convolution in one pass upon a fully con-
volution network (FCN) [30], considering its strong capa-
bility. As a result, pedestrian detection is simply formulat-
ed as a straightforward center and scale prediction task via
convolution. The overall pipeline of the proposed method,
denoted as Center and Scale Prediction (CSP) based detec-
tor, is illustrated in Fig. 1.

As for general object detection, starting from the pioneer
work of the Viola-Jones detector [45], it generally requires
sliding-window classiﬁers in tradition or anchor-based pre-
dictions in CNN-based methods. These detectors are es-
sentially local classiﬁers used to judge the pre-deﬁned win-
dows or anchors as being objects or not. However, either
of these approaches requires tedious conﬁgurations in win-
dows or anchors. Generally speaking, object detection is to
tell where the object is, and how big it is. Traditional meth-
ods combines the ”where” and ”how” subproblems into a
single one through the overall judgement of various scales
of windows or anchors. In contrast, the proposed CSP de-
tector separates the ”where” and ”how” subproblems into
two different convolutions. This makes detection a more
natural way, and enjoys a window-free or anchor-free set-
ting, signiﬁcantly reducing the difﬁculty in training.

There is another line of research which inspires us a lot.
Previously, FCN has already been applied to and made a
success in multi-person pose estimation [5, 34], where sev-
eral keypoints are ﬁrstly detected merely through responses
of full convolutions, and then they are further grouped in-
to complete poses of individual persons. In view of this,
recently two inspirational works, CornerNet [18] and TLL
[42], successfully go free from windows and anchors, which
perform object detection as convolutional keypoint detec-
tions and their associations. Though the keypoint associ-
ation require additional computations, sometimes complex
as in TLL, the keypoint prediction by FCN inspires us to go
a step further, achieving center and scale prediction based
pedestrian detection in full convolutions.

In summary, the main contributions of this work are as

follows: (i) We show a new possibility that pedestrian de-
tection can be simpliﬁed as a straightforward center and s-
cale prediction task through convolutions, which bypasses
the limitations of anchor-based detectors and gets rid of the
complex post-processing of recent keypoint pairing based
detectors. (ii) The proposed CSP detector achieves the new
state-of-the-art performance on two challenging pedestrian
detection benchmarks, CityPersons [51] and Caltech [9].

2. Related Works

2.1. Anchor based object detection

One key component of anchor-based detectors is the an-
chor boxes of pre-deﬁned scales and aspect ratios. In this
way, detection is performed by classifying and regressing
these anchor boxes. Faster R-CNN [36] is known as a
two-stage detector, which generates objectness proposal-
s and further classify and reﬁne these proposals in a sin-
gle framework. In contrast, singe-stage detectors, popular-
ized by SSD [27], remove the proposal generation step and
achieve comparable accuracy while are more efﬁcient than
two-stage detectors. In terms of pedestrian detection, Faster
R-CNN has become the predominant framework. For ex-
ample, RPN+BF [48] adapts the RPN and re-scores these
proposals via boosted forests. MS-CNN [3] also applies the
Faster R-CNN framework but generates proposals on multi-
scale feature maps. Zhang et al. [51] contribute ﬁve strate-
gies to adapt the plain Faster R-CNN for pedestrian detec-
tion. RepLoss [46] and OR-CNN [52] design two novel re-
gression losses to tackle the occluded pedestrian detection
in crowded scenes. Bi-Box [53] proposes an auxiliary sub-
network to predict the visible part of a pedestrian instance.
Most recently, single-stage detectors also present competi-
tive performance. For example, ALFNet [28] proposes the
asymptotic localization ﬁtting strategy to evolve the default
anchor boxes step by step into precise detection results, and
[21] focuses on the discriminative feature learning based on
the original SSD architecture.

2.2. Anchor free object detection

Anchor-free detectors bypass the requirement of anchor
boxes and detect objects directly from an image. DeNet [44]
proposes to generate proposals by predict the conﬁdence of
each location belonging to four corners of objects. Follow-
ing the two-stage pipeline, DeNet also appends another sub-
network to re-score these proposals. Within the single-stage
framework, YOLO [35] appends fully-connected layers to
parse the ﬁnal feature maps of a network into class conﬁ-
dence scores and box coordinates. Densebox [14] devises a
uniﬁed FCN that directly regresses the classiﬁcation scores
and distances to the boundary of a ground truth box on all
pixels, and demonstrates improved performance with land-
mark localization via multi-task learning. Most recently,

25188

CornerNet [18] also applies a FCN but to predict objects’
top-left and bottom-right corners and then group them vi-
a associative embedding [34]. Enhanced by the novel cor-
ner pooling layer, CornerNet achieves superior performance
on MS COCO object detection benchmark [22]. Similar-
ly, TLL [42] proposes to detect an object by predicting the
top and bottom vertexes. To group these paired keypoints
into individual instances, it also predicts the link edge be-
tween them and employs a post-processing scheme based
on Markov Random Field. Applying on pedestrian detec-
tion, TLL achieves signiﬁcant improvement on Caltech [9],
especially for small-scale pedestrians.

Our work also falls in the anchor-free object detection,
but with signiﬁcant differences to all above methods. We
try to answer to what extent a single FCN can be simpli-
ﬁed for pedestrian detection, and demonstrate that a single
center point is feasible for object localization. Along with
the scale prediction, CSP is able to generate bounding boxes
without any requirements of extra post-processing schemes
except the Non-Maximum Suppression (NMS).

2.3. Feature detection

Feature detection is a long-standing problem in com-
puter vision with extensive literatures. Generally speak-
ing, it mainly includes edge detection [4, 41], corner de-
tection [37, 38], blob detection [33, 7] and so on. Tradi-
tional leading methods [4, 41] mainly focus on the utiliza-
tion of local cues, such as brightness, colors, gradients and
textures. With the development of CNN, a series of CNN-
based method are proposed that signiﬁcantly push forward
the state of the arts in the task of feature detection. For ex-
ample, there is a recent trend of using CNN to perform edge
detection [39, 47, 2, 29], which have substantially advanced
this ﬁeld. However, different from these low-level feature
points like edge, corners and blobs, the proposed method
goes for a higher-level abstraction task, that is, we focus
on detecting central points where there are pedestrians, for
which modern deep models are already capable of.

3. Proposed Method

3.1. Preliminary

The CNN-based object detectors often rely on a back-
bone network (e.g. ResNet [12]). Taking an image I as
input, the network may generate several feature maps with
different resolutions, which can be deﬁned as follows:

φi = fi(φi−1) = fi(fi−1(...f2(f1(I)))),

(1)

where φi represents feature maps output by the ith lay-
er. These feature maps decrease in size progressively and
are generated by fi(.), which may be a combination of
convolution or pooling, etc. Given a network with N
layers, all the generated feature maps can be denoted as

Φ = {φ1, φ2, ..., φN }, which is further utilized by detec-
tion heads.

Generally speaking, the CNN-based object detectors d-
iffer in how to utilize Φ. We denote these feature maps that
are responsible for detection as Φdet.
In RPN [36], only
the ﬁnal feature map φN is used to perform detection, thus
the ﬁnal set of feature maps for detection is Φdet = {φN }.
While in SSD [27], the detection feature maps can be repre-
sented as Φdet = {φL, φL+1, ..., φN }, where 1 < L < N .
Further, in order to enrich the semantic information of shal-
lower layers for detecting small-scale objects, FPN [23]
and DSSD [10] utilize the lateral connection to combine
feature maps of different resolutions, resulting in Φdet =
i(i = L, L+1, ...N ) is a com-
{φ
bination of φi(i = L, L + 1, ...N ).

N }, where φ

L+1, ..., φ

L, φ

′

′

′

′

Besides Φdet, in anchor-based detectors, another key
component is called anchor boxes (denoted as B). Given
Φdet and B in hand, detection can be formulated as:

Dets = H(Φdet, B)

= {cls(Φdet, B), regr(Φdet, B)},

(2)

where B is pre-deﬁned according to the corresponding set of
feature maps Φdet, and H(.) represents the detection head.
Generally, H(.) contains two elements, namely cls(.) which
predicts the classiﬁcation scores, and regr(.) which predict-
s the scaling and offsets of the anchor boxes.

While in anchor-free detectors, detection is performed

merely on the set of feature maps Φdet, that is,

Dets = H(Φdet)

(3)

3.2. Overall architecture

The overall architecture of the proposed CSP detector is
illustrated in Fig. 2. The backbone network are truncated
from a standard network pretrained on ImageNet [8] (e.g.
ResNet-50 [12] and MobileNet [13]).

Feature Extraction. Taking ResNet-50 as an example,
its Conv layers can be divided into ﬁve stages, in which the
output feature maps are downsampled by 2, 4, 8, 16, 32
w.r.t. the input image. As a common practice [46, 42], the
dilated convolutions are adopted in stage 5 to keep its out-
put as 1/16 of the input image size. We denote the output
of stage 2, 3, 4 and 5 as φ2, φ3, φ4 and φ5, in which the
shallower feature maps can provide more precise localiza-
tion information, while the coarser ones contain more se-
mantic information with increasing the sizes of receptive
ﬁelds. Therefore, we fuse these multi-scale feature maps
from each stage into a single one in a simple way, that is,
a deconvolution layer is adopted to make multi-scale fea-
ture maps with the same resolution before concatenation.
Since the feature maps from each stage have different s-
cales, we use L2-normalization to rescale their norms to 10,

35189

Figure 2. Overall architecture of CSP, which mainly comprises two components, i.e. the feature extraction module and the detection head.
The feature extraction module concatenates feature maps of different resolutions into a single one. The detection head merely contains a
3x3 convolutional layer, followed by two prediction layers, one for the center location and the other for the corresponding scale.

which is similar to [21]. To investigate the optimal com-
bination from these multi-scale feature maps, we conduc-
t an ablative experiment in Sec. 4.2 and demonstrate that
Φdet = {φ3, φ4, φ5} is the best choice. Given an input im-
age of size H × W , the size of ﬁnal concatenated feature
maps is H/r × W/r, where r is the downsampling fac-
tor. Similarly to [42], r = 4 gives the best performance as
demonstrated in our experiments, because a larger r means
coarser feature maps which struggle on accurate localiza-
tion, while a smaller r brings more computational burden-
s. Note that more complicated feature fusion strategies like
[23, 15, 17] can be explored to further improve the detection
performance, but it is not in the scope of this work.

Detection Head. Upon the concatenated feature maps
Φdet, a detection head is appended to parse it into detection
results. As stated in [26], the detection head plays a signif-
icant role in top performance, which has been extensively
explored in the literature [10, 26, 20, 19]. In this work, we
ﬁrstly attach a single 3x3 Conv layer on Φdet to reduce its
channel dimensions to 256, and then two sibling 1x1 Conv
layers are appended to produce the center heatmap and s-
cale map, respectively. Also, we do this for simplicity and
any improvement of the detection head [10, 26, 20, 19] can
be ﬂexibly incorporate into this work to be a better detector.
A drawback from the downsampled feature maps is the
problem of poor localization. Optionally, to slightly adjust
the center location, an extra offset prediction branch can be
appended in parallel with the above two branches.

3.3. Training

Ground Truth. The predicted heatmaps are with the
same size as the concatenated feature maps (i.e. H/r ×
W/r). Given the bounding box annotations, we can gen-
erate the center and scale ground truth automatically. An
illustration example is depicted in Fig. 3 (b). For the cen-

Figure 3. (a) is the bounding box annotations commonly adopt-
ed by anchor-based detectors. (b) is the center and scale ground
truth generated automatically from (a). Locations of all objects’
center points are assigned as positives, and negatives otherwise.
Each pixel is assigned a scale value of the corresponding object
if it is a positive point, or 0 otherwise. We only show the height
information of the two positives for clarity. (c) is the overall Gaus-
sian mask map M deﬁned in Eq.4 to reduce the ambiguity of these
negatives surrounding the positives.

ter ground truth, the location where an object’s center point
falls is assigned as positive while all others are negatives.

Scale can be deﬁned as the height and/or width of object-
s. Towards high-quality ground truth for pedestrian detec-
tion, line annotation is ﬁrst proposed in [50, 51], where tight
bounding boxes are automatically generated with a uniform
aspect ratio of 0.41. In accordance to this annotation, we
can merely predict the height of each object and generate
the bounding box with the predetermined aspect ratio. For
the scale ground truth, the kth positive location is assigned
with the value of log(hk) corresponding to the kth objec-
t. To reduce the ambiguity, log(hk) is also assigned to the
negatives within a radius 2 of the positives, while all other
locations are assigned as zeros. Alternatively, we can al-
so predict the width or height+width but with slightly poor
performance for pedestrian detection as demonstrated in our

45190

experiments (Sec. 4.2).

r ⌋).

r ⌋, yk

r − ⌊ yk

When the offset prediction branch is appended,

the
ground truth for the offsets of those centers can be deﬁned
as ( xk

r − ⌊ xk
Loss Function. For the center prediction branch, we for-
mulate it as a classiﬁcation task via the cross-entropy loss.
Note that it is difﬁcult to decide an ’exact’ center point, thus
the hard-designation of positives and negatives brings more
difﬁculties for training.
In order to reduce the ambiguity
of these negatives surrounding the positives, we also apply
a 2D Gaussian mask G(.) centered at the location of each
positive. An illustration example of the overall mask map
M is depicted in Fig. 3 (c). Formally, it is formulated as:

Mij = max

k=1,2,...,K

G(i, j; xk, yk, σwk , σhk ),

G(i, j; x, y, σw, σh) = e

−( (i−x)2

2σ2
w

+ (j−y)2
2σ2
h

)

,

(4)

the number of objects

where K is
in an image,
(xk, yk, wk, hk) is the center coordinates, width and height
of the kth object, and the variances (σk
h) of the Gaus-
sian mask are proportional to the height and width of indi-
vidual objects. If these masks have overlaps, we choose the
maximum values for the overlapped locations. To combat
the extreme positive-negative imbalance problem, the focal
weights [24] on hard examples are also adopted. Thus, the
classiﬁcation loss can be formulated as:

w, σk

W/r

H/r

Lcenter = −

αij(1 − ˆpij)γlog(ˆpij),

(5)

∑j=1

where

1
K

∑i=1
ˆpij ={pij
αij ={1

In the above, pij ∈ [0, 1] is the network’s estimated prob-
ability indicating whether there is an object’s center or not
in the location (i, j), and yij ∈ {0, 1} speciﬁes the ground
truth label, where yij = 1 represents the positive location.
αij and γ are the focusing hyper-parameters, we experimen-
tally set γ = 2 as suggested in [24]. To reduce the ambigu-
ity from those negatives surrounding the positives, the αij
according to the Gaussian mask M is applied to reduce their
contributions to the total loss, in which the hyper-parameter
β controls the penalty. Experimentally, β = 4 gives the
best performance, which is similar to the one in [18]. For
positives, αij is set as 1.

For scale prediction, we formulate it as a regression task

via the smooth L1 loss [11]:

Lscale =

1
K

K

∑k=1

SmoothL1(sk, tk),

(7)

where sk and tk represents the network’s prediction and the
ground truth of each positive, respectively.

If the offset prediction branch is appended, the similar

smooth L1 loss in Eq. 7 is adopted (denoted as Lof f set).

To sum up, the full optimization objective is:

L = λcLcenter + λsLscale + λoLof f set,

(8)

where λc, λs and λo are the weights for center classiﬁca-
tion, scale regression and offset regression losses, which are
experimentally set as 0.01, 1 and 0.1, respectively.

Data Augmentation. To increase the diversity of the
training data, standard data augmentation techniques are
adopted. Firstly, random color distortion and horizontal ﬂip
are applied, followed by randomly scaled in the range of
[0.4, 1.5]. Secondly, a patch is cropped or expanded by
zero-padding such that the shorter side has a ﬁxed number
of pixels (640 for CityPersons [51], and 336 for Caltech
[9]). Note that the aspect ratio of the image is kept during
this process.

3.4. Inference

During testing, CSP simply involves a single forward of
FCN with several predictions. Speciﬁcally, locations with
conﬁdence score above 0.01 in the center heatmap are kept,
along with their corresponding scale in the scale map. Then
bounding boxes are generated automatically and remapped
to the original image size, followed by NMS with a thresh-
old of 0.5. If the offset prediction branch is appended, the
centers are adjusted accordingly before remapping.

Datasets. To demonstrate the effectiveness of the pro-
posed method, we evaluate on two of the largest pedestri-
an detection benchmarks, i.e. Caltech [9] and CityPerson-
s [51]. Caltech comprises approximately 2.5 hours of au-
todriving video with extensively labelled bounding boxes.
Following [51, 32, 46, 28, 52], we use the training data aug-
mented by 10 folds (42782 frames) and test on the 4024
frames in the standard test set, all experiments are conduct-
ed on the new annotations provided by [49]. CityPersons is
a more challenging large-scale pedestrian detection dataset
with various occlusion levels. We train the models on the
ofﬁcial training set with 2975 images and test on the valida-
tion set with 500 images.

One reason we choose these two datasets lies in that they
provide bounding boxes via central body line annotation
and normalized aspect ratio, this annotation procedure is
helpful to ensure the boxes align well with the centers of
pedestrians. Evaluation follows the standard Caltech eval-
uation metric [9], that is log-average Miss Rate over False
Positive Per Image (FPPI) ranging in [10−2, 100] (denoted

55191

if yij = 1
otherwise,

4. Experiments

4.1. Experiment settings

1 − pij

(1 − Mij)β

(6)

if yij = 1
otherwise.

Point

M R−2(%)

Prediction

IoU=0.5

IoU=0.75

Center point
Top vertex

Bottom vertex

4.62
7.75
6.52

36.47
44.70
40.25

Table 1. Comparisons of different high-level feature points. Bold
number indicates the best result.

Scale

M R−2(%)

Prediction

IoU=0.5

IoU=0.75

Height
Width

Height+Width

4.62
5.31
4.73

36.47
53.06
41.09

Table 2. Comparisons of different deﬁnitions for scale prediction.
Bold number indicates the best result.

Disturbance (pixels) M R−2(%) ∆M R−2(%)

0

[0, 4]
[0, 8]

4.62
5.68
8.59

-

↓ 1.06
↓ 3.97

Table 6. Performance drop with disturbances of the centers.

as M R−2). Tests are only applied on the original image
size without enlarging for speed consideration.

Training details. We implement the proposed method
in Keras [6]. The backbone is ResNet-50 [12] pretrained
on ImageNet [8] unless otherwise stated. Adam [16] is ap-
plied to optimize the network. We also apply the strate-
gy of moving average weights proposed in [43] to achieve
more stable training. For Caltech [9], a mini-batch con-
tains 16 images with one GPU (GTX 1080Ti), the learn-
ing rate is set as 10−4 and training is stopped after 15K
iterations. Following[51, 46, 28, 52], we also include ex-
periments with the model initialized from CityPersons [51],
which is trained with the learning rate of 2 × 10−5. For C-
ityPersons [51], we optimize the network on 4 GPUs with 2
images per GPU for a mini-batch, the learning rate is set as
2 × 10−4 and training is stopped after 37.5K iterations.

4.2. Ablation Study

In this section, an ablative analysis of the proposed
method is conducted on the Caltech dataset, evaluations are
based on the new annotations provided by [49].

Why is the Center Point? As a kind of high-level fea-
ture point, the center point is capable of locating an indi-
vidual object. A question comes in that how about other
high-level feature points. To answer this, we choose two
other high-level feature points as adopted in [42], i.e.
the
top and bottom vertexes. Comparisons are reported in Ta-
ble. 1. It is shown that both the two vertexes can succeed

in detection but underperform the center point by approxi-
mately 2%-3% under IoU=0.5, and the performance gap is
even larger under the stricter IoU=0.75. This is probably
because the center point is advantageous to perceive the full
body information and thus is easier for training.

How important is the Scale Prediction? Scale predic-
tion is another indespensible component for bounding box
generation.
In practice, we merely predict the height for
each detected center in accordance to the line annotation in
[50, 51]. To demonstrate the generality of CSP, we have al-
so tried to predict Width or Height+Width for comparison.
For Height+Width, the only difference in network architec-
ture lies in that the scale prediction branch has two channel-
s responsible for the height and width respectively. It can
be observed in Table 2 that Width and Height+Width pre-
diction can also achieve comparable but suboptimal results
to Height prediction. This result may be attributed to the
line annotation adopted in [50, 51] which provides accurate
height information with less noise during training. Besides,
the ground truth for width is automatically generated by the
annotated height information, thus is not able to provide
additional information for training. With the comparable
performance from Height +Width prediction, it makes CSP
potentially feasible for other object detection tasks requir-
ing both height and width.

How important is the Feature Resolution? In the
proposed method, the ﬁnal set of feature maps (denoted
as Φr
det) is downsampled by r w.r.t the input image. To
explore the inﬂuence from r, we train the models with
r = 2, 4, 8, 16 respectively. For r = 2, Φ2
det are up-
sampled from Φ4
det by deconvolution. To remedy the is-
sue of poor localization from downsampling, the offset pre-
diction branch is alternatively appended for r = 4, 8, 16
to adjust the center location. Evaluations under IoU=0.75
are included to verify the effectiveness of additional offset
prediction when stricter localization quality is required. As
can be seen from Table. 3, without offset prediction, Φ4
det
presents the best result under IoU=0.5, but performs poor-
ly under IoU=0.75 when compared with Φ2
det, which indi-
cates that ﬁner feature maps are beneﬁcial for precise lo-
calization. Though Φ2
det performs the best under IoU=0.75,
it does not bring performance gain under IoU=0.5 though
with more computational burdens. Not surprisingly, a larger
r witnesses a signiﬁcant performance drop, which is mainly
due to that coarser feature maps lead to poor localization.
In this case, the offset prediction plays a signiﬁcant role.
Notably, additional offset prediction can substantially im-
prove the detector upon Φ16
det by 12.86% and 41.30% under
the IoU threshold of 0.5 and 0.75, respectively. It can al-
so achieve an improvement of 7.67% under IoU=0.75 for
the detector upon Φ4
det, even though the performance gain
is saturating under IoU=0.5. It is worth noting that the ex-
tra computation cost from the offset prediction is negligible,

65192

Feature for
Detection

+Offset

Test Time
(ms/img)

M R−2(%)

∆M R−2(%)

IoU=0.5

IoU=0.75

IoU=0.5

IoU=0.75

Φ2

det

Φ4

det

Φ8

det

Φ16
det

69.8
58.2
59.6
49.2
50.4
42.0
42.7

5.32
4.62
4.54
7.00
6.08
20.27
7.41

30.08
36.47
28.80
54.25
32.93
75.17
33.87

-

-

+0.08

+7.67

+0.92

+21.32

+12.86

+41.30

X

X

X

Table 3. Comparisons of different downsampling factors of the feature maps, which are denoted as Φr
det downsampled by r w.r.t the input
image. Test time is evaluated on the image with size of 480x640 pixels. ∆M R−2 means the improvement from the utilization of the offset
prediction. Bold numbers indicate the best result.

Feature Maps

ResNet-50[12]

MobileNetV1[13]

φ3
φ2
X X

φ4

φ5

# Parameters

X X

X X

X X X

X X X

X X X X

4.7MB
16.1MB
37.4MB
16.7MB
40.0MB
40.6MB

Test Time M R−2(%)
36.2ms/img
44.5ms/img
54.4ms/img
46.0ms/img
58.2ms/img
61.1ms/img

9.96
5.68
5.84
6.34
4.62
4.99

# Parameters

2.1MB
6.0MB
10.7MB
6.3MB
12.3MB
12.6MB

Test Time M R−2(%)
27.3ms/img
32.3ms/img
34.5ms/img
33.3ms/img
38.2ms/img
40.5ms/img

34.96
8.33
10.03
8.43
9.59
9.05

Table 4. Comparisons of different combinations of multi-scale feature representations deﬁned in Sec. 3.2. φ2, φ3, φ4 and φ5 represent the
output of stage 2, 3, 4 and 5 of a backbone network, respectively. Bold numbers indicate the best results.

with approximately 1ms per image of 480x640 pixels.

How important is the Feature Combination? It has
been revealed in [42] that multi-scale representation is vi-
tal for pedestrian detection of various scales. In this part,
we conduct an ablative experiment to study which combi-
nation of the multi-scale feature maps from the backbone is
the optimal one. As the much lower layer has limited dis-
criminant information, in practice we choose the output of
stage 2 (φ2) as a start point and the downsampling factor r
is ﬁxed as 4. In spite of the ResNet-50[12] with stronger
feature representation, we also choose a light-weight net-
work like MobileNetV1[13] as the backbone. The results in
Table 4 shows that the much shallower feature maps like φ2
result in poorer accuracy, while deeper feature maps like φ4
and φ5 are of great importance for superior performance,
and the middle-level feature maps φ3 are indispensable to
achieve the best results. For ResNet-50, the best perfor-
mance comes from the combination of {φ3, φ4, φ5}, while
{φ3, φ4} is the optimal one for MobileNetV1.

4.3. Comparison with the State of the Arts

Caltech. The proposed method are extensively com-
pared with the state of the arts on three settings: Reason-
able, All and Heavy Occlusion. As shown in Fig. 4, CSP
achieves M R−2 of 4.5% on the Reasonable setting, which
outperforms the best competitor (5.0 of RepLoss [46]) by
0.4%. When the model is initialized from CityPersons[51],

CSP also achieves a new state of the art of 3.8%, compared
to 4.0% of RepLoss [46], 4.1% of OR-CNN [52], and 4.5%
of ALFNet [28].
It presents the superiority on detecting
pedestrians of various scales and occlusion levels as demon-
strated in Fig . 4 (b). Moreover, Fig. 4 (c) shows that CSP
also performs very well for heavily occluded pedestrians,
outperforming RepLoss [46] and OR-CNN [52] which are
explicitly designed for occlusion cases.

CityPersons. Table 5 shows the comparisons with previ-
ous state of the arts on CityPersons. Besides the reasonable
subset, following [46], we also evaluate on three subsets
with different occlusion levels, and following [51], results
on three subsets with various scale ranges are also includ-
ed. It can be observed that CSP beats the competitors and
performs fairly well on occlusion cases even without any
speciﬁc occlusion-handling strategies [46, 52]. On the Rea-
sonable subset, CSP with offset prediction achieves the best
performance, with a gain of 1.0% M R−2 upon the closest
competitor (ALFNet [28]), while the speed is comparable
on the same running environment with 0.33 second per im-
age of 1024x2048 pixels.

4.4. Discussions

Note that CSP only requires object centers and scales for
training, though generating them from bounding box or cen-
tral line annotations is more feasible since centers are not al-
ways easy to annotate. Besides, the model may be puzzled

75193

Figure 4. Comparisons with the state of the arts on Caltech using new annotations.

Method

Backbone

Reasonable Heavy

Partial Bare

Small Medium Large Test Time

FRCNN[51]

FRCNN+Seg[51]

OR-CNN[52]
RepLoss[46]

TLL[42]

TLL+MRF[42]

ALFNet[28]

CSP(w/o offset)
CSP(with offset)

VGG-16
VGG-16
VGG-16
ResNet-50
ResNet-50
ResNet-50
ResNet-50
ResNet-50
ResNet-50

15.4
14.8
12.8
13.2
15.5
14.4
12.0
11.4
11.0

-
-

55.7
56.9
53.6
52.0
51.9
49.9
49.3

-
-

15.3
16.8
17.2
15.9
11.4
10.8
10.4

-
-

6.7
7.6
10.0
9.2
8.4
8.1
7.3

25.6
22.6

-
-
-
-

19.0
18.2
16.0

7.2
6.7

-
-
-
-

5.7
3.9
3.7

7.9
8.0

-
-
-
-

6.6
6.0
6.5

-
-
-
-
-
-

0.27s/img
0.33s/img
0.33s/img

Table 5. Comparison with the state of the arts on CityPersons[51]. Results test on the original image size (1024x2048 pixels) are reported.
Red and green indicate the best and second best performance.

on ambiguous centers during training. To demonstrate this,
we randomly disturbed object centers in the range of [0,4]
and [0,8] pixels during training. From the results shown
in Table 6, it can be seen that performance drops with in-
creasing annotation noise. For Caltech, we also apply the
original annotations but with inferior performance to TLL
[42], which is also anchor-free. A possible reason is that
TLL includes a series of post-processing strategies in key-
point pairing. For evaluation with tight annotations based
on central lines, as results of TLL on Caltech are not report-
ed in [42], comparison to TLL is given in Table 5 on the C-
ityPersons, which shows the superiority of CSP. Therefore,
the proposed method may be limited for annotations with
ambiguous centers, e.g. the traditional pedestrian bounding
box annotations affected by limbs. In view of this, it may
also be not straightforward to apply CSP to generic object
detection without further improvement or new annotations.

When compared with anchor-based methods, the advan-
tage of CSP lies in two aspects. Firstly, CSP does not re-
quire tedious conﬁgurations on anchors speciﬁcally for each
dataset. Secondly, anchor-based methods detect objects by
overall classiﬁcations of each anchor where background in-
formation and occlusions are also included and will confuse
the detector’s training. However, CSP overcomes this draw-

back by scanning for pedestrian centers instead of boxes in
an image, thus is more robust to occluded objects.

5. Conclusion

Inspired from the traditional feature detection task, we
provide a new perspective where pedestrian detection is
motivated as a high-level semantic feature detection task
through straightforward convolutions for center and scale
predictions. This way, the proposed method enjoys anchor-
free settings and is also free from complex post-processing
strategies as in recent keypoint-pairing based detectors. As
a result, the proposed CSP detector achieves the new state-
of-the-art performance on two challenging pedestrian de-
tection benchmarks, namely CityPersons and Caltech. Due
to the general structure of the CSP detector, it is interest-
ing to further explore its capability in other tasks like face
detection, vehicle detection, and general object detection.

Acknowledgment

This work was partly supported by the Nation-
al Key Research
(Grant
No.2016YFC0801003),
the NSFC Project #61672521,
the NLPR Independent Research Project #Z-2018008, and
the IIAI ﬁnancial support.

and Development Plan

85194

References

[1] https://en.wikipedia.org/wiki/Feature

detection (computer vision).

[2] Gedas Bertasius,

Jianbo Shi, and Lorenzo Torresani.
Deepedge: A multi-scale bifurcated deep network for top-
down contour detection. In 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 4380–
4389. IEEE, 2015.

[3] Z. Cai, Q. Fan, R.S.Feris, and N. Vasconcelos. A uniﬁed
multi-scale deep convolutional neural network for fast ob-
ject detection. In European Conference on Computer Vision,
pages 354–370. Springer, 2016.

[4] John Canny. A computational approach to edge detection.
IEEE Transactions on pattern analysis and machine intelli-
gence, (6):679–698, 1986.

[5] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. arXiv preprint arXiv:1611.08050, 2016.

[6] Franois Chollet.

Keras. published on github (http-

s://github.com/fchollet/keras),. 2015.

[7] Hongli Deng, Wei Zhang, Eric Mortensen, Thomas Diet-
terich, and Linda Shapiro. Principal curvature-based region
detector for object recognition.
In Computer Vision and
Pattern Recognition, 2007. CVPR’07. IEEE Conference on,
pages 1–8. IEEE, 2007.

[8] J. Deng, W. Dong, R. Socher, L.-J Li, K. Li, and F.-F Li. Im-
agenet: A large-scale hierarchical image database. In Com-
puter Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pages 248–255. IEEE, 2009.

[9] Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Per-
ona. Pedestrian detection: An evaluation of the state of the
art. IEEE transactions on pattern analysis and machine in-
telligence, 34(4):743–761, 2012.

[10] Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi,
and Alexander C Berg. Dssd: Deconvolutional single shot
detector. arXiv preprint arXiv:1701.06659, 2017.

[11] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[14] Lichao Huang, Yi Yang, Yafeng Deng, and Yinan Yu. Dense-
box: Unifying landmark localization with end to end object
detection. arXiv preprint arXiv:1509.04874, 2015.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[17] Tao Kong, Fuchun Sun, Chuanqi Tan, Huaping Liu, and
Wenbing Huang. Deep feature pyramid reconﬁguration for
object detection. In The European Conference on Computer
Vision (ECCV), September 2018.

[18] Hei Law and Jia Deng. Cornernet: Detecting objects as
paired keypoints. In The European Conference on Computer
Vision (ECCV), September 2018.

[19] Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jian-
jun Qian, Jian Yang, Chengjie Wang, Jilin Li, and Feiyue
Huang. Dsfd: Dual shot face detector. arXiv preprint arX-
iv:1810.10220, 2018.

[20] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yang-
dong Deng, and Jian Sun. Light-head r-cnn: In defense of
two-stage object detector. arXiv preprint arXiv:1711.07264,
2017.

[21] Chunze Lin,

Jiwen Lu, Gang Wang, and Jie Zhou.
Graininess-aware deep feature learning for pedestrian detec-
tion. In The European Conference on Computer Vision (EC-
CV), September 2018.

[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014.

[23] Y.-T Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
arXiv preprint arXiv:1612.03144, 2016.

[24] Y.-T Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Fo-
arXiv preprint arX-

cal loss for dense object detection.
iv:1708.02002, 2017.

[25] Tony Lindeberg. Scale selection properties of generalized
scale-space interest point detectors. Journal of Mathematical
Imaging and vision, 46(2):177–210, 2013.

[26] Songtao Liu, Di Huang, and andYunhong Wang. Receptive
ﬁeld block net for accurate and fast object detection. In The
European Conference on Computer Vision (ECCV), Septem-
ber 2018.

[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S.Reed, C.-
Y Fu, and A.C. Berg. Ssd: Single shot multibox detector.
In European conference on computer vision, pages 21–37.
Springer, 2016.

[28] Wei Liu, Shengcai Liao, Weidong Hu, Xuezhi Liang, and Xi-
ao Chen. Learning efﬁcient single-stage pedestrian detectors
by asymptotic localization ﬁtting. In The European Confer-
ence on Computer Vision (ECCV), September 2018.

[29] Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, and
Xiang Bai. Richer convolutional features for edge detection.
In Computer Vision and Pattern Recognition (CVPR), 2017
IEEE Conference on, pages 5872–5881. IEEE, 2017.

[15] Seung-Wook Kim, Hyong-Keun Kook, Jee-Young Sun,
Mun-Cheon Kang, and Sung-Jea Ko. Parallel feature pyra-
mid network for object detection. In The European Confer-
ence on Computer Vision (ECCV), September 2018.

[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 3431–3440, 2015.

95195

[31] David G Lowe. Distinctive image features from scale-
International journal of computer vi-

invariant keypoints.
sion, 60(2):91–110, 2004.

[46] Xinlong Wang, Tete Xiao, Yuning Jiang, Shuai Shao, Jian
Sun, and Chunhua Shen. Repulsion loss: Detecting pedestri-
ans in a crowd. arXiv preprint arXiv:1711.07752, 2017.

[47] Saining Xie and Zhuowen Tu. Holistically-nested edge de-
International Journal of Computer Vision, 125(1-

tection.
3):3–18, 2017.

[48] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He.
Is faster r-cnn doing well for pedestrian detection?
In
European Conference on Computer Vision, pages 443–457.
Springer, 2016.

[49] Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan
Hosang, and Bernt Schiele. How far are we from solving
pedestrian detection? In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pages
1259–1267, 2016.

[50] Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan
Hosang, and Bernt Schiele. Towards reaching human perfor-
mance in pedestrian detection. IEEE transactions on pattern
analysis and machine intelligence, 40(4):973–986, 2018.

[51] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. C-
itypersons: A diverse dataset for pedestrian detection. arXiv
preprint arXiv:1702.05693, 2017.

[52] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and S-
tan Z. Li. Occlusion-aware r-cnn: Detecting pedestrians in
a crowd. In The European Conference on Computer Vision
(ECCV), September 2018.

[53] Chunluan Zhou and Junsong Yuan. Bi-box regression for
In The Eu-
pedestrian detection and occlusion estimation.
ropean Conference on Computer Vision (ECCV), September
2018.

[32] Jiayuan Mao, Tete Xiao, Yuning Jiang, and Zhimin Cao.
What can help pedestrian detection? In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
volume 1, page 3, 2017.

[33] Jiri Matas, Ondrej Chum, Martin Urban, and Tom´as Pajd-
la. Robust wide-baseline stereo from maximally stable ex-
tremal regions.
Image and vision computing, 22(10):761–
767, 2004.

[34] Alejandro Newell, Zhiao Huang, and Jia Deng. Associa-
tive embedding: End-to-end learning for joint detection and
grouping.
In Advances in Neural Information Processing
Systems, pages 2277–2287, 2017.

[35] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 779–788, 2016.

[36] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015.

[37] Edward Rosten and Tom Drummond. Machine learning for
In European conference on

high-speed corner detection.
computer vision, pages 430–443. Springer, 2006.

[38] Edward Rosten, Reid Porter, and Tom Drummond. Faster
and better: A machine learning approach to corner detec-
tion.
IEEE transactions on pattern analysis and machine
intelligence, 32(1):105–119, 2010.

[39] Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, and Zhi-
jiang Zhang. Deepcontour: A deep convolutional feature
learned by positive-sharing loss for contour detection.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3982–3991, 2015.

[40] Stephen M Smith and J Michael Brady. Susan: a new ap-
proach to low level image processing. International journal
of computer vision, 23(1):45–78, 1997.

[41] Irwin Sobel. Camera models and machine perception. Tech-
nical report, Computer Science Department, Technion, 1972.

[42] Tao Song, Leiyu Sun, Di Xie, Haiming Sun, and Shiliang
Pu. Small-scale pedestrian detection based on topological
line localization and temporal feature aggregation.
In The
European Conference on Computer Vision (ECCV), Septem-
ber 2018.

[43] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In Advances in neural
information processing systems, pages 1195–1204, 2017.

[44] Lachlan Tychsen-Smith and Lars Petersson. Denet: Scalable
real-time object detection with directed sparse sampling. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 428–436, 2017.

[45] Paul Viola and Michael J Jones. Robust real-time face detec-
International journal of computer vision, 57(2):137–

tion.
154, 2004.

105196

