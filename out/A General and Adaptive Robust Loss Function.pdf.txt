A General and Adaptive Robust Loss Function

Jonathan T. Barron
Google Research

Abstract

We present a generalization of the Cauchy/Lorentzian,
Geman-McClure, Welsch/Leclerc, generalized Charbon-
nier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss func-
tions. By introducing robustness as a continuous param-
eter, our loss function allows algorithms built around ro-
bust loss minimization to be generalized, which improves
performance on basic vision tasks such as registration and
clustering.
Interpreting our loss as the negative log of a
univariate density yields a general probability distribution
that includes normal and Cauchy distributions as special
cases. This probabilistic interpretation enables the training
of neural networks in which the robustness of the loss auto-
matically adapts itself during training, which improves per-
formance on learning-based tasks such as generative im-
age synthesis and unsupervised monocular depth estima-
tion, without requiring any manual parameter tuning.

Many problems in statistics and optimization require ro-
bustness — that a model be less inﬂuenced by outliers than
by inliers [17, 19]. This idea is common in parameter es-
timation and learning tasks, where a robust loss (say, ab-
solute error) may be preferred over a non-robust loss (say,
squared error) due to its reduced sensitivity to large errors.
Researchers have developed various robust penalties with
particular properties, many of which are summarized well
in [3, 39]. In gradient descent or M-estimation [16] these
losses are often interchangeable, so researchers may exper-
iment with different losses when designing a system. This
ﬂexibility in shaping a loss function may be useful because
of non-Gaussian noise, or simply because the loss that is
minimized during learning or parameter estimation is dif-
ferent from how the resulting learned model or estimated
parameters will be evaluated. For example, one might train
a neural network by minimizing the difference between the
network’s output and a set of images, but evaluate that net-
work in terms of how well it hallucinates random images.

In this paper we present a single loss function that is a
superset of many common robust loss functions. A single
continuous-valued parameter in our general loss function
can be set such that it is equal to several traditional losses,

and can be adjusted to model a wider family of functions.
This allows us to generalize algorithms built around a ﬁxed
robust loss with a new “robustness” hyperparameter that can
be tuned or annealed to improve performance.

Though new hyperparameters may be valuable to a prac-
titioner, they complicate experimentation by requiring man-
ual tuning or time-consuming cross-validation. However,
by viewing our general loss function as the negative log-
likelihood of a probability distribution, and by treating the
robustness of that distribution as a latent variable, we show
that maximizing the likelihood of that distribution allows
gradient-based optimization frameworks to automatically
determine how robust the loss should be without any manual
parameter tuning. This “adaptive” form of our loss is par-
ticularly effective in models with multivariate output spaces
(say, image generation or depth estimation) as we can intro-
duce independent robustness variables for each dimension
in the output and thereby allow the model to independently
adapt the robustness of its loss in each dimension.

The rest of the paper is as follows: In Section 1 we de-
ﬁne our general loss function, relate it to existing losses,
and enumerate some of its useful properties.
In Sec-
tion 2 we use our loss to construct a probability distri-
bution, which requires deriving a partition function and a
sampling procedure. Section 3 discusses four representa-
tive experiments:
In Sections 3.1 and 3.2 we take two

Figure 1. Our general loss function (left) and its gradient (right)
for different values of its shape parameter α. Several values of α
reproduce existing loss functions: L2 loss (α = 2), Charbonnier
loss (α = 1), Cauchy loss (α = 0), Geman-McClure loss (α =
−2), and Welsch loss (α = −∞).

14331

vision-oriented deep learning models (variational autoen-
coders for image synthesis and self-supervised monocular
depth estimation), replace their losses with the negative log-
likelihood of our general distribution, and demonstrate that
allowing our distribution to automatically determine its own
robustness can improve performance without introducing
any additional manually-tuned hyperparameters.
In Sec-
tions 3.3 and 3.4 we use our loss function to generalize
algorithms for the classic vision tasks of registration and
clustering, and demonstrate the performance improvement
that can be achieved by introducing robustness as a hyper-
parameter that is annealed or manually tuned.

1. Loss Function

The simplest form of our loss function is:

α 
  (x/c)2
f (x, α, c) = |α − 2|

|α − 2|

α/2

+ 1!

− 1
 (1)

Here α ∈ R is a shape parameter that controls the robust-
ness of the loss and c > 0 is a scale parameter that controls
the size of the loss’s quadratic bowl near x = 0.

Though our loss is undeﬁned when α = 2, it approaches

L2 loss (squared error) in the limit:

lim
α→2

f (x, α, c) =

(x/c)2

1
2

When α = 1 our loss is a smoothed form of L1 loss:

f (x, 1, c) =p(x/c)2 + 1 − 1

This is often referred to as Charbonnier loss [5], pseudo-
Huber loss (as it resembles Huber loss [18]), or L1-L2 loss
[39] (as it behaves like L2 loss near the origin and like L1
loss elsewhere).

Our loss’s ability to express L2 and smoothed L1 losses
is shared by the “generalized Charbonnier” loss [34], which
has been used in ﬂow and depth estimation tasks that require
robustness [6, 23] and is commonly deﬁned as:

α/2

(4)

(cid:0)x2 + ǫ2(cid:1)

Our loss has signiﬁcantly more expressive power than the
generalized Charbonnier loss, which we can see by set-
ting our shape parameter α to nonpositive values. Though
f (x, 0, c) is undeﬁned, we can take the limit of f (x, α, c)
as α approaches zero:

lim
α→0

f (x, α, c) = log(cid:18) 1

2

(x/c)2 + 1(cid:19)

(5)

This yields Cauchy (aka Lorentzian) loss [2]. By setting
α = −2, our loss reproduces Geman-McClure loss [13]:

f (x,−2, c) =

2 (x/c)2
(x/c)2 + 4

(6)

(2)

(3)

In the limit as α approaches negative inﬁnity, our loss be-
comes Welsch [20] (aka Leclerc [25]) loss:

lim

α→−∞

f (x, α, c) = 1 − exp(cid:18)−

1
2

(x/c)2(cid:19)

(7)

With this analysis we can present our ﬁnal loss function,
which is simply f (·) with special cases for its removable
singularities at α = 0 and α = 2 and its limit at α = −∞.

ρ (x, α, c) =

1

2 (x/c)2
log(cid:16) 1
2 (x/c)2 + 1(cid:17)
1 − exp(cid:16)− 1
2 (x/c)2(cid:17)
α (cid:18)(cid:16) (x/c)2
|α−2| + 1(cid:17)

|α−2|

α/2

if α = 2

if α = 0
if α = −∞
− 1(cid:19) otherwise




(8)
As we have shown,
this loss function is a superset of
the Welsch/Leclerc, Geman-McClure, Cauchy/Lorentzian,
generalized Charbonnier, Charbonnier/pseudo-Huber/L1-
L2, and L2 loss functions.

To enable gradient-based optimization we can derive the

derivative of ρ (x, α, c) with respect to x:

∂ρ
∂x

(x, α, c) =

x
c2

2x




x2+2c2
x

c2 exp(cid:16)− 1
c2 (cid:16) (x/c)2

2 (x/c)2(cid:17)
|α−2| + 1(cid:17)(α/2−1)

x

if α = 2
if α = 0
if α = −∞
otherwise

(9)
Our loss and its derivative are visualized for different values
of α in Figure 1.

The shape of the derivative gives some intuition as to
how α affects behavior when our loss is being minimized by
gradient descent or some related method. For all values of α
the derivative is approximately linear when |x| < c, so the
effect of a small residual is always linearly proportional to
that residual’s magnitude. If α = 2, the derivative’s magni-
tude stays linearly proportional to the residual’s magnitude
— a larger residual has a correspondingly larger effect. If
α = 1 the derivative’s magnitude saturates to a constant 1/c
as |x| grows larger than c, so as a residual increases its ef-
fect never decreases but never exceeds a ﬁxed amount. If
α < 1 the derivative’s magnitude begins to decrease as |x|
grows larger than c (in the language of M-estimation [16],
the derivative, aka “inﬂuence”, is “redescending”) so as the
residual of an outlier increases, that outlier has less effect
during gradient descent. The effect of an outlier diminishes
as α becomes more negative, and as α approaches −∞ an
outlier whose residual magnitude is larger than 3c is almost
completely ignored.

We can also reason about α in terms of averages. Be-
cause the empirical mean of a set of values minimizes total
squared error between the mean and the set, and the empir-
ical median similarly minimizes absolute error, minimizing

4332

our loss with α = 2 is equivalent to estimating a mean, and
with α = 1 is similar to estimating a median. Minimizing
our loss with α = −∞ is equivalent to local mode-ﬁnding
[35]. Values of α between these extents can be thought of
as smoothly interpolating between these three kinds of av-
erages during estimation.

Our loss function has several useful properties that we
will take advantage of. The loss is smooth (i.e., in C∞)
with respect to x, α, and c > 0, and is therefore well-suited
to gradient-based optimization over its input and its param-
eters. The loss is zero at the origin, and increases monoton-
ically with respect to |x|:
ρ (0, α, c) = 0

(10)

∂ρ
∂|x|

(x, α, c) ≥ 0

The loss is invariant to a simultaneous scaling of c and x:

∀k>0 ρ(kx, α, kc) = ρ(x, α, c)

The loss increases monotonically with respect to α:

∂ρ
∂α

(x, α, c) ≥ 0

(11)

(12)

This is convenient for graduated non-convexity [4]: we can
initialize α such that our loss is convex and then gradually
reduce α (and therefore reduce convexity and increase ro-
bustness) during optimization, thereby enabling robust esti-
mation that (often) avoids local minima.

We can take the limit of the loss as α approaches inﬁnity,

which due to Eq. 12 must be the upper bound of the loss:

ρ (x, α, c) ≤ lim

α→+∞

ρ (x, α, c) = exp(cid:18) 1

2

(x/c)2(cid:19) − 1

(13)
We can bound the magnitude of the gradient of the loss,
which allows us to better reason about exploding gradients:

∂ρ
∂x

(cid:12)(cid:12)(cid:12)(cid:12)

(x, α, c)(cid:12)(cid:12)(cid:12)(cid:12)

≤


2 )

1

α−1(cid:17)( α−1
c (cid:16) α−2

|x|
c2

≤ 1

c

if α ≤ 1
if α ≤ 2

(14)

L1 loss is not expressible by our loss, but if c is much
smaller than x we can approximate it with α = 1:

f (x, 1, c) ≈ |x|

c − 1

if c ≪ x

(15)

See the supplement for other potentially-useful properties
that are not used in our experiments.

2. Probability Density Function

With our loss function we can construct a general prob-
ability distribution, such that the negative log-likelihood
(NLL) of its PDF is a shifted version of our loss function:

p (x | µ, α, c) =

1

cZ (α)

exp (−ρ (x − µ, α, c))

(16)

Z (α) =Z ∞

−∞

exp (−ρ (x, α, 1))

(17)

where p (x | µ, α, c) is only deﬁned if α ≥ 0, as Z (α) is
divergent when α < 0. For some values of α the partition
function is relatively straightforward:

Z (0) = π√2
Z (2) = √2π

Z (1) = 2eK1(1)
Z (4) = e1/4K1/4(1/4)

(18)

1

2d

G 0,0

e| 2d

(2π)(d−1)

n − 1(cid:12)(cid:12)

n −1|q(cid:12)(cid:12)

where Kn(·) is the modiﬁed Bessel function of the second
kind. For any rational positive α (excluding a singularity at
α = 2) where α = n/d with n, d ∈ N, we see that
2d(cid:19)2d!
p,q  ap
bq (cid:12)(cid:12)(cid:12)(cid:12)
(cid:18) 1
Z(cid:16) n
d(cid:17) =
n −
2d(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:12)(cid:12)(cid:12)(cid:12)
bq =(cid:26) i
2(cid:27) ∪(cid:26) i
i = 1, ..., 2d − 1(cid:27)
n(cid:12)(cid:12)(cid:12)(cid:12)
ap =(cid:26) i
where G(·) is the Meijer G-function and bq is a multiset
(items may occur twice). Because the partition function is
difﬁcult to evaluate or differentiate, in our experiments we
approximate log(Z (α)) with a cubic hermite spline (see the
supplement for details).

, ..., n −
i = −
i = 1, ..., n − 1(cid:27)

(19)

1
2

3

Just as our loss function includes several common loss
function as special cases, our distribution includes several
common distributions as special cases. When α = 2 our
distribution becomes a normal (Gaussian) distribution, and
when α = 0 our distribution becomes a Cauchy distri-
bution. These are also both special cases of Student’s t-
distribution (ν = ∞ and ν = 1, respectively), though these
are the only two points where these two families of distribu-
tions intersect. Our distribution resembles the generalized
Gaussian distribution [28, 33], except that it is “smoothed”
so as to approach a Gaussian distribution near the origin re-
gardless of the shape parameter α. The PDF and NLL of our
distribution for different values of α can be seen in Figure 2.
In later experiments we will use the NLL of our general
distribution − log(p(·|α, c)) as the loss for training our neu-
ral networks, not our general loss ρ (·, α, c). Critically, us-
ing the NLL allows us to treat α as a free parameter, thereby
allowing optimization to automatically determine the de-
gree of robustness that should be imposed by the loss be-
ing used during training. To understand why the NLL must
be used for this, consider a training procedure in which we
simply minimize ρ (·, α, c) with respect to α and our model
weights. In this scenario, the monotonicity of our general
loss with respect to α (Eq. 12) means that optimization can
trivially minimize the cost of outliers by setting α to be as
small as possible. Now consider that same training pro-
cedure in which we minimize the NLL of our distribution

4333

3. Experiments

We will now demonstrate the utility of our loss function
and distribution with four experiments. None of these re-
sults are intended to represent the state-of-the-art for any
particular task — our goal is to demonstrate the value of our
loss and distribution as useful tools in isolation. We will
show that across a variety of tasks, just replacing the loss
function of an existing model with our general loss function
can enable signiﬁcant performance improvements.

In Sections 3.1 and 3.2 we focus on learning based vi-
sion tasks in which training involves minimizing the differ-
ence between images: variational autoencoders for image
synthesis and self-supervised monocular depth estimation.
We will generalize and improve models for both tasks by
using our general distribution (either as a conditional dis-
tribution in a generative model or by using its NLL as an
adaptive loss) and allowing the distribution to automatically
determine its own degree of robustness. Because robustness
is automatic and requires no manually-tuned hyperparame-
ters, we can even allow for the robustness of our loss to
be adapted individually for each dimension of our output
space — we can have a different degree of robustness at
each pixel in an image, for example. As we will show, this
approach is particularly effective when combined with im-
age representations such as wavelets, in which we expect to
see non-Gaussian, heavy-tailed distributions.

In Sections 3.3 and 3.4 we will build upon existing al-
gorithms for two classic vision tasks (registration and clus-
tering) that both work by minimizing a robust loss that is
subsumed by our general loss. We will then replace each
algorithm’s ﬁxed robust loss with our loss, thereby intro-
ducing a continuous tunable robustness parameter α. This
generalization allows us to introduce new models in which
α is manually tuned or annealed, thereby improving per-
formance. These results demonstrate the value of our loss
function when designing classic vision algorithms, by al-
lowing model robustness to be introduced into the algorithm
design space as a continuous hyperparameter.

3.1. Variational Autoencoders

Variational autoencoders [22, 30] are a landmark tech-
nique for training autoencoders as generative models, which
can then be used to draw random samples that resemble
training data. We will demonstrate that our general distribu-
tion can be used to improve the log-likelihood performance
of VAEs for image synthesis on the CelebA dataset [26]. A
common design decision for VAEs is to model images us-
ing an independent normal distribution on a vector of RGB
pixel values [22], and we use this design as our baseline
model. Recent work has improved upon this model by us-
ing deep, learned, and adversarial loss functions [8, 15, 24].
Though it’s possible that our general loss or distribution
can add value in these circumstances, to more precisely iso-

4334

Figure 2. The negative log-likelihoods (left) and probability den-
sities (right) of the distribution corresponding to our loss function
when it is deﬁned (α ≥ 0). NLLs are simply losses (Fig. 1) shifted
by a log partition function. Densities are bounded by a scaled
Cauchy distribution.

instead of our loss. As can be observed in Figure 2, reduc-
ing α will decrease the NLL of outliers but will increase
the NLL of inliers. During training, optimization will have
to choose between reducing α, thereby getting “discount”
on large errors at the cost of paying a penalty for small er-
rors, or increasing α, thereby incurring a higher cost for
outliers but a lower cost for inliers. This tradeoff forces op-
timization to judiciously adapt the robustness of the NLL
being minimized. As we will demonstrate later, allowing
the NLL to adapt in this way can increase performance on
a variety of learning tasks, in addition to obviating the need
for manually tuning α as a ﬁxed hyperparameter.

Sampling from our distribution is straightforward given
the observation that − log (p (x | 0, α, 1)) is bounded from
below by ρ(x, 0, 1) + log(Z(α)) (shifted Cauchy loss). See
Figure 2 for visualizations of this bound when α = ∞,
which also bounds the NLL for all values of α. This lets
us perform rejection sampling using a Cauchy as the pro-
posal distribution. Because our distribution is a location-
scale family, we sample from p (x | 0, α, 1) and then scale
and shift that sample by c and µ respectively. This sam-
pling approach is efﬁcient, with an acceptance rate between
∼ 45% (α = ∞) and 100% (α = 0). Pseudocode for sam-
pling is shown in Algorithm 1.

Algorithm 1 Sampling from our general distribution
Input: Parameters for the distribution to sample {µ, α, c}
Output: A sample drawn from p (x | µ, α, c).
1: while True:
2:

x ∼ Cauchy(x0 = 0, γ = √2)
u ∼ Uniform(0, 1)
if u <

exp(−ρ(x,0,1)−log(Z(α))) :

p(x | 0,α,1)

return cx + µ

3:

4:

5:

late our contribution we will explore the hypothesis that the
baseline model of normal distributions placed on a per-pixel
image representation can be improved signiﬁcantly with the
small change of just modeling a linear transformation of a
VAE’s output with our general distribution. Again, our goal
is not to advance the state of the art for any particular im-
age synthesis task, but is instead to explore the value of our
distribution in an experimentally controlled setting.

In our baseline model we give each pixel’s normal distri-
bution a variable scale parameter σ(i) that will be optimized
over during training, thereby allowing the VAE to adjust the
scale of its distribution for each output dimension. We can
straightforwardly replace this per-pixel normal distribution
with a per-pixel general distribution, in which each output
dimension is given a distinct shape parameter α(i) in ad-
dition to its scale parameter c(i) (i.e., σ(i)). By letting the
α(i) parameters be free variables alongside the scale param-
eters, training is able to adaptively select both the scale and
robustness of the VAE’s posterior distribution over pixel
values. We restrict all α(i) to be in (0, 3), which allows
our distribution to generalize Cauchy (α = 0) and Normal
(α = 2) distributions and anything in between, as well as
more platykurtic distributions (α > 2) which helps for this
task. We limit α to be less than 3 because of the increased
risk of numerical instability during training as α increases.
We also compare against a Cauchy distribution as an ex-
ample of a ﬁxed heavy-tailed distribution, and against Stu-
dent’s t-distribution as an example of a distribution that can
adjust its own robustness similarly to ours.

Regarding implementation, for each output dimension
i we construct unconstrained TensorFlow variables {α(i)
ℓ }
and {c(i)

ℓ } and deﬁne
α(i) = (αmax − αmin) sigmoid(cid:16)α(i)
c(i) = softplus(cid:16)c(i)

ℓ (cid:17) + cmin

αmin = 0, αmax = 3, cmin = 10−8

ℓ (cid:17) + αmin

(20)

(21)

(22)

The cmin offset avoids degenerate optima where likelihood
is maximized by having c(i) approach 0, while αmin and
αmax determine the range of values that α(i) can take. Vari-
ables are initialized such that initially all α(i) = 1 and
c(i) = 0.01, and are optimized simultaneously with the au-
toencoder’s weights using the same Adam [21] optimizer
instance.

Though modeling images using independent distribu-
tions on pixel intensities is a popular choice due to its sim-
plicity, classic work in natural image statistics suggest that
images are better modeled with heavy-tailed distributions
on wavelet-like image decompositions [9, 27]. We there-
fore train additional models in which our decoded RGB per-
pixel images are linearly transformed into spaces that bet-
ter model natural images before computing the NLL of our

8,662
Pixels + RGB
31,837
DCT + YUV
Wavelets + YUV 31,505

Normal Cauchy
9,602
31,295
35,779

t-dist.
10,177
32,804
36,373

Ours
10,240
32,806
36,316

Table 1. Validation set ELBOs (higher is better) for our varia-
tional autoencoders. Models using our general distribution better
maximize the likelihood of unseen data than those using normal
or Cauchy distributions (both special cases of our model) for all
three image representations, and perform similarly to Student’s t-
distribution (a different generalization of normal and Cauchy dis-
tributions). The best and second best performing techniques for
each representation are colored orange and yellow respectively.

Normal

Cauchy

t-distribution

Ours

B
G
R
+
s
l
e
x
i

P

V
U
Y
+
T
C
D

V
U
Y
+
s
t
e
l
e
v
a
W

Figure 3. Random samples from our variational autoencoders. We
use either normal, Cauchy, Student’s t, or our general distributions
(columns) to model the coefﬁcients of three different image rep-
resentations (rows). Because our distribution can adaptively inter-
polate between Cauchy-like or normal-like behavior for each co-
efﬁcient individually, using it results in sharper and higher-quality
samples (particularly when using DCT or wavelet representations)
and does a better job of capturing low-frequency image content
than Student’s t-distribution.

distribution. For this we use the DCT [1] and the CDF 9/7
wavelet decomposition [7], both with a YUV colorspace.
These representations resemble the JPEG and JPEG 2000
compression standards, respectively.

Our results can be seen in Table 1, where we report the
validation set evidence lower bound (ELBO) for all com-
binations of our four distributions and three image repre-
sentations, and in Figure 3, where we visualize samples
from these models. We see that our general distribution per-

4335

forms similarly to a Student’s t-distribution, with both pro-
ducing higher ELBOs than any ﬁxed distribution across all
representations. These two adaptive distributions appear to
have complementary strengths: ours can be more platykur-
tic (α > 2) while a t-distribution can be more leptokurtic
(ν < 1), which may explain why neither model consistently
outperforms the other across representations. Note that the
t-distribution’s NLL does not generalize the Charbonnier,
L1, Geman-McClure, or Welsch losses, so unlike ours it
will not generalize the losses used in the other tasks we will
address. For all representations, VAEs trained with our gen-
eral distribution produce sharper and more detailed samples
than those trained with normal distributions. Models trained
with Cauchy and t-distributions preserve high-frequency
detail and work well on pixel representations, but systemat-
ically fail to synthesize low-frequency image content when
given non-pixel representations, as evidenced by the gray
backgrounds of those samples. Comparing performance
across image representations shows that the “Wavelets +
YUV” representation best maximizes validation set ELBO
— though if we were to limit our model to only normal dis-
tributions the “DCT + YUV” model would appear superior,
suggesting that there is value in reasoning jointly about dis-
tributions and image representations. After training we see

shape parameters {α(i)} that span (0, 2.5), suggesting that

is just a consequence of allowing {α(i)

an adaptive mixture of normal-like and Cauchy-like distri-
butions is useful in modeling natural images, as has been
observed previously [29]. Note that this adaptive robustness
ℓ } to be free variables
during training, and requires no manual parameter tuning.
See the supplement for more samples and reconstructions
from these models, and a review of our experimental proce-
dure.

3.2. Unsupervised Monocular Depth Estimation

Due to the difﬁculty of acquiring ground-truth direct
depth observations, there has been recent interest in “unsu-
pervised” monocular depth estimation, in which stereo pairs
and geometric constraints are used to directly train a neural
network [10, 11, 14, 41]. We use [41] as a representative
model from this literature, which is notable for its estima-
tion of depth and camera pose. This model is trained by
minimizing the differences between two images in a stereo
pair, where one image has been warped to match the other
according to the depth and pose predictions of a neural net-
work. In [41] that difference between images is deﬁned as
the absolute difference between RGB values. We will re-
place that loss with different varieties of our general loss,
and demonstrate that using annealed or adaptive forms of
our loss can improve performance.

The absolute loss in [41] is equivalent to maximizing the
likelihood of a Laplacian distribution with a ﬁxed scale on
RGB pixel values. We replace that ﬁxed Laplacian distri-

lower is better

higher is better

0.294
0.286
0.268
0.261
0.267
0.260
0.254

0.676 0.885
0.726 0.895
0.738 0.906
0.766 0.911
0.737 0.904
0.756 0.911
0.766 0.916

0.221 2.226 7.527
0.208 2.773 7.085
0.194 2.138 6.743
2.407 6.649
0.190 1.922 6.648
0.184 2.063 6.697
0.181
2.144 6.454

Avg AbsRel SqRel RMS logRMS < 1.25 < 1.252 < 1.253
Baseline [41] as reported
0.954
0.407
Baseline [41] reproduced
0.953
0.398
Ours, ﬁxed α = 1
0.960
0.356
Ours, ﬁxed α = 0
0.960
0.350 0.187
Ours, ﬁxed α = 2
0.961
0.349
Ours, annealing α = 2→ 0 0.341
0.963
Ours, adaptive α ∈ (0, 2)
0.332
0.965
Table 2. Results on unsupervised monocular depth estimation us-
ing the KITTI dataset [12], building upon the model from [41]
(“Baseline”). By replacing the per-pixel loss used by [41] with
several variants of our own per-wavelet general loss function in
which our loss’s shape parameters are ﬁxed, annealed, or adap-
tive, we see a signiﬁcant performance improvement. The top three
techniques are colored red, orange, and yellow for each metric.

t
u
p
n
I

e
n
i
l
e
s
a
B

s
r
u
O

h
t
u
r
T

Figure 4. Monocular depth estimation results on the KITTI bench-
mark using the “Baseline” network of [41]. Replacing only the
network’s loss function with our “adaptive” loss over wavelet co-
efﬁcients results in signiﬁcantly improved depth estimates.

bution with our general distribution, keeping our scale ﬁxed
but allowing the shape parameter α to vary. Following our
observation from Section 3.1 that YUV wavelet representa-
tions work well when modeling images with our loss func-
tion, we impose our loss on a YUV wavelet decomposition
instead of the RGB pixel representation of [41]. The only
changes we made to the code from [41] were to replace its
loss function with our own and to remove the model compo-
nents that stopped yielding any improvement after the loss
function was replaced (see the supplement for details). All
training and evaluation was performed on the KITTI dataset
[12] using the same training/test split as [41].

Results can be seen in Table 2. We present the error
and accuracy metrics used in [41] and our own “average”
error measure: the geometric mean of the four errors and
one minus the three accuracies. The “Baseline“ models use
the loss function of [41], and we present both the numbers
in [41] (“as reported”) and our own numbers from running

4336

the code from [41] ourselves (“reproduced”). The “Ours”
entries all use our general loss imposed on wavelet coefﬁ-
cients, but for each entry we use a different strategy for set-
ting the shape parameter or parameters. We keep our loss’s
scale c ﬁxed to 0.01, thereby matching the ﬁxed scale as-
sumption of the baseline model and roughly matching the
shape of its L1 loss (Eq. 15). To avoid exploding gradients
we multiply the loss being minimized by c, thereby bound-
ing gradient magnitudes by residual magnitudes (Eq. 14).
For the “ﬁxed” models we use a constant value for α for all
wavelet coefﬁcients, and observe that though performance
is improved relative to the baseline, no single value of α is
optimal. The α = 1 entry is simply a smoothed version
of the L1 loss used by the baseline model, suggesting that
just using a wavelet representation improves performance.
In the “annealing α = 2 → 0” model we linearly inter-
polate α from 2 (L2) to 0 (Cauchy) as a function of train-
ing iteration, which outperforms all “ﬁxed” models. In the
“adaptive α ∈ (0, 2)” model we assign each wavelet co-
efﬁcient its own shape parameter as a free variable and we
allow those variables to be optimized alongside our network
weights during training as was done in Section 3.1, but with
αmin = 0 and αmax = 2. This “adaptive” strategy out-
performs the “annealing” and all “ﬁxed” strategies, thereby
demonstrating the value of allowing the model to adaptively
determine the robustness of its loss during training. Note
that though the “ﬁxed” and “annealed” strategies only re-
quire our general loss, the “adaptive” strategy requires that
we use the NLL of our general distribution as our loss —
otherwise training would simply drive α to be as small as
possible due to the monotonicity of our loss with respect
to α, causing performance to degrade to the “ﬁxed α = 0”
model. Comparing the “adaptive” model’s performance to
that of the “ﬁxed” models suggests that, as in Section 3.1,
no single setting of α is optimal for all wavelet coefﬁcients.
Overall, we see that just replacing the loss function of [41]
with our adaptive loss on wavelet coefﬁcients reduces aver-
age error by ∼ 17%.

In Figure 4 we compare our “adaptive” model’s output to
the baseline model and the ground-truth depth, and demon-
strate a substantial qualitative improvement. See the sup-
plement for many more results, and for visualizations of the
per-coefﬁcient robustness selected by our model.

3.3. Fast Global Registration

Robustness is often a core component of geometric regis-
tration [37]. The Fast Global Registration (FGR) algorithm
of [40] ﬁnds the rigid transformation T that aligns point sets
{p} and {q} by minimizing the following loss:

ρgm (kp − Tqk, c)

(23)

X(p,q)

σ =

Mean RMSE ×100
0

0.0025

0.005

Max RMSE ×100
0

0.0025

0.005

FGR [40]
0.373
shape-annealed gFGR 0.374
0.370
gFGR*

0.518
0.510
0.509

0.821
0.802
0.806

0.591
0.590
0.545

1.040
0.997
0.961

1.767
1.670
1.669

Table 3. Results on the registration task of [40], in which we
compare their “FGR” algorithm to two versions of our “gFGR”
generalization.

Figure 5. Performance (lower is better) of our gFGR algorithm
on the task of [40] as we vary our shape parameter α, with the
lowest-error point indicated by a circle. FGR (equivalent to gFGR
with α = −2) is shown as a dashed line and a square, and shape-
annealed gFGR for each noise level is shown as a dotted line.

where ρgm(·) is Geman-McClure loss. By using the Black
and Rangarajan duality between robust estimation and line
processes [3] FGR is capable of producing high-quality reg-
istrations at high speeds. Because Geman-McClure loss is a
special case of our loss, and because we can formulate our
loss as an outlier process (see supplement), we can gener-
alize FGR to an arbitrary shape parameter α by replacing
ρgm(·, c) with our ρ(·, α, c) (where setting α = −2 repro-
duces FGR).
This generalized FGR (gFGR) enables algorithmic im-
provements. FGR iteratively solves a linear system while
annealing its scale parameter c, which has the effect of grad-
ually introducing nonconvexity. gFGR enables an alterna-
tive strategy in which we directly manipulate convexity by
annealing α instead of c. This “shape-annealed gFGR” fol-
lows the same procedure as [40]: 64 iterations in which a
parameter is annealed every 4 iterations. Instead of anneal-
ing c, we set it to its terminal value and instead anneal α
over the following values:

2, 1, 1/2, 1/4, 0,−1/4,−1/2,−1,−2,−4,−8,−16,−32

Table 3 shows results for the 3D point cloud registration
task of [40] (Table 1 in that paper), which shows that an-
nealing shape produces moderately improved performance
over FGR for high-noise inputs, and behaves equivalently
in low-noise inputs. This suggests that performing gradu-
ated non-convexity by directly adjusting a shape parameter
that controls non-convexity — a procedure that is enabled
by our general loss – is preferable to indirectly controlling
non-convexity by annealing a scale parameter.

4337

]
2
3
[

]
6
3
[

I

-

W
C
A

s
t
u
C
N

-

G
M
D
L

]
8
3
[

C
I
P

]
1
3
[

-

R
D
C
C
R

]
1
3
[

C
C
R

*
C
C
R
g

.
r
p
m

I

.
l
e
R

-

-

0.928
0.871

0.941
0.965

0.767
0.853
0.679
0.801
0.728
0.525
0.471
0.291
0.364

0.945
0.888
0.761
0.518
0.775
0.527
0.523
0.591
0.382

Dataset
YaleB
COIL-100
MNIST
YTF
Pendigits
Mice Protein
Reuters
Shuttle
RCV1
Table 4. Results on the clustering task of [31] where we compare
their “RCC” algorithm to our “gRCC*” generalization in terms
of AMI on several datasets. We also report the AMI increase of
“gRCC*” with respect to “RCC”. Baselines are taken from [31].

0.4%
11.6%
7.9%
31.9%
15.1%
0.2%
1.1%
0.9%
23.2%

0.974
0.957
0.828
0.874
0.854
0.638
0.553
0.513
0.442

0.975
0.962
0.901
0.888
0.871
0.650
0.561
0.493
0.338

0.975
0.957
0.893
0.836
0.848
0.649
0.556
0.488
0.138

0.752
0.813
0.536
0.545
0.000
0.140

0.676
0.467
0.394
0.057

0.015

-

Another generalization is to continue using the c-
annealing strategy of [40], but treat α as a hyperparameter
and tune it independently for each noise level in this task.
In Figure 5 we set α to a wide range of values and report
errors for each setting, using the same evaluation of [40].
We see that for high-noise inputs more negative values of
α are preferable, but for low-noise inputs values closer to
0 are optimal. We report the lowest-error entry for each
noise level as “gFGR*” in Table 3 where we see a signiﬁ-
cant reduction in error, thereby demonstrating the improve-
ment that can be achieved from treating robustness as a hy-
perparameter.

3.4. Robust Continuous Clustering

In [31] robust losses are used for unsupervised cluster-

ing, by minimizing:

(24)

kxi − uik2

2 + λ X(p,q)∈E

wp,qρgm (kup − uqk2)

Xi
where {xi} is a set of input datapoints, {ui} is a set of “rep-
resentatives” (cluster centers), and E is a mutual k-nearest
neighbors (m-kNN) graph. As in Section 3.3, ρgm(·) is
Geman-McClure loss, which means that our loss can be
used to generalize this algorithm. Using the RCC code
provided by the authors (and keeping all hyperparameters
ﬁxed to their default values) we replace Geman-McClure
loss with our general loss and then sweep over values of α.
In Figure 6 we show the adjusted mutual information (AMI,
the metric used by [31]) of the resulting clustering for each
value of α on the datasets used in [31], and in Table 4 we
report the AMI for the best-performing value of α for each
dataset as “gRCC*”. On some datasets performance is in-
sensitive to α, but on others adjusting α improves perfor-
mance by as much as 32%. This improvement demonstrates
the gains that can be achieved by introducing robustness as
a hyperparameter and tuning it accordingly.

Figure 6. Performance (higher is better) of our gRCC algorithm
on the clustering task of [31], for different values of our shape
parameter α, with the highest-accuracy point indicated by a dot.
Because the baseline RCC algorithm is equivalent to gRCC with
α = −2, we highlight that α value with a dashed line and a square.

4. Conclusion

existing

one-parameter

We have presented a two-parameter

generalizes many
loss functions:

loss function
ro-
that
the Cauchy/Lorentzian, Geman-
bust
McClure, Welsch/Leclerc,
generalized Charbonnier,
Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions.
By reducing a family of discrete single-parameter losses
to a single function with two continuous parameters, our
loss enables the convenient exploration and comparison
of different robust penalties. This allows us to generalize
and improve algorithms designed around the minimiza-
tion of some ﬁxed robust loss function, which we have
demonstrated for registration and clustering. When used
as a negative log-likelihood,
this loss gives a general
probability distribution that includes normal and Cauchy
distributions as special cases. This distribution lets us train
neural networks in which the loss has an adaptive degree
of robustness for each output dimension, which allows
training to automatically determine how much robustness
should be imposed by the loss without any manual param-
eter tuning. When this adaptive loss is paired with image
representations in which variable degrees of heavy-tailed
behavior occurs, such as wavelets, this adaptive training ap-
proach allows us to improve the performance of variational
autoencoders for image synthesis and of neural networks
for unsupervised monocular depth estimation.

Acknowledgements: Thanks to Rob Anderson, Jesse En-
gel, David Gallup, Ross Girshick, Jaesik Park, Ben Poole,
Vivek Rathod, and Tinghui Zhou.

4338

References

[1] Nasir Ahmed, T Natarajan, and Kamisetty R Rao. Discrete

cosine transform. IEEE Transactions on Computers, 1974.

[2] Michael J Black and Paul Anandan. The robust estimation
of multiple motions: Parametric and piecewise-smooth ﬂow
ﬁelds. CVIU, 1996.

[3] Michael J. Black and Anand Rangarajan. On the uniﬁcation
of line processes, outlier rejection, and robust statistics with
applications in early vision. IJCV, 1996.

[4] Andrew Blake and Andrew Zisserman. Visual Reconstruc-

tion. MIT Press, 1987.

[5] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and
Michel Barlaud. Two deterministic half-quadratic regular-
ization algorithms for computed imaging. ICIP, 1994.

[6] Qifeng Chen and Vladlen Koltun. Fast mrf optimization with

application to depth reconstruction. CVPR, 2014.

[7] Albert Cohen,

Ingrid Daubechies, and J-C Feauveau.
Biorthogonal bases of compactly supported wavelets. Com-
munications on pure and applied mathematics, 1992.

[8] Alexey Dosovitskiy and Thomas Brox. Generating images
with perceptual similarity metrics based on deep networks.
NIPS, 2016.

[9] David J. Field. Relations between the statistics of natural
images and the response properties of cortical cells. JOSA A,
1987.

[10] John Flynn, Ivan Neulander, James Philbin, and Noah
Snavely. Deepstereo: Learning to predict new views from
the world’s imagery. CVPR, 2016.

[11] Ravi Garg, BG Vijay Kumar, Gustavo Carneiro, and Ian
Reid. Unsupervised cnn for single view depth estimation:
Geometry to the rescue. ECCV, 2016.

[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. CVPR, 2012.

[13] Stuart Geman and Donald E. McClure. Bayesian image anal-
ysis: An application to single photon emission tomography.
Proceedings of the American Statistical Association, 1985.

[14] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. CVPR, 2017.

[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. NIPS, 2014.

[16] Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw,
and Werner A. Stahel. Robust Statistics: The Approach
Based on Inﬂuence Functions. Wiley, 1986.

[17] Trevor Hastie, Robert Tibshirani, and Martin Wainwright.
Statistical Learning with Sparsity: The Lasso and General-
izations. Chapman and Hall/CRC, 2015.

[18] Peter J. Huber. Robust estimation of a location parameter.

Annals of Mathematical Statistics, 1964.

[19] Peter J. Huber. Robust Statistics. Wiley, 1981.

[20] John E. Dennis Jr. and Roy E. Welsch. Techniques for non-
linear least squares and robust regression. Communications
in Statistics-simulation and Computation, 1978.

[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. ICLR, 2015.

[22] Diederik P. Kingma and Max Welling. Auto-encoding vari-

ational bayes. ICLR, 2014.

[23] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient nonlocal

regularization for optical ﬂow. ECCV, 2012.

[24] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo
Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. ICML, 2016.

[25] Yvan G Leclerc. Constructing simple stable descriptions for

image partitioning. IJCV, 1989.

[26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.

Deep learning face attributes in the wild. ICCV, 2015.

[27] St´ephane Mallat. A theory for multiresolution signal decom-

position: The wavelet representation. TPAMI, 1989.

[28] Saralees Nadarajah. A generalized normal distribution. Jour-

nal of Applied Statistics, 2005.

[29] Javier Portilla, Vasily Strela, Martin J. Wainwright, and
Image denoising using scale mixtures

Eero P. Simoncelli.
of gaussians in the wavelet domain. IEEE TIP, 2003.

[30] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-
stra. Stochastic backpropagation and approximate inference
in deep generative models. ICML, 2014.

[31] Sohil Atul Shah and Vladlen Koltun. Robust continuous

clustering. PNAS, 2017.

[32] Jianbo Shi and Jitendra Malik. Normalized cuts and image

segmentation. TPAMI, 2000.

[33] M Th Subbotin. On the law of frequency of error. Matem-

aticheskii Sbornik, 1923.

[34] Deqing Sun, Stefan Roth, and Michael J. Black. Secrets of

optical ﬂow estimation and their principles. CVPR, 2010.

[35] Rein van den Boomgaard and Joost van de Weijer. On
the equivalence of local-mode ﬁnding, robust estimation and
mean-shift analysis as used in early vision tasks. ICPR, 2002.
[36] Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting
Zhuang. Image clustering using local discriminant models
and global integration. TIP, 2010.

[37] Christopher Zach.

Robust bundle adjustment revisited.

ECCV, 2014.

[38] Wei Zhang, Deli Zhao, and Xiaogang Wang. Agglomerative
clustering via maximum incremental path integral. Pattern
Recognition, 2013.

[39] Zhengyou Zhang. Parameter estimation techniques: A tuto-

rial with application to conic ﬁtting, 1995.

[40] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global

registration. ECCV, 2016.

[41] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G.
Lowe. Unsupervised learning of depth and ego-motion from
video. CVPR, 2017.

4339

