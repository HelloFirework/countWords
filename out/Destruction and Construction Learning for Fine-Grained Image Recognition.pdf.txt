Destruction and Construction Learning for Fine-grained Image Recognition

Yue Chen1 ∗

Yalong Bai2 ∗ Wei Zhang3
JD AI Research, Beijing, China

Tao Mei4

1chenyue21@jd.com, 2ylbai@outlook.com, 3wzhang.cu@gmail.com, 4tmei@live.com

Abstract

Delicate feature representation about object parts plays
a critical role in ﬁne-grained recognition. For example, ex-
perts can even distinguish ﬁne-grained objects relying only
on object parts according to professional knowledge. In this
paper, we propose a novel “Destruction and Construction
Learning” (DCL) method to enhance the difﬁculty of ﬁne-
grained recognition and exercise the classiﬁcation model to
acquire expert knowledge. Besides the standard classiﬁca-
tion backbone network, another “destruction and construc-
tion” stream is introduced to carefully “destruct” and then
“reconstruct” the input image, for learning discriminative
regions and features. More speciﬁcally, for “destruction”,
we ﬁrst partition the input image into local regions and then
shufﬂe them by a Region Confusion Mechanism (RCM). To
correctly recognize these destructed images, the classiﬁ-
cation network has to pay more attention to discrimina-
tive regions for spotting the differences. To compensate the
noises introduced by RCM, an adversarial loss, which dis-
tinguishes original images from destructed ones, is applied
to reject noisy patterns introduced by RCM. For “construc-
tion”, a region alignment network, which tries to restore the
original spatial layout of local regions, is followed to model
the semantic correlation among local regions. By jointly
training with parameter sharing, our proposed DCL injects
more discriminative local details to the classiﬁcation net-
work. Experimental results show that our proposed frame-
work achieves state-of-the-art performance on three stan-
dard benchmarks. Moreover, our proposed method does not
need any external knowledge during training, and there is
no computation overhead at inference time except the stan-
dard classiﬁcation network feed-forwarding. Source code:
https://github.com/JDAI-CV/DCL.

1. Introduction

In the past decade, generic object recognition has
achieved steady progress with efforts from both large-scale

∗Equal contribution.

annotated dataset and sophisticated model design. How-
ever, recognizing ﬁne-grained object categories (e.g., bird
species [3], car models [14] and aircraft [18]) is still a
challenging task, which attracts extensive research atten-
tion. Although ﬁne-grained objects are visually similar by a
rough glimpse, they can be correctly recognized by details
in discriminative local regions.

Learning discriminative feature representations from
discriminative parts plays the key role in ﬁne-grained image
recognition. Existing ﬁne-grained recognition methods can
be roughly grouped into two categories, as illustrated in Fig-
ure 1. One group (a) ﬁrst locates the discriminative object
parts and then classiﬁes based on the discriminative regions.
These two-steps methods [21, 11, 1] mostly need additional
bounding box annotations on objects or parts, which are ex-
pensive to collect. The other group (b) tries to automatically
localize discriminative regions by attention mechanism in
an unsupervised manner, and thus does not needs extra an-
notations. However, these methods [7, 42, 41, 22] usually
need additional network structure (e.g., attention mecha-
nism), and thus introduce extra computation overhead for
both training and inference stages.

In this paper, we propose a novel ﬁne-grained image
recognition framework named “Destruction and Construc-
tion Learning” (DCL), as shown in Figure 1 (c). Besides
the standard classiﬁcation backbone network, we introduce
a DCL stream to learn from discriminative regions automat-
ically. An input image is ﬁrst carefully destructed to em-
phasize discriminative local details, and then reconstructed
to model the semantic correlation among local regions. On
one hand, DCL automatically localizes discriminative re-
gions, and thus does not need any extra knowledge while
training. On the other hand, the DCL structure is only
adopted at the training stage, and thus introduces no com-
putational overhead at inference time.

For “Destruction”, we propose a Region Confusion
Mechanism (RCM) to deliberately “confuse” the global
structure, which partitions the input
image into local
patches and then shufﬂes them randomly (Figure 3). For
ﬁne-grained recognition, local details play a more important
role than global structures, since images from different ﬁne-

43215157

Figure 1. Illustrations of two previous general frameworks (a,b) and our proposed framework (c) for ﬁne-grained classiﬁcation. (a) Two-
stage part detection based framework. (b) Attention based framework. (c) Our proposed destruction and construction learning framework.
The network structures in dashed lines are disabled during inference.

grained categories usually share the same global structure
or shape, but only differ in local details. Discarding global
structure and keeping local details could force the network
to identify and focus on the discriminative local regions for
recognition. After all, the devil is in the details. Shufﬂing
is also adopted in natural language processing [15] to let
the neural network focus on discriminative words. Simi-
larly, if local regions in an image are “shufﬂed”, irrelevant
regions that are non-critical to ﬁne-grained recognition will
be neglected, and the network will be forced to classify im-
ages based on the discriminative local details. With RCM,
the visual appearance of the image has been substantially
changed. As shown in the bottom row of Figure 3, though it
becomes more difﬁcult for recognition, bird experts can still
spot the difference easily. Car enthusiasts can distinguish
car models by only examining parts of car [34]. Similarly,
the neural network also needs to learn expert knowledge to
classify the destructed images.

It is worth noting that “destruction” is not always beneﬁ-
cial. As a side effect, RCM introduces several noisy visual
patterns as in Figure 3. To offset the negative impact, we ap-
ply an adversarial loss to distinguish original images from
destructed ones. As a result, the effect of noisy patterns can
be minimized, keeping only beneﬁcial local details. Con-
ceptually, the adversarial and classiﬁcation losses work in
an adversarial manner to carefully learn from “destruction”.
For “Construction”, a region alignment network is intro-
duced to restore the original region arrangement, which acts
in the opposite way of RCM. By learning to restore the orig-
inal layout as in [19, 6], the network needs to understand
the semantics of each region, including those discrimina-
tive ones. Through “construction”, the correlation between
different local regions can be modeled.

The main contributions are summarized as follows:

• A novel “Destruction and Construction Learning
(DCL)” framework is proposed for ﬁne-grained recog-
nition. For destruction, the region confusion mecha-
nism (RCM) forces the classiﬁcation network to learn
from discriminative regions, and the adversarial loss
prevents over-ﬁtting the RCM-induced noisy patterns.
For construction, the region alignment network re-
stores the original region layout by modeling the se-
mantic correlation among regions.

• State-of-the-art performances are reported on three
standard benchmark datasets, where our DCL consis-
tently outperforms existing methods.

• Compared to existing methods, our proposed DCL
does not need extra part/object annotation and intro-
duces no computational overhead at inference time.

2. Related works

Researches for ﬁne-grained image recognition task
mainly proceed along two dimensions. One is learning
better visual representations from the original image di-
rectly [26, 25, 28] and the other is using part/attention based
methods [41, 42, 7, 13] to obtain discriminative regions in
images and learn region-based feature representations.

Due to the success of deep learning, ﬁne-grained recog-
nition methods have shifted from multistage frameworks
based on hand-crafted features [39, 36, 23, 10] to multi-
stage frameworks with CNN features [13, 31, 29]. Sec-
ond order bilinear feature interactions were shown to have
a signiﬁcant improvement for visual representations learn-
ing [16, 30]. This method was later extended to a series of
related works with further improvements [12, 4, 8]. Deep
metric learning is also used to capture subtle visual differ-
ences. Zhang et al. [40] introduced label structures and a
generalization of triplet loss to learn ﬁne-grained feature
representations. Chen et al. [27] investigate simultaneously
predicting categories of different levels in the hierarchy and
integrating this structured correlation information into the
network by an embedding method. However, these pair-
wise neural network models often bring complex network
computing.

There is also a large amount of part localization based
methods proposed regarding the theory that the object parts
are essential to learning discriminative features for ﬁne-
grained classiﬁcation [32]. Fu et al. [7] proposed a re-
inforced attention proposal network to obtain discriminat-
ing attention regions and region-based feature representa-
tion of multiple scales. Sun et al. [20] proposed a one-
squeeze multi-excitation module to learns multiple atten-
tion region features of each input image, and then apply a
multi-attention multi-class constraint in a metric learning
framework. Zheng et al. [42] adopted a channel grouping
network to generate multiple parts by clustering, then classi-

43225158

(c)(a)(b)DetectionAttentionConstructionDestructionFigure 2. The framework of the proposed DCL method which consists of four parts. (1) Region Confusion Mechanism: a module to
shufﬂe the local regions of the input image. (2) Classiﬁcation Network: the backbone classiﬁcation network that classiﬁes images into
ﬁne-grained categories. (3) Adversarial Learning Network: an adversarial loss is applied to distinguish original images from destructed
ones. (4) Region Alignment Network: appended after the classiﬁcation network to recover the spatial layout of local regions.

ﬁed these parts features to predict the categories of input im-
ages. Compared with earlier part/attention based methods,
some of the recent methods tend to be weak supervised and
do not require the annotations of parts or key areas [21, 35].
In particular, Peng et al. [21] proposed a part spatial con-
straint to make sure that the model could select discrimina-
tive regions, and a specialized clustering algorithm is used
to integrate the features of these regions. Yang et al. [35]
introduced a method to detect informative regions and then
scrutinizes them to make ﬁnal predictions. However, the
correlation among regions is helpful to build deep under-
standing about the objects, it is usually ignored by previous
works. The research [19] also shows that utilizing the loca-
tion information of regions can enhance the visual represen-
tation ability of the neural network and result in improving
performance on classiﬁcation and detection tasks.

Our proposed method differs previous works in three as-
pects: First, by training classiﬁer with our proposed RCM,
the discriminative regions can be automatically detected
without using any prior knowledge except object labels.
Second, our formulation considers not only the ﬁne-grained
local region feature representations but also the semantic
correlation among different regions in the whole image.
Third, our proposed method is highly efﬁcient, that there
is no additional overhead except backbone network feed-
forward in prediction time.

3. Proposed Method

In this section, we present our proposed Destruction and
Construction Learning (DCL) method. As shown in Fig-
ure 2, the whole framework is composed of four parts.

Please note that only the “classiﬁcation network” is needed
during inference time.

3.1. Destruction Learning

The devil is in the details. For ﬁne-grained image recog-
nition, local details are much more important than the global
structure. In most cases, different ﬁne-grained categories
usually share a similar global structure and only differ in
certain local details. In this work, we propose to carefully
destruct the global structure by shufﬂing the local regions
for better identifying discriminative regions and learning
discriminative features (Section 3.1.1). To prevent the net-
work learning from noisy patterns introduced by destruc-
tion, an adversarial counterpart (Section 3.1.2) is proposed
to reject RCM-induced patterns that are irrelevant to ﬁne-
grained classiﬁcation.

3.1.1 Region Confusion Mechanism

As an analogy [15] to natural language processing, shufﬂing
words in a sentence would force the neural network to focus
on discriminative words and neglect irrelevant ones. Simi-
larly, if local regions in an image are “shufﬂed”, the neural
network would be forced to learn from discriminative re-
gion details for classiﬁcation.

As shown in Figure 3, our proposed Region Confusion
Mechanism (RCM) is designed to disrupt the spatial layout
of local image regions. Given an input image I, we ﬁrst
uniformly partition the image into N × N sub-regions de-
noted by Ri,j , where i and j are the horizontal and vertical
indices respectively and 1 ≤ i, j ≤ N . Inspired by [15],
our proposed RCM shufﬂes these partitioned local regions

43235159

Region Confusion MechanismClassification  NetworkFeature VectorAdversarial Learning NetworkAdv LossCls LossRegion Alignment NetworkLoc LossFeature MapsLocation Matrix··· ··· ··· RCM(1)

(2)

in their 2D neighbourhood. For the jth row of R, a ran-
dom vector qj of size N is generated, where the ith element
qj,i = i + r, where r ∼ U(−k, k) is a random variable fol-
lowing a uniform distribution in the range of [−k, k]. Here,
k is a tunable parameter (1 ≤ k < N ) deﬁning the neigh-
bourhood range. Then we can get a new permutation σrow
of regions in jth row by sorting the array qj , verifying the
condition:

j

∀i ∈ {1, ..., N } ,(cid:12)(cid:12)σrow

j

(i) − i(cid:12)(cid:12) < 2k.

Similarly, we apply the permutation σcol
column-wisely, verifying the condition:

i

to the regions

∀j ∈ {1, ..., N } ,(cid:12)(cid:12)σcol

i

(j) − j(cid:12)(cid:12) < 2k.

Therefore, the region at (i, j) in original region location is
placed to a new coordinate:

σ(i, j) = (σrow

j

(i), σcol

i

(j)).

(3)

This shufﬂing method destructs the global structure and en-
sures that the local region jitters inside its neighbourhood
with a tunable size.

The original image I, its destructed version φ(I) and its
ground truth one-vs-all label l indicating the ﬁne-grained
categories are coupled as hI, φ(I), li for training. The clas-
siﬁcation network maps input image into a probability dis-
tribution vector C(I, θcls), where θcls is all learnable pa-
rameters in the classiﬁcation network. The loss function of
the classiﬁcation network Lcls can be written as:

Lcls = −XI ∈I

l · log [C (I) C (φ(I))] ,

(4)

where I is the image set for training.

Since the global structure has been destructed, to rec-
ognize these randomly shufﬂed images, the classiﬁcation
network has to ﬁnd the discriminative regions and learn the
delicate differences among categories.

3.1.2 Adversarial Learning

Destructing images with RCM does not always bring ben-
eﬁcial information for ﬁne-grained classiﬁcation. For ex-
ample in Figure 3, RCM also introduces noisy visual pat-
terns as we shufﬂe the local regions. Features learned from
these noise visual patterns are harmful to the classiﬁcation
task. To this end, we propose another adversarial loss LAdv
to prevent overﬁtting the RCM-induced noise patterns from
creeping into the feature space.

Considering the original images and the destructed ones
as two domains, the adversarial loss and classiﬁcation loss
work in an adversarial manner to 1) keep domain-invariant
patterns, and 2) reject domain-speciﬁc patterns between I
and φ(I).

Figure 3. Example images for ﬁne-grained recognition (top) and
the corresponding “destructed” images by our proposed RCM
(bottom).

We label each image as a one-hot vector d ∈ {0, 1}2
indicating whether the image is destructed or not. A dis-
criminator can be added as a new branch in the framework
to judge whether an image I is destructed or not by:

D(I, θadv) = softmax(θadvC(I, θ[1,m]

cls

)),

(5)

cls

where C(I, θ[1,m]
) is the feature vector extract from the out-
puts of the mth layer in backbone classiﬁcation network,
θ[1,m]
is the learnable parameters from the 1st layer to mth
cls
layer in the classiﬁcation network, and θadv ∈ Rd×2 is a
linear mapping. The loss of the discriminator network Ladv
can be computed as:

Ladv = −XI ∈I

d · log [D(I)] + (1 − d) · log [D(φ(I))] . (6)

Justiﬁcation. To better understand how the adversarial
loss tunes feature learning, we further visualize the features
of backbone network ResNet-50 with and without the ad-
versarial loss. Given an input image I, we denote the kth
feature map in mth layer by F k
m(I). For ResNet-50, we ex-
tract feature from the outputs of the convolutional layer with
average pooling next to the last fully-connect layer for ad-
versarial learning. Thus, the response of kth ﬁlter in the last
convolutional layer for ground truth label c can be measured
by rk(I, c) = ¯F k
[k, c] is
the weight between the kth feature map and the cth output
label.

[k, c], where θ[m+1]

m(I) × θ[m+1]

cls

cls

We compare the responses of different ﬁlters for origi-
nal image and its destructed version in scatter plot shown
as Figure 4, where every ﬁlter with positive response is
mapped to the data point (r(I, c), r(φ(I), c)) in the scat-
ter plot. We can ﬁnd that the distributions of feature maps
trained by Lcls is more compact than those trained by
Lcls +Ladv. It means that the ﬁlters have large responses on
the noise patterns introduced by RCM may also have large
responses on the original image (as the visual patterns vi-
sualized in A, B and C, there are lots of ﬁlters responding

43245160

RCMLocation Matrix(0,0)(1,3)(5,6)(6,6)(5,4)(0,1)··· ··· ··· (0,0)(0,1)(6,6)(5,6)(6,5)(1,0)··· ··· ··· N×Nto edge-style visual patterns or irrelevant patterns that are
introduced by RCM). These ﬁlters may mislead the predic-
tions on the original image.

We also colored the points in scatter plot about backbone

network trained by Lcls + Ladv, according to the value of

δk = ¯F k

m(I) × θadv[k, 1] − ¯F k

m(φ(I)) × θadv[k, 2],

(7)

m(·) and the label representing orignal

where θadv[k, 1]
is the weight connecting feature map
F k
image, and
θadv[k, 2] is the weight connecting F k
m(·) and the label rep-
resenting destructed image. δk evaluates whether the kth ﬁl-
ter tends to be visual patterns in original image or not. It can
be observed that the ﬁlter respond to noisy visual pattern
can be distinguished (D VS. F ) by using adversarial loss.
The points in ﬁgures can be divides into three parts. D:
ﬁlters that tend to respond to noisy patters (RCM-induced
image features); F : ﬁlters that tend to respond to global
context description (orignal image speciﬁc image features);
E: the vast majority of ﬁlters are related to the detailed local
region descriptions that enhanced by Lcls (common image
feature maps between orignal image and destructed image).
Lcls and Ladv together contribute to the “destruction”
learning, where only discriminative local details are en-
hanced and irrelevant features are ﬁltered out.

3.2. Construction Learning

Considering it is the combination of correlative regions
in images constitute the complex and diverse visual pat-
terns, we propose another learning method to model the
correlation among local regions. Speciﬁcally, we propose
a region alignment network with region construction loss
Lloc, that measures the location precision of different re-
gions in images, to induce backbone network to model the
semantic correlative among regions by end-to-end training.
Given an image I and its corresponding destructed ver-
sion φ(I), the region Ri,j located at (i, j) in I is consistent
with the region Rσ(i,j) in φ(I). Region alignment network
works on the output features of one convolution layer of the
classiﬁcation network C(·, θ[1,n]
cls ), where the nth layer is a
convolutional layer. The features are processed by a 1 × 1
convolution to obtain outputs with two channels. Then the
outputs are handled by an ReLU and an average pooling to
get a map with the size of 2 × N × N . The outputs of region
alignment network can be written as:

M (I) = h(cid:16)C(I, θ[1,n]

cls ), θloc(cid:17) ,

(8)

where the two channels in M (I) correspond to the loca-
tion coordinates of rows and columns, respectively, h is our
proposed region alignment network, and θloc is the param-
eters in region alignment network. We denote the predicted
location of Rσ(i,j) in I as Mσ(i,j) (φ(I)), predicted loca-
tion of Ri,j in I as Mi,j (I, i, j). Both ground truth of

Figure 4. Visualization of ﬁlters learned by using Lcls and Lcls +
Ladv respectively. The 1st row shows the original image I and its
destructed version φ(I). The left side of 2nd and 3rd rows show
the scatter plots about the ﬁlters’ responses to I and φ(I). The
right side of 2nd and 3rd rows show the visualization of feature
maps belongs to ﬁlters that have various responses on I and φ(I).
A,D: ﬁlters with larger response to φ(I). C,F : ﬁlters with larger
response to I. B,E: ﬁlters with large responses on both of I and
φ(I). (The ﬁgure is best viewed in color.)

Mσ(i,j) (φ(I)) and Mi,j (I) should be (i, j). The region
alignment loss Lloc is deﬁned as the L1 distance between
the predicted coordinates and original coordinates, which
can be expressed as:

N

N

Lloc = XI ∈I

Xi=1

Xj=1

(cid:12)(cid:12)(cid:12)(cid:12)

+(cid:12)(cid:12)(cid:12)(cid:12)
j(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)1
Mσ(i,j) (φ(I))−(cid:20)i

j(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)1
Mi,j (I)−(cid:20)i

(9)

The region construction loss is helpful to locate the main
objects in images and tends to ﬁnd the correlation among
sub-regions. By end-to-end training, the region construc-
tion loss can help the classiﬁcation backbone network to
build deep understanding about objects and model the struc-
ture information, such as the shape of objects and semantic
correlation among parts of object.

43255161

Cls LossCls Loss + Adv LossACBACBDFEDFE3.3. Destruction and Construction Learning

In our framework, the classiﬁcation, adversarial and re-
gion alignment losses are trained in an end-to-end man-
ner, in which the network can leverage both enhanced local
details and well-modeled object parts correlation for ﬁne-
grained recognition. Speciﬁcally, we want to minimize the
following objective:

L = αLcls + βLadv + γLloc.

(10)

Figure 2 shows the architecture of the DCL framework.
The destruction learning mainly helps to learn from dis-
criminative regions, while the construction learning helps
to re-arrange the learned local details according to seman-
tic correlation among regions. Hence, DCL yields to a set
of complex and diverse visual representations based on the
well-structured detail features from discriminative regions.
cls ) is used for predicting the cate-
gory label of given images. Thus, there is no external com-
putational overhead except the backbone classiﬁcation net-
work for inference.

Note that only f (·, θ[1,l]

4. Experiments

We evaluate the performance of our proposed DCL
on three standard ﬁne-grained object recognition datasets:
CUB-200-2011 (CUB) [3], Stanford Cars (CAR) [14] and
FGVC-Aircraft (AIR) [18]. We do not use any bounding
box/part annotations in all our experiments.

4.1. Implementation Details

Backbone network: We evaluate our proposed method
on two classiﬁcation widely used backbone networks:
ResNet-50 [9] and VGG-16 [25]. These two networks are
pre-trained on ImageNet dataset. The category label of the
image is the only annotation used for training. The input
images are resized to a ﬁxed size of 512 × 512 and ran-
domly cropped into 448 × 448. Random rotation and ran-
dom horizontal ﬂip are applied for data augmentation. All
above settings are standard in the literature. To recognize
high-resolution images on VGG-16 without sub-sampling,
the ﬁrst two fully connected layers in VGG-16 are trans-
formed into two convolution layers respectively. For all the
experiments in this paper, the feature maps of the last convo-
lutional layer of backbone network are feed into the region
alignment network, and the feature vector formed by the
output of average pooling following the last convolutional
layer is feed into the adversarial learning network.

The number of regions N in RCM is based on the back-
bone network and the size of the input image. The width w
and length h of the region should be divisible by the stride
of the last convolutional layer, which is 32 for VGG-16 and
ResNet-50. Meanwhile, to ensure the feasibility of region
alignment, the width and height of the input image should

also be divisible by N . Without special mention, the default
value of division number N for RCM is set to 7 in this paper.
The inﬂuence of choice of N is discussed in Section 4.4.

All models in experiments were trained for 180 epochs,
and learning rates decay by a factor of 10 for every 60
epochs. At test time, RCM is disabled, and the networks
structures for adversarial loss and region construction are
removed. The input images are center cropped and then
feed into the backbone classiﬁcation network for ﬁnal pre-
dictions.

4.2. Performance Comparison

The results on CUB-200-2011, Stanford Cars, and
FGVC-Aircraft are presented in Table 1. Considering that
some of the compared methods use image-level labels or
bounding box annotations, the information of extra anno-
tations is also presented in parentheses for direct compar-
isons. The single model and single crop performance of our
proposed DCL achieved state-of-the-art with no extra anno-
tation on all of the three datasets.

We set α = β = 1 for all experiments reported in this pa-
per. For non-rigid objects recognition tasks like CUB-200-
2011, the correlation among different regions is important
for building deep understanding about objects. Thus we set
γ = 1. While for rigid objects recognition tasks like Stan-
ford Cars and FGVC-Aircraft, parts of objects are discrim-
inative and complementary. Thus object and part location
may play a signiﬁcant role [34]. We set γ = 0.01 for rigid
objects recognition tasks to highlight the role of destruction
learning in learning detail visual representations from dis-
criminative regions. Different from other ﬁne-grained cate-
gories like bird and car, the structure of aircraft can change
with their design signiﬁcantly [18]. For example, the num-
ber of wings, undercarriages, wheels per undercarriage, en-
gines, etc. varies. Thus we set N as 2 for DCL on FGVC-
Aircraft in Table 1 to retain the structure information to a
certain extent.

Tables 1, 2 show that our ResNet-50 baseline is already
very competitive. Luckily, our proposed DCL can still out-
perform the strong baseline with a large margin (e.g., 2.3%
absolute improvement on average) on all of the three tasks.

4.3. Ablation Studies

We conduct ablation studies to understand different com-
ponents in our proposed DCL. We design different runs in
three datasets using ResNet-50 as the backbone network
and report the results in Table 2. The results show that the
proposed DCL boosts the performance signiﬁcantly. The
performance improvement caused by destruction learning
(DL) proves that a well-structured visual feature space that
distinguishing the noisy visual pattern, detail visual pattern
and the global visual pattern are beneﬁcial to ﬁne-grained
recognition task. Likewise, the shape and constitution in-

43265162

Method

Base Model

CUB-200-2011

Accuracy (%)
Stanford Cars

FGVC-Aircraft

CoSeq(+BBox) [13]
FCAN(+BBox) [17]
B-CNN [16]
HIHCA [2]
RA-CNN [7]
OPAM [21]
Kernel-Pooling [5]
Kernel-Pooling [5]
MA-CNN [42]
DFL-CNN [33]

DCL
DCL

VGG-19
ResNet-50
VGGnet
VGG-16
VGG-19
VGG-16
VGG-16
ResNet-50
VGG-19
ResNet-50

VGG-16
ResNet-50

82.8
84.7
84.1
85.3
85.3
85.8
86.2
84.7
86.5
87.4

86.9
87.8

92.8
93.1
91.3
91.7
92.5
92.2
92.4
91.1
92.8
93.1

94.1
94.5

-
-

84.1
88.3
88.2

-

86.9
85.7
89.9
91.7

91.2
93.0

Table 1. Comparison results on three different standard datasets. Base Model means the backbone network used in the method.

Method

ResNet-50
+ RCM
DL
CL
DCL

Accuracy (%)

CUB CAR AIR

85.5
86.2
87.2
86.7
87.8

92.7
93.4
94.4
94.1
94.5

90.3
89.9
91.6
90.7
92.2

Table 2. Ablation studies of the proposed method regarding recog-
nition accuracy on three different datasets. ResNet-50: ResNet-
50 ﬁnetuned on ﬁne-grained tasks. + RCM: Model trained by
Lcls. DL: Model trained by Lcls + Ladv. CL: Model trained
by Lcls + Lloc. DCL: Model trained by L.

N=Divisor( 448
32 )

CUB

CAR

AIR

1
2
7
14

85.5% 92.7% 90.3%
86.5% 93.5% 93.0%
87.8% 94.5% 92.2%
85.7% 93.0% 92.1%

Table 3. The recognition accuracy on three datasets of the pro-
posed method by using different N .

formation of objects modeled by construction learning (CL)
can further improve the performance of ﬁne-grained classi-
ﬁcation model. Moreover, the adversarial learning and re-
gion construction are highly complementary.

4.4. Discussions

Partition Granularity (N ): The number of partitions
N for RCM is an important parameter for the proposed
method. Table 3 shows the recognition accuracy on three
datasets with all feasible N with the size of input images
448 × 448.

It can be observed that the recognition accuracy in-
creases ﬁrst and then decreases while N increases. The best
performance is achieved at N = 7 on CUB-200-2011 and
Stanford Cars. For experiments on FGVA-Aircraft, our pro-

posed method can still get better performance than the state-
of-the-art method with 0.5% absolute improvement even if
we set N = 7. In general, if we set N as a small num-
ber, the advantage of our proposed method would likely to
be restricted. On the other hand, if we set N bigger, the
visual patterns can be learned from regions would be more
limited, and the region construction network would be more
difﬁcult to converge. In particular, the performance of our
proposed DCL is equivalent to ResNet-50 baseline when
setting N = 1.

Ratio of Destructed Images in a Min-batch: The de-
fault ratio of original images and destructed images in a
min-batch is set as 1 : 1. Table 4 shows the recognition ac-
curacy on CUB-200-2011 with this ratio ranging from 1 : 0
to 0 : 1. As shown, the performance decreases by a large
margin when we set the ratio as 0 : 1, since there is no
global context information in the training data.

Ratio

1:0

1:1

1:2

1:3

0:1

Accuracy(%)

85.5

87.8

86.8

86.5

84.1

Table 4. The recognition accuracy on CUB-200-2011 of the model
trained with different composition of training samples. The ratio
represents the proportion of original images and images with RCM
in one batch.

Feature Visualization: We visualize the feature maps of
the last convolution layer in Figure 5. Comparing the fea-
ture maps from baseline model and proposed method, we
can ﬁnd that the feature map responses of DCL are more
concentrated in discriminative regions. With different shuf-
ﬂing, the discriminative regions can be consistently high-
lighted by DCL based model, which demonstrating the ro-
bustness of our DCL method.

Object Localization: We also tested DCL on weakly
supervised object localization task on VOC2007 dataset us-
ing SPN [43]. We choose Pointing Localization Accu-
racy (P LAcc) as the evaluation criterion, which measures

43275163

Method

VGG-16

Center [24]
Deconv [37]
Grad [24]
c-MWP [38]
SPN [43]
DCL

69.5
75.5
76.0
80.0
87.5
88.1

Table 5. Pointing localization accuracy (%) on VOC2007 test set.
Center is a baseline method which uses the image centers as esti-
mation of object centers.

k

0

1

2

3

4

5

6

Acc.(%)

85.5

86.7

87.8

87.6

87.4

87.3

87.2

Table 6. The recognition accuracy on CUB-200-2011 of the model
trained with different value of k.

creases. The best performance is obtained at k = 2.
In
particular, the accuracy decreased slowly when k increased
from 2 to 6, which indicates that our method is not particu-
larly sensitive to k.

Model Complexity: During training, DCL only re-
quires a simple operation (RCM) and two lightweight net-
work structures (Adversarial Learning Network and Re-
gion Alignment Network). For ResNet-50 + DCL, there
are 8,192 new parameters introduced by DCL, which is
only 0.034% more parameters than the baseline ResNet-
50. Since there are only negligible additional parameters in
DCL, the network is efﬁcient to train. Moreover, it takes the
same number of iterations as the baseline for ﬁnetuning the
network upon convergence.

During testing, only the backbone classiﬁcation network
is activated. Compared with the ResNet-50 baseline, our
method yields a signiﬁcantly better result (+2.3%) with the
same time cost at inference, which adds extra practical value
to our proposed method.

5. Conclusion

In this paper, we propose a novel DCL framework for
ﬁne-grained image recognition. The destruction learning in
DCL enhances the difﬁculty of recognition to guide the net-
work learn expert knowledge for ﬁne-grained recognition.
While the construction learning can model the semantic cor-
relation among parts of the object. Our method does not re-
quire extra supervision information and can be trained end-
to-end in one stage. Extensive experiments against state-
of-the-art methods exhibit the superior performances of our
method on various ﬁne-grained recognition tasks. Also, our
proposed method is lightweight, easy to train, agile for in-
ference and has a good practical value.
Acknowledgement. This work is partly supported by Na-
tional Natural Science Foundation of China No. 61602463.

43285164

Figure 5. Visualization of the feature maps from the last con-
volution layer of ResNet-50. For each dataset, the ﬁrst column
shows the orignal image and two destructed versions; the 2nd and
3rd columns show the feature maps of two ﬁlters from baseline
ResNet-50; the 4th and 5th columns show the feature maps of two
different ﬁlters from the ResNet-50 guided by DCL. This ﬁgure is
best viewed in color.

whether the network can locate the correct regions of the
target. The experimental results is shown in Table 5. We can
ﬁnd that, after applying DCL, P LAcc was improved from
87.5% to 88.7%, which serves another numerical evidence
that DCL is helpful to learn correct regions.

Destruction Hyperparameter (k): Since RCM in our
proposed method requires the selection of a hyperparameter
k, we conduct experiments to study the sensitivity of clas-
siﬁcation performance to the choice of k in Table 6. The
recognition accuracy improved, and then decreased as k in-

ImagesBaselineDCLReferences

[1] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs,
and P. N. Belhumeur. Birdsnap: Large-scale ﬁne-grained
visual categorization of birds.
In 2014 IEEE Conference
on Computer Vision and Pattern Recognition, pages 2019–
2026, June 2014.

[2] S. Cai, W. Zuo, and L. Zhang. Higher-order integration of
hierarchical convolutional activations for ﬁne-grained visual
categorization. In 2017 IEEE International Conference on
Computer Vision, pages 511–520, Oct 2017.

[3] W. Catherine, B. Steve, W. Peter, P. Pietro, and B. Serge. The
caltech-ucsd birds-200-2011 dataset. (CNS-TR-2011-001),
2011.

[4] Y. Chaojian, Z. Xinyi, Z. Qi, Z. Peng, and Y. Xinge. Hier-
archical bilinear pooling for ﬁne-grained visual recognition.
pages 595–610, 2018.

[5] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie.
Kernel pooling for convolutional neural networks. In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3049–3058, July 2017.

[6] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual
representation learning by context prediction. In 2015 IEEE
International Conference on Computer Vision, pages 1422–
1430, Dec 2015.

[7] J. Fu, H. Zheng, and T. Mei. Look closer to see bet-
ter: Recurrent attention convolutional neural network for
ﬁne-grained image recognition.
In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, pages 4476–
4484, July 2017.

[8] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact
bilinear pooling.
In 2016 IEEE Conference on Computer
Vision and Pattern Recognition, pages 317–326, June 2016.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 770–778, June
2016.

[10] C. Huang, Z. He, G. Cao, and W. Cao. Task-driven pro-
gressive part localization for ﬁne-grained object recognition.
IEEE Transactions on Multimedia, 18(12):2372–2383, Dec
2016.

[11] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn for
ﬁne-grained visual categorization. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, pages 1173–
1182, June 2016.

[12] S. Kong and C. Fowlkes. Low-rank bilinear pooling for ﬁne-
grained classiﬁcation.
In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 7025–7034,
July 2017.

[13] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations.
In 2015 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5546–5555, June 2015.

[14] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization.
In 2013 IEEE
International Conference on Computer Vision Workshops,
pages 554–561, Dec 2013.

[15] G. Lample, A. Conneau, L. Denoyer, and M. Ranzato. Un-
supervised machine translation using monolingual corpora
only. 2018.

[16] T. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In 2015 IEEE Inter-
national Conference on Computer Vision, pages 1449–1457,
Dec 2015.

[17] X. Liu, T. Xia, J. Wang, and Y. Lin. Fully convolutional at-
tention localization networks:efﬁcient attention localization
for ﬁne-grained recognition. CoRR, abs/1603.06765, 2016.

[18] S. Maji, E. Rahtu, J. Kannala, M.B. Blaschko, and A.
Vedaldi. Fine-grained visual classiﬁcation of aircraft. CoRR,
abs/1306.5151, 2013.

[19] N. Mehdi and F. Paolo. Unsupervised learning of visual rep-
resentations by solving jigsaw puzzles. In Computer Vision
– ECCV 2016, pages 69–84, Cham, 2016. Springer Interna-
tional Publishing.

[20] S. Ming, Y. Yuchen, Z. Feng, and D. Errui. Multi-attention
multi-class constraint for ﬁne-grained image recognition.
pages 834–850, 2018.

[21] Y. Peng, X. He, and J. Zhao. Object-part attention model
for ﬁne-grained image classiﬁcation. IEEE Transactions on
Image Processing, 27(3):1487–1500, March 2018.

[22] Pau Rodr´ıguez, Josep M Gonfaus, Guillem Cucurull, F
XavierRoca, and Jordi Gonzalez. Attend and rectify: a
gated attention mechanism for ﬁne-grained recovery. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 349–364, 2018.

[23] X. Shu, J. Tang, G. Qi, Z. Li, Y. Jiang, and S. Yan. Image
classiﬁcation with tailored ﬁne-grained dictionaries.
IEEE
Transactions on Circuits and Systems for Video Technology,
28(2):454–467, Feb 2018.

[24] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Deep inside convolutional networks: Visualising image clas-
siﬁcation models and saliency maps. 2013.

[25] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[26] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D.
Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Go-
ing deeper with convolutions. In 2015 IEEE Conference on
Computer Vision and Pattern Recognition, pages 1–9, June
2015.

[27] C. Tianshui, W. Wenxi, G. Yuefang, D. Le, L. Xiaonan, and
L. Liang. Fine-grained representation learning and recogni-
tion by exploiting hierarchical semantic embedding. In The
26th ACM International Conference on Multimedia, MM
’18, pages 2023–2031. ACM, 2018.

[28] J. Wang, J. Fu, Y. Xu, and T. Mei. Beyond object recognition:
Visual sentiment analysis with deep coupled adjective and
noun neural networks. August 2016.

[29] Y. Wang, J. Choi, V. I. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, pages 1163–1172, June 2016.

[30] Xing Wei, Yue Zhang, Yihong Gong, Jiawei Zhang, and
Nanning Zheng. Grassmann pooling as compact homoge-
neous bilinear pooling for ﬁne-grained visual classiﬁcation.

43295165

In Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 355–370, 2018.

[31] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang.
The application of two-level attention models in deep convo-
lutional neural network for ﬁne-grained image classiﬁcation.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition, pages 842–850, June 2015.

[32] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang.
The application of two-level attention models in deep convo-
lutional neural network for ﬁne-grained image classiﬁcation.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition, pages 842–850, June 2015.

[33] W. Yaming, M. Vlad I, and D. Larry S. Learning a discrim-
inative ﬁlter bank within a cnn for ﬁne-grained recognition.
In 2018 IEEE Conference on Computer Vision and Pattern
Recognition, pages 4148–4157, 2018.

[34] L. Yang, P. Luo, C. C. Loy, and X. Tang. A large-scale
car dataset for ﬁne-grained categorization and veriﬁcation.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition, pages 3973–3981, June 2015.

[35] Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao,
and Liwei Wang. Learning to navigate for ﬁne-grained clas-
siﬁcation.
In Proceedings of the European Conference on
Computer Vision (ECCV), pages 420–435, 2018.

[36] B. Yao, G. Bradski, and L. Fei-Fei. A codebook-free and
annotation-free approach for ﬁne-grained image categoriza-
tion. In 2012 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 3466–3473, June 2012.

[37] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In European conference on
computer vision, pages 818–833. Springer, 2014.

[38] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan
Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-
ral attention by excitation backprop. International Journal
of Computer Vision, 126(10):1084–1102, 2018.

[39] N. Zhang, R. Farrell, F. Iandola, and T. Darrell. Deformable
part descriptors for ﬁne-grained recognition and attribute
prediction. In 2013 IEEE International Conference on Com-
puter Vision, pages 729–736, Dec 2013.

[40] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding label
In 2016
structures for ﬁne-grained feature representation.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1114–1123, June 2016.

[41] B. Zhao, X. Wu, J. Feng, Q. Peng, and S. Yan. Diversiﬁed
visual attention networks for ﬁne-grained object classiﬁca-
tion. IEEE Transactions on Multimedia, 19(6):1245–1256,
June 2017.

[42] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention
convolutional neural network for ﬁne-grained image recog-
nition. In 2017 IEEE International Conference on Computer
Vision, pages 5219–5227, Oct 2017.

[43] Yi Zhu, Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin
Jiao. Soft proposal networks for weakly supervised object
localization. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 1841–1850, 2017.

43305166

