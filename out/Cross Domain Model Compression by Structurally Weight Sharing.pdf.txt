Cross Domain Model Compression by Structurally Weight Sharing

Shangqian Gao1, Cheng Deng2, and Heng Huang∗1,3

1Electrical and Computer Engineering Department, University of Pittsburgh, PA, USA

2School of Electronic Engineering, Xidian University, Xi’an, Shaanxi, China

shg84@pitt.edu, chdeng.xd@gmail.com, heng.huang@pitt.edu

3JD Digits

Abstract

Regular model compression methods focus on RGB in-
put. While cross domain tasks demand more DNN models,
each domain often needs its own model. Consequently, for
such tasks, the storage cost, memory footprint and compu-
tation cost increase dramatically compared to single RGB
input. Moreover, the distinct appearance and special struc-
ture in cross domain tasks make it difﬁcult to directly ap-
ply regular compression methods on it. In this paper, thus,
we propose a new robust cross domain model compression
method. Speciﬁcally, the proposed method compress cross
domain models by structurally weight sharing, which is
achieved by regularizing the models with graph embedding
at training time. Due to the channel wise weights sharing,
the proposed method can reduce computation cost without
specially designed algorithm. In the experiments, the pro-
posed method achieves state-of-the-art results on two di-
verse tasks: action recognition and RGB-D scene recogni-
tion.

Performance

Single domain 
compression

Single domain 
compression

Compression
rate

Performance

Cross domain 
weights sharing

Compression
rate

Figure 1: A demonstration of difference between our
method and single domain compression method. Upper ﬁg-
ure shows that when it comes to cross domain compression,
simply using regular compression method won’t achieve
satisfactory trade-off between performance and compres-
sion rate. Lower ﬁgure shows that sharing weights across
domain can achieve good result.

1. Introduction

In recent years, Convolution Neural Networks (CNNs)
have become very popular in many related ﬁelds, for in-
stance, image classiﬁcation [3, 23] , action recognition [37,
4] , self-driving cars [1], and so on. However, as CNNs go
deeper and deeper [10, 14], the memory footprint and com-
putational cost have increased dramatically, making it im-
practical to deploy on platform with limited resources such
as mobile phone and embedded device. To resolve such
problem, countless efforts have been made [8, 43, 7, 17].
These methods for CNN model compression can be sepa-
rated to four categories: pruning [8], sparsity induced reg-

∗Corresponding Author. S. Gao, H. Huang were partially supported
by U.S. NSF IIS 1836945, IIS 1836938, DBI 1836866, IIS 1845666, IIS
1852606, IIS 1838627, IIS 1837956.

ularization [43], weight quantization [7], and low rank fac-
torization [17].

Although compression techniques have been widely de-
veloped for RGB input. Cross domain applications are sel-
dom considered for applying compression algorithms. De-
spite little attention is given on cross domain tasks, the
memory cost and computation demand are even higher than
single RGB domain. Popular cross domain applications
like RGB-D scenes recognition [6], action recognition [37],
cross domain retrieval [20, 25] etc, usually use two or more
DNN models to collect domain speciﬁc information from
different sources. Thus the storage cost, memory footprint
and computation cost are at least two times higher than sin-
gle RGB task. As a result, it is worthy to explore how to get
compact models for cross domain tasks.

8973

In cross domain tasks, the distinct spatial structure and
appearance among different data sources often prohibit di-
rectly using mainstream compression methods.
Indeed,
when applied on cross domain tasks, mainstream compres-
sion methods have many drawbacks. Cross domain models
are extremely sensitive to channel wise pruning. The hyper-
parameter search is more difﬁcult for sparsity induced meth-
ods. Furthermore, mainstream compression methods can’t
utilize underlying cross domain relationships to achieve bet-
ter compression rate.

To tackle above problems, we propose a new cross do-
main compression method which is robust to hyperparame-
ter settings and can utilize cross domain relationships for
better model compression.
In the proposed method, the
weights are structurally shared across domains. To achieve
structured weight sharing, cross domain models are trained
with graph embedding regularization. After training com-
plete, the weights are clustered based on intermediate fea-
ture similarity graph. In the end, the cross domain models
are ﬁne-tuned to get ﬁnal result.

The main contribution of this paper can be summarized

to three aspects:

1. We identiﬁed the difﬁculties of cross domain compres-
sion when using regular sparsity induced methods as
well as pruning algorithms.

2. We proposed a new method specially tailored for cross
domain compression by using graph embedding as a
constraint at training time. Proposed method is robust
to hyperparameter tuning and it can naturally achieve
computation cost reduction.

3. Proposed method can achieve the best results on two
diverse tasks (action recognition and scene recogni-
tion) compared to other methods.

2. Related Works

The related work for this paper can be separated into two
different perspectives, the ﬁrst part is related to model com-
pression, and the second part is about cross domain tasks.

2.1. Model Compression

Pruning and weight sharing methods are most related to
our method. Thus, we mainly focused on these algorithms.
For weight sharing, there is a group of algorithms [33,
7, 16, 34, 50] studying how to clustering the scalar values
of weights into several clusters. This kind of algorithms is
also known as quantization. One of the earliest works [7]
which combines quantization and hamming coding comes
from this category. With weight quantization, the weights
can be reduced to at most 1 bit binary value [15] from 32-
bits ﬂoat point numbers. Many works show that quantizing
weights to 8-bits [49] often won’t hurt the performance. A

20

40

60

80

100

120

140

160

20

40

60

80

100

120

140

160

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

20

40

60

80

100

120

140

160

20

40

60

80

100

120

140

160

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

(a) visualization of weights cor-
relation

(b) visualization of features cor-
relation

Figure 2: (a) is the weights correlation of cross domain
MNIST experiment. (b) is the feature correlation of cross
domain MNIST experiment. From (a), (b), we can see that
model trained with GrOWL can’t capture the cross domain
information in inputs.

series of less popular approaches is about structured weight
sharing. Unlike quantization, structured weight sharing fo-
cus on ﬁnding the structure level similarity across chan-
nels or ﬁlters. Learning to share [47] belongs to this cat-
egory, which aims to ﬁnd similarities across input chan-
nels by using a regularization term called group weighted
order lasso (GrOWL) [31]. WSNet [21] tries to create a
shared ﬁlter bank instead of ﬁnding similarities. In audio
classiﬁcation tasks, WSNet can achieve state-of-the-art re-
sult. Our method for cross domain compression is close re-
lated to these methods. Despite the similarity between our
method and structured weight sharing methods, our method
is a fully weight-sharing approach unlike Learning to Share,
and the shared ﬁlter banks is learned during training, which
is also different with the pre-designed ﬁlter bank in WSNet.
For pruning weights, numerous researches [51, 26, 8, 13,
11] have shown that removing a large portion of connec-
tions or neurons won’t cause signiﬁcant performance drop.
Pruning algorithms often seek certain ways to introduce a
criterion for evaluating the relative importance of channel,
ﬁlter or individual weight. Then, such criterion is used
for pruning, where least important weights can be pruned.
Sparsity induced methods [43, 47, 28] can be regarded as
a similar methods compared to pruning.
In [43], group
lasso are used as a regularization at training time. After the
weights are close to zero, it can be safely pruned from the
network. But using sparsity constraints often results in near
zero solutions, some works [44] argue that small weights
are in fact important for preserving performance. Some
data-driven pruning methods [13] can avoid this problem
by designing the criterion based on the intermediate feature
maps. Other than data-driven approaches, certain optimiza-
tion methods [47] for sparsity constraints can alleviate this
problem too. From another perspective, the goal of pruning
algorithm is to reduce the unique weights in the model and
remove the others. Our method have the same goal here,
however, we don’t remove other weights, we make them

8974

share the same channel.

Besides weight pruning and sharing, other popular meth-
ods include matrix factorization [35], knowledge distilla-
tion [12, 45, 22], and variational inference approaches [29,
27].

2.2. Cross Domain Applications

In this paper, we focus on two types of cross domain
tasks, the ﬁrst one is two-stream action recognition, the sec-
ond is RGB-D scene classiﬁcation.

One of the most popular methods regarding action recog-
nition is two-stream CNNs. After [37] proposed a method
that uses RGB and stacked optical ﬂow frames as ap-
pearance and motion information respectively, this kind of
methods gets more and more attentions [42, 5]. Our cross
domain compression framework is based on this series of
works, because the architecture of these kind of methods
are close to image classiﬁcation task, which makes it possi-
ble to apply numerous compression methods on such meth-
ods. Another reason to choose two-stream methods is that
RNN based algorithms [4] for action recognition rely on
the features or outputs from corresponding RGB or ﬂow
CNN models and most memory usage and computation cost
come from CNN part. Other action recognition methods
like C3D [19], 3D-resnet [9] use 3-D convolution kernels to
learn spatial and temporal information together. But exist-
ing compression techniques are harder to be applied on 3D
CNN.

Scene classiﬁcation is one of the basic problems in com-
puter vision research. With cost affordable depth senor,
Kinect, depth images can be used in scene classiﬁcation
task. Compared to RGB images, depth images can provide
additional strong illumination and color invariant geometric
cues. RGB and depth images fusion then become a promis-
ing way for scene classiﬁcation. In this paper, we consider
score level RGB-D fusion [6, 18], leave the intermediate
feature maps untouched. RGB-D models are also suitable
to apply compression techniques.

3. Method

In this section, we ﬁrst show that previous weight sharing
methods like Learning to Share [47] can’t utilize underlying
cross domain relationships. Then, we will introduce our
method.

3.1. Learning to Share Revisit

In learning to share [47], they formulate the compression
problem as a regularization problem. Group Lasso related
methods have similar formulation, the regularization term
is different. The formulation can be represented as:

Here, in most classiﬁcation task L is a cross entropy loss
and R is the regularization term, fθ is a neural network pa-
rameterized by θ. For learning to share, the regularization
term is:

L

Nl−1

R(θ) =

λl,ikθl,ik,

(2)

Xl=1

Xi=1

is

and θl

lth layer,

the weight of

∈
where θl
Rwl×hl×Nl−1×Nl . w, h, Nl−1, Nl are the width, height,
number of input channels and number of output channels
in lth layer. The group is predeﬁned along channel dimen-
i=1 λlikθlik is a
special regularization term called Group Ordered Weighted
Lasso (GrOWL), which can force sparsity and learn under-
lying correlations among inputs at the same time.

sion. As we mentioned in section 2, PNl−1

A natural way to extend Learning to Share is to add
GrOWL regularization to cross domain models. We only
consider two domains in our experiments.
To verify
whether Learning to Share can learn cross domain corre-
lation within the inputs, we designed a simple task. In this
simple task, some modiﬁcation are done on MNIST and two
datasets MNIST-Rot (rotation by 45 degrees) and MNIST-
Blur (motion blurred) are created as two toy domains. The
weight correlation is calculated by:

S(i, j) =

θT
l,iθl,j

kθl,ik2kθl,jk2

.

(3)

The feature correlation is calculated in Eq. 5.

We use LeNet-5 on each domain. GrOWL is applied
it
except the ﬁrst layer and the last layer.
clearly shows that after GrOWL regularization the correla-
tion across weights from different domain model is close to
zero, which indicates that GrOWL can’t utilize underlying
cross domain relationships.

In Fig. 2,

Besides such drawback, hyper-parameter tuning is difﬁ-

cult and each layer has its own λl.

3.2. Cross Domain Task

To better explain our method, a formal deﬁnition of cross
domain task is given. We use same network architecture
on two domains except for the ﬁrst layer, since inputs may
have different number of channels. A typically DNN layer
can be deﬁned as a function parameterized by its weights,
which can be expressed as: yl = fθl (xl). Without loss
of generality, the model in ﬁrst domain can be deﬁned as
yA,l = fθA,l (xA,l). The second model can be deﬁned by
replacing A with B. Suppose the dataset D have m sam-
ples: D = {(xA1 , xB1 , y1), . . . , (xAm , xBm , ym)}. Then
the objective function has the form:

min
θA,θB

L(fθA (xAi ), fθB (xBi )) + R(θA, θB),

(4)

min

θ

L(fθ(x)) + R(θ).

(1)

where L is cross domain task loss, and R is regularization
loss.

8975

Shared weight space

2.2
1.3
1.6
1.3

2.3
2.0
0.5
0.7

1.6
1.8
0.4
-1

1.9
2.4
0.6
0.3

…

…

…

…

Neuron wise 
weight sharing

(cid:2194)(cid:3398)(cid:2778)th layer
(cid:2194)th layer

…

…

…

…

Fully connected layer weight sharing

Weights

(cid:2194)th layer weights of domain A
(cid:2191)th channel for domain A

Channel wise 
weight sharing

…

…

Shared weight 
space

(cid:2191)th channel for domain B
(cid:2205)(cid:2194)(cid:2190)(cid:2194)

(cid:2194)th layer weights of domain B

(cid:2185)(cid:2194)

Convolutional layer channel wise weight sharing

Figure 3: Left ﬁgure shows weight sharing in fully connected layers. Right ﬁgure shows weight sharing in convolution
layers.

3.3. Graph Embedding as a Regularization

In section 3.1, we argue that Learning to Share is not
sufﬁcient for cross domain tasks not only because they
can’t discover cross domain correlation but also the hyper-
parameter tuning is too time-consuming. Similar argument
can be applied to Group Lasso method. When training
model with GrOWL and Group Lasso, all weights in a layer
often become zeros. If this happens, one have to adjust the
hyper-parameter to train it again.

Hence, to solve these two problems, we aim to compress
model by structured weight sharing. During training, the
model is regularized with graph embedding constraint. Af-
ter the model is fully trained, we cluster the weights ac-
cording to transformed features. If we use fully shared ap-
proach, we won’t suffer from the problem of training insta-
bility mentioned above. Fully shared approach won’t turn
all the weights in a layer to zero.

Algorithm 1 Graph Embedding Regularization

1: input: Middle layer output, xA

l or xB

l , l = 1, . . . , L;

Data set D with (xA

i , xB

i , yi), i = 1, . . . , m

t,l = Trim(xA
l )
t,l = Trim(xA
l )

2: initialization : fA, fB, RSpectral
3: for epoch = 1 to N
4: xA
5: xA
6: RSpectral = RSpectral(concate(xA
7: min
θA,θB

t,l, xB
L(fθA(xA), fθB (xB)) + RSpectral

t,l), θs)

8: end for
9: output: fA, fB, RSpectral

show how we represent intermediate features. A naive way
to represent similarity between input channels is to calcu-
late the correlation between input features. Given an input
of lth layer xl ∈ RWl×Hl×Cl , Wl is the width of feature
map and Hl is the height of feature map, Cl is the num-
ber of input channels. Suppose the number of data points
in D is m, the inputs cross all samples can be represented
as Xl ∈ RWl×Hl×Cl×m, this Xl can be reshaped to a 2D
representation X 2D
∈ RCl×mWlHl . We can represent the
similarity between input channels as below:

l

Sxl (i, j) =

X 2D

l
kX 2D

l

(i, :)T X 2D
(i, :)k2kX 2D

l

(j, :)
(j, :)k2

l

.

(5)

In Eq.5, if input channel i and j is similar, then the inner
product between xl from all samples should be large too.
However, the computation cost is expensive if xl is large.
For example, if l is the 13th layer of VGG-16, and we have
5 × 104 samples, then each vector in X 2D
l will have 2.45
million dimensions. The computation cost will prohibit us
to update input feature similarity matrix when training.

To make the update of input feature similarity matrix af-
fordable, we apply average pooling to the feature map to re-
duce the size of it. If the feature map has size W × H × C,
then the reduced feature map has size w × h × C, where
wh is much less than W H. The size of feature map can
be further reduced by random sample part of it. By doing
so, the computation of similarity matrix is largely decreased
and we call this operation Trim. For each input xl, trimmed
input feature map xt,l is:

xt,l = T rim(xl).

(6)

Before introducing graph embedding constraint, we ﬁrst

The similarity calculation is same in Eq. 5 with xl replaced
by xt,l. During training, we replace m with the batch size

8976

b for forward and backward calculation. After we have the
similarity matrix between input channels of a layer, we try
to cluster the weights according to the similarity map. Di-
rectly clustering weights on similarity map can result in per-
formance drop. For this reason, graph embedding can be
used as a constraint. Another reason we can use graph em-
bedding [30, 41] is that it’s well known for clustering on
similarity graph which we already have.

Within the scope of graph embedding, similar formula-
tion from SpectralNet [36], a recent proposed deep spec-
tral clustering method, is used. Spectral clustering can be
inserted into R in Eq. 3 and regularize the complexity of
the model.
In below, the detail of graph embedding reg-
ularization is given. As above mentioned, we use trun-
cated input feature map to enable affordable intermediate
layer similarity calculation. The intermediate similarity ma-
trix calculation for cross domains uses Eq.5 by replacing
X 2D
(i, :)). ‘concate’
is a simple operation to join two vectors into one vector.

(i, :) = concate(X A,2D

(i, :), X B,2D

t,l

t,l

l

Then, the spectral clustering can be applied on interme-
diate similarity graph. Given a speciﬁc layer l, the graph
embedding constraint has such form:

Input for (cid:2194)th layer

(cid:2159)(cid:2194)

weights for (cid:2194)th layer

(cid:2159)(cid:2194)

Convolution

Reduce number of 
input channels

Shared weights (cid:2189)(cid:2191)

Produce the same 
outputs

Convolution

(cid:2193)(cid:2194)

Reduced input for (cid:2194)th layer

(cid:2193)(cid:2194)

Reduced weights for (cid:2194)th layer

Figure 4: Illustration of how to reduce computation cost
for our proposed method. It can be understood as reducing
the number of input channels in feature maps and weights.
Original and reduced version can both produce the same
output.

RSpectral =

L

Xl=1

1

4C 2

l Xi,j=1:2Cl

Sl(i, j)kzl,i − zl,jk2
2,

(7)

where Sl ∈ R2Cl×2Cl is the similarity matrix of lth layer
inputs across two domains, Cl is the number of channels
in input xA
l . zl ∈ R2Cl×kl is the output of spectral
clustering, kl is the target number of clusters for layer l. For
spectral clustering, there is an additional constraint on zl:

l or xB

1
2Cl

zT
l zl = I.

(8)

And it requires to compute eigendecomposition on Sl to get
zl. However, the eigendecomposition is time-consuming
to compute. Similar to SpectralNet, we use a neural net-
work fsl with a orthogonal layer to approximate the eigen-
decomposition. The orthogonal output is achieved by using
Cholesky decomposition, interested readers can refer to Ap-
pendix B in [36]. By inserting fsl:

zl,i = fsl(X 2D

l

(i, :)).

(9)

l

t,l

(i, :) = concate(X A,2D

As aforementioned, X 2D
(i, :
), X B,2D
(i, :)). Simply use standard spectral clustering
may cause the unbalance of clusters, which will limit the
capacity of the model. Alternatively, we use normalized
spectral clustering to impose balanced clusters.

t,l

RSpectral =

L

Xl=1

1

4C 2

l Xi,j=1:2Cl

Sl(i, j)k

zl,i
di

−

zl,j
dj

k2
2,

(10)

where di = P2Cl

i

our method can be expressed as:

Sl(i, j). The ﬁnal objective function for

min
θA,θB

L(θA, θB) + RSpectral,

(11)

where RSpectral and L(θA, θB) are deﬁned in Eq.10 and
Eq.4 separately.

3.4. Weight Sharing

After training the model with objective function Eq. 11,
we are ready to cluster the features according to the zl ∈
R2Cl×kl for each layer. As normal spectral clustering pro-
cess, we use K-means to cluster the features based on zl.
Since we have the clusters of features, it can be used to
guide the clustering of weights. If channels i, j from inter-
mediate features are in the same cluster, weights of channels
i, j will also have the same cluster. The detailed sharing
process is depicted in Fig. 3. Once weight clustering ﬁn-
ished, we ﬁne-tune the model according to clustering result.
Suppose the ith group of weights in layer l have nl,i input
channels, the weights in ith group is replaced by the centers
gl,i of this group. The gradient computation of centers is:

∂L
∂gl,i

=

1

nl,i Xθl,j ∈Gl,i

∂L
∂θl,j

,

(12)

where Gl,i is the set containing all instances in this group.

3.5. Improve Inference Speed

In this work, we mainly focus on compressing the model
instead of reduce computation cost, but we still can achieve

8977

(a) RGB and optical ﬂow frames

(b) RGB and depth images

Figure 5: Example of dataset images, (a) is the RGB and
optical ﬂow images within UCF-101 dataset, (b) is the RGB
and depth images from SUN RGB-D dataset

moderate improvement concerning computation cost. It can
be shown that we can reduce the number of channels by a
fraction of Cl
. Unlike WSNet, we don’t need a special de-
kl
signed algorithm to reduce computation cost. The weight
channels in the same group can be replaced by one chan-
nel, and corresponding feature maps can be replaced by one
feature map averaging all feature maps in the group. Such
replacement won’t change the outputs. Details are shown in
Fig. 4.

are based on the implementation of [52]. Sparsity induced
methods are only applied on scene classiﬁcation task, since
in action recognition task, we can’t ﬁnd suitable hyper-
parameters for GrOWL or Group Lasso, some of layers
always become zero whether we use proximal gradient or
soft-thresholding as optimization method.

For SUN-RGBD dataset, we train model with graph em-
bedding constraint for 100 epochs with batch size of 128.
SGD with momentum is used as optimizer, momentum is
set to 0.9 and start learning rate is 0.03. Learning rate is de-
cayed by a factor of 0.1 for every 30 epochs. After training
completely, weights sharing are performed as described in
section 3.4. In weight sharing stage, the model is ﬁne-tuned
for 60 epochs with the same optimizer and learning rate is
set as 3 × 10−3 with the same scheduler.

For action recognition dataset, the models are trained on
each domain separately with the settings in [42] and ﬁve-
crops data augmentation. The models are put together and
trained with graph embedding constraints for 80 epochs
with SGD and momentum 0.9, the start learning rate is
1 × 10−4 and batch size is 32. After clustering, models
are ﬁne-tuned with the same learning rate for 60 epochs.

3.6. Beneﬁt of Cross Domain Sharing

4.2. Datasets

As we described above, one of the beneﬁts of weight
sharing is that it provides a natural way to speed up at in-
ference time. Another advantage of cross domain weight
sharing is it allows larger model capacity compared to any
other single domain compression method. For a speciﬁc
layer with input size ninput, if we want a 20× compres-
sion rate, for single model compression, we can only keep
5% of the weights for each model, but for cross domain
weight sharing, we can have 0.1ninput clusters, which is
two times more than single domain compression method.
Notice that weight sharing is key to achieve such result.
The relative larger model capacity is especially important
if required compression rate is extreme.

SUN-RGBD Dataset [39] contains 10,355 RGB and
Depth image pairs captured from different cameras. We
follow the experimental settings in [18]. 19 categories are
kept for our experiments with 4,845 images for training and
4,659 images for testing.

UCF-101 Dataset [40] comprises of realistic videos col-
lected from Youtube. It contains 101 action categories, with
13,320 videos in total (9,537 videos for training, the rest for
testing). UCF-101 split-1 is used for training and testing.

HMDB-51 Dataset [24] contains a total of about 7,000
video clips distributed in a large set of 51 action categories.
Each category contains a minimum of 101 video clips. We
use split-1 in ofﬁcial release of HMDB-51 dataset.

4. Experiment

4.3. RGB D Scene Classiﬁcation

We assess the proposed method on three different
datasets with two tasks. We compare our method with a
series of pruning and sparsity induced methods. The prun-
ing algorithms including structured weight pruning [13, 26]
and individual weight pruning [51, 8]. The reason why we
only compare pruning and sparsity induced method is that
these methods are the majority of model compression al-
gorithms. Moreover, quantization methods focus on sin-
gle weight value sharing and can be applied on the basis of
pruning algorithms and the proposed method.

4.1. Implementation Details

Our method and related comparison methods are all im-
plemented in pytorch [32], some of the comparison methods

For SUN-RGBD dataset. we follow the same experiment
setting in [18]. HHA images are extracted follows [6]. As
we discussed in Section 3, we calculate average score fu-
sion across two domains. Also class weighted cross entropy
is used as a common practice, the weight for each class is
given by w(t) = Ncmax −Ncmin
+τ , where N (t) is the number
of examples in tth class, cmax is the class with most sam-
ples, cmin is the class with least samples. For both domains,
we use AlexNet pre-trained on Placed365 dataset [48].

Nt−Ncmin

In Table 1, we list the network settings for Sun RGB-
D dataset. kA and kB are two different settings for our
method. The setting for GrOWL is the result after train-
ing with GrOWL regularization. The number in the list is
the unique input channels for cross domain models.

8978

Table 1: Network settings for AlexNet [23] on SUN RGB-D
dataset.

comparison we set the pruning rate (p-rate in Table 4) equal
to 0.3, 0.5 or 0.75 separately.

Layer

original

conv1
conv2
conv3
conv4
conv5

fc1
fc2
fc3

6

128
384
784
512

18432
8192
8192

kA
6
32
96
192
128
1024
512
8192

kB
6
16
48
96
64
512
512
8192

GrOWL

6
12
12
21
72

1037
423
8192

Table 2: Results of SUN RGB-D dataset.

Method

Original

GrOWL [47]

Ours kA
Ours kB

Performance Rate

47.32%
44.28%
47.21%
47.01%

1

17.6
14.8
22.8

In Table 2, it can be shown that the performance of
GrOWL is lower than our proposed method by near 3%.
Even though the compression rate of GrOWL is similar to
setting kB of our method. This shows that, for cross do-
main models, sparsity induced method usually gives sub-
optimal solutions for cross domain compression. Further-
more, our method can be regarded as GrOWL without spar-
sity. In this experiment, we give two settings kA and kB for
our method. Though, the compression rate is variant, only
little difference is observed for performance, which shows
that our method is robust against hyper-parameter tuning.
On the other hand, GrOWL is sensitive to hyper-parameter,
the result in Table 2 is achieved by more than ten rounds
of experiments given different hyper-parameter settings in
GrOWL.

4.4. Action Recognition Dataset

For action recognition tasks, during training we combine
two popular methods TSN [42] and two-stream [5]. VGG-
16 is used for action recognition task. As in [5], we use
5-crops data augmentation in training. The optical ﬂow im-
ages are extracted based on [46]. Following TSN, we split a
video into three segments, and random samples RGB frame
for each segment. Once we have the index of RGB frame,
we sample the same index and following 10 frames in hori-
zontal and vertical optical optical ﬂow. The horizontal and
vertical ﬂow images are stacked to a 224 × 224 × 20 cubic
to feed into optical ﬂow DNN model.

For our method, we set hyperparameter kl = 2Cl

r . r
is set to 2, 4 or 8 for different settings. For a relative fair

Table 3: Network settings for VGG-16 [38] of action recog-
nition dataset for proposed method

Layer

original

conv1
conv2,3
conv4,5

conv6 to 8
conv9 to 13

fc1
fc2
fc3

23
128
256
512
1024
50176
8192
8192

kA
23
32
64
128
256
1024
512
8192

kB
23
32
64
128
128
512
512
8192

kC
23
16
16
64
64
256
256
8192

Table 4: Network settings for VGG-16 [38] on action recog-
nition dataset for comparison methods.

Layer

original

p-rate 0.3

p-rate 0.5

p-rate 0.75

conv1
conv2,3
conv4,5

conv6 to 8
conv9 to 13

fc1
fc2
fc3

3
64
128
256
512

25088
4096
4096

3
44
90
180
358

17561
2867
4096

3
32
64
128
256

12544
2048
4096

3
16
32
64
128
6272
1024
4096

In Tables 3 and 4 we list the detail of target network
structure of our method and comparison methods. The ma-
jor difference between Table 3 and Table 4 is that in Table
3, all the settings are for both domains, on the contrary,
4 are only for single domain. For example, in conv2 of
kA, we have 32 unique channels for 128 channels in both
RGB and optical ﬂow models. In conv2 of p-rate 0.5, 32 is
also given here, this is only for RGB or optical ﬂow model,
for both models, at p-rate 0.5, there are 64 unique chan-
nels in weight matrix. Table 5 shows the results for UCF-
101 dataset and HMDB-51 dataset. The number follow-
ing comparison methods is the pruning rate (p-rate) for the
method. For example, ‘prune or not prune 0.5’ means prune
or not prune method at pruning rate of 0.5. Clearly, our
method can achieve the best results (trade off between per-
formance and compression rate) compared two all the other
methods. Morever, individual weight pruning algorithms is
signiﬁcant better than group weight pruning algorithms (al-
most 10% absolute improvement). Group weight sharing
methods like Apoz [13] and Efﬁcient Network [26] often
suffer from large performance drop (6% to 10% compared
to original) even only with a small fraction of pruning-rate

8979

Table 5: Overall results for action recognition dataset

Method

Performance Rate

UCF-101 Dataset

Original

Prune or not prune 0.5 [51]

Sensitity [8] 0.5

Efﬁcient convnet[26] 0.5

Apoz 0.5 [13]

Prune or not prune 0.75 [51]

Sensitity 0.75[8]

Efﬁcient convnet [26] 0.75

Apoz 0.75 [13]

Ours kA
Ours kB
Ours kC

Original 5-crops
Ours kB 5-crops

88.52%
87.7%
87.9%
78.3%
79.6%
83.8%
77.9%
58.9%
69.6%
88.21%
88.9%
87.7%
90.8%
91%

HMDB51 Dataset

Original

Apoz 0.3 [13]

Efﬁcient convnet 0.3 [26]

Apoz 0.5 [13]

Efﬁcient convnet 0.5 [26]

Ours kB
Ours kC

Original 5-crops
Ours kB 5-crops

57.51%
53.6%
51.8%
47.7%
20.8%
57.4%
56.9%
59.9%
59.8%

1
2
2
2
2
4
4
4
4
12
23
46
1
23

1

1.42
1.42

2
2
23
46
1
23

(0.3 or 0.5). These observations are inconsistent with single
RGB model pruning results. At least, at pruning rate 0.3 or
0.5, many algorithms can maintain the performance. There
might be many reasons for this phenomena, the model ca-
pacity required for non-RGB domain might be larger than
RGB domain, thus, pruning some channels may hurt the
performance severely. Another possibility is the difﬁculty
of the dataset, HMDB-51 is believed to be more difﬁcult
than UCF-101. As a result, it’s not easy to keep perfor-
mance on HMDB-51 dataset.

Another interesting phenomena is that our method are
robust to a set of different hyperparameters. The perfor-
mance starts to drop (less than 1% absolute performance
lost) after a relative high compression rate (46 times). For
5 different settings across three datasets, the largest differ-
ence before and after compression is 0.8%. In setting kB of
UCF-101, our method is better than original by 0.4%. Over-
all speaking, our method is much easier for hyperparameter
searching compared to sparsity induced method, and it can
achieve better trade-off compared to pruning algorithms.

60

50

40

30

20

10

0

1

2

3

4

5

6

7

8

9

10

600

500

400

300

200

100

0

1

2

3

4

5

6

7

8

9

10

(a) setting A

(b) simple constraint

Figure 6: Group size of largest 10 groups in layer conv13
of VGG-16. Setting A, in ﬁgure (a), can achieve 88.2%.
Simple constraint in ﬁgure (b) can achieve 87.5%. Random
group can achieve 87.3%.

4.5. Study of group size

Our method are further compared with random sharing
and simple similarity constraint. Naively, given a similarity
map Siml at layer l, we deﬁne:

Rs =(1 − Sl(i, j),

Sl(i, j)

if Sl(i, j) ≤ t,
otherwise.

(13)

This indicates that we push the feature maps and weights
closer if their channel similarity is greater than t. t is set as
0.3. Using such constraint will result in highly unbalanced
group in the compressed model. From Fig. 6, it is obvi-
ous that large and unbalanced group hurt the performance
and make the results close to random sharing. This shows
that one key ingredient for our method is to have balanced
groups.

There are some groups with group size 1 in both do-
mains. This can be regarded as domain private parts which
only captures domain speciﬁc information. In domain sepa-
ration networks [2], one can ﬁnd similar arguments. Result-
ing compressed model can be separate into two parts, do-
main common parts and domain separate parts. Following
this argument, our method can be viewed as an approach to
identify domain common part within cross domain models.
Domain common part is essential for cross domain model
compression, since it can be reused across different domain.

5. Conclusion

In this paper, we solve the problem of model compres-
sion in cross domain settings. To achieve such goal, we use
graph embedding as a regularization for cross domain mod-
els. The weights are structurally shared according to the re-
sults of clustered features. Our method can achieve the state
of the art result on compression rate with little performance
loss on two different tasks. Group size within each layer is
identiﬁed to be one of the key elements to the success of our
method.

8980

References

[1] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner,
B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller,
J. Zhang, et al. End to end learning for self-driving cars.
arXiv preprint arXiv:1604.07316, 2016.

[2] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In Advances in Neu-
ral Information Processing Systems, pages 343–351, 2016.

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. Ieee, 2009.

[4] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description.
In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 2625–2634, 2015.

[5] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional
two-stream network fusion for video action recognition. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1933–1941, 2016.

[6] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learn-
ing rich features from rgb-d images for object detection and
segmentation. In European Conference on Computer Vision,
pages 345–360. Springer, 2014.

[7] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.

[8] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network.
In Advances
in neural information processing systems, pages 1135–1143,
2015.

[9] K. Hara, H. Kataoka, and Y. Satoh. Learning spatio-temporal
features with 3d residual networks for action recognition. In
Proceedings of the ICCV Workshop on Action, Gesture, and
Emotion Recognition, volume 2, page 4, 2017.

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[11] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. Amc:
Automl for model compression and acceleration on mobile
devices.
In Proceedings of the European Conference on
Computer Vision (ECCV), pages 784–800, 2018.

[12] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[13] H. Hu, R. Peng, Y.-W. Tai, and C.-K. Tang. Network trim-
ming: A data-driven neuron pruning approach towards efﬁ-
cient deep architectures. arXiv preprint arXiv:1607.03250,
2016.

[14] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, vol-
ume 1, page 3, 2017.

[15] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks. In Advances in neural
information processing systems, pages 4107–4115, 2016.

[16] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural net-
works with low precision weights and activations. The Jour-
nal of Machine Learning Research, 18(1):6869–6898, 2017.
[17] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions.
arXiv preprint arXiv:1405.3866, 2014.

[18] Z. JG, H. KQ, et al. Df2net: A discriminative feature learn-
ing and fusion network for rgb-d indoor scene classiﬁcation.
2018.

[19] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. IEEE transactions
on pattern analysis and machine intelligence, 35(1):221–
231, 2013.

[20] Q.-Y. Jiang. Deep cross-modal hashing. In In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017.

[21] X. Jin, Y. Yang, N. Xu, J. Yang, N. Jojic, J. Feng, and S. Yan.
Wsnet: Compact and efﬁcient networks through weight sam-
pling.
In International Conference on Machine Learning,
pages 2357–2366, 2018.

[22] J. Kim, S. Park, and N. Kwak.

Paraphrasing complex
network: Network compression via factor transfer. arXiv
preprint arXiv:1802.04977, 2018.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[24] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recogni-
tion. In Computer Vision (ICCV), 2011 IEEE International
Conference on, pages 2556–2563. IEEE, 2011.

[25] C. Li, C. Deng, N. Li, W. Liu, X. Gao, and D. Tao. Self-
supervised adversarial hashing networks for cross-modal re-
trieval. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4242–4251, 2018.

[26] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P.
Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint
arXiv:1608.08710, 2016.

[27] C. Louizos, K. Ullrich, and M. Welling. Bayesian compres-
sion for deep learning. In Advances in Neural Information
Processing Systems, pages 3288–3298, 2017.

[28] C. Louizos, M. Welling, and D. P. Kingma. Learning sparse
neural networks through l 0 regularization. arXiv preprint
arXiv:1712.01312, 2017.

[29] K. Neklyudov, D. Molchanov, A. Ashukha, and D. P. Vetrov.
Structured bayesian pruning via log-normal multiplicative
noise. In Advances in Neural Information Processing Sys-
tems, pages 6775–6784, 2017.

[30] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering:
Analysis and an algorithm. In Advances in neural informa-
tion processing systems, pages 849–856, 2002.

[31] U. Oswal, C. Cox, M. Lambon-Ralph, T. Rogers, and
R. Nowak. Representational similarity learning with applica-

8981

[47] D. Zhang, H. Wang, M. Figueiredo, and L. Balzano. Learn-
ing to share: Simultaneous parameter tying and sparsiﬁca-
tion in deep learning. 2018.

[48] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva.
Places: An image database for deep scene understanding.
arXiv preprint arXiv:1610.02055, 2016.

[49] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.
Dorefa-net: Training low bitwidth convolutional neural
networks with low bitwidth gradients.
arXiv preprint
arXiv:1606.06160, 2016.

[50] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. arXiv preprint arXiv:1612.01064, 2016.

[51] M. Zhu and S. Gupta. To prune, or not to prune: explor-
ing the efﬁcacy of pruning for model compression. arXiv
preprint arXiv:1710.01878, 2017.

[52] N. Zmora, G. Jacob, and G. Novik. Neural network distiller,

June 2018.

tion to brain networks. In International Conference on Ma-
chine Learning, pages 1041–1049, 2016.

[32] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017.

[33] A. Polino, R. Pascanu, and D. Alistarh. Model com-
pression via distillation and quantization. arXiv preprint
arXiv:1802.05668, 2018.

[34] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In European Conference on Computer Vision,
pages 525–542. Springer, 2016.

[35] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and
B. Ramabhadran. Low-rank matrix factorization for deep
neural network training with high-dimensional output tar-
gets. In Acoustics, Speech and Signal Processing (ICASSP),
2013 IEEE International Conference on, pages 6655–6659.
IEEE, 2013.

[36] U. Shaham, K. Stanton, H. Li, B. Nadler, R. Basri, and
Y. Kluger. Spectralnet: Spectral clustering using deep neural
networks. arXiv preprint arXiv:1801.01587, 2018.

[37] K. Simonyan and A. Zisserman. Two-stream convolutional
In Advances
networks for action recognition in videos.
in neural information processing systems, pages 568–576,
2014.

[38] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[39] S. Song, S. P. Lichtenberg, and J. Xiao. Sun rgb-d: A rgb-
d scene understanding benchmark suite. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 567–576, 2015.

[40] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[41] U. Von Luxburg. A tutorial on spectral clustering. Statistics

and computing, 17(4):395–416, 2007.

[42] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
L. Van Gool. Temporal segment networks: Towards good
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[43] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems, pages 2074–2082,
2016.

[44] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the
smaller-norm-less-informative assumption in channel prun-
ing of convolution layers. arXiv preprint arXiv:1802.00124,
2018.

[45] J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowl-
edge distillation: Fast optimization, network minimization
and transfer learning. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 2, 2017.

[46] C. Zach, T. Pock, and H. Bischof. A duality based approach
for realtime tv-l 1 optical ﬂow. In Joint Pattern Recognition
Symposium, pages 214–223. Springer, 2007.

8982

