Learning for Single-Shot Conﬁdence Calibration in Deep Neural Networks

through Stochastic Inferences

Seonguk Seo∗∗ 1

Paul Hongsuck Seo∗ 1,2

Bohyung Han1

1Computer Vision Lab., ECE & ASRI, Seoul National University, Korea

2Computer Vision Lab., POSTECH, Korea

{seonguk, bhhan}@snu.ac.kr

hsseo@postech.ac.kr

Abstract

We propose a generic framework to calibrate accuracy
and conﬁdence of a prediction in deep neural networks
through stochastic inferences. We interpret stochastic regu-
larization using a Bayesian model, and analyze the relation
between predictive uncertainty of networks and variance of
the prediction scores obtained by stochastic inferences for
a single example. Our empirical study shows that the ac-
curacy and the score of a prediction are highly correlated
with the variance of multiple stochastic inferences given by
stochastic depth or dropout. Motivated by this observation,
we design a novel variance-weighted conﬁdence-integrated
loss function that is composed of two cross-entropy loss
terms with respect to ground-truth and uniform distribu-
tion, which are balanced by variance of stochastic predic-
tion scores. The proposed loss function enables us to learn
deep neural networks that predict conﬁdence calibrated
scores using a single inference. Our algorithm presents out-
standing conﬁdence calibration performance and improves
classiﬁcation accuracy when combined with two popular
stochastic regularization techniques—stochastic depth and
dropout—in multiple models and datasets;
it alleviates
overconﬁdence issue in deep neural networks signiﬁcantly
by training networks to achieve prediction accuracy propor-
tional to conﬁdence of prediction.

1. Introduction

Deep neural networks have achieved remarkable perfor-
mance in various tasks, but have critical limitations in relia-
bility of their predictions. One example is that inference re-
sults are often overly conﬁdent even for unseen or ambigu-
ous examples. Since many practical applications including
medical diagnosis, autonomous driving, and machine in-
spection require accurate uncertainty estimation as well as
high prediction score for each inference, such an overcon-

∗Equal contribution

ﬁdence issue makes deep neural networks inappropriate to
be deployed for real-world problems in spite of their im-
pressive accuracy.

Regularization is a common technique in training deep
neural networks to avoid overﬁtting problems and improve
generalization performance [10, 11, 24]. Although regular-
ization is effective to learn robust models, its objective is
not directly related to generating score distributions aligned
with uncertainty of predictions. Hence, existing deep neural
networks are often poor at calibrating prediction accuracy
and conﬁdence.

Our goal is to learn deep neural networks that are able
to estimate uncertainty of each prediction while maintain-
ing accuracy. In other words, we propose a generic frame-
work to calibrate prediction score (conﬁdence) with accu-
racy in deep neural networks. The main idea of our algo-
rithm starts with an observation that the variance of predic-
tion scores measured from multiple stochastic inferences is
highly correlated with the accuracy and conﬁdence of the
average prediction. We also show that a Bayesian interpre-
tation of stochastic regularizations such as stochastic depth
and dropout leads to the consistent conclusion with the ob-
servation. By using the empirical observation with the theo-
retical interpretation, we design a novel loss function to en-
able a deep neural network to predict conﬁdence-calibrated
scores based only on a single prediction, without multiple
stochastic inferences. Our contribution is summarized as

• We provide a generic framework to estimate uncer-
tainty of a prediction based on stochastic inferences in
deep neural networks, which is supported by empirical
observations and theoretical analysis.

• We propose a novel variance-weighted conﬁdence-
integrated loss function in a principled way, which en-
ables networks to produce conﬁdence-calibrated pre-
dictions even without performing stochastic inferences
and introducing hyper-parameters.

• The proposed framework presents outstanding perfor-
mance to reduce overconﬁdence issue and estimate ac-

19030

(a) Baseline (ECE = 0.346)

(b) CI[Oracle] (ECE = 0.122)

(c) VWCI (ECE = 0.053)

Figure 1. Reliability diagrams of VGG-16 models trained with baseline, CI (ours) and VWCI (ours) losses in Tiny ImageNet dataset. This
diagram shows expected accuracy as a function of conﬁdence, i.e., classiﬁcation score. ECE (Expected Calibration Error) denotes the
average gap between conﬁdence and expected accuracy. The proposed algorithm (VWCI) achieves well-calibrated results compared to the
baseline and the best estimate by a simpler version of ours (CI).

curate uncertainty in various combinations of network
architectures and datasets.

The rest of the paper is organized as follows. We review
the prior research and describe the theoretical background
in Section 2 and 3, respectively. Section 4 presents our con-
ﬁdence calibration algorithm through stochastic inferences,
and Section 5 demonstrates experimental results.

2. Related Work

Uncertainty modeling and estimation in deep neural net-
works is a critical problem and receives growing attention
from machine learning community. Bayesian approach is
a common tool to provide a mathematical framework for
uncertainty estimation. However, the exact Bayesian in-
ference is not tractable in deep neural networks due to its
high computational cost, and various approximate inference
techniques—MCMC [17], Laplace approximation [14] and
variational inference [1, 4, 8, 20]—have been proposed.
Recently, a Bayesian interpretation of multiplicative noise
is employed to estimate uncertainty in deep neural net-
works [3, 15]. Besides, there are several approaches outside
Bayesian modeling, e.g., post-processing [5, 18, 22, 28] and
deep ensembles [12]. All the post-processing methods re-
quire a hold-out validation set to adjust prediction scores
after training, and the ensemble-based technique employs
multiple models to estimate uncertainty.

Stochastic regularization is a well-known technique to
improve generalization performance by injecting random
noise to deep neural networks. The most notable method
is dropout [24], which rejects a subset of hidden units in a
layer based on Bernoulli random noise. There exist several
variants, for example, dropping weights [27] or skipping
layers [10]. Most stochastic regularization methods perform
stochastic inferences during training, but make determinis-

tic predictions using the full network during testing. On the
contrary, we also employ stochastic inferences to obtain di-
verse and reliable outputs during testing.

Although the following works do not address uncertainty
estimation, their main idea is related to our objective more
or less. Label smoothing [25] encourages models to be less
conﬁdent, by preventing a network from assigning the full
probability to a single class. A similar loss function is dis-
cussed to train conﬁdence-calibrated classiﬁers in [13], but
it focuses on how to discriminate in-distribution and out-of-
distribution examples, rather than estimating uncertainty or
alleviating miscalibration of in-distribution examples. On
the other hand, [21] claims that blind label smoothing and
penalizing entropy enhances accuracy by integrating loss
functions with the same concept with [13, 25], but its im-
provement is marginal in practice.

3. Preliminaries

This section describes a Bayesian interpretation of
stochastic regularization in deep neural networks, and dis-
cusses the relationship between stochastic regularization
and uncertainty modeling.

3.1. Stochastic Methods for Regularizations

A popular class of regularization techniques is stochastic
regularization, which introduces random noise for perturb-
ing network structures. Our approach focuses on the multi-
plicative binary noise injection, where random binary noise
is applied to the inputs or weights by elementwise mul-
tiplication, since such stochastic regularization techniques
are widely used [10, 24, 27]. Note that input perturba-
tion can be reformulated as weight perturbation. For ex-
ample, dropout—binary noise injection to activations—is
interpretable as weight perturbation that masks out all the
weights associated with the dropped inputs. Therefore, if a

9031

20.040.060.080.0100.0Score (%)020406080100Accuracy (%)OutputsGap20.040.060.080.0100.0Score (%)020406080100OutputsGap20.040.060.080.0100.0Score (%)020406080100OutputsGapclassiﬁcation network modeling p(y|x, θ) with parameters
θ is trained with stochastic regularization methods by mini-
mizing cross entropy, the loss function is deﬁned by

LSR(θ) = −

1
N

N

Xi=1

log p (yi|xi, ˆωi),

(1)

where ˆωi = θ ⊙ ǫi is a set of perturbed parameters by el-
ementwise multiplication with random noise sample ǫi ∼
p(ǫ), and (xi, yi) ∈ D is a pair of input and output in train-
ing dataset D.

At inference time, the network is parameterized by the
expectation of the perturbed parameters, Θ = E[ω] = θ ⊙
E[ǫ], to predict an output ˆy, which is given by

ˆy = arg max

p (y|x, Θ) .

(2)

y

3.2. Bayesian Modeling

Given the dataset D with N examples, Bayesian objec-
tive is to estimate the posterior distribution of the model
parameter, denoted by p(ω|D), to predict a label y for an
input x, which is given by

p(y|x, D) = Zω

p(y|x, ω)p(ω|D)dω.

(3)

A common technique for the posterior estimation is varia-
tional approximation, which introduces an approximate dis-
tribution qθ(ω) and minimizes Kullback-Leibler (KL) diver-
gence with the true posterior DKL(qθ(ω)||p(ω|D)) as fol-
lows:

LVA(θ) = −

N

Xi=1

Zω

qθ(ω) log p(yi|xi, ω)dω

+ DKL(qθ(ω)||p(ω)).

(4)

The intractable integration and summation over the entire
dataset in Eq. (4) is approximated by Monte Carlo method
and mini-batch optimization, resulting in

ˆLVA(θ) = −

N
M S

M

S

Xi=1

Xj=1

log p (yi|xi, ˆωi,j)

+ DKL (qθ(ω)||p(ω)) ,

(5)

where ˆωi,j ∼ qθ(ω) is a sample from the approximate dis-
tribution, S is the number of samples, and M is the size of
a mini-batch. Note that the ﬁrst term is data likelihood and
the second term is divergence of the approximate distribu-
tion with respect to the prior distribution.

3.3. Bayesian View of Stochastic Regularization

Suppose that we train a classiﬁer with ℓ2 regularization
by a stochastic gradient descent method. Then, the loss

function in Eq. (1) is rewritten as

ˆLSR(θ) = −

1
M

M

Xi=1

log p (yi|xi, ˆωi) + λ||θ||2
2,

(6)

where ℓ2 regularization is applied to the deterministic pa-
rameters θ with weight λ. Optimizing this loss function
is equivalent to optimizing Eq. (5) if there exists a proper
prior p(ω) and qθ(ω) is approximated as a Gaussian mix-
ture distribution [3]. Note that [3] casts dropout training
as an approximate Bayesian inference. Thus, we can in-
terpret training with stochastic depth [10] within the same
framework by a simple modiﬁcation. (See our supplemen-
tary document for the details.) Then, the predictive distri-
bution of a model trained with stochastic regularization is
approximately given by

ˆp(y|x, D) = Zω

p(y|x, ω)qθ(ω)dω.

(7)

Following [3] and [26], we estimate the predictive mean and
uncertainty using a Monte Carlo approximation by drawing
parameter samples {ˆωi}T

i=1 as

E ˆp[y = c] ≈

1
T

T

Xi=1

ˆp(y = c|x, ˆωi),

Cov ˆp[y] ≈ E ˆp[yy

⊺] − E ˆp[y]E ˆp[y]⊺,

(8)

(9)

where y = (y1, . . . , yC)⊺ denotes a score vector of C class
labels. Eq. (8) and Eq. (9) mean that the average prediction
and its predictive uncertainty can be estimated from multi-
ple stochastic inferences.

4. Methods

We present a novel conﬁdence calibration technique for
prediction in deep neural networks, which is given by a
variance-weighted conﬁdence-integrated loss function. We
present our observation that variance of multiple stochas-
tic inferences is closely related to accuracy and conﬁdence
of predictions, and provide an end-to-end training frame-
work for conﬁdence self-calibration. Then, we show that
the prediction accuracy and uncertainty are directly acces-
sible from a predicted score from a single forward pass.

4.1. Empirical Observations

Eq. (9) implies that the variation of models results in
the variance of multiple stochastic predictions for a single
example. Figure 2 presents how the variance of multiple
stochastic inferences given by stochastic depth or dropout
is related to the accuracy and conﬁdence of the correspond-
ing average prediction, where the conﬁdence is measured by
the maximum score of the average prediction. In the ﬁgure,
the accuracy and the score of each bin are computed with

9032

(a) Prediction uncertainty characteristics with stochastic depth in ResNet-34

(b) Prediction uncertainty characteristics with dropout in VGGNet with 16 layers

Figure 2. Uncertainty observed from multiple stochastic inferences with two stochastic regularization methods, (a) stochastic depth and
(b) dropout. We present (left, middle) tendency of accuracy and score of the average prediction with respect to normalized variance of
stochastic inferences and (right) relation between score and accuracy. In regularization methods, average accuracy and score drop gradually
as normalized variance increases. The red lines indicate coverage (cumulative ratio) of examples. We present results from CIFAR-100.

the examples belonging to the corresponding bin of the nor-
malized variance. We present results from CIFAR-100 with
ResNet-34 and VGGNet with 16 layers. The histograms il-
lustrate the strong correlation between the predicted vari-
ance and the reliability—accuracy and conﬁdence—of a
prediction; we can estimate accuracy and uncertainty of an
example effectively based on its prediction variances given
by multiple stochastic inferences.

4.2.Variance Weighted Conﬁdence Integrated Loss

The strong correlation of accuracy and conﬁdence with
the predicted variance observed in Figure 2 shows great po-
tential to make conﬁdence-calibrated predictions through
stochastic inferences. However, variance computation in-
volves multiple stochastic inferences by executing multiple
forward passes. Note that this property incurs additional
computational cost and may produce inconsistent results.

To alleviate these limitations, we propose a generic
framework for training accuracy-score calibrated networks
whose prediction score from a single forward pass directly
provides the conﬁdence of a prediction. This objective
achieved by designing a new loss function, which augments
a conﬁdence-calibration term to the standard cross-entropy
loss, while the two terms are balanced by the variance mea-
sured by multiple stochastic inferences. Speciﬁcally, our
variance-weighted conﬁdence-integrated loss LVWCI(·) for

the whole training data (xi, yi) ∈ D is deﬁned by a lin-
ear combination of the standard cross-entropy loss with the
ground-truth LGT(·) and the cross-entropy with a uniform
distribution LU(·), which is formally given by

LVWCI(θ) =

N

Xi=1

(1 − αi)L(i)

GT(θ) + αiL(i)

U (θ)

=

1
T

N

T

Xi=1

Xj=1

−(1 − αi) log p(yi|xi, ˆωi,j)

+ αiDKL(U (y)||p(y|xi, ˆωi,j)) + ξi

(10)

where αi ∈ [0, 1] is a normalized variance, ˆωi,j(= θ ⊙ ǫi,j)
is a sampled model parameter with binary noise for stochas-
tic prediction, T is the number of stochastic inferences, and
ξi is a constant.

The two terms in our variance-weighted conﬁdence-
integrated loss pushes the network toward the opposite di-
rections; the ﬁrst term encourages the network to ﬁt the
ground-truth label while the second term forces the net-
work to make a prediction close to the uniform distribution.
These terms are linearly interpolated by an instance-speciﬁc
balancing coefﬁcient αi, which is given by normalizing the
prediction variance of an example obtained from multiple
stochastic inferences. Note that the normalized variance αi
is distinct for each training example and is used to measure

9033

0.050.10.150.2Normalized Variance020406080100Accuracy (%)020406080100Sample Coverage (%)AccuracySample Coverage0.050.10.150.2Normalized Variance020406080100Average Score (%)020406080100Sample Coverage (%)Average ScoreSample Coverage30.050.070.090.0Score (%)020406080100Accuracy (%)0.050.10.150.20.25Normalized Variance020406080100Accuracy (%)020406080100Sample Coverage (%)AccuracySample Coverage0.050.10.150.20.25Normalized Variance020406080100Average Score (%)020406080100Sample Coverage (%)Average ScoreSample Coverage30.050.070.090.0Score (%)020406080100Accuracy (%)model uncertainty. Therefore, the optimization of our loss
function produces gradient signals, which lead the predic-
tions toward a uniform distribution for the examples with
high uncertainty derived by high variances while increasing
the prediction scores of the examples with low variances.

By training deep neural networks using the proposed loss
function, we estimate the uncertainty of each testing exam-
ple with a single forward pass. Unlike the ordinary models,
a prediction score of our model is well-calibrated and rep-
resents conﬁdence of a prediction, which means that we can
rely more on the predictions with higher scores.

4.3. Conﬁdence Integrated Loss

Our claim is that an adaptive combination of the cross-
entropy losses with respect to the ground-truth and a uni-
form distribution is a reasonable choice to learn uncertainty.
As a special case of the proposed loss, we also present a
blind version of the combination, which can be used as a
baseline uncertainty estimation technique. This baseline
loss function is referred to as the conﬁdence-integrated loss,
which is given by

LCI(θ) = LGT(θ) + βLU(θ)

=

N

Xi=1

− log p(yi|xi, θ)

+ βDKL(U (y)||p(y|xi, θ)) + ξ,

(11)

where p(y|xi, θ) is the predicted distribution with model pa-
rameter θ and ξ is a constant. The main idea of this loss
function is to regularize with a uniform distribution by ex-
pecting the score distributions of uncertain examples to be
ﬂattened ﬁrst while the distributions of conﬁdent ones re-
main intact, where the impact of the conﬁdence-integrated
loss term is controlled by a global hyper-parameter β.

The proposed loss function is also employed in [21] to
regularize deep neural networks and improve classiﬁcation
accuracy. However, [21] does not discuss conﬁdence cali-
bration issues while presenting marginal accuracy improve-
ment. On the other hand, [13] discusses a similar loss func-
tion but focuses on differentiating between in-distribution
and out-of-distribution examples by measuring the loss of
each example using only one of the two loss terms depend-
ing on its origin.

Contrary to the existing approaches, we employ the loss
function in Eq. (11) to estimate prediction conﬁdence in
deep neural networks. Although the conﬁdence-integrated
loss makes sense intuitively, such blind selection of a hyper-
parameter β limits its generality compared to our variance-
weighted conﬁdence-integrated loss.

4.4. Relation to Other Calibration Approaches

There are several score calibration techniques [5, 16, 18,
29] by adjusting conﬁdence scores through post-processing,

among which [5] presents a method to calibrate conﬁdence
of predictions by scaling logits of a network using a global
temperature τ . The scaling is performed before apply-
ing the softmax function, and τ is trained with a valida-
tion dataset. As discussed in [5], this simple technique is
equivalent to maximize entropy of the output distribution
p(yi|xi).
It is also identical to minimize KL-divergence
DKL(p(yi|xi)|| U (y)) because

DKL(p(yi|xi)|| U (y))

p(yc

i |xi) log p(yc

= Xc∈C
= −H(p(yi|xi)) + ξ′,

i |xi) − p(yc

i |xi) log U (yc)

(12)

where C is a class set and ξ′ is a constant. We can formulate
another conﬁdence-integrated loss with the entropy as

L′

CI(θ) =

N

Xi=1

− log p(yi|xi, θ) − γH(p(yi|xi, θ)),

(13)

where γ is a constant. Eq. (13) implies that temperature
scaling in [5] is closely related to our framework.

5. Experiments

5.1. Experimental Settings

We select four most widely used deep neural network ar-
chitectures to test the proposed algorithm: ResNet [7], VG-
GNet [23], WideResNet [30] and DenseNet [9].

We employ stochastic depth in ResNet as proposed in
[7] while employing dropouts [24] before every fc layer ex-
cept for the classiﬁcation layer in other architectures. Note
that, as discussed in Section 3.3, both stochastic depth and
dropout inject multiplicative binary noise to within-layer
activations or residual blocks, they are equivalent to noise
injection into network weights. Hence, training with ℓ2 reg-
ularization term enables us to interpret stochastic depth and
dropout by Bayesian models.

We evaluate the proposed framework on two bench-
marks, Tiny ImageNet and CIFAR-100, which contain 64 ×
64 images in 200 object classes and 32 × 32 images in 100
object classes, respectively. There are 500 training images
per class in both datasets. For testing, we use the validation
set of Tiny ImageNet and the test set of CIFAR-100, which
have 50 and 100 images per class, respectively. To test the
two benchmarks with the same architecture, we resize im-
ages in Tiny ImageNet to 32 × 32.

All networks are trained by a stochastic gradient decent
method with the momentum 0.9 for 300 epochs. We set the
initial learning rate to 0.1 with the exponential decay in a
factor of 0.2 at epoch 60, 120, 160, 200 and 250. Each batch
consists of 64 training examples for ResNet, WideResNet
and DenseNet and 256 for VGGNet. To train networks with

9034

Table 1. Classiﬁcation accuracy and calibration scores for several combinations of network architectures and datasets. We compare models
trained with baseline, CI and VWCI losses. Since CI loss involves a hyper-parameter β, we present mean and standard deviation of results
from models with ﬁve different β’s. In addition, we also show results from the oracle CI loss, CI[Oracle], which are the most optimistic
values out of results from all β’s in individual columns. Note that the numbers corresponding to CI[Oracle] may come from different β’s.
Refer to the supplementary document for the full results.

Dataset

Architecture

Method

Accuracy [%]

Tiny ImageNet

CIFAR-100

ResNet-34

VGG-16

WideResNet-16-8

DenseNet-40-12

ResNet-34

VGG-16

WideResNet-16-8

DenseNet-40-12

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

Baseline

CI

VWCI

CI[Oracle]

50.82

ECE

0.067

MCE

0.147

NLL

2.050

Brier Score

0.628

50.09 ± 1.08

0.134 ± 0.079

0.257 ± 0.098

2.270 ± 0.212

0.665 ± 0.037

52.80
51.45

46.58

0.027
0.035

0.346

0.076
0.171

0.595

1.949
2.030

4.220

0.605
0.620

0.844

46.82 ± 0.81

0.226 ± 0.095

0.435 ± 0.107

3.224 ± 0.468

0.761 ± 0.054

48.03
47.39

55.92

0.053
0.122

0.132

0.142
0.320

0.237

2.373
2.812

1.974

0.659
0.701

0.593

55.80 ± 0.44

0.115 ± 0.040

0.288 ± 0.100

1.980 ± 0.114

0.594 ± 0.017

56.66
56.38

42.50

0.046
0.050

0.020

0.136
0.208

0.154

1.866
1.851

2.423

0.569
0.572

0.716

40.18 ± 1.68

0.059 ± 0.061

0.152 ± 0.082

2.606 ± 0.208

0.748 ± 0.035

43.25
41.21

77.19

0.025
0.025

0.109

0.089
0.094

0.304

2.410
2.489

1.020

0.712
0.726

0.345

77.56 ± 0.60

0.134 ± 0.131

0.251 ± 0.128

1.064 ± 0.217

0.360 ± 0.057

78.64
78.54

73.78

0.034
0.029

0.187

0.089
0.087

0.486

0.908
0.921

1.667

0.310
0.321

0.437

73.75 ± 0.35

0.183 ± 0.079

0.489 ± 0.214

1.526 ± 0.175

0.436 ± 0.034

73.87
73.78

77.52

0.098
0.083

0.103

0.309
0.285

0.278

1.277
1.289

0.984

0.391
0.396

0.336

77.35 ± 0.21

0.133 ± 0.091

0.297 ± 0.108

1.062 ± 0.180

0.356 ± 0.044

77.74
77.53

65.91

0.038
0.074

0.074

0.101
0.211

0.134

0.891
0.931

1.238

0.314
0.327

0.463

64.72 ± 1.46

0.070 ± 0.040

0.138 ± 0.055

1.312 ± 0.125

0.482 ± 0.028

67.45
66.20

0.026
0.019

0.094
0.053

1.161
1.206

0.439
0.456

the proposed variance-weighted conﬁdence-integrated loss,
we draw T samples with network parameters ωi for each in-
put image, and compute the normalized variance α based on
T forward passes. The normalized variance is given by the
mean of the Bhattacharyya coefﬁcients between individual
predictions and the average prediction, and, consequently,
in the range of [0.1].

5.2. Evaluation Metric

We measure classiﬁcation accuracy and calibration
scores—expected calibration error (ECE), maximum cal-
ibration error (MCE), negative log likelihood (NLL) and
Brier score—of the trained models.

Let Bm be a set of indices of test examples whose pre-
diction scores for the ground-truth labels fall into interval
( m−1
M ], where M (= 20) is the number of bins. ECE

M , m

and MCE are formally deﬁned by

ECE =

M

Xm=1

|Bm|
N ′

|acc(Bm) − conf(Bm)| ,

MCE = max

m∈{1,...,M }

|acc(Bm) − conf(Bm)| ,

where N ′ is the number of the test samples. Also, accuracy
and conﬁdence of each bin are given by

acc(Bm) =

conf(Bm) =

1

|Bm| Xi∈Bm
|Bm| Xi∈Bm

1

✶(ˆyi = yi),

pi,

where ✶ is an indicator function, ˆyi and yi are predicted and
true label of the ith example and pi is its predicted conﬁ-

9035

Table 2. Comparison between VWCI and TS of multiple datasets and architectures.

Dataset

Architecture

Method

Accuracy [%]

ECE MCE NLL

Brier Score

Tiny ImageNet

CIFAR-100

ResNet-34

VGG-16

WideResNet-16-8

DenseNet-40-12

ResNet-34

VGG-16

WideResNet-16-8

DenseNet-40-12

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

TS (case 1)
TS (case 2)

VWCI

50.82
47.20
52.80

46.58
46.53
48.03

55.92
53.95
56.66

42.50
41.63
43.25

77.67
77.40
78.64

73.66
72.69
73.87

77.52
76.42
77.74

65.91
64.96
67.45

0.162
0.021
0.027

0.358
0.028
0.053

0.200
0.027
0.046

0.037
0.024
0.025

0.133
0.036
0.034

0.197
0.031
0.098

0.144
0.028
0.038

0.095
0.082
0.026

0.272
0.080
0.076

0.604
0.067
0.142

0.335
0.224
0.136

0.456
0.109
0.089

0.356
0.165
0.089

0.499
0.074
0.309

0.400
0.101
0.101

0.165
0.163
0.094

2.241
2.159
1.949

4.425
2.361
2.373

2.259
1.925
1.866

2.436
2.483
2.410

1.162
0.886
0.908

1.770
1.193
1.277

1.285
0.891
0.891

1.274
1.306
1.161

0.660
0.661
0.605

0.855
0.671
0.659

0.627
0.595
0.569

0.717
0.728
0.712

0.354
0.323
0.310

0.445
0.389
0.391

0.361
0.332
0.314

0.468
0.481
0.439

dence. NLL and Brier score are another ways to measure
the calibration [2, 5, 6], which are deﬁned as

NLL = −

N ′

Xi=1

log p(yi|xi, θ),

Brier =

N ′

C

Xi=1

Xj=1

(p( ˆyi = j|xi, θ) − ✶(yi = j))2.

We note that low values for all these calibration scores
means that the network is well-calibrated.

5.3. Results

Table 1 presents accuracy and calibration scores for sev-
eral combinations of network architectures and benchmark
datasets. The models trained with VWCI loss consistently
outperform the models with CI loss, which is a special case
of VWCI, and the baseline on both classiﬁcation accuracy
and conﬁdence calibration performance. We believe that
the accuracy gain is partly by virtue of the stochastic regu-
larization with multiple samples [19]. Performance of CI is
given by the average and variance from 5 different cases of
β(= 1, 10−1, 10−2, 10−3, 10−4)1 and CI[Oracle] denotes
the most optimistic value among the 5 cases in each column.
Note that VWCI presents outstanding results in most cases

even when compared with CI[Oracle] and that performance
of CI is sensitive to the choice of β’s. These results im-
ply that the proposed loss function balances two conﬂicting
loss terms effectively using the variance of multiple stochas-
tic inferences while performance of CI varies depending on
hyper-parameter setting in each dataset.

We also compare the proposed framework with the
state-of-the-art post-processing method, temperature scal-
ing (TS) [5]. The main distinction between post-processing
methods and our work is the need for held-out dataset; our
method allows to calibrate scores during training without
additional data while [5] requires held-out validation sets
to calibrate scores. To illustrate the effectiveness of our
framework, we compare our approach with TS in the fol-
lowing two scenarios: 1) using the entire training set for
both training and calibration and 2) using 90% of train-
ing set for training and the remaining 10% for calibration.
Table 2 presents that case 1 suffers from poor calibration
performance and case 2 loses accuracy substantially due to
training data reduction although it shows comparable cali-
bration scores to VWCI. Note that TS may also suffer from
the binning artifacts of histograms although we do not in-
vestigate this limitation in our work.

5.4. Discussion

1These 5 values of β are selected favorably to CI based on our prelim-

inary experiment.

To show the effectiveness of the propose framework, we
analyze the proposed algorithm with ablative experiments.

9036

Figure 3. ECE of VWCI loss with respect to the number of samples
(T) on Tiny ImageNet (top) and CIFAR-100 (bottom) dataset.

Table 3. Comparisons between the models based on the VWCI
losses, trained from scratch and the uncalibrated pretrained net-
works (denoted by VWCI*).

Architecture Method Acc. [%] ECE MCE NLL Brier

t
e
N
e
g
a
m

I

y
n
i
T

0
0
1
-
R
A
F
I
C

ResNet-34

VGG-16

ResNet-34

VGG-16

Baseline
VWCI
VWCI∗

Baseline
VWCI
VWCI∗

Baseline
VWCI
VWCI∗

Baseline
VWCI
VWCI∗

77.19
78.64
77.87

73.78
73.87
74.17

50.82
52.80
52.77

46.58
48.03
46.98

0.109 0.304 1.020 0.345
0.034 0.089 0.908 0.310
0.026 0.069 1.013 0.346

0.187 0.486 1.667 0.437
0.098 0.309 1.277 0.391
0.074 0.243 1.227 0.385

0.067 0.147 2.050 0.628
0.027 0.076 1.949 0.605
0.034 0.099 1.965 0.605

0.346 0.595 4.220 0.844
0.053 0.142 2.373 0.659
0.056 0.162 2.446 0.683

Effect of sample size for stochastic inferences Figure 3
illustrates ECE of the models trained with our VWCI loss
by varying the number of stochastic inferences (T ) during
training. The increase of T is helpful to improve accuracy
and calibration quality at the beginning but its beneﬁt is sat-
urated when T is between 5 and 10 in general. Such ten-
dency is consistent in all the tested architectures, datasets,
and evaluation metrics including ECE. We set T to 5 in all
the experiments.

Training cost Although our approach allows single-shot
conﬁdence calibration at test-time, it increases time com-
plexity for training due to multiple stochastic inferences.
Fortunately, the calibrated models can be trained more ef-
ﬁciently without stochastic inferences in the majority (≥
80%) of iterations by initializing the networks with the pre-
trained baseline models. Table 3 conﬁrms that the perfor-
mance of our models trained from the uncalibrated pre-
trained models is as competitive as (or often even better
than) the ones trained from scratch with the VWCI losses.

Figure 4. Coverage of ResNet-34 models with respect to conﬁ-
dence interval on Tiny ImageNet (top) and CIFAR-100 (bottom).
The coverage is computed by the portion of examples with higher
accuracy and conﬁdence than the thresholds shown in x-axis. We
present results from multiple CI models with best performances
with respect to individual metrics, which are shown in the legends.

Reliability Our approach effectively maintains examples
with high accuracy and conﬁdence, which is a desirable
property for building reliable real-world systems. Figure 4
illustrates portion of test examples with higher accuracy
and conﬁdence than the various thresholds in ResNet-34,
where VWCI presents better coverage of the examples than
CI[Oracle]. Note that coverage of CI often depends on the
choice of β signiﬁcantly as demonstrated in Figure 4 (right)
while VWCI maintains higher coverage than CI using accu-
rately calibrated prediction scores. These results imply that
using the predictive uncertainty for balancing the loss terms
is preferable to setting with a constant coefﬁcient.

6. Conclusion

We presented a generic framework for uncertainty es-
timation of a prediction in deep neural networks by cali-
brating accuracy and score based on stochastic inferences.
Based on Bayesian interpretation of stochastic regulariza-
tion and our empirical observation results, we claim that
variation of multiple stochastic inferences for a single ex-
ample is a crucial factor to estimate uncertainty of the aver-
age prediction. Inspired by this fact, we design the variance-
weighted conﬁdence-integrated loss to learn conﬁdence-
calibrated networks and enable uncertainty to be estimated
by a single prediction. The proposed algorithm is also use-
ful to understand existing conﬁdence calibration methods
in a uniﬁed way, and we compared our algorithm with other
variations within our framework to analyze their properties.

Acknowledgments This work was partly supported by
Samsung Advanced Institute of Technology and Korean
ICT R&D program of the MSIP/IITP grant [2014-0-00059,
2017-0-01778].

9037

246810The number of samples (T)0.010.020.030.0ECE (%)ResNet-34VGG-16DenseNet-40-12WideResNet-16-8246810The number of samples (T)0.05.010.015.020.0ECE (%)0.80.850.90.951.0Threshold0.010.020.030.040.0Sample Coverage (%)VWCICI[=0.1; Acc., ECE, MCE, NLL, Brier]0.80.850.90.951.0Threshold0.025.050.075.0Sample Coverage (%)VWCICI[=0.01; NLL]CI[=0.1; ECE, MCE, Brier]CI[=1; Acc.][20] Nick Pawlowski, Andrew Brock, Matthew CH Lee, Martin
Rajchl, and Ben Glocker. Implicit Weight Uncertainty in Neu-
ral Networks. arXiv preprint arXiv:1711.01297, 2017. 2

[21] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz
Kaiser, and Geoffrey Hinton. Regularizing Neural Networks
by Penalizing Conﬁdent Output Distributions. arXiv preprint
arXiv:1701.06548, 2017. 2, 5

[22] John Platt. Probabilistic Outputs for Support Vector Ma-
chines and Comparisons to Regularized Likelihood Methods.
Advanced in Large Margin Classiﬁers, 10, 06 2000. 2

[23] Karen Simonyan and Andrew Zisserman. Very Deep Con-
In

volutional Networks for Large-scale Image Recognition.
ICLR, 2015. 5

[24] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way
to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014. 1, 2, 5

[25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the In-
ception Architecture for Computer Vision.
In CVPR, 2016.
2

[26] Mattias Teye, Hossein Azizpour, and Kevin Smith. Bayesian
Uncertainty Estimation for Batch Normalized Deep Net-
works. arXiv preprint arXiv:1802.06455, 2018. 3

[27] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and
Rob Fergus. Regularization of Neural Networks using Drop-
connect. In ICML, 2013. 2

[28] Bianca Zadrozny and Charles Elkan.

Obtaining Cali-
brated Probability Estimates from Decision Trees and Naive
Bayesian Classiﬁers. In ICML, 2001. 2

[29] Bianca Zadrozny and Charles Elkan. Transforming Classi-
ﬁer Scores into Accurate Multiclass Probability Estimates. In
KDD, 2002. 5

[30] Sergey Zagoruyko and Nikos Komodakis. Wide Residual

Networks. In BMVC, 2016. 5

References

[1] David Barber and Christopher M Bishop. Ensemble Learning

for Multi-layer Networks. In NIPS, 1998. 2

[2] Glenn W Brier and Roger A Allen. Veriﬁcation of weather
In Compendium of meteorology, pages 841–848.

forecasts.
Springer, 1951. 7

[3] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian
Approximation: Representing Model Uncertainty in Deep
Learning. In ICML, 2016. 2, 3

[4] Alex Graves. Practical variational inference for neural net-

works. In NIPS, 2011. 2

[5] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
On Calibration of Modern Neural Networks. In ICML, 2017.
2, 5, 7

[6] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The
Elements of Statistical Learning. Springer New York Inc.,
2001. 7

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep Residual Learning for Image Recognition.
2016. 5

[8] Matthew D. Hoffman, David M. Blei, Chong Wang, and John
Paisley. Stochastic Variational Inference. J. Mach. Learn.
Res., 14(1):1303–1347, May 2013. 2

[9] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely Connected Convolutional Net-
works. In CVPR, 2017. 5

[10] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q

Weinberger. In ECCV. 1, 2, 3

[11] Sergey Ioffe and Christian Szegedy. Batch Normalization:
Accelerating Deep Network Training by Reducing Internal
Covariate Shift. In ICML, 2015. 1

[12] Balaji Lakshminarayanan, Alexander Pritzel, and Charles
Blundell. Simple and Scalable Predictive Uncertainty Esti-
mation using Deep Ensembles. In NIPS, 2017. 2

[13] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.
Training Conﬁdence-calibrated Classiﬁers for Detecting Out-
of-Distribution Samples. In ICLR, 2018. 2, 5

[14] David J. C. MacKay. A Practical Bayesian Framework for
Backpropagation Networks. Neural Comput., 4(3):448–472,
May 1992. 2

[15] Patrick McClure and Nikolaus Kriegeskorte. Representa-
tion of uncertainty in deep neural networks through sampling.
CoRR, abs/1611.01639, 2016. 2

[16] Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos
Hauskrecht. Obtaining Well Calibrated Probabilities Using
Bayesian Binning. In AAAI, 2015. 5

[17] Radford M. Neal. Bayesian Learning for Neural Networks.

Springer-Verlag, 1996. 2

[18] Alexandru Niculescu-Mizil and Rich Caruana. Predicting
Good Probabilities with Supervised Learning. In ICML, 2005.
2, 5

[19] Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, and Bo-
hyung Han. Regularizing Deep Neural Networks by Noise:
Its Interpretation and Optimization. In NIPS, 2017. 7

9038

