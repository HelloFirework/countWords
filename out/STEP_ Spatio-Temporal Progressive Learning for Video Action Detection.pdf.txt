STEP: Spatio-Temporal Progressive Learning for Video Action Detection

Xitong Yang1 ∗ Xiaodong Yang2 Ming-Yu Liu2

Fanyi Xiao3 ∗ Larry Davis1

Jan Kautz2

1University of Maryland, College Park 2NVIDIA 3University of California, Davis

Abstract

In this paper, we propose Spatio-TEmporal Progressive
(STEP) action detector—a progressive learning framework
for spatio-temporal action detection in videos. Starting
from a handful of coarse-scale proposal cuboids, our ap-
proach progressively reﬁnes the proposals towards actions
over a few steps. In this way, high-quality proposals (i.e.,
adhere to action movements) can be gradually obtained at
later steps by leveraging the regression outputs from pre-
vious steps. At each step, we adaptively extend the pro-
posals in time to incorporate more related temporal con-
text. Compared to the prior work that performs action de-
tection in one run, our progressive learning framework is
able to naturally handle the spatial displacement within ac-
tion tubes and therefore provides a more effective way for
spatio-temporal modeling. We extensively evaluate our ap-
proach on UCF101 and AVA, and demonstrate superior de-
tection results. Remarkably, we achieve mAP of 75.0% and
18.6% on the two datasets with 3 progressive steps and us-
ing respectively only 11 and 34 initial proposals.

1. Introduction

Spatio-temporal action detection aims to recognize the
actions of interest that present in a video and localize them
in both space and time.
Inspired by the advances in the
ﬁeld of object detection in images [8, 21], most recent work
approaches this task based on the standard two-stage frame-
work: in the ﬁrst stage action proposals are produced by a
region proposal algorithm or densely sampled anchors, and
in the second stage the proposals are used for action classi-
ﬁcation and localization reﬁnement.

Compared to object detection in images, spatio-temporal
action detection in videos is however a more challenging
problem. New challenges arise from both of the above
two stages when the temporal characteristic of videos is
taken into account. First, an action tube (i.e., a sequence of
bounding boxes of action) usually involves spatial displace-

∗Work done during an internship at NVIDIA Research.

Figure 1: A schematic overview of spatio-temporal progres-
sive learning for action detection. Starting with a coarse-
scale proposal cuboid, it progressively reﬁnes the proposal
towards the action, and adaptively extends the proposal to
incorporate more related temporal context at each step.

ment over time, which introduces extra complexity for pro-
posal generation and reﬁnement. Second, effective tempo-
ral modeling becomes imperative for accurate action classi-
ﬁcation, as a number of actions are only identiﬁable when
temporal context information is available.

Previous work usually exploits temporal information by
performing action detection at the clip (i.e., a short video
snippet) level. For instance, [12, 17] take as input a se-
quence of frames and output the action categories and re-
gressed tubelets of each clip.
In order to generate action
proposals, they extend 2D region proposals to 3D by repli-
cating them over time, assuming that the spatial extent is
ﬁxed within a clip. However, this assumption would be vi-
olated for the action tubes with large spatial displacement,
in particular when the clip is long or involves rapid move-
ment of actors or camera. Thus, using long cuboids directly
as action proposals is not optimal, since they introduce ex-
tra noise for action classiﬁcation and make action localiza-
tion more challenging, if not hopeless. Recently, there are
some attempts to use adaptive proposals for action detection
[16, 20]. However, these methods require an ofﬂine linking
process to generate the proposals.

264

In this paper, we present a novel learning framework,
Spatio-TEmporal Progressive (STEP) action detector, for
video action detection. As illustrated in Figure 1, unlike ex-
isting methods that directly perform action detection in one
run, our framework involves a multi-step optimization pro-
cess that progressively reﬁnes the initial proposals towards
the ﬁnal solution. Speciﬁcally, STEP consists of two com-
ponents: spatial reﬁnement and temporal extension. Spa-
tial reﬁnement starts with a small number of coarse-scale
proposals and updates them iteratively to better classify and
localize action regions. We carry out the multiple steps in
a sequential order, where the outputs of one step are used
as the proposals for next step. This is motivated by the
fact that the regression outputs can better follow actors and
adapt to action tubes than the input proposals. Temporal ex-
tension focuses on improving classiﬁcation accuracy by in-
corporating longer-range temporal information. However,
simply taking a longer clip as input is inefﬁcient and also
ineffective since a longer sequence tends to have larger spa-
tial displacement, as shown in Figure 1. Instead, we pro-
gressively process longer sequences at each step and adap-
tively extend proposals to follow action movement. In this
manner, STEP can naturally handle the spatial displacement
problem and therefore provide more efﬁcient and effective
spatio-temporal modeling. Moreover, STEP achieves su-
perior performance by using only a handful (e.g., 11) of
proposals, obviating the need to generate and process large
numbers (e.g., >1K) of proposals due to the tremendous
spatial and temporal search space.

To our knowledge, this work provides the ﬁrst end-to-
end progressive optimization framework for video action
detection. We bring up the spatial displacement problem
in action tubes and show that our method can naturally han-
dle the problem in an efﬁcient and effective way. Extensive
evaluations ﬁnd our approach to produce superior detection
results while only using a small number of proposals.

2. Related Work

Action Recognition. A large family of the research
in video action recognition is about action classiﬁcation,
which provides fundamental tools for action detection, such
as two-stream networks on multiple modalities [28, 35],
3D-CNN for simultaneous spatial and temporal feature
learning [4, 13], and RNNs to capture temporal context and
handle variable-length video sequences [25, 36]. Another
active research line is the temporal action detection, which
focuses on localizing the temporal extent of each action.
Many methods have been proposed, from fast temporal ac-
tion proposals [15], region convolutional 3D network [34],
to budget-aware recurrent policy network [23].

Spatio-Temporal Action Detection. Inspired by the re-
cent advances in image object detection, a number of efforts
have been made to extend image object detectors (e.g., R-

CNN, Fast R-CNN and SSD) to the task as frame-level ac-
tion detectors [10, 26, 27, 30, 33, 37, 38]. The extensions
mainly include: ﬁrst, optical ﬂow is used to capture motion
cues, and second, linking algorithms are developed to con-
nect frame-level detection results as action tubes. Although
these methods have achieved promising results, the tempo-
ral property of videos is not explicitly or fully exploited as
the detection is performed on each frame independently. To
better leverage the temporal cues, several recent work has
been proposed to perform action detection at clip level. For
instance, ACT [17] takes as input a short sequence of frames
(e.g., 6 frames) and outputs the regressed tubelets, which
are then linked by a tubelet linking algorithm to construct
action tubes. Gu et al. [12] further demonstrate the impor-
tance of temporal information by using longer clips (e.g.,
40 frames) and taking advantage of I3D pre-trained on the
large-scale video dataset [4]. Rather than linking the frame
or clip level detection results, there are also some methods
that are developed to link the proposals before classiﬁcation
to generate action tube proposals [16, 20].

Progressive Optimization. This technique has been
explored in a range of vision tasks from pose estimation
[3], image generation [11] to object detection [2, 6, 7, 24].
Speciﬁcally, the multi-region detector [6] introduces itera-
tive bounding box regression with R-CNN to produce bet-
ter regression results. AttractioNet in [7] employs a multi-
stage procedure to generate accurate object proposals that
are then input to Fast R-CNN. G-CNN [24] trains a regres-
sor to iteratively move a grid of bounding boxes towards ob-
jects. Cascade R-CNN [2] proposes a cascade framework
for high-quality object detection, where a sequence of R-
CNN detectors are trained with increasing IoU thresholds
to iteratively suppress close false positives.

3. Method

In this section, we introduce the proposed progressive
learning framework STEP for video action detection. We
ﬁrst formulate the problem and provide an overview of our
approach. We then describe in details the two primary com-
ponents of STEP including spatial reﬁnement and temporal
extension. Finally, the training algorithm and implementa-
tion details are presented.

3.1. Framework Overview

Proceeding with the recent work [12, 17], our approach
performs action detection at clip level, i.e., detection results
are ﬁrst obtained from each clip and then linked to build ac-
tion tubes across a whole video. We assume that each action
tubelet of a clip has a constant action label, considering the
short duration of a clip, e.g., within one second.

Our target is to tackle the action detection problem
through a few progressive steps, rather than directly detect-
ing actions all at one run. In order to detect the actions in a

265

Figure 2: Example of the 11 initial proposals: 2D boxes are
replicated across time to obtain cuboids.

i=1

and b0

i (cid:9)M

clip It with K frames, according to the maximum progres-
sive steps Smax, we ﬁrst extract the convolutional features
for a set of clips I = {It−Smax+1, ..., It, ..., It+Smax−1}
using a backbone network such as VGG16 [29] or I3D [4].
The progressive learning starts with M pre-deﬁned proposal
i ∈ RK×4, which are sparsely
cuboids B0 = (cid:8)b0
sampled from a coarse-scale grid of boxes and replicated
across time to form the initial proposals. An example of the
11 initial proposals used in our experiments is illustrated
in Figure 2. These initial proposals are then progressively
updated to better classify and localize the actions. At each
step s, we update the proposals by performing the following
processes in order:
• Extend: the proposals are temporally extended to the ad-
jacent clips to include longer-range temporal context, and
the temporal extension is adaptive to the movement of ac-
tions, as described in Section 3.3.

• Reﬁne: the extended proposals are forwarded to the spa-
tial reﬁnement, which outputs the classiﬁcation and re-
gression results, as presented in Section 3.2.

• Update: all proposals are updated using a simple greedy
algorithm, i.e., each proposal is replaced by the regres-
sion output with the highest classiﬁcation score:

.
= ls

bs
i

i (c∗), c∗ = arg max

ps
i (c),

(1)

c

i ∈ R(C+1) is the probability
where c is an action class, ps
distribution of the ith proposal over C action classes plus
i ∈ RK×4×C denotes its parameterized co-
background, ls
ordinates (for computing the localization loss in Eq. 3) at
.
= indicates decoding the pa-
each frame for each class, and
rameterized coordinates. We summarize the outline of our
detection algorithm in Algorithm 1.

3.2. Spatial Reﬁnement

At each step s, the spatial reﬁnement solves a multi-
task learning problem that involves action classiﬁcation and
localization regression. Accordingly, we design a two-
branch architecture, which learns separate features for the
two tasks, as illustrated in Figure 3. Our motivation is that
the two tasks have substantially different objectives and re-

Algorithm 1: STEP Action Detection for Clip It
: video clips I, initial proposals B0, and

Input

maximum steps Smax

Output: detection results (cid:8)(pSmax

)(cid:9)M
i=1
1 extract convolutional features for video clips I
2 for s ← 1 to Smax do
3

if s == 1 then

, lSmax

i

i

4

5

6

7

8

9

10

11

12

13

// initial proposals
˜Bs−1 ← B0

else

// temporal extension (Sec.3.3)
˜Bs−1 ← Extend(Bs−1)

i , ls

end
// spatial refinement (Sec.3.2)
(cid:8)(ps
// update proposals (Eq.1)
Bs ← Update(cid:0)(cid:8)(ps

i=1 ← Reﬁne( ˜Bs−1)

i )(cid:9)M

i )(cid:9)M

i=1(cid:1)

i , ls

14 end

quire different types of information. For accurate action
classiﬁcation, it demands context features in both space and
time, while for robust localization regression, it needs more
precise spatial cues at frame level. As a result, our two-
branch network consists of a global branch that performs
spatio-temporal modeling on the entire input sequence for
action classiﬁcation, as well as a local branch that performs
bounding box regression at each frame.

Given the frame-level convolutional features and the
tubelet proposals for the current step, we ﬁrst extract re-
gional features through an ROI pooling [8]. Then we
take the regional features to the global branch for spatio-
temporal modeling and produce the global feature. Each
global feature encodes the context information of a whole
tubelet and is further used to predict the classiﬁcation out-
put ps
i . Moreover, the global feature is concatenated with
the corresponding regional features at each frame to form
the local feature, which is used to generate the class-speciﬁc
regression output ls
i . Our local feature not only captures
the spatio-temporal context of a tubelet but also extracts
the local details of each frame. By jointly training the two
branches, the network learns the two separate features that
are informative and adaptable for their own tasks.

Training Loss. We enforce a multi-task loss to jointly
train for action classiﬁcation and tubelet regression. Let P s
denote the set of selected positive samples and N s the set
of negative samples at step s (the sampling strategy is de-
scribed in Section 3.4). We deﬁne the training loss Ls as:

Lloc(ls

i (ui), vi),

(2)

Ls = X

Lcls(ps

i , ui) + λ X
i∈P s

i∈{P s, N s}

266

Figure 3: Left: the architecture of our two-branch network. Right: the illustration of our progressive learning framework,
where “S” indicates spatial reﬁnement, “T” temporal extension, “P” classiﬁcation, and “L” localization, the numbers corre-
spond to the steps, and “L0” denotes the initial proposals.

where ui and vi are the ground truth class label and lo-
calization target for the ith sample, and λ is the weight to
control the importance of the two loss terms. We employ
the multi-class cross-entropy loss as the classiﬁcation loss
Lcls(ps
i (ui) in Eq. 2. We deﬁne the local-
ization loss using the averaged ℓ1,smooth between predicted
and ground truth bounding boxes over the frames of a clip:

i , ui) = − log ps

introduce two methods to enable the temporal extension to
be adaptive as described in the following.

Extrapolation. By assuming that the spatial movement
of an action satisﬁes a linear function approximately within
a short temporal range, such as a 6-frame clip, we can ex-
tend the tubelet proposals by using a simple linear extrapo-
lation function:

Lloc(ls

i (ui), vi) =

1
K

K

X

k=1

ℓ1,smooth(ls

i,k(ui) − vi,k).

(3)

We apply the same parameterization for vi,k as in [9] by
using a scale-invariant center translation and a log-space
height/width shift relative to the bounding box.

3.3. Temporal Extension

Video temporal information, especially the long-term
temporal dependency, is critical for accurate action classi-
ﬁcation [4, 36]. In order to leverage longer range of tem-
poral context, we extend the proposals to include in more
frames as input. However, the extension is not trivial since
the spatial displacement problem becomes even more severe
for longer sequences, as illustrated in Figure 1. Recently,
some negative impacts caused by the spatial displacement
problem for action detection have also been observed by
[12, 17], which simply replicate 2D proposals across time
to increase longer temporal length.

With the intention to alleviate the spatial displacement
problem, we perform temporal extension progressively and
adaptively. From the second step, we extend the tubelet pro-
posals to the two adjacent clips at a time. In other words, at
each step 1 ≤ s < Smax, the proposals Bs with length K s
are extended to ˜Bs = Bs
+1 with length K s +2K,
where ◦ denotes concatenation. Additionally, the temporal
extension is adaptive to action movement by taking advan-
tage of the regressed tubelets from the previous step. We

−1 ◦Bs ◦Bs

Bs

+1,k = Bs

K s +

k

K − 1

(Bs

K s − Bs

K s−K+1).

(4)

A similar function can be applied to Bs
−1 to adapt to the
movement trend, but the assumption would be violated for
long sequences and therefore results in drifted estimations.
Anticipation. We can also achieve the adaptive tempo-
ral extension by location anticipation, i.e., training an ex-
tra regression branch to conjecture the tubelet locations in
adjacent clips based on the current clip. Intuitively, the an-
ticipation requires the network to infer the movement trend
in adjacent clips by action modeling in the current clip. A
similar idea is explored in [37], where location anticipation
is used at the region proposal stage.

We formulate our location anticipation as a residual
learning problem [14, 22] based on the assumption that the
tubelets of two adjacent clips differ from each other by a
small residual. Let x indicate the features forwarded to the
output layer f of the location regressor Ls = f (x) at step
s. So the anticipated locations can be obtained as:

Ls

−1 = Ls + f−1(x), Ls

+1 = Ls + f+1(x),

(5)

where f−1 and f+1 are the anticipation regressors, which
are lightweight and introduce negligible computational
overhead. Ls
−1 and Ls
+1 are then decoded to the proposals
−1 and Bs
Bs
+1. The loss function of location anticipation is
deﬁned in a similar way as Eq. 3, and combined with Lcls
and Lloc with a coefﬁcient γ to form the overall loss.

267

3.4. Network Training

Step 1

Step 2

Step 3

Although STEP involves multiple progressive steps, the
whole framework can be trained end-to-end to optimize the
models at different steps jointly. Compared against the step-
wised training scheme used in [24], our joint training is sim-
pler to implement, runs more efﬁciently, and achieves better
performance in our experiments.

Given a mini-batch of training data, we ﬁrst perform an
(Smax − 1)-step inference pass, as illustrated in the right
of Figure 3, to obtain the inputs needed for all progres-
i )}M
sive steps. In practice, the detection outputs {(ps
i=1
at each step are collected and used to select the positive
and negative samples P s and N s for training. We accu-
mulate the losses of all steps and back-propagate to update
the whole model at the same time.

i , ls

Distribution Change. Compared to the prior work that
performs detection in one run, our training could be more
challenging as the input/output distributions change over
steps. As shown in Figure 4, the input distribution is right-
skewed or centered in a low-IoU level at early steps, and
reverses at later steps. This is because our approach starts
from a coarse-scale grid (see Figure 2) and progressively re-
ﬁnes them towards generating high-quality proposals. Ac-
cordingly, the range of output distribution (i.e., the scale of
offset vectors) decreases over steps.

Inspired by [2], we tackle the distribution change in three
ways. First, separate headers are used at different steps to
adapt to the different input/output distributions. Second, we
increase IoU thresholds over the multiple steps. Intuitively,
a lower IoU threshold at early steps tolerates the initial pro-
posals to include sufﬁcient positive samples and a higher
IoU threshold at late steps encourages high-quality detec-
tion. Third, a hard-aware sampling strategy is employed to
select more informative samples during training.

Hard-Aware Sampling. We design the sampling strat-
egy based on two principles: (i) the numbers of positive and
negative samples should be roughly balanced, and (ii) the
harder negatives should be selected more often. To measure
the “hardness” of a negative sample, we use the classiﬁca-
tion scores from the previous step. The tubelet with a high
conﬁdence but a low overlap to any ground truth is viewed
as a hard sample. We calculate the overlap of two tubelets
by averaging the IoU of bounding boxes over K frames of
the target clip. So the negative samples with higher classiﬁ-
cation scores will be sampled with a higher chance.

Formally, given a set of proposals and the overlap thresh-
old τ s at step s, we ﬁrst assign positive labels to the candi-
dates with the highest overlap with ground truth. This is to
ensure that each ground truth tube has at least one positive
sample. After that, the proposals having an overlap higher
than τ s with any ground truth tube are added to the posi-
tive pool and the rest to the negative pool. We then sam-
ple |P s| positives and |N s| negatives from the two pools,

IoU

IoU

IoU

Figure 4: Change of input distribution (IoU between input
proposals and ground truth) over steps on UCF101.

respectively, with the sampling probability proportional to
the classiﬁcation score. For the ﬁrst step, the highest over-
lap with ground truth tubes is used as the score for sampling.
Each selected positive in P s is assigned to the ground truth
tube with which it has the highest overlap. Note that a single
proposal can be assigned to only one ground truth tube.

3.5. Full Model

We can also integrate our model with the common prac-
tices for video action detection [12, 17, 30], such as two-
stream fusion and tubelet linking.

Scene Context. It has been proven to be beneﬁcial to ob-
ject and action detection [20, 32]. Intuitively, some action-
related semantic clues from scene context can be utilized to
improve action classiﬁcation, for example, the scene of a
basketball court for recognizing “basketball dunk”. We in-
corporate scene context by concatenating extended features
to original regional features in the global branch. The ex-
tended features can be obtained by RoI pooling of the whole
image. So the global features encode both spatial and tem-
poral context useful for action classiﬁcation.

Two-Stream Fusion. Most previous methods use late
fusion to combine the results at test time, i.e., the detections
are obtained independently from the two streams and then
fused using either mean fusion [17] or union fusion [30]. In
this work, we also investigate early fusion for two-stream
fusion, which concatenates RGB frames and optical ﬂow
maps in channel and input to the network as a whole. In-
tuitively, early fusion can model the low-level interactions
between the two modalities and also obviates the need for
training two separate networks. In addition, a hybrid fusion
can be further performed to combine detection results from
the early fusion and the two streams. Our experiment shows
that early fusion outperforms late fusion, and hybrid fusion
achieves the best performance.

Tubelet Linking. Given the clip-level detection results,
we link them in space and time to construct the ﬁnal ac-
tion tubes. We follow the same linking algorithm as de-
scribed in [17], apart from that we do not apply global
non-maximum suppression across classes but perform tem-
poral trimming over the linked paths as commonly used
in [20, 27]. The temporal trimming enforces consecutive
boxes to have smooth classiﬁcation scores by solving an en-
ergy maximization problem via dynamic programming.

268

4. Experiments

In this section, we describe the experiments to evalu-
ate STEP and compare against the recent competing algo-
rithms. We start by performing a variety of ablation stud-
ies to better understand the contributions of each individual
component in our approach. We then report comparisons to
the state-of-the-art methods, provide in-depth analysis, and
present the qualitative detection results.

4.1. Experimental Setup

Datasets. We evaluate our approach on the two bench-
marks: UCF101 [31] and AVA [12]. In comparison with
other action detection datasets, such as J-HMDB and UCF-
the two benchmarks are much larger and more
Sports,
challenging, and more importantly,
they are temporally
untrimmed, which ﬁts better to the spatio-temporal action
detection task. UCF101 is originally an action classiﬁca-
tion dataset collected from online videos, and a subset of
24 classes with 3,207 videos are provided with the spatio-
temporal annotations for action detection. Following the
standard evaluation protocol [17], we report results on the
ﬁrst split of the dataset. AVA contains complex actions and
scenes sourced from movies. We use the version 2.1 of
AVA, which consists of the annotations at 1 fps over 80 ac-
tion classes. Following the standard setup in [12], we report
results on the most frequent 60 classes that have at least 25
validation examples per class.

Evaluation Metrics. We report the frame-level mean
average precision (frame-mAP) with an IoU threshold of
0.5 for both datasets. This metric allows us to evaluate the
quality of the detection results independently of the link-
ing algorithm. We also use the video-mAP on UCF101 to
compare with the state-of-the-art results.

Implementation Details.

For the experiments on
UCF101, we use VGG16 [29] pre-trained on ImageNet [5]
as the backbone network. Although more advanced models
are available, we choose the same backbone as [17] for fair
comparisons. For the temporal modeling in global branch,
we use three 3D convolutional layers with adaptive max
pooling along the temporal dimension. All frames are re-
sized to 400 × 400 and the clip length is set to K = 6.
Similar to [17], 5 consecutive optical ﬂow maps are stacked
as a whole for the optical ﬂow input. We train our models
for 35 epochs using Adam [19] with a batch size of 4. We
set the initial learning rate to 5 × 10−5 and perform step
decay after 20 and 30 epochs with the decay rate 0.1.

For the experiments on AVA, we adopt I3D [4] (up to
Mixed 4f) pre-trained on Kinetics-400 [18] as the back-
bone network. We take the two layers Mixed 5b and
Mixed 5c of I3D for temporal modeling in our global
branch. All frames are resized to 400 × 400 and the clip
length is set to K = 12. We use 34 initial proposals and
perform temporal extension only at the third step. As the

Smax

1
2
3
4

s

Mode

f-mAP

1

51.5
56.6
57.1
58.2

2

-

60.7
61.8
62.1

3

-
-

62.6
62.8

4

-
-
-

62.7

RGB
Flow
Late
Early
Hybrid

66.7
63.5
70.7
74.3
75.0

Table 1: Comparisons of frame-mAP (%) of our models
trained with different numbers of steps (left), and different
input modalities and fusion methods (right).

classiﬁcation is more challenging on AVA, we ﬁrst pre-train
our model for an action classiﬁcation task using the spatial
ground truth of training set. We then train the model for ac-
tion detection with a batch size of 4 for 10 epochs. We do
not use optical ﬂow on this dataset due to the heavy compu-
tation and instead combine results of two RGB models. Our
initial learning rate is 5 × 10−6 for the backbone network
and 5 × 10−5 for the two-branch networks, and step decay
is performed after 6 epochs with the decay rate 0.1.

For all experiments, we extract optical ﬂow (if used) with
Brox [1], and perform data augmentation to the whole se-
quence of frames during training, including random ﬂipping
and cropping. More architecture and implementation details
are available in the supplementary material.

4.2. Ablation Study

We perform various ablation experiments on UCF101
to evaluate the impacts of different design choices in our
framework. For all experiments in this section, we employ
the 11 initial proposals as shown in Figure 2 and RGB only,
unless explicitly mentioned otherwise, and frame-mAP is
used as the evaluation metric.

Effectiveness of Spatial Reﬁnement. Our primary de-
sign of STEP is to progressively tackle the action detec-
tion problem through a few steps. We thus ﬁrst verify the
effectiveness of progressive learning by comparing the de-
tection results at different steps with the spatial reﬁnement.
No temporal extension is applied in this comparison. Table
1(a) demonstrates the step-wise performance under differ-
ent maximum steps Smax. Since our approach starts from
the coarse-scale proposals, performing spatial reﬁnement
once is insufﬁcient to achieve good results. We observe
that the second step improves results consistently and sub-
stantially, indicating that the updated proposals have higher
quality and provide more precise information for classiﬁ-
cation and localization. Further improvement can be ob-
tained by additional steps, suggesting the effectiveness of
our progressive spatial reﬁnement. We use 3 steps for most
of our experiments as the performance saturates after that.
Note that using more steps also improves the results of early
steps, due to the beneﬁts of our multi-step joint training.

269

Figure 5: Comparison of frame-mAP (%) of our models
trained with and without temporal extension.

Effectiveness of Temporal Extension.

In addition to
the spatial reﬁnement, our progressive learning contains the
temporal extension to progressively process a longer se-
quence at each step. We compare the detection results with
and without temporal extension in Figure 5. We show the
results of the models taking K = 6 and K = 30 frames as
inputs directly without temporal extension, and the results
of the extrapolation and anticipation methods. Note that the
models with temporal extension also deal with 30 frames at
the third step (extension process: 6 → 18 → 30).

Both of the temporal extension methods outperform the
baseline (K = 6) by a large margin, which clearly shows
the beneﬁt of incorporating longer-range temporal context
for action classiﬁcation. More remarkably, simply taking
K = 30 frames as input without temporal extension re-
sults in inferior performance, validating the importance of
adaptively extending the temporal scale in the progressive
manner. Furthermore, we observe that anticipation per-
forms better than extrapolation for longer sequences, indi-
cating that anticipation can better capture nonlinear move-
ment trends and therefore generate better extensions.

Fusion Comparison. Table 1(b) presents the detection
results of different fusions: late, early and hybrid fusion. In
all cases, using both modalities improves the performance
compared to individual ones. We ﬁnd that early fusion out-
performs late fusion, and attribute the improvement to mod-
eling between the two modalities at the early stage. Hybrid
fusion achieves the best result by further utilizing the com-
plementary information of different methods.

Miscellaneous. We describe several techniques to im-
prove the training in Section 3, including incorporating
scene context, hard-award sampling and increasing IoU
threshold. To validate the contributions of the three tech-
niques, we conduct ablation experiments by removing one
at a time, which correspondingly results in a performance
drop of 2.5%, 1.5% and 1%. In addition, we observe that
incorporating scene context provides more gains for later
steps, suggesting that scene context is more important for
action classiﬁcation when bounding boxes become tight.

Figure 6: Analysis of runtime of our approach under var-
ious settings: (a) the inference speeds using different step
numbers with and without temporal extension, and (b) the
detection results (green dots) and speeds (blue bars) using
different numbers of initial proposals.

4.3. Runtime Analysis

Although STEP involves a multi-step optimization, our
model is efﬁcient since we only process a small number of
proposals. STEP runs at 21 fps using early fusion with 11
initial proposals and 3 steps on a single GPU, which is com-
parable with the clip based approach (23 fps) [17] and much
faster than the frame based method (4 fps) [26]. Figure 6(a)
demonstrates the speeds of our approach with increasing
number of steps under the settings with and without tem-
poral extension. We also report the running time and detec-
tion performance of our approach (w/o temporal extension
for 3 steps) with increasing number of initial proposals in
Figure 6(b). We observe substantial gains in detection ac-
curacy by increasing the number of initial proposals, but it
also results in slowed inference speed. This trade-off be-
tween accuracy and speed can be controlled according to a
speciﬁed time budget.

4.4. Comparison with State of the Art Results

We compare our approach with the state-of-the-art meth-
ods on UCF101 and AVA in Tables 2 and 3. Following the
standard settings, we report the frame-mAP at IoU thresh-
old 0.5 on both datasets and the video-mAP at various IoU
thresholds on UCF101. STEP consistently performs better
than the state-of-the-art methods on UCF101, and brings
a clear gain in frame-mAP, producing 5.5% improvement
over the second best result. Our approach also achieves su-
perior result on AVA, outperforming the recently proposed
ACRN by 1.2%. Notably, STEP performs detection sim-
ply from a handful of initial proposals, while other compet-
ing algorithms rely on a great amount of densely sampled
anchors or an extra person detector trained with external
large-scale image object detection datasets.

4.5. Qualitative Results

We visualize the detection results of our approach at dif-
ferent steps in Figure 7. Each row indicates the detection
outputs at a certain step. A bounding box is labeled in red if
the detection result is correct, otherwise it is labeled in blue.

270

Figure 7: Examples of the detection results on UCF101. Red boxes indicate correct detection and blue ones misclassiﬁcation.
(a) illustrates the effect of progressive learning to improve action classiﬁcation over steps. (b) demonstrates the regression
outputs by spatial reﬁnement at each step.

Method

MR-TS [26]
ROAD [30]
CPLA [37]
RTPR [20]
PntMatch [38]
T-CNN [16]
ACT [17]
Ours

frame-mAP

video-mAP

0.5

65.7

-
-
-

67.0
67.3
69.5
75.0

0.05

78.8

-

79.0
81.5
79.4
78.2

-

0.1

77.3

-

77.3
80.7
77.7
77.9

-

84.6

83.1

0.2

72.9
73.5
73.5
76.3
76.2
73.1
76.5
76.6

Table 2: Comparison with the state-of-the-art methods on
UCF101 by frame-mAP and video-mAP under different
IoU thresholds.

Method
Single Frame∗ [12]
I3D [12]
I3D∗ [12]
ACRN∗ [32]
Ours

frame-mAP

14.2
14.7
15.6
17.4
18.6

Table 3: Comparison with the state-of-the-art methods on
AVA by frame-mAP under IoU = 0.5. “*” means the results
obtained by incorporating optical ﬂow.

Figure 7(a) demonstrates the effect of progressive learning
for more accurate action classiﬁcation. It can be observed
by the fact that the blue boxes are eliminated at later steps.
In Figure 7(b), the ﬁrst row corresponds to the initial pro-
posals and the next two rows show the effect of spatial re-
ﬁnement of the proposals over steps.
It is clear that the
proposals progressively move towards the persons perform-

Figure 8: Examples of the small scale action detection by
our approach. Red boxes indicate the initial proposals and
orange ones the detection outputs.

ing the actions and better localization results are obtained at
later steps. Although starting from coarse-scale proposals,
our approach is robust to various action scales thanks to the
progressive spatial reﬁnement, as illustrated in Figure 8.

5. Conclusion

In this paper, we have proposed the spatio-temporal pro-
gressive learning framework STEP for video action detec-
tion. STEP involves spatial reﬁnement and temporal exten-
sion, where the former starts from sparse initial proposals
and iteratively updates bounding boxes, and the latter grad-
ually and adaptively increases sequence length to incorpo-
rate more related temporal context. STEP is found to be
able to more effectively make use of longer temporal infor-
mation by handling the spatial displacement problem in ac-
tion tubes. Extensive experiments on two benchmarks show
that STEP consistently brings performance gains by using
only a handful of proposals and a few updating steps.

Acknowledgement. Davis acknowledges the support from
IARPA via Department of Interior/Interior Business Center
(DOI/IBC) under contract number D17PC00345.

271

References

[1] Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim
Weickert. High accuracy optical ﬂow estimation based on a
theory for warping. In ECCV, 2004. 6

[2] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-

ing into high quality object detection. In CVPR, 2018. 2, 5

[3] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Ji-
tendra Malik. Human pose estimation with iterative error
feedback. In CVPR, 2016. 2

[4] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? A new model and the kinetics dataset. In CVPR,
2017. 2, 3, 4, 6

[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 6

[6] Spyros Gidaris and Nikos Komodakis. Object detection via a
multi-region and semantic segmentation-aware CNN model.
In ICCV. 2

[7] Spyros Gidaris and Nikos Komodakis. Attend reﬁne repeat:
In

Active box proposal generation via in-out localization.
BMVC, 2016. 2

[8] Ross Girshick. Fast R-CNN. In ICCV, 2015. 1, 3
[9] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, 2014. 4

[10] Georgia Gkioxari and Jitendra Malik. Finding action tubes.

In CVPR, 2015. 2

[11] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez
Rezende, and Daan Wierstra. DRAW: A recurrent neural
network for image generation. In ICML, 2015. 2

[12] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Car-
oline Pantofaru, David A Ross, George Toderici, Yeqing
Li, Susanna Ricco, Rahul Sukthankar, and Cordelia Schmid.
AVA: A video dataset of spatio-temporally localized atomic
visual actions. In CVPR, 2018. 1, 2, 4, 5, 6, 8

[13] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
spatiotemporal 3D CNNs retrace the history of 2D CNNs
and ImageNet? In CVPR, 2018. 2

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 4

[15] Fabian Heilbron, Juan Niebles, and Bernard Ghanem. Fast
temporal activity proposals for efﬁcient detection of human
actions in untrimmed videos. In CVPR, 2016. 2

[16] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolu-
tional neural network (T-CNN) for action detection in videos.
In ICCV, 2017. 1, 2, 8

[17] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari,
and Cordelia Schmid. Action tubelet detector for spatio-
temporal action localization. In ICCV, 2017. 1, 2, 4, 5, 6, 7,
8

[18] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, and Paul Natsev. The Kinetics hu-
man action video dataset. arXiv:1705.06950, 2017. 6

[19] Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. ICLR, 2015. 6

[20] Dong Li, Zhaofan Qiu, Qi Dai, Ting Yao, and Tao Mei. Re-
current tubelet proposal and recognition networks for action
detection. In ECCV, 2018. 1, 2, 5, 8

[21] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander Berg.
SSD: Single shot multibox detector. In ECCV, 2016. 1

[22] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael Jor-
dan. Unsupervised domain adaptation with residual transfer
networks. In NeurIPS, 2016. 4

[23] Behrooz Mahasseni, Xiaodong Yang, Pavlo Molchanov, and
Jan Kautz. Budget-aware activity detection with a recurrent
policy network. In BMVC, 2018. 2

[24] Mahyar Najibi, Mohammad Rastegari, and Larry Davis. G-
In CVPR,

CNN: An iterative grid based object detector.
2016. 2, 5

[25] Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijaya-
narasimhan, Oriol Vinyals, Rajat Monga, and George
Toderici. Beyond short snippets: Deep networks for video
classiﬁcation. In CVPR, 2015. 2

[26] Xiaojiang Peng and Cordelia Schmid. Multi-region two-
stream R-CNN for action detection. In ECCV, 2016. 2, 7,
8

[27] Suman Saha, Gurkirt Singh, Michael Sapienza, Philip Torr,
and Fabio Cuzzolin. Deep learning for detecting multiple
space-time action tubes in videos. In BMVC, 2016. 2, 5

[28] Karen Simonyan and Andrew Zisserman. Two-stream con-
In

volutional networks for action recognition in videos.
NeurIPS, 2014. 2

[29] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. ICLR,
2015. 3, 6

[30] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip Torr,
and Fabio Cuzzolin. Online real-time multiple spatiotempo-
ral action localisation and prediction. In ICCV, 2017. 2, 5,
8

[31] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
UCF101: A dataset of 101 human actions classes from
videos in the wild. arXiv:1212.0402, 2012. 6

[32] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Mur-
phy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric
relation network. In ECCV, 2018. 5, 8

[33] Philippe Weinzaepfel, Zaid Harchaoui,

and Cordelia
Schmid. Learning to track for spatio-temporal action local-
ization. In ICCV, 2015. 2

[34] Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: Region
convolutional 3D network for temporal activity detection. In
ICCV, 2017. 2

[35] Xiaodong Yang, Pavlo Molchanov, and Jan Kautz. Mul-
tilayer and multimodal fusion of deep neural networks for
video classiﬁcation. In ACM MM, 2016. 2

[36] Xiaodong Yang, Pavlo Molchanov, and Jan Kautz. Making
convolutional networks recurrent for visual sequence learn-
ing. In CVPR, 2018. 2, 4

[37] Zhenheng Yang, Jiyang Gao, and Ram Nevatia. Spatio-
temporal action detection with cascade proposal and location
anticipation. In BMVC, 2017. 2, 4, 8

[38] Yuancheng Ye, Xiaodong Yang, and Yingli Tian. Discover-

ing spatio-temporal action tubes. JVCI, 2019. 2, 8

272

