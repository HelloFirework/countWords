Revisiting Self-Supervised Visual Representation Learning

Alexander Kolesnikov*, Xiaohua Zhai*, Lucas Beyer*

Google Brain

Z¨urich, Switzerland

{akolesnikov,xzhai,lbeyer}@google.com

Abstract

Unsupervised visual representation learning remains
a largely unsolved problem in computer vision research.
Among a big body of recently proposed approaches for un-
supervised learning of visual representations, a class of
self-supervised techniques achieves superior performance
on many challenging benchmarks. A large number of the
pretext tasks for self-supervised learning have been stud-
ied, but other important aspects, such as the choice of con-
volutional neural networks (CNN), has not received equal
attention. Therefore, we revisit numerous previously pro-
posed self-supervised models, conduct a thorough large
scale study and, as a result, uncover multiple crucial in-
sights. We challenge a number of common practices in self-
supervised visual representation learning and observe that
standard recipes for CNN design do not always translate
to self-supervised representation learning. As part of our
study, we drastically boost the performance of previously
proposed techniques and outperform previously published
state-of-the-art results by a large margin.

1. Introduction

Automated computer vision systems have recently made
drastic progress. Many models for tackling challenging
tasks such as object recognition, semantic segmentation or
object detection can now compete with humans on com-
plex visual benchmarks [15, 45, 14]. However, the success
of such systems hinges on a large amount of labeled data,
which is not always available and often prohibitively ex-
pensive to acquire. Moreover, these systems are tailored to
speciﬁc scenarios, e.g. a model trained on the ImageNet
(ILSVRC-2012) dataset [38] can only recognize 1000 se-
mantic categories or a model that was trained to perceive
road trafﬁc at daylight may not work in darkness [5, 4].

As a result, a large research effort is currently focused on
systems that can adapt to new conditions without leverag-

*equal contribution

Figure 1. Quality of visual representations learned by various self-
supervised learning techniques signiﬁcantly depends on the convo-
lutional neural network architecture that was used for solving the
self-supervised learning task. For instance, ResNet50 v1 excels
when trained with the relative patch location self-supervision [7],
but produces suboptimal results when trained with the rotation
self-supervision [11]. In our paper we provide a large scale in-
depth study in support of this observation and discuss its implica-
tions for evaluation of self-supervised models.

ing a large amount of expensive supervision. This effort in-
cludes recent advances on transfer learning, domain adapta-
tion, semi-supervised, weakly-supervised and unsupervised
learning. In this paper, we concentrate on self-supervised
visual representation learning, which is a promising sub-
class of unsupervised learning. Self-supervised learning
techniques produce state-of-the-art unsupervised represen-
tations on standard computer vision benchmarks [11, 34, 2].
The self-supervised learning framework requires only
unlabeled data in order to formulate a pretext learning task
such as predicting context [7] or image rotation [11], for
which a target objective can be computed without supervi-
sion. These pretext tasks must be designed in such a way
that high-level image understanding is useful for solving
them. As a result, the intermediate layers of convolutional
neural networks (CNNs) trained for solving these pretext
tasks encode high-level semantic visual representations that
are useful for solving downstream tasks of interest, such as

11920

Rotation [10]Exemplar [8]Rel. Patch Loc. [6]Jigsaw [29]3540455055Downstream ImageNet Accuracy [%]RevNet50ResNet50 v2ResNet50 v1image recognition.

Most of the prior work, which aims at improving perfor-
mance of self-supervised techniques, does so by proposing
novel pretext tasks and showing that they result in improved
representations. Instead, we propose to have a closer look
at CNN architectures. We revisit a prominent subset of the
previously proposed pretext tasks and perform a large-scale
empirical study using various architectures as base models.
As a result of this study, we uncover numerous crucial in-
sights. The most important are summarized as follows:

• Standard architecture design recipes do not neces-
sarily translate from the fully-supervised to the self-
supervised setting. Architecture choices which neg-
ligibly affect performance in the fully labeled set-
ting, may signiﬁcantly affect performance in the self-
supervised setting.

• In contrast to previous observations with the AlexNet
architecture [11, 48, 31], the quality of learned repre-
sentations in CNN architectures with skip-connections
does not degrade towards the end of the model.

• Increasing the number of ﬁlters in a CNN model and,
consequently, the size of the representation signiﬁ-
cantly and consistently increases the quality of the
learned visual representations.

• The evaluation procedure, where a linear model is
trained on a ﬁxed visual representation using stochastic
gradient descent, is sensitive to the learning rate sched-
ule and may take many epochs to converge.

In Section 4 we present experimental results supporting the
above observations and offer additional in-depth insights
into the self-supervised learning setting. Some of these in-
sights are illustrated in Figure 1. We make the code for re-
producing our core experimental results publicly available1.
In our study we obtain new state-of-the-art results for
visual representations learned without labeled data. Inter-
estingly, the context prediction [7] technique that sparked
the interest in self-supervised visual representation learning
and that serves as the baseline for follow-up research, out-
performs all currently published results (among papers on
self-supervised learning) if the appropriate CNN architec-
ture is used.

2. Related Work

Self-supervision is a learning framework in which a su-
pervised signal for a pretext task is created automatically,
in an effort to learn representations that are useful for solv-
ing real-world downstream tasks. Being a generic frame-
work, self-supervision enjoys a wide number of applica-
tions, ranging from robotics to image understanding.

1https://github.com/google/revisiting-self-supervised

In robotics, both the result of interacting with the world,
and the fact that multiple perception modalities simultane-
ously get sensory inputs are strong signals which can be
exploited to create self-supervised tasks [21, 41, 26, 10].

Similarly, when learning representation from videos, one
can either make use of the synchronized cross-modality
stream of audio, video, and potentially subtitles [35, 39, 23,
44], or of the consistency in the temporal dimension [41].

In this paper we focus on self-supervised techniques that
learn from image databases. These techniques have demon-
strated impressive results for learning high-level image rep-
resentations.
Inspired by unsupervised methods from the
natural language processing domain which rely on predict-
ing words from their context [28], Doersch et al. [7] pro-
posed a practically successful pretext task of predicting the
relative location of image patches. This work spawned a
line of work in patch-based self-supervised visual represen-
tation learning methods. These include a model from [31]
that predicts the permutation of a “jigsaw puzzle” created
from the full image and recent follow-ups [29, 33].

In contrast to patch-based methods, some methods gen-
erate cleverly designed image-level classiﬁcation tasks. For
instance, in [11] Gidaris et al. propose to randomly rotate
an image by one of four possible angles and let the model
predict that rotation. Another way to create class labels is
to use clustering of the images [2]. Yet another class of pre-
text tasks contains tasks with dense spatial outputs. Some
prominent examples are image inpainting [37], image col-
orization [47], its improved variant split-brain [48] and mo-
tion segmentation prediction [36]. Other methods instead
enforce structural constraints on the representation space.
Noroozi et al. propose an equivariance relation to match the
sum of multiple tiled representations to a single scaled rep-
resentation [32]. Authors of [34] propose to predict future
patches in via autoregressive predictive coding.

Our work is complimentary to the previously discussed
methods, which introduce new pretext tasks, since we show
how existing self-supervision methods can signiﬁcantly
beneﬁt from our insights.

Finally, many works have tried to combine multiple pre-
text tasks in one way or another. For instance, Kim et al.
extend the “jigsaw puzzle” task by combining it with col-
orization and inpainting in [22]. Combining the jigsaw puz-
zle task with clustering-based pseudo labels as in [2] leads
to the method called Jigsaw++ [33]. Doersch and Zisser-
man [8] implement four different self-supervision methods
and make one single neural network learn all of them in
a multi-task setting. Chen et al. [3] combined the self-
supervised loss from [11] with the GANs [13] objective.

The latter work is similar to ours since it contains a com-
parison of different self-supervision methods using a uniﬁed
neural network architecture, but with the goal of combining
all these tasks into a single self-supervision task. The au-

1921

thors use a modiﬁed ResNet101 architecture [16] without
further investigation and explore the combination of multi-
ple tasks, whereas our focus lies on investigating the inﬂu-
ence of architecture design on the representation quality.

3. Self-supervised study setup

In this section we describe the setup of our study and mo-
tivate our key choices. We begin by introducing six CNN
models in Section 3.1 and proceed by describing the four
self-supervised learning approaches used in our study in
Section 3.2. Subsequently, we deﬁne our evaluation metrics
and datasets in Sections 3.3 and 3.4. Further implementa-
tion details can be found in Supplementary Material.

3.1. Architectures of CNN models

A large part of the self-supervised techniques for vi-
sual representation approaches uses AlexNet [24] architec-
ture.
In our study, we investigate whether the landscape
of self-supervision techniques changes when using modern
network architectures. Thus, we employ variants of ResNet
and a batch-normalized VGG architecture, all of which
achieve high performance in the fully-supervised training
setup. VGG is structurally close to AlexNet as it does not
have skip-connections and uses fully-connected layers.

In our preliminary experiments, we observed an intrigu-
ing property of ResNet models:
the quality of the repre-
sentations they learn does not degrade towards the end of
the network (see Section 4.5). We hypothesize that this is
a result of skip-connections making residual units invert-
ible under certain circumstances [1], hence facilitating the
preservation of information across the depth even when it
is irrelevant for the pretext task. Based on this hypothesis,
we include RevNets [12] into our study, which come with
stronger invertibility guarantees while being structurally
similar to ResNets.

ResNet was introduced by He et al. [16], and we use the
width-parametrization proposed in [46]: the ﬁrst 7 × 7 con-
volutional layer outputs 16 × k channels, where k is the
widening factor, defaulting to 4. This is followed by a se-
ries of residual units of the form y := x + F(x), where
F is a residual function consisting of multiple convolu-
tions, ReLU non-linearities [30] and batch normalization
layers [19]. The variant we use, ResNet50, consists of four
blocks with 3, 4, 6, and 3 such units respectively, and we
refer to the output of each block as block1, block2, etc. The
network ends with a global spatial average pooling produc-
ing a vector of size 512 × k, which we call pre-logits as it is
followed only by the ﬁnal, task-speciﬁc logits layer. More
details on this architecture are provided in [16].

In our experiments we explore k ∈ {4, 8, 12, 16}, result-
ing in pre-logits of size 2048, 4096, 6144 and 8192 respec-
tively. For some self-supervised techniques we skip conﬁg-

urations that do not ﬁt into memory.

Moreover, we analyze the sensitivity of

the self-
supervised setting to underlying architectural details by
using two variants of ordering operations known as
ResNet v1 [16] and ResNet v2 [17] as well as a variant with-
out ReLU preceding the global average pooling, which we
mark by a “(-)”. Notably, these variants perform similarly
on the pretext task.

RevNet
slightly modiﬁes the design of the residual unit
such that it becomes analytically invertible [12]. We note
that the residual unit used in [12] is equivalent to double ap-
plication of the residual unit from [20] or [6]. Thus, for con-
ceptual simplicity, we employ the latter type of unit, which
can be deﬁned as follows. The input x is split channel-wise
into two equal parts x1 and x2. The output y is then the
concatenation of y2 := x2 and y1 := x1 + F(x2).

It easy to see that this residual unit is invertible, because
its inverse can be computed in closed form as x2 = y2 and
x1 = y1 − F(x2).

Apart from this slightly different residual unit, RevNet is
structurally identical to ResNet and thus we use the same
overall architecture and nomenclature for both. In our ex-
periments we use RevNet50 network, that has the same
depth and number of channels as the original Resnet50
model. In the fully labelled setting, RevNet performs only
marginally worse than its architecturally equivalent ResNet.

VGG as proposed in [42] consists of a series of 3 × 3 con-
volutions followed by ReLU non-linearities, arranged into
blocks separated by max-pooling operations. The VGG19
variant we use has 5 such blocks of 2, 2, 4, 4, and 4 con-
volutions respectively. We follow the common practice of
adding batch normalization between the convolutions and
non-linearities.

In an effort to unify the nomenclature with ResNets, we
introduce the widening factor k such that k = 8 corre-
sponds to the architecture in [42], i.e. the initial convolu-
tion produces 8 × k channels and the fully-connected layers
have 512 × k channels. Furthermore, we call the inputs to
the second, third, fourth, and ﬁfth max-pooling operations
block1 to block4, respectively, and the input to the last fully-
connected layer pre-logits.

3.2. Self supervised techniques

In this section we describe the self-supervised techniques

that are used in our study.

Rotation [11]: Gidaris et al. propose to produce 4 copies of
a single image by rotating it by {0°, 90°, 180°, 270°} and let
a single network predict the rotation which was applied—a
4-class classiﬁcation task. Intuitively, a good model should
learn to recognize canonical orientations of objects in natu-
ral images.

Exemplar [9]: In this technique, every individual image

1922

corresponds to its own class, and multiple examples of it
are generated by heavy random data augmentation such as
translation, scaling, rotation, and contrast and color shifts.
We use data augmentation mechanism from [43]. [8] pro-
poses to use the triplet loss [40, 18] in order to scale this
pretext task to a large number of images (hence, classes)
present in the ImageNet dataset. The triplet loss avoids ex-
plicit class labels and, instead, encourages examples of the
same image to have representations that are close in the Eu-
clidean space while also being far from the representations
of different images. Example representations are given by a
1000-dimensional logits layer.

Jigsaw [31]: the task is to recover relative spatial position of
9 randomly sampled image patches after a random permu-
tation of these patches was performed. All of these patches
are sent through the same network, then their representa-
tions from the pre-logits layer are concatenated and passed
through a two hidden layer fully-connected multi-layer per-
ceptron (MLP), which needs to predict a permutation that
was used.
In practice, the ﬁxed set of 100 permutations
from [31] is used.

In order to avoid shortcuts relying on low-level im-
age statistics such as chromatic aberration [31] or edge
alignment, patches are sampled with a random gap be-
tween them. Each patch is then independently converted to
grayscale with probability 2⁄3 and normalized to zero mean
and unit standard deviation. More details on the preprocess-
ing are provided in Supplementary Material. After train-
ing, we extract representations by averaging the representa-
tions of nine uniformly sampled, colorful, and normalized
patches of an image.

Relative Patch Location [7]: The pretext task consists of
predicting the relative location of two given patches of an
image. The model is similar to the Jigsaw one, but in this
case the 8 possible relative spatial relations between two
patches need to be predicted, e.g. “below” or “on the right
and above”. We use the same patch prepossessing as in the
Jigsaw model and also extract ﬁnal image representations
by averaging representations of 9 cropped patches.

3.3. Evaluation of Learned Visual Representations

We follow common practice and evaluate the learned vi-
sual representations by using them for training a linear lo-
gistic regression model to solve multiclass image classiﬁca-
tion tasks requiring high-level scene understanding. These
tasks are called downstream tasks. We extract the represen-
tation from the (frozen) network at the pre-logits level, but
investigate other possibilities in Section 4.5.

In order to enable fast evaluation, we use an efﬁcient
convex optimization technique for training the logistic re-
gression model unless speciﬁed otherwise. Speciﬁcally, we
precompute the visual representation for all training images

and train the logistic regression using L-BFGS [27].

For consistency and fair evaluation, when comparing to
the prior literature in Table 2, we opt for using stochastic
gradient descent (SGD) with momentum and use data aug-
mentation during training.

We further investigate this common evaluation scheme in
Section 4.3, where we use a more expressive model, which
is an MLP with a single hidden layer with 1000 channels
and the ReLU non-linearity after it. More details are given
in Supplementary material.

3.4. Datasets

In our experiments, we consider two widely used image
classiﬁcation datasets: ImageNet [38] and Places205 [49].
ImageNet contains roughly 1.3 million natural images
that represent 1000 various semantic classes. There are
50 000 images in the ofﬁcial validation and test sets, but
since the ofﬁcial test set is held private, results in the liter-
ature are reported on the validation set. In order to avoid
overﬁtting to the ofﬁcial validation split, we report numbers
on our own validation split (50 000 random images from the
training split) for all our studies except in Table 2, where for
a fair comparison with the literature we evaluate on the of-
ﬁcial validation set.

The Places205 dataset consists of roughly 2.5 million
images depicting 205 different scene types such as airﬁeld,
kitchen, coast, etc. This dataset is qualitatively different
from ImageNet and, thus, a good candidate for evaluating
how well the learned representations generalize to new un-
seen data of different nature. We follow the same procedure
as for ImageNet regarding validation splits for the same rea-
sons.

4. Experiments and Results

In this section we present and interpret results of our
large-scale study. All self-supervised models are trained
on ImageNet (without labels) and consequently evaluated
on our own hold-out validation splits of ImageNet and
Places205. Only in Table 2, when we compare to the re-
sults from the prior literature, we use the ofﬁcial ImageNet
and Places205 validation splits.

4.1. Evaluation on ImageNet and Places205

In Table 1 we highlight our main evaluation results: we
measure the representation quality produced by six differ-
ent CNN architectures with various widening factors (Sec-
tion 3.1), initialized randomly or trained using any of four
self-supervised learning techniques (Section 3.2). We use
the pre-logits of the trained self-supervised networks as rep-
resentation. We follow the standard evaluation protocol
(Section 3.3) which measures representation quality as the
accuracy of a linear regression model trained and evaluated
on the ImageNet dataset.

1923

Table 1. Evaluation of representations from self-supervised techniques based on various CNN architectures. The scores are accuracies (in
%) of a linear logistic regression model trained on top of these representations using ImageNet training split. Our validation split is used
for computing accuracies. The architectures marked by a “(-)” are slight variations described in Section 3.1. Sub-columns such as 4×
correspond to widening factors. Top-performing architectures in a column are bold; the best pretext task for each model is underlined.

Model

RevNet50
ResNet50 v2
ResNet50 v1

RevNet50 (-)
ResNet50 v2 (-)

VGG19-BN

Rnd

4×

8.1
4.4
2.5

8.4
5.3

2.7

Rotation

Exemplar

RelPatchLoc

Jigsaw

4×

47.3
43.8
41.7

45.2
38.6

8×

12× 16×

50.4
47.5
43.4

51.0
44.5

53.1
47.2
43.3

52.8
47.3

53.7
47.6
43.2

53.7
48.2

4×

42.4
43.0
42.8

38.0
33.7

8×

45.6
45.7
46.9

42.6
36.7

12×

46.4
46.6
47.7

44.3
38.2

4×

40.6
42.2
46.8

33.8
38.6

8×

45.0
46.7
50.5

43.5
43.4

4×

40.1
38.4
42.2

36.1
32.5

8×

43.7
41.3
45.4

41.5
34.4

16.8

14.6

16.6

22.7

26.4

28.3

29.0

28.5

29.4

19.8

21.1

Now we discuss key insights that can be learned from
the table and motivate our further in-depth analysis. First,
we observe that similar models often result in visual rep-
resentations that have signiﬁcantly different performance.
Importantly, neither is the ranking of architectures con-
sistent across different methods, nor is the ranking of
methods consistent across architectures. For instance, the
RevNet50 v2 model excels under Rotation self-supervision,
but is not the best model in other scenarios. Similarly, rel-
ative patch location seems to be the best method when bas-
ing the comparison on the ResNet50 v1 architecture, but
not otherwise. Notably, VGG19-BN consistently demon-
strates the worst performance, even though it achieves per-
formance similar to ResNet50 models on standard vision
benchmarks [42]. Note that VGG19-BN performs better
when using representations from layers earlier than the pre-
logit layer are used, though still falls short. We investigate
this in Section 4.5. We depict the performance of the mod-
els with the largest widening factor in Figure 2 (left), which
displays these ranking inconsistencies.

Our second observation is that increasing the number
of channels in CNN models improves performance of self-
supervised models. While this ﬁnding is in line with the
fully-supervised setting [46], we note that the beneﬁt is
more pronounced in the context of self-supervised represen-
tation learning, a fact not yet acknowledged in the literature.

We further evaluate how visual representations trained in
a self-supervised manner on ImageNet generalize to other
datasets. Speciﬁcally, we evaluate all our models on the
Places205 dataset using the same evaluation protocol. The
performance of models with the largest widening factor are
reported in Figure 2 (right) and the full result table is pro-
vided in Supplementary Material. We observe the follow-
ing pattern: ranking of models evaluated on Places205 is
consistent with that of models evaluated on ImageNet, indi-
cating that our ﬁndings generalize to new datasets.

4.2. Comparison to prior work

In order to put our ﬁndings in context, we select the
best model for each self-supervision from Table 1 and com-
pare them to the numbers reported in the literature. For
this experiment only, we precisely follow standard proto-
col by training the linear model with stochastic gradient de-
scent (SGD) on the full ImageNet training split and eval-
uating it on the public validation set of both ImageNet and
Places205. We note that in this case the learning rate sched-
ule of the evaluation plays an important role, which we elab-
orate in Section 4.7.

Table 2 summarizes our results. Surprisingly, as a result
of selecting the right architecture for each self-supervision
and increasing the widening factor, our models signiﬁ-
cantly outperform previously reported results. Notably,

Figure 2. Different network architectures perform signiﬁcantly
differently across self-supervision tasks. This observation gener-
alizes across datasets: ImageNet evaluation is shown on the left
and Places205 is shown on the right.

1924

RotationExemplarRel. Patch Loc.Jigsaw2025303540455055Downstream ImageNet Accuracy [%]RotationExemplarRel. Patch Loc.Jigsaw253035404550Downstream Places205 Accuracy [%]RevNet50RevNet50 (-)ResNet50 v2ResNet50 v2 (-)ResNet50 v1VGG19-BN) of the representa-
Figure 3. Comparing linear evaluation (
tions to non-linear (
) evaluation, i.e. training a multi-layer per-
ceptron instead of a linear model. Linear evaluation is not limiting:
conclusions drawn from it carry over to the non-linear evaluation.

context prediction [7], one of the earliest published meth-
ods, achieves 51.4 % top-1 accuracy on ImageNet. Our
strongest model, using Rotation, attains unprecedentedly
high accuracy of 55.4 %. Similar observations hold when
evaluating on Places205.

y
l
i

m
a
F

ImageNet

Places205

Prev. Ours

Prev. Ours

A Rotation[11]
R Exemplar[8]
R Rel. Patch Loc.[8]
A Jigsaw[31, 48]

V CC+vgg-Jigsaw++[33]
A Counting[32]
A Split-Brain[48]
V DeepClustering[2]

R CPC[34]

R Supervised RevNet50
R Supervised ResNet50 v2
V Supervised VGG19

38.7
31.5
36.2
34.7

37.3
34.3
35.4
41.0

48.7†

74.8
75.3
72.7

55.4
46.0
51.4
44.6

-
-
-
-

-

74.4
75.8
75.0

35.1

-
-

35.5

37.5
36.3
34.1
39.8

-

-
-

58.9

48.0
42.7
45.3
42.2

-
-
-
-

-

58.9
61.6
61.5

† marks results reported in unpublished manuscripts.

Table 2. Comparison of the published self-supervised models to
our best models. The scores correspond to accuracy of linear lo-
gistic regression that is trained on top of representations provided
by self-supervised models. Ofﬁcial validation splits of ImageNet
and Places205 are used for computing accuracies. The “Family”
column shows which basic model architecture was used in the ref-
erenced literature: AlexNet, VGG-style, or Residual.

Figure 4. A look at how predictive pretext performance is to even-
tual downstream performance. Colors correspond to the architec-
tures in Figure 3 and circle size to the widening factor k. Within
an architecture, pretext performance is somewhat predictive, but it
is not so across architectures. For instance, according to pretext
accuracy, the widest VGG model is the best one for Rotation, but
it performs poorly on the downstream task.

Importantly, our design choices result in almost halving
the gap between previously published self-supervised result
and fully-supervised results on two standard benchmarks.
Overall, these results reinforce our main insight that in self-
supervised learning architecture choice matters as much as
choice of a pretext task.

4.3. A linear model is adequate for evaluation.

Using a linear model for evaluating the quality of a repre-
sentation requires that the information relevant to the evalu-
ation task is linearly separable in representation space. This
is not necessarily a prerequisite for a “useful” representa-
tion. Furthermore, using a more powerful model in the eval-
uation procedure might make the architecture choice for a
self-supervised task less important. Hence, we consider an
alternative evaluation scenario where we use a multi-layer
perceptron (MLP) for solving the evaluation task, details of
which are provided in Supplementary Material.

Figure 3 clearly shows that the MLP provides only
marginal improvement over the linear evaluation and the
relative performance of various settings is mostly un-
changed. We thus conclude that the linear model is adequate
for evaluation purposes.

4.4. Better performance on the pretext task does not

always translate to better representations.

In many potential applications of self-supervised meth-
ods, we do not have access to downstream labels for eval-
uation. In that case, how can a practitioner decide which
model to use? Is performance on the pretext task a good
proxy?

In Figure 4 we plot the performance on the pretext task
against the evaluation on ImageNet. It turns out that per-
formance on the pretext task is a good proxy only once the

1925

354045505560RevNet50ResNet50 v2ResNet50 v1RotationExemplarRel. Patch Loc.Jigsaw354045505560RevNet50 (-)RotationExemplarRel. Patch Loc.JigsawResNet50 v2 (-)RotationExemplarRel. Patch Loc.Jigsaw2025303540VGG19-BNDownstream ImageNet Accuracy [%]9192939495102030405060Downstream ImageNet Accuracy [%]Rotation55606570Pretext Task Accuracy [%]Rel. Patch Loc.93959799JigsawFigure 5. Evaluating the representation from various depths within the network. The vertical axis corresponds to downstream ImageNet
performance in percent. For residual architectures, the pre-logits are always best.

deteriorates towards the end of the network. We believe that
this happens because the models specialize to the pretext
task in the later layers and, consequently, discard more
general semantic features present in the middle layers.

In contrast, we observe that this is not the case for mod-
els with skip-connections: representation quality in ResNet
consistently increases up to the ﬁnal pre-logits layer. We
hypothesize that this is a result of ResNet’s residual units
being invertible under some conditions [1]. Invertible units
preserve all information learned in intermediate layers, and,
thus, prevent deterioration of representation quality. We fur-
ther test and conﬁrm this hypothesis by performing a study,
where we ablate residual connections in a ResNet model,
see Supplementary Material for more details.

Additionally, we investigate beneﬁts of using the RevNet
model that has stronger invertibility guarantees. Indeed, it
boosts performance by more than 5 % on the Rotation task,
albeit it does not result in improvements across other tasks.
We leave identifying more scenarios where Revnet results
in signiﬁcant boost of performance for the future research.

4.6. Model width and representation size strongly

inﬂuence the representation quality.

Table 1 shows that using a wider network architecture
consistently leads to better representation quality. It should
be noted that increasing the network’s width has the side-
effect of also increasing the dimensionality of the ﬁnal rep-
resentation (Section 3.1). Hence, it is unclear whether the
increase in performance is due to increased network capac-
ity or to the use of higher-dimensional representations, or to
the interplay of both.

In order to answer this question, we take the best rota-
tion model (RevNet50) and disentangle the network width
from the representation size by adding an additional linear
layer to control the size of the pre-logits layer. We then
vary the widening factor and the representation size inde-
pendently of each other, training each model from scratch
on ImageNet with the Rotation pretext task. The results,
evaluated on the ImageNet classiﬁcation task, are shown in

1926

Figure 6. Disentangling the performance contribution of network
widening factor versus representation size. Both matter indepen-
dently, and larger is always better. Scores are accuracies of logis-
tic regression on ImageNet. Black squares mark models which are
also present in Table 1.

model architecture is ﬁxed, but it can unfortunately not be
used to reliably select the model architecture. Other label-
free mechanisms for model-selection need to be devised,
which we believe is an important and underexplored area
for future work.

4.5. Skip connections prevent degradation of rep 
resentation quality towards the end of CNNs.

We are interested in how representation quality depends
on the layer choice and how skip-connections affect this de-
pendency. Thus, we evaluate representations from ﬁve in-
termediate layers in three models: Resnet v2, RevNet and
VGG19-BN. The results are summarized in Figure 5.

Similar

for
AlexNet [25], the quality of representations in VGG19-BN

to prior observations

[11, 48, 31]

Block1Block2Block3Block4Pre-logitsRotation1020304050Block1Block2Block3Block4Pre-logitsExemplarBlock1Block2Block3Block4Pre-logitsRel. Patch Loc.Block1Block2Block3Block4Pre-logitsJigsaw1020304050RevNet50ResNet50 v2VGG19-BN512102420483072409661448192Representation Size1×2×4×6×8×12×16×Width Multiplier31323435343535374042424343444144474748504943464849505152424648505051524345495051535343454950505154Figure 7. Performance of the best models evaluated using all data
as well as a subset of the data. The trend is clear: increased widen-
ing factor increases performance across the board.

Figure 6. In essence, it is possible to increase performance
by increasing either model capacity, or representation size,
but increasing both jointly helps most. Notably, one can
signiﬁcantly boost performance of a very thin model from
31 % to 43 % by increasing representation size.

Low-data regime.
In principle, the effectiveness of in-
creasing model capacity and representation size might only
work on relatively large datasets for downstream evaluation,
and might hurt representation usefulness in the low-data
regime. In Figure 7, we depict how the number of channels
affects the evaluation using both full and heavily subsam-
pled (10 % and 5 %) ImageNet and Places205 datasets.

We observe that increasing the widening factor consis-
tently boosts performance in both the full- and low-data
regimes. We present more low-data evaluation experi-
ments in Supplementary Material. This suggests that self-
supervised learning techniques are likely to beneﬁt from us-
ing CNNs with increased number of channels across wide
range of scenarios.

4.7. SGD for training linear model takes long time

to converge

In this section we investigate the importance of the SGD
optimization schedule for training logistic regression in
downstream tasks. We illustrate our ﬁndings for linear eval-
uation of the Rotation task, others behave the same and are
provided in Supplementary Material.

We train the linear evaluation models with a mini-batch
size of 2048 and an initial learning rate of 0.1, which we
decay twice by a factor of 10. Our initial experiments sug-
gest that when the ﬁrst decay is made has a large inﬂuence
on the ﬁnal accuracy. Thus, we vary the moment of ﬁrst de-
cay, applying it after 30, 120 or 480 epochs. After this ﬁrst
decay, we train for an extra 40 extra epochs, with a second
decay after the ﬁrst 20.

Figure 8 depicts how accuracy on our validation split

Figure 8. Downstream task accuracy curve of the linear evaluation
model trained with SGD on representations from the Rotation task.
The ﬁrst learning rate decay starts after 30, 120 and 480 epochs.
We observe that accuracy on the downstream task improves even
after very large number of epochs.

progresses depending on when the learning rate is ﬁrst de-
cayed. Surprisingly, we observe that very long training
(≈ 500 epochs) results in higher accuracy. Thus, we con-
clude that SGD optimization hyperparameters play an im-
portant role and need to be reported.

5. Conclusion

In this work, we have investigated self-supervised visual
representation learning from the previously unexplored an-
gles. Doing so, we uncovered multiple important insights,
namely that (1) lessons from architecture design in the fully-
supervised setting do not necessarily translate to the self-
supervised setting; (2) contrary to previously popular archi-
tectures like AlexNet, in residual architectures, the ﬁnal pre-
logits layer consistently results in the best performance; (3)
the widening factor of CNNs has a drastic effect on perfor-
mance of self-supervised techniques and (4) SGD training
of linear logistic regression may require very long time to
converge. In our study we demonstrated that performance
of existing self-supervision techniques can be consistently
boosted and that this leads to halving the gap between self-
supervision and fully labeled supervision.

Most importantly, though, we reveal that neither is the
ranking of architectures consistent across different meth-
ods, nor is the ranking of methods consistent across archi-
tectures. This implies that pretext tasks for self-supervised
learning should not be considered in isolation, but in con-
junction with underlying architectures.

Acknowledgements. We thank Sylvain Gelly for many
fruitful discussions and Marvin Ritter for helping us to run
experiments using TPUs.

1927

RotationExemplarRel. Patch Loc.Jigsaw4x8x12x16x4x8x12x16x4x8x12x4x8x12x4x8x4x8x4x8x4x8x202530354045505560Downstream Accuracy [%]ImageNetImageNet (10%)Places205Places205 (5%)0100200300400500Epochs40455055Downstream ImageNet Accuracy [%]Decay at 30Decay at 120Decay at 480References

[1] J. Behrmann, D. Duvenaud, and J.-H. Jacobsen. Invertible
residual networks. arXiv preprint arXiv:1811.00995, 2018.

[2] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep
clustering for unsupervised learning of visual features. Eu-
ropean Conference on Computer Vision (ECCV), 2018.

[3] T. Chen, X. Zhai, M. Ritter, M. Lucic, and N. Houlsby. Self-
supervised generative adversarial networks. In Conference
on Computer Vision and Pattern Recognition (CVPR). 2019.

[4] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Do-
main adaptive faster R-CNN for object detection in the wild.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.

[5] D. Dai and L. Van Gool. Dark model adaptation: Seman-
tic image segmentation from daytime to nighttime. arXiv
preprint arXiv:1810.02575, 2018.

[6] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estima-
tion using real NVP. In International Conference on Learn-
ing Representations (ICLR), 2017.

[7] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In Inter-
national Conference on Computer Vision (ICCV), 2015.

[8] C. Doersch and A. Zisserman. Multi-task self-supervised
In International Conference on Computer

visual learning.
Vision (ICCV), 2017.

[9] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and
T. Brox. Discriminative unsupervised feature learning with
convolutional neural networks. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2014.

[10] F. Ebert, S. Dasari, A. X. Lee, S. Levine, and C. Finn.
Robustness via retrying: Closed-loop robotic manipulation
with self-supervised learning. Conference on Robot Learn-
ing (CoRL), 2018.

[11] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised rep-
In In-
resentation learning by predicting image rotations.
ternational Conference on Learning Representations (ICLR),
2018.

[12] A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. The re-
versible residual network: Backpropagation without storing
activations.
In Advances in neural information processing
systems (NIPS), 2017.

[13] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, 2014.

[14] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.
In International Conference on Computer Vision (ICCV).
IEEE, 2017.

[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In International conference on computer vi-
sion (ICCV), pages 1026–1034, 2015.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In Conference on Computer Vision

for image recognition.
and Pattern Recognition (CVPR), 2016.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
In European conference on com-

deep residual networks.
puter vision (ECCV). Springer, 2016.

[18] A. Hermans, L. Beyer, and B. Leibe.

In Defense of the
Triplet Loss for Person Re-Identiﬁcation. arXiv preprint
arXiv:1703.07737, 2017.

[19] S. Ioffe and C. Szegedy. Batch normalization: Acceler-
ating deep network training by reducing internal covari-
ate shift.
International Conference on Machine Learning
(ICML), 2015.

[20] J. Jacobsen, A. W. M. Smeulders, and E. Oyallon. i-RevNet:
In International Conference on

Deep invertible networks.
Learning Representations (ICLR), 2018.

[21] E. Jang, C. Devin, V. Vanhoucke, and S. Levine. Grasp2Vec:
Learning object representations from self-supervised grasp-
ing. In Conference on Robot Learning, 2018.

[22] D. Kim, D. Cho, D. Yoo, and I. S. Kweon. Learning image
representations by completing damaged jigsaw puzzles. Win-
ter Conference on Applications of Computer Vision (WACV),
2018.

[23] B. Korbar, D. Tran, and L. Torresani. Cooperative learning
of audio and video models from self-supervised synchroniza-
tion. arXiv preprint arXiv:1807.00230, 2018.

[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems (NIPS),
2012.

[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems (NIPS),
2012.

[26] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese,
L. Fei-Fei, A. Garg, and J. Bohg. Making sense of vi-
sion and touch: Self-supervised learning of multimodal
representations for contact-rich tasks.
arXiv preprint
arXiv:1810.10191, 2018.

[27] D. C. Liu and J. Nocedal. On the limited memory bfgs
method for large scale optimization. Mathematical program-
ming, 45(1-3):503–528, 1989.

[28] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013.

[29] T. N. Mundhenk, D. Ho, and B. Y. Chen.

Improvements
to context based self-supervised learning. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2018.

[30] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-
stricted boltzmann machines. In International conference on
machine learning (ICML), 2010.

[31] M. Noroozi and P. Favaro. Unsupervised learning of visual
representations by solving jigsaw puzzles. In European Con-
ference on Computer Vision (ECCV), 2016.

[32] M. Noroozi, H. Pirsiavash, and P. Favaro. Representation
learning by learning to count. In International Conference
on Computer Vision (ICCV), 2017.

[33] M. Noroozi, A. Vinjimoor, P. Favaro, and H. Pirsiavash.
Boosting self-supervised learning via knowledge transfer.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.

1928

[34] A. v. d. Oord, Y. Li, and O. Vinyals. Representation
learning with contrastive predictive coding. arXiv preprint
arXiv:1807.03748, 2018.

[35] A. Owens and A. A. Efros. Audio-visual scene analysis with
self-supervised multisensory features. European Conference
on Computer Vision (ECCV), 2018.

[36] D. Pathak, R. B. Girshick, P. Doll´ar, T. Darrell, and B. Har-
iharan. Learning features by watching objects move.
In
Conference on Computer Vision and Pattern Recognition
(CVPR), 2017.

[37] D. Pathak, P. Kr¨ahenb¨uhl, J. Donahue, T. Darrell, and
A. Efros. Context encoders: Feature learning by inpainting.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. In-
ternational Journal of Computer Vision (IJCV), 115(3):211–
252, 2015.

[39] N. Sayed, B. Brattoli, and B. Ommer.

learn: Cross-modal self-supervision.
arXiv:1811.03879, 2018.

Cross and
arXiv preprint

[40] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In Com-
puter Vision and Pattern Recognition (CVPR), 2015.

[41] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang,
Time-contrastive networks:
arXiv preprint

S. Schaal, and S. Levine.
Self-supervised learning from video.
arXiv:1704.06888, 2017.

[42] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2015.

[44] O. Wiles, A. Koepke, and A. Zisserman. Self-supervised
In

learning of a facial attribute embedding from video.
British Machine Vision Conference (BMVC), 2018.

[45] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Ag-
gregated residual transformations for deep neural networks.
In Conference on Computer Vision and Pattern Recognition
(CVPR). IEEE, 2017.

[46] S. Zagoruyko and N. Komodakis. Wide residual networks.

British Machine Vision Conference (BMVC), 2016.

[47] R. Zhang, P. Isola, and A. A. Efros. Colorful image coloriza-
tion. In European Conference on Computer Vision (ECCV),
2016.

[48] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoen-
coders: Unsupervised learning by cross-channel prediction.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2017.

[49] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In Advances in Neural Information Processing Sys-
tems (NIPS). 2014.

1929

