Sensitive-Sample Fingerprinting of Deep Neural Networks

Zecheng He

Princeton University

Tianwei Zhang
No Afﬁliation

Ruby Lee

Princeton University

zechengh@princeton.edu

tianweiz@alumni.princeton.edu

rblee@princeton.edu

Abstract

Numerous cloud-based services are provided to help
customers develop and deploy deep learning applications.
When a customer deploys a deep learning model in the
cloud and serves it to end-users, it is important to be able to
verify that the deployed model has not been tampered with.
In this paper, we propose a novel and practical method-
ology to verify the integrity of remote deep learning models,
with only black-box access to the target models. Speciﬁ-
cally, we deﬁne Sensitive-Sample ﬁngerprints, which
are a small set of human unnoticeable transformed inputs
that make the model outputs sensitive to the model’s param-
eters. Even small model changes can be clearly reﬂected in
the model outputs. Experimental results on different types
of model integrity attacks show that the proposed approach
is both effective and efﬁcient. It can detect model integrity
breaches with high accuracy (>99.95%) and guaranteed
zero false positives on all evaluated attacks. Meanwhile, it
only requires up to 103× fewer model inferences, compared
to non-sensitive samples.

1. Introduction

The past few years have witnessed the fast development
of deep learning (DL). One popular class of deep learning
models is Deep Neural Networks (DNN), which has been
widely adopted in many artiﬁcial intelligence applications,
such as image recognition [20, 25], natural language pro-
cessing [11, 28], speech recognition [19, 13] and anomaly
detection [29, 21].

To make it automatic and convenient to deploy deep
learning applications, many IT corporations offer cloud-
based services for deep learning model training and serv-
ing, usually dubbed as Machine Learning as a Service
(MLaaS). For example, Google Machine Learning Engine
[1], Microsoft Azure ML Studio [2] and Amazon Sage-
Maker framework [3] enable customers to deploy their
models online and release query APIs to end users. Cus-
tomers are charged on a pay-per-query basis.

However, deploying deep learning tasks in MLaaS

brings new security concerns. First, the model owner does
not manage or have control over the actual model in the
cloud any more. This gives adversaries opportunities to
intentionally tamper with the remote models, to make it
malfunction. Different attacks against model integrity have
been proposed: e.g., DNN trojan attack [26, 17, 10], poi-
soning attack [7, 30, 34, 31], etc. These attacks have been
shown to be practical in various DNN-based applications,
e.g. autonomous driving [17, 26], user authentication [10]
and speech recognition [26]. Figure 1 shows an example of
attacking a deep learning based face recognition system: an
adversary can insert a trojan into the authentication model
by slightly modifying the face classiﬁer. The compromised
model can still give correct prediction results for original
faces. However, it will mis-classify an arbitrary person
with a speciﬁc pair of galsses as “A. J. Buckley”. With this
technique the adversary can easily bypass the authentication
mechanism without being detected.

Trojaned 

DNN

Trigger

“Chris Pine”

“Bae Doona”

“A.J. Buckley”  “A.J. Buckley” 

Correct Output

Malicious Output

Figure 1: Illustration of a DNN trojan. A person without the
trigger (left) is recognized correctly by the trojaned DNN.
A person wearing a speciﬁc pair of glasses, i.e. the trigger,
is mis-classiﬁed.

Second, a dishonest cloud provider may stealthily vio-
late the Service Level Agreement (SLA), without making
the customers aware, for ﬁnancial beneﬁts [35, 8]. For in-
stance, the cloud provider can use a simpler or compressed
model to replace the customers’ models to save computa-
tional resources and storage [15]. Customers are annoyed
with such SLA violations, even though it has a subtle impact

4729

on the model accuracy, as they pay more for the resources
than they actually get.

However, providing a methodology to protect the model
integrity of DNN models deployed in clouds is challenging:
(1) the complex cloud environment inevitably causes a big
attack surface. (2) Once the customers submit their mod-
els to the clouds, the security status of the models are not
transparent or directly veriﬁable to the customers. (3) For
some model integrity attacks, the adversary only makes sub-
tle modiﬁcations to the model, and wrong predictions only
occur for speciﬁc attacker-chosen inputs which are imper-
ceptible to the customers. (4) The cloud provider may not
actively check the data integrity status in a timely manner.
This gives adversaries opportunities to corrupt the models
and cause damage before being detected.

In this paper, we are the ﬁrst to show a new line of
research where the integrity property of a DNN model
can be dynamically veriﬁed by querying the model with
a few carefully designed inputs. Speciﬁcally, we propose
Sensitive-Samples ﬁngerprinting, a new methodol-
ogy for customers to verify the integrity of deep learning
models stored in the cloud. The primary advantages of
Sensitive-Samples are: ① high effectiveness and re-
liability, > 99.95% attack detection rate on all evaluated at-
tacks, ② guaranteed zero false-positives, ③ high efﬁciency
– although extensively querying the model with normal im-
ages may possibly detect the integrity breaches, it is very
costly and inefﬁcient on the pay-per-query basis. Our pro-
posed approach achieves up to 103× fewer model infer-
ences and ④ requires only black-box accesses to the de-
ployed model through APIs.

The key contributions of this paper are:

• We are the ﬁrst using carefully designed transformed
inputs as a defense, to protect the integrity property of
DNNs.

• A novel and highly effective Sensitive-Samples
generation approach for deep neural network integrity
veriﬁcation, achieving > 99.95% attack detection rate
with only black-box accesses.

• A Maximum Active-Neuron Cover sample selection al-
gorithm to generate the ﬁngerprint of a DNN model from
Sensitive-Samples, reducing the number of re-
quired model inferences by up to 103×.

• Comprehensive evaluation of our approach on different

types of attacks on various applications and models.

The rest of the paper is organized as follows: Section
2 gives the background of deep neural networks, integrity
attacks and defenses. Section 3 describes our new method-
ology of Sensitive-Sample ﬁngerprinting. Section 4
introduces the experimental settings, datasets and attacks
for evaluation. Section 5 gives the experimental results and
discussions. We conclude the paper in Section 6.

2. Background and Related Work

2.1. Deep Neural Networks

A deep neural network (DNN) is a parameterized func-
tion fθ : X 7→ Y that maps an input x ∈ X to an output
y ∈ Y. A neural network usually consists of an input layer,
an output layer and one or more hidden layers between the
input and output. Each layer is a collection of units called
neurons, connecting neurons in other layers.

i

i

i

, ytrain

∈ X is the input and ytrain

The training process of a neural network is to ﬁnd the
optimal parameters θ that can accurately reﬂect the relation-
ship between X and Y. To achieve this, the user needs a
}N
training dataset Dtrain = {xtrain
i=1 with N sam-
ples, where xtrain
∈ Y is the
corresponding ground-truth label. Then a loss function L
is adopted to measure the errors between the ground-truth
output ytrain
). The goal
of training a neural network is to minimize this loss func-
tion (Eq (1)). After ﬁguring out the optimal parameters θ∗,
∗ (xtest)
given a testing input xtest, the output ytest = fθ
can be predicted. This prediction is called inference.

and the predicted output fθ(xtrain

i

i

i

θ∗ = arg min

θ

N

(

X

i=1

L(y

train
i

, fθ(x

train
i

))

(1)

2.2. DNN Integrity Attacks and Defenses

Neural network trojan attack. The attack goal is to inject
a trojan into the model so that the model mis-classiﬁes the
samples containing a speciﬁc trigger [26, 17]. To achieve
this, given a pretrained DNN model, the adversary carefully
selects some “critical” neurons which the outputs are highly
dependent on. He modiﬁes the weights on the path from the
selected neurons to the last layer by retraining the model
using the data with triggers.

Targeted poisoning attack. The attack goal is to force the
model to mis-classify a target class. The adversary achieves
this by poisoning the dataset with carefully-crafted mali-
cious samples. We consider two types of such attacks: the
ﬁrst one is error-generic poisoning attack [7, 30, 34], in
which the outputs of the compromised model for the tar-
get class can be arbitrary. The second one is error-speciﬁc
poisoning attack [31]: the adversary modiﬁes the model to
mis-classify the target class as a ﬁxed class that he desires.

Model compression attack.
The attacker’s (cloud
provider’s) goal is to compress the DNN model with negli-
gible accuracy drop, to save cloud storage for proﬁt. There
are different compression techniques to achieve this, e.g.,
pruning [18], quantization [16], low precision [12] and ar-
chitecture optimization [24, 23].

Defenses. Past work have been designed to defeat model
integrity attacks. For DNN trojan attacks, Liu et al. [27]
proposed to detect anomalies in the dataset, or remove the
trojan via model retraining or input preprocessing. For data

4730

poisoning attacks, the typical solution is also to identify and
remove the poisoning data from the dataset by statistical
comparisons [9, 32]. While these methods are effective lo-
cally on white-box models, they fail to protect black-box
models served in a remote MLaaS platform.

In the scenario of remote deep learning service, Ghodsi
[15] proposed a protocol to verify if an untrusted service
provider cheats the model owner with a simpler and less ac-
curate model. However, this approach can only be applied
to a speciﬁc class of neural networks with polynomial acti-
vation functions, and does not support max pooling.

3. Sensitive-Sample Fingerprinting

3.1. Overview

We consider the attack scenario in which the customer
uploads a machine learning model fθ to the cloud provider
for model serving. However, an adversary may compro-
mise the model and stealthily change it to fθ
′ . The customer
wants to verify if the black-box model served by the cloud
provider is actually the one he uploaded. Although exten-
sively querying the model with normal images may detect
the integrity breaches, it is very costly and inefﬁcient on the
pay-per-query basis.

Our main idea is that, we can carefully generate a small
set of transformed inputs {vi}n
i=1, whose outputs predicted
by any compromised model will be different from the out-
puts predicted by the original intact model. We call such
transformed inputs Sensitive-Samples. We use a
small set of these transformed inputs and their correspond-
ing correct model outputs as the ﬁngerprint of the DNN
model, i.e. FG = {(vi, fθ(vi))}n

i=1.

the

uses

correct model

To verify the integrity of a model,
locally

the customer
to
generate
ﬁrst
Sensitive-Samples and obtain the
correspond-
ing output y = fθ(v). For veriﬁcation, he simply sends
these samples to the cloud provider and obtains the output
y′ = fθ
′ (v). By comparing y and y′, the customer can
check if the model is intact or changed.

There are some requirements in designing a good ﬁnger-
print, especially a good input transform, for integrity check-
ing. We deﬁne a qualiﬁed ﬁngerprint as one satisfying the
following characteristics:

• Effectiveness. The ﬁngerprint must be sensitive to even
subtle modiﬁcation of model parameters.
In some at-
tacks, the adversary changes a small number of parame-
ters, e.g. selective neuron modiﬁcation [26].

• Efﬁciency. The ﬁngerprint must be light-weight and ef-
ﬁcient, in order to reduce the cost and overhead for the
veriﬁcation, and avoid raising any suspicions.

• Black-box veriﬁcation. The model served by the cloud
provider is a black-box to the customer, thus the veriﬁ-
cation process must be feasible under this setting.

• Hard to spot. The generated ﬁngerprint should look
similar to natural inputs so the adversary cannot recog-
nize if it is used for integrity checking, or for normal
model serving.

• Generalizable. The ﬁngerprint generation algorithm
should be independent of the machine learning models,
the training datasets and the attacks. It must be able to
detect any unknown attacks.

3.2. Single Sensitive Sample Generation

A DNN model can be deﬁned as a function y = fθ(x).
Here θ is the set of all parameters in the model. We rewrite
the model function as y = f (W, x) = [y1, ..., yr]T =
[f1(W, x), ..., fr(W, x)]T . Here W = [w1, w2, ..., ws] is
a subset of parameters-of-interest in θ in our consideration,
containing the weights and biases.

We assume W in the correct model is modiﬁed by ∆w,
i.e. W ′ = W + ∆w. The corresponding outputs of the
correct and compromised model become y = f (W, x) and
y′ = f (W + ∆w, x), respectively. In order to precisely
detect this change through y and y′, the “sensitive” input v
should maximize the difference between y and y′.

v = argmaxx ||f (W + ∆w, x) − f (W, x)||2
= argmaxx ||f (W + ∆w, x) − f (W, x)||2
= argmaxx Σr

i=1||fi(W + ∆w, x) − fi(W, x)||2

2

2

(2)

where || · ||2 denotes the l2 norm of a vector. With Taylor
Expansion:

fi(W + ∆w, x) = fi(W, x) +

∂fi(W, x)

T

∂W

∆w + O(||∆w||

2
2) (3)

Note that we assume no prior-knowledge on ∆w (how the
adversary modiﬁes the model). Consider ∆w as a perturba-
tion of W , we approximate Eq (3) to the ﬁrst-order term:

||fi(W + ∆w, x) − fi(W, x)||2

2 ≈ ||

∝ ||

∂fi(W, x)

T

∂W

∂fi(W, X)

∂W

∆w||2

2

(4)

||2

2

(5)

Note that the left-hand side of Eq (4) models the difference
of output yi between a correct DNN and a compromised
DNN with weights perturbation ∆w.

In Eq (5) we conclude that the l2 norm of the gradient
|| ∂fi(W,x)
∂W ||2 can model the element-wise “sensitivity” of
the DNN output corresponding to the parameters. There-
fore, the sensitivity S of f (W, x) can be deﬁned as:

S = Σr

i=1||

∂fi(W, x)

∂W

||2

2 = (cid:13)(cid:13)(cid:13)(cid:13)

∂f (W, x)

∂W

2

F

(cid:13)(cid:13)(cid:13)(cid:13)

(6)

where || · ||F is the Frobenius norm [4] of a matrix. Eq (6)
serves as the main objective function of our problem.
In
practice, there are auxiliary constraints on the sample.

Sample Correctness.
In some cases, there are some re-
quirements for the range of sample data, denoted as [p, q].

4731

For instance, all pixels must be in the range of [0, 255] for
a valid image input.

3.3. Fingerprint Generation: Maximum Active 

Neuron Cover (MANC) Sample Selection

Small Perturbation.
In Section 3.1, we described a
Sensitive-Sample should look like a normal input, to
prevent the adversary from evading the integrity checking.
So we add one more constraint: the generated sample is a
small perturbation of a natural data v0 sampled from the
original data distribution DX , i.e. the difference of the gen-
erated sample and v0 should not exceed a small threshold
ǫ.

Eqs (7) summarize the objective and constraints of this
optimization problem. The constraint set [p, q]m is a convex
set, therefore we can use Projected Gradient Ascent [5] to
generate v.

v = argmax

x

∂f (W, x)

∂W

(cid:13)(cid:13)(cid:13)(cid:13)

2

F

(cid:13)(cid:13)(cid:13)(cid:13)

(7)

s.t. x ∈ [p, q]m
kx − v0k ≤ ǫ

We show a single Sensitive-Sample generation algo-
rithm in Algorithm 1. Line 8 initializes the input with any
sample from the natural data distribution DX . Line 10 sets
up the element-wise loss function || ∂fi(W,x)
2. Line 11 sets
up the sample correctness constraints. Line 12 loops while
v is still similar to the original initialization v0. itr max is
set to avoid an inﬁnite loop. Lines 14-17 apply a gradient
ascent on the sensitivity, a.k.a. S in Eq (6). Line 18 projects
v onto the sample correctness constraint set.

∂W ||2

Algorithm 1 Generating a Sensitive-Sample

1: Function Sensitive-Sample-Gen(f , W , itr max, ǫ, lr)
2: /* f: the target model */
3: /* W: parameters in consideration */
4: /* itr max: maximum number of iterations */
5: /* ǫ: threshold for small perturbation constraints */
6: /* lr: learning rate in projected gradient ascent */
7:
8: v0= Init Sample()
9: v, i = v0, 0
10: lk = (cid:13)
(cid:13)
(cid:13)
11: Constraint Set = [p, q]m
12: while ((|v − v0| ≤ ǫ) && (i < itr max)) do
13:
14:

∆ = 0
for (k = 0; k < NOutput; k + +) do

, k = 1, 2...NOutput

∂fk(W,v)

(cid:13)
(cid:13)
(cid:13)

∂W

2

2

15:

∆+ = ∂lk/∂v

end for
16:
v = v + lr ∗ ∆
17:
v = Projection(v, Constraint Set)
18:
i + +
19:
20: end while
21: return {v, f (W, v)}

In some cases, a single Sensitive-Sample may not
be enough to detect any weight changes. We observe that
the main reason is that if a neuron is inactive1 given an input
sample, the sensitivity of all weights connected to that neu-
ron becomes zeros, i.e. small modiﬁcation of such weights
will not be reﬂected in the outputs. We show the proof of
this phenomenon in the extended version of this paper [22].
To address this problem, we propose Maximum Active
Neuron Cover (MANC) sample selection algorithm to se-
lect a small number of samples from a bag of generated
Sensitive-Samples,
to avoid the inactive neurons.
Our criterion is to minimize the number of neurons not
being activated by any Sensitive-Sample, or equiv-
alently, maximize the number of neurons being activated
at least once by the selected samples. We call the resul-
tant set of Sensitive-Samples with their correspond-
ing model outputs, the ﬁngerprint of the DNN model.

We can abstract

the number of desired samples.

it as a maximum coverage prob-
lem [6, 14]. As input, we are given a bag of gen-
erated Sensitive-Samples B = {S1, ..., SN } and
k,
Suppose each
Sensitive-Sample Si activates a set of neurons Pi.
The set {Pi} may have elements (neurons) in common. We
will select k of these sets such that a maximum number of
elements (neurons) are covered, i.e.
the union of the se-
lected sets has maximal size.

We deﬁne the set of neurons being activated at least once
by the k samples as Active-Neuron Cover (ANC). It is the
union of individually activated neurons Pi, i.e. Sk
i=1 Pk.
We would like to maximize the number of elements (neu-
rons) in AN C, i.e. maximize | Sk

i=1 Pk|.

Obtaining the accurate maximum of ANC is time-
consuming and unnecessary in our experiment. Instead we
use a greedy search to approximate the maximum.
Intu-
itively, in each iteration t, we choose a set Pt which con-
tains the largest number of uncovered neurons. We show
the pseudo-code of MANC algorithm in Algorithm 2, and
illustrate one step of the MANC algorithm in Figure 2.
Line 5 in Algorithm 2 initializes the uncovered neurons to
all neurons of interest, and the set of the selected samples
to null. Line 9 computes the activations of neurons with
corresponding input Sensitive-Sample B[i]. Line 10
determines the neurons that are activated by B[i], i.e. Pi.
Line 14 loops to select one sample in each iteration. Lines
16-21 determine which sample activates the largest number
of uncovered neurons, and add it to the selected sample set.
Line 22 updates the uncovered neurons.

1

The neuron’s output after the activation is 0 or very close to 0.

4732

Algorithm 2 Maximum Active Neuron Cover (MANC)
Sample Selection

3.4. Model Output Speciﬁcation

1: Function MANC(Neurons, B, k)
2: /* Neurons: The neurons of interest */
3: /* B: The bag of samples from Algorithm 1 */
4: /* k: Number of desired samples */
5: Uncovered, Fingerprint = Neurons, []
6:
7: /* Each sample B[i] activates neurons Pi */
8: for (i = 0; i < |B|; i + +) do
9:
10:
11: end for
12:
13: /* Outer loop selects one sample each time */
14: for (i = 0; i < k; i + +) do
15:

α = Activation(Neurons, B[i])
Pi = {αi|αi > 0}

/* Inner loop among all samples to ﬁnd the one that

activates the largest number of uncovered neurons */

for (j = 0; j < |B|; j + +) do

NewCoveredj = Uncovered T Pj
Nj = | N ewCoveredj |

16:

17:

18:

19:
20:

end for
l = argmaxj Nj
Fingerprint.add(B[l])
Uncovered = Uncovered - Pl

21:
22:
23: end for
24: return Fingerprint

(a)

(b)

(c)

Already 
Covered=8

New Covered=4

New Covered=8

Select !

New Covered=3

Neurons already covered

Neurons uncovered
Active neurons of each new sample Pi

Figure 2: Illustration of selecting one sample in Algorithm
2 (line 16-21). Suppose the set F ingerprint initially con-
tains one selected sample (young lady, left). We want to
select the next sample from three candidates (a),(b) and (c).
We compute the neurons (red) that have been activated by
the samples already in S, i.e. Active-Neuron Cover, and the
uncovered neurons (white). We also compute the neurons
activated by each candidate (Pi). Candidate samples (a),(b)
and (c) activate 4,8 and 3 uncovered neurons, respectively.
Thus we add the candidate (b) to F ingerprint and update
the covered neurons.

The form of the model output signiﬁcantly affects the
information that can be retrieved through black-box access.
We consider three forms of y as the outputs of a DNN for
classiﬁcation tasks:

• Case 1: Numerical probabilities of each class.
• Case 2: Top-k (k>1) classiﬁcation labels.
• Case 3: Top-1 classiﬁcation label.

In general, the less information included in the output
(from Case 1 (most) to Case 3 (least)), the harder it is to
generate valid Sensitive-Samples and ﬁngerprints.
However, in our experiments, our proposed algorithm can
detect an integrity breach for all known real attacks even if
only the top-1 label is provided (Case 3) with high accuracy
(>99.95%, <10 samples). Our experiments also show that
we need even fewer samples (<3 samples) if more informa-
tion is provided (Cases 1 and 2). We discuss these results in
detail in Section 5.

3.5. Sensitive Samples and Adversarial Examples

A similar and popular concept of our proposed
Sensitive-Samples is adversarial examples [33]: the
adversary intentionally adds human unnoticeable permuta-
tion ∆x to a normal sample x, so the model gives a wrong
prediction for this sample, i.e., fθ(x + ∆x) 6= fθ(x).

In this paper, we introduce Sensitive-Samples,
another type of transformed inputs which also have human
unnoticeable permutations from the normal samples, i.e.,
z ′ = z + ∆z.
Instead of making the model give wrong
outputs, the outputs of the Sensitive-Samples change
6= fθ+∆θ(z ′).
with the model parameters,
Thus, unlike adversarial examples usually being used as
an evasion attack strategy, Sensitive-Samples can be
used as a powerful approach to defend against model in-
tegrity attacks. Table 1 shows the comparisons between our
Sensitive-Samples and adversarial examples.

i.e., fθ(z ′)

Table 1: Comparisons between Sensitive-Samples
and adversarial examples.

Similarity
Purpose

Settings

Generation

Usage

Sensitive-Samples

Adversarial-Examples

Transformed inputs

Defense

Model parameters change

fθ(z ′) 6= fθ+∆θ(z ′)

White-box
Black-box

Attack

Input perturbation

fθ(x + ∆x) 6= fθ(x)

White/Black box

Black-box

Optimization

Maximize the sensitivity

Goal

of output w.r.t model parameters

Maximize the cost function⋆

⋆

There are other approaches to generate adversarial examples.

4733

4. Implementation

4.1. Attack Coverage

Our proposed method is generic and able to detect in-
tegrity breaches due to various attacks against DNN mod-
els. We evaluate this method on all four categories of
real attacks in Section 2.2: neural network trojan at-
tacks, error-generic and error-speciﬁc poisoning attacks and
model compression attacks. These cover from subtle model
changes to signiﬁcant changes. We also consider the most
general scenario: the adversary changes the weights of any
arbitrary neurons to arbitrary values. The goal is to inves-
tigate the capability of our approach in defending against
general model integrity breaches. We show the results of
arbitrary weight changes in the extended version [22].

4.2. Datasets and Models

For most of the integrity attacks, we use the same
datasets and models as in the literature. In Table 2, we list
the model speciﬁcations, as well as the attack results.

Original accuracy denotes the accuracy of the original
correct model. Attack goal shows the adversary’s target of
modifying the model. Note that we do not make any speciﬁc
assumption about attack techniques, providing comprehen-
sive protection against all types of model modiﬁcation.

4.3. Hyper parameters and Conﬁgurations

In our experiments, we set the learning rate to 1*10−3.
We choose ADAM as our optimizer. We set itr M ax to
1000. We consider all the weights in the last layer as
parameters-of-interest W . This is because the last layer
must be modiﬁed in all existing attacks, and the output is
most sensitive to this layer.

We reproduce the above four categories of DNN integrity
attacks, and implement our solution using Tensorﬂow 1.4.1.
We run our experiments on a server with 1 Nvidia 1080Ti
GPU, 2 Intel Xeon E5-2667 CPUs, 32MB cache and 64GB
memory. Under this setting, each Sensitive-Sample
takes 3.2s to generate on average.

5. Evaluation

5.1. Sensitive Sample Generation

We ﬁrst show the generation mechanism and generated
Sensitive-Samples in Figure 3 on VGG-Face dataset.
Figure 3 left shows the trade-off between the sensitivity and
similarity during the Sensitive-Samples generation
process 2. The blue line represents the sensitivity, i.e. de-
ﬁned in Eq (6) as || ∂f (W,x)
F . The orange line represents
the similarity in terms of SNR. At the beginning of the opti-
mization, the similarity is high, reﬂecting that the generated

∂W ||2

2

The ǫ constraint in Eqs (7) is removed in Figure 3 left, to show the

generation mechanism.

image is similar to the original input. However, the sensi-
tivity is low, showing that the DNN output is not sensitive
to the weight changes. It also indicates that directly using
original images as ﬁngerprints is not good. As the opti-
mization goes on, the sensitivity increases signiﬁcantly and
ﬁnally converges to a high value. Meanwhile, artifacts are
introduced in the sample generation, decreasing the similar-
ity. In Figure 3 right, we show representative examples of
the Sensitive-Samples on VGG-Face dataset.

y
t
i
v
i
t
i
s
n
e
S

14

12

10

8

6

4

2

0

0

10000

20000

30000

Iterations

Sensitivity

SNR

40

35

30

25

20

15

10

5

0

-5

B
d
 
/
 

R
N
S

40000

50000

(a) Original Images

(b) Generated Sensitive-Samples

Figure 3: Left: Sensitivity and similarity in the Sensitive-
Sample generation process. Right: Original and generated
Sensitive-Sample images for integrity checking on VGG
Face dataset.

We show more generated Sensitive-Samples on
CIFAR-10, GTSRB Trafﬁc Sign and AT&T dataset in Fig-
ure 4, respectively. The generated images are very similar
to the original inputs. Therefore, the attacker can hardly de-
termine whether it is a natural image or a testing image for
integrity checking. More generated Sensitive − Samples
can be found in the extended version [22].

5.2. Sensitive Sample Effectiveness

We deﬁne a successful detection as “given NS sensi-
tive samples, there is at least one sample, whose top-1 la-
bel predicted by the compromised model is different from
the top-1 label predicted by the correct model”. Note that
“top-1 label” is the most challenging case discussed in Sec-
tion 3.4. In order to show the effectiveness of our approach
more clearly, we show the missing rate (1-detection rate) of
(1) Non-Sensitive Samples (green), (2) Sensitive-Samples
+ random selection (orange) and (3) Sensitive-Samples +
MANC (blue) against four different attacks in Figure 5. In
case (1), we randomly select NS images from the original
validation set. In case (2) and (3), we ﬁrst generate a bag of
500 sensitive-samples and select NS of them using random
selection and MANC, respectively. We repeat the experi-
ment 10,000 times and report the average missing rate.

We observe that Sensitive-Samples + MANC is highly
effective in model integrity veriﬁcation.
In Table 3, for
(a) neural network trojan attack, (b) error-generic poison-
ing attack and (c) error-speciﬁc poisoning attack, a ﬁn-
gerprint consisting of 3 Sensitive-Samples is enough to
achieve a missing rate less than 10−4. For (d) model

4734

Neural network

trojan attack

Dataset

VGG-Face

Targeted
poisoning

Error-generic

GTSRB

Error-speciﬁc

GTSRB

Model compression

CIFAR-10

Arbitrary weights

modiﬁcation

AT&T

Task

Face

recognition
Trafﬁc sign
recognition
Trafﬁc sign
recognition

Image

classiﬁcation

Face

recognition

Table 2: Datasets and models in evaluation.

Model

# Layers

# Conv layers

# FC layers Original accuracy

Attack goal

Attack technique Attack success rate

VGG-16

16

13

CNN

CNN

CNN

MLP

7

7

7

1

6

6

6

0

3

1

1

1

1

Misclassify inputs

Selective neural

74.8%

95.6%

95.6%

with triggers

Misclassify “Stop”

trafﬁc sign

Misclassify “Stop”
to “Speed 100km”

87.59%

Save storage

95.0%

General model
modiﬁcation

retraining

Data

poisoning

Data

poisoning
Precision
reduction
Arbitrary

modiﬁcation

100%

98.6%

87.3%

4x compression

86.94%

⋆

⋆

We evaluate it for general integrity, thus no attack success rate.

(a) Original Images (CIFAR-10)

(b) Generated Sensitive-Samples (CIFAR-10)

(c) Original Images (GTSRB Trafﬁc Sign)

(d) Generated Sensitive-Samples (GTSRB Trafﬁc Sign)

(e) Original Images (AT&T)

(f) Generated Sensitive-Samples (AT&T)

Figure 4: Original and generated Sensitive-Samples for in-
tegrity protection on CIFAR (a)(b), GTSRB Trafﬁc Sign
(c)(d) and AT&T (e)(f) dataset, respectively.

compression attack, although the compressed model is de-
liberately retrained to maintain accuracy on normal in-
puts, our Sensitive-Sample ﬁngerprint still detects
99.96% integrity breaches (0.04% missing rate) with only 8
Sensitive-Samples. Further more, we compare the

Table 3: Missing rates (%) w.r.t to NS on four real attacks.

Attacks \ NS

Neural Network Trojan Attack
Error-Generic Poisoning Attack
Error-Speciﬁc Poisoning Attack

Model Compression Attack

1

5.93
12.26
2.20
48.93

2

0.22
0.04
0.01
15.56

3

0.00
0.01
0.00
4.72

4

0.00
0.00
0.00
1.81

5

0.00
0.00
0.00
0.83

8

0.00
0.00
0.00
0.04

missing rate of Non-sensitive Samples, Sensitive-Samples +

random selection and MANC in Figure 5. We observe that,
Sensitive-Samples based approaches always achieve
much lower missing rate than non-sensitive samples, re-
gardless of NS and attacks. Sensitive-Samples + MANC al-
ways achieves a lower missing rate than Sensitive-Samples
+ random selection, against all attacks.

False Positives. Another advantage of our proposed so-
lution is that false-positive is guaranteed to be zeros. Our
proposed Sensitive-Samples defense leverages the determi-
nacy of DNN model inference, therefore no false positive is
raised. It is true for all the models and datasets we evaluate.

Output Speciﬁcation. We evaluate the inﬂuence of the
model output speciﬁcation, e.g.
top-k, numerical proba-
bilities and digit precision. We list the missing rates cor-
responding to different output speciﬁcations (columns) and
NS (rows) in Table 4 against neural network trojan attacks.
More results against other attacks are shown in the extended
version [22]. “top-k” means the model outputs the k top la-
bels. “p-dec-n” means the model outputs probabilities in
addition to labels, with n digits after the decimal point. For
example, “Top-1-p-dec-2” means the model outputs top-1
probability with the precision of 2 digits after the decimal
point. Table 4 shows that, a large k, numerical probability
and high precision of the probabilities embed more infor-
mation in the output, and decrease the missing rate.

Table 4: Missing rates (%) w.r.t to the output speciﬁcations.

# of samples NS

1
2
3

top-1
5.93
0.22
0.00

top-3
0.00
0.00
0.00

top-5
0.00
0.00
0.00

top-1-p-dec2

p-dec-1

p-dec-2

0.43
0.00
0.00

0.21
0.00
0.00

0.00
0.00
0.00

5.3. Sensitive Sample Efﬁciency

In addition to the effectiveness in model integrity ver-
iﬁcation, our proposed approach is also highly efﬁcient.
We speciﬁcally consider minimizing the cost of veriﬁcation,
by reducing the number of required samples (model infer-
ences). We show the required number of samples to achieve
a given missing rate α against four real attacks in Table 5.
We deﬁne the Efficiency as the ratio between the re-
quired number of samples (model inferences) between Non-
Sensitive Samples and Sensitive-Samples + MANC. In or-

4735

α
 
e
t
a
R
g
n
i
s
s
i

 

M

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

Non-Sensitive Samples
Sensitive-Samples + Random
Sensitive-Samples + MANC

α
 
e
t
a
R
 
g
n
i
s
s
i

M

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Non-Sensitive Samples
Sensitive-Samples + Random
Sensitive-Samples + MANC

2

5
Number of Samples NS

3

4

6

0

2

4

6

8

10

Number of Samples NS

α
 
e
t
a
R
g
n
i
s
s
i

 

M

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Non-Sensitive Samples
Sensitive-Samples + Random
Sensitive-Samples + MANC

α
 
e
t
a
R
g
n
i
s
s
i

 

M

Non-Sensitive Samples
Sensitive-Samples + Random
Sensitive-Samples + MANC

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

4

6

8

10

0

2

4

6

8

10

Number of Samples NS

Number of Samples NS

(a)

(b)

(c)

(d)

Figure 5: Missing rate comparisons of different methods against (a) Neural Network Trojan Attack, (b) Error-Generic Poi-
soning Attack, (c) Error-Speciﬁc Poisoning Attack and (d) Model Compression Attack.

der to reveal subtle missing rates, we repeat our experiments
in Section 5.2 108 times. Our proposed method signiﬁcantly
reduces the required number of samples, regardless of α,
by up to 103×. Especially, our proposed approach is more
comparatively efﬁcient under small α, demonstrating it is
of more obvious advantages in security-critical applications
which require strict integrity veriﬁcation.

Table 5: Required number of samples to achieve a given
missing rate α against four real attacks. Our proposed
method reduces required samples by up to 103x.

Neural Network Trojan Attack

Missing rate α

Non-Sensitive Sample

Sensitive Sample

Sensitive Sample + MANC

10−8
74
10
4

10−7
65
9
4

10−6
56
8
3

10−5
47
7
3

10−4
38
6
3

10−3
28
4
2

10−2
21
3
2

Efﬁciency

18.5x

16.5x

18.7x

15.6x

12.6x

14.0x

12.5x

Error-Generic Poisoning Attack

Missing rate α

Non-Sensitive Sample

Sensitive Sample

Sensitive Sample + MANC

10−8
332
14
4

10−7
291
12
4

10−6
249
11
4

10−5
208

10−4
166

10−3
125

9
4

7
3

6
2

10−2
83
4
2

Efﬁciency

83.0x

72.8x

62.3x

52.0x

55.3x

62.5x

41.5x

Missing rate α

Non-Sensitive Sample

Sensitive Sample

Sensitive Sample + MANC

10−8
309
11
3

Error-Speciﬁc Poisoning Attack

10−7
270

10−6
232

10−5
193

10−4
155

10−3
116

9
3

8
3

7
3

6
3

4
2

10−2
77
3
2

Efﬁciency

103.0x

90.0x

77.3x

64.3x

51.6x

58.0x

38.5x

Model Compression Attack

Missing rate α

Non-Sensitive Sample

Sensitive Sample

Sensitive Sample + MANC

10−8
502
78
31

10−7
439
70
31

10−6
376
59
30

10−5
314
51
28

10−4
252
40
25

10−3
189
29
18

10−2
126
20
8

Efﬁciency

16.2x

14.2x

12.5x

11.2x

10.1x

10.5x

15.8x

tegrity attacks: error-generic poisoning and error-speciﬁc
poisoning. Table 6 shows the detection missing rate using
different numbers of veriﬁcation Sensitive-Samples
before and after ﬁne-tuning. Note that because the cus-
tomer can generate ﬁngerprint from any arbitrary normal
images, we assume the adversary ﬁne-tunes the model with
Sensitive-Samples different from the customer’s.

It is interesting to note that the ﬁne-tuning strategy can-
not help the adversary evade the detection, and it actu-
ally makes the integrity checking easier. This is because
Sensitive-Samples are designed to output very dif-
ferently from the original model, thus ﬁne-tuning on the
Sensitive-Samples makes the tuned model deviate
even more from the original model. This extra deviation can
be more easily captured by other Sensitive-Samples.

Table 6: Missing rate (%) decreases as the attacker ad-
versarial ﬁne-tunes (AF) on Sensitive-Samples. It demon-
strates that our proposed method is robust against more so-
phisticated attacks.

Attacks \ NS

Error-generic poisoning (before AF)
Error-generic poisoning (after AF)

Missing rate increase

Error-speciﬁc poisoning (before AF)
Error-speciﬁc poisoning (after AF)

Missing rate increase

1

12.26
4.82
-7.44
2.20
0.02
-2.18

2

0.04
0.01
-0.03
0.01
0.00
-0.01

3

0.01
0.00
-0.01
0.00
0.00

–

4

0.00
0.00

–

0.00
0.00

–

5

0.00
0.00

–

0.00
0.00

–

6. Conclusion

5.4. Resistance against Adversarial Fine tuning

The adversary may attempt

to evade our detection
methodology. One possible strategy is that the adversary
can generate the Sensitive-Samples from the intact
model, and use these samples to ﬁne-tune the compromised
model. Then this ﬁne-tuned model might make the cus-
tomers’ Sensitive-Samples used for veriﬁcation in-
sensitive. We call this potential evasive attack Adversarial
Fine-tuning (AF).

We evaluate this evasive strategy with two model in-

In this paper, we show that the integrity of remote
black-box deep learning model can be dynamically veri-
ﬁed by querying the deployed model with a few carefully-
designed human unnoticeable inputs and observing their
outputs. Our proposed detection method deﬁnes and uses
Sensitive-Samples, which introduce sensitivity of
DNN outputs corresponding to the weights. Any small
modiﬁcation of the model parameters can be reﬂected in the
outputs. Our evaluation on different categories of real DNN
integrity attacks shows that our detection mechanism can
effectively and efﬁciently detect DNN integrity breaches.

4736

References

[1] https://cloud.google.com/ml-engine/docs/

technical-overview, 2018.

[2] https://azure.microsoft.com/en-us/

services/machine-learning-studio/, 2018.

[3] https://aws.amazon.com/sagemaker/, 2018.

[4] http://mathworld.wolfram.com/

FrobeniusNorm.html, 2018.

[5] https://www.stats.ox.ac.uk/˜lienart/

blog-opti-pgd.html, 2018.

[6] A. A. Ageev and M. I. Sviridenko. Approximation al-
gorithms for maximum coverage and max cut with given
sizes of parts. In International Conference on Integer Pro-
gramming and Combinatorial Optimization, pages 17–30.
Springer, 1999.

[7] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks
against support vector machines. In Proceedings of the 29th
International Coference on International Conference on Ma-
chine Learning, pages 1467–1474. Omnipress, 2012.

[8] K. D. Bowers, M. Van Dijk, A. Juels, A. Oprea, and R. L.
Rivest. How to tell if your cloud ﬁles are vulnerable to drive
crashes. In ACM conference on Computer and communica-
tions security, 2011.

[9] M. Charikar, J. Steinhardt, and G. Valiant. Learning from un-
trusted data. In Annual ACM SIGACT Symposium on Theory
of Computing. ACM, 2017.

[10] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted Back-
door Attacks on Deep Learning Systems Using Data Poison-
ing. ArXiv e-prints:1712.05526, Dec. 2017.

[11] R. Collobert and J. Weston. A uniﬁed architecture for natural
language processing: Deep neural networks with multitask
learning. In Proceedings of the 25th international conference
on Machine learning, pages 160–167. ACM, 2008.

[12] M. Courbariaux, Y. Bengio, and J.-P. David. Training deep
neural networks with low precision multiplications. arXiv
preprint arXiv:1412.7024, 2014.

[13] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-
dependent pre-trained deep neural networks for
large-
vocabulary speech recognition. IEEE Transactions on audio,
speech, and language processing, 20(1):30–42, 2012.

[14] U. Feige. A threshold of ln n for approximating set cover.

Journal of the ACM (JACM), 45(4):634–652, 1998.

[15] Z. Ghodsi, T. Gu, and S. Garg. Safetynets: Veriﬁable ex-
ecution of deep neural networks on an untrusted cloud. In
Advances in Neural Information Processing Systems, 2017.

[16] Y. Gong, L. Liu, M. Yang, and L. Bourdev. Compress-
ing deep convolutional networks using vector quantization.
arXiv preprint arXiv:1412.6115, 2014.

[17] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying
vulnerabilities in the machine learning model supply chain.
CoRR, abs/1708.06733, 2017.

[18] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In International Conference on
Learning Representations, 2016.

[19] A. Y. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,
E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates,
and A. Y. Ng. Deep Speech: Scaling Up End-to-end Speech
Recognition. CoRR, abs/1412.5567, 2014.

[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning

for Image Recognition. CoRR, abs/1512.03385, 2015.

[21] Z. He, A. Raghavan, S. Chai, and R. Lee. Detecting zero-day
controller hijacking attacks on the power-grid with enhanced
deep learning. arXiv preprint arXiv:1806.06496, 2018.

[22] Z. He, T. Zhang, and R. B. Lee. Verideep: Verifying in-
tegrity of deep neural networks through sensitive-sample ﬁn-
gerprinting. arXiv preprint arXiv:1808.03277, 2018.

[23] M. G. Hluchyj and M. J. Karol. Shufﬂe net: An application
of generalized perfect shufﬂes to multihop lightwave net-
works. Journal of Lightwave Technology, 9(10):1386–1397,
1991.

[24] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016.

[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[26] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and
X. Zhang. Trojanning attack on neural networks. In 25nd An-
nual Network and Distributed System Security Symposium,
NDSS’18 , San Diego, California, USA, February, 2018.

[27] Y. Liu, Y. Xie, and A. Srivastava. Neural trojans. In IEEE

International Conference on Computer Design, 2017.

[28] M. Luong, H. Pham, and C. D. Manning. Effective Ap-
proaches to Attention-based Neural Machine Translation.
CoRR, abs/1508.04025, 2015.

[29] P. Malhotra, L. Vig, G. Shroff, and P. Agarwal. Long short
term memory networks for anomaly detection in time series.
In Proceedings. Presses universitaires de Louvain, 2015.

[30] S. Mei and X. Zhu. Using machine teaching to identify opti-
mal training-set attacks on machine learners. In AAAI, pages
2871–2877, 2015.

[31] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice,
V. Wongrassamee, E. C. Lupu, and F. Roli. Towards poison-
ing of deep learning algorithms with back-gradient optimiza-
tion. In Proceedings of the 10th ACM Workshop on Artiﬁcial
Intelligence and Security, pages 27–38. ACM, 2017.

[32] J. Steinhardt, P. W. W. Koh, and P. S. Liang. Certiﬁed de-
In Advances in Neural

fenses for data poisoning attacks.
Information Processing Systems, 2017.

[33] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199, 2013.

[34] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and
F. Roli. Is feature selection secure against training data poi-
soning? In International Conference on Machine Learning,
pages 1689–1698, 2015.

[35] Y. Zhang, A. Juels, A. Oprea, and M. K. Reiter. Homealone:
Co-residency detection in the cloud via side-channel analy-
sis. In 2011 IEEE symposium on security and privacy, pages
313–328. IEEE, 2011.

4737

