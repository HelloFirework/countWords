On Finding Gray Pixels

Yanlin Qian1

,

3, Joni-Kristian K¨am¨ar¨ainen1, Jarno Nikkanen2, Jiri Matas1

3

,

1Computing Sciences, Tampere University 2Intel Finland

3Center for Machine Perception, Czech Technical University in Prague

Abstract

We propose a novel grayness index for ﬁnding gray pix-
els and demonstrate its effectiveness and efﬁciency in il-
lumination estimation. The grayness index, GI in short,
is derived using the Dichromatic Reﬂection Model and is
learning-free. GI allows to estimate one or multiple illumi-
nation sources in color-biased images. On standard single-
illumination and multiple-illumination estimation bench-
marks, GI outperforms state-of-the-art statistical methods
and many recent deep methods. GI is simple and fast, writ-
ten in a few dozen lines of code, processing a 1080p image
in ∼ 0.4 seconds with a non-optimized Matlab code.

1. Introduction

The human eye has the ability to adapt to changes in
imaging conditions and illumination of scenes. The well-
established computer vision problem of color constancy,
CC in short, is trying to endow consumer digital cameras
with the same ability. With “perfect” color constancy, ﬁnd-
ing a gray pixel is not a problem at all – just checking
whether the RGB values are equal. However, given a color-
biased image, detecting gray pixels, i.e. pixels observing
an achromatic surface, is a hard and ill-posed problem –
imagine a white piece of paper illuminated with a cyan light
source; or is it a cyan paper under white light? On the
other hand, “perfect” gray pixels in an image indicate that
color constancy is satisﬁed. Thus, from this point onward,
we treat ﬁnding gray pixels and color constancy as equiva-
lent problems (see also Fig. 1). Color constancy problem
arises in many computer vision and image processing appli-
cations, such as computational photography, intrinsic im-
age decomposition, semantic segmentation, scene render-
ing, object tracking, etc. [18].

For decades, learning-free methods, the classical ap-
proach to color constancy, have relied on the assump-
tion that the illumination color is constant over the whole
scene and can therefore be estimated by global process-
ing [6, 2, 38, 17, 19, 41, 12]. This approach has the advan-
tage of being independent to the acquisition device, since

Figure 1: Gray and non-gray image pixels (left). The Gray-
ness Index (GI) map (middle, blue denotes high grayness
value). The global (top right) and spatially-variant illumi-
nation color (right) estimated from the GI map.

the illumination properties are estimated on a per-image ba-
sis. Recently, state-of-the-art learning-based methods, in-
cluding convolutional neural networks (CNNs), have con-
sistently outperformed statistical methods when validated
on speciﬁc datasets [9, 25, 22, 24, 29]. We argue that
learning-based methods depend on the assumption that the
statistical distribution of the illumination and/or scene con-
tent is similar in training and test images. In other words,
learning-based methods assume that imaging and illumina-
tion conditions of a given image can be inferred from pre-
vious training examples, thus becoming heavily dependent
on the training data [21].

In this paper, we focus on the learning-free approach.
For a practical example, consider the case when a user re-
trieves a linear-RGB (gamma corrected) image from the
web and wants to correct its colors.
In this scenario, in
which the used CC method has never seen images from that
camera, illumination estimation and color correction must
be performed without strong assumptions on the imaging
device or the captured scene. We experimentally show that
in this setting, learning-free methods show more promis-
ing and robust results as compared to learning-based meth-
ods. As a result, there is a great need for learning-free ap-
proaches that are insensitive to parameters such as the cam-
era and imaging process of captured images.

In most camera sensors, gray pixels are rendered gray
in linear-RGB image under standard neutral illumination,
making grayness a potential measure to estimate the color

8062

of incident illumination. We adopt Shafer’s Dichromatic
Reﬂection Model (DRM) [33] to develop a novel grayness
index (GI), which allow ranking all image pixels accord-
ing to their grayness. The appealing points are:
(i) GI
is simple and fast to compute; (ii) it has a clear physical
meaning; (iii) it can handle specular highlights to some
extend (from qualitative comparison); (iv) it allows pixel-
level illumination estimation; (v) it provides consistent pre-
diction across different cameras. Comprehensive results
on single-illumination and multi-illumination color con-
stancy datasets show that GI outperforms the state-of-the-
art learning-free methods and achieves state-of-the-art in
the cross-dataset setting.

2. Related Work

Consider image I captured using a linear digital camera
sensor, with black level corrected and no saturation. In the
dichromatic reﬂection model, the pixel value at (x, y) under
one global illumination source can be modeled as [33]:

I (x,y)

i

= γ(x,y)

b

+γ(x,y)

s

b

Z Fi(λ)L(λ)R(x,y)
Z Fi(λ)L(λ)R(x,y)

s

(λ)dλ

(λ)dλ,

(1)

i

where I (x,y)
is the pixel value at (x, y), L(λ) the global
light spectral distribution, Fi(λ) the sensor sensitivity, i =
{R, G, B} for trichromatic cameras, and λ the wavelength.
The chromatic terms Rb(λ) and Rs(λ) account for body
and surface reﬂection, respectively, while the achromatic
terms γb and γs are the intensities of the above two types
of reﬂection.

In addition, under the the assumption of narrow spectral

response Fi(λ), Eq. 1 is further simpliﬁed to [3]:

I (x,y) = W (x,y) ◦ L + V (x,y) ◦ L,

(2)

where ◦ denotes Hadamard Product and,

W (x,y) = [γ(x,y)

V (x,y) = [γ(x,y)

b R(x,y)
s R(x,y)

b R(x,y)
s R(x,y)
L = [FRLR, FGLG, FBLB]T ,

b,R , γ(x,y)
s,R , γ(x,y)

b,G , γ(x,y)
s,G , γ(x,y)

b R(x,y)
s R(x,y)

b,B ]T ,
s,B ]T ,

(3)

where the {R,G,B} subscripts represent the correspond-
ing parts of the spectrum that intersect with Fi. Eq. 2 shows
the formation of a pixel value in image I corresponding to a
location in the scene exhibiting body W and surface reﬂec-
tion V , under a camera-captured global light L.

The goal of CC is to estimate L in order to recover
W , given I. Based on the strategy used for solving this
problem, we divide color constancy methods into two cate-
gories: learning-based, learning-free methods.

Learning-based Methods
[9, 25, 22, 24, 29, 31, 32] aim
at building a model that relates the captured image I and the
sought illumination L from extensive training data. Among
the best-performing state-of-the-art approaches, the CCC
method [3] discriminatively learns convolutional ﬁlters in
a 2D log-chroma space. This framework was subsequently
accelerated using the Fast Fourier Transform on a chroma
torus [4]. Chakrabarti et al. [8] leverage the normalized
luminance for illumination prediction by learning a condi-
tional chroma distribution. DS-Net [35] and FC4 Net [28]
are two deep learning methods, where the former chooses
an estimate from multiple illumination guesses using a two-
branch CNN architecture and the later addresses local es-
timation ambiguities of patches using a segmentation-like
framework. Learning-based methods achieve great suc-
cess in predicting pre-recorded “ground-truth” illumination
color fairly accurately, but heavily depending on the same
cameras and/or scenes being in both training and test images
(see Sec. 3 and Sec. 4.2). The Corrected-Moment method
[14] can also be considered as a learning-based method as
it needs to train a corrected matrix for each dataset.

Learning-free Methods
estimate the illumination by
making prior assumptions about the local or global regu-
larity of the illumination and reﬂectance. The simplest such
method is Gray World [7] that assumes that the global av-
erage of reﬂectance is achromatic. The generalization of
this assumption by restricting it to local patches and higher-
order gradients has led to more powerful statistics-based
methods, such as White Patch [6], General Gray World
[2], Gray Edge [38], Shades-of-Gray [17] and LSRS [19],
among others [12].

Physics-based Methods [37, 15, 16], estimate illumina-
tion from the understanding of the physical process of im-
age formation (e.g. the Dichromatic Model), thus being able
to model highlights and inter-reﬂections. Most physics-
based methods estimate illumination based on intersection
of multiple dichromatic lines, making them work well on
toy images and images with only a few surfaces but often
failing on natural images [16]. The latest physics-based
method relies on the longest dichromatic line segment as-
suming that the Phong reﬂection model holds and an ambi-
ent light exists [39]. Although our method is based on the
Dichromatic Model, we classify our approach as statistical
since the core of the method is ﬁnding gray pixels based on
some observed image statistics. We refer readers to [26] for
more details about physics-based methods.

are Xiong et al.

The Closest Methods to GI
[40] and
Gray Pixel by Yang et al. [41]. Xiong et al. [40] method
searches for gray surfaces based on a special LIS space, but
it is camera-dependent. Gray Pixel [41] is closest to our
work and is therefore outlined in details in Sec. 3.

8063

3. Grayness Index

We ﬁrst review the previous Gray Pixel [41] (derived
from the Lambertian model) in the context of dichromatic
reﬂection model (DRM).

3.1. Gray Pixel in [41]

Yang et al. [41] claims that gray pixels can be sought
by a set of constraints. However, their formulation often
identiﬁes gray pixels that clearly are color pixels. This
phenomenon has been noticed, but not properly analyzed.
Herein we analyze GP using DRM and point out the poten-
tial failure cases of the original formulation.

Assuming narrow band sensor, Eq. 1 simpliﬁes to:

I (x,y)

i

= γ(x,y)

b

FiLiR(x,y)

b,i +γ(x,y)

s

FiLiR(x,y)
i ∈ {R,G,B}.

s,i

,

(4)

Then, following Yang et al. [41], we apply log(·) and a local
constrast operator C{·} (Laplacian of Gaussian, see Sec. 4
for more details) on the both sides, and obtain

C{log(I (x,y)

i

)} = C{log(FiLiR(x,y)

b,i

)}

+C(log γ(x,y)

b

+ γ(x,y)

s

s,i

R(x,y)
R(x,y)

b,i !) .

If γs = 0 (means no surface reﬂection), we obtain:

(a)

(b)

(c)

(d)(e)

(f)

Figure 2: Finding gray pixels. (a) input image. (b) com-
puted grayness index GI. darker blue indicates higher de-
(c) the N % most gray pixels rendered
gree of grayness.
using the corresponding pixel color (greenish) in (a). (d)
estimated illumination color. (e) ground truth color. (f) cor-
rected image using (d).

where |I| denotes the luminance magnitude (IR +IG +IB).
In this case, the neutral interface reﬂection (NIR) as-
sumption establishes that, for gray pixels, we have that
Rj,R = Rj,G = Rj,B = ¯Rj with j ∈ {s, b} [30].
In
this case, Eq. 7 simpliﬁes to:

C{log(IR)−log(|I|)} = C{log(FRLR)+log(γb ¯Rb +γs ¯Rs)}

−C{log((FRLR +FGLG +FBLB)(γb ¯Rb +γc ¯Rs))}. (8)

(5)

In a small local neighborhood, the casting illumination
and sensor response can be assumed constant [41], such
that C{log(FRLR)} = 0 and C{log((FRLR + FGLG +
FBLB)} = 0, leading to:

C{log(I (x,y)

i

)} = C{log(γ(x,y)

b R(x,y)

b,i

)} .

(6)

Rs,i
If γs 6= 0, due to the interaction between γb and γs
Rb,i
in Eq. 5, those colored pixels can be wrongly identiﬁed as
gray pixels. Central to GP is that, when γs = 0, a non-
uniform intensity casting on a homogeneous gray surface
can induce the same amount of “contrast” in each channel.
Varying intensity of light may result from the geometry be-
tween surface and illumination (shading) and that among
different surfaces (occlusion). In order to resolve this prob-
lem we adopt the Dichromatic Reﬂection Model, exploring
another path to identify gray pixels in a more complex en-
vironment.

3.2. Grayness Index using Dichromatic Reﬂection

Model

For simplicity, in the sequel we will drop the superscripts
(x, y), as all operations are applied in a local neighborhood
centered at (x, y). We ﬁrst calculate the residual of the red
channel and luminance in log space and then apply local
contrast operator C{·} to Eq. 5 as:

C{log(IR)−log(|I|)} = C{log(FRLR)+log(γbRb,R +γsRs,R)}
−C{log(FRLR(γbRb,R +γsRs,R)+FGLG(γbRb,G +γsRs,G)
+FBLB(γbRb,B +γsRs,B))},

(7)

C{log(IR)−log(|I|)} = C(cid:26)log

γb ¯Rb +γc ¯Rs

γb ¯Rb +γc ¯Rs(cid:27) gray

= 0.

(9)

Eq. (9) is a necessary yet not a sufﬁcient condition for
gray pixels. A more restrictive requirement for the detec-
tion of gray pixels is given by extending Eq. 9 to one more
color channel (using all channels in redundant, the spectral
response of R and B rarely overlap in sensors) as:

C{log(IR)−log(|I|)} = C{log(IB)−log(|I|)} = 0. (10)

From Eq. (7), we deﬁne the grayness index w.r.t. I(x, y)

as:

GI(x, y) = k[C{log(IR) − log(|I|)},
C{log(IB) − log(|I|)}]k,

(11)

where k · k refers to the ℓ2 norm. The smaller the GI is,

the more likely the corresponding pixel is gray.

In addition, we impose a restriction on the local contrast
to ensure that a “small” GI value comes from grey pixels
in varying intensity of light, not a ﬂatten color patch (no
spatial cues), written as:

C{Ii} > ǫ, ∀i ∈ {R, G, B},

(12)

where ǫ is a small contrast threshold.

The process of computing GI is in two steps:

8064

1. Compute a preliminary GI map using Eq. 11.

Datasets

2. Discard pixels in GI with no spatial cues using Eq. 12.
To weaken the effect of isolated gray pixels mainly due
to camera noise, GI map is averaged in 7 × 7 window.

For illustration, Fig. 2 shows a ﬂowchart of computing GI
and its predicted illumination.

The proposed GI differs from GP in two important as-
pects. At ﬁrst, it utilizes a novel mechanism to detect gray
pixels based on a more complete image formation model
that leads to different formulation. Secondly, the proposed
GI works without selectively enhancing bright and dark pix-
els according to their luminance. In other words, the pro-
posed GI does not weaken the inﬂuence of dark pixels.

3.3. GI Application in Color Constancy

Color Constancy is a direct application of gray pix-
els. Here we describe two pipelines to compute illumina-
tion color from gray pixels: single illumination and multi-
illumination pipelines.

When a scene contains only one global illumination, the
pipeline is straightforward. As shown in Fig. 2, after rank-
ing all image pixels according to their GI, the global illumi-
nation is computed as the average of top N % pixels.

Given a scene cast by more than one light source, the
desired output is a pixel-wise illumination map. Similar
to [41], the GI map is ﬁrst computed and then followed by a
K-means clustering of the top N % pixels into preset num-
ber of M clusters. Now, the averaging is applied on cluster
basis, giving a illumination vector Lm for the cluster m.
The ﬁnal spatial illumination map is computed using:

Li(x, y) =

M

Xm=1

ωmLi

m, i ∈ {R, G, B}

(13)

where ωm controls the connection between the pixel I(x, y)
to the cluster m, written as:

ωm = e− Dm

2σ2 /

e− Dn
2σ2 ,

M

Xn=1

(14)

where Dm is the Euclidean distance from the pixel to the
centroid of cluster m. Eq. 14 encourages nearby pixels to
share a similar illumination.

4. Evaluation

We evaluated GI in two color constancy settings: (1)
single-illumination estimation, where the illumination of
the whole captured scene is described by a single chroma
vector for the red, green and blue channels; and (2) multi-
illumination estimation, where in each scene there are two
or more effective illuminants. Moreover, we conducted ex-
periments in the cross-dataset setting which is very chal-
lenging for the learning-based methods.

• The Gehler-Shi Dataset [34, 22]: single illumination,

568 high dynamic linear images, 2 cameras 1.

• The NUS 8-Camera Dataset [12]: single illumination,
1, 736 high dynamic linear images, 8 cameras (see Ta-
ble 2 for the camera list).

• MIMO Dataset [5]: multi-illumination, 78 linear im-
ages, 58 laboratory images and 20 harder wild images.

Single-illumination Experiment Settings

• The local contrast operator in Eq. 11 is the Laplacian

of Gaussian ﬁlter of the size 5 pixels.

• The proportion of the best gray pixels used for color

estimation is set to N = 0.1%.

• The contrast threshold is set to ǫ = 1e−4

These parameters were selected based on preliminary grid
search (see Section 4.3) and remained ﬁxed for all experi-
ments with the both datasets.

Multi-illumination Experiment Settings

• The local contrast operator and the contrast threshold

are the same as in the single-illuminant experiment.

• The proportion of chosen pixels is set to N = 10.0%

as more illuminants are involved.

• The tested number of clusters M were 2,4 and 6.

Dataset Bias of Learning-based Methods When trained
with images from a single data that is divided to training
and testing sets, the state-of-the-art learning-based methods
(e.g. [4]) outperform the best learning-free methods by a
clear margin. However, it is important to know how these
values are biased since images in the training and test sets
often share the same camera(s) and same scenes. It can hap-
pen that a learning-based method overﬁts to the camera and
scene features that are not available in the real case. To
investigate the dataset bias, we evaluated several top per-
forming learning-based methods in the cross-dataset set-
ting, where the methods were trained on one dataset (e.g.,
the Gehler-Shi) and tested with another. This allows evalu-
ating the performance of learning-based algorithms for un-
seen cameras and scenes.

Performance Metric As the standard tool in color con-
stancy papers we adopted the angular error arccos( LT ˆL
)
kLkk ˆLk
between the estimated illumination ˆL and ground-truth L as
the performance metric. Obtained results are summarized in
Table 1 and discussed in Sections 4.1 and 4.2.

4.1. Single dataset Setting

Single-dataset setting is the most common setting in re-
lated works, allowing extensive pre-training using k-fold

1cameras: Canon 1D, Canon 5D

8065

Table 1: Quantitative Evaluation of CC methods. All values correspond to angular error in degrees. We report the results of
the related work in the following order: 1) the cited paper, 2) Table [1] and Table [2] from Barron et al. [4, 3] considered to
be up-to-date and comprehensive, 3) the color constancy benchmarking website [23]. We left dash on unreported results. In
(a) results of learning-based methods worse than ours are marked in gray. The training time and testing time are reported in
seconds, averagely per image, if reported in the original paper.

(a) single-dataset setting

Mean Median Trimean Best 25% Worst 25% Mean Median Trimean Best 25% Worst 25%

Gehler-Shi

NUS 8-camera

Learning-based Methods (camera-known setting)

Edge-based Gamut [25]
Pixel-based Gamut [25]
Bayesian [22]
Natural Image Statistics [24]
Spatio-spectral (GenPrior) [9]
Corrected-Moment1(19 Edge) [14]
Corrected-Moment1(19 Color) [14]
Exemplar-based [29]∗
Chakrabarti et al. 2015 [8]
Cheng et al. 2015 [13]
DS-Net (HypNet+SelNet) [35]
CCC (dist+ext) [3]
FC4 (AlexNet) [28]
FFCC [4]

GI

6.52
4.20
4.82
4.19
3.59
3.12
2.96
2.89
2.56
2.42
1.90
1.95
1.77
1.78

3.07

5.04
2.33
3.46
3.13
2.96
2.38
2.15
2.27
1.67
1.65
1.12
1.22
1.11
0.96

1.87

5.43
2.91
3.88
3.45
3.10
2.59
2.37
2.42
1.89
1.75
1.33
1.38
1.29
1.14

2.16

1.90
0.50
1.26
1.00
0.95
0.90
0.64
0.82
0.52
0.38
0.31
0.35
0.34
0.29

0.43

13.58
10.72
10.49
9.22
7.61
6.46
6.69
5.97
6.07
5.87
4.84
4.76
4.29
4.62

7.62

4.40
5.27
3.50
3.45
3.06
3.03
3.05
–
–
2.18
2.24
2.38
2.12
1.99

2.91

3.30
4.26
2.36
2.88
2.58
2.11
1.90
–
–
1.48
1.46
1.48
1.53
1.31

1.97

3.45
4.45
2.57
2.95
2.74
2.25
2.13
–
–
1.64
1.68
1.69
1.67
1.43

2.13

0.99
1.28
0.78
0.83
0.87
0.68
0.65
–
–
0.46
0.48
0.45
0.48
0.35

0.56

9.83
11.16
8.02
7.18
6.17
7.08
7.41
–
–
5.03
6.08
5.85
4.78
4.75

6.67

1 For Correct-Moment [14] we report reproduced and more detailed results by [3], which slightly differs with the original results: mean: 3.5, median: 2.6 for 19 colors and mean:
2.8, median: 2.0 for 19 edges on Gehler-Shi Dataset.
∗ We mark Exemplar-based method with asterisk as it is trained and tested on a uncorrected-blacklevel dataset.

(b) cross-dataset setting

Training set
Testing set

NUS 8-Camera

Gehler-Shi

Gehler-Shi

NUS 8-Camera

Average

runtime (s)

Mean Median Trimean Best 25% Worst 25% Mean Median Trimean Best 25% Worst 25% Train

Test

Learning-based Methods (agnostic-camera setting), Our rerun

Bayesian [22]
Chakrabarti et al. 2015 [8] Empirical
Chakrabarti et al. 2015 [8] End2End
Cheng et al. 2015 [10]
FFCC [4]

4.75
3.49
3.52
5.52
3.91

3.11
2.87
2.71
4.52
3.15

3.50
2.95
2.80
4.79
3.34

IIC [36]
Woo et al. 2018 [39]

13.62
4.30

13.56
2.86

13.45
3.31

Physics-based Methods

Double-Opponency [20]
ASM 2017 [1]

White Patch [6]
Grey World [7]
General GW [2]
2st-order grey-Edge [38]
1st-order grey-Edge [38]
Shades-of-grey [17]
Grey Pixel (edge) [41]
LSRS [19]
Cheng et al. 2014 [12]
GI

Biological Methods

4.00
3.80

Learning-free Methods

7.55
6.36
4.66
5.13
5.33
4.93
4.60
3.31
3.52
3.07

2.60
2.40

5.68
6.28
3.48
4.44
4.52
4.01
3.10
2.80
2.14
1.87

–
2.70

6.35
6.28
3.81
4.62
4.73
4.23
–
2.87
2.47
2.16

1.04
0.94
0.86
1.96
1.22

9.46
0.71

–
–

1.45
2.33
1.00
2.11
1.86
1.14
–
1.14
0.50
0.43

11.28
7.24
7.72
12.10
7.94

17.98
10.14

–
–

16.12
10.58
10.09
9.26
10.43
10.20
–
6.39
8.74
7.62

3.65
3.87
3.89
4.86
3.19

–
–

–
–

9.91
4.59
3.20
3.36
3.35
3.67
3.15
3.45
2.93
2.91

3.08
3.25
3.10
4.40
2.33

–
–

–
–

7.44
3.46
2.56
2.70
2.58
2.94
2.20
2.51
2.33
1.97

3.16
3.37
3.26
4.43
2.52

–
–

–
–

8.78
3.81
2.68
2.80
2.76
3.03
–
2.70
2.42
2.13

1.03
1.34
1.17
1.72
0.84

–
–

–
–

1.44
1.16
0.85
0.89
0.79
0.99
–
0.98
0.78
0.56

7.33
7.50
7.95
8.87
7.01

–
–

–
–

21.27
9.85
6.68
7.14
7.18
7.75
–
7.32
6.13
6.67

764
–
–
245
98

97
0.30
0.30
0.25
0.029

–
–

–
–

–
–
–
–
–
–
–
–
–
–

–
–

–
–

0.16
0.15
0.91
1.30
1.10
0.47
0.88
2.60
0.24
0.40

cross-validation for learning-based methods. The results
for this setting are summarized in Table 1a. Among all

the compared methods, up to the date of submission of
this paper, FFCC [4] achieves the best overall performance

8066

Table 2: Each-camera evaluation on the NUS 8-Camera Dataset. Std in the last column refers to the standard deviation of
statistics (e.g. mean angular error) on 8 cameras.

NUS 8-camera Dataset

Canon

1DS Mark3

Canon
600D

Fujiﬁlm Nikon Olympus
X-M1

D5200

E-PL6

Samsung
Panasonic
DMC-GX1 NX2000

Sony

SLT-A57

Std

2.76
1.87
2.05
0.55
6.31

3.31
2.50
2.72
0.85
7.11

2.84
1.97
2.15
0.56
6.70

2.96
2.02
2.31
0.67
6.66

3.30
2.49
2.69
0.84
7.12

2.86
1.97
2.17
0.56
6.75

2.91
2.03
2.22
0.66
6.48

3.30
2.48
2.68
0.83
7.16

2.86
1.97
2.13
0.55
6.81

2.93
2.33
2.42
0.78
6.13

3.32
2.56
2.75
0.86
7.12

2.75
1.89
2.07
0.53
6.51

0.1152
0.1465
0.1309
0.0798
0.3558

0.1056
0.1171
0.1365
0.0390
0.1312

0.0753
0.0420
0.0321
0.0114
0.2198

Cheng et al. 2014 [12]
Mean
Median
Tri
Best-25%
Worst-25%

2.93
2.01
2.22
0.59
6.82

2.81
1.89
2.12
0.55
6.50

3.15
2.15
2.41
0.65
7.30

2.90
2.08
2.19
0.56
6.73

Chakrabarti et al. [8] (best), trained on Gehler-Shi, tested here

Mean
Median
Tri
Best-25%
Worst-25%

GI

Mean
Median
Tri
Best-25%
Worst-25%

3.00
2.17
2.31
0.74
6.77

3.02
1.87
2.16
0.54
7.29

3.26
2.48
2.64
0.83
7.04

2.85
1.96
2.12
0.55
6.79

3.12
2.45
2.60
0.83
6.89

2.89
1.98
2.15
0.55
6.86

Table 3: Quantitative Evaluation on the MIMO dataset.

Method

Doing Nothing
Gijsenij et al. [27]
CRF [5]
GP (best) [41]

GI (M=2)
GI (M=4)
GI (M=6)

Laboratory(58)

Real-world(20)
Median Mean Median Mean

10.5
4.2
2.6
2.20

2.09
2.09
2.07

10.6
4.8
2.6
2.88

2.66
2.65
2.60

8.8
3.8
3.3
3.51

3.32
3.47
3.49

8.9
4.2
4.1
5.68

3.79
3.96
3.94

3.26
2.48
2.64
0.83
7.04

2.85
1.96
2.12
0.55
6.79

7
7

.

0

1
8

.

0

8
7

.

2

0
5

.

2

with the both datasets.
It is important to remark that
cross-validation makes no difference to the performance
of statistical methods. Therefore, in order to avoid repe-
tition, the performance of competing non-learning methods
are shown only once in Table 1b. For visualization pur-
poses, results of learning-based methods that are outper-
formed by the proposed GI are highlighted in gray. Remark-
ably, it is clear that, even in the setting which is friendly
to learning-based method, GI outperforms several popular
learning-based methods (from Gamut [25] to the industry-
standard Corrected-Moment [14]) without the need of ex-
tensive training and parameter tuning. Visual examples of
GI are shown in Fig. 3.

Comparing to the best learning-based methods (e.g. [8]),

Figure 3: Qualitative results on the single-illumination
Gehler-Shi. From left to right: angular error, input image,
GI, top 1% pixels chosen as gray pixel, estimated illumina-
tion color, the ground truth color and corrected image using
the predicted illumination. Macbeth Color Checker is al-
ways masked as GI ﬁnds perfect gray patch as gray pixels.

GI has a noticeable heavy tail in its angular error distribu-
tion (e.g. amont the worst 25% cases), which suggests that
GI would be more optimal if gray pixels would be i.i.d over
the whole datasets (e.g. natural images). Learning-based
methods perform well on these “rare” cases using 3-fold
cross-validation, and can further improve “rarity case” per-

8067

mean;Gehler-Shi

median;Gehler-Shi

mean;NUS

median;NUS

1e-2

3.03

3.14

3.54

1e-2

1.84

1.89

2.13

1e-2

3.24

2.93

2.88

1e-2

2.22

1.97

1.91

e
g
a
t
n
e
c
r
e
p

 
,

N

1e-1

3.07

3.07

3.47

1e0

3.21

3.27

3.64

e
g
a
t
n
e
c
r
e
p

 
,

N

1e-1

1.87

1.87

2.10

1e0

2.00

2.01

2.34

e
g
a
t
n
e
c
r
e
p

 
,

N

1e-1

3.23

2.91

2.84

1e0

3.32

2.99

2.89

e
g
a
t
n
e
c
r
e
p

 
,

N

1e-1

2.27

1.97

1.95

1e0

2.39

2.07

1.96

1e-5

1e-4

1e-3

1e-5

1e-4

1e-3

1e-5

1e-4

1e-3

1e-5

1e-4

1e-3

(a)

(b)

(c)

(d)

Figure 5: The colormaps of mean and median angular errors
corresponding to various N and ǫ (see the text) for (a,b)
Gehler-Shi; (c,d) NUS 8-camera.

well-performing learning-free and learning-based methods.
These results are summarized in Table 2, where GI achieves
much more stable results (standard variance is smaller)
across 8 cameras. Due to space limitations, we refer readers
to [12] for more results on individual cameras with other
methods, including but not limited to [2, 38, 25, 22, 9].
Among all methods in Table 2 of [12], GI is less sensitive
to camera hardware.

Figure 4: Qualitative results on (multi-illumination) MIMO
dataset. From left to right, color-biased input, groundtruth
spatial illumination, our spatial estimation using GI, our
corrected image.

formance by including more training data (e.g. via 10-fold
cross-validation) [8].

4.3. Grid Search on Parameters

4.2. Cross Dataset Setting

We were able to re-run the Bayesian method [22],
Chakrabarti et al.[8], FFCC [4], and the method by Cheng et
al. 2015 [13], using the codes provided by the original au-
thors. Note that this list of methods includes FFCC, which
showed the best overall performance in the camera-known
setting. From the provided code we found different ap-
proaches to correct the black level and saturated pixels. For
consistency, we used a uniform correction process (given in
supplement), which was applied to GI as well.

When we trained on one dataset and tested with another,
we made sure that the datasets share no common cameras.
For the results reported in this section, we used the best or ﬁ-
nal setting for each method: Bayes (GT) for Bayesian; Em-
pirical and End-to-End training for Chakrabarti et al.
[8];
30 regression trees for Cheng et al.; full image resolution
and 2 channels for FFCC. Obtained results are summarized
in Table 1b. From this table, it is clear that GI outperforms
all learning-based and statistical methods.

All selected learning-based methods perform worse in
this setting, as compared to some statistical methods (e.g.
LSRS [19], Cheng et al. 2014 [12]). It is not surprising
that the performance of learning-based methods degrades
in this scenario. For example, in [4] it is visualized that
FFCC models two varying camera sensitivity for Gehler-Shi
in preconditioning ﬁlter (two wrap-around line segments),
which in cross-dataset setting will be improperly used to
evaluate performance on the NUS 8-camera Dataset.

A special feature of the NUS 8-Camera Benchmark is
that it includes 8 cameras that share the same scenes. We
leveraged this feature to evaluate the robustness of the

The only two parameters in GI are: the percentage N %
of pixels chosen as gray for illumination estimation, and the
threshold ǫ of Eq. 12 used to remove regions without spatial
cues. The former restricts the domain range where illumi-
nation norm is measured, analogous to the receptive ﬁeld in
deep learning, while the later one passes only noticeable ac-
tivation, like the ReLU activation. Figure 5 summarizes the
obtained median and mean angular errors corresponding to
a grid search of the parameters, with N ∈ {10−2, 10−1, 1
and ǫ ∈ {10−5, 10−4, 10−3}, on Gehler-Shi Dataset and
NUS 8-camera Dataset. The setting (N = 1e−1 and
ǫ = 1e−4) results in a good trade-off between mean and
median error on both datasets. The shown parameter grid
seems loose, but on the contrary, this shows that our method
is robust to parameter tuning across orders of magnitude.

4.4. Multi illumination Setting

As a side product of grayness index, we evaluate the pro-
posed method on a multi-illumination dataset. Table 3 in-
dicates that despite the fact that GI is not designed to deal
with spatial illumination changes, it still outperforms well-
performing methods [5, 41] with a clear margin. From the
mean value over real-world images, it is obvious that GI
can better handle multi-illumination situations. Increasing
the number of clusters M from 2 to 6 further improved our
results on indoor images, but not for wild ones. Figure 4
shows the spatial estimation predicted using GI. Due to Eu-
clidean distance used by the K-means, GI predictions are
not sharp in some scenes with complex geometry but still
obtain the best overall error rate and plausible visual color
correction.

8068

(a)

(b)

(c)

Figure 6:
(a) Example images from the Gehler-Shi cor-
rected using groundtruth, where two different illuminations
(red arrow A and B) exist. (b) We test CC methods in de-
creasing box sizes (from A to E). (c) Color-biased (a).

Table 4: Testing GI, FFCC on varying-size cropped images
from Gehler-Shi, given illumination split from [11].

(a) Double-illumination Setting

(b) Single-illumination Setting

Gehler-Shi: 66 two-illumination Images

Gehler-Shi: 502 single-illumination Images

Mean Median Trimean Best-25% Worst-25%

Mean Median Trimean Best-25% Worst-25%

A 6.12
B 6.06
C 6.02
D 5.46
E 4.96

A 3.11
B 3.44
C 4.01
D 4.64
E 4.99

4.54
3.88
3.63
3.46
2.94

1.67
1.84
2.47
3.13
3.29

GI

5.24
4.90
5.04
4.13
3.45

0.70
0.92
0.92
0.77
0.53

FFCC [4]
2.25
2.39
2.92
3.53
3.72

0.44
0.42
0.56
0.62
0.60

13.72
14.08
14.55
13.69
12.42

8.00
8.69
10.03
11.38
11.92

A 2.78
B 2.95
C 3.32
D 3.93
E 4.81

A 1.68
B 1.72
C 1.84
D 2.13
E 2.39

GI
2.03
2.12
2.49
3.14
3.94

FFCC [4]

1.16
1.20
1.29
1.43
1.58

1.79
1.86
2.30
2.97
3.79

0.94
1.01
1.11
1.29
1.39

0.41
0.41
0.50
0.70
0.82

0.27
0.27
0.29
0.36
0.38

6.75
7.28
7.96
8.90
10.74

4.22
4.30
4.58
5.45
6.17

5. Problems with the “Ground-truth”

We investigated those cases where GI made erratic pre-
dictions (see the supplement for erratic cases) and have ob-
served that, in some images, there exists gray pixels casted
by two illumination sources. A similar problem was noticed
by Cheng et al. [11], who claimed that in the Gehler-Shi
[34], there are 66 two-illumination images. An example of
this problem is illustrated in Fig. 6. In Fig. 6(a), where pix-
els near arrows A and B share the same surface (white wall)
but have different illuminations, the color of pixel in the
neighborhood of B is close to the Macbeth Color Checker
(MCC). In such case, our GI does a good job in identify-
ing gray pixels by following the designed rules and ﬁnding
gray pixels lying in two illuminants, but this comes at the
cost of a large angular error. As a ﬁrst impression, we sup-
pose this is due to the MCC being more dominated by one
of the illuminants.

We designed a simple experiment to investigate our ob-
servation. For the list of 66 two-illumination images (given
in [11]) in the Gehler-Shi and the remaining 502 single-
illumination images, we test GI and FFCC [4] (full resolu-
tion, 2 channels, pretrained on whole Gehler-Shi) on images

cropped by boxes of decreasing sizes centered at the MCC
(from box A to box E in Fig. 6b. Speciﬁcally, the boxes are
generated by halving the width and height of the previous
box.

The results summarized in Tables 4a and 4b show a cru-
cial fact: in the single-illumination subset, GI yields larger
angular errors as the testing box gets smaller (from box A
to E). In contrast, in the double-illumination subset, this
tendency is reversed. It makes sense that the performance
of GI decreases as the testing box shrinks since less refer-
ence points are available. A reasonable explanation to the
abnormal tendency in the two-illumination subset is that the
MCC is placed mainly in one illumination, reﬂecting a bi-
ased “ground-truth”. This problem restricts the upper limit
of the performance of GI and possibly also other statistical
color constancy methods. Learning-based methods (espe-
cially CNN-based method) suffer less from this problem, as
they can learn to reason about some structural information,
e.g. whole-image chroma histogram, the physical geometry
of the scene, the location where MCC is placed. As ex-
pected, FFCC performs worse on smaller boxes. Bearing
these results in mind, we argue that learning-based meth-
ods and statistical methods should be compared by consid-
ering their corresponding advantages and limitations in both
single-dataset and cross-dataset scenarios.

6. Conclusions

We derived a method to compute grayness in a novel
way – Grayness Index. It relies on the Dichromatic Reﬂec-
tion Model and can detect gray pixels accurately. Exper-
iments performed on the tasks of single-illumination esti-
mation and multi-illumination estimation veriﬁed the effec-
tiveness and efﬁciency of GI. On standard benchmarks, GI
estimates illumination more accurately than state-of-the-art
learning-free methods in about 0.4 seconds. GI has a clear
physical interpretation, which we believe can be used for
other vision tasks, e.g. intrinsic image decomposition.

Other conclusions also emerged from the research:
learning-based methods generally perform worse in the
cross-dataset setting; When testing on an image with color
checker masked by zeros, learning-based methods can still
exploit the location of the color checker and overﬁt to scene
and camera speciﬁc features.

Acknowledgments

This work is supported by Business Finland under Grant
J. Matas was supported by the OP
No. 1848/31/2015.
VVV funded project CZ.02.1.01/0.0/0.0/16 019/000076
Research Center for Informatics.

References

[1] A. Akbarinia and C. A. Parraga. Colour constancy beyond

the classical receptive ﬁeld. TPAMI, 2017. 5

8069

[2] K. Barnard, V. Cardei, and B. Funt. A comparison of compu-
tational color constancy algorithms. i: Methodology and ex-
periments with synthesized data. TIP, 11(9):972–984, 2002.
1, 2, 5, 7

[3] J. T. Barron. Convolutional color constancy. In ICCV, 2015.

2, 5

[4] J. T. Barron and Y.-T. Tsai. Fast fourier color constancy. In

CVPR, 2017. 2, 4, 5, 7, 8

[5] S. Beigpour, C. Riess, J. Van De Weijer, and E. An-
gelopoulou. Multi-illuminant estimation with conditional
random ﬁelds.
IEEE Transactions on Image Processing,
23(1):83–96, 2014. 4, 6, 7

[6] D. H. Brainard and B. A. Wandell. Analysis of the retinex
theory of color vision. JOSA A, 3(10):1651–1661, 1986. 1,
2, 5

[7] G. Buchsbaum. A spatial processor model for object colour
perception. Journal of the Franklin Institute, 310(1):1–26,
1980. 2, 5

[8] A. Chakrabarti. Color constancy by learning to predict chro-

maticity from luminance. In NIPS, 2015. 2, 5, 6, 7

[9] A. Chakrabarti, K. Hirakawa, and T. Zickler. Color con-
stancy with spatio-spectral statistics. TPAMI, 34(8):1509–
1519, 2012. 1, 2, 5, 7

[10] X. Chen and C. Zitnick. Minds eye: A recurrent visual rep-
In CVPR, 2015.

resentation for image caption generation.
5

[11] D. Cheng, A. Kamel, B. Price, S. Cohen, and M. S. Brown.
Two illuminant estimation and user correction preference. In
CVPR, 2016. 8

[12] D. Cheng, D. K. Prasad, and M. S. Brown. Illuminant estima-
tion for color constancy: why spatial-domain methods work
and the role of the color distribution. JOSA A, 31(5):1049–
1058, May 2014. 1, 2, 4, 5, 6, 7

[13] D. Cheng, B. Price, S. Cohen, and M. S. Brown. Effective
learning-based illuminant estimation using simple features.
In CVPR, 2015. 5, 7

[14] G. D. Finlayson. Corrected-moment illuminant estimation.

In ICCV, pages 1904–1911, 2013. 2, 5, 6

[15] G. D. Finlayson and G. Schaefer. Convex and non-convex
illuminant constraints for dichromatic colour constancy. In
CVPR, volume 1, pages I–I. IEEE, 2001. 2

[16] G. D. Finlayson and G. Schaefer. Solving for colour con-
stancy using a constrained dichromatic reﬂection model.
IJCV, 42(3):127–144, 2001. 2

[17] G. D. Finlayson and E. Trezzi. Shades of gray and colour
constancy. In Color Imaging Conference (CIC), 2004. 1, 2,
5

[18] D. H. Foster. Color constancy. Vision research, 51(7):674–

700, 2011. 1

[19] S. Gao, W. Han, K. Yang, C. Li, and Y. Li. Fefﬁcient color
constancy with local surface reﬂectance statistics. In ECCV,
2014. 1, 2, 5, 7

[20] S.-B. Gao, K.-F. Yang, C.-Y. Li, and Y.-J. Li. Color con-
stancy using double-opponency. TPAMI, 37(10):1973–1985,
2015. 5

[21] S.-B. Gao, M. Zhang, C.-Y. Li, and Y.-J. Li. Improving color
constancy by discounting the variation of camera spectral
sensitivity. JOSA A, 34(8):1448–1462, 2017. 1

[22] P. V. Gehler, C. Rother, A. Blake, T. Minka, and T. Sharp.
Bayesian color constancy revisited. In CVPR, 2008. 1, 2, 4,
5, 7

[23] A. Gijsenij. Color constancy research website: http://
In http://colorconstancy.com,

colorconstancy.com.
2019. 5

[24] A. Gijsenij and T. Gevers. Color constancy using natural
image statistics and scene semantics. TPAMI, 33(4):687–
698, 2011. 1, 2, 5

[25] A. Gijsenij, T. Gevers, and J. Van De Weijer. Generalized
gamut mapping using image derivative structures for color
constancy. IJCV, 86(2-3):127–139, 2010. 1, 2, 5, 6, 7

[26] A. Gijsenij, T. Gevers, and J. Van De Weijer. Computational
color constancy: Survey and experiments. TIP, 20(9):2475–
2489, 2011. 2

[27] A. Gijsenij, T. Gevers, and J. Van De Weijer.

Improving
color constancy by photometric edge weighting. TPAMI,
34(5):918–929, 2012. 6

[28] Y. Hu, B. Wang, and S. Lin. Fully convolutional color con-
stancy with conﬁdence-weighted pooling. In CVPR, 2017.
2, 5

[29] H. R. V. Joze and M. S. Drew. Exemplar-based color con-
stancy and multiple illumination. TPAMI, 36(5):860–873,
2014. 1, 2, 5

[30] H.-C. Lee, E. J. Breneman, and C. P. Schulte. Modeling light
reﬂection for computer color vision. TPAMI, 12(4):402–409,
1990. 3

[31] Y. Qian, K. Chen, J. K¨am¨ar¨ainen, J. Nikkanen, and J. Matas.
Deep structured-output regression learning for computa-
tional color constancy. In ICPR, 2016. 2

[32] Y. Qian, K. Chen, J. K¨am¨ar¨ainen, J. Nikkanen, and J. Matas.

Recurrent color constancy. In ICCV, 2017. 2

[33] S. A. Shafer. Using color to separate reﬂection components.

Color Research & Application, 10(4):210–218, 1985. 2

[34] L. Shi and B. Funt. Re-processed version of the gehler color
constancy dataset of 568 images. accessed from http://
www.cs.sfu.ca/˜colour/data/, 2010. 4, 8

[35] W. Shi, C. C. Loy, and X. Tang. Deep specialized network

for illumination estimation. In ECCV, 2016. 2, 5

[36] R. T. Tan, K. Ikeuchi, and K. Nishino. Color constancy
through inverse-intensity chromaticity space.
In Digitally
Archiving Cultural Objects, pages 323–351. Springer, 2008.
5

[37] S. Tominaga. Multichannel vision system for estimating sur-
face and illumination functions. JOSA A, 13(11):2163–2173,
1996. 2

[38] J. Van De Weijer, T. Gevers, and A. Gijsenij. Edge-based

color constancy. TIP, 16(9):2207–2214, 2007. 1, 2, 5, 7

[39] S.-M. Woo, S.-h. Lee, J.-S. Yoo, and J.-O. Kim. Improving
color constancy in an ambient light environment using the
phong reﬂection model. TIP, 27(4):1862–1877, 2018. 2, 5

[40] W. Xiong, B. Funt, L. Shi, S.-S. Kim, B.-H. Kang, S.-D.
Lee, and C.-Y. Kim. Automatic white balancing via gray sur-
face identiﬁcation. In Color and Imaging Conference (CIC),
2007. 2

[41] K.-F. Yang, S.-B. Gao, and Y.-J. Li. Efﬁcient illuminant es-
In CVPR,

timation for color constancy using grey pixels.
2015. 1, 2, 3, 4, 5, 6, 7

8070

