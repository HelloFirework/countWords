Deep Asymmetric Metric Learning via Rich Relationship Mining

Xinyi Xu1, Yanhua Yang1, Cheng Deng1 ∗, Feng Zheng2

1School of Electronic Engineering, Xidian University, Xian 710071, China

2Department of Computer Science and Engineering, Southern University of Science and Technology

xyxu.xd@gmail.com, yanhyang@xidian.edu.cn, chdeng.xd@gmail.com, zhengf@sustc.edu.cn

Abstract

Learning effective distance metric between data has
gained increasing popularity, for its promising performance
on various tasks, such as face veriﬁcation, zero-shot learn-
ing, and image retrieval. A major line of researches em-
ploys hard data mining, which makes efforts on searching
a subset of signiﬁcant data. However, hard data mining
based approaches only rely on a small percentage of data,
which is apt to overﬁtting. This motivates us to propose
a novel framework, named deep asymmetric metric learn-
ing via rich relationship mining (DAMLRRM), to mine rich
relationship under satisfying sampling size. DAMLRRM
constructs two asymmetric data streams that are differently
structured and of unequal length. The asymmetric struc-
ture enables the two data streams to interlace each other,
which allows for the informative comparison between new
data pairs over iterations. To improve the generalization
ability, we further relax the constraint on the intra-class
relationship. Rather than greedily connecting all possible
positive pairs, DAMLRRM builds a minimum-cost spanning
tree within each category to ensure the formation of a con-
nected region. As such there exists at least one direct or in-
direct path between arbitrary positive pairs to bridge intra-
class relevance. Extensive experimental results on three
benchmark datasets including CUB-200-2011, Cars196,
and Stanford Online Products show that DAMLRRM effec-
tively boosts the performance of existing deep metric learn-
ing approaches.

1. Introduction

Metric learning aims at ﬁnding appropriate similarity
measurements of data, whose major thinking is to keep the
distance between similar instances close and dissimilar in-
stances far away in an embedding space. This topic is of
great practical importance due to its wide applications, in-
cluding face recognition [12, 52, 45], clustering [9, 44, 53],

∗Corresponding author.

and retrieval [57, 49, 51, 50, 22, 10]. Conventional Ma-
halanobis metric learning approaches learn a linear trans-
formation of the data and measure the similarity based on
Euclidean distance, which fail to capture the high-order
correlation[15, 42, 47]. Riding on the development of deep
neural network [21, 33, 37], deep metric learning (DML)
has gained a lot of attention. Guided by a metric loss, DML
projects data into an embedding space with rich semantic
information through convolutional neural network. It shows
potential capability even in challenging tasks, such as ﬁne-
grained classiﬁcation [8, 41, 55, 25], large-category classi-
ﬁcation [2, 31, 46], and zero-shot learning [28, 56, 6, 26].

According to the types of loss, DML can be roughly
divided into contrastive and triplet approaches. However,
enumerating all possible pairs or triplets will arise nearly
exponential sampling size, which is impractical even for a
moderate number of instances. One common solution is to
sample a subset of instances as a training pool. The fact
is that, when the sampled training pool merely covers easy
instances that contribute little to the optimization, only a
weak embedding model can be obtained. Therefore, hard
data mining aiming to ﬁnd out confusing instances becomes
an important topic, and a large number of methods are pro-
posed [35, 34, 16, 11, 43, 54, 14]. Those methods tackle
this topic to a certain extent yet are still deﬁcient in the fol-
lowing three aspects. First, a complicated data preprocess-
ing is involved to select hard data, whereas the hard level is
changing with the evolution of the model [34]. Second, only
a small subset of relationship is exploited. Third, the hard
level is difﬁcult to control. When the selected instances are
not hard enough, the learned model is not discriminative.
Conversely, when the instances are selected too hard, the
overﬁtting problem often occurs [43].

In this work, we propose a novel framework, named deep
asymmetric metric learning via rich relationship mining
(DAMLRRM). DAMLRRM ﬁrstly builds two asymmetric
data streams, which interlace to each other so that contin-
ues new pairs are compared during iterations. Compared
with conventional one stream metric learning approaches,
DAMLRRM can mine considerably richer relationship un-

14076

der lower sampling size. Furthermore, DAMLRRM relaxes
the constraint on positive pairs to extend the generaliza-
tion capability. Speciﬁcally, we build positive pairs training
pool by constructing a minimum connected tree for each
category instead of considering all positive pairs within a
mini-batch. As a result, there will exist a direct or indirect
path between any positive pair, which ensures the relevance
being bridged to each other. The inspiration comes from
ranking on manifold [58] that spreads the relevance to their
nearby neighbors one by one. The connected graph loss
can help maintain the inherent distribution of the data and
achieve a good generalization ability. In experiments, we
empirically show the state-of-the-art results on CUB200-
2011 [39], Cars196 [1], and Stanford online products [35]
datasets for clustering and retrieval tasks. In a nutshell, this
paper makes the following contributions:

i) We departure from the traditional hard data min-
ing based technique and propose a novel asymmetric two
streams based deep learning framework for metric learning,
which also differs from conventional methods only involv-
ing one stream.

ii) We devise a relaxation technique for positive pairs
to improve the model generalization ability,

constraint
which is veriﬁed in our empirical study.

iii) Our proposed model achieves better accuracy when
using fewer than ten percents of sampling size compared
with the peer methods including the lifted method [35] and
N-pair [34].

2. Related Work

Siamese network [5] is the seminal work of the con-
trastive DML. It ﬁrstly employs twins networks to nonlin-
early map two signature instances into feature space. And
subsequently, a contrastive loss is employed to optimize
the mapping procedure. The contrastive loss minimizes the
distance between positive pairs and enlarges the distance
between negative pairs if they are closer than a predeter-
mined margin. Based on the siamese network, a collection
of approaches are proposed to settle dimensionality reduc-
tion and face veriﬁcation tasks [13, 7, 36, 38].

Although making great progress, contrastive metric
learning approaches suffer from one drawback, that focus
on absolute distance whereas relative distance matters more
for most tasks [30, 31, 35]. Triplet loss, an evolution formu-
lation of contrastive loss, has been proposed to tackle this
issue. It trains a model on a triplets training pool, where
each triplet consists of an anchor, a positive and a nega-
tive instance. The anchor and the positive instances share
the same label, while the anchor and the negative instances
have different labels. The training process encourages the
network to ﬁnd an embedding where the distance between
positive pairs is smaller than the distance between negative
pairs with some margins.

Nevertheless, contrastive and triplet losses tend to be dif-
ﬁcult to optimize in practice, mainly inﬂuenced by the way
of selecting the training pool. Confusing instances, doing
a crucial contribution to optimization, should be paid huge
attention to. FaceNet [31] targets on the online hard data
generation, which uses large mini-batches in the order of a
few thousand instances and only computes the argmin and
argmax within a mini-batch. However, the batch size is
1800, which is a big memory obstacle when implementa-
tion. To take full advantage of relative relationship, Song
[35] allow mining the negatives from both the left
et al.
and right data pair instead of negative being deﬁned only
according to anchor points. Chen et al. [16] introduce a
position-dependent deep metric unit, which can be used to
select hard instances to guide the deep embedding learn-
ing in an online and robust manner. Sohn et al. [34] indi-
cate that a minority of negative instance based loss function
suffers from poor local optima. As a result, they propose
an (N + 1)-tuplet loss that optimizes to identify a posi-
tive instance from N − 1 negative instances, which gains
some performance improvement. More recently, Duan et
al. [11] propose a deep adversarial metric learning frame-
work to generate synthetic hard negatives from the observed
negative instances.

The fundamental philosophy behind hard data mining is
that for a pair of positive instances, select a signiﬁcant neg-
ative sample through ofﬂine or online and penalize on the
relative distance if they violate the constraint. However,
both ofﬂine-based and online-based hard data mining strate-
gies exist defects. Ofﬂine-based methods select the hard
instances before training which will not be updated with
the updating models. It is unreasonable as a hard relation-
ship is dynamically decided by different models. Online-
based methods decide the hard negative sample within a
mini-batch along with training, which makes the compar-
ison within a very small subset of instances. The hard qual-
ity is not guaranteed. One common drawback of these two
forms is that the learned metric is insufﬁcient because of
the low utilization of pairs or triplets. Therefore, we make
efforts on exploiting more pairs while controlling the sam-
pling size in this paper.

Graph is a mathematical structure used to model pair-
wise relations between objects [4, 3]. A graph in the con-
text is made up of vertices and points which are connected
by edges. Graph knowledge is used to express the cor-
relation network in many applications, such as image re-
trieval [32], Linguistics processing[17] and saliency detec-
tion [48]. More recently, Iscen et al. [18] utilize an undi-
rected graph to mine an efﬁcient training pool without label,
which veriﬁes the priority of graph in building correlation.
In this paper, we take advantage of the graph to relax the
constraint between positive pairs, which is quite helpful to
boost the generalization ability.

24077

Figure 1. The asymmetric deep metric learning framework of our proposed method. Two different structured batches are employed for two
stream network training, where the upper one is neatly arranged and the lower one is shufﬂed. Mapped by two shared networks, the feature
embedding of instances are obtained and the learning process is supervised by two loss functions.

3. Proposed Approach

Figure 1 illustrates the framework of our proposed
method DAMLRRM. Two weight-shared networks are em-
ployed to map two asymmetric data batches, where the
upper stream accepts neatly arranged data (neat stream)
and the lower stream takes shufﬂed data as input (shufﬂed
stream). Our model builds a minimum-cost spanning tree
for each class in the neat stream which establishes a stable
intra-class manifold. Furthermore, the strong discrimina-
tion capability is achieved by adopting a shufﬂed stream to
provide various negative instances for the neat stream. We
detail our proposed model in the following subsections.

3.1. Preliminaries

Let X =

{xi|i = 1, 2, · · · , N x} and S
=
{si|i = 1, 2, · · · , N s} be the training pools of
two
streams, where N x and N s are the numbers of instances
in X and S respectively. The target of DML is to learn
a nonlinear transformation to semantic embedding space
ed → Rd, which is a differentiable deep network with
f : R
parameter θ. We measure the similarity of (xi, xj) in term
of the Euclidean distance in the embedding space, which is
computed as Dij = kf (xi) − f (xj)k2. Furthermore, we
construct each category as an undirected weighted subgraph
G = (V, E, D), where each node in V corresponds to a
sample, the edges in E connect positive pairs, and D stores
the edge weights.

3.2. Rich Relationship Mining with Asymmetric

Structure

To obtain rich relationship, we propose an asymmetric
framework for metric learning. Asymmetry is reﬂected in

structure and quantity, respectively. In structure, two total
different structured data batches are built for two streams re-
spectively. The data batch of the upper stream is neat while
the other one is randomly shufﬂed. It can be clearly shown
on the left side of Figure 1 and the formulations are

1; x2

1, · · · , xk

Bx = {x1
Bs = {s1, s2, · · · , sb/2}
B = {Bx, Bs}.

2, · · · , xk

2; · · · ; x1

m, · · · , xk

m}

(1)

The neat data batch Bx is composed of m categories where
there are k instances for each category. And the shuf-
ﬂed data batch Bs contains b/2 randomly instances, where
b/2 = m ∗ k. Hence for each iteration, the training batch
B consists of two parts: one neat data batch Bx and one
shufﬂed data batch Bs.

In quantity, the training pools’ sizes of the two streams
are unequal, namely N x 6= N s. Quantity asymmetric
makes it possible that the same instances in one data stream
compare with different instances in another data stream at
l and Bs
different iteration times. For example, Bs
n
in Figure 2 include the same instances at different itera-
tion times, while they compares with different instances in
stream 1. Speciﬁcally, B1 and Bl are composed by

1, Bs

B1 = {Bx
Bx
1 6= Bx

1 ; Bs
l , Bs

1}, Bl = {Bx
1 = Bs
l .

1 ; Bs
l }

(2)

By doing so, our model can exploit abundant relationship
while not increase the sampling size.

The intuitive motivations behind the asymmetric metric
learning come from two aspects: 1) The neat stream mainly
focus on establishing the consistent intra-class relationship
by a minimum-cost spanning tree, which constrains positive

34078

SharedCNN + one FCRandomly Shuffled SamplesGraph LossShuffled Loss𝒌𝒌samples / class𝑥𝑥11𝑥𝑥12𝑥𝑥1𝑘𝑘𝑥𝑥21𝑥𝑥22𝑥𝑥2𝑘𝑘𝑥𝑥𝑚𝑚1𝑥𝑥𝑚𝑚2𝑥𝑥𝑚𝑚𝑘𝑘𝒎𝒎classes / batchFigure 2. The interlaced batches. The lengths of the two data streams are various, which make the same instances contained by stream2
interacting with different instances in stream 1. Under the same sampling size, such interlaced batches can mine more pair correlation.

pairs to form a uniﬁed manifold. However, such an intra-
class relationship is not stable enough because it observes
limited negative instances; 2) the shufﬂed stream gener-
ates diverse and numerous negative instances for the neat
stream, which aims to establish a discriminative inter-class
relationship. It is worth note that, the two batches do not
cause extra memory or computation cost, because we just
split half of the batch size which previous approaches em-
ploy and the two networks share all weights.

and Vunvisted = V. Initialize a tree with a single arbi-
trary vertex Vstart ∈ V. Add Vstart into Vvisted and
remove it from Vunvisted.

(2) Grow the tree by one edge: choosing a minimum-
weight edge Eminimum ∈ E which connect Vvisted
and Vunvisted, then attach it to the tree. Add the
minimum-weight-connected vertex into Vvisted and re-
move it from Vunvisted.

3.3. Connected Graph based Loss Functions

(3) Repeat step 2 until all vertices are covered in the tree

Previous approaches constrain on all possible positive
pairs in a mini-batch, which is too strict and causes an
overﬁtting problem. Inspired by the method of ranking on
data manifold [58], that the global consistency is obtained
by spreading the relevance of source point to its nearest
neighbors one by one, we relax the constraint of positive
pairs. Rather than connecting all positive pairs, we build
a minimum-cost spanning tree for each category. By do-
ing so, a connected ﬁeld within one class is obtained, which
ensure a direct or indirect path exists between arbitrary pos-
itive pairs and not too much pressure is employed on the
pairs which are not visually similar. In other words, the in-
stances that far distributed in the original visual space are
allowed indirectly associated and their distance being larger
than the threshold. The central idea is to retain the intrin-
sic distribution of data to the utmost extent while ensuring
semantic consistency.

We employ a simple minimum-spanning tree algorithm
named prim [27] to build the connected graph. Prim algo-
rithm is a greedy algorithm which ﬁnds a minimum span-
ning tree for a weighted undirected graph. It ﬁnds a sub-
set of the edges to form a tree which includes every vertex,
where the total weight of all edges in the tree is minimized.
The procedure of prim algorithm is summarized as follows:

(1) Build a weighted graph G = (V, E, D), where D is
measured by Euclidean distance. Set Vvisted = {∅}

(Vvisted = V).

Figure 3 gives a concrete example. Suppose the starting
vertex being the point 1 (Figure 3(a)), then the next vertex
will reach point 2 (Figure 3(b)) by choosing the minimum
weight connected to point 1. Then ﬁnd out the minimum
weight of all edges connected to both point 1 and 2 and
hence reached point 4. Repeat this progress until all vertices
are included in the tree like Figure 3(c). For the situation in
Figure 3,

P P = {(x1, x2); (x2, x3); (x2, x4); (x4, x5); (x4, x6)},

(3)
where P P is the connected positive pairs pool. Notably,
this minimum-cost spanning tree is quite different from
simply choosing the nearest positive pairs which does not
ensure a connected ﬁeld within a category.

The objective function is deﬁned based on the built pos-
itive pairs pool. Predeﬁne a boundary α and a margin β,
the optimize goal is limiting the distance of positive pairs
smaller than α − β. For the negative instances, we hope
they will not break into the tree, so the distance is forced to
be bigger than α + β. The graph loss function is deﬁned as


 X

i,j∈P P

[Di,j −α+β]2

[−Di,j +α+β]2

+

+ +X

i,j∈N P


 ,

(4)

Lg =

1
Pg

44079

Stream 1Stream 2𝐵𝐵1𝑥𝑥𝐵𝐵𝑙𝑙𝑥𝑥𝐵𝐵𝑛𝑛𝑥𝑥The training pool of stream 1The training pool of stream 2𝐵𝐵1𝑠𝑠,𝐵𝐵𝑙𝑙𝑠𝑠and 𝐵𝐵𝑛𝑛𝑠𝑠contain the same instancesIterationIteration 1Iteration 𝑙𝑙Iteration𝑛𝑛𝐵𝐵1𝑠𝑠𝐵𝐵𝑙𝑙𝑠𝑠𝐵𝐵𝑛𝑛𝑠𝑠1

1

2

5

4

3

3.5

1.5

6

4

4

3

2

1.5

5

1

5

5

4

3

3.5

1

2

1.5

6

4

4

3

2

1.5

5

1

5

3

1

2

6

1.5

4

5

1.5

3.5

3

(a) An Undirected Weighted Graph

(b) An Undirected Weighted Graph

(c) Minimum-Cost Spanning Tree

Figure 3. The building procedure of minimum-cost spanning tree. The relevance is bridged from one to another at the smallest weight cost
until all instances within this category are connected directly or indirectly.

where Pg is the number of pairs that violate the constraint
and NP is the negative pairs pool.

For the instances in the shufﬂed stream, the positive in-
stances are expected to join into the prim tree, we force it
to connect to the nearest point in the tree. And the negative
instances are constrained to be far away. Hence the shufﬂed
loss Ls is deﬁned as

Ls=

1
Ps


 X

i,j∈NN

[Di,j −α+β]2

[−Di,j +α+β]2

+

+ +X

i,j∈N P


 ,

(5)
where Ps is the number of violated pairs and NN is the top
1 nearest positive pairs pool between two streams.

Combining the two loss functions, the ﬁnal objective

function can be formulated as:

L = Lg + Ls,

(6)

where we do not employ any balance parameter when com-
bining the two loss functions, as the motivation behind them
are the same. The principle that we obey to design the ob-
jective function is respecting the data distribution most as
long as the semantic consistency satisﬁed. The goal, accom-
plished by connecting connect the nearest positive pairs of
the two stream and build prim tree within a neat stream, is
to relax the constraint and achieve generalization ability.

4. Experiments

In this section, we evaluate the effectiveness of our pro-
posed DAMLRRM on three public benchmark datasets for
both image retrieval and clustering tasks. The Caffe pack-
age [20] is used through the experiments. All images are
resized to 256-by-256 at ﬁrst. For data augmentation, the
training instances are performed standard random crop and
horizontal mirroring, while a single center crop for testing.
The embedding size is set to d = 512 for all embedding
vectors [40, 11]. GoogLeNet [37] pretrained on ImageNet
ILSVRC dataset [29] is used for initialization and a ran-
domly initialized fully connected layer is added. The base

learning rate is set to 10e − 4 and 10 times faster for the
newly added fully connected layer. We use SGD with 40k
training iterations and 60 mini-batch size for each stream.

4.1. Benchmark Datasets

We conduct our experiments on CUB-200-2011 [39],
Cars196 [1] and Stanford Online Products [35] . For all
datasets, we follow the conventional protocol of splitting
training and testing [35]:

CUB-200-2011 [39] dataset covers 200 species of birds
with 11, 788 instances, where the ﬁrst 100 species (5, 864
images) are used for training and the rest of 100 species
(5, 924 images) are used for testing.

Cars196 [1] dataset is composed by 16, 185 cars images
of 196 classes. We use the ﬁrst 98 classes (8, 054 images)
for training and the other 98 classes (8, 131 images) for test-
ing.

Stanford Online Products [35] dataset contains 22, 634
classes with 120, 053 product images in total, where the ﬁrst
11, 318 classes (59, 551 images) are used for training and
the remaining 11, 316 classes (60, 502 images) are used for
testing.

When building the tree in the neat stream, we set k = 5
for CUB-200-2011 and Cars196 , and k = 3 for Stanford
Online Products because each product has only about 5.3
images.

4.2. Baselines

To verify the superiority of our proposed method, we
compare with eight baseline deep metric learning algo-
rithms, which are 1) DDML [23]; 2) contrastive embedding
loss (Contrastive) [13]; 3) Triplet embedding loss (Triplet)
[42]; 4) triplet loss with N-pair sampling, (Triplet+N-pair);
5) Lifted [35]; 6) N-pair loss (N-pair) [34]; 7) Angular loss
(Angular) [40]; and 8) adversarial metric loss (AML) [11].
As the central issue of this work is the sufﬁcient relation-
ship mining under a small sampling size, we did not employ
any hard negative mining strategies to complicate the com-

54080

Table 1. Comparison of clustering and retrieval on CUB 200 2011
[39] dataset

Table 4. Comparison of different boundary α on CUB-200-2011
[39] dataset

Clustering(%)
NMI

F1

Recall@a(%)

Varying

Recall@a(%)

R@1 R@2

R@4 R@8

α

R@1 R@2

R@4 R@8

Method

DDML[23]
Contrastive[13]
Triplet[42]
Triplet+N-pair
Lifted[35]
N-pair[34]
Angular[40]
AML[11]
OURS

47.3
47.2
49.8
54.1
56.4
60.2
61.0
61.3
61.7

13.1
12.5
15.0
20.0
22.6
28.2
30.2
29.5
31.2

31.2
27.2
35.9
42.8
43.6
51.9
53.6
52.7
55.1

41.6
36.3
47.7
54.9
56.6
64.3
65.0
65.4
66.5

54.7
49.8
59.1
66.2
68.6
74.9
75.3
75.5
76.8

67.1
62.1
70.0
77.6
79.6
83.2
83.7
84.3
85.3

Table 2. Comparison of clustering and retrieval on the Cars196 [1]
dataset

Method

DDML[23]
contrastive[13]
Triplet[42]
Triplet+N-pair
Lifted[35]
N-pair[34]
Angular[40]
AML[11]
OURS

Clustering(%)
NMI

F1

Recall@a(%)

R@1 R@2

R@4 R@8

41.7
42.3
52.9
54.3
55.1
62.7
62.4
63.1
64.2

10.9
10.5
17.9
19.6
25.1
31.8
31.8
31.9
33.5

32.7
27.6
45.1
46.3
48.3
68.9
71.3
72.5
73.5

43.9
38.3
57.4
59.9
61.1
78.9
80.7
82.1
82.6

56.5
51.0
69.7
71.4
71.8
85.8
87.0
88.5
89.1

68.8
63.9
79.2
81.3
81.1
90.9
91.8
92.9
93.5

Table 3. Comparison of clustering and retrieval on the stanford
online products [35] dataset

Method

DDML[23]
Contrastive[13]
Triplet[42]
Triplet+N-pair
Lifted[35]
N-pair [34]
Angular[40]
AML[11]
OURS

Clustering(%)
NMI

F1

Recall@a(%)

R@1 R@10

R@100

83.4
82.4
86.3
86.4
87.2
87.9
87.8
89.1
88.2

10.7
10.1
20.2
21.0
25.3
27.1
26.5
31.7
30.5

42.1
37.5
53.9
58.1
62.6
66.4
67.9
66.3
69.7

57.8
53.9
72.1
76.0
80.9
82.9
83.2
82.8
85.2

73.7
71.0
85.7
89.1
91.2
92.1
92.2
92.5
93.2

parison. However, our work can be easily combined with
any hard negative mining method.

4.3. Evaluation Metrics

Following the standard protocol used in [35, 34], we cal-
culate the Recall@a metric [19] for retrieval task. Specif-
ically, for each query image, top a nearest images will be
returned based on Euclidean distance, then the recall score
will be 1 if at least one positive image appears in the re-
turned a images and 0 otherwise. For clustering evaluation,
we adopt the k-means algorithm to cluster testing instances
and the quality is reported in terms of the standard F1 and
NMI metrics. Refer to [35] for detailed formulation.

α = 26
α = 28
α = 30
α = 32

52.9
53.1
55.1
54.5

65.4
65.3
66.5
66.0

76.1
76.1
76.8
76.4

85.1
84.7
85.3
85.3

Table 5. Comparison of different margin β on CUB-200-2011 [39]
dataset

Varying

Recall@a(%)

β

R@1 R@2

R@4 R@8

β = 0.1
β = 0.3
β = 0.5
β = 0.7
β = 1.0

51.9
52.7
55.1
53.9
53.2

64.5
64.9
66.5
65.7
66.3

75.8
75.6
76.8
76.2
76.8

84.9
84.4
85.3
85.3
85.4

4.4. Result Analysis

Retrieval and clustering. Table 1, 2 and 3 report the
clustering and retrieval results for CUB-200-2011, Cars196
and Stanford Online Products separately. We color the best
results with red and the second best with blue. The Com-
parison between traditional contrastive or triplet and Lifted
or N-pair shows that hard data mining indeed help to boost
the performance. N-pair can be cooperated with many met-
ric learning approaches and achieve improvement mainly
because of the advance in its batch construction. Among
all baselines, our proposed method DAMLRRM achieves
state-of-the-art performance in most cases. It worth men-
tioning that, DAMLRRM does not need complicated ofﬂine
data preprocessing and release from hard data mining.

Figure 4 and 5 show the visualization results of CUB-
200-2011 and Cars196, which implemented by dimension-
ality reduction algorithm t-SNE [24]. We zoom in four re-
gions to highlight several representative classes and the var-
ious colors of the bounding box are corresponding to dif-
ferent categories. Two of the zoom-in regions are used for
demonstrating the compact feature embedding of intra-class
and the rest two for illustrating the discrimination between
different classes. Despite the large pose and appearance
variation, our method effectively generates a signiﬁcant fea-
ture mapping that preserves semantic similarity. Figure 6
gives some instances of query and top-5 ranking images for
Stanford Online Products. Despite the huge changes in the
viewpoint, conﬁguration, and illumination, our method can
successfully retrieve instances from the same class.

Ablation study: effect of boundary α and margin
β. There are two hyperparameters involved in our method,
which are boundary α and margin β respectively. Table 4
and 5 study the impact of various parameters for the re-
trieval task on CUB-200-2011 dataset. We set β = 0.5

64081

Figure 4. Visualization of feature embedding computed by our method using t-SNE on CUB-200-2011 dataset.

Figure 5. Visualization of feature embedding computed by our method using t-SNE on Cars196 dataset.

Table 6. Comparison of one stream data batch construction and
asymmetric data batch construction of CUB-200-2011 [39] dataset

Table 7. Comparison of full combined positive pairs and prim tree
connected positive pairs on CUB-200-2011 [39] dataset

Method

Lifted[35]
N-pair[34]
OURS1

# Sampling

Recall@a(%)

Size

700K
500K
36K

R@1 R@2

R@4 R@8

46.9
51.0
52.3

59.8
63.3
65.5

71.2
74.3
76.2

81.5
83.2
85.5

Method

Recall@a(%)

R@1 R@2

R@4 R@8 R@16

Full PPs
OURS 2

48.2
51.2

61.3
63.5

72.8
74.6

84.3
84.5

90.1
91.2

when varying the value of α, and set α = 0.5 when dis-
cussing β. It can be seen that the best performance is ob-
tained when α = 30, β = 0.5. Furthermore, DAMLRRM

is not sensitive to the two parameters and we set boundary
to 30 and margin to 0.5 throughout our experiments.

Ablation study: effect of asymmetric batches. In or-
der to verify the effectiveness of asymmetric structure, we

74082

Query

Retrieval

Query

Retrieval

Figure 6. Examples of successful queries on our Stanford Online Products dataset using our embedding (size 512). Images in the ﬁrst
column are query images and the rest are ﬁve nearest neighbors.

remove the graph loss and keep the shufﬂed loss only, which
is denoted as OURS1. We compare it with conventional one
stream data batch construction methods: Lifted and N-pair
algorithms. Table 6 reports the retrieval metrics of CUB-
200-2011 and demonstrates the priority of two asymmet-
ric stream batches construction. Notably, our method only
samples about 36K images which are about ten percentage
of Lifted and N-pair.

Ablation study: effect of graph pairs construction. To
illustrate the difference between two positive training pools
established by minimum-cost spanning tree and fully com-
bination, we remove the shufﬂed loss from DAMLRRM and
keep graph loss. We denote them as OURS2 and Full PPs
respectively. Table 7 reports the retrieval result of CUB-
200-2011. We can observe that minimum-cost tree based
positive pairs training pool is signiﬁcant for improving the
performance, which is mainly because relaxing the con-
straint employed on positive pairs and the generalization
ability is enhanced.

tional complexity of prim tree is: Op = Pk−1

Algorithmic complexity analysis. Compared with
Lifted[35] and N-pair[34], our proposed method builds a
prim tree within each category additionally. The computa-
i=1 i · (k − i),
where k is the number of instances in a tree. The compar-
ison of training time cost is shown in Table 8, we believe
that the additional ofﬂine training time is worthy given the
signiﬁcantly improved accuracy. For testing, all instances
are mapped by one stream model, and the time cost is the
same.

5. Conclusion

In this paper, we propose a novel asymmetric loss for
deep metric learning, which targets at mining the rich rela-
tionship and enhance generalization ability at the same time.

Table 8. Comparison of training time on CUB 200 2011[39]
dataset.

Method
Iterations/Sec
Training Time

Lifted[35] N-pair[34] OURS

2.2
5.1 h

2.2
5.1 h

0.84
13.2 h

To min the rich relationship, we construct two structured
and quantiﬁed asymmetric data streams, which interlace to
each other during iterations. Such an asymmetric structure
enables continuous newly combined pairs to be compared
when optimizing the model, and hence a rich relationship is
mined under a small amount of sampling size. To enhance
its generalization ability, we relax the constraint on posi-
tive pairs. Instead of connecting all possible positive pairs,
we build a minimum-cost spanning tree within one cate-
gory to ensure the form of connected ﬁeld. Minimum-cost
spanning tree based sampling algorithm obeys the inherent
distribution of data, where not all positive instances are as-
sociated directly. Our proposed model releases from hard
data mining and achieves higher accuracy while even at the
cost of fewer than ten percents sampling images compared
with the peer methods including the lifted method [35] and
N-pair [34].

6. Acknowledgment

Our work was also supported by the National Natural
Science Foundation of China under Grant 61572388 and
61703327, Key R&D Program-The Key Industry Innova-
tion Chain of Shaanxi under Grant 2017ZDCXL-GY-05-
04-02, 2017ZDCXL-GY-05-02, and 2018ZDXM-GY-176,
and the National Key R&D Program of China under Grant
2017YFE0104100.

84083

References

[1] 3d object representations for ﬁne-grained categorization. In
International IEEE Workshop on 3D Representation and
Recognition, 2013.

[2] S. Bell and K. Bala. Learning visual similarity for product
design with convolutional neural networks. ACM Transac-
tions on Graphics, 34(4):98, 2015.

[3] B. Bollob´as and O. M. Riordan. Mathematical results on
scale-free random graphs. Handbook of graphs and net-
works: from the genome to the internet, pages 1–34, 2003.

[4] J. A. Bondy, U. S. R. Murty, et al. Graph theory with appli-

cations, volume 290. 1976.

[5] J. Bromley, I. Guyon, Y. LeCun, E. S¨ackinger, and R. Shah.
Signature veriﬁcation using a” siamese” time delay neural
network. In NeurIPS, pages 737–744, 1994.

[6] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-
bedding consistency by metric learning for zero-shot classif-
ﬁcation. In ECCV, pages 730–746. Springer, 2016.

[7] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, volume 1, pages 539–546, 2005.

[8] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained cate-
gorization and dataset bootstrapping using deep metric learn-
ing with humans in the loop. In CVPR, pages 1153–1162,
2016.

[9] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.
Information-theoretic metric learning. In ICML, pages 209–
216, 2007.

[10] C. Deng, E. Yang, T. Liu, W. Liu, J. Li, and D. Tao. Unsu-
pervised semantic-preserving adversarial hashing for image
search. IEEE Trans. Image Process., 2019.

[11] Y. Duan, W. Zheng, X. Lin, J. Lu, and J. Zhou. Deep adver-

sarial metric learning. In CVPR, pages 2780–2789, 2018.

[12] M. Guillaumin, J. Verbeek, and C. Schmid.

Is that you?
metric learning approaches for face identiﬁcation. In ICCV,
pages 498–505, 2009.

[13] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In null, pages 1735–
1742, 2006.

[14] B. Harwood, B. Kumar, G. Carneiro, I. Reid, T. Drummond,
et al. Smart mining for deep metric learning. In ICCV, pages
2821–2829, 2017.

[15] S. C. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma. Learning dis-
tance metrics with contextual constraints for image retrieval.
In CVPR, volume 2, pages 2072–2078, 2006.

[16] C. Huang, C. C. Loy, and X. Tang. Local similarity-aware
In NeurIPS, pages 1262–1270,

deep feature embedding.
2016.

[17] N. Ide and K. Suderman. Graf: A graph-based format for
linguistic annotations. In proceedings of the Linguistic An-
notation Workshop, pages 1–8, 2007.

[18] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Mining on
manifolds: Metric learning without labels. arXiv preprint
arXiv:1803.11095, 2018.

[19] H. Jegou, M. Douze, and C. Schmid. Product quantization

for nearest neighbor search. TPAMI, 33(1):117–128, 2011.

[20] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding.
In ACM Interna-
tional Conference on Multimedia, pages 675–678, 2014.

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[22] C. Li, C. Deng, N. Li, W. Liu, X. Gao, and D. Tao. Self-
supervised adversarial hashing networks for cross-modal re-
trieval. In CVPR, pages 4242–4251, 2018.

[23] J. Lu, J. Hu, and Y.-P. Tan. Discriminative deep metric learn-
ing for face and kinship veriﬁcation. TIP, 26(9):4269–4282,
2017.

[24] L. V. D. Maaten. Accelerating t-SNE using tree-based algo-

rithms. JMLR.org, 2014.

[25] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and
S. Singh. No fuss distance metric learning using proxies. In
ICCV, pages 360–368, 2017.

[26] H. Oh Song, S. Jegelka, V. Rathod, and K. Murphy. Deep
metric learning via facility location. In CVPR, pages 5382–
5390, 2017.

[27] R. C. Prim. Shortest connection networks and some gener-
alizations. Bell system technical journal, 36(6):1389–1401,
1957.

[28] O. Rippel, M. Paluri, P. Dollar, and L. Bourdev. Metric
learning with adaptive density discrimination. arXiv preprint
arXiv:1511.05939, 2015.

[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein.
Imagenet large scale visual recognition challenge.
IJCV,
115(3):211–252, 2015.

[30] R. Salakhutdinov and G. Hinton. Learning a nonlinear em-
bedding by preserving class neighbourhood structure. In Ar-
tiﬁcial Intelligence and Statistics, pages 412–419, 2007.

[31] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
pages 815–823, 2015.

[32] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, and H. T. Shen.
Unsupervised deep hashing with similarity-adaptive and dis-
crete optimization. TPAMI, 2018.

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[34] K. Sohn.

Improved deep metric learning with multi-class

n-pair loss objective. In NeurIPS, pages 1857–1865. 2016.

[35] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016.

[36] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
In

face representation by joint identiﬁcation-veriﬁcation.
NeurIPS, pages 1988–1996, 2014.

[37] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. pages 1–9, 2014.

[38] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, pages 1701–1708, 2014.

94084

[39] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical re-
port, 2011.

[40] J. Wang, F. Zhou, S. Wen, X. Liu, and Y. Lin. Deep metric
learning with angular loss. In ICCV, pages 2612–2620, 2017.
[41] Y. Wang, J. Choi, V. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In CVPR, pages 1163–1172, 2016.

[42] K. Q. Weinberger and L. K. Saul. Distance metric learn-
ing for large margin nearest neighbor classiﬁcation. JMLR,
10(Feb):207–244, 2009.

[43] C.-Y. Wu, R. Manmatha, A. J. Smola, and P. Kr¨ahenb¨uhl.
In ICCV,

Sampling matters in deep embedding learning.
2017.

[44] E. P. Xing, M. I. Jordan, S. J. Russell, and A. Y. Ng. Dis-
tance metric learning with application to clustering with
side-information. In NeurIPS, pages 521–528, 2003.

[45] J. Xu, L. Luo, C. Deng, and H. Huang. Bilevel distance
In NeurIPS,

metric learning for robust image recognition.
page 42024211, 2018.

[46] J. Xu, L. Luo, C. Deng, and H. Huang. Multi-level metric
learning via smoothed wasserstein distance. In IJCAI, page
29192925, 2018.

[47] J. Xu, L. Luo, C. Deng, and H. Huang. New robust met-
ric learning model using maximum correntropy criterion. In
SIGKDD, page 25552564. ACM, 2018.

[48] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang.
In

Saliency detection via graph-based manifold ranking.
CVPR, pages 3166–3173, 2013.

[49] E. Yang, C. Deng, C. Li, W. Liu, J. Li, and D. Tao. Shared
predictive cross-modal deep quantization. IEEE Trans. Neu-
ral Netw. Learn. Syst., 29(11):5292–5303, 2018.

[50] E. Yang, C. Deng, W. Liu, X. Liu, D. Tao, and X. Gao. Pair-
wise relationship guided deep hashing for cross-modal re-
trieval. In AAAI, pages 1618–1625, 2017.

[51] E. Yang, T. Liu, C. Deng, and D. Tao. Adversarial examples

for hamming space search. IEEE Trans. Cybern., 2018.

[52] M. Yang, C. Deng, and F. Nie. Adaptive-weighting dis-
criminative regression for multi-view classiﬁcation. Pattern
Recogn., 88(4):236–245, 2019.

[53] X. Yang, C. Deng, X. Liu, and F. Nie. New l2, 1-norm relax-

ation of multi-way graph cut for clustering. In AAAI, 2018.

[54] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cas-

caded embedding. In CVPR, pages 814–823, 2017.

[55] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding label
structures for ﬁne-grained feature representation. In CVPR,
pages 1114–1123, 2016.

[56] Z. Zhang and V. Saligrama. Zero-shot learning via joint
In CVPR, pages 6034–6042,

latent similarity embedding.
2016.

[57] D. Zhou, O. Bousquet, T. N. Lal,

J. Weston, and
B. Sch¨olkopf. Learning with local and global consistency.
In NeurIPS, pages 321–328, 2004.

[58] D. Zhou,

J. Weston, A. Gretton, O. Bousquet, and
B. Sch¨olkopf. Ranking on data manifolds. In NeurIPS, pages
169–176, 2004.

104085

