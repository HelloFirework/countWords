Moving Object Detection under Discontinuous Change in Illumination Using

Tensor Low-Rank and Invariant Sparse Decomposition

Moein Shakeri, Hong Zhang

Department of Computing Science, University of Alberta

Edmonton, Alberta, Canada

{shakeri,hzhang}@ualberta.ca

Abstract

Although low-rank and sparse decomposition based
methods have been successfully applied to the problem of
moving object detection using structured sparsity-inducing
norms, they are still vulnerable to signiﬁcant illumination
changes that arise in certain applications. We are inter-
ested in moving object detection in applications involv-
ing time-lapse image sequences for which current methods
mistakenly group moving objects and illumination changes
into foreground. Our method relies on the multilinear (ten-
sor) data low-rank and sparse decomposition framework
to address the weaknesses of existing methods. The key to
our proposed method is to create ﬁrst a set of prior maps
that can characterize the changes in the image sequence
due to illumination. We show that they can be detected by
a k-support norm. To deal with concurrent, two types of
changes, we employ two regularization terms, one for de-
tecting moving objects and the other for accounting for il-
lumination changes, in the tensor low-rank and sparse de-
composition formulation. Through comprehensive experi-
ments using challenging datasets, we show that our method
demonstrates a remarkable ability to detect moving objects
under discontinuous change in illumination, and outper-
forms the state-of-the-art solutions to this challenging prob-
lem.

1. Introduction

Moving object detection in an image sequence captured
under uncontrolled illumination conditions is a common
problem in computer vision applications such as visual
surveillance [26], trafﬁc monitoring [5], and social signal
processing [28]. Although moving object detection and
background subtraction is a well established area of re-
search and many solutions have been proposed, still most
of the existing solutions are vulnerable to complex illumi-
nation changes that frequently occur in practical situations,

Figure 1: First row of each sequence: images captured in a
industrial or wildlife monitoring system. Second row: re-
sults of our proposed method to detect foreground objects.

especially when the changes are discontinuous in time. In
such cases, current methods are often not able to distin-
guish between illumination changes (including those due
to shadow), and changes caused by moving objects in the
scene. In general, outdoor illumination conditions are un-
controlled, making moving object detection a difﬁcult and
challenging problem. This is a common problem for many
surveillance systems in industrial or wildlife monitoring ar-
eas in which a motion triggered camera or a time-lapse pho-
tography system is employed for detecting objects of inter-
est over time. Fig. 1 shows four image sequences under dis-
continuous changes in illumination, which illustrate these
applications. Due to signiﬁcant and complex changes in il-
lumination and independent changes of the moving objects
between images of the sequences, detection of the moving
objects is extremely challenging. The second row of each
sequence in Fig. 1 shows the sample results of our proposed
method with detected moving objects.

Among the leading methods for the problem addressed
in this paper is a group based on low-rank and sparse de-
composition. This group of methods exploit the fact that
the background in an image sequence can be described as
a low-rank matrix whose columns are image pixels that are
correlated [20, 3]. However, image sequences with mov-
ing objects under discontinuous change in illumination and
object location using the timer-lapse photography are qual-

7221

 itatively different from regular frame-rate video sequences.
While some existing solutions are able to handle the dis-
continuity in object location with limited success, there is a
need to improve their ability to distinguish between moving
objects and changes due to illumination.

Taking the idea of using the low-rank components of a
matrix to capture the image background, the most recent de-
velopment relies on tensors, which are higher dimensional
data structures than 2D-matrices. Since the real world data
are ubiquitously multi-dimensional, tensors are often more
appropriate than 2D-matrices to capture higher order rela-
tions in data. It is not surprising that tensor low-rank meth-
ods have been successfully developed with promising re-
sults on real-time video sequences. However, such methods
have yet to be studied for detecting moving objects under
discontinuous changes in illumination and object position,
such as those found in time-lapse image sequences.

In this paper, we propose a solution to the problem of
moving object detection within the tensor low-rank frame-
work that speciﬁcally addresses the problem of discontin-
uous changes in illumination and object location. We for-
mulate the problem in a uniﬁed framework named tensor
low-rank and invariant sparse decomposition (TLISD). To
separate illumination changes from moving objects, ﬁrst we
compute multiple prior maps as illumination invariant rep-
resentations of each image to build our tensor data structure.
These prior maps provide us with information about the ef-
fect of illumination in different parts of an image. We show
that by deﬁning two speciﬁc penalty terms using these prior
maps, our proposed method is able to decompose an im-
age into background, illumination changes and foreground
objects, with a signiﬁcant boost in performance of moving
object detection.

The main contributions are as follows.

• We propose to use multiple priors to model the effect
of illumination in natural images by exploiting invari-
ance properties of color image chromaticity.

• We make use of the priors in a tensor representation

for the problem of moving object detection.

• We propose a low-rank tensor decomposition using
group sparsity and k-support norm as two regulariza-
tion terms to separate moving objects and illumination
variations that undergo discontinuous changes.

• We introduce an extended illumination change dataset
with over 80k real images captured by motion trigger
cameras in industrial and wildlife monitoring systems.

2. Related Work

One successful approach to moving object detection at-
tempts to decompose a matrix D representing an image se-
quence into a low-rank matrix L and sparse matrix S, so
as to recover the background and the foreground [2]. The

problem is initially solved by the robust principal compo-
nent analysis (RPCA). Since the foreground objects are de-
scribed by the sparse matrix S, we can categorize existing
methods by the types of constraints on S. The ﬁrst group
of these methods use l1-norm to constrain S [3, 34, 29] and
solve the following convex optimization.

min
L,S

kLk∗ + λkSk1 s.t. D = L + S

(1)

where kLk∗ denotes the nuclear norm of matrix L, and
kSk1 is the l1-norm of S.

The second group of methods used the additional prior
knowledge on the spatial continuity of objects to constrain
sparse matrix S and improve the detection accuracy [10, 6].
Using spatial continuity (e.g., l2,1-norm in [10]) to enforce
the block-sparsity of the foreground, results become more
stable than conventional RPCA in the presence of illumi-
nation changes. However, it remains a challenge to han-
dle moving shadows or signiﬁcant changes in illumination.
Furthermore, the position of an object in a time-lapse im-
age sequence is discontinuous from one image to another
so that the continuity assumption is invalid as a way to sep-
arate moving objects and changes in illumination.

The third group of methods also imposed the connec-
tivity constraint on S [32, 30, 35, 21, 31, 17] using other
formulations than the second group. For example, Liu et
al. [17] attempted to use a structured sparsity norm [19]
and a motion saliency map, to improve the accuracy of mov-
ing object segmentation under sudden illumination changes.
However,
this method still cannot handle shadows and
severe illumination changes, especially in time-lapse se-
quences with independent object locations among the im-
ages in the sequence that change similarly to shadow and
illumination. In general, although the low-rank framework
is well-known to be robust against moderate illumination
changes in frame-rate sequences, the existing methods are
still not able to handle discontinuous change in illumination
and shadow, especially in time-lapse sequences.

To effectively separate discontinuous changes due to
moving objects and those due to illumination, Shakeri et
al. [23] proposed a method called LISD. This method re-
lies on an illumination regularization term combined with
the standard low-rank framework to explicitly separate the
sparse outliers into sparse foreground objects and illumina-
tion changes. Although this regularization term can signif-
icantly improve the performance of object detection under
signiﬁcant illumination changes, LISD assumes a) the in-
variant representation [22] of all images in a sequence are
modeled by only one invariant direction and b) all illumina-
tion variations are removed in the invariant representation
of images, which are not strictly valid in practice.

Recently, multi-way or tensor data analysis has attracted
much attention and has been successfully used in many ap-
plications. Formally and without loss of generality, denote

7222

a 3-way tensor by D ∈ Rn1×n2×n3 . Tensor low-rank meth-
ods attempt to decompose D ∈ Rn1×n2×n3 into a low-rank
tensor L and an additional sparse tensor S [8]. This de-
composition is applicable in solving many computer vision
problems, including moving object detection. One of the
most recent methods relevant to our research is proposed by
Lu et al. [18]. A tensor nuclear norm was used to estimate
the rank of tensor data and RPCA was extended from 2D to
3D to formulate the following tensor robust PCA (TRPCA):

min
L,S

kLk∗ + λkSk1 s.t. D = L + S

(2)

They showed that the tensor nuclear norm on tensor data
can capture higher order relations in data. Tensor data
is used for background subtraction and foreground detec-
tion [24, 13, 12, 4, 16] by stacking two dimensional im-
ages into a three dimensional data structure, using which
tensor decomposition can capture moving object due to the
continuity of object positions in the third dimension. Ob-
viously, this approach only works for frame-rate sequences
with continuous foreground motion, but is not applicable to
time-lapse image sequences with discontinuous changes in
both object location and illumination.

In this paper, we introduce a new formulation for mov-
ing object detection under the framework of tensor low-rank
representation and invariant sparse outliers. We ﬁrst build
a set of prior maps for each image in the image sequence
and treat it as a tensor. These prior maps enable us to use
two regularization terms to distinguish between moving ob-
jects and illumination changes. We demonstrate that their
use within our proposed method signiﬁcantly improves the
performance of moving object detection in the case of dis-
continuous changes in illumination, a problem that most of
the existing methods cannot handle effectively.

3. Tensor Low-Rank and Invariant Sparse De-

composition

Our proposed formulation seeks to decompose tensor
data D into a low-rank tensor L, an illumination change
tensor C, and a sparse foreground tensor S as follows.

D = L + S + C

(3)

In (3), both S and C are stochastic in time-lapse image se-
quences due to discontinuous change in object locations and
illumination changes, and separating them is an ill-posed
problem. To solve this issue, we compute a set of prior maps
using multiple representations of an image, which are more
robust against illumination change than RGB images. These
prior maps enable us to ﬁnd higher order relations between
the different invariant representations and the intensity im-
ages, in both space and time. These relations are exploited
as the basis for separating S from C as will be detailed in
Section 3.1. It is worth mentioning that on one hand, illumi-
nation changes are related to the material in a scene, which

is invariant in all frames leading to a correlation between
them. On the other hand, these changes are also related
to the source of lighting, which is not necessarily corre-
lated between frames. Consequently, illumination changes
should be accounted for by both the low-rank part and the
sparse part in an image decomposition. In our method, we
model the highly correlated part of illumination with the
low-rank tensor L as background, and we model the inde-
pendent changes in illumination as the foreground, while
recognizing that uncorrelated illumination changes are not
necessarily sparse. To accomplish such illumination model-
ing, we propose to use a balanced norm or k−support norm.
We introduce our formulation in details in Section 3.2, and
we describe a solution to the formulation in Section 3.3.

3.1. Generation of Prior Maps and Tensor Data D

In this section we focus on obtaining the prior infor-
mation that will enable us to distinguish between moving
objects and illumination changes in our proposed formula-
tion. In the case of discontinuous change in illumination,
which is common in time-lapse image sequences, variation
of shadows and illumination are unstructured phenomena
and they are often mistakenly considered by many meth-
ods as moving objects. We address this problem through
creating illumination-invariant and shadow-free images, a
problem that has been well studied.

One of the most popular methods for this problem is
proposed by Finlayson et al. [7], which computes the two-
vector log-chromaticity χ′ using red, green and blue chan-
nels. [7] showed that with changing illumination, χ′ moves
along a straight line e roughly. Projecting the vector χ′ onto
the vector orthogonal to e, which is called invariant direc-
tion, an invariant representation I = χ′e⊥ can be computed.
This method works well when the assumption deﬁned above
hold true but in practice this assumption never holds exactly,
i.e., χ′ does not move along a straight line. As a result, the
correspond invariant representation is ﬂawed and can lead
to sub-optimal performance.

Fig. 2 shows an example of the variability of the illumi-
nation invariant direction in an image sequence and its im-
pact on generating a illumination-invariant image represen-
tation. Fig. 2(a) shows the invariant directions of an image
sequence of 200 frames while illumination changes (blue
line), one direction for each image, varying mostly between
−4o and 13o. Fig. 2(c) shows a selected image from the se-
quence, which is image 11 and corresponds to the red line in
Fig. 2(a). The invariant direction for this image is found to
be 13◦ while the average invariant direction of the sequence
is around 5◦, when we assume χ′ moves exactly along a
straight line. Fig. 2(d) compares the two invariant repre-
sentations created with invariant directions of 5◦ and 13◦,
respectively, and Fig. 2(e) shows the detected foreground
objects using these two different representations from the

7223

Figure 2: (a) Best invariant direction of each image in a
sequence (y-axis: angle of the invariant directions e⊥ in
degrees), (b) Dominant directions (yellow bars) after clus-
tering, (c) 11th image in the sequence as shown with a red
line in (a), where its best invariant direction is 13◦, (d) The
ﬁrst and the second rows show the invariant representations
of the selected image using the average direction of the se-
quence (5◦) and its best direction (13◦), respectively. (e)
Obtained outliers of the invariant representations.

RPCA method where the use of the optimal invariant direc-
tion (13o) produces much more desirable result than that of
the sub-optimal direction (5o). This example clearly shows
the importance of the choice of the invariant direction in
creating the invariant representations, and the undesirable
outcome when these representations are created with a sub-
optimal invariant direction.

Our idea to account for the difference in the invariant
direction among the images in the sequence, is to ﬁrst es-
timate the image-speciﬁc invariant directions for the se-
quence, and then use a clustering algorithm to identify the
dominant directions (dotted lines in Fig. 2(a) or the domi-
nant yellow bars in Fig. 2(b)). Subsequently, for each im-
age, we create multiple invariant representations, one for
each dominant direction, and these multiple representations
serve as multiple prior maps for the image. In particular,
for each image, we ﬁrst use the method in [7] to deter-
mine its best invariant direction. With n2 images in an im-
age sequence, this results in n2 invariant directions where
n2 = 200 in Fig. 2(a). Second, we use k-means to iden-
tify k = 10 clusters of the n2 invariant directions. Third,
we choose the centroid of a cluster as a dominant invariant
direction if the cluster has support by at least 10% of the
images (yellow bars in Fig. 2(b)). By deﬁnition, there are
no more than 10 dominant directions.

Now, to construct the tensor D ∈ Rn1×n2×n3 formally

(see Fig. 3), let D(:, :, 1) be an observed image sequence
in our problem, where each column of D(:, :, 1) is a vec-
torized image from the sequence with n1 pixels, and n2
is the number of images in the sequence. pth frontal slice
D(:, :, p), p = 2, ..., n3 is a corresponding prior map, gen-
erated with a dominant invariant direction. Based on this
tensor data structure, we are ready to present our new ten-
sor low-rank and invariant sparse decomposition (TLISD)
to extract the invariant sparse outliers as moving objects.

3.2. TLISD Formulation

As mentioned in Section 2, to detect moving objects un-
der discontinuous illumination change in a sequence, cur-
rent low-rank methods are insufﬁcient when changes due to
illumination and moving shadows are easily lumped with
moving objects as the sparse outliers in the low-rank for-
mulation. To separate real changes due to moving ob-
jects from those due to illumination, we use multiple prior
illumination-invariant maps, introduced in Section 3.1, as
constraints on real changes and illumination changes.
In
particular, real changes should appear in all frontal slices.
Furthermore, lateral slices are completely independent from
each other in a time-lapse sequence, but the different repre-
sentations in each lateral slice (see Fig. 3) are from one im-
age and therefore, the locations of real changes should be
exactly the same in each lateral slice. Now, based on these
observations, real changes in each frame should satisfy the
group sparsity constraint, which is modeled with the mini-
mization of the l1,1,2−norm deﬁned as:

n1

n2

X

i=1

X

j=1

kSi,j,:k2

(4)

As discussed, illumination changes in an image sequence
should be accounted for by both the low-rank part and the
sparse part. The highly correlated part of illumination can
be modeled with the low-rank tensor L as background, but
the independent changes in illumination are grouped as the
foreground. To capture these uncorrelated illumination and
shadow changes, and separate them from real changes, we
recognize that they are not necessarily sparse. Fig. 4 shows
two samples extracted illumination changes using our pro-
posed method. Based on Fig. 4, it is easy to understand that
illumination changes are on entire image and so, those un-
correlated changes are not completely sparse. These prop-
erties can be conveniently modeled with the k−support
norm [1], which is a balanced norm and deﬁned as:

kC:,:,pksp

k =(cid:16)

k−r−1

(|c|↓

m)2 +

X

m=1

1

r + 1

(

d

X

m=k−r

1
2

|c|↓

m)2(cid:17)

(5)

Figure 3: Right: sample images with their corresponding
illumination invariant representations as prior maps. Left:
Tensor D. Frontal slices show pth representation of the im-
ages in the sequence. Lateral slices show different repre-
sentation of each image in the sequence.

m denote the pth frontal slice of C
where C:,:,p and |c|↓
and the mth largest element in |c|, respectively.
r ∈
{0; 1; ...; k − 1} is an integer that is computed automati-
cally by Algorithm 2 in the supplementary material. c =

7224

 (d) (e) (c) (a) Image index (b) Density of directions   D jth lateral slice  pth frontal slice  Figure 4: Two sample images and their corresponding illu-
mination changes captured by our proposed method

vec(C:,:,p) represents the vector constructed by concatenat-
ing the columns of C:,:,p and d = n1 × n2 is the dimension
of the frontal slice. The k-support norm has two terms: l2-
norm penalty for the large component, and l1-norm penalty
for the small components. k is a parameter of the cardinality
to achieve a balance between the l2-norm and the l1-norm
(k = n1 in our experiments). The k-support norm pro-
vides an appropriate trade-off between model sparsity and
algorithmic stability [1], and yields more stable solutions
than the l1-norm [14]. In this paper we show that the k-
support norm can estimate the illumination changes in an
image sequence accurately. Joining of this norm and (4) as
two constraints in one optimization framework enables us
to separate real changes from illumination changes.

To summarize, we propose the tensor low-rank and in-
variant sparse decomposition (TLISD) method, as follows.

min
L,S,C

kLk∗ + λ1kSk1,1,2 + λ2(kCksp

k )2

s.t. D = L + S + C

(6)

where kLk∗ is the tensor nuclear norm, i.e.
the aver-
age of the nuclear norm of all the frontal slices (kLk∗ =
n3 Pn3
1
p=1 kL:,:,pk∗), and it approximates the rank of L.
S and C are detected moving objects and illumination
changes, respectively.

3.3. Optimization Algorithm

In order to solve (6), we use the standard inexact aug-
mented Lagrangian method (ALM) with the augmented La-
grangian function H(L, S, C, Y; µ) whose main steps are
described in this section for completeness.

H(L, S, C, Y; µ) = kLk∗ + λ1kSk1,1,2 + λ2(kCksp
kD − L − S − Ck2
F

+ < Y, D − L − S − C > +

k )2

(7)

µ
2

where Y is a Lagrangian multiplier, µ is a positive auto-
adjusted scalar, and < A, B >= trace(AT B). λ1 =
1/pmax(n1, n2)n3 and λ2 is a positive scalar. Now we
solve the problem through alternately updating L, S, and C
in each iteration to minimize H(L, S, C, Y; µ) with other
variables ﬁxed until convergence as follows.

Lt+1 ← min
L

kLk∗ +

µ
2

kLt − (D − S t − Ct +

S t+1←min

S

λ1kSk1,1,2+

kS t−(D−Lt+1−Ct+

Ct+1←min

C

λ2(kCksp

k )2+

kCt−(D−Lt+1−S t+1+

Y t
µ
Y t
µ

)k2

F (8)

)k2

F (9)

Y t
µ

)k2
F

(10)

µ
2
µ
2

Y t+1 = Y t + µ(D − Lt+1 − Ct+1 − S t+1)

(11)

where µ = min(ρµ, µmax). Both (8) and (9) have closed
form solutions in [18] and [33] respectively, and (10) has an
efﬁcient solution in [14]. The error is computed as kD −
Lt − S t − CtkF /kDkF . The loop stops when the error falls
below a threshold (10−5 in our experiments). Details of the
solutions can be found in the supplementary material.

3.4. Time Complexity

In this work, we use ADMM to update L and S, which
have closed form solutions.
In these two steps the main
cost per-iteration lies in the update of Lt+1, which re-
quires computing FFT and n3 SVDs of n1 × n2 matrices.
Thus, time complexity of the ﬁrst two steps per-iteration is
O(n1n2n3logn3+n(1)n2
(2)n3), where n(1) = max(n1, n2)
and n(2) = min(n1, n2) [18]. To update Ct+1, we use
an efﬁcient solution based on binary search where the
time complexity is reduced to O((n1n2 + k)log(n1n2))
for each frontal slice per-iteration [14]. Therefore, the
total time complexity of the optimization problem (6) is
O(n1n2n3logn3 + n(1)n2
(2)n3 + (n1n2 + k)n3log(n1n2)).

4. Experimental Results and Discussion

In this section, we provide an experimental evaluation of
our proposed method, TLISD. We ﬁrst evaluate the effect
of each term in (6) and their λ coefﬁcients. Then, we eval-
uate TLISD on benchmark frame-rate image sequences or
those that are captured via time-lapse or motion-triggered
photography. We also introduce a new dataset captured by
industrial security cameras and wildlife monitoring systems
during three years, and evaluate our method on this dataset.

4.1. Experiment Setup

Existing datasets1: We evaluate our TLISD method on
eleven selected sequences from the CDnet dataset [9],
Wallﬂower dataset [27], I2R dataset [15], and ICD [23],
which include illumination change and moving shadows.
Extended Illumnation Change (EIC) dataset: Due to the
lack of a comprehensive dataset with various illumination
and shadow changes in a real environment, we have cre-
ated a new benchmark dataset called EIC with around 80k
images in 15 sequences, captured via available surveillance
systems in wildlife and industrial applications. Particularly,
ten sequences are captured via wildlife monitoring systems,
and ﬁve sequences from industrial applications, with three
railway sequences and two construction site sequences. Six
sample sequences of this dataset are shown in Fig. 8. All
sequences can be found in the supplementary material.
Evaluation metric: For quantitative evaluation, pixel-level
F-measure = 2 recall×precision
recall+precision is used. We also compare
the different methods in execution time in seconds.

1https://sites.google.com/site/backgroundsubtraction/test-sequences

7225

 4.2. Algorithm Evaluation: The effect of term C

In the ﬁrst set of experiments, we evaluate the effect of
term C in TLISD when we set different values for λ1, in
comparison with TLISD without term C, where (6) becomes

min
L,S

kLk∗ + λ1kSk1,1,2

s.t. D = L + S

(12)

Fig. 5(a) shows (12) can achieve around 70% accuracy
with a well-tuned λ1 = 0.002. Although the result shows
the importance of multiple priors and the effect of group
sparsity on them, the accuracy of (12) is still far below the
accuracy of proposed TLISD by at least 10%, even with a
well-tuned λ1. Fig. 5(a) also shows that adding term C and
k − support norm increases the robustness of our algorithm
against tuning λ1.
In fact, in (12) all illumination varia-
tions would be assigned to either of L or S. In this case,
those variations should be assigned to the background (L);
however, they do not actually belong to background (e.g.
moving shadows). As a result, the rank would be increased
to absorb these changes into L and naturally some parts
of the moving objects S would be also absorbed into the
background. Fig. 5(b) supports the conclusion and shows
the obtained rank through the iterations of the optimization.
Between iterations 15 and 20, the rank of our method with-

1

0.8

0.6

0.4

0.2

e
r
u
s
a
e
m
−
F

 

140

120

100

80

60

40

20

k
n
a
R
 
d
e
t
a
m

i
t
s
E

0

 

0

 

Ours with k−sp norm on C, λ
 = 0.002, λ
=0.03
2
1

Ours without term C, λ
 = 0.002
1

10

20

30

40

50

Iteration
(b)

 

Ours without term C
Ours with k−sp norm on C

 

0

0.0005 0.001

0.002

0.003

0.004

0.005

0.01

λ

1

(a)

140

120

100

80

60

40

20

k
n
a
R
 
d
e
t
a
m

i
t
s
E

 

Ours without term C
Ours with k−sp norm on C

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

e
r
u
s
a
e
m
−
F

0.01

0.02

 

0

 

0.0005 0.001 0.002 0.003 0.004 0.005

λ

1

(c)

Ours with k−sp norm on C
Ours with L

 norm on C

1

Ours without term C

60

55

50

45

40

35

30

25

s
n
o

i
t

a
r
e

t
I

20
 
0.0005 0.001 0.002 0.003 0.004 0.005

0.01

0.02

λ

1

(e)

0.12

0.1

0.08

0.06

0.04

0.02

r
o
r
r

E

0

 

0

0.45

 

0.0005 0.001

Ours with L
 norm on C, λ
 = 0.01
2
1
Ours with L
 norm on C, λ
 = 0.03
2
1
Ours with L
 norm on C, λ
 = 0.05
2
1

Ours with k−sp norm on C, λ
 = 0.01
2
Ours with k−sp norm on C, λ
 = 0.03
2
Ours with k−sp norm on C, λ
 = 0.05
2

0.002

0.003

0.004

0.005

0.01

λ

1

(d)

Ours with k−sp norm on C, λ

 = 0.001

1

 

Ours with k−sp norm on C, λ

 = 0.002

1

Ours with k−sp norm on C, λ

 = 0.003

1

Ours with L

 norm on C, λ

 = 0.001

Ours with L

 norm on C, λ

 = 0.002

Ours with L

 norm on C, λ

 = 0.003

Ours with L

 norm on C, λ

 = 0.005

1

1

1

1

1

1

1

1

Ours without term C, λ

 = 0.001

1

Ours without term C, λ

 = 0.002

1

Ours without term C, λ

 = 0.003

1

10

20

30

40

50

Iteration

(f)

Figure 5: Self evaluation of TLISD. (a) Average F-measure
with different values for λ1 on all ICD sequences between
TLISD and (12), (b) Estimated rank of TLISD and (12)
through iterations on sequence “Wildlife3”, (c) Estimated
rank of sequence “Wildlife3” with different values for λ1,
(d) Average F-measure with different values for λ1 and λ2
on all ICD sequences between TLISD and (13), (e) Average
number of iterations to converge TLISD, (12) and (13) on
all ICD sequences, (f) Convergence curves of minimization
error for TLISD, (12) and (13) on sequence “Wildlife3”.

out term C signiﬁcantly increases to absorb all variations
into L, and to complete the conclusion, Fig. 5(f) shows that
around the same iterations, the residual error of the method
without term C is signiﬁcantly reduced. This means, illu-
mination variations and shadow changes must grouped into
either of L or S, for (12) to converge. Estimated rank in
Fig 5(c) shows the proof of this concept. Obviously, with
a very small λ1, the estimated rank of L for (12) is small
and all illumination variations are easily lumped with mov-
ing objects in S. This causes less accuracy and sometimes
even cannot provide meaningful results. In contrast, TLISD
can estimate a balanced rank and classify illumination vari-
ations into term C with k − support-norm on it instead of
increasing the rank to absorb them into L.

To justify the use of k − support norm on C in TLISD,
we also compare the method with the other potential term
on C, which is l1-norm to absorb outliers, i.e., deﬁne (6) as

min
L,S,C

kLk∗+λ1kSk1,1,2+λ2kCk1 s.t. D =L+S +C (13)

For this experiment, we evaluate our method with both
l1 and k − support norms on C under different values of
λ1 and λ2. Fig. 5(d) illustrates the accuracy of our method
with either of regularizers. Although l1-norm can increase
the accuracy and robustness of the moving object detection
in comparison with (12) that we showed in Fig. 5(a), the
obtained accuracy is still less than TLISD. In addition, the
number of iterations to converge, for both (12) and (13) is
much more than that of in TLISD. Fig. 5(e) shows the av-
erage number of iterations for all three possible methods
with different setup for λ1 on all ICD sequences. For both
TLISD and (13), λ2 = 0.03, which produces robust results
over different values of λ1(refer to Fig. 5(d)). As discussed
in Section 3, illumination changes are not necessarily sparse
and can be found throughout an image. Therefore, l1-norm
is not a suitable regularizer to capture illumination changes.
In such cases, the same issue as (12) happens when the op-
timizer increases the rank to minimize the residual error.
Fig. 5(f) shows the error of all three methods through itera-
tions. For (13), the same pattern as (12) is seen to decrease
the error while the rank increases through optimization.

4.3. Evaluation on Benchmark Sequences

In this section we evaluate our method on the eleven
benchmark sequences described in Section 4.1. Fig. 6 shows
the qualitative results of TLISD on “Cubile” and “Back-
door”. The second and the third columns of Figs. 6(a) and
(b) illustrate the ﬁrst frontal slice of C and S, corresponding
to illumination changes and moving objects, respectively.
The high-quality of our detection result S is clearly visible.
Figs. 7(a) and (b) show qualitative results of our method
on two sample sequences of ICD, which has the most chal-
lenging conditions in terms of illumination changes. To ap-
preciate the signiﬁcant variations of illumination we show

7226

two images from each sequence. The second and the third
rows of each sub-ﬁgure show the ﬁrst frontal slice of C and
S, respectively. The results show the proposed method can
accurately separate the changes caused by illumination and
shadows from real changes.

We then compare TLISD quantitatively with two on-
line and eight related RPCA batch methods. From online
methods we select GMM [36] as a baseline method and
GRASTA [11] as an online method that uses the frame-
work of low-rank and sparse decomposition. Also among
batch methods, we select SSGoDec [34], PRMF [29],
PCP [3], Markov BRMF [30], DECOLOR [35], LSD [17],
ILISD [23], and TRPCA [18]. For all the competing meth-
ods we use their original settings through LRS Library [25],
which resulted in the best performance. For quantitative
evaluation of RPCA-related methods, a threshold criterion
is required to get the binary foreground mask. Similarly,
we adopt the same threshold strategy as in [25]. In TLISD,
λ1 = 1/pmax(n1, n2)n3 (similar to TRPCA) and λ2 =
0.03. Table 1 shows the performance of TLISD in com-
parison with the competing methods in terms of F-measure.
For all the sequences TLISD ranked among the top two of
all methods, and achieves the best average F-measure in
comparison with all other methods. Although DECOLOR,
LSD, and ILISD work relatively well, Only ILISD is com-
parable with our method due to the use of illumination reg-
ularization terms in ILISD. This evaluation shows the ef-
fectiveness of multiple prior maps and k − support norm as
two regularization terms for separating moving objects from
illumination changes, and boosting the overall performance
of object detection.

4.4. Evaluation of TLISD on EIC Dataset

In this section, we evaluate TLISD on the introduced
EIC dataset. Six sample sequences of ELC are shown in
Fig. 8. To understand the signiﬁcant variations of illumina-
tion and shadow, we show two images from each sequence
in Figs. 8(a) and (b). Columns (c) and (d) show the ﬁrst
frantal slices of C and S obtained by TLISD for the im-
ages in column (b), in order to capture illumination changes
and to detect moving objects. Table 2 show the capabil-
ity of TLISD in comparison with the four best competitive
methods (based on Table 1) in terms of F-measure, where
TLISD can outperform the other methods by a clear per-
formance margin. Fig. 9 also compares TLISD with IL-
ISD (the second best method in Table. 2) qualitatively. This

Figure 6: Columns from left to right show sample image,
illumination changes, and detected moving objects for (a)
cubicle and (b) backdoor sequences

Figure 7: First row: two sample images from (a) Wildlife1,
(b) Wildlife3 sequences. Second row: illumination changes
obtained from the ﬁrst frontal slice of C. Third row: de-
tected objects from the ﬁrst frontal slice S.

Figure 8: Columns (a) and (b): two sample images of each
sequence, (c) and (d): illumination changes captured in C,
and detected objects of images in (b), respectively

qualitative comparison shows that one prior map only is not
sufﬁcient for removing the effect of illumination variations
and shadow. As discussed in Section 3.1, due to the varia-
tion in the invariant direction for images in a sequence, in
some conditions separating illumination changes and shad-
ows from real changes is roughly impossible and selecting
multiple prior maps is essential. More results on all se-
quences can be found in the supplementary material.

4.5. Execution Time of TLISD

Based on Tables 1 and 2, since ILISD is the only method
with comparable results to ours, we examine our proposed
method and ILISD in terms of computation time. Table. 3
compares the execution time of both methods on seven se-
quences. Regarding the computation time of the proposed
method, our tensor-based method needs more time than [23]

7227

 (a) (b)  (b) (a)               (a)                               (b)                                (c)                                (d) Sequence
GMM [36]
GRASTA [11]
SSGoDec [34]
PRMF [29]
DECOLOR [35]
PCP [3]
BRMF [30]
LSD [17]
ILISD [23]
TRPCA [18]
TLISD

Backdoor CopyMachine Cubicle PeopleInShade LightSwitch Lobby Wildlife1 Wildlife2 Wildlife3 WinterStreet MovingSunlight

0.6512
0.6822
0.6611
0.7251
0.7656
0.7594
0.6291
0.7603
0.8150
0.7022
0.8276

0.5298
0.6490
0.5401
0.6834
0.7511
0.6798
0.3293
0.8174
0.8179
0.6805
0.8445

0.3410
0.4113
0.3035
0.3397
0.5503
0.4978
0.3746
0.4233
0.6887
0.5329
0.7350

0.3305
0.5288
0.2258
0.5163
0.5559
0.6583
0.3313
0.6168
0.8010
0.5683
0.7961

0.4946
0.5631
0.3804
0.2922
0.5782
0.8375
0.2872
0.6640
0.7128
0.6924
0.7429

0.3441
0.6727
0.0831
0.6256
0.7983
0.6240
0.3161
0.7313
0.7849
0.6176
0.8012

0.2374
0.3147
0.2912
0.2718
0.3401
0.5855
0.2743
0.6471
0.8033
0.4382
0.8862

0.2880
0.3814
0.2430
0.3991
0.3634
0.6542
0.2812
0.3790
0.7277
0.3926
0.8065

0.0635
0.2235
0.0951
0.07012
0.1202
0.3003
0.0735
0.0871
0.7398
0.2854
0.8010

0.1183
0.2276
0.1215
0.2108
0.4490
0.1938
0.0872
0.1604
0.6931
0.2721
0.7092

0.0717
0.1714
0.2824
0.2932
0.3699
0.3445
0.2408
0.3593
0.6475
0.3018
0.7122

Table 1: Comparison of F-measure score between our proposed method and other compared methods on benchmark real-time
sequences (best F-measure: bold, second best F-measure: underline)

5. Conclusions

In this paper, we have proposed a novel method based
on tensor low-rank and invariant sparse decomposition to
detect moving objects under discontinuous changes in il-
lumination, which frequently happen in video surveillance
applications. In our proposed method, ﬁrst we compute a
set of illumination invariant representations for each image
as prior maps, which provide us with cues for extracting
moving objects. Then we model illumination changes in
an image sequence using a k-support norm and derive a
new formulation to effectively capture illumination changes
and separate them from detected foregrounds. Currently,
many surveillance systems, especially security and wildlife
monitoring cameras, use motion triggered sensors and cap-
ture image sequences with signiﬁcant illumination changes.
Our proposed method can solve the problem with a perfor-
mance that is superior to the state-of-the-art solutions. Our
method is also able to extract natural outdoor illumination
as labeled data for learning-based methods, which can be an
effective alternative to optimization based methods such as
ours, but with a sequential formulation, to detect illumina-
tion changes and moving objects from image sequences.

Acknowledgment

This research is supported in part by NSERC through
its Discovery Grant and Strategic Network Grant (NCRN)
programs.

Sequence Backdoor Lobby Cubicle Wildlife1 Wildlife2 Wildlife3 MovingSunlight
ILISD
TLISD

0.49
0.98

0.53
2.38

0.74
1.79

1.24
2.52

1.33
4.26

1.18
4.08

2.2
5.16

Table 3: Comparison of execution time (in sec.) per image

Figure 10: Number of iterations to converge ILISD and
TLISD methods on twelve sequences

7228

Figure 9: Comparison of qualitative results between TLISD
and ILISD on four sequences of EIC dataset. Top to bottom:
Sample Image, Ground Truth, ILISD, and TLISD

Sequence
PCP [3]
DECOLOR [35]
ILISD [23]
TRPCA [18]
TLISD

Wildlife4 Wildlife5 Wildlife6 Railway1 Railway2 Industrial area1

0.4150
0.3475
0.6020
0.2934
0.7508

0.4016
0.2010
0.6104
0.3082
0.8049

0.3092
0.2604
0.6170
0.2855
0.7522

0.3634
0.2853
0.5983
0.3447
0.7241

0.4086
0.3021
0.5414
0.2805
0.7116

0.2869
0.3242
0.5626
0.2914
0.7035

Table 2: Comparison of F-measure score between our pro-
posed method and other compared methods on EIC dataset

for each iteration, which is normal due to use of the tensor
structure. However, the number of iterations in our method
is less than that of [23]. Fig. 10 shows the number of itera-
tions to converge for both ILISD and TLISD methods. IL-
ISD [23] has two independent optimization formulae: one
for providing a prior map and the other for separating mov-
ing objects from illumination changes, and they have in-
dependent numbers of iterations to converge. After con-
vergence, the optimized values are interchangeably used in
an outer loop, and hence the total number of iterations is
much more than that of our method which involves one op-
timization formula. As discussed in Section 3.4, the domi-
nant time in our method is SVD decomposition for frontal
slices, which are independent from each other, and so can
be solved in parallel on a GPU to speed up the computation.
Therefore, the total time of our method is at least compa-
rable with ILISD and can be even faster due to the fewer
number of iterations.

  020406080100120140ILISDTLISDIterations References

[1] Andreas Argyriou, Rina Foygel, and Nathan Srebro. Sparse
prediction with the k-support norm. In Advances in Neural
Information Processing Systems, pages 1457–1465, 2012.

[2] Thierry Bouwmans and El Hadi Zahzah. Robust pca via prin-
cipal component pursuit: A review for a comparative eval-
uation in video surveillance. Computer Vision and Image
Understanding, 122:22–34, 2014.

[3] Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright.
Robust principal component analysis? Journal of the ACM
(JACM), 58(3):11, 2011.

[4] Wenfei Cao, Yao Wang, Jian Sun, Deyu Meng, Can Yang,
Andrzej Cichocki, and Zongben Xu. Total variation regular-
ized tensor rpca for background subtraction from compres-
sive measurements. IEEE Transactions on Image Process-
ing, 25(9):4075–4090, 2016.

[5] Bo-Hao Chen and Shih-Chia Huang. An advanced moving
object detection algorithm for automatic trafﬁc monitoring in
real-world limited bandwidth networks. IEEE Transactions
on Multimedia, 16(3):837–847, 2014.

[6] Xinyi Cui, Junzhou Huang, Shaoting Zhang, and Dimitris N
Metaxas. Background subtraction using low rank and group
sparsity constraints. In European Conference on Computer
Vision (ECCV), pages 612–625. Springer, 2012.

[7] Graham D Finlayson, Mark S Drew, and Cheng Lu. Entropy
minimization for shadow removal. International Journal of
Computer Vision (IJCV), 85(1):35–57, 2009.

[8] Donald Goldfarb and Zhiwei Qin. Robust low-rank tensor
recovery: Models and algorithms. SIAM Journal on Matrix
Analysis and Applications, 35(1):225–253, 2014.

[9] Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Kon-
rad, and Prakash Ishwar. Changedetection. net: A new
change detection benchmark dataset.
In Computer Vision
and Pattern Recognition Workshops (CVPRW), IEEE Com-
puter Society Conference on, pages 1–8. IEEE, 2012.

[10] Charles Guyon, Thierry Bouwmans, and El-Hadi Zahzah.
Foreground detection based on low-rank and block-sparse
matrix decomposition.
In Image Processing (ICIP), 2012
19th IEEE International Conference on, pages 1225–1228.
IEEE, 2012.

[11] Jun He, Laura Balzano, and Arthur Szlam. Incremental gra-
dient on the grassmannian for online foreground and back-
ground separation in subsampled video. In 2012 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1568–1575. IEEE, 2012.

[12] Wenrui Hu, Yehui Yang, Wensheng Zhang, and Yuan Xie.
Moving object detection using tensor-based low-rank and
saliently fused-sparse decomposition. IEEE Transactions on
Image Processing (TIP), 26(2):724–737, 2017.

[13] Sajid Javed, Thierry Bouwmans, and Soon Ki Jung. Sbmi-
ltd: stationary background model initialization based on low-
rank tensor decomposition. In Proceedings of the Symposium
on Applied Computing, pages 195–200. ACM, 2017.

[14] Hanjiang Lai, Yan Pan, Canyi Lu, Yong Tang, and Shuicheng
Yan. Efﬁcient k-support matrix pursuit. In European Confer-
ence on Computer Vision (ECCV), pages 617–631. Springer,
2014.

[15] Liyuan Li, Weimin Huang, Irene Yu-Hua Gu, and Qi Tian.
Statistical modeling of complex backgrounds for foreground
object detection.
IEEE Transactions on Image Processing
(TIP), 13(11):1459–1472, 2004.

[16] Ping Li, Jiashi Feng, Xiaojie Jin, Luming Zhang, Xianghua
Xu, and Shuicheng Yan. Online robust low-rank tensor mod-
eling for streaming data analysis. IEEE transactions on neu-
ral networks and learning systems, (99):1–15, 2018.

[17] Xin Liu, Guoying Zhao, Jiawen Yao, and Chun Qi. Back-
ground subtraction based on low-rank and structured sparse
decomposition.
IEEE Transactions on Image Processing
(TIP), 24(8):2502–2514, 2015.

[18] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen
Lin, and Shuicheng Yan. Tensor robust principal component
analysis: Exact recovery of corrupted low-rank tensors via
convex optimization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
5249–5257, 2016.

[19] Julien Mairal, Rodolphe Jenatton, Francis R Bach, and Guil-
laume R Obozinski. Network ﬂow algorithms for structured
sparsity. In Advances in Neural Information Processing Sys-
tems (NIPS), pages 1558–1566, 2010.

[20] Nuria M Oliver, Barbara Rosario, and Alex P Pentland. A
bayesian computer vision system for modeling human inter-
actions. IEEE transactions on pattern analysis and machine
intelligence (PAMI), 22(8):831–843, 2000.

[21] Moein Shakeri and Hong Zhang. Corola: a sequential solu-
tion to moving object detection using low-rank approxima-
tion. Computer Vision and Image Understanding, 146:27–
39, 2016.

[22] Moein Shakeri and Hong Zhang. Illumination invariant rep-
resentation of natural images for visual place recognition.
In 2016 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 466–472. IEEE, 2016.

[23] Moein Shakeri and Hong Zhang. Moving object detection in
time-lapse or motion trigger image sequences using low-rank
and invariant sparse decomposition.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (ICCV), pages 5123–5131, 2017.

[24] Andrews Sobral, Christopher Baker, Thierry Bouwmans, and
El-hadi Zahzah. Incremental and multi-feature tensor sub-
space learning applied for background modeling and sub-
traction.
In International Conference Image Analysis and
Recognition, pages 94–103. Springer, 2014.

[25] Andrews Sobral, Thierry Bouwmans, and El-hadi Zahzah.
Lrslibrary: Low-rank and sparse tools for background mod-
eling and subtraction in videos.
In Robust Low-Rank and
Sparse Matrix Decomposition: Applications in Image and
Video Processing. CRC Press.

[26] YingLi Tian, Andrew Senior, and Max Lu. Robust and ef-
ﬁcient foreground analysis in complex surveillance videos.
Machine vision and applications, 23(5):967–983, 2012.

[27] Kentaro Toyama, John Krumm, Barry Brumitt, and Brian
Meyers. Wallﬂower: Principles and practice of background
maintenance. In Computer Vision, 1999. The Proceedings of
the Seventh IEEE International Conference on (ICCV), vol-
ume 1, pages 255–261. IEEE, 1999.

7229

[28] Alessandro Vinciarelli, Maja Pantic, and Herv´e Bourlard.
Social signal processing: Survey of an emerging domain. Im-
age and vision computing, 27(12):1743–1759, 2009.

[29] Naiyan Wang, Tiansheng Yao, Jingdong Wang, and Dit Ye-
ung. A probabilistic approach to robust matrix factorization.
In European Conference on Computer Vision (ECCV), pages
126–139. Springer, 2012.

[30] Naiyan Wang and Dit Yeung. Bayesian robust matrix fac-
torization for image and video processing.
In Interna-
tional Conference on Computer Vision (ICCV), pages 1785–
1792, 2013.

[31] Bo Xin, Yuan Tian, Yizhou Wang, and Wen Gao. Back-
ground subtraction via generalized fused lasso foreground
modeling. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 4676–
4684, 2015.

[32] Jia Xu, Vamsi K Ithapu, Lopamudra Mukherjee, James M
Rehg, and Vikas Singh. Gosus: Grassmannian online sub-
space updates with structured-sparsity. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV),
pages 3376–3383, 2013.

[33] Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, and
Misha Kilmer. Novel methods for multilinear data comple-
tion and de-noising based on tensor-svd.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3842–3849, 2014.

[34] Tianyi Zhou and Dacheng Tao. Godec: Randomized low-
In In-
rank & sparse matrix decomposition in noisy case.
ternational conference on machine learning (ICML). Omni-
press, 2011.

[35] Xiaowei Zhou, Can Yang, and Weichuan Yu. Moving ob-
ject detection by detecting contiguous outliers in the low-
rank representation. IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 35(3):597–610, 2013.

[36] Zoran Zivkovic. Improved adaptive gaussian mixture model
In null, pages 28–31. IEEE,

for background subtraction.
2004.

7230

