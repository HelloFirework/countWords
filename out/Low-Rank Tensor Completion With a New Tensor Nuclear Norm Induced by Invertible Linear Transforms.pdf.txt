Low-rank Tensor Completion with a New Tensor Nuclear Norm Induced by

Invertible Linear Transforms

Canyi Lu1, Xi Peng2, Yunchao Wei3,4

1 Carnegie Mellon University, 2 Sichuan University, 3 University of Illinois at Urbana-Champaign, 4 University of Technology Sydney

canyilu@gmail.com, pengx.gm@gmail.com, wychao1987@gmail.com

Abstract

This work studies the low-rank tensor completion prob-
lem, which aims to exactly recover a low-rank tensor from
partially observed entries. Our model is inspired by the re-
cently proposed tensor-tensor product (t-product) [9] based
on any invertible linear transforms. When the linear trans-
forms satisfy certain conditions, we deduce the new tensor
tubal rank, tensor spectral norm, and tensor nuclear nor-
m. Equipped with the tensor nuclear norm, we then solve
the tensor completion problem by solving a convex program
and provide the theoretical bound for the exact recovery un-
der certain tensor incoherence conditions. The achieved
sampling complexity is order-wise optimal. Our model and
result greatly extend existing results in the low-rank matrix
[5] and tensor completion [16]. Numerical experiments
verify our results and the application on image recovery
demonstrates the superiority of our method.

1. Introduction

With the availability of cheap memory and the advances
in modern computer technology, it is now possible to col-
lect, store and process more data for science and engineer-
ing applications than ever before. The real data are usu-
ally multidimensional in nature: the information is stored
in multiway arrays, known as tensors [26], whose entries
are indexed by several continuous or discrete variables. For
example, a color image is a 3-way object with column, row
and color modes; a greyscale video is indexed by two spatial
variables and one temporal variable. There are many appli-
cations which involve the tensor representation and opera-
tion, including signal processing [6], computer vision [27],
data mining [23], and many others. A common approach in
these applications is to manipulate the tensor data by tak-
ing the advantage of its multi-dimensional structure. Col-
lapsing the multiway data to matrices usually leads to infor-
mation loss and would cause performance degradation. It
is observed that the real tensor data are often of extreme-

ly high-dimension. But the tensor of interest is frequently
low-rank, or approximately so [11], and hence has a much
lower-dimensional structure. This motivates the low-rank
tensor estimation and recovery problem which is gaining
signiﬁcant attention in many different areas, both theoreti-
cally and practically: e.g., estimating latent variable graph-
ical models [1], classifying audio [20], image and video
completion [13, 16], to name a few.

In this paper, we focus on the low-rank tensor comple-
tion problem, which aims to exactly recover a low-rank ten-
sor from an incomplete observation. Such a problem can
be regarded as an extension of the low-rank matrix comple-
tion problem [3, 5], which has been applied to image de-
noising [18] and multi-label image classiﬁcation [2]. It is
shown that under certain incoherence conditions, the rank

r matrix M ∈ Rn×n with O(nr log2 n) observations, can

be recovered with high probability by solving the following
convex model [5]

X kXk∗, s.t. PΩ(X) = PΩ(M ),
min

(1)

where PΩ(X) denotes the projection of X on the observed
set Ω, and kXk∗ is the nuclear norm of X, deﬁned as
the sum of its singular values. The nuclear norm is the
convex envelope of the matrix rank within a certain set.
This leads to the order-wise optimal sampling complexity
O(nr log2 n) for (1), compared with the degrees of freedom
O(nr) for a rank r matrix [5].

It is expected to extend the low-rank matrix completion
model and analysis to the tensor case. There have been
several tensor recovery models proposed based on different
tensor rank deﬁnitions and convex surrogates. But they have
some limitations in real applications. The CP rank [11], de-
ﬁned as the smallest number of rank one tensor decompo-
sition, is NP-hard to compute. Its convex envelope is not
clear. This makes the low CP rank tensor recovery chal-
lenging. To avoid this issue, the tractable Tucker rank [11]
and its convex relaxation are more widely used. The Sum
of Nuclear Norms (SNN) [13] is used as a convex surrogate

15996

and applied for Low-Rank Tensor Completion (LRTC) [21]

min
X

k

Xi=1

λikX {i}k∗, s.t. P Ω(X ) = P Ω(M),

(2)

where X {i} is the mode-i matricization of X [11], P Ω(X )
denotes the projection of X on the observed set Ω, and
λi > 0. The effectiveness of this approach for image pro-
cessing has been well studied in [13, 7, 25, 24]. Howev-
er, SNN is not the tightest convex relaxation of the Tucker
rank [22]. In theory, the above model can be substantial-
ly suboptimal [21] as the required number of measurements
is much higher than the degrees of freedom of tensor with
Tucker rank (r, r,··· , r). This is different from the low-
rank matrix completion model using the nuclear norm min-
imization which owns the optimal recovery rate [5].

More recently, based on the tensor-tensor product (t-
product) and tensor SVD (t-SVD) [10], a new tensor nucle-
ar norm is proposed and applied in tensor completion [16]
and tensor robust PCA [14, 15]. The main advantage of
the t-product induced tensor nuclear norm based models is
that they own the same tight recovery bound as the matrix
cases [16].
In [9], the authors observe that the t-product
is based on a convolution-like operation, which can be im-
plemented using the Discrete Fourier Transform (DFT). In
order to properly motivate this transform based approach,
a more general tensor-tensor product deﬁnition is proposed
based on any invertible linear transforms. The transform
based t-product also owns a matrix-algebra based interpre-
tation in the spirit of [8]. Such a new transform based t-
product is of great interest in practice as it allows to use
different linear transforms for different real data formatted
as tensors. In this work, we focus on the important low-rank
tensor recovery problem based on the transform induced
t-product. Speciﬁcally, we are interested in the following
questions:

• How to deﬁne a new tensor rank and tensor nuclear
norm induced by the linear transform based t-product?

• Is there any restriction on the choice of the invertible

linear transform?

• With a new linear transform based tensor nuclear nor-
m, is there any corresponding recovery guarantee for
low-rank tensor completion?

The main contributions of this work are to solve the
above problems. We show that if the linear transform satis-
ﬁes certain condition, then a new tensor nuclear norm can be
deﬁned induced by the transform based t-product. We can
further deﬁne the same tensor tubal rank as in [10] based
on the t-SVD. Finally, we study the low tubal rank tensor
recovery problem and solve it by the transform based tensor
nuclear norm minimization which is convex. In theory, we

prove that under certain tensor incoherence conditions, the
underlying low tubal rank tensor can be exactly recovered
with high probability by convex optimization. Our model
and result are much more general than [16], since we have
much more general choices of the invertible linear trans-
forms. Our proof of the main result is much more challeng-
ing since the transform based t-product has no equivalent
formulation in the original domain. This is quite different
from the t-product [10] which is deﬁned based on the block
circulant matrix structure in the original domain. Such a
structure is important in the proofs in existing works, e.g.,
[16].

The rest of this paper is structured as follows. Section 2
gives some notations and presents the new tensor nuclear
norm induced by the transform based t-product. Section 3
solves the low-rank tensor completion problem by minimiz-
ing the proposed tensor nuclear norm and provides the exact
recovery guarantee in theory. Numerical experiments con-
ducted on both synthesis and real world data are presented
in Section 4. We ﬁnally conclude this work in Section 5.

2. A New Tensor Nuclear Norm Induced by

Transforms based T-product

In this section, we introduce some notations and deﬁni-

tions used in this paper. Some of these are from [10, 15].

2.1. Notations

We denote scalars by lowercase letters, e.g., a, vector
by boldface lowercase letters, e.g., a, matrices by boldface
capital letters, e.g., A, and tensors by boldface Euler scrip-
t letters, e.g., A. For a 3-way tensor A ∈ Rn1×n2×n3 ,
we denote its (i, j, k)-th entry as Aijk or aijk and use
the Matlab notation A(i, :, :), A(:, i, :) and A(:, :, i) to de-
note respectively the i-th horizontal, lateral and frontal s-
lice [11]. More often, the frontal slice A(:, :, i) is denot-
ed compactly as A(i). The tube is denoted as A(i, j, :).
The inner product between A and B in Rn1×n2 is deﬁned
as hA, Bi = Tr(A⊤B), where A⊤ denotes the trans-
pose of A and Tr(·) denotes the matrix trace. The in-
ner product between A and B in Rn1×n2×n3 is deﬁned as
i=1DA(i), B(i)E. We denote In as the n × n
hA, Bi = Pn3

sized identity matrix.

Some norms of vector, matrix and tensor are used. We

denote the ℓ1-norm as kAk1 = Pijk |aijk|, the inﬁnity
norm as kAk∞ = maxijk |aijk| and the Frobenius norm
as kAkF = qPijk a2
ijk, respectively. The above norm-
s reduce to the vector or matrix norms if A is a vector or
a matrix. For v ∈ Rn, the ℓ2-norm is kvk2 = pPi v2
i .
The spectral norm of a matrix A is denoted as kAk =
maxi σi(A), where σi(A)’s are the singular values of A.
The matrix nuclear norm is kAk∗ = Pi σi(A).

5997

2.2. Transform based T product Induced Tensor

Nuclear Norm

The work [10] gives the ﬁrst deﬁnition of tensor-tensor

product. For A ∈ Rn1×n2×n3 , we deﬁne the block circu-
lant matrix bcirc(A) ∈ Rn1n3×n2n3 of A as

Algorithm 1 Transform L based t-product [9]
Input: A ∈ Rn1×n2×n3 , B ∈ Rn2×l×n3 , and L :
Rn1×n2×n3 → Rn1×n2×n3 .
Output: C = A ∗L B ∈ Rn1×l×n3 .
1. Compute ¯A = L(A) and ¯B = L(B).

bcirc(A) =

A(1)
A(2)
...

A(n3)
A(1)
...

A(n3) A(n3−1)




··· A(2)
··· A(3)
...
. . .
··· A(1)




.

We also need the following two operators

unfold(A) =

A(1)
A(2)
...

A(n3)







, fold(unfold(A)) = A,

where the unfold operator maps A to a matrix of size
n1n3 × n2 and fold is its inverse operator. Let A ∈
Rn1×n2×n3 and B ∈ Rn2×l×n3 . Then the t-product C =
A ∗ B is deﬁned to be a tensor of size n1 × l × n3,
C = A ∗ B = fold(bcirc(A) · unfold(B)).

(3)

The block circulant matrix can be block diagonalized using
Discrete Fourier Transform (DFT) matrix F n3 , i.e.,

where ⊗ denotes the Kronecker product, and ¯A is a block
diagonal matrix with the i-th block ¯A(i) being the i-th
frontal slices of ¯A obtained by performing DFT of A a-
long the 3-rd dimension, i.e., ¯A = fft(A, [ ], 3) by using
the Matlab command fft. We denote

D = A ⊙ B

(5)

as the frontal-slice-wise product (Deﬁnition 2.1 in [9]), i.e.,

D(i) = A(i)B(i), i = 1,··· , n3.

(6)

Then the block diagonalized property in (4) implies that

¯C = ¯A ⊙ ¯B. So the t-product in (3) is equivalent to the

matrix-matrix product in the transform domain using DFT.
In [9], the authors propose a more general deﬁnition of
t-product based on any invertible linear transform L. In this
work, we consider the linear transform L : Rn1×n2×n3 →
Rn1×n2×n3 which gives ¯A in terms of transforms applied
to each tube ﬁber A(i, j, :). Or we have

2. Compute each frontal slice of ¯C by
, i = 1,··· , n3

¯C (i) = ¯A(i) ¯B

(i)

3. Compute C = L−1( ¯C).

where ×3 denotes the mode-3 product (Deﬁnition 2.5
in [9]), and L ∈ Rn3×n3 can be arbitrary invertible matrix1.
Similarly, we have the inverse mapping

L−1(A) = A ×3 L−1.

(8)

Then the transform L based t-product is deﬁned as follows.

Deﬁnition 2.1. (T-product) [9] Let L be any invertible
linear transform in (7), and A ∈ Rn1×n2×n3 and B ∈
Rn2×l×n3 . Then the transform L based t-product, denoted
as C = A∗L B, is deﬁned such that L(C) = L(A)⊙ L(B).
We denote ¯A ∈ Rn1n3×n2n3 as a block diagonal matrix
with its i-th block on the diagonal as the i-th frontal slice
¯A(i) of ¯A = L(A), i.e.,

¯A(1)

¯A(2)




,




. . .

¯A(n3)

where bdiag is an operator which maps tensor ¯A to the
block diagonal matrix ¯A. Then L(C) = L(A) ⊙ L(B) is
equivalent to ¯C = ¯A ¯B. This implies that the transform L
based t-product is equivalent to the matrix-matrix product
in the transform domain. Algorithm 1 gives the way for
computing t-product.

The t-product enjoys many similar properties to the
matrix-matrix product. For example, the t-product is as-
sociative, i.e., A ∗L (B ∗L C) = (A ∗L B) ∗L C.
Deﬁnition 2.2. (Tensor transpose) [9] Let L be any invert-
ible linear transform in (7), and A ∈ Rn1×n2×n3 . Then
the tensor transpose, denoted as A⊤, satisﬁes L(A⊤)(i) =
(L(A)(i))⊤, i = 1,··· , n3.
Deﬁnition 2.3. (Identity tensor) [9] Let L be any invertible
linear transform in (7). Let I ∈ Rn×n×n3 so that each
frontal slice of L(I) = ¯I is a n × n sized identity matrix.
Then I = L−1( ¯I) is called the identity tensor.

(F n3 ⊗ In1 ) · bcirc(A) · (F −1

n3 ⊗ In2 ) = ¯A,

(4)

¯A = bdiag( ¯A) =

¯A = L(A) = A ×3 L,

(7)

1We restrict L to be a real matrix for the sake of discussions. But our

results still hold with simple extensions if necessary for complex L [9].

5998

Figure 1: An illustration of the t-SVD of an n1 × n2 × n3 tensor.

Algorithm 2 T-SVD based on t-product ∗L [9]
Input: A ∈ Rn1×n2×n3 and invertible linear transform L.
Output: T-SVD components U , S and V of A.

1. Compute ¯A = L(A).

2. Compute each frontal slice of ¯U , ¯S and ¯V from ¯A by

for i = 1,··· , n3 do

[ ¯U (i), ¯S(i), ¯V (i)] = SVD( ¯A(i));

end for

3. Compute U = L−1( ¯U ), S = L−1( ¯S), and V =

L−1( ¯V).

It is clear that A ∗L I = A and I ∗L A = A given the
appropriate dimensions. The tensor ¯I = L(I) is a tensor
with each frontal slice being the identity matrix.

Deﬁnition 2.4. (Orthogonal tensor) [9] Let L be any in-
vertible linear transform in (7). A tensor Q ∈ Rn×n×n3 is
orthogonal if it satisﬁes Q⊤ ∗L Q = Q ∗L Q⊤ = I.

Deﬁnition 2.5. (F-diagonal Tensor) [10] A tensor is called
f-diagonal if each of its frontal slices is a diagonal matrix.

Theorem 2.1. (T-SVD) [9] Let L be any invertible linear
transform in (7), and A ∈ Rn1×n2×n3 . Then it can be

factorized as

A = U ∗L S ∗L V ⊤,

(9)

where U ∈ Rn1×n1×n3 , V ∈ Rn2×n2×n3 are orthogonal,
and S ∈ Rn1×n2×n3 is an f-diagonal tensor.

Figure 1 gives an intuitive illustration of the t-SVD fac-
torization. Also, t-SVD can be computed by performing
matrix SVD in the transform domain. See Algorithm 2.
For any invertible linear transform L, we have L(0) =
L−1(0) = 0. So both S and ¯S are f-diagonal tensors. We
can use their number of nonzero singular tubes to deﬁne the
tensor tubal rank as in [15].

Deﬁnition 2.6. (Tensor tubal rank) Let L be any invertible
linear transform in (7). For A ∈ Rn1×n2×n3 , the tensor
tubal rank, denoted as rankt(A), is deﬁned as the number
of nonzero singular tubes of S, where S is from the t-SVD

of A = U ∗L S ∗L V ⊤. We can write

rankt(A) =#{i, S(i, i, :) 6= 0}.

For A ∈ Rn1×n2×n3 with tubal rank r, we also have
the skinny t-SVD, i.e., A = U ∗L S ∗L V ⊤, where U ∈
Rn1×r×n3 , S ∈ Rr×r×n3 , and V ∈ Rn2×r×n3 , in which
U ⊤∗L U = I and V ⊤∗L V = I. We use the skinny t-SVD

throughout this paper. The tensor tubal rank is nonconvex.
In Section 3, we will study the low tubal rank tensor com-
pletion problem by convex surrogate function minimization.
At the following, we show how to deﬁne the convex tensor
nuclear norm induced by the t-product ∗L. We can ﬁrst de-
ﬁne the tensor spectral norm as in [15].

Deﬁnition 2.7. (Tensor spectral norm) Let L be any in-
vertible linear transform in (7), and A ∈ Rn1×n2×n3 . The
tensor spectral norm of A is deﬁned as kAk := k ¯Ak.

The tensor nuclear norm can be deﬁned as the dual nor-
m of the tensor spectral norm. To this end, we need the
following assumption on L given in (7), i.e.,

L⊤L = LL⊤ = ℓIn3 ,

(10)

where ℓ > 0 is a constant. Using (10), we have the follow-
ing important properties,

hA, Bi =

kAkF =

1

ℓ (cid:10) ¯A, ¯B(cid:11) ,
√ℓk ¯AkF .

1

(11)

(12)

For any B ∈ Rn1×n2×n3 and ˜B ∈ Rn1n3×n2n3 , we have

kAk∗ := sup

kBk≤1hA, Bi

= sup

k ¯Bk≤1
1
ℓ

1
ℓh ¯A, ¯Bi
k ˜Bk≤1h ¯A, ˜Bi
1
ℓk ¯Ak∗,

sup

≤

=

(13)

(14)

(15)

(16)

where (14) uses (11), (15) is due to the fact that ¯B is a block
diagonal matrix in Rn1n3×n2n3 while ˜B is an arbitrary ma-
trix in Rn1n3×n2n3 , and (16) uses the fact that the matrix
nuclear norm is the dual norm of the matrix spectral norm.

On the other hand, let A = U ∗L S ∗L V ⊤ be the t-SVD of
A and B = U ∗L V ⊤. We have

kAk∗ =hU ∗L S ∗L V ⊤, U ∗L V ⊤i
=hU ⊤ ∗L U ∗L S, V ⊤ ∗L Vi
=hS, Ii
1
ℓh ¯S, ¯Ii
=
1
Tr( ¯S)
ℓ
1
ℓk ¯Ak∗.

=

=

(17)

5999

Combining (13)-(16) and (17), we then have the following
deﬁnition of tensor nuclear norm.

Deﬁnition 2.8. (Tensor nuclear norm) Let L be any in-
vertible linear transform in (7) and it satisﬁes (10), and

A ∈ Rn1×n2×n3 . The tensor nuclear norm of A is deﬁned
as kAk∗ := 1

ℓk ¯Ak∗.

1

If we deﬁne the tensor average rank as ranka(A) =
ℓ rank( ¯A), then it can be proved that the above tensor nu-
clear norm is the convex envelope of the tensor average rank
within the domain {A|kAk ≤ 1} [15].
3. Tensor Completion with Exact Recovery

Guarantee

In this section, we consider the low rank tubal tensor
completion problem using the linear transform based tensor
nuclear norm minimization. Let L be any invertible linear
transform in (7) and it satisﬁes (10). Let M ∈ Rn1×n2×n3
be an unknown tensor and it has tubal rank rankt(A) = r.
Assume that the entries of M are observed independently
with probability p. The set of the index of the observed
entries is denoted as Ω.
In this setting, we denote that
Ω ∼ Ber(p). So, the problem of tensor completion is to
recover the underlying low tubal rank tensor M from the
observations {Mij, (i, j, k) ∈ Ω}. In this work, we solve
the tensor completion problem by the following convex pro-
gram based on the proposed tensor nuclear norm
X kXk∗, s.t. P Ω(X ) = P Ω(M).
min

(18)

The above model is convex and thus the optimal solution is
computable. Now, the question is, how can we exactly re-
cover M by solving (18)? It is observed that the recovery
is almost impossible for a low-rank matrix which is equal to
zero in nearly all of rows or columns [3]. Thus the incoher-
ence conditions are introduced to avoid such pathological
situations. For tensor completion, we suffer from the same
issue. To avoid the case that M is too sparse, we need the
following standard tensor incoherence conditions

At the following, we denote n(1) = max(n1, n2) and

n(2) = min(n1, n2). Then ℓ ≤ µ ≤ n(2)ℓ

r

.

Theorem 3.1. Let L be any invertible linear transform in
(7) and it satisﬁes (10), and M ∈ Rn1×n2×n3 with tubal
rank rankt(M) = r. Let M = U ∗L S∗L V ⊤ be the skinny
t-SVD of M. Suppose that the indices Ω ∼ Ber(p) and the
tensor incoherence conditions (19)-(20) hold. There exist
universal constants c0, c1, c2 > 0 such that if

c0µr log2(n(1)ℓ)

n(2)ℓ

p ≥

,

(21)

then M is the unique solution to (18) with probability at
least 1 − c1(n1 + n2)−c2 .

In theory, Theorem 3.1 shows that the optimal solution
ˆX to the convex program (18) exactly recovers the under-
lying low tubal rank tensor M. In particular, to recover a
tensor M ∈ Rn1×n2×n3 with high probability, the sam-
pling complexity is O(rn(1)n3 log2(n(1)ℓ)). Such a bound
is tight compared with O(rn(1)n3), which is the degree
of freedom of the underlying tensor M with tubal rank
r. Note that the above result further generalizes the exist-
ing low-rank tensor and low-rank matrix completion results.
For example, when the discrete Fourier transform matrix is
used as the invertible linear transform L, the t-product ∗L
reduces to the t-product in [10], and thus the proposed ten-
sor nuclear norm in Deﬁnition 2.8 reduces to the one in [14].
Then the result in Theorem 3.1 is equivalent to the recovery
guarantee in [16]. Furthermore, if M is a matrix, Theorem
3.1 reduces to the recovery guarantee in [5].

Problem (18) can be solved by the standard Alternat-
ing Direction Method of Multipliers (ADMM) [12]. We
omit the details of ADMM, but give the detail for solv-
ing the following key subproblem in ADMM, i.e., for any

Y ∈ Rn1×n2×n3 ,

min
X

τkXk∗ +

1
2kX − Yk2
F .

(22)

max

i=1,··· ,n1kU ⊤ ∗L˚eikF ≤ r µr
j=1,··· ,n2kV ⊤ ∗L˚ejkF ≤ r µr

max

n1ℓ

n2ℓ

,

,

(19)

(20)

Let Y = U ∗L S ∗L V ⊤ be the tensor-SVD of Y. For any
τ > 0, we deﬁne the Tensor Singular Value Thresholding
(T-SVT) operator as follows

where µ > 0, and˚ei is a tensor basis deﬁned below.

Deﬁnition 3.1. (Standard tensor basis) Let L be any in-
vertible linear transform in (7) and it satisﬁes (10). We de-
note˚ei as the tensor column basis, which is a tensor of size
n× 1× n3 with the (i, 1, 1)-th entry of L(˚ei) equaling 1 and
the rest equaling 0. Naturally its transpose˚e⊤
is called row
i
basis. The other standard tensor basis is called tube basis
˙ek, which is a tensor of size 1 × 1 × n3 with the (1, 1, k)-th
entry of L(˙ek) equaling 1 and the rest equaling 0.

where

Dτ (Y) = U ∗L S τ ∗L V ⊤,

S τ = L−1((L(S) − τ )+),

(23)

(24)

where t+ denotes the positive part of t, i.e., t+ = max(t, 0).
This operator simply applies a soft-thresholding rule to
L(S), effectively shrinking these towards zero. The T-SVT
operator is the proximity operator associated with the pro-
posed tensor nuclear norm.

6000

Table 1: Exact tensor completion on random data. The Discrete
Cosine Transform (DCT) is used as the linear transform L.

Table 2: Exact tensor completion on random data. A Random
Orthogonal Matrix (ROM) is used as the linear transform L.

X 0 ∈ Rn×n×n, r = rankt(X 0), m = pn3, dr = r(2n − r)n

X 0 ∈ Rn×n×n, r = rankt(X 0), m = pn3, dr = r(2n − r)n

n

50
50
50
50
100
100
100
100
200
200
200
200

r

3
5
8
10
5
10
15
20
5
10
20
30

m
dr

4
3
2
2
4
3
2
2
4
3
2
2

p

0.47
0.57
0.59
0.72
0.39
0.57
0.56
0.72
0.20
0.29
0.38
0.56

rankt( ˆX )

3
5
8
10
5
10
15
20
5
10
20
30

k ˆX −X kF

kX kF
3.5e−6
3.1e−6
4.2e−6
3.1e−6
1.1e−5
9.9e−6
6.8e−6
3.8e−6
6.2e−5
4.8e−5
4.7e−5
1.9e−5

n

50
50
50
50
100
100
100
100
200
200
200
200

r

3
5
8
10
5
10
15
20
5
10
20
30

m
dr

4
3
2
2
4
3
2
2
4
3
2
2

p

0.47
0.57
0.59
0.72
0.39
0.57
0.56
0.72
0.20
0.29
0.38
0.56

rankt( ˆX )

3
5
8
10
5
10
15
20
5
10
20
30

k ˆX −X kF

kX kF
3.6e−6
3.0e−6
4.4e−6
1.3e−6
1.5e−5
9.7e−6
7.0e−6
3.9e−6
5.8e−5
4.6e−5
4.7e−5
2.0e−5

Theorem 3.2. Let L be any invertible linear transform
in (7) and it satisﬁes (10). For any τ > 0 and Y ∈
Rn1×n2×n3 , the tensor singular value thresholding oper-
ator (23) obeys

Dτ (Y) = arg min

X

τkXk∗ +

1
2kX − Yk2
F .

(25)

The main cost of ADMM for solving (18) is to compute
Dτ (Y) for solving (25). For any general linear transfor-
m L in (7), it is easy to see that the per-iteration cost is
O(n1n2n2
(2)n3). For some speciﬁc linear trans-
forms, e.g., DFT, the per-iteration cost can be further re-
duced. The cost can be further reduced by taking the low-
rank structure priori of M as the matrix case in [12].

3 + n(1)n2

4. Experiments

In this section, we conduct numerical experiments to cor-
roborate our main results. We ﬁrst investigate the ability
of the convex program (18) to recover tensors with various
tubal ranks and sampling rates. We then apply it for image
recovery and compare the performance with existing low-
rank matrix and low-rank tensor completion models2.

4.1. Exact Recovery on Random Data

We conduct two experiments to demonstrate the prac-
tical applicability of the tensor nuclear norm heuristic for
recovering low-rank tensors. We ﬁrst verify the correct re-
covery guarantee in Theorem 3.1. Note that Theorem 3.1
holds for any invertible linear transform L with L in (7) sat-
isfying (10). We consider two cases of L: (1) Discrete Co-
sine Transform (DCT) [9]; (2) Random Orthogonal Matrix
(ROM)3. In both cases, (10) holds with ℓ = 1. We simply
consider tensors of size n× n× n, with n = [50, 100, 200].
We generate a tubal rank r tensor by M = P ∗L Q, where

2Codes of our method: http://github.com/canyilu.
3Codes: https://www.mathworks.com/matlabcentral/

fileexchange/11783-randorthmat.

the entries of P ∈ Rn×r×n and Q ∈ Rr×n×n are inde-
pendently sampled from an N (0, 1/n) distribution. Then
we sample m = pn3 elements uniformly from M to for-
m the known entries. A useful quantity for reference is
dr = r(2n − r)n. With P Ω(M), we solve (18) and obtain
the solution ˆX . Then we report the tubal rank of ˆX and the
relative errors k ˆX − MkF /kMkF . Table 1 and 2 respec-
tively give the recovery results for the two different choices
of the linear transforms L. It can be seen that the convex
program (18) gives the correct rank estimation of M in all
cases and the relative errors are small. Thus, these result-
s well verify the correct recovery guarantee as claimed in
Theorem 3.1 for convex program (18).

Theorem 3.1 shows the perfect recovery for incoherent
tensor based on rankt(M) and the sampling rate p. Now
we examine the recovery phenomenon with varying tubal
rank of M and varying sampling rate p. We generate M ∈
Rn×n×n, where n = 50, in the same way as the above ex-
periment. Then we sample m = pn3 elements uniformly to
form the known entries. We set p = [0.01 : 0.01 : 0.99],
and r = [1, 2, . . . , 38]. For each (r, p) pair, we simulate
10 test instances and declare a trial to be successful if the

recovered ˆX satisﬁes k ˆX − MkF /kMkF ≤ 10−3. Fig-
ure 2 plots the fraction of correct recovery for each pair
(blue = 0% and yellow = 100%) for different choices of
the linear transform L. It can be seen that the recovery phe-
nomenons in both cases are very similar. Both have a large
region in which the recovery is correct. This veriﬁes our
main result in Theorem 3.1 that the recovery guarantee is
invariant to different choices of the linear transforms when
property (10) holds. Our results are also consistent with the
phenomenons in existing low-rank matrix completion [3, 5]
and low-rank tensor completion [16].

4.2. Tensor Completion for Image Recovery

We consider the application of tensor completion for im-
age recovery. The motivation is that the real color images

6001

35

30

25

r

20

15

10

5

1

0.8

0.6

0.4

0.2

0

35

30

25

r

20

15

10

5

1

0.8

0.6

0.4

0.2

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

p

p

(a) discrete cosine transform

(b) random orthogonal matrix

Figure 2: Phase transitions for tensor completion. Fraction of
correct recoveries across 10 trials, as a function of sampling rate
p (x-axis) and tubal rank rankt(M) (y-axis). Left: discrete co-
sine transform is used as the linear transform L. Right: random
orthogonal matrix is used as the linear transform L.

can be well approximated by low-rank matrices on the three
channels independently. If we treat a color image as a three
way tensor, then it can be well approximated by low rank
tensors. There have many works which use such observa-
tions, e.g., [13, 28, 14, 15, 7], for image recovery.

Note that the t-product induced tensor nuclear norm is
orientation dependent. The recovery performance may be
different for different tensors constructed from a color im-
age. For a color image with size h × w, we can construct a
tensor M ∈ Rh×3×w, where the lateral slices correspond
to the three channels4. We randomly set m = 3phw entries
to be observed, where we use p = 0.3 in this experiment.
We consider four methods for image recovery and compare
their performance: (1) LRMC [3]: low-rank matrix com-
pletion applied on each channel of images separably; (2)
LRTC [13]: low-rank tensor completion model in (2); (3)
TNN [16]: tensor nuclear norm (TNN) based tensor com-
pletion model (a special case of our model (18) by using the
discrete Fourier transform as the linear transform L); (4)
TNN-DCT: our TNN minimization model (18) by using the
Discrete Cosine Transform (DCT)5 as the linear transform
L; (5) TNN-ROM: our TNN minimization model (18) by
using the Random Orthogonal Matrix (ROM) as the linear
transform L. The Peak Signal-to-Noise Ratio (PSNR) [16]
is used to evaluate the recovery performance.

Figure 4 shows the recovered images obtained by dif-
ferent methods on some sample images. Figure 3 further
shows the comparison on the PSNR values and running time
on the tested images. From these results, it can be seen that
our TNN-DCT achieves the best performance in most cas-
es, while TNN-ROM achieves the worst performance. In
particular, TNN-DCT has better recovery performance than

4It is observed in [16] that this way of tensor construction achieves the

best performance in practice.

5DCT is another interesting invertible linear transform discussed in the

original work of the new t-product deﬁnition in [9].

LRMC

LRTC

TNN

TNN-DCT

TNN-ROM

1

2

3

4

5

6

Image ID

LRMC

LRTC

TNN

TNN-DCT

TNN-ROM

50

40

30

R
N
S
P

20

10

50

40

30

20

10

0

e
m

i

i
t
 
g
n
n
n
u
R

1

2

3

4

5

6

Image ID

Figure 3: Comparison of the PSNR values and running time
(seconds) obtained by LRMC, LRTC, TNN, TNN-DCT and
TNN-ROM on the tested images in Figure 4.

TNN on the edges of images as shown in Figure 4. This
implies that the discrete Fourier transform used in [10] may
not be the best in image analysis, though the reason is not
very clear now. However, if the random orthogonal matrix
is used as the linear transform L, the results are very bad. It
may not be a good choice in image analysis. In a summary,
we observe that the choice of the linear transform L is cru-
cial in practice, though the best linear transform is currently
not clear. For the efﬁciency, Figure 3 shows that our new
methods is comparable with RPCA and the existing TNN
method.

5. Conclusions

Inspired by the recent invertible linear transforms based
t-product [9] which generalizes the existing discrete Fouri-
er transform based t-product [10], we give more general
deﬁnitions of tensor tubal rank and tensor nuclear norm.
We show that when the invertible linear transforms satis-
fy certain conditions, the tensor completion problem can be
solved by the proposed convex tensor nuclear norm mini-
mization. In theory, we give the order optimal bound for
the guarantee of exact recovery.
It is interesting that the
main result holds for a broad choices of the invertible lin-
ear transforms. This greatly generalizes the existing result
for low tubal rank tensor recovery. Numerical experiments
veriﬁed our theory and the application on image recovery
demonstrates the effectiveness of our model.

There have some interesting future works. First, as the t-
product ∗L and our tensor nuclear norm depend on the used
linear transform, it is interesting to learn the linear transfor-
m from the data in different tasks. It is expected to improve
the learning performance by optimizing the linear transfor-

6002

(a) orignal image

(b) observed image

(c) LRMC

(d) LRTC

(e) TNN

(f) TNN-DCT

Figure 4: Performance comparison for image recovery on some sample images. (a) Original image; (b) observed image; (c)-(f) recovered
images by LRMC, LRTC, TNN and TNN-DCT, respectively. Best viewed in ×2 sized color pdf ﬁle.

m for different types of data. Second, beyond the tensor
completion problem studied in this work, it is interesting
to use the tensor nuclear norm for some other tasks, e.g.,
tensor robust PCA [14] and low-rank representation model-
s [17]. Also, it is always of great interest to apply our new
method for different applications, e.g., motion segmentation
and background modeling [4].

Acknowledgement

Xi Peng was supported by the Fundamental Research
Funds for the Central Universities under Grant YJ201748,
by NFSC under Grant 61806135, by NFSC for Distin-
guished Young Scholar under Grant 61625204, and by the
Key Program of NFSC under Grant 61836006.

6003

References

[1] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Tel-
garsky. Tensor decompositions for learning latent variable
models. J. Machine Learning Research, 15(1):2773–2832,
2014. 1

[2] R. S. Cabral, F. Torre, J. P. Costeira, and A. Bernardino.
Matrix completion for multi-label image classiﬁcation.
In
Advances in Neural Information Processing Systems, pages
190–198, 2011. 1

[3] E. Cand`es and B. Recht. Exact matrix completion via convex
optimization. Foundations of Computational mathematics,
9(6):717–772, 2009. 1, 5, 6, 7

[4] E. J. Cand`es, X. D. Li, Y. Ma, and J. Wright. Robust principal

component analysis? J. ACM, 58(3), 2011. 8

[5] Y. Chen.

Incoherence-optimal matrix completion.

IEEE
Trans. Information Theory, 61(5):2909–2923, May 2015. 1,
2, 5, 6

[6] L. De Lathauwer and B. De Moor. From matrix to ten-
sor: Multilinear algebra and signal processing. In Institute
of Mathematics and its Applications Conference Series, vol-
ume 67, pages 1–16. Citeseer, 1998. 1

[7] S. Gandy, B. Recht, and I. Yamada. Tensor completion and
low-n-rank tensor recovery via convex optimization. Inverse
Problems, 27(2):025010, 2011. 2, 7

[8] D. F. Gleich, C. Greif, and J. M. Varah. The power and arnol-
di methods in an algebra of circulants. Numerical Linear
Algebra with Applications, 20(5):809–831, 2013. 2

[9] E. Kernfeld, M. Kilmer, and S. Aeron. Tensor–tensor prod-
ucts with invertible linear transforms. Linear Algebra and its
Applications, 485:545–570, 2015. 1, 2, 3, 4, 6, 7

[10] M. E. Kilmer and C. D. Martin. Factorization strategies for
third-order tensors. Linear Algebra and its Applications,
435(3):641–658, 2011. 2, 3, 4, 5, 7

[11] T. G. Kolda and B. W. Bader. Tensor decompositions and

applications. SIAM Rev., 51(3):455–500, 2009. 1, 2

[12] Z. Lin, M. Chen, and Y. Ma. The augmented Lagrange mul-
tiplier method for exact recovery of corrupted low-rank ma-
trices. arXiv preprint arXiv:1009.5055, 2010. 5, 6

[13] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion
for estimating missing values in visual data.
IEEE Trans.
Pattern Recognition and Machine Intelligence, 35(1):208–
220, 2013. 1, 2, 7

[14] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan. Ten-
sor robust principal component analysis: Exact recovery of
corrupted low-rank tensors via convex optimization. In Proc.
IEEE Conf. Computer Vision and Pattern Recognition. IEEE,
2016. 2, 5, 7, 8

[15] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan. Tensor
robust principal component analysis with a new tensor nu-
clear norm. IEEE Trans. Pattern Recognition and Machine
Intelligence, 2018. 2, 4, 5, 7

[16] C. Lu, J. Feng, Z. Lin, and S. Yan. Exact low tubal rank
tensor recovery from gaussian measurements. In Proc. Int’l
Joint Conf. Artiﬁcial Intelligence, 2018. 1, 2, 5, 6, 7

[17] C. Lu, Z. Lin, and S. Yan. Smoothed low rank and sparse
matrix recovery by iteratively reweighted least squares min-

imization.
Feb 2015. 8

IEEE Trans. Image Processing, 24(2):646–654,

[18] C. Lu, J. Tang, S. Yan, and Z. Lin. Nonconvex nonsmooth
low rank minimization via iteratively reweighted nuclear
norm. IEEE Trans. Image Processing, 25(2):829–839, 2016.
1

[19] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to e-
valuating segmentation algorithms and measuring ecological
statistics. In Proc. IEEE Int’l Conf. Computer Vision, vol-
ume 2, pages 416–423. IEEE, 2001.

[20] N. Mesgarani, M. Slaney, S. Shamma, et al. Discrimina-
tion of speech from nonspeech based on multiscale spectro-
temporal modulations. IEEE Trans. Audio, Speech, and Lan-
guage Processing, 14(3):920–930, 2006. 1

[21] C. Mu, B. Huang, J. Wright, and D. Goldfarb. Square deal:
Lower bounds and improved relaxations for tensor recovery.
In Proc. Int’l Conf. Machine Learning, pages 73–81, 2014.
2

[22] B. Romera-Paredes and M. Pontil. A new convex relaxation
In Advances in Neural Information

for tensor completion.
Processing Systems, pages 2967–2975, 2013. 2

[23] B. Savas and L. Eld´en. Handwritten digit classiﬁcation using
higher order singular value decomposition. Pattern recogni-
tion, 40(3):993–1003, 2007. 1

[24] M. Signoretto, Q. T. Dinh, L. De Lathauwer, and J. A.
Suykens. Learning with tensors: a framework based on
convex optimization and spectral regularization. Machine
Learning, 94(3):303–351, 2014. 2

[25] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of
low-rank tensors via convex optimization. arXiv preprint
arXiv:1010.0789, 2010. 2

[26] L. R. Tucker.

Implications of factor analysis of three-way
matrices for measurement of change. Problems in measuring
change, 15, 1963. 1

[27] M. A. O. Vasilescu and D. Terzopoulos. Multilinear analysis
of image ensembles: Tensorfaces. In Proc. European Conf.
Computer Vision, pages 447–460. Springer, 2002. 1

[28] Y. Xu, R. Hao, W. Yin, and Z. Su. Parallel matrix factoriza-
tion for low-rank tensor completion. Inverse Problems and
Imaging, 9(2):601–624, 2015. 7

6004

