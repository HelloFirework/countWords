Hierarchical Cross-Modal Talking Face Generation

with Dynamic Pixel-Wise Loss

Lele Chen Ross K. Maddox

Zhiyao Duan Chenliang Xu

University of Rochester, USA

{lchen63, rmaddox}@ur.rochester.edu, {zhiyao.duan, chenliang.xu}@rochester.edu

Abstract

We devise a cascade GAN approach to generate talk-
ing face video, which is robust to different face shapes,
view angles, facial characteristics, and noisy audio con-
ditions.
Instead of learning a direct mapping from au-
dio to video frames, we propose ﬁrst to transfer audio to
high-level structure, i.e., the facial landmarks, and then
to generate video frames conditioned on the landmarks.
Compared to a direct audio-to-image approach, our cas-
cade approach avoids ﬁtting spurious correlations between
audiovisual signals that are irrelevant to the speech con-
tent. We, humans, are sensitive to temporal discontinu-
ities and subtle artifacts in video. To avoid those pixel
jittering problems and to enforce the network to focus on
audiovisual-correlated regions, we propose a novel dynam-
ically adjustable pixel-wise loss with an attention mech-
anism. Furthermore, to generate a sharper image with
well-synchronized facial movements, we propose a novel
regression-based discriminator structure, which considers
sequence-level information along with frame-level informa-
tion. Thoughtful experiments on several datasets and real-
world samples demonstrate signiﬁcantly better results ob-
tained by our method than the state-of-the-art methods in
both quantitative and qualitative comparisons.

1. Introduction

Modeling the dynamics of a moving human face/body
conditioned on another modality is a fundamental prob-
lem in computer vision, where applications are ranging
from audio-to-video generation [28, 3, 2] to text-to-video
generation [23, 19] and to skeleton-to-image/video genera-
tion [21, 7]. This paper considers such a task: given a target
face image and an arbitrary speech audio recording, gener-
ating a photo-realistic talking face of the target subject say-
ing that speech with natural lip synchronization while main-
taining a smooth transition of facial images over time (see
Fig. 1). Note that the model should have a robust general-

 Atte ntio n  

Identity image 

 Motion 
 Result  

Audio signal

Tell where
 to change

Tell how
 to change

Figure 1: Problem description. The model takes an arbi-
trary audio speech and one face image, and synthesizes a
talking face saying the speech. The synthesized frames (last
row) consist of synthesized attention (ﬁrst row) and motion
(second row), which demonstrate where and how the dy-
namics are synthesizing. For example, the face in the green
box looks similar to the example face so that the attention
map is almost dark; the face in the red box differs much
from the example image, and hence the attention highlights
the mouth region and the motion part hints white pixels for
teeth.

ization capability to different types of faces (e.g., cartoon
faces, animal faces) and to noisy speech conditions (see
Fig. 7). Solving this task is crucial to enabling many ap-
plications, e.g., lip-reading from over-the-phone audio for
hearing-impaired people, generating virtual characters with
synchronized facial movements to speech audio for movies
and games.

The main difference between still image generation and
video generation is temporal-dependency modeling. There
are two main reasons why it imposes additional challenges:
people are sensitive to any pixel jittering (e.g., temporal dis-
continuities and subtle artifacts) in a video; they are also
sensitive to slight misalignment between facial movements
and speech audio. However, recent researchers [3, 12, 17]
tended to formulate video generation as a temporally inde-
pendent image generation problem. For example, Chung
et al. [3] proposed an encoder-decoder structure to gener-
ate one image from 0.35-second audio at each time. Song
et al. [27] adopted a recurrent network to consider tem-
poral dependency. They applied RNN in the feature ex-
traction part, however, each frame was generated inde-

43217832

In this paper, we pro-
pendently in the generation stage.
pose a novel temporal GAN structure, which consists of a
multi-modal convolutional-RNN-based (MMCRNN) gen-
erator and a novel regression-based discriminator struc-
ture. By modeling temporal dependencies, our MMCRNN-
based generator yields smoother transactions between ad-
jacent frames. Our regression-based discriminator struc-
ture combines sequence-level (temporal) information and
frame-level (pixel variations) information to evaluate the
generated video.

Another challenge of the talking face generation is to
handle various visual dynamics (e.g., camera angles, head
movements) that are not relevant to and hence cannot be in-
ferred from speech audio. Those complicated dynamics, if
modeled in the pixel space [30], will result in low-quality
videos. For example, in web videos [5, 24] (e.g., LRW and
VoxCeleb datasets), speakers move signiﬁcantly when they
are talking. Nonetheless, all the recent photo-realistic talk-
ing face generation methods [3, 12, 27, 1, 28, 35] failed to
consider this problem. In this paper, we propose a hierarchi-
cal structure that utilizes a high-level facial landmarks rep-
resentation to bridge the audio signal with the pixel image.
Concretely, our algorithm ﬁrst estimates facial landmarks
from the input audio signal and then generates pixel varia-
tions in image space conditioned on generated landmarks.
Besides leveraging intermediate landmarks for avoiding di-
rectly correlating speech audio with irrelevant visual dy-
namics, we also propose a novel dynamically adjustable
loss along with an attention mechanism to enforce the net-
work to focus on audiovisual-correlated regions. It is worth
to mention that in a recent audio-driven facial landmarks
generation work [8], such irrelevant visual dynamics are re-
moved in the training process by normalizing and identity-
removing the facial landmarks. This has been shown to
result in more natural synchronization between generated
mouth shapes and speech audio.

Combining the above features, which are designed to
overcome limitations of existing methods, our ﬁnal model
can capture informative audiovisual cues such as the lip
movements and cheek movements while generating ro-
bust talking faces under signiﬁcant head movements and
noisy audio conditions. We evaluate our model along with
state-of-the-art methods on several popular datasets (e.g.,
GRID [6], LRW [5], VoxCeleb [24] and TCD [13]). Exper-
imental results show that our model outperforms all com-
pared methods and all the proposed features contribute ef-
fectively to our ﬁnal model. Furthermore, we also show
additional novel examples of synthesized facial movements
of the human/cartoon characters who are not in any dataset
to demonstrate the robustness of our approach.

The contributions of our work can be summarized as
follows: (1) We propose a novel cascade network struc-
ture to reduce the effects of the sound-irrelevant visual

dynamics in the image space. Our model explicitly con-
structs high-level representation from the audio signal and
guides video generation using the inferred representation.
(2) We exploit a dynamically adjustable pixel-wise loss
along with an attention mechanism which can alleviate tem-
poral discontinuities and subtle artifacts in video genera-
tion.
(3) We propose a novel regression-based discrimi-
nator to improve the audio-visual synchronization and to
smooth the facial movement transition while generating
realistic looking images. The code has been released at
https://github.com/lelechen63/ATVGnet.

2. Related Work

In this section, we ﬁrst brieﬂy survey related work on the
talking face generation task. Then we discuss the related
work of each technique used in our model.
Talking Face Synthesizing The success of traditional ap-
proaches has been mainly limited to synthesizing a talking
face from speech audio of a speciﬁc person [11, 9, 29].
For example, Suwajanakorn et al. [29] synthesized a tak-
ing face of President Obama with accurate lip synchro-
nization, given his speech audio. The mechanism is to
ﬁrst retrieve the best-matched lip region image from a
database through audiovisual feature correlation and then
compose the retrieved lip region with the original face.
However, this method requires a large amount of video
footage of the target person. More recently, by combin-
ing the GAN/encoder-decoder structure and the data-driven
training strategy, [27, 4, 1, 12] can generate arbitrary faces
from arbitrary input audio.
High-Level Representations
In recent years, high-level
representations of images [31, 14, 34, 15] have been ex-
ploited in video generation tasks by using an encoder-
decoder structure as the main approach. Given a condi-
tion, we can transfer it to high-level representations and
feed them to a generative network to output a distribution
over locations that a pixel is predicted to move. By adopt-
ing human body landmarks, Villegas et al. [31] proposed an
encoder-decoder network which achieves long-term future
prediction. Suwajanakorn et al. [28] transferred the audio
signal to lip shapes and then synthesized the mouth texture
based on the transferred lip shapes. These works have in-
spired us to use the facial landmarks to bridge audio with
row pixel generation.
Attention Mechanism
Attention mechanism is an
emerging topic in natural language tasks [20] and im-
age/video generation task [26, 37, 22, 36]. Pumarola et
al. [26] generated facial expression conditioned on action
units annotations. Instead of using a basic GAN structure,
they exploited a generator that regresses an attention mask
and a RGB color transformation over the entire image. The
attention mask deﬁnes a per-pixel intensity specifying to
what extend each pixel of the original image will contribute

7833

(AT-net)

Time

MFCC

+

LSTM

 PCA

Dlib  
 

+

CRNN

+

Deconv 
Sigmoid 

(VG-net)

Figure 2: Overview of our network architecture. The blue part illustrates the AT-net, which transfers audio signal to low-
dimensional landmarks representation and the green part illustrates the VG-net, which generates video frames conditioned on
the landmark. During training, the input to VG-net are ground truth landmarks (p1:T ). During inference, the input to VG-net
are fake landmarks (ˆp1:T ) generated by AT-net. The AT-net and VG-net are trained separately to avoid error accumulation.

to the ﬁnal rendered image. We adopt this attention mech-
anism to make our network robust to visual variations and
noisy audio conditions. Feng et al. [10] observed that inte-
grating a weighted mask into the loss function during train-
ing can improve the performance of the reconstruction net-
work. Based on this observation, rather than using a ﬁxed
loss weights, we propose a dynamically adjustable loss by
leveraging the attention mechanism to emphasize the audio-
visual regions.

3. Architecture

This section describes the architecture of the proposed
model. Fig. 2 shows the overall diagram, which is decou-
pled into two parts: audio transformation network (AT-net)
and visual generation network (VG-net). First, we explain
the overall architecture and the training strategy in Sec. 3.1.
Then, we introduce two novel components: attention-based
dynamic pixel-wise loss in Sec. 3.2 and a regression-based
discriminator structure in Sec. 3.3 used in our VG-net.

3.1. Overview

Cascade Structure and Training Strategy We tackle
the task of talking face video generation in a cascade per-
spective. Given the input audio sequence a1:T , one exam-
ple frame ip and its landmarks pp, our model generates fa-
cial landmarks sequence ˆp1:T and subsequently generates
frames ˆv1:T . To solve this problem, we come up with a
novel cascade network structure:

ˆp1:T = Ψ(a1:T , pp) ,
ˆv1:T = Φ(ˆp1:T , ip, pp) ,

(1)

(2)

where the AT-net Ψ (see Fig. 2 blue part) is a condi-
tional LSTM encoder-decoder and the VG-net Φ (see Fig. 2
green part) is a multi-modal convolutional recurrent net-
work. During inference, the AT-net Ψ (see Eq. 1) observes

audio sequence a1:T and example landmarks pp and then
predicts low-dimensional facial landmarks ˆp1:T . By passing
ˆp1:T into VG-net Φ (see Eq. 2) along with example image
ip and pp, we subsequently get synthesized video frames
ˆv1:T . Ψ and Φ are trained in a decoupled way so that Φ can
be trained with teacher forcing strategy. To avoid the error
accumulation caused by ˆp1:T , Φ is conditioned on ground
truth landmarks p1:T during training.
Audio Transformation Network (AT-net)
the AT-net (Ψ) is formulated as:

Speciﬁcally,

[ht, ct] = ϕlmark(LSTM(faudio(at), flmark(hp), ct−1)), (3)

ˆpt = PCAR(ht) = ht ⊙ ω ∗ UT + M .

(4)

Here, the AT-net observes the audio MFCC at and land-
marks PCA components hp of the target identity and out-
puts PCA components ht that are paired with the input au-
dio MFCC. The faudio, flmark and ϕlmark indicate audio en-
coder, landmarks encoder and landmarks decoder. The ct−1
and ct are outputs from cell units. PCAR is PCA reconstruc-
tion and ω is a boost matrix to enhance the PCA feature.
The U corresponds to the largest eigenvalues and M is the
mean shape of landmarks in the training set.
In our em-
pirical study, we observe that PCA can decrease the effect
of none-audio-correlated factors (e.g., head movements) for
training the AT-net.
Visual Generation Network (VG-net)
Intuitively, simi-
lar to [34, 31], we assume that the distance between current
landmarks pt and example landmarks pp in feature space
can represent the distance between current image frame and
example image in image feature space. Based on this as-
sumption (see Eq. 5), we can obtain current frame feature
v′′
t (size of 128 × 8 × 8). Different from their methods,
we replace element-wise addition with channel-wise con-
catenation in Eq. 5, which better preserves original frame
information in our empirical study. At the meanwhile, we
can also compute an attention map (attpt ) based on the dif-

7834

Figure 3: The results of our baseline method. The synthe-
sized frames with pixel jittering problem. The discontinu-
ous problem and subtle artifacts will be ampliﬁed after com-
posing into a video.

ference between pt and pp (see Eq. 6). By feeding the com-
t and attpt along with example image feature i′
puted v′′
p
(size of 128 × 32 × 32) into the MMCRNN part, we obtain
the current image feature v′
t (see Eq. 7). The resultant image
feature v′
t will be used to generate video frames as detailed
in the next section. Speciﬁcally, the VG-net is performed
by:

v′′
t = fimg(ip) ⊕ (flmark(pt) − flmark(pp)) ,
attpt = σ(flmark(pt) ⊕ flmark(pp)) ,
v′
t = (CRNN(v′′

t )) ⊙ attpt + i′

p ⊙ (1 − attpt ) ,

(5)

(6)

(7)

where ⊕ and ⊙ are concatenation operation and element-
wise multiplication, respectively. The CRNN part consists
of Conv-RNN, residual block and deconvolution layers. i′
p
is the middle layer output of fimg(ip), and σ is Sigmoid ac-
tivation function. We omit some convolution operations in
equations for better understanding.

3.2. Attention Based Dynamic Pixel wise Loss

Recent works on video generation adopt either GAN-
based methods [1, 32, 27] or Encoder-Decoder-based meth-
ods [3]. However, one common problem is the pixel jit-
tering between adjacent frames (see Fig. 3). Pixel jittering
is not obvious in single image generation but is a severe
problem for video generation as humans are sensitive to any
pixel jittering, e.g., temporal discontinuities and subtle ar-
tifacts in a video. The reason is that GAN loss or L1/L2
loss can barely generate perfect frames that all pixels are
consistently changing in temporal domain, especially for
audiovisual-non-correlated regions, e.g., background and
head movements. In order to solve the pixel jittering prob-
lem, we propose a novel dynamic pixel-wise loss to enforce
the generator to generate consistent pixels along temporal
axis.

As mentioned in Sec. 2, Pumarola et al. [26] exploited a
generator that regresses an attention mask and a RGB color
transformation over the entire image. We adapt this atten-
tion mechanism in our VG-net to disentangle the motion
part from audiovisual-non-correlated regions. Therefore,
our ﬁnal frame output is governed by the combination:

ˆvt = αt ⊙ mt + (1 − αt) ⊙ ip ,

(8)

where attention αt is obtained by applying convolution and
Sigmoid activation operations on v′
t, motion mt is obtained
by applying another convolution and hyperbolic tangent ac-
tivation operations on v′
t. This step enforces the network to
generate stable pixels in audiovisual-non-correlated regions
while generating movements in audiovisual-correlated re-
gions.

From Fig. 5, we can conclude that

the pixels in
audiovisual-non-correlated regions (e.g., hair, background
etc.) usually attract less attention and are irrelevant to given
condition (audio).
In contrast, the network is mainly fo-
cusing on correlated regions (e.g., mouth, jaw, and cheek).
Intuitively, 0 ≤ αt ≤ 1 can be viewed as a spatial mask
that indicates which pixels of given face image ip need to
move at time step t. We can also regard αt as a reference to
represent to what extend each pixel contributes to the loss.
The audiovisual-non-correlated regions should contribute
less to the loss compared with the correlated regions. Thus,
we propose a novel dynamic adjustable pixel-wise loss by
leveraging the power of αt, which is deﬁned as:

Lpix =

T

X

t=1

k(vt − ˆvt) ⊙ (αt + β)k1) ,

(9)

where αt is the same as αt but without gradient. It repre-
sents the weight of each pixel dynamically that eases the
generation. We remove the gradient of αt when back-
propagating the loss to the network to prevent trivial so-
lutions (lower loss but no discriminative ability). We also
give base weights β to all pixels to make sure all pixels will
be optimized. Here, we manually tune the hyper-parameter
β and set β = 0.5 in all of our experiments.

3.3. Regression Based Discriminator

Recently, people ﬁnd that perceptual loss [16] is help-
ful for generating sharp images in GAN/VAE [27, 1]. Per-
ceptual loss utilizes high-level features to compare gen-
erated images and ground-truth images resulting in better
sharpness of the synthesized images. The key idea is that
the weights of the perceptual network part are ﬁxed, and
the loss will only contribute to the generator/decoder part.
Based on this intuition, we propose a novel discriminator
structure (see Fig. 4). The discriminator observes exam-
ple landmarks pp and either ground truth video frames v1:T
or synthesized video frames ˆv1:T , then regresses landmarks
shapes ˆp1:T paired with the input frames, and addition-
ally, gives a discriminative score s for the entire sequence.
Speciﬁcally, we formulate discriminator into frame-wise
part Dp (blue arrows in Fig. 4) and sequence-level part Ds
(red arrows in Fig. 4).

The Dp observes example landmarks and video frames,
then regresses the landmarks sequence based on observed
information. By yielding the facial landmark, it can evalu-
ate the input image based on high-level representation in a

7835

Fake/Real

Aggregation

+

LSTM
+

+

LSTM
+

+

LSTM

Time

Figure 4: The overview of the regression-based discrimina-
tor. The ⊕ means concatenation. The + means element-
wise addition. The blue arrow and red arrow represent Dp
and Ds, respectively.

frame-wise fashion. Speciﬁcally, the ˆpt is calculated by:

ˆpt = Dp(pp, vt)

= pp + LSTM(flmark(pp) ⊕ fimg(vt)) ,

(10)

which observes ground truth image during discriminator
training stage and observes synthesized image during gen-
erator training stage.

Besides Dp, the LSTM cell unit yields another branch
Ds, which obtains vectors from each LSTM cell unit and
aggregates them by average pooling. By passing through a
Sigmoid activation function, Ds yields ﬁnal discriminative
score s for the overall input sequence. The score s can ob-
tained by:

s = Ds(pp, v1:T )

= σ(

1
T

T

X

t=1

(LSTM(flmark(pp) ⊕ fimg(vt)))) .

(11)

The Dp part is optimized to minimize the L2 loss between
the predicted landmarks and the ground truth landmarks.
Thus our GAN loss can be expressed as:

Lgan =Epp,v1:T [log Ds(pp, v1:T )]+

Epp,p1:T ,ip [log(1 − Ds(pp, G(pp, p1:T , ip))]+
k(Dp(pp, G(pp, p1:T , ip)) − p1:T ) ⊙ Mpk2
2+
k(Dp(pp, v1:T ) − p1:T ) ⊙ Mpk2

2 ,

(12)

where Mp is a pre-deﬁned weight mask hyper-parameter
which can penalize more on lip regions. By updating the

parameters based on the regression loss when training the
discriminator, the Dp can learn to extract low-dimensional
representations from raw image data. When we train the
generator, we will ﬁx the weights of discriminator includ-
ing Ds and Dp so that Dp will not compromise to generator.
The loss back-propagated from Dp will enforce generator to
generate accurate face shapes (e.g., cheek shape, lip shape
etc.) and the loss back-propagated from Ds will enforce the
network to generate high-quality images.

3.4. Objective Function

By linearly combining all partial losses introduced in
Sec. 3.2 and Sec. 3.3, the full loss function L can be ex-
pressed as:

L = Lgan + λ ∗ Lpix ,

(13)

where λ is a hyper-parameter that controls the relative im-
portance of different loss terms. We set λ = 10.0 in our
experiments.

4. Experiments

In this section, we conduct thoughtful experiments to
demonstrate the efﬁciency and effectiveness of the proposed
architecture for video generation. Sec. 4.1 explains datasets
and implementation in detail. Sec. 4.2 shows our results
along with other state-of-the-art methods. We show user
studies and ablation study in Sec.4.3 and Sec. 4.4 respec-
tively.

4.1. Experimental Setup

Dataset We quantitatively and qualitatively evaluate our
ATVGnet on LRW dataset [4] and GRID dataset [6]. The
LRW dataset consists of 500 different words spoken by hun-
dreds of different speakers in the wild. We follow the same
train-test split as in [4]. In GRID dataset, there are 1000
short videos, each spoken by 33 different speakers in the
experimental condition. For the image stream, all the talk-
ing faces in the videos are aligned based on key-points (eyes
and nose) of the extracted landmarks using [18] at the sam-
pling rate of 25FPS, and then resize to 128 × 128. As for
audio data, each audio segment corresponds to 280ms au-
dio. We extract MFCC at the window size of 10ms and
use center image frame as the paired image data. Similar
to [3, 27], we remove the ﬁrst coefﬁcient from the original
MFCC vector, and eventually yield a 28×12 MFCC feature
for each audio chunk.
Implementation Details
Our network is implemented
using Pytorch 0.4 library. We adopt Adam optimizer during
training with the ﬁxed learning rate of 2 × 10-4. We ini-
tialize all network layers using random normalization with
mean=0.0, std=0.2. All models are trained and tested on a
single NVIDIA GTX 1080Ti. During the training, the AT-
net converges after 3 hours and the VG-net is stable after

7836

Ground 
Truth

VoxCeleb
Dataset

LRW
Dataset

Caetoon
Samples

Real-
 world
Sample

Figure 5: The outputs of ATVGnet. The inputs are one real-world audio sequence and different example identity images range
from real-world people to cartoon characters. The ﬁrst row is ground truth images paired with the given audio sequence. We
mark the different sources of the identity image on the left side. From this ﬁgure, we can ﬁnd that the lip movements of our
synthesized frames (e.g., the green box in the last row) are well-synchronized with the ground truth (red box in ﬁrst row).
Meanwhile, the attention (middle row of the green box) accurately indicates where need to move and the motion (last row of
the green box) indicates what the dynamics look like (e.g. white pixels for teeth and red pixels for lips).

Method

Real time ATVGnet(our) Chung et al.[3] Zhou et al.[12] Wiles et al.[35]

Inference time (FPS)

30

34.53

19.10

10.00

10.53

Table 1: The inference time of difference models. We use frame rate (FPS) to measure the time.

24 hours. Table 1 shows the generation time during infer-
ence stage. We can ﬁnd that our inference time can achieve
around 34.5 frames per second (FPS), which is much faster
than [34, 12, 3] and slightly faster than real time (30 FPS).

4.2. Results

Image results are illustrated in Fig. 5 and Fig. 7. To eval-
uate the quality of the synthesized video frames, we com-
pute PSNR and SSIM [33]. To evaluate whether the syn-
thesized video contains accurate lip movements that cor-
respond to the input audio, we adopt the evaluation ma-
trix Landmarks Distance (LMD) proposed in [1]. We
compare our model with other three state-of-the-art meth-

ods [1, 3, 35]. All of them are trained on LRW dataset while
Chung et al. [3] require extra VGG-M network pretrained
on VGG Face dataset [25] and Wilels et al. [35] need extra
MFCC feature extractor pretrained by [5]. The quantita-
tive results are illustrated in Table 2. The Baseline model is
a straightforward model without any features (e.g., DMA,
MMCRNN, DAL and RD explained in Sec. 4.4) as men-
tioned in Sec. 3. The model ATVG-ND has the same net-
work structure as ATVGnet. But it is trained end-to-end
without the decoupled training strategy (see Sec. 3.1). We
can ﬁnd that our ATVGnet achieves the best results both in
image quality (SSIM, PSNR) and the correctness of audio-
visual synchronization (LMD).

7837

c

i
t
s

i
l

a
e
r
 
f
o
e
r
o
c
s

 

 

e
h
T

 

n
o
i
t
a
z
i
n
o
r
h
c
n
y
s
 
f
o
e
r
o
c
s

 

 

e
h
T

T

otal 

L

R

T

C

V

o

x

W

 

D 

G

O

C

RID 
ele
b 

th
er 

T

otal 

L

R

T

V

C

W

 

D 

G

O

o

x

C

RID 
ele
b 

th
er 

Figure 6: Statistics of user studies. The y-axis is the per-
centage of votes and the x-axis is different data sources
(e.g., total means all the video samples, Other means sam-
pled videos from YouTube.) The left histogram is the rating
on authenticity. The right histogram is the rating on syn-
chronization between facial movements and audio.

Method

LRW

GRID

Chen [1]
Wiles [35]
Chung [3]

Baseline
ATVG-ND
ATVGnet

LMD SSIM PSNR LMD SSIM PSNR
29.33
1.73 0.73
29.39
1.60 0.75
1.63 0.77
29.87

29.65 1.59 0.76
29.82 1.48 0.80
29.91 1.44 0.79

1.71 0.72
1.35 0.78
1.37 0.81

28.95 1.82 0.77
30.27 1.34 0.79
30.91 1.29 0.83

28.78
30.51
32.15

Table 2: Quantitative results of different methods on LRW
dataset and GRID dataset. Our models mentioned in this
table are trained from scratch. We bold each leading score.

4.3. User Studies

Our goal is to generate realistic videos based on the au-
dio information. The evaluation in 4.2 can only evaluate
the quality in a single frame style. To evaluate the perfor-
mance in a video level, we conduct thoughtful user studies
in this section. Human subjects evaluation (see Fig. 6) is
conducted to investigate the visual qualities of our gener-
ated results compared with Chung et al. [3] and Zhou et
al. [12]. The ground truth videos are selected from different
sources: we randomly select samples from the testing set
of LRW [5], VoxCeleb [24], TCD [13], GRID [6] and real-
world samples from YouTube (in total 38 videos). Three
methods are evaluated w.r.t. two different criteria: whether
participants could regard the generated talking faces as re-
alistic and whether the generated talking faces temporally
sync with the corresponding audio. We shufﬂe all the sam-
ple videos and the participants are not aware of the mapping
between videos to methods. They are asked to score the im-

Method

LRW

GRID

ATVGnet
w/o DMA
w/o MM-
CRNN
w/o DAL
w/o RD
Baseline
ATVG-P

LMD SSIM PSNR LMD SSIM PSNR
33.45 0.70 0.89 33.84
0.80 0.86
0.98 0.83
30.22 1.10 0.84 29.90
30.61 0.81 0.86 32.68
1.03 0.80

0.86 0.86
0.82 0.84
1.27 0.81
0.90 0.84

31.35 0.76 0.87 33.11
32.84 0.73 0.88 33.25
29.55 1.17 0.80 29.45
30.45 0.75 0.87 31.78

Table 3: Ablation studies on the LRW dataset and the GRID
dataset. We remove each feature at a time. We bold the
highest scores.

ages on a scale of 0 (worst) to 10 (best). There are overall
10 participants involved, and the results are summed over
persons and video time steps.

According to the ratings from Fig. 6, we can ﬁnd that
our method outperforms other two methods in terms of the
extent of synchronization and authenticity. More speciﬁ-
cally, our model achieves the best results on all datasets in
terms of lip synchronization with audio input. As for image
authenticity, our model achieves the highest score the on
most of the datasets but slightly lower than Chung et al. [3]
on the VoxCeleb testing set. We attribute this to the audio
noise (e.g. background music) in the test samples.

4.4. Ablation Studies

We conduct ablation experiments to study the con-
tributions of the four components introduced in Sec. 3:
Dynamic Motion & Attention (DMA), Multi-Modal-crnn
(MMCRNN), Dynamically Adjustable Loss (DAL) and
Recreational Discriminator (RD). The ablation studies are
conducted on both LRW dataset and GRID dataset. Results
are shown in Table 3. Here we follow the protocols men-
tioned in Sec. 4.1. We test each model using ground truth
landmarks rather than fake landmarks generated by AT-net,
so that we can eliminate the errors caused by uncorrelated
noise and focus on each component.

As shown in Table 3, each component contributes to
the full model. We can ﬁnd that MMCRNN and DMA
are critical to our full model. We attribute this to the bet-
ter ability of generating smooth transactions between adja-
cent frames. The ATVG-P model has the same structure as
ATVGnet but conditioned on the last fake frame ˆvt−1 rather
than the example frame ip in Eq. 8 in Sec. 3.2. We suppose
it could yield better performance. However, the error am-
pliﬁes quickly through time until it overwhelms the visual
information from example frame, which leads to a trivial
solution that αt = 0n×n and decreases the performance.

We investigate the model performance w.r.t.

the gen-

7838

Ground
 Truth

 Our 

ATVGnet

Chung 
et al.[3]

Zhou 
et al. [12]

Ground
 Truth

 Our 

ATVGnet

Chung 
et al. [3]

Zhou

 et al. [12]

Figure 7: Qualitative results produced by ATVGnet, Chung et al. [3] and Zhou et al. [12] on samples from LRW and VoxCeleb
dataset. We can observe from it that our mouth opening is closer to ground truth compared with the other two methods. It is
worthwhile to mention that the second sample is recorded outside with loud background noise.

e
r
o
c
s
 

n
o

i
t

i

a
z
n
o
r
h
c
n
y
S

(a)

Realistic score

Ground truth frame

Synchronization score

The standard deviations of Gaussian noise
(b)
Ground Truth

Generated frames

Frame 
ID #1

Frame 
ID #5

Frame 
ID #8

e
r
o
c
s
 
c
i
t
s

i
l

a
e
R

Yaw angle

-15° 

0° 

5° 

15° 

20° 

25° 

45° 

65° 

Figure 8: The trend of image quality w.r.t.
(a) the land-
marks (top) and (b) the poses (bottom). Please zoom in on
a computer screen.

erated landmarks accuracy and different pose angles ( see
Fig. 8). We add Gaussian noises with different standard de-
viations to the generated landmarks during inference and
conduct user study on the generated videos. The image
quality drops (see Fig. 8(a)) if we increase the standard de-
viation. This phenomenon also indicates that our AT-net can
output promising intermediate landmarks. To investigate
the pose effects, we test different example images (different

pose angles) with the same audio. The results in Fig. 8(b)
demonstrate the robustness of our method w.r.t. the differ-
ent pose angles.

5. Conclusion and Discussion

In this paper, we present a cascade talking face video
generation approach utilizing facial landmarks as interme-
diate high-level representations to bridge the gap between
two different modalities. We propose a novel Multi-Modal
Convolutional-RNN structure, which considers the correla-
tion between adjacent frames in the generation stage. Mean-
while, we propose two novel components: dynamically ad-
justable loss and regression-based discriminator.
In our
perspective, these two techniques are general that could
be adopted in other tasks (e.g., human body generation
and facial expression generation) in the future. Our ﬁnal
model ATVGnet achieves the best performance on several
popular datasets in both qualitative and quantitative com-
parisons. For future work, applying other techniques to
enable our network to generate unconscious head move-
ments/expressions could be an interesting topic, which has
been bypassed in our current approach.
Acknowledgement.
This work was supported in part
by NSF IIS 1741472, IIS 1813709, and the University of
Rochester AR/VR Pilot Award. This article solely reﬂects
the opinions and conclusions of its authors and not the fund-
ing agents.

7839

References

[1] L. Chen, Z. Li, R. K. Maddox, Z. Duan, and C. Xu. Lip
movements generation at a glance.
In Computer Vision -
ECCV 2018 - 15th European Conference, Munich, Germany,
September 8-14, 2018, Proceedings, Part VII, pages 538–
553, 2018.

[2] L. Chen, S. Srivastava, Z. Duan, and C. Xu. Deep cross-
modal audio-visual generation. In Proceedings of the on The-
matic Workshops of ACM Multimedia 2017, Mountain View,
CA, USA, October 23 - 27, 2017, pages 349–357, 2017.

[3] J. S. Chung, A. Jamaludin, and A. Zisserman. You said that?
In British Machine Vision Conference 2017, BMVC 2017,
London, UK, September 4-7, 2017, 2017.

[4] J. S. Chung and A. Zisserman. Lip reading in the wild. In
Computer Vision - ACCV 2016 - 13th Asian Conference on
Computer Vision, Taipei, Taiwan, November 20-24, 2016,
Revised Selected Papers, Part II, pages 87–103, 2016.

[5] J. S. Chung and A. Zisserman. Out of time: Automated lip
sync in the wild. In Computer Vision - ACCV 2016 Work-
shops - ACCV 2016 International Workshops, Taipei, Tai-
wan, November 20-24, 2016, Revised Selected Papers, Part
II, pages 251–263, 2016.

[6] M. Cooke, J. Barker, S. Cunningham, and X. Shao. An
audio-visual corpus for speech perception and automatic
speech recognition. The Journal of the Acoustical Society
of America, 2006.

[7] X. Di, V. A. Sindagi, and V. M. Patel. Gp-gan: Gender
preserving gan for synthesizing faces from landmarks.
In
2018 24th International Conference on Pattern Recognition
(ICPR), pages 1079–1084, Aug 2018.

[8] S. E. Eskimez, R. K. Maddox, C. Xu, and Z. Duan. Generat-
ing talking face landmarks from speech. In Latent Variable
Analysis and Signal Separation - 14th International Confer-
ence, LVA/ICA 2018, Guildford, UK, July 2-5, 2018, Pro-
ceedings, pages 372–381, 2018.

[9] B. Fan, L. Wang, F. K. Soong, and L. Xie. Photo-real talking
head with deep bidirectional LSTM. In 2015 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, ICASSP 2015, South Brisbane, Queensland, Australia,
April 19-24, 2015, pages 4884–4888, 2015.

[10] Y. Feng, F. Wu, X. Shao, Y. Wang, and X. Zhou. Joint 3d
face reconstruction and dense alignment with position map
regression network. In Computer Vision - ECCV 2018 - 15th
European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part XIV, pages 557–574, 2018.

[11] P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. Varanasi,
P. P´erez, and C. Theobalt. Vdub: Modifying face video of
actors for plausible visual alignment to a dubbed audio track.
Comput. Graph. Forum, 34(2):193–204, 2015.

[12] Z. L. P. L. X. W. Hang Zhou, Yu Liu. Talking face generation
by adversarially disentangled audio-visual representation. In
AAAI Conference on Artiﬁcial Intelligence (AAAI), 2019.

[13] N. Harte and E. Gillen. TCD-TIMIT: an audio-visual corpus
of continuous speech. IEEE Trans. Multimedia, 17(5):603–
615, 2015.

[14] S. Hong, X. Yan, T. S. Huang, and H. Lee. Learning hi-
erarchical semantic image manipulation through structured

In S. Bengio, H. Wallach, H. Larochelle,
representations.
K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems 31, pages
2708–2718. Curran Associates, Inc., 2018.

[15] S. Hong, D. Yang, J. Choi, and H. Lee.

Inferring seman-
tic layout for hierarchical text-to-image synthesis.
In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.

[16] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
In European

real-time style transfer and super-resolution.
Conference on Computer Vision, 2016.

[17] T. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen.
Audio-driven facial animation by joint end-to-end learning
of pose and emotion. ACM Trans. Graph., 36(4):94:1–94:12,
2017.

[18] D. E. King. Dlib-ml: A machine learning toolkit. Journal of

Machine Learning Research, 2009.

[19] Y. Li, M. R. Min, D. Shen, D. E. Carlson, and L. Carin. Video
generation from text.
In Proceedings of the Thirty-Second
AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the
30th innovative Applications of Artiﬁcial Intelligence (IAAI-
18), and the 8th AAAI Symposium on Educational Advances
in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana,
USA, February 2-7, 2018, pages 7065–7072, 2018.

[20] T. Luong, H. Pham, and C. D. Manning. Effective ap-
proaches to attention-based neural machine translation.
In
Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2015, Lisbon, Por-
tugal, September 17-21, 2015, pages 1412–1421, 2015.

[21] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. V.
Gool. Pose guided person image generation. In Advances
in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, 4-
9 December 2017, Long Beach, CA, USA, pages 405–415,
2017.

[22] S. Ma, J. Fu, C. Wen Chen, and T. Mei. Da-gan: Instance-
level image translation by deep attention generative adversar-
ial networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[23] T. Marwah, G. Mittal, and V. N. Balasubramanian. Attentive
semantic video generation using captions. In IEEE Interna-
tional Conference on Computer Vision, ICCV 2017, Venice,
Italy, October 22-29, 2017, pages 1435–1443, 2017.

[24] A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb:
In INTER-

a large-scale speaker identiﬁcation dataset.
SPEECH, 2017.

[25] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face
recognition.
In Proceedings of the British Machine Vision
Conference 2015, BMVC 2015, Swansea, UK, September 7-
10, 2015, pages 41.1–41.12, 2015.

[26] A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and
F. Moreno-Noguer. Ganimation: Anatomically-aware fa-
cial animation from a single image.
In Computer Vision -
ECCV 2018 - 15th European Conference, Munich, Germany,
September 8-14, 2018, Proceedings, Part X, pages 835–851,
2018.

7840

[27] Y. Song, J. Zhu, X. Wang, and H. Qi. Talking face gener-
ation by conditional recurrent adversarial network. CoRR,
abs/1804.04786, 2018.

[28] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-
learning lip sync from

Shlizerman. Synthesizing obama:
audio. ACM Trans. Graph., 36(4):95:1–95:13, 2017.

[29] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-
learning lip sync from

Shlizerman. Synthesizing obama:
audio. ACM Trans. Graph., 36(4):95:1–95:13, 2017.

[30] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. Decom-
posing motion and content for natural video sequence pre-
diction. ICLR, 2017.

[31] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee.
Learning to generate long-term future via hierarchical pre-
diction. In Proceedings of the 34th International Conference
on Machine Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017, pages 3560–3569, 2017.

[32] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating
videos with scene dynamics. In Advances in Neural Infor-
mation Processing Systems 29: Annual Conference on Neu-
ral Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pages 613–621, 2016.

[33] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Trans. Image Processing, 2004.

[34] N. Wichers, R. Villegas, D. Erhan, and H. Lee. Hierarchi-
cal long-term video prediction without supervision. In Pro-
ceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Swe-
den, July 10-15, 2018, pages 6033–6041, 2018.

[35] O. Wiles, A. S. Koepke, and A. Zisserman. X2face: A net-
work for controlling face generation using images, audio,
and pose codes.
In Computer Vision - ECCV 2018 - 15th
European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part XIII, pages 690–706, 2018.

[36] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang,
and X. He. Attngan: Fine-grained text to image genera-
tion with attentional generative adversarial networks. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.

[37] H. Zhang, I. J. Goodfellow, D. N. Metaxas, and A. Odena.
CoRR,

Self-attention generative adversarial networks.
abs/1805.08318, 2018.

7841

