Combining 3D Morphable Models: A Large scale Face-and-Head Model

Stylianos Ploumpis1

3

,

Haoyang Wang1

Nick Pears2

William A. P. Smith2

Stefanos Zafeiriou1

3

,

1Imperial College London, UK

2University of York, UK

3FaceSoft.io

1{s.ploumpis,haoyang.wang15,s.zafeiriou}@imperial.ac.uk

2{nick.pears,william.smith}@york.ac.uk

Global Model

Black Model

Asian Model

Age under 7 Model

2

1

+

-

2

1

+

-

2

1

+

-

2

1

+

-

Mean

Mean

Mean

Mean

Figure 1. The bespoke Combined Face & Head Models. Visualisation of the ﬁrst two shape components along with the mean head shape.

Abstract

1. Introduction

Three-dimensional Morphable Models (3DMMs) are
powerful statistical tools for representing the 3D surfaces
of an object class. In this context, we identify an interesting
question that has previously not received research attention:
is it possible to combine two or more 3DMMs that (a) are
built using different templates that perhaps only partly over-
lap, (b) have different representation capabilities and (c)
are built from different datasets that may not be publicly-
available? In answering this question, we make two con-
tributions. First, we propose two methods for solving this
problem: i. use a regressor to complete missing parts of
one model using the other, ii. use the Gaussian Process
framework to blend covariance matrices from multiple mod-
els. Second, as an example application of our approach,
we build a new face-and-head shape model that combines
the variability and facial detail of the LSFM with the full
head modelling of the LYHM. The resulting combined shape
model achieves state-of-the-art performance and outper-
forms existing head models by a large margin. Finally, as an
application experiment, we reconstruct full head represen-
tations from single, unconstrained images by utilizing our
proposed large-scale model in conjunction with the Face-
Warehouse blendshapes for handling expressions.

Due to their ability of inferring and representing 3D sur-
faces, 3D Morphable Models (3DMMs) have many appli-
cations in computer vision, computer graphics, biometrics,
and medical imaging [4, 15, 1, 26]. Many registered raw
3D images (‘scans’) are required for correctly training a
3DMM, which comes at a very large cost of manual labour
for collecting and annotating such images with meta data.
Sometimes, only the resulting 3DMMs become available to
the research community, and not the raw 3D images. This is
particularly true of 3D images of the human face/head, due
to increasingly stringent data protection regulations. Fur-
thermore, even if 3DMMs have overlapping parts, their res-
olution and ability to express detailed shape variation may
be quite different, and we may wish to capture the best prop-
erties of multiple 3DMMs within a single model. However,
it is currently extremely difﬁcult to combine and enrich ex-
isting 3DMMs with different attributes that describe distinct
parts of an object without such raw data. Therefore, in this
paper, we present a general approach that can be employed
to combine 3DMMs from different parts of an object class
into a single 3DMM. Due to their widespread use in the
computer vision community, we fuse 3DMMs of the human
face and the full human head, as our exemplar, thus creating
the ﬁrst combined, large-scale, full-head morphable model.
The technique is readily extensible to incorporate detailed

110934

models of the ear [10] and the body, and indeed is applica-
ble to any object class well-described by 3DMMs.

More speciﬁcally, although there have been many mod-
els of the human face both in terms of identity [17, 30, 28]
and expression [8, 29], very few deal with the complete
head anatomy [11]. Building a high-quality, large-scale sta-
tistical model that describes the anatomy of the full human
head paves directions across numerous disciplines. First, it
will assist craniofacial clinicians in diagnosis, surgical plan-
ning, and assessment. Second, generating proportionally
correct head models based on the geometry of the face will
aid computer graphics designers to create realistic avatar-
like representations. Finally, a head model will give oppor-
tunities that aim at reconstructing a full head representation
from data-deﬁcient sources, such as 2D images.

Our key contributions are: (i) a methodology that aims to
fuse shape-based 3DMMs, using the human face and head
as an exemplar.
(Note that the texture component of the
3DMM is out-of-scope of this paper and the subject of our
future work.) In particular, we propose both a regression
method based on latent shape parameters, and a covari-
ance combination approach, utilized in a Gaussian process
framework, (ii) a combined large-scale statistical model of
the human head in terms of ethnicity, age and gender that
is signiﬁcantly more accurate than any other existing head
morphable model - we make this publicly-available for the
beneﬁt of the research community, including versions with
and without eyes and teeth, and (iii) an application experi-
ment in which we utilize the combined 3DMM to perform
full head reconstruction from unconstrained single images,
also utilizing the FaceWarehouse blendshapes to handle fa-
cial expressions.

2. Face and head model literature

The ﬁrst 3DMM was proposed by Blanz and Vetter [3].
They were the ﬁrst to to recognize the generative capabili-
ties of a 3DMM and they proposed a technique to capture
the variations of 3D faces. Only 200 scans were used to
build the model (100 male and 100 female) where dense
correspondences were computed based on optical ﬂow that
depends on an energy function that describes both the shape
and texture. The Basel Face Model (BFM) is the most
widely-used and well-known 3DMM, which was built by
Paysan et al. [23] and utilizes a better registration method
than the original Blanz-Vetter 3DMM. They use a known
template mesh in which all the vertices have known posi-
tions and then they register it to the training scans by utiliz-
ing an optimal step Non-rigid Iterative Closest Point algo-
rithm (NICP) [2]. Standard PCA was employed as a dimen-
sionality reduction technique to construct their model.

Recently, Booth et al. [7] built a Large-scale Face Model
(LSFM) by utilizing nearly 10, 000 face scans. The model is
constructed by applying a weighted version of the optimal-

step NICP algorithm [13], followed by a Generalized Pro-
crustes Analysis (GPA) and standard PCA. Due to the large
number of facial scans, a robust automated procedure was
carried out including 3D landmark localization and error
pruning of badly registered scans. This work was the ﬁrst
to introduce bespoke models in terms of age, gender and
ethnicity, and is the most information-rich 3DMM of face
shapes in neutral expression produced to date.

Li et al [20] used a total of 3, 800 head scans from the US
and European CEASAR body scan database [25] to build a
statistical model of the entire head. The aim of this work fo-
cuses mainly on the temporal registration of 3D scans rather
than on the topology of the head area. The data consists of
full body scans and the resolution in which the head topol-
ogy was recorded in is insufﬁcient to depict correctly the
shape of each individual human head. In addition, the tem-
plate used for registration in this method is extremely sparse
with only 5, 000 vertices which makes it difﬁcult to accu-
rately represent the entire head. Moreover, the registration
process incorporates coupling weights for the back of head
and the back of the neck, which constrains drastically the
actual statistical variation of the entire head area. An exten-
sion of this work is proposed in [24] in which a non-linear
model is constructed using convolution mesh autoencoders
focusing on facial expressions, but still it lacks the statis-
tical variation of the full cranium. Similarly, in the work
of Hu and Saito [16], a full head model is created from
single images mainly for real-time rendering. The work
aims at creating a realistic avatar model which includes 3D
hair estimation. The head topology is considered to be un-
changed for all subjects and only the face part of the head is
a statistically-correct representation.

The most accurate craniofacial 3DMM of the human
head both in terms of shape and texture, is the Liverpool-
York Head model (LYHM) [11]. In this work, global cran-
iofacial 3DMMs and demographic sub-population 3DMMs
were built from 1,212 distinct identities. Although this work
is the ﬁrst that describes the statistical correlation between
the cranium and the face part, it lacks detail of the facial
characteristics, as the spatial resolution of the facial region
is not signiﬁcantly higher than the cranial region.
In ef-
fect, the variance of the cranial and neck areas dominates
that of the facial region in the PCA parameterization. Also,
although the model describes how the cranium is affected
given the age of the subject, it is biased in terms of ethnic-
ity, due to the lack of ethnic diversity in the dataset.

3. Face and head shape combination

In this section, we propose two methods to combine the
LSFM face model with the LYHM full head model. The
ﬁrst approach, utilizes the latent PCA parameters and solves
a linear least squares problem to approximate the full head
shape, whereas the second constructs a combined covari-

10935

ance matrix that is later utilized as a kernel in a Gaussian
Process Morphable Model (GPMM) [22].

3.1. Regression modelling

Figure 2 illustrates the three-stage regression modeling
pipeline, which comprises 1) regression matrix calculation,
2) model combination and 3) full head model registration
followed by PCA modeling. Each stage is now described.

For stage 1, let us denote the 3D mesh (shape) of an ob-

ject with N points as a 3N × 1 vector

S = [xT

1 . . . xT

N ]T = [x1, y1, z1, . . . xN , yN , zN ]T

(1)

The LYHM is a PCA generative head model with Nh points,
described by an orthonormal basis after keeping the ﬁrst nh
principal components Uh ∈ R3Nh×nh and the associated
λh eigenvalues. This model can be used to generate novel
3D head instances as follows:

Sh(ph) = mh + Uhph

(2)

where ph = hph1 . . . phnhiT

are the nh shape parame-
ters. Similarly the LSFM face model with Nf number of
points, is described by a corresponding orthonormal basis
after keeping the nf principal components Uf ∈ R3Nf ×nf
and the associated λf eigenvalues. The model generates
novel 3D faces instances by:

Sf (pf ) = mf + Uf pf

(3)

where pf = hpf1 . . . pfnf iT

are the nf shape parameters.
In order to combine the two models, we synthesize data
directly from the latent eigenspace of the head model (Uh)
by drawing random samples from a Gaussian distribution
deﬁned by the principal eigenvalues of the head model. The
standard deviation for each of the distributions is equal to
the square root of the eigenvalue. In that way we produce
randomly nr distinct shape parameters.

After generating the random full head instances we apply
non-rigid registration (NICP) [13] between the head meshes
and the cropped mean face of the LSFM face model. We
perform this task in each one of the nr meshes in order to
get the facial part of the full head instance and describe it
in terms of the LSFM topology. Once we acquire those reg-
istered meshes we project them to the LSFM subspace and
we retrieve the corresponding shape parameters. Thus, for
each one of the randomly produced head instances, we have
a pair of shape parameters (ph, pf ) corresponding to the
full head representation and to the facial area respectively.
By utilizing those pairs we construct a matrix Ch ∈
Rnh×nr where we stack all the head shape parameters and a
matrix Cf ∈ Rnf ×nr where we stack the face shape param-
eters from the LSFM model. We would like to ﬁnd a matrix

Wh,f ∈ Rnh×nf to describe the mapping from the LSFM
face shape parameters pf to the corresponding LYHM full
head shape parameters ph. We solve this by formulating a
linear least square problem that minimizes:

kCh − Wh,f Cf k2

(4)

By utilizing the normal equation, the solution of Eq. 4 is
readily given by:

Wh,f = ChCT

f (cid:0)Cf CT

f (cid:1)−1

(5)

f (cid:17)−1

f (cid:16)Cf CT

where CT
is the right pseudo-inverse of Cf .
Given a 3D face instance Sf , we derive the 3D shape of the
full head, Sh, as follows:

Sh = mh + UhWh,f UT

f (Sf − mf )

(6)

In this way we can map and predict the shape of the cranium
region for any given face shape in terms of LYHM topology.
In stage 2 (Fig. 2), we employ the large MeIn3D database
[7] which includes nearly 10, 000 3D face images, and we
utilize the Wh,f regression matrix to construct new full
head shapes that we later combine with the real facial scans.
We achieve this by discarding the facial region of the the
full head instance which has less detailed information and
we replace it with the registered LSFM face of the MeIn3D
scan.
In order to create a unique instance we merge the
meshes together by applying a NICP framework, where we
deform only the outer parts of the facial mesh to match with
the cranium angle and shape so that the result is a smooth
combination of the two meshes. Following the formulation
in [13], this is accomplished by introducing higher stiffness
weights in the inner mesh (lower on the outside) while we
apply the NICP algorithm. To compute those weights we
measure the Euclidean distance of a given point from the
nose tip of the mesh and we assign a relative weight to that
point. The bigger the distance from the nose tip, the smaller
the weight of the point.

One of the drawbacks of the LYHM is the arbitrary neck
circumference, where the neck tends to get broader when
the general shape of the head increases. In stage 3 (Fig. 2),
we aim at excluding this factor from our ﬁnal head model
by applying a ﬁnal NICP step between the merged meshes
and our head template St. We utilized the same framework
as before with the point-weighted strategy where we assign
weights to the points based on their Euclidean distance from
the center of the head mass. This helps us avoid any in-
consistencies of the neck area that might appear from the
regression scheme. For the area around the ear, we have in-
troduced 50 additional landmarks to control the registration
and preserve the general shape of the ear area.

After implementing the aforementioned pipeline for
each one of the 10, 000 meshes, we perform PCA on the

10936

1) Regression Matrix Calculation

2) Model combination

3) Full head model registration

nr random 

LYHM head 

meshes

Shape 

Parameters

NICP with 
mean face 
of LSFM

Regressor 
Formulation

Shape 

Parameters

nr registered 
LSFM face 

meshes

10,000 faces of 

MeIn3D

10,000 Predicted 

Heads

Removing Low 

detail face

Merge face & head 
meshes with NICP

Head Template St

NICP head 

registration with 
merged meshes 

Statistical PCA 

Modeling

Per vertex template 

weights

Figure 2. The regression modeling pipeline. 1) The left part illustrates the matrix formulation from the original LYHM head model; 2) the
central part demonstrates how we utilize the MeIn3D database to produce highly-detailed head shapes; 3) the ﬁnal part on the right depicts
the registration framework along with the per-vertex template weights and the statistical modeling.

points of the mesh and we acquire a new generative full
head model that exhibits more detail in the face area in com-
bination with bespoke head shapes.

3.2. Gaussian process modeling

Gaussian processes for model combination is a less com-
plicated and more robust technique that does not generate
irregular head shapes due to poor regression values.

The concept of Gaussian Process Morphable Models
(GPMMs) was recently introduced in [22, 14, 18]. The
main contribution of GPMMs is the generalization of clas-
sic Point Distribution Models (such as are constructed using
PCA), with the help of Gaussian processes. A shape is mod-
eled as a deformation u from the reference shape SR i.e. a
shape can be represented as:

S = {x + u(x)|x ∈ SR}

(7)

where u is a deformation function u : Ω → R3 with
Ω ⊇ SR. The deformations are modeled as a Gaussian
process u ∼ GP (µ, k). Where µ : Ω → R3 is the mean de-
formation and k : Ω × Ω → R3×3 is a covariance function
or kernel.

The Gaussian process model is capable of operation out-
side of the space of valid face shapes. This depends highly
on the kernels chosen for this task.
In the classic ap-
proaches, the deformation function is learned through a se-
ries of typical example surfaces S1, . . . , Sn where a set of
deformation ﬁelds is learned {u1, . . . , un}, ui(x) : Ω →
Rd where ui(x) denotes the deformation ﬁeld that maps a
point x on the reference shape to the corresponding point
on the ith-training surface.

A Gaussian process GP (µP DM , kP DM ) that models
this characteristic deformations is obtained by estimating

the empirical mean:

µP DM (x) =

1
n

n

X

i=1

ui(x)

and the covariance function:

kP DM (x, y) =

1

1 − n

n

X

i=1

(ui(x) − µP DM (x))

(ui(y) − µP DM (y))T

(8)

(9)

This kernel is deﬁned as the empirical/sample covariance
kernel. This speciﬁc Gaussian process model is a contin-
uous analog to a PCA model and it operates in the facial
deformation spectrum. For each one of the models (LYHM,
LSFM), we know the principal orthonormal basis and the
eigenvalues. Hence the covariance matrix for each model is
deﬁned:

Kh = UhΛhUT
h
Kf = Uf Λf UT
f

(10)

where Kh ∈ R3Nh×3Nh and Kf ∈ R3Nf ×3Nf are the
covariance matrices, and the Λh ∈ R3nh×3nh and Λf ∈
Rnf ×nf are diagonal matrices with the eigenvalues in their
the main diagonal of the head and face model respectively.
We aim at constructing a universal covariance matrix
KU ∈ R3NU ×3NU that accommodates the high detailed fa-
cial properties of the LSFM and the head distribution from
the LYHM. We keep, as a reference, the mean of the head
model and we non-rigidly register the mean face of the
LSFM. Both PCA models must be in the same scale space
for this method to work, which was not necessary for the

10937

regression method. Similarly, we register our head template
St by utilizing the same pipeline as before for full head reg-
istration, which is going to be used as the reference mesh
for the new joined covariance matrix.

For each point pair i, j in St, there exists a local covari-
ance matrix Ki,j
U ∈ R3×3. In order to calculate its value,
we begin by projecting the points onto the mean head mesh.
If both points lie outside the face area that the registered
mean mesh of LSFM covers, we identify their exact lo-
cation in the mean head mesh in terms of barycentric co-
ordinates (ci
3) for
the jth point with respect to their corresponding triangles
ti = [vT

3) for the ith point and (cj

3 ]T , tj = [kT

2 , vT

1 , vT

1 , kT

2 , kT

2, cj

1, cj

3 ]T .

2, ci

1, ci

Each vertex pair (v, k) in between the triangles, has an
individual covariance matrix Kv,k
h ⊇
Kh. Therefore, we blend those local vertex-covariance ma-
trices to acquire our ﬁnal local Ki,j

h ∈ R3×3 with Kv,k

U as follows:

U = P3
v=1 P3
Ki,j
P3

v=1 P3

k=1 wi,j

v,kKv,k

h

k=1 wi,j

v,k

(11)

v+cj

v,k = ci

where wi,j
is a weighting scheme based on the
barycentric coordinates of the (i, j) points. An illustration
of the aforementioned methodology can be seen in Figure 3.

2

k

Mean Head LYHM

Mean face LSFM

Template St

NICP 

registration on 

the LYHM 
domain  

k2

c2

v2

k1

tj

c3

k3

j
c1

v1

c1

i

c3

c2

ti

v3

Figure 3. A graphical representation of the non-rigid registration
of all mean meshes along with our head template St and the cal-
culation of the local covariance matrix Ki,j
U based on the locations
of the ith and jth points.

In the case where the points lie in the face area, we ini-
tially repeat the same procedure by projecting and calculat-
ing a blended covariance matrix Ki,j
f given the mean face
mesh of LSFM, followed by a blended covariance matrix
Ki,j
h calculated given the mean head mesh of LYHM. We
formulate the ﬁnal local covariance matrix as:

+

+

2

where ρi,j = ρi+ρj
is a normalized weight, based on the
Euclidean distances (ρi, ρj) of the (i, j) points from the
nose-tip of the registered meshes. We apply this weighting
scheme to smoothly blend the properties of the head and
face model and to avoid the discontinuities that appear on
the borders of the face and head area.

Lastly, when the points belong to different areas i.e. (ith
point on face, jth point on head) we simply follow the ﬁrst
method that exploits just the head covariance matrix Kh,
since the correlation of the face/head shape only exist in
the LYHM. After repeating the aforementioned methodol-
ogy for every point pair in St and calculating the entire
joined covariance matrix KU , we are able to sample new
instances from the Gaussian process morphable model.

3.3. Model Reﬁnement

To reﬁne our model, we begin by exploiting the already
trained GPMM of the previous section. With our head tem-
plate St and the universal covariance matrix KU , we deﬁne
a kernel function:

kU (x, y) = KCP (St,x),CP (St,y)

U

(13)

where x and y are two given points from the domain where
the Gaussian process is deﬁned and the function CP (St, x)
returns the index of the closest point of x on the surface St.
We then deﬁne our GPMM as:

GP U (µU , kU )

(14)

where µU (x) = [0, 0, 0]T . For each scan in the MeIn3D
dataset, we ﬁrst try to reconstruct a full head registra-
tion with our GPMM using Gaussian Process Regression
[22, 14]. Given a set of observed deformations X sub-
ject to Gaussian noise ǫ ∼ N (0, σ2), Gaussian process
regression computes a posterior model GP p(µp, kp) =
posterior(GP, X). The landmark pairs between a refer-
ence mesh and the raw scan deﬁne a set of sparse mappings,
which tells us exactly how the points on the reference mesh
will deform. Any sample from this posterior model will
then have ﬁxed deformations on our observed points i.e. fa-
cial landmarks. The mean µp and covariance kp are com-
puted as:

µp(x) = µ(x) + KX(x)T (KXX + σ2I)−1X

(15)

kp(x, y) = ku(x, y) − KX(x)T (KXX + σ2I)−1KX(y)
(16)

where

KX(x) = (kU (x, xi)), ∀ xi ∈ X

KXX = (kU (xi, xj)), ∀ xi, xj ∈ X

(17)

(18)

For a scan S with landmarks LS = {l1, ...ln}, we ﬁrst
compute a posterior model based on the sparse deforma-
tions deﬁned by the landmarks:

Ki,j

U = ρi,j Ki,j

h + (1 − ρi,j)Ki,j

f

(12)

10938

GP 0

p(µ0

p, k0

p) = posterior(GP U , LS − LSt )

(19)

Model Refinement

ith iteration of ICP

Utilizing the modified               to build the

Raw Scan

mean

+

Sparse GP 
regression

Update
mean

+

Dence GP 
regression

NICP on 

face

Statistical 
Modeling

New Covariance 

Matrix :

Figure 4. The model reﬁnement pipeline. We start with the GP model deﬁned by the universal covariance matrix. For each scan in the
MeIn3D dataset we obtain full head reconstruction with GP Regression using the sparse landmarks and dense ICP algorithm. We then
non-rigidly align the face region of the full head reconstruction to the scan, and build a new sample covariance matrix to update our model.

Repeat GP regression

We then reﬁne the posterior model with Iterative Closest
Point algorithm. More speciﬁcally, at each iteration i we
compute the current regression result as Si
reg = {x +
µi−1
(x)|x ∈ St}, which is the reference shape wrapped
p
with the mean deformation of the posterior model GP i−1
.
We then ﬁnd the closest points Ui for each point in Si
reg on
S, and update our posterior model as:

p

GP i+1

p

(µi+1

p

, ki+1

p

) = posterior(GP 0

p, Ui − Si

reg) (20)

Since the raw scans in the MeIn3D database can be noisy,
we exclude a pair of correspondence (x, U(x)) if U(x) is
on the edge of S or the distance between x and U(x) exceed
a threshold. After the ﬁnal iteration we obtain the regression
result Sreg = {x+µf inal
(x)|x ∈ St}. We then non-rigidly
align the face region of Sreg to the face region of the raw
scan to obtain our ﬁnal reconstruction.

p

In practice, we noticed that the reconstructions often pro-
duce unrealistic head shapes. We therefore modify the co-
variance matrix KU before the Gaussian process regression.
We ﬁrst compute the principal components by decompos-
ing KU , then reconstruct the covariance matrix using Eq.
10 with fewer statistical components. With the full head re-
constructions from the MeIn3D dataset, we then compute a
new sample covariance matrix, and repeat the previous GP
regression process to reﬁne the reconstructions. Finally we
perform PCA on the reﬁned reconstructions to obtain our
ﬁnal reﬁned model.

4. Intrinsic evaluation of CFHM models

We name our combined full head model as the Combined
Face & Head Model (CFHM) and now show its compar-
ative performance. Following common practice, we eval-
uate our CFHM variations compared to LYHM by utiliz-
ing, compactness, generalization and speciﬁcity [12, 9, 5].
For all the subsequent experiments we utilise the original

head scans of [11] from which we have chosen 300 head
meshes that were excluded from the training procedure.
This test set was randomly chosen within demographic con-
strains to ensure ethnic, age and gender diversity. We name
our model variations as: CFHM-reg built by the regres-
sion method, CFHM-GP built by the Gaussian processes
kernels framework and ﬁnally, CFHM-ref built after reﬁne-
ment with Gaussian process regression. Also, we present
bespoke modes in terms of age and ethnicity, constructed
by the Gaussian processes kernels method coupled with re-
ﬁnement.

The top graphs in Figure 5 present the compactness mea-
sures of the CFHM models compared to LYHM. Compact-
ness calculates the percentage of variance of the training
data that is explained by the model, when certain number
of principal components are retained. The models CFHM-
reg, CFHM-GP express higher compactness compared to
the model after the reﬁnement. The compactness ability of
the all proposed methods is far greater than the LYHM as it
can be seen by the graph. Both global and bespoke CFHM
models can be considered sufﬁciently compact.

The center row of Fig. 5 illustrates the generalization
error which demonstrates the ability of the models to rep-
resent novel head shapes that are unseen during training.
To compute the generalization error for a given number of
principal components retained, we compute the per-vertex
Euclidian distance between every sample of the test set and
its corresponding model projection and then take the av-
erage value over all vertices and test samples. All of the
proposed models exhibit far greater generalization capabil-
ity compared to LYHM. The reﬁned model CFHM-ref tends
to generalize better than the other approaches, especially in
the range of 20 to 60 components. Additionally, we plot
the generalization error of the bespoke models against the
CFHM-ref in center Figure 5 (b). In order to derive a cor-
rect generalization measure for the bespoke CFHM-ref, for

10939

every mesh we use its demographic information, we project
it on the subspace of the corresponding bespoke model and
then we compute an overall average error. We observe that
the CFHM-ref mostly outperforms the bespoke generaliza-
tion models, which might be attributed to the fact that many
of the speciﬁc models are trained from smaller cohorts, and
so run out of interesting statistical variance.

Lastly, the bottom graphs of Figure 5 show the speciﬁcity
measures of the introduced models, which evaluate the va-
lidity of synthetic faces generated by a model. We ran-
domly synthesize 5,000 faces from each model for a ﬁxed
number of components and measure how close they are to
the real faces based on a standard per-vertex Euclidean dis-
tance metric. We observe that the model which holds the
best error results is the proposed reﬁned model CFHM-ref.
The LYHM model demonstrates better speciﬁcity error than
the CFHM-reg, CFHM-GP models only in the ﬁrst 20 com-
ponents. Both of the proposed combined models exhibit
steady error measures (≈ 3.8) after keeping components
greater than 20. This is due to the higher compactness that
both combined models demonstrate, which enables them to
maintain certain speciﬁcity error after the 20 components.
For all bespoke models we observe that the speciﬁcity er-
rors attain particularly low values, in the range of 1to 4 mm.
This is evidence that the synthetic faces generated by both
global and bespoke CFHM models are realistic enough.

Our results show that our combination techniques yield
models that are capable of exhibiting improved intrinsic
characteristics compared to the original LYHM head model.

5. Head reconstruction from single images

As an application experiment, we outline a methodology
that enables us to reconstruct the entire head shape from
unconstrained single images. We strictly utilize only one
view/pose for head reconstruction in contrast to [21] where
multiple images were utilized. We achieve this by regress-
ing from a latent space that represents the 3D face and ear
shape to the latent space of the full head models constructed
by the proposed methodologies. We begin by building a
PCA model of the inner face along with 50 landmarks on
each ear as described in [27]. We utilize the 10, 000 head
meshes produced by our proposed methods. After building
the face-ear PCA model, we project each one of the face-ear
examples to get the associated shape parameters pe/f . Sim-
ilarly, we project the full head mesh of the same identity to
the full head PCA model in order to the acquire the latent
shape parameters of the entire head ph. Similarly, as in sec-
tion 3.1, we construct a regression matrix which works as a
mapping from the latent space of the ear/face shape to the
full head representation.

In order to reconstruct the full head shape from 2D im-
ages we begin by ﬁtting a face 3DMM utilizing the In-the-
Wild feature-based texture algorithm proposed in [6]. Af-

(a)

(b)

Figure 5. Characteristics of the CFHM models compared to
LYHM. Top: compactness; Center: generalization; Bottom: speci-
ﬁcity. Left column (a): different methods, Right column (b):
demographic-speciﬁc 3DMMs based on the CFHM-ref model.

terwards, we implement an ear detector and an Active Ap-
pearance Model (AAM) as proposed in [27] to localize the
ear landmarks in the 2D image domain. Since we have ﬁt-
ted a 3DMM in the image space, we already know the cam-
era parameters,i.e., focal length, rotation, translation. To
this effect, we can easily retrieve the ear landmarks in the
3D space by solving an inverse perspective-n-point problem
[19] given the camera parameters and the depth values of
the ﬁtted mesh. We mirror the 3D landmarks with respect to
the z-axis to obtain the missing landmarks of the occluded
ear. After acquiring the facial part and the ear landmarks
we are able to attain the full head representation with the
help of the regression matrix. Since each proposed method
estimates a slightly different head shape for the 10, 000 face
scans, we repeat the aforementioned procedure by building
bespoke regression matrices for each head model. Some
qualitative results can be seen in Figure 6.

We evaluate quantitatively our methodology by render-
ing 30 distinct head scans from our test set in frontal and
side poses varying from 20 to −20 degrees around the y-
axis in order for the ears to be visible in the image space.
We apply our previous procedure, where we ﬁt a 3DMM
face and we detect the ear landmarks in the image plane.
Then for each method we exploit the bespoke regression
matrix to predict the entire head shape. We measure the
per-vertex error between the recovered head shape and the

10940

actual ground-truth head scan by projecting each point of
the ﬁtted mesh to the ground-truth and measuring the Eu-
clidean distance. Fig 7 shows the cumulative error distri-
bution for this experiment, for the four models under test.
Table 1 and 2 report the corresponding Area Under Curve
(AUC) and failure rates for the ﬁtted and the actual ground
truth 3D facial meshes respectively. In both situations, the
LYHM struggles to recover the head shapes. CFHM-reg
and CFHM-GP perform equally, whereas the model after
reﬁnement attains the best results. Finally, Fig. 8 illustrates
regression of the full head shape, when only the face of the
imaged subject is visible.

a)

b)

Figure 7. Accuracy results for head shape estimation, as cumula-
tive error distributions of the normalized dense vertex errors. a)
accuracy results based on the ﬁtted facial meshes to rendered im-
ages, b) accuracy results based on the actual ground truth 3D facial
meshes. Tables 1 and 2 report additional measures.

Method

AUC

Failure Rate (%)

CFHM-ref
CFHM-reg
CFHM-GP
LYHM [11]

0.751
0.693
0.681
0.605

3.64
6.88
7.55
19.21

Table 1. Head shape estimation accuracy results for the ﬁtted facial
meshes of our test set. Metrics are Area Under the Curve (AUC)
and Failure Rate of the Cumulative Error Distributions of Fig. 7.

Method

AUC

Failure Rate (%)

CFHM-ref
CFHM-GP
CFHM-reg
LYHM [11]

0.880
0.844
0.831
0.739

0.62
2.46
1.69
14.10

Table 2. Head shape estimation accuracy results for the actual
ground truth 3D facial meshes of our test set. Metrics are AUC
and Failure Rate of the Cumulative Error Distributions of Fig. 7.

Figure 6. Qualitative results of our in-the-wild 3D head reconstruc-
tion. While the facial texture is reconstructed from the image do-
main, the eyes, the inner mouth and the head texture were created
by an artist for a more realist representation.

6. Conclusion

We presented a pipeline to fuse multiple 3DMMs into
a single 3DMM and used it to combine the LSFM face
model and the LYHM head model. The resulting 3DMM
captures all the desirable properties of both constituent
3DMMs; namely high facial detail of the facial model and
the full cranial shape variations of the head model. The aug-

Figure 8. Regressing the full head when only face (left) is visible.

mented model is capable of representing and reconstructing
any given face/head shape due to the high variation of fa-
cial and head appearances existing in the original models.
We demonstrated that our methodology yielded a statisti-
cal model that is considerably superior to the original con-
stituent models. Finally we illustrated the model’s utility in
full head reconstruction from a single images.

Acknowledgements: S. Ploumpis was suppored by
EPSRC Project (EP/N007743/1) FACER2VM. S.Zafeiriou ac-
knowledges funding from a Google Faculty Award, as well as from
the EPSRC Fellowship DEFORM: Large Scale Shape Analysis of
Deformable Models of Humans (EP/S010203/1).

10941

References

[1] Oswald Aldrian and William AP Smith. Inverse rendering
of faces with a 3d morphable model. IEEE transactions on
pattern analysis and machine intelligence, 35(5):1080–1093,
2013. 1

[2] Brian Amberg, Sami Romdhani, and Thomas Vetter. Opti-
mal step nonrigid icp algorithms for surface registration. In
Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8, 2007. 2

[3] Volker Blanz and Thomas Vetter. A morphable model for the
synthesis of 3d faces. In Proc 26th annual conf on Computer
Graphics and Interactive Techniques, pages 187–194, 1999.
2

[4] Volker Blanz and Thomas Vetter. Face recognition based on
ﬁtting a 3d morphable model. IEEE Transactions on pattern
analysis and machine intelligence, 25(9):1063–1074, 2003.
1

[5] Timo Bolkart and Stefanie Wuhrer. A groupwise multilinear
correspondence optimization for 3d faces.
In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3604–3612, 2015. 6

[6] James Booth,

Epameinondas Antonakos,

Stylianos
Ploumpis, George Trigeorgis, Yannis Panagakis, Stefanos
Zafeiriou, et al. 3d face morphable models in-the-wild. In
Proceedings of the IEEE Conference on ComputerVision
and Pattern Recognition, 2017. 7

[7] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan
Ponniah, and David Dunaway. A 3d morphable model learnt
from 10,000 faces. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5543–
5552, 2016. 2, 3

[8] Alexander M Bronstein, Michael M Bronstein, and Ron
Kimmel. Expression-invariant 3d face recognition.
In in-
ternational conference on Audio-and video-based biometric
person authentication, pages 62–70, 2003. 2

[9] Alan Brunton, Augusto Salazar, Timo Bolkart, and Stefanie
Wuhrer. Review of statistical shape spaces for 3d data with
comparative analysis for human faces. Computer Vision and
Image Understanding, 128:1–17, 2014. 6

[10] Hang Dai, Nick Pears, and William Smith.

A data-
In Automatic
augmented 3d morphable model of the ear.
Face & Gesture Recognition (FG 2018), 2018 13th IEEE In-
ternational Conference on, pages 404–408. IEEE, 2018. 2

[11] Hang Dai, Nick Pears, William Smith, and Christian Dun-
can. A 3d morphable model of craniofacial shape and tex-
ture variation.
In 2017 IEEE International Conference on
Computer Vision (ICCV), pages 3104–3112, 2017. 2, 6, 8

[12] Rhodri Davies, Carole Twining, and Chris Taylor. Statisti-
cal models of shape: Optimisation and evaluation. Springer
Science & Business Media, 2008. 6

[13] Micha¨el De Smet and Luc Van Gool. Optimal regions for
linear model-based 3d face reconstruction. In Asian Confer-
ence on Computer Vision, pages 276–289, 2010. 2, 3

[14] Thomas Gerig, Andreas Morel-Forster, Clemens Blumer,
Bernhard Egger, Marcel Luthi, Sandro Sch¨onborn, and
Thomas Vetter. Morphable face models-an open framework.
In Automatic Face & Gesture Recognition (FG 2018), 2018

13th IEEE International Conference on, pages 75–82, 2018.
4, 5

[15] Guosheng Hu, Fei Yan, Chi-Ho Chan, Weihong Deng,
William Christmas, Josef Kittler, and Neil M Robertson.
Face recognition using a uniﬁed 3d morphable model. In Eu-
ropean Conference on Computer Vision, pages 73–89, 2016.
1

[16] Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jae-
woo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-
Chun Chen, and Hao Li. Avatar digitization from a single
image for real-time rendering. ACM Transactions on Graph-
ics (TOG), 36(6):195, 2017. 2

[17] Patrik Huber, Guosheng Hu, Rafael Tena, Pouria Mortaza-
vian, P Koppen, William J Christmas, Matthias Ratsch, and
Josef Kittler. A multiresolution 3d morphable face model
and ﬁtting framework. In Proceedings of the 11th Interna-
tional Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications, 2016. 2

[18] Paul Koppen, Zhen-Hua Feng, Josef Kittler, Muhammad
Awais, William Christmas, Xiao-Jun Wu, and He-Feng Yin.
Gaussian mixture 3d morphable face model. Pattern Recog-
nition, 74:617–628, 2018. 4

[19] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
Epnp: An accurate o (n) solution to the pnp problem. Inter-
national journal of computer vision, 81(2):155, 2009. 7

[20] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and
Javier Romero. Learning a model of facial shape and expres-
sion from 4d scans. ACM Transactions on Graphics (TOG),
36(6):194, 2017. 2

[21] Shu Liang, Linda G Shapiro, and Ira Kemelmacher-
Shlizerman. Head reconstruction from internet photos.
In
European Conference on Computer Vision, pages 360–374,
2016. 7

[22] Marcel L¨uthi, Thomas Gerig, Christoph Jud, and Thomas
Vetter. Gaussian process morphable models. IEEE transac-
tions on pattern analysis and machine intelligence, 2017. 3,
4, 5

[23] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3d face model for pose and
illumination invariant face recognition. In Advanced video
and signal based surveillance, 2009. AVSS’09. Sixth IEEE
International Conference on, pages 296–301, 2009. 2

[24] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and
Michael J Black. Generating 3d faces using convolutional
mesh autoencoders. arXiv preprint arXiv:1807.10267, 2018.
2

[25] Kathleen M Robinette, Sherri Blackwell, Hein Daanen,
Mark Boehmer, and Scott Fleming. Civilian american and
european surface anthropometry resource (caesar), ﬁnal re-
port. volume 1. summary. Technical report, 2002. 2

[26] Femke CR Staal, Allan JT Ponniah, Freida Angullia, Clifford
Ruff, Maarten J Koudstaal, and David Dunaway. Describ-
ing crouzon and pfeiffer syndrome based on principal com-
ponent analysis. Journal of Cranio-Maxillofacial Surgery,
43(4):528–536, 2015. 1

[27] Yuxiang Zhou and Stefanos Zaferiou. Deformable models
of ears in-the-wild for alignment and recognition. In 2017

10942

12th IEEE International Conference on Automatic Face &
Gesture Recognition (FG 2017), pages 626–633, 2017. 7

[28] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and
Stan Z Li. Face alignment across large poses: A 3d solu-
tion.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 146–155, 2016. 2

[29] Xiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan Z
Li. High-ﬁdelity pose and expression normalization for face
recognition in the wild.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
787–796, 2015. 2

[30] Xiangyu Zhu, Junjie Yan, Dong Yi, Zhen Lei, and Stan Z
Li. Discriminative 3d morphable model ﬁtting. In Automatic
Face and Gesture Recognition (FG), 2015 11th IEEE Inter-
national Conference and Workshops on, volume 1, pages 1–
8, 2015. 2

10943

