H+O: Uniﬁed Egocentric Recognition of 3D Hand-Object Poses and Interactions

Bugra Tekin1

Federica Bogo1

Marc Pollefeys1

2

,

1 Microsoft

2 ETH Z¨urich

Abstract

We present a uniﬁed framework for understanding 3D
hand and object interactions in raw image sequences from
egocentric RGB cameras. Given a single RGB image, our
model jointly estimates the 3D hand and object poses, mod-
els their interactions, and recognizes the object and action
classes with a single feed-forward pass through a neural
network. We propose a single architecture that does not
rely on external detection algorithms but rather is trained
end-to-end on single images. We further merge and prop-
agate information in the temporal domain to infer interac-
tions between hand and object trajectories and recognize
actions. The complete model takes as input a sequence of
frames and outputs per-frame 3D hand and object pose pre-
dictions along with the estimates of object and action cate-
gories for the entire sequence. We demonstrate state-of-the-
art performance of our algorithm even in comparison to the
approaches that work on depth data and ground-truth an-
notations.

1. Introduction

Human behavior can be characterized by the individual
actions a person takes in interaction with the surrounding
objects and the environment. A signiﬁcant amount of re-
search has focused on visual understanding of humans [8,
25, 36, 40, 59, 65, 70, 74] and objects [4, 28, 64, 66], in
isolation from each other. However, the problem of jointly
understanding humans and objects, although crucial for a
semantically meaningful interpretation of the visual scene,
has received far less attention. In this work, we propose,
for the ﬁrst time, a uniﬁed method to jointly recognize 3D
hand and object poses, and their interactions from egocen-
tric monocular color images. Our method jointly estimates
the hand and object poses in 3D, models their interactions
and recognizes the object and activity classes. An example
result is shown in Fig. 1. Our uniﬁed framework is highly
relevant for augmented and virtual reality [62], ﬁne-grained
recognition of people’s actions, robotics and telepresence.

Figure 1: Uniﬁed reasoning on ﬁrst-person views. Our method
takes as input color images and produces a comprehensive ego-
centric scene interpretation. We estimate simultaneously 3D hand
and object poses (shown as skeletons and 3D bounding boxes),
object class (e.g. juice bottle) and action category (e.g. pouring).

Capturing hands in action while taking into account the
objects in contact is an extremely challenging problem.
Jointly reasoning about hands and objects from moving,
egocentric cameras is even more challenging as this would
require understanding of the complex and often subtle inter-
actions that take place in cluttered real-world scenes where
the hand is often occluded by the object or the viewpoint.
Recent research in computer vision has successfully ad-
dressed some of the challenges in joint understanding of
hands and objects for depth and multi-camera input. Srid-
har et al. [60] have demonstrated that accounting jointly for
hands and objects helps predict the 3D pose more accu-
rately than models that ignore interaction. Pioneering works
by [21, 37, 41, 68] have proposed ways to model hand-
object interactions to increase robustness and accuracy in
recovering the hand motion.

Most of these works, however, are limited by the follow-
ing factors: Firstly, they either rely on active depth sensors
or multi-camera systems. Depth sensors are power hungry
and less prevalent than regular RGB cameras. On the other
hand, multi-camera systems are impractical due to the cost
and effort in setting up a calibrated and synchronous system
of sensors. Secondly, they do not reason about the action
the subject is performing. While estimating the 3D hand
pose is crucial for many applications in robotics and graph-
ics, the sole knowledge of the pose lacks semantic meaning
about the actions of the subject. Thirdly, they focus mostly

4511

on only capturing hand motion without recovering the ob-
ject pose in 3D [21, 37], and, therefore, lack environmental
understanding.

Hands and Objects. Many approaches in the literature
tackle the problem of estimating either hand or object pose
in isolation.

Our method aims at tackling all these issues. To this
end, we propose an approach for predicting simultaneously
3D hand and object poses, object classes and action cate-
gories through a novel data-driven architecture. Our model
jointly produces 3D hand and object pose estimates, action
and object classes from a single image and requires neither
external region proposals nor pre-computed detections. Our
method further models the temporal nature of 3D hand and
object motions to recognize actions and infer interactions.

Our contributions can be summarized as follows:

• We propose a uniﬁed framework for recognizing 3D
hand and object interactions by simultaneously solv-
ing four tasks in a feed-forward pass through a neural
network: 3D hand pose estimation, object pose estima-
tion, object recognition and activity classiﬁcation. Our
method operates on monocular color images and relies
on joint features that are shared among all the tasks.

• We introduce a novel single-shot neural network
framework that jointly solves for the 3D articulated
and rigid pose estimation problems within the same
architecture. Our scheme relies on a common out-
put representation for both hands and objects that
parametrizes their pose with 3D control points. Our
network directly predicts the control points in 3D
rather than in 2D, in contrast to the common single-
shot neural network paradigm [48, 66], dispenses with
the need to solve a 2D-to-3D correspondence prob-
lem [32] and yields large improvements in accuracy.

• We present a temporal model to merge and propagate
information in the temporal domain, explicitly model
interactions and infer relations between hand and ob-
jects, directly in 3D.

In Section 4, we show quantitatively that these contri-
butions allow us to achieve better overall performance in
targeted tasks, while running at real-time speed and not re-
quiring detailed 3D hand and object models. Our approach,
which we call Hand + Object (H+O), achieves state-of-the-
art results on challenging sequences, and outperforms ex-
isting approaches that rely on the ground-truth pose annota-
tions and depth data.

2. Related Work

We now review existing work on 3D hand and object
pose estimation – both jointly and in isolation – and action
recognition, with a focus on egocentric scenarios.

Brachman et al. [4] recover 6D object pose from single
RGB images using a multi-stage approach, based on regres-
sion forests. More recent approaches [28, 46] rely on Con-
volutional Neural Networks (CNNs). BB8 [46] uses CNNs
to roughly segment the object and then predict the 2D loca-
tions of the object’s 3D bounding box, projected in image
space. 6D pose is then computed from these estimates via
PnP [32]. SSD-6D [28] predicts 2D bounding boxes to-
gether with an estimate of the object pose. These methods
need a detailed textured 3D object model as input, and re-
quire a further pose reﬁnement step to improve accuracy.
Tekin et al. [66] overcome these limitations by introducing
a single-shot architecture which predicts 2D projections of
the object’s 3D bounding box in a single forward pass, at
real-time speed. All these approaches do not address the
problem of estimating object pose in hand-object interac-
tion scenarios, where objects might be largely occluded.

3D hand pose and shape estimation in egocentric views
has started receiving attention recently [7, 22, 37, 51, 70,
72, 73]. The problem is challenging with respect to third-
person scenarios [58, 74], due to self-occlusions and the
limited amount of training data available [37, 39, 51, 70].
Mueller et al. [37] train CNNs on synthetic data and com-
bine them with a generative hand model to track hands in-
teracting with objects from egocentric RGB-D videos. This
hybrid approach has then been extended to work with RGB
videos [36]. Iqbal et al. [25] estimate 3D hand pose from
single RGB images, from both ﬁrst- and third-person views,
regressing 2.5D heatmaps via CNNs. These methods focus
on hand pose estimation and try to be robust in the presence
of objects, but do not attempt to model hand and objects
together.

While reasoning about hands in action, object interac-
tions can be exploited as additional constraints [41, 49, 51,
53]. By observing that different object shapes induce differ-
ent hand grasps, the approaches in [7, 51] discriminatively
estimate 3D hand pose from depth input. Model-based ap-
proaches [54] have also been proposed to jointly estimate
hand and object parameters at a ﬁner level of detail. How-
ever, most approaches focus on third-view scenarios, taking
depth as input [41, 43, 67, 68].

To our knowledge, no approach in the literature jointly

estimates 3D hand and object pose from RGB video only.

Action Recognition. While action recognition is a long-
standing problem in computer vision [3, 10, 18, 26, 31,
38, 69], ﬁrst-person action recognition started to emerge
as an active ﬁeld only recently, largely due to the advent
of consumer-level wearable sensors and large egocentric
datasets [9, 20, 44, 52]. First-person views bring upon

4512

unique challenges to action recognition due to fast camera
motion, large occlusions and background clutter [33].

Early approaches for egocentric action recognition rely
on motion cues [29, 50, 55]. In particular, [29] uses opti-
cal ﬂow-based global motion descriptors to categorize “ego-
actions” across sports genres. [45] feeds sparse optical ﬂow
to 3D CNNs to index egocentric videos. Motion and ap-
pearance cues are used in conjunction with depth in [63]. In
addition to motion, features based on gaze information [13],
head motion [33], and, recently, CNN-learned features [35]
have been proposed by a number of studies. Another line of
work has focused speciﬁcally on hand and object cues for
ﬁrst person action recognition [12, 16, 27, 57, 61]. Pirsi-
avash and Ramanan [44] explore active object detection as
an auxiliary task for activity recognition. Koppula et al. [30]
learn a model of object affordances to understand activities
from RGB-D videos. EgoNet [2] detects “action objects”,
i.e. objects linked to visual or tactile interactions, from ﬁrst-
person RGB-D images. Fathi et al. [12, 14] use motion cues
to segment hands and objects, and then extract features from
these foreground regions. All these approaches, however,
focus on 2D without explicitly modeling hand-object inter-
actions in 3D. Recently, Garcia-Hernando et al. [17] demon-
strate that 3D heuristics are beneﬁcial to ﬁrst person action
recognition. However, they work on depth input and rely on
ground-truth object poses.

Similarly to us, Cai et al. [34] propose a structured ap-
proach where grasp types, object attributes and their con-
textual relationships are analyzed together. However, their
single-image framework does not consider the temporal di-
mension. Object Relation Network [1] models contextual
relationships between detected semantic object instances,
through space and time. All these approaches aim at un-
derstanding the scene only in 2D. Here, we model more
complex hand and object attributes in 3D, as well as their
temporal interaction.

3. Method

Our goal is to construct comprehensive interpretations of
egocentric scenes from raw image sequences to understand
human activities. To this end, we propose a uniﬁed frame-
work to jointly estimate 3D hand and object poses and rec-
ognize object and action classes.

3.1. Overview

The general overview of our Hand+Object model is
given in Fig. 2. Our model takes as input a sequence of
color frames It (1 ≤ t ≤ N ) and predicts per-frame 3D
hand and object poses, object classes and action categories,
along with per-sequence interaction classes. Here, we de-
ﬁne actions as verbs, e.g. “pour”, and interactions as (verb,
noun) pairs, e.g. “pour juice”. We represent hand and ob-
ject poses with Nc 3D control points. Details about control

point deﬁnitions are provided in Sec. 3.2. We denote the
number of object classes by No, the number of actions by
Na and the number of interactions by Nia.

Our model ﬁrst processes each frame, It, of a sequence
with a fully convolutional network (Fig. 2a) and divides the
input image into a regular grid Gt containing H × W × D
cells that span the 3D scene in front of the camera (Fig. 2b).
We keep the target values of our network for hands and
objects in tensor Gt (Fig. 2c-d). Namely, the target val-
ues for a hand or an object at a speciﬁc cell location,
i ∈ H ×W × D, are placed in the i-th cell of Gt in the form
of a multi-dimensional vector, vi. To be able to jointly esti-
mate the pose of a hand and an object potentially occluding
each other, we allow each cell to store two separate sets of
values, one for the hand, denoted by vh
i , and one for the
object, denoted by vo
i stores the con-
i ∈ R3Nc , action probabilities,
trol points for hand pose, yh
i ∈ RNa , and an overall conﬁdence value for the hand
pa
pose estimation, ch
i stores the control
points for object pose, yo
i ∈ R3Nc , object class probabil-
ities, po
i ∈ RNo , and an overall conﬁdence value for the
object pose estimation, co

i (Fig. 2e). Vector vh

i ∈ [0, 1]. Vector vo

i ∈ [0, 1].

We train our single pass network based on [48] to be able
to predict these target values. At test time, predictions at
cells with low conﬁdence values, i.e. where the hands or ob-
jects of interest are not present, are pruned. All these predic-
tions are obtained with a single forward pass in the network.
While very efﬁcient, this step works on each frame indepen-
dently, thus ignoring the temporal dimension. Therefore,
we add a recurrent module to integrate information across
frames and model the interaction between hands and ob-
jects (Fig. 2a). This module takes as input hand and object
predictions with high conﬁdence values, and outputs a prob-
ability vector, pia ∈ RNia , over interaction classes. In the
following sections, we describe each of these components
in more detail.

3.2. Joint 3D Hand Object Pose Estimation

In the context of rigid object pose estimation, [46, 66]
regress the 2D locations of 8 keypoints – namely, the pro-
jections of the 8 corners of the object’s 3D bounding box.
The object’s 6D pose is then estimated using a PnP algo-
rithm [32]. Adopting a similar approach would not work in
our case, since we aim at estimating also the articulated 3D
pose of the hand. To tackle this problem and jointly esti-
mate 3D articulated and rigid pose within the same archi-
tecture, we propose to use a common output representation
for both hands and objects. To this end, we parametrize
both hand and object poses jointly with 3D control points,
corresponding to 21 skeleton joints for the hand pose and
3D locations of object keypoints, corresponding to the lo-
cations on the 3D object bounding box. For simplicity, we
choose Nc = 21, and deﬁne 8 keypoints for the objects as

4513

(a)

(b)

(c)

(d)

(e)

Figure 2: Overview of our Hand+Object approach. (a) The proposed network architecture. Each frame It is passed through a fully-
convolutional network to produce a 3D regular ﬁxed grid Gt. (b) The H × W × D grid showing cells responsible for recognizing hands
and objects. (c) Each cell predicts the 3D hand pose and object bounding box coordinates in a 3D grid. (d) The output tensor from our
network, in which the target values for hands and objects are stored. (e) Cells are associated with a vector that contains target values for
hand and object pose, object and action categories and an overall conﬁdence value. Predictions with high conﬁdence values are then passed
through the interaction RNN to propagate information in the temporal domain and model interactions in 3D between hands and objects.

proposed in [46, 66], along with the 12 edge midpoints and
the centroid of the 3D bounding box.

Adopting a coherent parameterization for hand and ob-
ject simpliﬁes the regression task. We subdivide the input
image into a grid of H × W cells, and further discretize
depth into D cells. Note that discretization is deﬁned in
pixel space for the ﬁrst two dimensions, and in metric space
for depth. Therefore each cell has a size of Cu × Cv pix-
els, ×Cz meters. Within each cell, we predict offsets ∆u,
∆v, ∆z for the locations corresponding to the control points
with respect to the top-left corner of the cell that is closer
to the camera, (u, v, z). For the hand root joint and the ob-
ject centroid, we constrain the offset to lie between 0 and 1,
where a size of 1 corresponds to the full extent of an individ-
ual cell within grid dimensions. For other control points, we
do not constrain the network’s output as those points should
be allowed to fall outside the cell. The predicted location of
the control point ( ˆwu, ˆwv, ˆwz) is then computed as:

ˆwu = g(∆u) + u
ˆwv = g(∆v) + v
ˆwz = g(∆z) + z

(1)

(2)

(3)

where g(·) is chosen to be a 1D sigmoid function for the root
joint and the object centroid, and the identity function for
other control points. Here, (u, v, z) are indices for the cell
in grid dimensions. Given the camera intrinsics matrix K,
and the prediction for the grid location, ( ˆwu, ˆwv, ˆwz), the
3D location ˆy of the control point in the camera coordinate
system is then computed as:

ˆy = ˆwz · Cz · K −1[ ˆwu · Cu, ˆwv · Cv, 1]T .

(4)

3D joint predictions already deﬁne the hand pose. Given the
control point predictions on the 3D bounding box, 6D object
pose could also be computed efﬁciently by aligning the pre-

diction to the reference 3D bounding box with a rigid trans-
formation. This dispenses with the need to solve for a 2D-
to-3D correspondence problem using PnP as in [46, 66] and
recovers the 6D pose via Procrustes transformation [19].
Such a formulation also reduces depth ambiguities caused
by projection from 3D to 2D. We show in Sec. 4 that this
results in an improved object pose estimation accuracy.

In addition to hand and object control point locations,
our network also predicts high conﬁdence values for cells
where the hand (or the object) is present, and low con-
ﬁdence where they are not present. Computing reliable
conﬁdence values is key for obtaining accurate predictions
at test time. We deﬁne the conﬁdence of a prediction as
a function of the distance of the prediction to the ground
truth, inspired by [66]. Namely, given a predicted 2D loca-
tion ( ˆwu, ˆwv) and its Euclidean distance DT ( ˆwu, ˆwv) from
the ground truth, measured in image space, the conﬁdence
value cuv( ˆwu, ˆwv) is computed as an exponential function
with cut-off value dth and sharpness parameter α:

cuv( ˆwu, ˆwv) = e

α(cid:16)1−

DT ( ˆwu , ˆwv )

dth

(cid:17)

(5)

if DT ( ˆwu, ˆwv) < dth, and cuv( ˆwu, ˆwv) = 0 otherwise.
However, in contrast to [66] that computes a conﬁdence
value for 2D prediction, we need to also consider the depth
dimension. Therefore, for a given depth prediction, ˆwz, we
compute an additional conﬁdence value, cz( ˆwz), measur-
ing the distance (in metric space) between prediction and
ground truth as in Eq. 5. We then compute the ﬁnal conﬁ-
dence value c = 0.5 · cuv( ˆwu, ˆwv) + 0.5 · cz( ˆwz).

3.3. Object and Action Recognition

In addition to the 3D control locations and the conﬁdence
value, our model also predicts the object and action classes
(i.e. nouns and verbs, as deﬁned in Sec. 3.1). Intuitively,
features learned to predict hand and object pose could also

4514

help recognize actions and object classes. In order to predict
actions, at each cell i we store into vector vh
i , together with
i and the corresponding conﬁdence value ch
hand pose yh
i ,
the target action class probabilities pa
i . Similarly, to be able
to predict object classes, we store into vector vo
i , together
with object pose yo
i and the corresponding conﬁdence value
i , the target object class probabilities po
co
i .

i and vo

In total, vh

i stores 3 · Nc + Na + 1 values and vo

i stores
3 · Nc + No + 1 values. We train our network to be able
i for each cell i of the 3D grid Gt,
to predict vh
for each t. Given verb and noun predictions, our model
is able to recognize interactions from only a single image.
Ultimately, our network learns to jointly predict 3D hand
and object poses along with object, action and interaction
classes – with a single forward pass.

3.4. Temporal Reasoning and Interaction Modeling

While simple actions can be recognized by looking at
single frames, more complex activities require to model
longer-term dependencies across sequential frames. To rea-
son along the temporal dimension, we add a RNN module
to our architecture. We choose to use a Long Short-Term
Memory (LSTM) [23], given its popularity in the action
recognition literature [17]. We also experimented with dif-
ferent modules (e.g. Gated Recurrent Units [6]), without ob-
serving substantial differences in our results.

A straightforward approach would be to consider the
highest conﬁdence predictions for hand and object poses at
each frame, and give them as input to the RNN module, as
in [17]. The RNN output is then processed by a softmax
layer to predict the activity class. However, we can improve
upon this baseline by explicitly reasoning about interactions
between hands and objects. For example, in the context of
visual reasoning, [1] proposes to model relational depen-
dencies between objects in the scene and demonstrates that
such object-level reasoning improves accuracy in targeted
visual recognition tasks. In a similar manner, we aim to ex-
plicitly model interactions – in this case, however, of hands
and objects, and directly in 3D. Co-training of 3D hand and
object pose estimation networks already implicitly accounts
for interactions in a data-driven manner. We further propose
to model hand-object interactions at the structured output
level with an interaction RNN. To do so, instead of directly
feeding the hand and object poses as input to the temporal
module, inspired by [1, 56], we model dependencies be-
tween hands and objects with a composite learned function
and only then give the resulting mapping as input to RNN:

fφ(gθ(ˆyh, ˆyo))

(6)

where fφ is an LSTM and gθ is an MLP, parameterized by
φ and θ, respectively. Ultimately this mapping learns the
explicit dependencies between hand and object poses and
models interactions. We analyze and quantitatively evaluate
the beneﬁts of this approach in Sec. 4.

3.5. Training

The ﬁnal layer of our single-shot network produces, for
each cell i, hand and object pose predictions, ˆyh
i and ˆyo
i ,
with their overall conﬁdence values, ˆch
i , as well as
probabilities for actions, ˆpa
i . For
each frame t, the loss function to train our network is de-
ﬁned as follows:

i , and object classes, ˆpo

i and ˆco

L = λpose X

(||ˆyh

i − yh

i ||2

2 + ||ˆyo

i − yo

i ||2

2)+ (7)

i∈Gt

λconf X

i∈Gt

((ˆch

i − ch

i )2 + (ˆco

i − co

i )2)−

λactcls X

ˆpa
i log pa

i −

i∈Gt

λobjcls X

i log po
ˆpo
i .

i∈Gt

(8)

(9)

(10)

Here, the regularization parameters for the pose and clas-
siﬁcation losses, λpose, λactcls and λobjcls are simply set to
1. As suggested by [48], for cells that contain a hand or
an object, we set λconf to 5 and for cells that do not con-
tain any of them, we set it to 0.1 to increase model stability.
We feed the highest conﬁdence predictions of our network
to the recurrent module and deﬁne an additional loss based
on cross entropy for the action recognition over the entire
sequence. In principle, it is straightforward to merge and
train the single-image and temporal models jointly with a
softargmax operation [5, 71]. However, as backpropagation
requires to keep all the activations in memory, which is not
possible for a batch of image sequences, we found it ef-
fective to train our complete model in two stages. We ﬁrst
train on single frames to jointly predict 3D hand and object
poses, object classes and action categories. We then keep
the weights of the initial model ﬁxed and train our recurrent
network to propagate information in the temporal domain
and model interactions. The complete model takes as input
a sequence of images and outputs per-frame 3D hand-object
pose predictions, object and action classes along with the
estimates of interactions for the entire sequence.

4. Evaluation

In this section, we ﬁrst describe the datasets and the
corresponding evaluation protocols. We then compare our
Hand+Object approach against the state-of-the-art methods
and provide a detailed analysis of our general framework.

4.1. Datasets

We evaluate our framework for recognizing 3D hand-
object poses and interactions on the recently introduced
First-Person Hand Action (FPHA) dataset [17].
It is the
only publicly available dataset for 3D hand-object interac-
tion recognition that contains labels for 3D hand pose, 6D
object pose and action categories. FPHA is a large and di-
verse dataset including 1175 videos belonging to 45 dif-

4515

ferent activity categories performed by 6 actors. A total
of 105,459 frames are annotated with accurate hand poses
and action categories. The subjects carry out complex mo-
tions corresponding to daily human activities. A subset of
the dataset contains annotations for objects’ 6-dimensional
poses along with corresponding mesh models for 4 objects
involving 10 different action categories. We denote this sub-
set of the dataset as FPHA-HO.

As there are no other egocentric datasets containing la-
bels for both 3D hand pose and 6D object pose, we fur-
ther annotate 6D object poses on a part of the EgoDexter
hand pose estimation dataset [37], on which we validate our
joint hand-object pose estimation framework. To this end,
we annotate the Desk sequence between frames 350 and
500 which features a cuboid object. We use training data
from the SynthHands 3D hand pose estimation dataset [37].
We augment this dataset by randomly superimposing on the
image synthetic cuboid objects that we generate and ob-
ject segmentation masks from the third-person view dataset
of [60] which features the same object. To gain robustness
against background changes, we further replace the back-
grounds using random images from [11].

4.2. Evaluation Metrics

We evaluate our uniﬁed framework on a diverse set of
tasks: egocentric activity recognition, 3D hand pose estima-
tion and 6D object pose estimation, and use standard met-
rics and ofﬁcial train/test splits to evaluate our performance
in comparison to the state of the art. We use the percentage
of correct video classiﬁcations, percentage of correct key-
point estimates (3D PCK) and percentage of correct poses to
measure accuracy on activity recognition, 3D hand pose es-
timation and 6D object pose estimation, respectively. When
using the 3D PCK metric for hand pose estimation, we con-
sider a pose estimate to be correct when the mean distance
between the predicted and ground-truth joint positions is
less than a certain threshold. When using the percentage
of correct poses to evaluate 6D object pose estimation ac-
curacy, we take a pose estimate to be correct if the 2D pro-
jection error or the average 3D distance of model vertices is
less than a certain threshold (the latter being also referred to
as the ADD metric).

4.3. Implementation Details

We initialize the parameters of our single-image network
based on [48] with weights pretrained on ImageNet. The in-
put to our model is a 416 × 416 image. The output grid,
Gt, has the following dimensions: H = 13, W = 13
and D = 5. We set the grid size in image dimensions to
Cu = Cv = 32 pixels as in [48] and in depth dimension to
Cz = 15 cm. Further details about the architecture can be
found in the supplemental material. We set the sharpness
of the conﬁdence function, α, to 2, and the distance thresh-

old to 75 pixels for the spatial dimension and 75 mm for
the depth dimension. We use a 2-layer LSTM with a hidden
layer size of 512. The nonlinearity, gθ, is implemented as an
MLP with 1 hidden layer with ReLU activation consisting
of 512 units. We use stochastic gradient descent for opti-
mization. We start with a learning rate of 0.0001 and divide
the learning rate by 10 at the 80th and 160th epoch. All
models are trained with a batch size of 16 for 200 epochs.
We use extensive data augmentation to prevent overﬁtting.
We randomly change the hue, saturation and exposure of the
image by up to a factor of 50%. We also randomly translate
the image by up to a factor of 10%. Our implementation is
based on PyTorch.

4.4. Experimental Results

We ﬁrst report activity recognition accuracy on the
FPHA-HO dataset and compare our results to the state-of-
the-art results of [17] in Table 1. We further use the follow-
ing baselines and versions of our approach in the evaluation:

• SINGLE-IMAGE - Our single pass network that pre-
dicts the action (i.e. verb), and object class (i.e. noun).
The individual predictions for action (pa
i , e.g. open)
and object (po
i , e.g. bottle) class are combined to pre-
dict the interaction type (e.g. open bottle), i.e. (verb,
noun) pair. This version of our model does not use a
temporal model.

• HAND POSE - A temporal model that uses the hand
pose predictions of our approach as input to the RNN
to recognize activities.

• OBJECT POSE - similar to the previous baseline, but
trained to predict activities based on the predicted key-
points on the 3D object bounding box.

• HAND + OBJECT POSE - A version of our model
that combines 3D hand-object pose predictions to feed
them as input to the RNN.

• HAND POSE + OBJECT POSE + INTERACT - A com-
plete version of our model with temporal reasoning
and interaction modeling.

Method Model

Action Accuracy (%)

[17]

OURS

Ground-truth Hand Pose
Ground-truth Object Pose
Ground-truth Hand + Object Pose
SINGLE-IMAGE
HAND POSE
OBJECT POSE
HAND + OBJECT POSE
HAND + OBJECT POSE + INTERACT

87.45
74.45
91.97
85.56
89.47
85.71
94.73
96.99

Table 1: Action recognition results on FPHA-HO. We evaluate
the impact of hand and object poses for action recognition. We
demonstrate that hand-object predictions of our uniﬁed network
yield more accurate results than [17] which relies on ground-truth
pose as input. Furthermore, explicitly modeling interactions of
hand-object poses results in a clear improvement in accuracy.

4516

Figure 3: The impact of each ﬁnger (left) and each hand part
(right) in recognizing interactions. The impact is measured by the
normalized magnitude of neural network weights tied to the corre-
sponding hand joint positions and compared for a standard RNN
and an interaction RNN. MCP, PIP and DIP denote the 3 consec-
utive joints located inbetween wrist and ﬁngertip (TIP) on each
ﬁnger, in their respective order.

Recognizing Interactions. Our model achieves state-of-
the-art performance for recognizing egocentric activities
and 3D hand-object interactions even without ground-truth
3D hand and object poses. Our SINGLE-IMAGE baseline
already achieves close results to the state of the art. We
demonstrate that temporal reasoning on 3D hand and object
poses individually improves activity recognition accuracy.
The combination of hand and object poses further boosts
the overall scores. With interaction modeling, performance
improvements are even more noticeable. To analyze the
importance of interaction modeling further, in Fig. 3, we
quantify the importance of each input to the RNN by mea-
suring the magnitude of the network weights tied to the in-
puts, both for a standard RNN and our interaction RNN.
We demonstrate that, in contrast to a standard RNN, our
temporal model attributes more importance to the index ﬁn-
gers and ﬁngertips that interact more commonly with ob-
jects and learns which joints are relevant in interaction. In
Fig. 5, we provide some visual results, 3D hand and object
poses along with action and object labels that are produced
by our HAND + OBJECT + INTERACT model.

We further evaluate the performance of our approach for
the task of egocentric activity and interaction recognition on
the full FPHA dataset. On the full dataset, object poses are
not available for all the action categories, therefore, we train
our models only using 3D hand poses. Here, however, to in-
crease the descriptive power, we further leverage the object
class and action category predictions produced by our sin-
gle pass network in our temporal model. To this end we
augment the 3D hand pose input (HP) with the output ob-
ject class (OC) and action category (AC) probabilities. In
Table 2, we demonstrate that these additional features yield
improved action recognition accuracy. Overall, our method
consistently outperforms the baselines by a large margin,
including the ones that rely on depth data.

3D Hand Pose Prediction. We further compare the accu-
racy of our 3D hand pose predictions to the state-of-the-art
results on FPHA in Fig. 4. Even though we only use color
images, in contrast to [17] that uses depth input, we achieve

Figure 4: Comparison of the hand pose estimation results of our
approach with those of Garcia-Hernando et al. [17] using different
thresholds for the 3D pose error.

Model
Two-stream-color [15]
Two-stream-ﬂow [15]
Two-stream-all [15]
Joule-color [24]
HON4D [42]
Novel View [47]
Joule-depth [24]
[17] + Gram Matrix
[17] + Lie Group
[17] + LSTM
OURS - HP
OURS - HP + AC
OURS - HP + AC + OC

Input modality Accuracy

Color
Color
Color
Color
Depth
Depth
Depth
Depth
Depth
Depth
Color
Color
Color

61.56
69.91
75.30
66.78
70.61
69.21
60.17
32.22
69.22
72.06
62.54
74.20
82.43

Table 2: Action recognition results on the full FPHA dataset [17].
Our method signiﬁcantly improves upon the baselines, including
the ones that rely on depth data. We further provide action-speciﬁc
recognition accuracies in the supplemental material.

competitive 3D hand pose estimation accuracy. Further-
more, we do not assume knowledge of the hand bounding
box as in [17]; our model takes as input a single full color
image. Note also that the method of [17] is speciﬁcally
trained for 3D hand pose estimation, whereas this is a sub-
task of our method which simultaneously tackles multiple
tasks within a uniﬁed architecture.

6D Object Pose Prediction. To evaluate our object pose
accuracy, we compare our approach to the state-of-the-art
results of [66] in Fig. 6. To this end, we run their approach
on FPHA with their publicly available code. We demon-
strate that explicitly reasoning about 6D object pose in 3D,
in contrast to [66] that relies on solving 2D-to-3D corre-
spondences, yields improved pose estimation accuracy. We
conjecture that posing the 6D pose estimation problem in
2D is prone to depth ambiguities and our approach brings
in robustness against it by directly reasoning in 3D.

Uniﬁed Framework. We analyze the inﬂuence of simul-
taneously training hand and object pose estimation tasks
within the same single pass network on individual pose es-
timation accuracies. We compare the results of our Hand
+ Object network to those of the networks trained only for
hand pose estimation and only for object pose estimation in
Table 3. While we obtain similar accuracies for hand pose
estimation with co-training and individual training, object
pose estimation accuracy for the uniﬁed network is signiﬁ-
cantly better than that of the individual pose estimation net-

4517

Figure 5: Qualitative results on the FPHA and EgoDexter dataset. We visualize the 3D hand pose estimates, 3D object bounding boxes
which are transformed with the learned 6D object poses, and activity labels. The proposed approach can handle motion blur, self-occlusions,
occlusions by viewpoints, clutter and complex articulations. We provide further qualitative results in our supplemental material.

Figure 6: Comparison of the object pose estimation results of our
approach with those of Tekin et al. [66] using different thresholds
for the 2D projection (left) and ADD metric (right).

work by a large margin of 9.65%. This indicates that having
a joint representation which is shared across multiple tasks
leads to an improvement in combined pose estimation ac-
curacy. The results suggest that 3D hand pose highly con-
strains the 6D object pose, while the effect of rigid object
pose on the articulated hand pose is not as pronounced. We
further show that the prediction of the interaction class im-
proves the hand and object pose estimation accuracy. This
further validates that our uniﬁed framework allows us to
achieve better overall performance in targeted tasks.

Network
HAND ONLY
OBJECT ONLY
HAND + OBJECT
HAND + OBJECT + INTERACT

HP error OP error

16.15

-

16.87
15.81

-

28.27
25.54
24.89

Table 3: Comparison of the pose estimation results of our uniﬁed
network to those of the networks trained only for hand and object
pose estimation. Error metric is the mean 3D distance in mm.

Generalization. To demonstrate the generalization power
of our joint hand-object pose estimation framework, we
annotate a part of the EgoDexter hand pose estimation
dataset [37] with 6D object poses, as explained in Sec. 4.1,
and report quantitative results in Table 4. We demonstrate

Part
Fingertips
Object coordinates

Error (in cm)

4.84
2.37

Table 4: Results on the augmented EgoDexter dataset. Even when
trained on synthetic data, our approach yields accurate poses.

that even when trained on synthetic data, our approach gen-
eralizes well to unconstrained environments and results in
reliable and accurate joint 3D hand-object pose estimates.
We provide visual results on this dataset in Fig. 5.

Runtime. Our single pass network that produces per-
frame predictions simultaneously for 3D hand poses, 6D
object poses, object classes and action categories runs at
real-time speed of 25 fps on an NVIDIA Tesla M40. With-
out action and object recognition, when estimating only the
poses of hands and objects, our method runs at a greater
speed of 33 fps. Given hand and object poses, the interac-
tion RNN module further processes a sequence with virtu-
ally no time overhead, at an average of 0.003 seconds.

5. Conclusion

In this paper, we propose the ﬁrst method to jointly rec-
ognize 3D hand and object poses from monocular color im-
age sequences. Our uniﬁed Hand+Object model simultane-
ously predicts per-frame 3D hand poses, 6D object poses,
object classes and action categories, while being able to
run at real-time speeds. Our framework jointly solves 3D
articulated and rigid pose estimation problems within the
same single-pass architecture and models the interactions
between hands and objects in 3D to recognize actions from
ﬁrst-person views. Future work will apply the proposed
framework to explicitly capture interactions between two
hands and with other people in the scene.

4518

References

[1] F. Baradel, N. Neverova, C. Wolf, J. Mille, and G. Mori.

Object level Visual Reasoning in Videos. In ECCV, 2018.

[2] G. Bertasius, H. S. Park, S. X. Yu, and J. Shi. First-Person
Action-Object Detection with EgoNet. In Robotics: Science
and Systems, 2017.

[3] A. Bobick and J. Davis. The Recognition of Human Move-
ment Using Temporal Templates. PAMI, 23(3):257–267,
2001.

[4] E. Brachmann, F. Michel, A. Krull, M. Y. Yang, S. Gumhold,
and C. Rother. Uncertainty-Driven 6D Pose Estimation of
Objects and Scenes from a Single RGB Image.
In CVPR,
2016.

[5] O. Chapelle and M. Wu. Gradient Descent Optimization of
Information Re-

Smoothed Information Retrieval Metrics.
trieval, 13(3):216–235, 2009.

[6] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. Learning Phrase
Representations using RNN Encoder–Decoder for Statistical
Machine Translation. In EMNLP, 2014.

[7] C. Choi, S. H. Yoon, C. Chen, and K. Ramani. Robust Hand
Pose Estimation during the Interaction with an Unknown Ob-
ject. In ICCV, 2017.

[8] V. Choutas, P. Weinzaepfel, J. Revaud, and C. Schmid. Po-
Tion: Pose MoTion Representation for Action Recognition.
In CVPR, 2018.

[9] D. Damen, H. Doughty, G. M. Farinella, S. Fidler,
A. Furnari, E. K. D. Moltisanti, J. Munro, T. Perrett,
W. Price, and M. Wray. Scaling Egocentric Vision: The
EPIC-KITCHENS Dataset. In ECCV, 2018.

[10] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie. Behav-
ior Recognition via Sparse Spatio-temporal Features. In VS-
PETS, 2005.

[11] M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and
A. Zisserman. The PASCAL Visual Object Classes (VOC)
Challenge. IJCV, 88(2):303–338, 2010.

M. Mueller-Freitag, F. Hoppe, C. Thurau, I. Bax, and
R. Memisevic. The “Something Something” Video Database
for Learning and Evaluating Visual Common Sense.
In
ICCV, 2017.

[21] H. Hamer, K. Schindler, E. Koller-Meier, and L. V. Gool.

Tracking a Hand Manipulating an Object. In ICCV, 2009.

[22] Y. Hasson, G. Varol, D. Tzionas, I. Kalevatykh, M. J. Black,
I. Laptev, and C. Schmid. Learning Joint Reconstruction of
Hands and Manipulated Objects. In CVPR, 2019.

[23] S. Hochreiter and J. Schmidhuber. Long Short-Term Mem-

ory. Neural Computation, 9(8):1735–1780, 1997.

[24] J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang. Jointly Learning
Heterogeneous Features for RGB-D Activity Recognition. In
CVPR, 2015.

[25] U. Iqbal, P. Molchanov, T. Breuel, J. Gall, and J. Kautz. Hand
In

Pose Estimation via Latent 2.5D Heatmap Regression.
ECCV, 2018.

[26] H. Jhuang, T. Serre, L. Wolf, and T. Poggio. A Biologically

Inspired System for Action Recognition. In ICCV, 2007.

[27] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid.
In ICCV,

Joint Learning of Object and Action Detectors.
2017.

[28] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab.
SSD-6D: Making RGB-based 3D Detection and 6D Pose Es-
timation Great Again. In ICCV, 2017.

[29] K. Kitani, T. Okabe, Y. Sato, and A. Sugimoto. Fast Unsu-
pervised Ego-action Learning for First-person Sports Videos.
In CVPR, 2011.

[30] H. Koppula, R. Gupta, and A. Saxena. Learning Human Ac-
tivities and Object Affordances from RGB-D Videos. IJRR,
32(8):951–970, 2013.

[31] I. Laptev and T. Lindeberg. Space-time Interest Points. In

ICCV, 2003.

[32] V. Lepetit, F. Moreno-Noguer, and P. Fua. EPnP: An Accu-
rate O(n) Solution to the PnP Problem. IJCV, 81(2):155–166,
2009.

[12] A. Fathi, A. Farhadi, and J. Rehg. Understanding Egocentric

[33] Y. Li, Z. Ye, and J. Rehg. Delving into Egocentric Actions.

Activities. In ICCV, 2011.

In CVPR, 2015.

[13] A. Fathi, Y. Li, and J. Rehg. Learning to Recognize Daily

Actions Using Gaze. In ECCV, 2012.

[14] A. Fathi, X. Ren, and J. Rehg. Learning to Recognize Ob-

[34] Y. S. M. Cai, K. M. Kitani. Understanding Hand-Object
In

Manipulation with Grasp Types and Object Attributes.
Robotics: Science and Systems, 2016.

jects in Egocentric Activities. In CVPR, 2011.

[35] M. Ma, H. Fan, and K. Kitani. Going Deeper into First-

[15] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional
Two-Stream Network Fusion for Video Action Recognition.
In CVPR, 2016.

[16] D. Fouhey, W. Kuo, A. Efros, and J. Malik. From Lifestyle

VLOGs to Everyday Interaction. In CVPR, 2018.

Person Activity Recognition. In CVPR, 2016.

[36] F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Srid-
har, D. Casas, and C. Theobalt. GANerated Hands for Real-
Time 3D Hand Tracking from Monocular RGB. In CVPR,
2018.

[17] G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim. First-
Person Hand Action Benchmark with RGB-D Videos and 3D
Hand Pose Annotations. In CVPR, 2018.

[37] F. Mueller, D. Mehta, O. Sotnychenko, S. Sridhar, D. Casas,
and C. Theobalt. Real-time Hand Tracking under Occlusion
from an Egocentric RGB-D Sensor. In ICCV, 2017.

[18] G. Gkioxari, R. Girshick, P. Dollar, and K. He. Detecting and

Recognizing Human-Object Interactions. In CVPR, 2018.

[19] J. C. Gower. Generalized Procrustes Analysis. Psychome-

[38] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised Learn-
ing of Human Action Categories Using Spatio-temporal
Words. In BMVC, 2006.

trika, 40(1):33–51, 1975.

[20] R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska,
S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos,

[39] M. Oberweger, G. Riegler, P. Wohlhart, and V. Lepetit. Efﬁ-
ciently Creating 3D Training Data for Fine Hand Pose Esti-
mation. In CVPR, 2016.

4519

[61] S. Sundaram and W. Mayol-Cuevas. High Level Action
In

Recognition Using Low Resolution Wearable Vision.
CVPR Workshops, 2009.

[62] D. Surie, T. Pederson, F. Lagriffoul, L. Janlert, and D. Sj¨olie.
Activity Recognition Using an Egocentric Perspective of Ev-
eryday Objects. In UIC, 2007.

[63] Y. Tang, Y. Tian, J. Lu, J. Feng, and J. Zhou. Action Recog-

nition in RGB-D Egocentric Videos. In ICIP, 2017.

[64] A. Tejani, D. Tang, R. Kouskouridas, and T.-K. Kim. Latent-
Class Hough Forests for 3D Object Detection and Pose Esti-
mation. In ECCV, 2014.

[65] B. Tekin, P. Marquez-Neila, M. Salzmann, and P. Fua. Learn-
ing to Fuse 2D and 3D Image Cues for Monocular Body Pose
Estimation. In ICCV, 2017.

[66] B. Tekin, S. N. Sinha, and P. Fua. Real-Time Seamless Single

Shot 6D Object Pose Prediction. In CVPR, 2018.

[67] A. Tsoli and A. Argyros. Joint 3D Tracking of a Deformable

Object in Interaction with a Hand. In ECCV, 2018.

[68] D. Tzionas, L. Ballan, A. Srikantha, P. Aponte, M. Pollefeys,
and J. Gall. Capturing Hands in Action Using Discriminative
Salient Points and Physics Simulation.
IJCV, 118(2):172–
193, 2016.

[69] S.-F. Wong, T. K. Kim, and R. Cipolla. Learning Motion
Categories Using Both Semantic and Structural Information.
In CVPR, 2007.

[70] Q. Ye and T.-K. Kim. Occlusion-aware Hand Pose Estima-
tion Using Hierarchical Mixture Density Network. In ECCV,
2018.

[71] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. LIFT: Learned

Invariant Feature Transform. In ECCV, 2016.

[72] S. Yuan, G. Garcia-Hernando, B. Stenger, G. Moon, J. Y.
Chang, K. M. Lee, P. Molchanov, J. Kautz, S. Honari, L. Ge,
J. Yuan, X. Chen, G. Wang, F. Yang, K. Akiyama, Y. Wu,
Q. Wan, M. Madadi, S. Escalera, S. Li, D. Lee, I. Oikono-
midis, A. Argyros, and T.-K. Kim. Depth-Based 3D Hand
Pose Estimation: From Current Achievements to Future
Goals. In CVPR, 2018.

[73] S. Yuan, Q. Ye, B. Stenger, S. Jain, and T.-K. Kim. Big-
Hand2.2M Benchmark: Hand Pose Dataset and State of the
Art Analysis. In CVPR, 2017.

[74] C. Zimmermann and T. Brox. Learning to Estimate 3D Hand

Pose from Single RGB Images. In ICCV, 2017.

[40] M. Oberweger, P. Wohlhart, and V. Lepetit. Training a Feed-

back Loop for Hand Pose Estimation. In ICCV, 2015.

[41] I. Oikonomidis, N. Kyriazis, and A. Argyros. Full DOF
Tracking of a Hand Interacting with an Object by Modeling
Occlusions and Physical Constraints. In ICCV, 2011.

[42] O. Oreifej and Z. Liu. HON4D: Histogram of Oriented 4D
Normals for Activity Recognition from Depth Sequences. In
CVPR, 2013.

[43] P. Panteleris, N. Kyriazis, and A. Argyros. 3D Tracking
of Human Hands in Interaction with Unknown Objects. In
BMVC, 2015.

[44] H. Pirsiavash and D. Ramanan. Detecting Activities of Daily

Living in First-Person Camera Views. In CVPR, 2012.

[45] Y. Poleg, E. Phrat, S. Peleg, and C. Arora. Compact CNN

for Indexing Egocentric Videos. In WACV, 2016.

[46] M. Rad and V. Lepetit. BB8: A Scalable, Accurate, Robust
to Partial Occlusion Method for Predicting the 3D Poses of
Challenging Objects without Using Depth. In ICCV, 2017.

[47] H. Rahmani and A. Mian. 3D Action Recognition from

Novel Viewpoints. In CVPR, 2016.

[48] J. Redmon and A. Farhadi. YOLO9000: Better, Faster,

Stronger. In CVPR, 2017.

[49] X. Ren and C. Gu. Figure-ground Segmentation Improves
Handled Object Recognition in Egocentric Video. In CVPR,
2010.

[50] X. Ren and C. Gu. First-Person Activity Recognition: What

Are They Doing to Me? In CVPR, 2013.

[51] G. Rogez, J. Supancic, and D. Ramanan. First-Person Pose
Recognition Using Egocentric Workspaces. In CVPR, 2015.

[52] G. Rogez, J. Supancic, and D. Ramanan. Understanding
Everyday Hands in Action from RGB-D Images. In ICCV,
2015.

[53] J. Romero, H. Kjellstr¨om, and D. Kragic. Hands in Action:
Real-Time 3D Reconstruction of Hands in Interaction with
Objects. In ICRA, 2010.

[54] J. Romero, D. Tzionas, and M. J. Black. Embodied Hands:
Modeling and Capturing Hands and Bodies Together. ACM
Transactions on Graphics, (Proc. SIGGRAPH Asia), 36(6),
2017.

[55] M. Ryoo, B. Rothrock, and L. Matthies. Pooled Motion Fea-

tures for First-Person Videos. In CVPR, 2015.

[56] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. Lillicrap. A Simple Neural
Network Model for Relational Reasoning. In NIPS, 2017.

[57] G. Sigurdsson, A. Gupta, C. Schmid, A. Farhadi, and K. Ala-
hari. Actor and Observer: Joint Modeling of First and Third-
Person Videos. In CVPR, 2018.

[58] A. Spurr, J. Song, S. Park, and O. Hilliges. Cross-modal

Deep Variational Hand Pose Estimation. In CVPR, 2018.

[59] S. Sridhar, F. Mueller, A. Oulasvirta, and C. Theobalt. Fast
and Robust Hand Tracking Using Detection-Guided Opti-
mization. In CVPR, 2015.

[60] S. Sridhar, F. Mueller, M. Zollhoefer, D. Casas,
A. Oulasvirta, and C. Theobalt. Real-time Joint Tracking
of a Hand Manipulating an Object from RGB-D Input.
In
ECCV, 2016.

4520

