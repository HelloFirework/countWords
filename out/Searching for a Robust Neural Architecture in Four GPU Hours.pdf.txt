Searching for A Robust Neural Architecture in Four GPU Hours

Xuanyi Dong1,2, Yi Yang1

1University of Technology Sydney 2Baidu Research
xuanyi.dong@student.uts.edu.au, yi.yang@uts.edu.au

Abstract

Conventional neural architecture search (NAS) ap-
proaches are based on reinforcement learning or evolution-
ary strategy, which take more than 3000 GPU hours to ﬁnd
a good model on CIFAR-10. We propose an efﬁcient NAS
approach learning to search by gradient descent. Our ap-
proach represents the search space as a directed acyclic
graph (DAG). This DAG contains billions of sub-graphs,
each of which indicates a kind of neural architecture. To
avoid traversing all the possibilities of the sub-graphs, we
develop a differentiable sampler over the DAG. This sam-
pler is learnable and optimized by the validation loss after
training the sampled architecture. In this way, our approach
can be trained in an end-to-end fashion by gradient descent,
named Gradient-based search using Differentiable Archi-
tecture Sampler (GDAS). In experiments, we can ﬁnish one
searching procedure in four GPU hours on CIFAR-10, and
the discovered model obtains a test error of 2.82% with only
2.5M parameters, which is on par with the state-of-the-art.

1. Introduction

Designing an efﬁcient and effective neural architecture
requires substantial human effort and takes a long time [7,
9, 10, 12, 14, 20, 37, 44]. Since the birth of AlexNet [20]
in 2012, human experts have conducted a huge number of
experiments, and consequently devised several useful struc-
tures, such as attention [7] and residual connection [12].
However, the inﬁnite possible choices of network architec-
ture make the manual search unfeasible [1]. Recently, neu-
ral architecture search (NAS) has increasingly attracted the
interest of researchers [1, 6, 17, 22, 25, 31, 46]. These ap-
proaches learn to automatically discover good architectures.
They can thus reduce the labour of human experts and ﬁnd
better neural architectures than the human-invented archi-

∗Part of this work was done when Xuanyi Dong was a research intern
with Baidu Research. Code is publicly available on GitHub: https:
//github.com/D-X-Y/GDAS.
†Corresponding Author: Yi Yang

0

3

: input

: output 

: sampled
: unsampled 

1

GDAS

on a
DAG

0

2

3

Figure 1. We utilize a DAG to represent the search space of a neu-
ral cell. Different operations (colored arrows) transform one node
(square) to its intermediate features (little circles). Meanwhile,
each node is the sum of the intermediate features transformed from
the previous nodes. As indicated by the solid connections, the
neural cell in the proposed GDAS is a sampled sub-graph of this
DAG. Speciﬁcally, among the intermediate features between every
two nodes, GDAS samples one feature in a differentiable way.

tectures. Therefore, NAS is an important research topic in
machine learning.

Most NAS approaches apply evolutionary algorithms
(EA) [32, 23, 33] or reinforcement learning (RL) [46, 47, 3]
to design neural architectures automatically. In both RL-
based and EA-based approaches, their searching procedures
require the validation accuracy of numerous architecture
candidates, which is computationally expensive [47, 32].
For example, the typical RL-based method utilizes the val-
idation accuracy as a reward to optimize the architecture
generator [46]. An EA-based method leverages the valida-
tion accuracy to decide whether a model will be removed
from the population or not [33]. These approaches use a
large amount of computational resources, which is inefﬁ-
cient and unaffordable. This motivates researchers to reduce
the computational cost.

1761

In this paper, we propose a Gradient-based search-
ing approach using Differentiable Architecture Sampling
(GDAS). It can search for a robust neural architecture in
four hours with a single V100 GPU. GDAS signiﬁcantly
improves efﬁciency compared to the previous methods. We
start by searching for a robust neural “cell” instead of a neu-
ral network [46, 47]. A neural cell contains multiple func-
tions to transform features, and a neural network consists of
many copies of the discovered neural cell [22, 47]. Fig. 1 il-
lustrates our searching procedure in detail. We represent the
search space of a cell by a DAG. Every grey square node in-
dicates a feature tensor, numbered by the computation order.
Different colored arrows indicate different kinds of opera-
tions, which transform one node into its intermediate fea-
tures. Meanwhile, each node is the sum of the intermedi-
ate features transformed from the previous nodes. During
training, the proposed GDAS samples a sub-graph from the
whole DAG, indicated by solid connections in Fig. 1. In this
sub-graph, each node only receives one intermediate feature
from every previous node. Speciﬁcally, among the interme-
diate features between every two nodes, GDAS samples one
feature in a differentiable way. In this way, GDAS can be
trained by gradient descent to discover a robust neural cell
in an end-to-end fashion.

The fast searching ability of GDAS is mainly due to the
sampling behavior. A DAG contains hundreds of parametric
operations with millions of parameters. Directly optimizing
this DAG [24] instead of sampling a sub-graph leads to two
disadvantages. First, it costs a lot of time to update nu-
merous parameters in one training iteration, increasing the
overall training time to more than one day [24]. Second,
optimizing different operations together could make them
compete with each other. For example, different operations
could generate opposite values. The sum of these oppo-
site values tends to vanish, breaking the information ﬂow
between the two connected nodes and destabilizing the op-
timization procedure. To solve these two problems, the pro-
posed GDAS samples a sub-graph at one training iteration.
As a result, we only need to optimize a part of the DAG
at one iteration, which accelerates the training procedure.
Moreover, the inappropriate competition is avoided, which
makes the optimization effective.

In summary, GDAS has the following beneﬁts:

1. Compared to previous RL-based and EA-based meth-
ods, GDAS makes the searching procedure differentiable,
which allows us to end-to-end learn a robust searching rule
by gradient descent. For RL-based and EA-based methods,
feedback (reward) is obtained after a prolonged training tra-
jectory, while feedback (loss) in our gradient-based method
is instant and is given in every iteration. As a result, the
optimization of GDAS is potentially more efﬁcient.

2. Instead of using the whole DAG, GDAS samples one
sub-graph at one training iteration, accelerating the search-

ing procedure. Besides, the sampling in GDAS is learnable
and contributes to ﬁnding a better cell.

3. GDAS delivers a strong empirical performances while
using fewer GPU resources. On CIFAR-10, GDAS can ﬁn-
ish one searching procedure in several GPU hours and dis-
cover a robust neural network with a test error of 2.82%. On
PTB, GDAS discovers a RNN model with a test perplexity
of 57.5. Moreover, the networks discovered on CIFAR and
PTB can be successfully transferred to ImageNet and WT2.

2. Related Work

Recently, researchers have made signiﬁcant progress in
automatically discovering good architectures [46, 47, 23,
22, 33]. Most NAS approaches can be categorized in two
modalities: macro search and micro search.

Macro search algorithms aim to directly discover the
entire neural networks [5, 4, 38, 46, 21]. To search convo-
lutional neural networks (CNNs) [20], typical approaches
apply RL to optimize the searching policy to discover ar-
chitectures [1, 5, 46, 31]. Baker et al. [1] trained a learn-
ing agent by Q-learning to sequentially choose CNN lay-
ers. Zoph and Le [46] utilized long short-term memory
(LSTM) [13] as a controller to conﬁgure each convolu-
tional layer, such as the ﬁlter shape and the number of ﬁl-
ters. In these macro search algorithms [1, 5, 46], the num-
ber of possible networks is exponential to the depth of a
network, e.g., a depth of 12 can result in more than 1029
possible networks [31].
It is difﬁcult and ineffective to
search networks in such a large search space, and, there-
fore, these macro search methods [31, 46, 5] usually limit
the CNN models to be shallow, e.g., a depth is less than 12.
Since macro-discovered networks are shallower than deep
CNNs [12, 14], their accuracies are limited. In contrast, our
GDAS allows the network to be much deeper by stacking
tens of discovered cells [47] and thus can achieve a better
accuracy.

Micro search algorithms aim to discover neural cells
and design a neural architecture by stacking many copies
of the discovered cells [47, 32, 33, 31]. A typical mi-
cro search approach is NASNet [47], which extends the
approach of [46] to search neural cells in the proposed
“NASNet search space”. Following NASNet [47], many
researchers propose their methods based on the NASNet
search space [22, 24, 5, 32]. For example, Real et al. [32]
applied EA algorithm with a simple regularization tech-
nique to search neural cells. Liu et al. [22] proposed a
progressive approach to search cells from shallow to deep
gradually. These micro search algorithms usually take more
than 100 GPU days [22, 47]. Even though some of them re-
duce the searching cost, they still take more than one GPU
day [24]. Our GDAS is a also micro search algorithm, fo-
cusing on search cost reduction. In experiments, we can ﬁnd
a robust network within fewer GPU hours, which is 1000×

1762

less than the standard NAS approach [47].

Improving Efﬁciency. Since NAS algorithms usually
require expensive computational resources [46, 47, 32], an
increasing number of researchers focus on improving the
architecture search speed [5, 22, 31, 38, 24]. A variety
of techniques have been proposed, such as progressive-
complexity search stages [22], accuracy prediction [2], Hy-
perNet [4], Net2Net transformation [5], and parameter shar-
ing [31]. For instance, Cai et al. [5] reused weights of pre-
viously discovered networks to amortize the training cost.
Pham et al. [31] shared parameters between different child
networks to improve the efﬁciency of the searching proce-
dure. Brock et al. [4] utilized a network to generate model
parameters given a discovered network, avoiding fully train-
ing from scratch. Liu et al. [24] relaxed the search space to
be continuous, so that they can use gradient descent to effec-
tively search cells. Though these approaches successfully
accelerate the architecture search procedure, several GPU
days are still required [22, 5]. Our GDAS samples individ-
ual architecture in a differentiable way to effectively dis-
cover architecture. As a result, GDAS can ﬁnish the search
procedure in several GPU hours on CIFAR-10, which is
much faster than these efﬁcient methods.

Contemporary to this work, Xie et al. [39] applied a sim-
ilar technique to relax the discrete candidate sampling as
ours. They focus on ﬁxing the inconsistency between the
loss of attention-based NAS [24] and their objective.
In
contrast, we focus on making the sampling procedure dif-
ferentiable and accelerating the searching procedure.

3. Methodology

3.1. Search Space as a DAG

We search for the neural cell in the search space and
stack this cell in series to compose the whole neural net-
work. For CNN, a cell is a fully convolutional network that
takes output tensors of previous cells as inputs and gener-
ates another feature tensor. For recurrent neural network
(RNN), a cell takes the feature vector of the current step
and the hidden state of the previous step as inputs, and gen-
erates the current hidden state. For simpliﬁcation, we take
CNN as an example for the following description.

We represent the cell in CNN as a DAG G consisting
of an ordered sequence of B computational nodes. Each
computational node represents one feature tensor, which is
transformed from two previous feature tensors. This proce-
dure can be formulated as shown in Eq. (1) following [47].

Ii = fi,j(Ij) + fi,k(Ik)

s.t.

j < i & k < i,

(1)

where Ii, Ij , and Ik indicate the i-th, j-th, and k-th nodes,
respectively. fi,j and fi,k indicate two functions from the
candidate function set F. We denote the computational
nodes of a cell as B. Taking B = 4 as an example, a

Image

3x3
conv

block

Reduction

Cell

block

Reduction

Cell

block

soft
max

CIFAR

Architecture

ImageNet

Architecture

Normal

Normal

Normal

Cell

Cell
A block with N normal cells

Cell

Normal

Cell

Image

3x3
conv

3x3
conv

3x3
conv

stride 2

stride 2

stride 2

block

Reduction

Cell

block

Reduction

Cell

block

soft
max

Figure 2. The strategy to design CIFAR architecture (top) and
ImageNet architecture (bottom) based on the discovered cell. We
use the same block structure (middle) in two cases. Both normal
and reduction cells receive the outputs of two previous cells as
inputs, as illustrated in the middle.

I⌢

I⌢

cell contains 7 nodes in total, i.e., {Ii|1 ≤ i ≤ 7}. I1
and I2 nodes are the cell outputs in the previous two lay-
ers. I3, I4, I5, and I6 nodes are the computational nodes
calculated by Eq. (1). I7 indicates the output tensor of this
cell, which is the concatenation of the four computational
nodes, i.e., I7 = I⌢
In GDAS, the candidate
3
function set F contains the following 8 functions: (1) iden-
tity, (2) zeroize, (3) 3x3 depth-wise separate conv, (4) 3x3
dilated depth-wise separate conv, (5) 5x5 depth-wise sepa-
rate conv, (6) 5x5 dilated depth-wise separate conv, (7) 3x3
average pooling, (8) 3x3 max pooling. We use the same
candidate function set F as [24], which is similar to [47]
but removes some unused functions and adds some useful
functions.

I6.

4

5

From cell to network. We search for two kinds of cells,
i.e., a normal cell and a reduction cell. For the normal cell,
each function in F has the stride of 1. For the reduction cell,
each function in F has the stride of 2. Once we discover one
normal cell and one reduction cell, we stack many copies
of these discovered cells to make up a neural network. As
shown in Fig. 2, for the CIFAR architecture, we stack N
normal cells as one block. Given an image, it ﬁrst forwards
through the network head part, i.e., one 3 by 3 convolutional
layer. It then forwards through three blocks with two reduc-
tion cells in between. The ImageNet architecture is similar
to the CIFAR architecture, but the network head part con-
sists of three 3 by 3 convolutional layers. We follow [24] to
setup these two overall structures.

3.2. Searching by Differentiable Model Sampling

Formally, we denote a neural architecture as α and the
weights of this neural architecture as ωα. The goal of NAS
is to ﬁnd an architecture α, which can achieve the minimum
validation loss after being trained by minimizeing the train-
ing loss, as shown in Eq. (2).

min

α

E(x′,y′)∼DV − log Pr(y′|x′; α, ω∗

α),

s.t. ω∗

α = arg minω E(x,y)∼DT − log Pr(y|x; α, ωα),

(2)

1763

where ω∗
α is the best weight of α and achieves the mini-
mum training loss. We use the negative log likelihood as
the training objective, i.e., − log Pr. DT and DV indicate
the training set and the validation set, respectively. (x, y)
and (x′, y′) are the data associated with its label, which are
sampled from DT and DV , respectively.

An architecture α consists of many copies of the neural
cell. This cell is sampled from the search space represented
by G. Speciﬁcally, between nodei and nodej , we sample
one transformation function from F from a discrete proba-
bility distribution Ti,j . During the search, we calculate each
node in a cell as:

Ii =

i−1

X

j=1

fi,j (Ij; Wfi,j )

s.t. fi,j ∼ Ti,j,

(3)

where fi,j is sampled from Ti,j and Wfi,j is its associ-
ated weight. The discrete probability distribution Ti,j is
characterized by a learnable probability mass function as
in Eq. (4):

Pr(fi,j = Fk) =

i,j)

exp(Ak
k′=1 exp(Ak′

i,j)

PK

,

(4)

where Ak
i,j is the k-th element of a K-dimensional learn-
able vector Ai,j ∈ RK , and Fk indicates the k-th function
in F. K is the cardinality of F, i.e., K = |F|. Actually, Ai,j
encodes the sampling distribution of the function between
nodei and nodej . As a result, the sampling distribution of a
neural cell is encoded by all Ai,j , i.e., A = {Ai,j}.

Given Eq. (3) and Eq. (4), we can obtain α and ω, and
thus can calculate Pr(y|x; α, ω) in Eq. (2). However, since
Eq. (3) needs to sample from a discrete probability distri-
bution, we cannot back-propagate gradients through Ai,j
in Eq. (4) to optimize Ai,j . To allow back-propagation,
we ﬁrst use the Gumbel-Max trick [11, 27] to re-formulate
Eq. (3) as Eq. (5), which provides an efﬁcient way to draw
samples from a discrete probability distribution.

i−1

K

Ii =

X

X

hk
i,j

Fk(Ij; W k

i,j),

j=1

k=1

s.t. hi,j = one hot(arg max

k

(Ak

i,j + ok)),

(5)

(6)

i,j is the k-th element of hi,j . W k

where ok are i.i.d samples drawn from Gumbel (0,1)1, and
hk
i,j is the weight of Fk
for the transformation function between nodei and nodej .
Then, we use the softmax function to relax the arg max
function so as to make Eq. (5) being differentiable [16, 26].
Formally, we use Eq. (7) to approximate Eq. (5).

˜hk

i,j =

i,j + ok)/τ )

exp((Ak
k′=1 exp((Ak′

PK

i,j + ok′ )/τ )

,

(7)

1 oi = − log(− log(u)) with u ∼ Unif [0, 1]

Algorithm 1 Searching Algorithm based on AOS
Input: the training set DT , and the validation set DV

randomly initialized A and W, and the batch size n

while not converge do

i=1 from DT
i=1 ℓ(xi, yi) based on Eq. (8)

Sample batch of data Dt = {(xi, yi)}n
Calculate LT = Pn
Update W by gradient descent: W = W − ▽W LT
Sample batch of data Dv = {(xi, yi)}n
i=1 from DV
Calculate LV = Pn
i=1 ℓ(xi, yi) based on Eq. (8)
Update A by gradient descent: A = A − ▽ALV

end while

where τ is the softmax temperature. When τ → 0, ˜hk
i,j =
i,j . When τ → ∞, each element in ˜h will be the same
hk
and the approximated distribution will be smooth. To be
noticed, we use the arg max function in Eq. (5) during the
forward pass but the soft max function in Eq. (7) during the
backward pass to allow gradient back-propagation.

Training. Reviewing the objective of NAS in Eq. (2),
the main challenge is learning to ﬁnd architecture α. By
utilizing Eq. (7), we can make the sampling procedure dif-
ferentiable and learn a distribution of neural cells (repre-
senting architectures). However, it is still intractable to
directly solve Eq. (2), because the nested formulation in
Eq. (2) needs to calculate high order derivatives. In prac-
tice, to avoid calculating high order derivatives, we apply
the alternative optimization strategy to update the sampling
distribution TA and the weights of all functions W in an
iterative way. Given one data sample x and its associated
label y, we calculate the loss as:

ℓ(x, y) = − log Pr(y|x; α, ωα),

s.t. α ∼ TA & ωα ⊂ W,

(8)

(9)

where TA is the distribution encoded by A and W =
{W k
i,j} represents the weights of all functions in all cells of
the network. Note that, for one data sample, it ﬁrst samples
α from TA and then calculates the network output only on
its associated weight ωα, which is a part of W. As shown in
Alg. 1, we apply the alternative optimization strategy (AOS)
to update A based on the validation losses from DV and up-
date W based on the training losses from DT . It is essential
to train W on DT and A on DV , because (1) this strategy
is theoretically sound with the objective Eq. (2); and (2)
this strategy can improve the generalization ability of the
searched structure.

Architecture. After training, we need to derive the ﬁ-
nal architecture from the learned A. Each nodei connects
with T previous nodes. Following the previous works, we
use T = 2 for CNN [47, 24] and T = 1 for RNN [31, 24].
Suppose Ω is the candidate index set, we derive the ﬁnal
architecture by the following procedure: (1) deﬁne the im-
portance of the connection between nodei and nodej as:

1764

Input-1

Input-2

yielding fewer parameters and higher accuracy.

1x3 conv 1x2 stride

3x1 conv 2x1 stride

1x1 conv

3x3 MaxPool

3x3 MaxPool

1x3 conv 1x2 stride

concatenate

Output

3x1 conv 2x1 stride

4. Experimental Study

1x1 conv

4.1. Datasets

Figure 3. The designed reduction cell. “1x3 conv 1x2 stride”
indicates a convolutional layer with 1 by 3 kernel and 1 by 2 stride.

maxk∈Ω Pr(fi,j = Fk). (2) for each nodei, retain T con-
nections with the maximum importance from the previous
nodes. (3) for the retained connection between nodei and
nodej , we use the function Farg maxk∈Ω Pr(fi,j =Fk). Ω is
{1, ..., K} by default.

Acceleration. In Eq. (5), hi,j is a one-hot vector. As
a result, in the forward procedure, we only need to calcu-
late the function Farg max(hi,j ). During the backward pro-
cedure, we only back-propagate the gradient generated at
the arg max(˜hi,j). In this way, we can save most compu-
tation time and also reduce the GPU memory cost by about
|F| times. Within one training batch, each sample produces
a different hi,j , and, therefore, each element in Ai,j has a
high possibility of being updated with gradients.

One beneﬁt of this acceleration trick is that it allows
us to directly search on the large-scale dataset (e.g., Ima-
geNet) due to the saved GPU memory. We did some ex-
periments to directly search on ImageNet using the same
hyper-parameters as on the small datasets, however, failed
to obtain a good performance. Searching on a large-scale
dataset might require different hyper-parameters and needs
careful tuning. We will explore this in our future work.

3.3. Discussion on the Reduction Cell

Revisiting state-of-the-art architectures designed by hu-
man experts, AlexNet [20] and VGGNet [36] use the max
pooling to reduce the spatial dimension; ResNet [12] uses
a convolutional layer with stride of 2; and DenseNet [14]
uses a 1 by 1 convolutional layer followed by average pool-
ing to reduce dimension. These human-designed reduc-
tion cells are simple and effective. The automatically dis-
covered reduction cells are also usually similar and sim-
ple [47, 31, 24]. For example, the reduction cell discovered
by [24] only has max pooling and identity operations.

Most human-designed and automatically discovered re-
duction cells are simple and can achieve a high accuracy.
Moreover, compared to searching one normal cell, jointly
searching a normal cell and a reduction cell will greatly in-
crease the search space and make the optimization difﬁcult.
We hope to ﬁnd a better network by ﬁxing the reduction
cell. Inspired by [36, 24], we design a ﬁxed reduction cell
as shown in Fig. 3. In the experiments, with this human-
designed reduction cell, GDAS ﬁnds a better architecture,

CIFAR-10 and CIFAR-100 [19] consist of 50K training
images and 10K test images. CIFAR-10 categorizes images
into 10 classes, while CIFAR-100 has 100 classes.

ImageNet [34] is a large-scale and well-known bench-
mark for image classiﬁcation. It contains 1K classes, 1.28
million images for training, and 50K images for validation.
Penn Treebank (PTB) [28] is a corpus consisting of
over 4.5 million words of American English words. We pre-
process PTB following [30, 29].

WikiText-2 (WT2) [30] is a collection of 2 million to-
kens from the set of veriﬁed Good and Featured articles
on Wikipedia. The training set contains 600 articles with
2,088,628 tokens. The validation set contains 60 articles
with 217,646 tokens. The test set contains 60 articles with
245,569 tokens.

4.2. Search for CNN

CNN Searching Setup. The neural cells for CNN are
searched on CIFAR-10 following [46, 47, 22, 31]. We ran-
domly split the ofﬁcial training images into two groups,
with each group containing 25K images. One group is used
as the training set DT in Alg. 1, and the other is used as the
validation set DV in Alg. 1. The candidate function set F
has 8 different functions as introduced in Sec. 3.1. The de-
fault hyper-parameters for each function in F are the same
in [24, 47, 22]. By default, we set the number of initial
channels in the ﬁrst convolution layer C as 16; set the num-
ber of computational nodes in a cell B as 4; and the num-
ber of layers in one block N as 2. We train the model by
240 epochs in total. For ω, we use the SGD optimization.
We start with a learning rate of 0.025 and anneal it down
to 1e-3 following a cosine schedule. We use the momen-
tum of 0.9 and the weight decay of 3e-4. For α, we use the
Adam optimization [18] with the learning rate of 3e-4 and
the weight decay of 1e-3. The τ is initialized as 10 and is
linearly reduced to 1. To search the normal cell and the re-
duction cell on CIFAR-10, our GDAS takes about ﬁve hours
to ﬁnish the search procedure on a single NVIDIA Tesla
V100 GPU. As discussed in Sec. 3.3, we also run experi-
ments to only search the normal cell and ﬁx the reduction
cell as shown in Fig. 3, denoted as GDAS (FRC). When we
use GDAS (FRC) to search on CIFAR-10, it takes less than
four hours to ﬁnish one search procedure. Following [24],
we run GDAS 4 times with different random seeds and pick
the best cell based on its validation performance. This pro-
cedure can reduce the high variance of the searched results,
especially when searching the RNN structure.

1765

Type Method

Venue

GPUs

Human
expert

ResNet + CutOut [12]
DenseNet-BC [14]

Macro
search
space

MetaQNN [1]
Net Transformation [5]
SMASH [4]
NAS [46]
NAS + more ﬁlters [46]
ENAS [31]

Micro
search
space

Hierarchical NAS [23]
Progressive NAS [22]
NASNet-A [47]
NASNet-A + CutOut [47]
ENAS [31]
ENAS + CutOut [31]
DARTS (1st) + CutOut [24]
DARTS (2nd) + CutOut [24]
GHN + CutOut [41]
NAONet [25]
AmoebaNet-A + CutOut [32]
GDAS [C=36,N=6]
GDAS [C=36,N=6] + CutOut
GDAS (FRC) [C=36,N=6]
GDAS (FRC) [C=36,N=6] + CutOut

CVPR16
CVPR17

ICLR17
AAAI18
ICLR18
ICLR17
ICLR17
ICML18

ICLR18
ECCV18
CVPR18
CVPR18
ICML18
ICML18
ICLR19
ICLR19
ICLR19

NeurIPS18

AAAI19
CVPR19
CVPR19
CVPR19
CVPR19

−
−

10
5
1

800
800

1

200
100
450
450

1
1
1
1
1

200
450

1
1
1
1

Times
(days)

−
−

8-10

2
1.5

21-28
21-28
0.32

1.5
1.5
3-4
3-4
0.45
0.45
0.38

1

0.84

1
7

0.21
0.21
0.17
0.17

Params
(million) CIFAR-10 CIFAR-100

Error on

Error on

1.7
25.6

11.2
19.7
16.0
7.1
37.4
38.0

61.3
3.2
3.3
3.3
4.6
4.6
3.3
3.4
5.7
10.6
3.1
3.4
3.4
2.5
2.5

4.61
3.46

6.92
5.70
4.03
4.47
3.65
3.87

3.63
3.63
3.41
2.65
3.54
2.89
3.00
2.82
2.84
3.18
3.12
3.87
2.93
3.75
2.82

22.10
17.18

27.14

−
−
−
−
−
−

19.53

−
−

19.43

−
−

17.54†

−
−

18.93†
19.68
18.38
19.09
18.13

Table 1. Classiﬁcation errors of GDAS and baselines on CIFAR. † indicates the results trained using our setup. “FRC” indicates that we
ﬁx the reduction cell and only search the normal cell. Note that researchers might run their algorithms on different kinds of machines. The
searching costs are derived from the original papers, and we did not normalize them across different GPUs. Our experiments are based on
the V100 GPU; and if we run on Titan 1080Ti, the searching cost will increase to about seven GPU hours.

Clariﬁcations on the searching cost (GPU days) of dif-
ferent methods. The searching costs listed in Tab. 1 and
Tab. 2 are not normalized across different GPU devices.
Different algorithms might run on different machines, and
we simply refer the searching costs reported in their pa-
pers.2 If we use other GPU devices, the searching cost of
“GDAS (FRC)” could be a different number. For example,
if we use Titan 1080Ti, the search cost will increase to about
seven GPU hours.

Discussion on the acceleration step. If we do not apply
the acceleration step introduced in Sec. 3.2, each iteration
will cost |F|=8× more time and GPU memory than GDAS.
In the same time, without this acceleration step, it requires
less training epochs to converge but still costs more time
than applying the acceleration step.

Results on CIFAR. After the searching procedure, we
use C=36, B=4, and N=6 to form a CNN. Following the
previous works [24, 31, 47], we train the network by 600
epochs in total. We start the learning rate of 0.025 and re-
duce it to 0 with the cosine learning rate scheduler. We
set the probability of path dropout as 0.2 and the auxiliary

tower with the weight of 0.4 [46]. We use the standard pre-
processing and data augmentation, i.e., randomly cropping,
horizontally ﬂipping, normalization, and CutOut [8, 43].

We compare the models discovered by our approach with
other state-of-the-art models in Tab. 1. The models dis-
covered by the macro search algorithms obtain a higher er-
ror than the models discovered by the micro search algo-
rithms. Using GDAS, we discover a model with 3.3M pa-
rameters, which achieves 2.93% error on CIFAR-10. Using
GDAS (FRC), we discover a model with only 2.5M parame-
ters, which achieves 2.82% error on CIFAR-10. NASNet-A
achieves a lower error rate than ours, but it contains more
than 80% of the parameters than the model discovered by
GDAS (FRC). Notably, our GDAS discovers a comparable
model with the state-of-the-art, whereas the searching cost
of our approach is much less than the others. For example,
GDAS (FRC) takes less than 4 hours on a single V100 GPU,
which is about 0.17 GPU days. It is faster than NASNet by
almost 104 times. ENAS is a recent work that focuses on ac-
celerating the searching procedure. ENAS is very efﬁcient,
whereas our GDAS (FRC) is three times faster than ENAS.

2It is difﬁcult for us to run all algorithms on the same GPU.

Results on ImageNet. Following [47, 24, 31, 35], we

1766

Type

Method

Venue GPUs

Times Test Error (%) Params
(days) Top-1 Top-5 (million)

+×

(million)

Human expert

Micro search space

Inception-v1 [37]
MobileNet-V2 [35]
ShufﬂeNet [42]

CVPR15
CVPR18
CVPR18

ECCV18
Progressive NAS [22]
CVPR18
NASNet-A [47]
CVPR18
NASNet-B [47]
CVPR18
NASNet-C [47]
ICLR19
DARTS (2nd) [24]
ICLR19
GHN [41]
AAAI19
AmoebaNet-A [32]
AAAI19
AmoebaNet-B [32]
AAAI19
AmoebaNet-C [32]
GDAS [C=50,N=4]
CVPR19
GDAS (FRC) [C=52,N=4] CVPR19

−
−
−

100
450
450
450

1
1

450
450
450

1
1

−
−
−

1.5
3-4
3-4
3-4
1

0.84

7
7
7

0.21
0.17

30.2
28.0
26.3

25.8
26.0
27.2
27.5
26.9
27.0
25.5
26.0
24.3
26.0
27.5

10.1
−
−

8.1
8.4
8.7
9.0
9.0
8.7
8.0
8.5
7.6
8.5
9.1

6.6
3.4
∼5

5.1
5.3
5.3
4.9
4.9
6.1
5.1
5.3
6.4
5.3
4.4

1448
300
524

588
564
488
558
595
569
555
555
570
581
497

Table 2. Top-1 and top-5 errors of GDAS and baselines on ImageNet. +× indicates the number of multiply-add operations. We refer
results reported in [24] for Progressive NAS, NASNet, and AmoebaNet.

Architecture

V-RHN [45]
LSTM [29]

LSTM + SC [29]
LSTM + SE [40]

Perplexity Params Search Cost
test
val
(GPU days)
67.9 65.4
60.7 58.8
60.9 58.3
58.1 56.0

(M)
23
24
24
22

NAS [46]
ENAS [31]

− 64.0
60.8 58.6
DARTS (1st) [24]
60.2 57.6
DARTS (2nd) [24] 58.1 55.7
59.8 57.5

GDAS

25
24
23
23
23

−
−
−
−
104
0.5
0.13
0.25
0.4

Table 3. Comparison w.r.t.
the perplexity of different language
models on PTB (lower perplexity is better). V-RHN indicates
Variational RHN [45]. LSTM + SC indicates LSTM with skip
connection [29]. LSTM + SE indicates LSTM with 15 softmax
experts [40]. The ﬁrst four models are designed by human experts,
and the last four models are automatically searched by machine.

use the ImageNet-mobile setting, in which the input size is
224×224 and the number of multiply-add operations is re-
stricted to be less than 600M. We train models by SGD with
250 epochs and use the batch size of 128. We initialize the
learning rate of 0.1 and reduce it by 0.97 after each epoch.

We compare our results on ImageNet with the other
methods in Tab. 2. Most algorithms in Tab. 2 take more than
1000 GPU days to discover a good CNN cell. DARTS [24]
uses minimum resources among the compared algorithms,
whereas ours is even faster than DARTS [24] by more than
10 times. For GDAS (FRC), we use C=52 and N=4 to con-
struct the model following the setting in [24]. For GDAS, if
we use C=52 and N=4, the number of multiply-add opera-
tions will be larger than 600 MB, and thus we use C=50 to

Architecture

LSTM + AL [15]

LSTM [29]

LSTM + SC [29]
LSTM + SE [40]

Perplexity Params Search Cost
test
val
(GPU days)
91.5 82.0
69.1 65.9
69.1 65.9
66.0 63.3

(M)
28
33
23
33

−
−
−
−

ENAS [31]

72.4 70.4
DARTS (2nd) [24] 71.2 69.6
71.0 69.4

GDAS

33
33
33

0.5
0.25
0.4

Table 4. Comparison with different language models on WT2
(lower perplexity is better). LSTM + AL indicates LSTM with
augmented loss [15]. Other notation is the same as in Tab. 3.

restrict it to be less than 600MB. Our model, GDAS (FRC)
[C=52,N=4], costs about 20% less multiply-add operations
than [24] but obtains the same top-5 error. AmoebaNet-A
and Progressive NAS achieve a slightly lower test error than
ours. However, their methods cost a prohibitive amount of
GPU resources. The results in Tab. 2 show the discovered
cell on CIFAR-10 can be successfully transferred to Ima-
geNet and achieve competitive performance.

4.3. Search for RNN

RNN Searching Setup. The neural cells for RNN are
searched on PTB with the splits following [24, 31] The can-
didate function set F contain 5 functions, i.e., zeroize, Tanh,
ReLU, sigmoid, and identity. We use B=9 to search the
RNN cell. The RNN model consists of one word embed-
ding layer with a hidden size of 300, one RNN cell with
a hidden size of 300, and one decoder layer. We train the
model by 200 epochs with a batch size of 128 and a BPTT
length of 35. We optimize ω by Adam with a learning rate

1767

Architecture

Params Error on CIFAR-10

GDAS-N + GDAS-R

GDAS-N + FIX-R
FRC-N + GDAS-R

FRC-N + FIX-R

3.3
3.0
2.9
2.5

2.93
2.87
2.84
2.82

Table 5.
Different normal and reduction cell combinations.
GDAS-N and GDAS-R indicate the normal and reduction cells
discovered by GDAS, respectively. FRC-N and FIX-R mean the
normal cell from GDAS (FRC) and our designed reduction cell,
respectively.

of 20 and a weight decay of 5e-7. We optimize α by Adam
with a learning rate of 3e-3 and a weight decay of 1e-3.
Other setups are the same in [31, 24].

Results on PTB. We evaluate the RNN model formed by
the discovered recurrent cell on PTB. We use a batch size
of 64 and a hidden size of 850. We train the model using
the A-SGD by 2000 epochs. The learning rate is ﬁxed as 20
and the weight decay is 8e-7. DARTS [24] and ENAS [31]
greatly reduce the search cost compared to previous meth-
ods. Our GDAS incurs a lower search cost than all the pre-
vious methods. Note that our code is not heavily optimized
and the theoretical search cost should be less than the one
reported in Tab. 3.

We compare different RNN models in Tab. 3. The model
discovered by GDAS achieves a validation perplexity of
59.8 and a test perplexity of 57.5. The performance of our
discovered RNN is on par with the state-of-the-art mod-
els in Tab. 3. LSTM + SE [40] obtains better results than
ours, but it is an ensemble method using mixture of softmax.
By applying the SE technique [40], GDAS can achieve the
lower perplexity without doubt. LSTM [29] is an exten-
sively tuned model, whereas our automatically discovered
model is superior to it. Compared to other efﬁcient ap-
proaches, the search cost of GDAS is the lowest.

Results on WT2. To train the model on WT2, we use
the same experiment settings as PTB, but we use a hidden
size of 700 and a weight decay of 5e-7. We train the model
in 3000 epochs in total. Tab. 4 compares different RNN
models on WT2. Our approach achieves competitive results
among all automatically searching approaches. GDAS is
worse than “LSTM + SC” [29]. Since our model is searched
on a small dataset PTB, and the transferable ability of the
discovered model might be a little bit weak. If we directly
search the RNN model on WT2, we could obtain a better
model and improve the transferable ability.

4.4. Discussion

We visualize the discovered cells in Fig. 4. These au-
tomatically discovered cells are complex and hard to be de-
signed manually. Moreover, networks with these discovered
cells can achieve more superior performance than hand-

skip_connect

sep_conv_3x3

sep_conv_5x5

2

c_{k-2}

skip_connect

skip_connect

sep_conv_5x5

1

sep_conv_3x3

3

c_{k}

c_{k-1}

skip_connect

0

sep_conv_5x5

sep_conv_3x3

c_{k-1}

sep_conv_3x3

sep_conv_5x5

sep_conv_5x5

c_{k-2}

sep_conv_5x5

0

3

GDAS : normal cell

sep_conv_5x5

dil_conv_5x5

1

2

c_{k-2}

skip_connect

skip_connect

skip_connect

0

sep_conv_3x3

sep_conv_3x3

c_{k}

skip_connect

skip_connect

sep_conv_3x3

c_{k-1}

1

2

3

c_{k}

GDAS : reduction cell

GDAS (FRC): normal cell

identity

relu

4

3

identity

7

1

relu

x_{t}

0

relu

identity

2

h_{t-1}

tanh

relu

h_{t}

6

8

5

GDAS : recurrent cell

Figure 4. The top block and the middle-left block show the nor-
mal cell and the reduction cell discovered by GDAS, respectively.
The middle-right block shows the discovered normal cell when we
ﬁx the reduction cell. The bottom block shows the discovered re-
current cell.

crafted networks. This demonstrates that automated neural
architecture search is the future of architecture design.

Revisiting Sec. 3.3, we propose a new reduction cell as a
replacement for automated reduction cell. With this reduc-
tion cell, we can more effectively search neural cells. For
further analysis, we use the normal cell found by GDAS
and the proposed reduction cell to construct a new CNN,
denoted as “GDAS-N + FIX-R” in Tab. 5. The accuracy of
this network on CIFAR-10 is similar to “GDAS-N + GDAS-
R” and “FRC-N + FIX-R” in Tab. 5. This result implies that
the reduction cell might have a negligible effect on the per-
formance of networks and the hand-crafted reduction cell
could be on par with the automatically discovered one.

Most recent NAS approaches search neural networks on
the small-scale datasets, such as CIFAR, and then transfer
the discovered networks to the large-scale datasets, such as
ImageNet. The obstacle of directly searching on ImageNet
is the huge computational cost. GDAS is an efﬁcient NAS
algorithm and gives us an opportunity to search on Ima-
geNet. We will explore this research direction in our future
work.

5. Conclusion

In this paper, we propose a Gradient-based neural archi-
tecture search approach using Differentiable Architecture
Sampler (GDAS). Our approach is efﬁcient and reduces the
search cost of the standard NAS approach [47] by about 104
times. Moreover, both CNN and RNN models discovered
by our GDAS can achieve competitive performance com-
pared to state-of-the-art models.

1768

References

[1] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh
Raskar. Designing neural network architectures using rein-
forcement learning. In International Conference on Learning
Representations (ICLR), 2017.

[2] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil
Naik. Accelerating neural architecture search using perfor-
mance prediction. In International Conference on Learning
Representations (ICLR) Workshop, 2018.

[3] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le.
Neural optimizer search with reinforcement learning. In In-
ternational Conference on Machine Learning (ICML), pages
459–468, 2017.

[4] Andrew Brock, Theodore Lim, James M Ritchie, and Nick
SMASH: one-shot model architecture search
In International Conference on

Weston.
through hypernetworks.
Learning Representations (ICLR), 2018.

[5] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun
Wang. Efﬁcient architecture search by network transforma-
tion. In AAAI Conference on Artiﬁcial Intelligence (AAAI),
pages 2787–2794, 2018.

[6] Liang-Chieh Chen, Maxwell D Collins, Yukun Zhu, George
Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam,
and Jonathon Shlens. Searching for efﬁcient multi-scale ar-
chitectures for dense image prediction. In Advances in Neu-
ral Information Processing Systems (NeurIPS), pages 8713–
8724, 2018.

[7] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder–decoder for statistical machine translation. In Con-
ference on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1724–1734, 2014.

[8] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552, 2017.

[9] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan.
More is less: A more complicated network with less infer-
ence complexity.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
5840–5848, 2017.

[10] Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei,
Yi Yang, and Yaser Sheikh. Supervision-by-Registration:
An unsupervised approach to improve the precision of facial
landmark detectors. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
360–368, 2018.

[11] Emil Julius Gumbel. Statistical theory of extreme values and
some practical applications. NBS Applied Mathematics Se-
ries (AMS), 33, 1954.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, 2016.

[13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural Computation, 9(8):1735–1780, 1997.

[14] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2261–2269, 2017.

[15] Hakan Inan, Khashayar Khosravi, and Richard Socher. Ty-
ing word vectors and word classiﬁers: A loss framework for
language modeling. In International Conference on Learn-
ing Representations (ICLR), 2017.

[16] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-
rameterization with gumbel-softmax. In International Con-
ference on Learning Representations (ICLR), 2017.

[17] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider,
Barnabas Poczos, and Eric Xing. Neural architecture search
with bayesian optimisation and optimal transport.
In Ad-
vances in Neural Information Processing Systems (NeurIPS),
pages 2020–2029, 2018.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR), 2015.

[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Uni-
versity of Toronto, 2009.

[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
ImageNet classiﬁcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems (NeurIPS), pages 1097–1105, 2012.

[21] Xin Li, Yiming Zhou, Zheng Pan, and Jiashi Feng. Partial
order pruning: for best speed/accuracy trade-off in neural
architecture search. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2019.

[22] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon
Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan
Huang, and Kevin Murphy. Progressive neural architec-
ture search.
In European Conference on Computer Vision
(ECCV), pages 19–34, 2018.

[23] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha
Fernando, and Koray Kavukcuoglu. Hierarchical representa-
tions for efﬁcient architecture search. In International Con-
ference on Learning Representations (ICLR), 2018.

[24] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS:
Differentiable architecture search. In International Confer-
ence on Learning Representations (ICLR), 2019.

[25] Renqian Luo, Fei Tian, Tao Qin, and Tie-Yan Liu. Neural ar-
chitecture optimization. In Advances in Neural Information
Processing Systems (NeurIPS), pages 7827–7838, 2018.

[26] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The
concrete distribution: A continuous relaxation of discrete
random variables. In International Conference on Learning
Representations (ICLR), 2017.

[27] Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sam-
In Advances in Neural Information Processing Sys-

pling.
tems (NeurIPS), pages 3086–3094, 2014.

[28] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. Building a large annotated corpus of english: The
Penn Treebank. Computational Linguistics, 19(2):313–330,
1993.

1769

Vision and Pattern Recognition (CVPR), pages 6848–6856,
2018.

[43] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
Yi Yang. Random erasing data augmentation. arXiv preprint
arXiv:1708.04896, 2017.

[44] Linchao Zhu, Zhongwen Xu, and Yi Yang. Bidirectional
multirate reconstruction for temporal modeling in videos. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2653–2662, 2017.

[45] Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnık,
and J¨urgen Schmidhuber. Recurrent highway networks.
In International Conference on Machine Learning (ICML),
pages 4189–4198, 2017.

[46] Barret Zoph and Quoc V Le. Neural architecture search
with reinforcement learning. In International Conference on
Learning Representations (ICLR), 2017.

[47] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V.
Le. Learning transferable architectures for scalable image
recognition.
In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 8697–8710, 2018.

[29] Stephen Merity, Nitish Shirish Keskar, and Richard Socher.
Regularizing and optimizing LSTM language models. In In-
ternational Conference on Learning Representations (ICLR),
2018.

[30] Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. Pointer sentinel mixture models.
In In-
ternational Conference on Learning Representations (ICLR),
2017.

[31] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff
Dean. Efﬁcient neural architecture search via parameters
sharing. In International Conference on Machine Learning
(ICML), pages 4095–4104, 2018.

[32] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classiﬁer architecture
search. In AAAI Conference on Artiﬁcial Intelligence (AAAI),
2019.

[33] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Sax-
ena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey
Kurakin. Large-scale evolution of image classiﬁers. In In-
ternational Conference on Machine Learning (ICML), pages
2902–2911, 2017.

[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
ImageNet large
scale visual recognition challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015.

[35] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. MobileNetV2: Inverted
residuals and linear bottlenecks.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
4510–4520, 2018.

[36] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations (ICLR),
2015.

[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1–9, 2015.

[38] Tom Vniat and Ludovic Denoyer. Learning time/memory-
efﬁcient deep architectures with budgeted super networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 3492–3500, 2018.

[39] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.
SNAS: stochastic neural architecture search. arXiv preprint
arXiv:1812.09926, 2018.

[40] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W Cohen. Breaking the softmax bottleneck: A high-
rank rnn language model.
In International Conference on
Learning Representations (ICLR), 2018.

[41] Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hy-
pernetworks for neural architecture search. In International
Conference on Learning Representations (ICLR), 2019.

[42] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
ShufﬂeNet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In IEEE Conference on Computer

1770

