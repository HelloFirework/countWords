Improving Action Localization by Progressive Cross-stream Cooperation

Rui Su1

Wanli Ouyang1

2

,

Luping Zhou1

Dong Xu1

1 School of Electrical and Information Engineering, The University of Sydney

2 SenseTime Computer Vision Research Group, Australia

{rui.su,wanli.ouyang,luping.zhou,dong.xu}@sydney.edu.au

Abstract

Spatio-temporal action localization consists of three lev-
els of tasks: spatial localization, action classiﬁcation, and
temporal segmentation.
In this work, we propose a new
Progressive Cross-stream Cooperation (PCSC) framework
to use both region proposals and features from one stream
(i.e. Flow/RGB) to help another stream (i.e. RGB/Flow) to
iteratively improve action localization results and generate
better bounding boxes in an iterative fashion. Speciﬁcally,
we ﬁrst generate a larger set of region proposals by com-
bining the latest region proposals from both streams, from
which we can readily obtain a larger set of labelled training
samples to help learn better action detection models. Sec-
ond, we also propose a new message passing approach to
pass information from one stream to another stream in or-
der to learn better representations, which also leads to bet-
ter action detection models. As a result, our iterative frame-
work progressively improves action localization results at
the frame level. To improve action localization results at
the video level, we additionally propose a new strategy to
train class-speciﬁc actionness detectors for better temporal
segmentation, which can be readily learnt by focusing on
“confusing” samples from the same action class. Compre-
hensive experiments on two benchmark datasets UCF-101-
24 and J-HMDB demonstrate the effectiveness of our newly
proposed approaches for spatio-temporal action localiza-
tion in realistic scenarios.

1. Introduction

Deep learning has signiﬁcantly improved performance in
various computer vision tasks [6; 16; 15; 14; 26; 13; 30] in-
cluding human action detection. Human action detection,
also known as the spatio-temporal action localization, has
attracted increasing research interests due to its wide spec-
trum of applications in video surveillance (a brief review
of the related works is provided in Section 2). It consists
of three levels of tasks: i) spatial localization, i.e., ﬁnding
the spatial locations of persons within each frame, ii) ac-

tion classiﬁcation, i.e., identifying the action categories, and
iii) temporal segmentation, i.e., determining the beginning
and the end of actions. Action detection in videos is a very
challenging task due to cluttered background, occlusion and
large intra-class variance, etc., especially when targeting the
three levels of tasks together.

Our work builds upon the existing observations that ap-
pearance and motion information are often complementary
to each other in recognizing and localizing human actions
[21; 20; 17]. In addition, we further
at the feature level
observe that the two types of information are also comple-
mentary to each other at the region proposal level, so it is
beneﬁcial to fully exploit the two types of information at
both region proposal and feature levels in order to further
improve spatial-temporal action localization results, which
is our ﬁrst motivation.

Speciﬁcally, existing region proposals using either ap-
pearance or motion clues are not perfect, and they often
succeed or fail to detect region proposals in different sce-
narios. Therefore, they can help each other by providing
region proposals to each other. For example, when the mo-
tion clue is noisy because of subtle movement or cluttered
background, region proposal detectors based on motion in-
formation will fail, but region proposal detectors based on
appearance information may still successfully ﬁnd candi-
date action regions and remove a large amount of back-
ground or non-action regions.
In another example, it is
difﬁcult to detect region proposals based on appearance in-
formation when certain type human actions exhibit extreme
poses, but human actions could be captured by motion in-
formation from human movements. Therefore, we can use
the bounding boxes detected from the motion as the region
proposals for improving action detection results based on
the appearance information, and vice versa.

On the other hand, the detected bounding boxes for ac-
tions in individual frames need to be linked in order to form
action tubes and temporally segmented out from the en-
tire video clip. Current works [4; 17; 20; 23] for temporal
segmentation are mainly based on data association meth-
ods that depend on the temporal overlap and smoothness,

112016

as well as action class scores. It is observed that it is often
difﬁcult for such methods to precisely identify the temporal
boundaries of actions. When the appearance and motion
gradually change across temporal boundaries, the frames
near boundaries may have only subtle difference. In such
a case, it is extremely hard to precisely decide the temporal
boundary, and produces a large room for further improve-
ment of the existing temporal reﬁnement methods, which is
our second motivation.

Based on the ﬁrst motivation, in this paper, we propose a
progressive framework called Progressive Cross-stream Co-
operation (PCSC) to iteratively use both region proposals
and features from one stream to progressively help learn
better action detection models for another stream. To ex-
ploit the information from both streams at the region pro-
posal level, we propose to combine the latest region propos-
als from both streams in order to collect a larger set of train-
ing samples. At the feature level, we propose a new mes-
sage passing approach to pass information from one stream
to another stream in order to learn better representations. As
a result, we can progressively learn better action detection
models and improve action localization results at the frame-
level by leveraging both region proposals and features from
one stream to help another stream. Based on the second
motivation, we also propose a new temporal segmentation
method for training a set of class-speciﬁc binary classiﬁers
(also known as actionness detectors) to detect the happen-
ing of a certain type of actions. These actionness detectors
are trained by focusing on “confusing” samples from the ac-
tion tube of the same class, and therefore can learn critical
features that are good at discriminating the subtle changes
across the action boundaries.

Our contributions are brieﬂy summarized as follows:

• We propose the Progressive Cross-stream Cooperation
(PCSC) framework to iteratively use both features and
region proposals from one stream to help learn better
action detection models for another stream, which in-
cludes a new message passing approach and a simple
region proposal combination strategy.

• We also propose to learn class-speciﬁc actionness de-

tectors to improve temporal segmentation results.

• Comprehensive

experiments on two benchmark
datasets UCF-101-24 and J-HMDB demonstrate
that our approach outperforms the state-of-the-art
methods for localizing human actions both spatially
and temporally in realistic scenarios.

2. Related Work

2.1. Spatial temporal localization methods

Spatio-temporal action localization involves three types
of tasks: spatial localization, action classiﬁcation, and tem-

poral segmentation. A huge amount of efforts have been
dedicated to improve the three tasks from different perspec-
tives. First, for spatial localization, the state-of-the-art hu-
man detection methods are utilized to obtain precise ob-
ject proposals, (including the use of fast and faster R-CNN
in [3; 20] as well as Single Shot Multibox Detector (SSD)
in [23; 10]).

Second, discriminant features are also employed for both
spatial localization and action classiﬁcation. For example,
to remove the ambiguity of actions in each single frame,
some methods [10; 3] stack neighbouring frames near one
key frame to extract more discriminant features in order to
better represent this key frame. Other methods [22; 18; 3]
utilize recurrent neural networks to link individual frames
[2; 27] to exploit temporal informa-
or use 3D CNNs
tion. Meanwhile, complementary information from multi-
modalities is also utilized to improve feature extraction re-
sults. For example, a number of works [20; 10; 17; 3] fuse
the appearance and motion clues to extract more robust fea-
tures for action classiﬁcation.

Third, temporal segmentation is to form action tubes
from per-frame detection results. Methods for this task are
largely based on the association of per-frame detection re-
sults, such as the overlap, continuity and smoothness of ob-
jects, as well as the action class scores. To improve segmen-
tation accuracies, a variety of temporal reﬁnement meth-
ods have been proposed, e.g., the traditional temporal slid-
ing windows [17], dynamic programming [23; 20], tubelets
linking [10], and thresholding-based reﬁnement [3], etc.

Finally, several methods were also proposed to improve
action detection efﬁciency. For example, without requir-
ing time-consuming multi-stage training process, the works
in [20; 17] proposed to train a single CNN model by simul-
taneously performing action classiﬁcation and bounding
box regression. More recently, an online real-time spatio-
temporal localization method is also proposed in [23].

2.2. Two stream R CNN

Based on the observation that appearance and motion
clues are often complementary to each other, several state-
of-the-art action detection models [20; 10; 17; 3] followed
the standard two-stream R-CNN approach. The features ex-
tracted from the two streams are fused to improve action
detection performance. For example, in [20], the softmax
score of each motion bounding box is used to help the ap-
pearance bounding boxes with largest overlap. In [3], three
types of fusion strategies are discussed: i) simply averaging
the softmax outputs of the two streams, ii) learning per-class
weights to weigh the original pre-softmax outputs and ap-
plying softmax on the weighted sum, and iii) training a fully
connected layer on top of the concatenated output from each
stream. It is reported in [3] that the third fusion strategy
achieves the best performance.

12017

Please note that the appearance and motion fusion ap-
proaches in the existing works as discussed above are all
based on the late fusion strategy. They are only trained (if
there is any training process) on top of the detection net-
works of the two streams. In contrast, in this work we it-
eratively use both features and bounding boxes from one
stream to progressively help learn better action detection
models for another stream, which is intrinsically different
with these existing approaches [23; 10] that fuse two-stream
information only at the feature level in a late fusion fashion.

3. Action Detection Model

Building upon the two-stream framework [17], we
propose a Progressive Cross-stream Cooperation (PCSC)
model for action detection at the frame level. In this model,
the RGB (appearance) stream and the ﬂow (motion) stream
iteratively help each other at both features level and region
proposal level in order to achieve better localization results.
Finally, the action tube reﬁnement module introduced in
Section 3.4 is used to link the detection boxes at each frame.

3.1. PCSC Overview

The overview of our PCSC model (i.e., the frame-level
detection model) is given in Fig. 1. As shown in Fig. 1(a),
our PCSC is composed of a set of “stages”. Each stage
refers to one round of cross-stream cooperation, in which
the features and region proposals from one stream will help
improve action localization performance for another stream.
Speciﬁcally, each stage comprises of two cooperation mod-
ules and a detection head module. Our detection head
module is a standard one, which consists of several layers
for region classiﬁcation and regression.

The two cooperation modules include region-proposal-
level cooperation and feature-level cooperation, which are
introduced in details in Section 3.2 and Section 3.3, re-
spectively. For region-proposal-level cooperation, the de-
tection results from one stream (e.g, the RGB stream) are
used as the additional region proposals, which are com-
bined with the region proposals from the other stream (e.g,
the ﬂow stream) to reﬁne the region proposals and improve
the action localization results. Based on the reﬁned re-
gion proposals, we also perform feature-level cooperation
by ﬁrst extracting RGB/ﬂow features from these ROIs and
reﬁne these RGB/ﬂow features via a message-passing mod-
ule shown in Fig. 1(b), and Fig. 1(c), which will be in-
troduced in Section 3.3. The reﬁned ROI features in turn
lead to better region classiﬁcation and regression results in
the detection head module, which beneﬁts the subsequent
action localization process in the next stage. By perform-
ing the aforementioned processes for multiple rounds, we
can progressively improve the action detection results. The
whole network is trained in an end-to-end fashion by mini-
mizing the overall loss, which is the summation of the losses

from all stages.

After performing frame-level action detection, our ap-
proach links the per-frame detection results to form action
tubes, in which the temporal boundary is further reﬁned by
using our proposed class-speciﬁc actionness detectors. The
details are provides in Section 3.4.

3.2. Cross stream Region Proposal Cooperation

We employ the two-stream Faster R-CNN method [19]
for frame-level action localization. Each stream has its own
Region Proposal Network (RPN) [19] to generate candi-
date action regions, and these candidates are then used as
training samples to train a bounding box regression network
for action localization. Based on our observation, the re-
gion proposals generated by either stream can only partially
cover the true action regions, which degrades the detection
performance. Therefore, in our model, we use the region
proposals from one stream to help another stream.

In this paper, the bounding boxes from RPN are called
region proposals, while the bounding boxes from the de-
tection head are called detection boxes.

The region proposals from the RPNs of the two streams
are ﬁrst used to train their own detection head separately
in order to obtain their own corresponding detection boxes.
The set of region proposals for each stream is then reﬁned
by combining two subsets. The ﬁrst subset is from the de-
tection boxes of its own stream (e.g, RGB) at the previous
stage. The second subset is from the detection boxes of
another stream (e.g, ﬂow) at the current stage. To remove
more redundant boxes, a lower NMS threshold is used when
the detection boxes in another stream (e.g. ﬂow) is used for
the current stream (e.g. RGB).

t

t = B(i)

is updated as P (i)

Mathematically, let P (i) and B(i) denote the set of region
proposals and the set of the detection boxes, respectively,
where i indicates the ith stream, t denotes the tth stage. The
t−2 ∪ B(j)
region proposal P (i)
t−1,
and the detection box B(j)
t = G(P (j)
),
where G(·) denotes the mapping function from the detec-
tion head module. Initially, when t = 0, P (i)
is the region
0
proposal from RPN for the ith stream. This process is re-
peated between the two streams for several stages, which
will progressively improve the detection performance of
both streams.

is updated as B(j)

t

t

As can been seen, with our approach, the diversity of re-
gion proposals from one stream will be enhanced by using
the complementary boxes from another stream. This could
help reduce the missing bounding boxes. Moreover, only
bounding boxes with high conﬁdence are utilized in our ap-
proach, which increases the chances to add more precisely
detected bounding boxes and thus further help improve the
detection performance. These aforementioned strategies,
together with the cross-modality feature cooperation strat-
egy that will be described in Section 3.3, effectively im-

12018

Proposal Cooperation

Proposal Cooperation

Proposal Cooperation

Proposal Cooperation

Proposal Cooperation

Proposal Cooperation

Proposal Cooperation

Proposal Cooperation

RGB

RGB

RGB

RGB

Flow

RGB

Flow

RGB

Flow

Flow

Flow

Flow

RGB

RGB

Flow

Flow

Flow

Flow

RGB

RGB

RGB

RGB

Flow

Flow



=





∪ 

=



∪ 



=





∪ 

=



∪ 



=





∪



=



∪





= 



∪



= 

∪



1

0

1

0

2

0

2

1

0

1

3

3

2

2

3

3

0

0

1

1

4

2

4

2

Global RGB  
Features:  I

Global Flow  
Features:

I

Flow

RGB

RGB

Global RGB  
Features:  I

Feature
Global Flow  
Features:

Cooperation

Flow



I

1

Proposal Cooperation

Feature

Detection

RGB



Cooperation
Head

0

0

Detection

Feature

Head
Cooperation



2

Proposal Cooperation

Feature

Detection

Flow

=



Cooperation
Head

0



1

Proposal Cooperation

Proposal Cooperation

Detection

Feature

Head
Cooperation

3

Feature

Detection

RGB

= 

Cooperation
Head

1



2

Detection

Feature

Head
Cooperation

4

Feature

Detection

Flow

=



Cooperation
Head

2

∪ 

3

Detection

Head

RGB

RGB

Flow

Flow

RGB

Flow

Flow

RGB

=

∪ 



∪



∪

Global RGB  
Features:  I

Improved 
Features: 

RGB

Improved 
Features: 

RGB

RGB

RGB

RGB

Flow

Flow

Flow

F

F

F

F

Detection Boxes:

Detection Boxes:

RGB



1

1

Detection

Head
Stage 1

1

Feature

Cooperation
Stage 1

Improved 
Features: 

F

1

Improved 
Features: 

Improved 
Features: 

F

F

2

2

Improved 
Features: 

Improved 
Features: 

Improved 
Features: 

F

4

Improved 
Features: 

Flow

F

4





Detection
Detection Boxes:
Head

Flow



2

Stage 2

1

RGB

Feature

Detection Boxes:
Cooperation
Stage 2
Improved 
Features: 

F

2

Flow

2

3

Feature

Detection Boxes:
Cooperation
Stage 3
Improved 
Features: 

F

3

3



RGB

Detection
Detection Boxes:
Head
Stage 3

3



3

Detection

Flow

Detection Boxes:
Head



4



4

Flow

RGB

Feature

Detection Boxes:
Cooperation
Stage 4
Improved 
Features: 

F

4

Stage 4

Flow

RGB

RGB

Flow

Global Flow  
Features:

I

Flow

Detection Boxes:

RGB



1

Detection Boxes:

Flow



2

Detection Boxes:

RGB



3

Detection Boxes:

Flow



4

Stage 1

Stage 2

Stage 3

Stage 4

RGB

RGB

Flow

Flow









t

t

t

t

(a)

RGB

I

RGB

ROI Feature
Extractor

I

t

F

̂ RGB

ROI Feature
Extractor

̂ RGB

RGB

RGB

F

F

F

t

t

t

RGB

I

RGB

ROI Feature
Extractor

I

t

F

̂ RGB

ROI Feature
Extractor

̂ RGB

F

t

RGB

Flow



t



t

RGB

RGB





t

t

Flow

Flow





t

t

1 × 1          

1 × 1          

1 × 1          

1 × 1          

RGB

I

t

Flow

I

I

ROI Feature
Extractor

F

t

ROI Feature
Extractor

Flow

ROI Feature
Extractor

̂ Flow

Conv
 
Conv
 

Conv
Message 
Passing
 
Conv
 

RGB

F

t

Message 
Passing

RGB

I

Flow

I

I

̂ RGB

̂ RGB

1 × 1          

1 × 1          

1 × 1          

1 × 1          

F

F

t

t

F

F

F

t

t

t

̂ Flow

̂ Flow

Flow

1 × 1          

1 × 1          

ROI Feature
Extractor

F

t

ROI Feature
Extractor

Flow

ROI Feature
Extractor

̂ Flow

Flow



t

(c)

Conv
 
Conv
 

Conv
Message 
Passing
 
Conv
 

Message 
Passing

Flow

F

t

Message 
Passing

Conv
 
Conv
 

1 × 1          

1 × 1          

t

t

I

F

Flow

̂ Flow

ROI Feature
Extractor

Figure 1: (a) Overview of our Cross-stream Cooperation framework (four stages are used as an example for better illustration).
The region proposals and features from the ﬂow stream help improve action detection results for the RGB stream at Stage 1
and Stage 3, while the region proposals and features from the RGB stream help improve action detection results for the ﬂow
stream at Stage 2 and Stage 4. Each stage comprises of two cooperation modules and the detection head. The region-proposal
level cooperation module reﬁnes the region proposals P i
t and the feature-level cooperation module improves the features Fi
t,
where the superscript i ∈ {RGB, F low} denotes the RGB/Flow stream and the subscript t denotes stage number. The
detection head is used for estimating the action location and the class label. For region-proposal level cooperation, we
combine the most recent region proposals from the two streams. (b) (c) Details of our feature-level cooperation modules
through message passing from one stream to another stream. The ﬂow features are used to help the RGB features in (b),
while the RGB features are used to help the ﬂow features in (c).

ROI Feature
Extractor

̂ Flow

Flow

Flow

F

F

I

t

t

RGB



t

(b)

Message 
Passing

Conv
 
Conv
 

prove the frame-level action detection results.

Our cross-stream cooperation strategy at the region pro-
posal level shares similar high-level ideas with the two-view
learning method co-training [1], as both methods make
use of predictions from one stream/view to generate more
training samples (i.e., the additional region proposals in
our approach) to improve the prediction results for another
stream/view. However, our approach is intrinsically differ-
ent with co-training in the following two aspects. As a semi-
supervised learning method, the co-training approach [1]

selects unlabelled testing samples and assigns pseudo-labels
to those selected samples to enlarge the training set. In con-
trast, the additional region proposals in our work still come
from training videos instead of testing videos, so we know
the labels of the new training samples by simply compar-
ing these additional region proposals with the ground-truth
bounding boxes. Also, in co-training [1], the learnt classi-
ﬁers will be directly used for predicting the labels of testing
data in the testing stage so the complementary information
from the two views for the testing data is not exploited. In

12019

contrast, in our work, the same pipeline used in the training
stage will also be adopted for testing samples in the testing
stage, and thus we can further exploit the complementary
information from the two streams for the testing data.

3.3. Cross stream Feature Cooperation

To extract spatio-temporal features for action detection,
similar to [5], we use the I3D network as the backbone
network for each stream. Moreover, we follow Feature
Pyramid Network (FPN) [12] to build feature pyramid with
high-level semantics, which has been found to be useful
to improve bounding box proposals and object detection.
This involves a bottom-up pathway and a top-down path-
way and lateral connections. The bottom-up pathway uses
the feed-forward computation along the backbone I3D net-
work, which produces a feature hierarchy with increasing
semantic levels but decreasing spatial resolutions. Then
these features are upsampled by using the top-down path-
way, which are merged with the corresponding features in
the bottom-up pathway through lateral connections.

2, C i

4, C i

3, C i

Following [12], we use the feature maps at the layers
of Conv2c, Mixed3d, Mixed4f, Mixed5c in I3D to con-
struct the feature hierarchy in the bottom-up pathway, and
denote these feature maps as {C i
5}, where
i ∈ {RGB, Flow}, indicating the RGB and the ﬂow streams,
respectively. Accordingly, the corresponding feature maps
in the top-down pathway are denoted as {U i
5}.
Most two-stream action detection frameworks [23; 17;
10] only exploit the complementary RGB and ﬂow infor-
mation by fusing softmax scores or concatenating the fea-
tures from the ﬁnal classiﬁers, which are insufﬁcient for the
features from the two streams to exchange information from
one stream to another and beneﬁt from such information ex-
change. Based on this observation, we develop a message-
passing module to bridge these two streams, so that they
help each other for feature reﬁnement.

3, U i

4, U i

2, U i

We pass the messages between the feature maps in the
bottom-up pathway of the two streams. Denote l as the
index for the set of feature maps in {C i
5},
l ∈ {2, 3, 4, 5}. Let us use improvement of the RGB fea-
tures as an example (the same method is applied to improve-
ment of the ﬂow features). Our message-passing module
improves the features CRGB
CFlow

with the help of the features

as follows:

3, C i

4, C i

2, C i

l

l

CRGB

l

= fθ(CFlow

l

) ⊕ CRGB

l

.

(1)

where ⊕ denotes the element-wise addition of the feature
maps, fθ(·) is the mapping function (parameterized by θ)
of our message-passing module. The function fθ(CFlow
)
nonlinearly extracts the message from the feature CFlow
,
and then use the extracted message for improving the fea-
tures CRGB

.

l

l

l

l

l

The output of fθ(·) has to produce the feature maps with
and

the same number of channels and resolution as CRGB
CFlow
. To this end, we design our message-passing mod-
ule by stacking two 1 × 1 convolutional layer with relu as
the activation function. The ﬁrst 1 × 1 convolutional lay-
ers reduces the channel dimension and the second convolu-
tional layer restores the channel dimension back to its orig-
inal number. This design saves the number of parameters
to be learnt in the module and exchange message by us-
ing two-layer non-linear transform. Once the feature maps
CRGB
sponding features maps U RGB
are generated accordingly.

in the bottom-up pathway are reﬁned, the corre-

in the top-down pathway

l

l

The above process is for image-level messaging pass-
ing only. The image-level message passing is only per-
formed from the Flow stream to the RGB stream once.
This message passing provides good features to initialise
the message-passing stages.

t

t

t

and ˆF Flow

Denote the image-level feature map sets for the RGB and
ﬂow streams by IRGB and IFlow respectively. They are
used to extract features ˆF RGB
by ROI pool-
ing in each stage t, as shown in Fig. 1. At Stage 1 and
Stage 3, the ROI-feature ˆF Flow
of the ﬂow stream is used
to help improve the ROI-feature ˆF RGB
of the RGB stream,
as illustrated in Fig. 1 (b). Speciﬁcally, the improved RGB
feature F RGB
is obtained by applying the same method in
Eqn. (1). Similarly, at Stage 2 and Stage 4, the ROI-feature
ˆF RGB
the ROI-feature ˆF Flow
of the ﬂow stream, as illustrated in
Fig. 1 (c). The message passing between ROI-features aims
to provide better features for action box detection and re-
gression, which beneﬁts the next cross-stream cooperation
stage.

of the RGB stream is also used to help improve

t

t

t

t

3.4. Action Tube Reﬁnement

After frame-level detection results are generated, we
then build action tubes by linking them. Here we use the
same linking strategy as in [23], except that we do not apply
temporal labeling. Although this linking strategy is robust
to missing detection, it is still difﬁcult to accurately deter-
mine the start and the end of each action tube, which is a key
factor that degrades video-level performance of our action
detection framework.

To solve this problem, we develop a class-speciﬁc ac-
tionness detector to detect the actionness (i.e.the happening
of an action) at each given location (spatially and tempo-
rally). To simplify the action tube reﬁnement process, the
input features for actionness detection are the same features
for frame-level action detection. Speciﬁcally, we use the
features after feature level cooperation (see Section 3.3).
Taking advantage of the predicted action class labels from

12020

the frame-level detection results, we construct our action-
ness detector by using N binary classiﬁers, where N is
the number of action classes. Each classiﬁer addresses the
actionness of a speciﬁc class. This strategy is more ro-
bust than learning a general actionness classiﬁer for all ac-
tion classes. Speciﬁcally, after frame-level detection, each
bounding box has a class label, based on which, the bound-
ing boxes from the same class are traced and linked to form
action tubes [23]. To train the binary actionness classiﬁer
for each action class i, the bounding boxes that are within
the action tubes predicted as class i by the frame-level de-
tector are used as the training samples. Each bounding box
is labeled either as 1 when its overlap with the ground-truth
box is greater than 0.5, or as 0 otherwise. Note the training
samples may include those bounding boxes falsely detected
near a temporal boundary and included into the action tubes
of Class i. Therefore, they are useful for the actionness clas-
siﬁer to learn the subtle but critical features that determines
the begining and the end of this action. The output of the
actionness classiﬁer is a probability of actionness of class i.

At the testing stage, given an action tube formed us-
ing [23], we apply the class-speciﬁc actionness detector at
every frame-level bounding box in this tube to predict its
actionness probablity (called actionness score). Then a me-
dian ﬁlter over multiple frames is employed to smooth the
actionness scores of all bounding boxes in this tube. If a
bounding box has a smoothed score lower than a preset
threshold, it will be ﬁltered out from this action tube, and
then we can reﬁne the action tubes so that they have more
accurate temporal boundaries. Note that when a non-action
region near a temporal boundary is falsely detected, it is
included in the training set to train our class-speciﬁc action-
ness detector. Therefore, our approach takes advantage of
the “confusing samples” across temporal boundary to ob-
tain better action tubes at the testing stage.

3.5. Training Details

For better spatial localization at the frame-level, we fol-
low [10] to stack neighbouring frames to exploit temporal
context and improve action detection performance for key
frames. A key frame is a frame containing the ground-truth
actions. Each training sample, which is used to train the
RPN in our PCSC method, is composed of k neighouring
frames with the key frame in the middle. The region propos-
als generated from the RPN are assigned with positive la-
bels when they have an intersection-over-union (IoU) over-
lap higher than 0.5 with any ground-truth bounding box,
or negative labels if their IoU overlap is lower than 0.5
with all ground-truth boxes. This label assignment strategy
also applies to the additional bounding boxes from the as-
sistant stream during the region proposal-level cooperation
process.

4. Experimental results

We introduce our experimental setup and datasets in Sec-
tion 4.1, and then compare our method with the state-of-
the-art methods in Section 4.2, and conduct ablation study
in Section 4.3.

4.1. Experimental Setup

Datasets. We evaluate our PCSC model on two bench-
marks: UCF-101-24 [24] and J-HMDB-21 [9]. UCF-101-
24 contains 3207 untrimmed videos from 24 sports classes,
which is a subset of the UCF-101 dataset, with spatio-
temporal annotations provided by [23]. Following the com-
mon practice, we use the predeﬁned “split 1” protocol to
split the training and test sets, and report the results based
on this split. J-HMDB-21 contains 928 videos from 21 ac-
tion classes. All the videos are trimmed to contain the ac-
tions only. We experiment on three predeﬁned training-test
splits, and report the averaged results on this dataset.

Metrics. We evaluate the action detection performance
at both frame-level and video-level by mean Average pre-
cision (mAP). To calculate mAP, we consider a detection
box is correct when its overlap with a ground-truth box or
tube is greater than a threshold δ. The overlap between
our detection results and the ground truth is measured by
the intersection-over-union (IoU) at the frame-level and the
spatio-temporal tube overlap at the video-level. In addition,
we also report the results based on COCO evaluation met-
ric [11], which averages the mAPs over 10 different IoU
thresholds from 0.5 to 0.95 with an interval of 0.05.

Implementation Details. We use the I3D features [2]
for both streams, and the I3D model is pretrained with Ki-
netics. The optical ﬂow images are extracted from FlowNet
v2 [8]. The mini-batch size used to train the RPN and the
detection head is 256 and 512, respectively. Our PCSC
model is trained for 6 epochs by using three 1080Ti GPUs.
The initial learning rate is set as 0.01, which drops 10% at
the 5th epoch and another 10% at the 6th epoch.

4.2. Comparison with the State of the art methods

We compare our PCSC method with the state-of-the-art
methods. The results of the existing methods are quoted di-
rectly from their original papers. In addition, we also evalu-
ate the object detection model in [12] with the I3D network
as its backbone network. Speciﬁcally, the model in [12]
(denoted as “Faster R-CNN + FPN”) and our PCSC only
differ in that the PCSC method has the cross-stream coop-
eration modules while the work in [12] does not have them.
Therefore, the comparison between the two approaches can
better demonstrate the beneﬁt of our cross-stream coopera-
tion strategy.

12021

4.2.1 Results on the UCF-101-24 Dataset

The results on the UCF-101-24 dataset are reported in Ta-
ble 1 and Table 2.

Table 1: Comparison (mAPs % at the frame level) of dif-
ferent methods on the UCF-101-24 dataset when using the
IoU threshold δ at 0.5.

Weinzaepfel et al. [29]
Peng and Schmid [17]
Kalogeiton et al. [10]
Gu et al. [5]
Faster R-CNN + FPN [12]
PCSC (Ours)

mAPs
35.8
65.7
69.5
76.3
75.5
79.2

Table 2: Comparison (mAPs % at the video level) of differ-
ent methods on the UCF-101-24 dataset when using differ-
ent IoU thresholds.

IoU threshold δ
Weinzaepfel et al. [29]
Peng and Schmid [17]
Saha et al. [20]
Singh et al. [23]
Kalogeiton et al. [10]
Gu et al. [5]
Faster R-CNN + FPN [12]
PCSC + TR (Ours)

0.2
46.8
73.5
66.6
73.5
76.5

-

80.1
84.3

0.5

-

32.1
36.4
46.3
49.2
59.9
53.2
61.0

0.75

0.5:0.95

-

02.7
0.79
15.0
19.7

-

15.9
23.0

-

07.3
14.4
20.4
23.4

-

23.7
27.8

Table 1 shows the mAPs from different methods at the
frame-level on the UCF-101-24 dataset. All mAPs are cal-
culated based on the IoU threshold δ = 0.5. As can be
seen, our PCSC model achieves an mAP of 79.2%, outper-
forming all the existing methods by a large margin. Espe-
cially, PCSC performs better than [12] by an improvement
of 3.7%. This improvement can be fully due to the proposed
cross-stream cooperation framework, which is the only dif-
ference between [12] and our PCSC. It is interesting to ob-
serve that both methods [10] and [5] additionally utilize
the temporal context for per frame-level action detection.
They do not fully exploit the complementary of appear-
ance and motion information, and therefore are worse than
our method. As can be seen, our method outperforms [10]
and [5] by 9.7% and 2.9%, respectively, in terms of frame-
level mAPs.

Table 2 reports the video-level mAPs at various IoU
thresholds (0.2, 0.5, and 0.75) on the UCF-101-24 dataset.
The results based on the COCO evaluation metrics [11] are
reported in the last column of Table 2. Our method is de-
noted as “PCSC + TR” in Table 2, where the proposed tem-
poral reﬁnement (TR) method is applied to reﬁne the action

tubes generated from our PCSC model. Consistent with the
observations on the frame-level, our method outperforms
all the state-of-the-art methods under all evaluation metrics.
When using the IoU threshold δ = 0.5, we achieve an mAP
of 61.0% on the UCF-101-24 dataset. This result beats [10]
and [5], which only achieve the mAPs of 49.2% and 59.9%,
respectively. Moreover, as the IoU threshold increases, we
observe that the performance of our method drops less when
compared with other state-of-the-art methods. This demon-
strates that our detection method achieves higher localiza-
tion accuracy than other competitive methods.

4.2.2 Results on the J-HMDB Dataset

For the J-HMDB dataset, the results in terms of frame-level
mAPs and video-level mAPs are reported in Table 3 and Ta-
ble 4, respectively. Since the videos in J-HMDB dataset are
trimmed to only contain actions, the temporal reﬁnement
process is not required, so we do not apply our TR method
when generating action tubes on the J-HMDB dataset.

Table 3: Comparison (mAPs % at the frame level) of dif-
ferent methods on the J-HMDB dataset when using the IoU
threshold δ at 0.5.

Peng and Schmid [17]
Kalogeiton et al. [10]
Hou et al. [7]
Gu et al. [5]
Sun et al. [25]
Faster R-CNN + FPN [12]
PCSC (Ours)

mAPs
58.5
65.7
61.3
73.3
77.9
70.2
74.8

Table 4: Comparison (mAPs % at the video level) of differ-
ent methods on the J-HMDB dataset when using different
IoU thresholds.

IoU threshold δ
Gkioxari and Malik [4]
Wang et al. [28]
Weinzaepfel et al. [29]
Saha et al. [20]
Peng and Schmid [17]
Singh et al. [23]
Kalogeiton et al. [10]
Hou et al. [7]
Gu et al. [5]
Sun et al. [25]
Faster R-CNN + FPN [12]
PCSC (Ours)

0.2

-
-

63.1
72.6
74.1
73.8
74.2
78.4

-
-

79.1
82.6

0.5
53.3
56.4
60.7
71.5
73.1
72.0
73.7
76.9
78.6
80.1
78.5
82.2

0.75

0.5:0.95

-
-
-

-
-
-

43.3

40.0

-

44.5
52.1

-
-
-

-

41.6
44.8

-
-
-

57.2
63.1

47.6
52.8

We have similar observation as in the UCF-101-24

12022

dataset. At the video-level, our method is again the best per-
former under all evaluation metrics on the J-HMDB dataset
(see Table 4). When using the IoU threshold δ = 0.5, our
PCSC method outperforms [5] and [25] by 3.6% and 2.1%,
respectively.

At the frame-level (see Table 3), our PCSC method per-
forms the second best, which is only worse than a very
recent work [25]. However, the work in [25] uses S3D-
G as the backbone network, which provides much stronger
features when compared with the I3D features used in our
method. In addition, please note that, our method outper-
forms [25] in terms of mAPs at the video level (see Table 4),
which demonstrates promising performance of our PCSC
method. Moreover, as a general framework, our PCSC
method could also take advantage of strong features pro-
vided by the S3D-G model to further improve the results,
which will be explored in our future work.

4.3. Ablation Study

To investigate the contributions of different components
in our PCSC model, we construct a mini-UCF-101-24
dataset by sampling every 10 frames from the videos in
UCF-101-24, which is only used as the training data in our
ablation study. The test data of UCF-101-24 for evaluation
in all experiments are kept unchanged.

Table 5: Ablation study for our PCSC method at different
training stages on the mini-UCF-101-24 dataset.

Stage

PCSC w/o feature

cooperation

PCSC

0
1
2
3
4

72.7
73.3
73.5
73.9
73.9

75.7
76.1
76.4
76.6
76.7

Table 6: Ablation study for our temporal reﬁnement method
on the mini-UCF-101-24 dataset.

video mAP

Faster R-CNN + FPN [12]
PCSC
PCSC + TR

53.2
55.8
59.4

Progressive cross-stream cooperation. In Table 5, we
report the results of an alternative approach of our PCSC
(called PCSC w/o feature cooperation approach).
In the
second column, we remove the feature-level cooperation
module from Fig. 1, and only use the region-proposal-level
cooperation module. As our PCSC is conducted in a pro-
gressive manner, we also report the performance at differ-
ent stages to verify the beneﬁt of this progressive strategy.

It is worth mentioning that the output at each stage is ob-
tained by combining the detected bounding boxes from both
the current stage and all previous stages, and then we ap-
ply non-maximum suppression (NMS) to obtain the ﬁnal
bounding boxes. For example, the output at stage 4 is ob-
tained by applying NMS to the union set of the detected
bounding boxes from Stages 0, 1, 2, 3 and 4. At Stage 0, the
detection results from both RGB and the Flow streams are
simply combined. From Table 5, we observe that the detec-
tion performance of our PCSC method with or without the
feature-level cooperation module is improved as the num-
ber of stages increases. However, such improvement seems
to become saturated when reaching Stage 4, as indicated
by the marginal performance gain from Stage 3 to Stage 4.
Meanwhile, when comparing our PCSC with the alterna-
tive approach PCSC w/o feature cooperation at every stage,
we observe that both region-proposal-level and feature-level
cooperation contributes to performance improvement.

Action tubes reﬁnement. In Table 6, we also investi-
gate the effectiveness of our temporal reﬁnement method by
reporting the results with/without the temporal reﬁnement
module, in which video-level mAPs at the IoU threshold
δ = 0.5 are reported. By generating higher quality detec-
tion results at each frame, our PCSC method outperforms
the work in [12] that does not use the two-stream coopera-
tion strategy by 2.6%. After applying our action tube reﬁne-
ment method to further boost the video-level performance,
we arrive at the video-level mAP of 59.4%, which demon-
strates that it is beneﬁcial to use our action tube reﬁnement
method to reﬁne the temporal boundaries of action tubes.

5. Conclusion

In this work, we have proposed the Progressive Cross-
stream Cooperation (PCSC) framework to progressively
improve spatio-temporal action localization results at the
frame level, which consists of several
iterative stages.
At each stage, we improve action localization results
for one stream (i.e., RGB/ﬂow) by the leveraging the
information from another stream (ﬂow/RGB) at both
region proposal level and feature level. We addition-
ally propose a simple but effective approach to improve
temporal segmentation results by training class-speciﬁc
actionness detectors based on the training samples around
temporal boundaries.
The effectiveness of our newly
proposed approaches is demonstrated by extensive ex-
periments on both UCF-101-24 and J-HMDB datasets.

References

[1] A. Blum and T. Mitchell. Combining labeled and unlabeled
data with co-training. In Proceedings of the Eleventh Annual
Conference on Computational Learning Theory, COLT’ 98,
pages 92–100, New York, NY, USA, 1998. ACM. ISBN 1-

12023

58113-057-0. doi: 10.1145/279943.279962. URL http:
//doi.acm.org/10.1145/279943.279962. 4

[2] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
July 2017. 2, 6

[3] G. Ch´eron, A. Osokin, I. Laptev, and C. Schmid. Mod-
eling spatio-temporal human track structure for action lo-
calization. CoRR, abs/1806.11008, 2018. URL http:
//arxiv.org/abs/1806.11008. 2

[4] G. Gkioxari and J. Malik. Finding action tubes.

In The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 1, 7

[5] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li,
S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar,
C. Schmid, and J. Malik. Ava: A video dataset of spatio-
temporally localized atomic visual actions.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018. 5, 7, 8

[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 1

[7] R. Hou, C. Chen, and M. Shah. Tube convolutional neural
network (t-cnn) for action detection in videos. In The IEEE
International Conference on Computer Vision (ICCV), Oct
2017. 7

[8] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. In IEEE conference on computer vision
and pattern recognition (CVPR), volume 2, page 6, 2017. 6

[9] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black.
Towards understanding action recognition. In Proceedings of
the IEEE international conference on computer vision, pages
3192–3199, 2013. 6

[10] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid.
Action tubelet detector for spatio-temporal action localiza-
tion. In ICCV, 2017. 2, 3, 5, 6, 7

[11] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014. 6, 7

[14] W. Ouyang and X. Wang. Single-pedestrian detection aided
by multi-pedestrian detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3198–3205, 2013. 1

[15] W. Ouyang, K. Wang, X. Zhu, and X. Wang. Chained cas-
cade network for object detection.
In Proceedings of the
IEEE International Conference on Computer Vision, pages
1938–1946, 2017. 1

[16] W. Ouyang, X. Zeng, X. Wang, S. Qiu, P. Luo, Y. Tian, H. Li,
S. Yang, Z. Wang, H. Li, et al. Deepid-net: Object detection
with deformable part based convolutional neural networks.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 39(7):1320–1334, 2017. 1

[17] X. Peng and C. Schmid. Multi-region two-stream r-cnn for
action detection. In European Conference on Computer Vi-
sion, pages 744–759. Springer, 2016. 1, 2, 3, 5, 7

[18] L. Pigou, A. van den Oord, S. Dieleman, M. V. Herreweghe,
and J. Dambre. Beyond temporal pooling: Recurrence and
temporal convolutions for gesture recognition in video. In-
ternational Journal of Computer Vision, 126(2-4):430–439,
2018. 2

[19] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-
wards real-time object detection with region proposal net-
works. In Advances in Neural Information Processing Sys-
tems 28, pages 91–99. Curran Associates, Inc., 2015. 3

[20] S. Saha, G. Singh, M. Sapienza, P. H. S. Torr, and F. Cuz-
zolin. Deep learning for detecting multiple space-time action
tubes in videos. In Proceedings of the British Machine Vision
Conference 2016, BMVC 2016, York, UK, September 19-22,
2016, 2016. 1, 2, 7

[21] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos.
In Advances in
Neural Information Processing Systems 27, pages 568–576,
2014. 1

[22] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A
multi-stream bi-directional recurrent neural network for ﬁne-
grained action detection. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2016. 2

[23] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin.
Online real time multiple spatiotemporal action localisation
and prediction. 2017. 1, 2, 3, 5, 6, 7

[24] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. 2012. 6

[12] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017. 5, 6, 7, 8

[25] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Suk-
thankar, and C. Schmid. Actor-centric relation network.
In The European Conference on Computer Vision (ECCV),
September 2018. 7, 8

[13] L. Liu, H. Wang, G. Li, W. Ouyang, and L. Lin. Crowd
counting using deep recurrent spatial-aware network. In Pro-
ceedings of the 27th International Joint Conference on Arti-
ﬁcial Intelligence, pages 849–855. AAAI Press, 2018. 1

[26] S. Sun, J. Pang, J. Shi, S. Yi, and W. Ouyang. Fishnet: A
versatile backbone for image, region, and pixel level predic-
tion. In Advances in Neural Information Processing Systems,
pages 762–772, 2018. 1

12024

[27] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In Proceedings of the 2015 IEEE International Con-
ference on Computer Vision (ICCV), ICCV ’15, 2015. 2

[28] L. Wang, Y. Qiao, X. Tang, and L. Van Gool. Actionness
estimation using hybrid fully convolutional networks. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2016. 7

[29] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization.
In The IEEE
International Conference on Computer Vision (ICCV), De-
cember 2015. 7

[30] W. Zhang, W. Ouyang, W. Li, and D. Xu. Collaborative
and adversarial network for unsupervised domain adaptation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3801–3809, 2018. 1

12025

