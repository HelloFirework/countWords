Unsupervised Disentangling of Appearance and Geometry

by Deformable Generator Network

Xianglei Xing1, Tian Han2, Ruiqi Gao2, Song-Chun Zhu2, Ying Nian Wu2

1College of Automation, Harbin Engineering University, Harbin 150001, China
2Department of statistics, University of California, Los Angeles, California 90095
xingxl@hrbeu.edu.cn, {hantian,ruiqigao}@ucla.edu,{sczhu,ywu}@stat.ucla.edu

Abstract

We present a deformable generator model to disentan-
gle the appearance and geometric information in purely un-
supervised manner. The appearance generator models the
appearance related information, including color, illumina-
tion, identity or category, of an image, while the geometric
generator performs geometric related warping, such as ro-
tation and stretching, through generating displacement of
the coordinate of each pixel to obtain the ﬁnal image. Two
generators act upon independent latent factors to extrac-
t disentangled appearance and geometric information from
images. The proposed scheme is general and can be easily
integrated into different generative models. An extensive set
of qualitative and quantitative experiments shows that the
appearance and geometric information can be well disen-
tangled, and the learned geometric generator can be con-
veniently transferred to other image datasets to facilitate
knowledge transfer tasks.

1. Introduction

Learning disentangled structures behind the observations
[2, 26] is a fundamental problem towards understanding and
controlling modern deep models. Such disentangled repre-
sentations are useful not only in building more transparent
and interpretable deep models, but also in a large variety of
downstream AI tasks such as transfer learning and zero-shot
inference where humans excel but machines struggle [22].
Among others, deep generative models, e.g., generator
model, have shown great promise in learning representa-
tion of images in recent years. However, the learned repre-
sentation is often entangled and not interpretable. Learning
disentangled and interpretable representation for deep gen-
erative models is challenging, e.g., from face images where
no facial landmarks are given. However, only limited work
has been done in this direction.

In this paper, we propose to learn deformable generator

model that can disentangle the appearance and geometric
information in purely unsupervised manner under a uniﬁed
probabilistic framework. Speciﬁcally, our model integrates
two generator networks: one appearance generator and one
geometric generator with two sets of independent latent fac-
tors. The dense local displacements are generated by the ge-
ometric generator, which then act on the image intensities
generated by the appearance generator to obtain the ﬁnal
image through a differentiable warping function. The mod-
el is learned by introducing alternating back-propagation for
two latent factors, and it can also be easily extended to oth-
er generative models such as deformable variational auto-
encoder. The proposed method can learn well-disentangled
representation, which can transfer the appearance and geo-
metric knowledge to other datasets and tasks.
Our contributions are summarized below:

• Propose a deformable generator network to disentan-
gle the appearance and geometric information in pure-
ly unsupervised manner.

• The proposed method is general and agnostic. It can
be easily extended to different models, such as de-
formable variational auto-encoder.

• Perform extensive experiments both qualitatively and
quantitatively to show that appearance and geometric
information can be well disentangled, which can be ef-
fectively transferred to other datasets and tasks.

2. Related work

Existing work on learning disentangled representation
using deep generative models generally fall into two cate-
gories: implicit learning and explicit learning.

The implicit learning methods proceed through laten-
t factors disentanglement and are focused on two cate-
gories: the Generative Adversarial Networks (GANs) [10,
8, 28, 23, 33] and the Variational Auto-encoders (VAEs)
InfoGAN [5] and β-VAE [13] are rep-
[17, 30, 27, 21].
resentatives for the two families. Though implicit methods

110354

can be learned unsupervisely, their learned representation is
not controllable and not well separated.

The explicit methods, on the other hand, model appear-
ance and geometric separately and ﬁnd their roots in the
Active Appearance Models (AAM) which [6, 19] separate-
ly learn the appearance and geometric information. Recent-
ly, [18] incorporates the shape geometry into the GANs and
learns well separated appearance and geometric informa-
tion. However, these methods [19, 18] require annotated
facial landmarks for each image during training. Unsuper-
vised disentangling of the appearance and geometric infor-
mation is challenging and remains largely unexplored. An
independent work proposed recently by [31] follow this di-
rection, but their model is focused on the auto-encoder (AE)
only and is not developed under probabilistic framework
compared to ours.

3. Model and learning algorithm

3.1. Model

Figure 1. An illustration of the proposed model. The model con-
tains two generator networks: one appearance generator and one
geometric generator. The two generators are combined by a warp-
ing function to produce the ﬁnal image. The warping function in-
cludes a geometric transformation operation for image coordinates
and a differentiable interpolation operation. The reﬁning operation
is optional for improving the warping function.

The proposed model contains two generator network-
s: one appearance generator and one geometric generator,
which are combined by a warping function to produce the
ﬁnal images or video frames, as shown in ﬁgure 1. Suppose
an arbitrary image or video frame X ∈ RDx×Dy×3 is gener-
ated with two independent latent vectors, Z a ∈ Rda which
controls the appearance, and Z g ∈ Rdg which controls the
geometric information. Varying the geometric latent vec-
tor Z g and ﬁxing the appearance latent vector Z a, we can
transform an object’s geometric information, such as rotat-
ing it with certain angle and changing its shape. On the
other hand, varying Z a and ﬁxing Z g, we can change the
identity or the category of the object, while keeping it ge-
ometric information unchanged, such as the same viewing
angle or the same shape.

The model can be expressed as

X = F (Z a, Z g; θ)

= Fw(Fa(Z a; θa), Fg(Z g; θg)) + ǫ

(1)

where Z a ∼ N(0, Ida ), Z g ∼ N(0, Idg ), and ǫ ∼
N(0, σ2ID) (D = Dx × Dy × 3) are independent. Fw
is the warping function, which uses the displacements gen-
erated by the geometric generator Fg(Z g; θg) to warp the
image generated by the appearance generator Fa(Z a; θa) to
synthesize the ﬁnal output image X.

3.2. Warping function

A warping function usually includes a geometric trans-
formation operation for image coordinates and a differen-
tiable interpolation (or resampling) operation. The geomet-
ric transformation describes the target coordinate (x, y) for
every location (u, v) in the source coordinate. The geomet-
ric operation only modiﬁes the positions of pixels in an im-
age without changing the color or illumination. Therefore,
the appearance information and the geometric information
are naturally disentangled by the two generators in the pro-
posed model.

The geometric transformation Φ can be a rigid afﬁne
mapping, as used in the spatial transformer networks [16],
or a non-rigid deformable mapping, which is the case in our
work. Speciﬁcally, the coordinate displacement (dx, dy) (or
the dense optical ﬂow ﬁeld) of each regular grid (x, y) in
the output warped image X are generated by the geometric
generator Fg(Z g; θg). The point-wise transformation in this
deformable mapping can be formulated as

v(cid:19) = Φ(Z g ,θg)(cid:18)x
(cid:18)u

y + dy(cid:19)
y(cid:19) = (cid:18)x + dx

(2)

where (u, v) are the source coordinates of the image gener-
ated by the appearance generator Fa(Z a; θa).

Since the evaluated (u, v) by Eq.(2) do not always have
integer coordinates, each pixel’s value of the output warped
image X can be computed by a differentiable interpolation
operation. Let Xa = Fa(Z a; θa) denote the image generat-
ed by the appearance generator. The warping function Fw
can be formulated as

X(x, y) = FI (Xa(x + dx, y + dy)),

(3)

where FI is the differentiable interpolation function. We
use a differentiable bilinear interpolation:

X(x, y) =

Dy

Xj

Dx

Xi

Xa(i, j)M (1 − |u − i|)M (1 − |v − j|)

(4)
where M (·) = max(0, ·). The details of back-propagation
through this bilinear interpolation can be found in [16].

10355

The displacement (dx, dy) is used in the deformable
convolutional networks [7]. The computation of coordi-
nates displacement (dx, dy) is known as the optical ﬂow
estimation [14, 3, 32, 9, 15, 29]. Our work is concerned
with modeling and generating the optical ﬂow, in addition
to estimating the optical ﬂow.

The displacement (dx, dy) may result from the motion
of the objects in the scene, or the change of viewpoint rel-
ative to 3D objects in the scene. It is natural to incorporate
motion and 3D models into the geometric generator where
the change or variation of Z g depends on the motion and
3D information.

3.3. Inference and learning

To learn this deformable generator model, we introduce
a learning and inference algorithm for two latent vectors,
without designing and learning extra inference networks.
Our method is motivated by a maximum likelihood learn-
ing algorithm for generator networks [12]. Speciﬁcally,
the proposed model can be trained by maximizing the log-
likelihood on the training dataset {Xi, i = 1, . . . , N },

L(θ) =

=

1
N

1
N

N

Xi=1
Xi=1

N

log p(Xi; θ)

logZ p(Xi, Z a

i , Z g

i ; θ)dZ a

i dZ g

i , (5)

where we integrate out the uncertainties of Z a
in
the complete-data log-likelihood to get the observed-data
log-likelihood.

i and Z g

i

We can evaluate the gradient of L(θ) by the following

well-known result, which is related to the EM algorithm:

∂
∂θ

log p(X; θ)

=

1

p(X; θ)

∂

∂θ Z p(X, Z a, Z g)dZ adZ g

= Ep(Z a,Z g|X;θ)(cid:20) ∂

∂θ

log p(X, Z a, Z g; θ)(cid:21)

(6)

Since the expectation in Eq.(6) is usually analytically in-
tractable, we employ Langevin dynamics to draw samples
from the posterior distribution p(Za, Zg|X; θ) and compute
the Monte Carlo average to estimate the expectation term.
For each observation X, the latent vectors Z a and Z g can
be sampled from p(Z a, Z g|X; θ) alternately by Langevin
dynamics: we ﬁx Z g and sample Z a from p(Z a|X; Z g, θ)
∝ p(X, Z a; Z g, θ), and then ﬁx Z a and sample Z g from
p(Z g|X; Z a, θ) ∝ p(X, Z g; Z a, θ). At each sampling step,

the latent vectors are updated as follows:

Z a

t+1 = Z a

t +

t+1 = Z g
Z g

t +

δ2
2
δ2
2

∂
∂Z a log p(X, Z a
∂
∂Z g log p(X, Z g

t ; Z g

t , θ) + δE a

t

t ; Z a

t , θ) + δE g

t

(7)

where t is the number of steps in the Langevin sampling,
t , E g
E a
t are independent standard Gaussian noise to prevent
the sampling from being trapped in local modes, and δ is
the step size. The complete-data log-likelihood can be eval-
uated by

log p(X, Z a; Z g, θ) = log [p(Z a)p(X|Z a, Z g, θ)]

= −

1
2σ2 kX − F (Z a, Z g; θ)k2 −

kZ ak2 + C1
log p(X, Z g; Z a, θ) = log [p(Z g)p(X|Z a, Z g, θ)]

1
2

= −

1
2σ2 kX − F (Z a, Z g; θ)k2 −

1
2

kZ gk2 + C2

(8)

where C1 and C2 are normalizing constants.
It can be
shown that, given sufﬁcient sampling steps, the sampled Z a
and Z g follow their joint posterior distribution.

Obtaining fair samples from the posterior distribution by
MCMC is highly computational consuming. In this paper,
we run persistent sampling chains. That is, the MCMC sam-
pling at each iteration starts from the sampled Z a and Z g
in the previous iteration. The persistent updating results in
a chain that is long enough to sample from the posterior
distribution, and the warm initialization vastly reduces the
computational burden of the MCMC sampling. The con-
vergence of stochastic gradient descent based on persistent
MCMC has been studied by [34].

For each training example Xi, we run the Langevin dy-
namics following Eq.(7) to get the corresponding posterior
samples Z a
i . The sample is then used for gradi-
ent computation in Eq.(6). More precisely, the gradient of
log-likelihood over θ is estimated by Monte Carlo approxi-
mation:

i and Z g

N

∂
∂θ

∂
∂θ

1
N

L(θ) ≈

Xi=1
1
i , Z g
σ2 (Xi − F (Z a

=

1
N

N

Xi=1

log p(Xi, Z a

i , Z g

i ; θ)

i ; θ))

∂
∂θ

F (Z a

i , Z g

i ; θ).

(9)

The whole algorithm iterates through two steps:

(1)
inferential step which infers the latent vectors through
Langevin dynamics, and (2) learning step which learns the
network parameters θ by stochastic gradient descent. Gra-
dient computations in both steps are powered by back-
propagation. Algorithm 1 describes the details of the learn-
ing and inference algorithm.

10356

Algorithm 1 Learning and inference algorithm
Require:

(1) training examples {Xi ∈ RDx×Dy×3, i = 1, . . . , N }
(2) number of Langevin steps l
(3) number of learning iterations T

Ensure:

(1) learned parameters θ
(2) inferred latent vectors {Z a

i , Z g

i , i = 1, . . . , N }

1: Let t ← 0, initialize θ.
2: Initialize {Z a
repeat

i , Z g

i , i = 1, . . . , N }

i |Xi; Z g
i |Xi; Z a

i , θ), while ﬁxing Z g
i , θ), while ﬁxing Z a

3: Inference back-propagation: For each i, run l
steps of Langevin dynamics to alternatively sample Z a
i
i ; and sample Z g
from p(Z a
i
from p(Z g
i . Starting from
the current Z a
4: Learning back-propagation: Update θt+1 ←
θt + ηtL′(θt), with learning rate ηt, where L′(θt) is
computed according to Eq.(9).
5: Let t ← t + 1

i , each step follows Eq.(7).

i and Z g

until t = T

3.4. Deformable Variational Auto encoder

The proposed deformable generator scheme is general
and agnostic to different models. In fact, our method can al-
so be learned by VAE [17] to obtain deformable variational
auto-encoder, by utilizing extra inference network to infer
(Z a, Z g) through re-parametrization. Speciﬁcally, we learn
another q(Z a, Z g|X; φ) to approximate the intractable pos-
terior p(Z a, Z g|X; θ).
The appearance and geometric
latent vectors are assumed to be independent Gaussian
in the approximated distribution, i.e., q(Z a, Z g|X; φ) =
q(Z a|X; φ)q(Z g|X; φ), where the means and variances are
modeled by inference network with parameters φ. This de-
formable VAE model is a naturally extension of the pro-
posed deformable generator framework developed. We
show some preliminary results in Sec.4.1.1. Notice that the
proposed scheme can also be used in adversarial learning
methods[10], by designing a separate discriminator network
for shape and appearance. We leave it as our further work.
In this work, we focus on the current learning and inference
algorithm for the sake of simplicity, so that we do not resort
to extra networks.

4. Experiments

In this section, we ﬁrst qualitatively demonstrate that our
proposed deformable generator framework consistently dis-
entangles the appearance and geometric information. Then
we analyze the proposed model quantitatively. The struc-
tures and parameters of the proposed model are listed in the

Appendix. In the following experiments, in each row we
visualize the generated samples by varying a certain unit of
the latent vectors within the range [−γ, γ], where we set γ
to be 10.

4.1. Qualitative experiments

4.1.1 Experiments on CelebA

We ﬁrst train the deformable generator on the 10,000 ran-
dom selected face images from CelebA dataset [24]. Select-
ed images are processed by the OpenFace [1] and further
cropped to 64 × 64 pixels.

To study the performance of the proposed method in dis-
entangling the appearance and geometric information, we
investigate the effect of different combinations of the geo-
metric latent vector Z g and the appearance latent vector Z a.
(1) Set the geometric latent vector Z g to zero, and vary one
dimension of the appearance variable Z a from [−γ, γ] with
a uniform step 2γ
10 , while holding the other dimensions of
Z a at zero. Some typical generated images are shown in ﬁg-
ure 2. (2) Set Z a to be a ﬁxed value, and each time vary one
dimension of the geometric latent vector Z g from [−γ, γ]
with a uniform step 2γ
10 , while keeping the other dimension-
s of Z g at zero. Some representative generated results are
shown in ﬁgure 3.

Figure 2. Each dimension of the appearance latent vector encodes
appearance information such as color, illumination and gender. In
the ﬁst line, the color of background and the gender change. In the
second line, the moustache of the man and the hair of the woman
vary. In the third line, the skin color changes from dark to white. In
the fourth line, the illumination lighting changes from the left-side
of the face to the right-side of the face.

As we can observe from ﬁgure 2, (1) although the train-
ing faces from CelebA have different viewing angles, the
appearance latent vector only encodes front-view informa-
tion, and (2) each dimension of the appearance latent vector
encodes appearance information such as color, illumination
and identity. For example, in the ﬁst line of ﬁgure 2, from
left to right, the color of background varies from black to
white, and the identity of the face changes from a women to
a man. In the second line of ﬁgure 2, the moustache of the
man becomes thicker when the value of the corresponding
dimension of Z a decreases, and the hair of the woman be-
comes denser when the value of the corresponding dimen-
sion of Z a increases. In the third line, from left to right, the

10357

Figure 3. Each dimension of the geometric latent vector encodes
fundamental geometric information such as shape and viewing an-
gle. In the ﬁst line, the shape of the face changes from fat to thin
from left to the right. In the second line, the pose of the face varies
from left to right. In the third line, from left to right, the vertical
tilt of the face varies from downward to upward. In the fourth line,
the face width changes from stretched to cramped.

skin color varies from dark to white, and in the fourth line,
from left to right, the illumination lighting changes from the
left-side of the face to the right-side of the face.

From ﬁgure 3, we have the following interesting obser-
vations. (1) The geometric latent vectors does not encode
any appearance information. The color, illumination and
identity are the same across these generated images.
(2)
Each dimension of the geometric latent vector encodes fun-
damental geometric information such as shape and viewing
angle. For example, in the ﬁst line of ﬁgure 3, the shape
of the face changes from fat to thin from left to the right;
in the second line, the pose of the face varies from left to
right; in the third line, from left to right, the tilt of the face
varies from downward to upward; and in the fourth line, the
expression changes from stretched to cramped.

Figure 5. Geometry interpolation results by deformable VAE.

geometric warping (e.g. operations in ﬁgure 3) learned by
the geometric generator to all the canonical faces (e.g. gen-
erated faces in ﬁgure 2) learned by the appearance genera-
tor. Figure 6 demonstrates the effect of applying geometric
warping to the generated canonical faces in ﬁgure 2. Com-
paring ﬁgure 2 with ﬁgure 6, we ﬁnd that the rotation and
shape warping operations do not modify the identity infor-
mation of the canonical faces, which corroborates the disen-
tangling power of the proposed deformable generator mod-
el.

(a) Rotation warping.

(b) Shape warping.

Figure 6. Applying the (a) rotation warping and (b) shape warp-
ing operations learned by the geometric generator to the canonical
faces generated by the appearance generator. Compared with ﬁg-
ure 2, only the pose information varies, and the identity informa-
tion is kept in the process of warping.

Figure 4. Appearance interpolation results by deformable VAE.

The appearance and geometric information could also be
effectively disentangled by the introduced deformable VAE.
For the extra inference network, or encoder network, we use
the mirror structure of our generator model in which we use
convolution layers instead of convolution transpose layers.
The generator network structure as well as other parame-
ters are kept the same as the model learned by alternating
back-propagation. Figures 4 and 5 show interpolation re-
sults following the same protocol described before.

From the results in ﬁgures 2 and 3, we ﬁnd that the ap-
pearance and geometric information of face images have
been disentangled effectively. Therefore, we can apply the

1 , Z a

1 , Z g

2 ,. . . ,Z a

2 ,. . . ,Z g

7 and geometric vectors Z g

Furthermore, we evaluate the disentangling ability of the
proposed model by transferring and recombining geometric
and appearance vectors from different faces. Speciﬁcally,
we ﬁrst feed 7 unseen images from CelebA into our de-
formable generator model to infer their appearance vectors
Z a
7 using
the Langevin dynamics (with 300 steps) in Eq.(7). Then, we
transfer and recombine the appearance and geometric vec-
tors and use {Z a
7 } to generate six new
face images, as shown in the second row of ﬁgure 7. We
also transfer and recombine the appearance and geometric
vectors and use {Z a
1 } to generate anoth-
er six new faces, as shown in the third row of ﬁgure 7. From
the 2nd to the 7th column, the images in the second row
have the same appearance vector Z a, but the geometric la-
tent vectors Z g are swapped between each image pair. As

2 }, . . . , {Z a

1 },. . . , {Z a

1 , Z g

1 , Z g

2 , Z g

7 , Z g

10358

Figure 7. Transferring and recombining geometric and appearance
vectors. The ﬁrst row shows 7 unseen faces from CelebA. The sec-
ond row shows the generated faces by transferring and recombin-
ing 2th-7th faces’ geometric vectors with ﬁrst face’s appearance
vector in the ﬁrst row. The third row shows the generated faces by
transferring and recombining the 2th-7th faces’ appearance vec-
tors with the ﬁrst face’s geometric vector in the ﬁrst row.

we can observe from the second row of ﬁgure 7, (1) the geo-
metric information of the original images are swapped in the
synthesized images, and (2) the inferred Z g can capture the
view information of the unseen images. The images in the
third row of ﬁgure 7 have the same geometric vector Z g
1 , but
the appearance vectors Z a are swapped between each image
pair. From the third row of ﬁgure 7, we observe that (1) the
appearance information are exchanged. (2) The inferred Z a
capture the color, illumination and coarse appearance infor-
mation but lose more nuanced identity information. Only
ﬁnite features are learned from 10k CelebA images, and the
model may not contain the features necessary to model an
unseen face accurately.

4.1.2 Experiments on expression dataset

We next study the performance of the proposed deformable
generator model on the face expression dataset CK+ [25].
Following the same experimental protocol as the last sub-
section, we can investigate the change produced by each
dimension of the appearance latent vector (after setting the
value of geometric latent vector to zero) and the geometric
latent vector (after setting the appearance latent vector to
a ﬁxed value). The disentangled results are shown in ﬁg-
ure 8. We do not use the labels of expressions provided by
CK+ dataset in the learning. Although the dataset contains
faces of different expressions, the learned appearance latent
vector usually encodes a neutral expression. The geomet-
ric latent vector controls major variation in expression, but
does not change the identity information.

To test whether appearance and geometric information
are disentangled in the proposed model, we try to transfer
the learned expression from CK+ to another face dataset,
Multi-Pie [11], by ﬁne-turning the appearance generator on
the target face dataset while ﬁxing the parameters in the ge-
ometric generator. Figure 8(c) shows the result of transfer-
ring the expressions of 8(b) into the faces of Multi-Pie. The
expressions from the gray faces of CK+ have been trans-
ferred into the color faces of Multi-Pie.

(a) Interpolation of appearance latent vectors.

(b) Interpolation of geometric latent vectors.

(c) Transferring the expression in (b) to the face images in Multi-PIE

dataset.

Figure 8. Interpolation examples of (a) appearance latent vectors
and (b) geometric latent vectors. (c) Transferring the learned ex-
pression to the face images in Multi-PIE dataset.

4.1.3 Experiment on non-face dataset

We could transfer and learn the model on more general
dataset other than face images. For example, the learned
geometric information from the CelebA face images can be
directly transferred to the faces of animals such as cats and
monkeys, as shown in ﬁgure 9. The cat faces rotate from
left to right and the shape of monkey faces changes from fat
to thin, when the warpings learned from human faces are
applied.

We also learn our model on the CIFAR-10 [20] dataset,
which includes 50,000 training examples of various object
categories. We randomly sample Z a from N(0, Ida ). For
Z g, we interpolate one dimension from −γ to γ and ﬁx the
other dimensions to 0. Figure 9 shows interpolated exam-
ples generated by model learned from the car category. For
each row, we use different Z a and interpolate the same di-
mension of Z g. The results show that each dimension of Z g
controls a speciﬁc geometric transformation, i.e., shape and
rotation warping.

4.2. Quantitative experiments

4.2.1 Covariance between the latent vectors and geo-

metric variation

First we quantitatively study the covariance between each
dimension of the latent vectors (Z g, Z a) and input images
with geometric variation. We use images with ground-
truth labels that record geometric attributes, speciﬁcal-
ly the multi-view face images from the Multi-Pie dataset
[11]. For each of the 5 viewing angles {−30◦, −15◦,
0◦, 15◦, 30◦}, we feed 100 images into the learned
model to infer their geometric latent vector Z g and ap-
pearance latent vector Z a.
Under each view θ ∈

10359

Figure 9. Transferring and learning model from non-face dataset-
s. The ﬁrst two rows show geometric interpolation results of cat
and monkey faces after applying the rotation and shape warping
learned from CelebA. The last two rows show geometric interpo-
lation results of the model learned from car category of CIFAR-10
dataset.

0◦ (i), ¯Z g

−15◦ (i), ¯Z g

−30◦ (i), ¯Z g

{−30◦, −15◦, 0◦, 15◦, 30◦} , we compute the means ¯Z g
θ
and ¯Z a
θ of the inferred latent vectors. For each dimen-
sion i of Z g, we construct a 5-dimensional vector ¯Z g(i) =
[ ¯Z g
30◦ (i)]. Similarly,
we construct a 5-dimensional vector ¯Z a(i) under each di-
mension of Z a. We normalize the viewing angles vector
θ = [−30, −15, 0, 15, 30] to have unit norm. Finally, we
compute the covariance between each dimension of the la-
tent vectors (Z g, Z a) and input images with view variations
as follows:

15◦ (i), ¯Z g

Rg

i = | ¯Z g(i)⊤θ|, Ra

i = | ¯Z a(i)⊤θ|

(10)

where i denotes the i-th dimension of latent vector Z g or
Z a, and | · | denotes the absolute value. We summarize the
the covariance responses Rg and Ra of the geometric and
appearance latent vectors in ﬁgure 10. Rg tends to be much
larger than Ra.

Figure 10. Absolute value of covariance between each dimension
of the geometric (or appearance) latent vectors and view varia-
tions for the face images from Multi-Pie. The left subﬁgure shows
covariance with the geometric latent vector; the right subﬁgure
shows covariance with the appearance latent vector.

i and the largest Ra

Moreover, for the two largest Rg

i , we
plot covariance relationship between the latent vector ¯Z g(i)
(or ¯Z a(i)) and viewing angles vector θ in ﬁgure 11. As we
can observe from the left and the center subﬁgures from ﬁg-
ure 11, the ¯Z g(i) corresponding to the two largest Rg
i (Rg
5,

(a)

(b)

Figure 11. (a) Covariance relationship between the mean latent
vector ¯Z g(i) (or ¯Z a(i)) and viewing angles vector θ. We choose t-
wo dimensions of Z g (Z g
38, left and center) with the largest
covariance and one dimension of Z a with the largest covariance
(Z a
25, right). (b) Images generated by varying the values of the
three dimensions in (a) respectively, while ﬁxing the values of oth-
er dimensions to be zero.

5 and Z g

Rg
38) is obviously inversely proportional or proportional to
the change of viewing angle. However, as shown in the
right subﬁgure, the ¯Z a(i) corresponding to the largest Ra
i
(Ra
25) does not have strong covariance with the change of
viewing angle. We wish to point out that we should not
expect Z a to encode the identity exclusively and Z g to en-
code the view exclusively, because different persons may
have shape changes, and different views may have lighting
or color changes.

Furthermore, we generate face images by varying the di-
mension of Z g corresponding to the two largest covariance
responses from values [−γ, +γ] with a uniform step 2γ
10 ,
while holding the other dimensions of Z g to zero as we did
in the subsection 4.1.1. Similarly, we generate face images
by varying the dimension of Z a corresponding to the largest
covariance responses from values [−γ, +γ] with a uniform
step 2γ
10 , while holding the other dimensions of Z a to zero.
The generated images are shown in ﬁgure 11(b). We can
make several important observations. (1) The variation in
viewing angle in the ﬁrst two rows is very obvious, and the
magnitude of the change in view in the ﬁrst row is larger
than that in the second row. This is consistent with the fact
that Rg
38 and with the observation that the slope in the
left subﬁgure of ﬁgure 11(a) is steeper than that of the cen-
ter subﬁgure of ﬁgure 11(a). (2) In the ﬁrst row, the faces
rotate from right to left, where Rg
5 is inversely proportion-
al to the viewing angle. In the second row, the faces rotate
from left to right, where Rg
38 is proportional to the viewing
angle. (3) It is difﬁcult to ﬁnd obvious variation in view-
ing angle in the third row. These generated images further
verify that the geometric generator of the proposed model
mainly captures geometric variation, while the appearance

5 > Rg

10360

Methods

MSRE
30◦
15◦
0◦

−15◦
−30◦

all views

VAE

ABP

Ours

110.99 ± 0.11
88.98 ± 0.09
48.78 ± 0.05
87.89 ± 0.10
107.94 ± 0.12
89.02 ± 0.13

117.28 ± 0.12
94.81 ± 0.10
48.36 ± 0.06
94.12 ± 0.11
120.58 ± 0.13
94.66 ± 0.12

89.94 ± 0.10
70.64 ± 0.08
46.10 ± 0.06
75.11 ± 0.09
92.66 ± 0.11
76.52 ± 0.10

Table 1. Comparison of the Mean Square Reconstruction Er-
rors (MSRE) per image (followed by the corresponding standard
derivations inside the parentheses) of different methods for unseen
multi-view faces from the Multi-Pie dataset.

generator is not sensitive to geometric variation.

4.2.2 Reconstruction error on unseen multi-view faces

Since the proposed deformable generator model can disen-
tangle the appearance and geometric information from an
image, we can transfer the geometric warping operation
learned from one dataset into another dataset. Speciﬁcal-
ly, given 1000 front-view faces from the Multi-Pie dataset
[11], we can ﬁne-tune the appearance generator’s param-
eters while ﬁxing the geometric generator’s parameters,
which are learned from the CelebA dataset. Then we can
reconstruct unseen images that have various viewpoints. In
order to quantitatively evaluate the geometric knowledge
transfer ability of our model, we compute the reconstruction
error on 5000 unseen images from Multi-Pie for the views
{−30◦, −15◦, 0◦, 15◦, 30◦}, with 1000 faces for each view.
We compare the proposed model with the state-of-art gen-
erative models, such as VAE [17, 4] and ABP [12]. For
fair comparison, we ﬁrst train the original non-deformable
VAE and ABP models with the same CelebA training set
of 10,000 faces, and then ﬁne-tune them on the 1000 front-
view faces from the Multi-Pie dataset. We perform 10 in-
dependent runs and report the mean square reconstruction
error per image and standard derivation over the 10 trial-
s for each method under different views as shown in Table
1. Deformable generator network obtains the lowest recon-
struction error. When the testing images are from the view
closing to that from the training images, all the three meth-
ods can obtain small reconstruction errors. When various
views of the testing images are included, deformable gener-
ator network obtains obviously smaller reconstruction error.
Our model beneﬁts from the transferred geometric knowl-
edge learned from the CelebA dataset, while both the non-
deformable VAE and ABP models cannot efﬁciently learn
or transfer purely geometric information.

4.3. Balancing explaining away competition

Since the geometric generator only produces displace-
ment for each pixel without modifying the pixel’s value, the
color and illumination information and the geometric infor-
mation are naturally disentangled by the proposed model’s

speciﬁc structure. In order to properly disentangle the iden-
tity (or category) and the view (or geometry) information,
the learning capacity between the appearance generator and
geometric generator should be balanced. Two generators
cooperate with each other to generate the images. Mean-
while, they also compete against each other to explain away
the training images. If the learning of the appearance gen-
erator outpaces that of the geometric generator, the appear-
ance generator will encode most of the knowledge, includ-
ing the view and shape information, while the geometric
generator will only learn minor warping operations. On the
other hand, if the geometric generator learns much faster
than the appearance generator, the geometric generator will
encode most of the knowledge, including the identity or cat-
egory information, which should be encoded by the appear-
ance network.

To control the tradeoff between the two generators, we
introduce a balance parameter α, which is deﬁned as the
ratio of the number of ﬁlters within each layer of the ap-
pearance and geometric generators. We tune the α carefully
and set it to 0.625 in our experiments.

5. Conclusion

We propose a deformable generator model which aims
to disentangle the appearance and geometric information of
an image into two independent latent vectors Za and Zg.
The learned geometric generator can be transferred to other
datasets, or can be used for the purpose of data augmenta-
tion to produce more variations beyond the training dataset
for better generalization.

In addition to the learning and inference algorithm
adopted in this paper, the model can also be trained by VAE
and GAN, as well as their generalizations such as β-VAE
and info-GAN, which target for disentanglement in gener-
al.

The model can be generalized to model dynamic patterns
by adding transition models for the latent vectors. The tran-
sition model for the appearance vector may generate dy-
namic textures of non-trackable motion, while the transi-
tion model for the geometric vector may generate intuitive
physics of trackable motion. The geometric generator can
also be generalized to incorporate 3D information of rigid
or non-rigid 3D objects.

Acknowledgment

This work was done while the ﬁrst author worked as a
visiting scholar at UCLA and supported by Natural Sci-
ence Foundation of China No. 61703119, Natural Science
Fund of Heilongjiang Province of China No. QC2017070
and DARPA XAI project N66001-17-2-4029, ARO project
W911NF1810296, and ONR MURI N00014-16-1-2007.
We thank Mitchell K. Hill for his assistance with writing.

10361

References

[1] Brandon Amos, Bartosz Ludwiczuk, and Mahadev Satya-
narayanan. Openface: A general-purpose face recognition
library with mobile applications. Technical report, CMU-
CS-16-118, CMU School of Computer Science, 2016.

[2] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-
resentation learning: A review and new perspectives. IEEE
transactions on pattern analysis and machine intelligence,
35(8):1798–1828, 2013.

[3] Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim
Weickert. High accuracy optical ﬂow estimation based on
a theory for warping. In European conference on computer
vision, pages 25–36. Springer, 2004.

[4] Christopher P Burgess,

Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and Alexan-
der Lerchner. Understanding disentangling in beta-vae. arX-
iv preprint arXiv:1804.03599, 2018.

[5] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable repre-
sentation learning by information maximizing generative ad-
versarial nets. In Advances in Neural Information Processing
Systems, pages 2172–2180, 2016.

[6] Timothy F. Cootes, Gareth J. Edwards, and Christopher J.
Taylor. Active appearance models.
IEEE Transactions on
pattern analysis and machine intelligence, 23(6):681–685,
2001.

[7] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 764–773, 2017.

[8] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a laplacian pyramid of adver-
sarial networks. In NIPS, pages 1486–1494, 2015.

[9] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hauss-
er, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt,
Daniel Cremers, and Thomas Brox. Flownet: Learning opti-
cal ﬂow with convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision, pages
2758–2766, 2015.

[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing X-
u, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[11] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade,
Image Vision Comput.,

and Simon Baker. Multi-pie.
28(5):807–813, May 2010.

[12] Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Al-
ternating back-propagation for generator network. In AAAI,
pages 1976–1984, 2017.

[13] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. beta-vae: Learning basic visual con-
cepts with a constrained variational framework. 2016.

[14] Berthold KP Horn and Brian G Schunck. Determining opti-

cal ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981.

[15] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keu-
per, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0:
Evolution of optical ﬂow estimation with deep networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), volume 2, 2017.

[16] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in neural infor-
mation processing systems, pages 2017–2025, 2015.

[17] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. arXiv preprint arXiv:1312.6114, 2013.

[18] Jean Kossaiﬁ, Linh Tran, Yannis Panagakis, and Maja Pan-
tic. Gagan: Geometry-aware generative adverserial network-
s. arXiv preprint arXiv:1712.00684, 2017.

[19] Jean Kossaiﬁ, Georgios Tzimiropoulos, and Maja Pantic.
Fast and exact newton and bidirectional ﬁtting of active ap-
pearance models. IEEE transactions on image processing,
26(2):1040–1053, 2017.

[20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple

layers of features from tiny images. 2009.

[21] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakr-
ishnan. Variational inference of disentangled latent con-
cepts from unlabeled observations.
arXiv preprint arX-
iv:1711.00848, 2017.

[22] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum,
and Samuel J Gershman. Building machines that learn and
think like people. Behavioral and Brain Sciences, 40, 2017.
[23] Zejian Li, Yongchuan Tang, and Yongxing He. Unsuper-
vised disentangled representation learning with analogical
relations. arXiv preprint arXiv:1804.09502, 2018.

[24] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
[25] Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Sarag-
ih, Zara Ambadar, and Iain Matthews. The extended cohn-
kanade dataset (ck+): A complete dataset for action unit and
emotion-speciﬁed expression. In Computer Vision and Pat-
tern Recognition Workshops (CVPRW), 2010 IEEE Comput-
er Society Conference on, pages 94–101. IEEE, 2010.

[26] Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya
Ramesh, Pablo Sprechmann, and Yann LeCun. Disentan-
gling factors of variation in deep representation using adver-
sarial training. In Advances in Neural Information Process-
ing Systems, pages 5040–5048, 2016.

[27] Andriy Mnih and Karol Gregor. Neural variational inference

and learning in belief networks. In ICML, 2014.

[28] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016.

[29] Anurag Ranjan and Michael J Black. Optical ﬂow estima-
tion using a spatial pyramid network. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 2, 2017.

[30] Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra.
Stochastic backpropagation and approximate inference in
deep generative models. In NIPS, pages 1278–1286, 2014.

[31] Zhixin Shu, Mihir Sahasrabudhe, Alp Guler, Dimitris Sama-
ras, Nikos Paragios, and Iasonas Kokkinos. Deforming au-

10362

toencoders: Unsupervised disentangling of shape and ap-
pearance. arXiv preprint arXiv:1806.06503, 2018.

[32] Deqing Sun, Stefan Roth, and Michael J Black. A quan-
titative analysis of current practices in optical ﬂow estima-
tion and the principles behind them. International Journal of
Computer Vision, 106(2):115–137, 2014.

[33] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre-
sentation learning gan for pose-invariant face recognition. In
CVPR, volume 3, page 7, 2017.

[34] Laurent Younes. On the convergence of markovian stochas-
tic algorithms with rapidly decreasing ergodicity rates. S-
tochastics: An International Journal of Probability and S-
tochastic Processes, 65(3-4):177–228, 1999.

10363

