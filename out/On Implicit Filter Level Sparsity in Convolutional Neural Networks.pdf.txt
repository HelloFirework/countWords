On Implicit Filter Level Sparsity in Convolutional Neural Networks

Dushyant Mehta1

,

3 Kwang In Kim2 Christian Theobalt1

3

,

1MPI For Informatics 2UNIST 3Saarland Informatics Campus

Abstract

We investigate ﬁlter level sparsity that emerges in convo-
lutional neural networks (CNNs) which employ Batch Nor-
malization and ReLU activation, and are trained with adap-
tive gradient descent techniques and L2 regularization or
weight decay. We conduct an extensive experimental study
casting our initial ﬁndings into hypotheses and conclusions
about the mechanisms underlying the emergent ﬁlter level
sparsity. This study allows new insight into the perfor-
mance gap obeserved between adapative and non-adaptive
gradient descent methods in practice. Further, analysis of
the effect of training strategies and hyperparameters on the
sparsity leads to practical suggestions in designing CNN
training strategies enabling us to explore the tradeoffs be-
tween feature selectivity, network capacity, and generaliza-
tion performance. Lastly, we show that the implicit sparsity
can be harnessed for neural network speedup at par or bet-
ter than explicit sparsiﬁcation / pruning approaches, with
no modiﬁcations to the typical training pipeline required.

1. Introduction

In this work we show that ﬁlter1 level sparsity emerges in
certain types of feedforward convolutional neural networks.
In networks which employ Batch Normalization and
ReLU activation, after training, certain ﬁlters are observed
to not activate for any input.
Importantly, the sparsity
emerges in the presence of non sparsity inducing regulariz-
ers such as L2 and weight decay (WD), and vanishes when
regularization is removed. We investigate how this sparsity
manifests under different hyperparameter settings, and pro-
pose an experimentally backed hypothesis for the cause of
this emergent sparsity, and the implications of our ﬁndings.
We ﬁnd that adaptive ﬂavours of SGD produce a higher
degree of sparsity than (m)SGD, both with L2 regulariza-
tion and weight decay (WD). Further, L2 regularization re-
sults in a higher degree of sparsity with adaptive methods
than weight decay. Additionally, we show that a multitude

1Filter refers to the weights and the nonlinearity associated with a par-
ticular feature, acting together as a unit. We use ﬁlter and feature inter-
changeably throughout the document.

of seemingly unrelated factors such as mini-batch size, net-
work size, and task difﬁculty impact the extent of sparsity.
These ﬁndings are important in light of contemporary
attempts to explain the performance gap between (m)SGD
and adaptive variants. Any theoretical and practical explo-
rations towards explaining the performance gap between
SGD and adaptive variants should account for this inad-
vertent reduction in network capacity when using adaptive
methods, which interplays with both the test accuracy and
the generalization gap. Contemporaneous work [27] has
also observed that Adam induces ﬁlter sparsity in ReLU net-
works, but lacks a thorough investigation of the causes.

Through a systematic experimental study, we hypothe-
size that the emergence of sparsity is the direct result of a
disproportionate relative inﬂuence of the regularizer (L2 or
WD) viz a viz the gradients from the primary training ob-
jective of ReLU networks. Multiple factors subtly impact
the relative inﬂuence of the regularizer in previously known
and unknown ways, and various hyperparameters and de-
sign choices for training neural networks interplay via these
factors to impact the extent of emergent sparsity.

We show that understanding the impact of these de-
sign choices yields useful and readily controllable sparsity
which can be leveraged for considerable neural network
speed up, without trading the generalization performance
and without requiring any explicit pruning [18, 13] or spar-
siﬁcation [14] steps. The implicit sparsiﬁcation process can
remove 70-80% of the convolutional ﬁlters from VGG-16
on CIFAR10/100, far exceeding that for [13], and performs
comparable to [14] for VGG-11 on ImageNet.

2. Observing Filter Sparsity in CNNs

We begin with the setup for our initial experiments, and
present our primary ﬁndings. In subsequent sections we fur-
ther probe the manifestation of ﬁlter sparsity, and present an
experimentally backed hypothesis regarding the cause.

2.1. Setup and Preliminaries

Our basic setup is comprised of a 7-layer convolutional
network with 2 fully connected layers as shown in Figure 1.

This work was funded by the ERC Consolidator Grant 4DRepLy

(770784).

1520

Figure 1. BasicNet: Structure of the basic convolution network
studied in this paper. We refer to the individual convolution lay-
ers as C1-7. The fully connected head shown here is for CI-
FAR10/100 and ObjectNet3D [26] experiments, and a different
fully-connected structure is used for TinyImageNet and ImageNet.

The network structure is inspired by VGG [23], but is more
compact. We refer to this network as BasicNet in the rest
of the document. We use a variety of gradient descent ap-
proaches, a mini-batch size of 40, with a method speciﬁc
base learning rate for 250 epochs, and scale down the learn-
ing rate by 10 for an additional 75 epochs. We train on CI-
FAR10 and CIFAR 100 [12], with normalized images, and
random horizontal ﬂips. Xavier initialization [6] is used for
the network weights, with the appropriate gain for ReLU.
The base learning rates and other hyperparameters are as
follows: Adam (1e-3, β1=0.9, β2=0.99, ǫ=1e-8), Adadelta
(1.0, ρ=0.9, ǫ=1e-6), SGD (0.1, momemtum=0.9), Adagrad
(1e-2). Pytorch [21] is used for training, and we study the
effect of varying the amount and type of regularization on
the extent of sparsity and test error in Table 1.

L2 regularization vs. Weight Decay: We make a dis-
tinction between L2 regularization and weight decay. For a
parameter θ and regularization hyperparameter 1 > λ ≥ 0,
weight decay multiplies θ by (1 − λ) after the update step
based on the gradient from the main objective. While for
L2 regularization, λθ is added to the gradient ∇L(θ) from
the main objective, and the update step is computed using
this sum. See [16] for a detailed discussion.

Quantifying Feature Sparsity: We measure the
learned feature sparsity in two ways, by per-feature activa-
tion and by per-feature scale. For sparsity by activation, for
each feature we apply max pooling to the absolute activa-
tions over the entire feature plane, and consider the feature
inactive if this value does not exceed 10−12 over the en-
tire training corpus. For sparsity by scale, we consider the
scale γ of the learned afﬁne transform in the Batch Norm
[8] layer. Batch normalization uses additional learned scale
γ and bias β that casts each normalized convolution output
ˆxi to yi = γ ˆxi + β. We consider a feature inactive if |γ| for
the feature is less than 10−3. Explicitly zeroing the features
thus marked inactive does not affect the test error, which en-
sures the validity of our chosen thresholds. The thresholds
chosen are purposefully conservative, and comparable lev-
els of sparsity are observed for a higher feature activation
threshold of 10−4, and a higher |γ| threshold of 10−2.

2.2. Primary Findings

Table 1 shows the overall feature sparsity by activation
(Act.) and by scale (γ) for BasicNet. Only convolution
features are considered. The following are the key obser-
vations from the experiments and the questions they raise.
These are further discussed in Section 3.

1): The emergent sparsity relies on the strength of L2
regularization or weight decay. No sparsity is observed in
the absence of regularization, with sparsity increasing
with increasing L2 or weight decay. What does this tell us
about the cause of sparsiﬁcation, and how does the sparsity
manifest across layers?

2): Regardless of the type of regularizer (L2 or weight
decay), adaptive methods (Adam, Adagrad, Adadelta)
learn sparser representations than SGD for compara-
ble levels of test error, with Adam showing the most spar-
sity and most sensitivity to the L2 regularization parameter
amongst the ones studied. Adam with L2 sees about 70%
features pruned for CIFAR10, while SGD shows no spar-
sity for a comparable performance, with a similar trend for
CIFAR100, as well as when weight decay is used. What
are the causes for this disparity in sparsity between SGD
and adaptive methods? We will focus on understanding the
disparity between SGD and Adam.

3): SGD has comparable levels of sparsity with L2 reg-
ularization and with weight decay (for higher regulariza-
tion values), while for Adam, L2 shows higher sparsity
for comparable performance than weight decay (70% vs
40% on CIFAR10, 47% vs 3% on CIFAR100). Why is there
a signiﬁcant difference between the sparsity for Adam with
L2 regularization vs weight decay?

4): The extent of sparsity decreases on moving from
the simple 10 class classiﬁcation problem of CIFAR10 to
the comparatively harder 100 class classiﬁcation problem
of CIFAR100. What does the task dependence of the extent
of sparsity tell us about the origin of the sparsity?

3. A Detailed Look at the Emergent Sparsity

Possible Cause of Sparsity: The analysis of Table 1
in the preceding section shows that the regularizer (L2 or
weight decay) is very likely the cause of the sparsity, with
differences in the level of sparsity attributable to the par-
ticular interaction of L2 regularizer (and lack of interaction
of weight decay) with the update mechanism. The differ-
ences between adaptive gradient methods (Adam) and SGD
can additionally likely be attributed to differences in the na-
ture of the learned representations between the two. That
would explain the higher sparsity seen for Adam in the case
of weight decay.

Layer-wise Sparsity: To explore the role of the regular-
izer in the sparsiﬁcation process, we start with a layer-wise
breakdown of sparsity. For each of Adam and SGD, we con-

521

Table 1. Convolutional ﬁlter sparsity in BasicNet trained on CI-
FAR10/100 for different combinations of regularization and gra-
dient descent methods. Shown are the % of non-useful / inactive
convolution ﬁlters, as measured by activation over training cor-
pus (max act. < 10−12) and by the learned BatchNorm scale
(|γ| < 10−03), averaged over 3 runs. The lowest test error per
optimizer is highlighted, and sparsity (green) or lack of sparsity
(red) for the best and near best conﬁgurations indicated via text
color. L2: L2 regularization, WD: Weight decay (adjusted with
the same scaling schedule as the learning rate schedule). Note that
for SGD with momentum, L2 and WD are not equivalent [16].

CIFAR10

CIFAR100

0

1e-02
2e-03
1e-03
1e-04
1e-05
1e-06

2e-03
1e-03
5e-04
2e-04
1e-04
1e-05

% Sparsity Test % Sparsity Test
L2 by Act by γ Error by Act by γ Error
69 64.8
23 47.1
42.1
4
38.8
0
0
37.4
39.0
0
0
40.1
85 69.7
81 42.7
76 39.0
47 36.6
40.6
5
0
40.5
0
40.3
98 84.1
89 53.2
82 46.3
61 39.1
35.4
3
1
35.9
88 63.3
59 37.2
25 35.9
37.3
1
0
42.1

69
23
4
0
0
0
0
87
82
77
47
5
0
0
98
89
82
61
3
1
88
59
24
1
0
CIFAR100

54 30.9
27 21.8
16.3
9
13.1
0
0
11.8
10.5
0
0
11.3
85 21.3
86 14.7
83 13.1
70 10.5
48 10.7
24 10.9
0
11.0
97 36.8
92 20.6
89 16.7
82 13.6
40 11.3
1
10.2
75 11.3
65 11.2
56 11.3
28 11.9
0
13.6

54
27
9
0
0
0
0
82
88
85
71
48
24
3
97
92
89
82
40
1
75
65
56
27
0

1e-02
2e-03
1e-03
5e-04
2e-04
1e-04
2e-02
1e-02
5e-03
1e-03
1e-04

CIFAR10

0

% Sparsity Test % Sparsity Test
WD by Act by γ Error by Act by γ Error
100 99.0
1e-02 100
23 47.6
27
1e-03
5e-04
8
4
41.9
39.4
0
0
2e-04
37.7
1e-04
0
0
100 98.0
1e-02 100
1e-03
90
81 55.3
59 43.3
81
5e-04
16 37.3
60
2e-04
1e-04
40
3
36.2

100 90.0
27 21.6
8
15.8
13.3
0
0
12.4
100 82.3
90 27.8
81 18.1
60 13.4
40 11.2

100
23
4
0
0
100
81
59
16
3

D
G
S

]
1
1
[

m
a
d
A

]
9
2
[

a
t
l
e
d
a
d
A

]
5
[
d
a
r
g
a
d
A

D
G
S

]
1
1
[

m
a
d
A

sider both L2 regularization and weight decay in Table 2 for

CIFAR100. The table shows sparsity by scale (|γ| < 10−3)
for each convolution layer. For both optimizer-regularizer
pairings we pick the conﬁgurations from Table 1 with the
lowest test errors that also produce sparse features. For
SGD, the extent of sparsity is higher for earlier layers, and
decreases for later layers. The trend holds for both L2 and
weight decay, from C1-C6. Note that the higher sparsity
seen for C7 might be due to its interaction with the fully
connected layers that follow. Sparsity for Adam shows a
similar decreasing trend from early to middle layers, and
increasing sparsity from middle to later layers.

Surprising Similarities to Explicit Feature Sparsiﬁca-
tion:
In the case of Adam, the trend of layerwise sparsity
exhibited is similar to that seen in explicit feature sparsiﬁ-
cation approaches (See Table 8 in [15] for Network Slim-
ming [14]). If we explicitly prune out features meeting the
|γ| < 10−3 sparsity criteria, we still see a relatively high
performance on the test set even with 90% of the convo-
lutional parameters pruned. Network Slimming [14] uses
explicit sparsity constraints on BatchNorm scales (γ). The
similarity in the trend of Adam’s emergent layer-wise spar-
sity to that of explicit scale sparsiﬁcation motivates us to
examine the distribution of the learned scales (γ) and biases
(β) of the BatchNorm layer in our network. We consider
layer C6, and in Figure 2 show the evolution of the distribu-
tion of the learned bias and scales as training progresses on
CIFAR100. We consider a low L2 regularization value of
1e-5 and a higher L2 regularization value of 1e-4 for Adam,
and also show the same for SGD with L2 regularization of
5e-4. The lower regularization values, which do not induce
sparsity, would help shed light at the underlying processes
without interference from the sparsiﬁcation process.

Feature Selectivity Hypothesis: From Figure 2 the dif-
ferences between the nature of features learned by Adam
and SGD become clearer. For zero mean, unit variance
BatchNorm outputs {ˆxi}N
i=1 of a particular convolutional
kernel, where N is the size of the training corpus, due to the
use of ReLU, a gradient is only seen for those datapoints for
which ˆxi > −β/γ. Both SGD and Adam (L2: 1e-5) learn
positive γs for layer C6, however βs are negative for Adam,
while for SGD some of the biases are positive. This im-
plies that all features learned for Adam (L2: 1e-5) in this
layer activate for ≤ half the activations from the training
corpus, while SGD has a signiﬁcant number of features ac-
tivate for more than half of the training corpus, i.e., Adam
learns more selective features in this layer. Features which
activate only for a small subset of the training corpus, and
consequently see gradient updates from the main objective
less frequently, continue to be acted upon by the regularizer.
If the regularization is strong enough (Adam with L2: 1e-4
in Fig. 2), or the gradient updates infrequent enough (fea-
ture too selective), the feature may be pruned away entirely.
The propensity of later layers to learn more selective fea-

522

Table 2. Layerwise % ﬁlters pruned from BasicNet trained on CIFAR100, based on the |γ| < 10−3 criteria. Also shown are pre-pruning
and post-pruning test error, and the % of convolutional parameters pruned. C1-C7 indicate Convolution layer 1-7, and the numbers in
parantheses indicate the total number of features per layer. Average of 3 runs. Color and highlighting indicates high and low sparsity for
best and near best test errors, as in Table 1. Refer to the supplementary document for the corresponding table for CIFAR10.

Pruned
Train Test Test C1 C2 C3 C4 C5 C6 C7 Total
Loss Loss Err (64) (128) (128) (256) (256) (512) (512) (1856) (4649664) Test Err.

% Sparsity by γ or % Filters Pruned

% Param
Pruned

m
a
d
A

D
G
S

L2: 1e-3 1.06 1.41 39.0 56
L2: 1e-4 0.10 1.98 36.6 41
WD: 2e-4 0.34 1.56 37.3 55
WD: 1e-4 0.08 1.76 36.2 38
L2: 1e-3 1.49 1.78 47.1 82
L2: 5e-4 0.89 1.69 42.1 64
WD: 1e-3 1.49 1.79 47.6 82
WD: 5e-4 0.89 1.69 41.9 66

47
20
20
4
41
3
43
2

43
9
3
0
33
3
31
1

68
33
4
0
29
3
28
4

72
34
2
0
33
2
33
2

91
67
16
0
6
0
6
0

85
55
26
5
18
2
17
1

76
47
16
3
23
4
23
4

95
74
27
4
34
4
34
4

39.3
36.6
37.3
36.2
47.1
42.1
47.6
41.9

Figure 2. Emergence of Feature Selectivity with Adam The evolution of the learned scales (γ, top row) and biases (β, bottom row) for
layer C6 of BasicNet for Adam and SGD as training progresses. Adam has distinctly negative biases, while SGD sees both positive and
negative biases. For positive scale values, as seen for both Adam and SGD, this translates to greater feature selectivity in the case of Adam,
which translates to a higher degree of sparsiﬁcation when stronger regularization is used. Note the similarity of the ﬁnal scale distribution
for Adam L2:1e-4 to the scale distributions shown in Figure 4 in [14]

tures with Adam would explain the higher degree of spar-
sity seen for later layers as compared to SGD. Understand-
ing the reasons for emergence of higher feature selectivity
in Adam than SGD, and verifying if other adaptive gradi-
ent descent ﬂavours also exhibit higher feature selectivity
remains open for future investigation.

Quantifying Feature Selectivity:

Similar to feature
sparsity by activation, we apply max pooling to a feature’s
absolute activations over the entire feature plane. For a par-
ticular feature, we consider these pooled activations over the
entire training corpus and normalize them by the max of the
pooled activations over the entire training corpus. We then
consider the percentage of the training corpus for which this
normalized pooled value exceeds a threshold of 10−3. We

refer to this percentage as the feature’s universality. A fea-
ture’s selectivity is then deﬁned as 100-universality. Unlike
the selectivity metrics employed in literature [19], ours is
class agnostic.
In Figure 3, we compare the ‘universal-
ity’ of features learned with Adam and SGD per layer on
CIFAR100, for both low and higher regularization values.
For the low regularization case, we see that in C6 and C7
both Adam and SGD learn selective features, with Adam
showing visibly ‘more selectivity for C6 (blue bars shifted
left). The disproportionately stronger regularization effect
of L2 coupled with Adam becomes clearer when moving
to a higher regularization value. The selectivity for SGD
in C6 remains mostly unaffected, while Adam sees a large
fraction (64%) of the features inactivated (0% universality).

523

Figure 3. Layer-wise Feature Selectivity Feature universality for CIFAR 100, with Adam and SGD. X-axis shows the universality and
Y-axis (×10) shows the fraction of features with that level of universality. For later layers, Adam tends to learn less universal features than
SGD, which get pruned by the regularizer. Please be mindful of the differences in Y-axis scales between plots. Refer to the supplementary
document for a similar analysis for CIFAR10

Similarly for C7, the selectivity pattern remains the same on
moving from lower regularization to higher regularization,
but Adam sees more severe feature inactivation.

Interaction of L2 Regularizer with Adam: Next, we
consider the role of the L2 regularizer vs. weight decay.
We study the behaviour of L2 regularization in the low gra-
dient regime for different optimizers. Figure 4 shows that
coupling of L2 regularization with ADAM update equation
yields a faster decay than weight decay, or L2 regulariza-
tion with SGD, even for smaller regularizer values. This is
an additional source of regularization disparity between pa-
rameters which see frequent updates and those which don’t
see frequent updates or see lower magnitude gradients. It
manifests for certain adaptive gradient descent approaches.
Task ‘Difﬁculty’ Dependence: As per the hypothesis
developed thus far, as the task becomes more difﬁcult, for
a given network capacity, we expect the fraction of features
pruned to decrease corresponding to a decrease in selectiv-
ity of the learned features [30]. This is indeed observed in
Table 1 for BasicNet for all gradient descent methods on
moving from CIFAR10 to CIFAR100. For Adam with L2
regularization, 70% sparsity on CIFAR10 decreases to 47%
on CIFAR 100, and completely vanishes on ImageNet (See
Table 5). A similar trend is evident for VGG-16 in Tables 7
and 8. In Figure 5 note the distinct shift towards less selec-
tive features in BasicNet with increasing task difﬁculty.

Since the task difﬁculty cannot be cleanly decoupled
from the number of classes, we devise a synthetic exper-
iment based on grayscale renderings of 30 object classes
from ObjectNet3D [26]. We construct 2 identical sets of
≈ 50k 64×64 pixel renderings, one with a clean back-
ground (BG) and the other with a cluttered BG. We train
BasicNet with a mini-batch size of 40, and see that as ex-
pected there is a much higher sparsity (70%) with the clean
BG set than with the more difﬁcult cluttered set (57%). See

the supplemental document for representative images and a
list of the object classes selected.

4. Related Work

Effect of L2 regularization vs. Weight Decay for
Adam:
Prior work [16] has indicated that Adam with
L2 regularization leads to parameters with frequent and/or
large magnitude gradients from the main objective being
regularized less than the ones which see infrequent and/or
small magnitude gradients. Though weight decay is pro-
posed as a supposed ﬁx, we show that there are rather two
different aspects to consider. The ﬁrst is the disparity in
effective regularization due to the frequency of updates. Pa-
rameters which update less frequently would see more reg-
ularization steps per actual update than those which are up-
dated more frequently. This disparity would persist even
with weight decay due to Adam’s propensity for learning
more selective features, as detailed in the preceding section.
The second aspect is the additional disparity in regulariza-
tion for features which see low/infrequent gradient, due to
the coupling of L2 regularization with Adam.

Attributes of Generalizable Neural Network Fea-
tures: Dinh et al. [4] show that the geometry of minima is
not invariant to reparameterization, and thus the ﬂatness of
the minima may not be indicative of generalization perfor-
mance [9], or may require other metrics which are invariant
to reparameterization. Morcos et al. [19] suggest based on
extensive experimental evaluation that good generalization
ability is linked to reduced selectivity of learned features.
They further suggest that individual selective units do not
play a strong role in the overall performance on the task
as compared to the less selective ones. They connect the
ablation of selective features to the heuristics employed in
neural network feature pruning literature which prune fea-

524

Figure 4. The action of regularization on a scalar value, for a range of regularization values in the presence of simulated low gradients
drawn from a mean=0, std=10−5 normal distribution. The gradients for the ﬁrst 100 iterations are drawn from a mean=0, std=10−3 normal
distribution to emulate a transition into low gradient regime rather than directly starting in a low gradient regime. The learning rate for
SGD(momentum=0.9) is 0.1, and the learning rate for ADAM is 1e-3. We show similar plots for other adaptive gradient descent approaches
in the supplementary document.

tures whose removal does not impact the overall accuracy
signiﬁcantly [18, 13]. The ﬁndings of Zhou et al. [30] con-
cur regarding the link between emergence of feature selec-
tivity and poor generalization performance. They further
show that ablation of class speciﬁc features does not inﬂu-
ence the overall accuracy signiﬁcantly, however the speciﬁc
class may suffer signiﬁcantly. We show that the emergence
of selective features in Adam, and the increased propensity
for pruning the said selective features when using L2 regu-
larization presents a direct tradeoff between generalization
performance and network capacity which practitioners us-
ing Adam must be aware of.

Observations on Adaptive Gradient Descent: Several
works have noted the poorer generalization performance of
adaptive gradient descent approaches over SGD. Keskar et
al. [10] propose to leverage the faster initial convergence of
ADAM and the better generalization performance of SGD,
by switching from ADAM to SGD while training. Reddi
et al.
[22] point out that exponential moving average of
past squared gradients, which is used for all adaptive gradi-
ent approaches, is problematic for convergence, particularly
with features which see infrequent updates. This short term
memory is likely the cause of accelerated pruning of selec-
tive features seen for Adam in Figure 4(and other adaptive
gradient approaches), and the extent of sparsity observed
would be expected to go down with AMSGrad which tracks
the long term history of squared gradients.

Feature Pruning/Sparsiﬁcation: Among the various
explicit ﬁlter level sparsiﬁcation heuristics and approaches
[13, 24, 7, 25, 18, 20, 14, 28], some [28, 14] make use of
the learned scale parameter γ in Batch Norm for enforcing
sparsity on the ﬁlters. Ye et al. [28] argue that BatchNorm
makes feature importance less susceptible to scaling repa-
rameterization, and the learned scale parameters (γ) can be
used as indicators of feature importance. We ﬁnd that Adam
with L2 regularization, owing to its implicit pruning of fea-
tures based on feature selectivity, makes it an attractive al-
ternative to explicit sparsiﬁcation/pruning approaches. The
link between ablation of selective features and explicit fea-

ture pruning is also established in prior work [19, 30].

5. Further Experiments

We conduct additional experiments on various datasets
and network architectures to show that the intuition devel-
oped in the preceding sections generalizes. Further, we pro-
vide additional support by analysing the effect of various
hyperparameters on the extent of sparsity. We also com-
pare the emergent sparsity for different networks on various
datasets to that of explicit sparsiﬁcation approaches.

Datasets:

In addition to CIFAR10 and CIFAR100, we
also consider TinyImageNet [2] which is a 200 class sub-
set of ImageNet [3] with images resized to 64×64 pixels.
The same training augmentation scheme is used for Tiny-
ImageNet as for CIFAR10/100. We also conduct exten-
sive experiments on ImageNet. The images are resized to
256×256 pixels. and random crops of size 224×224 pixels
used while training, combined with random horizontal ﬂips.
For testing, no augmentation is used, and 1-crop evaluation
protocol is followed.

Network Architectures:

The convolution structure
for BasicNet stays the same across tasks, while the fully-
connected (fc) structure changes across task. We will use
‘[n]’ to indicate an fc layer with n nodes. Batch Norm and
ReLU are used in between fc layers. For CIFAR10/100 we
use Global Average Pooling (GAP) after the last convolu-
tion layer and the fc structure is [256][10]/[256][100], as
shown in Figure 1. For TinyImagenet we again use GAP
followed by [512][256][200]. On ImageNet we use average
pooling with a kernel size of 5 and a stride of 4, followed
by [4096][2048][1000]. For VGG-11/16, on CIFAR10/100
we use [512][10]/[512][100]. For TinyImageNet we use
[512][256][200], and for ImageNet we use the structure
in [23]. For VGG-19, on CIFAR10/100, we use an fc struc-
ture identical to [14]. Unless explicitly stated, we will be
using Adam with L2 regularization of 1e-4, and a batch size
of 40. When comparing different batch sizes, we ensure the
same number of training iterations.

525

maining layers. The fc-head remains unchanged. We see a
consistent decrease in the extent of sparsity with decreasing
network width in Table 6. Additionally note the decrease in
sparsity in moving from CIFAR10 to CIFAR100.

Table 3. BasicNet sparsity variation on CIFAR10/100 trained with
Adam and L2 regularization.

CIFAR 10

CIFAR 100

Batch Train Test Test %Spar. Train Test Test %Spar.
Size Loss Loss Err
0.43 0.45 15.2
20
0.29 0.41 13.1
40
0.18 0.40 12.2
80
20
0.17 0.36 11.1
0.06 0.43 10.5
40
80
0.02 0.50 10.1
160 0.01 0.55 10.6

Loss Loss Err
1.62 1.63 45.3
1.06 1.41 39.0
0.53 1.48 37.1
0.69 1.39 35.2
0.10 1.98 36.6
0.02 2.21 41.1
0.01 2.32 44.3

by γ
82
83
80
70
70
66
61

by γ
79
76
67
57
46
35
29

3
-
e
1

:
2
L

4
-
e
1

:
2
L

Table 4. Convolutional ﬁlter sparsity for BasicNet trained on Tiny-
ImageNet, with different mini-batch sizes.

Batch Train Val Top 1 Top 5 % Spar.
Size Loss Loss Val Err. Val Err. by γ

0.02 2.63
SGD 40
1.05 2.13
20
40
0.16 2.96
120 0.01 2.48

Adam

45.0
47.7
48.4
48.8

22.7
22.8
24.7
27.4

0
63
48
26

Table 5. Convolutional ﬁlter sparsity of BasicNet on ImageNet.

Batch Train Val Top 1 Top 5 % Sparsity
Size Loss Loss Val Err. Val Err.
2.05 1.58
64
256 1.63 1.35

by γ
0.2
0.0

38.0
32.9

15.9
12.5

Table 6. Effect of varying the number of features in BasicNet.

CIFAR 10

CIFAR 100

Train Test Test %Spar. Train Test Test %Spar.
Net
Loss Loss Err
Cfg.
64-1x
0.06 0.43 10.5
64-0.5x 0.10 0.41 11.0
32-0.25x 0.22 0.44 13.4

Loss Loss Err
0.10 1.98 36.6
0.11 2.19 39.8
0.51 2.05 43.4

by γ
70
51
23

by γ
46
10
0

5.2. Comparison With Explicit Feature Sparsiﬁca 

tion / Pruning Approaches

For VGG-16, we compare the network trained on
CIFAR-10 with Adam using different mini-batch sizes
against the handcrafted approach of Li et al. [13]. Similar
to tuning the explicit sparsiﬁcation hyperparameter in [14],
the mini-batch size can be varied to ﬁnd the sparsest repre-
sentation with an acceptable level of test performance. We
see from Table 7 that when trained with a batch size of 160,
83% of the features can be pruned away and leads to a better
performance that the 37% of the features pruned for [13].
For VGG-11 on ImageNet (Table 9), by simply varying the
mini-batch size from 90 to 60, the number of convolutional

526

Figure 5. Feature Selectivity For Different Mini-Batch Sizes for
Different Datasets Feature universality (1 - selectivity) plotted for
layers C4-C7 of BasicNet for CIFAR10, CIFAR100 and TinyIm-
agenet. Batch sizes of 40/160 considered for CIFAR, and 40/120
for TinyImagenet.

5.1. Analysis of Hyperparameters

Having established in Section 3 (Figures 3 and 2) that
with Adam, the emergence of sparsity is correlated with
feature selectivity, we investigate the impact of various hy-
perparameters on the emergent sparsity.

Effect of Mini-Batch Size:

Figure 5 shows the ex-
tent of feature selectivity for C4-C7 of BasicNet on CIFAR
and TinyImageNet for different mini-batch sizes. For each
dataset, note the apparant increase in selective features with
increasing batch size. However, a larger mini-batch size
is not promoting feature selectivity, and rather preventing
the selective features from being pruned away by provid-
ing more frequent updates. This makes the mini-batch size
a key knob to control tradeoffs between network capacity
(how many features get pruned, which affects the speed and
performance) and generalization ability (how many selec-
tive features are kept, which can be used to control over-
ﬁtting). We see across datasets and networks that increas-
ing the mini-batch size leads to a decrease in sparsity (Ta-
bles 3, 4, 5, 7, 8, 9, 10).

Network Capacity: Task ‘difﬁculty’ is relative to the
network’s learning capacity.
In the preceding section we
directly manipulated the task difﬁculty, and here we con-
sider variations of BasicNet in Table 6 to study the comple-
mentary effect of network capacity. We indicate the archi-
tecture presented in Figure 1 as ‘64-1x’, and consider two
variants: ‘64-0.5x’ which has 64 features in the ﬁrst con-
volution layer, and half the features of BasicNet in the re-
maining convolution layers, and ‘32-0.25x’ with 32 features
in the ﬁrst channel and a quarter of the features in the re-

Table 7. Layerwise % Sparsity by γ for VGG-16 on CIFAR10 and
100. Also shown is the handcrafted sparse structure of [13]

CIFAR 10

CIFAR 100

Conv #Conv
Layer Feat. B: 40 B: 80 B: 160 al.[13] B: 40 B: 80 B: 160

Adam, L2:1e-4

Adam, L2:1e-4

Li et

64
64
128
128
256
256
256
512
512
512
512
512
512

64
C1
18
C2
C3
50
12
C4
C5
46
C6
71
82
C7
C8
95
97
C9
97
C10
C11
98
99
C12
99
C13
%Feat. Pruned 86

0
0
47
5
40
66
80
96
97
97
98
99
99
84

0
0
51
6
36
63
79
96
97
96
98
98
99
83

50
0
0
0
0
0
0
50
50
50
50
50
50
37

49
4
29
0
10
26
44
86
95
96
98
98
98
76

1
0
40
0
5
5
12
74
90
93
97
98
98
69

58
8
54
3
27
7
0
55
94
93
96
99
96
69

Test Err

7.2

7.0

6.5

6.6

29.2 28.1

27.8

Table 8. Sparsity by γ on VGG-16, trained on TinyImageNet, and
on ImageNet. Also shown are the pre- and post-pruning top-1/top-
5 single crop validation errors. Pruning using |γ| < 10−3 criteria.

# Conv

TinyImageNet Feat. Pruned top1
L2: 1e-4, B: 20 3016 (71%) 45.1
L2: 1e-4, B: 40 2571 (61%) 46.7

Pre-pruning Post-pruning
top5
21.4
24.4

top5
21.4
24.4

top1
45.1
46.7

ImageNet

L2: 1e-4, B: 40

292

29.93 10.41 29.91 10.41

Table 9. Effect of different mini-batch sizes on sparsity (by γ) in
VGG-11, trained on ImageNet. Same network structure employed
as [14]. * indicates ﬁnetuning after pruning

Adam, L2: 1e-4, B: 90
Adam, L2: 1e-4, B: 60
Liu et al. [14] from [15]

71
140
85

# Conv

Feat. Pruned top1

top1

top5

Pre-pruning Post-pruning
top5
30.50 10.65 30.47 10.64
31.76 11.53 31.73 11.51
29.16

31.38*

-

Table 10. Sparsity by γ on VGG-19, trained on CIFAR10/100.
Also shown are the post-pruning test error. Compared with explicit
sparsiﬁcation approach of Liu et al. [14]

CIFAR 10

CIFAR 100

Adam, L2:1e-4 Liu et Adam, L2:1e-4 Liu et
B: 64 B: 512 al.[14] B: 64 B: 512 al.[14]

%Feat. Pruned 85

Test Err

7.1

81

6.9

70

6.3

75

62

50

29.9

28.8

26.7

features pruned goes from 71 to 140. This is in the same
range as the number of features pruned by the explicit spar-
siﬁcation approach of [13], and gives a comparable top-1
and top-5 validation error. For VGG-19 on CIFAR10 and
CIFAR100 (Table 10), we see again that varying the mini-
batch size controls the extent of sparsity. For the mini-batch
sizes we considered, the extent of sparsity is much higher
than that of [14], with consequently slightly worse perfor-
mance. The mini-batch size or other hyper-parameters can
be tweaked to further tradeoff sparsity for accuracy, and
reach a comparable sparsity-accuracy point as [14].

6. Discussion and Future Work

Our ﬁndings relate to the anecdotally known and poorly
understood ‘dying ReLU’ phenomenon [1], wherein some
features in ReLU networks get cut off while training, lead-
ing to a reduced effective learning capacity of the network.
Ameliorating it with Leaky ReLU [17] is ineffective be-
cause it does not address the root cause. BasicNet with
Leaky ReLU (negative slope of 0.01) on CIFAR-100 only
marginally reduces the extent of sparsity in the case of
Adam with L2: 10−4 (41% feature sparsity vs. 47% with
ReLU). Reducing the learning rate of BN parameter γ is
much more effective (33% sparsity). See Tables 2, 3 in the
supplemental document.

Our work opens several avenues of future investigation.
Understanding why features learned with Adam (and per-
haps other adaptive methods) are more selective than with
(m)SGD can further shed light on the practical differences
between adaptive methods and SGD. Also, our insights will
lead practitioners to be more aware of the implicit tradeoffs
between network capacity and generalization being made
below the surface, while changing hyperparameters such
as mini-batch size, which are seemingly unrelated to net-
work capacity. Also, we show that Adam with L2 regular-
ization works out of the box for speeding up neural net-
works and is a strong baseline for future efforts towards
ﬁlter-sparsiﬁcation-for-speedup approaches.

7. Conclusion

We show through extensive experiments that the root
cause for the emergence of ﬁlter level sparsity in CNNs is
likely the disproportionate regularization (L2 or weight de-
cay) of the parameters in comparison to the gradient from
the primary objective. We identify how various factors in-
ﬂuence the extent of sparsity by interacting in subtle ways
with the regularization process. We show that adaptive gra-
dient updates play a crucial role in the emergent sparsity (in
contrast to SGD), and Adam not only shows a higher degree
of sparsity but the extent of sparsity also has a strong depen-
dence on the mini-batch size. We show that this is caused
by the propensity of Adam to learn more selective features,
and the added acceleration of L2 regularization interacting
with the adaptive updates in low gradient regime.

Due to its targeting of selective features, the emergent
sparsity can be used to trade off between network capacity,
performance and generalization ability as per the task set-
ting, and common hyperparameters such as mini-batch size
allow direct control over it. We leverage this ﬁnegrained
control and show that Adam with L2 regularization can be
an attractive alternative to explicit network slimming ap-
proaches for speeding up test time performance of CNNs,
without any tooling changes to the traditional neural net-
work training pipeline supported by popular frameworks.

527

References

[1] CS231n

convolutional

neural

visual
http://cs231n.github.io/

networks

for

recognition.
neural-networks-1/.

[2] Tiny imagenet visual recognition challenge. https://

tiny-imagenet.herokuapp.com/.

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09, 2009.

[4] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua
In

Bengio. Sharp minima can generalize for deep nets.
ICML, 2017.

[5] John Duchi, Elad Hazan, and Yoram Singer. Adaptive sub-
gradient methods for online learning and stochastic opti-
mization. volume 12, pages 2121–2159, 2011.

[6] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
culty of training deep feedforward neural networks. In Pro-
ceedings of the thirteenth international conference on artiﬁ-
cial intelligence and statistics, pages 249–256, 2010.

[7] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung
Tang. Network trimming: A data-driven neuron pruning ap-
proach towards efﬁcient deep architectures. arXiv preprint
arXiv:1607.03250, 2016.

[8] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In Proc. ICML, volume 32, 2015.

[9] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,
Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-
batch training for deep learning: Generalization gap and
sharp minima. In ICLR, 2017.

[10] Nitish Shirish Keskar and Richard Socher. Improving gener-
alization performance by switching from adam to sgd. arXiv
preprint arXiv:1712.07628, 2017.

[11] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple

layers of features from tiny images. 2009.

[13] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
In

Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets.
ICLR, 2017.

[14] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. In Com-
puter Vision (ICCV), 2017 IEEE International Conference
on, pages 2755–2763. IEEE, 2017.

[15] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and
Trevor Darrell. Rethinking the value of network pruning. In
ICLR, 2019.

[16] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-
ularization in adam. arXiv preprint arXiv:1711.05101, 2017.
[17] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Recti-
ﬁer nonlinearities improve neural network acoustic models.
In Proc. ICML, volume 30, page 3, 2013.

[18] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for
resource efﬁcient inference. 2017.

[19] Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and
Matthew Botvinick. On the importance of single directions
for generalization. 2018.

[20] Michael C Mozer and Paul Smolensky. Skeletonization: A
technique for trimming the fat from a network via relevance
assessment. In Advances in neural information processing
systems, pages 107–115, 1989.

[21] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS-W, 2017.

[22] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the

convergence of adam and beyond. In ICLR, 2018.

[23] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[24] Suraj Srinivas and R Venkatesh Babu. Data-free parameter

pruning for deep neural networks. In BMVC, 2016.

[25] Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Fer-
enc Husz´ar. Faster gaze prediction with dense networks and
ﬁsher pruning. In ICLR, 2017.

[26] Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher
Choy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, and Sil-
vio Savarese. Objectnet3d: A large scale database for 3d ob-
ject recognition. In European Conference Computer Vision
(ECCV). 2016.

[27] Atsushi Yaguchi, Taiji Suzuki, Wataru Asano, Shuhei Nitta,
Yukinobu Sakata, and Akiyuki Tanizawa. Adam induces im-
plicit weight sparsity in rectiﬁer neural networks.
In 2018
17th IEEE International Conference on Machine Learning
and Applications (ICMLA), pages 318–325. IEEE, 2018.

[28] Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethink-
ing the smaller-norm-less-informative assumption in channel
pruning of convolution layers. In ICLR, 2018.

[29] Matthew D Zeiler. Adadelta: an adaptive learning rate

method. arXiv preprint arXiv:1212.5701, 2012.

[30] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba.
Revisiting the importance of individual units in cnns via ab-
lation. arXiv preprint arXiv:1806.02891, 2018.

528

