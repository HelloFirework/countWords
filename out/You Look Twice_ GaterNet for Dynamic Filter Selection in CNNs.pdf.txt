You Look Twice: GaterNet for Dynamic Filter Selection in CNNs

Zhourong Chen1

,

2 ∗, Yang Li1,

Samy Bengio1,

Si Si1

1Google Research, Mountain View

2The Hong Kong University of Science and Technology, Hong Kong

zchenbb@cse.ust.hk, {liyang, bengio, sisidaisy}@google.com

Abstract

The concept of conditional computation for deep nets
has been proposed previously to improve model perfor-
mance by selectively using only parts of the model condi-
tioned on the sample it is processing. In this paper, we in-
vestigate input-dependent dynamic ﬁlter selection in deep
convolutional neural networks (CNNs). The problem is in-
teresting because the idea of forcing different parts of the
model to learn from different types of samples may help
us acquire better ﬁlters in CNNs, improve the model gen-
eralization performance and potentially increase the inter-
pretability of model behavior. We propose a novel yet sim-
ple framework called GaterNet, which involves a backbone
and a gater network. The backbone network is a regular
CNN that performs the major computation needed for mak-
ing a prediction, while a global gater network is introduced
to generate binary gates for selectively activating ﬁlters in
the backbone network based on each input. Extensive ex-
periments on CIFAR and ImageNet datasets show that our
models consistently outperform the original models with a
large margin. On CIFAR-10, our model also improves upon
state-of-the-art results.

1. Introduction

It is widely recognized in neural science that distinct
parts of the brain are highly specialized for different types
of tasks [20]. It results in not only the high efﬁciency in
handling a response but also the surprising effectiveness of
the brain in learning new events. In machine learning, con-
ditional computation [3] has been proposed to have a sim-
ilar mechanism in deep learning models. For each speciﬁc
sample, the basic idea of conditional computation is to only
involve a small portion of the model in prediction. It also
means that only a small fraction of parameters needs to be
updated at each back-propagation step, which is desirable
for training a large model.

∗Student of The Hong Kong University of Science and Technology.

Work is done when interning at Google.

Bottleneck

Features

Maps

Filters

Maps

Filters

Prediction

Features

Maps

Filters

Maps

Filters

Gating

Gating

Same RGB Image

Gater Network

Backbone Network

Figure 1. Model Architecture (better viewed in color). The gater
extracts features and generates sparse binary gates for selecting
ﬁlters in the backbone network in an input-dependent manner.

One line of work that is closely related to conditional
computation is Mixture of Experts (MoE) [15], where mul-
tiple sub-networks are combined via an ensemble using
weights determined by a gating module. Particularly, sev-
eral recent works [25, 28] propose to ensemble a small sub-
set of dynamically selected experts in the model for each
input. By doing so, these models are able to reduce com-
putation cost while achieving similar or even better results
than baseline models. Note that both the expert architec-
tures and the number of experts in these works are pre-
deﬁned and ﬁxed. Another line of works that resemble con-
ditional computation focus on dynamic network conﬁgura-
tion [29, 1, 7, 2, 4]. There are no explicitly deﬁned experts
in these methods. Rather, they dynamically select the units,
layers, or other components in the main model for each in-
put. In these works, one small sub-module is usually added
to each position to be conﬁgured in the model. That is, each
sub-module added is making decisions locally speciﬁc to
the components it is conﬁguring.

In this paper, we propose a novel framework called
GaterNet for input-dependent dynamic ﬁlter selection in
convolutional neural networks (CNNs), as shown in Fig-
ure 1. We introduce a dedicated sub-network called gater,

9172

which extracts features from input and generates the binary
gates needed for controlling ﬁlters all at once based on the
features. The gating vector is then used to select the ﬁlters
in the backbone1 network (the main model in our frame-
work), and only the selected ﬁlters in the backbone network
participate in the prediction and learning. We used a dis-
cretization technique called Improved SemHash [17] to en-
able differentiable training of input-dependent binary gates
such that the backbone and the gater network can be trained
jointly via back-propagation.

Compared to previous works on dynamic network con-
ﬁguration, we use a dedicated sub-network (the gater) for
making global decisions on which ﬁlters in the backbone
network should be used. The decision on each gate (for
each ﬁlter) is made based on a shared global view of the
current input. We argue that such a global gating unit can
make more holistic decisions about how to optimally use the
ﬁlters in the network than local conﬁguration employed by
previous work. Note that in [28], a module of the network
is used to generate all the gates, which at a glance is similar
to our gater. However, there are two important differences.
Firstly, [28] is not based on an end-to-end approach. It re-
quires a pre-processing step to cluster classes of samples
and assign each cluster to a sub-branch of the network to
handle. The assignments provides explicit supervision for
training the gating module. Secondly, as mentioned above,
the sub-branch architectures and the number of branches are
both manually deﬁned and ﬁxed throughout training in [28].
In contrast, in our framework, each sample uses a dynam-
ically determined sub-branch depending on the ﬁlters be-
ing selected. As a result, our method potentially allows a
combinatorial number of choices of sub-branches or experts
given the number of ﬁlters to be controlled, which is more
amenable for capturing complex distribution manifested in
the data.

Our experiments on CIFAR [21] and ImageNet [24] clas-
siﬁcation datasets show that the gater in GaterNet is able to
learn effective gating strategies for selecting proper ﬁlters.
It consistently improves the original model with a signiﬁ-
cant margin. On CIFAR-10, our method gives better classi-
ﬁcation results than state-of-the-art models with only 1.2%
additional parameters. Our contributions are summarized as
follows:

• We propose a new framework for dynamic ﬁlter selec-
tion in CNNs. The core of the idea is to introduce a
dedicated gater network to take a glimpse of the input,
and then generate input-dependent binary gates to se-
lect ﬁlters in the backbone network for processing the
input. By using Improved SemHash, the gater network
can be trained jointly with the backbone in an end-to-
end fashion through back-propagation.

• We conduct extensive experiments on GaterNet, which

1The term backbone is also used in object detection and TSE-Net [5].

show that it consistently improves the generalization
performance of deep CNNs without signiﬁcantly in-
creasing the model complexity. In particular, our mod-
els achieve better results than several state-of-the-art
models on the CIFAR-10 dataset by only introducing a
small fraction of parameters.

• We perform an in-depth analysis about the model
behavior for GaterNet, which reveals that GaterNet
learns effective gating strategies by being relatively de-
terministic on the choice of ﬁlters to use in shallow lay-
ers but using more input-dependent ﬁlters in the deep
layers.

2. Related Work

The concept of conditional computation is ﬁrst discussed
by Bengio in [3]. Early works on conditional computation
focus on how to select model components on the ﬂy. Bengio
et al. have studied four approaches for learning stochastic
neurons in fully-connected neural networks for conditional
selection in [4]. On the other hand, Davis and Arel have
used low-rank approximations to predict the sparse activa-
tions of neurons at each layer [6]. Bengio et al. have also
tested reinforcement learning to optimize conditional com-
putation policies [2] .

More recently, Shazeer et al. have investigated the com-
bination of conditional computation with Mixture of Ex-
perts on language modeling and machine translation tasks
[25]. At each time step in the sequence model, they dy-
namically select a small subset of experts to process the in-
put. Their models signiﬁcantly outperformed state-of-the-
art models with a low computation cost. In the same vein,
Mullapudi et al. have proposed HydraNets that uses multi-
ple branches of networks for extracting features [28]. In this
work, a gating module is introduced to generate decisions
on selecting branches for each speciﬁc input. This method
requires a pre-processing step of clustering the ground-truth
classes to force each branch to learn features for a speciﬁc
cluster of classes as discussed in the introduction.

Dynamic network conﬁguration is another type of con-
ditional computation that has been studied previously. In
this line of works, no parallel experts are explicitly deﬁned.
Instead, they dynamically conﬁgure a single network by
selectively activating model components such as units and
layers for each input. Adaptive Dropout is proposed by Ba
and Frey to dynamically learn a dropout rate for each unit
and each input [1]. Denoyer and Ludovic have proposed a
tree structure neural network called Deep Sequential Neu-
ral Network [7]. A path from the root to a leaf node in
the tree represents a computation sequence for the input,
which is also dynamically determined for each input. Re-
cently, Veit and Belongie [29] have proposed to skip layers
in ResNet [10] in an input-dependent manner. The resulting
model is performing better and also more robust to adver-

9173

sarial attack than the original ResNet, which also leads to
reduced computation cost.

Previous works have also investigated methods that dy-
namically re-scale or calibrate the different components in
a model. The fundamental difference between these meth-
ods and dynamic network conﬁguration is that they gen-
erate a real-valued vector for each input, instead of a bi-
nary gate vector for selecting network components. SE-Net
proposed by Hu et al. [12] re-scales the channels in feature
maps on the ﬂy and achieves state-of-the-art results on Im-
ageNet classiﬁcation dataset. Stollenga et al. [26] have also
proposed to go through the main model for multiple passes.
The features resulting from each pass (except the last) are
used to generate a real-valued vector for re-scaling the chan-
nels in the next pass. In contrast to these works, our gater
network generates binary decisions to dynamically turn on
or off ﬁlters depending on each input.

3. GaterNet

Our model contains two convolutional neural sub-
networks, namely the backbone network and the gater net-
work as illustrated in Figure 1. Given an input, the gater
network decides the set of ﬁlters in the backbone network
for use while the backbone network does the actual predic-
tion. The two sub-networks are trained in an end-to-end
manner via back-propagation.

3.1. Backbone

The backbone network is the main module of our
model, which extracts features from input and makes the
ﬁnal prediction. Any existing CNN architectures such as
ResNet [10], Inception [27] and DenseNet [13] can be read-
ily used as the backbone network in our GaterNet.

Let us ﬁrst consider a standalone backbone CNN without
the gater network. Given an input image x, the output of
the l-th convolutional layer is a 3-D feature map Ol(x). In
a conventional CNN, Ol(x) is computed as:

Ol

i(x) = φ(F l

i ∗ I l(x)),

(1)

i(x) is the i-th channel of feature map Ol(x), F l
where Ol
i
is the i-th 3-D ﬁlter, I l(x) is the 3-D input feature map to
the l-th layer, φ denotes the element-wise nonlinear activa-
tion function, and ∗ denotes convolution. In general cases
without the gater network, all the ﬁlters F l
i in the current
layer are applied to I l(x), resulting in a dense feature map
Ol(x). The loss for training such a CNN for classiﬁcation
is L = − log P (y|x, θ) for a single input image, where y is
the ground-truth label and θ denotes the model parameters.

3.2. Gater

In contrast to the backbone, the gater network is an assis-
tant of the backbone and does not learn any features directly

used in the prediction. Instead, the gater network processes
the input to generate an input-dependent gating mask—a bi-
nary vector. The vector is then used to dynamically select
a particular subset of ﬁlters in the backbone network for the
current input. Speciﬁcally, the gater network learns a func-
tion as below:

G(x) = D(E(x)).

(2)

Here, E is an image feature extractor deﬁned as E : x →
f, x ∈ Rh′×w′×c′
, f ∈ Rh, with h′, w′, c′ being the height,
width and channel number of an input image respectively,
and h being the number of features extracted. D is a func-
tion deﬁned as D : f → g, f ∈ Rh, g ∈ {0, 1}c, where c
is the total number of ﬁlters in the backbone network. More
details about function E and D will be discussed in Sec-
tion 3.2.1 and Section 3.2.2 respectively.

From the above deﬁnition we can see that, the gater net-
work learns a function which maps input x to a binary gat-
ing vector g. With the help of g, we reformulate the compu-
tation of feature map Ol(x) in Equation (1) as below:

Ol

i(x) =(0,

φ(F l

i ∗ I l(x)),

if gl
if gl

i = 0
i = 1

(3)

Here gl
i is the entry in g corresponding to the i-th ﬁlter at
layer l, and 0 is a 2-D feature map with all its elements
being 0. That is, the i-th ﬁlter will be applied to I l(x) to
extract features only when gl
i = 0, the i-th ﬁlter
is skipped and 0 is used as the output instead. When gl
is a sparse binary vector, a large subset of ﬁlters will be
skipped, resulting in a sparse feature map. In this paper, we
implement the computation in Equation (3) by masking the
output channels using the binary gates:

i = 1. If gl

Ol

i(x) = φ(F l

i ∗ I l(x)) · gl

i

(4)

In the following subsections, we will introduce how we
design the functions E and D in Equation (2) and how we
enable end-to-end training through the binary gates.

3.2.1 Feature Extractor
Essentially, the function E(x) in Equation (2) is a feature
extractor which takes an image x as input and outputs a fea-
ture vector f . Similar to the backbone network, any existing
CNN architectures can be used here to learn the function
E(x). There are two main differences compared with the
backbone network: (1) The output layer of the CNN archi-
tecture is removed such that it outputs features for use in
the next step. (2) A gater CNN does not necessarily need to
be as complicated as the one for the backbone. One reason
is that the gater CNN is supposed to obtain a brief view of
the input. Having an over-complicated gater network may
encounter various difﬁculties in computation cost and opti-
mization. Another reason is to avoid the gater network ac-
cidentally taking over the task that is intended for the back-
bone network.

9174

3.2.2 Features to Binary Gates
Fully-Connected Layers with Bottleneck As deﬁned in
Equation (2), the function D(f ) needs to map the vector f
of size h to a binary vector g of size c. We ﬁrst consider us-
ing fully-connected layers to map f to a real-valued vector
g′ of size c. If we use one single layer to project the vector,
the projection matrix would be of size h × c. This can be
very large when h is thousands and c is tens of thousands.
To reduce the number of parameters in this projection, we
use two fully-connected layers to fulﬁll the projection. The
ﬁrst layer projects f to a bottleneck of size b, followed by
the second layer mapping the bottleneck to g′. In this way,
the total number of parameters becomes (h+c)×b. This can
be signiﬁcantly smaller than h × c when b is much smaller
than h and c. We ignore bias parameters here for simplicity.

In summary, the real-valued vector g′ is computed as:

f ′ = ReLU (BatchN orm(F C1(f )))
g′ = F C2(f ′)

where F C1 and F C2 denotes the two linear projections,
ReLU denotes the non-linear activation function in [23],
and BatchNorm means batch normalization [14].

Improved SemHash So far, one important question still
remains unanswered: how to generate binary gates g from
g′ such that we can back-propagate the error through the
discrete gates to the gater? In this paper, we adopt a method
called Improved SemHash [17, 18].

During training, we ﬁrst draw noise from a c-dimentional
Gaussian distribution with mean 0 and standard deviation
1. The noise ǫ is added to g′ to get a noisy version of the
vector: g′
ǫ = g′ + ǫ. Two vectors are then computed from
g′
ǫ:

gα = σ′(g′

ǫ) and gβ = 1(g′

ǫ > 0)

where σ′ is the saturating sigmoid function [19, 16]:

σ′(x) = max(0, min(1, 1.2σ(x) − 0.1))

ǫ. On the other hand, the gradient of gα w.r.t g′

with σ being the sigmoid function. Here, gα is a real-
valued gate vector with all the entries falling in the interval
[0.0, 1.0], while gβ is a binary vector. We can see that, gβ
has the desirable binary property that we want to use in our
model, but the gradient of gβ w.r.t g′
ǫ is zero for most val-
ues of g′
ǫ is
well deﬁned, but gα is not a binary vector. In forward prop-
agation, we randomly use g = gα for half of the training
samples and use g = gβ for the rest of the samples. When
gβ is used, we follow the solution in [17, 18] and deﬁne the
gradient of gβ w.r.t g′
ǫ to be the same as the gradient of gα
w.r.t g′

ǫ in the backward propagation.

The above procedure is designed for the sake of easy
training. Evaluation and inference are different to the train-
ing phase in two aspects. Firstly, we skip the step of draw-
ing noise and always set ǫ = 0. Secondly, we always use

the discrete gates g = gβ in forward propagation. That is,
the gate vector is always binarized in evaluation and infer-
ence phase. The interested readers are referred to [17, 18]
for more intuition behind Improved SemHash.

We use binary gates other than attention [30] or other
real-valued gates for two reasons. Firstly, binary gates
can completely deactivate some ﬁlters for each input, and
hence those ﬁlters will not be inﬂuenced by the irrelevant
inputs. This may lead to training better ﬁlters than real-
valued gates. Secondly, discrete gates open the opportunity
for model compression in the future.

Sparse Gates To encourage the gates g to be sparse, we
introduce a L1 regularization term into the training loss:

L = − log P (y|x, θ) + λ

kG(x)k1

c

where λ is the weight for the regularization term and c is the
size of g. Note that the backbone network receives no gradi-
ents from the second term, while the gater network receives
gradients from both the two terms.

3.3. Pre training

While our model architecture is straightforward, there
are several empirical challenges to train it well. First, it is
difﬁcult to learn these gates, which are discrete latent rep-
resentations. Although Improved SemHash has been shown
to work well in several previous works, it is unclear whether
the approximation of gradients mentioned above is a good
solution in our model. Second, the introduction of gater
network into the model has essentially changed the opti-
mization space. The current parameter initialization and
optimization technique may not be suitable for our model.
We leave the exploration of better binarization, initializa-
tion and optimization techniques to our future works.
In
this paper, we always initialize our backbone network and
gater network from networks pre-traiend on the same task,
and empirically ﬁnd it works well with a range of models.

4. Experiments

We ﬁrst conduct preliminary experiments on CIFAR [21]
with ResNet [10, 11], which gives us a good understand-
ing about the performance improvements our method can
achieve and also the gating strategies that our gater is learn-
ing. Then we apply our method to state-of-the-art mod-
els on CIFAR-10 and show that we consistently outper-
form these models. Lastly, we move on to a large-scale
classiﬁcation dataset, ImageNet 2012 [24], and show that
our method signiﬁcantly improves the performance of large
models, such as ResNet and Inception-v4 [27], as well.

4.1. Datasets

CIFAR-10 and CIFAR-100 contain natural images be-
longing to 10 and 100 classes respectively. There are 50,000

9175

training and 10,000 test images. We randomly hold out
5,000 training images as a validation set. All the ﬁnal re-
sults reported on test images are using models trained on
the complete training set. The raw images are with 32 × 32
pixels and we normalize them using the channel means
and standard deviations. Standard data augmentation by
random cropping and mirroring are applied to the training
set. ImageNet 2012 classiﬁcation dataset contains 1.28 mil-
lion training images and 50,000 validation images of 1,000
classes. We use the same data augmentation method as
the original papers of the baseline models in Table 3. The
images are of 224 × 224 and 299 × 299 in ResNet and
Inception-v4 respectively.

4.2. Cifar 10 and CIFAR 100

4.2.1 Preliminary Experiments with ResNet
We ﬁrst validate the effectiveness of our method using
ResNet as the backbone network on CIFAR-10 and CIFAR-
100 datasets. We consider a shallow version, ResNet-20,
and two deep versions, ResNet-56 and ResNet-1642 to gain
a better understanding on how our gating strategy can help
models with varying capacities. All our gated models em-
ploy ResNet-20 as the gater network. Table 1 shows the
comparison with baseline models on the test set. ResNet-
Wider is the ResNet with additional ﬁlters at each layer
such that it contains roughly the same number of parame-
ters as our model. ResNet-SE is the ResNet with squeeze-
and-excitation block [12]. The Gated Filters column shows
the number of ﬁlters under consideration in our models.

Classiﬁcation Results From the table we can see that, our
model consistently outperforms the original ResNet with a
signiﬁcant margin. On CIFAR100, the error rate of ResNet-
164 is reduced by 1.83%.

It is also evident that, our model is performing better
than ResNet-SE in all cases. Note that our gater network is
generating binary gates for the backbone network channels,
while ResNet-SE is re-scaling the channels. It is interesting
that, although our method is causing more information loss
in the forward pass of backbone network due to the sparse
discrete gates, our model still achieves better generalization
performance than ResNet and ResNet-SE. This to some ex-
tent validates our assumption that only a subset of ﬁlters are
needed for the backbone to process an input sample.

Ablation Analysis on Model Size
In all cases, ResNet-
Wider is better than the original ResNet as well. ResNet-
20-Wider is even the best among all the shallow models.
We hypothesize that ResNet-20 is suffering from under-
ﬁtting due to its small amount of ﬁlters and hence adding
additional ﬁlters signiﬁcantly improves the model. On the
other hand, although ResNet-20-Gated has a similar num-
ber of parameters as ResNet-20-Wider, a signiﬁcant portion

2Our ResNet-164 is slightly different to the one in [11]. The number of

ﬁlters in the ﬁrst group of residual units are 16, 4, 16 respectively.

(about a half) of its parameters belongs to the gater network,
rather than directly participating in prediction, and ResNet-
20-Gated still performed on par with ResNet-20-Wider.

The backbone network in ResNet-20-Gated suffers from
underﬁtting due to the lack of effective ﬁlters. The com-
parison among the deep models validates our hypothesis.
ResNet-50 and ResNet-164 contain many more ﬁlters than
ResNet-20, and adding ﬁlters to them shows only a mi-
nor improvement (see ResNet-50-Wider and ResNet-164-
Wider). In these cases, our models show a signiﬁcant im-
provement over the wider models and are the best among
all the deep models on both datasets. The comparison with
ResNet-Wider shows that the effectiveness of our model
is not solely due to the increase of parameter number, but
mainly due to our new gating mechanism.

Complexity It appears to be an issue at a glance if a com-
prehensive gater network is needed to assist a backbone net-
work, as it may greatly increase the number of parameters.
However, our experiments show that the gater network does
not need to be complex, and as a matter of fact, it can be
much smaller than the backbone network (see Table 1). Al-
though the number of ﬁlters (in the backbone network) un-
der consideration varies from 336 to 7200, the results show
that a simple gater network such as ResNet-20 is powerful
enough to learn input-dependent gates for the three models
that have a wide range of model capacity. As such, when the
backbone network is large (where our method shows more
signiﬁcant improvements over baselines),
the parameter
overhead introduced by the gater network becomes small.
For example, ResNet-164-Gated has only 20% more pa-
rameters than ResNet-164. In contrast, in other more com-
plicated backbone networks such as DenseNet and Shake-
Shake, this overhead is reduced to 1.2% as shown in Table 2.
Consequently, the complexity and the number of additional
parameters that our method brings to an existing model is
relatively small, especially to large models.

Gate Distribution One question that would naturally oc-
cur is how the distribution of the learned gates looks like.
Firstly, it is possible that the gater network is just randomly
pruning the backbone network and introducing regulariza-
tion effects similar to dropout into the backbone. It is also
possible that the gates are always the same for different
samples. Secondly, the generated gates may give us good
insights into the importance of ﬁlters at different layers.

To answer these questions, we analyzed the gates gen-
erated by the gater network in ResNet-164-Gated. We
ﬁrst conduct forward propagation in the gater network on
CIFAR-10 test set and collect gates for all the test samples.
As expected, three types of gates emerge: gates that are al-
ways on for all the samples, gates that are always off, and
gates that can be on or off conditioned on the input, i.e.,
input-dependent gates. We show the percentage of the three
types of gates at different depth in Figure 2. We can see

9176

Table 1. Classiﬁcation error rates on the CIFAR-10 and CIFAR-100 test set. All the methods are with data augmentation. ResNet-Wider
is the ResNet with additional ﬁlters at each layer such that it contains roughly the same number of parameters as our model. ResNet-SE is
the ResNet with squeeze-and-excitation blocks. All the ResNet-Gated models are using ResNet-20 as the gater network. The Gated Filters
column shows the number of ﬁlters subject to gating in our models. All the baseline results are from our reimplementation.

Gated Filters Param Error Rates % Param Error Rates %

Cifar10

Cifar100

ResNet-20 [10]
ResNet-20-Wider
ResNet-20-SE [12]
ResNet-20-Gated (Ours)
ResNet-56 [10]
ResNet-56-Wider
ResNet-56-SE [12]
ResNet-56-Gated (Ours)
ResNet-164 [11]
ResNet-164-Wider
ResNet-164-SE [12]
ResNet-164-Gated (Ours)

-
-
-
336
-
-
-
1,008
-
-
-
7,200

0.27M 8.06
0.56M 6.85
0.28M 7.81
0.55M 6.88 (↓1.18)
0.86M 6.74
1.08M 6.72
0.88M 6.27
1.14M 5.72 (↓1.02)
1.62M 5.61
2.04M 5.57
2.00M 5.51
1.96M 4.80 (↓0.81)

0.28M 32.39
0.57M 30.08
0.29M 31.22
0.60M 30.79 (↓1.60)
0.86M 28.87
1.09M 28.39
0.89M 28.00
1.14M 27.71 (↓1.16)
1.64M 25.39
2.07M 24.80
2.02M 23.83
1.98M 23.56 (↓1.83)

Figure 2. The distribution of Gates in each layer for ResNet-164-
Gated on Cifar-10 Test Set. There are totally 54 residual units.

that, a large subset (up to 68.75%) of the gates are always
off at the shallow residual blocks. As the backbone net-
work goes deeper, the proportion of always-on and input-
dependent gates increases gradually. In the last two residual
blocks, input-dependent gates become the largest subset of
gates with percentages of around 45%. The phenomenon is
consistent with the common belief that shallow layers are
usually extracting low-level features which are essential for
all kinds of samples, while deep layers are extracting high-
level features which are very sample-speciﬁc.

Although the above ﬁgures show that the gater network
is learning input-dependent gates, it does not show how of-
ten that these gates are on/off. For example, a gate that is
on for only one test sample but off for the rest would also
appear input-dependent. To investigate this further, we col-
lect all the input-dependent gates and plot the distribution
of number of times that they are on in Figure 3. There are
totally 1567 input-dependent gates out of the total number
of 7200 gates for the backbone network. While many of
these gates remain in one state—either on or off—in most
of the time, there are 1,124 gates that switch on and off more
frequently—they are activated for 100 ∼ 9900 samples out

Figure 3. Distribution: X-axis is the number of times an input-
dependent gate is on, while Y-axis is the number of gates.

Figure 4. Distribution: X-axis is the number of gates on, while
Y-axis is the number of samples.

of the 10,000 test samples.

We also examined how many gates are ﬁred when pro-
cessing each test example. The maximum and minimum
number of ﬁred gates per sample is 5380 and 5506 respec-
tively. The average number is around 5453. The number
of gates used each time seems to obey a normal distribution
(see Figure 4).

Lastly, we want to investigate what gating strategy has
been learned by the gater network. To do so, we represent
the ﬁlter usage of each test sample as a 7200-dimensional
binary vector where each element in the vector represents if
the corresponding gate is on (1) or off (0). We collect the
ﬁlter usage vector of each sample and reduce the dimension

9177

LayerPercentage0%25%50%75%100%1020304050Always OffAlways OnInput-Dependentnetwork to form our models respectively. Table 2 summa-
rizes the comparison of our models with the original mod-
els. The gater network in our method consistently improves
the state-of-the-art backbone network without signiﬁcantly
increasing the number of parameters. One of our models,
Shake-Shake-Gated 26 2x96d, has only 1.2% more param-
eters than the corresponding baseline model. Another in-
teresting ﬁnding is that, with the assistance of the gater net-
work, DenseNet-BC-Gated (L = 250, K = 24) is even per-
forming better than both DenseNet-BC (L = 190, K = 40)
and DenseNet-BC-Gated (L = 190, K = 40), although it
has much fewer parameters.

Note that in [8], it is shown when Shake-Shake 26 2x96d
is combined with a data pre-processing technique called
cutout, it can achieve 2.56% error rate on CIFAR-10 test
set. The technique is orthogonal to our method and can also
be combined with our method to give better results.

4.3. ImageNet

To test the performance of our method on large datasets,
we apply our method to models for ImageNet. We use
ResNet [10] and Inception-v4 [27] as the backbone network
and ResNet-18 [10] as the gater network to form our mod-
els. Table 3 shows the classiﬁcation results on ImageNet
validation set with baselines similar to the settings in Ta-
ble 1. We can see that, our method improves all the models
by 0.52% ∼ 1.85% in terms of top-1 error rate, and 0.14% ∼
0.78% in terms of top-5 error rate. Note that [29] proposes
to dynamically skip layers in ResNet-101, and the top-1
and top-5 error rates of their model are 22.63% and 6.26%
respectively. Our ResNet-101-Gated achives 21.51% and
5.72% on the same task, which is apparently much better
than their model. In addition, there are also two interesting
ﬁndings:

• The performance of ResNet-101 is signiﬁcantly
boosted with the help of the gater network. ResNet-
101-Gated is even performing better than ResNet-152
using much fewer layers.

• Similar to the results on CIFAR datasets, ResNet-
Wider is performing well when the original model is
shallow and small, but is outperformed by our models
when the original model contains enough ﬁlters.

4.4. Implementation Details

We train the baseline models by following the training
schemes in the original papers. We pre-train the backbone
and the gater network on the target task separately to prop-
erly initialize the weights. The training scheme here in-
cludes training conﬁgurations such as number of training
epochs, learning rate, batch size, weight decay and so on.

After pre-training, we train the backbone and the gater
network jointly as a single model. In addition to follow-
ing the original training scheme for each backbone archi-

9178

Figure 5. Visualization of the high dimensional gate vectors. PCA
and t-SNE are applied to reduce the vector dimension to 2. Dots
with the same color corresponds to test samples with the same
label. Better viewed in color.

of these vectors from 7200 to 400 using Principal Compo-
nent Analysis (PCA). We then project these vectors onto a
2-dimensional space via t-SNE [22] (see Figure 5). Interest-
ingly, we ﬁnd samples of the same class tend to use similar
gates. In the ﬁgure, each color of dots represents a ground-
truth label. This shows that the gater network learned to turn
on similar gates for samples from the same class— hence
similar parts of the backbone network are used to process
the samples from the class. On the other hand, we found the
clusters in Figure 5 is still far from perfectly setting samples
from different labels apart. It is indeed a good evidence that
the gater network doesn’t accidentally take over the predic-
tion task that the backbone network is intended to do, which
is what we want to avoid. We want the gater network to
focus on learning to make good decisions on which ﬁlters
in the backbone network should be used. From this anal-
ysis, we can see that the experiments are turned out as we
expected and the backbone network still does the essential
part of prediction for achieving the high accuracy.

We can draw the following conclusions from the above

observations and analyses:

• The gater network is capable of learning effective gates
for different samples. It tends to generate similar gates
for samples from the same class (label).

• The residual blocks at shallow layers are more redun-

dant than those at deep layers.

• Input-dependent features are more needed at deep lay-

ers than at shallow layers.

4.2.2 State-of-the-Art on CIFAR-10

Next we test the performance of our method with state-
of-the-art models, Shake-Shake [9] and DenseNet [13], on
CIFAR-10 dataset. We use Shake-Shake and DenseNet as
the backbone network and ResNet-20 again as the gater

Table 2. Classiﬁcation error rates on the CIFAR-10 test set. All the methods use data augmentation during training. All our models are
using ResNet-20 as the gater network. The Gated Filters column shows the number of ﬁlters subject to gating in our models. All the
baseline results are from our reimplementation.

DenseNet-BC (L = 100, k = 12) [13]
DenseNet-BC-Gated (L = 100, k = 12, Ours)
DenseNet-BC (L = 250, k = 24) [13]
DenseNet-BC-Gated (L = 250, k = 24, Ours)
DenseNet-BC (L = 190, k = 40) [13]
DenseNet-BC-Gated (L = 190, k = 40, Ours)
Shake-Shake 26 2x64d [9]
Shake-Shake-Gated 26 2x64d (Ours)
Shake-Shake 26 2x96d [9]
Shake-Shake-Gated 26 2x96d (Ours)

Gated Filters
-
540
-
2,880
-
3,600
-
3,584
-
5,376

Param Error Rates %
0.77M 4.48
1.05M 4.03
15.32M 3.61
15.62M 3.31
25.62M 3.52
25.93M 3.39
11.71M 3.05
12.01M 2.89
26.33M 2.82
26.65M 2.64

Table 3. Single-crop error rates on the ImageNet 2012 validation set. ResNet-Wider is the ResNet with additional ﬁlters at each layer such
that it contains roughly the same number of parameters as our model. ResNet-SE is the ResNet with squeeze-and-excitation units. All the
ResNet-Gated models are using ResNet-18 as the gater network. The Gated Filters column shows the number of ﬁlters subject to gating in
our models. All the baseline results are from our reimplementation, except † from the original paper.

Gated Filters Parameters Top-1 Error % Top-5 Error %

ResNet-34 [10]
ResNet-34-Wider
ResNet-34-SE [12]
ResNet-34-Gated (Ours)
ResNet-101 [10]
ResNet-101-Wider
ResNet-101-SE [12]
ResNet-101-Gated (Ours)
ResNet-152 [10]
ResNet-152-Wider
ResNet-152-SE [12]
ResNet-152-Gated (Ours)
Inception-v4 [27]
Inception-v4-Gated (Ours)

-
-
-
3,776
-
-
-
32,512
-
-
-
47,872
-
16,608

21.80M
33.89M
21.96M
34.08M
44.55M
59.17M
49.33M
64.21M
60.19M
81.37M
66.82M
83.80M
44.50M
61.67M

26.56
25.36
26.08
26.04 (↓0.52)
23.36
21.89
22.38†
21.51 (↓1.85)
22.34
21.50
21.57†
21.19 (↓1.15)
20.33
19.64 (↓0.69)

8.48
7.91
8.30
8.34 (↓0.14)
6.56
6.05
6.07†
5.78 (↓0.78)
6.22
5.67
5.73†
5.45 (↓0.77)
4.99
4.80 (↓0.19)

tecture, we introduce a few minor modiﬁcations. Firstly,
we increase the number of training epochs for DenseNet-
Gated and Shake-Shake-Gated by 20 and 30 respectively as
they seem to converge slowly at the end of training. Sec-
ondly, we set the initial learning rate for DenseNet-Gated
and Shake-Shake-Gated to a smaller value, 0.05.

Note that not all the ﬁlters in a backbone network are
subject to gating in our experiments. When ResNet is used
as the backbone, we apply ﬁlter selection to the last con-
volutional layer in each residual unit, which is similar to
SE-block [12]. As for DenseNet, we apply ﬁlter selection
to all the convolutional layers except the ﬁrst in each dense
block. There are multiple residual branches in each residual
block in Shake-Shake. We apply ﬁlter selection to the last
convolutional layer in each branch. In Inception-v4, there
are many channel concatenation operations. We apply ﬁlter
selection to feature maps after the concatenation operations.

and Inception-v4-Gated use a bottlneck size of 256, while
ResNet-152-Gated uses 1024.

5. Conclusions

In this paper, we have proposed GaterNet, a novel ar-
chitecture for input-dependent ﬁlter selection in CNNs. It
involves two distinct components: a backbone network that
conducts actual prediction and a gater network that decides
which part of backbone network should be used for pro-
cessing each input. Extensive experiments on CIFAR and
ImageNet indicate that our models consistently outperform
the original models with a large margin. On CIFAR-10, our
model improves upon state-of-the-art results. We have also
performed an in-depth analysis about the model behavior
that reveals an intuitive gating strategy learned by the gater
network.

6. Acknowledgments

For all our models on CIFAR, we set the size of the bot-
tleneck layer to 8. ResNet-34-Gated, ResNet-101-Gated

We want to thank Sergey Ioffe, Noam Shazeer, Zhenhai
Zhu and anonymous reviewers for their insightful feedback.

9179

[21] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, 2009.
[22] Laurens van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research,
9(Nov):2579–2605, 2008.

[23] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In ICML, pages 807–
814, 2010.

[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge.
International Journal of Computer Vision (IJCV),
115(3):211–252, 2015.

[25] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer. In ICLR, 2017.

[26] Marijn F Stollenga, Jonathan Masci, Faustino Gomez, and
J¨urgen Schmidhuber. Deep networks with internal selec-
tive attention through feedback connections. In NIPS, pages
3545–3553, 2014.

[27] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, 2017.

[28] Ravi Teja Mullapudi, William R. Mark, Noam Shazeer, and
Kayvon Fatahalian. Hydranets: Specialized dynamic archi-
tectures for efﬁcient inference. In CVPR, June 2018.

[29] Andreas Veit and Serge J. Belongie. Convolutional net-
works with adaptive inference graphs.
In ECCV, volume
11205 of Lecture Notes in Computer Science, pages 3–18.
Spriba2013adaptivenger, 2018.

[30] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention.
In ICML, pages 2048–2057,
2015.

References

[1] Jimmy Ba and Brendan Frey. Adaptive dropout for training

deep neural networks. In NIPS, pages 3084–3092, 2013.

[2] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and
Doina Precup. Conditional computation in neural networks
for faster models. arXiv preprint arXiv:1511.06297, 2015.

[3] Yoshua Bengio. Deep learning of representations: Look-
ing forward. In Statistical Language and Speech Processing,
pages 1–37, Berlin, Heidelberg, 2013. Springer Berlin Hei-
delberg.

[4] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
Estimating or propagating gradients through stochastic
neurons for conditional computation.
arXiv preprint
arXiv:1308.3432, 2013.

[5] Zhourong Chen, Xiaopeng Li, and Nevin L. Zhang. Learn-
ing sparse deep feedforward networks via tree skeleton ex-
pansion. CoRR, abs/1803.06120, 2018.

[6] Andrew Davis and Itamar Arel. Low-rank approximations
for conditional feedforward computation in deep neural net-
works. arXiv preprint arXiv:1312.4461, 2013.

[7] Ludovic Denoyer and Patrick Gallinari. Deep sequential
In Deep Learning and Representation

neural network.
Learning Workshop, NIPS 2014, 2014.

[8] Terrance Devries and Graham W. Taylor. Improved regular-
ization of convolutional neural networks with cutout. CoRR,
abs/1708.04552, 2017.

[9] Xavier Gastaldi. Shake-shake regularization. In ICLR Work-

shop, 2017.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In ECCV.

Identity mappings in deep residual networks.
Springer, 2016.

[12] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. In CVPR, 2018.

[13] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. In CVPR, 2017.

[14] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015.

[15] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and
Geoffrey E Hinton. Adaptive mixtures of local experts. Neu-
ral computation, 3(1):79–87, 1991.

[16] Łukasz Kaiser and Samy Bengio. Can active memory replace

attention? In NIPS, pages 3781–3789, 2016.

[17] Łukasz Kaiser and Samy Bengio. Discrete autoencoders for

sequence models. arXiv preprint arXiv:1801.09797, 2018.

[18] Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani,
Niki Parmar, Jakob Uszkoreit, and Noam Shazeer. Fast de-
coding in sequence models using discrete latent variables. In
ICML, 2018.

[19] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algo-

rithms. arXiv preprint arXiv:1511.08228, 2015.

[20] E.R. Kandel, J.H. Schwartz, and T.M. Jessell. Principles of

neural science. International edition. Elsevier, 1991.

9180

