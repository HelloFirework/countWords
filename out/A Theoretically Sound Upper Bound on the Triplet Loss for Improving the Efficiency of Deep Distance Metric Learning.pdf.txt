A Theoretically Sound Upper Bound on the Triplet Loss for Improving the

Efﬁciency of Deep Distance Metric Learning

Thanh-Toan Do1, Toan Tran2, Ian Reid2, Vijay Kumar3, Tuan Hoang4, Gustavo Carneiro2

1University of Liverpool 2University of Adelaide 3PARC 4Singapore University of Technology and Design

Abstract

We propose a method that substantially improves the ef-
ﬁciency of deep distance metric learning based on the op-
timization of the triplet loss function. One epoch of such
training process based on a na¨ıve optimization of the triplet

loss function has a run-time complexity O(N 3), where N is

the number of training samples. Such optimization scales
poorly, and the most common approach proposed to ad-
dress this high complexity issue is based on sub-sampling
the set of triplets needed for the training process. Another
approach explored in the ﬁeld relies on an ad-hoc lineariza-
tion (in terms of N ) of the triplet loss that introduces class
centroids, which must be optimized using the whole train-
ing set for each mini-batch – this means that a na¨ıve imple-

mentation of this approach has run-time complexity O(N 2).

This complexity issue is usually mitigated with poor, but
computationally cheap, approximate centroid optimization
methods. In this paper, we ﬁrst propose a solid theory on
the linearization of the triplet loss with the use of class cen-
troids, where the main conclusion is that our new linear
loss represents a tight upper-bound to the triplet loss. Fur-
thermore, based on the theory above, we propose a training
algorithm that no longer requires the centroid optimization
step, which means that our approach is the ﬁrst in the ﬁeld
with a guaranteed linear run-time complexity. We show
that the training of deep distance metric learning meth-
ods using the proposed upper-bound is substantially faster
than triplet-based methods, while producing competitive re-
trieval accuracy results on benchmark datasets (CUB-200-
2011 and CAR196).

1. Introduction

Deep distance metric learning (DML) aims at training a
deep learning model that transforms training samples into
feature embeddings that are close together for samples that
belong to the same class and far apart for samples from dif-
ferent classes [4, 8, 9, 14, 20, 28, 29, 30, 35, 40, 46, 49].
The use of DML is advantageous compared to more tradi-
tional classiﬁcation models because DML does not rely on

Figure 1. Left: the triplet loss requires the calculation of the loss
over 8 × 3 × 4 = 96 elements (cubic complexity in the number of
samples). Right: the proposed upper bound to the triplet loss only
requires the loss calculation for 8 × 2 = 16 elements (linear com-
plexity in the number of samples and centroids). In addition, note
that the class centroids are ﬁxed during training, overcoming the
expensive centroid optimization step, with quadratic complexity,
found in similar approaches [7, 32, 42, 45].

a classiﬁcation layer that imposes strong constraints on the
type of problems that the trained model can handle. For in-
stance, if a model is trained to classify 1000 classes, then
the addition of the 1001st class will force the design of a
new model structure that incorporates that extra classiﬁca-
tion node. In addition, DML requires no model structure up-
date, which means that the learned DML model can simply
be ﬁne-tuned with the training samples for the new class.
Therefore, DML is an interesting approach to be used in
learning problems that can be continuously updated, such
as open-world [2] and life-long learning problems [39].

One of the most common optimization functions ex-
plored in DML is the triplet loss, which comprises two
terms: 1) a term that minimizes the distance between pairs
of feature embeddings belonging to the same class, and 2)
another term that maximizes the distance between pairs of
feature embeddings from different classes. The training
process based on this triplet loss has run-time complexity

O(N 3/C) per epoch, with N representing the number of
samples and C < N , the number of classes. Given that
training sets are becoming increasingly larger, DML train-
ing based on such triplet loss is computationally challeng-
ing, and a great deal of work has been focused on the re-
duction of this complexity without affecting much the ef-

110404

fectiveness of DML training. One of the main ideas ex-
plored is the design of mechanisms that select representa-

tive subsets of the O(N 3) triplet samples — some examples

of this line of research are: hard or semi-hard triplet min-
ing [27, 29, 43] or smart triplet mining [9]. Unfortunately,
these methods still present high computational complexity,
with a worst case training complexity of O(N 2). Moreover,
these approaches also present the issue that the subset of se-
lected triplets may not be representative enough, increasing
the risk of overﬁtting such subsets. The mitigation of this
problem is generally achieved with the incorporation of an
additional loss term that optimizes a global classiﬁcation to
the whole triplet subset [9, 14, 40] in order to regularize the
training process.

Another idea explored in the ﬁeld is the ad-hoc lineariza-
tion of the triplet loss [7, 32, 42, 45], consisting of the
use of auxiliary class centroids. The training process con-
sists of two alternating steps: 1) an optimization function
that generally pulls embeddings towards their class centroid
and pushes embeddings away from all other class centroids
(hence, O(N C)); and 2) an optimization of the class cen-
troids using the whole training set after the processing of
each mini-batch (hence, O(N × N/B), where B represents
the number of samples in each mini-batch). Therefore, a
na¨ıve implementation of this method has run-time complex-

ity proportional to O(N 2).

In this paper, we provide a solid theoretical background
that fully justiﬁes the linearization of the triplet loss, pro-
viding a tight upper bound to be used in the DML train-
ing process, which relies on the use of class centroids. In
particular, our theory shows that the proposed upper-bound
differs from the triplet loss by a value that tends to zero
if the distances between centroids are large and these dis-
tances are similar to each other, and as the training process
progresses. Therefore, our theory guarantees that a mini-
mization of the upper bound is also a minimization of the
triplet loss. Furthermore, we derive a training algorithm
that no longer requires an optimization of the class cen-
troids, which means that our method is the ﬁrst approach
in the ﬁeld that guarantees a linear run-time complexity for
the triplet loss approximation. Figure 1 motivates our work.
We show empirically that the DML training using our pro-
posed loss function is one order of magnitude faster than
the training of recently proposed triplet loss based methods.
In addition, we show that the models trained with our pro-
posed loss produces competitive retrieval accuracy results
on benchmark datasets (CUB-200-2011 and CAR196).

2. Related Work

features extracted from one of the last layers of the deep
classiﬁcation models [13, 31] can be used for new classi-
ﬁcation tasks, involving classes not used for training.
In
addition, the run-time training complexity is quite efﬁcient:
O(N C), where N and C < N represent the number of
training samples and number of classes, respectively. How-
ever, these approaches rely on cross-entropy loss that tries
to pull samples over the classiﬁcation boundary for the class
of that sample, disregarding two important points in DML:
1) how close the point is to the class centroid; and 2) how
far the sample is from other class centroids (assuming that a
class centroid can be deﬁned to be at the centre of the clas-
siﬁcation volume for each class in the embedding space).
Current evidence in the ﬁeld shows that not explicitly ad-
dressing these two issues made these approaches not attrac-
tive for DML, particularly in regards to the classiﬁcation
accuracy for classes not used for training.

Pairwise loss. One approach, explored in [25, 29] em-
ploys a siamese model trained with a pairwise loss. One of
the most studied pairwise losses is the contrastive loss [3],
which minimizes the distance between pairs of training
samples belonging to same class (i.e., positive pairs) and
maximizes the distance between pairs of training samples
from different classes (i.e., negative pairs) as long as this
“negative distance” is smaller than a margin. There are few
issues associated with this approach. Firstly, the run-time

training complexity is O(N 2), which makes this approach

computationally challenging for most modern datasets. To
mitigate this challenge, mining strategies have been pro-

posed to select a subset of the original O(N 2) pairwise

samples. Such mining strategies focus on selecting pairs
of samples that are considered to be hard to classify by the
current model. For instance, Simo-Serra et al. [29] proposed
a method that samples positive pairs and sorts them in de-
scending order with respect to the distance between the two
samples in the embedding space. A similar approach is ap-
plied for negative pairs, but the sorting is in ascending or-
der. Then, the top pairs in both lists are used as the train-
ing pairs. The second issue is that the margin parameter is
not easy to tune because the distances between samples can
change signiﬁcantly during the training process. Another
issue is the fact that the arbitrary way of sampling pairs of
samples described above cannot guarantee that the selected
pairs are the most informative to train the model. The ﬁnal
issue is that the optimization of the positive pairs is inde-
pendent from the negative pairs, but the optimization should
force the distance between positive pairs to be smaller than
negative pairs.

Classiﬁcation loss.
It has been shown that deep networks
that are trained for the classiﬁcation task with a softmax loss
function can be used to produce useful deep feature embed-
dings. In particular, in [1, 26] the authors showed that the

Triplet loss. The triplet loss addresses the last issue men-
tioned above [27, 9, 24], and it is deﬁned based on three
data points: an anchor point, a positive point (i.e., a point

10405

belonging to the same class as the anchor), and a negative
point (i.e., a point from a different class of the anchor). The
loss will force the positive pair distance plus a margin to
be smaller than the negative pair distance. However, simi-
larly to pairwise loss, setting the margin in the triplet loss
requires careful tuning. Furthermore, and also similarly
to the pairwise loss, the training complexity is quite high

with O(N 3/C), hence several triplet mining strategies have

been proposed. For instance, in [27], the authors proposed a
“semi-hard” criterion, where a triplet is selected if the nega-
tive distance is small (i.e., within the margin) but larger than
the positive distance — this approach reduces the training

complexity to O(N 3/CM 2), where M < N represents the

number of mini-batches used for training. In [9], the authors
proposed to use fast approximate nearest neighbor search
for quickly identifying informative (hard) triplets for train-

ing, reducing the training complexity to O(N 2). In [21],
the mining is replaced by the use of P < N proxies, where
a triplet is re-deﬁned to be an anchor point, a similar and
a dissimilar proxy – this reduces the training complexity to

O(N P 2). Movshovitz et al. [21] show that this loss com-

puted with proxies represents an upper bound to the orig-
inal triplet loss, where this bound gets closer to the origi-
nal triplet loss as P → N , which increases the complexity
back to O(N 3). It is worth noting that the idea of using
proxies and learning the embedding such as minimizing the
distances between samples to their proxies has been inves-
tigated in [23]. However, different from [21] which is a
DML approach and is expected to capture the nonlinearity
between samples, thanks to the power of the deep model,
in [23] the authors used precomputed features, and to cap-
ture the nonlinearity, they relied on kernelization. We note
that the approach [21] has a non-linear term that makes
it more complex than the O(N C) complexity of our ap-
proach, for C ≈ P , which is usually the case. Moreover,
the approach [21] also requires the optimization of the num-
ber and locations of proxies [21] during training, while our
approach relies on a set of predeﬁned and ﬁxed C centroids.

Other losses.
In [14] the authors proposed a global loss
function that uses the ﬁrst and second order statistics of
sample distance distribution in the embedding space to al-
low for robust training of triplet network, but the complex-

ity is still O(N 3). Ustinova and Lempitsky [40] proposed a

histogram loss function that is computed by estimating two
distributions of similarities for positive and negative pairs.
Based on the estimated distributions, the loss will compute
the probability of a positive pair to have a lower similar-
ity score than a negative pair, where the training complex-
In [34] the authors proposed a loss which
optimizes a global clustering metric (i.e., normalized mu-
tual information). This loss ensures that the score of the
ground truth clustering assignment is higher than the score

ity is O(N 2).

of any other clustering assignment – this method has com-

plexity O(N Y 3), where Y < N represents the number of

clusters. Similarly to [21], this approach has a non-linear
term w.r.t. number of clusters, that makes it more complex
than the O(N C) complexity of our approach. In addition,
this method also requires to optimize the locations of clus-
ters during training, while our approach relies on a set of
predeﬁned and ﬁxed C centroids. In [33], the authors pro-
posed the N-pair loss which generalizes the triplet loss by
allowing joint comparison among more than one negative

example – the complexity of this method is again O(N 3).

More recent works [22, 44] proposed the use of ensemble
classiﬁers [22] and new similarity metrics [44] which can in
principle explore the O(N C) training loss that we propose.
A relevant alternative method recently proposed in the ﬁeld
is related to an ad-hoc linearization of the triple loss that,
differently from our approach, has not been theoretically
justiﬁed [7, 32, 42, 45]. In addition, even though these ap-
proaches rely on a loss function that has run-time complex-
ity O(N C), they also need to run an expensive centroid op-
timization step after processing each mini-batch, which has
complexity O(N ). Assuming that a mini-batch has N/B
samples, then the run-time complexity of this approach is
O(N C + N 2/B). Most of the research developed for these
methods are centered on the mitigation of this O(N 2) com-

plexity involved in the class centroid optimization. Interest-
ingly, this step is absent from our proposed approach, which
means that our method is the only approach in the ﬁeld that
is guaranteed to have linear run-time complexity.

3. Discriminative Loss

Assume that the training set is represented by T =
i=1 in which Ii ∈ RH×W and yi ∈ {1, ..., C} de-
{Ii, yi}N
note the training image and its class label, respectively. Let
xi ∈ RD be the feature embedding of Ii, obtained from
the deep learning model x = f (I, θ). To control the mag-
nitude of distance between feature embeddings, we assume
that kxk = 1 (i.e., all points lie on a unit hypersphere1).
From an implementation point of view, this assumption can
be guaranteed with the use of a normalization layer. Fur-
thermore, without loss of generalization, let us assume that
the dimension of the embedding space equals the number of
classes, i.e., D = C. Note that if D 6= C we can enforce
this assumption by adding a fully connected layer to project
features from D dimensions to C dimensions.

3.1. Discriminative Loss: Upper Bound on the

Triplet Loss

In order to avoid the cubic complexity (in the number
of training points) of the triplet loss and to avoid the com-
plicated hard-negative mining strategies, we propose a new

1We use l2 distance in this work.

10406

loss function that has linear complexity on N , but inherits
the property of triplet loss: feature embeddings from the
same classes are pulled together, while feature embeddings
from different classes are pushed apart.

Assume that we have a set S = {(i, j)|yi =
yj,}i,j∈{1,...,N } representing pairs of images Ii and Ij be-
longing to the same class. Let us start with a simpliﬁed form
of the triplet loss:

Lt(T ,S) =

X

ℓt(xi, xj, xk),

(i,j)∈S,(i,k) /∈S,i,j,k∈{1,...,N }

where ℓt is deﬁned as

ℓt(xi, xj, xk) = kxi − xjk − kxi − xkk .

(1)

(2)

Let C = {cm}C
m=1, where each cm ∈ RD is an auxiliary
vector in the embedding space that can be seen as the “cen-
troid” for the mth class (note that as the centroids represent
classes in the embedding space, they should be deﬁned in
the same domain with the embedding features, i.e., on the
surface of the unit hypersphere). According to the triangle
inequality, we have

and

kxi − xjk ≤ kxi − cyik + kxj − cyik ,

kxi − xkk ≥ kxi − cykk − kxk − cykk .

(3)

(4)

From (3) and (4) we achieve the upper bound for ℓt as fol-
lows:

ℓt(xi, xj, xk) ≤ ℓd(xi, xj, xk),

where

ℓd(xi, xj, xk) = kxi − cyik − kxi − cykk
+ kxj − cyik + kxk − cykk

From (1) and (5), we have

(5)

(6)

Lt(T ,S) ≤

X

ℓd(xi, xj, xk).

(i,j)∈S,(i,k) /∈S,i,j,k∈{1,...,N }

(7)
The central idea in the paper is to minimize the upper bound
deﬁned in (7). Assume that we have a balanced training
problem, where the number of samples in each class is equal
(for the imbalanced training, this assumption can be en-
forced by data augmentation), after some algebraic manip-
ulations, the RHS of (7) is equal to Ld(T ,S) which is our
proposed discriminative loss

Ld(T , S) = G

N

X

i=1


kxi − cyi k −

1

3(C − 1)

C

X

m=1,m6=yi

kxi − cmk


(8)

where the constant G = 3(C − 1) (cid:0) N

C − 1(cid:1) N
C .

Our goal is to minimize Ld(T ,S) which is a discrimi-
native loss that simultaneously pulls samples from the same

class close to their centroid and pushes samples far from
centroids of different classes. A nice property of Ld(T ,S)
is that it is not arbitrary far from Lt(T ,S). The difference
between these two losses is well bounded by Lemma 3.1.
Lemma 3.1. Assuming that kxi − cyik ≤ ǫ/2 (with ǫ ≥ 0),
1 ≤ i ≤ N . Let κmin = min1≤m,n≤C,m6=n kcm − cnk
and κmax = max1≤m,n≤C,m6=n kcm − cnk, then
0 ≤ Ld(T ,S) − Lt(T ,S) ≤ H (κmax − κmin + 3ǫ),
where the constant H = (cid:0) N
C (cid:1) is the num-
ber of all possible triplets.

C − 1(cid:1) N (cid:0)N − N

Proof. The proof is provided in the Appendix.

From Lemma 3.1, Ld will approach Lt when ǫ → 0
and κmin → κmax. (i) Note from (8) that ǫ will decrease
because the discriminative loss pulls samples from the same
class close to their corresponding centroid. (ii) In addition,
we can enforce that κmin ≈ κmax by ﬁxing the centroids
before the training starts, such that they are as far as possible
from each other and the distances between them are similar.
Therefore, with the observations (i) and (ii), we can expect
that ǫ → 0 and κmax − κmin → 0, which implies a tight
bound from Lemma 3.1. We discuss methods to generate
centroid locations below.

3.2. Centroid Generation

From the observations above, we should have centroids
on the surface of the unit hypersphere such that they are as
far as possible from each other and the distances between
them are as similar as possible. Mathematically, we want
to maximize the minimum distance between C centroids –
this problem is known as the Tammes problem [38]. Let
F be the surface of the hypersphere, we want to solve the
following optimization:

min
{cm}C

m=1

−w

s.t. w ≤ kcm − cnk , 1 ≤ m, n ≤ C, m 6= n (9)

cm ∈ F, m = 1, ..., C

Unfortunately, it is not possible to solve (9) analytically in
general [15]. We may solve it as an optimization problem.

However, this optimization will involve O(C 2) constraints,
hence the problem is still computationally hard to solve for
large C [11]. To overcome this challenge, we propose two
heuristics to generate the centroids.

One-hot centroids.
Inspired by the softmax loss, we de-
ﬁne the centroids as vertices of a convex polyhedron in
which each vertex is a one-hot vector, i.e., the centroid of
the mth class is the standard basis vector in mth direction
of the Cartesian coordinate system. With this conﬁguration,
each centroid is orthogonal to each other and the distance

between each pair of centroids is √2.

10407

{cm}C

K-means centroids. We ﬁrst uniformly generate a large
set of points on the surface of the unit hypersphere. We then
run K-means clustering to group points into C clusters. The
unit normalized cluster centroids will be used as centroids
m=1. Note that the process of uniformly generating
points on the surface of the unit hypersphere is not difﬁcult.
According to [19], for each point, we generate each of its
dimension independently with the standard normal distri-
bution. Then we unit normalize the point to project it to the
surface of the hypersphere.

3.3. Discussion on the Discriminative Loss

Training complexity. Table 1 compares the asymptotic
training complexity of several DML methods, including our
proposed discriminative loss (Discriminative) in terms of
the number of training samples N , number of mini-batches
M , size of mini-batch B, number of classes C, number of
proxies P [21] and number of clusters Y [34]. It is clear
from (8) that our proposed discriminative loss has linear
run-time complexity (in terms of both N and C), analogous
to the softmax loss [1]. The methods that optimize an ap-
proximate triplet loss, which have linear complexity in N
are represented by “centroids” [7, 32, 42, 45] in Table 1,
but note in the table that the optimization of the centroids
must be performed after processing each mini-batch, which
increases the complexity of the approach to be square in
N . Most of the research in the “centroids” approach goes
into the reduction of the complexity in the optimization of
class centroids with the design of poor, but computationally
cheap, approximate methods. For example, in [45], instead
of updating the centroids with respect to the entire training
set, the authors perform the update based on mini-batch.
This leads to a linear complexity in N . However by updat-
ing centroids based on mini-batch, a small batch size (e.g.
due to a large network structure, which is likely) may cause
a poor approximation of the real centroids.
In the worst
case, when not all classes are present in a batch, some cen-
troids are even not be updated. Interestingly, the centroid
update step is absent from our proposed approach.

There are other DML methods that are linear in N :
clustering [34] with O(N Y 3) and triplet+proxy [21] with
O(N P 2). There are two advantages of our approach com-

pared to these two methods in terms of training complexity:
1) our discriminative loss is linear not only in terms of the
dominant variable N , but also with respect to the auxiliary
variable C < N (where in general C ≈ P, Y ); and 2) in our
work, the number of centroids and their positions are ﬁxed
before the training process starts (as explained in Sec. 3.2),
hence there is no need to optimize the number and positions
of centroids during training — this contrasts with the fact
that the number and positions of clusters and proxies need
to be optimized in [34] and [21].

Trip.-smart [9] Trip.-cluster [34] Trip.-proxy [21] Centroids [7, 32, 42, 45]

Softmax [1]
O(N C)
O(N 2)

Pair-na¨ıve

Trip.-na¨ıve

Trip.-hard [27]

O(N 2)
O(N Y 3)

O(N 3/C)
O(N P 2)

Discriminative

O(N 3/(M 2C))
O(N C + N 2/B)

O(N C)

Table 1. Run-time training complexity of various DML approaches
and our proposed discriminative loss in terms of the number of
training samples N , number of mini-batches M , size of mini-
batch B, number of classes C, number of proxies P [21] and num-
ber of clusters Y [34].

Simplicity.
The discriminative loss only involves the
calculation of Euclidean distance between the embedding
features and the centroids. Hence it is straightforward to
implement and integrate into any deep learning models to
be trained with the standard back-propagation. Further-
more, different from most of the traditional DML losses
such as pairwise loss, triplet loss, and their improved ver-
sions [27, 33, 14, 21, 9], the discriminative loss does not
require setting margins, mining triplets, and optimizing the
number and locations of centroids during training. This re-
duction in the number of hyper-parameters makes the train-
ing simpler and improves the performance (compared to
standard triplet methods), as showed in the experiments.

4. Experiments

4.1. Dataset and Evaluation Metric

We conduct our experiments on two public benchmark
datasets that are commonly used to evaluate DML meth-
ods, where we follow the standard experimental protocol
for both datasets [9, 33, 34, 35]. The CUB-200-2011
dataset [41] contains 200 species of birds with 11,788 im-
ages, where the ﬁrst 100 species with 5,864 images are used
for training and the remaining 100 species with 5,924 im-
ages are used for testing. The CAR196 dataset [12] con-
tains 196 car classes with 16,185 images, where the ﬁrst 98
classes with 8,054 images are used for training and the re-
maining 98 classes with 8,131 images are used for testing.
We report the K nearest neighbor retrieval accuracy using
the Recall@K metric. We also report the clustering quality
using the normalized mutual information (NMI) score [18].

4.2. Network Architecture and Implementation De 

tails

For all experiments in Sections 4.3 and 4.4, we initialize
the network with the pre-trained GoogLeNet [36] – this is
also a standard practice in the comparison between DML
approaches [9, 33, 34, 35]. We then add two randomly
initialized fully connected layers. The ﬁrst layer has 256
nodes, which is the commonly used embedding dimension

10408

in previous works, and the second layer has C nodes. We
train the network for a maximum of 40 epochs. For the last
two layers, we start with an initial learning rate of 0.1 and
gradually decrease it by a factor of 2 every 5 epochs. Fol-
lowing [35], all GoogLeNet layers are ﬁne-tuned with the
learning rate that is ten times smaller than the learning rate
of the last two layers. The weight decay and the batch size
are set to 0.0005 and 128, respectively in all experiments.
As normally done in previous works, random cropping and
random horizontal ﬂipping are used when training.

4.3. Ablation Study

Effect of features from different layers.
In this exper-
iment we evaluate the embedding features from the last
two fully connected layers with dimensions 256 and C
(C = 100 for CUB-200-2011 and C = 98 for CAR196).
These results are based on the one-hot centroid generation
strategy, but note that the same evidence was produced with
the K-means centroid generation. The results in Table 2
show that the features from the second to last layer produce
better generalization for unseen classes than those from the
last layer. The possible reason is that the features from the
last layer may be too speciﬁc to the set of training classes.
Hence for tasks on unseen classes, the features from the sec-
ond to last layer produce better performance. The same ob-
servation is also found in [1] (although Razavian et al. [1]
experimented with AlexNet [13]). Hereafter, we only use
the features from the second to last fully connected layer
for the remaining experiments. Note that this also allows
for a fair comparison between our work and previous ap-
proaches in terms of feature extraction complexity because
these other approaches also use the feature embeddings ex-
tracted from the same layer.

Effect of centroid generation method.
In this section,
we evaluate the two proposed centroid generation methods,
explained in Sec. 3.2, where the hypersphere for the K-
means approach has C dimensions. The comparative per-
formances and statistics of distances between centroids are
shown in Table 3. The results show that there is not a sig-
niﬁcant difference in performance between the two centroid
generation methods. In the worst case, K-means is 1.5%
worse than one-hot on CAR196 dataset while on CUB-200-
2011 dataset, these two methods are comparable. Hereafter,
we only use one-hot centroid generation strategy for all re-
maining experiments.

According to Table 3, we note that the difference be-
tween the minimum and the maximum distances between
centroids is quite small for K-means and 0 for the one-hot
centroid generation methods. This is an important fact for
the triplet loss bound in the Lemma 3.1, where the smaller
this difference, the tighter the bound to the triplet loss.

4.4. Comparison with Other Methods

We compare our method to the baseline DML methods
that have reported results on the standard datasets CUB-
200-2011 and CAR196: the softmax loss, the triplet loss
with semi-hard negative mining [27], the lifted structured
loss [35], the N-pair [33] loss, the clustering loss [34],
the triplet combined with global loss [14], the histogram
loss [40], the triplet with proxies [21] loss, triplet with
smart mining [9] loss which uses the fast nearest neighbor
search for mining triplets.

Tables 4 and 5 show the recall and NMI scores for the
baseline methods and our approach (Discriminative). The
results on Tables 4 and 5 show that for the NMI metric,
most triplet-based methods achieve comparable results, ex-
cept for Triplet+proxy [21] which has a 5.2% gain over the
second best Discriminative on the CAR196 dataset. Un-
der Recall@K metric, the Discriminative improves over
most of methods that are based on triplet, (e.g., Semi-
hard [27]) or generalization of triplet (e.g., N-pair [33],
Triplet+Global [14]). Compared to the softmax loss, al-
though both discriminative loss and softmax loss have the
same complexity, Discriminative improves over Softmax
by a large margin for all measures on both datasets. This
suggests that the discriminative loss is more suitable for
DML than the softmax loss.

Discriminative also compares favorably with the recent
triplet+smart mining method [9], i.e., on the CAR196
dataset, Discriminative has 3.6% improvement in R@1
over the triplet+smart mining. Compared to the recent
Triplet+proxy on the CUB-200-2011 dataset, Discrimina-
tive shows better results at all ranks of K, where larger
improvements are observed at larger K, i.e., Discrimina-
tive has 10.4% (14.4% relative) improvement in R@8 over
Triplet+proxy. On the CAR196 dataset, Triplet+proxy
outperforms the Discriminative at low values of K, i.e.,
Triplet+proxy has 4.9% (7.2% relative) higher accuracy
than Discriminative at R@1. However,
for increas-
ing values of K, the improvement of Triplet+proxy de-
creases, and Discriminative achieves a higher accuracy
than Triplet+proxy at R@8.

We are aware that there are other triplet-based meth-
ods that achieve better performance on CUB and CAR196
datasets [5, 16, 22, 44, 48]. Table 6 presents their results.
However, it is important to note that although these meth-
ods use the triplet loss, they rely on additional techniques to
boost their accuracy. For instance, Yuan et al. [48] used
cascaded embedding to ensemble a set of models; Opitz
et al. [22] relied on boosting to combine different learners;
Wang et al. [44] combined angular loss with N-pair loss [33]
to boost performance; Duan et al. [5] and Lin et al. [16] used
generative adversarial network (GAN) to generate synthetic
training samples. We note that these techniques can in prin-
ciple replace their triplet loss by our discriminative loss to

10409

Last layer

Second to last layer

CUB-200-2011
R@4
R@2
72.52
62.32
64.23
74.31

R@1
49.49
51.43

CAR196

R@8
81.57
82.83

R@1
65.32
68.31

R@2
76.44
78.21

R@4
84.10
85.22

R@8
89.84
91.18

Table 2. Performance of embedding features from the last two layers.

CUB-200-2011

one-hot cent.
K-means cent.

R@1
51.43
50.75

R@2
64.23
63.54

R@4
74.31
73.26

R@8 min dist. max dist. mean dist.
82.83
82.36

√2

√2

√2

1.418

1.21

1.63

one-hot cent.
K-means cent.

68.31
66.93

78.21
76.74

85.22
83.80

91.18
90.37

CAR196

√2

1.18

√2

1.65

√2

1.416

std dist.

0

0.061

0

0.066

Table 3. Performance of two different centroid generation strategies and the statistics of distances (dist.) between centroids.

SoftMax

Semi-hard [27]

Lifted structure [35]

N-pair [33]

Triplet+Global [14]

Clustering [34]

Triplet+smart mining [9]

Triplet+proxy [21]

Histogram [40]
Discriminative

NMI
57.21
55.38
56.50
57.24
58.61
59.23
59.90
59.53

-

59.92

R@1
48.34
42.59
43.57
45.37
49.04
48.18
49.78
49.21
50.26
51.43

R@2
60.16
55.03
56.55
58.41
60.97
61.44
62.34
61.90
61.91
64.23

R@4
71.21
66.44
68.59
69.51
72.33
71.83
74.05
67.90
72.63
74.31

R@8
80.30
77.23
79.63
79.49
81.85
81.92
83.31
72.40
82.36
82.83

Table 4. Clustering and Recall performance on the CUB-200-2011 dataset.

SoftMax

Semi-hard [27]

Lifted structure [35]

N-pair [33]

Triplet+Global [14]

Clustering [34]

Triplet+smart mining [9]

Triplet+proxy [21]

Histogram [40]
Discriminative

NMI
58.38
53.35
56.88
57.79
58.20
59.04
59.50
64.90

-

59.71

R@1
62.39
51.54
52.98
53.90
61.41
58.11
64.65
73.22
54.34
68.31

R@2
72.96
63.78
65.70
66.76
72.51
70.64
76.20
82.42
66.72
78.21

R@4
80.86
73.52
76.01
77.75
81.75
80.27
84.23
86.36
77.22
85.22

R@8
87.37
82.41
84.27
86.35
88.39
87.81
90.19
88.68
85.17
91.18

Table 5. Clustering and Recall performance on the CAR196 dataset.

improve training efﬁciency. However, this is out of scope of
this paper, but we consider this to be future work.

CUB-200-2011

CAR196

[48]
53.6
73.7

[22]
55.3
78.0

[44]
54.7
71.4

[5]
52.7
75.1

[16]
52.7
82.0

Discrim.

51.4
68.3

Training time complexity. To demonstrate the efﬁciency
of the proposed method, we also compare the empirical
training time of the proposed discriminative loss to other
triplet-based methods, i.e., Semi-hard [27] and triplet with
smart mining [9]. All methods were tested on the same ma-
chine and we use the default conﬁgurations of [27] and [9].

Table 6. R@1 comparison to the state of the art on CUB-200-
2011 and CAR196 datasets. Although all these methods relied
on the triplet loss, they also use additional techniques speciﬁcally
designed to boost classiﬁcation accuracy.

10410

Semi-hard [27]

Triplet

Discrim.

CUB-200-2011

CAR196

660
1200

+smart mining [9]

680
1240

54
73

Table 7. Training time in minutes between different methods. The
CUB-200-2011 dataset consists of 5864 training images with 100
classes, and the CAR196 dataset consists of 8054 training images
with 98 classes.

CUB-200-2011

CAR196

NMI
61.49
62.14

R@1
57.74
78.15

R@2
68.46
85.70

R@4
78.07
90.71

R@8
85.40
94.21

posed discriminative loss is based on an upper bound to
the triplet loss, and we theoretically show that this bound
is tight depending on the distribution of class centroids. We
propose two methods to generate class centroids that en-
force that their distribution guarantees the tightness of the
bound. The experiments on two benchmark datasets show
that in terms of retrieval accuracy, the proposed method is
competitive while its training time is one order of magni-
tude faster than triplet-based methods. Consequently, this
paper proposes the most efﬁcient DML approach in the
ﬁeld, with competitive DML retrieval performance.

Table 8. Clustering and recall performance when using VGG-16
network with discriminative loss.

Acknowledgments

The results in Table 7 show that the training time of the
proposed methods (Discrim.)
is around 13 and 17 times
faster than the recent state-of-the-art triplet with smart
mining [9] on CUB and CAR196 datasets, respectively.
The results also conﬁrm that our loss scales linearly w.r.t.
number of training images and number of classes,
i.e.,
(5864 × 100)/(8054 × 98) ≈ 54/73.
4.5. Improving with Different Network Architec 

tures

As presented in Section 3.3, the proposed loss is simple
and it is easy to integrate into any deep learning models. To
prove the ﬂexibility of the proposed loss, in this section we
experiment with the VGG-16 network [31]. Speciﬁcally,
we apply a max-pooling on the last convolutional layer of
VGG to produce a 512-D feature representation. After that,
similarly to GoogleNet in Section 4.2, we add two fully con-
nected layers whose dimensions are 256 and C. The outputs
of the second to last layer are used as embedding features.
Table 8 presents the results when using our discriminative
loss with VGG network. From Tables 4, 5 and 8, we can
see that using discriminative loss with VGG network sig-
niﬁcantly boosts the performance on both datasets, e.g., at
R@1, it improves over GoogleNet 6.3% and 9.8% for CUB-
200-2011 and CAR196, respectively.

We note that using other advance network architec-
tures such as Inception [37], ResNet [10] rather than
GoogleNet [36], VGG [31], may give performance boost
as showed in recent works [6, 17, 47]. However, that is not
the focus of this paper. Our work targets on developing a
linear complexity loss that approximates the triplet loss but
offers faster training process with a similar accuracy to the
triplet loss.

5. Conclusion

This work was partially supported by the Australian Re-

search Council project (DP180103232).

Appendix: Proof for the Lemma 3.1

Proof. The lower bound, i.e., 0 ≤ Ld(T ,S) − Lt(T ,S) is
straightforward by (5). Here we prove the upper bound.
By the assumption, for any xi and its centroid cyi we

have

kxi − cyik ≤ ǫ/2

(10)

By using the triangle inequality and (10), for any xi and the
centroids cyi , cyk where cyk 6= cyi , we have
kxi − cykk ≥ kcyi − cykk − kxi − cyik ≥ κmin − ǫ/2
(11)
From (6), (10), (11), for any triplet (xi, xj, xk) we have

ℓd ≤ −κmin + 2ǫ

(12)

By using the norm property and (10), for any pair of xi

and xk that are not same class, we have

kxi − xkk = kxi − cyi + cyi − cyk + cyk − xkk

(13)

≤ κmax + ǫ

From (2), (13), for any triplet (xi, xj, xk) we have

− ℓt ≤ κmax + ǫ

(14)

The upper bound for Ld − Lt is achieved by adding (14)
and (12) over all possible triplets.

References

[1] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and
S. Carlsson. From generic to speciﬁc deep representations
for visual recognition. In CVPR Workshops, 2015. 2, 5, 6

[2] A. Bendale and T. Boult. Towards open world recognition.

In CVPR, 2015. 1

In this paper we propose the ﬁrst deep distance metric
learning method that approximates the triplet loss and is
guaranteed to have linear training complexity. Our pro-

[3] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 2

10411

[4] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and
T. Brox. Discriminative unsupervised feature learning with
convolutional neural networks. In NIPS, 2014. 1

[5] Y. Duan, W. Zheng, X. Lin, J. Lu, and J. Zhou. Deep adver-

sarial metric learning. In CVPR, 2018. 6, 7

[6] W. Ge. Deep metric learning with hierarchical triplet loss. In

ECCV, 2018. 8

[7] S. Guerriero, B. Caputo, and T. Mensink. Deepncm: Deep
nearest class mean classiﬁers. In ICLR Workshop, 2018. 1,
2, 3, 5

[8] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.
Matchnet: Unifying feature and metric learning for patch-
based matching. In CVPR, 2015. 1

[9] B. Harwood, B. G. V. Kumar, G. Carneiro, I. D. Reid, and
T. Drummond. Smart mining for deep metric learning. In
ICCV, 2017. 1, 2, 3, 5, 6, 7, 8

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 8

[11] H. Huang, P. M. Pardalos, and Z. Shen. A point balance
algorithm for the spherical code problem. Journal of Global
Optimization, 19(4):329–344, 2001. 4

[12] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In ICCV Work-
shops, 2013. 5

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 2, 6

Imagenet
In

[14] B. G. V. Kumar, G. Carneiro, and I. Reid. Learning local im-
age descriptors with deep siamese and triplet convolutional
networks by minimising global loss functions.
In CVPR,
2016. 1, 2, 3, 5, 6, 7

[15] P. Leopardi. Distributing points on the sphere: Partitions,
separation, quadrature and energy. PhD thesis, School of
Mathematics and Statistics, the University of New South
Wales, 2006. 4

[16] X. Lin, Y. Duan, Q. Dong, J. Lu, and J. Zhou. Deep varia-

tional metric learning. In ECCV, 2018. 6, 7

[17] R. Manmatha, C. Wu, A. J. Smola, and P. Kr¨ahenb¨uhl. Sam-
pling matters in deep embedding learning. In ICCV, 2017.
8

[18] C. D. Manning, P. Raghavan, and H. Sch¨utze. Introduction
to Information Retrieval. Cambridge University Press, 2008.
5

[19] G. Marsaglia. Choosing a point from the surface of a sphere.
The Annals of Mathematical Statistics, 43(2):645–646, 1972.
5

[20] J. Masci, D. Migliore, M. M. Bronstein, and J. Schmidhuber.
Descriptor learning for omnidirectional image matching. In
Registration and Recognition in Images and Videos, pages
49–62. Springer, 2014. 1

[21] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and
S. Singh. No fuss distance metric learning using proxies. In
ICCV, 2017. 3, 5, 6, 7

[23] M. Perrot and A. Habrard. Regressive virtual metric learning.

In NIPS, 2015. 3

[24] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual cat-
egorization via multi-stage metric learning. In CVPR, 2015.
2

[25] F. Radenovic, G. Tolias, and O. Chum. CNN image retrieval
learns from bow: Unsupervised ﬁne-tuning with hard exam-
ples. In ECCV, 2016. 2

[26] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. CNN features off-the-shelf: An astounding baseline for
recognition. In CVPR Workshops, 2014. 2

[27] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
2015. 2, 3, 5, 6, 7, 8

[28] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
CVPR, 2016. 1

[29] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and
F. Moreno-Noguer. Discriminative learning of deep convo-
lutional feature point descriptors. In ICCV, 2015. 1, 2

[30] K. Simonyan, A. Vedaldi, and A. Zisserman. Learning local
feature descriptors using convex optimisation. TPAMI, 2014.
1

[31] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. CoRR, 2014. 2,
8

[32] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks

for few-shot learning. In NIPS, 2017. 1, 2, 3, 5

[33] K. Sohn.

Improved deep metric learning with multi-class

n-pair loss objective. In NIPS, 2016. 3, 5, 6, 7

[34] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep
metric learning via facility location. In CVPR, 2017. 3, 5, 6,
7

[35] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 1, 5, 6, 7

[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 5, 8

[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 8

[38] P. M. L. Tammes. On the origin of number and arrangements
of the places of exit on the surface of pollen-grains. Recueil
des Travaux Botaniques N´eerlandais, pages 1–84, 1930. 4

[39] S. Thrun and L. Pratt. Learning to learn. Springer Science

& Business Media, 2012. 1

[40] E. Ustinova and V. S. Lempitsky. Learning deep embeddings

with histogram loss. In NIPS, 2016. 1, 2, 3, 6, 7

[41] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.

The Caltech-UCSD birds-200-2011 dataset. 2011. 5

[42] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille. Normface: L2
hypersphere embedding for face veriﬁcation. In ACM MM,
2017. 1, 2, 3, 5

[22] M. Opitz, G. Waltner, H. Possegger, and H. Bischof. Bier-
boosting independent embeddings robustly. In ICCV, 2017.
3, 6, 7

[43] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained im-
age similarity with deep ranking. In CVPR, 2014. 2

10412

[44] J. Wang, F. Zhou, S. Wen, X. Liu, and Y. Lin. Deep metric

learning with angular loss. In ICCV, 2017. 3, 6, 7

[45] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016. 1, 2, 3, 5

[46] P. Wohlhart and V. Lepetit. Learning descriptors for object

recognition and 3d pose estimation. In CVPR, 2015. 1

[47] H. Xuan, R. Souvenir, and R. Pless. Deep randomized en-

sembles for metric learning. In ECCV, 2018. 8

[48] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cas-

caded embedding. In ICCV, 2017. 6, 7

[49] S. Zagoruyko and N. Komodakis. Learning to compare im-
In CVPR,

age patches via convolutional neural networks.
2015. 1

10413

