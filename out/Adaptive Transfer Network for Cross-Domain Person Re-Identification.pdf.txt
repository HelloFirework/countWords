Adaptive Transfer Network for Cross-Domain Person Re-Identiﬁcation

Jiawei Liu1, Zheng-Jun Zha1*, Di Chen1, Richang Hong2, Meng Wang2

1University of Science and Technology of China, China

2HeFei University of Technology, China

{ljw368,cdrom000}@mail.ustc.edu.cn, zhazj@ustc.edu.cn, {hongrc,wangmeng}@hfut.edu.cn

Abstract

Recent deep learning based person re-identiﬁcation
approaches have steadily improved the performance for
benchmarks, however they often fail to generalize well
from one domain to another.
In this work, we propose a
novel adaptive transfer network (ATNet) for effective cross-
domain person re-identiﬁcation. ATNet looks into the es-
sential causes of domain gap and addresses it following
the principle of “divide-and-conquer”. It decomposes the
complicated cross-domain transfer into a set of factor-wise
sub-transfers, each of which concentrates on style trans-
fer with respect to a certain imaging factor, e.g., illumina-
tion, resolution and camera view etc. An adaptive ensem-
ble strategy is proposed to fuse factor-wise transfers by per-
ceiving the affect magnitudes of various factors on images.
Such “decomposition-and-ensemble” strategy gives ATNet
the capability of precise style transfer at factor level and
eventually effective transfer across domains. In particular,
ATNet consists of a transfer network composed by multi-
ple factor-wise CycleGANs and an ensemble CycleGAN as
well as a selection network that infers the affects of differ-
ent factors on transferring each image. Extensive experi-
mental results on three widely-used datasets, i.e., Market-
1501, DukeMTMC-reID and PRID2011 have demonstrated
the effectiveness of the proposed ATNet with signiﬁcant per-
formance improvements over state-of-the-art methods.

1. Introduction

Person re-identiﬁcation is the task of matching a probe
pedestrian image from a large-scale gallery collected by
non-overlapping camera networks at diverse locations [17,
35, 18]. It has been widely investigated due to its impor-
tance for many practical applications, such as automated
surveillance, content-based retrieval and behavior analysis
etc. [20, 38, 43, 36]. Recently, deep learning technique has
been applied for person re-identiﬁcation, leading to steady

* Corresponding author.

Figure 1. Illustration of the domain disparity among Market1501,
DukeMTMC-reID and PRID2011 benchmarks, presenting signif-
icant variances in illumination, resolution and camera viewpoint
etc.

performance improvement on popular benchmarks [34, 41].

Despite remarkable progress on person re-identiﬁcation
[26, 31, 11],
it still remains a challenging task due to
dramatic variances in imaging device, condition and envi-
ronment among different surveillance cameras/camera net-
works. In practice, visual appearance of a pedestrian ob-
served by different cameras at various locations and times
varies drastically due to different camera conﬁguration,
lighting condition and viewing angles etc. This results in in-
tensive disparities across pedestrian image galleries known
as the challenge of domain gap in literatures [23, 16], hin-
dering the application of exiting person re-identiﬁcation
systems. Existing re-identiﬁcation models trained on one
domain often fail to generalize well to another and suffer
from severe performance drop. For example, GoogleNet
[29] trained on Market-1501 dataset achieves rank-1 recog-
nition rate of only 5.0% on PRID2011. Figure 1 illustrates
the domain disparity among three popular benchmarks for
person re-identiﬁcation. They are collected at different
places (e.g., supermarket, campus and street) and present

17202

signiﬁcant variances in illumination, resolution and camera
viewpoint, etc.

A promising solution for bridging domain gap is un-
supervised domain adaptation (UDA), which is a class of
techniques aiming to use a source domain with labeled
samples to learn a classiﬁer with good capability on a un-
labeled target domain. Typical UDA approaches assume
that the source and target domain contain the same set of
classes. Hence, they could not applied directly to person
re-identiﬁcation task as different re-identiﬁcation datasets
consist of entirely different pedestrian identities (classes).
Recently, a few of UDA approaches tailored for person re-
identiﬁcation [2, 45, 46, 33, 3] have bee proposed upon
the domain translation model CycleGAN [48]. These ap-
proaches encompass two phases typically. First, pedestrian
images labeled with identities from a source domain are
transferred into the style of a target domain, while preserv-
ing pedestrian identities. Second, the style-transferred im-
ages with labels are used to train a re-identiﬁcation model
for the target domain. These approaches treat the domain
gap as a “black box” and attempt to tackle it resorting to
a single style transformer. Actually, inter-domain dispar-
ities arise from the variations in multiple essential factors
(e.g.
illumination, resolution and camera viewpoint) dur-
ing imaging process [22]. Even for each different image,
the factors may hold different impacts on its imaging, lead-
ing to various cases of discrepancy across domains. Such
complexity of domain discrepancy mixed with various fac-
tors challenge existing approaches, resulting in suboptimal
performance.

In this work, we propose a novel Adaptive Transfer
Network (ATNet) for effective cross-domain person re-
identiﬁcation. The ATNet looks into the “black box” of
domain gap and proposes to address it following the prin-
ciple of “divide-and-conquer”. To the best of our knowl-
edge, this work is the ﬁrst one that looks into the essen-
tial factors of domain gap. It decomposes the complicated
cross-domain transfer into a set of intermediate sub-tasks,
each of which concentrates on style transfer at ﬁne-grained
level with respect to a certain factor. The sub-transformers
are optimized jointly and assembled together to address the
domain discrepancy. The ensemble of sub-transformers is
self-adaptive to each image according to the affects of dif-
ferent factors. This gives ATNet the capability to trans-
fer styles precisely with the perception of factor-wise af-
fects. In particular, the proposed ATNet is built upon Cy-
cleGAN [48]. As illustrated in Figure 2, it consists of a
transfer network composed by multiple factor GANs and
an ensemble GAN as well as a selection network. Each fac-
tor GAN concentrates on transferring images to the target
style of a certain imaging factor. The illumination, res-
olution and camera-view are three critical factors of do-
main disparity and are investigated in this work. It is note-

worthy that ATNet is ﬂexible to incorporate transfer mod-
ules of other factors. The ensemble GAN is designed to
fuse the factor GANs adaptively towards painting precise
style-transferred images. The selection network is to in-
fer the affects of different factors on transferring each im-
age, representing as sample-wise affect magnitudes which
are used for the adaptive ensemble of factor GANs. We
conduct extensive experiments to evaluate ATNet on three
widely-used person re-identiﬁcation datasets, i.e., Market-
1501 [42], DukeMTMC-reID [44] and PRID2011 [9], and
report superior performance over state-of-the-art methods.
The main contributions of this paper are three-fold: (1)
we propose a novel adaptive transfer network (ATNet) for
effective cross-domain person re-identiﬁcation following
the principle of “divide-and-conquer” ; (2) we propose a
ﬂexible network architecture consisting of multiple factor
GANs and an ensemble GAN. While the former performs
factor-wise style transfer at more ﬁne-grained level across
domains, the latter synergizes factor GANs for effective do-
main transfer; (3) we propose an sample-wise adaptive en-
semble of factor GANs by inferring the affects of various
imaging factors on images.

2. Related Work

This work is closely related with unsupervised domain
adaptation and feature learning in person re-identiﬁcation.
We will brieﬂy summarize these two aspects of works.

2.1. Unsupervised Domain Adaptation

The proposed work relates to unsupervised domain adap-
tation (UDA) where images in the target domain are un-
labeled.
In the UDA community, most of the previous
methods [25, 6, 5, 27, 28, 37, 32] attempt to align the
source domain to the target domain by reducing the di-
vergence of feature distributions. These methods assume
that class labels are the same across domains, while dif-
ferent re-identiﬁcation datasets contain different person IDs
(classes). Thus, these approaches can not be applied di-
rectly for person re-identiﬁcation.

Recently, a few cycle generative adversarial Networks
(CycleGAN) [48, 1, 7] based UDA approaches [2, 45, 46,
33, 3] are proposed for person re-identiﬁcation, which fo-
cus on learning a generator network that transforms sam-
ples in the pixel space from one domain to another. For
example, Deng et al.
[3] proposed a similarity preserv-
ing generative adversarial network (SPGAN) which pre-
served self-similarity of an image before and after transla-
tion, and domain-dissimilarity of a translated source image
and a target image. Zhun et al. [45] introduced a Hetero-
Homogeneous Learning (HHL) model, which enforced
camera invariance, learned by positive pairs formed by un-
labeled target images and their camera style transferred im-
ages, and domain connectedness, by regarding source / tar-

7203

Figure 2. The overall architecture of the proposed ATNet approach. It consists of a transfer network for precise factor-wise style transfers
and adaptive ensemble of them as well as a selection network for inferring affect magnitude of various imaging factors (e.g., illumination,
resolution and camera view) on images.

get images as negative matching pairs. Slawomir et al. [2]
proposed a three-step domain adaptation technique, which
translated the Synthetic Person Re-Identiﬁcation dataset to
the target conditions by employing cycle-consistent adver-
sarial networks. Wei et al. [33] proposed a Person Transfer
Generative Adversarial Network (PTGAN) for bridging do-
main gap, which introduced a identity loss and a style loss
to keep the identity of pedestrians and ensure the transferred
images with similar style of target domain during transfer.
Zhong et al. [46] proposed a camera style (CamStyle) adap-
tation method with a label smooth regularization (LSR) for
person re-identiﬁcation, which can serve as a data augmen-
tation approach that smooths the camera style disparities
and alleviate the impact of noise caused by the new gen-
erated samples.

2.2. Feature Learning

Deep learning based methods [47, 14, 26, 15, 40] for
feature extraction have shown substantial advantage over
traditional hard-crafted features on most of person re-
identiﬁcation datasets. For example, Xiao et al. [34] pre-
sented a pipeline for learning global full-body representa-
tions from multiple domains by a Domain Guided Dropout
layer to discard useless neurons for each domain. Liu et al.
[19] proposed a multi-scale triplet CNN which captures vi-
sual appearance of a person at various scales by a compara-

tive similarity loss on massive sample triplets. McLaughlin
et al. [21] presented a recurrent neural network architecture
for video-based person re-identiﬁcation, which utilized op-
tical ﬂow, recurrent layers and mean-pooling layer to learn
video features containing appearance and motion informa-
tion. Li et al. [14] formulated a method of jointly learning
local and global features in a CNN model by optimizing
multiple classiﬁcation losses in different context.

3. The Proposed Method

In this section, we ﬁrst present the overall architecture of

the proposed ATNet and then elaborate its components.

3.1. Problem Formulation

Given an annotated dataset S from source domain and
an unlabeled dataset T from target domain for person re-
identiﬁcation, the goal of unsupervised domain adaptation
is to use the labeled source images to train a re-identiﬁcation
model that generalizes well to the unlabeled dataset on tar-
get domain. Considering that data bias caused by differ-
ent inﬂuence factors Θ, we require a transfer model G(·)
to translate the annotated dataset S from source domain to
target domain, and learn effective generalized features of
pedestrian with the new created dataset G(S; w; Θ). The
unsupervised domain adaptation problem can be formulated

7204

as:

arg min

Djs(PT (y)kPG(x; w; Θ))

w

(1)

where Djs denotes the Jensen-Shannon divergence between
two distributions, PT denotes the distribution of the tar-
get domain over data y, PG denotes the distribution of the
transfer model over data x from source domain S. w and Θ
refer to the parameters of the transfer model and the factors
(illumination, resolution, camera viewpoint, etc).

To learn an effective transfer model, we look into the
“black box” of domain gap and address it following the
principle of “divide-and-conquer”. The complicated cross-
domain transfer are decomposed into a set of factor-wise
sub-transformers, each of which concentrates on style trans-
fer at ﬁne-grained level with respect to a certain factor,
which are then assembled together to address the domain
discrepancy. Moreover, the factors may hold different im-
pacts on imaging process, the sub-transformers should be
self-adaptive to each image according to the impacts of dif-
ferent factors for transferring style precisely. Thus, we pro-
pose a novel ATNet for effective cross-domain person re-
identiﬁcation. As shown in Figure 2, the ATNet consists of
a transfer network containing multiple factor GANs and an
ensemble GAN, and a selection network. Each factor GAN
focuses on transferring images to the target style of a cer-
tain imaging factor. The ensemble GAN is designed to fuse
the factor GANs adaptively towards painting precise style-
transferred images. The selection network is to infer the
weight scores of different factors on transferring each im-
age, representing as sample-wise affect magnitudes which
are used for the adaptive ensemble of factor GANs. After
that, following the works [3, 33], we adopt the ResNet-50
[8] and GoogleNet [29] models as baseline to evaluate the
performance on the target domain.

3.2. Transfer Network

Inter-domain disparities arise from the variations in mul-
tiple essential factors during image processing. The transfer
network decomposes the complicated cross-domain transfer
into a set of factor-wise sub-transformers, each of which fo-
cuses on style transfer with respect to a certain factor. The
proposed framework is generic and ﬂexible to include sub-
transfers of other factors. We select illumination, resolution
and camera view in this work as they are common and crit-
ical factors as discussed in literature. It optimizes the sub-
transformers for these factors jointly and assembles them
together to address the domain discrepancy. Moreover, the
ensemble of sub-transformers is self-adaptive to each image
based on the affects of different factors for generating more
realistic images with a similar style of target domain.

Speciﬁcally,

the transfer network contains three fac-
tors GANs and a emsemble GAN. They are all based

on the CycleGAN model, which contains two generator-
discriminator pairs, {G, DT } and {F, DS }, producing a
translated sample that is indistinguishable from samples in
the other domain. The two generators G : S → T and
F : T → S are the mapping functions. The two ad-
versarial discriminators DT ,DS are used to distinguish
whether samples are translated from source (target) domain.
For simpliﬁcation, we only consider that mapping a sample
from source domain S to target domain T and ignore the
reverse process. Similar to [30], the overall loss of the four
GANs for image-to-image translation is expressed as:

Lgan = Ladv + λ1 · Lcyc + λ2 · Lide

(2)

where Ladv is used for matching the distribution of trans-
lated images to the data distribution in the target domain,
Lcyc attempts to recover the original sample after a cycle
of translation and reverse translation, and Lide encourages
the style transfer to keep the color consistency between the
original sample and translated sample.

Different from the original CycleGAN model with the
adversarial loss, cycle-consistent loss and identity mapping
loss, the three factor GANs are elaborately designed to con-
centrate on transferring images to the target style of the
imaging factors, i.e., illumination, resolution and camera
viewpoint. On the one hand, the three factor GANs are
pre-trained on the pair of datasets whose inter-domain dif-
ference are mainly induced by the three factors respectively
to provide a good initialization. For pre-training illumina-
tion GAN, a collection of images with different illumination
conditions is created by utilizing random gamma correction
[24] in source domain. The created collection together with
souce dataset are used for pre-training. For pre-training the
resolution GAN, we downsample images in source domain
to create a collection of images with multiple resolutions.
For pre-training camera-view GAN, we use images from
any two different cameras in source domain for pre-training.
All created images will not be used in subsequent end-to-
end training procedure of the network. On the other hand,
an illumination constraint and a resolution constraint are in-
troduced to the illumination GAN and resolution GAN re-
spectively, for further guaranteeing that the style difference
between original images and translated images focuses on
the variations of illumination and resolution. The formula-
tion of the illumination constraint is shown as follows:

Lill(G, F, H) = E

x∼p(x)[kH(G(x)) − H(x)k1]

(3)

where H(·) denotes abstracting illumination insensitive fea-
tures [39]. This constraint is able to enforce the style con-
sistency between the original image and translated image
except the illumination condition. Thus, the ﬁnal overall
loss of the illumination GAN is: Lgan + η1 · Lill. The for-
mulation of the resolution constraint is shown as follows:

7205

Lres(G, F, I) = E

x∼p(x)[kI(G(x)) − I(x)k2
2]

(4)

where I(·) denotes extracting resolution-insensitive fea-
tures [13]. This constraint keeps the style consistency ex-
cept the resolution variation. The ﬁnal overall loss of the
resolution GAN is: Lgan + η2 · Lres. Moreover, the losses
of the three factor GANs reﬂect the degree of style dispar-
ity between the translated samples and the target samples,
which can be viewed as the different impacts of the three
factors that result in the domain gap. If the associated loss
is smaller, the factor is more critical for the domain gap.
Therefore, the weight scores of the three factors are the
reciprocal of the associated losses. Then the three weight
scores β = (β1, β2, β3) are normalized by a softmax func-
tion, and are used for the emsemble GAN.

The emsemble GAN takes the adaptive fused image fea-

ture zx as input, which is computed by:

zx = [β1 · z

1

x; β2 · z

2

x; β3 · z

3

x], zx ∈ R64×64×768

(5)

3

2

1

x, z

x, z

x ∈ R64×64×256 refer to the image features
where z
extracted from the associated encoders of the three factor
GANs. Afterwards, the fused image feature are send to a
convolution layer with 1 × 1 × 256 ﬁlters and a decoder
to generate the ﬁnal translated image. The emsemble GAN
also has a discriminator to distinguish whether the sample is
real or fake. Moreover, a Jensen-Shannon divergence con-
straint is added to the image features z
x for enforc-
ing the learned features possessing different semantic infor-
mation, which is formulated as follows:

x, z

x, z

1

2

3

Ljs(z

1

x, z

2

x, z

3

x) = f (z

1

x, z

2

x) + f (z

1

x, z

3

x) + f (z

2

x, z

3

x)
(6)

3

2

1

x, z

x, and z

where f denotes the reciprocal of Jensen-Shannon diver-
x are the
gence between two distributions, z
normalized image features by a softmax function. The over-
all loss of the emsemble GAN is (Lgan + η3 · Ljs), which
is used to optimize the parameters of the transfer network.
The emsemble GAN and the three factor GANs have the
similar architecture, in which the generators contain 9 resid-
ual blocks [8] and four convolution layers, while the dis-
criminators are 70 × 70 PatchGANs [10]. More details can
be founded in [48]. The decoders and the discriminators in
the emsemble GAN and the three factor GANs share same
parameters.

3.3. Selection Network

The selection network is developed to infer the weight
scores of different factors β = (β1, β2, β3) on transferring

each image, representing as sample-wise affect magnitudes
which are used for the adaptive ensemble of factor GANs.
We use the selection network to infer β. This allows ATNet
to avoid going through the process of generating fake im-
ages and calculating the losses during testing, thus greatly
reduces computational cost. The selection network con-
tains four convolution layers and one fully connected layer.
Speciﬁcally, the kernel size of the four convolution layers is
4 × 4 × 64, 4 × 4 × 128, 4 × 4 × 256, 4 × 4 × 256, respec-
tively, the padding and the stride of these layers are 1 and
2. Each convolution layer is followed by a batch normaliza-
tion (BN) and a rectiﬁed linear unit (ReLU) layer. The last
fully connected layer has 6 hidden units. The output fea-
ture of the fully connected layer represents the three weight
scores of a pair of images, which are then passed through
two softmax operations, respectively. In the training stage,
the selection network takes a pair of images from a source
domain and a target domain as input, the weight scores of
the pair of images calculated from the transfer network is
viewed as the ground-truth. We optimize the selection net-
work with MSE loss. In the testing stage, the output weight
scores of the selection network is provided to the emsemble
GAN for generating the ﬁnal style-transferred image.

3.4. Feature Learning

Once we obtain the style-transferred dataset G(S),
which is composed of the translated images with the asso-
ciated labels, the feature learning step is the same as super-
vised person re-identiﬁcation methods. Since we mainly
focus on the step of source-target image translation, we
adopt the ResNet-50 and GoogleNet models as baseline,
following the works [3, 33]. During testing, we can extract
the 2048-dim pedestrian feature from ResNet-50 model and
4096-dim pedestrian feature from GoogleNet model for re-
trieval under the Euclidean distance, and test the perfor-
mance on the target domain.

4. Experiments

In this section, we conduct extensive experiments to
evaluate the performance of the proposed ATNet on three
widely used person re-identiﬁcation benchmarks and com-
pare the ATNet to state-of-the-art methods. The experimen-
tal results show that ATNet achieves superior performance
of UDA in person re-identiﬁcation over the state-of-the-art
methods. Moreover, we investigate the effectiveness of the
proposed ATNet including the three factor GANs and the
emsemble GAN.

4.1. Experimental Settings

Datasets - In this work, extensive experiments are con-
ducted on three widely used datasets, i.e, Market-1501,
DukeMTMC-reID and PRID2011 for fair comparison and
evaluation.

7206

The Market-1501 dataset contains 32,643 images of
1,501 identities captured by 6 cameras. All images are au-
tomatically detected by the Deformable Part Model (DPM)
detector [4]. The dataset is ﬁxedly divided into two parts
respectively, one part contains 12,936 images of 750 identi-
ties as training set and the other contains 19,732 images of
751 identities as testing set.

The DukeMTMC-reID dataset contains 36,411 hand-
drawn bounding boxes of 1,812 identities from 8 high-
resolution cameras. It is ﬁxedly divided into two parts re-
spectively, one part contains 16,522 images of 702 identities
as training set and the other contains 17,661 gallery images
of 702 identities as testing set. In addition, there are 2,228
query pedestrian images.

The PRID2011 dataset

is captured from two static
surveillance camera views. Camera view A contains 385
persons, camera view B contains 749 persons, with 200 of
them appearing in both views. Therefore, there are 200 per-
son image pairs in the dataset. These image pairs are ran-
domly split into a training and a testing set of equal size.

Evaluation Metrics - Evaluation Metrics Cumulative
Matching Characteristic (CMC) is adopted for quantitative
evaluation of person re-identiﬁcation. The rank-k recogni-
tion rate in the CMC curve indicates the probability that a
query identity appears in the top-k position. The other eval-
uation metric is the mean average precision (mAP), consid-
ering person re-identiﬁcation as a retrieval task.

Implementation Details - The implementation of the
proposed method is based on the Pytorch framework with
eight NVIDIA Titan XP GPUs. Images in the three datasets
are resized to 256 × 256 × 3, the number of mini-batches is
8. The proposed architecture is optimized by 20,000 itera-
tions in each epoch, and 20 epochs in total. For the transfer
network, we adopt the Adam optimizer [12] with a learning
rate of 0.0002. The learning rate remains unchanged for the
ﬁrst 10 epochs and linearly decay to zero over the last 10
epochs. The parameters λ1, λ2, η1, η2, η3 are set to 10, 5, 2,
1, 1, respectively. The three factor GANs are pre-trained on
the source dataset and the generated dataset with the varia-
tions of the three factors (illumination, resolution and cam-
era viewpoint), the emsemble GAN is trained from scratch.
For the selection network, the stochastic gradient descent
(SGD) algorithm is started with learning rate lr of 0.01, the
weight decay of 1e−5 and the Nesterov momentum of 0.9.

4.2. Comparison to State of the Arts

Transfer from large dataset to large dataset. Ta-
ble 1 shows the performance comparison of the proposed
ATNet against 5 methods in terms of CMC accuracy and
mAP on the large target datasets (DukeMTMC-reID and
Market-1501). We employ ResNet-50 model as the baseline
for feature learning, following the work [3]. When tested
on DukeMTMC-reID, Market-1501 is used as the source

Figure 3. Examples of original images and their style-transferred
images after image-to-image translation. (Best viewed in color)

dataset, and vice versa. “Supervised learning” denotes us-
ing labeled training sets from the target datasets.
“Di-
rect Transfer” means directly applying the source-trained
model to the target datasets. CycleGAN(based), CycleGAN
(base+ Lide) and SPGAN are the state-of-the-art methods.
When comparing the supervised learning method and the
direct transfer method (66.7% vs 33.1%, 75.8% vs 43.1%),
it can be observed that a large performance drop when us-
ing the direct transfer method on the target domain, due to
the bias of data distributions in different domains. When
tested on DukeMTMC-reID, the proposed ATNet achieves
45.1% rank-1 recognition rate and 24.9% mAP score. We
can see that our method improves the 2nd best compared
method SPGAN by 3.7% rank-1 recognition rate and 2.6%
mAP score. When tested on Market-1501, the proposed AT-
Net achieves 55.7% rank-1 recognition rate and 25.6% mAP
score. It can be observed that our method improves the 2nd
best compared method SPGAN by 4.2% rank-1 recognition
rate and 2.8% mAP score. The comparison indicates that
the effectiveness of the proposed ATNet to generate more
realistic translated images for bridging domain gap. An il-
lustration of some generated results is given in Figure 3.

Transfer from large dataset to small dataset. Table 2
shows the performance comparison of the proposed ATNet
against 3 methods in terms of CMC accuracy on the small
target dataset (PRID2011). We employ GoogleNet model
as the baseline for feature learning, following the work [33].
Market-1501 and PRID2011 are used as the source dataset

7207

Method

Market-1501→DukeMTMC-reID

DukeMTMC-reID→Market-1501

rank-1

rank-5

rank-10

rank-20 mAP

rank-1

rank-5

rank-10

rank-20 mAP

Supervised Learning

Direct Transfer

CycleGAN (base) [48]

CycleGAN (base+ Lide) [48]

SPGAN [3]

ATNet

66.7

33.1

38.1

38.5

41.4

45.1

79.1

49.3

54.4

54.6

56.6

59.5

83.8

55.6

60.5

60.8

63.0

64.2

88.7

61.9

65.9

66.6

69.6

70.1

46.3

16.7

19.6

19.9

22.3

24.9

75.8

43.1

45.6

48.1

51.5

55.7

89.6

60.8

63.8

66.2

70.1

73.2

92.8

68.1

71.3

72.7

76.8

79.4

95.4

74.7

77.8

80.1

82.4

84.5

52.2

17.0

19.1

20.7

22.8

25.6

Table 1. Performance comparison to the state-of-the-art methods in terms of rank-k recognition rate and mAP scores on DukeMTMC-reID
and Market-1501 datasets, respectively.

Market-1501→PRID2011

Method

cam1/cam2

cam2/cam1

Rank-1

Rank-10

Rank-1

Rank-10

Supervised

Direct Transfer

PTGAN(cam1)[33]
PTGAN(cam2) [33]

ATNet(cam1)
ATNet(cam2)

13.0

5.0
17.5
10.0
24.0
15.0

43.0

26.0
50.5
31.5
51.5
51.0

11.0

11.0
8.5
10.5
21.5
14.0

38.5

40.0
28.5
37.5
46.5
41.5

Table 2. Performance comparison to the state-of-the-art methods
in terms of rank-k recognition rate on PRID2011 dataset.

Method

ResGAN

CamGAN

illumGAN

ATNet w/o illumGAN

ATNet w/o CamGAN

ATNet w/o ResGAN

ATNet w/o adaptive

ATNet

Market-1501→DukeMTMC-reID

Rank-1

Rank-5

Rank-20 mAP

37.9

38.1

39.8

41.2

42.1

43.3

42.6

45.1

53.9

53.8

54.3

55.5

55.6

57.8

56.6

59.5

64.0

63.9

65.2

66.4

66.2

68.8

67.5

70.1

21.3

21.4

21.7

22.9

23.1

23.7

23.4

24.9

Table 3. Evaluation of the effectiveness of each component within
ATNet on DukeMTMC-reID dataset.

and the target dataset, respectively. The subscript cam1 and
cam2 represent the transferred target dataset PRID-cam1
and PRID-cam2. “cam1/cam2” means using samples in
PRID-cam1 as query set and samples from PRID-cam2
as gallery set, and vice versa. “Supervised learning” de-
notes using labeled training set of the target dataset. “Direct
Transfer” means directly applying the source-trained model
to the PRID2011 datasets. PTGAN is the state-of-the-art
method. GoogLeNet trained on the Marker-1501 dataset,
only achieves the Rank-1 accuracy of 5.0% on PRID2011,
which implies substantial domain gap between Market-
1501 and PRID2011. When transfered on PRID-cam1, the
proposed ATNet achieves 24.0% and 21.5% rank-1 recog-

nition rate for PRID-cam1 and PRID-cam2 as query set,
respectively.
It can observed that our method improves
the compared method PTGAN by 6.5% and 13.0% rank-1
recognition rate, respectively. When transfered on PRID-
cam2, the proposed ATNet obtains 15.0% and 14.0% rank-
1 recognition rate for PRID-cam1 and PRID-cam2 as query
set, respectively, boosting the compared method PTGAN
by 5.0% and 3.5% rank-1 recognition rate, respectively.
The comparison indicates that the effectiveness of the pro-
posed ATNet and it can achieve reasonable re-identiﬁcation
performance on PRID2011 dataset, training on the other
dataset. An illustration of some generated results is given
in Figure 3.

4.3. Ablation Studies

To demonstrate the effectiveness and contribution of
each component of the ATNet, we conduct a series of
ablation experiments on DukeMTMC-reID dataset, using
Market-1501 dataset as the source domain.

The impact of the proposed three factor GANs. We
conduct the experiment to verify the inﬂuence of the three
factor GANs on performance in Table 3. ATNet w/o il-
lumGAN, ATNet w/o CamGAN and ATNet w/o ResGAN
refer to the ATNet model without the illumination GAN,
the camera viewpoint GAN and resolution GAN, respec-
tively. These models achieve 41.2%, 42.1% and 43.3%
rank-1 recognition rate, as well as 22.9%, 23.1%, 23.7%
mAP score, respectively. From Table 3, we can observe
that their performances are inferior to the ATNet, which
shows the effectiveness of ATNet by incorporating the phys-
ical priors into UDA and utilizing the multiple factor GANs
to decompose the complicated problem of bridging domain
gap into handling the inter-domain the discrepancy caused
by different factors. Moreover, by comparing the perfor-
mance of the three models, it shows that the illumination
GAN is the most important network branch to bridge do-
main gap.

The impact of the proposed emsemble GAN. We also
conduct the experiment to verify the effectiveness of the

7208

emsemble GAN with the adaptive ensemble strategy in Ta-
ble 3. ResGAN, CamGAN and illumGAN donates only us-
ing the individual resolution GAN, the camera viewpoint
GAN and resolution illumination GAN for UDA. ATNet
w/o adaptive refers to the ATNet without the adaptive en-
semble strategy (β1 = β2 = β3 = 1/3). From table 3,
it can observed that the ATNet w/o adaptive obtains better
performance of 42.6% rank-1 recognition rate and 23.4%
mAP scores as compared to the other three models, which
indicates that the effectiveness of the ensemble GAN for
handling the inter-domain discrepancy caused by multiply
factors over one factor GAN for one factor. Moreover, the
performance of the ATNet w/o adaptive model is inferior to
the ATNet, demonstrating that the effectiveness of the adap-
tive ensemble strategy based on the different weight scores
of the three factors, since the factors may hold different im-
pacts on imaging process for each different sample. We also
show some generated results from the three factor GANs
with their associated weight scores in Figure 4. The image
style of the translated images from the three factor GANs
are different, as compared to the source images. We can see
that the images from the illumination GAN slant dark, the
images from the resolution GAN is more ambiguous, which
show the factor GANs is able to handle the inter-domain
differences caused by the different factors. By comparing
the weight scores of the factors, it can be the observed that
illumination condition are dominant for the domain gap.

Figure 4. Visual examples of image-to-image translation in
Market-1501 with the weight scores. The images in the ﬁrst
column are from Market-1501. The images in the middle three
columns are the translated images from the illumination, camera
viewpoint and resolution GANs. The images in the last column is
the ﬁnal generated images. (Best viewed in color)

Sensitivity of ATNet to key parameters. The param-
eters η1, η2 and η3 are key parameters for the proposed
ATNet, which controls the relative importance of the pro-
posed illumination constraint, resolution constraint and the
Jensen-Shannon divergence constraint, respectively. We
conduct experiment to evaluate the impact of η1, η2, η3 re-

spectively, and the results are shown in Figure 5. When
adjusting the value of one parameter, the other parameters
are ﬁxed. From Figure 5, we can see that when η1 = 2,
η2 = 1, η3 = 1, the ATNet yields the best re-identiﬁcation
performance, which is superior to the ATNet without the
three additional constraints (η1 = 0, η2 = 0, η3 = 0). This
comparison veriﬁes the effectiveness of the proposed ATNet
by using the three additional constraints.

Figure 5. Evaluation of the proposed ATNet with different values
of parameter η1, η2, η3.

5. Conclusion

In this paper, we have addressed the cross-domain per-
son re-identiﬁcation problem by proposing a novel adaptive
transfer network (ATNet), which looks into the essential
imaging factors that engender dramatic inter-domain dis-
crepancy. We proposed a “decomposition-and-ensemble”
solution to tackle the complicated cross-domain transfer.
ATNet was designed to contain multiple factor GANs, an
ensemble GAN and a selection network. While each fac-
tor GAN concentrates on factor-wise precise style transfer
at ﬁne-grained level, the ensemble GAN adaptively fuses
the factor GANs for effective domain transfer. The factor
and ensemble GANs are jointly optimized in an end-to-end
manner. The selection network was developed to perceive
the affects of various factors on transferring different im-
ages to the target domain. Extensive experiments on multi-
ple benchmarks have shown that the proposed ATNet out-
performs state-of-the-art methods by a large margin.

Acknowledgement

This work was supported by the National Key R&D Pro-
gram of China under Grant 2017YFB1300201, the National
Natural Science Foundation of China (NSFC) under Grants
61622211 and 61620106009 as well as the Fundamental
Research Funds for the Central Universities under Grant
WK2100100030.

References

[1] Cycada: Cycle consistent adversarial domain adaptation. In
Proceedings of the 35th International Conference on Ma-

7209

chine Learning, pages 1989–1998, 2018.

[2] S. Bak, P. Carr, and J.-F. Lalonde. Domain adaptation
through synthesis for unsupervised person re-identiﬁcation.
In Proceedings of the European Conference on Computer Vi-
sion, September 2018.

[3] W. Deng, L. Zheng, G. Kang, Y. Yang, Q. Ye, and
J. Jiao. Image-image domain adaptation with preserved self-
similarity and domain-dissimilarity for person reidentiﬁca-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 6–16, 2018.

[4] P. Felzenszwalb, D. McAllester, and D. Ramanan. A dis-
criminatively trained, multiscale, deformable part model. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1–8. IEEE, 2008.

[5] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In Proceedings of the IEEE International Conference
on Computer Vision, pages 2960–2967, 2013.

[6] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow
kernel for unsupervised domain adaptation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2066–2073, 2012.

[7] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets.
In Proceedings of the Inter-
national Conference on Neural Information Processing Sys-
tems, pages 2672–2680, 2014.

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[9] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof. Per-
son re-identiﬁcation by descriptive and discriminative classi-
ﬁcation. In Proceedings of the Scandinavian Conference on
Image Analysis, pages 91–102. Springer, 2011.

[10] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks.
In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 2500–2510, 2017.

[11] M. M. Kalayeh, E. Basaran, M. G¨okmen, M. E. Kamasak,
and M. Shah. Human semantic parsing for person re-
identiﬁcation.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1062–
1071, 2018.

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proceedings of the International Conference
on Learning Representations, 2015.

[13] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a gener-
ative adversarial network. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4–14, 2017.

[14] W. Li, X. Zhu, and S. Gong. Person re-identiﬁcation by deep
joint learning of multi-loss classiﬁcation. In Proceeding of
the International Joint Conference on Artiﬁcial Intelligence,
pages 2194–2200, 2017.

[15] W. Li, X. Zhu, and S. Gong. Harmonious attention network
for person re-identiﬁcation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2–12, 2018.

[16] Y.-J. Li, F.-E. Yang, Y.-C. Liu, Y.-Y. Yeh, X. Du, and Y.-
C. F. Wang. Adaptation and re-identiﬁcation network: An
unsupervised deep transfer learning approach to person re-
identiﬁcation.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[17] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identiﬁcation
by local maximal occurrence representation and metric
learning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2197–2206,
2015.

[18] G. Lisanti, I. Masi, and A. Del Bimbo. Matching people
across camera views using kernel canonical correlation anal-
ysis. In Proceedings of the International Conference on Dis-
tributed Smart Cameras, page 10. ACM, 2014.

[19] J. Liu, Z.-J. Zha, Q. Tian, D. Liu, T. Yao, Q. Ling, and T. Mei.
Multi-scale triplet cnn for person re-identiﬁcation. In Pro-
ceedings of the ACM Conference on Multimedia Conference,
pages 192–196. ACM, 2016.

[20] J. Liu, Z.-J. Zha, H. Xie, Z. Xiong, and Y. Zhang. Ca3net:
Contextual-attentional attribute-appearance network for per-
son re-identiﬁcation. In Proceedings of the ACM Conference
on Multimedia Conference, pages 737–745. ACM, 2018.

[21] N. McLaughlin, J. Martinez del Rincon, and P. Miller. Re-
current convolutional network for video-based person re-
identiﬁcation.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1325–1334,
2016.

[22] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do-
main adaptation: A survey of recent advances. IEEE signal
processing magazine, 32(3):53–69, 2015.

[23] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang,
and Y. Tian. Unsupervised cross-dataset transfer learning for
person re-identiﬁcation.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1306–1315, 2016.

[24] E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley. Color
transfer between images. IEEE Computer Graphics and Ap-
plications, 21(5):34–41, 2001.

[25] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In Proceedings of the
European Conference on Computer Vision, pages 213–226,
2010.

[26] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, and
G. Wang. Dual attention matching network for context-aware
feature sequence based person re-identiﬁcation. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1249–1258, 2018.

[27] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy
domain adaptation. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, pages 8–18, 2016.

[28] B. Sun and K. Saenko. Deep coral: Correlation alignment
for deep domain adaptation. In Proceedings of the European
Conference on Computer Vision, pages 443–450, 2016.

7210

[43] W.-S. Zheng, S. Gong, and T. Xiang. Person re-identiﬁcation
by probabilistic relative distance comparison.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 649–656. IEEE, 2011.

[44] Z. Zheng, L. Zheng, and Y. Yang. Unlabeled samples gener-
ated by gan improve the person re-identiﬁcation baseline in
vitro. In Proceedings of the IEEE International Conference
on Computer Vision, 2017.

[45] Z. Zhong, L. Zheng, S. Li, and Y. Yang. Generalizing a
person retrieval model hetero- and homogeneously. In Pro-
ceedings of the European Conference on Computer Vision,
September 2018.

[46] Z. Zhong, L. Zheng, Z. Zheng, S. Li, and Y. Yang. Cam-
era style adaptation for person re-identiﬁcation. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5157–5166, 2018.

[47] S. Zhou, J. Wang, J. Wang, Y. Gong, and N. Zheng. Point to
set similarity based deep feature learning for person reiden-
tiﬁcation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, volume 6, 2017.

[48] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In Proceedings of the IEEE International Conference
on Computer Vision, pages 2242–2251, 2017.

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015.

[30] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-
domain image generation. arXiv preprint arXiv:1611.02200,
2016.

[31] R. R. Varior, M. Haloi, and G. Wang. Gated siamese
convolutional neural network architecture for human re-
identiﬁcation. In Proceedings of the European Conference
on Computer Vision, pages 791–808. Springer, 2016.

[32] J. Wang, X. Zhu, S. Gong, and W. Li. Transferable joint
attribute-identity deep learning for unsupervised person re-
identiﬁcation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[33] L. Wei, S. Zhang, W. Gao, and Q. Tian. Person transfer gan

to bridge domain gap for person re-identiﬁcation.

[34] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep
feature representations with domain guided dropout for per-
son re-identiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1249–
1258, 2016.

[35] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li. Salient
color names for person re-identiﬁcation. In Proceedings of
the European Conference on Computer Vision, pages 536–
551. Springer, 2014.

[36] J. You, A. Wu, X. Li, and W.-S. Zheng. Top-push video-
based person re-identiﬁcation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1345–1353, 2016.

[37] H.-X. Yu, A. Wu, and W.-S. Zheng. Cross-view asymmetric
metric learning for unsupervised person re-identiﬁcation. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 994–1002, 2017.

[38] L. Zhang, T. Xiang, and S. Gong. Learning a discriminative
null space for person re-identiﬁcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1239–1248, 2016.

[39] T. Zhang, Y. Y. Tang, B. Fang, Z. Shang, and X. Liu. Face
recognition under varying illumination using gradientfaces.
IEEE Transactions on Image Processing, 18(11):2599–2606,
2009.

[40] L. Zhao, X. Li, Y. Zhuang, and J. Wang. Deeply-learned
part-aligned representations for person re-identiﬁcation. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 3239–3248, 2017.

[41] K. Zheng, X. Fan, Y. Lin, H. Guo, H. Yu, D. Guo, and
S. Wang. Learning view-invariant features for person iden-
tiﬁcation in temporally synchronized videos taken by wear-
able cameras. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 2877–2885. IEEE, 2017.

[42] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.
Scalable person re-identiﬁcation: A benchmark. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1116–1124, 2015.

7211

