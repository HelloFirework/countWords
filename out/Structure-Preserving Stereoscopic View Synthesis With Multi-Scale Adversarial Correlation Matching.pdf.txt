Structure-Preserving Stereoscopic View Synthesis with

Multi-Scale Adversarial Correlation Matching

Yu Zhang1

,

2, Dongqing Zou1 ∗, Jimmy S. Ren1, Zhe Jiang1, Xiaohao Chen1

1SenseTime Research 2Tsinghua University

{zhangyu1,zoudongqing,rensijie,jiangzhe,chenxiaohao}@sensetime.com

Abstract

;LH HWDO

This paper addresses stereoscopic view synthesis from a
single image. Various recent works solve this task by reor-
ganizing pixels from the input view to reconstruct the target
one in a stereo setup. However, purely depending on such
photometric-based reconstruction process, the network may
produce structurally inconsistent results.

Regarding this issue, this work proposes Multi-Scale Ad-
versarial Correlation Matching (MS-ACM), a novel learn-
ing framework for structure-aware view synthesis. The pro-
posed framework does not assume any costly supervision
signal of scene structures such as depth. Instead, it mod-
els structures as self-correlation coefﬁcients extracted from
multi-scale feature maps in transformed spaces. In train-
ing, the feature space attempts to push the correlation dis-
tances between the synthesized and target images far apart,
thus amplifying inconsistent structures. At the same time,
the view synthesis network minimizes such correlation dis-
tances by ﬁxing mistakes it makes. With such adversarial
training, structural errors of different scales and levels are
iteratively discovered and reduced, preserving both global
layouts and ﬁne-grained details. Extensive experiments on
the KITTI benchmark show that MS-ACM improves both
visual quality and the metrics over existing methods when
plugged into recent view synthesis architectures.

1. Introduction

3D display is becoming universal nowadays. Automatic
conversion of the rich 2D images and videos to 3D is now
a demand that can beneﬁt various industrial ﬁelds. To ful-
ﬁll this demand, binocular views are rendered to form the
stereoscopic format for an input scene, while only one of
them is known beforehand. Such single-image based view
synthesis problem, however, is still challenging.

In its early research, view synthesis is often based on
at least two known views (or continuous video sequences),

∗Correspondence should be addressed to zoudonqging@sensetime.com

1LNODXVHWDO

*RGDUGHWDO

06$&0

*URXQG7UXWK

Figure 1. Structure preservation for view synthesis. Photomet-
ric losses commonly adopted by existing approaches (e.g., Xie et
al. [39], Niklaus et al. [21] and Godard et al. [9]) often lead to
blurred and distorted structures, which is more severe for thin, un-
salient objects. The proposed MS-ACM addresses this limitation
via a novel adversarial training process that accounts for both large
and ﬁne-grained structures. Best viewed in color with zoom.

so that the 3D scene geometry is well-deﬁned [30, 38, 14].
For a single input view, the gap of 3D understanding is
ﬁlled very recently by the strong statistical modeling power
of deep learning. Among these methods, 3D view trans-
formations are formulated as 2D warping ﬁelds (e.g. pixel
ﬂows [42, 13, 23], spatially-variant kernels [39, 21], or ho-
mographies [15]), which guide the target view to “copy”
pixels from the input image. Photometric reconstruction er-
rors across views are usually adopted to supervise this pro-
cess in training. However, as such loss functions optimize

5860

color consistency in average statistics, structure degenera-
tion often happens as blurred, distorted details.
It harms
especially the objects from the “minority”, e.g., the small
and thin poles with ambiguous appearance shown in Fig. 1.
To maintain structural consistency during view synthe-
sis, various methods leverage explicit supervisions from the
3D world in addition to the photometric consistency.
It
ﬁnds forms of scene depths/normals [15, 43], multi-view
inputs [7, 13, 33], and 3D correspondences from CAD mod-
els [32, 23, 27]. Despite the rich 3D information, either of
these is costly and difﬁcult to obtain. Moreover, 3D super-
vision is only restricted to a small number of scene/object
types, limiting the model’s applicability in the wild.

In this paper we propose Multi-Scale Adversarial Corre-
lation Matching (MS-ACM), a novel approach for learning
stereoscopic view synthesis. MS-ACM learns the structural
priors directly from data, instead of assuming any costly
form of 3D supervisions. In the proposed approach, a struc-
ture critic network is appended to the view synthesis one,
which transforms the synthesized and target views into la-
tent feature spaces for structure matching. Each feature
location computes normalized correlations within its sur-
rounding window, whose responses serve as surrogates of
local structural conﬁgurations. By training the critic net-
work to maximize the distances of correlation coefﬁcients
between the synthesized and target views, it learns to am-
plify any structural mistakes it sees. This in turn guides the
view synthesis network to correct its mistakes by asking it
to minimize the same distance. Such adversarial training is
performed on multi-scale feature maps, so as to be aware of
both coarse-level and ﬁne-grained structures. To avoid get-
ting to bad minima, novel strategies are proposed to make
the critic network adapted to high-level structures and ro-
bust to subtle noise. We show the effectiveness of MSACM
by plugging it into two recent representative view synthesis
architectures [39, 21]. Extensive results on the challenging
KITTI benchmark [8] demonstrate that MS-ACM improves
visual quality as well as quantitative metrics.

This paper makes the following contributions:

1) We propose a novel adversarial training framework for
structure-preserving stereoscopic view synthesis. It is
friendly to various existing view synthesis models, im-
proving both their performance and generalizability.

2) Correlation based structure representation is proposed
for adversarial training, which effectively captures
scene structures at different scales. Various strategies
are presented to avoid bad local minima as well.

2. Related Works

Rendering novel viewpoints of a given scene was solved
with multi-view geometry for more than two decades. Per-

forming this task with a single image, however, is relatively
new. This section brieﬂy reviews these related approaches.

Multiple-view based synthesis assumes the input scene
is given from multiple known viewpoints. Rich physical 3D
scene structure is provided in this manner, such that corre-
spondences across views can be explicitly established. This
idea arises since 90’s [19, 30, 1]. Later works improved
this pipeline by proposing stronger 3D scene representa-
tions [35, 24], better occlusion handling models [17, 5] and
more powerful texture transfer techniques [25, 37]. Besides
static scene modeling, view synthesis in videos was also
extensively explored to facilitate stabilization tasks [14, 3].
Recent deep learning methods propose to learn direct multi-
to-novel view synthesis functions [7, 33, 22, 20]. Although
multi-view inputs provide more comprehensive understand-
ing of the 3D structure, it does not ﬁt many applications,
especially those based on a single view.

Single-view based synthesis, on the other hand, gener-
ates novel views based on only a single image. Various ap-
proaches ﬁrst infer the scene geometry (e.g. depths and nor-
mals [15, 43], then synthesize target views with geometry-
grounded view transformations. CAD models as another
form of geometrical signal for object-level novel view syn-
thesis [27, 23, 32, 41]. However, while scene depth/normal
is costly to collect, CAD models are limited to object cate-
gories and provide little knowledge to scene understanding.

On the other hand, several works advocate a self-taught
learning process that directly reorganizes pixels from the
input image to match the target one [42, 39, 34], without
depending on explicit geometrical supervisions. The ratio-
nale behind is that the collective power of massive training
data provides regularizations on the learned view transfor-
mations. Similar idea has also been explored for other tasks,
including depth estimation [9] and visual tracking [36].
However, usually the only training signals are average pho-
tometric errors. Such errors focus on preserving the struc-
tures of majority cases but may neglect uncommon scenar-
ios, leading to over-smoothed details distortion.

Structure regularization with adversarial training
has been explored recently on image segmentation [18, 40,
11]. In these works, the network outputs and groundtruth
segmentations are fed into a shared structure analysis net-
work, which is adversarially trained to exaggerate predic-
tion errors. The proposed idea is inspired from this line
of works, but has two novel aspects. First, we process high-
dimensional signals (i.e. the synthesized images), instead of
low-dimensional segmentation maps. Novel strategies are
introduced to stabilize training and get rid of bad local opti-
mum. Second, rather than training on feature-space ℓ1 dis-
tances, we propose to adopt feature correlations as the struc-
ture surrogate. In this manner, the network is encouraged to
discover high-level edges in the scene, allowing structure-
related mid-level representations to be more easily learned.

5861

3. The Proposed Approach

3.1. Adversarial Correlation Matching

Before delving into our view synthesis framework, we
ﬁrst introduce Adversarial Correlation Matching (ACM), a
novel adversarial training process for structure-aware learn-
ing. The proposed framework consists of a structure predic-
tor P and a critic network S. The predictor takes an input x
and generates a structured output y, i.e. y = P (x; wP ),
controlled by model parameters wP . For example,
in
stereoscopic view synthesis the input is a left-view image,
and the output is its right view. The structure critic network
S takes responsibility of transforming y into a latent fea-
ture space for structure analysis, i.e. f = S (y; wS ). We
assume that f takes the form of convolutional feature maps
with spatial information preserved. For a spatial location p,
its feature is accessed by f (p).

In this learned feature space, ACM models structure as
mutual correlations among different spatial locations. More
speciﬁcally, for each location p, its local structure conﬁg-
uration is represented by the feature cosine distances com-
puted with its spatial neighbours:

c (p) = vec

kf (p)k2 kf (q)k2)q∈Nk(p)
( f (p)T f (q)


 ,

(1)

where Nk (p) is the set of neighbour locations of p within
a k-sized spatial window, and k·k2 denotes the ℓ2 norm.
The vec (·) operation reorganizes input values into a vec-
tor. With the structure representation of the synthesized im-
age c, we can now quantize errors with that of groundtruth.
To this end, groundtruth of y, denoted by yg, is fed into
the same S and produces structure representations cg. The
structural error is thus measured by

ds(cid:0)y, yg(cid:1) =

1

|P|Xp∈P

kc (p) − cg (p)k1 ,

(2)

i.e. the average ℓ1 distance over all the feature locations P.
For simplicity, we refer (2) to the corr-ℓ1 distance.

In adversarial training, the structure critic network S pur-
sues a feature space that best distinguishes between y and
yg by maximizing (2). Meanwhile, the prediction network
P attempts to produce structured output y that can mini-
mize it. In this manner, it is expected that any structural
difference can be ampliﬁed during training, which in turn
provides sufﬁcient signals to supervise predictor training.
In the following, we provide several remarks on ACM.
Link to self-similarity. The proposed approach corre-
lates with the concept of self-similarity for visual match-
ing established before a decade [31]. Self-similarity assigns
each image location a descriptor that characterizes its local
layout patterns, computed by comparing a template window

with a larger search region around the same location. In this
manner, per-image textures are ﬁltered out and only struc-
tural conﬁgurations are kept, making the matching process
robust. Our structure representation (1) ﬁts this idea and can
be considered as normalized correlations between a size-1
template and a search window.

1

|P|Pp∈P kf (p) − f g (p)k1.

Intuitions behind corr-ℓ1 distance. Previous works ad-
vocate using feature ℓ1 distance for adversarial structure
learning [40, 11], i.e.
Intu-
itively, corr-ℓ1 loss explicitly models local structural pat-
terns, which should mitigate the difﬁculty of encoding
structures directly into features. By computing cosine sim-
ilarities among features, only feature-level “edges” are pre-
served while impact of other factors is reduced. This would
save a great power of network capacities in learning tex-
tures, brightness, etc., that are irrelevant to scene structures.
Another shortage of ℓ1 loss, when applied for adversarial
training, is its sensitiveness to the magnitude of features.
It says that when S maximizes feature distance, it tends to
scale the feature magnitudes up and make training unstable,
as recognized in both [40] and [11]. Weight clipping was
adopted to prevent this issue, introducing difﬁculty in pa-
rameter tuning and limiting the model’s capacity. Instead,
corr-ℓ1 is a bounded, magnitude-insensitive loss. Thus, the
network does not need to scale up features to conform the
training objective. Recent ﬁndings also support this claim
and show its positive effect for stabilizing training [16].

3.2. Getting Rid of Bad Minima

Discriminator in adversarial networks easily gets stuck
into bad local minima when trained on high dimensional
signals [26]. There is no exception for ACM as in tasks
like view synthesis, the structure critic network operates on
color images. We address this issue as follows.

Introducing robustness to noise. The prediction y and
groundtruth y0 often have an inherent distribution gap de-
pending on the generation process of the predictor P. For
example, the synthesized pixels of the predicted view are
usually more correlated than those in groundtruth, due to
the interpolation or warping operations during view synthe-
sis. They can also differ in lighting and textures caused by
camera len settings and the data capture environment. If the
critic network notices them, it pushes the predictions and
groundtruths into bad modes far away in feature space, and
contributes nothing to learning.

In training GANs, such distribution gap problem was ac-
tively studied and a working trick is Instance Noise [2]. We
adapt this idea into ACM as follows. When training S, we
add random noises into the groundtruth yg to generate yn,
and feed it into S to get the structure representation cn. We
ask S to learn noise resistant features, by constraining cn to

5862

Figure 2. The proposed framework for stereoscopic view synthesis. The view synthesis network predicts the synthesized view of the
input image, which is fed into the structure critic network along with its groundtruth to produce mutli-scale feature maps. Meanwhile, a
noisy version of the groundtruth image goes through the same procedure. During training, the view synthesis network minimizes the pixel
ℓ1 distance, the ℓ1 and corr-ℓ1 distances of extracted feature maps between the synthesized image and groundtruth. The structure critic
network maximizes the same corr-ℓ1 distance, while minimizing it between the groundtruth and its noisy transform. At the same time, the
extracted feature maps reconstruct the inputs with a regularization network jointly trained with the critic. Best viewed in color.

be close to cg. It equals to minimizing

In summary, the ACM training objective for C is

dn(cid:0)yg, yn(cid:1) =

1

|P|Xp∈P

kcg (p) − cn (p)k1 .

(3)

max
wC ,wR

In this manner, predictor/dataset-speciﬁc characteristics are
broken by noise, forcing S aware of the real image content.
Making features content-aligned. Although in princi-
ple S ﬁnds any differences between two images, it is better
to make learned features align with the inputs. This idea
was originally proposed by Hwang et al. [11], which facil-
itates the network to learn good structure basis more effec-
tively. To this end, a structure regularization network R is
appended behind S, which consumes its output features and
reconstructs the input image. Networks R and S are jointly
trained, minimizing the ℓ1 reconstruction loss

dr(cid:0)y, yg(cid:1) = ky − R (c; wR)k1 +(cid:13)(cid:13)yg − R (cg; wR)(cid:13)(cid:13)1

.
(4)
Closing the gap of feature scaling. Since corr-ℓ1 is in-
sensitive of feature magnitudes, there exists a potential risk
Imagine that S pushs the predictions and
of overﬁtting.
groundtruths into different feature spaces with their own
scale of magnitude, but correlation values are still the same.
If this happens, optimizing structure distance in two differ-
ent feature spaces may generate unpredictable results. To
prevent this from happening, we train the predictor P to
pursue the feature space of groundtruth:

df(cid:0)y, yg(cid:1) =

1

|P|Xp∈P

kf (p) − f g (p)k1 .

(5)

LC(cid:0)y, yg, yn(cid:1) = − λnds(cid:0)yn, yg(cid:1)

−

λr
2

dr(cid:0)y, yg(cid:1) + ds(cid:0)y, yg(cid:1) ,

(6)

where λn and λr are positive weights. For P, the training
objective is deﬁned by

min

wP

LP(cid:0)y, yg(cid:1) = ds(cid:0)y, yg(cid:1) + df(cid:0)y, yg(cid:1) .

In the rest of this section, we show how ACM is instan-

(7)

tiated in solving stereoscopic view synthesis.

3.3. View Synthesis with Multi Scale ACM

The proposed training framework for stereoscopic view
synthesis is summarized in Fig. 2. In this framework, the
view synthesis network takes a left view as input and reorga-
nizes its pixels to generate a predicted right view. The pre-
dicted view, groundtruth, and a noisy version of groundtruth
are fed into the critic network for structure analysis. During
testing, only the view synthesis network is kept and other
parts are discarded.

The view synthesis network can be implemented with
various existing architectures [42, 39, 21]. It is trained with
the ℓ1 photometric reconstruction loss as well as the ACM
loss (7). The structure critic network S and regularization
network R from a encoder-decoder structure, for which we

5863

adopt U-Net [28]. It consists of three downsampling stages,
and three upsampling ones. Each downsampling stage has
two convolution layers interleaved with Leaky ReLU non-
linearity. Average pooling is applied after each stage. As
such, the structure critical network actually provides feature
maps of three scales. We perform ACM at each scale to
capture structures at different granularities. We refer this
extended version of ACM to Multi-Scale ACM (MS-ACM).
The training algorithm. Following the practice of train-
ing GANs [10], we alternate updating P and S till conver-
gence. At each training step, the groundtruth is transformed
by three types of noises: additive Gaussian noise, Gaussian
blur and random pixel shifts, as well as their combinations.
For random pixel shifting, we generate a small local random
offset ﬁeld at all pixel locations, and apply bilinear warp-
ing [12, 44]. The strength of noise is decayed overtime.
In this manner, we expect S to focus on high-level coarse
structures and neglect other details at ﬁrst to avoid bad min-
ima. We summarize the training algorithm in Alg. 1.

Algorithm 1 Training algorithm of MS-ACM for stereo-
scopic view synthesis.
Require: training set: left views X, and right views Yg

repeat

i=1

1. Sample a batch(cid:8)x(i)(cid:9)m
2. Get predictions y(i) = P(cid:0)x(i); wP(cid:1), and generate

g om
i=1 ∈ X,ny(i)

n , i ∈ {1, 2, · · · , m};

∈ Yg;

noisy groundtruth y(i)
g , c(i)
3. Compute feature correlations c(i), c(i)
4. Update S, R by ascending their gradients:
∇wS ,wR
5. Update P by descending its gradients:
∇wP
6. (Optionally) decay learning rate and noise;

i=1 LC(cid:16)y(i), y(i)
mPm
g (cid:13)(cid:13)(cid:13)
i=1(cid:16)(cid:13)(cid:13)(cid:13)
mPm

+ LP(cid:16)y(i), y(i)

y(i) − y(i)

n (cid:17);

until maximum training iteration is reached.

g , y(i)

g (cid:17)(cid:17);

n by (1);

1

1

4. Experiments

4.1. Experimental Settings

Dataset and evaluation metrics. To benchmark exist-
ing approaches for stereoscopic view synthesis, we set up
experiments on the challenging KITTI dataset [8]. The raw-
form KITTI contains a total of 42382 rectiﬁed stereo pairs
captured from 61 scenes. We benchmark models on the 400
pairs provided as the ofﬁcial training set in KITTI’s 2015
challenge. These images span across 28 scenes, which are
excluded and the rest 33 ones are kept for training, result-
ing into 34071 training pairs in total. The Eigen split [6]
is also included in evaluation. It provides a test split cover-
ing 697 pairs from 29 scenes, and suggests training with the
23488 pairs sampled from the rest 32 scenes. Across this

section, these two splits will be referred to KITTI-Raw and
KITTI-Eigen, respectively.

We follow previous works on view synthesis [15, 42]
and adopt Root Mean Square Deviation (RMSE), Peak
Signal-to-noise Ratio (PSNR) and Structure Similarity In-
dex (SSIM) as evaluation metrics. As this work aims to
improve the quality of structures, we also perform evalua-
tions in gradient space. Speciﬁcally, the metrics Grad. x
and Grad. y measure the mean squared errors between the
gradients of the synthesized and groundtruth images in hor-
izontal and vertical directions, respectively.

Baselines. We integrate MSACM into two recent rep-
resentative architectures, Deep3D [39] and SepConv [22].
SepConv is originally designed for video frame interpo-
lation, which requires two frames as input. We tailor it
for stereoscopic view synthesis by removing one image in-
put and keeping other layers ﬁxed. We choose these two
baselines for their concise designs and strong performance.
However, it should be noted that the proposed approach is
general and not restricted to certain architectures.

Besides Deep3D and SepConv, we also compare with
LRDepth [9]. All these approaches do not assume addi-
tional inputs such as scene depths or multi-view images,
thus are directly comparable. For LRDepth, we make use
of the models released by the authors. As Deep3D and Sep-
Conv do not report results on KITTI or release the training
scripts, we retrain them by integrating the authors’ source
codes into our training framework, as described as follows.
We ensure that our integrations keep their original details of
model deﬁnition that can reproduce their released results.

Implementation details. During training,

the high-
resolution KITTI images are ﬁrstly downsampled by half
at resolution 188 × 621. Patches of size 128 × 256 are ran-
domly cropped on the downsampled images, which form
mini-batches of 8 images. We apply Adam optimizer with
the ﬁrst and second moment decay equal 0.5 and 0.999, re-
spectively. Training lasts for 50 epochs, with a learning rate
10−4 that is exponentially decayed by half every 20 epochs.
In training MS-ACM, noise is decayed every epoch with
exponential factor 0.95. During testing, the image is down-
sampled to a size 188 × 621, on which a 160 × 608 region
is cropped from the top-left corner, to meet the aspect ratio
requirement of baselines.

Throughout the evaluations, the weights λr and λn in (6)
are set to 10, while the window size for computing correla-
tions is set to 3, if not speciﬁcally explained.

4.2. Comparisons with Existing Approaches

Benchmarking results on KITTI. The results are sum-
marized in Table 1. Besides the baselines trained with the
ℓ1 pixel reconstruction loss, we also compare with a vari-
ant trained with multi-scale SSIM, an extensively adopted
structure-aware loss. As the table shows, the proposed ap-

5864

Table 1. Benchmarking results on the KITTI-Raw (left) and KITTI-Eigen (right) datasets. Arrow ↑ (↓) denotes the larger (smaller) number,
the better results. Bold highlights the top place while underline the second.

Models

RMSE ↓

PSNR ↑ Grad. x ↓ Grad. y ↓

SSIM ↑

RMSE ↓

PSNR ↑ Grad. x ↓ Grad. y ↓

SSIM ↑

LRDepth

28.052

19.590

205.124

131.621

0.751

29.868

19.103

203.210

138.895

0.737

Deep3D

19.466
+MS-SSIM 19.520
+MS-ACM 18.062

SepConv

19.556
+MS-SSIM 19.825
+MS-ACM 18.370

22.854
22.790
23.577

22.861
22.709
23.467

137.803
135.494
120.626

141.467
142.557
128.214

81.960
82.256
75.248

83.520
93.204
79.415

0.829
0.833
0.844

0.827
0.832
0.835

22.694
23.017
22.159

23.796
23.801
23.519

21.400
21.295
21.624

21.010
20.987
21.120

162.112
156.849
158.053

174.754
171.366
170.658

111.935
110.052
110.584

119.061
119.858
119.543

0.775
0.782
0.787

0.764
0.766
0.768

/5'HSWK

'HHS'

06$&0

6HS&RQY

06$&0

*7

Figure 3. Qualitative results on the KITTI dataset. In each example, red rectangle marks the regions for comparison.

proach improves over baseline approaches consistently on
nearly all the metrics. On the KITTI-raw dataset, a large
improvement is achieved on the gradient-speciﬁc measures,
illustrating that the proposed approach makes model train-
ing sensitive to scene boundaries.

Besides result comparisons, Table 1 also suggests sev-
eral observations that worth to discuss. First, although MS-
ACM does not apply SSIM as a training loss, it achieves
better SSIM numbers even than training directly with SSIM.
It seems strange at the ﬁrst glance, as the model should de-
vote its capacity to optimizing this speciﬁc metric and it
indeed gets a lower SSIM loss during training. We attribute
this improvement to the stronger generalization ability of
MS-ACM, which leads to better testing behavior.
In the
next subsection, we further demonstrate this point.

Second, although the proposed approach still achieves
the best results on KITTI-Eigen, the gap is closer than that
on KITTI-Raw. We suspect that it is caused by the bias of
dataset sampling. As the distributions of training and testing
data of KITTI-Raw are more different (the sites where the
data are captured do not overlap), it requires the model to
have a better generalization ability. For KITTI-Eigen, on
the contrary, training and testing distributions overlap much
and the improvement is relatively small.

Qualitative results.

In Fig. 3, we show representa-
tive results generated by different approaches. With adver-
sarial training, MS-ACM pays attention to any noticeable
structural differences. As one can see, it preserves object
shapes better, recovers over-smoothed details and success-
fully handles deformation caused by occlusions.
In con-

5865

/

0666,0

06$&0

Figure 4. Visual comparisons between MS-ACM and MS-SSIM.
See text for details.

Table 2. Analyzing different window parameters on KITTI-Raw
dataset. Arrow ↑ (↓) denotes the larger (smaller) number, the better
results. Bold highlights the top place while underline the second.

Multi-Scale? Win. Size RMSE ↓

PSNR ↑

SSIM ↑

✗

✗

✗

✓

✓

✓

3
7
11
3
7
11

20.870
22.124
20.393
18.370
18.500
18.848

22.257
21.660
22.470
23.467
23.371
23.167

0.813
0.773
0.802
0.835
0.829
0.826

trary, the baselines either sacriﬁce the small and thin details
to achieve a better average quality (e.g. Deep3D and Sep-
Conv), or exhibit large distortions due to the errors in dis-
parity estimation (e.g. LRDepth).

Comparisons with SSIM criterion. SSIM is a differ-
entiable structure-aware criterion, thus is widely adopted
for training. Essentially, SSIM optimizes the consistency
of ﬁrst and second-order moments within multi-scale lo-
cal windows between the predicted and groundtruth images.
Such statistical matching, however, renders it not sensitive
to local deformations and small details [29]. As shown
in Fig. 4, although SSIM ﬁxes coarse structural mistakes
but leaves the ﬁne-grained errors unaddressed. As a result,
blurred boundaries and over-smoothed details still happen.
MS-ACM, on the contrary, does not have such limitation.

Visualization of disparities. The Deep3D or SepConv
architectures estimate for each output pixel the likelihoods
that it equals to the input pixels at several ﬁxed horizontal
offsets. The disparities could be thus produced by aggregat-
ing the offsets weighted by the learned likelihoods, which
we show in Fig. 5. As one can see, the disparities trained
with SSIM are more visually smooth, but not accurate along
object boundaries. In contrary, for MS-ACM disparities are

Figure 5. Comparing the learned disparities. For each example, we
show disparities and the synthesized views trained with MS-SSIM
and MS-ACM, respectively.

Table 3. Parameter study on λn and λr.

λn/λr

0.1/0.1

0.1/1

1/0.1

1/10

10/1

10/10

PSNR
SSIM

22.96
0.83

23.06
0.83

22.97
0.84

23.59
0.84

23.62
0.84

23.92
0.85

adapted to scene edges and exhibits sharp depth boundaries.
However, in textureless regions (e.g. road), they are not that
accurate and smooth. Adding smoothness constraint solves
this problem, but is not desired for view synthesis as it may
smooth out object boundaries and cause distortions.

4.3. Performance Analysis

In this section, we conduct extensive experiments to see
how the proposed approach works under various situations.
All the experiments are based on the SepConv baseline.

Parameter analysis. At ﬁrst, we study how different
window sizes impact the proposed approach. We also con-
sider a single-scale variant, where only the deepest scale is
involved for structure matching. From the results in Table 2,
we conclude that multi-scale matching is consistently bene-
ﬁcial, as learning different feature scales enables both local
and global structural mistakes to be ﬁxed. However, larger
window sizes do not necessarily help improve the results.
We suspect that as deep representations already capture suf-
ﬁcient local context, a small window would sufﬁce.

In Table 3, we evaluate different combinations of param-
eters λn and λr in Eqn. (6). We ﬁnd that they both improve
they both improve results as a stable behaviour: as long as
they are large enough (i.e. λr, λn ≥ 1), the ﬁnal results are
not very sensitive to them.

Ablation study of design choices. In the second exper-
iment, we show empirically the necessity of several impor-

5866

Table 4. Ablation study of design choices on the KITTI 2015 split. Arrow ↑ (↓) denotes the larger (smaller) number, the better results. Bold
highlights the top place while underline the second.

Loss

Noise?

Feat. Reg.?

Self Recon.? RMSE ↓

PSNR ↑ Grad. x ↓ Grad. y ↓

SSIM ↑

Corr-ℓ1
Corr-ℓ1
Corr-ℓ1
Corr-ℓ1

ℓ1

✗

✓

✓

✓

✓

✗

✗

✓

✓

✓

✗

✗

✗

✓

✓

44.662
19.558
19.280
18.370
18.921

15.272
22.841
22.961
23.461
23.111

386.909
141.227
137.666
128.214
132.578

338.504
87.518
86.353
79.415
85.043

0.491
0.819
0.825
0.835
0.819

*7

ZRVHOIUHFRQ

ZRQRLVH

)HDWƐ



ZRIHDW5HJ

&RUUƐ



Figure 7. The features learned by the structure critic network, vi-
sualized by PCA projection.

Figure 6. Studying different components of the proposed approach
by visual comparisons. See text for details.

tant design choices. The numbers are reported in Table 4,
and a visual comparison is provided in Fig. 6. Without en-
forcing noise resistance (w/o noise), the model simply does
not learn much. The structure critic network notices the
inherent distribution differences between the synthesized
and real input, thus the view synthesis network tends to
copy the input to make them look real. After adding noise
(w/o feat. reg.), trainings succeeds, but details are missing.
Feature regularization (w/o self recon.)
improves the de-
tails, but does not address overall distortion. Incorporating
self-reconstruction (corr-ℓ1) helps a lot by learning features
tightly correlated with the spatial context of the scene.

We also replace the corr-ℓ1 loss with the standard fea-
ture ℓ1 loss for adversarial training, and it gets worse per-
formance. We believe that explicit modeling of structures in
MS-ACM eases the difﬁculty of encoding them with feature
learning. As shown in Fig. 6, ℓ1 loss does not learn the thin
structure although equipped with the same other strategies.
Generalizability to unseen dataset. As mentioned pre-
viously, we believe that an advantage of MS-ACM is its bet-
ter generalizability over classic metrics. The intuition is that
adversarial training provides easy-to-hard dynamic training
signals, which may prevent the model from continuously
optimizing a ﬁxed objective and getting overﬁtting. To illus-
trate this point, we evaluate the model trained on KITTI-raw
dataset to the test set of Cityscapes benchmark [4], without
further ﬁnetuning. The input image is resized to resolution
192 × 384, which matches the scale of the trained model. In
Table 5, it shows that while MS-SSIM does not apparently
improves over the baseline, MS-ACM signiﬁcantly boosts

Table 5. Model generalizability on the Cityscapes test set. Arrow
↑ (↓) denotes the larger (smaller) number, the better results. Bold
highlights the top place while underline the second.

Models

SepConv

+MS-SSIM +MS-ACM

RMSE ↓
PSNR ↑
SSIM ↑

19.547
22.620
0.650

19.586
22.603
0.661

17.731
23.465
0.693

the performance in nearly all metrics.

Visualization of learned features. Finally, we visual-
ize the learned features in the structure critic network by
PCA projection, and show them in Fig. 7. As expected, the
ﬁrst scale learns local edges to represent ﬁne-level informa-
tion. From the second scale, the model seems to ﬁlter out
low-level colors and emphasize more on region shapes (see
the marked regions). The third scale, as it shows, captures
more complex structural patterns that the model ﬁnds best
to represent the global layout of the scene.

5. Conclusion

This paper proposes Multi-Scale Adversarial Correlation
Matching for stereoscopic view synthesis. MS-ACM trans-
forms the synthesized results and groundtruths into multi-
scale feature spaces, in which feature correlations are com-
puted as structural representation. By adversarial training
on the distances of such representations, errors of differ-
ent scales are discovered and reduced, enabling structure
preservation at various granularities.

In the future work, we are interested in introducing high-
level cues ( e.g. semantics, object contours) to incorporate
scene-level knowledge for better structure learning.

5867

References

[1] S. Baker, R. Szeliski, and P. Anandan. A layered approach
to stereo reconstruction. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 1998.

[2] C. Kaae Sønderby, J. Caballero, L. Theis, W. Shi, and
F. Husz´ar. Amortised MAP Inference for Image Super-
resolution. ArXiv 1610.04490 [cs.CV], 2016.

[3] C.-H. Chu. Video stabilization for stereoscopic 3d on 3d
mobile devices. In IEEE International Conference on Multi-
media and Expo (ICME), 2014.

[4] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016.

[5] A. Criminisi, A. Blake, C. Rother, J. Shotton, and P. H. S.
Torr. Efﬁcient dense stereo with occlusions for new view-
synthesis by four-state dynamic programming. International
Journal of Computer Vision (IJCV), 71(1):89–110, 2007.

[6] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Processing Systems (NIPS),
2014.

[7] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

[8] A. Geiger, P. Lenz, Christoph Stiller, and Raquel Urtasun.
Vision meets robotics: The kitti dataset. International Jour-
nal of Robotics Research (IJRR), 2013.

[9] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised
monocular depth estimation with left-right consistency.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017.

[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems (NIPS), 2014.

[11] J.-J. Hwang, T.-W. Ke, J. Shi, and S. X. Yu. Adversarial
Structure Matching Loss for Image Segmentation. ArXiv
1805.07457 [cs.CV], 2018.

[12] M. Jaderberg, K. Simonyan, A. Zisserman,

and K.
Kavukcuoglu. Spatial transformer networks. In Advances in
Neural Information Processing Systems (NIPS), pages 2017–
2025, 2015.

[13] D. Ji, J. Kwon, M. McFarland, and S. Savarese. Deep view
morphing. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2017.

[14] F. Liu, M. Gleicher, H. Jin, and A. Agarwala. Content-
preserving warps for 3d video stabilization. ACM Transac-
tions on Graphics (TOG), 28(3), 2009.

[15] M. Liu, X. He, and M. Salzmann. Geometry-aware deep net-
work for single-image novel view synthesis. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018.

[16] C. Luo, J. Zhan, L. Wang, and Q. Yang. Cosine Normal-
ization: Using Cosine Similarity Instead of Dot Product in
Neural Networks. ArXiv 1702.05870 [cs.ML], 2017.

[17] G. Luo, Y. Zhu, Z. Li, and L. Zhang. A hole ﬁlling approach
based on background reconstruction for view synthesis in 3d
video. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

[18] G. M´attyus and R. Urtasun. Matching adversarial networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2018.

[19] L. McMillan and G. Bishop. Plenoptic modeling: an image-
based rendering system. In Annual Conference on Computer
Graphics and Interactive Techniques (SIGGRAPH), 1995.

[20] S. Niklaus and F. Liu. Context-aware synthesis for video
frame interpolation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.

[21] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation
In IEEE International

via adaptive separable convolution.
Conference on Computer Vision (ICCV), 2017.

[22] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation
In IEEE International

via adaptive separable convolution.
Conference on Computer Vision (ICCV), 2017.

[23] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C.
Berg. Transformation-grounded image generation network
for novel 3d view synthesis. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017.

[24] E. Penner and L. Zhang. Soft 3d reconstruction for view syn-
thesis. ACM Transactions on Graphics (TOG), 36(6):235:1–
235:11, 2017.

[25] S. Pujades, F. Devernay, and B. Goldluecke. Bayesian view
synthesis and image-based rendering principles.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2014.

[26] A. Radford, L. Metz, and S. Chintala. Unsupervised Repre-
sentation Learning with Deep Convolutional Generative Ad-
versarial Networks. ArXiv 1511.06434 [cs.ML], 2015.

[27] K. Rematas, C. H. Nguyen, T. Ritschel, M. Fritz, and T.
Tuytelaars. Novel views of objects from a single image.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence (TPAMI), 39(8):1576–1590, 2017.

[28] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In Med-
ical Image Computing and Computer-Assisted Intervention
(MICCAI), 2015.

[29] M. P. Sampat, Z. Wang, S. Gupta, A. C. Bovik, and M. K.
Markey. Complex wavelet structural similarity: A new im-
age similarity index. IEEE Transactions on Image Process-
ing (TIP), 18(11):2385–2401, 2009.

[30] D. Scharstein. Stereo vision for view synthesis.

In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 1996.

[31] E. Shechtman and M. Irani. Matching local self-similarities
across images and videos. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2007.

[32] H. Su, F. Wang, E. Yi, and L. J. Guibas. 3d-assisted feature
synthesis for novel views of an object. In IEEE International
Conference on Computer Vision (ICCV), 2015.

5868

[33] S.-H. Sun, M. Huh, Y.-H. Liao, N. Zhang, and J. J. Lim.
Multi-view to novel view: Synthesizing views with self-
learned conﬁdence. In European Conference on Computer
Vision (ECCV), 2018.

[34] S. Tulsiani, R. Tucker, and N. Snavely. Layer-structured 3d
scene inference via view synthesis. In European Conference
on Computer Vision (ECCV), 2018.

[35] G. Vogiatzis, P. H. S. Torr, and R. Cipolla. Multi-view stereo
via volumetric graph-cuts. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2005.

[36] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and
K. Murphy. Tracking emerges by colorizing videos. In Eu-
ropean Conference on Computer Vision (ECCV), 2018.

[37] O. J. Woodford, I. D. Reid, and A. W. Fitzgibbon. Efﬁ-
cient new-view synthesis using pairwise dictionary priors. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2007.

[38] O. J. Woodford, I. D. Reid, P. H. S. Torr, and A. W. Fitzgib-
In

bon. On new view synthesis using multiview stereo.
British Machine Vision Conference (BMVC), 2007.

[39] J. Xie, R. B. Girshick, and A. Farhadi. Deep3d: Fully au-
tomatic 2d-to-3d video conversion with deep convolutional

neural networks. In European Conference on Computer Vi-
sion (ECCV), 2016.

[40] Y. Xue, T. Xu, H. Zhang, L. R. Long, and X. Huang. Segan:
Adversarial network with multi-scale l1 loss for medical im-
age segmentation. Neuroinformatics, 16(3):383–392, 2018.

[41] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee. Weakly-
supervised disentangling with recurrent transformations for
3d view synthesis. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2015.

[42] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View
synthesis by appearance ﬂow. In European Conference on
Computer Vision (ECCV), 2016.

[43] H. Zhu, H. Su, P. Wang, X. Cao, and R. Yang. View extrapo-
lation of human body from a single image. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018.

[44] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei. Deep feature
ﬂow for video recognition. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

5869

