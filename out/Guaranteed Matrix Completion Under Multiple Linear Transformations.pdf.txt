Guaranteed Matrix Completion under Multiple Linear Transformations

Chao Li†, Wei He†, Longhao Yuan‡†, Zhun Sun†, Qibin Zhao†§*

†RIKEN Center for Advanced Intelligence Project, Japan

‡Saitama Institute of Technology, Japan

§School of Automation, Guangdong University of Technology, China
{chao.li, wei.he, longhao.yuan, zhun.sun, qibin.zhao}@riken.jp

Abstract

Low-rank matrix completion (LRMC) is a classical
model in both computer vision (CV) and machine learn-
ing, and has been successfully applied to various real ap-
plications. In the recent CV tasks, the completion is usually
employed on the variants of data, such as “non-local” or
ﬁltered, rather than their original forms. This fact makes
that the theoretical analysis of the conventional LRMC is no
longer suitable in these applications. To tackle this prob-
lem, we propose a more general framework for LRMC, in
which the linear transformations of the data are taken into
account. We rigorously prove the identiﬁability of the pro-
posed model and show an upper bound of the reconstruc-
tion error. Furthermore, we derive an efﬁcient completion
algorithm by using augmented Lagrangian multipliers and
the sketching trick. In the experiments, we apply the pro-
posed method to the classical image inpainting problem and
achieve the state-of-the-art results.

1. Introduction

Low-rank matrix completion (LRMC) is a classical
model to utlize the low-rank structure of the matrix to
recover the missing entries given a small number of ob-
servations [5], and has many applications in different
ﬁelds [17]. Especially in computer vision (CV), LRMC has
been widely applied to image/video inpainting [45], reﬂec-
tion removal [14] and occlusion removal [40] and so on.
Although the black-box methods like generative adversar-
ial nets (GANs) become popular recently and also achieve
good performance in these tasks [30, 42], one of the ad-
vantage of LRMC is that the completion precision is the-
oretically guaranteed, which is of importance in practical
applications.

*Q. Zhao is the corresponding author. This work is partially supported
by JSPS KAKENHI (Grant No. 17K00326) and NSFC China (Grant No.
61773129).

One well known LRMC method is to perform nuclear
norm minimization (NNM) on the incomplete matrix [32,
41]. Due to the fact that nuclear norm is the convex enve-
lope of the matrix rank [32], solving NNM is equivalent to
searching the optimal low-rank approximation of the obser-
vations. Furthermore, it has been proved that, under mild
conditions, the reconstruction error of NNM is bounded by

O(cid:18)q n(p+2)

p

δ + 2δ(cid:19) where p denotes a constant w.r.t. the

rank of the n × n matrix and δ denotes the strength of the
noise. Although there exist many studies on more efﬁcient
algorithms for NNM [15, 34], NNM cannot give satisfac-
tory performance when the matrix is of high rank or several
whole rows or columns are missing, and such situations of-
ten happen in practice.

On the other hand, recent CV-driven studies show that
we can obtain better performance than NNM if exploit-
ing the low-rank structure on a “transformed” variant of
the matrix. For example, in low-level CV tasks, the “non-
local” methods are popularly used for image inpainting and
denoising [6, 8], in which the observations are split into
“non-local” groups, and the low-rank approximation is then
applied to each group. Likewise, in the problem of im-
age occlusion removal, the low-rank structure of the ob-
scured part of the images is used to regularize the regres-
sion model [40], implying that the low-rank approximation
is employed on the down-sampled data rather than the origi-
nal ones in the applications. In addition, the low-rank struc-
tures of the ﬁltered or transformed data are also studied for
different problems [10, 26, 33, 43]. Although these meth-
ods have achieved the state-of-the-art performance in their
ﬁelds, the existing theoretical results for the conventional
LRMC are no longer suitable for them.

To ﬁll the gap of the theoretical study on the aforemen-
tioned CV applications, we derive a new upper bound of the
reconstruction error for matrix completion by imposing the
linear transformations into the conventional NNM frame-
work. In contrast to the conventional NNM model, we min-
imize a sum of nuclear norms of the linearly transformed

111136

matrices. For different applications, we can specify the lin-
ear transformations as down-sampling or ﬁltering, etc.. By
choosing some speciﬁc but simple linear transformations
(as shown in the experiment), the new framework can easily
handle the row/column missing situations. In summary, this
paper makes the following contributions:

• We rigorously prove the theoretical guarantee of the
proposed model, and give an upper bound of the re-
construction error inﬂuenced by noise.

• We propose an efﬁcient algorithm to minimize the new
model, by which our model signiﬁcantly outperforms
the state-of-the-art methods for image inpainting.

1.1. Related works

We ﬁrst review the existing approaches for low-rank ma-
trix completion. One line of work focuses on low-rank de-
composition of the incomplete matrix [20, 27, 38, 39]. In
these methods, the observations are represented by the mul-
tiplication of latent factors. Although low-rank matrix de-
composition is a non-convex problem, in recent studies it
has been proved that the local minimum of the model is
also the global minimum in general cases [11]. However,
how to determine the optimal rank for the decomposition is
still a tough work in practice. The second line of work is
to directly estimate the optimal low-rank approximation of
the observation [5, 15, 19, 24]. As mentioned in Section 1,
NNM is the most well known method, in which the matrix
nuclear norm is minimized to ﬁnd the optimal low-rank ap-
proximation. Recent studies along this line attempt to ﬁnd
more suitable surrogate of the matrix rank than the nuclear
norm [13] or to impose additional regularization items on
the model [16].
In general, such modiﬁcation can elimi-
nate the bias brought by the matrix nuclear norm, but it also
leads to the non-convex model and the completion perfor-
mance lacking of theoretical guarantee. Furthermore, all the
aforementioned LRMC methods only consider the low-rank
structure of the original matrix, but our work is to explore
the additional low-rank structures resulted by linear trans-
formations, even if the original matrix might be high-rank.
Next, we review the studies for the theoretical properties
of LRMC. The theory for LRMC can be seen as an exten-
sion of compressed sensing [3]. Based on the perspective of
the restricted isometry property (RIP) [7, 12], many stud-
ies proved the sample complexity of LRMC, and looked for
tighter bound of the essential number of the observations
for exact recovery.
Instead of RIP, the approaches taken
in [2, 4, 31] are based on dual theory, by which the no-
tion of dual certiﬁcate is proposed as a condition to ensure
the exact recovery. However, all the works do not concern
how linear transformations inﬂuence the exact recovery of
LRMC. In this paper, we extend the existing theoretical
studies based on the dual theory, but we impose the linear

transformations in the proof. From the theoretical results,
we reveal how the characteristic of the linear transforma-
tions inﬂuences the completion performance.

Besides matrix completion, there also exist some meth-
ods which solve the similar problem. One example is ma-
trix sensing [29].
In contrast to LRMC, matrix sensing
uses Gaussian measure to obtain the observations instead
of down-sampling. As another example, tensor completion
is recently well studied using different decomposition mod-
els [9, 21, 25]. Especially, the convex tensor decomposition
(CTD), as an extension of NNM, exploits the multi-linear
low-rank structure to ﬁnd the optimal low-rank approxima-
tion of a tensor [35, 36]. In the next section, we will discuss
the differences of our model with these methods, and show
that CTD is a special case of our model.

2. Preliminaries

2.1. Notation

Given a positive integer d, let [d] denote the set of in-
tegers from 1 to d. We denote the scalars and matrices by
Italic letters, e.g. a, K, and boldface capital letters, e.g. X,
respectively. For a matrix X, let X(i, j) denote the i-th row
and j-th column entry. Let k · k2 and k · kF respectively
denote the spectral norm the Frobenius norm. Let k · k∗ de-
note the matrix nuclear norm, which equals the sum of the
singular values. We denote the linear functions on matrix
by calligraphic script letters, e.g. Q : Rm1×m2 → Rn1×n2 ,
and its conjugate by Q⋆. If we deﬁne the basis for both input
and output space of Q, then the linear function can be rep-
resented by a 4-th order tensor, e.g. Q ∈ Rm1×m2×n1×n2 .
Let [Q]hii, i ∈ [3] denote unfolding the tensor Q along the
ﬁrst i-orders [28]. By using the tensor form, we denote the
condition number of Q as cond(Q), which is deﬁned as

Deﬁnition 1 (Condition number). The condition number of
a linear transformation with its tensor representation Q ∈
Rm1×m2×n1×n2 is deﬁned as the ratio of the largest and
smallest non-zero singular values of [Q]h2i.

The condition number in Deﬁnition 1 is used to measure
how much error in the output results from an error in the
input for the linear function Q.

2.2. Nuclear norm minimization (NNM)

Assuming a perturbed low-rank matrix Y ∈ Rm1×m2 ,

the notion of NNM is formulized as:

min

X∈Rm1×m2

kXk∗ ,

s.t. kPΩ(X) − PΩ(Y)kF < δ,

(1)

where X denotes the recovered matrix, PΩ(·) is the down-
sampling operation under the index set of the missing pat-
tern Ω such that PΩ(Y) represents the observed entries.

Here we use the same notation for the linear functions and their tensor

representation without ambiguity.

11137

2.3. Generalization of LRMC

To leverage the additional low-rank structures of the
“transformed” matrix as mentioned in Section 1, we pro-
pose a generalization of LRMC called Matrix Comple-
tion under Multiple linear Transformations (MCMT). In
the model, we impose K linear transformations Qi
:
(i)
Rm1×m2 → Rn
2 , i ∈ [K] and simultaneously con-
sider the low-rank structures of the transformed matrices.
Speciﬁcally, the model of MCMT is given by

(i)
1 ×n

min

X∈Rm1×m2 Xi∈[K]

kQi (X)k∗ ,

(2)

s.t. kPΩ(X) − PΩ(Y)kF < δ.

We can see that (2) degenerates (1) when K = 1 and Q1
is the identical function. But the difference from NNM is
that MCMT seeks for the low-rank solution over the linear
transformations rather than the matrix itself. It implies that,
providing proper Qi’s, (2) can be used to complete the ma-
trix that has a high-rank structure.
Comparison with matrix sensing. Matrix sensing is to re-
cover the original matrix from the Gaussian measurements.
The model is formalized as

min

X∈Rm1×m2

kXk∗ ,

s.t. kQ(X) − Q(Y)kF < δ,

(3)

where the entries of Q follows the i.i.d. Gaussian distri-
bution. Compared to (2), the model (3) only considers the
linear transformation Q in the constraint term. Furthermore,
similar to NNM, matrix sensing exploits the low-rank struc-
ture of the original matrix, while MCMT takes into account
the additional low-rank structures under linear transforma-
tions.
Comparison with CTD. As mentioned in the related
works, CTD is to seek for the approximation of a tensor
with multi-linear low-rank structures. For a K-th order ten-
sor and its perturbed variant Y, CTD is given by [35]

min

X ∈Rm1×m2 Xi∈[K](cid:13)(cid:13)(cid:13)[X ](i)(cid:13)(cid:13)(cid:13)∗

s.t. kPΩ(X ) − PΩ(Y)kF < δ,

,

where [X ](i) denotes unfolding the tensor X along i-th or-
der [18]. Due to the fact that the unfolding operations are
linear functions, the model (4) is a special case of MCMT
when Qi(·) = [ · ](i). It is worthwhile to mention that tensor
unfolding only rearrange the tensor into different shapes,
but MCMT can exploit more general linear functions like
re-sampling, rotation and ﬁltering in the linear space to dig
more structures of the matrix.

2.4. Examples of Qi in MCMT

In MCMT, the linear transformations Qi, ∀i can be used
to formulate various operations in various CV applications.
Below we show some examples.

Example 1 (“Non-local” image restoration). To exploit the
non-local similarity of the images, the methods usually split
the whole matrix into many “non-local groups”, and each
group is a concatenation of similar patches of the image.
We can see that such grouping operation is mathematically
a re-sampling (deﬁnitely linear) function from the image to
the non-local groups. Therefore, each Qi(X), i ∈ [K] in
(2) corresponds to K non-local groups, and solving (2) is
to ﬁnd the optimal low-rank approximation for each non-
local group and then merge the approximations back to the
global image.

Example 2 (Occlusion removal). In the occlusion removal
problem, the original image is partially covered by other
objects, and the aim of this application is to recover the
hidden part of the image. To solve this problem, a previ-
ous study [40] assumes that both the original image and
the covered part have the low-rank structures. Under the
framework of MCMT, we can specify K = 2, set Q1 to be
the identical function to catch the low-rank structure of the
image, and set Q2 as down-sampling to obtain the covered
sub-image with the low-rank structures.

Besides these examples, we can also specify Qi as the
2-D wavelet ﬁlters to catch the short-term ﬂuctuation of the
image under multiple resolutions or even random reshuf-
ﬂing [22].

3. Identiﬁablity

One of the advantage of LRMC is that the completion
performance is theoretically guaranteed. In this section, we
theoretically analyze the reconstruction error for MCMT,
and reveal what conditions Qi, ∀i should satisfy for exact
recovery.
In the rest of this section, we ﬁrst establish an
upper bound of MCMT under a single linear transformation,
i.e. K = 1. After that, we extend the results to the case of
multiple transformations.

(4)

3.1. Single linear transformation

Assume that M0 ∈ Rm1×m2 denotes the “true” matrix
that we want to recover, and its rank equals R. The per-
turbed variant of M0 is generated by Y = M0 + H where
the entries of H obey the i.i.d. Gaussian distribution, i.e.
H(i, j) ∼ N (0, σ2) for all i ∈ [m1], j ∈ [m2]. With the
single linear transformation, we simplify (2) as

min

X∈Rm1×m2

kQ(X)k∗

s.t. kPΩ(X) − PΩ(Y)kF ≤ δ,

(5)

where the subscript of Q ∈ Rm1×m2×n1×n2
is re-
Let Q(M0) = UDV⊤ be the
moved for brevity.
truncated singular value decomposition (SVD), in which
only the singular vectors and non-zero singular values

11138

are kept. Furthermore, we deﬁne a linear space T =

nUX⊤ + YV⊤| ∀X ∈ Rn1×R, Y ∈ Rn2×Ro, which re-

ﬂects the properties of the neighborhood around M0. Let
T⊥ denote the orthogonal complement to T. Based on the
dual theory, we deﬁne the dual certiﬁcate for the unique so-
lution to (5) as follow:

Deﬁnition 2 (Dual certiﬁcate). A matrix Λ ∈ Rm1×m2 is
deﬁned as a dual certiﬁcate of (5), if PΩ(Λ) = Λ and Λ
can be decomposed as

Λ = Q⋆(cid:16)UV⊤ + RΛ(cid:17) ,

(6)

where RΛ = PT⊥ (Λ), PT⊥ denotes the projection to T⊥
and kRΛk2 ≤ 1.

The existence of the dual certiﬁcate was proved as a crit-
ical condition to ensure the exact recovery in the previous
work [4]. The following lemma shows how Deﬁnition 2
certiﬁes that the M0 is one of the optimal solutions to (5).

Proof. For contradiction, assume that Assumption 1 is not
satisﬁed, then there exists a non-zero perturbation H ∈ NΩ
on the missing entries such that Q (H) = 0 holds. Thus the
objective function

kQ(M0 + H)k∗ = kQ(M0) + Q(H)k∗ = kQ(M0)k∗.

(8)
It implies that ˆM = M0 + H 6= M0 is also the solution of
(5), which violates the assumption in the proposition.

The strong convexity of (5) guarantees the uniqueness
of the solution. With the dual certiﬁcate and Assumption 1,
we propose the main result of our paper in the case of single
linear transformation.

Theorem 1 (Error bound for a single Q). With Assump-
tion 1, and suppose the additional assumptions:

i) There exists a dual certiﬁcate obeying kRΛk2 < 1;

Lemma 1. Assume that there exists a dual certiﬁcate Λ and
the perturbation H obeys PΩ(H) = 0. Then the inequality

ii) ∃p > 0, s.t.PTQPΩQ⋆PT (cid:23) pI

kQ(M0 + H)k∗
≥ kQ(M0)k∗ + (1 − kRΛk2) kPT⊥ Q(H)k∗

(7)

iii) The product [Q]h2i · [Q]⋆

h2i is a diagonal matrix.

Then the solution of (5) ˆM obeys

holds.

The proof is given in the supplementary material.
Lemma 1 reﬂects how the perturbation H changes the
objective function in (5), and any perturbation around M0
cannot decrease the objective function. It means that M0 is
one of the solutions to (5). Furthermore, Lemma 1 degener-
ates to Lemma 4 in [4] if we specify Q to be the identity.

However,

the imposed linear transformation Q leads
to a new problem compared to the conventional studies.
Lemma 1 cannot guarantee that M0 is the unique solution
to (5). It can be seen from the second term of the right side
of (7) that the perturbation H could vanish in the null space
of Q. To prove the theoretical guarantee for (5), we further
make the following assumption to restrict the relation of the
null space of Q and PΩ:

Assumption 1. Let NQ and NΩ denote the null spaces of
the linear transformations Q and PΩ, respectively. We as-
sume that the relation NQ ∩ NΩ = {0} obeys.

It can be inferred from Assumption 1 that Q (H) 6= 0
for all the perturbation H with PΩ(H) = 0. Assumption 1
can be also considered as the strong convexity assumption
of (5), which is illustrated by the following proposition:

Proposition 1. Assume that the null space of PΩ is not triv-
ial, i.e. NΩ 6= {0}, and M0 is the unique solution to (5),
then Assumption 1 should be satisﬁed.

k ˆM − M0kF

cond(Q)

1 − kRΛk2s min{n1, n2}(p + k [Q]h2i k2

p

2)

(9)

.

≤ 2δ ·

The proof is given in the supplemental material.
First, We consider the four assumptions given in Theo-
rem 1. As mentioned above, Assumption 1 and (i) guarantee
the uniqueness of the solution to (1). However, it is not en-
sured that the unique solution ˆM equals the “true” matrix
M0 because of the existence of projection PT⊥ in (7). To
constraint the structure of PT⊥ , we impose the assumption
(ii), which implies that a sequential product of the functions
PT, Q and PΩ is positive deﬁnite, and the constant p can
be considered as the “incoherence” level among these func-
tions. The assumption (iii) further constraints the rows of
[Q]h2i to be orthogonal to each other, which is useful for
the proof procedure of the theorem.

Second, we consider the upper bound given in (9). We
can ﬁnd that the reconstruction error of (5) is upper bounded
by not only the noise strength δ but also the properties of the
linear transformation Q. If Q is a well-posed linear func-
tion, then the upper bound from (9) tends to be zero when
decreasing δ. It implies that the missing entries can be ex-
actly recovered if there is no noise.

11139

3.2. Extension to multiple Q’s

Compared to the case using a single Q, the model (2)
exploits the multiple low-rank structures under different
linear transformations. To bound the reconstruction er-

ematically, the entries of the constructed 4th-order tensor

ror of (2), we construct a block diagonal tensor eQ by
which we have keQ(X)k∗ = Pi∈[K] kQi(X)k∗. Math-
eQ ∈ Rm1×m2× Qi∈[K] n
eQ(:, :, Yk∈[i−1]
: Yk∈[i]

1 , Yk∈[i−1]

(j)
2 obey the following

: Yk∈[i]

1 ×Qj∈[K] n

equation:

N (i)
2 )

N (i)
2

N (i)
1

N (i)

(i)

= Qi, i ∈ [K],

where we use the Matlab syntax to denote a sub-block of a

(10)

holds

kQi(X)k∗,

∀X ∈ Rm1×m2 .

onal matrix of which each block equals Qi(X). Due to the

tensor. Using eQ, it is easy to ﬁnd that eQ(X) is a block diag-
block diagonal structure of eQ(X), the following equation
keQ(X)k∗ = Xi∈[K]
single transformation eQ. Hence Theorem 1 can be directly
Corollary 1 (Error bound for multiple Qi’s). Let eQ be a
concatenation of Qi, i ∈ [K] by using (10), and eQ obeys the

Using (11), the model (2) under multiple linear transforma-
tions Qi, i ∈ [K] can be converted to the model (5) under a

assumptions given in Theorem 1. Then the reconstruction
error of the solution to (2) obeys

extended to bound the reconstruction of (2).

(11)

k ˆM − M0kF

≤ 2δ ·

cond(eQ)

1 − kRΛk2

vuut min{n1, n2}(p + kheQih2i

p

k2
2)

.

(12)

Although MCMT under multiple Qi’s can be converted

to the case under the single eQ, we ﬁnd that imposing more

linear transformations could result in easier conditions to
exactly recover the missing entries. This statement can be
partially supported by the following proposition:

NQi .

Proposition 2. Let NQi and N eQ be the null space of

Qi, i ∈ [K] and eQ, respectively. Then it yields that
N eQ =Ti∈[K]
2 that the null space of eQ is “smaller” than the one of any

The proof is trivial. It can be inferred from Proposition

https://www.mathworks.com/products/matlab.html

single linear transformation Qi. It means that Assumption

1 can be still held by eQ even if each Qi cannot satisfy As-

sumption 1. Therefore, it might be easier for MCMT to
exactly reconstruct the missing entries if we impose more
Qi’s.

To develop an efﬁcient algorithm for MCMT, it would
be better to consider the following optimization model, in
which the constraint in (2) is absorbed into the objective
function such that solving MCMT becomes a unconstrained
problem.

min

X∈Rm1×m2

1
2

kPΩ(X) − PΩ(Y)k2

λikQi(X)k∗.

F + Xi∈[K]

(13)
Besides the constraint term, we also impose a set of tuning
parameters λi, i ∈ [K] for each Qi to trade-off the inﬂu-
ences on the objective function by different linear transfor-
mations. To analyze the performance of (13), we assume
λ = λi, ∀i for brevity. The error bound of (13) is derived
by the following theorem:

Theorem 2. With the assumptions in Theorem 1 for the con-

k ˆM − M0kF

reconstruction error of (13) is bounded by

catenation eQ, and further assume that the tuning parame-
ter satisﬁes λ > kPΩ (H) k2/pmin{m1, m2}. Then the
2 }k [Qi]h2i k2
≤ 8λmin{m1, m2} + Xi∈[K]qmin{n(i)
2 }(p + kheQi<2>
cond(eQ) · min{Q n(i)

1 ,Q n(i)

p(1 − kRΛk2)2

1 , n(i)

k2)

.

·

(14)

Note that λ can be an arbitrarily small number if the
strength of the perturbation matrix H is weak enough.
Therefore the performance of (13) is theoretically guaran-
teed under above conditions.

4. Algorithm

In this section, we present an efﬁcient algorithm to solve

the optimization problem (13).

Due to the convexity of the model, we use the augmented
Lagrangian multipliers to search the minimum of the model.
Inspired by the existing tensor nuclear norm minimization
methods [25], we also impose auxiliary variables to get a
simple algorithm. Speciﬁcally, we rewrite (13) as:

min

X,Wi, i∈[K]

1
2

kPΩ(X) − PΩ(Y)k2

s.t. Wi = Qi (X) , i ∈ [K]

F + Xi∈[K]

λikWik∗

,

(15)

11140

where Wi, i ∈ [K] denote the auxiliary matrices to decom-
pose the objective function of (13) into several parts with
independent variables. But it is easy to ﬁnd that (15) is with
the same solution to (13). The corresponding augmented
Lagrangian function of (15) is given by

its solutions is given by

−1

Q⋆

X+ :=P ⋆
i Qi
ΩPΩ + σ Xi∈[K]
i (Zi − Wi)
·(P ⋆
ΩPΩ) Y − Xi∈[K]

Q⋆

Updating Zi and σ. We update the Lagrangian multi-

.

(21)

λikWik∗

,

σ

2 Xi∈[K]

kQi(X) − Wik2
F

plers Zi, i ∈ [K] and σ by using

Z+
i

:= Zi + σ(Qi(X) − Wi)

(16)

and

σ+ := ρσ,

(22)

(23)

L(X, Wi, Zi, ∀i ∈ [K], σ)
1
2

kPΩ(X) − PΩ(Y)k2

=

F + Xi∈[K]

hZi, Qi(X) − Wii +

+ Xi∈[K]

where Zi denotes the Lagrangian multipliers for all i ∈ [K]
and σ > 0. In the algorithm, we alternatively update each
variable in (16) until convergence, and the updating order is
given by

1) Wi, i ∈ [K] ⇒ 2) X ⇒ 3) Zi, i ∈ [K] ⇒ 4) σ.

Updating Wi. We treat all variables except Wi as con-
stants during updating. In this case, (16) can be rewritten
as

ˆL(Wi) =

λikWik∗ + hZi, Qi(X) − Wii +

σ
2

kQ(X) − Wik2
F ,
(17)

where we omit the constant terms. It is known from [1] that
minimizing (17) has a closed-form solution, which is given
by

W+
i

:= D λi

σ (cid:18)Qi(X) +

1
σ

Zi(cid:19) ,

(18)

where ρ > 1 is a constant. The details of the algorithm are
given by Algorithm 1.

Algorithm 1 Matrix completion under multiple linear trans-
formations (MCMT)
Input: Observation PΩ(Y) and its corresponding down-
sampling projection PΩ. The linear transformations
Qi, and tuning parameters λi > 0, i ∈ [K]

Output: Reconstructed matrix X.
1: Initialize X0 by PΩ(X0) = PΩ(Y), and ﬁll unob-
i equal Qi(X0) for all
served entries by zero. Let W0
i ∈ [K]. Initialize Z0
i ), where sgn(·) de-
notes elementwisely choosing the sign of the entries.
Let σ = 1, ρ = 1.01.

i = sgn(W0

2: Repeat
3:

4:

5:

Update Wi, ∀i by (18)
Update X by (21)
Update Zi, ∀i by (22)
Update σ by (23)

6:
7: Until convergence

σ

(·) denotes the soft-thresholding operation [23].
where D λi
If X = UDV⊤ is the SVD of X, then Dλ(X) = U ¯DV⊤,
where the entries ¯D(i, j), ∀i, j satisﬁes

¯D(i, j) =(cid:26) D(i, j) − λ D(i, j) > 0

otherwise

0

.

(19)

Updating X. Likewise, we treat all the variables except
In this case, (16) is

X as the constants during updating.
rewritten as

ˆL(X) =

+ Xi∈[K]

1
2
hZi, Qi(X) − Wii +

kPΩ(X) − PΩ(Y)k2
F
σ

2 Xi∈[K]

kQi(X) − Wik2
F

.

(20)

We can see that solving (20) is a least squares problem, and

4.1. The sketching trick

(i)

We can see one computational bottleneck in Algorithm
1 is the numerous SVD operations when updating Wi. To
speed up our method, we utilize the sketching trick to re-
duce the computational complexity. Speciﬁcally, we gener-
2 ×l(i)
ate the Gaussian random projection matrix Pi ∈ Rn
w.r.t. each Qi. By using the random projections, we esti-
mate the left singular vectors of Qi(X) by using QR de-
[ ˆUi, ∼] = qr (Qi (X) · Pi). When ob-
composition, i.e.
taining ˆUi’s, the SVD operation on Qi(X) in Algorithm 1
can be replaced by Qi(X) = ˆUiDtmpV⊤
tmp, where Dtmp
and Vtmp respectively denote the matrices which contain
the singular values and right singular vectors of ˆU⊤
i Qi(X).
By using the sketching trick, the computational complexity
in the SVD procudure can be reduced from O(n3) to O(n2)
if n = n(i)

2 and l(i) ≪ n.

1 = n(i)

11141

Table 1. Performance comparison for image inpainting by using different methods. In the experiment, we utilize PSNR(dB) to quantify the
reconstruction. Furthermore, we consider various missing patterns, including Uniformly random missing (U), Row missing (R), Column
missing (C), and their combinations.

Missing
pattern

U

R

C

RC

URC

Observation

ratio
0.1
0.3
0.5
0.7
0.9
0.1
0.3
0.5
0.7
0.9
0.1
0.3
0.5
0.7
0.9
0.1
0.3
0.5
0.7
0.9
0.1
0.3
0.5
0.7
0.9

FBCP

SPC

TRALS

FaLRTC

MCMT

NNM

19.18 ± 1.06
22.62 ±1.27
25.06 ± 1.41
27.78 ± 1.46
32.88 ±1.47
1.01 ±7.73
12.70 ±3.23
19.86±1.96
25.21 ± 1.86
31.36 ± 1.48

2.92 ±3.35
12.70 ±3.29
19.74 ±1.50
24.86 ± 1.20
31.00 ± 1.57
6.73 ± 1.24
13.93 ±1.84
18.14 ±1.13
22.35 ± 1.17
28.74 ± 1.34
4.85 ± 1.15
14.89 ± 1.61
17.94 ± 0.99
21.57 ± 0.97
27.09 ± 1.34

17.68 ±1.01
21.05 ±0.99
23.36 ± 1.02
26.05 ±1.04
31.15 ±1.06
12.37 ± 1.80
18.17 ± 1.00
21.15 ± 0.85
24.47 ±0.82
30.02 ±0.95
2.27 ± 1.86
18.14±1.20
21.20 ± 1.01
24.45 ±0.94
29.93 ±1.00
5.20 ±1.11

15.44 ± 1.22
18.52 ± 0.96
21.59 ±0.85
26.88 ±0.86
4.84 ±1.15
13.01 ± 1.30
17.31 ±1.04
20.38 ±0.92
25.56 ±0.89

17.70 ±1.68
23.03 ± 1.34
24.93 ±1.33
27.29 ±1.33
32.13 ±1.33

-
-

11.95 ±5.45
24.14 ±2.32
30.72 ±1.82

-
-

15.20 ±2.64
24.12 ±1.42
30.19 ±1.60

-
-

9.24±5.94
21.52 ±1.79
27.51 ±1.55

-
-

11.59 ±5.94
20.67±1.57
26.42 ± 1.26

11.50 ±1.24
18.25 ±1.05
21.93 ±0.99
26.27 ±1.18
33.43±1.52
5.59 ±1.14
9.86 ±0.98
16.14 ±1.01
21.60 ±0.89
29.22 ±1.15
5.58 ±1.14
9.88 ±1.10
16.23 ±1.12
21.59 ±1.11
28.94 ±1.08
4.91 ±1.15
6.45 ±1.10
12.43 ±0.97
18.43 ±0.97
25.84 ±0.95
4.84 ±1.15
5.21 ±1.14
10.18 ±0.99
16.92 ±1.02
24.56 ±1.01

22.08±1.58
26.39±2.10
29.03±2.25
31.20±2.28
33.22±2.24
17.51±1.45
22.38±1.75
25.54±1.80
28.67±2.22
31.97±2.25
17.30±1.37
22.13±1.33
25.40±1.42
28.42±1.74
32.03±2.16
15.01±1.23
19.69±1.24
22.94±1.54
26.30±1.76
30.61±2.18
12.68±1.24
18.11±0.94
21.74±1.23
25.31±1.70
29.90±2.07

14.29±1.47
22.05±1.53
25.61±1.53
28.48±1.46
31.40±1.19
6.23±0.96
9.10±0.95
10.67±1.78
9.68±1.84
12.55±1.93
7.74±0.86
5.08±1.68
0.38±1.45
-0.80±1.43
2.42±1.59
5.18±0.99
7.45±0.64
9.97±1.74
2.08±1.66
1.43± 1.64
5.15±0.99
5.63±0.96
7.87±0.74
14.74±0.84
13.90±1.83

MCMT

(sketching)
22.06±1.57
26.78±2.10
28.75±2.30
30.69±2.39
32.37±2.42
17.51±1.45
22.38±1.75
25.49±1.79
28.47±2.22
31.78±2.33
17.30±1.37
22.13±1.32
25.36±1.42
28.26±1.76
31.52±2.27
15.01±1.23
19.69±1.24
22.93±1.54
26.24±1.76
30.27±2.22
12.68±1.23
18.10±0.93
21.75±1.23
25.27±1.70
29.59±2.10

5. Experiments

In this section, we employ the proposed method on the
classical grayscale image inpainting problem to demon-
strate the effectiveness of our method.

Although there exist many methods proposed for the im-
age inpainting problem, it is still a challenging problem to
complete a grayscale image with missing entries. Com-
pared to the RGB images, there is no additional similarity
from RGB channels in the grayscale images, which can be
utilized to improve the completion performance.

In the experiment, we choose 12 benchmark images
(256 × 256) to evaluate the performance of the proposed
method. To generate the test dataset, we employ 5 differ-
ent missing patterns to remove the entries including Uni-
formly random missing (U), random Row-missing (R), ran-
dom Column-missing (C) and their combinations (RC and
URC). In addition, we set 5 different observation ratios
{0.1, 0.3, 0.5, 0.7, 0.9} for each type of the missing pattern.
To achieve more reliable experimental results, we i.i.d. gen-
erate 20 samples for each image with the given missing pat-
tern and observation ratio.

The experimental results are shown in Table 1 and Fig. 1,
in which the mean and the standard deviation of the PSNR

The benchmark images include cameraman, house, jetplane, lake,
lena, livingroom, mandril, peppers, pirate, walkbridge and blonde. All
these images are shown in the supplementary material.

is calculated using all images and samples under each miss-
ing pattern and observation ratio. For comparison, we also
employ the current state-of-the-art methods in the exper-
iment, including three tensor decomposition based meth-
ods (FBCP [46], FaLRTC [25] and TRALS [37]), a spa-
tial smoothness based method (SPC) [44], and NNM as the
baseline.

For our method, we specify the number of the linear
transformation to be equal to K = 1 and λ = 1 for brevity,
and simply set the transformation Q to be a two dimensional
(2D) differential ﬁltering by using the taps

T =(cid:20) 1 −1
1 (cid:21) ,

−1

(24)

and the tensor representation of Q is given by the circu-
lar form of the 2D ﬁlter T. Furthermore, we also consider
using sketching to speed up our method, by which the di-
mension is decreased to 100 after random projection (the
original dimension equals 256).

As shown in Table 1, MCMT outperforms other meth-
ods in all cases. Especially when the observation ratio is
low, the performance gap between our method and others
is signiﬁcant. For example, the PSNR of MCMT is av-
eragely 4dB higher than the state-or-the-art methods for
all the missing pattern when the observation ratio equals
0.5. Furthermore, the performance of MCMT is about 5dB
higher than the baseline method NNM in the uniformly ran-

11142

Figure 1. Examples of the completion results by using different methods, in which the rows correspond different missing patterns with the
obervation ratio equals 0.7, and the columns correspond different methods. It can be easily seen that MCMT and its sketching version
obtain higher qualiﬁed results.

dom missing pattern (U), and has more than 10dB perfor-
mance improvement in other missing patterns. Comparing
the performance between MCMT and its sketching version,
we can ﬁnd the sketching trick does not decease the perfor-
mance of MCMT with a appropriate projection dimensions.
The performance improvement by MCMT can be ex-
pected because the existence of Q can enhance the incoher-
ence between low-rank structure of the (transformed) ma-
trix and its missing pattern. For example, NNM fails to
recover the whole row/column missing because minimiz-
ing the rank of the original matrix tends to ﬁll the miss-
ing rows/columns with zeros, which is deﬁnitely incorrect
for completion. However, imposing the linear transforma-
tions can make the method take into account the dependence
of the entries in different rows and columns to recover the
missing entries.

also a challenging task to develop a theoretical framework
to analyze the performance of the completion methods. In
this paper, inspired by the existing studies on the CV appli-
cations, we proposed a new framework to leverage the ad-
ditional low-rank structures of the data for the completion
problem. In contrast to the conventional matrix completion
methods, we impose multiple linear transformations into the
model. As the theoretical result, we rigorously prove an up-
per bound for the reconstruction error of our method, and
it implies that our model can get theoretically guaranteed
performance under some conditions. Besides the theoreti-
cal works, we also developed an efﬁcient algorithm by us-
ing the augmented Lagrangian multipliers. The experimen-
tal results show that the proposed method signiﬁcantly im-
proves the performance for image inpainting compared with
the state-of-the-art methods.

6. Conclusion

References

Although there exist lots of methods for the data com-
pletion problem, matrix completion is still a basic but im-
portant issue for the further development of more sophis-
ticated methods like tensor completion. Furthermore, it is

[1] J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value thresh-
olding algorithm for matrix completion. SIAM Journal on
Optimization, 20(4):1956–1982, 2010. 6

[2] E. Candes and T. Tao. The power of convex relaxation: Near-
optimal matrix completion. IEEE Transactions on Informa-

11143

ObservationFBCPSPCTRALSFaLRTCMCMTNNMMCMT (Sketching)URCRCURCtion Theory, 56(5):2053–2080, 2010. 2

[3] E. J. Candes. The restricted isometry property and its impli-
cations for compressed sensing. Comptes rendus mathema-
tique, 346(9-10):589–592, 2008. 2

[4] E. J. Candes and Y. Plan. Matrix completion with noise.

Proceedings of the IEEE, 98(6):925–936, 2010. 2, 4

[5] E. J. Cand`es and B. Recht. Exact matrix completion via con-
vex optimization. Foundations of Computational mathemat-
ics, 9(6):717, 2009. 1, 2

[6] Q. Cheng, H. Shen, L. Zhang, and P. Li.

Inpainting for
remotely sensed images with a multichannel nonlocal total
variation model. IEEE Transactions on Geoscience and Re-
mote Sensing, 52(1):175–187, 2014. 1

[7] M. A. Davenport and J. Romberg. An overview of low-rank
matrix recovery from incomplete observations. IEEE Jour-
nal of Selected Topics in Signal Processing, 10(4):608–622,
2016. 2

[8] W. Dong, G. Shi, and X. Li. Nonlocal image restoration with
IEEE

bilateral variance estimation: a low-rank approach.
transactions on image processing, 22(2):700–711, 2013. 1

[9] S. Gandy, B. Recht, and I. Yamada. Tensor completion and
low-n-rank tensor recovery via convex optimization. Inverse
Problems, 27(2):025010, 2011. 2

[10] R. S. Ganti, L. Balzano, and R. Willett. Matrix completion
under monotonic single index models. In Advances in Neural
Information Processing Systems, pages 1873–1881, 2015. 1

[11] R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spu-
In Advances in Neural Information

rious local minimum.
Processing Systems, pages 2973–2981, 2016. 2

[12] D. Gross. Recovering low-rank matrices from few coefﬁ-
cients in any basis. IEEE Transactions on Information The-
ory, 57(3):1548–1566, 2011. 2

[13] S. Gu, L. Zhang, W. Zuo, and X. Feng. Weighted nuclear
norm minimization with application to image denoising. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2862–2869, 2014. 2

[14] B.-J. Han and J.-Y. Sim. Reﬂection removal using low-rank
matrix completion. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5438–
5446, 2017. 1

[15] P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix com-
pletion using alternating minimization. In Proceedings of the
forty-ﬁfth annual ACM symposium on Theory of computing,
pages 665–674. ACM, 2013. 1, 2

[16] V. Kalofolias, X. Bresson, M. Bronstein, and P. Van-
dergheynst. Matrix completion on graphs. arXiv preprint
arXiv:1408.1717, 2014. 2

[17] Z. Kang, C. Peng, and Q. Cheng. Top-n recommender sys-
tem via matrix completion. In Thirtieth AAAI Conference on
Artiﬁcial Intelligence, 2016. 1

[18] T. G. Kolda and B. W. Bader. Tensor decompositions and

applications. SIAM review, 51(3):455–500, 2009. 3

[19] V. Koltchinskii, K. Lounici, A. B. Tsybakov, et al. Nuclear-
norm penalization and optimal rates for noisy low-rank ma-
trix completion. The Annals of Statistics, 39(5):2302–2329,
2011. 2

[20] Y. Koren. Factorization meets the neighborhood: a multi-
faceted collaborative ﬁltering model. In Proceedings of the
14th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 426–434. ACM, 2008. 2

[21] D. Kressner, M. Steinlechner, and B. Vandereycken. Low-
rank tensor completion by riemannian optimization. BIT Nu-
merical Mathematics, 54(2):447–468, 2014. 2

[22] C. Li, M. E. Khan, S. Xie, and Q. Zhao. Low-rank tensor
decomposition via multiple reshaping and reordering opera-
tions. arXiv preprint arXiv:1805.08465, 2018. 3

[23] C. Li, Q. Zhao, J. Li, A. Cichocki, and L. Guo. Multi-tensor
completion with common structures. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015. 6

[24] Z. Lin, M. Chen, and Y. Ma. The augmented lagrange mul-
tiplier method for exact recovery of corrupted low-rank ma-
trices. arXiv preprint arXiv:1009.5055, 2010. 2

[25] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor com-
pletion for estimating missing values in visual data.
IEEE
transactions on pattern analysis and machine intelligence,
35(1):208–220, 2013. 2, 5, 7

[26] Q. Liu, S. Li, J. Xiao, and M. Zhang. Multi-ﬁlters guided
low-rank tensor coding for image inpainting. Signal Pro-
cessing: Image Communication, 2018. 1

[27] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social
recommendation using probabilistic matrix factorization. In
Proceedings of the 17th ACM conference on Information and
knowledge management, pages 931–940. ACM, 2008. 2

[28] I. V. Oseledets. Tensor-train decomposition. SIAM Journal

on Scientiﬁc Computing, 33(5):2295–2317, 2011. 2

[29] D. Park, A. Kyrillidis, C. Caramanis, and S. Sanghavi. Non-
square matrix sensing without spurious local minima via the
burer-monteiro approach. arXiv preprint arXiv:1609.03240,
2016. 2

[30] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context encoders: Feature learning by inpainting. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2536–2544, 2016. 1

[31] B. Recht. A simpler approach to matrix completion. Journal
of Machine Learning Research, 12(Dec):3413–3430, 2011.
2

[32] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-
rank solutions of linear matrix equations via nuclear norm
minimization. SIAM review, 52(3):471–501, 2010. 1

[33] L. Sun, B. Jeon, Y. Zheng, and Z. Wu. Hyperspectral image
restoration using low-rank representation on spectral differ-
ence image. IEEE Geoscience and Remote Sensing Letters,
14(7):1151–1155, 2017. 1

[34] R. Sun and Z.-Q. Luo. Guaranteed matrix completion via
non-convex factorization. IEEE Transactions on Information
Theory, 62(11):6535–6579, 2016. 1

[35] R. Tomioka, K. Hayashi, and H. Kashima. On the exten-
sion of trace norm to tensors. In NIPS Workshop on Tensors,
Kernels, and Machine Learning, page 7, 2010. 2, 3

[36] R. Tomioka and T. Suzuki. Convex tensor decomposition via
structured schatten norm regularization. In Advances in neu-
ral information processing systems, pages 1331–1339, 2013.
2

11144

[37] W. Wang, V. Aggarwal, and S. Aeron. Efﬁcient low rank
tensor ring completion.
In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 5697–5705,
2017. 7

[38] Z. Wen, W. Yin, and Y. Zhang. Solving a low-rank factor-
ization model for matrix completion by a nonlinear succes-
sive over-relaxation algorithm. Mathematical Programming
Computation, 4(4):333–361, 2012. 2

[39] Y. Xu, R. Hao, W. Yin, and Z. Su. Parallel matrix fac-
torization for low-rank tensor completion. arXiv preprint
arXiv:1312.1254, 2013. 2

[40] J. Yang, L. Luo, J. Qian, Y. Tai, F. Zhang, and Y. Xu. Nu-
clear norm based matrix regression with applications to face
recognition with occlusion and illumination changes. IEEE
transactions on pattern analysis and machine intelligence,
39(1):156–171, 2017. 1, 3

[41] J. Yang and X. Yuan. Linearized augmented lagrangian and
alternating direction methods for nuclear norm minimiza-
tion. Mathematics of computation, 82(281):301–329, 2013.
1

[42] R. A. Yeh, C. Chen, T. Yian Lim, A. G. Schwing,
M. Hasegawa-Johnson, and M. N. Do. Semantic image in-
painting with deep generative models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5485–5493, 2017. 1

[43] T. Yokota, B. Erem, S. Guler, S. K. Warﬁeld, and H. Hontani.
Missing slice recovery for tensors using a low-rank model in
embedded space.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 8251–
8259, 2018. 1

[44] T. Yokota, Q. Zhao, and A. Cichocki. Smooth parafac de-
IEEE Transactions on

composition for tensor completion.
Signal Processing, 64(20):5423–5436, 2016. 7

[45] D. Zhang, Y. Hu, J. Ye, X. Li, and X. He. Matrix completion
by truncated nuclear norm regularization. In 2012 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2192–2199. IEEE, 2012. 1

[46] Q. Zhao, L. Zhang, and A. Cichocki. Bayesian cp factor-
ization of incomplete tensors with automatic rank determi-
nation. IEEE transactions on pattern analysis and machine
intelligence, 37(9):1751–1763, 2015. 7

11145

