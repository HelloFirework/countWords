LAF-Net: Locally Adaptive Fusion Networks for Stereo Conﬁdence Estimation

Sunok Kim1,2, Seungryong Kim1,2, Dongbo Min3, Kwanghoon Sohn1*

1Yonsei University 2 ´Ecole polytechnique f´ed´erale de Lausanne (EPFL)

3Ewha Womans University

{kso428,khsohn}@yonsei.ac.kr seungryong.kim@epfl.ch dbmin@ewha.ac.kr

Conceptual figure. (fig1)

Abstract

We present a novel method that estimates conﬁdence map
of an initial disparity by making full use of tri-modal in-
put, including matching cost, disparity, and color image
through deep networks. The proposed network, termed
as Locally Adaptive Fusion Networks (LAF-Net), learns
locally-varying attention and scale maps to fuse the tri-
modal conﬁdence features. The attention inference net-
works encode the importance of tri-modal conﬁdence fea-
tures and then concatenate them using the attention maps
in an adaptive and dynamic fashion. This enables us to
make an optimal fusion of the heterogeneous features, com-
pared to a simple concatenation technique that is commonly
used in conventional approaches.
In addition, to encode
the conﬁdence features with locally-varying receptive ﬁelds,
the scale inference networks learn the scale map and warp
the fused conﬁdence features through convolutional spatial
transformer networks. Finally, the conﬁdence map is pro-
gressively estimated in the recursive reﬁnement networks to
enforce a spatial context and local consistency. Experimen-
tal results show that this model outperforms the state-of-
the-art methods on various benchmarks.

1. Introduction

Stereo matching for reconstructing geometric conﬁgura-
tion of a scene is one of the fundamental and essential prob-
lems in computer vision ﬁelds [36]. For decades, numer-
ous methods have been proposed for this task by leveraging
handcrafted [43, 10] and/or machine learning based [45, 38]
techniques. However, because of its challenging elements
such as reﬂective surfaces, textureless regions, repeated pat-
tern regions, occlusions [23, 13, 6], and photometric de-
formations incurred by illumination and camera speciﬁca-
tion variations [44, 9], the stereo matching still remains an
unsolved problem. To alleviate these inherent challenges,

This research was supported by Next-Generation Information Com-
puting Development Program through the National Research Founda-
tion of Korea(NRF) funded by the Ministry of Science and ICT (NRF-
2017M3C4A7069370). ∗Corresponding author

t
s
o
C
g
n
h
c
t
a
M

i

y
t
i
r
a
p
s
i
D

e
g
a
m

i
 
r
o
o
C

l

t
e
N
‐
F
A
L

e
c
n
e
d
i
f
n
o
C

Figure 1. Illustration of LAF-Net: using tri-modal input, consist-
ing of matching cost, disparity, and color image, LAF-Net esti-
mates conﬁdence of disparity.

most methods [39, 27, 29, 20, 18, 21] have adopted the
conﬁdence estimation step that detects unreliable disparities
and reﬁnes them for improving the quality of stereo match-
ing results.

Formally, the conﬁdence estimation pipeline involves
ﬁrst extracting the conﬁdence features and then training the
conﬁdence classiﬁers using ground-truth conﬁdences [39,
27, 30]. Conventionally, there exist several handcrafted
conﬁdence measures using different input modalities, such
as matching cost, disparity, and color image [12, 28]. Since
any single conﬁdence measures cannot handle all failure
cases in stereo matching, various combination of hand-
designed conﬁdence measures extracted from the tri-modal
input [8, 39, 27, 29, 20] has been used to learn shallow clas-
siﬁers, such as random decision forest [2, 22]. Despite per-
formance improvement by the joint usage of the tri-modal
input, they still show a limited performance due to their low
discriminative power.

Recent approaches have attempted to estimate the con-
ﬁdence by leveraging deep convolutional neural networks
(CNNs) thanks to their high robustness [30, 37, 18, 21],
demonstrating the substantial accuracy gain over the hand-
crafted approaches. However, unlike handcrafted ap-
proaches [8, 39, 27] that make full use of the tri-modal in-
put, CNN-based approaches have been formulated by par-

1205

tially using single- or bi-modal input, e.g., matching cost
only [38], disparity only [30, 37], matching cost and dis-
parity [18, 21], or disparity and color [7, 40]. Moreover, a
simple concatenation technique [16] is commonly used to
fuse multi-modal conﬁdence features, disregarding that the
fusion weights may vary for each pixel depending on the
characteristic of conﬁdence features.

Meanwhile, the receptive ﬁelds for conﬁdence features
can vary for each pixel. This assumption has been used in
conventional handcrafted methods [39, 27, 29, 20] in a way
of extracting multi-scale conﬁdence features. For instance,
it was reported in [27] that the median disparity deviation
value in different scales is the most important conﬁdence
features for both outdoor [24] and indoor database [34].
A similar idea has also been adopted in some conﬁdence
estimation approaches based on deep CNNs. In [21], the
multi-scale disparity feature extraction networks have been
proposed to learn the conﬁdence features from disparity in
different scales. Also, the dilated convolution that extracts
local contextualized information with different dilation fac-
tors was proposed by Fu et al. [7]. Tosi et al.
[40] pro-
posed local-global conﬁdence networks to effectively com-
bine both local and global context from the input images.
However, there is still no mechanism that explicitly consid-
ers locally-varying scale ﬁelds.

On the other hand, in order to consider the spatial con-
text and local consistency, the output conﬁdence map was
reﬁned using joint ﬁltering [20] or using deep CNNs [31],
generating more reliable conﬁdence map.

In this paper, we propose novel conﬁdence estimation
networks, called Locally Adaptive Fusion Networks (LAF-
Net), that utilize tri-modal input consisting of matching
cost, disparity, and color image as illustrated in Fig. 1.
The networks consist of conﬁdence feature extraction net-
works, attention inference networks, scale inference net-
works, and recursive conﬁdence reﬁnement networks.
In
the attention inference networks, we fuse the tri-modal in-
put adaptively with locally-varying attention maps to bene-
ﬁt from the joint usage of the tri-modal conﬁdence features.
In the scale inference networks, locally adaptive scale pa-
rameters are learned for all pixels, which enables the net-
works to extract the conﬁdence features within locally op-
timal receptive ﬁelds.
In addition, the output conﬁdence
is further reﬁned through the recursive conﬁdence reﬁne-
ment networks. The proposed method is extensively eval-
uated through an ablation study and comparison with con-
ventional handcrafted and CNNs-based methods on various
benchmarks, including Middlebury 2006 [34], Middlebury
2014 [33], and KITTI 2015 [24].

2. Related Works

Handcrafted approaches.
there have
been extensive literatures in conﬁdence estimation, mainly

In last decades,

based on handcrafted conﬁdence measures [6, 5, 25]. In a
comprehensive study of conﬁdence measures has been pre-
sented by Hu and Mordohai [12]. Various single conﬁdence
measures have been analyzed and categorized according to
different input by Park et al. [28]. From matching cost, the
peak ratio of the matching costs [11] and naive peak ra-
tio [12] have been widely used to remove unreliable pix-
els. The maximum margin [12] and winner margin [35]
were computed with the difference of matching costs. From
disparity, a left-right consistency [5] has been most widely
used for ﬁnding the correctness of matched pixels. The vari-
ances of the disparity (VAR) [8] and the median disparity
deviation (MDD) [8] in a local window were also measured
to estimate unreliable pixels. Several conﬁdence measures
extracted from image have been introduced in [28]. The
variance of intensities might be used, especially separating
the homogeneous regions from the well-textured regions as
well as the magnitude of the image gradients. A distance-
to-edge measure incorporated the texturedness of a pixel.

Since there is no single conﬁdence feature that yields
stably optimal performance, various approaches to beneﬁt
from the feature combination among a different set of sin-
gle conﬁdence measures have been proposed [8, 39] which
trained a shallow classiﬁer such as random decision for-
est [1, 22]. However, the performance of the aforemen-
tioned methods is still limited since the selected conﬁdence
features are not optimal. To select the set of (sub-)optimal
conﬁdence features among multiple conﬁdence features,
Park and Yoon [27] utilized the permutation importance
measures to select important set of conﬁdence feautres.
In [27], they found the MDD in different scales are impor-
tant to measure unrelibale pixels. Similarly, Poggi and Mat-
toccia [29] employed the set of conﬁdence features from
only disparity map that can be computed in O(1) complex-
ity without losing the conﬁdence estimation performance.
While the aforementioned methods detect unconﬁdent pix-
els in a pixel-level, Kim et al. [20] leveraged a spatial con-
text to estimate conﬁdence in a superpixel-level. In [20],
the resulting conﬁdence map was further reﬁned through
hierarchical conﬁdence map aggregation. However, all of
these methods used handcrafted conﬁdence features, and
they may not be optimal to detect unrelibale pixels on chal-
lenging scenes.

Deep CNN-based approaches. Recent approaches have
tried to measure the conﬁdence through deep CNNs [30, 37,
31, 18, 21]. A quantitative evaluation of conﬁdence mea-
sures that use machine learning approaches has been per-
formed in [32]. Formally, these CNN-based methods ﬁrst
extract the conﬁdence features from single- or bi-modal in-
put and then predict the conﬁdence by jointly learning the
feature extractor and classiﬁer. Various methods have been
proposed that use the single- or bi-modal input, i.e., a left
disparity [30], both left and right disparity [37], a matching

206

Feature Extraction Networks

Attention Inference Networks

Scale Inference Networks

 

t
s
o
C
K
‐
p
o
T

y
t
i
r
a
p
s
i
D

l

r
o
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

𝑋(cid:3004)

𝑋(cid:3005)

𝑋(cid:3010)

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

 

 

N
B
+
v
n
o
C

x
a
m

t
f
o
S

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

𝑌

 

 

N
B
+
v
n
o
C

i

d
o
m
g
S

i

Bilinear sampler & Conv.

*
Recursive Refinement Networks

e
c
n
e
d
i
f
n
o
C

i

d
o
m
g
S

i

 

 

N
B
+
v
n
o
C

U
L
e
R
+

 

 
 

 

 

N
B
+
v
n
o
C

𝑄(cid:4593)

𝑌(cid:3020)

𝑍

Figure 2. The network conﬁguration of LAF-Net which consists of four sub-networks, including feature extraction networks, attention
inference networks, scale inference network, and recursive reﬁnement networks. Given matching cost, disparity, and color image as input,
our networks output conﬁdence of the disparity. The detail of scale inference network is illustrated in Fig. 4.

cost [38], matching cost and disparity [18, 21], and dispar-
ity and color [7, 40]. In order to extract conﬁdence features
from matching cost and disparity, Kim et al. [21] proposed
the top-K pooling layer to normalize matching cost and im-
proved the discriminative power to classify the unrelibale
pixels. Although these methods improved the conﬁdence
estimation performance, they did not make full use of the
tri-modal input.

In [21], the multi-scale disparity feature extractor was
proposed, while dilation convolution was applied in [7] to
gain local contexualized information effectively.
In [40],
they proposed global conﬁdence measure using encoder-
decoder networks by looking at the whole image and dispar-
ity content. By using the output of global conﬁdence, they
proposed local-global approach by fusing the local conﬁ-
dence, the global conﬁdence, and disparity. All of these
methods considered only ﬁxed and pre-deﬁned scale ranges
and did not estimate a scale that varying for each pixel.
On the other hand, the conﬁdence reﬁnement networks [31]
were also developed, which can improve the accuracy of the
estimated conﬁdence map by leveraging a local consistency
within the conﬁdence map.

3. Proposed Method

3.1. Problem Statement and Motivation

Let us deﬁne a pair of stereo images as I l and I r, re-
spectively. The objective of stereo matching is to estimate a
disparity Di between the stereo image pairs that is deﬁned

i and I r

for each pixel i = [ix, iy]T . The matching costs Ci,d be-
i′ , where i′ = i − [d, 0]T , among disparity
tween I l
candidates d = {1, ..., dmax} are ﬁrst measured, and then
aggregated and optimized for computing the disparity Di.
Most existing methods for stereo matching [10, 17, 45] can-
not provide fully reliable results due to its challenging ele-
ments, thus several approaches [39, 27, 37, 18, 20] have
presented an additional module to predict a conﬁdence Qi
of the disparity Di. By leveraging the conﬁdence Qi, they
reﬁne the initial disparity Di through subsequent disparity
reﬁnement pipeline.

To realize this, we design a novel network architecture
that estimates the conﬁdence by fully exploiting match-
ing cost C, disparity map D, and color image I. The
overall networks consist of four sub-networks, including
conﬁdence feature extraction networks, attention inference
networks, scale inference networks, and recursive conﬁ-
dence reﬁnement networks, as illustrated in Fig. 2.
In
feature extraction networks, conﬁdence features are ﬁrst
extracted from tri-modal input. The intermediate features
from this network are then fed to learn locally-varying at-
tention maps in attention inference networks. The atten-
tion maps are used to adaptively concatenate the tri-modal
conﬁdence features, unlike existing approaches [18, 21,
7, 40] that use a simple concatenation technique. Then,
locally-varying scale ﬁelds are learned for extracting conﬁ-
dence features within geometrically-aligned receptive ﬁelds
through scale inference networks, different from conven-
tional approaches [30, 37, 21] with a ﬁxed-size convolution.

207

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3. The visulization of the attention maps: (a) top-1 match-
ing cost, (b) initial disparity, (c) left color image, (d)-(f) the atten-
tion maps for matching cost, disparity, and color, respectively.

Finally, the conﬁdence is progressively reﬁned in recursive
conﬁdence reﬁnement networks to enforce a spatial context
and local consistency inspired by [20, 31].

3.2. Conﬁdence Feature Extraction Networks

The conﬁdence feature extraction networks are designed
to extract the tri-modal conﬁdence features denoted as X C ,
X D, and X I from matching cost C, disparity D, and left
color image1 I l by feed-forward processes such that X C =
F(C; W C), X D = F(D; W D), and X I = F(I l; W I )
with network parameters W C , W D, and W I , respectively.
The network parameters for each network are seperately
learned, not shared, to encode the heterogeneous charater-
istics of the tri-modal input. The size and absolute value
of raw matching cost Craw vary depending on the search
range of stereo image pairs and stereo matching methods.
Additionally, its distribution is often non-discriminative as
mentioned in [37, 7]. To alleviate these limitations, the in-
put matching cost Craw is converted into a top-K match-
ing probabiliry2 C as in [18, 21], which enables the search
range-invariant convolutions.

The conﬁdence feature extraction networks consist of 3
convolutional layers (Conv) with 3 × 3 kernels producing
64 feature channel, followed by batch normalization (BN)
and rectiﬁed linear units (ReLU).

3.3. Attention Inference Networks

Due to their heterogeneous attributes, a direct concate-
nation of these tri-modal input does not provide an optimal
performance [7]. Alternatively, some methods [18, 7, 40,
21] ﬁrst extract the bi-modal conﬁdence features and then
concatenate them. However, such a simple approach that
ﬁxes the fusion weights at inference often fails to perform

1We use a left color image only to estimate the conﬁdence of left dis-
parity and a right image can be used when estimating the conﬁdence of
right disparity.

2We denote this as the matching cost for the sake of clarity.

an optimal feature fusion.

i , AD

i , and AI

To alleviate this limitation, inspired by [15], we build the
attention inference networks for inferring an optimal fusion
weight among the tri-modal features, i.e., X C , X D, and
X I . The locally-varying attention for each modality at pixel
i is deﬁned as AC
i for matching cost, disparity,
and color image, respectively. These attentions are learned
such that AC
D ), and
AI
i = F(X I
C , W A
D ,
and W A
I , and these attentions then undergoes a softmax
function to make the sum of attentions for each pixel to be 1,
i ) = 1. Note that the attention inference
D , and
I ) are not shared but independently learned depending

network parameters for each modality (i.e., W A
W A
on their attributes.

i ; W A
I ) with the network parameters W A

i.e., P∗∈C,D,I (A∗

i = F(X C
i ; W A

i = F(X D

C ), AD

C , W A

i ; W A

The learned attentions are then applied to the conﬁdence

feature as

Yi = Π(cid:0)X C

i ⊙ AC

i , X D

i ⊙ AD

i , X I

i ⊙ AI
i(cid:1) ,

(1)

where Π(·) is a concatenation operator and ⊙ is an element-
wise multiplication operator. Note that unlike methods [7,
21, 40] that use the ﬁxed fusion weights, the attentions AC ,
AD, and AI , are estimated conditioned on input and varies
locally, thus enabling the data-adaptive fusion more effec-
tively. The visualization of attention maps for different in-
put modalities is exempliﬁed in Fig. 3. The attention of
top-K matching cost is high for pixels having high matching
probability. On the other hand, the attention of disparity has
high value in noisy region, indicating informative features
can be extracted from the differnet disparity assignments,
as considered similar to VAR or MDD [8] in handcafted
features. In color image, the attentions near image bound-
ary are high and this indicates a image texture can give a
useful cue to estimate conﬁdence. By adaptively weighting
the conﬁdence features with these attention maps, we can
obtain more discriminative conﬁdence features.

The attention learning networks consist of 2 Conv with
3 × 3 kernels. The ﬁrst Conv produces 64 channel feature,
followed by BN and ReLU, and the second Conv produces
1 channel feature followed by only BN.

3.4. Scale Inference Networks

The optimal receptive ﬁelds for conﬁdence features can
vary at each pixel. In order to encode conﬁdence features of
different scales, some approaches [27, 7, 21, 40] have been
proposed, but they consider only ﬁxed and pre-deﬁned scale
ranges and do not estimate scales that vary for each pixel.

To determine the optimal receptive ﬁelds for conﬁdence
features at each pixel, we present the scale inference net-
works that learn locally-varying scale ﬁelds. It ﬁrst infers
the scale ﬁelds through subsequent convolutions such that
Si = F(Yi; W S) with network parameters W S. With these
scale ﬁelds Si, the intermediate features are warped through

208

 

 

(cid:1861)(cid:1845)(cid:3036)

a

(cid:1862)
(cid:1851)

(cid:1862)(cid:3020)

Conv.

(cid:1851)(cid:3020)

(cid:1852)

Figure 4. Illustration of a bilinear sampler in the scale inferent net-
works: for each pixel i in the feature Y can be warped as enlarged
size feature Y S . The neighbors j S is convolved as Z with stride.





an image sampling on a parameterized grid, similar to spa-
tial transformer networks (STNs) [14].

However, a spatially-varying parameterized sampling
grid cannot be directly realized with the original STNs [14]
that is designed for a global geometric ﬁeld. To deal with
locally-varying scale ﬁelds, we ﬁrst build a locally-varying
sampling grid for N × N neighbors j ∈ Ni indenpendently,
and then warp the convolutional activation for each sam-
pling grid as used in [4, 19]. Concretely, the locally-varying
sampling grid jS = [jS

x , jS

y ]T is deﬁned such that
jy − iy (cid:21) +(cid:20) ix
iy (cid:21) ,

0 Si (cid:21)(cid:20) jx − ix

0

(cid:20) jS

y (cid:21) = (cid:20) Si

jS

x

(2)

for all pixels i and their neighbors j within receptive ﬁelds
on the regular grid. For each grid sample jS = [jS
y ]T ,
receptive ﬁelds for convolutional layers are warped through
the bilinear sampler [14] independently such that

x , jS

Y S

i,j = Xi

Yimax(0, 1 − |jS

x − ix|)max(0, 1 − |jS

y − iy|),

(3)
where Y S
i,j is the warped convolutional activation of Yi,j .
Since this scale-varying convolutional features are deﬁned
for all i and j independently, the spatial size of Y S is en-
larged as |N | times of the size of Y without overlap as il-
lustrated in Fig. 4. Then, Y S passes through a subsequent
convolution with the stride N to convolve the warped fea-
tures independently and generate the scale-adaptive conﬁ-
dence features Z. We chose N as 3 since the kernel size of
following convolutional layer is 3 × 3.

The scale learning networks consist of 2 Conv with 3 × 3
kernels. The ﬁrst Conv produces 64 channel feature, fol-
lowed by BN and ReLU, and the second Conv produces
1 channel feature followed by only BN. The output passes
through the sigmoid layer to generate the scale parameter
for each pixel.

(a)

(c)

(e)

(b)

(d)

(f)

Figure 5. The effectiveness of the proposed recursive conﬁdence
reﬁnement networks: (a) left color image, (b) initial disparity, (c)
estimated conﬁdence map without recursive module, (d) thresh-
olded disparity with (c), (e) estimated conﬁdence map with recur-
sive module, (f) thresholded disparity with (e). The mismatched
pixels in the red boxes are reliably detected with the proposed re-
cursive conﬁdence reﬁnement networks.

networks. From the conﬁdence feature Zi, we ﬁnally for-
mulate the conﬁdence prediction networks to estimate the
conﬁdence Qi such that Qi = F(Zi; W P ) with the pre-
diction parameters W P . The iterative reﬁnement proce-
dure of output conﬁdence can improve the conﬁdence es-
timation accuracy as studied in the handcrafted approach
using joint ﬁltering [20] and CNNs-based approach [31].
Inspired by this, we propose the recursive conﬁdence reﬁne-
ment networks, where the previously estimated conﬁdence
serves as a guidance of the current conﬁdence estimation.
To realize this recursive module, we formulate the networks
such that Qt
are
the estimated conﬁdences at tth and (t − 1)th iteration, re-
spectively. The initial conﬁdence Q0
i is deﬁned as zeros.
As evolving the iterations, the conﬁdence accuracy is im-
proved gradually and the ﬁnal conﬁdence map is obtained
as Q′ = Qtmax . The effectiveness of the recursive conﬁ-
dence reﬁnement networks is shown in Fig. 5. Here, we set
0.9 to threshold. With the recursive module, the ability to
predict mismatched pixels on initial disparity is improved.
The recursive conﬁdence reﬁnement networks consist of
2 Conv and ﬁnal sigmoid layer similar to the scale learning
networks. For the number of iteration, we set tmax to 3.

i = F(Zi, Qt−1

; W P ) where Qt

i and Qt−1

i

i

The proposed method employs the cross-entropy loss
function [38, 21] with respect to the ground-truth conﬁ-
dence Q∗ and the estimated conﬁdence Q′
i.

4. Experimental Results

3.5. Recursive Conﬁdence Reﬁnement Networks

4.1. Experimental Settings

So far we introduce our networks that fuse tri-modal con-
ﬁdence features through the attention and scale inference

The proposed method was implemented in MATLAB
with VLFeat MatConvNet toolbox [42] and simu-

209

Match. cost
Disparity
Color
MID 2006
MID 2014
KITTI 2015

X

X

X

X

X

X

X

X

X

0.0431 0.0392 0.0381 0.0375 0.0364
0.0762 0.0703 0.0687 0.0685 0.0683
0.0347 0.0245 0.0237 0.0231 0.0225

0.25

0.2

0.15

0.1

0.05

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

Table 1. Ablation study for the various combination of input
modalities in LAF-Net on MID 2006 [34], MID 2014 [33], and
KITTI 2015 [24] dataset, when the raw matching cost is obtained
using MC-CNN [45].

Attention
Scale
Recursive
MID 2006
MID 2014
KITTI 2015

X

X

X

X

X

X

X

X

0.0374 0.0375 0.0372 0.0371 0.0364
0.0686 0.0688 0.0685 0.0685 0.0683
0.0235 0.0236 0.0231 0.0229 0.0225

Table 2. Ablation study for the effectivness of each sub-networks
in LAF-Net on MID 2006 [34], MID 2014 [33], and KITTI
2015 [24] dataset, when the raw matching cost is obtained using
MC-CNN [45]. The average AUC values for simple concatenation
without fusion methods are 0.0386, 0.0689, and 0.0238 for MID
2006, MID 2014, and KITTI 2015, respectively.

lated on a PC with TitanX GPU. We make use of the
stochastic gradient descent with momentum, and set the
learning rate to 1 × 10−6 and the batch size to 16. To com-
pute a raw matching cost, we used a census transform with a
5×5 local window and MC-CNN [45], respectively. For the
census transform, we applied SGM [10] on estimated cost
volumes by setting P1 = 0.008 and P2 = 0.126 as in [27].
For computing the MC-CNN, ‘KITTI 2012 fast network’
was used, provided at the author’s website [46]. We set
σ as 100 and 0.05 for census-SGM and MC-CNN, respec-
tively, as in [21]. We trained our networks using MPI Sintel
dataset [3] and KITTI 2012 dataset [24], and evaluated each
model on Middlebury 2006 (MID 2006) [34], Middlebury
2014 (MID 2014) [33], and KITTI 2015 dataset [24]. In ad-
dition, we used the half-sized KITTI database due to mem-
ory constraints, so we measured the error rates and AUC
values in the half-sized resolution. For Middlebury, we
used the third-sized images provided by [34]. The ground-
truth conﬁdence maps are obtained by thresholding an ab-
solute difference between estimated disparity and ground-
truth disparity to 1. In inference, the LAF-Net takes about
0.912s, 2.413s, and 0.783s for MID 2006 (368×424), MID
2014 (496×792), and KITTI 2015 (608×184), respectively,
while [40] takes 0.750s, 1.628s, and 0.552s in the same set-
tings. Due to the bilinear sampler and recursive procedure,
the LAF-Net takes longer than [40]. In contrast, the number
of parameters in LAF-Net and [40] is 1,337K and 9,289K,
proving that LAF-Net is lighter while achieving a better ac-
curacy.

0.14

0.12

0.1

0.08

0.06

0.04

0.02

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

0

0

0.25

0.2

]

%
[
e
t
a
r
 
l
e
x
i
p
 
d
a
B

0.15

0.1

0.05

0

0

10

20

30

40

50

60

70

80

90

100

Sparsification[%]

Sparsification[%]

(a)

(b)

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

0

0

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

Sparsification[%]

Sparsification[%]

(c)

(d)

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

0.35

0.3

0.25

0.2

0.15

0.1

0.05

]

%
[
e
t
a
r
 
l
e
x
i
p
 
d
a
B

0

0

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

0

0

10

20

30

40

50

60

70

80

90

100

Sparsification[%]

Sparsification[%]

(e)

(f)

Figure 6. The sparsiﬁcation curves of selected images for MID
2006 [34], MID 2014 [33], and KITTI 2015 dataset [24] using (a),
(c), (e) census-SGM and (b), (d), (f) MC-CNN. The sparsiﬁcation
curve for the ground-truth conﬁdence map is described as ‘opti-
mal’.

In the following, we evaluated the proposed method in
comparison to conventional handcrafted approaches, such
as Haeusler et al. [8], Spyropoulos et al. [39], Park and
Yoon [27], Poggi and Mattoccia [29], Kim et al. [20]. Sev-
eral CNNs-based approaches using single- or bi-modal in-
put are also compared, where using disparity only, such
as Poggi and Mattoccia (CCNN) [30], Seki and Pollefey
(PBCP) [37], matching cost only, such as Shaked et al. [38],
both disparity and matching cost, such as Kim et al. [21],
and both color and disparity, such as Fu et al. (LFN) [7]
and the global measures of Tosi et al. (ConfNet) [40] and
local and global measures (LGC-Net) [40]. We obtained
the results of [27], [20], and [21] by using the author-
provided code, while the results of [8], [39], [37], [38],
and [7] were obtained by our own implementation. We re-
implemented methods of [29], [30], and [40] based on the
author-provided code.

To evaluate the performance of conﬁdence estimation
quantitatively, we used the sparsiﬁcation curve and its area
under curve (AUC) as used in [8, 39, 27, 37, 21]. The spar-

210

Datasets

Haeusler et al. [8]
Spyropoulos et al. [39]
Park and Yoon [27]
Poggi et al. [29]
Kim et al. [20]
CCNN [30]
PBCP [37]
Shaked et al. (Conf) [38]
Kim et al. (Conf) [21]
LFN [7]
ConfNet [40]
LGC-Net [40]
LAF-Net
Optimal

MID 2006 [34]

MID 2014 [33]

KITTI 2015 [24]

Census-SGM MC-CNN

Census-SGM MC-CNN

Census-SGM MC-CNN

0.0454
0.0447
0.0438
0.0439
0.0430
0.0454
0.0462
0.0464
0.0419
0.0416
0.0451
0.0413
0.0405
0.0340

0.0417
0.0420
0.0426
0.0413
0.0409
0.0402
0.0413
0.0495
0.0394
0.0393
0.0428
0.0389
0.0364
0.0323

0.0841
0.0839
0.0802
0.0791
0.0772
0.0769
0.0791
0.0806
0.0749
0.0752
0.0783
0.0735
0.0718
0.0569

0.0750
0.0752
0.0734
0.0707
0.0701
0.0716
0.0718
0.0736
0.0694
0.0692
0.0721
0.0685
0.0683
0.0527

0.0585
0.0536
0.0527
0.0461
0.0430
0.0419
0.0439
0.0531
0.0407
0.0405
0.0486
0.0392
0.0385
0.0348

0.0308
0.0323
0.0303
0.0263
0.0294
0.0258
0.0272
0.0292
0.0250
0.0253
0.0277
0.0236
0.0225
0.0170

Table 3. The average AUC values for MID 2006 [34], MID 2014 [33], and KITTI 2015 [24] dataset. The AUC value of ground truth
conﬁdence is measured as ‘Optimal’. The result with the lowest AUC value in each experiment is highlighted.

(a)

(b)

Figure 7. Comparisons of AUC values for (a) census-based SGM
and (b) MC-CNN for the KITTI 2015 dataset [24]. We sort the
AUC values in the ascending order according to the AUC values.

siﬁcation curve draws a bad pixel rate while successively
removing pixels in descending order of conﬁdence values
in the disparity map, thus it enables us to observe the ten-
dency of estimation errors. For the higher accuracy of the
conﬁdence measure, AUC value is lower and the optimal
AUC is measured using ground-truth conﬁdence.

4.2. Ablation Study

We analyzed our conﬁdence estimation networks with
the ablation evaluations, with respect to various combina-

tion of different modalities and the effectiveness of the pro-
posed sub-networks.

The effects on tri-modal input.
In Table 1, ablation ex-
periments to validate the effects of multi-modal input show
the necessity of using the tri-modal input. Note that the
attention inference module is not used for input of single
modality. Although the bi-modal input improved the ability
to predict reliable pixels, the full usage of tri-modal input
shows the best performance.

The effects on various fusion methods.
In Table 2, abla-
tion experiments to validate the effects of the proposed fu-
sion methods. Compared to the simple concatenation tech-
nique, the conﬁdence estimator is improved with the atten-
tion and scale obtained from the attention and scale infer-
ence networks. Also, the recursive conﬁdence reﬁnement
networks show the additional improvement.

4.3. Conﬁdence Estimation Analysis

In order to measure the performance of the conﬁdence
estimator in comparison to other methods, we compared
the average AUC values of our method with conventional
learning-based approaches using handcrafted conﬁdence
measures [8, 39, 27, 29, 20] and CNNs-based methods [37,
30, 7, 40]. For fair comparison, we also evaluated the conﬁ-
dence estimation performance only for [38, 21], i.e., Shaked
et al. (Conf) [38] and Kim et al. (Conf) [21].

Sparsiﬁcation curves for MID 2006 [34], MID 2014 [33],
and KITTI 2015 [24] with census-based SGM and MC-
CNN are shown in Fig. 6. Fig. 7 describes the AUC
values, which are sorted in ascending order, for the KITTI
2015 [24] with census-based SGM and MC-CNN, respec-
tively. The results have shown that the proposed conﬁ-
dence estimator exhibits a better performance than both

211

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 8. The conﬁdence maps on MID 2006 dataset [34] (ﬁrst two rows) and MID 2014 dataset [33] (last two rows) using census-SGM
and MC-CNN. (a) color images, (b) initial disparity map, (c)-(f) are estimated conﬁdence maps by (c) Kim et al. [21], (d) LFN [7], (e)
LGC-Net [40], (f) LAF-Net, and (g) ground-truth conﬁdence map.

Figure 9. The conﬁdence maps on KITTI 2015 dataset [24] using census-SGM (ﬁrst two rows), and MC-CNN (last two raws). (From top to
bottom, left to right) color images, initial disparity map, estimated conﬁdence maps by CCNN [30], PBCP [37], Kim et al. [21], LFN [7],
LGC-Net [40], and LAF-Net.

conventional handcrafted approaches and CNN-based ap-
proaches. The average AUC with census-based SGM and
MC-CNN for MID 2006, MID 2014, and KITTI 2015
datasets were summarized in Table 3. The handcrafted ap-
proaches showed inferior performance than the proposed
method due to low discriminative power. CNNs-based
methods [30, 37, 38, 7] have improved conﬁdence estima-
tion performance compared to existing handcrafted meth-
ods such as [8, 39, 27, 29, 20], but they are still infe-
rior to our method as they rely on single- [30, 38] or bi-
modal [37, 21, 7, 40] input rather than tri-modal input. The
estimated conﬁdence maps are shown in Fig. 8 and Fig. 9.

5. Conclusion

We presented LAF-Net that estimates conﬁdence with
tri-modal input,
including matching cost, disparity, and
color image through deep networks. The key idea of the
proposed method is to design locally adaptive attention
and scale inference networks to generate optimal fusion
weights.
In addition, the conﬁdence estimation perfor-
mance is further improved with recursive conﬁdence reﬁne-
ment networks. A direction for further study is to examine
how conﬁdence estimation networks could be learned in an
unsupervised manner as proposed in [26, 41].

212

References

[1] C. Biernacki, G. Celeux, and G. Govaert. Assessing a mix-
ture model for clustering with the integrated completed like-
lihood. IEEE Trans. Pattern Anal. Mach. Intell., 22(7):719–
725, 2000.

[2] L. Breiman. Random forests. Mach. Learn., 63(4):5–32,

2001.

[3] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation. in
Proc. Eur. Conf. Comput. Vis., pages 611–625, Oct. 2012.

[4] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker. Uni-
versal correspondence network. in Proc. Advances in Neural
Inf. Process. Syst., pages 2414–2422, Dec. 2016.

[5] G. Egnal, M. Mintz, and R. Wildes. A stereo conﬁdence
metric using single view imagery with comparison to ﬁve
alternative approaches.
Image. Vis. Comput., 22(12):943–
957, 2004.

[6] G. Egnal and R. P. Wildes. Detecting binocular half-
occlusions: Empirical comparisons of ﬁve approaches. IEEE
Trans. Pattern Anal. Mach. Intell., 24(8):1127–1133, 2002.

[7] Z. Fu and M. A. Fard.

Learning conﬁdence measures
by multi-modal convolutional neural networks.
in Proc.
IEEE Winter Conf. Applicat. Comput. Vis., pages 1321–
1330, 2018.

[8] R. Haeusler, R. Nair, and D. Kondermann. Ensemble learn-
ing for conﬁdence measrues in stereo vision. in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., pages 305–312, Jun.
2013.

[9] Y. Heo, K. Lee, and S. Lee. Robust stereo matching using
adaptive normalized cross correlation. IEEE Trans. Pattern
Anal. Mach. Intell., 33(4):807–822, 2011.

[10] H. Hirschmuller. Stereo processing by semiglobal matching
and mutual information. IEEE Trans. Pattern Anal. Mach.
Intell., 30(2):328–341, 2008.

[11] H. Hirschmuller, P. Innocent, and J. Garibaldi. Real-time
correlation-based stereo vision with reduced border errors.
Int. J. Comput. Vis., 47(1–3):229–246, 2002.

[12] X. Hu and P. Mordohai. A quantitative evaluation of conﬁ-
dence measures for stereo vision. IEEE Trans. Pattern Anal.
Mach. Intell., 34(11):2121–2133, 2012.

[13] M. Humenberger, C. Zinner, M. Weber, W. Kubinger, and
M. Vincze. A fast stereo matching algorithm suitable for em-
bedded real-time systems. Comput. Vis. Image. Understand.,
114(11):1180–1202, 2010.

[14] M. Jaderberg, K. Simonyan, and A. Zisserman. Spatial trans-
former networks. in Proc. Advances in Neural Inf. Process.
Syst., pages 2017–2025, Dec. 2015.

[15] X. Jia, B. D. Brabandere, T. Tuytelaars, and L. V. Gool. Dy-
namic ﬁlter networks. in Proc. Advances in Neural Inf. Pro-
cess. Syst., pages 667–675, Dec. 2016.

[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., pages 1725–1732, Jun. 2014.

[17] S. Kim, B. Ham, B. Kim, and K. Sohn. Mahalanobis
distance cross-correlation for illumination invariant stereo

matching. IEEE Trans. Circ. Syst. Vid. Techn., 24(11):1844–
1859, 2014.

[18] S. Kim, D. Min, B. Ham, S. Kim, and K. Sohn. Deep stereo
in Proc. IEEE

conﬁdence prediction for depth estimation.
Conf. Image. Process., Sep. 2017.

[19] S. Kim, D. Min, B. Ham, S. Lin, and K. Sohn. Fcss: Fully
convolutional self-similarity for dense semantic correspon-
dence. IEEE Trans. Pattern Anal. Mach. Intell., 2017.

[20] S. Kim, D. Min, S. Kim, and K. Sohn. Feature augmentation
for learning conﬁdence measure in stereo matching. IEEE
Trans. Image Process., 26(12):6019–6033, 2017.

[21] S. Kim, D. Min, S. Kim, and K. Sohn. Uniﬁed conﬁdence
estimation networks for robust stereo matching. IEEE Trans.
Image Process., 28(3):1299–1313, 2019.

[22] A. Liaw and M. Wiener. Classiﬁcation and regression by

random forest. R news, 2(3):18–22, 2002.

[23] X. Mei, X. Sun, M. Zhou, S. Jiao, H. Wang, and X. Zhang.
On building an accurate stereo matching system on graphics
hardware. in Proc. IEEE Int. Conf. Comput. Vis. Work., pages
467–474, Nov. 2011.

[24] M. Menze and A. Geiger. Object scene ﬂow for autonomous
vehicles. in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., pages 3061–3070, Jun. 2015.

[25] P. Mordohai. The self-aware matching measure for stereo. in
Proc. IEEE Int. Conf. Comput. Vis., pages 1841–1848, Sep.
2009.

[26] C. Mostegel, M. Rumpler, F. Fraundorfer, and H. Bischof.
Using self-contradiction to learn conﬁdence measures in
stereo vision.
in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., pages 4067–4076, Jun. 2016.

[27] M. Park and K. Yoon. Leveraging stereo matching with
in Proc. IEEE Conf.

learning-based conﬁdence measures.
Comput. Vis. Pattern Recognit., pages 101–109, Jun. 2015.

[28] M. Park and K. Yoon. Learning and selecting conﬁdence
IEEE Trans. Pattern

measures for robust stereo matching.
Anal. Mach. Intell., 2018.

[29] M. Poggi and S. Mattoccia. Learning a general-purpose con-
ﬁdence measure based on o(1) features and a smarter aggre-
gation strategy for semi global matching. in Proc. IEEE Int.
Conf. 3D Vis., pages 509–518, Oct. 2016.

[30] M. Poggi and S. Mattoccia. Learning from scratch a con-
in Proc. Brit. Mach. Vis. Conf., 10, Sep.

ﬁdence measure.
2016.

[31] M. Poggi and S. Mattoccia. Learning to predict stereo re-
liability enforcing local consistency of conﬁdence maps. in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017.

[32] M. Poggi, F. Tosi, and S. Mattoccia. Quantitative evaluation
of conﬁdence measures in a machine learning world. in Proc.
IEEE Int. Conf. Comput. Vis., Oct. 2017.

[33] D. Scharstein, H. Hirschmuller, Y. Kitajima, G. Krathwohl,
N. Nesic, X. Wang, and P. Westling. High-resolution stereo
datasets with subpixel-accurate ground truth. in Proc. Ger-
man Conf. Pattern Recognit., pages 31–42, Sep. 2014.

[34] D. Scharstein and C. Pal. Learning conditional random ﬁelds
for stereo. in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., pages 1–8, Jun. 2007.

213

[35] D. Scharstein and R. Szeliski. Stereo matching with non-

linear diffusion. Int. J. Comput. Vis., 28(2):155–174, 1998.

[36] D. Scharstein and R. Szeliski. A taxonomy and evaluation
of dense two-frame stereo correspondence algorithms. Int. J.
Comput. Vis., 47(1–3):7–42, 2002.

[37] A. Seki and M. Pollefeys. Patch based conﬁdence prediction
for dense disparity map. in Proc. Brit. Mach. Vis. Conf., 10,
Sep. 2016.

[38] A. Shaked and L. Wolf. Improved stereo matching with con-
stant highway networks and reﬂective conﬁdence learning. in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2017.
[39] A. Spyropoulos, N. Komodakis, and P. Mordohai. Learning
to detect ground control points for improving the accuracy of
stereo matching. in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., pages 1621–1628, Jun. 2014.

[40] F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia. Beyond
local reasoning for stereo conﬁdence estimation with deep
learning. in Proc. Eur. Conf. Comput. Vis., Sep. 2018.

[41] F. Tosi, M. Poggi, A. Tonioni, L. D. Stefano, and S. Mattoc-
cia. Learning conﬁdence measures in the wild. in Proc. Brit.
Mach. Vis. Conf., page 2, Sep. 2017.

[42] A. Vedaldi and K. Lnc. Matconvnet: Convolutional neural
in Proc. ACM Int. Conf. Multimedia,

networks for matlab.
pages 689–692, Oct. 2015.

[43] K. J. Yoon and I. S. Kweon. Adaptive support-weight ap-
proach for correspondence search. IEEE Trans. Pattern Anal.
Mach. Intell., 28(4):650–656, 2006.

[44] R. Zabih and J. Woodﬁll. Non-parametric local transforms
in Proc. Eur. Conf.

for computing visual correspondence.
Comput. Vis., pages 151–158, May 1994.

[45] J. Zbontar and Y. LeCun. Computing the stereo matching
cost with a convolutional neural network.
in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1592–1599, Jun.
2015.

[46] J. Zbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. Jour-
nal of Machine Learning Research, 17:1–32, 2016.

214

