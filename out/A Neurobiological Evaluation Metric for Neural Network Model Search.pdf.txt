A Neurobiological Evaluation Metric for Neural Network Model Search

Nathaniel Blanchard

Jeffery Kinnison

Computer Science and Engineering

Computer Science and Engineering

University of Notre Dame

University of Notre Dame

nblancha@nd.edu

Brandon RichardWebster

jkinniso@nd.edu

Pouya Bashivan

Computer Science and Engineering

University of Notre Dame

McGovern Institute for Brain Research and
Dept. of Brain and Cognitive Sciences, MIT

brichar1@nd.edu

bashivan@mit.edu

Walter J. Scheirer

Dept. of Computer Science and Engineering

University of Notre Dame

walter.scheirer@nd.edu

Abstract

Neuroscience theory posits that the brain’s visual sys-
tem coarsely identiﬁes broad object categories via neural
activation patterns, with similar objects producing similar
neural responses. Artiﬁcial neural networks also have inter-
nal activation behavior in response to stimuli. We hypothe-
size that networks exhibiting brain-like activation behavior
will demonstrate brain-like characteristics, e.g., stronger
generalization capabilities.
In this paper we introduce a
human-model similarity (HMS) metric, which quantiﬁes the
similarity of human fMRI and network activation behavior.
To calculate HMS, representational dissimilarity matrices
(RDMs) are created as abstractions of activation behav-
ior, measured by the correlations of activations to stimulus
pairs. HMS is then the correlation between the fMRI RDM
and the neural network RDM across all stimulus pairs. We
test the metric on unsupervised predictive coding networks,
which speciﬁcally model visual perception, and assess the
metric for statistical signiﬁcance over a large range of hy-
perparameters. Our experiments show that networks with
increased human-model similarity are correlated with bet-
ter performance on two computer vision tasks: next frame
prediction and object matching accuracy. Further, HMS
identiﬁes networks with high performance on both tasks. An
unexpected secondary ﬁnding is that the metric can be em-
ployed during training as an early-stopping mechanism.

Figure 1. A primary goal of biologically-inspired deep learning
work is achieving generalization capabilities that more closely re-
semble those of biological brains. Along these lines, we propose
that model search frameworks for neural network training can be
guided by a human-model similarity metric. The metric correlates
internal activation behavior of the human brain and neural net-
works over shared stimuli. In this work, we examine the speciﬁc
case of fMRI recordings [23] and predictive coding networks [29].
Internal behavior is measured by the dissimilarity in activations
between two stimuli. Human-model similarity is the comparison
of internal behavior of a brain and a model on a stimulus set, where
higher similarity implies better model generalization.

15404

1. Introduction

Researchers originally designed artiﬁcial neural net-
works based on neurobiological structure and function in
the hope that such networks would approximate the perfor-
mance of the biology that inspired them [44]. With the ad-
vent of modern deep learning techniques, neural networks
are ﬁnally beginning to realize this original objective across
some pattern recognition problems [25]. However, we need
only consider the learning and processing power of the brain
to know that neural network performance is a far stretch
from generalized human capabilities [5, 16, 40, 41]. This
shortcoming has inspired researchers to design new net-
works which better approximate neurobiological structure,
utilizing the architectural elements of machine learning to
build networks that embody modern theories of brain or-
ganization [29, 30, 42, 43, 47, 53]. In this paper we look
beyond structural similarity and consider behavioral simi-
larity between biological brains and trained networks (i.e.,
models), as measured by similarity of activation behavior
across a set of stimuli. We hypothesize that networks with
increased behavioral similarity will exhibit better general-
ization capabilities across different visual recognition tasks.
One neurobiologically-inspired network is the unsuper-
vised predictive coding network [29, 39]. Predictive cod-
ing networks combine the empirical successes of neural
networks with insights from computational neuroscience to
train unsupervised models with increased biological ﬁdelity
(i.e., the correspondence of an algorithm’s representations,
transformations, and learning rules with those of their coun-
terparts in the brain). The networks are designed [29] and
demonstrated [30] to embody the theory that in-the-wild bi-
ological vision systems are continuously predicting the next
input signal [39]. Additionally, these networks are trained
using unsupervised video data, something also done by
biological beings [25], allowing large-scale unsupervised
learning. Finally, the networks have been shown to perform
well on at least two different tasks: next frame prediction
and object matching [29].

Predictive coding networks are architecturally designed
to emulate neural processing. However, the ability of bi-
ological beings to generalize and adapt extends from both
structure and internal behavior. Internally, the visual sys-
tem processes similar objects with similar patterns of cell
activations [7, 14, 33]. This activation behavior is an ob-
servable manifestation of the brain’s ability to generalize
beyond its experience, such as automatically allowing the
classiﬁcation of unseen instances of object classes (e.g., cor-
rectly identifying a car despite never having seen this spe-
ciﬁc car before). We hypothesize that predictive coding net-
works which mimic the brain’s visual behavior will exhibit
increased biological ﬁdelity and thus possess strong gener-
alization ability compared to coding networks that do not
exhibit this behavior. To test this hypothesis we investigate

a new human-model similarity metric (HMS) that evalu-
ates the networks for internal behavioral similarity to fMRI
recordings of the human brain (Fig. 1).

Systems such as predictive coding networks and biologi-
cal brains both exhibit internal behavior through their neural
activations. Therefore, assessing networks for internal be-
havior indicative of biological ﬁdelity requires measuring
the similarity of activations. To do this, we make use of
the recently established technique of representational simi-
larity analysis (RSA) [23, 32]. RSA utilizes a set of stimuli
to quantify behavioral similarity from activations. For any
brain or network, the activation power can be measured in
response to a stimulus. The internal behavior can then be
deﬁned as the dissimilarity in activations over a set of stim-
uli. In the case of visual recognition, we expect like-stimuli
to have like-activations. We utilize a set of stimuli selected
to exhibit a range of both similar and dissimilar objects [24].
One problem with assessing the similarity of activation
behavior of biological beings and neural networks is the ab-
sence of a one-to-one mapping between the neurons of the
brain and the neurons of the networks. With RSA, com-
plex systems are abstracted into representational dissimilar-
ity matrices (RDMs), composed of the internal behavior of
the system, which is the activation dissimilarity over the set
of stimuli. The full process to abstract a system into an
RDM is illustrated in Fig. 2. Two input systems can be di-
rectly mapped when both are abstracted into RDMs with the
same stimuli. Our proposed HMS metric measures human-
model similarity as the correlation of a human fMRI RDM
and a neural network RDM.

We evaluate the HMS metric in a Monte Carlo scenario
across a broad range of hyperparameterized networks, data
domains, and alternative network metrics. This approach
allows us to explore the range of internal behavioral sim-
ilarity that we can expect to ﬁnd in predictive coding net-
works. Additionally, this method allows us to consider how
a metric for human-model similarity could be used in the
model search process for neural network training. While
RSA has been employed to analyze similarities between
convolutional neural networks (CNNs) and biological be-
haviors [21, 32, 51, 52], a generalized human-model simi-
larity metric as an evaluation of a network’s neurbiological
ﬁdelity and its use in neural network model search has re-
mained largely untested until now. Our goal is to present a
data-driven study of the HMS metric in order to promote it
as a tool for studying generalization in computer vision.

In summary, we make the following contributions: (1)
The introduction and evaluation of a new human-model
similarity metric, dubbed HMS, to measure network gen-
eralizability.1
(2) The implementation of a metric evalu-
ation framework to assess new machine learning perfor-
(3) The discovery of HMS as an indica-
mance metrics.

1https://github.com/CVRL/human-model-similarity

5405

tor of a predictive coding network’s performance via ex-
periments on the KITTI [15], VLOG [13], and “Gazoobian
Object” [48] datasets. (4) The identiﬁcation of HMS as an
early stopping mechanism for training.

2. Related Work

How to best evaluate machine learning algorithms is an
ongoing discussion. Traditional evaluations focus on ex-
ternal performance on a dataset, but there are no guarantees
against overﬁtting or unpredictable network performance on
real-world data [49]. One alternative evaluation regime is
visual psychophysics, which monitors neural network per-
formance while increasingly perturbing stimuli [26, 40, 41].
This evaluation centers on the observation that a network
which inconsistently recognizes perturbed stimuli cannot be
trustworthy. However, these evaluations are still focused on
creating variability within a dataset, offering no guarantee
that the network is not simply overﬁt to it. Moving be-
yond datasets, our proposed evaluation metric HMS quanti-
ﬁes consistency in a network’s internal behavior by directly
comparing against the internal behavior of one of the most
generalizable vision systems in the world:
the biological
brain [7, 14, 33].

HMS uses human participant fMRI data as ground-truth
internal behavior that leads to good generalization. The
comparison between networks and human fMRI data is in-
spired by Kriegeskorte et al. [23], who described how net-
work or neural activations could be abstracted into an RDM.
An RDM is an abstract representation that can be directly
compared against another RDM, as long as both are created
from a joint set of stimuli. Fig. 2 shows how internal behav-
ior is calculated and abstracted into RDMs, and how RDMs
can be compared. Sec. 3.3 describes the formal RDM cre-
ation process. Kriegeskorte has a long history of utilizing
RDMs to study neural behavior [21, 22, 23, 24, 34, 35].

With respect to the intersection between neuroscience
and machine learning, the neuroimaging technique of fMRI
has been used as ground-truth for designing features [8],
interpreting neural network features [19, 28], and study-
ing network performance [46]. Fong et al. [12] recently
found that raw fMRI data could be used to weight sup-
port vector machines to improve performance, indicating
that coarse-level brain data can potentially help machine
learning networks generalize. The success of that study,
alongside the public release of human fMRI data in RDM
form by Nili et al. [35] further motivated us to use fMRI
data as ground-truth in our network evaluation. The speciﬁc
contributions fMRI data can make in expanding our under-
standing of neural networks are still to be explored, but to
our knowledge this is the ﬁrst instance of fMRI data being
deployed for neural network model search, where the task
is to screen different hyperparameter and architecture con-
ﬁgurations for models that perform well on a given task.

There is signiﬁcant recent interest in optimization meth-
ods, search strategies, and infrastructure for neural network
model search [3, 10, 18, 27, 36]. In this context, our work
represents a new capability for such searches.

Extensive research has been performed comparing the
neural activity of macaques to CNNs [17, 20, 50, 51, 52].
These studies map CNN layers to anatomical visual ar-
eas measured with electrode arrays. Recently, research has
shown that these internal representations are not predictive
of primate behavior at the image level [38, 45], suggest-
ing CNNs are not mimicking internal behavior well enough.
Given these recent ﬁndings, we opted to study more biolog-
ically plausible predictive coding networks [29, 39]. These
networks are unsupervised and are relatively unexplored
for many problem domains, but yield state-of-the-art per-
formance for problems such as next frame prediction. We
selected the PredNet architecture because of research es-
tablishing its emergent properties that are consistent with
biological vision [30], meaning it is not grounded only in
theory. Nonetheless, there are many biologically-inspired
neural network architectures [37, 42, 43, 47, 53], and in-
terest in them continues to grow [2]. All such networks
warrant an investigation into internal behavior as well.

3. Methods

In this section we introduce the core methodologies
surrounding the HMS metric.
First, we introduce the
biologically-inspired predictive coding network used for the
experiments. We then explain the evaluation framework
that was used to study HMS, and discuss the computer vi-
sion tasks (object matching and next frame prediction) that
network performance was evaluated on. Finally, we detail
the metric itself (Fig. 2), explaining: (1) the abstraction of
both the fMRI recordings and neural networks into individ-
ual RDMs by measuring activations in response to stimuli,
and (2) the correlation of the fMRI and the neural network
RDMs, which results in the HMS score.

3.1. PredNet: A Biologically Inspired Network

PredNet [29] is a recently introduced, unsupervised, bi-
Its archi-
ologically inspired, predictive coding network.
tecture consists of multiple layers (which can vary based
on conﬁguration) each incorporating representation neurons
(convolutional LSTM units), which output layer-speciﬁc
predictions at each time step when processing a sequence
of data. This output is then compared against a target to
calculate an error term, which is propagated laterally and
vertically throughout the network. We follow the PredNet
training regime laid out by Lotter et al. [29]. PredNet is
trained without supervision:
the network is shown a ran-
domly sampled set of sequential frame sequences and upon
viewing each frame, the network attempts to predict the next

5406

Figure 2. The proposed human-model similarity metric HMS is calculated by comparing the neural activation behavior from two systems:
predictive coding networks and fMRI recordings of the human brain. Neural activations are obtained by exposing the systems to stimuli.
We abstractly summarize a source based on its internal behavior, generating a similarity score via ψ from activation patterns for each
stimulus pair. We then store this internal behavior to the stimuli in an RDM (R1 and R2 above). Finally, the HMS metric ρ is equal to the
Spearmen’s rank correlation coefﬁcient of the internal behavior of the two sources as measured by the stimulus pairs.

3.2. Metric Evaluation Framework

Because we are focused on improving generalizability,
we assess the value of HMS as a predictor of other, more
standard, performance measures. This involves varying hy-
perparameters within a network type, obtaining a Monte
Carlo-style statistical sample of the search space, and corre-
lating HMS with standard computer vision evaluation met-
rics across networks in the sample (Fig. 3). We analyze
the networks by studying the mean, standard deviation, and
Spearman’s rho (correlation coefﬁcient) of several perfor-
mance metrics across the set of sampled networks. We
ensure signiﬁcance by reporting Spearman’s rho p values,
which correspond to the likelihood that correlations occur
by chance. We also adhere to Cohen’s standard recom-
mendation for interpreting effect sizes [6], and do not con-
sider small correlations (less than 0.2) when comparing two
different metrics — even if they reached statistical signif-
icance. Further, we perform Bonferroni correction, which
conservatively adjusts signiﬁcance to counteract the multi-
ple testing problem where multiple inferences increase the
likelihood of erroneous inferences [9]. In all of our results,
Bonferroni adjusted p values are reported.

In this study, we correlate HMS with mean-squared er-
ror (MSE) on the next frame prediction task (the default
mode of PredNet), as well as object matching accuracy. In
the experiments, following the protocol established by Lot-
ter et al. [29], MSE is computed as the square of the mean
pixel-wise difference of the predicted next frame and the
actual next frame. Object matching accuracy is evaluated

5407

Figure 3. We assess our proposed HMS evaluation metric on ran-
domly hyperparameterized predictive coding networks to study a
Monte Carlo-style statistical sample of the space. We evaluate
each network with three metrics: HMS, an object matching ac-
curacy metric, and a next frame prediction error metric. We then
compare metric performance across the full set of trained net-
works. We ﬁnd that networks with higher HMS have high per-
formance on other computer vision metrics, and performance is
linked both across and within networks.

frame. The network is optimized to reduce the next frame
prediction error on the training set.

by ﬁrst extracting the neural activations of the ﬁnal layer in
response to a probe image. Neural activations from the ﬁ-
nal layer are then extracted across a gallery of 50 images,
one of which is the same object with altered lighting, color,
viewing angle, or a combination thereof. Cosine similarity
is computed between the probe and the gallery activations,
and the gallery image with the highest activation similarity
to the probe is the predicted match.

3.3. The HMS Metric for Model Search

Several steps are involved in computing the proposed
HMS metric (with the major ones highlighted in Fig. 2).
The RDM creation process described below follows the pro-
cedure of the RSA toolbox [35].

Stimuli selection for RDM construction. The stimuli
were chosen by Kriegeskorte et al. [24] to compare human-
primate neural inferior temporal (IT) object representations.
The stimuli were selected to provide a hierarchical range of
dissimilar and similar objects, such as animate and inani-
mate objects, not human and human objects, and face and
body objects. The full set of stimuli is described in Sec. 1.1
of the Supp. Mat.

Human fMRI collection. The human fMRI data were
released as part of the Representational Dissimilarity Tool-
box [35]. All data are in RDM format, meaning we did not
directly process the fMRI data, but instead received the set
already in usable form. As such, anyone can utilize this data
without speciﬁc fMRI domain knowledge, which makes the
HMS metric broadly applicable to machine learning tasks.
Although data from four participants were collected over
two sessions, following the methods of Mur et al. [34], we
averaged the subject RDMs together into a mean human
brain RDM, which reduced noise. RDMs were constructed
from activations in the bilateral IT region of the brain.

The full details of the human fMRI data collection can
be found in [24]. Nonetheless, for completeness we brieﬂy
describe the procedure Kriegeskorte et al. [24] used to col-
lect human fMRI data. Eight RDMs were constructed from
fMRI recordings of four subjects over two sessions in re-
sponse to 92 stimuli. Recordings were from measurements
of 1.95 × 1.95 × 2mm3 within an occipitotemporal mea-
surement slab (5cm thick). Subjects were presented with a
random sequence of the 92 stimuli. Each stimulus was dis-
played for 300 milliseconds, every 3700 milliseconds, with
four seconds between stimuli. Not all voxels were used to
construct the RDM. Voxels of interest were selected based
on voxel responses to stimuli from an independent dataset.
No spatial smoothing or voxel averaging was performed.

PredNet activations to stimuli. Using the exact same
set of 92 stimuli, we construct an RDM using network acti-
vations as features from PredNet’s internal representation
neurons. Speciﬁcally, activations are recorded from the
convolutional LSTM units. Predictive coding networks are

time-based networks, and thus we present the stimuli for a
ﬁxed ﬁve frames and record activations at each time step.
We discard the ﬁrst time step as it corresponds to a “blank”
prediction. Activation patterns from PredNet for this style
of stimuli presentation mimic biological neural responses
for perception [30].

RDM construction. Given a single feature f and a sin-
gle stimulus s, v = f (s), where v is the value of feature f
in response to s. Likewise, the vector

~v =

T





v1
v2
...
vn





=





f1(s)
f2(s)

...

fn(s)

T





(1)

can represent the feature values of a collection of n features,
f1, f2, ..., fn, in response to s. If one expands the repre-
sentation of s to a set of m stimuli S = s1, s2, ..., sm, the
natural extension of ~v is the set of feature value collections
V = ~v1, ~v2, ..., ~vm, in which si ∈ S is paired with ~vi ∈ V
for each i = 1, 2, ..., m. The last step prior to constructing
an RDM is to deﬁne the dissimilarity score between any two
~vi ∈ V and ~vj ∈ V . We use the symmetric function

ψ(~vi, ~vj) := 1 −

(~vi − ¯vi) · (~vj − ¯vj)
k~vi − ¯vik2k~vj − ¯vjk2

(2)

where ¯v is the mean of the features in ~v. An RDM R may
then be constructed from S, V , and ψ as:

ψ(~v1, ~v2) ψ(~v1, ~v3)
ψ(~v2, ~v3)

R =





. . .
. . .
. . .

ψ(~v1, ~vm)
ψ(~v2, ~vm)

...

ψ(~vm−1, ~vm)





(3)

Human-model similarity (HMS). Given any two
RDMs R1 and R2 from the same set of stimuli S, one can
compute their similarity to determine how similar the acti-
vation behavior is in response to S. The similarity function

HM S = ρ( ˆR1, ˆR2)

(4)

computes a Spearman’s rank correlation coefﬁcient repre-
sented by ρ, where ˆR is the ﬂattened RDM.

Thus HMS is calculated as the correlation between the
averaged human fMRI RDM and a constructed PredNet net-
work RDM, obtained from the network activations to the
stimuli. The resulting score is deﬁned over the real interval
[−1, 1], with 1 indicating perfect correlation, −1 indicating
perfect negative correlation, and 0 indicating the two RDMs
are completely uncorrelated.

5408

Evaluation Task

Metric

Mean

(SD) Top Ten HMS Mean (SD)

Next Frame Prediction Error
Object Matching
Human-Model Similarity

Pixel MSE
Accuracy
RDM Correlation

0.092 (0.148)
0.367 (0.134)
0.106 (0.055)

0.009 (0.003)
0.459 (0.049)
0.178 (0.011)

Table 1. A statistical overview of evaluation scores for a sample of 95 randomly hyperparameterized PredNet networks. These scores
indicate the range of scores we expect to obtain from an arbitrary PredNet network. The top ten HMS mean score refers to the average
score for each metric for the ten networks with the highest human-model similarity. The top ten average shows that networks with high
HMS also achieve high performance on the other tasks. The object matching task was intentionally designed to be difﬁcult — the network
must distinguish ﬁne-grained differences in unseen, ﬁctional “Gazoobian” objects [48] where task chance is (0.02). Networks are trained
using KITTI [15] and evaluated on next frame prediction using a held-out set of KITTI data. Pixel MSE is mean squared error of the
predicted-to-actual frame at the pixel level. SD is standard deviation.

4. Experiments

Our experiments assess how the biological ﬁdelity of
predictive coding networks affects two computer vision
tasks: next frame prediction and object matching. We iden-
tify biological ﬁdelity as more similar internal activation be-
havior to human fMRI, as measured through RDMs.

Four datasets are utilized. We evaluate HMS, as de-
scribed in Sec. 3.3, on a dataset of 92 stimuli with a range
of similar and dissimilar objects, from real human faces
to animated objects [23]. Computer vision capabilities are
evaluated on two tasks: next frame prediction and object
matching accuracy, as described in Sec. 3.2. Next frame
prediction is assessed by measuring pixel-level MSE on the
KITTI dataset [15], a video dataset composed of image se-
quences from a car mounted camera. We also experimented
with another video dataset, VLOG [13]. For object match-
ing, we used a randomly generated “Gazoobian Objects”
dataset (following the procedure described by Tenenbaum
et al. [48]), composed of otherworldly objects guaranteed to
be unseen in training. Gazoobian stimuli mirror the stimuli
presentation of the HMS stimuli. Even though these objects
are well out of domain compared to the natural images used
for training, humans are able to generalize to them with ease
[48], making them an excellent basis from which to study
model generalization at inference time. Objects were varied
in rotation, lighting, color, or a combination thereof. Exam-
ple images from all datasets can be found in Sec. 1 of the
Supp. Mat.

4.1. Does HMS Discover Models that Generalize?

Initially, we evaluated a random Monte Carlo-style sam-
ple of hyperparameters in order to test how HMS, next
frame prediction, and object matching varied across Pred-
Net networks.
In typical model search fashion, we var-
ied six hyperparameters including the number of training
epochs, the number of video sequences used for validation
after training for an epoch, the number of video sequences
used to train within an epoch, the batch size, the learning
rate, and the size of the convolutional ﬁlters across all lay-

ers. The exact space searched is listed in Sec. 2.1 of the
Supp. Mat. We trained 95 4-layer PredNets with randomly
selected hyperparameters using HyperOpt [4], a software
package for distributed hyperparameter optimization.

In Table 1 we report the metrics’ mean and standard de-
viation for the 95 trained PredNets. Next frame prediction
was within range of Lotter et al. [29]. The accuracy scores
highlight the difﬁculty of the object matching task, which
focuses on speciﬁc object matching from a 50 image gallery
of stimuli (chance = 0.02). The evaluation scores indicate
our parameters were well suited for sampling: performance
was above chance but below ceiling. Impressively, the mean
HMS was within a standard deviation of the average human-
human similarity score of 0.19 (SD = 0.09). We also con-
ﬁrmed that these results are stable in a cross-dataset context
using the VLOG dataset [13] (these experiments are dis-
cussed in Sec. 3 of the Supp. Mat.).

We next examined how high HMS similarity corre-
sponds with other metrics by looking at the 10 networks
with the highest HMS scores (reported in Table 1). These
networks achieve much higher performance over the set of
all networks on the two computer vision tasks. We also
examined the 10 networks with the lowest HMS, and note
that they have much worse than average performance: mean
next frame prediction error was 0.314 (SD = 0.138), mean
object matching accuracy was 0.13 (0.15), and mean HMS
was −0.008 (0.027). This shows HMS is an effective met-
ric for predicting performance. Networks with high HMS
perform well, and those with low HMS perform poorly.

To be useful, HMS needs to be an effective predictor
across all models, not just high and low performing models.
We veriﬁed that, across all models, higher HMS is associ-
ated with higher performance on the other metrics by com-
puting Spearman’s rho across the sampled networks (Ta-
ble 2). Further, the p values of these correlations are the
probability our ﬁndings occur by chance, with p < 0.001
indicating a less than 0.001 probability (0.1%) that our cor-
relations occur by chance (see Sec 3.2. for details of these
safeguards). The strength of the correlations between the
metrics are moderate to strong, with p < 0.001. This con-

5409

Variable

Accuracy

HMS

Learning Rate

Next Frame Prediction Error
Object Matching Accuracy
Human-Model Similarity

-0.791**

.
.

-0.646**
0.575**

.

0.635**
-0.517**
-0.452**

**p < 0.001

Table 2. Spearman’s rho of metrics for 95 trained PredNets with random hyperparameters. The correlations conﬁrm that HMS is predictive
of network performance on other metrics. The negative correlation between Next Frame Prediction Error and the two other metrics occurs
because next frame prediction is measured by error, which should minimized, while HMS and Accuracy are metrics to be maximized.
Precautions taken in determining statistical signiﬁcance are described in Sec. 3.2. Learning rate was correlated with each metric, but was
not determined to be a signiﬁcant contributing factor to HMS as a predictor of network performance after partial correlation analysis.

ﬁrms that HMS is predictive of network performance on
computer vision tasks. Additionally, we calculated correla-
tion scores for all hyperparameters to verify that no individ-
ual parameter was responsible for these results. We include
the learning rate (LR) hyperparameter in Table 2 because it
is moderately correlated with the other metrics.

The correlation with LR indicated a possible risk that LR
is strongly inﬂuencing the results. We investigated its inﬂu-
ence with a partial correlation analysis, which measures the
relationship between metrics while controlling for the inﬂu-
ence of LR. The correlations between metrics from Table 2
were not statistically signiﬁcant (p < 0.001); however, the
sample size was too small for the breadth of LRs tested. We
addressed this by repeating the partial correlation on a much
larger set of networks (N = 1811). For this sample, the par-
tial correlations between the metrics were statistically sig-
niﬁcant (p < 0.001), with similar correlation strength for
the sample. This conﬁrms HMS is signiﬁcantly correlated
with the other metrics regardless of the inﬂuence of LR on
training. More discussion of this experiment can be found
in Sec. 2.2 of the Supp. Mat.

All of the ﬁndings discussed above provide evidence that
HMS is an effective search metric. HMS was indicative of
performance for both computer vision tasks across all mod-
els (via correlation) and extremes (top and bottom models).
Networks which exhibited more brain-like internal behavior
generalized better to other evaluation tasks.

4.2. Metric Stability During Model Search

How stable are our evaluation metrics during network
training? Do evaluations of network performance vary
across identically hyperparameterized models?
If HMS
ﬂuctuates wildly during training, it may be an unreliable in-
dicator of performance. Through further experimentation,
we found that this is not the case, and show that HMS is a
predictor earlier in training than the other metrics.

Within-network stability. We ﬁrst investigated how the
metrics varied during training on a sample of 74 4-layer
PredNets trained for 150 epochs, evaluating performance
every 5 epochs. We focused our analysis on 10 networks
where MSE was below 0.01 by the 150th epoch, implying

convergence. We found each metric had its own predictable
behavior, illustrated by a representative network in Fig. 4,
consistent across hyperparameters. Once HMS was stable
(SD ≤ 0.01) for 25 epochs it remained so. Object match-
ing accuracy tended to start higher, before dropping, and
ﬁnally rising again as the training unfolded. Finally, next
frame prediction error either continuously decreased, lead-
ing to a good network, or increased, leading to a degenerate
network. The correlations from Table 2 imply that any of
the metrics could be used as a predictor, but the training
behavior offers insight into how these metrics would need
to be utilized. HMS stabilizes ﬁrst, after an average of 32
epochs. Accuracy stabilizes next, after an average of 66.5
epochs (SD = 36), although some scores did not plateau
but continued to increase. In cases where next frame pre-
diction error (MSE) decreased with training, it typically de-
creased throughout all 150 epochs, making a poor indicator

Figure 4. Within-network stability analysis for a representative
PredNet model. We ﬁnd that each metric has its own stereotyp-
ical behavior during training. Object matching accuracy is incon-
sistent early in training, but eventually stabilizes and continues to
increase. Next frame prediction error (MSE) either falls consis-
tently (the case shown above) or rises unpredictably, but is heav-
ily dependent on training time. HMS is inconsistent very early in
training, but stabilizes more quickly than accuracy, which is unsta-
ble for longer, or MSE, which requires a long training time before
stabilizing. These ﬁndings mean that HMS can be used to identify
poor-performing networks for early stopping in network search.

5410

of performance. Note that in the 95 model sample, MSE
was the only metric correlated with the number of epochs
(−0.332, p < 0.001). Further details, results, and experi-
ments on across-network stability can be found in Sec. 5 of
the Supp. Mat.

4.3. A Mechanism for Early Stopping

An outcome of the ﬁndings from Sec. 4.2 is that our pro-
posed HMS metric can be employed during network train-
ing as a way to discard (i.e., stop training) models that will
ultimately perform poorly. To demonstrate this, we con-
ducted a post hoc analysis of the 95 PredNets from Sec. 4.1.
On the left-hand side of Fig. 5 we present time saved by
early stopping with HMS and accuracy using the conver-
gence criteria of Sec. 4.2. Overall, early stopping with HMS
could have reduced training time by 67% at no cost to ﬁ-
nal performance. We also tested a threshold strategy which
considered a network to be stopped during training if its
HMS score was below a threshold of 0.161 (the mean HMS
from Table 1 plus one standard deviation). Only 13 of the
95 models (13.7%) were above this threshold. The right-
hand side of Fig. 5 depicts the accuracy scores of the mod-
els with respect to the side of the HMS threshold they are
on. Our analysis shows that even with a high threshold to
stop training, and the loss of some models with high perfor-
mance, most retained models are high performing and were
more likely to have high performance on both tasks. Addi-
tionally, in this case the highest performance model on both
computer vision tasks is retained, but a number of other re-
tained higher performing models have trivial differences in
performance, and would be just as useful had the top model
been discarded. Complete details for these experiments and
additional results can be found in Sec. 6 of the Supp. Mat.

5. Discussion

There are several beneﬁts to utilizing HMS over tradi-
tional human-model comparisons.
(1) HMS is useful for
model searches because activation patterns for learned rep-
resentations emerge early in training, whereas other evalua-
tions require fully training a network. (2) There is evidence
that HMS is indicative of a model’s ability to generalize to
unseen data and tasks, since PredNet models with higher
HMS are more likely to perform well on both object match-
ing accuracy and next frame prediction. (3) Compared with
other evaluations of perceptive consistency, such as visual
Psychophysics [40, 41], HMS is much less computationally
expensive. Consider the computational cost of HMS eval-
uation compared with the accuracy evaluation, which uti-
lizes psychophysical stimuli (varying lighting and texture).
HMS only requires a network to process 92 stimuli. The
PredNet accuracy metric requires the network to process 51
stimuli (1 probe, 50 gallery) per trial, for 500 trials (25,500
stimuli). (4) We used fMRI data as a benchmark because

Time Saved with Different 

Early Stopping Metrics

Early Stopping Threshold 

with Network Accuracy

0.60

y
c
a
r
u
c
c
A

0.30

HMS

Accuracy

None

0.00

-0.05

0.20

Metric Used for Early Stopping

HMS

300

150

0

)
s
r
u
o
H

i

(
 
e
m
T
 
g
n
n
a
r
T

i

i

Figure 5. The left-hand plot shows how HMS-driven early stop-
ping for the sample of 95 PredNets cut down training time by 67%,
using the criteria for convergence of SD ≤ 0.01 for 25 epochs.
Using the same convergence criteria for accuracy was not as ef-
fective. The right-hand scatter plot shows the accuracy of models
above and below an early stopping threshold (0.161). 82 models
left of the line would be discarded at no cost to ﬁnal performance.
These experiments utilize the ﬁndings for metric stability estab-
lished in Sec. 4.2 to quantify the potential outcome of utilizing
early stopping on the model sample.

it overcomes the difﬁculty in labeling the correct similarity
between different objects. For example, as humans we in-
stinctively know a pair of faces should have highly similar
activation behavior, but what about a hand and face? Neu-
ral data provides an implicit answer to this question. One
concern that can be raised is the perceived difﬁculty of ob-
taining fMRI data. Fortunately, there is a growing open sci-
ence movement within neuroscience. The fMRI data used
in this study is publicly available and can be utilized by any-
one [35], and it is far from the only data available. Vast pub-
lic fMRI repositories exist for vision, text, and audio tasks,
and researchers do not need to be experts in order to uti-
lize them. A few examples are the Donders repository [11],
OpenNeuro [1], and Oasis [31] brains.

We believe that networks with more biological ﬁdelity
in function will be essential to overcome the shortcomings
of today’s networks in replicating biological vision. The
future of artiﬁcial intelligence research will need to bridge
the gap between network structure and internal behavior,
which requires reassessing how we evaluate networks. In
the past, unexpected network behavior has blinded-sided re-
searchers, e.g., susceptibility to adversarial images. And
it is important to remember that current networks are not
consistent with human behavior [40, 41]. Evaluations mea-
suring internal behavior should prove useful for avoiding
unforeseen issues, and may help us achieve the next gener-
alization breakthrough.

6. Acknowledgements

Funding was

under
D16PC00002 and NSF DGE 1313583.

provided

IARPA contract

5411

References

[1] https://openneuro.org/. 8

[2] D. G. Barrett, A. S. Morcos, and J. H. Macke. Analyzing
biological and artiﬁcial neural networks: challenges with op-
portunities for synergy? arXiv preprint arXiv:1810.13373,
2018. 3

[3] P. Bashivan, M. Tensen, and J. J. DiCarlo. Teacher guided ar-
chitecture search. arXiv:1808.01405 [cs], Aug. 2018. arXiv:
1808.01405. 3

[4] J. Bergstra, D. Yamins, and D. D. Cox. Making a science of
model search: Hyperparameter optimization in hundreds of
dimensions for vision architectures. ICML (1), 28:115–123,
2013. 6

[5] M. F. Bonner and R. A. Epstein. Computational mecha-
nisms underlying cortical responses to the affordance prop-
erties of visual scenes.
PLOS Computational Biology,
14(4):e1006111, Apr. 2018. 2

[6] J. Cohen. Statistical power analysis for the behavioral sci-

ences. 2nd edition. Lawrence Erlbaum Associates, 1988. 4

[7] M. N. Coutanche and G. E. Koch. Creatures great and small:
Real-world size of animals predicts visual cortex represen-
tations beyond taxonomic category. NeuroImage, 183:627–
634, Dec. 2018. 2, 3

[8] J. G. Daugman. Uncertainty relation for resolution in
space, spatial frequency, and orientation optimized by two-
dimensional visual cortical ﬁlters.
J. Opt. Soc. Am. A,
2(7):1160–1169, Jul 1985. 3

[9] O. J. Dunn. Estimation of the means of dependent variables.

Ann. Math. Statist., 29(4):1095–1111, 12 1958. 4

[10] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture
search: A survey. arXiv preprint arXiv:1808.05377, 2018. 3

[11] FAIRsharing Team. Donders Repository, 2018.

type:

dataset. 8

[12] R. Fong, W. J. Scheirer, and D. Cox. Using human brain ac-
tivity to guide machine learning. Scientiﬁc Reports, 8:5397,
March 2018. 3

[13] D. F. Fouhey, W.-c. Kuo, A. A. Efros, and J. Malik. From
lifestyle vlogs to everyday interactions. In IEEE/CVF CVPR,
2018. 3, 6

[14] F. E. Garcea, J. Almeida, M. H. Sims, A. Nunno, S. P. Mey-
ers, Y. M. Li, K. Walter, W. H. Pilcher, and B. Z. Mahon.
Domain-speciﬁc diaschisis: Lesions to parietal action ar-
eas modulate neural responses to tools in the ventral stream.
Cerebral Cortex, 2018. 2, 3

[15] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision
International Journal

meets robotics: The KITTI dataset.
of Robotics Research, 32(11):1231–1237, 2013. 3, 6

[16] R. Geirhos, C. R. M. Temme, J. Rauber, H. H. Schuett,
M. Bethge, and F. A. Wichmann. Generalisation in humans
and deep neural networks. arXiv preprint arXiv:1808.08750,
2018. 2

[17] H. Hong, D. L. Yamins, N. J. Majaj, and J. J. DiCarlo. Ex-
plicit information for category-orthogonal object properties
increases along the ventral stream. Nature Neuroscience,
19(4):613, 2016. 3

[18] C.-H. Hsu, S.-H. Chang, D.-C. Juan, J.-Y. Pan, Y.-T. Chen,
W. Wei, and S.-C. Chang. Monas: Multi-objective neu-
ral architecture search using reinforcement learning. arXiv
preprint arXiv:1806.10332, 2018. 3

[19] I. Kalfas, K. Vinken, and R. Vogels. Representations of
regular and irregular shapes by deep convolutional neural
networks, monkey inferotemporal neurons and human judg-
ments. PLOS Computational Biology, 14(10):e1006557,
2018. 3

[20] S. R. Kheradpisheh, M. Ghodrati, M. Ganjtabesh, and
T. Masquelier. Deep networks can resemble human feed-
forward vision in invariant object recognition. Scientiﬁc Re-
ports, 6:32672, Sept. 2016. 3

[21] N. Kriegeskorte. Relating population-code representations
between man, monkey, and computational models. Frontiers
in Neuroscience, 3(3):363–373, 2009. 2, 3

[22] N. Kriegeskorte. Pattern-information analysis: from stimu-
lus decoding to computational-model testing. Neuroimage,
56(2):411–421, 2011. 3

[23] N. Kriegeskorte, M. Mur, and P. Bandettini. Representational
similarity analysis – connecting the branches of systems neu-
roscience. Frontiers in Systems Neuroscience, 2, Nov. 2008.
1, 2, 3, 6

[24] N. Kriegeskorte, M. Mur, D. A. Ruff, R. Kiani, J. Bodurka,
H. Esteky, K. Tanaka, and P. A. Bandettini. Matching cate-
gorical object representations in inferior temporal cortex of
man and monkey. Neuron, 60(6):1126–1141, Dec. 2008. 2,
3, 5

[25] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436, 2015. 2

[26] J. Z. Leibo, C. d. M. d’Autume, D. Zoran, D. Amos,
C. Beattie, K. Anderson, A. G. Casta˜neda, M. Sanchez,
S. Green, A. Gruslys, et al. Psychlab: a psychology labora-
tory for deep reinforcement learning agents. arXiv preprint
arXiv:1801.08116, 2018. 3

[27] C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, and K. Murphy. Progressive neural ar-
chitecture search. In ECCV, September 2018. 3

[28] B. Long and T. Konkle. The role of textural statistics vs.
outer contours in deep CNN and neural responses to objects.
In Conference on Computational Cognitive Neuroscience,
page 4, Philadelphia, Pennsylvania, 2018. 3

[29] W. Lotter, G. Kreiman, and D. Cox. Deep predictive coding
networks for video prediction and unsupervised learning. In
ICLR, 2017. 1, 2, 3, 4, 6

[30] W. Lotter, G. Kreiman, and D. Cox. A neural network trained
to predict future video frames mimics critical properties of
biological neuronal responses and perception. arXiv preprint
arXiv:1805.10734, 2018. 2, 3, 5

[31] D. S. Marcus, A. F. Fotenos, J. G. Csernansky, J. C. Mor-
ris, and R. L. Buckner. Open Access Series of Imaging
Studies: Longitudinal MRI Data in Nondemented and De-
mented Older Adults. Journal of Cognitive Neuroscience,
22(12):2677–2684, Dec. 2010. 8

[32] P. McClure and N. Kriegeskorte. Representational distance
learning for deep neural networks. Frontiers in Computa-
tional Neuroscience, 10:131, 2016. 2

5412

[48] J. B. Tenenbaum, C. Kemp, T. L. Grifﬁths, and N. D. Good-
man. How to grow a mind: Statistics, structure, and abstrac-
tion. Science, 331(6022):1279–1285, 2011. 3, 6

[49] A. Torralba and A. A. Efros. Unbiased look at dataset bias.

In IEEE CVPR, 2011. 3

[50] D. Yamins and J. J. DiCarlo. Using goal-driven deep learn-
ing models to understand sensory cortex. In Nature Neuro-
science, volume 19, pages 356 – 365, 2016. 3

[51] D. Yamins, H. Hong, C. Cadieu, and J. J. DiCarlo. Hi-
erarchical modular optimization of convolutional networks
achieves representations similar to macaque IT and human
ventral stream. In NIPS, 2013. 2, 3

[52] D. Yamins, H. Hong, C. F. Cadieu, E. A. Solomon, D. Seib-
ert, and J. J. DiCarlo. Performance-optimized hierarchi-
cal models predict neural responses in higher visual cor-
tex. Proceedings of the National Academy of Sciences,
111(23):8619–8624, 2014. 2, 3

[53] I. Yildirim, W. Freiwald, and J. Tenenbaum. Efﬁcient inverse

graphics in biological face processing. bioRxiv, 2018. 2, 3

[33] H. E. Moss, J. M. Rodd, E. A. Stamatakis, P. Bright, and
L. K. Tyler. Anteromedial temporal cortex supports ﬁne-
grained differentiation among objects. Cerebral Cortex,
15(5):616–627, 2004. 2, 3

[34] M. Mur, M. Meys, J. Bodurka, R. Goebel, P. A. Bandettini,
and N. Kriegeskorte. Human object-similarity judgments re-
ﬂect and transcend the primate-it object representation. Fron-
tiers in Psychology, 4, 2013. 3, 5

[35] H. Nili, C. Wingﬁeld, A. Walther, L. Su, W. Marslen-Wilson,
and N. Kriegeskorte. A toolbox for representational similar-
ity analysis. PLoS Computational Biology, 10(4):e1003553,
2014. 3, 5, 8

[36] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Ef-
ﬁcient neural architecture search via parameter sharing. In
ICML, 2018. 3

[37] N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-
throughput screening approach to discovering good forms of
biologically inspired visual representation. PLoS Computa-
tional Biology, 5(11):e1000579, 2009. 3

[38] R. Rajalingham, E. B. Issa, P. Bashivan, K. Kar, K. Schmidt,
and J. J. DiCarlo. Large-scale, high-resolution comparison
of the core visual object recognition behavior of humans,
monkeys, and state-of-the-art deep artiﬁcial neural networks.
Journal of Neuroscience, 38(33):7255–7269, 2018. 3

[39] R. P. Rao and D. H. Ballard. Predictive coding in the visual
cortex: a functional interpretation of some extra-classical
receptive-ﬁeld effects. Nature Neuroscience, 2(1):79, 1999.
2, 3

[40] B. RichardWebster, S. E. Anthony, and W. J. Scheirer. Psy-
phy: A psychophysics driven evaluation framework for vi-
sual recognition. IEEE T-PAMI, 2018. To Appear. 2, 3, 8

[41] B. RichardWebster, S. Y. Kwon, C. Clarizio, S. E. Anthony,
and W. J. Scheirer. Visual psychophysics for making face
recognition algorithms more explainable. In ECCV, 2018. 2,
3, 8

[42] M. Riesenhuber and T. Poggio. Hierarchical models of ob-
ject recognition in cortex. Nature Neuroscience, 2(11):1019,
1999. 2, 3

[43] M. Roper, C. Fernando, and L. Chittka. Insect bio-inspired
neural network provides new evidence on how simple feature
detectors can enable complex visual generalization and stim-
ulus location invariance in the miniature brain of honeybees.
PLoS Computational Biology, 13(2):e1005333, 2017. 2, 3

[44] F. Rosenblatt. The perceptron: a probabilistic model for in-
formation storage and organization in the brain. Psychologi-
cal review, 65(6):386, 1958. 2

[45] A. Rosenfeld, M. D. Solbach, and J. K. Tsotsos. Totally
looks like – how humans compare, compared to machines.
In Mutual Beneﬁts of Cognitive and Computer Vision Work-
shop, 2018. 3

[46] J. Sacramento, R. P. Costa, Y. Bengio, and W. Senn. Den-
dritic cortical microcircuits approximate the backpropaga-
tion algorithm. In NIPS, 2018. 3

[47] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Pog-
gio. Robust object recognition with cortex-like mechanisms.
IEEE T-PAMI, 29(3):411–426, 2007. 2, 3

5413

