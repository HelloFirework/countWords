Feature Space Perturbations Yield More Transferable Adversarial Examples

Nathan Inkawhich, Wei Wen, Hai (Helen) Li and Yiran Chen

Duke University

Electrical and Computer Engineering Department, Durham, NC 27708

{nathan.inkawhich,wei.wen,hai.li,yiran.chen}@duke.edu

Abstract

Many recent works have shown that deep learning mod-
els are vulnerable to quasi-imperceptible input perturba-
tions, yet practitioners cannot fully explain this behavior.
This work describes a transfer-based blackbox targeted ad-
versarial attack of deep feature space representations that
also provides insights into cross-model class representa-
tions of deep CNNs. The attack is explicitly designed for
transferability and drives feature space representation of
a source image at layer L towards the representation of
a target image at L. The attack yields highly transferable
targeted examples, which outperform competition winning
methods by over 30% in targeted attack metrics. We also
show the choice of L to generate examples from is impor-
tant, transferability characteristics are blackbox model ag-
nostic, and indicate that well trained deep models have sim-
ilar highly-abstract representations.

1. Introduction

Many researchers have shown how to intentionally fool
well trained deep learning algorithms with adversarial at-
tacks [23, 5, 2, 20, 8]. The focus of most works is to have a
maximally devastating effect on the model, while introduc-
ing minimal perturbation to the data. In the case of image-
based data, the attacker works to disrupt the classiﬁcation
ability of the network with imperceptible perturbations to
the image [23, 5, 1, 16, 15]. These attacks are of even
greater concern because we as a community do not have a
ﬁrm understanding of what is happening inside these largely
uninterpretable deep models. However, attacks may also
provide a way to study the inner-workings of such models.
One of the more difﬁcult threat models for an adversary
operating on CNNs is a blackbox targeted attack. In this
scenario, the adversary only has access to inputs and out-
puts, with no working knowledge of the underlying weights
or architecture. Even more, the adversary may choose the
target class the oracle network misclassiﬁes to. In this work,
we design a black-box targeted adversarial attack for deep

Figure 1: Illustration of Activation Attack. Given that the
whitebox model (fw) and blackbox model (fb) are initially
correct, the attack drives the layer L activations of the dog
image towards the layer L activations of the plane. After
attack, the dog’s activations are similar to the plane’s and
the perturbed image is classiﬁed as a plane to fw and fb.

CNN models. We design the attack using the property of
adversarial transferability, in which examples that are ad-
versarial to one model will often be adversarial to other
models [11, 18, 13]. The unique aspect of this attack is
that it explicitly perturbs the feature space of a deep model
with the intentions of creating more transferable adversar-
ial examples. The intuition comes from the observation that
intermediate features of well trained models are transferable
[26]. Thus, perturbations of intermediate features may also
be transferable.

To test this hypothesis, we design the Activation Attack
(AA), as shown in Fig. 1. By perturbing the source im-
age, the algorithm drives the layer L activations of a white-
box model on the source image, towards the layer L ac-
tivations of that model for a target image. The net effect
is perturbations in feature space. We test the transferabil-
ity of the perturbed features by feeding the perturbed image

7066

into the blackbox model. These perturbations then have a
not-directly-observable effect on the layer of the blackbox
model that has learned those same features.

This work makes several contributions to adversarial at-
tack and model interpretability research. First, we show that
constructing adversarial examples with feature space per-
turbations yields transferable adversarial examples. Also,
the layer at which features are perturbed has a large im-
pact on the transferability of adversarial examples. Next,
we use canonical CNN architectures to show the efﬁcacy of
the attack to powerful algorithms, rather than custom mod-
els for simple tasks (i.e. MNIST classiﬁers). For model
interpretability, we show that blackbox model architecture
does not affect the characteristics of layer-wise transferabil-
ity. We also provide evidence for why a particular layer
produces more transferable examples than another. Finally,
this work gives testimony that intermediate feature repre-
sentations of deep learning models with fundamentally dif-
ferent architectures are similar. Also, those well trained
models have similar decision boundaries with similar class-
wise orientations in feature space.

2. Related Work

One area of related work is adversarial attack research.
The goal of many attacks is to introduce imperceptible per-
turbations to the input image that have devastating impacts
to the classiﬁers performance, but have no impact to hu-
man recognition. Szegedy et al. [23] and Goodfellow et al.
[5] were among the ﬁrst to show that adversarial examples
generated on one model may also transfer to other models.
However, the transferability was not designed for, rather it
was a consequence of the method and the nature of well
trained models. Papernot et al. [18] developed a method
to train a substitute model using reservoir sampling so that
the substitute model generated more transferable examples.
The work showed results across a spectrum of machine
learning algorithms such as SVMs, decision trees, DNNs,
and was less focused on the implication in modern CNNs.

More recently, Tram´er et al. [25] explored transferable
adversarial examples in more detail, and studied why trans-
ferability occurs through small custom MNIST models and
untargeted attacks. Speciﬁcally, they found model decision
boundaries are similar in arbitrary directions and adversar-
ial examples span a low-dimensional subspace of the input.
They also discuss perturbations of feature maps in direc-
tions of class-means, but results on CNNs are poor and not
conclusive. Finally, the authors provide evidence that in
some cases, well trained models for the same task, that both
exhibit vulnerability to simple attacks, are likely to trans-
fer adversarial examples. Our work differs in that the at-
tacks are targeted, we perform extensive experimentation of
layer-wise transferability on canonical CNNs for non-trivial
problems, and we further analyze why examples transfer.

Also related, Sabour et al. [21] were among the ﬁrst to ex-
plicitly describe a whitebox only attack in feature space.
They use an expensive L-BFGS perturbation method and
show that examples of different classes can be forced very
near each other in feature space but still maintain their orig-
inal image structure. However, the authors brieﬂy mention
that the perturbed examples do not transfer well to blackbox
models as targeted attacks and the attack is quite expensive.
In 2017 there was a competition for adversarial at-
tacks and defenses [11] in which the goal of the attack-
ers was to defeat a blackbox model with both non-targeted
and targeted-attacks. Most of the top performing attacks
[4, 11, 13, 14] generated and transferred adversarial exam-
ples from an ensemble of whitebox models, as introduced
in [13, 24]. The winners also used a momentum gradi-
ent denoising technique [4], which is incorporated in this
work. Most of the attacks were direct extensions of existing
attacks to operate with an ensemble. We will not use en-
sembles here, but will rather focus on a new technique for
perturbation that may be extended with ensembles as future
work.

Another area of related work is in model interpretabil-
ity, speciﬁcally in terms of shared feature representations
of deep models. Yosinski et al. [26] showed that deep fea-
ture representations of models trained with data from sim-
ilar distributions are transferable. They also measured the
transition from generalized features to highly class speciﬁc
(i.e. specialized) features as a function of layer depth. This
now serves as a seminal work in transfer learning. Although
[26] is not an adversarial attack, it does provide intuition as
to why feature space attacks may transfer.

3. Transferability Metrics

In this work we will deﬁne success through four met-
rics: error rate (error), untargeted transfer rate (uTR),
targeted success rate (tSuc), targeted transfer rate (tTR).
Importantly, we assume that all of the examples are cor-
rectly classiﬁed by the whitebox and blackbox model,
and an attack strength of ǫ = 0 means no attack.
We will denote the whitebox model as the function fw
and the blackbox model as fb, both of which output a
classiﬁcation prediction. We deﬁne our original dataset
Dorig = {(x(1), y(1)
true)} as a set of
N data/label pairs for which fb(x(i)) = fw(x(i)) =
y(i)
true. For each attack, we create an adversarial dataset
Dadv = {(x(1)
true)}
where each data in Dorig has been perturbed by a targeted
attack method on fw, making it an adversarial example.

true), . . . , (x(N ), y(N )

true), . . . , (x(N )

target, y(N )

target, y(1)

adv , y(N )

adv, y(1)

The error rate (error), or fooling rate of an attack is the
percentage of adversarial examples generated with fw that
are misclassiﬁed by fb.
In other words, error is the per-
centage of examples in Dadv for which fb(xadv) 6= ytrue.

7067

Larger error indicates more effective attacks.

The untargeted transfer rate (uTR) is the rate at which
the particular examples that fool the whitebox model also
fool the blackbox model. Note, fooling in this context
means the prediction does not equal the true label. To mea-
sure uTR, we deﬁne DuT R ⊆ Dadv which contains only
the elements of Dadv that are misclassiﬁed by fw. Thus,

uT R =

1

|DuT R| X

(xadv ,ytrue)

✶[(fb(xadv)) 6= ytrue)],

(1)

∈DuT R

where ✶ is the indicator function that is 1 if the condition
is true and 0 otherwise. This metric intuitively encodes the
likelihood that a successful untargeted adversarial example
to the attacker’s whitebox model will also be adversarial to
the blackbox model.

Since the attacks considered are all targeted, we measure
the targeted success rate (tSuc).
tSuc is the rate at which
adversarial examples generated with fw are classiﬁed by fb
as the target label. In other words, tSuc is the percentage of
examples in Dadv for which fb(xadv) = ytarget. The larger
tSuc, the more effective the attack at generating targeted
examples for the blackbox model.

The ﬁnal metric is targeted transfer rate (tTR), which
measures the rate at which successful targeted adversarial
examples measured on the whitebox model are also suc-
cessful targeted examples on the blackbox model. For this,
we deﬁne DtT R ⊆ Dadv (also a subset of DuT R) which
contains all elements of Dadv that are misclassiﬁed by fw
as the speciﬁed target label. Formally,

tT R =

1

|DtT R| X

(xadv ,ytarget)

✶[(fb(xadv)) = ytarget)].

∈DtT R

(2)
tTR encodes a likelihood that a successful targeted exam-
ple observed on the whitebox model will be a successful
targeted example to the blackbox model. Current targeted
attack literature commonly measures error and tSuc. We in-
troduce uTR and tTR as new metrics for attacks that are use-
ful if the attacker wants to maximize his chance of success
in a limited number of attempts. Also note, even though the
attacks are targeted, we also measure untargeted statistics
as they are still relevant to the power of the attacks.

4. Activation Attack Methodology

The Activation Attack (AA) is a blackbox targeted attack
designed for transferability. We can think of the Activation
Attack methodology as being split into two segments, which
mimic that of many deep learning attack practices. First, we
specify a loss function to optimize for. Then, we establish

the attack algorithm and perturbation method to adjust the
data based on the loss.

4.1. Loss Function

The AA loss function is deﬁned as the Euclidean dis-
tance between the vectorized source image activations and
vectorized target image activations at some layer L. Let
fL be a truncated version of fw (the whitebox model) that
takes an image as input and outputs the activations at layer
L. So AL
s = fL(Is) are the source image (Is) activations
and AL
t = fL(It) are the target image (It) activations at L.
The loss function JAA between two images is then,

JAA(It, Is) = kfL(It) − fL(Is)k2 = (cid:13)(cid:13)AL

t − AL

s (cid:13)(cid:13)2 . (3)

The intuition behind the AA loss function is to make the
source image closer to an image of the target class in fea-
ture space. The implications/assumptions of this loss are
three-fold. First, adjustments of deep feature space repre-
sentations have a sizable impact on the classiﬁcation result.
Since we are not explicitly optimizing on classiﬁcation loss,
we rely on a byproduct of feature space perturbations be-
ing signiﬁcant classiﬁcation disruption. Because the feature
space representation is so large and uninterpretable, it is not
immediately obvious that this will be the case. Also, due to
the size (i.e. number of parameters) of the feature space, we
must assume that constrained image domain perturbations
will be able to move the original sample to be close enough
to the target sample (in feature space), as to be within re-
gions of the target class.

The second major assumption made with this loss func-
tion is: since intermediate layer features of deep models
have shown to be transferable [26], explicit attacks in fea-
ture space will yield transferable adversarial examples. Be-
cause modern deep models are unintelligible, there is no
way of measuring exactly what features are captured and
learned in each layer of a model. Thus, there is no way
of deﬁnitively knowing if two models have learned similar
feature sets, especially in deep and highly abstract layers.
This attack principally assumes that deep layers of different
deep models have learned similar features, and thus perturb-
ing these highly abstract features in one model will perturb
the same features in the other model. This is a reasonable
assumption because in transferability attacks, we train the
whitebox model on data from the same distribution as the
blackbox model’s training data. Thus, we expect that the
models have learned similar hierarchical feature sets in or-
der to properly model the classes of data. Since each model
architecture differs in complexity, number of layers, and
overall architecture, ﬁnding the layers at which features are
most transferable is left to experimentation.

The third major assumption is in regards to the learned
decision boundaries, and class orientations in feature space.

7068

Speciﬁcally, we assume that two different models trained
with data from the same distribution learn similar decision
boundaries and class orientations. This is particularly rel-
evant for the targeted attacks because in order for a trans-
ferred targeted example to be successful, the regions of that
target class in feature space must have the same orientation
w.r.t. the source image. In other words, if we start with the
source example’s feature space representation in the white-
box model and move it in the direction of the target sample,
this assumption states that the direction of movement is the
same (or at least similar) in the blackbox model.

4.2. Attack Algorithm

The perturbation mechanism is similar to that of the L∞
constrained iterative gradient sign attack with momentum
(TMIFGSM)[4]. Our attack algorithm iteratively perturbs
the source image using the sign of the momentum term,
where momentum is calculated as the weighted accumula-
tion of gradients. The difference here is that gradients are
not computed against classiﬁcation loss. Rather, they are
computed against (3). Also, gradients ﬂow backward start-
ing from layer L. Thus, momentum is calculated as

mk+1 = mk +

∇Ik JAA(It, Ik)

||∇Ik JAA(It, Ik)||1

,

(4)

where m0 = 0 and Ik is the perturbed source image at it-
eration k. Note, I0 = Is. The perturbation method for this
targeted L∞ constrained Activation Attack is

Ik+1 = Clip(Ik − α ∗ sign(mk+1), 0, 1).

(5)

Notice, the perturbed image is always clipped to range
[0, 1], as to maintain the distribution of the original image.
The intrinsic meaning of (5) is that we are slightly adjusting
each pixel of the image in the direction that will minimize
our JAA loss. The momentum term is used to intuitively
denoise or smooth the gradient directions, and is described
in [4] as an accumulation of a velocity vector in the gradient
direction. Also, keep in mind, we are perturbing the image
with the explicit intentions of altering the feature space rep-
resentation at some layer L. Thus any effects on classiﬁca-
tion are implicit, as we are not speciﬁcally accounting for
classiﬁcation loss.

There are also some hyperparameters to set for the algo-
rithm. Since this is an iterative algorithm, we must choose
the number of iterations K to perturb for, the total perturba-
tion amount ǫ, and the per-iteration perturbation amount α.
In all tests, we set K = 10, vary the ǫ, and set α = ǫ/K.

5. Experimental Setup

As observed in [25], examples tend to transfer between
models that achieve low error for a source task. Thus, we
choose CIFAR-10 [9] as our primary testing dataset, as it is

non-trivial, but state of the art models can achieve less than
10% test error [12]. For the primary experiment, we select
and train three canonical CNN model architectures of dif-
ferent design complexities that are capable of achieving low
error on CIFAR-10. We use ResNet-50 [6] which achieves
6.62% top-1 test error, DenseNet-121 [7] which achieves
4.72% error, and VGG19bn [22] which tests at 6.48% er-
ror. All models were trained in PyTorch [19] using code
from [12]. For completeness, we also extend some exper-
iments to ImageNet [3] trained models. We use pretrained
DenseNet-121 and ResNet-50 from PyTorchs Torchvision
Models. These models incur signiﬁcantly higher error for
the source task, where DenseNet-121 has 25.35% top-1 er-
ror and ResNet-50 has 23.85% error.

For CIFAR-10 tests we measure the four primary metrics
over the full 10k test set. First, we use DenseNet-121 as a
whitebox model and evaluate transferability to VGG19bn
and ResNet-50 blackbox models, separately. Next, we use
VGG19bn as a whitebox model and evaluate transferabil-
ity to DenseNet-121 and ResNet-50 blackbox models, sep-
arately. This allows us to see trends across whitebox mod-
els and blackbox models together. For ILSVRC2012 tests,
we run one primary experiment on a 15k randomly sampled
subset of the full 50k test set. Here, we test a DenseNet-121
whitebox model and a ResNet-50 blackbox model.

A note about the setup is in regards to the selection of
the target image in the AA. For each dataset, we keep a li-
brary of examples from each class which have all been ran-
domly sampled from the test splits. In the case of CIFAR-
10, we keep 100 examples of each class and for ImageNet,
20 examples of each class. For a given source image, we
randomly select a target class, then choose the target im-
age from the library as the one with the furthest layer L
activations (as measured by Euclidean distance) from the
source image activations. Also, note that layer depth in the
following experiments is relative, i.e.
layer 2 refers to a
layer closer to the input than layer 10. Also, in all tests the
deepest layer tested is the ﬁnal FC layer which produces the
output class logits, and the sampled layers are evenly dis-
tributed across the model. A table decoding the layers for
each model is in the Supplemental Materials.

6. Experimental Results

To gain an understanding of whether or not the AA at-
tack is feasible, we will ﬁrst do testing and gather empirical
results, then analyze our ﬁndings. There are two major axis
of parameters to test along: epsilon and depth. Epsilon tests
treat attacks from each layer separately and measure the im-
pacts of epsilon on transferability. Testing along the depth
axis involves ﬁxing the strength of the attack and measuring
attack performance as a function of which layer we gener-
ate AA examples from. For these tests we use three base-
lines. The iterative targeted class method (ITCM) [10] is

7069

Figure 2: Transferability versus epsilon results for CIFAR-10 attacks transferring from different whitebox models to a
ResNet-50 blackbox model. DN and VGG represent DenseNet121 and VGG19bn whitebox models.

a targeted variant of the basic iterative method and repre-
sents a simple method. The targeted projected gradient de-
scent (TPGD) attack with random start from [14] represents
a more complex method which routinely performs between
the other baselines. And the targeted momentum iterative
fast gradient sign method (TMIFGSM) [4], which won the
2017 NIPS Attacks and Defences competition [11] for both
targeted and untargeted attacks, represents our most com-
plex method. The performance of each attack on the white-
box model is shown in the supplementals.

6.1. Epsilon Results

The epsilon tests are a result of two experiments with
a common goal, to fool a ResNet-50 classiﬁer. First, we
measure the four transferability metrics when transferring
examples from DenseNet-121 to ResNet-50 (DN121 →
RN50), then run a VGG19bn → RN50 test. For both, ǫ is
swept as [0.0, 0.01, 0.03, 0.05, 0.07], where ǫ = 0 indicates
no attack. The results are shown in Fig. 2. The represented
AA attacks are from the best layers.

The ﬁrst trend we see is that for all attacks across all
metrics, as ǫ increases the attack strength increases. We
also see the DenseNet-121 AA (DN-AA) is the most pow-
erful attack, while the ITCM attacks are the least effective,
and the TMIFGSM’s fall in-between. At ǫ = 0.07, DN-
AA achieves random accuracy on the blackbox model at
91.42% error with an improvement of 7.4% over the best
baseline.
It outperforms the DN-TMIFGSM baseline in
terms of uTR, tSuc, and tTR by 7.2%, 32.6%, and 32.5%,
respectively. Also, both DN-AA and VGG-AA are higher
than all baselines in terms of tSuc and tTR, indicating that
both activation based attacks are better blackbox targeted at-
tacks than the baselines. The fact that DN-AA bests VGG-
AA indicates that whitebox model architecture does affect
AA performance. However, it is not intuitively surprising
that the more complex DN121 model is more transferable
to the RN50 blackbox model because both models are quite
deep in comparison to the relatively shallow VGG.

6.2. Depth Results

The most important results for our hypothesis are the
depth results. Here, we ﬁx ǫ = 0.07 as this is where the
most powerful attack was able to achieve random accuracy
on the blackbox model, although our conclusions hold for
all tested epsilons. We then test the AA at different depths
of the models, running a full test step at each. Fig. 3 shows
the results of the depth sweep tests and Table 1 shows the
numerical results of the most powerful layer AA attack ver-
sus baselines.
In Table 1, DN and VGG are CIFAR-10
trained whitebox models, and DNIN is ImageNet trained.
two rows of Fig. 3 are transfers from a
DN121 whitebox model, and the bottom two are from the
VGG19bn whitebox model. We see that layer-wise trans-
ferability characteristics are not dependent on blackbox
model. Meaning, the shape of the trendlines do not change
with the blackbox model. This is crucial to the feasibility
of the attack because it means the attacker may use their
own blackbox model to ﬁnd the best transferring layer, then
use that knowledge to attack the true target blackbox model.
This limits the amount of queries to the target model. Fur-
ther, both DN121 whitebox tests show powerful transfer-

The ﬁrst

Table 1: Numerical Transfer Results (RN50 Blackbox)

Base
DN

VGG

DNIN

Attack
Error
ITCM
58.62
TPGD
61.33
TMIFGSM 84.12
AAL=21
91.49
53.72
ITCM
TPGD
58.85
TMIFGSM 76.19
AAL=6
80.81
21.14
ITCM
TPGD
25.23
TMIFGSM 47.75
AAL=7
80.58

uTR
58.88
61.52
84.32
91.48
54.94
59.55
76.75
80.97
21.18
25.29
47.76
81.77

tSuc
39.97
35.84
42.53
75.13
37.40
36.60
37.72
55.64
0.99
0.94
2.25
2.57

tTR
40.39
36.15
42.90
75.38
41.88
39.15
41.14
55.98
1.01
0.97
2.25
8.63

7070

Figure 3: Error, uTR, tSuc, and tTR rates versus depth for multiple transfer scenarios. The top two rows are transfers from a
DN121 whitebox model and the bottom two rows are transfers from a VGG19bn whitebox model. Dataset: CIFAR-10.

ability in the deeper layers, and both VGG19bn tests show
strong transferability in the middle layers. We also see that
AA’s from some layers yield less powerful attacks, indicat-
ing the choice of AA layer is critical.

If we choose the single best layer to attack from, we can
compare numerical performance to the best baseline, which
in all cases is TMIFGSM (see Table 1). For the DN121
model the best AA is from L = 21, and for VGG19bn
the best AA is from L = 6. When transferring to RN50,
the DN 121L=21 attack outperforms the best baseline by
7.4% in error, 7.2% in uTR, 32.6% in tSuc, and 32.5% in
tTR. Similarly, when transferring to RN50 from VGG19bn,
the V GG19L=6 attack outperforms the best baseline by
4.6%, 4.2%, 17.9%, 14.8% in error, uTR, tSuc, and tTR,
respectively. Notice, for DN121 whitebox, error rate and
untargeted transfer rate are both about 91.5% and targeted
success and transfer rate are both about 75.2%. Such high
numbers indicate that these deep and complex models have
learned similar feature sets and similar decision boundary
structures. Also, adversarial directions for one model are
adversarial in other models. The targeted transfer metrics
further indicate that the orientation of the decision bound-

aries in feature space and the directions to move from one
class to another in feature space are similar across models.
Finally, we can compare performance of AA attacks di-
rectly. When transferring to RN50, DN 121L=21 outper-
forms V GG19L=6 by 10.6%, 10.5%, 19.5%, 19.4% in er-
ror, uTR, tSuc, and tTR, respectively. Thus, we again ﬁnd
that the choice of whitebox model to transfer from is impor-
tant.

6.3. Analysis

The next natural question to ask is why are some lay-
ers better than others, and in particular, why DN 121L=21
and V GG19L=6? Given that the transferability trends in
Fig. 3 do not change with the blackbox model, to answer
this question we only consider characteristics of the white-
box model and the perturbed data. The ﬁrst experiment is
to measure the separability of the class representations in
feature space. Intuitively, we expect that an AA with good
transferability characteristics would come from a layer with
well separated class representations. Fig. 4 shows the av-
erage angular distance between examples of the same class
(intra-class) and examples of different classes (inter-class),

7071

for DN121, and earlier in VGG19bn. Thus, the best layers
to transfer from (DN 121L=21 and V GG19L=6) have well
separated class representations. However, the layer with the
largest difference is not necessarily the most transferable.

Another experiment is to look purely at the data and mea-
sure the average distance between original and perturbed
examples from each layer. We measure Euclidean distance
in the image domain and in two dimensions. Here, we
project to the ﬁrst two principal component directions of
the clean data to measure the attack’s effects along the di-
rections of greatest variance. Instinctively, we may expect
that layers with better transferability characteristics would
produce adversarial examples that are farther from the orig-
inal data, as they may be more likely to have crossed a deci-
sion boundary. Fig. 5 shows the results of these experiments
on the DN121 and VGG19 whitebox models for layers with
some amount of feature separability. For DN121, layers 16
through 21 produce perturbed examples that are further in
two dimensions from the original data, but closer in the im-
age domain. Even though L = 21 doesnt produce the fur-
thest examples, the trend in the two dimensional measure-
ments is similar to the DN121 transferability trends from
Fig. 3 where layers 16-21 have the best transferability char-
acteristics. Similarly, for the VGG19 test, L = 6 produces
examples that are furthest from the originals in two dimen-
sions, while in the image domain these perturbed examples
tend to be closer to the original data. The VGG19 trend in
two dimensions also mimics Fig. 3 trends.

From this analysis, we observe a few tendencies that are
indicative of layers that produce transferable adversarial ex-
amples. First, these layers have well separated class repre-
sentations in feature space. And second, these layers pro-
duce examples that are closer to the original data in the im-
age domain, but further in the ﬁrst two principal component
directions. Unfortunately, none of these results have been
absolutely conclusive. Thus, we leave it to future work to
more thoroughly explore the reasons for layer-wise transfer-
ability characteristics. However, with these analysis tech-
niques, we may now make informed predictions about what
layers will be most transferable when using different data
and models, avoiding the expensive step of sweeping the
layer. See the Supplemental Materials for analysis-ﬁrst ex-
periments on SVHN [17] trained models.

6.4. ImageNet Results

We now extend the experiments to ImageNet trained
classiﬁers. Here, we measure the transferability of a DN121
→ RN50 attack and do a similar analysis of results. Fig.
6 shows transferability results for a ﬁxed ǫ = 0.07 at-
tack, and DNIN rows of Table 1 show numerical results.
Interestingly, we see several different trends here. First,
early and middle layer DN121 AAs are more transferable,
unlike the CIFAR-10 tests. Next,
the targeted transfer

7072

Figure 4: Average angular distance between feature maps
of examples from the same class (intra-class) and examples
of different classes (inter-class) for DN121 and VGG19bn
CIFAR-10 trained models.

Figure 5: L2 distance between original and AA perturbed
images as measured in the image domain (red) and when
projected onto the ﬁrst two principal component directions
of the clean data (blue). Examples generated from DN121
and VGG19bn CIFAR-10 trained models.

across layer depths. For well separated layers, we expect the
inter-class distance to be high and the intra-class distance to
be low, while the difference between the two is large. Fig.
4 shows that classes become well separated in later layers

Figure 6: Error, uTR, tSuc, and tTR rates versus depth for ImageNet trained DN121 → RN50 transfer scenario.

statistics, tSuc and tTR, are lower for all attacks. Con-
sider DN 121L=7 which is arguably the best AA layer. As
compared to the best baseline, the DN 121L=7 AA has
32.8%, 34.0%, 0.3%, 6.3% better error, uTR, tSuc, and tTR,
respectively. The drop in tSuc and tTR from the CIFAR-10
tests speaks to the dissimilarity in decision boundaries of
these less accurate models.

The AA achieves a large margin over the baselines in
terms of fooling rate and likelihood of misclassiﬁcation (i.e.
uTR). This gives an indication that the direction from an ex-
ample to a nearby decision boundary is similar in some lay-
ers. However, the overall layout and orientation of classes
in feature space is not the same between models. This high-
lights a fundamental weakness of single-model transfer-
based targeted attacks. If the models are not well trained
for the source task, the decision boundaries in feature space
are not highly similar, so it is difﬁcult to ﬁnd a direction
to move toward target class regions. An interesting future
work would be to generate AA examples from an ensemble,

Figure 7: Analysis of ImageNet trained DN121 layer-
wise feature similarity (top) and distance between original
and adversarial examples generated with Activation Attack
from this whitebox model (bottom).

which may ﬁnd a mean direction to move.

Fig. 7 (top) shows the analysis of layer separability and
distance between original (clean) and perturbed examples.
Not surprisingly from the tSuc and tTR results, none of the
layers in the DN121 ImageNet model appear to have well
separated classes in feature space. Thus, we would expect
that driving our source features towards features of a single
target example does not necessarily mean we are driving
towards large regions of that target class in feature space. A
potential future work is to drive towards a centroid of target
class examples, which may inﬂate the targeted performance
and boost AA’s tSuc rate.

We also see a changing trend in Fig. 7 (bottom) when
we look at layers with some amount of separability. Rather
than layers 16 through 21 producing examples that are fur-
ther in two dimensions, earlier layers (10 through 15) gen-
erate further examples. This is similar to the Fig. 6 ﬁndings,
where earlier layers transfer better than later ones. This is
further evidence that more transferable AA layers produce
perturbed examples that are further from the originals along
the principal component directions, yet closer to the origi-
nals in the image domain.

7. Conclusion

This work describes an adversarial attack using feature
space perturbations that also provides insights into how
deep learning models make decisions. We show for well
trained models, feature space perturbations are highly trans-
ferable and the layer at which perturbations are transferred
has a large impact on attack effectiveness. Also, the layer-
wise transferability characteristics of a whitebox model are
blackbox model agnostic. Through analysis we ﬁnd layers
that are best to attack from have well separated class rep-
resentations and produce examples that are perturbed more
along the principal component directions. Towards inter-
pretability, we indicate deep CNNs of differing architec-
tures learn similar hierarchical representations of the data
by showing that perturbing features of one model also per-
turbs those features of other models.

Acknowledgements We acknowledge the support of

AFRL (FA8750-18-2-0057).

7073

[18] N. Papernot, P. D. McDaniel, and I. J. Goodfellow. Transfer-
ability in machine learning: from phenomena to black-box
attacks using adversarial samples. CoRR, abs/1605.07277,
2016.

[19] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017.

[20] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and
G. Chowdhary. Robust deep reinforcement learning with ad-
versarial attacks. In AAMAS, 2018.

[21] S. Sabour, Y. Cao, F. Faghri, and D. J. Fleet. Adversarial ma-
nipulation of deep representations. CoRR, abs/1511.05122,
2015.

[22] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[23] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. J. Goodfellow, and R. Fergus. Intriguing properties of neu-
ral networks. CoRR, abs/1312.6199, 2013.

[24] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. D. Mc-
Daniel. Ensemble adversarial training: Attacks and defenses.
CoRR, abs/1705.07204, 2017.

[25] F. Tram`er, N. Papernot, I. J. Goodfellow, D. Boneh, and P. D.
McDaniel. The space of transferable adversarial examples.
CoRR, abs/1704.03453, 2017.

[26] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In NIPS, 2014.

References

[1] N. Carlini and D. A. Wagner. Towards evaluating the robust-
ness of neural networks. 2017 IEEE Symposium on Security
and Privacy (SP), pages 39–57, 2017.

[2] N. Carlini and D. A. Wagner. Audio adversarial examples:
Targeted attacks on speech-to-text. 2018 IEEE Security and
Privacy Workshops (SPW), pages 1–7, 2018.

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
Imagenet: A large-scale hierarchical image database. 2009
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 248–255, 2009.

[4] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li.

Boosting adversarial attacks with momentum. 2017.

[5] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples. CoRR, abs/1412.6572,
2014.

[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 770–
778, 2016.

[7] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks. 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2261–2269, 2017.

[8] J. Kos, I. Fischer, and D. X. Song. Adversarial examples for
generative models. 2018 IEEE Security and Privacy Work-
shops (SPW), pages 36–42, 2018.

[9] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian

institute for advanced research).

[10] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial

machine learning at scale. CoRR, abs/1611.01236, 2016.

[11] A. Kurakin, I. J. Goodfellow, S. Bengio, Y. Dong, F. Liao,
M. Liang, T. Pang, J. Zhu, X. Hu, C. Xie, J. Wang,
Z. Zhang, Z. Ren, A. L. Yuille, S. Huang, Y. Zhao, Y. Zhao,
Z. Han, J. Long, Y. Berdibekov, T. Akiba, S. Tokui, and
M. Abe. Adversarial attacks and defences competition.
CoRR, abs/1804.00097, 2018.

[12] K. Liu.

pytorch-cifar.

https://github.com/

kuangliu/pytorch-cifar, 2017.

[13] Y. Liu, X. Chen, C. Liu, and D. X. Song. Delving into trans-
ferable adversarial examples and black-box attacks. CoRR,
abs/1611.02770, 2016.

[14] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to adver-
sarial attacks. CoRR, abs/1706.06083, 2017.

[15] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi,

and
P. Frossard. Universal adversarial perturbations. 2017 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 86–94, 2017.

[16] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: A simple and accurate method to fool deep neural net-
works. 2016 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 2574–2582, 2016.

[17] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. 2011.

7074

