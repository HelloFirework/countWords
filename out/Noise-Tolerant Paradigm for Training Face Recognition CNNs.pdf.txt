Noise-Tolerant Paradigm for Training Face Recognition CNNs

Wei Hu1 Yangyu Huang2∗

Fan Zhang1 Ruirui Li1

1Beijing University of Chemical Technology, China

2Yunshitu Corporation, China

1{huwei,zhangf,liruirui}@mail.buct.edu.cn

2yangyu.huang.1990@gmail.com

Abstract

Beneﬁt from large-scale training datasets, deep Convo-
lutional Neural Networks(CNNs) have achieved impressive
results in face recognition(FR). However, tremendous scale
of datasets inevitably lead to noisy data, which obviously
reduce the performance of the trained CNN models. Kick-
ing out wrong labels from large-scale FR datasets is stil-
l very expensive, although some cleaning approaches are
proposed. According to the analysis of the whole process of
training CNN models supervised by angular margin based
loss(AM-Loss) functions, we ﬁnd that the θ distribution of
training samples implicitly reﬂects their probability of be-
ing clean. Thus, we propose a novel training paradigm that
employs the idea of weighting samples based on the above
probability. Without any prior knowledge of noise, we can
train high performance CNN models with large-scale FR
datasets. Experiments demonstrate the effectiveness of our
training paradigm. The codes are available at https:
//github.com/huangyangyu/NoiseFace.

1. Introduction

Large-scale datasets are crucial for training deep CNNs
in FR, and the scale of training datasets is growing tremen-
dously. For example, a widely used FR training dataset,
MS-Celeb-1M [11], contains about 100K celebrities and
10M images. However, a previous work [42] points out
that a million scale FR dataset typically has a noise rate
higher than 30% (about 50% in the original MS-Celeb-1M).
The presence of noisy training data may adversely affec-
t the ﬁnal performance of trained CNNs. Though a recent
work [35] reports that deep CNNs still perform well even on
noisy datasets containing sufﬁcient clean data, this conclu-
sion cannot be transferred to FR, and experiments demon-
strate that noisy data apparently decrease the performance
of the trained FR CNNs [42].

Large-scale datasets with high-quality label annotation-
s are very expensive to obtain. Cleaning large-scale FR
datasets with automatic or semi-automatic approaches [11,

49, 5] cannot really solve this problem. As can be seen,
existed large-scale FR datasets, such as MS-Celeb-1M and
MegaFace [19], still consist considerable incorrect labels.
To obtain a noise-controlled FR dataset, manual annota-
tion is inevitable. Although an approach is introduced to
effectively build a high-quality dataset IMDB-Face [42], it
actually further demonstrates the difﬁculties of obtaining a
large-scale well-annotated FR dataset. For example, it took
50 annotators one month to clean the IMDB-Face dataset,
which only contains 59K celebrities and 1.7M images.

Numerous training approaches have been investigated
aiming to train classiﬁcation CNNs with noisy dataset-
s [31, 50, 38, 10, 41, 12, 21, 9], but most of them are
not suitable for training FR models, because of the spe-
cial characters of FR datasets (discussed in the Section 2).
Recently, weighting training samples is a promising direc-
tion [18, 13, 26] to deal with noisy data. However, ex-
tra datasets or complex networks are required in these ap-
proaches, limiting the use of them in FR.

The CNN models in FR are usally trained with loss func-
tions, which aim to maximize inter-identity variation and
minimize intra-identity variation under a certain metric s-
pace. Very recently, some angular margin based loss(AM-
Loss for short in the paper) functions [24, 5, 45, 43] are
proposed and achieve the state-of-the-art performance.

In this paper, we propose a noise-tolerant paradigm to
learn face features on a large-scale noisy dataset directly,
different from other related approaches [49, 5] which aim
to clean the noisy dataset ﬁrstly. When training an AM-
Loss supervised CNN model, the θ histogram distribution
of training samples(the θ distribution for short, described in
Section 3.2) is employed to measure the possibility that a
sample is correctly labeled, and this possibility is then used
to determine the training weight of the sample. Through-
out the training process, the proposed paradigm can allevi-
ate the impact of noisy data by dynamically adjusting the
weight of samples, according to their θ distribution at that
time.

To summarize, our major works are as follows:

1. We observe that the θ value of a clean sample has high-

11887

er probability to be smaller than that of a noisy sample
for an AM-Loss function. In other word, the possibil-
ity that a sample is clean can be dynamically reﬂected
by its position in the θ distribution.

2. Based on the above observation, we employ the idea
of weighting training samples, and present a novel
noise-tolerant paradigm to train FR CNN models with
a noisy dataset end-to-end. Without any prior knowl-
edge of noise in the training dataset (noise rate, small
clean sets, etc.), the models can achieve comparable,
or even better performance, compared with the model-
s trained by traditional methods with the same dataset
without noisy samples.

3. Although many approaches are proposed to train clas-
siﬁcation models with noisy datasets, none of them
aims to train FR CNN models. To the best of our
knowledge, the proposed paradigm is the ﬁrst to study
the method to signiﬁcantly eliminate adverse effects
of extremely noisy data with deep CNN models in FR.
Our paradigm is also the ﬁrst to estimate the noise rate
of a noisy dataset accurately in FR. Furthermore, our
trained models can also achieve good performance on
clean datasets too.

2. Related Works

2.1. Training with Noisy Data

Learning with noisy datasets has been widely explored
in image classiﬁcation training [7]. In classic image classi-
ﬁcation datasets, the real-world noisy labels exhibit multi-
mode characteristics. Therefore, many approaches use pre-
deﬁned knowledge to learn the mapping between noisy
and clean annotations, and focus on estimating the noise
transition matrix to remove or correct mis-labeled sam-
ples [27, 23, 30]. Recently, it has also been studied in the
[50] relies on manually labeling
context of deep CNNs.
[38, 10] add layer in CNN mod-
to estimate the matrix.
[41, 14] use a s-
els to learn the noise transition matrix.
mall clean dataset to learn a mapping between noisy and
clean annotations. [31, 9] use noise-tolerant loss functions
to correct noisy labels. Li et al. [21] construct a knowledge
graph to guide the learning process. Han et al. [12] propose
a human-assisted approach which incorporates an structure
prior to derive a structure-aware probabilistic model. Dif-
ferent from the common classiﬁcation datasets, FR datasets
always contain a very large number of classes(persons), but
each class contains relatively small number of images, mak-
ing it difﬁcult to ﬁnd relationship patterns from noisy da-
ta. Furthermore, noisy labels behave more like independent
random outliers in FR datasets. Therefore, the transition
matrix or the relationship between noisy and clean labels is
very hard to be estimated from FR datasets.

Some approaches attempt to update the trained CNNs
only with separated clean samples, instead of correcting
the noisy labels. A Decoupling technique [26] trains two
CNN models to select samples that have different predic-
tions from these two models, but it cannot process heavy
noisy datasets. Very recently, weighting training samples
becomes a hot topic to learn with noisy datasets.

2.1.1 Weighting Training Samples

Weighting training samples is a well studied technique and
can be applied to adjust the contributions of samples for
training CNN models [8, 22, 20]. Huber loss [8] reduces the
contribution of hard samples by down-weighting the loss of
them. In contrast, Focal loss [22] adds a weighting factor
to emphasize hard samples for training high accurate detec-
tor. Multiple step training is adopted in [20] to encourage
learning easier samples ﬁrst.

The idea of weighting training samples is employed to
train models with noisy datasets too, since clean/noisy sam-
ples are usually corresponding to easy/hard samples. The
key of weighting samples is an effective method to measure
the possibility that a sample is easy/clean. Based on Cur-
riculum learning [2], MentorNet [18] and Coteaching [13]
try to select clean samples using the small-loss strategy, but
the noise level should be provided in advanced in [13], and
a small clean set and a pre-training model are suggested in
[18]. Ren et al. [34] also employ the clean validation set to
help learning sample weights. Although FR can be regard-
ed as a classiﬁcation problem, its small subset/validation
dataset is usually uncertain to be clean, even not available
at all. To conclude, weighting samples is a promising di-
rection to train CNN models with noisy datasets, but esti-
mating sample weights usually requires complex techniques
and extra knowledge.

2.2. Loss functions in FR

Deep face recognition has been one of the most active
ﬁeld in these years. Usually, FR is trained as a multi-class
classiﬁcation problem in which the CNN models are usual-
ly supervised by the softmax loss [40, 39, 46]. Some met-
ric learning loss functions, such as contrastive loss [51, 4],
triplet loss [15, 36] and center loss [47], are also applied
to boost FR performance greatly. Other loss function-
s [6, 53] also demonstrate effective performance on FR. Re-
cently, some normalization [25, 44, 54] and angular mar-
gin [24, 5, 43, 45] based methods are proposed and achieve
outperforming performance, and get more attention.

11888

3. Preliminaries

3.1. Angular Margin based Losses

Recently, the angular margin based loss functions, which
have intrinsic consistency with Softmax loss, greatly im-
proves the performance of FR. The traditional Softmax loss
is presented as

L = −

1
N

N

X

i=1

log

xi+byi

yi

eWT
j=1 eWT

j

PC

,

xi+bj

(1)

where xi denotes the feature of the i-th sample which be-
longs to the yi-th class. Wj denotes the j-th column of the
weights W in the layer and b is the bias term. N and C is
the batch size and the class number. In all AM-Losses, the
bias bj is ﬁxed to be 0 and kWjk is set to 1, then the target
logit [32] can be reformulated as

WT
j

xi = kxikcos θi,j,

(2)

where θi,j is the angle between Wj and xi. We can further
ﬁx kxik = s, and the Softmax loss can be reformulated as

L = −

1
N

N

X

i=1

log

es cos θi,yi
j=1 es cos θi,j

PC

,

(3)

and we refer this loss as L2-Softmax in this paper. Actu-
ally, other AM-Losses have similar loss functions, but with
slightly different decision boundaries.

In all AM-Losses, Wj can be regarded as the anchor of
the j-th class. During training, the angle θi,j will be min-
imized for an input feature xi belonging to the j-th class,
and the angle θi,k(k6=j) will be maximized at the same time.
As discussed in [5], the θi,j of an input xi belonging to the
j-th class reﬂects the difﬁculty of training the corresponding
sample for a full trained CNN model, and the θ distribution
of all training samples can implicitly demonstrate the per-
formance of the model. We ﬁrstly investigate the effect of
noisy data on the θ distributions.

3.2. Effect of Noise on θ Distributions

To investigate the effect of noise, WebFace-Clean1 con-
taining 10K celebrities and 455K images is chosen as the
clean dataset. It is cleaned by manually removing incorrect
images from the original CASIA-WebFace [51] containing
494K images. [42] estimates that there are about 9.3%-13%
mis-labeled images in the original CASIA-WebFace, so we
can regard WebFace-Clean as a noise-free dataset. Sever-
al experiments are performed, and their input and training
settings are described in the Section 5.

We ﬁrstly build another new noise-free FR dataset,
named WebFace-Clean-Sub, which contains 60% images

1github.com/happynear/FaceVerification

randomly chosen from all celebrities in WebFace-Clean.
The remaining 40% images are used to synthesis noisy data,
named WebFace-Noisy-Sub. Noise in FR datasets mainly
fall into two types:
label ﬂips, where an image has been
given a label of another class within the dataset, and out-
liers, where an image does not belong to any of the classes,
but mistakenly has one of their labels, or non-faces can be
found in the image. To generate Web-Noisy-Sub, we syn-
thesize label ﬂips noise by randomly changing face label-
s into incorrect classes, and simulate outlier noise by ran-
domly polluting data with images from MegaFace [19], and
we keep the ratio of label ﬂips and outliers at 50%:50%.
Therefore, we get a new noisy dataset WebFace-All con-
taining WebFace-Clean-Sub and WebFace-Noisy-Sub, and
its noise rate is 40%.

A ResNet-20 model(CNN-All-L2) [24] supervised with
L2-Softmax(s = 32) is trained with WebFace-All. For
comparison, we also train another ResNet-20 model(CNN-
Clean-L2) with WebFace-All, but with a small modiﬁca-
tion: a sample will be dropped in training if it belongs
to WebFace-Noisy-Sub. Therefore, CNN-Clean-L2 can be
considered to be trained only with clean samples.

In each training iteration, we use Wj to compute θi,j
for all samples belonging to the j-th class. For simpliﬁ-
cation, only cosθ is computed in our implementation. We
refer the cos θ histogram distributions(the bin size is 0.01)
of all training samples as Histall. Moreover, we compute
the distributions of clean and noisy samples separately as
Histclean and Histnoisy.

Figure 1 shows the distributions of CNN-Clean-L2 and
CNN-All-L2. The test accuracy on LFW [16] is used to
demonstrate the performance of the trained CNNs. From
Figure 1, we have following observations:

1. Histclean and Histnoisy are all Gaussian-like distri-
butions throughout the training process for CNN-All-
L2 and CNN-Clean-L2. Experiments in ArcFace [5]
also demonstrate this phenomenon. The Gaussian-like
distributions should be caused by similar quality dis-
tributions [42] in FR datasets.

2. At the beginning of training,Histclean and Histnoisy
are largely overlapping, making them impossible to be
separated from each other, since the CNNs are initially
untrained.

3. After a short period, Histclean starts to move to the
right. If noisy samples are involved in training (CNN-
All-L2), Histnoisy moves to the right too, but it has
been always on the left side of Histclean. Therefore,
the samples with larger cosθ values have larger proba-
bility to be clean. This phenomenon is mainly because
that the CNNs memorize easy/clean samples quickly,
and can also memorize hard/noisy samples eventual-
ly [1].

11889

(a) CNN-Clean-L2

Figure 1. The cosθ histogram distributions of CNN-Clean-L2 (top) and CNN-All-L2 (bottom). Histclean, Histnoisy and Histall are
colored with Green, Red and Yellow respectively. The curve edge of each distribution is smoothed with a mean ﬁlter with size = 5 to
remove noise (described in the Section 4.2.1).

(b) CNN-All-L2

4. In the latter stage of training process, the cosθ of a
sample in Histclean reﬂects the quality of the corre-
sponding face image. Some face images of easy, semi-
hard, and hard clean samples are provided in Figure 1.

5. The performance of CNN-All-L2 is adversely affected
by noisy data. From the distribution in Figure 1(b), we
can observe the negative impact in two aspects: (1)
Histclean and Histnoisy have large overlapping re-
gions throughout the training process; (2) Compared
with the Histclean in Figure 1(a), the Histclean in
Figure 1(b) is on the left side.

These observations can be explained in theory, and more
experiments are further performed to conﬁrm them: (1)We
increase the noise rate from 40% to 60%; (2)ArcFace [5]
is employed to supervise the CNNs; (3) we replace the
ResNet-20 with a deeper ResNet-64 [24]; (4) Another clean
dataset IMDB-Face [42]2 is chosen to replace WebFace-
Clean. Their ﬁnal cosθ distributions shown in Figure 2 fur-
ther approve our observations.

CNN-Clean-L2 is actually trained by using a ideal
paradigm: ignoring all noisy sample in training. However, it
is difﬁcult to predict if a sample is noisy in real training. In
this paper, we propose a paradigm to minimize the impact
of noisy samples based on the cosθ distributions.

4. The Proposed Paradigm

We propose a new training paradigm for learning face
features from large-scale noisy data based on above obser-

2We only downloaded 1.2M images of total 1.7M images with the pro-

vided URLs

76.61% accuracy on LFW

92.33% accuracy on LFW

40000

s
r
e
b
m
u
N

30000

20000

10000

0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

40000

s
r
e
b
m
u
N

30000

20000

10000

0

0.6

0.8

1.0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

0.6

0.8

1.0

(a) 60% noisy samples

(b) ArcFace loss function

94.20% accuracy on LFW

92.35% accuracy on LFW

40000

s
r
e
b
m
u
N

30000

20000

10000

0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

40000

s
r
e
b
m
u
N

30000

20000

10000

0

0.6

0.8

1.0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

0.6

0.8

1.0

(c) ResNet-64 model

(d) IMDB-Face dataset

Figure 2. The ﬁnal cosθ distributions of other four models. These
distributions further conﬁrm our observations.

vations. In each mini-batch training, we compute cosθ for
all training samples, and the current distribution Histall.
We deﬁne δl and δr are the leftmost/rightmost cosθ values
in Histall. Based on the ﬁrst observation in Section 3.2, no
more than 2 peaks can be detected in Histall. µl and µr
denote the cosθ values of the left/right peaks respectively,
and µl = µr if there is only one peak.

The target of the paradigm is to correctly evaluate the
probability of a sample being clean in training, and then
adjust the weight of the sample according to this probability.
The key idea of our paradigm can be brieﬂy introduced as:

11890

(a) All samples are equally treated

(b) Clean samples are emphasized

(c) Semi-hard clean samples are emphasized

Figure 3. The cosθ distributions and the corresponding ω of three strategies.

1. At the beginning of training process, all samples are

treated equally.

According to the 2nd observation in Section 3.2, at the
beginning of training process, the trained CNN does
not have the ability for face recognition. Therefore, all
samples should have the same weight for training.

2. After a short period of training, samples with larger

cosθ have larger weight.

According to the 3rd observation in Section 3.2, af-
ter a short period of training, samples with larger cosθ
should have larger weight for training.

3. In the end of training, semi-hard clean samples are em-

phasized to further promote the performance.

According to the 4th observation in Section 3.2, easy,
semi-hard and hard clean samples can be distinguished
according to their cosθ in Histclean at this time.
The pre-condition of training semi-hard clean sam-
ples is that the trained CNN already has good per-
formance, and the overlapping area of Histclean and
Histnoisy is relatively small.
In this circumstance,
semi-hard clean samples will have larger weight than
other samples for training. Training semi-hard clean
samples is also implicitly adopted in ArcFace [5] and
FaceNet [36].

We present three corresponding strategies to compute

sample weights as following:

Strategy One In this strategy (see Figure 3(a)), all sam-

ples have the same weight as

where z = cosθi,j −µl
, and sof tplus(x) = log(1 + ex) is
a smooth version of the RELU activation[29]. λ is used to
normalize the function, and λ = 10 in all experiments.

δr −µl

Strategy Three In this strategy (see Figure 3(c)), the
semi-hard clean samples are emphasized. We deﬁne µr as
the cosθ value of the right peak in Histall (µl correspond-
ing to the left peak), which can be consider as the center of
Histclean (according to the 5-th and the 2-th observation).
The sample weight is computed as

ω3,i = e−(cosθi,j −µr)2/2σ2

,

(6)

where σ2 is the variance of the Histclean, which can be
approximated by using the part to the right of µr. We set
σ = (δr − µr)/2.576 to cover 99% samples in Histclean.

4.1. Compute Time Varying Fusion Weight

Three weighting sample strategies are introduced as
above, but how to select the applied strategy in training?
There is no clear criteria. Inspired by the gradually learn-
ing technique in Co-teaching [13], we compute the sample
weight in a fusion way.

According to our observations, δr is a good signal to
approximately reﬂect the performance of the trained CNN.
As the CNN achieves better performance, δr will gradually
move to the right. We deﬁne a threshold ζ to divide possible
δr values into two ranges: [0, ζ] and (ζ, 1]. The selected s-
trategy is changed from the 1st one to the 2nd one in the ﬁrst
range, and from 2nd one to the 3rd one in the second range.
In all experiments, we set ζ = 0.5. Then, we compute the
sample weight as

ωi = α(δr)ω1,i + β(δr)ω2,i + γ(δr)ω3,i,

(7)

ω1,i = 1,

(4)

where

where ω1,i is the weight of an input sample xi.

Strategy Two In this strategy (see Figure 3(b)), the sam-

ples with larger cosθ have larger weight as

ω2,i =

sof tplus(λz)
sof tplus(λ)

,

(5)

1

1

−

1 + e5−20δr

α(δr) = (2−

1 + e20δr −15 )⌈0.5−δr⌉, (8)
, β(δr) = 1 − α(δr) − γ(δr) and γ(δr) = α(1.0 − δr). As
shown in Figure 4, at ﬁrst, ω1,i has the greatest impact. As
δr gradually moves to right, ω2,i and then ω3,i begin to play
more important roles.

11891

(a) α(δr), β(δr), and γ(δr)

(b) Two fusion examples

Figure 4. The left ﬁgure demonstrates three functions: α(δr), β(δr), and γ(δr). The right ﬁgure shows two fusion examples. According
to the ω, we can see that the easy/clean samples are emphasized in the ﬁrst example(δr < 0.5), and the semi-hard clean samples are
emphasized in the second example(δr > 0.5).

4.1.1 Discussion on Weight Computation

Equation 4, Equation 5 and Equation 6 are used to compute
the sample weight during training. These 3 equations di-
rectly reﬂect the key idea of our paradigm, but they are on-
ly introduced empirically and have some free parameters.
The same situation also exists in Equation 7. The key con-
tribution of our approach is the ideas of training paradigm
and weight fusion. We also perform some experiments, and
found that good performance can be achieved as long as the
used equations can correctly reﬂect the key ideas. There-
fore, these equations are provided for reference, and more
exploration could be conducted to ﬁnd other theoretical jus-
tiﬁed equations.

4.2. Implementation Details

4.2.1 Histall Related Variables

According to the Equation 7, cosθi,j , δr, δl and µr are re-
quired to compute the ﬁnal weight of the sample xi belong-
ing to the j-th class. Except cosθi,j , other variables are
computed based on Histall.

In theory, Histall should be computed in each mini-
batch training, which is very time-consuming because the
number of samples is usually very large in FR datasets. In
our implementation, the cosθ values of recent K training
samples are stored to compute another distribution HistK .
The training samples are pre-shufﬂed, HistK can be con-
sidered as an approximate Histall with a suitable K. We
set K = 64, 000 (1000 batches) in our experiments.

To resist noise, a mean ﬁlter with size 5 is ﬁrstly ap-
plied to HistK to remove noise. We select the top 0.5%
leftmost/rightmost cosθ values as δl and δr. A very simple
method is applied to ﬁnd all peaks in HistK : the number
of frequency in a bin is larger than all of its left/right neigh-
bour bins (Radius = 5). Theoretically, we can only ﬁnd
one or two peaks during training. However, we sometimes
ﬁnd more than two peaks, or ﬁnd no peak at all, because
Histclean and Histnoisy are actually not always Gaussian-

like distributions. We employ a simple technique to ﬁnd µr:
if there is only one peak ∈ (ζ, 1], its cosθ is the µr, and if
more than one peaks are found, we choose the highest one.
Similar method can be used to ﬁnd µl.

When the noise rate is very high, µr may become dif-
ﬁcult to detect.
In this circumstance, the key is to train
easy/clean samples as much as possible, and ω2,i should
play more important role than ω3,i. Therefore, missing µr
may have few impact on the ﬁnal performance. In the con-
trary, if the noise rate is very low, missing µl may also have
few impact on the ﬁnal performance.

4.2.2 Weighting in AM-Losses

Method 1 Usually, the weight is applied to minimize the
loss and the weighted loss function of L2-Softmax is

L1 = −

1
N

N

X

i=1

ωilog

es cos θi,yi
j=1 es cos θi,j

PC

.

(9)

Moreover, there is another method to apply sample weights.
Method 2 In AM-Losses, an input xi is normalized and
re-scaled with a parameter s (kxik can be regarded as s in
SphereFace). The scaling parameter s is better to be a prop-
erly large value as the hypersphere radius, according to the
discussion in [45, 5]. A small s can lead to insufﬁcient con-
vergence even no convergence. The lower bound of s is also
discussed in [45, 33]. Inspired by the effect of s, we can
also apply the weights ωi to the scaling parameter during
training CNNs. Therefore, the loss function of L2-Softmax
can be formulated as

L2 = −

1
N

N

X

i=1

log

eωis cos θi,yi
j=1 eωis cos θi,j

PC

.

(10)

Similar method can be applied in other AM-Losses too.
Two methods all can be employed to train CNNs with noisy
datasets. According to our experiments, the latter one shows
better performance in most cases.

11892

The 1000-th iteration, 60.18% accuracy on LFW

The 8000-th iteration, 70.10% accuracy on LFW

The 30000-th iteration, 88.91% accuracy on LFW

The 200000-th iteration, 93.90% accuracy on LFW

40000

s
r
e
b
m
u
N

30000

20000

10000

0

-0.4

-0.2

0.0

40000

s
r
e
b
m
u
N

30000

20000

10000

0.6

0.8

1.0

-0.4

-0.2

0.0

0

0.4

0.2

 cos θ 

40000

s
r
e
b
m
u
N

30000

20000

10000

0.6

0.8

1.0

-0.4

-0.2

0.0

0

0.4

0.2

 cos θ 

40000

s
r
e
b
m
u
N

30000

20000

10000

0

0.6

0.8

1.0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

0.6

0.8

1.0

0.4

0.2

 cos θ 

Figure 5. The cosθ distributions of our CNN model trained with 40% noisy samples.

5. Experiments

87.61% accuracy on LFW

97.23% accuracy on LFW

To verify the effectiveness of our method, several exper-
iments are performed.
In these experiments, face images
and landmarks are detected by MTCNN [52], then aligned
by similar transformation as [49], and cropped to 128 × 128
RGB images. Each pixel in RGB images is normalized by
subtracting 127.5 then dividing by 128. We use Caffe [17]
to implement CNN models. For fair comparison, all CN-
N models are trained with SGD algorithm with the batch
size of 64 on 1 TitanX GPUs. The weight decay is set to
0.00005. The learning rate is initially 0.1 and divided by
10 at the 80K, 160K iterations, and we ﬁnish the training
process at 200K iterations.

First, we perform a similar experiment as in the Sec-
tion 3.2, but using the proposed training paradigm. Figure 5
shows the cosθ distributions during training. It is obvious
that Histclean are separated from Histnoisy. According
to the ﬁnal distributions in Figure 5, Figure 1(a) and Fig-
ure 1(b), the adverse effect from noisy samples is largely
eliminated this time.

Corresponding to 4 models in Figure 2, we re-train them
using the proposed paradigm, and the ﬁnal distributions are
shown in Figure 6. Our method also gets better results.

We perform experiments with different noise rates, su-
pervised AM-Losses and computing weighted loss method-
s(see Section 4.2.2) with the experiment in the Section 3.2.
The models are evaluated on Labelled Faces in the Wild
(LFW) [16], Celebrities in Frontal Proﬁle (CFP) [37], and
Age Database (AgeDB) [28]. As shown in Table 1, com-
petitive performance can be achieved using our paradigm,
without any prior knowledge about noise in training data.
We can surprisedly see that some results of CN Nm2 are
even better than the results of CN Nclean. This improve-
ment is mainly caused by semi-hard training in the ﬁnal
stage. It can be seen that the 2nd method in Section 4.2.2
demonstrates a better performance.

For comparison, we also implemented a recently pro-
posed noise-robust method for image classiﬁcation: Co-
teaching [13] (CN Nct), which selects small-loss samples
from each mini-batch. Note the noise rate should be pre-
given in Co-teaching. The results prove that general noise-
robust approaches cannot achieve satisﬁed performance in

40000

s
r
e
b
m
u
N

30000

20000

10000

0

-0.4

-0.2

0.0

40000

s
r
e
b
m
u
N

30000

20000

10000

0

0.6

0.8

1.0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

0.6

0.8

1.0

0.4

0.2

 cos θ 

(a) 60% noisy samples

(b) ArcFace loss function

96.56% accuracy on LFW

96.11% accuracy on LFW

40000

s
r
e
b
m
u
N

30000

20000

10000

0

-0.4

-0.2

0.0

40000

s
r
e
b
m
u
N

30000

20000

10000

0

0.6

0.8

1.0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

0.6

0.8

1.0

0.4

0.2

 cos θ 

(c) ResNet-64 model

(d) IMDB-Face dataset

Figure 6. The ﬁnal cosθ distributions of four models (correspond-
ing to four models Figure 2) using our paradigm.

FR.

5.1. Estimating Noise Rate

There is an interesting observation from Figure 5 and
Figure 6: at the end of training process, the region on the
left of µl approximately contains half of noisy samples, so
we can estimate the noise rate in the training dataset.
If
the left peak (µl) is not detected, the region on the right of
µr, which contains about half of clean samples, also can
be used. The estimated rates in Table 1 further prove the
effectiveness of our method.

5.2. Learning from Original MS Celeb 1M

The original MS-Celeb-1M [11] contains 99,892 celebri-
ties, and 8,456,240 images. For comparison, two ResNet-
64 [24], CN Nours and CN Nnormal, supervised with Ar-
cFace [5] are employed to learn face features from MS-
Celeb-1M, one using the proposed paradigm and the other
not. To accelerate convergence speed, these ResNet-64 are
ﬁrstly trained with Casia-WebFace [51], then ﬁnetuned with
MS-Celeb-1M. Other training parameters are similar with
the previous experiments. A reﬁned MS-Celeb-1M, con-
taining 79,077 celebrities and 4,086,798 images, is provid-

11893

Loss

L2-

Softmax

ArcFace

Noise
Rate
0%
20%
40%
60%
0%
20%
40%
60%

CNNclean

CNNnormal

CNNct

CNNm1

LFW AgeDB
79.95
94.65
79.33
94.18
76.51
92.71
70.28
91.15
97.95
88.48
88.75
97.80
84.93
96.53
94.56
80.75

CFP
82.04
81.00
77.10
74.74
91.07
89.54
84.81
80.52

LFW AgeDB
79.95
94.65
66.83
89.05
58.95
85.63
51.38
76.61
97.95
88.48
82.83
96.48
72.68
92.33
84.05
58.73

CFP
82.04
71.55
68.78
63.12
91.07
82.52
74.11
67.70

LFW

-

92.12
87.10
83.66

-

96.53
94.25
90.36

LFW
95.00
92.95
89.91
86.11
97.11
96.83
95.88
93.66

CNNm2
LFW AgeDB
84.05
96.28
81.91
95.26
78.38
93.90
64.43
87.61
98.11
88.61
88.46
97.76
86.03
97.23
95.15
81.45

CFP
87.88
84.77
81.37
70.54
90.81
90.22
88.41
83.25

Estimated
Noise Rate

2%
18%
42%
56%
2%
18%
36%
54%

Table 1. Comparison of accuracies(%) on LFW, AgeDB(30), and CFP(FP). ResNet-20 models are used. CN Nclean is trained only
with clean data (WebFace-Clean-Sub) as Upper Bound. CN Nnormal is trained with the noisy dataset WebFace-All using the traditional
method. CN Nct is trained with WebFace-All using our implemented Co-teaching(with pre-given noise rates). CN Nm1 and CN Nm2
are all trained with WebFace-All but using the proposed approach, and they respectively use the 1st and 2nd method to compute loss(see
Section 4.2.2). CN Nm1 and CN Nct are only evaluated on LFW.

ed in LightCNN [49], so the noise rate is about 51.6%. We
also train a CNN (CN Nclean) with the reﬁned MS-Celeb-
1M for comparison. Histall, together with Histclean and
Histnoisy approximated according to the reﬁned dataset,
are presented in Figure 7. According to Histnoisy, we es-
timate that the noise rate of the original MS-Celeb-1M is
about 43%.

s
r
e
b
m
u
N

400K

300K

200K

100K

0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

s
r
e
b
m
u
N

400K

300K

200K

100K

0

0.6

0.8

1.0

-0.4

-0.2

0.0

0.4

0.2

 cos θ 

0.6

0.8

1.0

Figure 7. The ﬁnal cosθ distributions of CN Nnormal (left) and
CN Nours (right).

The trained CNNs are then evaluated on LFW, AgeDB-
30, CFP-FP, YTF [48] and MegaFace Challenge 1 [19], as
shown in Table 2. CosFace [45] and ArcFace [5] are added
for comparison since they use the same network or AM-
Loss with ours. The competitive performance of CN Nours
demonstrates the effectiveness of our training paradigm.

6. Conclusion and Future Work

In this paper, we propose a FR training paradigm, which
employs the idea of weighting training samples, to train
AM-Loss supervised CNNs with large-scale noisy data. At
different stages of training process, our paradigm adjust-
s the weight of a sample based on the cosθ distribution to
improve the robustness of the trained CNN models. Experi-
ments demonstrate the effectiveness of our approach. With-
out any prior knowledge of noise, the CNN model can be
directly trained with an extremely noisy dataset (> 50%
noisy samples), and achieves comparable performance with
the model trained with an equal-size clean dataset. More-
over, the noise rate of a FR dataset can also be approximated

Method

CosFace [45]
ArcFace [5]
CN Nnormal
CN Nclean
CN Nours

-

LFW AgeDB
99.73
99.83
99.21
99.67
99.72

98.08
90.85
96.50
96.70

CFP

-

96.82
93.38
95.74
96.40

YTF
97.6

-

95.64
97.12
97.36

MF1
77.11
83.27
70.16
78.21
78.69

Table 2. Comparison of accuracies(%) on several public bench-
marks. Accuracies of CosFace and ArcFace are cited from their
original papers. CosFace is trained with a clean dataset contain-
ing 90K identities and 5M images. ArcFace is trained with a
manually reﬁned dataset containing 93K identities and 6.9M im-
ages. Their datasets all are composed of several public datasets
including reﬁned VGG2 [3], MS-Celeb-1M, etc. CN Nours and
CN Nnormal are trained only with the original noisy MS-Celeb-
1M(noise rate ≈ 50%). CN Nclean is trained with the reﬁned
MS-Celeb-1M [49]. An improved ResNet-100 is used in ArcFace,
and other 4 methods all use a ResNet-64 CNN model.

with our approach.

The proposed paradigm also has its limitations. First-
ly, it shares the same limitations with most of noise-robust
training methods: the hard clean samples also have smal-
l weight, which might affect the performance. However,
reducing effects of noisy samples should have higher pri-
ority while learning with a heavy noisy dataset. Secondly,
Guassian-like distributions cannot be guaranteed through-
out the whole training process. Fortunately, our method to
ﬁnd left/right peaks and end points does not heavily depend
on this assumption. Lastly, we need to study the reason
that the 2nd method is superior to the 1st method in Sec-
tion 4.2.2.

To conclude, this work will greatly reduce the require-
ment for clean datasets when training FR CNN models, and
makes constructing huge-scale noisy FR datasets a valuable
job. Moreover, our approach also can be employed to help
reﬁne a large-scale noisy FR dataset.

11894

References

[1] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio,
M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Ben-
gio, et al. A closer look at memorization in deep networks.
In IMCL, 2017. 3

[2] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-
riculum learning. In Proceedings of the 26th annual interna-
tional conference on machine learning, pages 41–48. ACM,
2009. 2

[3] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman.
Vggface2: A dataset for recognising faces across pose and
age. In Automatic Face & Gesture Recognition (FG 2018),
2018 13th IEEE International Conference on, pages 67–74.
IEEE, 2018. 8

[4] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR (1), pages 539–546. IEEE Computer Society, 2005.
2

[5] J. Deng, J. Guo, and S. Zafeiriou. Arcface: Additive angular
margin loss for deep face recognition. June 2018. 1, 2, 3, 4,
5, 6, 7, 8

[6] J. Deng, Y. Zhou, and S. Zafeiriou. Marginal loss for deep
face recognition.
In IEEE Conference on Computer Vi-
sion and Pattern Recognition Workshops, pages 2006–2014,
2017. 2

[7] B. Fr´enay and M. Verleysen. Classiﬁcation in the presence of
label noise: a survey. IEEE transactions on neural networks
and learning systems, 25(5):845–869, 2014. 2

[8] J. Friedman, T. Hastie, and R. Tibshirani. The elements of
statistical learning, volume 1. Springer series in statistics
New York, NY, USA:, 2001. 2

[9] A. Ghosh, H. Kumar, and P. Sastry. Robust loss functions
under label noise for deep neural networks. In AAAI, pages
1919–1925, 2017. 1, 2

[10] J. Goldberger and E. Ben-Reuven. Training deep neural-
networks using a noise adaptation layer. In ICLR, 2017. 1,
2

[11] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m:
A dataset and benchmark for large-scale face recognition. In
European Conference on Computer Vision, pages 87–102,
2016. 1, 7

[12] B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, and
M. Sugiyama. Masking: A new perspective of noisy super-
vision. In NIPS, 2018. 1, 2

[13] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang,
and M. Sugiyama. Co-teaching: Robust training of deep neu-
ral networks with extremely noisy labels. In NIPS, 2018. 1,
2, 5, 7

[14] D. Hendrycks, M. Mazeika, D. Wilson, and K. Gimpel. Us-
ing trusted data to train deep networks on labels corrupted by
severe noise. In NIPS, 2018. 2

[15] E. Hoffer and N. Ailon. Deep metric learning using triplet
network. In A. Feragen, M. Pelillo, and M. Loog, editors,
SIMBAD, volume 9370 of Lecture Notes in Computer Sci-
ence, pages 84–92. Springer, 2015. 2

[16] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. La-
beled faces in the wild: A database forstudying face recog-
nition in unconstrained environments. Technical Report, U-
niversity of Massachusetts, 2007. 3, 7

[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding.
In Proceedings of
the 22Nd ACM International Conference on Multimedia, M-
M ’14, pages 675–678, New York, NY, USA, 2014. ACM.
7

[18] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Men-
tornet: Learning data-driven curriculum for very deep neural
networks on corrupted labels. In International Conference
on Machine Learning, pages 2309–2318, 2018. 1, 2

[19] I. Kemelmachershlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In Computer Vision and Pattern Recog-
nition, pages 4873–4882, 2016. 1, 3, 8

[20] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning
for latent variable models. In Advances in Neural Informa-
tion Processing Systems, pages 1189–1197, 2010. 2

[21] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and L.-J. Li. Learning
In ICCV, pages 1928–

from noisy labels with distillation.
1936, 2017. 1, 2

[22] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. IEEE transactions on pattern
analysis and machine intelligence, 2018. 2

[23] T. Liu and D. Tao. Classiﬁcation with noisy labels by impor-
IEEE Transactions on pattern analysis

tance reweighting.
and machine intelligence, 38(3):447–461, 2016. 2

[24] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion. In CVPR, pages 6738–6746. IEEE Computer Society,
2017. 1, 2, 3, 4, 7

[25] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin soft-
max loss for convolutional neural networks. In International
Conference on International Conference on Machine Learn-
ing, pages 507–516, 2016. 2

[26] E. Malach and S. Shalev-Shwartz. Decoupling ”when to up-
date” from ”how to update”. In Advances in Neural Informa-
tion Processing Systems, pages 960–970, 2017. 1, 2

[27] A. Menon, B. Van Rooyen, C. S. Ong, and B. Williamson.
Learning from corrupted binary labels via class-probability
estimation. In International Conference on Machine Learn-
ing, pages 125–134, 2015. 2

[28] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kot-
sia, and S. Zafeiriou. Agedb: The ﬁrst manually collected,
in-the-wild age database. In IEEE Conference on Comput-
er Vision and Pattern Recognition Workshops, pages 1997–
2005, 2017. 7

[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve
restricted boltzmann machines. In Proceedings of the 27th
international conference on machine learning (ICML-10),
pages 807–814, 2010. 5

[30] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari.
Learning with noisy labels. In Advances in neural informa-
tion processing systems, pages 1196–1204, 2013. 2

11895

[47] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In Euro-
pean Conference on Computer Vision, pages 499–515, 2016.
2

[48] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-
constrained videos with matched background similarity. In
Proceedings of the 2011 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR ’11, pages 529–534,
Washington, DC, USA, 2011. IEEE Computer Society. 8

[49] X. Wu, R. He, Z. Sun, and T. Tan. A light cnn for deep
face representation with noisy labels. IEEE Transactions on
Information Forensics and Security, PP(99):1–1, 2015. 1, 7,
8

[50] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learn-
ing from massive noisy labeled data for image classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2691–2699, 2015. 1, 2

[51] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-

tation from scratch. arXiv:1411.7923, 2014. 2, 3, 7

[52] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection
and alignment using multitask cascaded convolutional net-
works. IEEE Signal Processing Letters, 23(10):1499–1503,
2016. 7

[53] X. Zhang, Z. Fang, Y. Wen, Z. Li, and Y. Qiao. Range loss
for deep face recognition with long-tailed training data. In
IEEE International Conference on Computer Vision, pages
5419–5428, 2017. 2

[54] Y. Zheng, D. K. Pal, and M. Savvides. Ring loss: Con-
vex feature normalization for face recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018. 2

[31] G. Patrini, A. Rozza, A. K. Menon, R. Nock, and L. Qu.
Making deep neural networks robust to label noise: A loss
correction approach. In Proc. IEEE Conf. Comput. Vis. Pat-
tern Recognit.(CVPR), pages 2233–2241, 2017. 1, 2

[32] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hin-
ton. Regularizing neural networks by penalizing conﬁdent
output distributions. arXiv preprint arXiv:1701.06548, 2017.
3

[33] R. Ranjan, C. D. Castillo, and R. Chellappa. L2-constrained
arX-

softmax loss for discriminative face veriﬁcation.
iv:1703.09507, 2017. 6

[34] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to
reweight examples for robust deep learning. In International
Conference on Machine Learning, 2018. 2

[35] D. Rolnick, A. Veit, S. Belongie, and N. Shavit. Deep learn-
ing is robust to massive label noise. arXiv preprint arX-
iv:1705.10694, 2017. 1

[36] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
pages 815–823. IEEE Computer Society, 2015. 2, 5

[37] S. Sengupta, J. C. Chen, C. Castillo, V. M. Patel, R. Chel-
lappa, and D. W. Jacobs. Frontal to proﬁle face veriﬁcation
in the wild. In IEEE Winter Conference on Applications of
Computer Vision, pages 1–9, 2016. 7

[38] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fer-
gus. Training convolutional networks with noisy labels. In
ICLR, 2015. 1, 2

[39] Y. Sun, X. Wang, and X. Tang. Deep learning face repre-
sentation from predicting 10, 000 classes. In CVPR, pages
1891–1898. IEEE Computer Society, 2014. 2

[40] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, pages 1701–1708. IEEE Computer Society,
2014. 2

[41] A. Veit, N. Alldrin, G. Chechik, I. Krasin, A. Gupta, and
S. J. Belongie. Learning from noisy large-scale datasets with
minimal supervision. In CVPR, pages 6575–6583, 2017. 1,
2

[42] F. Wang, L. Chen, C. Li, S. Huang, Y. Chen, C. Qian, and
C. C. Loy. The devil of face recognition is in the noise. In
European Conference on Computer Vision, pages 780–795.
Springer, 2018. 1, 3, 4

[43] F. Wang, J. Cheng, W. Liu, and H. Liu. Additive margin
softmax for face veriﬁcation. IEEE Signal Processing Let-
ters, 25(7):926–930, 2018. 1, 2

[44] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille. Normface:
In ACM

L2 hypersphere embedding for face veriﬁcation.
Conference on Multimedia, 2017. 2

[45] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li,
and W. Liu. Cosface: Large margin cosine loss for deep face
recognition. June 2018. 1, 2, 6, 8

[46] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In B. Leibe,
J. Matas, N. Sebe, and M. Welling, editors, ECCV (7), vol-
ume 9911 of Lecture Notes in Computer Science, pages 499–
515. Springer, 2016. 2

11896

