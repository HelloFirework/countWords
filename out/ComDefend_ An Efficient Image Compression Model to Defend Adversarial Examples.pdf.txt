ComDefend: An Efﬁcient Image Compression Model to Defend Adversarial

Examples

Xiaojun Jia1,2, Xingxing Wei3 ∗, Xiaochun Cao1,2 ∗, Hassan Foroosh4
1Institute of Information Engineering, Chinese Academy of Sciences

2Cyberspace Security Research Center, Peng Cheng Laboratory, Shenzhen 518055, China

3Department of Computer Science and Technology, Tsinghua University,

4Department of Electrical Engineering and Computer Science, University of Central Florida

jiaxiaojun@iie.ac.cn, xwei11@mail.tsinghua.edu.cn, caoxiaochun@iie.ac.cn, foroosh@cs.ucf.edu

Abstract

Deep neural networks (DNNs) have been demonstrat-
ed to be vulnerable to adversarial examples. Speciﬁcal-
ly, adding imperceptible perturbations to clean images can
fool the well trained deep neural networks.
In this pa-
per, we propose an end-to-end image compression model to
defend adversarial examples: ComDefend. The proposed
model consists of a compression convolutional neural net-
work (ComCNN) and a reconstruction convolutional neu-
ral network (RecCNN). The ComCNN is used to maintain
the structure information of the original image and puri-
fy adversarial perturbations. And the RecCNN is used to
reconstruct the original image with high quality. In other
words, ComDefend can transform the adversarial image to
its clean version, which is then fed to the trained classiﬁ-
er. Our method is a pre-processing module, and does not
modify the classiﬁer’s structure during the whole process.
Therefore, it can be combined with other model-speciﬁc de-
fense models to jointly improve the classiﬁer’s robustness.
A series of experiments conducted on MNIST, CIFAR10 and
ImageNet show that the proposed method outperforms the
state-of-the-art defense methods, and is consistently effec-
tive to protect classiﬁers against adversarial attacks.

1. Introduction

As we know, deep learning technique [14] plays the lead-
ing role in Artiﬁcial Intelligence (AI) area, and has ushered
in a new development climax in the ﬁelds such as image
recognition [8], natural language processing [3], and speech
processing [9]. However, Szegedy et al. [19] formally pro-
pose the concept of adversarial examples which bring the
great danger to the neural networks. Speciﬁcally, impercep-

* Corresponding Author

Figure 1. The main idea of our end-to-end image compression
model to defend adversarial examples. The perturbation between
the adversarial image and the original image is very tiny, but the
perturbation is ampliﬁed during the high-level representation of
the image classiﬁcation model. We use ComCNN to remove the
redundant information of the adversarial image and RecCNN to re-
construct the clean image. In this way, the inﬂuence of adversarial
perturbations is suppressed.

tible perturbations added to clean images can induce net-
works to make incorrect predictions with high conﬁdence
during the test time, even when the amount of perturba-
tion is very small, and imperceptible to human observers.
What’s more, [12] has proved that adversarial examples al-
so exist in the physical-world scenarios. The existence of
adversarial examples has become a major security concern
in real-world applications of deep networks, such as self-
driving cars and identity recognition, etc.

In recent years, a lot of methods defending the adversar-
ial examples have been proposed. These methods can be
roughly categorized into two classes. The ﬁrst class is to
enhance the robustness of neural networks itself. Adver-
sarial training [20] is a typical method among them, which
injects adversarial examples into the training data to retrain
the network. Label smoothing [22], which converts one-hot
labels to soft targets, also belongs to this class. The second

43216084

Adversarial imageImage classification modelwell-trianedLabel:dog(wrong)ComCNN12bits mapRecCNNclean imageImage classification modelwell-trianedLabel:Panda(Correct)Figure 2. The overview of ComDefend. The ComCNN is used to preserve the main structure information of original images. The original
24 bits map for RGB three channels is compressed into 12 bits map (each channel is assigned 4 bits). And the RecCNN is responsible for
reconstructing the clean-version original images. The gaussian noise is added on the compressed compact representation to improve the
reconstructed quality, and further enhance the defense ability.

one denotes the various pre-processing methods. For exam-
ple, In [18], Song et al. propose the PixelDefend, which
can transform the adversarial images into clean images be-
fore they are fed into the classiﬁer. Similarly, [15] regards
the imperceptible perturbations as the noises, and designs a
high-level representation guided denoiser (HGD) to remove
these noises. HGD wins the ﬁrst place in the NIPS2017
adversarial vision challenge [13]. Generally speaking, the
latter methods are more efﬁcient because they don’t need to
retrain the neural networks. However, HGD still requires a
lot of adversarial images when training the denoiser . There-
fore, it is hard to get a good HGD in the case of few adver-
sarial images. The main idea of PixelDefend is to simulate
the distribution of image space. When the space is too large,
the result of the simulation will be bad.

Image compression is a low-level image transformation
task. Because there is strong similarity and relevance be-
tween neighbor pixels in the local structure, image com-
pression can help reduce the redundant information of an
image, while retaining the dominant information. Based on
this observation, we devised ComDefend, which utilizes the
image compression to remove adversarial perturbations or
destroy the structure of adversarial perturbations. The basic
idea of ComDefend is listed in Figure 1.

ComDefend consists of two CNN modules. The ﬁrst C-
NN, called compression CNN (ComCNN), is used to trans-
form the input image into a compact representation. In de-

tails, the original 24-bits pixel is compressed into 12 bits.
The compact representation extracted from the input image
is expected to retain the enough information of the original
image. The second CNN, called reconstruction CNN (Rec-
CNN), is used to reconstruct the original image with high
quality. The ComCNN and RecCNN are ﬁnally combined
into a uniﬁed end-to-end framework to learn their weight-
s. Figure 2 gives the illustration of ComDefend. Noted
that ComDefend is trained on the clean images.
In this
way, the network will learn the distribution of clean im-
ages, and thus can reconstruct a clean-version image from
the adversarial image. Compared with HGD and PixelDe-
fend, ComDefend doesn’t require the adversarial examples
in training phase, and thus reduces the computation cost.
In addition, ComDefend is performed on an image with the
patch-by-patch manner instead of the whole image, which
improve the processing efﬁciency. The code is released at
https://github.com/jiaxiaojunQAQ/Comdefend.git.

In summary, this paper has the following contributions:
1) We propose the ComDefend, an end-to-end image
compression model to defend adversarial examples. The
ComCNN extracts the structure information of the original
image and removes the imperceptible perturbations. The
RecCNN reconstructs the input image with high quality.
During the whole process, the deployed model is not modi-
ﬁed.

2) We design a uniﬁed learning algorithm to simulta-

43226085

ComCNN............NN............NNSigmoid layer............NNRecCNN............NNInput ConvConv+EluDown-Conv+EluUp-Conv+EluOutput Conv3bits map3bits map8bits map8bits mapGaussian noiseOriginal imageReconstructed imageneously learn the weights of two CNN modules within
ComDefend. In addition, we ﬁnd that adding gaussian noise
to the compact representation can help reconstruct better
images, and further improve the defending performance.

3) Our method greatly improves the resilience across
a wide variety of strong attacking methods, and defeats
the state-of-the-art defense models including the winner of
NIPS 2017 adversarial challenge.

The remainder of this paper is organized as follows. Sec-
tion 2 brieﬂy reviews the related work. Section 3 introduces
the details of the proposed ComDefend. Section 4 shows a
series of experimental results. Finally, Section 5 shows the
conclusion.

2. Related work

We investigate the related work from two aspects: Attack
methods to generate adversarial examples, and Defensive
methods to resist adversarial examples.

2.1. Attack methods

In [6], Goodfellow et al. propose the Fast Gradient Sign
Method (FGSM). An adversarial example is produced by
adding increments in the gradient direction of the loss gra-
dient. After that, Basic Iterative Method (BIM) which is
the improved version of the FGSM, is proposed in [12].
Compared with FGSM, BIM performs multiple steps. This
method is also called Projected Gradient Descent (PGD) in
[16]. To deal with the selection of parameters in FGSM,
in [17], Moosavi-Dezfooli et al. propose to use an itera-
tive linearization of the classiﬁer and geometric formulas to
generate an adversarial example. In [1], Carlini-Wagner et
al. design an efﬁcient optimization objective (C&W) to ﬁnd
the smallest perturbations. The C&W can reliably produce
samples correctly classiﬁed by human subjects but misclas-
siﬁed in speciﬁc targets by the well-trained classiﬁer.

2.2. Defensive methods

In [20], Adversarial training adds the adversarial images
generated by different attack methods to the training image
dataset. The growth of the training image dataset makes the
image classiﬁcation model easier to simulate the distribu-
tion of the entire image space. And in [21], Warde-Farley
and Goodfellow propose label smoothing method which us-
es soft targets to replace one-hot labels. The image classiﬁ-
er is trained on the one-hot labels at ﬁrst, and then the soft
targets are generated by the well-trained image classiﬁer.
In [25], Xu et al. propose to use feature squeezing methods
which include the color bit depth of each pixel and spatial s-
moothing to achieve defend adversarial examples. PiexlDe-
fend is proposed in [18]. The basic idea of PiexlDefend is to
purify input images before they are fed to the image classi-
ﬁer. In [15], the authors propose a high-level representation

guided denoiser(HGD) method to defend adversarial exam-
ples. The proposed model is trained on the training dataset
which includes 210k clean and adversarial images.

3. End-to-end image compression model

3.1. The basic idea of ComDefend

Let us ﬁrst look back at the reason of adversarial ex-
amples. The adversarial examples are generated by adding
some imperceptible perturbation to the clean images. The
added perturbation is too slight to be perceptible to human-
s. However, when the adversarial examples are fed to a
deep learning network, the effect of the imperceptible per-
turbation increases rapidly along with the deepth of the net-
work. Therefore, the carefully designed perturbation will
fool powerful CNNs. More speciﬁcally, from previously
related researches, we can regard the imperceptible pertur-
bation as the noise with the particular structure. Kurakin et
al. in [12] consider that this kind of noise which can fool
powerful CNNs exists in the real world. In other words, the
perturbations do not affect the structure information of the
original image. The imperceptible perturbations can be con-
sidered as the redundant information of the images. From
this point of view, we can use the characteristics of image
redundancy information in image compression model to de-
fend adversarial examples.

In order to remove the imperceptible perturbations or
break up the particular structure of the imperceptible pertur-
bations, we propose an end-to-end image compression mod-
el which not only compresses the input image but also trans-
forms the input image to a clean image. As shown in Figure
2, the image compression model contains the compression
and reconstruction processes. During the compression pro-
cess, the ComCNN extracts the image structure information
and removes the redundant information of the image. Dur-
ing the reconstruction process, the RecCNN reconstructs
the input image without the adversarial perturbations.
In
particular, the ComCNN compress the 24-bits pixel image
into 12 bits. That is to say, the 12-bits pixel image removes
the redundancy information of the original image and pre-
serves the main information of the original image. And thus
the RecCNN use the 12-bits pixel image to reconstruct the
original image. During the whole process, we hope that the
12-bits pixel images extracted from the original image and
adversarial example are as same as possible. Therefore, we
can transform the adversarial example into the clean image.

3.2. Structure of the ComCNN

ComCNN consists of 9 weight layers, which can com-
press the input image into the 12-bits pixel image. That is
to say, the main structure information of the input image
is reserved and the redundancy information including the
imperceptible perturbation of the input image is removed.

43236086

Figure 3. The comparison results in ComDefend whether to add gaussian noises. In each subﬁgure, The top images are original images,
The middle images are the compressed 12bits maps and the bottom images are the reconstructed images. (a) ComDefend reconstructs the
image through the un-binarized 12bits map. (b) Without gaussian noises, ComDefend reconstructs the image through the binarized 12bits
map. (c) With gaussian noises, ComDefend reconstructs the image through the binarized 12bits map. We see the reconstructed quality in
(c) is the same with that in (a). That means the increment information of un-binarized maps are actually noises. Therefore, when gaussian
noises are added on the binarized maps, the better images are reconstructed.

Table 1. Hyperparameters of the ComCNN Layers
input channels

output channels

type

layer

1st layer
2nd layer
3rd layer
4th layer
5th layer
6th layer
7th layer
8th layer
9th layer

conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU

Conv

16
32
64
128
356
128
64
32
12

3
16
32
64
128
256
128
64
32

Table 2. Hyperparameters of the RecCNN Layers
input channels

output channels

type

layer

1st layer
2nd layer
3rd layer
4th layer
5th layer
6th layer
7th layer
8th layer
9th layer

Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU
Conv+ELU

Conv

32
64
128
256
128
64
32
16
3

12
32
64
128
256
128
64
32
16

ﬁlter size

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

ﬁlter size

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

3 × 3

The combination of convolution and ELU [2] are used in
ComCNN. As shown in Table 1, ComCNN consists of two
components, the ﬁrst one is used to extract the features of
the original image and generate 256 feature maps. The 1st

to the 4th layers which consist of 32 ﬁlters of size 3 × 3 × 3,
64 ﬁlters of size 3 × 3 × 32, 128 ﬁlters of size 3 × 3 × 64
and 256 ﬁlters of size 3 × 3 × 128 are the main part of the
ﬁrst component. And the ELU nonlinearity is used as an
activation function. The second one is used to downscale
and enhance the features of the input image. The 5th to the
9th layers which consist of 128 ﬁlters of size 3 × 3 × 256,
64 ﬁlters of size 3 × 3 × 128, 64 ﬁlters of size 3 × 3 × 64,
32 ﬁlters of size 3 × 3 × 64 and 3 ﬁlters of size 3 × 3 × 32
are the main part of the second component. The ComCN-
N is used to extract the features of the original image and
construct the compact representation.

3.3. Structure of the RecCNN

RecCNN consists of 9 weight layers, which is used to re-
construct the original image without the imperceptible per-
turbation. As shown in Table 2, For the 1st layer to the
9th layers, 32 ﬁlters of size 3 × 3 × 12, 64 ﬁlters of size
3 × 3 × 32, 128 ﬁlters of size 3 × 3 × 64, 256 ﬁlters of size
3 × 3 × 128, 128 ﬁlters of size 3 × 3 × 256, 64 ﬁlters of size
3 × 3 × 128, 64 ﬁlters of size 3 × 3 × 64, 32 ﬁlters of size
3 × 3 × 64 and 3 ﬁlters of size 3 × 3 × 32 are used, and ELU
is added. The RecCNN makes use of the compact repre-
sentation to reconstruct the output image. The output image
has the fewer perturbations than the input image. That is to

43246087

(1)10000steps(2)30000steps(3)50000steps(1)10000steps(2)30000steps(3)50000steps(1)10000steps(2)30000steps(3)50000steps(a)(b)(c)say, the output image can break up the particular structure
of the perturbations. In this way, the compression model
can defend the adversarial examples.

Table 3. The experiments versus selection of compression bits

Compressed bits

8

10

12

14

16

PSNR

31.01

31.01

31.78

28.77

30.95

3.4. Loss functions

As for ComCNN, the goal of the ComCNN is to use
more 0 to encode the image information. Therefore, the
loss function of the ComCNN can be deﬁned as:

L1(θ1) = λkCom(θ1, x)k2,

(1)

where θ1 is the trainable parameter of the ComCNN,
Com() represents the ComCNN, and λ is a super param-
eter which we use a large number experiments to certify.
Please refer to Section 4 for more details.

As for RecCNN, the goal of the RecCNN is expected to
reconstruct the original image with high quality. Therefore,
we use MSE to deﬁne the loss function of the RecCNN:

L2(θ2) =

1
2N

ΣkRec(θ2, Com(θ1, x) + ϕ) − xk2,

(2)

where Com() is the ComCNN, θ2 represents the trainable
parameter of the RecCNN, Rec() represents the RecCNN,
θ1 represents the trained parameter of the ComCNN. And ϕ
represents the random Gaussian noise.

In order to make the compression model more effective,
we design a uniﬁed loss function to simultaneously update
the parameters of ComCNN and RecCNN. It is deﬁned as:

L(θ1, θ2) =

1
2N

ΣkRec(θ2, Com(θ1, x) + ϕ) − xk2

(3)

+λkCom(θ1, x)k2.

According to this loss function, it is clear that both Com-
CNN and RecCNN work together to resist the noise attack.
The parameters θ1, θ2 are upgraded at the same time during
the model training.

the image y to between 0 and 1. Note that, the sigmoid out-
put makes use of the different shades of gray information to
represent the input image instead of 0 and 1. And RecCNN
can reconstruct the original image through these shades of
gray information. If these shades of gray information are
binarized, the main structure information of original image
is completely lost. In order to deal with this problem, we
propose to use the noise attack.

In particular, we add the random Gaussian noise ϕ (the
mean of the gaussian noise is 0 and the variance of the gaus-
sian noise is ϕ) to the output before the sigmoid function.
The information encoded with 0 and 1 is easier to resist the
noise attack. Therefore, during the training, the compres-
sion model learns to use the binary information to defend
the noise attack. As shown in Figure 3, we can see that
adding the random gaussian noise contributes to improving
the performance of the compression model. In addition, We
choose the compression bits mainly according to the recon-
structed performance. We try different compression bits in
Table 3, and ﬁnd the 12 bits show the best PSNR recon-
structed performance.

3.6. Network implementation

The weights of the ComCNN and the RecCNN are ini-
tialized by using the method in [7]. We also use Adam
algorithm [10] with parameters setting α = 0.001, β1 =
0.9, β2 = 0.999 and ε = 10−8 to upgrade the weights of
the compression model. After the hyperparameters γ and λ
being conﬁrmed, we train ComCNN and RecCNN for 30 e-
pochs using a batch size of 50. The learning rate is decayed
exponentially from 0.01 to 0.0001 for 30 epochs.

3.5. Learning algorithm

4. Experimental results and analysis

In order to train the compression model, we design a u-
niﬁed learning algorithm for both ComCNN and RecCNN.
The optimization goal for ComDefend is formulated as:

(θ1, θ2) = arg min(

1
2N

ΣkRec(θ2, Rec(θ1, x) + ϕ) − xk2

+kCom(θ1, x)k2),
(4)

where x is the input image. ϕ represents the random Gaus-
sian noise. θ1 and θ2 are the parameters of ComCNN and
RecCNN respectively. Com() represents the ComCNN and
Rec() represents RecCNN.

During the whole process, the ComCNN encodes the in-
put image x into a same size image y with each pixel oc-
cupies 12 ﬂoats. Then the sigmoid function is used to limit

In this section, in order to evaluate the performance
of the proposed method, we conduct several experiments,
which include: generation of adversarial examples, selec-
tion of hyper parameters in neural networks, image classi-
ﬁcation with the proposed method, comparisons with other
defensive methods and performance analysis. The proposed
method can signiﬁcantly perform well against the state-of-
the-art adversarial attacks.

4.1. Datasets for training and testing

In order to clearly verify our proposed method, the Com-
CNN and RecCNN training are based on the 50,000 clean
(not perturbed) images of the CIFAR-10 dataset [11]. For
testing, we use 10,000 testing images in the CIFAR-10
dataset, 10,000 testing images in the Fashion-mnist [23] and

43256088

ϕ = 25.0

ϕ = 20.0

ϕ = 10.0

Table 4. THE SELECTION OF PARAMETERS IN THE OUR PROPOSED METHOD .
ϕ = 1.0
68.39%
88.99%
89.61%
89.06%
90.00%
85.19%

86.69%
72.23%
91.82%
90.99%
91.45%
86.63%

90.22%
89.64%
90.77%
91.65%
90.39%
90.53%

87.52%
90.23%
90.98%
91.37%
91.27%
90.27%

86.12%
91.09%
89.45%
90.74%
91.01%
89.88%

85.42%
90.41%
91.24%
91.05%
90.88%
89.80%

86.22%
90.55%
90.61%
90.25%
88.18%
89.16%

86.71%
90.56%
90.33%
90.65%
90.10%
89.67%

ϕ = 30.0

ϕ = 35.0

ϕ = 40.0

ϕ = 50.0

Average
84.66%
87.96%
90.60%
90.72%
90.41%
88.89%

λ = 0.01
λ = 0.001
λ = 0.0001
λ = 0.00001

λ = 0.0
Average

1000 random images of the imagenet dataset [4]. We also
train ResNet [8] which is one of the state-of-the-art deep
neural network image classiﬁers in recent years on these
three datasets.

4.2. Adversarial examples

In the literature, three common distance metrics are used
for generating adversarial examples: L0, L2, L∞. L0 rep-
resents the number of the different pixels between the clean
image and adversarial example. L2 measures the standard
Euclidean distance between the clean image and adversari-
al example. L∞ represents the maximum value of the im-
perceptible perturbation in the adversarial example. In [22],
Goodfellow et al. argue to use L∞ to construct the adversar-
ial examples. And the related research literature main use
L2 and L∞ to conduct related researches. Therefore, we
make use of L2 and L∞ to achieve the adversarial attack-
s. In particular, we use the L∞ distance metric to achieve
FSGM, BIM and DeepFool adversarial attacks and the L2
distance metric to achieve C&W adversarial attacks.

4.3. Selection of hyper parameters

There are two hyper parameters in the neural network-
s that need to be determined by a large number of exper-
iments. The ﬁrst one is the standard normal distribution
gaussian noise parameter ϕ, and the second one is the penal-
ty item parameter λ. In order to improve the performance of
the proposed method, the value of ϕ and λ is depending on
the performance of image classiﬁcation. Speciﬁcally, image
compression discards part of the image information even if
it retains the main structural information of the image. In
order to keep the accuracy of image classiﬁer, we compute
the average accuracy of the well-trained Resnet50 on the
1000 random images of the cifar-10 training dataset. For
more details, please refer to Table 4.

From Table 4, we can see that when the parameter λ is
ﬁxed, the accuracy of the classiﬁer ﬁrst increases and then
decreases with the increase of parameter ϕ. More speciﬁ-
cally, when parameter λ = 0.0001 and ϕ = 1.0 ∼ 20.0, the
average accuracy increases constantly. But when parameter
λ = 0.0001 and ϕ = 20.0 ∼ 50.0, the average accuracy
decreases constantly. That is, when the noise is too large,
the network is not enough to resist it, resulting in a decline
in network performance and when the noise is too small, the
network learns to use the gray scale information between 0

Figure 4. The classiﬁcation accuracy of ResNet-50 on adversari-
al images produced by four attacks using the proposed method at
the test time and at training and test time. The dotted line repre-
sents the accuracy of the ResNet-50 model on adversarial images
without any defense.

and 1 to encode the image instead of using 0 and 1. Similar-
ly, when the parameter ϕ is ﬁxed, the accuracy of the clas-
siﬁer ﬁrst increases and then decreases with the decrease of
parameter λ. Therefore, the appropriate parameter settings
can protect the accuracy of image classiﬁcation models. In
accordance with Table 4, λ = 0.0001 and ϕ = 20.0 can
obtain the best performance of the classiﬁer. In addition, In
this paper, the value of the parameter λ is 0.0001 and the
value of the parameter ϕ is 20.0.

4.4. Image classiﬁcation with the proposed method

Simply detecting adversarial images is not sufﬁcient for
the task of the image classiﬁcation.
It is often critical to
be able to correctly classify adversarial examples. In this
section, there are two scenarios where our proposed method
is used to defend the adversarial attacks. One is using the
image compression at test time, the other is using the image
compression at training and test time.

4.4.1

Image compression at test time

The image classiﬁcation model has been trained on the
clean images. The test images consist of clean images and

43266089

Table 5. THE RESULT OF COMPARISONS WITH OTHER DEFENSIVE METHODS(CIFAR-10 ,L∞ = 2/8/16)

Network

Defensive method

Clean

FGSM

BIM

DeepFool

C&W

In training time

Resnet50

Normal

Adversarial FGSM

Adversarial BIM
Label Smoothing
Proposed method
Feature Squeezing

In test time

PiexlDefend

Proposed method

39%/20%/18%
88%/91%/91%
80%/52%/34%
73%/54%/28%

17%/00%/00%
92%/92%/92%
20%/00%/07%
91%/91%/91%
76%/42%/08%
87%/87%/87%
92%/92%/92%
30%/02%/02%
92%/92%/92% 89%/89%/87% 84%/47%/40% 90%/90%/90% 91%/90%/90%
78%/78%/78%
84%/84%/84%
85%/85%/88%
78%/78%/78%
91%/91%/91% 86%/84%/83% 78%/41%/34% 88%/88%/88% 89%/87%/87%

08%/00%/00%
24%/07%/00%
74%/32%/06%
59%/08%/01%

21%/01%/01%
45%/00%/00%
79%/48%/25%
56%/20%/10%

13%/00%/00%
71%/46%/25%

31%/20%/18%
73%/46%/24%

75%/75%/75%
80%/80%/80%

Table 6. THE RESULT OF COMPARISONS WITH OTHER DEFENSIVE METHODS(Fashion-mnist ,L∞ = 8/25)

Defensive

Method
Normal

Network

Resnet50

Clean

FGSM

BIM

DeepFool

C&W

93%/93%
Adversarial FGSM 93%/93%
92%/91%
93%/83%
84%/84%
89%/89%
93%/93% 89%/89% 70%/60%

38%/24%
85%/85%
84%/79%
73%/45%
70%/28%
87%/82%

Adversarial BIM
Label Smoothing
Feature Squeezing

06%/06%
00%/00%
63%/07%
51%/00%
82%/72%
76%/63%
29%/06%
16%/00%
56%/25%
83%/83%
85%/83% 88%/88%

00%/00%
67%/21%
81%/70%
33%/14%
83%/83%
88%/88%
90%/89% 88%/89%

Proposed method

PiexlDefend

Table 9. Comparison results with ICLR2018 on ImageNet
Network

Defensive Method

FGSM-12

Deepfool

FGSM-8

C&W

IncResV2

Normal

ICLR2018
Our method

34%
62%
62%

32%
50%
61%

13%
55%
60%

0%
59%
61%

adversarial images. They are ﬁrst compressed and recon-
structed by the proposed method, and then they are fed to
the well-trained classiﬁer. Fig 4 shows the accuracy of im-
age classiﬁcation model(Resnet50) which are tested on the
adversarial examples produced by the four attacks. The dot-
ted lines show the accuracy of image classiﬁcation models
tested on the adversarial images with no defense. In this re-
spect, the proposed method using at the test time increases
accuracy on the FGSM strongest attack from 35% to 83%,
the BIM strongest attack from 0% to 31%, the DeepFool
strongest attack from 1% to 89% and the C&W strongest
attack from 0% to 87% for CIFAR-10.

4.4.2

Image compression at training and test time

There is another way to defend the adversarial examples,
that is to say, we apply the proposed method during the
training and test time.
In particular, During the training
time, we train the image classiﬁcation models on trans-
formed cifar-10 training images. We ﬁrst use the ComCNN
to compress the input image into the compact representa-
tion, and then use the RecCNN to reconstruct the input im-
age before feeding it to the network. As for the test time,
the test image is transformed by the proposed method be-
fore being fed to the well-trained classiﬁer. As shown in Fig
4, we can see that the proposed method at training and test
time increases accuracy on the FGSM strongest attack from
35% to 83%, the BIM strongest attack from 0% to 31%, the
DeepFool strongest attack from 1% to 89% and the C&W
strongest attack from 0% to 87% for CIFAR-10.

4.4.3 Comparisons with other defensive methods

In order to quantitatively measure the performance of our
proposed method, we compare the proposed method with
other conventional schemes under the L∞ distance metric.
The result of the comparison on the Cifar-10 image dataset
is shown in Table 5. During training and test time, com-
pared with these methods, our proposed method achieves
huge performance improvement. In particular, it achieves
nearly 90% accuracy on the FGSM, DeepFool and C&W
attack methods. Compared with the image classiﬁcation
model on the clean images, the accuracy of the model on the
adversarial examples does not decline a lot. As for defense
applied in test time, the proposed method can achieve about
85% accuracy on the FGSM, DeepFool and C&W attack
methods. As for BIM attack, the performance is improved
by using our proposed method. And Table 6 shows the re-
sult of the comparison with other defensive methods on the
Fashion-mnist image dataset. The performance is improved
a lot by using the proposed method on FGSM, DeepFool
and C&W attack methods. More importantly, we do com-
parison experiment with HGD and ICLR2018[24] method
on the imagenet dataset. As shown in Table 7, the pro-
posed method improves the performance of defending the
FGSM, DeepFool and C&W attack methods. And to test
more attacking methods, we add the deepfool and C&W
methods. The results are shown in Table 8. We see that
the proposed method achieves the higher defensive accura-
cy against FGSM, DeepFool, MI-FGSM [5] and C&W, and
the competitive accuracy against IFGSM compared with
HGD, which demonstrates the effectiveness of the proposed
method. Note that the ǫ for IFGSM is set to 3 and 5, rather 8
and 16, because we ﬁnd when the attack is too strong (when
L∞ >= 8), the noises are perceptible to human eyes. And
thus, the adversarial examples can be easily distinguished

43276090

Table 7. THE RESULT OF COMPARISONS WITH OTHER DEFENSIVE METHODS(Imagenet ,L∞ = 8/13/20)

Network

Resnet101

Defensive
Method
Normal
HGD

Proposed method

Clean

FGSM

BIM

DeepFool

C&W

76%/76%/76%
54%/54%/54%
67%/67%/67% 56%/56%/56% 12%/12%/10%

03%/03%/03%
51%/50%/50%

00%/00%/00%
01%/01%/01%
36%/36%/36% 52%/52%/52%

00%/00%/00%
51%/51%/51%
53%/53%/52% 54%/54%/53%

Table 8. Comparison results with HGD on ImageNet (L∞ = 8/16)

Network

IncResV2

IncV3

IncV4

Defense
Normal

HGD

Our method

Normal

HGD

Our method

Normal

HGD

Our method

FGSM

C&W
0%/0%

IFGSM(3/5) MI-FGSM Deepfool
13%/11%

Clean
86% 34%/30% 10%/5%
54% 47%/48% 42%/42%
77% 62%/61% 51%/42%
83% 20%/18% 57%/49%
70% 60%/60% 62%/61%
74% 62%/61% 64%/60%
88% 28%/26% 6%/1%
64% 56%/56% 51%/50% 57%/52% 59%/59% 59%/59%
74% 58%/56% 50%/46%
50%/40% 60%/60% 61%/60%

13%/7%
46%/44% 48%/48% 48%/48%
50%/40% 60%/60% 61%/63%
57%/50% 12%/11%
62%/62% 60%/60% 59%/59%
69%/64% 60%/60% 60%/60%

17%/15%

0%/0%

4%/1%

0%/0%

by human beings, and the defense methods are not neces-
sary. In fact, the core advantage of our method is that
we train our network on the clean images rather than
adversarial images. In this way, we don’t need to use at-
tacking methods to generate adversarial examples, and
thus the training data set is much smaller than HGD,
and the training time is also much less than HGD. Be-
sides, our method performes the compression based on
the patch rather than the entire image, therefore, the
testing time is reduced (the HGD takes 2.7 seconds to
process an image. But the proposed method only takes
1.2 seconds to process the same image). We give the com-
parison results with [24] in Table 9. The results shows that
our method can achieve the higher accuracy against deep-
fool, C&W, and FGSM with ǫ = 8, 12 than [24], which
veriﬁes the effectiveness of our method. Furthermore, our
proposed method does not depend on attacking methods and
classiﬁers. And it can be combined with other defensive
methods.

4.5. Analysis for the proposed method

For the test time, the proposed method transforms the in-
put image to a clean image. And it breaks up the particular
structure of the perturbations in the adversarial examples.
Speciﬁcally, the ComCNN encode the input image into a
compact representation. During this process, the impercep-
tible perturbations do not affect the result of the compact
representation.
In other words, the output images of the
clean image and adversarial image are as same as possi-
ble. Because during the training of the network, the network
learns to resist the stronger gaussian noise attack to encode
the input images. For the training and test time, the pro-
posed method compresses the space of the real samples. For
the uncompressed space, there are 32×32×28 ×28 ×28 im-
ages in this space. But for the compressed space, there only
are 32 × 32 × 24 × 24 × 24 images in this space. In this way,

the proposed method makes the existing image classiﬁca-
tion models easier to simulate the image distribution. The
mezzanine is between the decision surface trained by the
classiﬁer and the real surface of the sample data becomes
smaller. That is to say, the probability of the adversarial
example occurrence becomes smaller than before.

5. Conclusion

In this paper, we propose an end-to-end image compres-
sion model to defend adversarial examples. ComDefend
can be used in test time and in training and test time. As for
test time, it defends the adversarial examples by destroying
the structure of adversarial perturbations in the adversarial
image. As for training and test time, it achieves defense by
compressing the image space. In this way, it reduces the
search space available for an adversary to construct adver-
sarial examples. ComDefend can achieve higher accuracy
on FGSM, DeepFool and C&W attack methods compared
with the state-of-the-art defense methods. And the perfor-
mance on BIM attack also improves by using our proposed
method. More importantly, ComDefend is performed on an
image with the patch-by-patch manner instead of the whole
image, which is taken less time to deal with the input image.
Our work demonstrates that the performance of classifying
the adversarial examples is dramatically improved by using
the proposed method.

6. Acknowledgements

Supported by the National Key R&D Program of Chi-
na (Grant No.2018YFB0803701). National Natural Sci-
ence Foundation of China (No.U1636214, 61861166002,
U1803264, 61806109). Beijing Natural Science Founda-
tion (No.L182057). Project funded by China Postdoctoral
Science Foundation (No.2018M641360).

43286091

Vision and Pattern Recognition, pages 2574–2582, 2016.

[18] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman.
Pixeldefend: Leveraging generative models to understand
and defend against adversarial examples. arXiv preprint arX-
iv:1710.10766, 2017.

[19] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199, 2013.

[20] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Bone-
h, and P. McDaniel. Ensemble adversarial training: Attacks
and defenses. arXiv preprint arXiv:1705.07204, 2017.

[21] D. Warde-Farley and I. Goodfellow. 11 adversarial perturba-
tions of deep neural networks. Perturbations, Optimization,
and Statistics, page 311, 2016.

[22] D. Warde-Farley, I. Goodfellow, T. Hazan, G. Papandreou,
and D. Tarlow. Adversarial perturbations of deep neural net-
works. perturbations. Optimization, and Statistics, 2, 2016.

[23] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a nov-
el image dataset for benchmarking machine learning algo-
rithms. arXiv preprint arXiv:1708.07747, 2017.

[24] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating
adversarial effects through randomization. In International
Conference on Learning Representations, 2018.

[25] W. Xu, D. Evans, and Y. Qi. Feature squeezing: Detecting
adversarial examples in deep neural networks. arXiv preprint
arXiv:1704.01155, 2017.

References

[1] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In 2017 IEEE Symposium on Security
and Privacy (SP), pages 39–57. IEEE, 2017.

[2] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(elus). arXiv preprint arXiv:1511.07289, 2015.

[3] R. Collobert and J. Weston. A uniﬁed architecture for natural
language processing: Deep neural networks with multitask
learning. In Proceedings of the 25th international conference
on Machine learning, pages 160–167. ACM, 2008.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. Ieee, 2009.

[5] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li.
Boosting adversarial attacks with momentum. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 9185–9193, 2018.

[6] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples (2014). arXiv preprint arX-
iv:1412.6572.

[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proceedings of the IEEE international con-
ference on computer vision, pages 1026–1034, 2015.

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[9] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath,
et al. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82–97, 2012.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[11] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian
institute for advanced research). URL http://www. cs. toron-
to. edu/kriz/cifar. html, 2010.

[12] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-
chine learning at scale. arXiv preprint arXiv:1611.01236,
2016.

[13] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao,
M. Liang, T. Pang, J. Zhu, X. Hu, C. Xie, et al. Adver-
sarial attacks and defences competition. arXiv preprint arX-
iv:1804.00097, 2018.

[14] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,

521(7553):436, 2015.

[15] F. Liao, M. Liang, Y. Dong, T. Pang, J. Zhu, and X. Hu.
Defense against adversarial attacks using high-level repre-
sentation guided denoiser. arXiv preprint arXiv:1712.02976,
2017.

[16] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to adver-
sarial attacks. arXiv preprint arXiv:1706.06083, 2017.

[17] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: a simple and accurate method to fool deep neural net-
works. In Proceedings of the IEEE Conference on Computer

43296092

