Learning to Calibrate Straight Lines for Fisheye Image Rectiﬁcation

Zhucun Xue, Nan Xue, Gui-Song Xia∗, Weiming Shen

CAPTAIN-LIESMARS, Wuhan University, Wuhan, 430079, China

{zhucun.xue, xuenan, guisong.xia, shenwm}@whu.edu.cn

Abstract

This paper presents a new deep-learning based method
to simultaneously calibrate the intrinsic parameters of ﬁsh-
eye lens and rectify the distorted images. Assuming that
the distorted lines generated by ﬁsheye projection should be
straight after rectiﬁcation, we propose a novel deep neu-
ral network to impose explicit geometry constraints onto
processes of the ﬁsheye lens calibration and the distorted
image rectiﬁcation. In addition, considering the nonlinear-
ity of distortion distribution in ﬁsheye images, the proposed
network fully exploits multi-scale perception to equalize
the rectiﬁcation effects on the whole image. To train and
evaluate the proposed model, we also create a new large-
scale dataset labeled with corresponding distortion param-
eters and well-annotated distorted lines. Compared with the
state-of-the-art methods, our model achieves the best pub-
lished rectiﬁcation quality and the most accurate estimation
of distortion parameters on a large set of synthetic and real
ﬁsheye images.

1. Introduction

Fisheye cameras have been widely used in many com-
puter vision tasks [31, 34, 9, 16, 2] because of their large
ﬁeld of view (FOV), however, the images taken by ﬁsheye
cameras always suffer from severe geometric distortion si-
multaneously. When processing the geometric toward vi-
sion systems equipped with ﬁsheye lens, calibrating the in-
trinsic parameters is usually the ﬁrst step we should do to
rectify the distorted images.

1.1. Motivation and Objective

Early work viewed the calibration of ﬁsheye cameras
as an optimization problem by ﬁtting the relationship be-
tween 2D/3D calibration patterns from images with differ-
ent viewpoints [19, 14, 30, 28]. But these methods usually
demand pre-prepared calibration patterns and extra manual
operations, and even often involve heavy off-line estima-
tions, which seriously limit their usage scenarios in real ap-
plications. To overcome these limitations and toward an au-

∗Corresponding author: guisong.xia@whu.edu.cn

Figure 1. Learning to calibrate straight lines for ﬁsheye image rec-
tiﬁcation. 1st-Row: original ﬁsheye images overlaid by detected
distorted lines which should be straight after calibration; 2nd-
Row: rectiﬁed straight lines; 3rd-Row: rectiﬁed image.

tomatic self-calibration solution, subsequent investigations
were proposed to detect geometric objects (e.g., conics and
lines) from a single image and further exploit their corre-
spondences in 3D world [13, 36, 4, 22, 12, 1]. These ap-
proaches have reported promising performances on ﬁsheye
camera calibrations only when speciﬁed geometric objects
in ﬁsheye images can be accurately detected. Nevertheless,
it is worth noticing that the involved detection of geometric
objects in ﬁsheye images itself is another challenging prob-
lem in computer vision. Recently, alternative approaches
have been proposed based on deep convolutional neural net-
works (CNNs) [27, 35]. Avoiding the difﬁcult to directly
detect the geometric objects, these methods tried to learn
more discriminative visual features with CNNs to rectify the
distorted image. Although aforementioned methods have
reported the state-of-the-art performances on ﬁsheye image
rectiﬁcations, as well as avert the difﬁculties of detecting
geometric objects, the geometry characteristics in ﬁsheye
calibration task are not fully exploited using CNNs.

Regardless of the difﬁculties to detect geometric objects
in ﬁsheye images, one should observe that the explicit scene
geometries are still the strong constrains to rectify distorted
images. It is of great interest to investigate how to apply the
fundamental geometric property under the pinhole camera

11643

model, i.e., the projection of straight line from space to the
camera should be a line [13], to ﬁsheye image calibration
networks. As shown in Fig. 1, we propose a a novel net-
works which further exploit this explicit scene geometries,
aiming at solving the challenges of ﬁsheye camera calibra-
tion and image rectiﬁcation by a deep neural network simul-
taneously.

1.2. Overview of Our Method

The above-mentioned challenges motivate following two
issues: (1) how to design a powful CNNs to explicitly de-
pict the scene geometry in ﬁsheye images. (2) how to train
a deep network effectively and efﬁciently using geometric
information. To address these problems, we propose to ﬁrst
train a neural network to detect distorted lines that should
be straight after being rectiﬁed in ﬁsheye images , and then
feed these detected distorted lines into another deep sub-
network to recover the distortion parameters of ﬁsheye cam-
era. As shown in Fig. 2, our network includes three main
modules as follows:

• Module for detecting distorted straight lines. This
module is designed to extract distorted lines in the
given input ﬁsheye image, which are supposed to be
straight in an expected rectiﬁed image. Some exam-
ples of detected distorted lines are displayed in the ﬁrst
row of Fig. 1.

• Module for recovering the distortion parameters.
Fed with detected distorted lines and the original ﬁsh-
eye image, this module attempts to recover the dis-
tortion parameters of the ﬁsheye lens.
In particular,
multi-scale perceptron is desined to eliminate the non-
linearity of distortion distribution in ﬁsh-eye images by
combining both the local and global learning.

• Rectiﬁcation module. This differentiable rectiﬁca-
tion module serves as a connector between distortion
parameters and geometric constraint. As shown the
second row in Fig. 1, the line map without distortion
which are rectiﬁed from the distorted lines in the ﬁrst
row are displayed.

These three modules are trained by minimizing the loss
function composed of three terms: a multi-scale constraint
of global and local perception on distortion parameters, as
well as a curvature constraint on detected distorted lines.
Since all of the calibration and rectiﬁcation steps are mod-
eled with one deep neural network, it is naturally to train
it in an end-to-end manner. To get a better performance of
proposed network, well-annotations of distorted lines and
distortion parameters are required for every ﬁsheye image
during the training phase. Thus, we create a new synthetic
dataset for the ﬁsheye lens calibration by converting the
wireframe dataset [17] to distorted Wireframe collections
(D-Wireframe) and the 3D model repository [29] to ﬁsh-
eye SUNCG collections (Fish-SUNCG). In detail, the D-
wireframe collections is created by distorting the perspec-
tive images with randomly generated distortion parameters,

and the Fish-SUNCG collections is built by rendering the
formation of real ﬁsheye lens in 3D virtual scenarios.

1.3. Related Work

In the past decades, many researches have been devot-
ing themselves to ﬁsheye calibration and distortion correc-
tion studies. Earlier works attempted to estimate the dis-
tortion parameters by correlating the detected 2D/3D fea-
tures in speciﬁc calibration ﬁeld [19, 14, 30, 28, 3, 5, 32].
However, it is costly to build such calibration ﬁelds of
large-scale and high-precision as well as it usually turns to
be laborious and time-consuming to manually label every
calibration pattern. By contrast, self-calibration methods
which rely on structural information detected in distorted
images [13, 36, 5, 22, 12, 1, 25] require less manual work
and are more efﬁcient. Devernay et al. [13] proposed that
the straight line segments in the real world should maintain
their line property even after the projection of ﬁsheye lens.
Along this axis, Bukhari et al. [12] recently proposed to use
an extended Hough Transform of lines to correct radial dis-
tortions. With similar assumption, the ‘plumb-lines’ have
used to rectify ﬁsheye distortions [36, 22, 1] . However,
their correction effects are limited by the accuracy of geo-
metric object detecting results. Our work in this paper also
follows the same assumption as suggested in [13], while we
propose a deep convolutional neural network to handle the
aforementioned problems and generate a more accurate re-
sult of distorted lines extraction in ﬁsheye images.

To mitigate the difﬁculty of detecting geometric objects
in distorted images, the deep learning methods were pro-
posed [27, 35] which imposed the representational features
learned by CNNs to the processes of ﬁsheye calibration and
image rectiﬁcation. Among them, the FishEyeRecNet [35]
proposed an end-to-end CNNs which introduce scene pars-
ing semantic into the rectiﬁcation of ﬁsheye images. It has
reported promising results, but it is still not clear which kind
of high-level geometric information learned from their net-
works are important for ﬁsheye image rectiﬁcation. More-
over, The works [36, 22, 1] explicit geometry like ‘plumb-
lines’ are very efﬁcient for distortion corrections, but how
to encode them with CNNs in an effective way is still an
open problem.

Another topic closely related to our work is distorted
lines extraction in ﬁsheye images. Various arc detection
methods and optimizing strategies have been utilized in the
calibrating process [11, 7, 12, 1, 36], but they are not robust
to detect arcs especially in the environments with noises or
texture absence. Although recent deep learning based meth-
ods [33, 21, 8, 18, 26] show a promising performance on
edge detection, none of them is well qualiﬁed to deal with
ditorted lines in ﬁsheye images.

1.4. Our Contributions

In this paper, we propose a novel end-to-end network
to calibrate ﬁsheye lens and rectify distorted images si-

21644

Figure 2. Architecture of the overall system. The whole network architecture consists of three parts: line-guided parameter estimation
module (LPE), distorted line segments perception module (DLP), and a rectiﬁcation layer. DLP could detect the map of curves which
should be straight line in rectiﬁed image, and take the ouput from DLP and RGB ﬁsheye image into the LPE to estimate the global and
local ﬁsheye distortion parameters. The distortion parameters are used in rectiﬁcation layer to implement the curvature constraint.

multaneously with further exploit of geometric constraints.
Speciﬁcally, we make the following three contributions:

• We proposed an end-to-end CNNs to impose explicit
geometry constraints onto the process of ﬁsheye lens
calibration and distorted image rectiﬁcation, which
achieves the state-of-the-art performance.

• Multi-scale perception is designed to balance the non-
linear distribution of distortion effects in ﬁsh-eye im-
ages. And more robust distortion parameters obtained
from global and local learning, so as to achieve better
rectiﬁcation effect.

• We construct a new large-scale ﬁsheye dataset to train
networks and to evaluate the effectiveness and efﬁ-
ciency of ﬁsheye image rectiﬁcation methods.

2. General Fisheye Camera Model

Given a normal pinhole camera with focal length f ,
the perspective projection model can be written as r =
f tan θ, where r indicates the projection distance between
the principal point and the points in the image, and θ is
the angle between the incident ray and the camera’s optical
axis. While, ﬁsheye lens violates this perspective projec-
tion model [23, 6], and has been often approximated by a
general polynomial projection model [19], i.e.,

r(θ) = Xn

i=1

kiθ2i−1, n = 1, 2, 3, 4, . . .

(1)

Usually, this model can accurately approximate the image
formation of ﬁsheye lenses when n reaches 5 [19].

Given a 3D scene point Pc := (xc, yc, zc)T ∈ R3 in
the camera coordinate system, it will be projected into the
image plane with pd := (xd, yd)T ∈ R2 refracted by the
ﬁsheye lens and p := (x, y)T ∈ R2 through perspective

lens without distortion. The correspondence between pd
and p can be expressed as, pd = r(θ)(cos ϕ, sin ϕ)T , with
ϕ = arctan ((yd − y)/(xd − x)) indicating the angle be-
tween the ray that connects the projected point and the cen-
ter of image and the x-axis of the image coordinate system.
Assuming that the pixel coordinate system is orthogonal,
we can get pixel coordinates (u, v) converted by image co-
ordinates pd as

v(cid:19) = (cid:18)mu
(cid:18)u

0 mv(cid:19)(cid:18)xd

v0(cid:19)
yd(cid:19) +(cid:18)u0

0

(2)

The principal point of ﬁsheye image is represented as
(u0, v0), and mu , mv describe the number of pixels per unit
distance in horizontal and vertical direction respectively.

By using Eq. (2), the distortion effect of ﬁsheye im-
ages can be rectiﬁed once we can get the parameters Kd =
(k1, k2, k3, k4, k5, mu, mv, u0, v0). Therefore, we are go-
ing to accurately estimate the parameters Kd for every given
ﬁsheye image and simultaneously eliminate the distortion in
this paper.

3. Deep Calibration and Rectiﬁcation Model

In this section, we mainly exploit the relationship be-
tween scene geometry of distorted lines and the correspond-
ing distortion parameters of ﬁsheye images by CNNs, and
learn mapping functions from raw input ﬁsheye image to
the the rectiﬁed image.

3.1. Network Architecture

As shown in Fig. 2, our network is mainly composed of
distorted lines perception module (DLP) which solves the
problem of distorted lines extraction, line-guided parameter
estimation module (LPE) which provides estimated distor-
tion parameters Kd, as well as rectiﬁcation module which

31645

serves as connector between geometry and distortion pa-
rameters. For a RGB ﬁsheye image I with size of H × W ,
the distorted lines map h ∈ RH×W could be acquired from
DLP, and then fed the distorted line map h and the original
ﬁsheye image I together into LPE to learn the global and
local parameters through multi-scale perception. And the
rectiﬁcation module could verify the accuracy of learned
parameters Kd by analyzing whether the lines in rectiﬁed
distorted lines map ˆh′ have be straight after being rectiﬁed
with Kd. Thereafter, we are able to learn the distortion pa-
rameters and rectiﬁed images without distortion in the end-
to-end manner.

Every training data sample for our network contains: (1)
a ﬁsheye image I, (2) the ground truth distortion parameters
ˆKgt, (3) the ground truth of distorted lines map ˆh of image
I, (4) the ground truth of the corresponding rectiﬁed line
map ˆh′ and (5) the corresponding (rectiﬁed) line segments
L = {xi, x′
i ∈
R2 are two endpoints of a line segment.

i=1of image I, where the xi ∈ R2 and x′

i}K

Distorted Line Perception Module. Followed by the re-
cent advances of edge and line segment detection [33, 17],
we use the Pyramid Residual Modules (RPM) and Stacked
Hourglass network [24] to learn the distorted line segments
map h ∈ RH×W from the input images.
In details, we
ﬁrstly use two RPMs to extract feature maps with size of
H
4 × W
4 × 256 from input image with size of H × W × 3.
Then, we pass the feature map into 5 stacked hourglass
modules. The resulted features are then upscaled by us-
ing two deconvolution layers to get the feature with size of
H × W × 16.
In the end, we use a convolutional layer
with 1 × 1 kernel size to predict the distorted lines map h.
Excepting for the prediction layer, the Batch-Normalization
and ReLU are used for each (de)convolution layer. The tar-
get of line segment map ˆh is deﬁned pixelwised by

ˆh(p) = (cid:26) d(l)

0

if p is (nearly) on l ∈ L,
otherwise,

(3)

where the function d(li) is read as d(li) = kxi − x′
ik2. The
resulted map h not only can indicate if a pixel p is passed
through a line segment, the predicted length of rectiﬁed line
segment also implicitly contains the information for the dis-
tortion parameters.

Line-guided Parameters Estimation Module. This
module aims at estimating the distortion parameters from
images. As mentioned above, the predicted distorted lines
map could characterize the distortion of ﬁsheye images in
some extent. Based on this, regarding the distorted lines
map h output from DLP module as a geometric guidance
providing high-level structural information to LPE, we con-
catenate it with the input ﬁsheye image together with size
of H × W × 4, as the input of LPE to estimate the mutli-
scale distortion parameters. As shown in Fig. 2, we adopt
the level 1 to 4 of the ResNet-50 [15] as backbone of LPE

module, and design a bifurcated structure with a global and
a local stream respectively to multi-scale perception, con-
sidering the nonlinearity of the distortion distribution in the
domain of ﬁsheye images.

The global stream treats the entire feature map to esti-
mate the distortion parameters with 2 convolutional layers
and 3 fully connected (FC) layers. Before the ﬁrst FC layer
of the global stream, we use the global average pooling op-
erator to extract the abstracted global information from im-
ages. The last FC layer output a 9-D vector representing the
distortion parameters denoted by Kg.

Because of the nonlinearity of the distortion distribution,
we explicitly use the cropped feature maps from the out-
put of the backbone to estimate the distortion parameters
locally. We divide this sideoutput into ﬁve smaller blocks
- the central region with size of 6 × 6 × 1024 and four
5 × 5 × 1024 feature maps of upper left, lower left, upper
right and lower right, and then send these ﬁve set of sub-
feature maps into two FC layers and a linear ﬁlter separately
to predict the local parameters, denoted by {K k
k=1. The
parameter settings of these two FC layers are same as those
in global stream, meanwhile the weight of them are shared
across these ﬁve set of sub-feature maps. Since the param-
eters mu, mv and u0, v0 are related to the entire image, the
linear ﬁlter only reserve the ﬁrst ﬁve distortion parameters
k1, . . . , k5 in the previous output. Each output K k
loc is thus
a 5D vector.

loc}5

In training phase, the predicted parameters K k

loc is used
as a constraint to regularize the prediction of the global
stream. And the output of DLP is the averaged distortion
coefﬁcients of global and local parameters, denoted as Kd.

Rectiﬁcation Module.
In this module, we take the pre-
dicted distortion parameters Kd of LPE module as input to
rectify the input ﬁsheye image and the corresponding dis-
torted line segments map from the DLP module. Suppose
that the pixel coordinate in rectiﬁed and ﬁsheye images are
p = (x, y) and pd = (xd, yd), their relationship can be read

pd = T (p, Kd) = (cid:18)u0

v0(cid:19) +

r(θ)p
kpk2

(4)

With the Eq. (4), the distorted lines map and ﬁsheye image
can be rectiﬁed by using bilinear interpolation.

The signiﬁcance of the above rectiﬁcation layer is to ex-
plicitly bridge the relation between distortion parameters
and geometry structures. The more accurate the estimated
distortion parameters, the distorted lines map will be recti-
ﬁed better.

3.2. Loss Function and Training Scheme

In our network, we can end-to-end output

the dis-
torted lines map h, estimated distortion parameters Kg and
K k
loc, k = 1, . . . , 5 as well as the rectiﬁed line segment map
for every input image I. Inspired by the deeply supervised
net [20] and HED [33], we make supervision to the outputs
of each module.

41646

Loss of Distorted Lines Map Learning. Considering the
fact that distorted line segments are 0-measure geometric
primitives in 2D images, most of pixels for the target ˆh de-
ﬁned in Eq. (3) will be 0. In other words, most of pixels
will not be passed through any distorted line segment. For
the sake of representation simplicity, the pixels not being
on any distorted line segment are collected to the negative
class Ω− and the rest of pixels are collected in the positive
class Ω+, with Ω+ = Ω − Ω−. Then, we weight these two
classes in the loss function as

Lline =

|Ω−|

|Ω| Xp∈Ω+

D(p) +

|Ω+|

|Ω| Xp∈Ω−

D(p),

(5)

where D(p) is deﬁned as D(p) = kh(p) − ˆh(p)k2
2.
Loss of Distortion Parameter Estimation.
In the LPE
module, we try to learn the distortion parameters with a
bifurcate structure, which results the parameters Kg and
{K k
k=1. Ideally, we want the outputs of LPE module
close to the ground-truth distortion parameters. For the out-
put Kg, we deﬁne the loss as

loc}5

Lg =

1
9

9

Xi=1

wi(Kgt(i) − Kgt(i))2,

(6)

where the Kg(i) and Kgt(i) are the i-th component of pre-
dicted parameter Kg and the ground truth Kgt. The weight
wi is used to rescale the magnitude between different com-
ponents of distortion parameter. On the other side of the
bifurcate, the loss of parameters estimated by the sub fea-
ture maps are deﬁned as

e
m
a
r
f
e
r
i

W
D

-

G
C
N
U
S
-
h
s
i
F

Figure 3. Data samples from the distorted wireframe (top) and ﬁsh-
eye SUNCG collections (bottom) of our proposed dataset. Every
data sample is shown vertically for the original image and corre-
sponding ﬁsheye image.

Training Scheme. The network training procedure con-
sists of two phases. In the ﬁrst phase, we train the distorted
line perception module from scratch with the loss function
deﬁned in Eq. (5). Once the DLP module is trained, we ﬁx
their parameters and then learn the distortion parameters in
the second phase. The total loss we used here is deﬁned as

L = λgLg + λloc

5

Xk=1

Lk

loc + λcLc,

(9)

which aims at ﬁtting the parameters and simulating the dis-
tortion effect of ﬁsheye during training. The λg, λloc and
λc used in Eq. (9) are weight parameters to balance the dif-
ferent terms.

Lk

loc =

1
5

5

Xi=1

wi(K k

loc(i) − K k

gt(i))2,

(7)

4. Synthetic Dataset for Calibration

loc.

loc(i) is the i-th component of K k

where the K k
Global Curvature Constraint Loss. The Lg and Lloc en-
force the network ﬁt the distortion parameters, however,
only optimizing them is not enough and prone to get stuck
in the local minimums. Meanwhile, the relation between the
parameters and the geometry of distorted line which should
be straight in rectiﬁed images provides a stronger constraint
to boost the optimizing.
If the distorted line is not com-
pletely corrected to a straight line, the estimated distortion
parameters are not accurate enough, and vice versa. There-
fore, we calculate the pixel-errors between the rectiﬁed line
map by estimated parameters Kd output from LPE and the
ground truth of line map as the global curvature constraint
loss Lc:

Lc =

1

N Xpd∈Ω+

(F(pd, Kd) − F(pd, Kgt))2,

(8)

where F is the inverse function of T which described in
Eq. (4), and N is the number of pixels that belong to the
distorted line segment.

There still remains a crucial problem for training the
proposed neural network which requires real distortion pa-
rameters as well as well-annotated distorted and rectiﬁed
line maps. However, to the best of our knowledge, there
is no such large scale dataset that satisfy all above re-
quirements. Thanks to the recently released wireframe
dataset [16] which has the labeling of straight lines and
the large-scale dataset of 3D scenes SUNCG [29] which
provides diverse semantic 3D scenes, we construct a new
dataset with well-annotated 2D/3D line segments L as well
as the corresponding distortion parameters Kgt for train-
ing. The two subsets of our dataset, the distorted wireframe
collection (D-Wireframe) from wireframe dataset and the
ﬁsheye SUNCG collection (Fish-SUNCG) from 3D model
repository, are described below, as shown in Fig. 3.

Distorted Wireframe Collection (D-Wireframe). For
any image in the wireframes datase proposed by [16]
which contains 5462 normal perspective images marked
with straight line segments, we randomly generate four dif-
ferent sets of distortion parameters Ki to transform this per-
spective image into ﬁsheye image with different distortion

51647

Figure 4. Generation schematic diagram of Fish-SUNCG. Equip
each camera with perspective lens and ﬁsheye lens respectively.

effects by Eq. (1). Thus, the perspective image and the cor-
responding line segment annotations can be distorted to the
ﬁsheye image with distorted line segments. In summary, we
generate this collection Dwf and split it into training set and
testing set with 20, 000 and 1848 samples respectively.

Fisheye SUNCG Collection (Fish-SUNCG). The D-
wireframe collection could provide beneﬁts in terms of di-
versity and ﬂexibility of ﬁsheye distortion types for the net-
work training. However, artiﬁcially distorting the images
which taken by perspective cameras cannot fully character-
ize the ﬁsheye distortion for real scenarios. We address this
problem by simulating the image formation for both per-
spective and ﬁsheye cameras at the same observation posi-
tions from the 3D models of SUNCG [29] which contains
45K different virtual 3D scenes with manually created re-
alistic room and furniture layouts.
In details, we use the
Blender [10] to render images by specifying the camera
pose and imaging formation models. The rendering proto-
col is illustrated in Fig. 4. For the line segments generating,
we remove the texture of 3D models to get the wireframe
model of 3D objects. After that, we manually remove the
edges of wireframe manually to get the line segments that
are on the boundary of objects. Since we are able to control
the image formation without metric errors, the data samples
can be used to train our network without loss of informa-
tion. In the end, we generate 6,000 image pairs from 1,000
scenes for training and 300 image pairs from 125 scenes for
testing. This collection is denoted as Dsun.

5. Experiments

5.1. Implementation Details

We follow our training scheme described in the Sec-
tion 3.2. We use the distorted ﬁsheye images and the cor-
responding line segment map of distorted wireframe collect
Dwf for training the DLP module in the ﬁrst step. After
that, we ﬁx the weights of DLP module and train the rest of
our network by using the Dwf and Dsun together. The size
of input images for our network is set as 320 × 320 for both
training and testing phases.

The weight parameters in Eq. (9) are set to as follow for
our experiments: λc = 50, λloc = λg = 1, and the balance
parameters are set to as follow: W = {w1 = 0.1, w2 =
0.1, w3 = 0.5, w4 = 1, w5 = 1, w6 = 0.1, w7 = 0.1, w8 =
0.1, w9 = 0.1}. The optimization method we used for the
training is the stochastic steepest descent method (SGD).
The initial learning rate is set to 0.01, and then decrease it

T
G

d
e
t
c
e
t
e
D

Figure 5. Distorted lines detection results from DLP. First Row
is ground truth with manual labeling of accurate distorted lines
information. The Second Row is the our detection.

by a multiple of 0.1 after 100 epochs. The network will con-
verge after 300 epochs. And our network is implemented on
the PyTorch platform with a single Titan-X GPU device.

5.2. Evaluation Metrics

′

Beneﬁting from the DLP module of our proposed ap-
proach, we are able to compare the effects of eliminating
distortion and the performance on recovering line geometry
by evaluating the rectiﬁed distorted lines map ˆh
from recti-
ﬁcation module and the ground truth image ˆh. What’s more,
the Precision and Recall are redeﬁned to quantitatively eval-
uate the error between ˆh
and ˆh. Further, the reprojection
error (RPE) is proposed to evaluate the overall rectiﬁcation
effects by measuring the pixels deviation between rectiﬁed
image and ﬁsh image. On the other hand, we also follow
the evaluation metrics used in previous works [27, 35] that
utilize the peak signal to noise ratio (PSNR) and structure
similarity index (SSIM) for evaluating the rectiﬁed images.

′

Precision v.s. Recall. The precision and recall rate of the
line segment map prediction is deﬁned as

P recision = |P ∩ G|/|P |, Recall = |P ∩ G|/|G|, (10)

where the P is the set of edge pixels on the rectiﬁed line
segment map and G is the set of edge pixels in the ground
truth of line segment map without distortion.

PSNR and SSIM. These two metrics are widely used to
describe the degree of pixel blurring and structure distortion
respectively. We use them here for comparing the rectiﬁed
ﬁsheye images. In general, the larger the value of PSNR
and SSIM, the better the rectiﬁcation quality.

Reprejection Error (RPE). This metric is generally used
to quantify the distance between an estimate of a 2D/3D
point and its true projection. So we use the real distortion
parameters Kgt and the estimated ones Kd to rectify the
pixels of ﬁsheye image, and get the projection F(Kgt) and
F(Kd) respectively, where the F is the function representa-
tion of Eq. (8). The mean square error (MSE) of the RPE in
the whole ﬁsheye image is deﬁned by 1
F(kd))2.

N Ppd∈Ω(F(Kgt)−

61648

Table 1. Comparisons with the state-of-the-arts, using the PSNR,
SSIM and reprojection error (RPE) calculated on rectiﬁed results
obtained by different methods.

Methods Bukhar[12] AlemnFlores[1] Rong[27] Ours
27.61
12.92
PSNR
9.3391
0.3154 0.8746
SSIM 0.1782
0.4761
121.6
164.7
RPE

10.23
0.2587
125.4

Distorted Lines Bukhari [12] AlemnFlores[1] Rong [27]

Ours

Figure 6. Rectify the distorted lines. First column is the detec-
tion results by DLP. Other columns is the rectiﬁcation results by
different methods.

e
m
a
r
f
e
r
i

W
D

-

G
C
G
N
U
S
-
h
s
i
F

Figure 7. The precision-recall curves of different rectiﬁcation
methods for the line map rectiﬁcation [12, 1, 27].

5.3. Comparison with State of the art Methods

Our network mainly includes distorted lines detection in
DLP and distortion parameters estimation in DPE, aiming
at performing better rectiﬁcation effects.

For DLP, the highly accurate detection of distorted lines
output from this module is the premise of accurate distortion
parameters learning. As shown in Fig. 5, the detection re-
sults by our method from DLP show excellent performance
as well as close to the ground truth in visual effects. Instead
of directly evaluating the result of distorted line detection,
we jointly evaluate the performance of distorted lines detec-
tion and the accuracy of parameters estimation compared
with existing state-of-the-art methods [12, 1, 27], and the
details will be discussed in the following.

For DPE, the effective use of geometric constraints is
the key to guarantee the rectiﬁcation effects. According to
previous analysis, the evaluation for the rectiﬁed line seg-
ment map can explicitly illustrate the geometric accuracy of
rectiﬁcation. In other words, if the rectiﬁed distorted lines
map still exists curved geometry or has deviation from the
ground truth, it shows that the learned distortion parameters
are not accurate enough. In visual effect, we show the geo-
metric rectiﬁcation of the distorted lines map which output
from the rectiﬁcation module intuitively in Fig. 6 to ver-
ify our network actually has the ability of recovering the
straight line characteristics. The results show that the rec-
tiﬁed line map through our network is indeed straightened,
while those rectiﬁed by other methods are still distorted in
some extent and it proves the validity of the geometric con-

Input

Bukhari AlemnFlores

Rong

Ours

GT

Figure 8. Qualitative rectiﬁcation comparison on D-Wireframe
dataset and ﬁsh-SUNGCG dataset. From left to right, the input
ﬁsheye images, the ground truth, results of three state-of-the-art
methods (Bukhari [12], AlemnFlores [1], Rong [27]), our results.

straint in our network. Further more, we report the preci-
sion and recall curves in Fig. 7 to show the comparison in
quantitative. Obviously, our method is far superior to other
methods [12, 1, 27] in terms of line geometric structure re-
covery, and achieves the best result (F-value =.819).

We also follow the evaluation metrics PSNR, SSIM as
well as reprojection errors (RPE) for qualitative evaluation
of all rectiﬁed ﬁsheye images on our test set, as reported in
Tab. 1. From the evaluation results, it demonstrate that no
matter in image rectiﬁcation or in structure maintenance,
our method is obviously superior to other methods and has
achieved the highest score on PSNR, SSIM as well as RPE.
It is worth mentioning that the reprojection error of the
whole image calculated by our method is within one pixel,
while other methods far behind us. Just for the reason that
the estimated ﬁsheye distortion parameters are highly con-
sistent with the true parameters, our reprojection error can
be controlled in such small range, resulting in the best rec-
tiﬁcation visual effect.

For the rectiﬁcation effects in visual effect, we visualize
the rectiﬁcation effects by our method and start-of-the-art
methods [12, 1, 27] on the test set of D-Wireframe and Fish-
SUNCG collections respectively, as shown in Fig. 8. For the
D-Wireframe, we selected special images of different types
of ﬁsheye distortion, such as typical full-frame ﬁsheye im-
age, full circle ﬁsheye image and drum ﬁsheye image, and
the results show that our method has excellent rectiﬁcation
effect in visual effects, while other methods can not satisfy
the needs of correcting various distortion effects of ﬁsheye
images. For the Fish-SUNCG, our network also achieves

71649

Input

Bukhari [12] AlemnFlores [1] Rong [27]

Ours

Input

Bukhari [12] AlemnFlores [1] Rong [27]

Ours

Figure 9. Qualitative rectiﬁcation comparison on ﬁsheye images taken by real ﬁsheye cameras.

Input

w/o CVC&MSP

w/o CVC

w/o MSP

only RGB

only Line

Ours

Ground Truth

Figure 10. Ablation experiments to verify effectiveness of the curvature constraint(CVC) and the local perception(LP) of ﬁsheye image.

the best rectiﬁcation performance and the rectiﬁed ﬁsheye
image is more closer to ground truth.

Finally, we add an additional set of comparison experi-
ments to rectify the ﬁsheye image in real world for the gen-
eralization performance of the proposed network, as shown
in Fig. 9. Although we do not have the internal camera pa-
rameters for taking these ﬁsheye images, it demonstrate that
our method has excellent rectiﬁcation performance even for
real ﬁsheye images from the visual effect and prove that our
network has higher generalization ability.

5.4. Ablation Study

In this section, we mainly analyze the validity of our
network structures involving geometric learning, including
the fourth dimensional input of concatenating the input im-
age and detected distorted lines map in LPE, curvature con-
straint (CVC), as well as multi-scale perception (MSP) for
locally and globally estimating the distortion parameters.
As shown in Fig. 10, once there are lack of CVC or MSP,
the rectiﬁcation effect of the network will become unstable,
meanwhile problems of over-rectiﬁed and under-rectiﬁed
will occur, and the network with RGB ﬁsheye image and
one-dimensional line map input performs best in rectiﬁca-
tion performance.

For qualitative evaluation, we evaluated the quality of
rectiﬁed images on PSNR, SSIM and RPE, as shown in
Tab. 2. It demonstrates the ability level differences of recti-
ﬁcation more intuitively, and proves that CVC, MSP, as well
as the four-dimensional input (RGB+Line) do play critical
roles in our network. According to our analysis, it is prob-

Table 2. Ablation study to evaluate the rectiﬁed image quality of
PSNR, SSIM and reprojection error (RPE).

Loss

Strategy

Input

Methods

PSNR SSIM RPE
w/o CVC&MSP 13.05 0.4222 78.32
19.17 0.7074 4.326
19.78 0.6999 3.175
21.35 0.7158 1.872
22.41 0.7369 1.568
27.61 0.8746 0.4761

w/o CVC
w/o MSP
only RGB
only Line

Ours

ably that the network learned high-level structural informa-
tion from the distorted lines that boost the effects. In addi-
tion, the worst results in this experiment is still better than
state-of-the-art methods. It also proves the scientiﬁcity and
reasonability of our network.

6. Conclusion

In this paper, we proposed a network that utilize line con-
straints to calibrate the ﬁsheye lenses and eliminate the dis-
tortion effects automatically from single image. To train the
network, we reuse the existing datasets that have rich 2D
and 3D geometric information to generate the a synthetic
dataset for ﬁsheye calibration. The proposed method takes
the advantages of geometry aware deep features, curvature
constraints and multi-scale perception blocks to achieve the
best performance compared to the state-of-the-art methods,
both qualitatively and quantitatively.
Acknowledgment:This work is supported by NSFC-
projects under contracts No.61771350 and 61842102. Nan
Xue is supported by China Scholarship Council. Thanks for
the support and help from tutors and seniors.

81650

[22] R. Melo, M. Antunes, J. P. Barreto, G. Falcao, and
N. Goncalves. Unsupervised intrinsic calibration from a sin-
gle frame using a. In ICCV, 2013. 1, 2

[23] K. Miyamoto. Fish eye lens. JOSA, 54(8):1060–1061, 1964.

3

[24] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-

works for human pose estimation. In ECCV, 2016. 4

[25] S. R. Peter Sturm. Self-calibration of a general radially sym-

metric. In ECCV, 2006. 2

[26] D. Piotr and Z. C Lawrence. Fast edge detection using struc-

tured forests. PAMI, 37(8):1558–1570, 2015. 2

[27] J. Rong, S. Huang, Z. Shang, and X. Ying. Radial lens distor-
tion correction using convolutional neural networks trained
with synthesized images. In ACCV, pages 35–49, 2016. 1, 2,
6, 7, 8

[28] D. Scaramuzza, A. Martinelli, and R. Siegwart. A ﬂexi-
ble technique for accurate omnidirectional camera calibra-
tion and structure from motion. In ICVS, 2006. 1, 2

[29] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser. Semantic scene completion from a single
depth image. In CVPR, 2017. 2, 5, 6

[30] P. Sturm and S. Ramalingam. A generic concept for camera

calibration. In ECCV, 2004. 1, 2

[31] R. Szeliski and H.-Y. Shum. Creating full view panoramic
image mosaics and environment maps. In SIGGRAPH, 1997.
1

[32] C. Toepfer and T. Ehlgen. A unifying omnidirectional cam-

era model and its applications. In ICCV, 2007. 2

[33] S. Xie and Z. Tu. Holistically-nested edge detection.

In

ICCV, 2015. 2, 4

[34] Y. Xiong and K. Turkowski. Creating image-based vr using

a self-calibrating ﬁsheye lens. In CVPR, 1997. 1

[35] X. Yin, X. Wang, J. Yu, M. Zhang, P. Fua, and D. Tao.
Fisheyerecnet: A multi-context collaborative deep network
for ﬁsheye image rectiﬁcation. ECCV, 2018. 1, 2, 6

[36] M. Zhang, J. Yao, M. Xia, K. Li, Y. Zhang, and Y. Liu. Line-
based multi-label energy optimization for ﬁsheye image rec-
tiﬁcation and calibration. In CVPR, 2015. 1, 2

References

[1] M. Alem´an-Flores, L. Alvarez, L. Gomez, and D. Santana-
Cedr´es. Automatic lens distortion correction using one-
parameter division models. IPOL, 4:327–343, 2014. 1, 2,
7, 8

[2] H. A. Alhaija, S. K. Mustikovela, L. Mescheder, A. Geiger,
and C. Rother. Augmented reality meets computer vision:
Efﬁcient data generation for urban driving scenes.
IJCV,
126(9):961–972, 2018. 1

[3] D. G. Aliaga. Accurate catadioptric calibration for real-time
pose estimation in room-size environments. In ICCV, 2001.
2

[4] J. Barreto, J. Roquette, P. Sturm, and F. Fonseca. Automatic
camera calibration applied to medical endoscopy. In BMVC,
2009. 1

[5] J. P. Barreto and H. Araujo. Geometric properties of central
catadioptric line images and their application in calibration.
PAMI, 27(8):1327–1333, 2005. 2

[6] A. Basu and S. Licardie. Alternative models for ﬁsh-eye

lenses. Pattern recognition letters, 16(4):433–441, 1995. 3

[7] J.-C. Bazin, I. Kweon, C. Demonceaux, and P. Vasseur. Rect-

angle extraction in catadioptric images. 2007. 2

[8] G. Bertasius, J. Shi, and L. Torresani. Deepedge: A multi-
scale bifurcated deep network for top-down contour detec-
tion. In CVPR, 2014. 2

[9] M. Bertozzi, A. Broggi, and A. Fascioli. Vision-based in-
telligent vehicles: State of the art and perspectives. RAS,
32(1):1–16, 2000. 1

[10] Blender Online Community. Blender - a 3d modelling and
rendering package. Blender Foundation, Blender Institute
Amsterdam, 2014. 6

[11] C. Brauer-Burchardt and K. Voss. A new algorithm to cor-
rect ﬁsh-eye-and strong wide-angle-lens-distortion from sin-
gle images. In ICIP, 2001. 2

[12] F. Bukhari and M. N. Dailey. Automatic radial distortion
estimation from a single image. JMIV, 45(1):31–45, 2013.
1, 2, 7, 8

[13] F. Devernay and O. Faugeras. Straight lines have to be

straight. MVA, 13(1):14–24, 2001. 1, 2

[14] M. D. Grossberg and S. K. Nayar. A general imaging model
and a method for ﬁnding its parameters. In ICCV, 2001. 1, 2
[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 4

[16] J. Huang, Z. Chen, D. Ceylan, and H. Jin. 6-dof vr videos

with a single 360-camera. In VR, 2017. 1, 5

[17] K. Huang, Y. Wang, Z. Zhou, T. Ding, S. Gao, and Y. Ma.
Learning to parse wireframes in images of man-made envi-
ronments. In CVPR, 2018. 2, 4

[18] J. J. Hwang and T. L. Liu. Pixel-wise deep learning for con-

tour detection. In ICLR, 2015. 2

[19] J. Kannala and S. S. Brandt. A generic camera model and
calibration method for conventional, wide-angle, and ﬁsh-
eye lenses. PAMI, 28(8):1335–1340, 2006. 1, 2, 3

[20] C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATSZ, 2015. 4

[21] Y. Liu, M.-M. Cheng, X. Hu, K. Wang, and X. Bai. Richer
convolutional features for edge detection. In CVPR, 2017. 2

91651

