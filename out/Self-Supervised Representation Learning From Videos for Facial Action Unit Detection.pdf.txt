Self-supervised Representation Learning from Videos

for Facial Action Unit Detection

Yong Li1

,

2, Jiabei Zeng1, Shiguang Shan1

2

3

,

,

,

4 , Xilin Chen1

2

,

1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),

Institute of Computing Technology, CAS, Beijing 100190, China

2University of Chinese Academy of Sciences, Beijing 100049, China

3CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, 200031, China

4Peng Cheng Laboratory, Shenzhen, 518055, China

yong.li@vipl.ict.ac.cn, {jiabei.zeng, sgshan, xlchen}@ict.ac.cn

Abstract

In this paper, we aim to learn discriminative representa-
tion for facial action unit (AU) detection from large amount
of videos without manual annotations.
Inspired by the
fact that facial actions are the movements of facial mus-
cles, we depict the movements as the transformation be-
tween two face images in different frames and use it as the
self-supervisory signal to learn the representations. How-
ever, under the uncontrolled condition, the transformation
is caused by both facial actions and head motions. To re-
move the inﬂuence by head motions, we propose a Twin-
Cycle Autoencoder (TCAE) that can disentangle the facial
action related movements and the head motion related ones.
Speciﬁcally, TCAE is trained to respectively change the fa-
cial actions and head poses of the source face to those of
the target face. Our experiments validate TCAE’s capa-
bility of decoupling the movements. Experimental results
also demonstrate that the learned representation is discrim-
inative for AU detection, where TCAE outperforms or is
comparable with the state-of-the-art self-supervised learn-
ing methods and supervised AU detection methods.

1. Introduction

Facial actions convey varied and nuanced meanings, in-
cluding a person’s intentions, affective and physical states.
To study the facial actions comprehensively, Ekman and
Friesen developed the Facial Action Coding System (FACS)
which deﬁnes a unique set of about 40 atomic non-
overlapping facial muscle actions called Action Units (AUs)
[8]. AU detection has drawn signiﬁcant interest from com-
puter scientists and psychologists over recent decades, as
it holds promise to an abundance of applications, such as
affect analysis, mental health assessment, and human com-

(cid:639)

(cid:3)

source image 

(cid:639)

(cid:3)

re-generate 

AU-related  
movements 

AU  

feature 

pose-related  
movements 

re-generate 

AU-changed 

pose-changed 

(cid:639)(cid:3)

target image 

Figure 1. Main idea of the proposed self-supervised learning
framework Twin-Cycle Autoencoder (TCAE). TCAE learns AU
discriminative features by predicting the disentangled movements
that change the AUs and head poses respectively. TCAE ensures
the quality of the discovered movements by transforming the AU-
changed and pose-changed faces back to the source.

puter interaction.

Recently, the development of AU detection is facilitated
by the progress in deep learning [47, 22, 21, 32]. However,
it is data starved to make full use of these supervised meth-
ods, because labelling AUs is time consuming, error prone,
and confusing. It takes 30 minutes or more for an FACS
expert to manually code an AU for a one-minute video [46].
To alleviate the demand for adequate and accurate anno-
tations, we exploit the practically inﬁnite amount of unla-
belled videos to learn discriminative AU representations in
a self-supervised manner. Considering that AUs appear as
the local movements within the face and the movements are
easy be to detected without manual annotations, we propose
to use the movements as the supervisory signals in learning
the AU representations. However, the detected movements

10924

are always caused by both AUs and head motions. In some
cases, especially in uncontrolled scenarios, head motions
are the dominant contributors to the movements. If we do
not remove the movements of head motions from the super-
visory signals, the learned features would not be discrimi-
native enough for AU detection, because they would encode
more information about head poses than those about AUs.

To address the learning issue from entangled move-
ments, we propose a Twin-Cycle Autoencoder (TCAE)
that self-supervisedly learns two embeddings to encode the
movements of AUs and head motions, respectively. Fig. 1
illustrates the main idea of the proposed TCAE. We sam-
ple two face images (the source image and the target image)
of a subject from a video where he/she is talking and mov-
ing with varied expressions. TCAE is tasked to change the
AUs or head poses of the source frame to those of the target
frame by predicting the AU-related and pose-related move-
ments, respectively. Thus, TCAE distills the information re-
quired to compute the AU- or pose-related movements sepa-
rately into the corresponding embeddings. During the train-
ing, TCAE enforces the generated face images to be realis-
tic because their quality implies how well the movement is
discovered and thus implies how good the representation is.
Since we do not have the real face images that merely AUs
or poses are changed, we introduce a twin-cycle mechanism
to control the quality of the generated face images. In each
cycle, either the predicted AU-changed face or the pose-
changed face is mapped back to the source. Meanwhile,
the AU-related and pose-related movements are combined
to map the source to the target. We show that our proposed
TCAE can disentangle the movements caused by AUs and
head motions.

In summary, our contributions are two folds: 1) We pro-
pose a self-supervised learning framework Twin-Cycle Au-
toencoder (TCAE) to learn AU representations from un-
labelled videos. Experimental results show that TCAE
outperforms or is comparable to the state-of-the-art self-
supervised learning methods and supervised AU detection
methods. 2) TCAE can successfully disentangle the AU-
related movements from the pose-related ones. It indicates
potential applications in editing face images.

2. Related work

Faction unit detection. AU detection has been studied
for decades and various methods have been proposed [24].
To achieve good performance, researchers have designed
different features to represent AU. The features include the
appearance texture of the whole face [44] or near the fa-
cial landmarks [35, 9, 37], or the combination of geometry
shape with texture [10]. Most of these features are based
on general features in computer vision task, such as SIFT,
HOG, LBP, etc. To make the features discriminative for AU,
some works considered that AU is tightly correlated to the

motions within local regions of the face [13] and thus in-
troduced sparsity-induced algorithms [45, 34] to reduce the
inﬂuence of uncorrelated facial regions.

Over the last few years, deep learning has become a dom-
inating approach due to their capability and capacity of rep-
resentation learning. These methods [47, 22, 21, 32] learn
rich local features to capture facial deformation. For exam-
ple, Zhao et al. [47] proposed a locally connected convolu-
tional layer that learns region-speciﬁc convolutional ﬁlters
from sub-areas of the face. JPML [45], EAC-Net [22] and
ROI [21] extracted features around facial landmarks that are
robust with respect to non-rigid shape changes. JAA-Net
[32] proposed to jointly learn AU detection and face align-
ment in a uniﬁed framework. These methods have achieved
promising performance on annotated datasets, e.g., CK+
[23], DISFA [25], BP4D [42]. However, these methods de-
pend on accurately labelled images and often overﬁt on a
speciﬁc dataset because of insufﬁcient training data.

To alleviate the dependence of AUs annotations, several
works start to focus on learning model in a semi-supervised
[40, 4], weakly-supervised [46, 31, 43] or self-supervised
manner [38]. The semi-supervised learning methods usu-
ally incorporate both labelled and unlabelled data by assum-
ing the faces to be clustered by AUs, or to have a smooth
label space. The weakly supervised methods exploit noisy,
incomplete AU annotations [46]. They usually learn AU
classiﬁers from domain knowledge [43], or naturally exist-
ing constraints on AUs [31]. We adopt the self-supervised
learning paradigm because it can learn AU discriminative
features without AU labels and it is regardless of the as-
sumptions on label distribution.

Self-supervised learning.

Self-supervised learning
adopts supervisory signals that are inferred from the struc-
ture of the data itself. The signals include image coloriza-
tion [41], order of a set of frames [26, 18, 11], camera trans-
formations between pairs of images [1] etc. A typical self-
supervised method is SplitBrain [41], which consists of two
sub-nets. For the images, each sub-net predicts a subset of
the channels according the other subset. The features ex-
tracted by the two sub-nets are concatenated and serve as
the generic representations.

Since the proposed TCAE is supervised by the disen-
tangled AU-related and pose-related movements, we review
the related self-supervised methods that can adopt the mo-
tion information as the supervisory signal, or can disentan-
gle different factors.

The former learn visual representations from videos with
the help of motion information, e.g., optical ﬂow [36, 29],
pixelwise correspondence [12], egomotion [17, 1], or ap-
pearance ﬂow [38, 39]. The most related work to TCAE
is Fab-Net [38], which is optimized to map a source frame
to a target frame by predicting a ﬂow ﬁeld between them.
However, the learned embedding of Fab-Net is not a ded-

10925

source image

Cycle with AU Changed

Feature Disentangling

target image

AU-related 
displacements

AU-changed image

generated

source image

source image

overall 

displacements

source image

Target Reconstruction

encoder

AU decoder

pose decoder

AU feature

pose feature

 elementwise product

elementwise add

generated target

target image

bilinear sample

source image

Cycle with pose Changed

generated image

attention mask

pose-related 
displacements

pose-changed image

generated

source image

Figure 2. The framework of TCAE. Given a source image Is and target image It , TCAE encodes their AU (φs or φt) and pose (ψs or ψt)
embeddings. The two AU embeddings are decoded into the displacements T A reﬂecting the changing of AUs between Is and It. Similarly,
the two pose embeddings are decoded into the pose-related displacements T p. In target reconstruction, the integrated displacements are
used to transform Is to It. In the two cycles, TCAE generates an AU-changed face image and a pose-changed face image respectively and
then maps them back to the source face image.

icated AU representation. Fab-Net cannot distinguish the
information on AUs from that on poses.

TCAE is trained to respectively change the source face’s
facial actions or head poses to the ones in the target face.

The latter disentangle the representation without annota-
tions as the supervisory singals [7, 19, 3, 33]. For exam-
ple, DRIT [19] factorized an image into the representations
in content and attribute space with cross-cycle consistency
loss. Dr-Net [7] decomposed a video frame into the sta-
tionary component and the temporally varying component
by forcing the latter to carry no information about identity.
For face analysis, Shu et al. [33] introduced Deforming Au-
toencoders (DeformAE) that disentangles shape from ap-
pearance in a self-supervised manner. In DeformAE, the en-
tangled appearance branch contains an aligned face with no
morphing, and the entangled shape branch contains a mor-
phing ﬁeld that reserve both head pose and facial morphing
information.

3. Twin-Cycle Autoencoder

We propose a symmetric encoder-decoder architecture
called Twin-Cycle Autoencoder (TCAE) to learn AU rep-
resentations in a self-supervised way. Without the manual
annotations, TCAE is trained with pairs of face images of
the same person with different facial actions and head poses.
Each two face images are sampled from a video where a
subject is talking and moving with varied expressions. We
denote the two images as the source Is and the target It.

Fig. 2 illustrates the training framework of TCAE given
the two face images. It consists of four parts: feature dis-
entangling, target reconstruction, cycle with pose changed,
and cycle with AU changed.
In feature disentangling,
TCAE learns the features by respectively predicting the
AU-related and pose-related movements between two im-
ages. In target reconstruction, TCAE integrates the sepa-
rated movements and uses it in transforming the source im-
age to the target one, ensuring that the two movements are
sufﬁcient to represent the changing between the two face
images. To make the separated movements realistic, TCAE
introduces two cycles with AU or pose changed.
It uses
the AU-/pose- related movements to generate an AU-/pose-
changed face image, which are then transformed back to the
source one. TCAE requires the regenerated source image to
be consistent to the original one. Below, we present details
of the four parts in TCAE.

3.1. Feature disentangling

In order to disentangle the information about AUs and
poses, TCAE has a nearly symmetric structure with two
branches. As can be seen in Fig. 2, TCAE ﬁrst encodes
both the source and target images using the encoder E and
gets their embeddings [ψs, φs] and [ψt, φt], respectively. ψs
and ψt denote the pose-related features. φs and φt denote

10926

the AU-related features. Then, TCAE concatenates the two
AU-related features φs and φt and passes them into the AU-
related decoder DA. DA decodes how the facial actions
in the source face is changed to those in the target face,
and where the change happens. Symmetrically, the con-
catenated pose features ψs and ψt are passed to the pose-
related decoder Dp that decodes how and where the pose is
changed. Since the decoders DA and Dp are in symmetrical
branches, we describe one of them in details and the other
is similar. Let us take DA as an example. Since DA takes
the AU-related embeddings of both the source face and tar-
get face as the inputs, it is capable to capture the AU-related
movement between the two faces. The movement caused by
AUs is depicted as the displacements of pixels between the
source face and the AU-changed face. The displacements
are formulated as a matrix of vectors T A ∈ RW ×H×2,
where W and H are the width and height of the images.
TCAE generates the AU-changed face by sampling the pix-
els from the source image. T A
xy = (δx, δy) is the vector at
position (x, y) and it means the offset of the pixel location
(x, y) in the source face. That is, the pixel at location (x, y)
in the source face is moved to the location (x+δx, y+δy) in
the AU-changed face. For the pixels that do not have corre-
sponding ones in the source face, we adopt bilinear interpo-
lation. Therefore, T A serves as an operator T A : Is (cid:3)→ IA
which transforms the source face Is into the AU-changed
face IA pixel by pixel. Similar to T A, we get the pose-
related displacements T p from the decoder Dp.

To distinguish the displacements caused by the change
of AU and poses, we add the L1 regularization on T A to
keep the AU-related movements sparse and subtle, which is
formulated as:

1 = (cid:2)
LA

x,y

||T A

xy||1,

(1)

where x,y enumerate all the locations in the face images.
T A should be sparse because facial actions are the move-
ments of one or a group of muscles and they only lead to
regional changes of the face pixels. Minimizing the L1-
norm of T A also enforces the AU-related movements to be
subtle. Facial actions appears as motions of smaller range
than that of head motions. Therefore, the absolute values of
T A’s elements should be smaller than those in T p.

3.2. Target reconstruction

To ensure that the decoded movements can represent the
changing from the source image to the target, we integrate
the displacements T A and T p, and then use the integrated
displacements to generate a target image from the source.

TCAE integrates T A and T p by linearly combining each
of their elements. Each element of the integrated displace-
ments T at location (x, y) is computed by:
xyT p
xy,

Txy = αA

xy + αp

xyT A

xy, T p

where Txy, T A
xy ∈ R2 are vectors denoting the offsets
of the pixel at location (x, y) in the source face. αA
xy and
xy are scalers that weighing the contributions of T A
αp
xy and
T p
xy, respectively. They satisfy that αA
xy = 1.
In
the images of size W × H, the weights for all the locations
compose an attention mask AA ∈ RW ×H or Ap ∈ RW ×H .
The masks are the outputs of decoder DA and Dp, indicat-
ing where the AU or pose is changing.

xy + αp

The integrated T serves as a transforming operator that
maps the source image Is to the target It. Thus, we require
the reconstructed target image to be similar to the original
one by minimizing their discrepancy. We formulate the re-
construction loss as :

Lrec = ||T (Is) − It||1,

(2)

where T (Is) ∈ RW ×H×3 denotes the generated RGB im-
ages from the source face. Is and It are the images of the
source and the target face images, respectively.

3.3. Twin cycles with AU or pose changed

The quality of the generated AU-changed and pose-
changed face images implies how well the movements are
disentangled. Since we have no pixel-wise or label level su-
pervisions for the generated face, we exploit the property
that the transformation should be “cycle consistent” [48].

TCAE includes two symmetric cycles. In one of them,
a face image is generated by changing the facial actions of
the source. Then, the AU-changed face is transformed back
to the source. In the other, we generate a pose-changed face
image and then transform it back to the source.

Fig. 2 illustrates the details in the cycles. Let us take the
cycle with AU changed as the example. After we get the
AU-related displacements T A in the feature disentangling
part, we use them to generate the AU-changed face image
T A(Is) from the source. We extract the AU and pose fea-
tures of the AU-changed face image using the same encoder
E in the feature disentangling part. Then, the AU features
of the source and the AU-changed face images are concate-
nated and fed into the AU-related decoder DA. By DA,
we get the displacements T −A which change the facial ac-
tions in AU-changed face to those in the source. It is noting
that the only differences between the source image Is and
the AU-changed one T A(Is) are the facial actions. If we
change the facial actions of T A(Is) using T −A, the new
image T −A (cid:3)T A(Is)(cid:4) should be similar to Is. Therefore,
mize the pixel discrepancy between T −A (cid:3)T A(Is)(cid:4) and Is

we formulate a cycle consistent reconstruction loss to mini-

as:

cyc = ||T −A (cid:3)T A(Is)(cid:4) − Is||1.
LA

(3)

Similarly, in the cycle with pose changed, we get the dis-
placements T −p that change the pose in pose-changed face

10927

T p(Is) to that in the source. we formulate the cycle consis-
tent reconstruction loss to minimize the pixel discrepancy
between T −p (T p(Is)) and Is as:

Lp

cyc = ||T −p (T p(Is)) − Is||1,

(4)

where T −p (T p(Is)) is the generated face image from the
pose-changed face using T −p.

Besides the pixel level consistency, we also exploit the
consistency within the embeddings. In the cycle with AU
changed, the AU-changed face image and the source image
are of the same pose. Thus, their pose embeddings should
be similar. Meanwhile, the AU-changed face image and the
target image are of the same facial actions. Thus, their AU
embeddings should be similar. We minimize the discrep-
ancy of the embeddings for AU-changed face image by

LA

emb = ||ψA

s − ψs||2 + ||φA

s − φt||2,

(5)

s and φA

where ψA
s are the pose embeddings and AU em-
beddings of the AU-changed face image, respectively. ψs is
the pose embeddings of the source image and φt is the AU
embeddings of the target image.

Similarly, in the cycle with pose changed, the pose-
changed face image should have similar AU embeddings
to the source image, and have similar pose embeddings to
the target image. The consistency of the embeddings are
constrained by

Lp

emb = ||φp

s − φs||2 + ||ψp

s − ψt||2,

(6)

s and ψp

where φp
s are the AU embeddings and pose embed-
dings of the pose-changed face image, respectively. φs is
the AU embeddings of the source image and ψt is the pose
embeddings of the target image.

4. Experimental results

In this section, we validated the effectiveness of the pro-
posed TCAE. First, we compared TCAE with other self-
supervised methods, descriptors, and supervised AU detec-
tion methods on three AU datasets. Then, we analyzed the
generated face images and displacements.

4.1. Implementation details

Detailed structures of the encoder and decoders:
Fig. 3 (a) illustrates the encoder used in our experiments.
It contains a backbone network followed by two parallel
branches. The backbone is shared because the features from
early layers are usually general. The encoder takes an RGB
image in size of 256×256 as the input and outputs two 256-
dimensional embeddings that represent AU and head pose,
respectively. Fig. 3 (b) shows the decoder which contains
eight blocks. In each block, an upsampling layer is placed
before the convolution layer to double the width and height

 

 

 

3
x
6
5
2

 

 

x
6
5
2

 

 

 

 

 

 

2
3

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

4
6

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

8
2
1

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

 

 

 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

 

 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

6
5
2

 
,

v
n
o
c
 
4
x
4

 

u
l
e
R

 
,

N
B

 
 

6
5
2

 
,

v
n
o
c
 
4
x
4

 

g
n
i
d
d
e
b
m
e
 
U
A

 

g
n
i
d
d
e
b
m
e
 
e
s
o
p

 

 

6
5
2
x
1
x
1

 

 

 

 

6
5
2
x
1
x
1

 

 

 

(a) Encoder.  In each conv, stride is 2, pad is 1. 

 

 

 

 

 

 

 

2
1
5
x
1
x
1

 

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

 

2
3

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

 

4
6

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

8
2
1

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

6
5
2

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

6
5
2

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

6
5
2

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

6
5
2

 
,

v
n
o
c
 
3
x
3

 

u
l
e
R

 
,

N
B

 

 
e
g
a
m

i
 
t
u
p
n
i

 

g
n
i
d
d
e
b
m
e
 
d
e
t
a
n
e
t
a
c
n
o
c

(b) Decoder. In each conv, stride is 1, pad is 1. 

 

 

g
n
i
l
p
m
a
s
 
p
u

 

3

 
,

v
n
o
 
c
 
3
x
3

 

h
n
a
t

 
s
t
n
e
m
e
c
a
l
p
s
i
d

 

 

k
s
a
m
n
o
i
t
n
e
t
t
a

 

 

 

2
x
6
5
2
x
6
5
2

 

 

 

 

 

1
x
6
5
2
x
6
5
2

 

 

Figure 3. Structure of the encoder (top) and decoders (bottom) in
TCAE. BN denotes batch normalization.

of the input feature maps. The last block uses tanh as the
activation function. The decoder outputs a three-channel
feature map with in the size of 256 × 256 × 3. The ﬁrst two
channels serve as the displacements, and the third serves as
the attention mask. It is noting that the en/de-coder can be
with any suitable blocks (e.g., residual blocks) rather than
the vanilla convolutional blocks in our experiments.

1

λ2

cyc + Lp

Training of TCAE: TCAE was trained end-to-end by
minimizing the combination of losses in Eq. (1)∼(6). The
1 +
full loss is formulated as L =
cyc) + λ3
W ×H×3 (LA
emb), where W and
H denote the width and height of the image. The weights
λ1, λ2 and λ3 are set as 0.01, 0.1, 0.1. We implemented
TCAE1 using PyTorch [30] and optimized it by SGD with
an initial learning rate of 0.001, and batch size of 64. It took
around 1000 epochs to reach the convergency.

W ×H×3 Lrec + λ1
emb + Lp

W ×H×2 LA

256 (LA

TCAE was trained on the union of VoxCeleb1 [28] and
VoxCeleb2 [6] datasets. The two datasets consist of videos
of interviews containing around 7,000 subjects. The iden-
tities were randomly split as train/val/test with percentages
of 75/15/10. During training, TCAE was fed with face pairs
that were randomly sampled from a video in the merged
VoxCeleb. The faces were detected by Cadcade-CNN [20]
and aligned according to the facial landmarks[16]. Each
face was cropped to a 256 × 256 image.

We adopted the curriculum learning [2] strategy to train
the model with progressively difﬁcult samples, because the
randomly sampled image pairs may contain large devia-
tions, which are too challenging to learn. Given a batch
of image pairs, TCAE executed a forward pass to obtain
the loss for each sample. The samples were sorted by their
losses in an ascending order. Then the loss L was only back-
propagated to the samples ranking in the top 50% within the
batch in the beginning. It was back-propagated to the sam-
ples ranking between top 10% and 60% when L on the val-
idation set saturated, and to the ones ranking between top
20% and 70% when another saturation reached. We kept

1Code available at https://github.com/mysee1989/TCAE

10928

Methods/AU

1

2

Table 1. F1 on BP4D dataset.
10

4

6

7

12

14

15

17

23

24

ave

Descriptor

Handcrafted [40]*

ResNet-80 face
VGG emotion

43.4 40.7 43.3 59.2
39.3 40.6 38.5 64.2
46.4 36.3 49.6 76.0

Supervised

Self-

supervised

AlexNet [5]*
DRML [47]*
EAC-Net [22]*

ROI [21]*

JAA-Net [32]*

SplitBrain [41]
DeformAE [33]

Fab-Net [38]
TCAE (ours)

40.3 39.0 41.7 62.8
36.4 41.8 43.0 55.0
39.0 35.2 48.6 76.1
36.2 31.6 43.4 77.1
47.2 44.0 54.9 77.5

39.0 32.0 39.7 72.9
39.5 34.5 40.8 70.5
43.3 35.7 41.6 72.9
43.1 32.2 44.4 75.1

61.3
67.5
77.6

54.2
67.0
72.9
73.7
74.6

70.6
68.4
63.0
70.5

62.1 68.5 52.5 36.7 54.3 39.5 37.8 50.0
71.0 65.3 57.2 37.8 51.3 35.1 32.6 49.9
80.2 87.8 60.8 40.4 59.1 43.7 48.2 58.8

75.1 78.1 44.7 32.9 47.3 27.3 40.1 48.6
66.3 65.8 54.1 33.2 48.0 31.7 30.0 48.3
81.9 86.2 58.8 37.5 59.1 35.9 35.8 55.9
85.0 87.0 62.6 45.7 58.0 38.3 37.4 56.4
84.0 86.9 61.9 43.6 60.3 42.7 41.9 60.0

78.2 83.7 57.8 37.3 53.6 32.3 45.1 53.5
76.3 82.9 60.7 23.1 54.1 34.3 43.1 52.3
75.9 83.5 57.7 26.5 48.2 33.6 42.4 52.0
80.8 85.5 61.8 34.7 58.5 37.2 48.7 56.1

* means that the values are reported in the original papers.

Descriptor

Supervised

Self-

supervised

Methods/AU

ResNet-80 face
VGG emotion

DRML [47]*
EAC-Net [22]*
JAA-Net [32]*

SplitBrain [41]
DeformAE [33]

Fab-Net [38]
TCAE (ours)

Table 2. F1 on DISFA dataset.
6

1

2

4

24.9 17.9 49.5 41.2
35.5 25.5 58.1 53.8

17.3 17.7 37.4 29.0
41.5 26.4 66.4 50.7
43.7 46.2 56.0 41.4

13.1 10.6 35.7 40.2
17.6 12.3 46.7 43.5
15.5 16.2 43.2 50.4
15.1 15.2 50.5 48.7

9

12

25

26

ave

26.2
32.4

10.7
80.5
44.7

30.2
26.0
23.2
23.3

48.6 56.4 32.8 37.2
74.4 79.0 55.7 51.8

37.7 38.5 20.1 26.7
89.3 88.9 15.6 48.5
69.6 88.3 58.4 56.0

57.5 77.4 40.3 38.1
62.7 64.8 47.6 40.1
69.6 72.4 42.4 41.6
72.1 82.1 52.9 45.0

* means that the values are reported in the original papers.

changing the samples until the max iteration.

Evaluation protocols: After the training process, we
obtained the encoder for AU detection. We trained a lin-
ear AU classiﬁer from the learned embedding. The lin-
ear classiﬁer consists of two layers: a batch-norm layer
followed by a linear fully connected layer with no bias.
The linear classiﬁer was trained with a binary cross entropy
loss for each AU. As all the AU datasets are highly imbal-
anced, the samples from the under-represented categories
were reweighed inversely proportionally to the class fre-
quencies. We adopted F1 score (F 1 = 2RP
R+P ) to evaluate
the performance of the method, where R and P denote re-
call and precision, respectively. We also computed the aver-
age over all AUs (ave) to measure the overall performance.
Evaluation datasets: We evaluated the methods on
BP4D [42], GFT [13] and DISFA [25] datasets. BP4D con-
tains 41 participants (23 females and 18 males). There are
about 146000 frames with available AU labels. DISFA con-
sists of 26 participants. The AUs are labelled with intensi-
ties from 0 to 5. The frames with intensities greater than
1 were considered as positive, while others were treated as
negative. We totally obtained about 130,000 AU-labelled

frames. GFT contains 96 participants in 32 three-person
groups. The moderate out-of-plane head motion and occlu-
sion make AU detection challenging. For BP4D and DISFA
dataset, we split the dataset into 3 folds based on subject IDs
and conducted a 3-fold cross-validation. We used 12 AUs
in BP4D dataset and and 8 AUs in DISFA dataset for eval-
uation. For GFT dataset, we followed the original train/test
splits in [13] (about 108000 facial images for training and
24600 images for evaluation) and used totally 10 AUs for
evaluation.

4.2. Comparisons with other methods

We compared TCAE with the state-of-the-art self-
supervised methods, typical descriptors, and supervised AU
detection methods. Table 1, 2, 3 report the F1-score of the
methods on BP4D, DISFA, and GFT datasets.

Comparison with other self-supervised methods:
The TCAE was compared with the state-of-the-art self-
supervised learning methods: SplitBrain [41], Deformin-
gAE [33], Fab-Net [38]. We re-trained the three models on
the merged VoxCeleb dataset using the released codes. In
SplitBrain [41], we used the down-sampled output of conv3

10929

Methods/AU

1

2

4

6

10

12

14

15

23

24

ave

Table 3. F1 on GFT dataset.

Descriptor

Handcrafted [13]*

ResNet-50 face
VGG emotion

AlexNet [13]*

Supervised

ResNet-50

Self-

supervised

SplitBrain [41]
DeformAE [33]

Fab-Net [38]
TCAE (ours)

13

32

38
42.9
24.3 50.7 18.2 39.9 44.7 41.6 17.4 27.8 31.0 25.9 32.2
23.9 40.6 26.4 73.6 69.3 74.4 21.1 24.9 26.0 20.2 40.0

67

64

78

15

29

49

44

46

2
44
23.5 37.8 3.5

19.0 40.6 8.7
17.3 40.1 4.8
44.4 42.3 9.4
43.9 49.5 6.3

72

42.8
73
79.1 70.1 82.1 20.9 11.7 49.1 40.3 41.8

82

19

43

42

5

26.7 22.9 32.3 35.8
60.2 66.6 75.4 5.6
25.2 31.2
3.9
64.1 69.1 72.1 7.8
1.7
20.8 33.3
60.6 68.7 70.4 8.7
71.0 76.2 79.5 10.7 28.5 34.5 41.7 44.2

8.0
5.5

AU displacements  AU-changed 

AU displacements  AU-changed 

* means that the values are reported in the original papers.

(cid:639)(cid:3)

source 

target 

source 

pose displacements 

pose-changed 

pose displacements 

pose-changed 

(a) 

(b) 

AU displacements  AU-changed 

AU displacements  AU-changed 

(cid:639)(cid:3)

source 

target 

source 

pose displacements 

pose-changed 

(c) 

pose displacements 

pose-changed 

(d) 

(cid:639)(cid:3)

target 

(cid:639)(cid:3)

target 

Figure 4. Visualizations of the displacements and the generated face images. The source is transformed to the AU-changed and pose-
changed face images through the AU displacements and pose displacements respectively. AU-changed face image should has the same
AUs as the target and the same pose as the source. Pose-changed face image should has the same pose as the target and the same AUs as
the source. Better viewed in color.

layer as the feature. In DeformAE [33], we changed the in-
put size to 256 × 256 and used the 512 dimensional latent
representation as the feature. In Fab-Net, we followed the
settings in [38]. All the features were used in training an
AU detector as they were used in TCAE.

As shown in table 1, 2, 3, TCAE outperforms other self-
supervised methods in the average F1 score. Because the
decoupled AU representation can better reﬂect the facial ac-
tions. The advantage of TCAE is the most obvious on GFT
dataset. It suggests that the decoupled AU representation
is robust against head pose variation, while the entangled
representation in [38, 33] is vulnerable to the head pose.

Although TCAE achieved the best average F1 score,
it had inconsistent performance on different AUs. TCAE
failed on AU9 because AU9 is nose wrinkle and it can-
not be generated by moving pixels in the faces without

AU9. TCAE showed its success on AU6 (cheek raiser),
AU7 (lid tightener), AU10 (upper lip raiser), AU12 (lip cor-
ner puller), AU23 (lip tightener), AU24 (lip pressor), AU25
(lips part) etc. It is because these AUs can be easily gener-
ated by moving pixels in the source face.

Comparison with other descriptors: We compared
TCAE with the handcrafted features [40, 13], face and emo-
tion descriptors. The face descriptor was extracted from a
80 layer ResNet [15] trained on MS-Celeb-1M dataset [14]
for face recognition. The emotion descriptor was extracted
from a VGG-16 net trained on AffectNet dataset [27] for
emotion classiﬁcation.

The TCAE-learned AU representation outperforms the
handcrafted features because the handcrafted features are
general and are not specially designed for AU detection.
TCAE also outperforms the face descriptor because the face

10930

preserve the poses in the source but have similar facial ac-
tions to the targets, e.g., the open mouth in (a), the stretched
mouth in (b), the lower lip corner in (c), and the closed eyes
in (d). Meanwhile, the pose-changed face images preserve
the facial actions in the source but have similar poses to the
targets. Despite some defections, the generated target face
images look similar to the real targets. It indicates that the
accumulative effect of the AU and pose displacements com-
pose the changing between the source and the target. All
the generated faces look like real ones, thus TCAE shows
its potential applications in editing faces.

xi , T A

i = (cid:5)(T A

Displacements: We analyse the displacements both
quantitatively and qualitatively. Fig. 4 visualizes the AU
and pose displacements. The pose displacements are in
nearly homogeneous directions while the AU displacements
have diverse directions, because pose displacements reﬂect
the rigid motion of the head while AU displacements reﬂect
the non-rigid motion of facial muscles. We randomly sam-
pled 6400 image pairs and calculated the per vector length
in AU displacements as T A
yi )2, where
(T A
yi ) denotes the AU offset at position (xi, yi). The
AU displacements between a image pair were averaged as
LA
, where N = W × H denotes prod-
uct of the image width and height. Lp
ave was calculated in a
similar manner. Fig. 5 plots the histogram of LA
ave
from the overall image pairs. As can be seen, the AU dis-
placements are shorter in the average length than the pose
displacements. The overall average length of the pose dis-
placements is 0.044. It is ﬁve times larger than that of the
AU displacements, which is 0.008. The reason is that AUs
are local movements within the face but head motions are
relatively large and global movements.

xi )2 + (T A

N (cid:6)N

ave and Lp

ave = 1

i=1 T A

i

5. Conclusion

This paper presented a Twin-Cycle Autoencoder
(TCAE) to learn discriminative representations for AU de-
tection in a self-supervised manner. TCAE successfully dis-
entangled the AU representation from the poses by factoriz-
ing the movement between two faces into the AU-related
and pose-related displacements. The decoupled AU rep-
resentation is discriminative for AU detection. Extensive
experiments demonstrated that TCAE outperformed or was
comparable with the state-of-the-art self-supervised learn-
ing methods and supervised AU detection methods. The
proposed TCAE can be further used in editing face or de-
coupling other factors.
Acknowledgement:

This work is partially sup-
ported by National Key R&D Program of China (No.
2017YFA0700800) and National Natural Science Founda-
tion of China (No. 61702481). We also thank Zijia Lu for
discussion. J. Zeng, S. Shan, and X. Chen are the corre-
sponding co-authors of this paper.

10931

Figure 5. Histogram of the average length of displacements for AU
and pose displacements.

descriptor was optimized to be expression invariant. On
GFT dataset, TCAE outperforms the emotion descriptor.
But TCAE lags behind the emotion descriptor on BP4D and
DISFA datasets. It is because the emotion descriptor is emo-
tion discriminative and emotions are tightly correlated with
AUs, e.g., activated AU1 (inner brow raiser) often appears
in facial expression images labelled with fear or surprise.
Activated AU4 (brow lower) is a partial indicator of anger
and sadness. Thus, emotion descriptor is also AU discrimi-
native. However, the emotion descriptor depends on a large
amount of labelled data. It is not easy to annotate emotions.
Comparison with supervised methods: We compared
TCAE with the state-of-the-art AU detection methods, in-
cluding DRML [47], EAC-Net [22], ROI [21], JAA-Net
[32]. For a fair comparison, TCAE was evaluated under
the same protocol as the ones in [47, 22, 21, 32]. On GFT
dataset, we trained a 50-layer ResNet [15] from scratch .

TCAE is comparable to supervised methods. It outper-
forms AlexNet [5], DRML [47], EAC-Net [22] on BP4D
dataset, and outperforms DRML [47] on DISFA dataset.
On GFT dataset, TCAE outperforms the supervised meth-
ods [13] with no exceptions. TCAE is comparable with ROI
[21] on BP4D dataset. TCAE lags behind JAA-Net [32] and
EAC-Net [22] on DISFA dataset, which both adopt facial
landmarks to learn region speciﬁc representations.

4.3. Analysis

To investigate how well the movements are disentan-
gled, we visualized and analyzed the generated faces and
the learned displacements.

Generated face images: Fig. 4 visualizes the gener-
ated faces and learned displacements during the training of
TCAE. TCAE shows its capability in separating the changes
caused by facial actions and head motions, as it generated
reasonable AU-changed and pose-changed face images. As
can be seen in each sub-ﬁgure, the AU-changed face images

References

[1] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning

to see by moving. In CVPR, 2015.

[2] Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Ja-

son Weston. Curriculum learning. In ICML. ACM, 2009.

[3] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel.
Infogan: Interpretable rep-
resentation learning by information maximizing generative
adversarial nets. In NIPS, 2016.

[4] Wen-Sheng Chu, Fernando De la Torre, and Jeffery F Cohn.
Selective transfer machine for personalized facial action unit
detection. In CVPR, 2013.

[5] Wen-Sheng Chu, Fernando De la Torre, and Jeffrey F Cohn.
Learning spatial and temporal cues for multi-label facial ac-
tion unit detection. In FG. IEEE, 2017.

[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.
Voxceleb2: Deep speaker recognition. In INTERSPEECH,
2018.

[7] Emily L Denton et al. Unsupervised learning of disentangled

representations from video. In NIPS, 2017.

[8] Paul Ekman and Wallace V Friesen. Manual for the facial
action coding system. Consulting Psychologists Press, 1978.
[9] Stefanos Eleftheriadis, Ognjen Rudovic, and Maja Pantic.
Multi-conditional latent variable model for joint facial action
unit detection. In CVPR, 2015.

[10] C Fabian Benitez-Quiroz, Ramprakash Srinivasan, and
Aleix M Martinez. Emotionet: An accurate, real-time al-
gorithm for the automatic annotation of a million facial ex-
pressions in the wild. In CVPR, 2016.

[11] Basura Fernando, Hakan Bilen, Efstratios Gavves, and
Stephen Gould. Self-supervised video representation learn-
ing with odd-one-out networks. In CVPR. IEEE, 2017.

[12] Chuang Gan, Boqing Gong, Kun Liu, Hao Su, and
Leonidas J Guibas. Geometry guided convolutional neural
networks for self-supervised video representation learning.
In CVPR, 2018.

[13] Jeffrey M Girard, Wen-Sheng Chu, L´aszl´o A Jeni, and Jef-
frey F Cohn. Sayette group formation task (gft) spontaneous
facial expression database.
In FG, pages 581–588. IEEE,
2017.

[14] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for
large-scale face recognition. In ECCV. Springer, 2016.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[16] Zhenliang He, Jie Zhang, Meina Kan, Shiguang Shan, and
Xilin Chen. Robust fec-cnn: A high accuracy facial land-
mark detection system. In CVPRW, 2017.

[17] Dinesh Jayaraman and Kristen Grauman. Learning image
representations tied to egomotion from unlabeled video. In-
ternational Journal of Computer Vision, 125(1-3):136–161,
2017.

[19] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. In ECCV, 2018.
[20] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and
Gang Hua. A convolutional neural network cascade for face
detection. In CVPR, 2015.

[21] Wei Li, Farnaz Abtahi, and Zhigang Zhu. Action unit de-
tection with region adaptation, multi-labeling learning and
optimal temporal fusing. In CVPR. IEEE, 2017.

[22] Wei Li, Farnaz Abtahi, Zhigang Zhu, and Lijun Yin. Eac-
net: A region-based deep enhancing and cropping approach
for facial action unit detection. In FG, 2017.

[23] Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih,
Zara Ambadar, and Iain Matthews. The extended cohn-
kanade dataset (ck+): A complete dataset for action unit and
emotion-speciﬁed expression. In CVPRW. IEEE, 2010.

[24] Brais Martinez, Michel F Valstar, Bihan Jiang, and Maja
Pantic. Automatic analysis of facial actions: A survey. IEEE
Transactions on Affective Computing, 2017.

[25] S Mohammad Mavadati, Mohammad H Mahoor, Kevin
Bartlett, Philip Trinh, and Jeffrey F Cohn. Disfa: A spon-
taneous facial action intensity database. IEEE Transactions
on Affective Computing, 4(2):151–160, 2013.

[26] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-
ﬂe and learn: unsupervised learning using temporal order
veriﬁcation. In ECCV. Springer, 2016.

[27] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-
hoor. Affectnet: A database for facial expression, valence,
and arousal computing in the wild.
IEEE Transactions on
Affective Computing, 2017.

[28] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman.
Voxceleb: a large-scale speaker identiﬁcation dataset. In IN-
TERSPEECH, 2017.

[29] Joe Yue-Hei Ng, Jonghyun Choi, Jan Neumann, and Larry S
Davis. Actionﬂownet: Learning motion representation for
action recognition. In WACV). IEEE, 2018.

[30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[31] Guozhu Peng and Shangfei Wang. Weakly supervised fa-
cial action unit recognition through adversarial training. In
CVPR, pages 2188–2196, 2018.

[32] Zhiwen Shao, Zhilei Liu, Jianfei Cai, and Lizhuang Ma.
Deep adaptive attention for joint facial action unit detection
and face alignment. ECCV, 2018.

[33] Zhixin Shu, Mihir Sahasrabudhe, Alp Guler, Dimitris Sama-
ras, Nikos Paragios, and Iasonas Kokkinos. Deforming au-
toencoders: Unsupervised disentangling of shape and ap-
pearance. In ECCV, 2018.

[34] Sima Taheri, Qiang Qiu, and Rama Chellappa. Structure-
preserving sparse decomposition for facial expression anal-
ysis. IEEE Transactions on Image Processing, 23(8):3590–
3603, 2014.

[18] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-
Hsuan Yang. Unsupervised representation learning by sort-
ing sequences. In ICCV. IEEE, 2017.

[35] Michel Valstar and Maja Pantic. Fully automatic facial ac-
tion unit detection and temporal analysis. In CVPRW. IEEE,
2006.

10932

[36] Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense
optical ﬂow prediction from a static image. In CVPR, 2015.
[37] Ziheng Wang, Yongqiang Li, Shangfei Wang, and Qiang Ji.
Capturing global semantic relationships for facial action unit
recognition. In ICCV, 2013.

[38] Olivia Wiles, A Koepke, and Andrew Zisserman. Self-
supervised learning of a facial attribute embedding from
video. BMVC, 2018.

[39] Olivia Wiles, A Sophia Koepke, and Andrew Zisserman.
X2face: A network for controlling face generation using im-
ages, audio, and pose codes. In ECCV, 2018.

[40] Jiabei Zeng, Wen-Sheng Chu, Fernando De la Torre, Jef-
frey F Cohn, and Zhang Xiong. Conﬁdence preserving ma-
chine for facial action unit detection. In CVPR, 2015.

[41] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain
autoencoders: Unsupervised learning by cross-channel pre-
diction. In CVPR, 2017.

[42] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Cana-
van, Michael Reale, Andy Horowitz, and Peng Liu. A
high-resolution spontaneous 3d dynamic facial expression
database. In FG. IEEE, 2013.

[43] Yong Zhang, Weiming Dong, Bao-Gang Hu, and Qiang Ji.
Weakly-supervised deep convolutional neural network learn-
ing for facial action unit intensity estimation. In CVPR, 2018.
[44] Guoying Zhao and Matti Pietikainen. Dynamic texture
recognition using local binary patterns with an application
to facial expressions. IEEE transactions on pattern analysis
and machine intelligence, 29(6):915–928, 2007.

[45] Kaili Zhao, Wen-Sheng Chu, Fernando De la Torre, Jeffrey F
Joint patch and multi-label

Cohn, and Honggang Zhang.
learning for facial action unit detection. In CVPR, 2015.

[46] Kaili Zhao, Wen-Sheng Chu, and Aleix M Martinez. Learn-
ing facial action units from web images with scalable weakly
supervised clustering. In CVPR, pages 2090–2099, 2018.

[47] Kaili Zhao, Wen-Sheng Chu, and Honggang Zhang. Deep
region and multi-label learning for facial action unit detec-
tion. In CVPR, pages 3391–3399, 2016.

[48] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In CVPR. IEEE, 2017.

10933

