Re-ranking via Metric Fusion for Object Retrieval and Person Re-identiﬁcation

Song Bai1 Peng Tang2 Philip H.S. Torr1 Longin Jan Latecki3

1University of Oxford 2Huazhong University of Science and Technology 3Temple University
{songbai.site,tangpeng723}@gmail.com, philip.torr@eng.ox.ac.uk, latecki@temple.edu

Abstract

This work studies the unsupervised re-ranking procedure
for object retrieval and person re-identiﬁcation with a spe-
ciﬁc concentration on an ensemble of multiple metrics (or
similarities). While the re-ranking step is involved by run-
ning a diffusion process on the underlying data manifolds,
the fusion step can leverage the complementarity of multiple
metrics.

We give a comprehensive summary of existing fusion
with diffusion strategies, and systematically analyze their
pros and cons. Based on the analysis, we propose a uni-
ﬁed yet robust algorithm which inherits their advantages
and discards their disadvantages. Hence, we call it Uni-
ﬁed Ensemble Diffusion (UED). More interestingly, we de-
rive that the inherited properties indeed stem from a the-
oretical framework, where the relevant works can be ele-
gantly summarized as special cases of UED by imposing
additional constraints on the objective function and vary-
ing the solver of similarity propagation. Extensive experi-
ments with 3D shape retrieval, image retrieval and person
re-identiﬁcation demonstrate that the proposed framework
outperforms the state of the arts, and at the same time sug-
gest that re-ranking via metric fusion is a promising tool to
further improve the retrieval performance of existing algo-
rithms.

1. Introduction

Due to the advance in the acquisition, storage, and shar-
ing of visual content, the image and multimedia collec-
tions have shown a continuous and consistent growth, both
in scope and diversity. Consequently, the development of
methods for indexing and retrieving such information has
become essential. Given a query instance, the goal of visual
retrieval is to ﬁnd objects sharing similar visual appearances
with the query in a large database. Therefore, a reliable
metric (or similarity) function is vital to the retrieval perfor-
mance.

However, traditional object retrieval systems perform
only pairwise comparisons, i.e., computing distance (or

similarity) measures between object pairs and ignoring the
contextual information encoded in the relationships among
objects. To address this issue, re-ranking approaches [33,
4, 5, 21] have been proposed for the sake of reﬁning
the retrieval results without the need of user intervention.
Such methods (e.g., manifold ranking [71], diffusion pro-
cess [10]) replace the pairwise distances by more global
similarity measures, capable of analyzing data collections
more globally and taking into account the underlying man-
ifold structure to reveal the intrinsic relationship between
objects.

Meanwhile, with the long-standing development of fea-
ture learning, plenty of visual descriptors have been pro-
posed, from the conventional handcrafted ones [56, 55, 45,
29] to deep-learned ones [11, 60, 66, 25]. Different visual
descriptors generally focus on different visual characteris-
tics of objects. As a result, signiﬁcant efforts [36] have been
devoted recently to metric fusion to leverage the comple-
mentary nature. Generally, metric (or similarity) fusion can
be done in any stage of a typical retrieval pipeline (e.g., fea-
ture learning stage [37], indexing stage [48, 65, 39]). In this
work, we consider metric fusion in the re-ranking stage, par-
ticularly diffusion process [10], to capture the geometrical
structure of multiple data manifolds.

Existing fusion with diffusion methods can be coarsely
divided into three categories. Naive Fusion (NF) sim-
ply averages the edge weights of multiple afﬁnity graphs,
such as locally constrained mixed diffusion [32], graph fu-
sion [68, 67], and Yang et al. [61].
In order to combine
two distinct and complementary metrics, Tensor Product
Fusion (TPF) [72] considers a homogeneous fusion on a
tensor product graph. To handle noisy input metric, Reg-
ularized Ensemble Diffusion (RED) [7] performs similarity
learning and weight learning simultaneously to maximize
the smoothness of multiple graph-based manifolds.

As detailed in Sec. 2, NF is the fastest among these meth-
ods, but it is extremely susceptible to noisy similarities. By
contrast, TPF considers the interplay of two similarities, at-
taining robustness to noise to a certain extent. However, it
can only fuse two similarities each time. Although RED
can eliminate the inﬂuence of noises via a dynamic weight

740

learning mechanism, it is relatively computationally expen-
sive as the diffusion step must be done for each input indi-
vidually.

With these observations, we propose in this work a new
fusion with diffusion algorithm called Uniﬁed Ensemble
Diffusion (UED). The primary contributions of UED are
three folds:

1) UED combines the advantages of three existing types
of fusion with diffusion methods without inheriting
their drawbacks. In particular, UED is more robust to
noisy input than RED, since it considers the interplay
of two similarities as TPF does. Meanwhile, it can han-
dle more than two similarities, instead of merely two
in the case of TPF. Furthermore, the diffusion step of
UED can be executed much faster than RED, almost as
fast as naive fusion. We will demonstrate those proper-
ties both theoretically (see Sec. 3) and experimentally
(see Sec. 5).

2) More importantly, by deeply analyzing the relation-
ship between UED and existing methods, we observe
that the inherited properties indeed stem from a uniﬁed
framework, where all those methods can be summa-
rized as special cases of UED. The inherent differences
lie in the additional constraints on the objective func-
tion and the variation of similarity propagation (see
Sec. 4).

3) UED has undergone a careful design of formulation
and derivation. Unfortunately,
it becomes a non-
convex optimization, which is hard to solve. A by-
product contribution of our work is, for the ﬁrst time
to our knowledge, to introduce the replicator equa-
tion [40, 41] as a powerful optimizer to learn the metric
weights in the re-ranking stage.

Extensive experiments are conducted with 3D shape re-
trieval on the ModelNet40 [59] and ModelNet10 datasets,
image retrieval on the Holidays [22] and Ukbench [34]
datasets, and person re-identiﬁcation on the Market-1501
dataset [69].
The state-of-the-art performance ﬁrmly
demonstrates the effectiveness of the proposed framework.

2. Metric Fusion Revisited

Let G = {G1, G2, . . . , GM } be a multi-graph, where
Gµ = (X, Wµ) is the µ-th (1 ≤ µ ≤ M ) afﬁnity graph
parameterized by the µ-th metric (or similarity). The ver-
tex set X = {x1, x2, . . . , xN } denotes the objects and
Wµ ∈ RN ×N denotes the adjacency matrix with W µ
ij be-
ing the initial similarity between xi and xj associated with
the µ-th metric. Usually, a transition matrix is deﬁned via
Sµ = (Dµ)−1/2Wµ(Dµ)−1/2, where Dµ ∈ RN ×N is a

diagonal matrix with elements Dµ
ij . The ba-
sic objective is to learn a new similarity A ∈ RN ×N on
G in an unsupervised manner so that the indexed candidate
images for a given query (or probe) can be re-ranked.

ii = PN

j=1 W µ

To enable re-ranking, various methodologies can be
used, such as learning to rank [8], metric learning [36],
manifold ranking [71], etc. In this work, we consider a rep-
resentative branch called diffusion process [10] in retrieval,
upon which we build the fusion paradigm to integrate mul-
tiple metrics. Among the variants of diffusion process sum-
marized in [10], we select tensor product diffusion as the
backbone as it has been demonstrated [62] to be more ro-
bust in the scope of object retrieval.

2.1. Naive Fusion

Naive Fusion (NF) is a two-step solution:

Fusion Step. Simply average the multiple similarities to
generate the transition matrix as

S =

1
M

M

X

µ=1

Sµ.

(1)

Diffusion Step. Run a diffusion process with S to obtain
the target similarity A as

A(t+1) = αSA(t)ST + (1 − α)I,

(2)

where t is the number of iteration, α ∈ (0, 1) is a trade-
off parameter, and I ∈ RN ×N is the identity matrix. As
the transition matrix S is a symmetric matrix, we will inter-
changeably use S = ST subsequently.

It is proven [4, 5] that after a sufﬁcient number of itera-

tions, Eq. (2) converges to

A∗ = (1 − α)vec−1 (cid:0)(I − αS ⊗ S)−1vec(I)(cid:1) ,

(3)

where ⊗ denotes the Kronecker product, vec(·) is the vec-
torization of the input matrix by stacking its columns one
by one, and its inverse function is vec−1. To simplify the
notation, we will use ~Y = vec(Y) for any input matrix Y.

2.2. Tensor Product Fusion

Tensor Product Fusion (TPF) is a one-step solution:

Fusion with Diffusion Step. Simultaneously fuses two
metrics in one diffusion step. When fusing the µ-th and
the ν-th afﬁnity graph, it is deﬁned as

A(t+1) = αSν A(t)Sµ + (1 − α)I.

(4)

It is proven [72] that after a sufﬁcient number of itera-

tions, Eq. (4) converges to

A∗ = (1 − α)vec−1 (cid:16)(I − αSµ ⊗ Sν)−1~I)(cid:17) .

(5)

741

2.3. Regularized Ensemble Diffusion

Regularized Ensemble Diffusion (RED) [7] is a two-step

solution proposed recently:

Diffusion Step. Given β = {β1, β2, . . . , βM } with βµ (1 ≤
µ ≤ M ) being the weight of the µ-th afﬁnity graph, the
diffusion step of RED is deﬁned as

A(t+1) =

M

X

µ=1

αµSµA(t)Sµ + (1 −

M

X

µ=1

αµ)I,

(6)

where

αµ =

βµ
γ + PM
µ′=1 βµ′

.

(7)

Therein, γ > 0 is a small weight constant to ensure that the
state of convergence

A∗ = vec−1(cid:16)(1 −

can be obtained.

M

X

µ=1

αµ)(I −

M

X

µ=1

αµSµ ⊗ Sµ)−1~I(cid:17) (8)

Fusion Step. The vector with metric weight β is not deter-
mined empirically. RED can dynamically learn the metric
weights to amplify the contributions of discriminative afﬁn-
ity graphs and suppress those of noisy ones.

By initializing with equal weights 1

M , the weight β can
be optimized via coordinate descent.
It has been proven
that by alternating the diffusion step and the fusion step, an
optimal similarity A∗ and weight conﬁguration β can be
derived. Details can be found in [7].

2.4. Summary of Pros and Cons

The three existing types of fusion methods, including
Naive Fusion (NF), Tensor Product Fusion (TPF), and Reg-
ularized Ensemble Diffusion (RED), have different pros and
cons.

First, NF is the most efﬁcient one. As can be seen from
Eq. (1), NF conducts the fusion step of input similarities
ﬁrst, then the diffusion step is only executed once. How-
ever, it is quite vulnerable to noisy similarities as it weights
each input equally. As a consequence, when less discrim-
inative similarities exist, the retrieval performance of NF
may easily deteriorate.

Second, TPF considers the complementarity and the in-
terplay of two distinct similarities, as shown in Eq. (4). In
comparison, NF and RED both consider input similarities
individually, by simply averaging them with equal weights
(see Eq. (1)) or dynamic weights (see Eq. (6)). However,
one primary defect of TPF is that it can only tackle two in-
puts, limiting its promotion and usage where more than two
metrics are available.

learning paradigm to the diffusion step. However, as Eq. (6)
says, each diffusion step has to be done for each input sim-
ilarity individually. Hence, it is more computationally ex-
pensive although the scale of time complexity is the same
as NF and TPF. Interested readers can refer to [7] for more
detailed analysis.

To address the limitations of existing types of fusion
methods, we will present a novel method called Uniﬁed En-
semble Diffusion (UED) in Sec. 3 which inherits the advan-
tages of those methods. More interestingly, we theoretically
analyze in Sec. 4 that the inherited advantages stem from a
uniﬁed framework, where NF, TPF, and RED can be ele-
gantly summarized as special cases of UED.

3. Proposed Method

A pertinent suggestion of Uniﬁed Ensemble Diffusion
(UED) is to ﬁrst compute a weighted average of input simi-
larities as

S =

βµSµ,

(9)

M

X

µ=1

where the weight β = {β1, β2, . . . , βM } will be learned
afterwards. Although Eq. (9) appears to be a simple modi-
ﬁcation of naive fusion, we will demonstrate in this section
that it leads to some nice mathematical properties and prac-
tical beneﬁts (e.g., it allows us to consider the interplay of
all pairs of afﬁnity graphs), which constitutes the base for
the core contribution of this work in Sec. 4.

3.1. Objective Function

UED learns the target similarity A by solving the fol-

lowing optimization problem

βTHβ + γkA − IkF + ηkβk2
2,

min
A,β
s.t. β ∈ ∆ = {β ∈ RM ×1 : β ≥ 0, kβk1 = 1},

(10)

where matrix H ∈ RM ×M with its entries deﬁned as

H µν =

1
2

N

X

i,j,k,l=1

W µ
ij

W ν

kl(

Aki
pDµ

ii

Dν

kk

−

Alj
qDµ

jj

Dν
ll

)2

(11)

= ~A

T(I − Sµ ⊗ Sν ) ~A

measures the smoothness of A with respect to all the input
similarity pairs Wµ (1 ≤ µ ≤ M ) and Wν (1 ≤ ν ≤ M ).
kA − IkF computes the difference of A from the identity
matrix I, meaning that the self-similarity should be pre-
served with the weight γ > 0. kβk2
2 computes the squared
L2 norm of β, whose contribution to the overall loss is
weighted by η > 0 to avoid overﬁtting to a speciﬁc input.

3.2. Derivation

At last, among the three methods, RED is the most ro-
bust one to noisy similarities since it exerts a robust weight

As there are two variables to learn, i.e., the target sim-
ilarity A and the weight conﬁguration β, we decompose

742

Eq. (10) into two sub-problems, then adopt an alternating
manner to solve the optimization problem.

By applying vec(·) to its both sides and using the property
of Kronecker product, we have

Diffusion Step. When learning A, we ﬁx β. Consequently,
the third term in Eq. (10) is a constant and can be omitted.
Then, Eq. (10) is equivalent to

~A(t+1) =

1
Λ

M

X

µ,ν=1

βµβν(Sµ ⊗ Sν) ~A(t) +

~I.

γ
Λ

(19)

min

A

M

X

µ,ν=1

βµβν ~AT(I − Sµ ⊗ Sν) ~A + γk ~A − ~Ik2

2. (12)

By taking the partial derivative with respect to ~A, we obtain

2

M

X

µ,ν=1

βµβν(I − Sµ ⊗ Sν) ~A + 2γ( ~A − ~I).

(13)

By setting it to zero, we derive the closed-form solution

~A =

γ
Λ

(I −

1
Λ

where

Λ = γ +

M

X

µ,ν=1

M

X

µ,ν=1

βµβν Sµ ⊗ Sν)−1~I,

(14)

βµβν = γ + 1.

(15)

By applying vec−1 to both sides of Eq. (14), the optimal
solution A can be obtained.

To efﬁciently learn A in practice, we use an iteration-

based solver given as

A(t+1) =

1
Λ

(

M

X

ν=1

βν Sν)A(t)(

M

X

µ=1

βµSµ) +

γ
Λ

I.

(16)

By substituting Eq. (9) into Eq. (16), one can simplify it to

A(t+1) =

1
Λ

SA(t)S +

γ
Λ

I.

(17)

A key observation drawn from Eq. (17) is that UED
ﬁrstly computes a weighted average of multiple input sim-
ilarities and conducts one diffusion step in one trial. Com-
pared with NF (Eq. (2)), the diffusion step of UED is ad-
equately efﬁcient but less susceptible to noise owing to a
weight learning mechanism. Compared with RED deﬁned
in Eq. (6) which needs to conduct a diffusion step for each
input similarity individually, the diffusion step of UED is
more computationally efﬁcient because only one diffusion
step is enough for multiple input similarities.

Now, we prove the iteration in Eq. (16) can approximate
the closed-form solution in Eq. (14). Eq. (16) is equivalent
to

As proven in the supplementary material, Eq. (19) con-
verges to the closed-form solution in Eq. (14). To see this
directly, one could set ~A(t+1) = ~A(t) in Eq. (19). Then, the
solution would look like Eq. (14).

Fusion Step. When learning β, we ﬁx A. Consequently,
the second term in Eq. (10) is a constant and can be omitted.
Then, the objective function becomes

min

β

βTHβ + ηkβk2

2, s.t. β ∈ ∆,

(20)

which is an optimization of a quadratic function on the sim-
plex ∆. Unfortunately, Eq. (20) is not guaranteed to be a
convex optimization with respect to β, e.g., H + ηI is not
positive semi-deﬁnitive.

To address this issue, we prove that after some algebraic
transformations, a replicator equation [40, 41] can be used
to obtain a proper local maximizer of the following equiva-
lent objective function

βT ¯Hβ, s.t. β ∈ ∆,

max

β

(21)

where ¯H = −H/2 − HT/2 − ηI + C and C ∈ RM ×M is a
matrix with all its entries equal to the maximum element of
(H/2 + HT/2 + ηI). Due to the space limitation, the de-
tailed derivation is put in the supplementary material. Then,
Eq. (21) can be solved by using the replicator equation as

β(t+1) =

β(t) ⊙ ¯Hβ(t)
β(t)T ¯Hβ(t)

,

(22)

where t is the number of iteration and ⊙ denotes the
element-wise multiplication. Two conditions need to be sat-
isﬁed for the sake of the convergence of replicator equa-
tion [30]. First, ¯H is symmetric and all its entries are non-
negative, which can be simply obtained from the deﬁnition
of ¯H. Second, every trajectory staring in the simplex ∆ will
remain in the simplex. To this end, we need to prove the L1
norm of β(t+1) is always equal to 1. Equivalently, we need
to prove the L1 norm of the numerator of Eq. (22) is equal
to the denominator of Eq. (22). It holds, since

kβ(t) ⊙ ¯Hβ(t)k1 =

M

X

µ=1

β(t)
µ

M

X

ν=1

¯H µνβ(t)
ν

(23)

β(t)
µ

¯H µνβ(t)

ν = β(t)T ¯Hβ(t).

A(t+1) =

1
Λ

M

X

µ,ν=1

βνβµSν A(t)Sµ +

γ
Λ

I.

(18)

=

743

M

X

µ,ν=1

Algorithm 1: Uniﬁed Ensemble Diffusion

4.2. Variation of Iteration

Input:
M adjacency matrices {W µ}M
Output:
The target similarity A.
begin

µ=1 ∈ RN ×N , γ, η.

Initialize the weight βµ = 1
repeat

M , ∀µ.

Compute S using Eq. (9).
Update A using S and Eq. (17).
Compute H using Eq. (11).
Update β using H and Eq. (22).

until convergence
return A

We alternate the diffusion step and the fusion step. The
whole optimization is guaranteed to converge to an equi-
librium. The overall procedure is summarized in Alg. 1.
Comparing with the previous works, UED possesses some
nice properties, as we will state in Sec. 4.

Different regularizations on the simplex ∆o are sub-
jected to different
the
iteration-based solver of UED in Eq. (16) and Eq. (17).
Then, a uniﬁed framework can be built as follows.

iteration-based solver.

Recall

Naive Fusion. It is easy to show that with equal weights,
Eq. (17) degenerates to the diffusion step of NF in Eq. (2).
One subtle identity is needed for the equivalence, i.e., α =
1/Λ. According to the deﬁnition of Λ in Eq. (15), 1 − α =
γ/Λ.

Tensor Product Fusion. The similarity propagation in
Eq. (16) can be transformed into

(

M

X

ν=1

βν Sν)A(t)(

M

X

µ=1

βµSµ) =

M

µ=1

X
|

β2
µ

SµA(t)Sµ

+

{z

RED

}

M

µ6=ν

X
|

βµβν Sν A(t)Sµ

.

(28)

{z

TPF

}

4. A Uniﬁed Framework

In this section, we demonstrate that existing fusion meth-
ods can be summarized in a uniﬁed framework deﬁned by
the proposed Uniﬁed Ensemble Diffusion (UED).

4.1. Regularization on Simplex

By substituting the simplex in Eq. (26) into Eq. (28) and
selecting the µ-th and the ν-th afﬁnity graph, we can obtain
the fusion with diffusion step of TPF in Eq. (4) by deﬁning
α = 1/Λ.

Regularized Ensemble Diffusion. By substituting the sim-
plex in Eq. (27) into Eq. (28), Eq. (16) becomes

Recall the objective function of UED in Eq. (10), and a
uniﬁed framework can be built by imposing an additional
simplex ∆o. Then, the constraint becomes

A(t+1) =

1
Λ

M

X

µ=1

β2
µ

SµA(t)Sµ +

γ
Λ

I,

(29)

β ∈ ∆ ∩ ∆o,

(24)

which is the intersection of the original simplex ∆ of UED
and the additional simplex ∆o.

Naive Fusion sets ∆o to

∆o = {β : βµ =

1
M

, ∀µ},

(25)

which means that all input similarities have equal weights
and keep unchanged.

Tensor Product Fusion sets ∆o to

∆o = {β : if µ = ν, βµ = βν = 0; else = 1}

(26)

which means that only two different similarities are fused,
both having weight 1.

Regularized Ensemble Diffusion sets ∆o to

∆o = {β : βµβν = 0, ∀µ 6= ν},

(27)

which means that no interplay between two different simi-
larities are encouraged. All the input similarities are fused
individually.

which is equivalent to the diffusion step of Eq. (6) if consid-
ering β2

µ (1 ≤ µ ≤ M ) as the target weight to be learned.

Finally, it should be mentioned that the fusion step of

weight learning varies with different methods.

4.3. Summary of Main Contributions

As summarized in Sec. 2.4, existing fusion methods have
different pros and cons. In comparison, UED inherits the
advantages and discards the disadvantages with a delicate
design of objective function and derivation.

First, the diffusion step of UED is almost as fast as naive
fusion. As Eq. (9) shows, it can also merge multiple in-
put similarities in one trial, and does not need to exhaus-
tively apply diffusion step to each input as tensor product
fusion and regularized ensemble diffusion. Second, we can
draw from Eq. (28) that UED can also consider the inter-
play of two distinct afﬁnity graphs as tensor product fusion,
so that the complementarity between metrics can be better
exploited. More importantly, UED is not limited to only
fusing two inputs as tensor product fusion. Instead, it can
also tackle more than two input similarities as naive fusion

744

Baselines

ModelNet40

ModelNet10

B1
B2
B3
B4

AUC

77.19
80.12
80.39
45.10

mAP

76.52
79.41
79.53
44.52

AUC

88.97
89.02
91.24
62.37

mAP

87.98
88.17
89.97
61.47

Table 1. The performance (%) of four baselines on the Model-
Net40 and ModelNet10 dataset.

and regularized ensemble diffusion. Third, due to the dy-
namic weight learning paradigm, UED is robust to noisy
input similarities. Meanwhile, to tackle the non-convex op-
timization, we also introduce replicator equation as an ef-
fective optimizer for weight learning.

At last, we emphasize that UED is not merely an algo-
rithm about metric fusion in re-ranking. More importantly,
it can summarize existing methods in a uniﬁed framework
with a theoretically-sound explanation.

5. Experiments

In this section, we evaluate the proposed framework on
various retrieval tasks, including 3D shape retrieval, image
retrieval, and person re-identiﬁcation.

5.1. 3D Shape Retrieval

3D shape retrieval has been an important topic in 3D vi-
sion especially in recent years. The experimental compari-
son is done on the ModelNet dataset [59], which is a repre-
sentative large-scale 3D shape repository. The current ver-
sion of ModelNet consists of 151, 128 3D CAD models, di-
vided into 662 object categories. Following [54, 6], we use
two subsets to evaluate the retrieval performance, i.e., Mod-
elNet40, containing 12, 311 shapes in 40 object categories,
and ModelNet10, containing 4, 899 shapes in 10 object cat-
egories. We use the same training-testing split as in [6, 23,
54, 52, 17] and employ Area Under precision-recall Curve
(AUC) and mean Average Precision (mAP) as the evalua-
tion metrics.

Baselines. In order to ensure a fair comparison, we adopt
exactly the same four baseline similarity measures as in [7],
including GIFT [6], ResNet [16], Volumetric CNN [42],
and PANORAMA [37]. For the notation simpliﬁcation, we
denote them as B1, B2, B3, and B4, respectively. The base-
line performance is presented in Table 1.

Comparison with Fusion Methods. In Tables 2 and 3, we
compare the results of those fusion with diffusion methods
summarized in the proposed framework on the ModelNet40
and ModelNet10 datasets, respectively. As TPF can only
fuse two similarities each time, its results are given in a
range. The evaluation is done by fusing the 3-combination
of the similarity sets or all the four similarities.

As can be drawn from Table 2, the proposed UED ob-
tains the best performance in most similarity combinations
on the ModelNet40 dataset. For example, when fusing B2,
B3, and B4, UED reports AUC 88.05 and mAP 87.30. In
terms of AUC, the reported performance is better than RED
by 1.57, the best trial of TPF by 2.05, and NF by 3.41, re-
spectively.
In terms of mAP, UED outperforms RED by
1.59, the best trial of TPF by 2.18, and NF by 3.37, respec-
tively. It ﬁrmly testiﬁes that UED can inherit the merits of
existing fusion with diffusion methods to learn a more ro-
bust similarity.

An abnormal case arises when fusing B1, B2, and B3,
where UED only achieves AUC 87.27 and mAP 86.55, a
comparable performance with the best competitor NF. As
analyzed above, NF is vulnerable to noisy similarities. Nev-
ertheless, Table 1 presents that B1, B2, and B3 have very
similar performances, while the performance of B4 is much
inferior, indicating that much more noisy edges are involved
in the afﬁnity graph parameterized by B4. Therefore, when
B4 is involved, NF fails to work well due to the lack of
a weight learning mechanism to mitigate the negative inﬂu-
ence of noise. By contrast, combining B1, B2, and B3 using
equal weights is justiﬁed, and NF is a cheap solution in this
situation.

In Table 4, we present the weights learned by RED and
UED. In RED [7], the weight of B4 is set to 0 in order to
totally eliminate its negative contribution to the similarity
learning. However, in UED, the weight of B4 is 0.014, a
small but non-zero value. Such a difference originates from
the fact that RED fuses multiple similarities by consider-
ing each input similarity individually, while UED is able to
consider the interplay of two distinct similarities as shown
in Eq. (28). Even though B4 brings in more noisy edges,
it can still provide complementary information if integrated
with other heterogeneous similarities.

Comparison with State-of-the-arts. Table 5 gives a thor-
ough comparison with state-of-the-art methods on the Mod-
elNet dataset. The results are quoted from the leader-
board of ModelNet, available at http://modelnet.
cs.princeton.edu/.

As can be observed from the table, UED achieves the
best AUC and the second best mAP on both datasets. As a
view-based algorithm, SeqViews2SeqLabels [15] proposes
an encoder-decoder RNN structure with attention to aggre-
gate the sequential views and reports the best mAP 89.09
on the ModelNet40 dataset. Meanwhile, PANORAMA-
ENN [48] is an extension of PANORAMA-NN [49] which
uses the panoramic views for model training. It further ex-
ploits a new 3-channel schema representation and an en-
semble of multiple models, then achieves the best mAP
93.28 on the ModelNet10 dataset. Nevertheless, as an al-
gorithm about re-ranking and metric fusion, it can be antic-
ipated that UED can lead to a better performance if fusing

745

Baselines

B1+B2+B3
B1+B2+B4
B1+B3+B4
B2+B3+B4
B1+B2+B3+B4

Baselines

B1+B2+B3
B1+B2+B4
B1+B3+B4
B2+B3+B4
B1+B2+B3+B4

AUC

TPF

83.99∼86.00
68.56∼84.01
68.56∼84.79
70.69∼86.00
68.56∼86.00

NF

87.53
80.02
83.54
84.64
85.26

RED

87.04
83.60
85.06
86.48
87.03

Ours

87.27
84.70
86.29
88.05
87.22

NF

86.77
79.32
82.83
83.93
84.55

mAP

TPF

83.15∼85.12
67.16∼83.23
67.16∼83.86
69.15∼85.12
67.16∼85.12

Table 2. The performance comparison (%) of fusion methods on the ModelNet40 dataset.

AUC

TPF

91.63∼92.60
84.34∼92.38
83.97∼92.60
83.97∼92.14
83.97∼92.60

NF

92.80
91.45
91.35
90.67
91.72

RED

93.20
92.65
93.23
92.35
93.20

Ours

93.37
92.85
93.27
92.49
93.36

NF

91.65
90.25
90.03
89.71
90.49

mAP

TPF

90.56∼91.48
82.85∼91.41
82.56∼91.48
82.56∼91.11
82.56∼91.48

Table 3. The performance comparison (%) of fusion methods on the ModelNet10 dataset.

RED

86.30
82.82
84.24
85.71
86.30

RED

92.15
91.50
92.17
91.23
92.15

Ours

86.55
83.92
85.38
87.30
86.50

Ours

92.26
91.74
92.08
91.41
92.25

Methods

RED [7]
UED (ours)

B1

0.356
0.335

B2

0.348
0.336

B3

0.296
0.312

B4

0.000
0.014

Table 4. The learned weights on the ModelNet40 dataset.

Methods

SPH [24]
LFD [9]
PANORAMA [37]
ShapeNets [59]
Geometry Image [53]
DeepPano [52]
MVCNN [54]
GIFT [6]
PANORAMA-NN [49]
GVCNN [12]
RED [7]
PANORAMA-ENN [48]
SeqViews2SeqLabels [15]
UED (ours)

ModelNet40

ModelNet10

AUC mAP

AUC mAP

34.47
42.04
45.00
49.94

-

77.63

-

83.10

-
-

87.03

-
-

88.05

33.26
40.91
46.13
49.23
51.30
76.81
79.50
81.94
83.45
85.70
86.30
86.34
89.09
87.30

45.97
51.70
60.72
69.28

-

85.45

-

92.35

-
-

93.20

-
-

93.37

44.05
49.82
60.32
68.26
74.90
84.18

-

91.12
87.39

-

92.15
93.28
91.43
92.26

Table 5. The performance comparison (%) with state-of-the-arts
on the ModelNet40 and ModelNet10 dataset. The best and second
best results are marked in red and blue, respectively.

SeqViews2SeqLabels [15] and PANORAMA-ENN [48] as
the input similarities.

5.2. Image Retrieval

We then evaluate the retrieval performance on the Holi-
days [22] dataset. Holidays dataset is a widely-used bench-
mark dataset for image retrieval, which is comprised of
1, 491 images and 500 queries. The evaluation metric is

Baselines

B1+B2+B3
B1+B2+B4
B1+B3+B4
B2+B3+B4
B1+B2+B3+B4

NF

92.43
90.65
89.85
88.91
90.69

TPF

RED

Ours

90.03∼92.46
87.36∼92.46
85.12∼91.87
85.12∼90.12
85.12∼92.46

93.32
93.09
92.55
90.34
93.32

93.31
93.13
93.22
90.37
93.56

Table 6. The performance comparison of different fusion methods
on the Holidays dataset.

mean Average Precision (mAP). Four baseline similarities
are used, including NetVLAD [1]: mAP 88.29, SPoC [2]:
mAP 86.07, ResNet [16]: mAP 81.83, and HSV color his-
togram [68]: mAP 61.83. We denote them by B1, B2, B3,
and B4, respectively in Table 6.

In line with previous experiments, UED beats NF, TPF,
and RED with all but one similarity combinations as pre-
sented in Table 6. Meanwhile, by simply fusing four base-
line similarities in the re-ranking stage, UED achieves mAP
93.56 on the Holidays dataset. This achievement is already
better than the state-of-the-art methods, including Pairwise
Geometric Matching [27]: 89.2, Gordo et al. [13]: 89.1, Is-
cen et al. [20]: 87.5, Radenovi´c et al. [44]: 82.5, and only
slightly inferior to Gordo et al. [14]: 94.8. However, it can
be envisioned that the performance of UED can be better if
more discriminative features [14, 35, 38] and an ensemble
of models [19, 21, 43] are used.

Here, we do not report the experimental results on the
UKbench dataset [34], because the performance on it has
already gotten saturated. With the upper bound of the per-
formance being N-S score 4, some previous works have re-
ported nearly perfect scores. For example, Gordo et al. [14]
report 3.91 by enhancing R-MAC descriptor [57]. There-
fore, we include the comparison on the Ukbench dataset in

746

the supplementary material.

Methods

Rank-1 Accuracy

5.3. Person Re identiﬁcation

In recent years, person re-identiﬁcation (re-ID) has at-
tracted much attention in the vision community, driven by
the demand of video surveillance. Particularly, re-ranking-
based approaches [70, 47, 31, 64, 63, 28] become a popular
tool to automatically reﬁne the search results.

In this section, we evaluate the proposed method on the
Market-1501 dataset [69]. Market-1501 is a widely-used
large scale benchmark for person re-identiﬁcation. It con-
sists of 1501 identities. 750 identities (12, 936 images) are
used for training, 751 identities (19, 732 images) are used
for testing, and 3, 368 images act as queries. We utilize
three baseline similarities. First, we ﬁnetune a ResNet-50
model [16] with softmax loss and triplet loss [18]. Then,
we extract the L2 normalized activations of networks before
the loss layer as image features and compute the Euclidean
distance to measure the similarities between images. We
denote the two baselines as B1 and B2 respectively. More-
over, Mancs [58], a recent work using attention mechanism,
acts as the 3rd baseline similarity B3. The performance is
measured via rank-1 accuracy and mean Average Precision
(mAP) in single-query setting. The baseline performances
of B1, B2, and B3 are 91.66, 89.22, and 93.17 in rank-1
accuracy, 78.90, 75.33, and 82.51 in mAP, respectively.

Since massive works have reported performance on the
Market-1501 dataset, it is simply intractable to compare all
of them. Hence, we only include the state-of-the-art meth-
ods published in the year 2018 and those about re-ranking
or metric fusion in Table 7. Among them, K-reciprocal [70],
SSM [3], PSE+ECN [47], and RED [7] are also re-ranking-
based approaches as ours. We also reproduce the results
of K-reciprocal and RED with publicly available codes us-
ing the same baselines to ensure a fair comparison. Since
K-reciprocal can only handle one feature, we concatenate
multiple features as its input. As can be drawn from the ta-
ble, the results (either original or reproduced ones) of the
re-ranking algorithms are all inferior to that of UED. In
Fig. 1, we give a qualitative evaluation by exhibiting several
probe images and their 1-nearest neighbors with a disjoint
camera ID. The matching pairs are correctly retrieved by
UED, while RED∗ and K-reciprocal∗ fail to identify these
persons.

UED also outperforms some latest representatives by a
large margin, including AWTL [46], HA-CNN [26], and
Mancs [58]. Moreover, UED achieves mAP 92.75, which
is the ﬁrst work reporting mAP larger than 90 to our best
knowledge. In this sense, it will be a feasible way to im-
prove the recognition rate of re-ID systems by using model
ensemble and re-ranking in the future work.

AWTL [46]
HA-CNN [26]
Mancs [58]
K-reciprocal [70]
SSM [3]
PSE+ECN [47]
RED∗ [7]
K-reciprocal∗ [70]
UED (ours)

89.46
91.20
93.17
77.11
82.21
90.30
94.74
94.69
95.90

mAP

75.67
75.70
82.51
63.63
68.80
84.00
91.00
91.87
92.75

Table 7. The performance comparison (%) on the Market-1501
dataset. The results marked with ∗ are reproduced with publicly
available codes using the same baselines.

e
b
o
r
P

y
r
e
l
l
a
G

Figure 1. Example matching pairs of probe and gallery images cor-
rectly retrieved by UED on the Market-1501 dataset.

6. Conclusion

In this paper, we have concentrated on re-ranking with
the capacity of metric (or similarity) fusion for object re-
trieval and person re-identiﬁcation. The proposed Uniﬁed
Ensemble Diffusion (UED) is not only an effective algo-
rithm which achieves the state-of-the-art retrieval perfor-
mance on benchmark datasets, but also a uniﬁed and the-
oretical framework, within which existing fusion methods
are summarized as its special cases. By deeply analyzing
the principles of existing fusion methods, UED has under-
gone a careful design of objective function and derivation,
which enables it to have a fast diffusion step, consider the
interplay of all input pairs, handle multiple inputs, and be
robust to noise.

Most current re-ranking methods are not end-to-end
trainable, only serving as a post-processing procedure to re-
ﬁne the retrieval results. Recently, several works [50, 51]
have suggested to construct the afﬁnity graph in a mini-
batch in a deep model and achieved a promising perfor-
mance improvement. However, it is difﬁcult to well sam-
ple the manifold structure given a small set of data points.
Therefore, how to include the contextual information in a
mini-batch is still an open-problem. We leave this as our
future work.

Acknowledgements This work was supported by Huawei,
EPSRC grant Seebibyte EP/M013774/1, EPSRC/MURI
grant EP/N019474/1 and NSF grant IIS-1814745.

747

References

[1] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic.
Netvlad: Cnn architecture for weakly supervised place
recognition. In CVPR, 2016. 7

[2] A. Babenko and V. Lempitsky. Aggregating local deep fea-
tures for image retrieval. In ICCV, pages 1269–1277, 2015.
7

[3] S. Bai, X. Bai, and Q. Tian. Scalable person re-identiﬁcation

on supervised smoothed manifold. In CVPR, 2017. 8

[4] S. Bai, X. Bai, Q. Tian, and L. J. Latecki. Regularized diffu-
sion process for visual retrieval. In AAAI, pages 3967–3973,
2017. 1, 2

[5] S. Bai, X. Bai, Q. Tian, and L. J. Latecki. Regularized dif-
fusion process on bidirectional context for object retrieval.
TPAMI, 2019. 1, 2

[6] S. Bai, X. Bai, Z. Zhou, Z. Zhang, and L. J. Latecki. Gift:
A real-time and scalable 3d shape search engine. In CVPR,
2016. 6, 7

[7] S. Bai, Z. Zhou, J. Wang, X. Bai, L. J. Latecki, and Q. Tian.
Ensemble diffusion for retrieval. In ICCV, pages 774–783,
2017. 1, 3, 6, 7, 8

[8] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using gra-
dient descent. In ICML, pages 89–96, 2005. 2

[9] D. Y. Chen, X. P. Tian, Y. T. Shen, and M. Ouhyoung. On
visual similarity based 3d model retrieval. Comput. Graph.
Forum, 22(3):223–232, 2003. 7

[10] M. Donoser and H. Bischof. Diffusion processes for retrieval

revisited. In CVPR, pages 1320–1327, 2013. 1, 2

[11] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and
E. Wong. 3d deep shape descriptor. In CVPR, pages 2319–
2328, 2015. 1

[12] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and Y. Gao. Gvcnn:
Group-view convolutional neural networks for 3d shape
recognition. In CVPR, pages 264–272, 2018. 7

[13] A. Gordo, J. Almaz´an, J. Revaud, and D. Larlus. Deep image
retrieval: Learning global representations for image search.
In ECCV, pages 241–257, 2016. 7

[14] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. End-to-end
learning of deep visual representations for image retrieval.
IJCV, 124(2):237–254, 2017. 7

[15] Z. Han, M. Shang, Z. Liu, C.-M. Vong, Y.-S. Liu,
M. Zwicker, J. Han, and C. P. Chen. Seqviews2seqlabels:
Learning 3d global features via aggregating sequential views
by rnn with attention. TIP, 28(2):658–672, 2019. 6, 7

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 6, 7, 8

[17] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-center

loss for multi-view 3d object retrieval. In CVPR, 2018. 6

[18] A. Hermans, L. Beyer, and B. Leibe.

loss for person re-identiﬁcation.

triplet
arXiv:1703.07737, 2017. 8

In defense of the
arXiv preprint

[19] A. Iscen, Y. Avrithis, G. Tolias, T. Furon, and O. Chum. Fast

spectral ranking for similarity search. In CVPR, 2018. 7

[21] A. Iscen, G. Tolias, Y. S. Avrithis, T. Furon, and O. Chum.
Efﬁcient diffusion on region manifolds: Recovering small
objects with compact cnn representations. In CVPR, 2017.
1, 7

[22] H. Jegou, M. Douze, and C. Schmid. Hamming embedding
and weak geometric consistency for large scale image search.
In ECCV, pages 304–317, 2008. 2, 7

[23] E. Johns, S. Leutenegger, and A. J. Davison. Pairwise de-
composition of image sequences for active multi-view recog-
nition. In CVPR, 2016. 6

[24] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation
invariant spherical harmonic representation of 3d shape de-
scriptors. In SGP, pages 156–164, 2003. 7

[25] Q. Ke and Y. Li. Is rotation a nuisance in shape recognition?

In CVPR, pages 4146–4153, 2014. 1

[26] W. Li, X. Zhu, and S. Gong. Harmonious attention network

for person re-identiﬁcation. In CVPR, 2018. 8

[27] X. Li, M. Larson, and A. Hanjalic. Pairwise geometric
In CVPR, pages

matching for large-scale object retrieval.
5153–5161, 2015. 7

[28] C. Liu, C. Change Loy, S. Gong, and G. Wang. Pop: Per-
son re-identiﬁcation post-rank optimisation. In ICCV, pages
441–448, 2013. 8

[29] M. Liu, B. C. Vemuri, S. ichi Amari, and F. Nielsen. Shape
retrieval using hierarchical total bregman soft clustering.
TPAMI, 34(12):2407–2419, 2012. 1

[30] V. Losert and E. Akin. Dynamics of games and genes: Dis-
crete versus continuous time. Journal of Mathematical Biol-
ogy, 17(2):241–251, 1983. 4

[31] C. C. Loy, C. Liu, and S. Gong. Person re-identiﬁcation by

manifold ranking. In ICIP, pages 3567–3571, 2013. 8

[32] L. Luo, C. Shen, C. Zhang, and A. van den Hengel. Shape
similarity analysis by self-tuning locally constrained mixed-
diffusion. TMM, 15(5):1174–1183, 2013. 1

[33] T. Mei, Y. Rui, S. Li, and Q. Tian. Multimedia search rerank-
ing: A literature survey. ACM Comput. Surv., 46(3):38:1–
38:38, 2014. 1

[34] D. Nist´er and H. Stew´enius. Scalable recognition with a vo-

cabulary tree. In CVPR, pages 2161–2168, 2006. 2, 7

[35] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han. Large
In

scale image retrieval with attentive deep local features.
ICCV, pages 3456–3465, 2017. 7

[36] S. Paisitkriangkrai, C. Shen, and A. Van Den Hengel. Learn-
ing to rank in person re-identiﬁcation with metric ensembles.
In CVPR, pages 1846–1855, 2015. 1, 2

[37] P. Papadakis, I. Pratikakis, T. Theoharis, and S. J. Perantonis.
Panorama: A 3d shape descriptor based on panoramic views
for unsupervised 3d object retrieval. IJCV, 89(2-3):177–192,
2010. 1, 6, 7

[38] M. Paulin, M. Douze, Z. Harchaoui, J. Mairal, F. Perronin,
and C. Schmid. Local convolutional features with unsuper-
vised training for image retrieval.
In ICCV, pages 91–99,
2015. 7

[20] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Mining on
manifolds: Metric learning without labels. In CVPR, 2018.
7

[39] D. C. G. Pedronette and R. D. S. Torres. Image re-ranking
and rank aggregation based on similarity of ranked lists. Pat-
tern Recognition, 46(8):2350–2360, 2013. 1

748

[40] M. Pelillo. Replicator equations, maximal cliques, and graph
isomorphism. Neural Computation, 11(8):1933–1955, 1999.
2, 4

[41] M. Pelillo. Replicator equations, maximal cliques, and graph

isomorphism. In NIPS, pages 550–556, 1999. 2, 4

for person re-identiﬁcation. In ECCV, pages 365–381, 2018.
8

[59] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shape modeling. In CVPR, 2015. 2, 6, 7

[42] C. R. Qi, H. Su, M. Niessner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view cnns for object classiﬁ-
cation on 3d data. In CVPR, 2016. 6

[60] J. Xie, Y. Fang, F. Zhu, and E. Wong. Deepshape: Deep
learned shape descriptor for 3d shape matching and retrieval.
In CVPR, pages 1275–1283, 2015. 1

[61] F. Yang, B. Matei, and L. S. Davis. Re-ranking by multi-
feature fusion with diffusion for image retrieval. In WACV,
pages 572–579, 2015. 1

[62] X. Yang, L. Prasad, and L. J. Latecki. Afﬁnity learning
with diffusion on tensor product graph. TPAMI, 35(1):28–
38, 2013. 2

[63] M. Ye, C. Liang, Y. Yu, Z. Wang, Q. Leng, C. Xiao, J. Chen,
and R. Hu. Person reidentiﬁcation via ranking aggrega-
tion of similarity pulling and dissimilarity pushing. TMM,
18(12):2553–2566, 2016. 8

[64] M. Ye, A. J. Ma, L. Zheng, J. Li, and P. C. Yuen. Dynamic la-
bel graph matching for unsupervised video re-identiﬁcation.
In ICCV, 2017. 8

[65] T. Yu, Y. Wu, S. D. Bhattacharjee, and J. Yuan. Efﬁcient ob-
ject instance search using fuzzy objects matching. In AAAI,
pages 4320–4326, 2017. 1

[66] T. Yu, J. Yuan, C. Fang, and H. Jin. Product quantization
network for fast image retrieval. In ECCV, pages 186–201,
2018. 1

[67] S. Zhang, M. Yang, T. Cour, K. Yu, and D. N. Metaxas.
Query speciﬁc fusion for image retrieval. In ECCV, pages
660–673, 2012. 1

[68] S. Zhang, M. Yang, T. Cour, K. Yu, and D. N. Metaxas.
TPAMI,

Query speciﬁc rank fusion for image retrieval.
37(4):803–815, 2015. 1, 7

[69] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.
In ICCV,

Scalable person re-identiﬁcation: A benchmark.
pages 1116–1124, 2015. 2, 8

[70] Z. Zhong, L. Zheng, D. Cao, and S. Li. Re-ranking person
re-identiﬁcation with k-reciprocal encoding. In CVPR, pages
3652–3661, 2017. 8

[71] D. Zhou,

J. Weston, A. Gretton, O. Bousquet, and
B. Sch¨olkopf. Ranking on data manifolds. In NIPS, pages
169–176, 2004. 1, 2

[72] Y. Zhou, X. Bai, W. Liu, and L. J. Latecki. Similarity fusion

for visual tracking. IJCV, pages 1–27, 2016. 1, 2

[43] F. Radenovic, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.
Revisiting oxford and paris: Large-scale image retrieval
benchmarking. In CVPR, 2018. 7

[44] F. Radenovi´c, G. Tolias, and O. Chum. CNN image retrieval
learns from BoW: Unsupervised ﬁne-tuning with hard exam-
ples. In ECCV, 2016. 7

[45] B. Ramesh, C. Xiang, and T. H. Lee. Shape classiﬁcation us-
ing invariant features and contextual information in the bag-
of-words model. Pattern Recognition, 48(3):894–906, 2015.
1

[46] E. Ristani and C. Tomasi. Features for multi-target multi-

camera tracking and re-identiﬁcation. In CVPR, 2018. 8

[47] M. Saquib Sarfraz, A. Schumann, A. Eberle, and R. Stiefel-
hagen.
A pose-sensitive embedding for person re-
identiﬁcation with expanded cross neighborhood re-ranking.
In CVPR, 2018. 8

[48] K. Sﬁkas, I. Pratikakis, and T. Theoharis. Ensemble of
panorama-based convolutional neural networks for 3d model
classiﬁcation and retrieval. Computers & Graphics, 71:208–
218, 2018. 1, 6, 7

[49] K. Sﬁkas, T. Theoharis, and I. Pratikakis. Exploiting the
panorama representation for convolutional neural network
classiﬁcation and retrieval. In 3DOR, 2017. 6, 7

[50] Y. Shen, H. Li, T. Xiao, S. Yi, D. Chen, and X. Wang. Deep
group-shufﬂing random walk for person re-identiﬁcation. In
CVPR, pages 2265–2274, 2018. 8

[51] Y. Shen, H. Li, S. Yi, D. Chen, and X. Wang. Person re-
identiﬁcation with deep similarity-guided graph neural net-
work. In ECCV, pages 508–526, 2018. 8

[52] B. Shi, S. Bai, Z. Zhou, and X. Bai. Deeppano: Deep
IEEE

panoramic representation for 3-d shape recognition.
Signal Processing Letters, 22(12):2339–2343, 2015. 6, 7

[53] A. Sinha, J. Bai, and K. Ramani. Deep learning 3d shape
surfaces using geometry images. In ECCV, pages 223–240,
2016. 7

[54] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3d shape
recognition. In ICCV, pages 945–953, 2015. 6, 7

[55] H. Tabia, M. Daoudi, J.-P. Vandeborre, and O. Colot. A new
3d-matching method of nonrigid and partially similar models
using curve analysis. TPAMI, 33(4):852–858, 2011. 1

[56] H. Tabia, H. Laga, D. Picard, and P.-H. Gosselin. Covariance
descriptors for 3d shape matching and retrieval. In CVPR,
pages 4185–4192, 2014. 1

[57] G. Tolias, R. Sicre, and H. J´egou. Particular object retrieval
with integral max-pooling of cnn activations. In ICLR, 2015.
7

[58] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang. Mancs:
A multi-task attentional network with curriculum sampling

749

