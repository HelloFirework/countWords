MBS: Macroblock Scaling for CNN Model Reduction

Yu-Hsun Lin, Chun-Nan Chou, and Edward Y. Chang

HTC Research & Healthcare (DeepQ)

{lyman lin, jason.cn chou, edward chang}@htc.com

Abstract

In this paper we propose the macroblock scaling (MBS)
algorithm, which can be applied to various CNN architec-
tures to reduce their model size. MBS adaptively reduces
each CNN macroblock depending on its information redun-
dancy measured by our proposed effective ﬂops. Empiri-
cal studies conducted with ImageNet and CIFAR-10 attest
that MBS can reduce the model size of some already com-
pact CNN models, e.g., MobileNetV2 (25.03% further re-
duction) and ShufﬂeNet (20.74%), and even ultra-deep ones
such as ResNet-101 (51.67%) and ResNet-1202 (72.71%)
with negligible accuracy degradation. MBS also performs
better reduction at a much lower cost than the state-of-the-
art optimization-based methods do. MBS’s simplicity and
efﬁciency, its ﬂexibility to work with any CNN model, and
its scalability to work with models of any depth make it an
attractive choice for CNN model size reduction.

1. Introduction

CNN models have been widely used by image-based
applications, thanks to the breakthrough performance of
AlexNet [15]. However, a very deep and wide CNN model
consists of many parameters, and as a result, the trained
model may demand a large amount of DRAM and a large
number of multiplications to perform a prediction. Such
high resource and computation requirements lead to latency,
heat, and power consumption problems, which are subopti-
mal for edge devices such as mobile phones and IoTs [25].
Therefore, reducing CNN model size is essential for im-
proving resource utilization and conserving energy.

Several CNN model reduction algorithms have re-
cently been proposed [25]. These algorithms can be di-
vided into two categories: micro-level (performing reduc-
tion/quantization inside a ﬁlter) and macro-level reduction
(removing redundant ﬁlters). These two categories are com-
plement to each other. (More details are presented in the
related work section.) We focus our study in this paper on
macro-level reduction. For lucid exposition, we establish
an essential concept that removing redundant ﬁlters are also

referred as removing redundant channels since each ﬁlter
outputs a corresponding channel in a CNN model [10].

There are two categories of macro-level reduction: opti-
mization based and channel-scaling based. Each category
has multiple methods and algorithms. The optimization-
based category typically estimates the ﬁlter importance by
formulating an optimization problem with the adopted cri-
teria (e.g., ﬁlter weight magnitude). Removing a ﬁlter (or a
channel, which is formally deﬁned in Section 3) will affect
both the former and latter layers. The ﬁlter pruning step of
the optimization-based method must take into account the
inter-connected structures between CNN layers. Therefore,
a CNN model such as DenseNet [11] and ShufﬂeNet [31]
with more complex inter-connected structures may prevent
the optimization-based approach from being effective.

The channel-scaling based category uses an α-scalar to
reduce channel width. For instance, MobileNet [10] uses
the same α-scaler to prune the widths of all channels. Ap-
plying the same α-scaler to all convolutional layers without
considering each information density is a coarse-grained
method. A ﬁne-grained method that ﬁnds the optimal α-
scalar for each convolutional layer should be ideal. How-
ever, the increasingly complicated inter-layer connection
structures of CNN models forbid ﬁne-grained scaling to be
computationally feasible.

To address the shortcomings of the current model-
compaction methods, we propose macroblock scaling
(MBS). A macroblock consists of a number of convolu-
tion layers that exhibit similar characteristics, such as hav-
ing the same resolution or being a segment of convolution
layers with customized inter-connects. Having macroblock
as a structure abstraction provides the ﬂexibility for MBS
to inter-operate with virtually any CNN models of various
structures, and also permits channel-scaling to be performed
in a “ﬁner”-grained manner. To quantify information den-
sity for each macroblock so as to determine an effective
macroblock-dependent scalar, MBS uses effective ﬂops to
measure each macroblock’s information density. (We de-
ﬁne effective ﬂops to be the number of convolution ﬂops
required for the activated non-zero ReLU outputs.) Experi-
mental results show that the reduction MBS can achieve is

19117

more signiﬁcant than those achieved by all prior schemes.

2.2. Reduction Methods of CNN Models

Our contributions can be summarized as follows:

• MBS employs macroblock to address the issues that
both coarse-grained and ﬁne-grained scaling cannot
deal with, and hence allows channel-scaling to be per-
formed with any CNN models.

• MBS proposes using an effective and efﬁcient mea-
sure, effective ﬂops, to quantify information density
to decide macroblock-dependent scaling factors. As
shown in the algorithm section,
the complexity of
MBS is linear with respect to the number of training in-
stances times the number of parameters, which is more
efﬁcient than the optimization-based methods [16, 30].

• Extensive empirical studies on two representative
datasets and various well-known CNN models (e.g.,
MobileNet, ShufﬂeNet, ResNet,
and DenseNet)
demonstrate that MBS outperforms all state-of-the-art
model-reduction methods in reduction size while pre-
serving the same level of prediction accuracy. Due
to its simple and effective nature, MBS remains to be
effective even with ultra-deep CNNs like ResNet-101
on ImageNet (51.67% reduction) and ResNet-1202 on
CIFAR-10 (72.71% reduction).

The remaining parts of this paper are organized into three
main sections. The Related Work section highlights some
previous efforts of reducing CNN models. The Method
section explains our proposed MBS algorithm. The Ex-
periment section shows the encouraging results of applying
MBS on various CNN models.

2. Related Work

We review related work in two parts. We ﬁrst review key
CNN properties relevant to the inception of MBS and then
review some representative model-reduction methods.

2.1. Relevant CNN Properties

There are research works [4, 27, 29] integrating the early
stop (or early exit) mechanism of the initial CNN layers in
order to speed up the inference process. This phenomenon
demonstrates that the outcome of a CNN model at early
stages can be adequate for predicting an image label with
high conﬁdence. This result provides supporting evidence
for us to group convolution layers into two types: former
convolution layers (near to the input image) as the base lay-
ers, and latter convolution layers (close to the label output)
as the enhancement layers. The early stop mechanism moti-
vates that the information density in the enhancement layers
should be lower than that in the base layers, and therefore,
more opportunities exist in the enhancement layers to re-
duce model size.

As mentioned in the introduction that model reduction
can be divided into micro-level and macro-level approaches.
Binary approximation of a CNN ﬁlter is one important
direction for micro-level model reduction [2, 12, 17, 23].
Maintaining prediction accuracy of a binary CNN is a
challenging issue [26]. The sparse convolution modules
[1, 3, 13, 18, 28] or deep compression [6] usually introduce
irregular structures. However, these micro-level model re-
duction methods with irregular structures often require spe-
cial hardware for acceleration [5].

The macro-level model reduction approach removes ir-
relevant ﬁlters and maintains the existing structures of
CNNs [7,9,16,20]. The methods of this reduction approach
estimate the ﬁlter importance by formulating an optimiza-
tion problem based on some adopted criteria (e.g., the ﬁlter
weight magnitudes or the ﬁlter responses).

The research work of [30] addresses the ﬁlter impor-
tance issue by formulating the problem into binary integer
programming with the aid of feature ranking [24], which
achieves the state-of-the-art result. For an n-convolution-
layer model with np parameters and N training images, the
complexity of acquiring the CNN outputs is O(np × N ).
The complexity of the feature ranking step is O(N 2.37)
[24]. In addition to the preprocessing step, the binary in-
teger programming is an NP-hard problem. The detail com-
plexity is not speciﬁed in [30]. In general, a good approx-
imate solution for np variables still requires high compu-
tational complexity (e.g., O(n3.5
p ) by linear programming
[14]).
In contrast, MBS enjoys low complexity that is
O(np×N ) in computing information density and O(M ×n)
in computing the scaling factors for a CNN with n convo-
lution layers and M macroblocks, which is detailed in the
next section.

In a sense, some model reduction methods belonging
to network pruning are related to the topic of architecture
search. Our proposed MBS also falls into this type. Treat-
ing network pruning as architecture search only requires a
one-pass training, but the search space is restricted to all
sub-networks inside a large network [21]. By contrast, full
architecture search such as DARTS [19] considers more op-
tions, e.g., activation functions or different layer orders, and
usually pays the cost of requiring more passes to ﬁnd the
goal architecture. Thus, applying architecture search on
large datasets like ImageNet directly demands considerable
computation cost.

3. MBS Algorithm

This section presents our proposed macroblock scal-
ing (MBS) algorithm for reducing an already trained CNN
model. We ﬁrst deﬁne key terms including channel, ﬁl-
ter, channel scaling, macroblock, and the parameters used

9118

RF(c7) = 26

RF(c8) = 36

RF(c9) = 44

[z=32]RF = min {RF(cj) | ∀j RF(cj) > z} = 36

Enhancement Layers: C9 , C10, C11

e
g
a
m

I

c1

c0
c2
Macroblock m0

Feature Map Size: 32 x 32

g
n

i
l

o
o
P

 
.
g
v
A

c3

c4

g
n

i
l

o
o
P

 
.
g
v
A

c7

c5

c6
Macroblock m1

Feature Map Size: 16 x 16

 
l

a
b
o
G

l

g
n

i
l

o
o
P

C
F

x
a
m

t
f
o
S

l

e
b
a
L

c11

c9

c8
c10
Macroblock m2

Feature Map Size: 8 x 8

Figure 1. An example CNN model contains three CNN macroblocks for the CIFAR image (32 × 32 pixels). Each macroblock mi consists
of the convolution layers whose output feature maps are of the same size (i.e., same width and height). Besides, k sizej = 3 ∀j.

by MBS. We then explain how MBS computes information
density, and how that information is used by MBS to reduce
model size. Finally, we analyze computational complexity
of MBS and compare its efﬁciency with competing model-
compaction algorithms.

Let us use image applications to explain a CNN pipeline.
A typical CNN pipeline accepts N training images as in-
put. These N training instances are of the same height and
width. To simplify our notation, we assume all input im-
ages are in the square form with the same resolution L × L.
A CNN model is composed of multiple convolution layers.
The input to a convolution layer is a set of input tensors (or
input activations), each of which is called a channel [25].
Each layer generates a successively high-level abstraction
of the input tensors, call a output tensor or feature map.

More speciﬁcally, the jth convolution layer cj , j =
0, . . . , n − 1, takes s sizej × s sizej × c widthj input ten-
sor and produces s sizej+1 × s sizej+1 × c widthj+1 output
tensor, where s size is the spatial size of input or output
tensors, and c width is the input/output channel width (i.e.,
number of channels). Particularly, s size0 is equal to L. Let
k sizej denote the spatial size of the square kernel of cj , the
required number of parameters of cj can be written as

k sizej × k sizej × c widthj × c widthj+1.

(1)

MBS groups convolution layers into macroblocks. A
macroblock consists of the convolution layers whose output
tensors (feature maps) are of the same size. We use M to
denote the number of macroblocks in a CNN model. Figure
1 depicts an example CNN model with M = 3. The size of
output tensors is down-sampled by the pooling layers with
stride size 2. Hence, macroblock mi is deﬁned as

mi = {cj |∀j s.t. s sizej+1 =

L
2i }.

(2)

Operation scaling reduces channel width.

Intuitively,
MBS would like to prune channels that cannot provide pos-
itive contributions to accurate prediction. For instance, Mo-
bileNet [10] scales down all channel widths by a constant
ratio 0 < α < 1, or we call this baseline scheme α-scaling.
MobileNet uses the same α value for all convolution layers.

Receptive Field Width: RF(ck)

i

L
 
:
t
h
g
e
H
e
g
a
m

 

I

cj
Conv. Layer
(base layer)

ck
Conv. Layer

(enhancement layer)

Image Width: L

Figure 2. An example of the receptive ﬁeld of the neuron in the
base layer cj and the enhancement layer ck.

However, an effective channel-scaling scheme should esti-
mate the best scaling ratio for each convolution layer based
on its information density

But quantifying and determining the scaling ratio for
each convolution layer disturbs the designs of CNN models.
To preserve the design structure of an original CNN model,
MBS performs reduction at the macroblock level instead of
at the convolution-layer level. For some CNN models that
have convolution layers connected into a complex structure,
MBS treats an entire such segment as a macroblock to pre-
serve its design. Our macroblock approach, as its name
suggests, does not deal with detailed inter-layer connection
structure. The macroblock abstraction thus makes model
reduction simple and adaptive.

3.1. Grouping Convolution Layers by Receptive

Fields

An effective CNN model requires a sufﬁcient number of
convolution layers to capture good representations from in-
put data. However, as the number of the convolution lay-
ers grows beyond a threshold, the additional beneﬁt in im-
proving prediction accuracy can diminish. One may argue
that the former convolution layers may learn low-level rep-
resentations such as edges and contours, whereas latter lay-
ers high-level semantics. As we will demonstrate shortly,
the latter layers may cover receptive ﬁelds that are larger
than the input images, and their learned information may
not contribute to class prediction. The effective receptive
ﬁeld in CNN is the region of the input image that affects a

9119

flop(cj): flops of filter cj

pj: prob. of non-zero output

cj

*
Conv. Layer

Input Tensor

y =ReLU(x)

ya > 0
yb = 0

Activation

Output Tensor

Figure 3. Effective ﬂop calculates of the ﬂops considering the non-
zero probability pj of the ReLU output.

particular neuron of the network [22]. Figure 2 shows an ex-
ample, where a neuron of a former convolution layer covers
a region inside the input image, whereas a neuron of a latter
layer may cover a region larger than the input image. Hence,
we categorize convolution layers into two types, base layers
and enhancement layers, which are deﬁned as follows:

• Base convolution layers: The former convolution lay-
ers (near to the input) of a CNN model learn essential
representations from the training data. Though repre-
sentations captured in the base layers could be redun-
dant, they are fundamental for class prediction.

• Enhancement convolution layers: The latter convolu-
tion layers may cover receptive ﬁelds larger than the
input areas1. Therefore, opportunities are available for
channel pruning to remove redundant information.

Now, we deﬁne a function RF(cj) to compute the recep-
tive ﬁeld size of layer cj . For simplicity, assume that the
receptive ﬁeld region of ﬁlter cj is RF(cj) × RF(cj). The
possible set of values of RF(cj) is discrete, which is deter-
mined by the conﬁguration of the kernel size and the stride
step of a CNN model. For lucid exposition, we deﬁne ⌈z⌉RF
to characterize the minimum receptive ﬁeld boundary that is
greater than a given value z as follows:

⌈z⌉RF = min{RF(cj) | ∀j RF(cj) > z}.

(3)

We use this boundary ⌈z⌉RF to divide base convolution lay-
ers and enhancement convolution layers in a CNN pipeline.
Revisit macroblocks in Figure 1. Convolution layer c9
belonging to macroblock m2 is the ﬁrst enhancement layer,
where its receptive ﬁeld is larger than the receptive ﬁeld
boundary ⌈z = 32⌉RF = 36. We estimate information re-
dundancy of each macroblock mi by measuring the infor-
mation density ratio contributed by the enhancement layers.
We can determine the base layers of a CNN by setting
the value of z. As we have previously explained, the area
beyond and at the boundary of an image contains less use-
ful information. Thus, setting z = L is reasonable to sepa-
rate those layers that can contribute more to class prediction

1Due to data augmentation and boundary patching operations applied
to raw input images, a training input image may contain substantial useless
information at its boundaries.

from the other layers. A macroblock can contain base layers
only, enhancement layers only, or a mixture of the two.

3.2. Information Density Estimation within

Macroblocks by Effective Flops

MBS uses convolution FLOP to estimate information
density. A FLOP (denoted by the lowercase “ﬂop” in the
remainder of the paper) is a multiply-and-add operation in
convolution. The more frequently that ReLU outputs a zero
value means that the less information that convolution layer
contains. Therefore, only those ﬂops that can produce a
non-zero ReLU output are considered to be effective.

A neuron on a convolution layer covers a region, i.e., re-
ceptive ﬁeld of the input image. The physical meaning of
effective ﬂops represents effective feature matchings on the
input image whose the feature pattern size is the same as
that of the receptive ﬁelds. Since effective ﬂops quantify
the effective feature matchings of CNN models for a given
image, it plays the key role in estimating information den-
sity.

Figure 3 shows the computation of the effective ﬂops of
a convolution layer. Let ecj denote effective ﬂops of layer
cj , and pj represent the non-zero probability of its ReLU
output. We can deﬁne ecj as

ecj = pj × ﬂop(cj).

(4)

To evaluate information density of macroblock mi, we
tally the total effective ﬂops from the beginning of the CNN
pipeline to the end of macroblock mi. We can write the sum
of the effective ﬂops as

Etotal(mi) = X ecj , cj ∈ {m0, · · · , mi}.

(5)

Next, we compute the effective ﬂops in the base layers or
those ﬂops taking place within the receptive ﬁeld as

Ebase(mi) = X ecj , cj ∈ {RF(cj) ≤ ⌈z⌉RF},

(6)

where the base layers have the maximum receptive ﬁeld size
⌈z⌉RF.

Based on the total ﬂops Etotal(mi) and base ﬂops
Ebase(mi), we deﬁne the difference between the two as the
enhancement ﬂops, which is denoted as Eenchancement(mi)
and can be written as Etotal(mi) − Ebase(mi). The redun-
dancy ratio ri of macroblock mi is then deﬁned as the total
enhancement ﬂops over the total ﬂops, or

ri =

Eenhancement(mi)

Etotal(mi)

= 1 −

Ebase(mi)
Etotal(mi)

.

(7)

We estimate the channel-scaling factor for each macroblock
mi based on this derived redundancy ri, which is illustrated
in the next subsection.

9120

Algorithm 1 Macroblock Scaling
Input: Fn(), I0∼N −1 /*Pre-trained model, training images
Output: [c widthc
mM −1 ]/*Compact model

, · · · , c widthc

m0

Procedure:

• NZ() /*Computes the number of non-zero elements

• RF() /*Computes receptive ﬁeld size

• ﬂop() /*Computes the number of FLOPs

Variable:

• vI

j /*The jth ReLU output tensor of Fn(I)

BEGIN

1: for j = 0, · · · , n − 1 do
2:

for i = 0, · · · , N − 1 do
Ii
j )

NZ(v

s sizej+1×s sizej+1×c widthj+1

pIi
j ←
end for
pj ← 1
j /*Compute non-zero output prob.
ecj ← pj × ﬂop(cj) /*Effective ﬂops for each cj

N P pIi

6:
7: end for
8: for i = 0, · · · , M − 1 do
9:

Etotal(mi) ← 0 /*Initialization
Ebase(mi) ← 0 /*Initialization
for j = 0, · · · , n − 1 do

if cj ∈ {m0, · · · , mi} then

Etotal(mi) ← Etotal(mi) + ecj /*Total ecj

end if
if cj ∈ {RF(cj) ≤ ⌈z⌉RF} then

Ebase(mi) ← Ebase(mi) + ecj /*Base layer ecj

end if
end for
if Etotal(mi) > Ebase(mi) then

ri ← 1 − Ebase(mi)
βi ← 1
1+ri

Etotal(mi) /*Compute redundancy

3:

4:

5:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

mi ← ⌈βi × c widthmi ⌉ /*Compact model

, · · · , c widthc

mk−1 ]

m0

else

βi ← 1

end if
c widthc

25:
26: end for
27: return [c widthc
END

3.3. Channel Scaling Factor Estimation

We deﬁne the relation between the original channel
width c widthmi of macroblock mi and the compact chan-
nel width c widthc
after the reduction process, which is
depicted as

mi

c widthmi = (1 + ri) × c widthc
mi

.

(8)

If there is no redundancy in macroblock mi (i.e., ri = 0),
the original channel c widthmi
is equal to the compact
channel width c widthc
. Therefore, the channel width
multiplier βi for the macroblock mi is

mi

βi =

1

1 + ri

,

(9)

where this estimation makes βi > 0.5 since ri < 1 ac-
cording to Eq. (7). The lower bound of the channel-scaling
factor βi is in accordance with the observation made by Mo-
bileNet [10] that a scaling factor that is less than 0.5 can
introduce noticeable distortion.

mi

Algorithm 1 presents our MBS algorithm, which esti-
mates the scaling factor βi for each macroblock mi and
derives the compact channel width c widthc
. The MBS
algorithm takes the pre-trained model Fn() with n convo-
lution layers and N training images as input. The convo-
lution results of the pre-trained model Fn() for the training
images are utilized for estimating the scaling factors. The
inner loop from steps 2 to 4 collects non-zero statistics of
the ReLU outputs pj . The steps after the inner loop (steps 5
and 6) take an average over N training instances, and then
derive the effective ﬂop for each convolution layer cj .

The macroblock process starts from step 8. The total and
base effective ﬂops for each macroblock are initialized in
steps 9 and 10, respectively. The MBS algorithm ﬁrst tallies
the total and base effective ﬂops for each macroblock (steps
11 to 18). Afterwards, MBS computes the redundant ratio
ri for macroblock mi (steps 19 to 24). The scaling factor βi
is derived from redundancy ratio ri in step 20. After βi has
been computed, MBS estimates the compact channel width
c widthc

for each macroblock mi in step 25.

mi

After Algorithm 1 outputs the new set of channel widths,
the CNN is retrained with the new channel setting to gen-
erate a more compact model F ′
n(). Instead of ﬁne-tuning,
the reason why we retrain the model with the new channel
setting is that ﬁne-tuning the amended model with inherited
weights is no better than training it from scratch, which is
consistent with the observation in [21]. In the experimental
section, we will evaluate the effectiveness of MBS by exam-
ining the performance (prediction accuracy and model-size
reduction) achieved by F ′

n() over Fn().

The pre-trained model Fn() has n convolution layers
with np parameters and N training images. The required
complexity of MBS consists of two parts:
the non-zero
statistics pj collection (from steps 1 to 7) and the redun-
dancy ri estimation (from steps 8 to 26). In the ﬁrst part,
we collect pj by inferencing the N training images, which
the statement in step 3 can be absorbed into the forward
pass of the pre-trained model. Hence, the computational
complexity is O(np × N ). The second part traverses all
the convolution layers of the pre-trained model for deriv-
ing the compact model. The complexity of the second part

9121

Table 1. Model reduction results of ResNet on CIFAR-10.

Model

Acc. [Diff.]

Reduction

ResNet-20
MBS (L)

ResNet-32
MBS (L)

ResNet-44
MBS (L)

ResNet-56
MBS (L)

ResNet-110
MBS (L)

ResNet-1202
MBS (L)

ResNet-110
[16] 110-A
[16] 110-B
[30] NISP
MBS (3.4 × L)

91.86%
91.19% [0.67]

92.24%
91.82% [0.42]

92.85%
92.16% [0.69]

93.09%
92.48% [0.61]

93.58%
92.61% [0.97]

94.04%
93.06% [0.98]

93.58%
93.55% [0.03]
93.30% [0.28]
93.35% [0.23]
93.47% [0.11]

-

29.63%

-

46.81%

-

53.03%

-

59.30%

-

66.47%

-

72.71%

-

2.30%
32.40%
43.25%
50.29%

is O(M × n) since we have already derived each ecj from
the ﬁrst part. The wall-clock time of the ﬁrst part usually
takes 50 minutes on a PC with NVIDIA 1080-Ti for pre-
trained MobileNet on ImageNet. Notice that we only have
to conduct the ﬁrst part once for each pre-trained model.
The wall-clock time of the second part is negligible, which
is less than one second on the same PC.

4. Experiments

We applied MBS to various CNNs on CIFAR-10 and Im-
ageNet to evaluate its effectiveness in model reduction. Our
experiments aim to answer three main questions:

1. How aggressively can one reduce the size of a CNN
model without signiﬁcantly degrading prediction ac-
curacy? (Section 4.1.1)

2. Can MBS work effectively with deep and already

highly compact CNN models? (Section 4.2.2)

3. Can MBS outperform competing model-reduction

schemes? (Sections 4.1.2 and 4.2.1)

4.1. Results on CIFAR 10 Dataset

CIFAR-10 consists of 50k training images and 10k test-
ing images of 10 classes. We follow the training settings
in [11]: batch size is 128, weight decay is 10−4, and learn-
ing rate is set to 0.1 initially and divided by 10 at the 50%
and 75% of the total training epochs, respectively.

4.1.1 Accuracy and Reduction Tradeoff on ResNet

We evaluated the effect of setting different receptive ﬁeld
size threshold z on prediction accuracy on CIFAR-10 with
ResNet. The threshold z is set to z = k × L, which k ranges
from 1.4 to 0.6 with step size 0.2 (from the leftmost point
to the rightmost point for each line in Figure 4(a)).

Figure 4(a) shows two results. The x-axis depicts model
reduction ratio from low to high, and the y-axis predic-
tion accuracy. We ﬁrst observe that on all ResNet models
(ResNet-20, 32, 44, 56, and 110), the more number of en-
hancement layers (i.e., MBS employing smaller z value, see
the ﬁve z values on each line from large on the left to small
on the right), the better the model reduction ratio. Second,
the tradeoff between model reduction and prediction accu-
racy exhibits in all ResNet models, as expected.

Figure 4(b) provides an application designer a handbook
to guide selecting the receptive ﬁeld setting that can ful-
ﬁll the design goal.
If accuracy out-weights model size,
a larger k is desirable (i.e., fewer enhancement layers). If
model size is the primary concern for power-conservation
and frame-rate improvement (e.g., video analysis requiring
30 fps), then the designer can select a smaller k. For in-
stance, on ResNet-32, the bitrate of k = 0.6 is 200 KB, but
the bitrate of k = 1.4 is 300 KB. From the ﬁgure ResNet-
56 seems to be a better choice than ResNet-110, since given
the same accuracy requirement, ResNet-56 always enjoys a
much lower bitrate than ResNet-110.

4.1.2 MBS vs. Other Reduction Schemes

Table 1 compares the reduction achieved by MBS and
some representative methods. The top-half of the table
lists our evaluation on all ResNet models. For instance,
MBS reduces the model size of ResNet-1202 signiﬁcantly
(72.71%) with negligible accuracy drop (0.98%). The bot-
tom half of the table compares MBS with recently published
methods with the best reduction ratios. We set MBS at the
same accuracy level, MBS achieves the highest reduction
ratio (50.29%).

We also compared MBS with the naive α-scaling method
used by ResNet. The α-scaling multiplies the entire model
with the same scaling factor α, whereas MBS adaptively
sets the scaling factor by the information density of each
macroblock. Figure 5 plots the range of α from 0.6 to 0.9
with step size 0.1. Under the condition of having the similar
resultant model size, MBS outperforms α-scaling in predic-
tion accuracy on four model sizes.

4.2. ImageNet Dataset

ImageNet consists of 1.28 million training images and
50k images for the 1, 000 class validation. We trained all
models (except for DenseNet, explained next) by 90 epochs
with batch size set as 256. The learning rate is initially set to

9122

Accuracy vs. Reduction (%) CIFAR-10

Accuracy vs. Bitrate (KB) CIFAR-10

93.5

93

92.5

92

91.5

91

90.5

90

y
c
a
r
u
c
c
A

k=1.4

k=1.2

k=1.0

k=0.8

10

15

20

25

30

35

ResNet-20

ResNet-32

k=0.6

45

40
50
Reduction (%)
ResNet-44

93.5

93

92.5

92

91.5

91

90.5

90

y
c
a
r
u
c
c
A

55

60

65

70

75

80

k=1.4

k=1.2

k=1.0
k=0.8

k=0.6

0

100

200

ResNet-56

ResNet-110

ResNet-20

ResNet-32

300
400
Bitrate (KB)
ResNet-44

500

600

700

ResNet-56

ResNet-110

(a) Accuracy vs. Reduction (%)

(b) Accuracy vs. Bitrate (KB)

Figure 4. Tradeoff between accuracy and model reduction under different receptive ﬁeld settings. The value k for z = k × L ranges from
1.4 to 0.6 with step 0.2 for (a) accuracy vs. model reduction ratio (%) and (b) accuracy vs. model size (KB).

Table 2. Model reduction results of CNN models with standard convolution on ImageNet.

Top-1 [Diff.]

Top-5 [Diff.]

Parameters (×106) Reduction

Model

ResNet-18
MBS (L)

ResNet-101
MBS (L)

69.76%
69.40% [0.36]

89.08%
88.88% [0.20]

77.37%
76.66% [0.72]

93.56%
93.19% [0.37]

11.69
9.94

44.55
21.53

7.98
6.04

-

14.97%

-

51.67%

-

24.31%

Conﬁguration

[64, 128, 256, 512]
[64, 128, 256, 453]

[64, 128, 256, 512]
[64, 128, 174, 337]

DenseNet-BC-121
MBS (L)

74.65%
74.35% [0.20]

92.17%
91.92% [0.25]

β = [1, 1, 1, 1]
β = [1, 0.987, 0.832, 0.809]

α-scaling vs. Proposed (ResNet-110 on CIFAR-10)

4.2.1 Results of CNNs with Standard Convolution

y
c
a
r
u
c
c
A

94

93.5

93

92.5

92

91.5

0.62

0.86

1.1

1.39

Number of Parameters (106)
alpha-scaling
Proposed

Figure 5. The x-axis denotes the model size and the y-axis rep-
resents the prediction accuracy. The α-scaling sets the α value
ranges from 0.6 to 0.9 with step size 0.1. The proposed algorithm
compares the performance with similar model size.

0.1 and divided by 10 at epochs 30 and 60, respectively. For
DenseNet, we trained its model by 100 epochs with batch
size 128 and divided the learning rate by 10 at epochs 90 as
suggested in [11]. The data augmentation follows the Ima-
geNet script of PyTorch, which is the same as ResNet [8].
The weight decay is 10−4 for the CNNs with standard con-
volution (e.g., ResNet and DenseNet). The CNNs with
depth-wise separable convolution (e.g., ShufﬂeNet and Mo-
bileNet) set the weigh decay to 4 × 10−5 according to the
training conﬁgurations as suggested in ShufﬂeNet [31].

Table 2 shows that MBS is ﬂexible to work with differ-
ent CNN designs including very deep and complex CNN
models such as ResNet-101 and DenseNet-121. As shown
in Table 1, MBS can work with different depth conﬁgura-
tions of ResNet on the CIFAR-10 dataset. Table 2 further
shows consistent results when working on ImageNet. MBS
achieves 51.67% model reduction for ResNet-101, while
maintaining the same prediction accuracy. On a highly op-
timized deep model DenseNet-121 (a version of DenseNet-
BC-121 deﬁned in [11]), which has bottleneck modules and
transition layers already highly compressed by 50%. MBS
still can achieve additional 24.31% model reduction with
negligible accuracy loss.

To exhaustively compare with all prior works, we also
conducted experiments with ResNet-34. We divided the
learning rate by 10 at epoch 90 and trained the reduced
ResNet-34 with additional 10 epochs as a simpliﬁed ﬁne-
tune process. The state-of-the-art method NISP-34-B did
not specify its computation complexity in [30]. How-
ever, additional preprocessing steps are required to derive
the ﬁlter’s importance. For a model with np parameters
and N training images, these preprocessing steps require
O(np × N ) to acquire the CNN outputs and sort the fea-
tures with O(N 2.37) [24]. Table 3 shows that MBS slightly

9123

Table 3. Model reduction results of ResNet-34 on ImageNet.

Top-1 [Diff.]

Top-5 [Diff.]

Parameters (×106) Reduction

Conﬁguration

ResNet-34 (Original)
[16]
[30] NISP-34-A
[30] NISP-34-B

73.30%
72.17% [1.13]
72.95% [0.35]
72.31% [0.99]

91.42%
-
-
-

21.80
19.30

-
-

-

[64, 128, 256, 512]

10.80%
27.14%
43.68%

-
-
-

MBS (0.8 × L)

72.31% [0.99]

90.87% [0.55]

12.10

44.50% [64, 128, 192, 359]

Table 4. Model reduction results of MobileNet and ShufﬂeNet on ImageNet.

Model

Top-1 [Diff.]

Top-5 [Diff.]

Param. (×106) Reduction

ShufﬂeNet (g = 3)
Proposed (L)

MobileNet (L = 224)
Proposed (L)
Proposed (0.8 × L)

MobileNet (L = 192)
Proposed (L)
Proposed (0.8 × L)

65.01%
63.95% [1.06]

70.73%
70.52% [0.21]
69.90% [0.83]

85.89%
85.15% [0.74]

89.59%
89.57% [0.02]
89.21% [0.38]

68.88%
68.98% [−0.10]
68.05% [0.83]

88.34%
88.37% [−0.03]
87.77% [0.57]

MobileNetV2 (L = 224)
Proposed (1.2 × L)

71.8%
70.81% [0.99]

91.00%
89.89% [1.11]

1.88
1.49

4.23
4.00
3.50

4.23
3.93
3.14

3.47
3.14

-

20.74%

-

5.43%
17.26%

-

7.10%
25.77%

-

25.03%

Conﬁguration

[24, 240, 480, 960]
[24, 240, 444, 792]

[32, 64, 128, 256, 512, 1024]
[32, 64, 128, 256, 512, 958]
[32, 64, 128, 256, 474, 879]

[32, 64, 128, 256, 512, 1024]
[32, 64, 128, 256, 512, 937]
[32, 64, 128, 256, 441, 825]

β = [1, 1, 1, 1]
β = [1, 1, 0.9447, 0.7978]

outperforms NISP-34-B on ResNet-34 (by 0.8% at the same
accuracy level) and the complexity is only O(np × N ).

state-of-the-art compact CNN, MobileNetV22, and achieves
25.03% reduction with negligible distortion.

4.2.2 Results of CNNs with Depth-wise Convolution

5. Conclusion

We applied MBS to two CNN models with depth-wise con-
volution structures, ShufﬂeNet and MobileNet. The depth-
wise convolution structure already reduces CNN model size
signiﬁcantly. Table 4 shows that MBS can further reduce
these highly compact models. On ShufﬂeNet, MBS reduces
the model size by additional 20.74% with negligible distor-
tion. The depth-wise convolution and the unique shufﬂing
operation of ShufﬂeNet would increase the difﬁculty of the
objective function formulation for the optimization-based
methods. On the contrary, MBS can simply estimate the
channel-scaling factor for each CNN macroblock and per-
form model reduction.

We also evaluated MBS with MobileNet at different in-
put image resolutions. Table 4 shows that MBS achieves
17.26% and 25.77% reduction on L = 224 and L = 192,
respectively. Notice that when we set z = L, the prediction
accuracy of MobileNet-192 improves slightly. This result
suggests a possible smaller threshold value of z for Mo-
bileNet. Hence, we applied a slightly more aggressive set-
ting of z = 0.8 × L, which achieved a 25.77% model-size
reduction. Furthermore, our method is also applicable to the

We proposed a novel method to estimate the channel-
scaling factor for each CNN macroblock. Our proposed
MBS algorithm reduces model size guided by an informa-
tion density surrogate without signiﬁcantly degrading class-
prediction accuracy. MBS is ﬂexible in that it can work
with various CNN models (e.g., ResNet, DenseNet, Shuf-
ﬂeNet and MobileNet), and is also scalable in its ability to
work with ultra deep and highly compact CNN models (e.g.,
ResNet-1202). MBS outperforms all recently proposed
methods to reduce model size at low computation complex-
ity. With an adjustable receptive ﬁeld parameter, an applica-
tion designer can determine a proper tradeoff between pre-
diction accuracy and model size (implying DRAM size and
power consumption) by looking up a tradeoff table similar
to the table presented in Figure 4.

Acknowledgment

We would like to thank Tzu-Wei Sung for his help in

performing experiments on MobileNetV2.

2From TensorFlow GitHub.

9124

References

[1] A. Aghasi, A. Abdi, N. Nguyen, and J. Romberg. Net-trim:
Convex pruning of deep neural networks with performance
guarantee. In NIPS, pages 3180–3189. 2017. 2

[2] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, pages 3123–3131. 2015. 2

[3] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus.
Exploiting linear structure within convolutional networks for
efﬁcient evaluation.
In NIPS, NIPS’14, pages 1269–1277,
Cambridge, MA, USA, 2014. MIT Press. 2

[4] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang,
D. Vetrov, and R. Salakhutdinov. Spatially adaptive compu-
tation time for residual networks. In IEEE CVPR, July 2017.
2

[5] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
and W. J. Dally. Eie: Efﬁcient inference engine on com-
pressed deep neural network. ISCA, 2016. 2

[6] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. ICLR, 2016. 2

[7] B. Hassibi and D. G. Stork. Second order derivatives for
network pruning: Optimal brain surgeon.
In NIPS, pages
164–171, San Francisco, CA, USA, 1993. Morgan Kauf-
mann Publishers Inc. 2

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In IEEE CVPR, June 2016. 7

[9] Y. He, X. Zhang, and J. Sun. Channel pruning for acceler-
ating very deep neural networks. In IEEE ICCV, Oct 2017.
2

[10] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. CoRR, abs/1704.04861, 2017. 1, 3, 5

[11] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, July
2017. 1, 6, 7

[12] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks. In NIPS, pages 4107–
4115. 2016. 2

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions. In
BMVC. BMVA Press, 2014. 2

[14] N. Karmarkar. A new polynomial-time algorithm for linear

programming. Combinatorica, 4(4):373–395, Dec 1984. 2

[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, NIPS’12, pages 1097–1105, 2012. 1

Imagenet
In

[16] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.

[17] X. Lin, C. Zhao, and W. Pan. Towards accurate binary con-
volutional neural network. In NIPS, pages 344–352. 2017.
2

[18] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In IEEE CVPR, June
2015. 2

[19] H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable
architecture search. arXiv preprint arXiv:1806.09055, 2018.
2

[20] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. In IEEE ICCV, Oct 2017. 2

[21] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell. Rethink-

ing the value of network pruning. In ICLR, 2019. 2, 5

[22] W. Luo, Y. Li, R. Urtasun, and R. Zemel. Understanding
the effective receptive ﬁeld in deep convolutional neural net-
works. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett, editors, NIPS, pages 4898–4906. Curran As-
sociates, Inc., 2016. 4

[23] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV 2016, pages 525–542, 2016. 2

[24] G. Roffo, S. Melzi, and M. Cristani. Inﬁnite feature selec-
tion. In IEEE ICCV 2015, pages 4202–4210, Dec 2015. 2,
7

[25] V. Sze, Y. H. Chen, T. J. Yang, and J. S. Emer. Efﬁcient
processing of deep neural networks: A tutorial and survey.
Proceedings of the IEEE, 105(12):2295–2329, Dec 2017. 1,
3

[26] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In AAAI. 2017.
2

[27] S. Teerapittayanon, B. McDanel,

and H. T. Kung.
Branchynet: Fast inference via early exiting from deep neu-
ral networks. In editor, editor, ICPR, 2016. 2

[28] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In NIPS, pages
2074–2082. 2016. 2

[29] Z. Wu, T. Nagarajan, A. Kumar, S. Rennie, L. S. Davis,
K. Grauman, and R. Feris. Blockdrop: Dynamic inference
paths in residual networks. In IEEE CVPR, June 2018. 2

[30] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han,
M. Gao, C.-Y. Lin, and L. S. Davis. Nisp: Pruning networks
using neuron importance score propagation. In IEEE CVPR,
June 2018. 2, 6, 7, 8

[31] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mobile

Pruning ﬁlters for efﬁcient convnets. ICLR, 2017. 2, 6, 8

devices. CoRR, abs/1707.01083, 2017. 1, 7

9125

