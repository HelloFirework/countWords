Weakly Supervised Person Re-Identiﬁcation

Jingke Meng1,3 , Sheng Wu1 , and Wei-Shi Zheng1,2 ∗

1

School of Data and Computer Science, Sun Yat-sen University, China

2Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China

3Accuvision Technology Co. Ltd, China

mengjke@mail2.sysu.edu.cn, wush43@mail2.sysu.edu.cn, wszheng@ieee.org

Abstract

(cid:51)(cid:85)(cid:82)(cid:69)(cid:72)(cid:3)(cid:86)(cid:72)(cid:87)

(cid:42)(cid:68)(cid:79)(cid:79)(cid:72)(cid:85)(cid:92)(cid:3)(cid:86)(cid:72)(cid:87)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)
(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

In the conventional person re-id setting, it is assumed
that the labeled images are the person images within the
bounding box for each individual; this labeling across mul-
tiple nonoverlapping camera views from raw video surveil-
lance is costly and time-consuming. To overcome this difﬁ-
culty, we consider weakly supervised person re-id modeling.
The weak setting refers to matching a target person with an
untrimmed gallery video where we only know that the iden-
tity appears in the video without the requirement of anno-
tating the identity in any frame of the video during the train-
ing procedure. Hence, for a video, there could be multiple
video-level labels. We cast this weakly supervised person
re-id challenge into a multi-instance multi-label learning
(MIML) problem. In particular, we develop a Cross-View
MIML (CV-MIML) method that is able to explore potential
intraclass person images from all the camera views by in-
corporating the intra-bag alignment and the cross-view bag
alignment. Finally, the CV-MIML method is embedded in-
to an existing deep neural network for developing the Deep
Cross-View MIML (Deep CV-MIML) model. We have per-
formed extensive experiments to show the feasibility of the
proposed weakly supervised setting and verify the effective-
ness of our method compared to related methods on four
weakly labeled datasets.

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)
(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:51)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:36)

(cid:51)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:37)

(cid:51)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:36)

(cid:51)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:37)

(a) Conventional fully supervised setting

Probe set

Gallery set

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

Person A

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

Person B

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

Person C

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

Person D

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

Unknown identities

{Person A, Person B, Person C}

{Person B, Person D}

Missing targets

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445) (cid:445)

(b) Proposed weakly supervised setting

Figure 1. Comparison of two settings. (a) Conventional fully su-
pervised setting: image sequences in the probe and gallery set are
manually trimmed and labeled from video surveillance in a frame-
by-frame manner.
the
untrimmed videos in the gallery set are tagged by multiple video-
level labels, while the speciﬁc label of each individual is absent
from the labeling process.

(b) Proposed weakly supervised setting:

1. Introduction

Given an image from a set of probe images, the objec-
tive of person re-identiﬁcation (re-id) is to identify the same
person across a set of gallery images from nonoverlapping
camera views. The changes in illumination, camera view-
point, background and occlusions lead to considerable visu-
al ambiguity and appearance variation and make person re-
id a challenging problem. Several representative methods
[33, 32, 45, 20] have been developed to solve this problem.

∗Corresponding author

While numerous methods have been developed for ful-
ly supervised person re-id, conventionally, it is assumed
that for model training, 1) the images in the probe set and
gallery set are manually trimmed and labeled from raw
video surveillance (probably with the assistance of detec-
tion) frame-by-frame (as shown in Figure 1(a)), and 2) al-
l training samples are of the target to be matched, and no
outliers exist. Although such precise annotations could e-
liminate the difﬁculty of learning robust person re-id mod-
els, they require strong supervision, which makes the entire
learning process difﬁcult to adapt to large-scale person re-id
in a more practical and challenging scenario.

760

Instead of relying on costly labeling/annotations, we
wish to investigate the person re-id modeling in a weak-
ly supervised setting. This setting assumes that annotators
only need to take a rough glance at the raw videos to de-
termine which identities appear in such videos, and they do
not need to annotate the identity in any frame of the video.
That is, only the video-level label indicating the presence
of the identity is given, while the ground-truth regarding in
which frame and which bounding box in a frame the identi-
ty is present is not provided. In such a setting, the labeling
cost of person re-id can be greatly reduced compared to the
conventional fully supervised setting. We call this setting
weakly supervised person re-id.

More speciﬁcally, as shown in Figure 1(b), the ﬁrst row
of a video clip in the gallery set is annotated with a set of
video-level labels {Person A, Person B, Person C} indicat-
ing that Person A, Person B and Person C have appeared
in this video clip, but there is no additional prior knowl-
edge that precisely indicates which individual is Person A,
Person B or Person C. Hence, these labels are weak. Note
that it is possible that some labels for a video are missing
because the annotators fail to recognize (e.g., pedestrian-
s framed by yellow dotted lines in Figure 1(b)). It is also
practically possible that unknown identities appear in the
untrimmed video clips (e.g., pedestrians framed by red dot-
ted lines in Figure 1(b)). Overall, the videos in the gallery
set are untrimmed and tagged with the multiple video-level
weak labels in this weakly supervised setting. Based on this
setting, we aim to ﬁnd in the gallery the raw videos where
the target person appears, given a probe set of images from
nonoverlapping camera views.

To solve the problem of weakly supervised person re-
id, we consider every video clip in the gallery set as a bag;
each bag contains multiple instances of the person images
detected in each raw video clip and associates with mul-
tiple bag-level labels. For the probe set, it contains the
target individuals to be searched for in the gallery; thus,
each input is a set of manually trimmed images of the tar-
get person. For convenience, we also regard the probe input
as a bag. We consider the whole weakly supervised per-
son re-id problem as a multi-instance multi-label learning
(MIML) problem and develop a Cross-View MIML (CV-
MIML) method. Compared to existing MIML algorithm-
s [3, 2, 16, 15, 26, 46, 10], our CV-MIML is able to ex-
ploit similar instances within a bag for intra-bag alignment
and mine potential matched instances between bags that are
captured across camera views through embedding distribu-
tion prototype into MIML, which is called the cross-view
bag alignment in our modeling. Finally, we embed this CV-
MIML method into a deep neural network to form an end-
to-end deep cross-view multi-label multi-instance learning
(Deep CV-MIML) model.

To the best of our knowledge, this paper is the ﬁrst to

propose and study the weakly supervised problem in person
re-id. We have performed comprehensive experiments on
four datasets with one genuine dataset and three simulated
datasets. Since existing person re-id methods do not suit
the weakly supervised setting, we compare the proposed
method to other state-of-the-art MIML methods and several
state-of-the-art one-shot, unsupervised and sully supervised
person re-id methods. The results demonstrate the feasibil-
ity of the weakly supervised person re-id method and show
that the proposed Deep CV-MIML model is a superior ap-
proach to solving the problem.

2. Related Work

2.1. Person Re-identiﬁcation

Most studies of person re-id are supervised [43, 31, 7,
33, 32, 45, 20, 35, 28, 5] and require annotating each per-
son in the video precisely (e.g., indicating the frame and the
position in the frame within the video). It is impractical to
extend to the above person re-id methods in a more practi-
cal and challenging scenario due to the expensive cost of the
labeling process. So we propose the weakly supervised set-
ting for person re-id which only requires video-level weak
labels.

Recently, several unsupervised learning methods have
been developed to learn person re-id models [40, 9, 24,
23, 39, 19, 4]. The general idea of these methods is to
explore unlabeled data progressively by alternately assign-
ing pseudo-labels to unlabeled data and updating the model
according to these pseudo-labeled data. The unsupervised
learning process can be easily adapted to large-scale per-
son re-id since the unlabeled data can be accessed without
manual operations. However, the performance of these un-
supervised methods is limited because the visual ambiguity
and appearance variations are not easy to address due to the
lack of clear supervised information.

In the weakly supervised setting, the gallery set is com-
posed of the raw videos, which is closely related to the per-
son search [34] that aims to search for the target person
from the whole raw images. However, in the setting of the
person search, the manually annotated bounding boxes for
the gallery set are required to train the model in a fully su-
pervised manner, which is much more supervised than our
weakly supervised setting.

2.2. Multi-Instance Multi-Label Learning

In general, an object of interest has its inherent struc-
ture and it can be represented as a bag of instances with
multiple labels associated on the bag level. Multi-Instance
Multi-Label learning (MIML) [44] provides a framework
for handling this kind of problems. Due to the limitation of
the current person re-id methods in the weakly supervised
setting, we adopt the MIML formulation to solve our weak-

761

ly supervised re-id problem. During the past few years,
many related algorithms have been investigated and devel-
oped for MIML problems [3, 2, 16]. The MIML formu-
lation has also been applied in many practical vision do-
mains, such as image annotation [36, 25] and classiﬁcation
tasks [37, 6, 38, 41].

While it is possible to apply existing MIML to our prob-
lem, there still exist several intractable issues that may not
be readily resolved because of the following: 1) the exist-
ing MIML methods ignore mining the intra-bag variation
between similar instances belonging to the same person;
2) previous approaches are based on the idea that highly
relevant labels mean sharing common instances among the
corresponding classes, but the class labels are independent
from each other in person re-id; and 3) most MIML method-
s are not able to mine potential matched instances between
bags effectively when applied to person re-id for cross-view
matching. The proposed Deep Cross-View MIML model
for the person re-id can overcome the above limitations by
exploiting similar instances within a bag for intra-bag align-
ment and mining potential matched instances across camera
views simultaneously.

3. The Proposed Approach

In this section, we formally introduce the weakly super-
vised person re-id setting and then introduce the Deep CV-
MIML model for addressing this problem.

3.1. Problem Statement and Notation

In the weakly supervised person re-id setting, our goal
is to ﬁnd the videos that the target person appears in, given
a probe set of images from nonoverlapping camera views.
Suppose that we have C known identities from V camera
views and that every known identity appears in at least two
camera views. Since some unknown identities (e.g., pedes-
trians framed by red dotted lines in Figure 1(b)) would ap-
pear in the untrimmed videos, these unknown identities can
be afﬁliated to a novel class; we deﬁne an extra 0-class to
represent it. For simplicity, we denote the overall number
of classes by ˜C = C + 1.

In our learning, given NX videos, the training set X con-
sists of two distinct parts: the probe set Xp and the gallery
set Xg. The videos in the gallery set are untrimmed and
tagged with the multiple video-level weak labels that indi-
cate the presence of individuals as shown in Figure 1(b); the
person images within a raw video in the gallery set are au-
tomatically detected in advance. Note that even though the
person images are detected during this stage, the speciﬁc
label of each individual is still unknown.

We consider every raw video as a bag; each bag con-
tains multiple instances of the person images detected in
each video. For the probe set, each query is composed of

b , y1

b , ..., yC

a set of detected images of the same person. For conve-
nience, we also regard each query in the probe set as a bag.
More speciﬁcally, the training set can be denoted by X =
{Xp, Xg}, where the probe set is Xp = {(Xb, yb, vb)}Np
b=1
and the gallery set is Xg = {(Xb, yb, vb)}Ng
b=1, NX =
Np + Ng. For the bags (videos) in the probe set, each bag
Xb containing the same person images is labeled by yb un-
der the vb-th camera view, where vb ∈ {1, 2, ..., V }, and
b ] ∈ {0, 1} ˜C is a label vector containing
yb = [y0
˜C class labels, in which yc
b = 1 if the c-th label is tagged
for Xb, and yc
b = 0 otherwise. In contrast to the conven-
tional person re-id, for the bags (videos) in the gallery set,
yc
b = 1 denotes that the c-th identity appears in this bag
(video), while yc
b = 0 denotes uncertainty of whether the c-
th identity has appeared in this video. Moreover, the bag Xb
consists of nb instances xb,1, xb,2, ..., xb,i, ..., xb,nb , where
xb,i = fe(Ib,i; θ) ∈ Rd is the feature vector extracted from
the corresponding person image Ib,i, and fe(·; θ) is a learn-
able feature extractor.

3.2. Cross-View MIML for Person Re-id

We cast

the weakly supervised person re-id as the
problem of multi-instance multi-label learning (MIML)
and present the cross-view multi-instance multi-label (CV-
MIML) learning method to solve this problem.

3.2.1 Weakly Supervised Person Re-id by MIML

For the probe set Xp, all instances {xb,i}nb

For the task of weakly supervised classiﬁcation, we formu-
late a MIML classiﬁer for our weakly supervised person re-
id. With this classiﬁer fc(·; W ), the high-dimensional input
xb,i ∈ Rd can be transformed into a ˜C-dimensional vector
˜C that can be interpreted as a label
˜yb,i = fc(xb,i; W ) ∈ R
distribution, embedding the similarities among all classes.
i=1 in each bag
Xb are tagged with the same label yb. For the gallery set
Xg, all instances {xb,i}nb
i=1 in each bag Xb share the same
weak video-level label yb. The softmax classiﬁer cannot be
directly applied to the instances in the gallery set because
the speciﬁc label of each instance is absent. Therefore, the
instances from the probe set Xp and gallery set Xg are sep-
arately processed by the following two procedures to learn
a classiﬁcation model.

On the one hand, we expect the estimated label distribu-
tion to eventually approximate the true one; thus, the clas-
siﬁcation loss for these instances in the probe set can be
written as follows:

Lp =

1
Np

(cid:2)
Xb∈Xp

(cid:2)

(cid:2)

i∈{1,··· ,nb}

c∈{0,··· ,C}

(−yc

b log ˜yc

b,i),

where yc
Xb at the c-th entry, ˜yc

(1)
b denotes the ground truth video-level labels of bag
b,i is the c-th estimated probability

762

Detector

{Person B, Person D}

Intra-bag 
Alignment

Person B
Person D
Novel Class 

Figure 2. Illustration of the intra-bag alignment. The instances in
the rectangular with dotted purple lines are the seed instances cor-
responding to the Person B and Person D, respectively. Then two
groups (e.g., framed by the purple oval dotted lines) are formed
around these two seed instances. In the intra-bag alignment pro-
cess, the label distributions of instances belonging to the same
group are aligned such that these instances can be compact be-
tween each other in the learned feature space.

of the i-th instance in bag Xb, and Np indicates the overall
number of instances involved in the loss calculation.

On the other hand, we further expect that our classiﬁer
can fully exploit the weak labels to learn a more robust re-id
model. More speciﬁcally, for any tagged label c in bag Xb,
we select an instance with the largest prior probability w.r.t
the c-th class as the seed instance xb,qc , where the index qc
can be deﬁned by

qc = argmaxi∈{1,2,··· ,nb}{˜yc

b,i}.

(2)

Then we force the estimated label of the seed instance ap-
proximate to the corresponding tagged video-level label.
Accordingly, we deﬁne the classiﬁcation loss for the gallery
set as follows:

Lg =

1
Ng

(cid:2)
Xb∈Xg

(cid:2)

c∈{0,··· ,C}

(−yc

b log max{˜yc

b,1, ˜yc

b,2, ..., ˜yc

b,nb}),

(3)
where the operation max{˜yc
} is used to se-
lect the largest prior probability of the seed instance xb,qc .
In such a case, the classiﬁcation model can be leveraged to
infer the prior probability of each instance in the bag.

b,2, ..., ˜yc

b,1, ˜yc

b,nb

Combining the two classiﬁcation losses for the probe set
(Eq.(1)) and the gallery set (Eq.(3)), we obtain the following
MIML classiﬁcation loss:

LC = Lp + Lg.

(4)

3.2.2

Intra-bag Alignment

Since individuals often appear in a video across several con-
secutive frames (e.g., green dotted lines in Fig. 2), there will
be a set of instances, probably of the same person, in a bag

in the weakly labeled gallery set. These instances are ex-
pected to be merged into a group such that the instances
belonging to the same group should be close to each other
in the learned feature space. However, the MIML classiﬁ-
er cannot achieve this agglomeration and the classiﬁer only
processes the instance with the largest prior probability w.r.t
the corresponding classes, which we call the seed instance.

To this end, we expect that the set of instances probably
of the same person can be gathered around the seed instance
xb,qc that has the largest prior probability with respect to the
c-th class in the bag Xb. Then, we form a group that con-
tains the instances gathered around the seed instance xb,qc
by Gb,c = {p|xb,p ∈ Nqc and ˜yc
}. In this group,
the selected instances should be among the K-nearest neigh-
bors Nqc in the feature space around the seed instance xb,qc .
Additionally, the prior probability corresponding to the c-th
class of these instances should be no less than γ ˜yc
, where
˜yc
is the prior probability of the corresponding seed in-
b,qc
stance. Here, γ ∈ (0, 1) is a relaxation parameter. Then,
the intra-bag alignment loss can be deﬁned as follows:

b,p ≥ γ ˜yc

b,qc

b,qc

LIA =

1

NIA

(cid:2)
Xb∈Xg

(cid:2)

c∈{0,··· ,C}

(cid:2)
p∈Gb,c

yc
bDKL(˜yb,p(cid:4)˜yb,qc ),

(5)

DKL(˜yb,p(cid:4)˜yb,qc ) = (cid:2)

c∈{0,··· ,C}

˜yc
b,p(log ˜yc

b,p − log ˜yc

b,qc ).

(6)
The intra-bag alignment loss in Eq.(5) is designated to e-
valuate the discrepancy of the label distribution between the
instances within the group Gb,c and the corresponding seed
instance xb,qc . The discrepancy between two label distribu-
tions is deﬁned by the Kullback-Leibler divergence depict-
ed in Eq.(6). As illustrated in Figure 2, by minimizing the
intra-bag alignment loss, the features of the same group can
become closer to each other due to the alignment between
potential instances of the same class in a bag.

3.2.3 Cross-view Bag Alignment

The intra-bag alignment term mainly considers the person
images that appear in the same bag. We further expect to
mine potential matched images of the same person between
bags not only from the same camera view but also from non-
overlapping camera views. In the meantime, all instances
belonging to the same person should form a compact clus-
ter in the learned feature space. For this purpose, we in-
troduce a distribution prototype for each class, and then all
the potential matched images of the same person from all
the camera views are expected to be aligned to the corre-
sponding distribution prototype. Formally, the distribution
prototype of the c-th class at the current epoch t is denoted

763

{Person A, Person B, Person C}

{Person B, Person D}

Dataset

# Camera

views

# Identities

(training/testing)

WL-DukeMTMC-REID

WL-PRID2011
WL-iLIDS-VID

WL-MARS

8
2
2
6

880/1695
100/100
150/150
631/630

# Training

BBoxes

(probe/gallery)
60,267/923,879

11,201/8,191
9,731/11,278

# Testing
BBoxes

(probe/gallery)
116,128/904,066

12,129/8,512
12,129/8,512

38,324/460,236

36,988/472,978

Table 1. Detailed information of the one genuine and three new
simulated datasets for the weakly supervised person re-id.

conﬁdence score is obtained from a deep network that is
pretrained on the probe set.

3.5. Testing

In the testing phase, the probe set and gallery set are
formed in the same manner as the training set. Accordingly,
our goal is to ﬁnd the raw videos where the target person
appears in the weakly supervised setting. Speciﬁcally, for a
bag Xp in the testing probe set, the feature of this bag xp
is the average pooling of features over all image frames in
this sequence. Then, the distance between the bag xp in the
testing probe set and the bag xq in the testing gallery set is

D(p, q) = min{d(xp, xq,1), d(xp, xq,2), ..., d(xp, xq,np )}
(12)

where d is the Euclidean distance operator.

4. Experiments

4.1. Datasets and Settings

the

The experiments were carried out on one genuine dataset
WL-DukeMTMC-REID and three simulated datasets WL-
PRID 2011, WL-iLIDS-VID and WL-MARS. The probe
set contained all the target individuals to search for in the
gallery set, and every known identity had trimmed image
sequences in the probe set for all datasets. The remainder
of the videos formed the gallery set. The four datasets were
constructed as follows.
WL-DukeMTMC-REID For
genuine WL-
DukeMTMC-REID dataset, a set of raw videos DukeMTM-
C [27] is available. DukeMTMC is a multi-camera dataset
recorded outdoors at the Duke University campus with
8 synchronized cameras.
The WL-DukeMTMC-REID
dataset was constructed from the ﬁrst 50-minute raw HD
videos. We split the raw videos into halves; the training
set and testing set both have 25-minute raw videos. There
are 880 and 1,695 identities appearing in at least two
camera views in the training and testing sets. To form
the gallery set for the WL-DukeMTMC-REID dataset, we
ﬁrst randomly split the raw video into short video clips,
with each clip comprising between 20 and 120 raw frames.
Afterwards, we applied Mask-RCNN [12] to these video
clips to detect individuals. Note that even though we obtain
the bounding boxes, the speciﬁc label of each individual is
still unknown for the gallery set. The details of this dataset
is shown in Table 1.

For the three simulated datasets WL-PRID 2011, WL-
the raw videos of these

iLIDS-VID and WL-MARS,

datasets are unavailable, so we formed the simulated
datasets as follows.
First, we randomly selected one
trimmed image sequence for every known identity to for-
m the probe set, and the rest of videos were used to form
the gallery set. Then, 3 ∼ 8 sequences were randomly se-
lected to form a weakly labeled bag, where only bag-level
labels were available, and the speciﬁc label of each individ-
ual was unknown. In this way, we converted three existing
video-based person re-id datasets PRID 2011 [14], iLIDS-
VID [30] and MARS [42] to WL-PRID 2011, WL-iLIDS-
VID and WL-MARS, respectively, for weakly supervised
person re-id. The details of these new datasets are shown in
Table 1.

4.2. Evaluation Protocol

To evaluate the performance of our method, the wide-
ly used cumulative matching characteristics (CMC) curve
and mean average precision (mAP) are used for quantita-
tive measurement.

4.3. Evaluation of the Deep CV-MIML Model

In our modeling of Deep CV-MIML, we introduce 1)
the intra-bag alignment term, 2) the cross-view bag align-
ment term, and 3) an entropy regularization to eliminate
outlier instances. To evaluate the efﬁciency of the each
component, we adopt the MIML classiﬁer (Eq. (4)) as the
baseline method and conduct ”baseline with IA”, ”baseline
with CA” and ”baseline with entropy” for comparison to
prove the effectiveness of all proposed components sepa-
rately. The results are reported in Table 2.

Comparing the CV-MIML method to the baseline MIML
classiﬁer in Table 2, it is clear that our CV-MIML method
is very effective in handling the weakly supervised person
re-id problem. By simultaneously minimizing the intra-
bag alignment and cross-view bag alignment loss func-
tions, the same identities from the same camera view and
nonoverlapping camera views could be more coherent with
each other. These results represent a notable improvemen-
t in the rank-1 matching accuracy, e.g., 10.79%, 5.00%,
18.67% and 13.41% improvements were observed on the
WL-DukeMTMC-REID, WL-PRID 2011, WL-iLIDS-VID
and WL-MARS datasets, respectively. Considering mAP,
we also obtain 8∼14% improvement on these four weakly
labeled re-id datasets.

Moreover, as reported in Table 2, the ablation study in-
dicates that adopting the intra-bag alignment term will lead
to a signiﬁcant rise of the model performance because the
intra-bag alignment term facilitates forming a coherent clus-
tered structure for instances of the same identity. Addition-
ally, including the cross-view bag alignment term would al-
so notably increase the performance of CV-MIML (with ap-
proximately 5%, 1%, 11% and 10% rise of rank-1 matching
accuracy on the WL-DukeMTMC-REID, WL-PRID 2011,

765

WL-DukeMTMC-REID

CV-MIML

baseline + IA
baseline+CA

baseline+entropy

baseline

WL-PRID2011

CV-MIML
baseline+IA
baseline+CA

baseline+entropy

baseline

WL-iLIDS-VID

CV-MIML
baseline+IA
baseline+CA

baseline+entropy

baseline

WL-MARS
CV-MIML
baseline+IA
baseline+CA

baseline+entropy

baseline

r=1
78.05
74.69
72.92
70.56
67.26

r=1
72.00
69.00
68.00
70.00
67.00

r=1
60.00
55.33
52.67
44.67
41.33

r=1
66.88
62.15
63.09
60.88
53.47

r=5
90.50
88.50
87.96
85.90
84.90

r=5
89.00
89.00
87.00
89.00
86.00

r=5
80.00
80.67
78.00
69.33
70.00

r=5
82.02
80.44
79.97
79.34
71.77

r=10
93.75
92.15
92.04
90.15
89.50

r=10
95.00
93.00
96.00
96.00
95.00

r=10
87.33
89.33
88.00
81.33
83.33

r=10
87.22
85.80
84.23
85.49
79.02

r=20
95.99
94.81
94.75
92.68
92.68

r=20
99.00
98.00
98.00
99.00
97.00

r=20
96.67
95.33
95.33
92.67
94.67

r=20
91.48
89.75
88.96
89.43
85.49

mAP
59.53
56.97
55.30
53.05
50.96

mAP
70.78
65.89
63.72
67.32
62.87

mAP
56.01
53.78
50.58
44.99
42.26

mAP
55.16
50.27
50.61
49.13
40.31

Table 2. Ablation study of the proposed CV-MIML method. The
matching accuracy values (%) at rank(r) = 1, 5, 10, 20 and mAP
are shown on the four datasets. The best results are shown in black
boldface font.

WL-iLIDS-VID and WL-MARS datasets, respectively) be-
cause the cross-view bag alignment is useful for making the
features of the same identities from nonoverlapping camera
views aligned to each other in the feature space.

Finally, Table 2 indicates that the entropy regularization
term also plays a signiﬁcant role in our CV-MIML model,
as with it, the effect of outlier instances can be eliminated,
thus boosting the performance of our model.

WL-DukeMTMC-REID

MIMLfast[16]
DeepMIML[10]
Deep CV-MIML

WL-PRID2011
MIMLfast[16]
DeepMIML[10]
Deep CV-MIML

WL-iLIDS-VID

MIMLfast[16]
DeepMIML[10]
Deep CV-MIML

WL-MARS
MIMLfast[16]
DeepMIML[10]
Deep CV-MIML

r=1
13.63
65.37
78.05

r=1
29.00
67.00
72.00

r=1
28.00
44.00
60.00

r=1
20.50
47.16
66.88

r=5
44.66
82.30
90.50

r=5
56.00
90.00
89.00

r=5
58.67
70.00
80.00

r=5
37.22
70.19
82.02

r=10
55.69
86.90
93.75

r=10
72.00
94.00
95.00

r=10
69.33
81.33
87.33

r=10
43.06
76.18
87.22

r=20
64.78
90.68
95.99

r=20
87.00
99.00
99.00

r=20
78.67
89.33
96.67

r=20
52.05
81.07
91.48

mAP
10.05
48.02
59.53

mAP
31.66
61.80
70.78

mAP
27.42
43.49
56.01

mAP
11.39
36.59
55.16

Table 3. Comparison with state-of-the-art MIML methods. The
best results are in black boldface font.

WL-MARS datasets, respectively. Moreover, comparing
the proposed method to the Deep MIML method, the mAP
matching gain on all datasets can reach 11.51%, 8.98%,
12.52% and 18.57% on the WL-DukeMTMC-REID, WL-
PRID 2011, WL-iLIDS-VID and WL-MARS datasets, re-
spectively. These results indicate the advantage of our Deep
CV-MIML model in handling the weakly supervised person
re-id problem. The better performance is mainly due to the
newly designed intra-bag alignment term and cross-view
bag alignment term. With these terms, the features of the
same individual obtained from the same camera view and
across nonoverlapping camera views can be more coherent,
while the functions of these two terms are not considered in
MIMLfast and DeepMIML.

4.4. Comparison with State-of-the-Art MIML

4.5. Comparison with Related Re-id Methods

Methods

In Table 3, we report the comparison of our method
to existing state-of-the-art MIML learning methods Deep-
MIML [10] and MIMLfast [16]. The DeepMIML [10]
method is an end-to-end deep neural network that integrates
the instance representation learning process into the MIM-
L learning. For a fair comparison, we reimplemented this
method using the same CNN structure and the same train-
ing process. The MIMLfast [16] approach is a conventional
two-stage framework that ﬁrst requires extracting the image
features and then learns a discriminative representation. In
this study, we extracted the features from a Resnet-50 C-
NN that was pretrained on the 3DPeS [1], CUHK01 [21],
CUHK03 [22], Shinpuhkan [17] and VIPeR [11] person re-
id datasets and then performed the MIML learning based on
the MIMLfast method.

The comparison shows that the proposed Deep CV-
MIML model outperformed the existing MIML methods.
The proposed Deep CV-MIML model clearly outperformed
the second-best method DeepMIML on the four datasets.
Speciﬁcally, the extra gain of the rank-1 matching accuracy
between the Deep CV-MIML network and the DeepMIML
method is 12.68%, 5.00%, 16.00% and 19.72% on the WL-
DukeMTMC-REID, WL-PRID 2011, WL-iLIDS-VID and

As existing supervised person re-id methods could not
be applied to our weakly supervised setting directly, we
compare our method to unsupervised person re-id method-
s, such as CAMEL [40], PUL [8] and the one-shot person
re-id method called EUG [32]. Among the listed method-
s, the CAMEL method is a conventional two-stage frame-
work that ﬁrst requires extracting the image features and
then learns an asymmetric representation. PUL and EU-
G are progressive methods that alternate between assign-
ing the pseudo-labels to the tracklets and training the CNN
model according to these pseudo-labeled data samples. To
further demonstrate the effectiveness of our method, we also
compared with a state-of-the-art fully supervised approach
PCB[29]. The results are reported in Table 4. Compared to
unsupervised or one-shot methods, the performance of these
methods is consistently unsatisfactory in comparison to that
of the proposed Deep CV-MIML model. The table can also
tell us that the performance of our model (Deep CV-MIML)
is comparable to the fully supervised model PCB on the
WL-DukeMTMC-REID and WL-MARS datasets.

4.6. Hyperparameter Analysis

There are four hyperparameters involved in our CV-
MIML formulation. The trade-off parameter δ is used to

766

WL-DukeMTMC-REID

CAMEL [40]

PUL[8]
EUG[32]

Deep CV-MIML

PCB[29]

WL-PRID2011

CAMEL [40]

PUL[8]
EUG[32]

Deep CV-MIML

PCB[29]

WL-iLIDS-VID

CAMEL [40]

PUL[8]
EUG[32]

Deep CV-MIML

PCB[29]

WL-MARS
CAMEL [40]

PUL[8]
EUG[32]

Deep CV-MIML

PCB[29]

r=1
0.53

-

35.93
78.05
79.82

r=1
2.00
32.00
55.00
72.00
88.00

r=1
4.67
20.00
26.67
60.00
72.00

r=1
0.32

-

25.87
66.88
68.14

r=5
0.77

-

50.74
90.50
90.38

r=5
11.00
58.00
83.00
89.00
97.00

r=5
16.00
44.00
60.67
80.00
89.33

r=5
1.10

-

39.59
82.02
84.07

r=10
1.18

-

59.41
93.75
93.45

r=10
20.00
71.00
93.00
95.00
99.00

r=10
26.67
59.33
72.00
87.33
92.67

r=10
2.52

-

46.21
87.22
86.28

r=20
3.24

-

66.96
95.99
96.17

r=20
44.00
85.00
97.00
99.00
99.00

r=20
43.33
76.00
86.67
96.67
96.00

r=20
5.52

-

55.21
91.48
90.54

mAP
0.90

-

21.94
59.53
62.09

mAP
4.59
35.28
53.26
70.78
87.35

mAP
6.26
22.56
29.86
56.01
69.87

mAP
0.56

-

15.63
55.16
54.18

Table 4. Comparison with related re-id methods. The 1st/2nd best
results are indicated in red/blue.

balance the weight of LIA, LCA and LE with respect to
the overall CV-MIML loss in Eq.
(10). During train-
ing, we consider δ = w(t), a time-dependent function of
time t. To verify the advantage of this approach, we com-
pared the performance to that of a ﬁxed value of δ, where
δ = 0.01, 0.1, 1, 10 to investigate the impact of δ on the
overall performance on the WL-PRID 2011 and WL-iLIDS-
VID datasets. As shown in Figure 4(a), the time-dependent
setting is preferable. The reason is that the reliability of the
intra-bag alignment and cross-view bag alignment process
is tightly related to the conﬁdence of the re-id model by the
seed instances selection and the distribution prototype cal-
culation. Additionally, the conﬁdence of the re-id model is
fairly low in the beginning and then steadily increases dur-
ing the training procedure. Similarly, the weight parameter
δ ∈ [0, 1] initially increases during the early training stage,
subsequently reaching saturation at approximately the max-
imum value 1 once the model has been sufﬁciently trained.

The group formed in the intra-bag alignment process is
closely related to parameters K and γ. Parameter K rep-
resents selecting the K-nearest neighbors in the feature s-
pace, and parameter γ controls the number of instances cor-
responding to those with the largest prior probabilities that
should be shared with the same weak label. The impacts of
K and γ are reported in Figure 4(b) and Figure 4(c). The
results suggest that the best performance can be reached on
both datasets if γ = 0.2 and K is approximately 15.

The impact of α is presented in Figure 4(d). Parameter
α controls the impact of the historical distribution prototype
when calculating the distribution prototype for the curren-
t epoch in Eq. (8). The ﬁgure suggests that the performance
with and without historical information in the calculation of
the distribution prototype is distinct. Speciﬁcally, the worst
performance is observed if α = 0, i.e., involving the histori-

70

60

50

40

30

20

10

)

%

(
 

y
c
a
r
u
c
c
a

 

g
n
i
h
c
t
a
M

0
0.01

Rank-1 on WL-iLIDS-VID
mAP on WL-iLIDS-VID
Rank-1 on WL-PRID 2011
mAP on WL-PRID 2011

0.1

1

10

wt

(a) Parameter δ

)

%

(
 
y
c
a
r
u
c
c
a
g
n
h
c
t
a
(cid:48)

i

 

70

60

50

40

30
0.

Rank-1 on WL-iLIDS-VID
mAP on WL-iLIDS-VID
Rank-1 on WL-PRID 2011
mAP on WL-PRID 2011

0.2

0.4

0.6

0.8

i

h
c
t
a
(cid:48)

)

%

(
 
y
c
a
r
u
c
c
a
g
n

 

i

h
c
t
a
M

)

%

(
 

y
c
a
r
u
c
c
a
g
n

 

75

70

65

60

55

50

45

70

65

60

55

50

Rank-1 on WL-iLIDS-VID
mAP on WL-iLIDS-VID
Rank-1 on WL-PRID 2011
mAP on WL-PRID 2011

5

10

15

20

(b) Parameter K

Rank-1 on WL-iLIDS-VID
mAP on WL-iLIDS-VID
Rank-1 on WL-PRID 2011
mAP on WL-PRID 2011

45
0.0

0.2

0.4

0.6

0.8

(c) Parameter γ

(d) Parameter α

Figure 4. Performance illustrations for the Deep CV-MIML model
with different hyperparameters.

cal information that eliminates the bias of the current output
is useful for the calculation of the distribution prototype.

5. Conclusion

We aim to remove the need for costly labeling effort-
s for conventional person re-id by considering weakly su-
pervised person re-id modeling. In this weakly supervised
setting, no speciﬁc annotations of individuals inside gallery
videos are necessary; the only requirement is the indication
of whether or not a person appears in a given video. In such
a setting, one can search for individuals and the videos that
they appear in, given a (set of) probe person image(s). We
cast the weakly supervised person re-id problem as a multi-
instance-multi-label (MIML) problem. We develop a cross-
view MIML (CV-MIML) method, which is able to mine po-
tential intraclass variation in a bag and potential cross-view
change between instances of the same person across bags
from all camera views. Finally, CV-MIML is optimized by
being embedded in a deep neural network. The experimen-
tal results have veriﬁed the feasibility of weakly supervised
modeling for person re-id and have also shown the effec-
tiveness of the proposed CV-MIML models.

Acknowledgment

This work was supported partially by the Nation-
al Key Research and Development Program of Chi-
na (2018YFB1004903), NSFC(61522115), Guangdong
Province Science and Technology Innovation Leading Tal-
ents (2016TX03X157), and the Royal Society Newton Ad-
vanced Fellowship (NA150459).

767

References

[1] Davide Baltieri, Roberto Vezzani, and Rita Cucchiara. 3d-
pes: 3d people dataset for surveillance and forensics. In J-
HGBU Workshop, pages 59–64, 2011. 7

[2] Forrest Briggs, Xiaoli Z Fern, and Raviv Raich. Context-
aware MIML instance annotation. In ICDM, pages 41–50,
2013. 2, 3

[3] Forrest Briggs, Xiaoli Z Fern, Raviv Raich, and Qi Lou.
Instance annotation for multi-instance multi-label learning.
TKDD, 7(3):1–14, 2013. 2, 3

[4] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Deep
association learning for unsupervised video person re-
identiﬁcation. In BMVC, 2018. 2

[5] Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, and Jian-
Huang Lai. Person re-identiﬁcation by camera correlation
aware feature augmentation. TPAMI, 40(2):392–408, 2018.
2

[6] Zenghai Chen, Zheru Chi, Hong Fu, and Dagan Feng. Multi-
instance multi-label image classiﬁcation: A neural approach.
Neurocomputing, 99:298–306, 2013. 3

[7] De Cheng, Yihong Gong, Xiaojun Chang, Weiwei Shi,
Alexander Hauptmann, and Nanning Zheng. Deep feature
learning via structured graph Laplacian embedding for per-
son re-identiﬁcation. PR, 82:94–104, 2018. 2

[8] Hehe Fan, Liang Zheng, Chenggang Yan, and Yi Yang.
Unsupervised person re-identiﬁcation: clustering and ﬁne-
tuning. TOMM, 14(4):83, 2018. 7, 8

[9] Hehe Fan, Liang Zheng, and Yi Yang. Unsupervised person
re-identiﬁcation: clustering and ﬁne-tuning. arXiv preprint
arXiv:1705.10444, 2017. 2

[10] Ji Feng and Zhi-Hua Zhou. Deep MIML network. In AAAI,

pages 1884–1890, 2017. 2, 7

[11] Douglas Gray, Shane Brennan, and Hai Tao. Evaluating ap-
pearance models for recognition, reacquisition, and tracking.
In PETS Workshop, pages 1–7, 2007. 7

[12] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask R-CNN. In ICCV, pages 2980–2988, 2017. 5,
6

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 5

[14] Martin Hirzer, Csaba Beleznai, Peter M. Roth, and Horst
Bischof. Person re-identiﬁcation by descriptive and discrim-
inative classiﬁcation. In SCIA, pages 91–102, 2011. 6

[15] Yu-Feng Li Ju-Hua Hu and Yuan Jiang Zhi-Hua Zhou. To-
wards discovering what patterns trigger what labels. In AAAI,
pages 1012–1018, 2012. 2

[16] Sheng-Jun Huang, Wei Gao, and Zhi-Hua Zhou. Fast multi-
In AAAI, pages 1868–1874,

instance multi-label learning.
2014. 2, 3, 7

[17] Yasutomo Kawanishi, Yang Wu, Masayuki Mukunoki, and
Michihiko Minoh. Shinpuhkan2014: A multi-camera pedes-
trian dataset for tracking people across multiple cameras. In
FCV Workshop, 2014. 7

[19] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised
person re-identiﬁcation by deep learning tracklet association.
In CVPR, 2018. 2

[20] Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang.
Diversity regularized spatiotemporal attention for video-
based person re-identiﬁcation.
In CVPR, pages 369–378,
2018. 1, 2

[21] Wei Li and Xiaogang Wang. Locally aligned feature trans-

forms across views. In CVPR, pages 3594–3601, 2013. 7

[22] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-
reid: Deep ﬁlter pairing neural network for person re-
identiﬁcation. In CVPR, pages 152–159, 2014. 7

[23] Zimo Liu, Dong Wang, and Huchuan Lu. Stepwise metric
promotion for unsupervised video person re-identiﬁcation.
In ICCV, pages 2448–2457, 2017. 2

[24] Xiaolong Ma, Xiatian Zhu, Shaogang Gong, Xudong Xie,
Jianming Hu, Kin-Man Lam, and Yisheng Zhong. Person re-
identiﬁcation by unsupervised video matching. PR, 65:197–
210, 2017. 2

[25] Cam-Tu Nguyen, De-Chuan Zhan, and Zhi-Hua Zhou.
Multi-modal image annotation with multi-instance multi-
label LDA. In IJCAI, pages 1558–1564, 2013. 3

[26] Anh T Pham, Raviv Raich, Xiaoli Z Fern, Jes´us P´erez Arria-
ga, et al. Multi-instance multi-label learning in the presence
of novel class instances. In ICML, pages 2427–2435, 2015.
2

[27] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,
and Carlo Tomasi. Performance measures and a data set
for multi-target, multi-camera tracking. In ECCV workshop,
pages 17–35, 2016. 6

[28] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang
Wang. Mask-guided contrastive attention model for person
re-identiﬁcation. In CVPR, pages 1179–1188, 2018. 2

[29] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin
Wang. Beyond part models: Person retrieval with reﬁned
part pooling (and a strong convolutional baseline). In ECCV,
pages 480–496, 2018. 7, 8

[30] Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin
Wang. Person re-identiﬁcation by video ranking. In ECCV,
pages 688–703, 2014. 6

[31] Lin Wu, Yang Wang, Junbin Gao, and Xue Li. Deep adaptive
feature embedding with local sample distributions for person
re-identiﬁcation. PR, 73:275–288, 2018. 2

[32] Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang,
and Yi Yang. Exploit the unknown gradually: One-shot
video-based person re-identiﬁcation by stepwise learning. In
CVPR, pages 5177–5186, 2018. 1, 2, 7, 8

[33] Yang Wu, Jie Qiu, Jun Takamatsu, and Tsukasa Ogasawara.
Temporal-enhanced convolutional network for person re-
identiﬁcation. In AAAI, pages 7412–7419, 2018. 1, 2

[34] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xi-
aogang Wang. End-to-end deep learning for person search.
arXiv preprint arXiv:1604.01850, 2, 2016. 2

[18] Samuli Laine and Timo Aila. Temporal ensembling for semi-
supervised learning. arXiv preprint arXiv:1610.02242, 2016.
5

[35] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli
Ouyang. Attention-aware compositional network for person
re-identiﬁcation. arXiv preprint arXiv:1805.03344, 2018. 2

768

[36] Xin-Shun Xu, Xiangyang Xue, and Zhi-Hua Zhou. Ensem-
ble multi-instance multi-label learning approach for video
annotation task. In ACM MM, pages 1153–1156, 2011. 3

[37] Oksana Yakhnenko and Vasant Honavar. Multi-instance
multi-label learning for image classiﬁcation with large vo-
cabularies. In BMVC, pages 1–12, 2011. 3

[38] Hao Yang, Joey Tianyi Zhou, Jianfei Cai, and Yew Soon
Ong. MIML-FCN+: Multi-instance multi-label learning via
fully convolutional networks with privileged information. In
CVPR, pages 5996–6004, 2017. 3

[39] Mang Ye, Andy Jinhua Ma, Liang Zheng, Jiawei Li, and
Pong C Yuen. Dynamic label graph matching for unsuper-
vised video re-identiﬁcation.
In ICCV, pages 5152–5160,
2017. 2

[40] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Cross-
view asymmetric metric learning for unsupervised person re-
identiﬁcation. In ICCV, 2017. 2, 7, 8

[41] Zheng-Jun Zha, Xian-Sheng Hua, Tao Mei, Jingdong Wang,
Guo-Jun Qi, and Zengfu Wang.
Joint multi-label multi-
instance learning for image classiﬁcation. In CVPR, pages
1–8, 2008. 3

[42] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su,
Shengjin Wang, and Qi Tian. Mars: A video benchmark for
large-scale person re-identiﬁcation.
In ECCV, pages 868–
884, 2016. 6

[43] Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin,
Yubing Li, Yihong Gong, and Nanning Zheng. Deep self-
paced learning for person re-identiﬁcation. PR, 76:739–751,
2018. 2

[44] Zhi-Hua Zhou, Min-Ling Zhang, Sheng-Jun Huang, and
AI,

Yu-Feng Li. Multi-instance multi-label
176(1):2291–2320, 2012. 2

learning.

[45] Xiaoke Zhu, Xiao-Yuan Jing, Xinge You, Xinyu Zhang, and
Taiping Zhang. Video-based person re-identiﬁcation by si-
multaneously learning intra-video and inter-video distance
metrics. TIP, 27(11):5683 – 5695, 2018. 1, 2

[46] Yue Zhu, Kai Ming Ting, and Zhi-Hua Zhou. Discover mul-
tiple novel labels in multi-instance multi-label learning. In
AAAI, pages 2977–2984, 2017. 2

769

