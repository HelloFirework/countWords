Layout-Graph Reasoning for Fashion Landmark Detection

Weijiang Yu1, Xiaodan Liang1

2∗, Ke Gong1
Nong Xiao1, Liang Lin1

,

,

2

,

2, Chenhan Jiang1,

1Sun Yat-sen University,

2DarkMatter AI Research

weijiangyu8@gmail.com, xdliang328@gmail.com, kegong936@gmail.com,

jchcyan@gmail.com, xiaon6@sysu.edu.cn, linliang@ieee.org

Abstract

Detecting dense landmarks for diverse clothes, as a fun-
damental technique for clothes analysis, has attracted in-
creasing research attention due to its huge application po-
tential. However, due to the lack of modeling underly-
ing semantic layout constraints among landmarks, prior
works often detect ambiguous and structure-inconsistent
landmarks of multiple overlapped clothes in one person.
In this paper, we propose to seamlessly enforce structural
layout relationships among landmarks on the intermediate
representations via multiple stacked layout-graph reason-
ing layers. We deﬁne the layout-graph as a hierarchical
structure including a root node, body-part nodes (e.g. up-
per body, lower body), coarse clothes-part nodes (e.g. col-
lar, sleeve) and leaf landmark nodes (e.g. left-collar, right-
collar). Each Layout-Graph Reasoning(LGR) layer aims
to map feature representations into structural graph nodes
via a Map-to-Node module, performs reasoning over struc-
tural graph nodes to achieve global layout coherency via
a layout-graph reasoning module, and then maps graph
nodes back to enhance feature representations via a Node-
to-Map module. The layout-graph reasoning module in-
tegrates a graph clustering operation to generate repre-
sentations of intermediate nodes (bottom-up inference) and
then a graph deconvolution operation (top-down inference)
over the whole graph. Extensive experiments on two public
fashion landmark datasets demonstrate the superiority of
our model. Furthermore, to advance the ﬁne-grained fash-
ion landmark research for supporting more comprehensive
clothes generation and attribute recognition, we contribute
the ﬁrst Fine-grained Fashion Landmark Dataset (FFLD)
containing 200k images annotated with at most 32 key-
points for 13 clothes types.

*Corresponding Author

Figure 1: (a) Traditional DCNNs [25] suffer from great per-
formance drops when facing heavy overlapping of humans
and clothes nesting because of lacking structural constraint
and commonsense knowledge. (e.g. the left waistline and
right sleeve on the lady are wrongly predicted on the child.)
(b) Our LGR provides graph-based inferences among land-
marks, leveraging symmetric and hierarchical relations to
constrain landmark’s layout in one person.

1. Introduction

Fashion landmark detection that targets at localizing key-
points at the functional regions of clothes (e.g. collar, waist-
line), has attracted research attention and lots of demands
driven by the boom of electronic commerce, such as cloth-
ing retrieval [36, 30, 5, 16], clothes generation [10, 21] and
fashion image classiﬁcation [36, 27, 8]. To support these
comprehensive high-level applications, the landmark detec-
tors need to effectively deal with arbitrary clothing appear-
ances, diverse clothes layouts and styles, multiply human

2937

Root NodeFeatureBody-part Node FeatureClothes-part Node FeatureLandmark Node FeaturePrediction……Prediction(a)(b)Landmark Node FeatureBottom-up inferenceTop-down inferenceMessage propagationpose, different lights and background clutters.

Recent research efforts on fashion landmark detection
[18, 26, 36, 31, 14, 32, 19] are mainly devoted to design-
ing more advanced deep feature representations [18, 36],
attention mechanism [31], pyramid module [32] and so on.
These models are limited in regarding fashion landmark
detection as an end-to-end regression problem and ignore
rich semantic layout relations among different landmarks,
such as symmetric relation (e.g. left/right collar), subordi-
nate relation (e.g. left collar belongs to collar) and human
commonsense (e.g. one clothing generally owns a pair of
sleeves). Consequently, unreasonable detection results that
deviate from human and clothes layouts may be generated,
as shown in Figure 1(a).

Nevertheless, some researches have resorted to external
guidance to enhance the interpretation of CNNs [24, 2, 33,
27, 15, 20]. For example, Yang et al. [33] introduced to in-
corporate domain knowledge for explicitly facilitating fea-
tures for better localization. Wang et al. [27] proposed a
grammar model in fashion visual understanding and used
a bidirectional recurrent neural network for message pass-
ing. Fashion landmarks naturally lie in an underlying hier-
archical structure which includes different levels of seman-
tic nodes (e.g. body-part nodes, clothes-part nodes and leaf
landmark nodes). However, they used a plain graph struc-
ture to represent knowledge and disregarded the intrinsic
hierarchical and multi-level layouts of landmarks for better
mining subordinate relations.

To address all above-mentioned issues, we propose to
endow the deep networks with the capability of structural
graph reasoning in order to make detected fashion land-
marks be coherent with human and clothes layouts from a
global perspective. We deﬁne a hierarchical layout-graph
that encodes prior commonsense knowledge in terms of hu-
man body part layouts and clothes part layouts, consisting
of a root node, body-part nodes (e.g. upper body, lower
body), coarse clothes-part nodes (e.g. collar, sleeve) and
leaf landmark nodes (e.g. left-collar, right-collar). We then
propose a novel Layout-Graph Reasoning (LGR) layer that
is able to explicitly enforce hierarchical human-clothes lay-
out constraints and semantic relations of fashion landmarks
on deep representations for facilitating landmark detection.
Our LGR layer is a general and ﬂexible network layer which
can be stacked and injected between any convolution lay-
ers, containing three modules: 1) a Map-to-Node mod-
ule that maps convolutional features into each graph leaf
node; 2) a layout-graph reasoning module to perform hi-
erarchical graph reasoning over structural graph nodes to
achieve global layout coherency; 3) a Node-to-Map mod-
ule to learn appropriate associations between the evolved
graph leaf nodes and convolutional features, which in turn
enhance local feature representations with global reasoning.

Given graph node representations for leaf landmark

nodes from the Map-to-Node module, our layout-graph
reasoning module ﬁrst performs a graph clustering oper-
ation to generate representations of intermediate nodes in
the spirit of bottom-up inference, that is, propagating from
(leaf landmark nodes)→(clothes-part nodes)→(body-part
nodes)→(root node). Then a graph deconvolution opera-
tion to evolve representations of bottom nodes guided by the
higher-level structure nodes in the spirit of top-down infer-
ence, that is, (root node)→(body-part nodes)→(clothes-part
nodes)→(leaf landmark nodes). Beneﬁting from integrating
graph clustering and graph deconvolution operations, our
LGR layer enables to achieve global structural coherency
and effectively enhance each landmark node representation
for better predictions.

Moreover, existing fashion landmark datasets [36, 19,
31] annotated with at most 8 landmarks for all clothes ap-
pearance. To advance the development of ﬁne-grained do-
main knowledge in fashion landmark detection research, we
contribute a new Fine-grained Fashion Landmark Dataset
containing 200k images annotated with at most 32 key-
points for 13 clothes types, named as FFLD. More details
of FFLD are presented in supplementary material.

Our contributions are summarized in the following as-

pects:

1) we propose a general Layout-Graph Reasoning (LGR)
layer and incorporate multiply LGR layers into deep net-
works to seamlessly enforce structural
layout relations
among clothing landmarks on the intermediate representa-
tions to achieve global structure coherency.

2) We deﬁne the layout-graph as a hierarchical structure
for mining contextual graph semantic information from spe-
ciﬁc nodes to abstraction nodes. The graph clustering and
graph deconvolution operation are integrated into each LGR
layer for hierarchical graph reasoning.

3) We construct the ﬁrst Fine-grained Fashion Landmark
Dataset (FFLD) that provides more comprehensive land-
mark annotations for diverse clothes types.

4) Our model performs the superior ability compared
with state-of-the-art approaches over two public fashion
landmark datasets (e.g. FLD [19] and DeepFashion [36]).

2. Related Work

Fashion Landmark Detection and Localization. Re-
cently many research efforts have been devoted to joint lo-
calization and landmark detection [18, 35, 7, 26, 29, 33,
28, 31, 36, 19, 27, 22]. Newell et al. [22] proposed a
model for human pose estimation using a repeated pool-
ing down and upsampling process to learn the spatial dis-
tribution of resolution. Liu et al. [19] proposed deep fash-
ion alignment using the pseudo-label scheme to enhance in-
variability of fashion landmark. Wang et al. [27] captured
kinematic and symmetry grammar of clothing landmark for
mining geometric relationships among landmarks. They

2938

modeled grammar message passing processing as a bidirec-
tional convolutional recurrent neural network for training in
an end-to-end manner.

The models of deep learning above demonstrate the pow-
erful representations of neural networks. Few of them con-
sider combining knowledge-guide information with fashion
landmark detection in a hierarchical way. Motivated by
Rothrock et al. [24, 12, 34], we build a hierarchical architec-
ture to model global-local fashion landmark correlations for
facilitating contextual information across each landmark.

Knowledge-guide Information in Graph. Recently
some research efforts model domain knowledge as graph
for mining correlations among labels or objects in images,
which has been proved effective in many tasks [12, 3, 23,
34, 13, 20, 24, 1, 4, 6]. Li et al. [13] proposed a subgraph-
based model for scene graph generation using bottom-up re-
lationship inference of objects in images. Liang et al. [15]
modeled semantic correlations using semantic neuron graph
for explicitly incorporating semantic concept hierarchy dur-
ing propagation. Yang et al. [33] built a prior knowledge-
guide graph for body part locations to well consider the
global pose conﬁgurations.

As far as we know, there is no work considering mod-
eling layout-graph across all scales and levels in the lay-
out among fashion landmarks. We incorporate cross-layer
graph relations, low-level and high-level graph relations by
layout-graph reasoning layers into a uniﬁed model, which
consists of Map-to-Node module, layout-graph reasoning
module and Node-to-Map module.

Fashion Understanding Datasets. Many human-
centric applications depend on reliable fashion image un-
derstanding. DeepFashion [17] is a large-scale clothing
dataset labeled with clothes categories, attributes, at most 8
clothes landmarks and bounding boxes. FLD [19] is a fash-
ion landmark dataset (FLD) with large pose and scale vari-
ations, annotated with at most 8 landmarks and bounding
boxes. Yan et al. [31] contributed an unconstrained land-
mark dataset (ULD), which comprises 30k images with at
most 8 fashion landmark annotations. To advance the de-
velopments of domain knowledge for ﬁne-grained fashion
landmark, we propose a large-scale dataset towards the ﬁrst
ﬁne-grained fashion landmark detection task, which con-
tains 200k images annotated with at most 32 key-points for
13 clothes categories.

3. Proposed Approach

3.1. Overview

Considering the layout of fashion landmarks, we model
layout-graph reasoning to enforce detected fashion land-
marks be coherent with human and clothes layouts from a
global perspective. We propose a model that seamlessly en-
forces layout relationships among landmarks on the inter-

mediate features via multiple stacked Layout-Graph Rea-
soning (LGR) layers, as shown in Fig.2. Each LGR layer
aims to map deep convolutional features into structural
graph nodes via Map-to-Node module, perform reasoning
over multi-level layout-graph nodes to achieve global lay-
out coherency via a layout-graph reasoning module, and
then map evolved graph nodes back to enhanced convolu-
tional features via a Node-to-Map module. Finally a sig-
moid function and a 1 × 1 convolution are employed to pro-
duce heatmaps of fashion landmarks. Inspired by Yang et
al. [32], we enhance intermediate features by pyramid mod-
ule and decrease data bias by residual addition [9].

3.2. Layout graph Deﬁnition

We deﬁne the layout-graph as a hierarchical structure for
mining semantic correlations and constraints among differ-
ent fashion landmarks. Speciﬁcally, we deﬁne a layout-
graph constructed by graph nodes characterizing landmark
categories (e.g. right-collar, left-sleeve) and graph edges
representing spatial layouts (e.g. right-collar belongs to col-
lar, collar and sleeve belong to upper body), which is de-
noted as G = (V, E). The graph nodes V consist of the
set of leaf nodes Vleaf (e.g. leaf landmark nodes) and inter-
mediate nodes Vmiddle (e.g. clothes-part nodes, body-part
nodes, root node). We deﬁne the leaf node representations
as Xleaf ∈ RNleaf ×d, which is generated via Map-to-Node
module. We deﬁne the intermediate node representations
as Xmiddle ∈ RNmiddle×d, which is generated via layout-
graph reasoning module. The Nleaf and Nmiddle are the
number of leaf nodes and intermediate nodes, d means the
feature dimension of each node.

The edges E consist of the set of leaf edges Eleaf to rep-
resent the connections between each leaf nodes (e.g. right-
collar and left-collar), and intermediate edges Emiddle to
represent the connections between each intermediate node
(e.g. collar and sleeve). The leaf node adjacency weight
matrix Aleaf ∈ {0, 1}Nleaf ×Nleaf is initialized according
to the edge connections in Eleaf as shown in Fig.3, where
0 means disconnection and 1 means connection. Similarly,
we deﬁne the intermediate node adjacency weight matrix
as Amiddle. In implementation, we perform normalization
on all A by following [12] to obtained normalized adja-
cency weight matrix. The A is crucial for layout-graph in-
formation embedded in the fashion joint layouts to beneﬁt
point-wise fashion landmark detection, which can be easily
designed according to human commonsense about fashion
layouts as illustrated in Sec.2 of supplementary material.

3.3. Layout graph Reasoning Layer

The LGR layer aims to enhance convolutional features
by layout-graph reasoning. Each LGR layer consists of
three modules: 1) Map-to-Node module to map convolu-
tional features into structural graph leaf nodes; 2) layout-

2939

Figure 2: Illustration of our model that incorporates basic convolutional network for features extraction and stacked Layout-
Graph Reasoning layers for structural graph reasoning. Residual addition processing and pyramid feature post-processing
are appended between each stacked architecture for reducing bias and capturing rich representations across multiple scales.
A 1× 1 convolution with sigmoid activation function is utilized to produce ﬁnal fashion landmark heatmaps. For better
viewing of all ﬁgures in this paper, please see the original zoomed-in color pdf ﬁle.

Figure 3: Layout-graph deﬁnitions for different fashion landmark datasets. Each leaf node (red circles) represents the position
and type of one clothing landmark. Each leaf edge (black lines) indicates the correlations between landmark points.

graph reasoning module to model global-local clothing
landmark correlations for feature enhancement, containing
a graph clustering operation and a graph deconvolution op-
eration; 3) Node-to-Map module to map evolved leaf node
representations back to enhance feature representations.

3.3.1 Map-to-Node Module

This module is to seamlessly map convolutional feature
maps to graph node representations. Given the input convo-
lutional feature maps after dimension transformation (F ∈
RH×W ×C → F ∈ RHW ×C , where H, W and C repre-
sent the height, weight and channel), this module produces
graph leaf node representations Xleaf ∈ RNleaf ×d. The
formulation is:

Xleaf = σ(Φ(FWm)T FWt),

(1)

where Wm ∈ RC×Nleaf and Wt ∈ RC×d are trainable
sampling matrices. The Φ denotes normalized function soft-
max to sum all rows to one, and the σ denotes non-linear
function Relu.

3.3.2 Layout-Graph Reasoning Module

Given graph leaf node representations Xleaf via the Map-
to-Node module, our layout-graph reasoning module ﬁrst
performs a graph clustering operation to generate repre-
sentations of intermediate nodes in the spirit of bottom-
up inference,
that is, propagating from (leaf landmark
nodes)→(clothes-part nodes)→(body-part nodes)→(root

node). Then a graph deconvolution operation to evolve
representations of bottom nodes guided by the higher-
level structure nodes in the spirit of top-down infer-
ence, that is, (root node)→(body-part nodes)→(clothes-part
nodes)→(leaf landmark nodes). Beneﬁting from integrating
the graph clustering and graph deconvolution operations,
the module achieves global structural coherency.

Graph Clustering Operation. This operation generates
intermediate node representations by graph clustering. For
different levels of bottom-up inference, the clustered graph
nodes and graph edges change as shown in Fig.4. The graph
clustering operation of every levels is similar. Here we take
Xleaf → Xmiddle as an example to illustrate the cluster-
ing operation. Given the input Xleaf and Aleaf , this oper-
ation generates intermediate node representations Xmiddle
and intermediate node adjacency weight matrix Amiddle,
which is formulated as:

Xmiddle = σ (cid:0)WT
Amiddle = σ (cid:0)WT
p

p′ Aleaf Xleaf Wh(cid:1) ,

Aleaf Wp(cid:1) ,

(2)

(3)

where Wp′ ∈ RNleaf ×Nmiddle and Wp ∈ RNleaf ×Nmiddle
are both trainable clustering matrices. The Wh ∈ Rd×d is
a trainable weight matrix. We use graph convolution from
[12] for graph reasoning to perform over Xleaf and Aleaf
using Wh to update the leaf graph node representations.
Then we utilize Wp′ to cluster updated Xleaf into Xmiddle,
which is formulated as Eq.2. We cluster Aleaf using Wp
to generate Amiddle, which is formulated as Eq.3. Wp is
a permutation matrix and obeys distribution of WT
Wp =
p

2940

Basic Network…1x1Conv,SigmoidLGR layerPyramid Feature1x1Conv, ReluElement-wise AdditionLGR layerLGR layerLeft CollarRight CollarRight WaistlineRight SleeveRight HemLeft WaistlineLeft SleeveLeft HemRight HemLeft HemCenter CollarLeft CollarRight CollarRight ShoulderRight ArmpitRight ChestRight External ElbowRight External SleeveRight Internal SleeveRight Internal ElbowRight WaistlineLeft ShoulderLeft ArmpitLeft ChestLeft Internal ElbowLeft External ElbowLeft WaistlineLeft Internal SleeveLeft External SleeveRight Lower HeadLeft Lower HeadCrotchRight Internal KneeRight External KneeRight External BottomRight Internal BottomLeft Internal KneeLeft External KneeLeft Internal BottomLeft External BottomOther DatasetsFFLDFigure 4: Illustration of our Layout-graph Reasoning (LGR) layer which contains Map-to-Node module, layout-graph rea-
soning module and Node-to-Map module. In Map-to-Node and Node-to-Map module, weighted sample operations vote all
convolution features (evolved leaf landmark nodes) to leaf landmark nodes (enhanced convolution features) by weighted sam-
ple. In Layout-Graph Reasoning module, the graph is propagated from leaf landmark nodes to root node by Graph Clustering
and Graph Reasoning. The root node is propagated back again by Graph Deconvolution and Graph Reasoning for producing
evolved leaf landmark nodes. We use graph convolution from [12] for Graph Reasoning with supervising adjacency matrix.
A skip connection is employed for restricting the consistency of clustering and deconvolution operations.

I. We use graph reasoning to perform over Xmiddle and
Amiddle to update the clustered graph node representations
Xmiddle ∈ RNmiddle×d.

Graph Deconvolution Operation.

This operation
evolves representations of bottom nodes guided by the
higher-level structure nodes in the spirit of top-down infer-
ence as shown in Fig.4. Again, we take Xmiddle → Xleaf
an as example to illustrate the deconvolution operation.
Given the input intermediate node representations Xmiddle
and adjacency matrix Amiddle from higher-level structure,
we utilize the formulation like Eq.2 and Eq.3 to produce leaf
node representations Xleaf and leaf node adjacency weight
matrix Aleaf . Furthermore, to integrate the high-level and
low-level structure information, we utilize a matrix addition
over the node representations before clustering and after de-
convolution, followed by the graph reasoning to update the
evolved leaf node representations Xleaf .

3.3.3 Node-to-Map Module

We map evolved graph nodes into enhanced convolu-
tional features via Node-to-Map module. Given the in-
put convolutional features F and evolved leaf node repre-
sentations Xleaf , this module aims to generate enhanced
convolutional feature representations Fr. We ﬁrst per-
form the dimension transformation for F ∈ RHW ×C →
F ∈ RHW ×N ×C and Xleaf ∈ RNleaf ×d → Xleaf ∈
RHW ×Nleaf ×d. Then we concatenate F and Xleaf to Xa ∈
RHW ×Nleaf ×(C+d) for richer feature representations. We

formulate this module as:

Fr = σ(Φ(XaWm′ )σ(Xleaf Wt′ )),

(4)

Eq 4 is to map node representations Xleaf ∈ RNleaf ×d
to enhanced convolutional features Fr ∈ RHW ×C , where
Wm′ ∈ RC+d is a vector with C + d dimension and Wt′ ∈
Rd×C is a trainable sampling matrices.

4. Experiments

4.1. Experimental Settings

Network Architecture. Following the baseline of [36,
19, 31, 27], we use VGG16 [25] with four stacked LGR lay-
ers for feature extraction and layout-graph reasoning. Each
LGR layer contains Map-to-Node module,
layout-graph
reasoning module and Node-to-Map module. We map con-
volutional features into graph leaf node representations via
Map-to-Node module. On DeepFashion and FLD, we set 8
leaf nodes (e.g. left-collar, right-hem), 6 intermediate nodes
including 4 clothes-part nodes (collar, hem) and 2 body-
part nodes (e.g. upper body, lower body), and 1 root node.
On FFLD, we set 32 leaf nodes (e.g. left-shoulder, crotch),
14 intermediate nodes including 12 clothes-part nodes (e.g.
sleeve, knee) and 2 body-part nodes (e.g. upper body, lower
body), and 1 root node. More details of node’s layout
can be seen in supplementary material. Then we model
layout-graph of fashion landmarks via layout-graph reason-
ing module to evolve graph node representations guiding

2941

GraphClusteringGraph ClusteringGraph ClusteringGraph DeconvolutionGraph DeconvolutionGraph DeconvolutionSkip ConnectionSkip ConnectionSkip ConnectionConvolutionFeatures 𝐅EnhancedConvolutionFeatures 𝐅rℰLearnable Adjacency MatrixNode RepresentationGraph ClusteringGraph DeconvolutionWeighted SampleNode-to-MapLayout-graph Reasoning ModuleMap-to-Nodeby deﬁned graph correlations as shown in Fig.3. The Node-
to-Map module to map evolved graph node representations
into convolutional features for enhancing the feature repre-
sentations, which results are fed into a 1× 1 convolution
with sigmoid activation to get the prediction. A residual
addition and pyramid feature post-processing are appended
between each LGR layer for reducing bias and capturing
rich representations across multi-scales.

Three Benchmarks and Evaluation. We evaluate and
report the results and comparisons on three datasets. Deep-
Fashion [36] is the largest fashion dataset so far, which con-
tains 289,222 images annotated with bounding boxes and at
most 8 landmarks. FLD [19] is a fashion landmark dataset
with more diverse variations (e.g. pose, scale, background),
which contains 123, 016 images annotated at most 8 land-
marks and bounding boxes per image. FFLD is our con-
tributed ﬁne-grained fashion landmark dataset, which con-
tains 200k images annotated with at most 32 key-points and
bounding boxes for 13 clothes categories. Following [27],
209,222 fashion images are used for training; 40, 000 im-
ages are used for validation and remaining 40, 000 images
are for testing in DeepFashion. Following the protocol in
FLD [19], 83, 033 images and 19, 992 fashion images are
used for training and validating, 19, 991 images are used
for testing. In FFLD, we use 120K images as a training set,
40K images as a validation set and 40K images as a test set.
Normalized error(NE) metric [19] is adopted for evaluation.
We utilize l2 function to calculate the distance between pre-
dicted heatmaps and ground-truth in normalized coordinate
space (i.e. normalized by the height and width of image).

Training Strategy and Object Function. We use LGR
layer for fashion landmark detection over FLD [19], Deep-
Fashion [36] and FFLD separately without any pre-trained
models. Following [27], we ﬁrst crop each image using la-
beled bounding boxes, resize the cropped image to 224 ×
224, and extract features for graph reasoning. The training
data are augmented by scaling, rotation, and ﬂipping. We
train all the models using stochastic gradient descent with a
batch size of 16 images, which is optimized by Adam op-
timizer [11] with an initial learning rate of 1.e-3 on an 11
GB NVIDIA 1080Ti GPU. Betas of Adam are from 0.9 to
0.999. On FLD, we linearly drop the learning rate by a fac-
tor of 10 every 20 epochs. On DeepFashion and FFLD, we
linearly decrease the learning rate by a factor of 10 every
10 epochs. We stop training when no improvements on the
validation set. We set the mean squared error (MSE) equa-
tion as an objective function between the ﬁnal predicted
heatmaps and ground-truth.

4.2. Comparison with the state of the arts

LGR achieves an obvious improvement over the two
large datasets compared with PyraNet [32], FashionNet
[36], DFA [19], DLAN [31] and BCRNNs [27]. Note that

PyraNet is human pose estimation model with two stages.
We train the PyraNet following the same strategy as [32].
Our LGR outperforms SOTA at 0.0419 on FLD and 0.0336
on DeepFashion, which is much lower than the closest com-
petitor (0.0583 and 0.0484), as shown in Table.1. Compared
with traditional DCNNs [36] and grammar model [27], we
further model layout-graph reasoning to enforce detected
fashion landmarks be coherent with human and clothes lay-
outs from a global perspective. Beneﬁting from the joint
reasoning with hierarchical structures of fashion landmarks,
we achieve the SOTA performs over all existing models by
a large improvement. Note that our model consistently de-
creases the NE in all landmarks on DeepFashion.

4.3. Ablation Study

Different Stack Numbers. There are six experiments
to display the performance of different stacked LGR lay-
ers, which are shown in the ﬁrst list of Table.2. VGG16
without any graph reasoning achieves 0.0871 average NE,
which is the worst result compared with other knowledge-
guide model. Comparing the variants of different stages, the
performances get better with the stack increasing, which is a
coarse-to-ﬁne processing to repeatedly reﬁning the predic-
tion. Five stacked LGR gets the best performance(0.0405
NE), while it needs more GPU memory and time during
training/validating/testing progress. The gap of all land-
marks between ﬁve stacks and four stacks are closed. Lim-
ited by device and time, we select four stacks in our stand
model and apply it to all extensive experiments.

Different Graph Layers. In the second list of Table.2,
we built an ablation study of different normal graph lay-
ers on FLD and DeepFashion. To demonstrate the supe-
rior ability of graph clustering and graph deconvolution, we
replace a LGR layer with one graph layer without graph
clustering and deconvolution, which is presented as one-
layer. Beneﬁting from graph reasoning, two graph layers
can get the best performance, while the performance tends
to be destroyed with the depth increasing of graph layers
(e.g. two-layer:0.0471, eight-layer:0.0954). Purely increas-
ing the graph layers can not simply get better performance,
but the spent time also grows up with the model size increas-
ing. LGR can attenuate the shortcoming as above and get
better performance with layer increasing by graph cluster-
ing and graph deconvolution. Compared the LGR layer with
normal graph layer, eight graph layers got worse perfor-
mance compared with three-clustering (0.0954 and 0.0419).
Note that three-clustering got close speed compared with
eight graph layers (e.g. 0.00379s and 0.00357s). Due to the
LGR layer contains graph clustering, graph deconvolution
and graph reasoning operations, which has more processes
compared with the same size of normal graph layers.

Different Number of Graph Clustering and Deconvo-
In the third list of Table.2, we explore the effec-

lution.

2942

Table 1: Comparison with the state-of-the-art model on the FLD test set and DeepFashion test set using the NE metric.

Methods

L.Collar

R.Collar

L.Sleeve

FashionNet [36]

PyraNet [32]

DFA [19]
DLAN [31]

BCRNNs [27]

LGR(ours)

.0784
.0341
.048
.0531
.0463
.0423

.0803
.0341
.048
.0547
.0471
.0152

.0975
.0610
.091
.0705
.0627
.0502

Methods

L.Collar

R.Collar

L.Sleeve

FashionNet [36]

PyraNet [32]

DFA [19]
DLAN [31]

BCRNNs [27]

LGR(ours)

.0854
.0343
.0628
.0570
.0415
.0270

.0902
.0343
.0637
.0611
.0404
.0116

.0973
.0602
.0658
.0672
.0496
.0286

FLD
R.Sleeve

.0923
.0620
.089
.0735
.0614
.0735

L.Waistline

R.Waistline

.0874
.0920

-

.0752
.0635
.0195

.0821
.0921

-

.0748
.0692
.0512

DeepFashion
R.Sleeve

L.Waistline

R.Waistline

.0935
.0613
.0621
.0647
.0449
.0347

.0854
.0920
.0726
.0703
.0502
.0307

.0845
.0931
.0702
.0694
.0523
.0435

L.Hem R.Hem
.0893
.0802
.0291
.0314
.071
.072
.0675
.0693
.0527
.0635
.0452
.0393

L.Hem R.Hem
.0823
.0812
.0291
.0308
.0663
.0658
.0627
.0624
.0537
.0551
.0162
.0160

Avg.
.0859
.0723
.068
.0672
.0583
.0419

Avg.
.0872
.0719
.0660
.0643
.0484
.0336

Figure 5: Qualitative results for VGG16 [25], PyraNet [32], LGR w.o clustering/deconvolution (two graph layers without
graph clustering and deconvolution) and LGR over DeepFashion (ﬁrst row), FLD (second row) and FFLD (bottom row). The
detected landmarks (red circle) are performed on different variations such as occlusion and complicate background. Please
see the zoomed-in color pdf ﬁle.

tiveness of different numbers of graph clustering operation
and graph deconvolution operation in LGR layer, the dia-
gram as shown in Fig.6. With the depth of operations in-
creasing, more prior commonsense knowledge in terms of
richer human body part layouts and clothes part layouts will
be got to better guide the learning processing. The experi-
ment results have shown that three-clustering get better per-
formance than one clustering (0.0419 and 0.0488 on FLD,
0.0336 and 0.0403 on DeepFashion).

Different Injected Layers. The fourth list of Table.2
compares the variants of injecting four stacked LGR layers
into different convolution blocks (ConvBlock) of VGG16
[25] over FLD. The stacked LGR layers are injected into
right before the block. The performance of adding LGR
layers after Block3 is worse than adding LGR layers after
ConvBlock5 of VGG16. We show the possible reason that
the deeper layers can encode more semantically high-level
feature representation, which is more suitable for layout-

Figure 6: The different structures of graph clustering and
graph deconvolution. The number of nodes in each graph
reasoning layer is labeled on the top.
Please see the
zoomed-in color pdf ﬁle.

graph reasoning.

4.4. Qualitative Results

The results showed different abilities of traditional DC-
NNs [25], PyraNet [32], normal graph reasoning without
graph clustering and deconvolution and LGR. We select the
best structure (two-layer) of normal graph layers demon-

2943

VGG16LGR w.oclustering/deconvolutionLGR(our)GTVGG16LGR(our)GTLGR w.oclustering/deconvolutionPyraNetPyraNet8844one-clustering884422two-clustering88442211three-clusteringGraph Reasoning LayerGraph ClusteringGraph DeconvolutionTable 2: Ablation study on FLD and DeepFashion using
NE metric (Avg.). The structures with different numbers
of graph clustering and deconvolution are shown in Fig.6.
We also present the results generate by different numbers of
normal graph convolutional layers that replacing the graph
clustering and deconvolution. We also compare the average
execution time for testing (Time).

Different Stack numbers

FLD

Methods

VGG16 [25]

one stack
two stacks
three stacks
four stacks
ﬁve stacks

Methods
one-layer
two-layer
four-layer
six-layer
eight-layer

Avg.
.0871
.0711
.0535
.0529
.0419
.0405

∆Avg.
.0452
.0292
.0116
.0110

-

.0014

Time(s) ∆Time(s)
.00065
.00155
.00236
.00346
.00379
.00472

.00314
.00224
.00143
.00033

.00093

-

Different Graph Layers

FLD

DeepFashion

Avg.
.0531
.0471
.0639
.0644
.0954

Time(s)
.00241
.00273
.00279
.00289
.00357

Avg.
.0482
.0437
.0562
.0638
.0779

Time(s)
.00237
.00266
.00271
.00300
.00341

Different Number of Graph Clustering and Deconvolution

Methods

one-clustering
two-clustering

three-clustering

FLD

DeepFashion

Avg.
.0488
.0443
.0419

Time(s)
.00267
.00302
.00379

Avg.
.0403
.0372
.0336

Time(s)
.00261
.00336
.00352

Different Injected Layers

Methods

VGG16 ConvBlock1
VGG16 ConvBlock3
VGG16 ConvBlock5

FLD
Avg.
.0811
.0574
.0419

strated as above. In Fig.5, for complex background, diverse
clothes layouts and styles, multiple scales and views, the
pure DCNNs, pose estimation model [32] and normal graph
layers fail to detect correct fashion landmark. Beneﬁting
from modeling layout-graph relations of landmarks by a
hierarchical structure, LGR can mine semantic coherency
of layout-graph and enhance the semantic correlations and
constrains among landmarks (e.g. collar and sleeve belong
to upper body). LGR covers difﬁcult variance and generate
reasonable results guided by layout-graph reasoning during
the bottom-up, top-down inference. For example, LGR can
detect correct results by constraining fashion landmarks on
one clothing in complex background (ﬁrst row in Fig.5).

Table 3: Evaluation of different models on FFLD.

Methods

Avg. NE

FashionNet [36]

PyraNet [32]

GCN [12]

BCRNN [27]
LGR (ours)

.2031
.1423
.1272
.1226
.1180

the real application. More detailed deﬁnition and statistics
of FFLD can be seen in supplementary material.

We have shown four existing methods evaluated on
FFLD in Table 3 to comprehensively perform FFLD. The
FashionNet [36] and BCRNN [27] are the SOTA meth-
ods for fashion landmark detection, and the PyraNet [32]
is one of SOTA methods for human pose estimation. We
utilize VGG16 [25] stacked with two graph convolutional
layers, which is regard (GCN) [12] is a typical graph-based
methods, which VGG16 in this evaluation. Based on the
prior layout-graph as shown in Fig.3, layout-graph reason-
ing with four stacks is evaluated on FFLD.

As shown in Table.3, the LGR achieved 0.118 average
NE on FFLD, which is a worse performance compared with
FLD(0.0419 NE) and DeepFashion(0.0336 NE) due to more
consumer images, more ﬁne-grained fashion landmarks,
more challenge views and backgrounds. To demonstrate the
challenge of FFLD on other models, we perform fashion
landmark detection model (FashionNet [36] and BCRNN
[27]), human pose estimation model (PyraNet [32]) and nor-
mal graph layer (GCN [12]) on FFLD to achieve 0.2031
NE, 0.1226 NE, 0.1423 NE and 0.1272 NE. We perform
BCRNN on FFLD following the setting of [27], and the
fashion landmark grammars of FFLD consist of kinematics
grammar and symmetry grammar. More detailed fashion
grammars of FFLD can be seen in supplementary material.

5. Conclusion

In this paper, we proposed the Layout-Graph Reasoning
(LGR) that consists of three modules for fashion landmark
detection to seamlessly utilize structural graph reasoning in
a hierarchical way. We use LGR to achieve SOTA perfor-
mance over recent methods. We contribute a ﬁne-grained
fashion landmark dataset to advance the development of
knowledge graph to fashion landmark research.

6. Acknowledgements

4.5. Fine grained Fashion Landmark Dataset

(FFLD)

Compared with exited fashion landmark datasets [36, 19,
31], FFLD consists of more than 70% consumer images,
which is more challenge for multiple view and light, com-
plex background and deformable clothes appearance. Note
that the FFLD is the closest fashion landmark dataset with

This work was supported in part by the Sun Yat-
sen University Start-up Foundation Under Grant No.
76160-18841201, in part by the National Key Research
and Development Program of China under Grant No.
2018YFC0830103, in part by National High Level Talents
Special Support Plan (Ten Thousand Talents Program), and
in part by National Natural Science Foundation of China
(NSFC) under Grant No. 61622214, and 61836012.

2944

References

[1] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-
Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Ra-
poso, A. Santoro, R. Faulkner, et al. Relational inductive
biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018.

[2] X. Chen and A. Yuille. Articulated pose estimation by a
graphical model with image dependent pairwise relations. In
NIPS, 2014.

[3] M. Defferrard, X. Bresson, and P. Vandergheynst. Convo-
lutional neural networks on graphs with fast localized spec-
tral ﬁltering.
In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 29, pages 3844–3852. Curran
Associates, Inc., 2016.

[4] J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio,
Y. Li, H. Neven, and H. Adam. Large-scale object classiﬁ-
cation using label relation graphs. In ECCV, pages 48–64.
Springer, 2014.

[5] W. Di, C. Wah, A. Bhardwaj, R. Piramuthu, and N. Sundare-
san. Style ﬁnder: Fine-grained clothing style detection and
retrieval. In CVPR Workshops, pages 8–13, 2013.

[6] C. Gan, M. Lin, Y. Yang, G. de Melo, and A. G. Haupt-
mann. Concepts not alone: Exploring pairwise relationships
for zero-shot video activity recognition. In AAAI, 2016.

[7] K. Gong, X. Liang, D. Zhang, X. Shen, and L. Lin. Look
into person: Self-supervised structure-sensitive learning and
a new benchmark for human parsing. In CVPR, 2017.

[8] X. Han, Z. Wu, P. X. Huang, X. Zhang, M. Zhu, Y. Li,
Y. Zhao, and L. S. Davis. Automatic spatially-aware fash-
ion concept discovery. In ICCV, 2017.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016.

[18] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, 2015.

[19] Z. Liu, S. Yan, P. Luo, X. Wang, and X. Tang. Fashion land-

mark detection in the wild. In ECCV. Springer, 2016.

[20] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual re-
lationship detection with language priors. In ECCV, pages
852–869. Springer, 2016.

[21] S. L. A. C. B. T. L. B. M. Hadi Kiapour, Xufeng Han. Where
to buy it:matching street clothing photos in online shops. In
ICCV, 2015.

[22] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In ECCV. Springer, 2016.
[23] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convo-
lutional neural networks for graphs. In ICML, pages 2014–
2023, 2016.

[24] B. Rothrock and S.-C. Zhu. Human parsing using stochastic
and-or grammars and rich appearances. In ICCV, pages 640–
647. IEEE, 2011.

[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[26] G. Trigeorgis, P. Snape, M. A. Nicolaou, E. Antonakos, and
S. Zafeiriou. Mnemonic descent method: A recurrent pro-
cess applied for end-to-end face alignment. In CVPR, pages
4177–4187, 2016.

[27] W. Wang, Y. Xu, J. Shen, and S.-C. Zhu. Attentive fash-
ion grammar network for fashion landmark detection and
clothing category classiﬁcation. In CVPR, pages 4271–4280,
2018.

[28] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-

volutional pose machines. In CVPR, June 2016.

[29] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou.
Look at boundary: A boundary-aware face alignment algo-
rithm. In CVPR, pages 2129–2138, 2018.

[10] J. Huang, R. Feris, Q. Chen, and S. Yan. Cross-domain im-
age retrieval with a dual attribute-aware ranking network. In
ICCV. IEEE, 2015.

[30] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L.
Berg. Retrieving similar styles to parse clothing. T-PAMI,
37(5):1028–1040, 2015.

[11] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[12] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation
with graph convolutional networks. In International Confer-
ence on Learning Representations (ICLR), 2017.

[13] Y. Li, W. Ouyang, B. Zhou, Y. Cui, J. Shi, and X. Wang.
Factorizable net: An efﬁcient subgraph-based framework for
scene graph generation. arXiv preprint arXiv:1806.11538,
2018.

[14] X. Liang, K. Gong, X. Shen, and L. Lin. Look into per-
son: Joint body parsing & pose estimation network and a
new benchmark. T-PAMI, 2018.

[15] X. Liang, H. Zhou, and E. Xing. Dynamic-structured seman-

tic propagation network. In CVPR, pages 752–761, 2018.

[16] S. Liu, Z. Song, G. Liu, C. Xu, H. Lu, and S. Yan. Street-to-
shop: Cross-scenario clothing retrieval via parts alignment
and auxiliary set. In CVPR, pages 3330–3337, 2012.

[17] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion:
Powering robust clothes recognition and retrieval with rich
annotations. In CVPR, 2016.

[31] S. Yan, Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Un-
constrained fashion landmark detection via hierarchical re-
current transformer networks.
In Proceedings of the 2017
ACM on Multimedia Conference. ACM, 2017.

[32] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learn-
ing feature pyramids for human pose estimation. In ICCV,
volume 2, 2017.

[33] W. Yang, W. Ouyang, H. Li, and X. Wang. End-to-end learn-
ing of deformable mixture of parts and deep convolutional
neural networks for human pose estimation. In CVPR, 2016.
[34] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamil-
Hierarchical graph representa-
arXiv preprint

ton, and J. Leskovec.
tion learning withdifferentiable pooling.
arXiv:1806.08804, 2018.

[35] X. Zhu and D. Ramanan. Face detection, pose estimation,
and landmark localization in the wild. In CVPR, pages 2879–
2886. IEEE, 2012.

[36] S. Q. X. W. Ziwei Liu, Ping Luo and X. Tang. Deepfashion:
Powering robust clothes recognition and retrieval with rich
annotations. In CVPR, June 2016.

2945

