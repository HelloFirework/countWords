Parallel Optimal Transport GAN∗

Gil Avraham†

Yan Zuo†

Tom Drummond

ARC Centre of Excellence for Robotic Vision, Monash University, Australia

{gil.avraham, yan.zuo, tom.drummond}@monash.edu

Abstract

Although Generative Adversarial Networks (GANs) are
known for their sharp realism in image generation, they of-
ten fail to estimate areas of the data density. This leads to
low modal diversity and at times distorted generated sam-
ples. These problems essentially arise from poor estima-
tion of the distance metric responsible for training these
networks. To address these issues, we introduce an addi-
tional regularisation term which performs optimal trans-
port in parallel within a low dimensional representation
space. We demonstrate that operating in a low dimen-
sion representation of the data distribution beneﬁts from
convergence rate gains in estimating the Wasserstein dis-
tance, resulting in more stable GAN training. We empiri-
cally show that our regulariser achieves a stabilising effect
which leads to higher quality of generated samples and in-
creased mode coverage of the given data distribution. Our
method achieves signiﬁcant improvements on the CIFAR-
10, Oxford Flowers and CUB Birds datasets over several
GAN baselines both qualitatively and quantitatively.

1. Introduction

Optimal transport theory has found widespread applica-
tions in numerous ﬁelds, including various applications in
statistics and machine learning domains. Despite some dif-
ﬁculties accompanying the optimal transport approach, it
offers a solution that is both intuitive and numerically well-
behaved. This makes it a compelling approach for large
scale problems which are unstable in nature; of note, opti-
mal transport has been recently employed in the domain of
Generative Adversarial Networks (GANs) [12] with great
success.

Generative Adversarial Networks are generative models
where a density function is not explicitly approximated but

∗This work was supported by the Australian Research Council Centre

of Excellence for Robotic Vision (project number CE1401000016).

†Authors contributed equally

a sampling mechanism is provided. A GAN framework typ-
ically has two components: a Discriminator for estimating
a family of f -divergences [26] between a model distribu-
tion Pg and the data distribution Pr, and a Generator that
provides a mapping between a known noise distribution Pz
to the model distribution Pg. The framework is optimised
through a minimax two-step procedure where each network
is responsible for training its counterpart. In practice, the
family of f -divergences suffers from numerical instabilities
and brittle parameter selection, giving rise to many regular-
isation methods [22, 23, 28]; ultimately leading to training
instability issues such as vanishing gradients and mode col-
lapse [1, 6, 21, 29].

Optimal transport has played a key role in alleviating
these instability issues in GANs [2, 30]. Most notably, [2]
proposed an alternative to the value function being opti-
mised by GANs. This approach utilised ideas from trans-
portation theory, leveraging the Wasserstein distance to pro-
duce a value function which was shown to be less frag-
ile and better ﬁtted as a GAN value function when using
gradient-based optimisation methods. Whilst use of the
Wasserstein distance was shown to help stabilise training
of GANs, its convergence rate was still directly dependent
on the dimensionality of the problem [8, 11], making it
a less reliable distance when measuring high dimensional
data [5,31] (as is often the case in images). Additionally, the
current solution exists in the dual formulation, which im-
poses the constraint that the Discriminator must lie within
the space of 1-Lipschitz functions. Enforcement of this con-
straint requires regularising divergence or distance used for
GAN training which is imposed via gradient penalty [13].

We propose the Parallel Optimal Transport GAN (POT-
GAN), an unsupervised approach which regularises the
Generator using the Wasserstein distance in a low dimen-
sional latent representation of the data distribution and
model distribution Pr, Pg respectively. We show that sig-
niﬁcant convergence gains are achieved by reducing the di-
mensionality of the problem when estimating the Wasser-
stein distance. This allows for the direct computation of the
primal form of the optimal transportation problem, avoiding

4411

the need to carefully maintain the 1-Lipschitz constraint ac-
companying the dual form. The latent representation is ob-
tained using a Variational Autoencoder (VAE) [18] and for
estimating the Wasserstein distance, we use the Monge for-
mulation [34] where a transport map and a cost function are
estimated throughout the optimisation process. This pro-
cedure provides better initial estimates of the Wasserstein
distance at every iteration leading to more stable training,
where continuous matching of intrinsic representations will
result in increased mode coverage. We experimentally vali-
date our method and show signiﬁcant improvements on In-
ception [29] and FID [15] scores.

The main contributions of this paper are the following:

1. We introduce a novel GAN framework which uses an
optimal transport regulariser on the latent representa-
tion of the data distribution, aiding the Generator net-
work towards a more stable convergence and increased
modal diversity.

2. We demonstrate that both a transportation map and
cost function can be effectively estimated within a low
dimensional latent space. We employ a novel approach
to perform this task by utilising the non-linear mapping
capabilities of decision forests.

3. We show our method signiﬁcantly improves on sev-
eral GAN benchmarks on Inception and FID scores on
CIFAR-10, Oxford Flowers and CUB Birds datasets.

2. Background

The goal of

a Generative Adversarial Network
(GAN) [12] is to train a Generator which, given an
input sample z ∈ Rdz drawn from some arbitrary distri-
bution (usually standard Gaussian), is able to produce an
output sample G(z) ∈ Rdr , belonging to some distribution
Pg which closely approximates a target distribution Pr. A
GAN achieves this by setting up a game between the Gen-
erator G and a Discriminator D (typically represented by
neural networks), which can be described by the following
minimax objective loss function:

2.1. GAN Regularisation

Since GANs lack an explicit speciﬁcation of the den-
sity function, variational inference has been combined with
GANs to leverage advantages from each method [16]. [20]
introduced the VAEGAN which offered a trade-off between
the sharpness offered by a GAN in exchange for the stabil-
ity and diversity of a Variational Autoencoder (VAE). Sub-
sequently, [4] added an additional adversarial loss which
aimed to diffuse generated samples towards reconstructed
real samples. Both VAEGAN and MDGAN are similar
works in the sense that they attempt to add an additional
manifold matching penalisation term. A drawback to both
of these works is that the matching is being performed in the
high-dimensional pixel space, which harms these two meth-
ods substantially. ALI by [9] and BiGAN [7] both jointly
train a Generator and its inference while maintaining the ad-
versarial loss which mitigated sample quality degradation.
Most recently, VEEGAN [32] trained an auxiliary network
for inverting the Generator output in a GAN to match the
input noise distribution, using the mismatch to provide a
training signal for mitigating mode collapse and improve
modal diversity.

2.2. Optimal transport in GANs

Optimal transport [35] addresses the problem of match-
ing distributions from a perspective of mass transportation.
In this context, the Wasserstein distance is the measure be-
tween two distributions. For the set of all joint probability
distributions Γ(xr, xg), we have probability distributions
Pr, Pg over X ⊆ Rd and the cost function c(xr, xg) :
X × X → R+. The transport plan γ ∗ ∈ Γ(xr, xg) min-
imises the following:

Wc(Pr, Pg) =

inf

γ∈Γ(Pr ,Pg)

E(xr ,xg)∼γ[c(xr, xg)]

(2)

A study by [2] compared different distance metrics for mea-
suring distribution distances and showed the advantages of
using the Wasserstein distance over other probability dis-
tances including the Jensen-Shannon distance. To address
the expensive nature of estimating Eq. 2 for distributions
of high dimensions, [2] suggested using the Kantorovich-
Rubinstein duality [35]:

min

G

max

D

V (D, G) = Ex∼Pr [log D(x)]

+Ez∼Pz [log (1 − D ◦ G(z))]

Proper optimisation over Eq. 1 occurs when both D and G
each learn at the same rate and are evenly matched, ensuring
that both networks continue to improve. [12] showed that
training D to convergence estimates Jensen-Shannon (JS)
distance between the two distributions, and this signal is
provided towards improving G. Further works generalised
this approach leading to f -divergences [26] and mutual in-
formation minimisation criteria [3].

(1)

W1(Pr, Pg) = sup
f ∈FLip

Exr ∼Pr [f (xr)] − Exg ∼Pg [f (xg)]

(3)
With the special case of (X , c) as a metric space and FLip
are bounded 1-Lipschitz functions. This form of computing
the Wasserstein distance replaced the conventional way of
estimating the JS distance in Eq. 1, resulting in a new GAN
objective:

min

G

max

D

V (D, G) = min
θ∈Θ

Exr ∼Pr [Dω(xr)]

max
ω∈Ω
−Ez∼Pz [Dω ◦ Gθ(z)]

(4)

4412

This is motivated from a convergence stand point of the
Wasserstein distance and is discussed as follows.

We denote a probability distribution by P and the empir-
ical distribution by ˆPn. In the limit of large n, the Wasser-
stein distance of order k [35] approaches zero almost surely:

Wk(P, ˆPn) → 0 a.s.

(5)

In practice, having access to a limited number of samples
n drawn from P raises the question of how to quantify the
rate at which ˆPn converges to P . Unfortunately, as shown
in [8], the rate of convergence suffers from the curse of di-
mensionality [10]:

n(cid:19)
E[W1(P, ˆPn)] . (cid:18) 1

1
d

(6)

and in effect, for probability measures over Rd as the di-
mensionality of the space d grows larger, the representative
power of ˆPn towards P shrinks, needing more samples
to yield applicable convergence rates.
Recently [37]
generalised the original asymptotic bound by [8]. Here, we
show a simpliﬁed version of [37].

Theorem 1. Let k ∈ [1, ∞). The convergence rate of
the empirical distribution towards the k−order Wasserstein
distance is given by:

E[Wk(P, ˆPn)] . (cid:18) 1
n(cid:19)

1
d

(7)

full proof given in [37].

We deﬁne the convergence rate product as Ωa

b for da ≤

db.

Proposition 1. Let us deﬁne the distribution Pr with the
random variable X ∈ X ⊆ Rdr and Pz as the distribu-
tion of its latent encoding with the latent random variable
Z ∈ Z ⊆ Rdz , with {∀dr, dz ∈ Z+ : dz ≤ dr}. Given
the corresponding empirical distributions P ′
z along with
their associated convergence rates then:

r, P ′

dr +dz
dr dz

Ωz

n(cid:19)
r . (cid:18) 1

(8)

where Ωz
samples.

r is the convergence rate product for n number of

Proof. Obtain convergence rates for empirical distributions
P ′

z by applying Theorem 1.

r, P ′

By inspecting Eq. 8, one can immediately observe that
for dr ≫ dz, the convergence product is dominated by the
term dz, i.e.:

(9)

1
dz

Ωz

n(cid:19)
r . (cid:18) 1

4413

Figure 1: The data distribution Pr and model distribution
Pg are both high dimensional and compact in space X . Ob-
taining the optimal coupling between these distributions in
practical settings with a ﬁnite sample size suffers from a
weak convergence rate. Projecting both distributions (with
mapping Qα) onto a latent space Z offers an increased con-
vergence rate and better estimate of the Wasserstein dis-
tance. Estimation of Wasserstein distance in this space
encompasses learning a probability transformation Fℓ be-
tween the latent distributions, followed by computing the
transport map (a permutation). The operation of matching
the latent distribution serves as a guide for the overall goal
of matching distributions Pr, Pg.

With G, D being neural networks parameterised by θ, ω and
Pr, Pz being the data distribution and noise distribution re-
spectively. In [2], weight clipping was used to maintain the
1-Lipschitz bound on D; this was recently extended upon
by [13] which used gradient penalty instead to enforce the
Lipschitz constraint.
In this work we will show how the
primal form in Eq. 2 computed explicitly on the latent rep-
resentation of the data is used as a regulariser to guide the
Generator component in Eq. 4.

3. On convergence rates of Wasserstein dis-

tance

The work of [12] showed that given the data distribution
Pr and enough modelling capacity, the Generator in a GAN
setup recovers a model distribution Pg matching the data
distribution Pr. Complementing this ﬁnding, [24] provides
strong evidence that Pr lies on a low dimensional manifold
and [1] further rigorously proves that both Pr and Pg lie
on low dimensional manifolds. Our GAN framework con-
structs an optimal transport regulariser on the latent repre-
sentation which aims to help stabilise training, yielding a
better estimation of the true distribution (refer to Fig. 1).

Low-dimensionalLatent SpaceN-dimensional ManifoldM-dimensional ManifoldHigh-dimensional Image Spacez-dimensional ManifoldFor probability distributions of high dimensional data
such as images (i.e. when dr is high), computing the
Wasserstein distance in Eq. 2 is not feasible. Although
the Kantorovich-Rubinstein duality can be used under the
assumption of a compact metric space, maintaining the
Lipschitz-1 constraint is non-trivial and does not scale
well. The correct choice of latent representation size dz
in Eq. 9, in conjunction with a given dataset size can allow
for quicker convergence and obtaining an estimate of the
Wasserstein distance for the latent representation.

4. Parallel optimal transport in GANs

We deﬁne Pr, pr, Pg and pg to be the data and model
probability distributions and densities respectively. The la-
tent densities zr ∼ pr(z), zg ∼ pg(z) are deﬁned as:

pr(z) = ZX

pr(z|x)dpr(x),

pg(z) = ZX

pg(z|x)dpg(x)

with λP OT being a hyperparameter specifying the weight-
ing applied to the regularisation term. Permutation operator
σ∗
ρ is the transportation map and F ∗
ℓ is a component of the
cost function found by minimising:

F ∗

ℓ , σ∗

ρ = min

σρ∈Sρ,ℓ

i=1 kσρi (zr,i) − Fℓ(zg,i)k2
ΣN

(14)

ℓ , σ∗

where Sρ is the set of all possible permutations. We note
that F ∗
ρ need not be unique. The latent representations
zr, zg are obtained by using a pre-trained ﬁxed Variational
Auto-Encoder [18] and applying the Encoder on xr ∼ Pr,
xg ∼ Pg. The functions D and G are parameterised using
neural networks with parameters ω and θ respectively. F is
a learnable function parameterised by ℓ which is discussed
in Section 4.2. Eq. 12 and Eq. 14 each evaluate the differ-
ent metrics on their respective domains and are computed
in parallel; where their results are then used to update the
Generator in Eq. 13.

(10)

4.1. Latent Transform Map

For estimating the Wasserstein distance between latent dis-
tributions, we write out Eq. 2 and reformulate according
to [34]:

W (Pr, Pg) =

inf

γ∈Γ(Pr ,Pg)

E(zr ,zg)∼γ[c(zr, zg)] =

γ∈Γ(Pr ,Pg)ZZ×Z

inf

c(zr, zg)dγ(pr, pg) ≤

(11)

γσ ∈Γσ(Pr ,Pg)ZZ

inf

c(zr, zg)dpr(zr)δ[zg − σ(zr)]

with the ﬁnal inequality emerging from the Monge for-
σ is
mulation of the optimal transport problem [34].
the transport map and γσ is the transport plan where
γσ(zr, zg) = prδ[zg − σ(zr)] is limited to distributions that
a mass point cannot split (i.e.
there must be a one-to-one
mapping between Pr and Pg).

The bottom term in Eq. 11 recovers the Wasserstein dis-
tance under the conditions that the cost function in Eq. 2 is
convex and distributions Pr, Pg are continuous [34]. Min-
imising Eq. 11 requires estimating the transport map σ by
ﬁnding one-to-one corresponding pairs {zr,i, zg,i}i=1,...,N
under a dynamically changing cost function.

The Discriminator value function is expressed using the

WGAN-GP [13] form:

VDω = Ez∼Pz [Dω ◦ Gθ(z)]−
Exr ∼Pr [Dω(xr)] + λGP Eˆx∼P ˆx [(k∇ˆxDω(ˆx)k2 − 1)2]

(12)

we obtain the Generator value function by adding our

regularisation term:

λP OT E(zr ,zg)∼Pg δ[zg −σ∗

ρ(zr) − F ∗

VGθ = Ez∼Pz [−Dω ◦ Gθ(z)]+
2

(13)

ρ (zr)][(cid:13)(cid:13)σ∗

]

ℓ (zg)(cid:13)(cid:13)

The Wasserstein distance as formulated in Eq. 11 is the
mean cost between a one-to-one correspondence of match-
ing pairs after applying a probability transformation F (dis-
cussed in Section 4.2) on latent samples zg. The lower di-
mensions of the latent representation allows a quick conver-
gence to regularise Generator Gθ. The encoding model for
inferring the latent representation is denoted by Q and is a
neural network parameterised by α.
Estimating the cost function and the transportation map for
each batch is described in Algorithm 1. This procedure
ﬁnds the correspondences between the data and generated
pairs such that a sample zr, from the data distribution will
be covered by a generated sample zg, from the model dis-
tribution (i.e.{∀zi,r : minj c(zi,r, zj,g)}).
In the context
of neural networks, this means constructing a differentiable
operation that minimises the distances associated with cor-
responding pairs of real and generated samples. The trans-
port map is a permutation over a learned cost function, we
note that rearranging the data samples does not interfere
with the gradient-descent operation for minimising the loss
in Eq. 13. The cost function c is convex and also differen-
tiable as discussed in detail in the following section.

4.2. Learning a cost function

The transformation map σ in Eq. 11 requires a one-to-
one correspondence between latent samples zr to zg. Learn-
ing the cost function in the form of: c(a, b) = ka − Fℓ(b)k2
satisﬁes the convexity requirement and also estimates a
probability transformation Fℓ to every state in which distri-
bution Pg evolves throughout GAN training so that a trans-
formation map can be found. Omitting F from the cost
function in favour of a standard L2 cost function will re-

4414

Algorithm 1 Obtain transport map σρ and cost function
transformation Fℓ in minibatch of size N

ρ ← initialise permutation container
zr ∈ Qα(Xr) ← sample data batch
zg ∈ Qα(Xg) ← sample generated batch
for itr = 1 to iters do

for i = 1 to N do

ρi ← minj k(zi,r − Fℓ(zj,g)k2

end for
r ← Permute{zr, ρ}
z

′

i=1(cid:13)(cid:13)(cid:13)
ℓ ← Adam(∇ℓ(PN

end for
return (ρ, ℓ)

2

(z

′

i,r − Fℓ(zj,g)(cid:13)(cid:13)(cid:13)

), β1, β2)

sult in an inaccurate transformation map σ, leading to less
accurate Wasserstein distance estimated by the regulariser.

Latent Transformation Forest For the choice of F , [39,
40] showed that the non-linear discriminating power of de-
cision forests were able to disentangle complex, jointly dis-
tributed data in the compact latent representation space of
real images. In addition, [38] demonstrated that the use of
decision forests offers a smooth manifold on the low dimen-
sion space of image distributions. Given these insights, we
adopt the approach in [38] to employ a soft decision forest
as a learnable component of the cost function c. Since the
role of F is to operate as a probability transformation on the
latent representation of our data, we refer to it as a Latent
Transformation Forest (LTF). LTF is applied to the latent
representation of samples from the generated distribution
i.e.:

c(zr, zg) = kzr − Fℓ(zg)k2

(15)

where ℓ are the parameters of F , which hold the values
stored in the leaf nodes of the forest.

Constructing the Forest For a given latent encoding of a
generated sample, zg, we can construct the internal decision
nodes of the LTF through a reshape of zg. The internal de-
cision nodes utilise the values of the generated latent vector
to determine routing portions to the terminal leaf nodes of
the LTF. This is achieved by comparing the values of the
generated latent vector zg with a designated threshold value
tn and then passing this value through a sigmoid function
(i.e. σ(zn − tn)) which converts them into a value between
the range of [0, 1]. This outputs the routing portion to the
left child or leaf node of the current decision node. The
right portion is computed as 1 − σ(zn − tn) where tn is a
threshold value zn is compared against. This is illustrated
in Fig. 2.

The leaf nodes ℓ of the LTF hold values which represent
the learned values of the transformed generated latent vec-

Figure 2: The latent vector of generated samples zg is
remapped as soft decision nodes in our LTF. The weighted
sum of leaves of the LTF output the transformed generated
latent vector z′
g.

Algorithm 2 A full outline of the training procedure for the
POT-GAN model. For the complete formulation of LGAN
and the gradient penalisation term refer to [13]

α ← Pre-train VAE, discard Decoder and retain Encoder
(Q) parameters
θ, ω, ℓ, ← initialise Generator (G), Discriminator (D),
LTF (F) parameters respectively
for itr = 1 to iters do

Xr ← random mini-batch from dataset
Zg ← sample noise N (0, I)
Xg ← generate data Gθ(Zg)
(ρ, ℓ) ← Algorithm1(Xr, Xg, ℓ)
LGAN ← Dω ◦ Gθ(Zg) − Dω(Xr)+

λGP Eˆx∼P ˆx [(k∇ˆxDω(ˆx)k2 − 1)2]

LP OT ← kσρ ◦ Qα(Xr) − Fℓ ◦ Qα ◦ Gθ(Zg)k2
ω

+← Adam(∇ωLGAN , β1, β2)
+← Adam(∇θ(−LGAN + λP OT ∗ LP OT ), β1, β2)

θ

end for

tor z′
g before Eq. 15 is applied. These values are blended
according to weights dictated by the portions computed by
the internal decision nodes (referring to Fig. 2, the por-
tion allocated to the values held in leaf node ℓ0 would be
p0 × p1 × p3). This represents a non-linear transformation
on the generated samples’ from its original representation
to a transformed one where the L2 norm between latent dis-
tributions of the data and generated distributions are more
evenly distributed (for additional details refer to supplemen-
tary material).

4415

Reverse-KL Divergence : KL(Pg(x)||Pr(x))

GAN [12]

VEEGAN [32]

WGAN-GP [13]

POT-GAN (Ours)

8 Gaussian Ring (2D)
Gaussian Ball (3D)

0.2417±0.0113
3.0772±0.1014

0.1540±0.0127
2.5153±0.0708

0.0046±0.0004
0.9438±0.0044

0.0020±0.0002
0.7428±0.0038

Table 1: The reverse-KL divergence for learning Gaussian distributions. These values correlate with the visual results seen
in Fig. 3.

(a) GAN [12]

(b) VEEGAN [32]

(e) Ground Truth

(c) WGAN-GP [13]

(d) POT-GAN (Ours)

Figure 3: Results of learning a 3D Gaussian xr ∼ N3(0, I) distribution from a 2D Gaussian input z ∼ N2(0, I). The
2-Dimensional latent space, cannot fully cover the 3-Dimensional data distribution. The transparent light blue sphere in
Fig. 3a-3e has a radius of 3 standard deviations. The generated samples are the points inside the sphere. Points generated
outside the sphere were clipped for visualisation purposes. In this case, we treat the problem as covering the single mode as
best as possible. Aside from the quantitative results shown in Table 1, it is observable between Fig. 3a-3d, our method 3d
covers the sphere in the most convincing manner in relation to ground truth.

5. Experiments

For qualitative and quantitative evaluations, we perform
experiments on simulated data as well as three real datasets.
Empirically, we found that concurrently training of the VAE
and GAN does not lead to improved performance. As such,
we pre-train and subsequently ﬁx our VAE to reduce com-
putation costs and reduce training time. In Algorithm 2, we
detail the full procedure for training the POT-GAN model.
We compare POT-GAN to four GAN benchmarks, DC-
GAN [27], WGAN-GP [13], VEEGAN [32] and WAE-
GAN [33]. To our knowledge, VEEGAN represents the
most recent GAN framework which explicitly aims at sta-
bilising GAN training by Generator regularisation. POT-
GAN uses the WGAN-GP model in [13] as a baseline GAN.
For the latent representation, we used a pretrained Varia-

tional Autoencoder [18], trained with settings speciﬁed in
its respective paper (for more details on network architec-
tures refer to the supplementary material).

Our POT-GAN model is trained with a batch size of 64,
with network weights using the initialisation scheme de-
tailed in [14]. Similar to [27], we use the ADAM opti-
miser [17], specifying a learning rate of 0.0002, ﬁrst and
second moment terms of 0.5 and 0.9 respectively, minimis-
ing the GAN loss and regularisation term deﬁned in Sec-
tion 4. For size of latent vector z, we use the commonly
chosen 128 dimensions. For choice of λP OT , we experi-
mented with values of 1.0, 0.1 and 0.01. We found empiri-
cally that a value of λP OT = 0.01 performed best and use
this value for all our experiments.

4416

(a) IS

(b) FID

Figure 4: CIFAR-10 (a) Inception Score [29] and (b) FID
Score [15] performance plots over Generator iterations.
Both WGAN-GP [13] and POT-GAN were trained up to
100K iterations. The regularisation component in POT-
GAN restricts the optimisation space in a way that leads
to signiﬁcant improvement on both measures that correlate
well with quality and mode diversity of the model distribu-
tion.

5.1. Learning Gaussians

2D Gaussian Mixture Model We train a GAN [12],
VEEGAN [32] and WGAN-GP [13]
to learn a 2-
Dimensional 8 Gaussian mixture model. We compute the
reverse-KL divergence, KL(Pg||Pr), using a Parzen win-
dow and the results of 50 runs are shown in Table 1. As
observed, our method signiﬁcantly outperforms competing
methods, offering more than a relative 50% improvement in
reverse-KL Divergence compared to WGAN-GP.

3D Gaussian Ball Additionally, we set up a traditional
GAN, VEEGAN and WGAN-GP to learn a 3D Gaussian
ball (xr ∼ N3(0, I)) from a 2D Gaussian input (z ∼
N2(0, I)). This example is meant to train a Generator Gθ
to map between a space which does not have the capacity to
fully model the true distribution. In Fig. 3, we plot the re-
sults. The traditional GAN and VEEGAN fail to cover large
parts of the Gaussian ball, exhibiting partial mode coverage
behaviour. WGAN-GP is able to cover the sphere better, but
there are clearly noticeably large holes within the sphere,
representing parts of the distribution not covered. In con-
trast, our method covers the true distribution signiﬁcantly
better than WGAN-GP and there are no noticeable dead-
zones within the sphere. We compute the reverse-KL diver-
gence similarly to the 8-Gaussian mixture model example
and list the results of 50 runs in Table 1. These results cor-
relate well with the visual results in Fig. 3. Our method out-
performs GAN [12], VEEGAN [32] and WGAN-GP [13],
under this example where the Generator is made to learn
the best mapping it can for covering a single mode while
lacking the latent space capacity to fully represent the true
distribution.

5.2. Real Datasets

For real data, we experimented on three commonly used

datasets for obtaining comparative benchmarks:

CIFAR-10 The CIFAR-10 dataset [19] is a dataset con-
sisting of 50,000 32×32 training images and 10,000 32×32
testing images evenly distributed across 10 broad class cat-
egories.

Oxford Flowers The Oxford Flowers dataset [25] con-
sists of 8,189 images in 102 separate intra-class ﬂower cat-
egories. Following common practice, images were down-
sampled to 64×64 and samples were generated at the same
resolution.

CUB Birds The CUB Birds dataset [36] consists of
11,788 images in 200 separate intra-class bird categories.
Following common practice, images were downsampled to
64×64 and samples were generated at the same resolution.

5.3. Qualitative Results

In Fig. 5 we show qualitative results of generated sam-
ples comparing POT-GAN to the benchmark GANs. We
observe a noticeable improvement in both sample quality
and diversity of POT-GAN when compared to the bench-
mark GAN baselines.

5.4. Quantitative Results

For assessing the improvement in quality and modal di-
versity of the POT-GAN model, we use Inception Score [29]
and FID Score [15]. Methods such as VEEGAN [32] and
WAE-GAN [33] are constructed to prevent mode collapse
and increase modal diversity of the recovered distribution,
but in doing so sacriﬁce sample quality; they perform poorly
on both Inception and FID scores. In Table 2 we show In-
ception and FID scores of POT-GAN along with benchmark
models on the CIFAR-10, Oxford Flowers and CUB Birds
datasets. These results correlate with the sample quality
presented in Fig. 5 where POT-GAN achieves a consider-
able improvement over the other models. For an additional
ablation study on the choice of F , please refer to the sup-
plementary material.

5.5. Effects of regularisation

Finally, we study the effects of adding our optimal
transport regularisation term to the Generator used in the
WGAN-GP [13] framework over the entire GAN training
process. We show the added value the regularisation term
provides over this baseline along the entire optimisation
process. The Inception and FID scores for CIFAR-10 are
plotted over Generator iterations shown in Fig. 4. In both
plots we see POT-GAN converging to a better state than

4417

0246810TrainingIterations#10444.555.566.57ISWGAN-GPPOT-GAN0246810TrainingIterations#1043032343638404244464850FIDWGAN-GPPOT-GANInception Score : exp(ExKL(p(y|x)||p(y)))

DCGAN [27]

VEEGAN [32] WAE-GAN [33] WGAN-GP [13]

POT-GAN (Ours)

CIFAR-10 [19]
Oxford Flowers [25]
CUB Birds [36]

6.16±0.07
6.25±0.05
2.33±0.04
2.11±0.02
3.93±0.03
3.74±0.02
FID Score : km − mwk2

CIFAR-10 [19]
Oxford Flowers [25]
CUB Birds [36]

37.7
88.2
76.3

35.6
299.0
173.9

4.18±0.04
2.30±0.01
3.42±0.04

6.58±0.06
3.42±0.04
4.51±0.04

6.87±0.04
3.53±0.03
4.78±0.04

2 + T r(C + Cw − 2(CCw)1/2
34.4
98.7
70.4

87.7
145.9
143.3

32.5
65.7
58.6

Table 2: Inception and FID scores for DCGAN, VEEGAN, WAE-GAN, WGAN-GP and POT-GAN on CIFAR-10, Oxford
Flowers and CUB Birds datasets.

(a) DCGAN [27]

(b) VEEGAN [32]

(c) WGAN-GP [13]

(d) POT-GAN (Ours)

Figure 5: Qualitative results on Oxford Flowers and CUB Birds datasets. Visually, when compared to the benchmark GAN
models, POT-GAN offers a signiﬁcant improvement in sample quality and diversity. Additional samples for CIFAR-10 can
be found in the supplementary material.

WGAN-GP [13]. The additional regularisation term contin-
uously estimates a more accurate Wasserstein distance and
the Generator is constantly penalised for deviating from the
lower dimension representation. In the supplementary ma-
terial, we also show the critic loss curves for the CIFAR-10
dataset, which provides a good indication of the increased
stability and improvement in convergence of POT-GAN.

6. Conclusion

In this paper, we have presented POT-GAN, an unsu-
pervised learning approach for GANs which estimates the

Wasserstein distance on the latent representation of the data,
using this to regularise training of a GAN. We provide con-
vergence rate guarantees when working in a lower dimen-
sion and show that by applying our latent space regulari-
sation term to the Generator, we can yield signiﬁcant im-
provements in sample quality and diversity when sampling
from the recovered model distribution. Using our approach,
we demonstrate signiﬁcant improvements on the Inception
and FID scores over several GAN baselines.

4418

References

[1] M. Arjovsky and L. Bottou. Towards principled methods
for training generative adversarial networks. arXiv preprint
arXiv:1701.04862, 2017. 1, 3

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan.

arXiv preprint arXiv:1701.07875, 2017. 1, 2, 3

[3] I. Belghazi, S. Rajeswar, A. Baratin, R. D. Hjelm, and
A. Courville. Mine: mutual information neural estimation.
arXiv preprint arXiv:1801.04062, 2018. 2

[4] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode
regularized generative adversarial networks. arXiv preprint
arXiv:1612.02136, 2016. 2

[5] N. Courty, R. Flamary, and M. Ducoffe. Learning wasser-
stein embeddings. arXiv preprint arXiv:1710.07457, 2017.
1

[6] T. Doan, J. Monteiro, I. Albuquerque, B. Mazoure, A. Du-
rand, J. Pineau, and R. D. Hjelm. Online adaptative cur-
riculum learning for gans. arXiv preprint arXiv:1808.00020,
2018. 1

[7] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial fea-

ture learning. arXiv preprint arXiv:1605.09782, 2016. 2

[8] R. Dudley. The speed of mean glivenko-cantelli conver-
gence. The Annals of Mathematical Statistics, 40(1):40–50,
1969. 1, 3

[9] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. arXiv preprint arXiv:1606.00704, 2016.
2

[10] J. H. Friedman. On bias, variance, 0/1loss, and the curse-
of-dimensionality. Data mining and knowledge discovery,
1(1):55–77, 1997. 3

[11] A. Genevay, G. Peyr´e, and M. Cuturi. Learning gener-
arXiv preprint

ative models with sinkhorn divergences.
arXiv:1706.00292, 2017. 1

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014. 1, 2, 3, 6, 7

[13] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. Courville. Improved training of wasserstein gans. arXiv
preprint arXiv:1704.00028, 2017. 1, 3, 4, 5, 6, 7, 8

[14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proceedings of the IEEE international con-
ference on computer vision, pages 1026–1034, 2015. 6

[15] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. Gans trained by a two time-scale update rule
converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6626–6637, 2017. 2,
7

[16] Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing.
arXiv preprint

On unifying deep generative models.
arXiv:1706.00550, 2017. 2

[19] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 7, 8

[20] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and
O. Winther. Autoencoding beyond pixels using a learned
similarity metric. arXiv preprint arXiv:1512.09300, 2015. 2
[21] Z. Lin, A. Khetan, G. Fanti, and S. Oh. Pacgan: The power
of two samples in generative adversarial networks. arXiv
preprint arXiv:1712.04086, 2017. 1

[22] L. Mescheder, A. Geiger, and S. Nowozin. Which training
In International
methods for gans do actually converge?
Conference on Machine Learning, pages 3478–3487, 2018.
1

[23] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. arXiv
preprint arXiv:1802.05957, 2018. 1

[24] H. Narayanan and S. Mitter. Sample complexity of testing
the manifold hypothesis. In Advances in Neural Information
Processing Systems, pages 1786–1794, 2010. 3

[25] M.-E. Nilsback and A. Zisserman. A visual vocabulary for
ﬂower classiﬁcation. In Computer Vision and Pattern Recog-
nition, 2006 IEEE Computer Society Conference on, vol-
ume 2, pages 1447–1454. IEEE, 2006. 7, 8
[26] S. Nowozin, B. Cseke, and R. Tomioka.

f-gan: Training
generative neural samplers using variational divergence min-
imization.
In Advances in Neural Information Processing
Systems, pages 271–279, 2016. 1, 2

[27] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 6,
8

[28] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. Stabiliz-
ing training of generative adversarial networks through reg-
ularization. arXiv preprint arXiv:1705.09367, 2017. 1

[29] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training gans. In
Advances in Neural Information Processing Systems, pages
2234–2242, 2016. 1, 2, 7

[30] T. Salimans, H. Zhang, A. Radford, and D. Metaxas.
arXiv preprint

Improving gans using optimal transport.
arXiv:1803.05573, 2018. 1

[31] V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet,
and M. Blondel. Large-scale optimal transport and mapping
estimation. arXiv preprint arXiv:1711.02283, 2017. 1

[32] A. Srivastava, L. Valkoz, C. Russell, M. U. Gutmann, and
C. Sutton. Veegan: Reducing mode collapse in gans using
implicit variational learning. In Advances in Neural Infor-
mation Processing Systems, pages 3310–3320, 2017. 2, 6, 7,
8

[33] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf.
preprint

arXiv

Wasserstein
arXiv:1711.01558, 2017. 6, 7, 8

auto-encoders.

[34] C. Villani. Topics in optimal transportation (graduate studies

in mathematics, vol. 58). 2003. 2, 4

[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-

[35] C. Villani. Optimal transport: old and new, volume 338.

mization. arXiv preprint arXiv:1412.6980, 2014. 6

Springer Science & Business Media, 2008. 2, 3

[18] D. P. Kingma and M. Welling. Auto-encoding variational

[36] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.

bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 4, 6

The caltech-ucsd birds-200-2011 dataset. 2011. 7, 8

4419

[37] J. Weed and F. Bach. Sharp asymptotic and ﬁnite-sample
rates of convergence of empirical measures in wasserstein
distance. arXiv preprint arXiv:1707.00087, 2017. 3

[38] Y. Zuo, G. Avraham, and T. Drummond. Generative ad-
versarial forests for better conditioned adversarial learning.
arXiv preprint arXiv:1805.05185, 2018. 5

[39] Y. Zuo, G. Avraham, and T. Drummond.

Travers-
arXiv preprint

ing latent space using decision ferns.
arXiv:1812.02636, 2018. 5

[40] Y. Zuo and T. Drummond. Fast residual forests: Rapid en-
semble learning for semantic segmentation. In Conference
on Robot Learning, pages 27–36, 2017. 5

4420

