Unsupervised Learning of Dense Shape Correspondence

Oshri Halimi

Technion, Israel

Or Litany

Emanuele Rodol`a

Facebook AI Research

Sapienza University of Rome

oshri.halimi@gmail.com

orlitany@fb.com

rodola@di.uniroma1.it

Alex Bronstein
Technion, Israel

Ron Kimmel

Technion, Israel

bron@cs.technion.ac.il

ron@cs.technion.ac.il

Reference

Unsupervised
(this paper)

Supervised [28]

FM (SHOT) [36]

+PMF

SGMDS [4]

FM (SHOT)

SGMDS
+ PMF

Figure 1: Dense correspondence between articulated objects obtained with the proposed unsupervised loss, optimized on
a single (unlabeled) example. Our method is compared with the state-of-the-art supervised network pre-trained on human
shapes, as well as with two axiomatic methods, employing a post processing algorithm [49] on the axiomatic results. See
Section 5.1 for more details. Correspondence is visualized by colors mapped from the leftmost reference shape.

Abstract

We introduce the ﬁrst completely unsupervised corre-
spondence learning approach for deformable 3D shapes.
Key to our model is the understanding that natural deforma-
tions, such as changes in pose, approximately preserve the
metric structure of the surface, yielding a natural criterion
to drive the learning process toward distortion-minimizing
predictions. On this basis, we overcome the need for an-
notated data and replace it by a purely geometric crite-
rion. The resulting learning model is class-agnostic, and
is able to leverage any type of deformable geometric data
for the training phase. In contrast to existing supervised ap-
proaches which specialize on the class seen at training time,
we demonstrate stronger generalization as well as applica-
bility to a variety of challenging settings. We showcase our
method on a wide selection of correspondence benchmarks,
where the proposed method outperforms other methods in
terms of accuracy, generalization, and efﬁciency.

1. Introduction

The problem of ﬁnding accurate dense correspondence
between non-rigid shapes is fundamental in geometry pro-

cessing. It is a key component in applications such as de-
formation modeling, cross-shape texture mapping, pose and
animation transfer to name just a few. Dense deformable
shape correspondence algorithms can be broadly catego-
rized into two families. The ﬁrst can be referred to as
axiomatic or model-based for which a certain geometric
assumption is asserted and pursuit for by some numerical
scheme. Modeling assumptions attempt to characterize the
action of a class of deformations on some geometric quan-
tities commonly referred to as descriptors. Such geometric
quantities often encode local geometric information in the
vicinity of a point on the shape (point-wise descriptors) such
as normal orientation [47], curvature [37], and heat [45] or
wave [6] propagation properties. Another type of geometric
quantities are the global relations between pairs of points
(pair-wise descriptors), which include geodesic [21, 14, 4],
diffusion [17, 12] or commute time [50] distances. Given a
pair of shapes, a dense map between them is sought to min-
imize the discrepancy between such descriptors. While the
minimization of the point-wise discrepancies can be formu-
lated as a linear assignment problem (LAP) and solved ef-
ﬁciently for reasonable scales, the use of pair-wise descrip-
tors leads to a quadratic assignment problem (QAP) that is

14370

unsolvable for any practical scales. Numerous approxima-
tions and heuristics have been developed in the literature to
alleviate the computational demand of QAPs.

The second family of correspondence algorithms is data-
driven and takes advantage of modern efﬁcient machine
learning tools.
Instead of axiomatically modeling the
class of deformations and the geometric properties of the
shapes of interest, these methods infer such properties from
the data themselves. Among such approaches are learn-
able generalizations of the heat kernel signature [31], as
well as models interpreting correspondence as a labeling
problem [41]. Other recent methods generalize CNNs to
non-Euclidean structures for learning improved descriptors
[35, 10]. A recent method based on extrinsic deformation
of a null-shape was introduced in [23]. A common denom-
inator of these approaches is the supervised training regime
– they all rely on examples of ground truth correspondences
between exemplar shapes.

A major drawback of this supervised setting is the fact
that in the case of 3D shape correspondence the ground truth
data are scarce and expensive to obtain. For example, de-
spite being restricted to a single shape class (human bodies),
the MPI FAUST scanning and labeling system [8] required
substantial manual labor and considerable ﬁnancial costs.
In practice, labeled models are expected to be just a small
fraction of the existing geometric data, bringing into ques-
tion the scalability of any supervised learning algorithm.

1.1. Contribution

We propose an unsupervised learning scheme for dense
3D deformable shape correspondence based on a purely
geometric criterion. The suggested approach bridges be-
tween the model-based and the data-driven worlds by learn-
ing point-wise descriptors that result in correspondences
minimizing pair-wise geodesic distance distortion [21, 14].
The unsupervised loss is intimately related to the spec-
tral generalized multidimensional scaling [4] model. The
correspondence is solved by functional maps framework
[36], totally avoiding the computational burden of the pair-
wise methods. The point-wise descriptors are learned on a
surrogate task, only approximately characterizing the real
data, which deviate from the asserted isometric deforma-
tion model. Still, the method shows excellent generalization
capabilities exceeding the supervised counterparts without
ever seeing examples of ground truth correspondences. To
the best of our knowledge, this is the ﬁrst unsupervised ap-
proach applied to the geometric shape correspondence prob-
lem.

A major advantage of the proposed framework is when
the data themselves are scarce, in extreme conditions we
might have only one pair of shapes that we would like to
match and we do not have a training dataset that contains
similar shapes. While a supervised scheme depends on a

relatively large amount of labelled data to deduce a general-
izing model, with the unsupervised network we can simply
optimize on a single pair of shapes that by itself contains
two training samples, one in each direction of the corre-
spondence. Our experiments required only a few iterations
that take just a couple of minutes to run. As a result we
obtain an accurate matching between the shapes, see Fig-
ure 1. For a trained network the inference phase takes less
than a second. We believe that this strategy has its own
merits as a replacement of the existing computationally ex-
pensive methods that are based on pair-wise descriptors.
The framework can be interpreted as a fusion between the
previously proposed generalized multidimensional scaling
(GMDS) [4, 14] and the FMNet network architecture [28].
Here we use the discretization of the pair-wise geodesic dis-
tance distortion, as suggested in [4] and justiﬁed in [34].

2. Background

2.1. Minimum distortion correspondence

We model

shapes as Riemannian 2-manifolds X
equipped with a distance function dX : X × X → R in-
duced by the standard volume form dx. An isometry is a
map π : X → Y satisfying, for any pair x1, x2 ∈ X :

dX (x1, x2) = dY (π(x1), π(x2)) .

(1)

Correspondence seeking approaches optimize for a map
π satisfying the distance preservation criterion (1).
In
practical applications, only approximate realizations of an
isometry are expected; thus, one is interested in ﬁnding a
distortion-minimizing map of the form

π∗ = arg min

π:X →YZZx1,x2∈X

(dX (x1, x2) − dY (π(x1), π(x2)))2 dax1 dax2

(2)
In the discrete setting, we assume manifolds X , Y to be
represented as triangle meshes sampled at n vertices each.
Minimum distortion correspondence takes the form of a
quadratic assignment problem (QAP), where the minimum
is sought over the space of n × n permutation matrices.
Several studies have tried to reduce the complexity of this
QAP at the cost of getting an approximate solution via sub-
sampling [46, 39], hierarchical matching [14, 51, 38, 20] or
convex relaxations [3, 16]. However complicated to solve,
the minimum distortion criterion (2) is axiomatic and does
not require any annotated correspondences, making it a nat-
ural candidate for an unsupervised learning loss.

2.2. Descriptor learning

A common way to make the optimization of (2) more ef-
ﬁcient is by restricting the feasible set to include only poten-
tial matches among points with similar descriptors. By do-
ing so, one shifts the key difﬁculty from optimizing a highly

4371

non-linear objective to designing deformation-invariant lo-
cal point descriptors. This has been an active research goal
in shape analysis in the last few years, with examples in-
cluding GPS [42], heat and wave kernel signatures [45, 6],
and the more recent geodesic distance descriptors [44]. In
3D vision, several rotation-invariant geometric descriptors
have been proposed [47, 25]. Despite their lack of invari-
ance to isometric deformations, the adoption of extrinsic
descriptors has been advocated in deformable settings [40]
due to their locality and resilience to boundary effects.

Handcrafted descriptors suffer from an inherent draw-
back of requiring manual tuning. Learning techniques have
thus been proposed to deﬁne descriptors whose invariance
classes are learned from the data. Early examples include
approaches based on decision forests and metric learning
[31, 41, 19]; more recently, several papers have proposed
an adaptation of deep learning models to non-Euclidean do-
mains, achieving dramatic improvement.
In [33, 10, 35]
learnable local ﬁlters were introduced based on the notion
of patch operator. In [28] a task driven approach was taken
instead, where the network learns descriptors which excel
at the task at hand in a supervised manner. As we will show
in the sequel, our approach replaces the penalty of the latter
model with the one optimized for in the SGMDS model [4],
completely removing the need for supervision.

2.3. Functional maps

The notion of functional map was introduced in [36] as
a tool for transferring functions between surfaces without
the direct manipulation of a point-to-point correspondence.
Let F(X ), F(Y) be real-valued functional spaces deﬁned
on top of X and Y respectively. Then, given a bijection
π : X → Y, the functional map T : F(X ) → F(Y) is a
linear mapping acting as

T (f ) = f ◦ π−1 .

(3)

The functional map T admits a matrix representation with
respect to orthogonal bases {φi}i≥1, {ψi}j≥1 on X and Y
respectively, with coefﬁcients C = (cij) calculated by

T (f ) = Xij

hφi, f i hT φi, ψji

ψj .

(4)

|

cji

{z

}

While the functional maps formalism makes no further
requirements on the chosen bases, a typical choice is the
Laplace-Beltrami eigenbasis, where the justiﬁcation for the
optimality of this choice can be found in [2]. Truncating
these series to k coefﬁcients, one obtains a band-limited ap-
proximation of the functional correspondence T . Speciﬁ-
cally, the map

P : x 7→ Xi,j

cjiφi(x)ψj ,

(5)

also referred to as a soft map, will assign to each point
x ∈ X a function concentrated around y = π(x) with some
spread.

To solve for the matrix C, linear constrains are derived
from the knowledge of knowingly corresponding functions
on the two surfaces. Corresponding functions are functions
that preserve their value under the mapping T . Given a pair
of corresponding functions f : X → R and g : Y → R
with coefﬁcients ˆf = {hφi, f i}i and ˆg = {hψj, gi}j in
the bases {φi} and {ψj} respectively, the correspondence
imposes the following linear constraint on C

ˆg = Cˆf .

(6)

Each pair of such corresponding functions is translated into
a linear constraint.

Suppose there exists an operator receiving a shape X and
producing a set of descriptor functions on it. Let us further
assume that given another shape Y, the operator will pro-
duce a set of corresponding functions related by the latent
correspondence between X and Y. In other words, apply-
ing the above operator on the said pair of shapes produces
a set of pairs of corresponding functions (fi, gi), each pair
comprising fi deﬁned on X and gi on Y. We stack the cor-
responding coefﬁcients ˆf i and ˆgi into the columns of the
matrices ˆF and ˆG. The functional map matrix C is then
given by the (least squares, or otherwise regularized) solu-
tion to the system

ˆG = CˆF.

(7)

Thus, the requirement for speciﬁc knowledge of the point-
to-point correspondence is replaced by the relaxed require-
ment of knowledge about functional correspondence.

2.4. Deep functional maps

A signiﬁcant caveat in the above setting is that, unless
the shapes X and Y are related by a narrow class of defor-
mations, it is very difﬁcult to construct an operator produc-
ing a sufﬁcient quantity of stable and repeatable descriptors.
However, such an operator can be learned from examples.
The aim of the deep functional maps network (FMNet) in-
troduced in [28] was to learn descriptors which, when used
in the above system of equations, will induce an accurate
correspondence. At training time, FMNet operates on in-
put descriptor functions (e.g. SHOT descriptors), and im-
proves upon them by minimizing a geometric loss that is
deﬁned on the soft correspondence derived from the func-
tional map matrix. The differentiable functional map layer
(FM), solves the equation (7), with the current descriptor
functions in each iteration.

The network architecture described in [28] consists of 7
fully-connected residual layers with exponential linear units
(ELU) and no dimentionality reduction. The output of the
residual network is a dense vector-valued descriptor. Given

4372

Φ

X

SHOT

Θ

Y

SHOT

Ψ

1

s
e
R

1

s
e
R

2

s
e
R

2

s
e
R

· · ·

· · ·

K
s
e
R

K
s
e
R

h·, ·i

F

ˆF

FM

C

Corr

P

ℓF

G

h·, ·i

ˆG

Figure 2: Deep Functional Maps network architecture [28]

two shapes X and Y, the descriptors are calculated on each
shape using the same network, and are projected onto the
corresponding truncated LBO bases. The resulting coefﬁ-
cients are given as an input to the functional map (FM) layer
that calculates the functional map matrix C ∈ Rk×k ac-
cording to (7). The following correspondence layer (Corr)
produces a soft correspondence matrix P ∈ RnY ×nX out of
the functional map matrix C,

P = |ΨCΦT A|k·k.

(8)

Where we denoted the number of vertices on the discretized
shapes as nX and nY , and the diagonal matrix A normalizes
the inner products with the discrete area elements of X . The
absolute value and the L2 column normalization, denoted
by k·k, ensure that the values of p2
ji can be interpreted as the
probability of vertex j on shape Y being in correspondence
with vertex i on X . We denote the element-wise square of
P by Q = P◦P, with ◦ standing for the Hadamard product.
Treating the i-th column of Q, qi, as the distribution on
the points of Y corresponding to the point i on X , we can
evaluate the expected deviation from the ground truth cor-
respondence π∗(i). This is expressed by the second-order
moment

Ej∼qi d2

Y (j, π∗(i)) = Xj∈Y

qjid2

Y (j, π∗(i)).

(9)

where dY (j, π∗(i)) is the geodesic distance on Y between
the vertex j and the ground truth match π∗(i) of the vertex
i on X . As usual, this moment comprises a variance and a
bias terms; while the former is the result of the band-limited
approximation (due to the truncation of the basis), the latter
can be controlled. Averaging the above moment over all
points on X leads to the following supervised loss

ℓsup(X , Y) =

1

|X | Xi∈X Xj∈Y

qjid2

Y (j, π∗(i))

=

1
|X |

kP ◦ (DY Π∗)k2
F ,

(10)

where DY denotes the pairwise geodesic distance matrix
evaluated for each shape at the pre-processing stage, and
Π∗ is the ground truth permutation relating between the

shapes. The batch loss is the sum of ℓsup(X , Y) for all
the pairs in the minibatch.
Training an FMNet follows
the standard Siamese setting commonly used for descrip-
tor or metric learning, in which two copies of the network
with shared parameters produces the descriptors on X and
Y. From this perspective, the functional map and the soft
correspondence layers are parts of the Siamese loss rather
than of the network itself.

3. Unsupervised deep functional maps

The FMNet achieves state-of-the-art performance on
standard deformable shape correspondence benchmarks as
shown in [28]. However, one can argue that the supervised
training regime is prohibitive in terms of the amount of the
manually annotated data required.

The main contribution of this paper is replacing supervi-
sion by pointwise correspondences with standard geometric
quantities that do not require annotations, as formulated in
[4].

As mentioned before, human pose articulation can be
modeled as approximate isometries, that is, the latent cor-
respondence introduces little metric distortion. If two ver-
tices were at some geodesic distance on the source shape,
after mapping by the correct correspondence, the distance
between corresponding points on the target domain is pre-
served.

Let P be the output of the soft correspondence layer of
an FMNet; as before, its squared elements qji = p2
ji are
interpreted as probability distributions on Y. In these terms,
the ji-th element of the matrix QT DY Q

(QT DY Q)ji = Xm,n

p2
mip2

njdY (m, n)

(11)

represents the expected distance on Y between the images
of the vertices i, j ∈ X under the soft correspondence P.
This allows to deﬁne the following unsupervised loss

ℓuns(X , Y) =

1

|X |2 (cid:13)(cid:13)(cid:13)DX − QT DY Q(cid:13)(cid:13)(cid:13)

2

F

.

(12)

The batch loss is the sum of ℓuns(X , Y) for all the pairs
in the minibatch. This loss measures the L2 geodesic dis-
tance distortion and can be interpreted as the soft correspon-
dence version of the SGMDS loss, see [4]. Note that rather
than solving the QAP directly, we propose to train an FM-
Net using ℓuns, which promotes the network to generate de-
scriptors for which the resulting soft correspondence min-
imizes the expected pairwise distance distortion. The un-
supervised loss model thereby shares a common theoretical
framework with the spectral generalized multidimensional
scaling (SGMDS) model. Note, that for the unsupervised
loss the whole optimization could have been executed in
the spectral domain, as suggested by the SGMDS model

4373

[4]. In that case the Corr module in the FMnet architecture
presented in Fig. 2 could have been avoided. We plan to
explore such architectures in the future.

From the unsupervised perspective all the shapes in the
world could constitute a training set. Since the network is
ground-truth independent and general to any class of shapes
it is expected to improve when new shapes are encoun-
tered. The strict train-test separation applied in the super-
vised regime can be followed in the unsupervised regime
depending on the settings; If the training set is representa-
tive enough to generalize the test set, the network can learn
on the training set and infer on the test set, reducing the pro-
cessing time per shape. On the other hand, for a non repre-
sentative training set, the network can improve the learned
model by processing the test shapes as well. Learning could
in fact be executed even at inference time, as demonstrated
in 5.1. For the FAUST scans [7], for example, the authors
provide a training set with ground-truth labeling and a dis-
joint unlabeled test set. While the unsupervised scheme has
access to the same data, contrarily to the supervised coun-
terpart, it can leverage the unlabeled test shapes to improve
the prediction accuracy.

4. Implementation

We implemented our network in TensorFlow [1], run-
ning on a GeForce GTX 1080 Ti GPU. Data preprocessing
and correspondence reﬁnement were done in Matlab. We
provide a link to the code in the supplementary material.

4.1. Pre processing

To enable mini-batches of multiple shapes to ﬁt in mem-
ory, each shape in the training set was remeshed to between
n ∼ 5K and 7K vertices , by edge contraction [22]. For
each remeshed shape k ∼ 70 − 150 LBO eigenfunctions
were calculated as well as a 352-dimensional SHOT de-
scriptor [43], using 10 bins and a SHOT radius roughly
chosen to 5% of the maximal pairwise geodesic distance.
Geodesic distance matrices D were estimated using the fast
marching method [27]. These quantities constitute the input
to the network.

4.2. Network architecture and loss

For a more direct and fair comparison, we adopted the
same network architecture as FMNet presented in Fig. 2.
The input for each pair of shapes is their n × k truncated
LBO bases Φ and Ψ , the n × n pairwise distance matrices
DX and DY , and the n×352 SHOT descriptor ﬁelds. These
are fed to a 7-layer residual network [24] outputting 352-
dimensional dense descriptor ﬁelds F and G on X and Y
respectively, which can be thought of as non-linearly trans-
formed variants of SHOT. The computed descriptors are
then input to the functional map layer, yielding a functional

map matrix C according to (7), followed by a soft corre-
spondence layer producing the stochastic correspondence
matrix P as per Eq. (8). Finally, the unsupervised loss is
calculated according to Eq. (12). While in FMNet the loss
is calculated on a random sub-sampling of the vertices, we
found that this strategy introduces inaccuracies to the de-
scriptor coefﬁcients in the LBO basis; When sub-sampling
is used, the network evaluates an estimate of the projection
coefﬁcients, which quickly becomes inaccurate for descrip-
tors with high-frequency content. To avoid this, in our im-
plementation we perform the projection at full resolution
while decreasing the size of the mini-batch to 4–5 pairs of
shapes per mini-batch. In all our experiments we used no
more than a few thousand (3K–10K) mini-batch iterations.

4.3. Post processing

Point-wise map recovery. Following the protocol of FM-
Net, we apply the product manifold ﬁlter (PMF) [49], to
improve the raw prediction of the network,
in the full
synthetic shapes settings. We found the geodesic ker-
nel less effective when topological noise exists, e.g.
real
scans. Additionally, PMF is not well suited to partiality.
PMF algorithm takes noisy matches as input, and produces
a (guaranteed) bijective and smoother correspondence of
higher accuracy as output. The application of PMF boils
down to solving a series of linear assignment problems
arg maxΠt hΠt, KX Πt−1K⊤
Y iF , where Πt ranges over the
space of permutations, and KX , K⊤
Y are kernel matrices
acting as diffusion operators. We refer to [49] for additional
details.

Upscaling. Since we operate on remeshed shapes, we ﬁ-
nally apply an upscaling step to bring the correspondence
back to the original resolution. Again, we follow the proce-
dure described in FMNet [28], namely we solve a functional
map estimation problem of the form

Cup = arg min
C

kC ˆFup − ˆGupk2,1 ,

(13)

where ˆFup, ˆGup contain the LBO coefﬁcients (in the full
resolution basis) of delta functions supported at correspond-
ing points, extracted from the low resolution map C. The
ℓ2,1 norm (deﬁned as the sum of ℓ2 norms of the columns)
allows to down-weight potential mismatches.

5. Experiments

5.1. Learning to match a single pair

Before delving into training on large datasets, we begin
our experimental section with testing one extreme of the
shape matching problem: single input pair. Clearly, this
is the native environment for classical, non-learning based
methods. While learning-based methods have endowed us

4374

with better solutions given large train sets, they are not
equipped to handle entirely novel examples. We demon-
strate that the unsupervised network can be utilized as an
ad-hoc solver for a single pair, producing excellent results,
while initialized with random weights. Fig. 1 shows our un-
processed network result on a pair of shapes made by an
artist. Note that the ground-truth labeling is not provided
and therefore a supervised learning-based method can not
be ﬁne-tuned on the input pair. Instead we compare with
the unprocessed predictions of FMNet pre-trained on hu-
man shapes from FAUST. Additionally, we compare with
two axiomatic methods [4, 36], using the same number of
LBO eigenfunctions (150) and applying the post processing
procedure described in 4.3 on the axiomatic results. While
our method exhibit superior performance, axiomatic meth-
ods runtime exceeded one hour. Conversely, optimizing our
network took about 15 minutes. Furthermore, had we been
given an additional deformation of the same shape to solve
for, any axiomatic method would have to solve the prob-
lem from scratch. Differently, as our method had already
learned to convert the pair-wise optimization problem to a
descriptor matching problem, inference would take about
one second! See the supplementary material for fast infer-
ence experiment.

5.2. Faust synthetic

In this experiment we compare our unsupervised method
and its supervised counterpart in the same settings. We
show that (a) optimizing for the unsupervised loss results
in a correlated decrease of the supervised loss; (b) the un-
supervised method achieves the same accuracy as the su-
pervised one. For training our network, we used Faust syn-
thetic human shapes [7] and followed the same dataset split
as in [28] where the ﬁrst 80 shapes of 8 subjects are used
for training, and a disjoint set of 20 shapes of 2 other sub-
jects is used for testing. Each training mini-batch contained
4 shape pairs in their full resolution of 6890 vertices. We
used the same parameters as in [28], namely, k = 120
eigenfunctions and ADAM optimizer with a learning rate
of α = 10−3, β1 = 0.9, β2 = 0.999 and ǫ = 10−8. We
used 3K training mini-batches. Note that, as in [28], since
we train on shape pairs the effective train set size is 6400.

Loss function analysis. Fig. 3 displays the unsupervised
loss during the training process (top), alongside with the su-
pervised loss (bottom). Importantly, the unsupervised net-
work had no access to ground truth correspondence. From
the graphs, it can be observed that while the optimization
target is the unsupervised loss, the supervised loss is de-
creased as well. This demonstrates nicely that when our
underlying assumption of (quasi-) isometric deformations
holds, one can replace the expensive supervision altogether
with a single axiomatic-driven loss term.

Figure 3: Unsupervised loss (left axis) and supervised loss
(right axis) measured during the unsupervised training pro-
cess, in logarithmic scale.

Figure 4: Unsupervised and supervised network results,
evaluated on synthetic Faust intra-subject test pairs.

Figure 5: Synthetic Faust texture transfer. Four right models
show the predicted matching from the reference model.

Performance comparison. To compare our results with the
supervised network, we followed the same training scheme,
this time using the supervised loss. We used the 20 test
shapes to construct a test dataset of 400 pairs in total; 200
of which are intra-subject pairs, and the other 200 are inter-
subject pairs (Note that the matching is directional from
source to target, hence this set is not redundant). The intra-
subject pairs, are well modeled by isometry while the inter-
subject pairs exhibit deviation from isometry. Fig. 4 com-
pares the results for the 200 intra-subject test pairs and
Fig. 5 visualizes the calculated correspondences between
intra- as well as inter-subject test pairs. Additional visu-
alization is available in the supplementary material.

4375

5.3. Real scans

Traditionally, axiom-based methods were proven useful
only in the Computer Graphics regime. One of our goals
in introducing learned descriptors is to demonstrate the ap-
plicability of our method to real scanned data. To this
end, we make use of FAUST real scans benchmark. These
are very high-resolution, non-watertight meshes, many of
which contain holes and topological noise. We used the
whole scanned data in the benchmark. The scans were
down-sampled to a resolution of 7K vertices. For each
scan the distance matrix was calculated, as well as 352-
dimensional SHOT descriptors and k = 70 LBO eigenfunc-
tions. Each training mini-batch contained 4 pairs of shapes.
We trained our network for 10K iterations. The raw net-
work predictions were only upscaled but not reﬁned with
PMF, as explained in 4.3. Quantitative results were evalu-
ated through the online evaluation system. With an average
and worst case scores of 2.51 cm and 24.35 cm, respec-
tively, on the intra challenge, our network performs on par
with state of the art methods that do not use additional data;
namely, FMNet (2.44, 26.16), and Chen et al. [16] (4.86,
26.57). We perform slightly below the recent 3D-CODED
method [23] (1.98, 5.18) which uses an additional augmen-
tation of over 200K shapes at training. The same method,
when not using additional data achieves worse results by a
factor of ≈ 9. For completion, we also trained the network
in the same settings, using only the train-set. We noticed
a minor change in the performance: [Average error: slight
deterioration to 2.82 cm instead of 2.51 cm; maximum er-
ror: slight improvement to 20.64 cm instead of 24.35 cm],
keeping the result on-par with FM-Net.

5.4. Generalization

Having an unsupervised loss grants us the ability to train
on datasets without given dense correspondences, or even
to optimize on individual pairs. Both methods were demon-
strated in the previous experiments. In this subsection, how-
ever, we would like to pose a different question: what has
our network learned by training on a source dataset, and to
which extent this knowledge is transferable to a target one.
Transferability between training domains is a long-standing
research area that has recently re-gained lots of interest, yet
it hasen’t been explored as much in the shape analysis com-
munity. In the scope of this work we focus on transferring
from either the synthetic or scanned FAUST shapes to the
either (a) human shapes form Dynamic-FAUST, (b) human
shapes form SCAPE, and (c) Animal shapes from TOSCA.
We show the prediction of the networks that were trained
with Faust synthetic or scanned data, evaluated on (a,b,c),
without using train samples from these datasets.

Dynamic FAUST is a recent very large collection of human
shapes [8], including various sequences of activities. While

the shapes are triangulated in the same way as our train set
of synthetic FAUST, they signiﬁcantly differ in pose and ap-
pearance. Fig. 8 shows excellent generalization to this set,
suggesting that the small set of 80 synthetic FAUST shapes
were sufﬁcient to capture the pose and shape variability. For
additional visualizations see the supplementary material.

SCAPE [5] also comprises human shapes only. Yet, we’ve
witnessed a quite poor performance using the network
trained on synthetic FAUST. By the same reasoning behind
the former result, the network might have learned to special-
ize on synthetic connectivity. To circumvent this, we have
tested the network trained on scans, that demonstrate differ-
ent meshes. Indeed Figure 7 displays good generalization.

TOSCA dataset [15] includes various animal shapes. Im-
pressively, the network trained on human scans shows very
good performance without ever seeing a single animal shape
at train time. In Fig. 9 we compare with several axiomatic
models and with a supervised network [41] trained sepa-
rately on each animal category and show comparable results
before pre-processing, and near-perfect results after.

5.5. Partial correspondence

Partial shape correspondence is a notoriously hard prob-
lem, and techniques that aim at solving it often require spe-
cial care [29, 40, 30]. That said, in this experiment we
tested the performance of our method under extreme par-
tiality conditions as is, namely, without any modiﬁcation
to our network. To this end, we used the challenging “dog
with holse” class from [18]. We trained the network on a
small set of 10 partial shapes, and evaluated the results 26
test shapes. The network results are shown in Fig. 6. We
found that the mismatches occur typically near the bound-
ary of the partial shape. The reason might be the distortion
of the SHOT descriptor in these regions.

6. Discussion and conclusions

The main message of the paper is that a properly de-
signed unsupervised surrogate task can replace massive la-
beling. While we advocate the pure unsupervised approach
as a replacement to the supervised one, the two can also be
combined in a semi-supervised learning scheme. While we
demonstrate that the minimization of geodesic distance dis-
tortion achieves good generalization on a variety of bench-
marks, local scale variation and topological changes can
challenge the classic model and require a proper adapta-
tion.
In future studies, we intend to investigate training
tasks based on the preservation of more general scale- and
conformal-invariant pair-wise geometric quantities, as well
as topological properties, e.g. by utilizing pairwise diffu-
[12]. The proposed network ex-
sion distances, see e.g.
hibits surprisingly high performance on partial correspon-
dence tasks, even though the functional map layer is not

4376

Reference

Ours

[40]

Ours

[40]

100

80

60

40

20

s
e
c
n
e
d
n
o
p
s
e
r
r
o
C
%

Ours

Partial FM [40]

0

0

0.02

0.04

0.06

0.08

0.1

Geodesic error

Figure 6: Comparisons on the SHREC’16 benchmark [18] (dog class) for partial matching of deformable shapes. We
demonstrate results in line with partial functional maps [40], the current state of the art for this problem. The partial shapes
shown on the right are matched to the reference; corresponding points have similar color.

SCAPE

TOSCA

100

80

60

40

20

s
e
c
n
e
d
n
o
p
s
e
r
r
o
C
%

0

0

0.02

0.04

0.06

0.08

0.1

Geodesic error

Ours

FMNet [28]

OSD [32]

GCNN [33]

LSCNN [9]

ADD3 [11]

mADD3 [11]

100

80

60

40

20

s
e
c
n
e
d
n
o
p
s
e
r
r
o
C
%

Reference

Ours + PMF

Ours

RF [41]

PMF [48]

BIM [26]

SGMDS [4]

FM [36]

Figure 7: Generalization on SCAPE. For fair comparison,
we show our network prediction without PMF reﬁnement.

Dynamic FAUST

Reference

0

0

0.02

0.04

0.06

0.08

0.1

Geodesic error

100

80

60

40

20

s
e
c
n
e
d
n
o
p
s
e
r
r
o
C
%

Ours

0

0

0.02

0.04

0.06

0.08

0.1

Geodesic error

Figure 8: Generalization experiments on Dynamic FAUST.
We render the network predictions with PMF reﬁnement.

explicitly designed to treat partial data. Extending it to the
partial setting based on the recently introduced partial func-
tional map formalism [40, 29] and its relation to previous
explicit efforts [13] will be the subject of further investiga-
tion. Finally, we would like to explore additional descriptor

Figure 9: Generalization on TOSCA. We show our network
prediction before (dashed curve) and after (solid) PMF re-
ﬁnement. The rendered visualization is before reﬁnement.

ﬁelds with enhanced properties like increased sensitivity to
symmetries, increased robustness to partiality and non rigid
deformations. This paper presents a ﬁrst attempt to create a
fully unsupervised learning framework to solve the funda-
mental problem of non rigid shape correspondence. We be-
lieve that the fusion of axiomatic models and deep learning
is a promising direction that makes it possible to accommo-
date the expected future growth of 3D data.

Acknowledgements

We gratefully acknowledge Matteo Sala for providing the models
that appear in Fig. 1. Ron Kimmel and Oshri Halimi were sup-
ported by Israel Ministry of Science grant no. 3-14719, the Tech-
nion Hiroshi Fujiwara Cyber Security Research Center, and the
Israel Cyber Bureau. Alex Bronstein was supported by ERC grant
no. 335491 (RAPID). Emanuele Rodol`a was supported by ERC
grant no. 802554 (SPECGEO), and the MIUR under grant “Dipar-
timenti di eccellenza 2018-2022” of the Department of Computer
Science of Sapienza University.

4377

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5

[2] Y. Aﬂalo, H. Brezis, and R. Kimmel. On the optimality of
shape and data representation in the spectral domain. SIAM
Journal on Imaging Sciences, 8(2):1141–1160, 2015. 3

[3] Y. Aﬂalo, A. Bronstein, and R. Kimmel. On convex relax-
ation of graph isomorphism. Proceedings of the National
Academy of Sciences, 112(10):2942–2947, 2015. 2

[14] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. General-
ized multidimensional scaling: a framework for isometry-
invariant partial surface matching. PNAS, 103(5):1168–
1172, 2006. 1, 2

[15] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Nu-
merical geometry of non-rigid shapes. Springer Science &
Business Media, 2008. 7

[16] Q. Chen and V. Koltun. Robust Nonrigid Registration by
Convex Optimization.
In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 2039–2047,
2015. 2, 7

[17] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler,
F. Warner, and S. W. Zucker. Geometric diffusions as a tool
for harmonic analysis and structure deﬁnition of data: Diffu-
sion maps. PNAS, 102(21):7426–7431, 2005. 1

[18] L. Cosmo, E. Rodol`a, M. M. Bronstein, et al. SHREC’16:
In Proc. 3DOR,

Partial matching of deformable shapes.
2016. 7, 8

[4] Y. Aﬂalo, A. Dubrovina, and R. Kimmel. Spectral gen-
eralized multi-dimensional scaling. IJCV, 118(3):380–392,
2016. 1, 2, 3, 4, 5, 6, 8

[19] L. Cosmo, E. Rodol`a, J. Masci, A. Torsello, and M. M. Bron-
stein. Matching deformable objects in clutter. In Proc. 3DV,
2016. 3

[5] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers,
and J. Davis. SCAPE: Shape Completion and Animation of
People. TOG, 24(3):408–416, 2005. 7

[6] M. Aubry, U. Schlickewei, and D. Cremers. The wave kernel
signature: A quantum mechanical approach to shape analy-
sis. In Proc. ICCV Workshops, pages 1626–1633, 2011. 1,
3

[7] F. Bogo, J. Romero, M. Loper, and M. J. Black. FAUST:
Dataset and Evaluation for 3d Mesh Registration. In Proc.
CVPR, 2014. 5, 6

[8] F. Bogo, J. Romero, G. Pons-Moll, and M. J. Black. Dy-
namic FAUST: Registering human bodies in motion. In IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
July 2017. 2, 7

[9] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castel-
lani, and P. Vandergheynst. Learning class-speciﬁc descrip-
tors for deformable shapes using localized spectral convolu-
tional networks. Computer Graphics Forum, 34(5):13–23,
2015. 8

[10] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learn-
ing shape correspondence with anisotropic convolutional
neural networks.
In Advances in Neural Information Pro-
cessing Systems, pages 3189–3197, 2016. 2, 3

[11] D. Boscaini, J. Masci, E. Rodol`a, M. M. Bronstein, and
D. Cremers. Anisotropic diffusion descriptors. In Computer
Graphics Forum, volume 35, pages 431–441. Wiley Online
Library, 2016. 8

[12] A. M. Bronstein, M. Bronstein, M. M. R. Kimmel, and
G. Sapiro. A Gromov-Hausdorff framework with diffusion
geometry for topologically-robust non-rigid shape matching.
International Journal of Computer Vision, 89(2-3):266–286,
2010. 1, 7

[13] A. M. Bronstein, M. M. Bronstein, A. M. Bruckstein, and
R. Kimmel. Partial similarity of objects, or how to compare a
centaur to a horse. International Journal of Computer Vision,
84(2):163–183, 2009. 8

[20] A. Dubrovina and R. Kimmel. Matching shapes by eigende-
composition of the Laplace-Beltrami operator. Proc. 3DPVT,
2(3), 2010. 2

[21] A. Elad and R. Kimmel. On bending invariant signatures for

surfaces. PAMI, 25(10):1285–1295, 2003. 1, 2

[22] M. Garland and P. S. Heckbert. Surface simpliﬁcation us-
In Proceedings of the 24th an-
ing quadric error metrics.
nual conference on Computer graphics and interactive tech-
niques, pages 209–216. ACM Press/Addison-Wesley Pub-
lishing Co., 1997. 5

[23] T. Groueix, M. Fisher, V. G. Kim, B. Russell, and M. Aubry.
In

3d-coded : 3d correspondences by deep deformation.
ECCV, 2018. 2, 7

[24] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 5

[25] A. E. Johnson and M. Hebert. Using spin images for efﬁcient
object recognition in cluttered 3d scenes. IEEE Transactions
on Pattern Analysis & Machine Intelligence, (5):433–449,
1999. 3

[26] V. G. Kim, Y. Lipman, and T. A. Funkhouser. Blended in-

trinsic maps. Trans. Graphics, 30(4), 2011. 8

[27] R. Kimmel and J. A. Sethian. Computing geodesic paths on
manifolds. Proceedings of the national academy of Sciences,
95(15):8431–8435, 1998. 5

[28] O. Litany, T. Remez, E. Rodol`a, A. M. Bronstein, and M. M.
Bronstein. Deep functional maps: Structured prediction for
dense shape correspondence.
In Proc. ICCV, volume 2,
page 8, 2017. 1, 2, 3, 4, 5, 6, 8

[29] O. Litany, E. Rodol`a, A. M. Bronstein, and M. M. Bronstein.
Fully spectral partial shape matching. Computer Graphics
Forum, 36(2):247–258, 2017. 7, 8

[30] O. Litany, E. Rodol`a, A. M. Bronstein, M. M. Bronstein,
and D. Cremers. Non-rigid puzzles. In Computer Graphics
Forum, volume 35, pages 135–143. Wiley Online Library,
2016. 7

4378

[46] A. Tevs, A. Berner, M. Wand, I. Ihrke, and H.-P. Seidel. In-
trinsic shape matching by planned landmark sampling.
In
Computer Graphics Forum, volume 30, pages 543–552. Wi-
ley Online Library, 2011. 2

[47] F. Tombari, S. Salti, and L. Di Stefano. Unique signatures
of histograms for local surface description. In International
Conference on Computer Vision (ICCV), pages 356–369,
2010. 1, 3

[48] M. Vestner, Z. L¨ahner, A. Boyarski, O. Litany, R. Slossberg,
T. Remez, E. Rodol`a, A. Bronstein, M. Bronstein, R. Kim-
mel, and D. Cremers. Efﬁcient deformable shape correspon-
dence via kernel matching. In Proc. 3DV, 2017. 8

[49] M. Vestner, R. Litman, E. Rodol`a, A. M. Bronstein, and
D. Cremers. Product manifold ﬁlter: Non-rigid shape corre-
spondence via kernel density estimation in the product space.
In CVPR, pages 6681–6690, 2017. 1, 5

[50] U. Von Luxburg, A. Radl, and M. Hein. Hitting and commute
times in large random neighborhood graphs. The Journal of
Machine Learning Research, 15(1):1751–1798, 2014. 1

[51] C. Wang, M. M. Bronstein, A. M. Bronstein, and N. Para-
gios. Discrete minimum distortion correspondence problems
for non-rigid shape matching. In Proc. SSVM, 2011. 2

[31] R. Litman and A. M. Bronstein. Learning spectral de-
scriptors for deformable shape correspondence.
IEEE
transactions on pattern analysis and machine intelligence,
36(1):171–180, 2014. 2, 3

[32] R. Litman and A. M. Bronstein. Learning spectral descrip-
tors for deformable shape correspondence. IEEE Trans. Pat-
tern Anal. Mach. Intell., 36(1):171–180, Jan. 2014. 8

[33] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.
Geodesic convolutional neural networks on riemannian man-
ifolds. In Proceedings of the IEEE international conference
on computer vision workshops, pages 37–45, 2015. 3, 8

[34] F. M`emoli and G. Sapiro. A theoretical and computational
framework for isometry invariant recognition of point cloud
data. Foundations of Computational Mathematics, 5:313–
346, 2005. 2

[35] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and
M. M. Bronstein. Geometric deep learning on graphs and
manifolds using mixture model cnns.
In Computer Vision
and Pattern Recognition (CVPR), 2017 IEEE Conference on,
pages 5425–5434. IEEE, 2017. 2, 3

[36] M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, and
L. Guibas. Functional maps: a ﬂexible representation of
maps between shapes. TOG, 31(4):1–11, 2012. 1, 2, 3, 6,
8

[37] H. Pottmann, J. Wallner, Q.-X. Huang, and Y.-L. Yang. In-
tegral invariants for robust geometry processing. Computer
Aided Geometric Design, 26(1):37–60, 2009. 1

[38] D. Raviv, A. Dubrovina, and R. Kimmel. Hierarchical frame-
work for shape correspondence. Numerical Mathematics:
Theory, Methods and Applications, 6(245-261), 2013. 2

[39] E. Rodol`a, A. M. Bronstein, A. Albarelli, F. Bergamasco, and
A. Torsello. A game-theoretic approach to deformable shape
matching.
In 2012 IEEE Conference on Computer Vision
and Pattern Recognition, pages 182–189. IEEE, 2012. 2

[40] E. Rodol`a, L. Cosmo, M. M. Bronstein, A. Torsello, and
D. Cremers. Partial functional correspondence. In Computer
Graphics Forum, volume 36, pages 222–236. Wiley Online
Library, 2017. 3, 7, 8

[41] E. Rodol`a, S. Rota Bulo, T. Windheuser, M. Vestner, and
D. Cremers. Dense non-rigid shape correspondence using
random forests.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4177–
4184, 2014. 2, 3, 7, 8

[42] R. M. Rustamov. Laplace-beltrami eigenfunctions for de-
formation invariant shape representation. In Proceedings of
the ﬁfth Eurographics symposium on Geometry processing,
pages 225–233. Eurographics Association, 2007. 3

[43] S. Salti, F. Tombari, and L. Di Stefano. Shot: Unique signa-
tures of histograms for surface and texture description. Com-
puter Vision and Image Understanding, 125:251–264, 2014.
5

[44] G. Shamai and R. Kimmel. Geodesic distance descriptors.

In CVPR, pages 3624–3632, 2017. 3

[45] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and prov-
ably informative multi-scale signature based on heat diffu-
sion. In Computer graphics forum, volume 28, pages 1383–
1392. Wiley Online Library, 2009. 1, 3

4379

