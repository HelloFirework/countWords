Relational Knowledge Distillation

Wonpyo Park*

POSTECH, Kakao Corp.

Dongju Kim
POSTECH

Yan Lu

Microsoft Research

Minsu Cho
POSTECH

http://cvlab.postech.ac.kr/research/RKD/

Abstract

Knowledge distillation aims at transferring knowledge
acquired in one model (a teacher) to another model (a stu-
dent) that is typically smaller. Previous approaches can be
expressed as a form of training the student to mimic output
activations of individual data examples represented by the
teacher. We introduce a novel approach, dubbed relational
knowledge distillation (RKD), that transfers mutual rela-
tions of data examples instead. For concrete realizations
of RKD, we propose distance-wise and angle-wise distilla-
tion losses that penalize structural differences in relations.
Experiments conducted on different tasks show that the pro-
posed method improves educated student models with a sig-
niﬁcant margin. In particular for metric learning, it allows
students to outperform their teachers’ performance, achiev-
ing the state of the arts on standard benchmark datasets.

1. Introduction

Recent advances in computer vision and artiﬁcial intel-
ligence have largely been driven by deep neural networks
with many layers, and thus current state-of-the-art models
typically require a high cost of computation and memory
in inference. One promising direction for mitigating this
computational burden is to transfer knowledge in the cum-
bersome model (a teacher) into a small model (a student).
To this end, there exist two main questions: (1) ‘what con-
stitutes the knowledge in a learned model?’ and (2) ‘how
to transfer the knowledge into another model?’. Knowledge
distillation (or transfer) (KD) methods [3, 4, 11] assume the
knowledge as a learned mapping from inputs to outputs, and
transfer the knowledge by training the student model with
the teacher’s outputs (of the last or a hidden layer) as targets.
Recently, KD has turned out to be very effective not only in
training a student model [1, 11, 12, 27, 47] but also in im-
proving a teacher model itself by self-distillation [2, 9, 45].
In this work, we revisit KD from a perspective of the lin-

*The work was done when Wonpyo Park was an intern at MSR.

Figure 1: Relational Knowledge Distillation. While con-
ventional KD transfers individual outputs from a teacher
model (fT ) to a student model (fS) point-wise, our ap-
proach transfers relations of the outputs structure-wise. It
can be viewed as a generalization of conventional KD.

guistic structuralism [19], which focuses on structural rela-
tions in a semiological system. Saussure’s concept of the
relational identity of signs is at the heart of structuralist the-
ory; “In a language, as in every other semiological system,
what distinguishes a sign is what constitutes it” [30]. In this
perspective, the meaning of a sign depends on its relations
with other signs within the system; a sign has no absolute
meaning independent of the context.

The central tenet of our work is that what constitutes the
knowledge is better presented by relations of the learned
representations than individuals of those; an individual data
example, e.g., an image, obtains a meaning in relation to
or in contrast with other data examples in a system of rep-
resentation, and thus primary information lies in the struc-
ture in the data embedding space. On this basis, we intro-
duce a novel approach to KD, dubbed Relational Knowl-
edge Distillation (RKD), that transfers structural relations
of outputs rather than individual outputs themselves (Fig-
ure 1). For its concrete realizations, we propose two RKD
losses: distance-wise (second-order) and angle-wise (third-

13967

𝑓𝑇𝑓𝑆𝑡1𝑠1𝑓𝑇𝑓𝑆𝑡2𝑠2𝑓𝑇𝑓𝑆𝑡3𝑠3InputDNNOutputorder) distillation losses. RKD can be viewed as a gener-
alization of conventional KD, and also be combined with
other methods to boost the performance due to its comple-
mentarity with conventional KD. In experiments on metric
learning, image classiﬁcation, and few-shot learning, our
approach signiﬁcantly improves the performance of student
models. Extensive experiments on the three different tasks
show that knowledge lives in the relations indeed, and RKD
is effective in transferring the knowledge.

2. Related Work

There has been a long line of research and develop-
ment on transferring knowledge from one model to an-
other. Breiman and Shang [3] ﬁrst proposed to learn single-
tree models that approximate the performance of multiple-
tree models and provide better interpretability. Similar ap-
proaches for neural networks have been emerged in the
work of Bucilua et al. [4], Ba and Caruana [1], and Hin-
ton et al. [11], mainly for the purpose of model compres-
sion. Bucilua et al. compress an ensemble of neural net-
works into a single neural network. Ba and Caruana [1]
increase the accuracy of a shallow neural network by train-
ing it to mimic a deep neural network with penalizing the
difference of logits between the two networks. Hinton et
al. [11] revive this idea under the name of KD that trains a
student model with the objective of matching the softmax
distribution of a teacher model. Recently, many subsequent
papers have proposed different approaches to KD. Romero
et al. [27] distill a teacher using additional linear projection
layers to train a relatively narrower students. Instead of im-
itating output activations of the teacher, Zagoruyko and Ko-
modakis [47] and Huang and Wang [12] transfer an atten-
tion map of a teacher network into a student, and Tarvainen
and Valpola [36] introduce a similar approach using mean
weights. Sau et al. [29] propose a noise-based regularizer
for KD while Lopes et al. [17] introduce data-free KD that
utilizes metadata of a teacher model. Xu et al. [43] propose
a conditional adversarial network to learn a loss function
for KD. Crowley et al. [8] compress a model by grouping
convolution channels of the model and training it with an
attention transfer. Polino et al. [25] and Mishra and Marr
[20] combine KD with network quantization, which aims to
reduce bit precision of weights and activations.

A few recent papers [2, 9, 45] have shown that distilling
a teacher model into a student model of identical architec-
ture, i.e., self-distillation, can improve the student over the
teacher. Furlanello et al. [9] and Bagherinezhad et al. [2]
demonstrate it by training the student using softmax out-
puts of the teacher as ground truth over generations. Yim
et al. [45] transfers output activations using Gramian matri-
ces and then ﬁne-tune the student. We also demonstrate that
RKD strongly beneﬁts from self-distillation.

KD has also been investigated beyond supervised learn-

ing. Lopez-Paz et al. [18] unify two frameworks [11, 38]
and extend it to unsupervised, semi-supervised, and multi-
task learning scenarios. Radosavovic et al. [26] generate
multiple predictions from an example by applying multiple
data transformations on it, then use an ensemble of the pre-
dictions as annotations for omni-supervised learning.

With growing interests in KD, task-speciﬁc KD meth-
ods have been proposed for object detection [5, 6, 37], face
model compression [24], and image retrieval and Re-ID [7].
Notably, the work of Chen et al. [7] proposes a KD tech-
nique for metric learning that transfers similarities between
images using a rank loss.
In the sense that they transfer
relational information of ranks, it has some similarity with
ours. Their work, however, is only limited to metric learn-
ing whereas we introduce a general framework for RKD and
demonstrate its applicability to various tasks. Furthermore,
our experiments on metric learning show that the proposed
method outperforms [7] with a signiﬁcant margin.

3. Our Approach

In this section we ﬁrst revisit conventional KD and intro-
duce a general form of RKD. Then, two simple yet effective
distillation losses will be proposed as instances of RKD.

Notation. Given a teacher model T and a student model
S, we let fT and fS be functions of the teacher and the
student, respectively. Typically the models are deep neural
networks and in principle the function f can be deﬁned us-
ing output of any layer of the network (e.g., a hidden or
softmax layer). We denote by X N a set of N -tuples of
distinct data examples, e.g., X 2 = {(xi, xj)|i 6= j} and
X 3 = {(xi, xj, xk)|i 6= j 6= k}.

3.1. Conventional knowledge distillation

In general, conventional KD methods [1, 2, 8, 11, 12, 25,
27, 45, 47] can commonly be expressed as minimizing the
objective function:

LIKD = Xxi∈X

l(cid:0)fT (xi), fS(xi)(cid:1),

(1)

where l is a loss function that penalizes the difference be-
tween the teacher and the student.

For example, the popular work of Hinton et al. [11] uses
pre-softmax outputs for fT and fS, and puts softmax (with
temperature τ ) and Kullback-Leibler divergence for l:

Xxi∈X

KL(cid:16)softmax(cid:0)

fT (xi)

τ

(cid:1), softmax(cid:0)

fS(xi)

τ

(cid:1)(cid:17).

(2)

The work of Romero et al. [27] propagates knowledge of
hidden activations by setting fT and fS to be outputs of
hidden layers, and l to be squared Euclidean distance. As

3968

Figure 2: Individual knowledge distillation (IKD) vs. relational knowledge distillation (RKD). While conventional KD (IKD)
transfers individual outputs of the teacher directly to the student, RKD extracts relational information using a relational
potential function ψ(·), and transfers the information from the teacher to the student.

the hidden layer output of the student usually has a smaller
dimension than that of the teacher, a linear mapping β is
introduced to bridge the different dimensions:

Xxi∈X(cid:13)(cid:13)fT (xi) − β(cid:0)fS(xi)(cid:1)(cid:13)(cid:13)

2
2

.

(3)

Likewise, many other methods [1, 2, 8, 12, 25, 45, 47]
can also be formulated as a form of Eq. (1). Essentially,
conventional KD transfers individual outputs of the teacher
to the student. We thus call this category of KD methods as
Individual KD (IKD).

3.2. Relational knowledge distillation

RKD aims at transferring structural knowledge using
mutual relations of data examples in the teacher’s output
presentation. Unlike conventional approaches, it computes
a relational potential ψ for each n-tuple of data examples
and transfers information through the potential from the
teacher to the student.

For notational simplicity, let us deﬁne ti = fT (xi) and

si = fS(xi). The objective for RKD is expressed as

LRKD = X(x1,..,xn)∈X N

l(cid:0)ψ(t1, .., tn), ψ(s1, .., sn)(cid:1),

(4)

where (x1, x2, ..., xn) is a n-tuple drawn from X N , ψ is a
relational potential function that measures a relational en-
ergy of the given n-tuple, and l is a loss that penalizes dif-
ference between the teacher and the student. RKD trains the
student model to form the same relational structure with that
of the teacher in terms of the relational potential function
used. Thanks to the potential, it is able to transfer knowl-
edge of high-order properties, which is invariant to lower-
order properties, even regardless of difference in output di-
mensions between the teacher and the student. RKD can be
viewed as a generalization of IKD in the sense that Eq. (4)
above reduces to Eq. (1) when the relation is unary (N = 1)
and the potential function ψ is identity. Figure 2 illustrates
comparison between IKD and RKD.

As expected, the relational potential function ψ plays
a crucial role in RKD; the effectiveness and efﬁciency of
RKD relies on the choice of the potential function. For ex-
ample, a higher-order potential may be powerful in captur-
ing a higher-level structure but be more expensive in com-
putation. In this work, we propose two simple yet effec-
tive potential functions and corresponding losses for RKD,
which exploit pairwise and ternary relations of examples,
respectively: distance-wise and angle-wise losses.

3.2.1 Distance-wise distillation loss

Given a pair of training examples, distance-wise potential
function ψD measures the Euclidean distance between the
two examples in the output representation space:

ψD(ti, tj) =

1
µ

kti − tjk2 ,

(5)

where µ is a normalization factor for distance. To focus
on relative distances among other pairs, we set µ to be the
average distance between pairs from X 2 in the mini-batch:

µ =

1

|X 2| X(xi,xj )∈X 2

kti − tjk2 .

(6)

Since distillation attempts to match the distance-wise po-
tentials between the teacher and the student, this mini-batch
distance normalization is useful particularly when there is
a signiﬁcant difference in scales between teacher distances
kti − tjk2 and student distances ksi − sjk2, e.g., due to the
difference in output dimensions.
In our experiments, we
observed that the normalization provides more stable and
faster convergence in training.

Using the distance-wise potentials measured in both the
teacher and the student, a distance-wise distillation loss is
deﬁned as

LRKD-D = X(xi,xj )∈X 2

lδ(cid:0)ψD(ti, tj), ψD(si, sj)(cid:1),

(7)

3969

Individual Knowledge Distillation𝑥1𝑡1transferRelational Knowledge Distillation𝑥2𝑥𝑛𝑠1𝑠2……Relational InformationModel’s RepresentationTeacherStudent𝑥1𝑥2𝑥𝑛𝑠1𝑠2𝑠𝑛………Model’s RepresentationExamples𝑡𝑛𝑡1𝑡2transfer𝑡2…𝑡𝑛𝑠𝑛TeacherStudentExamples𝑟𝑡𝜓(𝑡1,…,𝑡𝑛)𝑟𝑠𝜓(𝑠1,…,𝑠𝑛)where lδ is Huber loss, which is deﬁned as

3.2.4 Distillation target layer

lδ(x, y) =( 1

2 (x − y)2
|x − y| − 1
2 ,

for |x − y| ≤ 1,

otherwise.

(8)

The distance-wise distillation loss transfers the rela-
tionship of examples by penalizing distance differences
between their output representation spaces. Unlike con-
ventional KD, it does not force the student to match the
teacher’s output directly, but encourages the student to fo-
cus on distance structures of the outputs.

3.2.2 Angle-wise distillation loss

Given a triplet of examples, an angle-wise relational poten-
tial measures the angle formed by the three examples in the
output representation space:

ψA(ti, tj, tk) = cos ∠titjtk = he

ij, e

kji

(9)

where e

ij =

ti − tj

kti − tjk2

, e

kj =

tk − tj

ktk − tjk2

.

Using the angle-wise potentials measured in both the
teacher and the student, an angle-wise distillation loss is de-
ﬁned as

LRKD-A = X(xi,xj ,xk)∈X 3

lδ(cid:0)ψA(ti, tj, tk), ψA(si, sj, sk)(cid:1),

(10)

where lδ is the Huber loss. The angle-wise distillation
loss transfers the relationship of training example embed-
dings by penalizing angular differences. Since an angle
is a higher-order property than a distance, it may be able
to transfer relational information more effectively, giving
more ﬂexibility to the student in training.
In our experi-
ments, we observed that the angle-wise loss often allows
for faster convergence and better performance.

3.2.3 Training with RKD

During training, multiple distillation loss functions, includ-
ing the proposed RKD losses, can be used either alone or to-
gether with task-speciﬁc loss functions, e.g., cross-entropy
for classiﬁcation. Therefore, the overall objective has a
form of

Ltask + λKD · LKD,

(11)

where Ltask is a task-speciﬁc loss for the task at hand, LKD
is a knowledge distillation loss, and λKD is a tunable hy-
perparameter to balance the loss terms. When multiple KD
losses are used during training, each loss is weighted with
a corresponding balancing factor. In sampling tuples of ex-
amples for the proposed distillation losses, we simply use
all possible tuples (i.e., pairs or triplets) from examples in a
given mini-batch.

For RKD, the distillation target function f can be chosen as
output of any layer of teacher/student networks in principle.
However, since the distance/angle-wise losses do not trans-
fer individual outputs of the teacher, it is not adequate to
use them alone to where the individual output values them-
selves are crucial, e.g., softmax layer for classiﬁcation. In
that case, it needs to be used together with IKD losses or
task-speciﬁc losses. In most of the other cases, RKD is ap-
plicable and effective in our experience. We demonstrate its
efﬁcacy in the following section.

4. Experiments

We evaluate RKD on three different tasks: metric learn-
ing, classiﬁcation, and few-shot learning. Throughout this
section, we refer to RKD with the distance-wise loss as
RKD-D, that with angle-wise loss as RKD-A, and that with
two losses together as RKD-DA. When the proposed losses
are combined with other losses during training, we assign
respective balancing factors to the loss terms. We com-
pare RKD with other KD methods, e.g., FitNet [27]1, At-
tention [47] and HKD (Hinton’s KD) [11]. For metric
learning, we conduct an additional comparison with Dark-
Rank [7] which is a KD method speciﬁcally designed for
metric learning. For fair comparisons, we tune hyperpa-
rameters of the competing methods using grid search.

Our code used for experiments is available online:

http://cvlab.postech.ac.kr/research/RKD/.

4.1. Metric learning

We ﬁrst evaluate the proposed method on metric learn-
ing where relational knowledge between data examples ap-
pears to be most relevant among other tasks. Metric learn-
ing aims to train an embedding model that projects data ex-
amples onto a manifold where two examples are close to
each other if they are semantically similar and otherwise far
apart. As embedding models are commonly evaluated on
image retrieval, we validate our approach using image re-
trieval benchmarks of CUB-200-2011 [40], Cars 196 [14],
and Stanford Online Products [21] datasets and we follow
the train/test splits suggested in [21]. For the details of the
datasets, we refer the readers to the corresponding papers.

For an evaluation metric, recall@K is used. Once all
test images are embedded using a model, each test image
is used as a query and top K nearest neighbor images are
retrieved from the test set excluding the query. Recall for
the query is considered 1 if the retrieved images contain the
same category with the query. Recall@K are computed by
taking the average recall over the whole test set.

1When FitNet is used, following the original paper, we train the model
with two stages: (1) train the model with FitNet loss, and (2) ﬁne-tune the
model with the task-speciﬁc loss at hand.

3970

Table 1: Recall@1 on CUB-200-2011 and Cars 196. The teacher is based on ResNet50-512. Model-d refers to a network
with d dimensional embedding. ‘O’ indicates models trained with ℓ2 normalization, while ‘X’ represents ones without it.

(a) Results on CUB-200-2011 [40]

Baseline

(Triplet [31])

FitNet [27] Attention [47] DarkRank [7]

ℓ2 normalization
ResNet18-16
ResNet18-32
ResNet18-64
ResNet18-128

ResNet50-512

O

37.71
44.62
51.55
53.92

61.24

O

42.74
48.60
51.92
54.52

O

37.68
45.37
50.81
55.03

O

46.84
53.53
56.30
57.17

(b) Results on Cars 196 [14]

Baseline

(Triplet [31])

FitNet [27] Attention [47] DarkRank [7]

ℓ2 normalization
ResNet18-16
ResNet18-32
ResNet18-64
ResNet18-128

ResNet50-512

O

45.39
56.01
64.53
68.79

77.17

O

57.46
65.81
70.67
73.10

O

46.44
59.40
67.24
71.95

O

64.00
72.41
76.20
77.00

RKD-D
O / X

46.34 / 48.09
52.68 / 55.72
56.92 / 58.27
58.31 / 60.31

Ours

RKD-A
O / X

45.59 / 48.60
53.43 / 55.15
56.77 / 58.44
58.41 / 60.92

RKD-DA
O / X

45.76 / 48.14
53.58 / 54.88
57.01 / 58.68
59.69 / 60.67

RKD-D
O / X

63.23 / 66.02
73.50 / 76.15
78.64 / 80.57
79.72 / 81.70

Ours

RKD-A
O / X

61.39 / 66.25
73.23 / 75.89
77.92 / 80.32
80.54 / 82.27

RKD-DA
O / X

61.78 / 66.04
73.12 / 74.80
78.48 / 80.17
80.00 / 82.50

For training, we follows the protocol of [42]. We ob-
tain training samples by randomly cropping 224 × 224 im-
ages from resized 256 × 256 images and applying random
horizontal ﬂipping for data augmentation. During evalua-
tion, we use a single center crop. All models are trained us-
ing Adam optimizer with batch size of 128 for all datasets.
For effective pairing, we follow batch construction from
FaceNet [31], and sample 5 positive images per category
in a mini-batch.

For a teacher model, ResNet50 [10], which is pre-trained
on ImageNet ILSVRC dataset [28], is used. We take layers
of the network upto avgpool and append a single fully-
connected layer with embedding size of 512 followed by ℓ2
normalization. For a student model, ResNet18 [10], which
is also ImageNet-pretrained, is used in a similar manner
but with different embedding sizes. The teacher models are
trained with the triplet loss [31], which is the most common
and also effective in metric learning.

Triplet [31]. When an anchor xa, positive xp and negative
xn are given, the triplet loss enforces the squared euclidean
distance between anchor and negative to be larger than that
between anchor and positive by margin m:

Ltriplet =hkf (xa) − f (xp)k2

2 − kf (xa) − f (xn)k2

2 + mi+

(12)

is known to stabilize training of the triplet loss by restricting
the range of distances between embedding points to [0, 2].
Note that ℓ2 normalization for embedding is widely used in
deep metric learning [7, 13, 21, 22, 34, 41].

RKD. We apply RKD-D and RKD-A on the ﬁnal embed-
ding outputs of the teacher and the student. Unlike the
triplet loss, the proposed RKD losses are not affected by
range of distance between embedding points, and do not
have sensitive hyperparameters to optimize such as mar-
gin m and triplet sampling parameters. To show the ro-
bustness of RKD, we compare RKD without ℓ2 normal-
ization to RKD with ℓ2 normalization. For RKD-DA, we
set λRKD-D = 1 and λRKD-A = 2. Note that for metric
learning with RKD losses, we do not use the task loss, i.e.,
the triplet loss, so that the model is thus trained purely by
teacher’s guidance without original ground-truth labels; us-
ing the task loss does not give additional gains in our exper-
iments.

Attention [47]. Following the original paper, we apply the
method on the output of the second, the third, and the fourth
blocks of ResNet. We set λTriplet = 1 and λAttention = 50.
FitNet [27]. Following the original paper, we train a model
in two stages; we ﬁrst initialize a model with FitNet loss,
and then ﬁne-tune the model, in our case, with Triplet. We
apply the method on outputs of the second, the third, and
the fourth blocks of ResNet, as well as the ﬁnal embedding.

.

We set the margin m to be 0.2 and use distance-weighted
sampling [42] for triplets. We apply ℓ2 normalization at
the ﬁnal embedding layer such that the embedding vectors
have a unit length, i.e., kf (x)k=1. Using ℓ2 normalization

DarkRank [7] is a KD method for metric learning that
transfers similarity ranks between data examples. Among
two losses proposed in [7], we use the HardRank loss as it is
computationally efﬁcient and also comparable to the other

3971

in performance. The DarkRank loss is applied on ﬁnal out-
puts of the teacher and the student. In training, we use the
same objective with the triplet loss as suggested in the pa-
per. We carefully tune hyperparameters of DarkRank to be
optimal: α = 3, β = 3, and λDarkRank = 1, and λTriplet = 1;
we conduct a grid search on α (1 to 3), β (2 to 4), λDarkRank
(1 to 4). In our experiment, our hyperparameters give better
results than those used in [7].

4.1.1 Distillation to smaller networks

Table 1 shows image retrieval performance of student mod-
els with different embedding dimensions on CUB-200-2011
[38] and Cars 196 [14]. RKD signiﬁcantly improves the
performance of student networks compared to the baseline
model, which is directly trained with Triplet, also outper-
forms DarkRank by a large margin. Recall@1 of Triplet
decreases dramatically with smaller embedding dimensions
while that of RKD is less affected by embedding dimen-
sions; the relative gain of recall@1 by RKD-DA increases
from 12.5, 13.8, 23.0, to 27.7 on CUB-200-2011, and from
20.0, 24.2, 33.5 to 45.5 on Cars 196. The results also show
that RKD beneﬁts from training without ℓ2 normalization
by exploiting a larger embedding space. Note that the ab-
sence of ℓ2 normalization has degraded all the other meth-
ods in our experiments. Surprisingly, by RKD on Cars
196, students with the smaller backbone and less embed-
ding dimension even outperform their teacher, e.g., 77.17 of
ResNet50-512 teacher vs. 82.50 of ResNet18-128 student.

4.1.2 Self-distillation

As we observe that RKD is able to improve smaller student
models over its teacher, we now conduct self-distillation ex-
periments where the student architecture is identical to the
teacher architecture. Here, we do not apply ℓ2 normaliza-
tion on students to beneﬁt from the effect as we observe
in the previous experiment. The students are trained with
RKD-DA over generations by using the student from the
previous generation as a new teacher. Table 2 shows the
result of self-distillation where ‘CUB’, ‘Cars’, and ‘SOP’
refer to CUB-200-2011 [40], Cars 196 [14], and Stanford
Online Products [21], respectively. All models consistently
outperform initial teacher models, which are trained with
the triplet loss. In particular, student models of CUB-200-
2011 and Cars 196 outperform initial teachers with a signif-
icant gain. However, the performances do not improve from
the second generation in our experiments.

4.1.3 Comparison with state-of-the art methods

We compare the result of RKD with state-of-the art meth-
ods for metric learning. Most of recent methods adopt
GoogLeNet [35] as a backbone while the work of [42] uses
a variant of ResNet50 [10] with a modiﬁed number of chan-
nels. For fair comparisons, we train student models on both

Table 2: Recall@1 of self-distilled models. Student and
teacher models have the same architecture. The model at
Gen(n) is guided by the model at Gen(n-1).

CUB [40]

Cars [14]

SOP [21]

ResNet50-512-Triplet
ResNet50-512@Gen1
ResNet50-512@Gen2
ResNet50-512@Gen3

61.24
65.68
65.11
64.26

77.17
85.65
85.61
85.23

76.58
77.61
77.36
76.96

GoogLeNet and ResNet50 and set the embedding size as the
same as other methods. RKD-DA is used for training stu-
dent models. The results are summarized in Table 3 where
our method outperforms all the other methods on CUB-200-
2011 regardless of backbone networks. Among those using
ResNet50, our method achieves the best performance on all
the benchmark datasets. Among those using GoogLeNet,
our method achieves the second-best performance on Car
196 and Stanford Online Products, which is right below
ABE8 [13]. Note that ABE8 [13] requires additional mul-
tiple attention modules for each branches whereas ours is
GoogLeNet with a single embedding layer.

4.1.4 Discussion

RKD performing better without ℓ2 normalization. One
beneﬁt of RKD over Triplet is that the student model is sta-
bly trained without ℓ2 normalization. ℓ2 norm forces out-
put points of an embedding model to lie on the surface of
unit-hypersphere, and thus a student model without ℓ2 norm
is able to fully utilize the embedding space. This allows
RKD to better perform as shown in Table 1. Note that Dark-
Rank contains the triplet loss that is well known to be fragile
without ℓ2 norm. For example, ResNet18-128 trained with
DarkRank achieves recall@1 of 52.92 without ℓ2 norm (vs.
77.00 with ℓ2 norm) on Cars 196.

Students excelling teachers. The similar effect has also
been reported in classiﬁcation [2, 9, 45]. The work of [2, 9]
explains that the soft output of class distribution from
the teacher may carry additional information, e.g., cross-
category relationships, which cannot be encoded in one-
hot vectors of ground-truth labels. Continuous target la-
bels of RKD (e.g., distance or angle) may also carry useful
information, which cannot properly be encoded in binary
(positive/negative) ground-truth labels used in conventional
losses, i.e., the triplet loss.

RKD as a training domain adaptation. Both Cars 196 and
CUB-200-2011 datasets are originally designed for ﬁne-
grained classiﬁcation, which is challenging due to severe
intra-class variations and inter-class similarity. For such
datasets, effective adaptation to speciﬁc characteristics of
the domain may be crucial; recent methods for ﬁne-grained
classiﬁcation focus on localizing discriminative parts of
target-domain objects [23, 44, 48]. To measure the degree

3972

Table 3: Recall@K comparison with state of the arts on CUB-200-2011, Car 196, and Stanford Online Products. We divide
methods into two groups according to backbone networks used. A model-d refers to model with d-dimensional embedding.
Boldfaces represent the best performing model for each backbone while underlines denote the best among all the models.

K
LiftedStruct [21]-128
N-pairs [34]-64
Angular [41]-512
A-BIER [22]-512
ABE8 [13]-512
RKD-DA-128
RKD-DA-512

Margin [42]-128
RKD-DA-128

CUB-200-2011 [40]

Cars 196 [14]

1

47.2
51.0
54.7
57.5
60.6
60.8
61.4

63.6
64.9

2

58.9
63.3
66.3
68.7
71.5
72.1
73.0

74.4
76.7

4

70.2
74.3
76.0
78.3
79.8
81.2
81.9

83.1
85.3

8

80.2
83.2
83.9
86.2
87.4
89.2
89.0

90.0
91.0

1

49.0
71.1
71.4
82.0
85.2
81.7
82.3

79.6
84.9

2

60.3
79.7
81.4
89.0
90.5
88.5
89.8

86.5
91.3

4

72.1
86.5
87.5
93.2
94.0
93.3
94.2

91.9
94.8

8

81.5
91.6
92.1
96.1
96.1
96.3
96.6

95.1
97.2

GoogLeNet [35]

ResNet50 [10]

1

Stanford Online Products [21]
1000
97.4
97.8
98.0
97.8
98.2
98.6
98.7

100
91.3
93.0
93.5
94.0
94.8
95.2
95.2

10
79.8
83.8
85.0
86.9
88.4
88.1
88.3

62.1
67.7
70.9
74.2
76.3
74.5
75.1

72.7
77.5

86.2
90.3

93.8
96.4

98.0
99.0

Figure 3: Recall@1 on the test split of Cars 196, CUB-200-
2011, Stanford Dog and CIFAR-100. Both Triplet (teacher)
and RKD-DA (student) are trained on Cars 196. The left
side of the dashed line shows results on the training domain,
while the right side presents results on other domains.

of adaptation of a model trained with RKD losses, we com-
pare recall@1 on a training data domain with those on dif-
ferent data domains. Figure 3 shows the recall@1 results
on different datasets using a student model trained on Cars
196. The student (RKD) has much lower recall@1 on dif-
ferent domains while the recall@1 of the teacher (Triplet)
remains similarly to pretrained feature (an initial model).
These results reveal an interesting effect of RKD that it
strongly adapts models on the training domain at the cost
of sacriﬁcing generalization to other domains.

4.2. Image classiﬁcation

We also validate the proposed method on the task of im-
age classiﬁcation by comparing RKD with IKD methods,
e.g., HKD [11], FitNet [27] and Attention [47]. We conduct
experiments on CIFAR-100 and Tiny ImageNet datasets.
CIFAR-100 contains 32 × 32 sized images with 100 object
categories, and Tiny ImageNet contains 64 × 64 sized im-
ages with 200 classes. For both datasets, we apply FitNet
and Attention on the output of the second, the third, and
the fourth blocks of CNN, and set λAttention = 50. HKD is
applied on the ﬁnal classiﬁcation layer on the teacher and
the student, and we set temperature τ of HKD to be 4 and

Table 4: Accuracy (%) on CIFAR-100 and Tiny ImageNet.

CIFAR-100 [15]

Tiny ImageNet [46]

Baseline
RKD-D
RKD-DA
HKD [11]

HKD+RKD-DA

FitNet [27]

FitNet+RKD-DA

Attention [47]

Attention+RKD-DA

Teacher

71.26
72.27
72.97
74.26
74.66
70.81
72.98
72.68
73.53

77.76

54.45
54.97
56.36
57.65
58.15
55.59
55.54
55.51
56.55

61.55

λHKD to be 16 as in [11]. RKD-D and RKD-A are applied
on the last pooling layer of the teacher and the student, as
they produce the ﬁnal embedding before classiﬁcation. We
set λRKD-D = 25 and λRKD-A = 50. For all the settings, we
use the cross-entropy loss at the ﬁnal loss in addition. For
both the teacher and the student, we remove fully-connected
layer(s) after the ﬁnal pooling layer and append a single
fully-connected layer as a classiﬁer.

For CIFAR-100, we randomly crop 32 × 32 images from
zero-padded 40 × 40 images, and apply random horizon-
tal ﬂipping for data augmentation. We optimize the model
using SGD with mini-batch size 128, momentum 0.9 and
weight decay 5 × 10−4. We train the network for 200
epochs, and the learning rate starts from 0.1 and is multi-
plied by 0.2 at 60, 120, 160 epochs. We adopt ResNet50 for
a teacher model, and VGG11 [32] with batch normalization
for a student model.

For Tiny ImageNet, we apply random rotation, color jit-
tering, and horizontal ﬂipping for data augmentation. We
optimize the model using SGD with mini-batch 128 and
momentum 0.9. We train the network for 300 epochs, and
the learning rate starts from 0.1, and is multiplied by 0.2 at
60, 120, 160, 200, 250 epochs. We adopt ResNet101 for a
teacher model and ResNet18 as a student model.

Table 4 shows the results on CIFAR-100 and Tiny Im-

3973

Cars 196CUB-200-2011Stanford DogCIFAR-10020406080Recall@1RKD-DATripletPretrainedFigure 4: Retrieval results on CUB-200-2011 and Cars 196 datasets. The top eight images are placed from left to right. Green
and red bounding boxes represent positive and negative images, respectively. T denotes the teacher trained with the triplet
loss while S is the student trained with RKD-DA. For these examples, the student gives better results than the teacher.

ageNet. On both datasets, RKD-DA combined with HKD
outperforms all conﬁgurations. The overall results reveal
that the proposed RKD method is complementary to other
KD methods; the model further improves in most cases
when RKD is combined with another KD method.

4.3. Few shot learning

Finally, we validate the proposed method on the task
of few-shot learning, which aims to learn a classiﬁer that
generalizes to new unseen classes with only a few exam-
ples for each new class. We conduct experiments on stan-
dard benchmarks for few-shot classiﬁcation, which are Om-
niglot [16] and miniImageNet [39]. We evaluate RKD us-
ing the prototypical networks [33] that learn an embedding
network such that classiﬁcation is performed based on dis-
tance from given examples of new classes. We follow the
data augmentation and training procedure of the work of
Snell et al. [33] and the splits suggested by Vinyals et al.
[39]. As the prototypical networks build on shallow net-
works that consist of only 4 convolutional layers, we use
the same architecture for the student model and the teacher,
i.e., self-distillation, rather than using a smaller student net-
work. We apply RKD, FitNet, and Attention on the ﬁnal
embedding output of the teacher and the student. We set
λRKD-D = 50 and λRKD-A = 100. When RKD-D and RKD-
A are combined together, we divide the ﬁnal loss by 2. We
set λAttention = 10. For all the settings, we add the prototyp-
ical loss at the ﬁnal loss. As the common evaluation pro-
tocol of [33] for few-shot classiﬁcation, we compute accu-
racy by averaging over 1000 randomly generated episodes
for Omniglot, and 600 randomly generated episodes for
miniImageNet. The Omniglot results are summarized in Ta-
ble 5 while the miniImageNet results are reported with 95%
conﬁdence intervals in Table 6. They show that our method
consistently improves the student over the teacher.

Table 5: Accuracy (%) on Omniglot [16].

5-Way Acc.

20-Way Acc.

1-Shot
98.58
98.64

5-Shot
99.65
99.64

1-Shot
95.45
95.52

5-Shot
98.72
98.67

RKD-D
RKD-DA

Teacher

98.55

99.56

95.11

98.68

Table 6: Accuracy (%) on miniImageNet [39].

RKD-D
RKD-DA
FitNet
Attention

Teacher

1-Shot 5-Way
49.66 ± 0.84

50.02 ± 0.83

50.38 ± 0.81
34.67 ± 0.65

5-Shot 5-Way
67.07 ± 0.67

68.16 ± 0.67
68.08 ± 0.65

46.21 ± 0.70

49.1 ± 0.82

66.87 ± 0.66

5. Conclusion

We have demonstrated on different tasks and bench-
marks that the proposed RKD effectively transfers knowl-
edge using mutual relations of data examples. In particular
for metric learning, RKD enables smaller students to even
outperform their larger teachers. While the distance-wise
and angle-wise distillation losses used in this work turn out
to be simple yet effective, the RKD framework allows us
to explore a variety of task-speciﬁc RKD losses with high-
order potentials beyond the two instances. We believe that
the RKD framework opens a door to a promising area of
effective knowledge transfer with high-order relations.

Acknowledgement: This work was supported by MSRA
Collaborative Research Program and also by Basic Sci-
ence Research Program and Next-Generation Information
Computing Development Program through the National
Research Foundation of Korea funded by the Ministry
of Science,
ICT (NRF-2017R1E1A1A01077999, NRF-
2017M3C4A7069369).

3974

References

[1] Jimmy Ba and Rich Caruana. Do deep nets really need to
In Advances in Neural Information Processing

be deep?
Systems. 2014. 1, 2, 3

[2] Hessam Bagherinezhad, Maxwell Horton, Mohammad
Rastegari, and Ali Farhadi. Label reﬁnery: Improving ima-
genet classiﬁcation through label progression. arXiv preprint
arXiv:1805.02641, 2018. 1, 2, 3, 6

[3] Leo Breiman and Nong Shang. Born again trees. Univer-
sity of California, Berkeley, Berkeley, CA, Technical Report,
1996. 1, 2

[4] Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-
Mizil. Model compression. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, 2006. 1, 2

[5] W. Cao, J. Yuan, Z. He, Z. Zhang, and Z. He. Fast deep
neural networks with knowledge guided training and pre-
dicted regions of interests for real-time video object detec-
tion. IEEE Access, 2018. 2

[6] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man-
mohan Chandraker. Learning efﬁcient object detection mod-
els with knowledge distillation. In Advances in Neural Infor-
mation Processing Systems. 2017. 2

[7] Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. Dark-
rank: Accelerating deep metric learning via cross sample
similarities transfer. AAAI Conference on Artiﬁcial Intelli-
gence, 2018. 2, 4, 5, 6

[8] Elliot J Crowley, Gavin Gray, and Amos Storkey. Moon-
shine: Distilling with cheap convolutions. Advances in Neu-
ral Information Processing Systems, 2018. 2, 3

[9] Tommaso Furlanello, Zachary Chase Lipton, Michael
Tschannen, Laurent Itti, and Anima Anandkumar. Born-
again neural networks. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, ICML, 2018. 1, 2,
6

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016. 5, 6, 7

[11] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015. 1, 2, 4, 7

[12] Zehao Huang and Naiyan Wang. Like what you like: Knowl-
edge distill via neuron selectivity transfer. arXiv preprint
arXiv:1707.01219, 2017. 1, 2, 3

[13] Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee,
and Keunjoo Kwon. Attention-based ensemble for deep met-
ric learning. In The European Conference on Computer Vi-
sion (ECCV), 2018. 5, 6, 7

[14] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for ﬁne-grained categorization. In
4th International IEEE Workshop on 3D Representation and
Recognition (3dRR-13), 2013. 4, 5, 6, 7

[15] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Master’s thesis, Department of
Computer Science, University of Toronto, 2009. 7

[16] Salakhutdinov Ruslan Lake, Brenden M and Joshua B
Tenenbaum. Human-level concept learning through prob-
abilistic program induction. Science, 350(6266), 2015. 8

[17] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner.
Data-free knowledge distillation for deep neural networks.
CoRR, abs/1710.07535, 2017. 2

[18] D. Lopez-Paz, B. Sch¨olkopf, L. Bottou, and V. Vapnik. Uni-
fying distillation and privileged information. In International
Conference on Learning Representations, 2016. 2

[19] Peter Hugoe Matthews and Peter Matthews. A short history
of structural linguistics. Cambridge University Press, 2001.
1

[20] Asit Mishra and Debbie Marr. Apprentice: Using knowledge
distillation techniques to improve low-precision network ac-
curacy. In International Conference on Learning Represen-
tations, 2018. 2

[21] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured feature
embedding.
In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016. 4, 5, 6, 7

[22] M. Opitz, G. Waltner, H. Possegger, and H. Bischof. Deep
metric learning with bier: Boosting independent embeddings
robustly.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 2018. 5, 7

[23] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part
attention model for ﬁne-grained image classiﬁcation. IEEE
Transactions on Image Processing, 2018. 6

[24] Ziwei Liu Xiaogang Wang Ping Luo, Zhenyao Zhu and Xi-
aoou Tang. Face model compression by distilling knowledge
from neurons. In AAAI Conference on Artiﬁcial Intelligence,
2016. 2

[25] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model
In Interna-

compression via distillation and quantization.
tional Conference on Learning Representations, 2018. 2, 3

[26] Ilija Radosavovic, Piotr Dollr, Ross Girshick, Georgia
Gkioxari, and Kaiming He. Data distillation: Towards omni-
supervised learning. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2

[27] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. International Conference on Learn-
ing Representations, 2015. 1, 2, 4, 5, 7

[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge.
International Journal of Computer Vision (IJCV),
2015. 5

[29] Bharat Bhusan Sau and Vineeth N Balasubramanian. Deep
model compression: Distilling knowledge from noisy teach-
ers. arXiv preprint arXiv:1610.09650, 2016. 2

[30] Ferdinand de Saussure. Course in general linguistics. 1916.

Trans. Roy Harris. London: Duckworth, 1983. 1

[31] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2015. 5

3975

Conference on Learning Representations, 2017. 1, 2, 3, 4, 5,
7

[48] Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao
Lin, and Qi Tian. Picking deep ﬁlter responses for ﬁne-
grained image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2016.
6

[32] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 7

[33] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-
cal networks for few-shot learning. In Advances in Neural
Information Processing Systems, 2017. 8

[34] Kihyuk Sohn.

Improved deep metric learning with multi-
class n-pair loss objective. In Advances in Neural Informa-
tion Processing Systems. 2016. 5, 7

[35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015. 6, 7

[36] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In Advances in Neural
Information Processing Systems. 2017. 2

[37] Jasper Uijlings, Stefan Popov, and Vittorio Ferrari. Revis-
iting knowledge transfer for training object class detectors.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 2

[38] Vladimir Vapnik and Rauf Izmailov. Learning using privi-
leged information: Similarity control and knowledge trans-
fer. J. Mach. Learn. Res., 2015. 2, 6

[39] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wier-
stra, et al. Matching networks for one shot learning. In Ad-
vances in Neural Information Processing Systems, 2016. 8

[40] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011. 4, 5, 6, 7

[41] Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing
Lin. Deep metric learning with angular loss. In The IEEE
International Conference on Computer Vision (ICCV), 2017.
5, 7

[42] Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and
Philipp Krahenbuhl. Sampling matters in deep embedding
learning.
In The IEEE International Conference on Com-
puter Vision (ICCV), 2017. 5, 6, 7

[43] Zheng Xu, Yen-Chang Hsu, and Jiawei Huang. Training
shallow and thin networks for acceleration via knowledge
distillation with conditional adversarial networks, 2018. 2

[44] Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao,
and Liwei Wang. Learning to navigate for ﬁne-grained clas-
siﬁcation. In The European Conference on Computer Vision
(ECCV), 2018. 6

[45] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.
1, 2, 3, 6

[46] Tiny imagenet visual recognition challenge.

//tiny-imagenet.herokuapp.com/.
2018-11-01]. 7

https:
[Accessed;

[47] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer.
International

3976

