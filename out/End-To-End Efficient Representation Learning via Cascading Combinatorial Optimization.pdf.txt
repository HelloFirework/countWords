End-to-End Efﬁcient Representation Learning

via Cascading Combinatorial Optimization

Department of Computer Science and Engineering, Seoul National University, Seoul, Korea

Yeonwoo Jeong, Yoonsung Kim, Hyun Oh Song

{yeonwoo, yskim227, hyunoh}@mllab.snu.ac.kr

Abstract

We develop hierarchically quantized efﬁcient embedding
representations for similarity-based search and show that
this representation provides not only the state of the art per-
formance on the search accuracy but also provides several
orders of speed up during inference. The idea is to hierar-
chically quantize the representation so that the quantization
granularity is greatly increased while maintaining the accu-
racy and keeping the computational complexity low. We also
show that the problem of ﬁnding the optimal sparse com-
pound hash code respecting the hierarchical structure can
be optimized in polynomial time via minimum cost ﬂow in an
equivalent ﬂow network. This allows us to train the method
end-to-end in a mini-batch stochastic gradient descent set-
ting. Our experiments on Cifar100 and ImageNet datasets
show the state of the art search accuracy while providing sev-
eral orders of magnitude search speedup respectively over
exhaustive linear search over the dataset.

1. Introduction

Learning the feature embedding representation that pre-
serves the notion of similarities among the data is of great
practical importance in machine learning and vision and
is at the basis of modern similarity-based search [21, 23],
veriﬁcation [26], clustering [2], retrieval [25, 24], zero-shot
learning [31, 5], and other related tasks. In this regard, deep
metric learning methods [2, 21, 23] have shown advances
in various embedding tasks by training deep convolutional
neural networks end-to-end encouraging similar pairs of data
to be close to each other and dissimilar pairs to be farther
apart in the embedding space.

Despite the progress in improving the embedding repre-
sentation accuracy, improving the inference efﬁciency and
scalability of the representation in an end-to-end optimiza-
tion framework is relatively less studied. Practitioners de-
ploying the method on large-scale applications often resort
to employing post-processing techniques such as embedding
thresholding [1, 32] and vector quantization [27] at the cost
of the loss in the representation accuracy. Recently, Jeong

& Song [11] proposed an end-to-end learning algorithm for
quantizable representations which jointly optimizes the qual-
ity of the convolutional neural network based embedding
representation and the performance of the corresponding
sparsity constrained compound binary hash code and showed
signiﬁcant retrieval speedup on ImageNet [20] without com-
promising the accuracy.

In this work, we seek to learn hierarchically quantizable
representations and propose a novel end-to-end learning
method signiﬁcantly increasing the quantization granularity
while keeping the time and space complexity manageable
so the method can still be efﬁciently trained in a mini-batch
stochastic gradient descent setting. Besides the efﬁciency
issues, however, naively increasing the quantization granu-
larity could cause severe degradation in the search accuracy
or lead to dead buckets hindering the search speedup.

To this end, our method jointly optimizes both the sparse
compound hash code and the corresponding embedding rep-
resentation respecting a hierarchical structure. We alternate
between performing cascading optimization of the optimal
sparse compound hash code per each level in the hierarchy
and updating the neural network to adjust the corresponding
embedding representations at the active bits of the compound
hash code.

Our proposed learning method outperforms both the re-
ported results in [11] and the state of the art deep metric
learning methods [21, 23] in retrieval and clustering tasks on
Cifar-100 [13] and ImageNet [20] datasets while, to the best
of our knowledge, providing the highest reported inference
speedup on each dataset over exhaustive linear search.

2. Related works

Embedding representation learning with neural networks
has its roots in Siamese networks [4, 9] where it was trained
end-to-end to pull similar examples close to each other and
push dissimilar examples at least some margin away from
each other in the embedding space. [4] demonstrated the idea
could be used for signature veriﬁcation tasks. The line of
work since then has been explored in wide variety of practical
applications such as face recognition [26], domain adaptation

111379

[22], zero-shot learning [31, 5], video representation learning
[28], and similarity-based interior design [2], etc.

Another line of research focuses on learning binary ham-
ming ranking [29, 33, 19, 14] representations via neural
networks. Although comparing binary hamming codes is
more efﬁcient than comparing continuous embedding repre-
sentations, this still requires the linear search over the entire
dataset which is not likely to be as efﬁcient for large scale
problems. [7, 16] seek to vector quantize the dataset and
back propagate the metric loss, however, it requires repeat-
edly running k-means clustering on the entire dataset during
training with prohibitive computational complexity.

We seek to jointly learn the hierarchically quantizable
embedding representation and the corresponding sparsity
constrained binary hash code in an efﬁcient mini-batch based
end-to-end learning framework. Jeong & Song [11] moti-
vated maintaining the hard constraint on the sparsity of hash
code to provide guaranteed retrieval inference speedup by
only considering ks out of d buckets and thus avoiding linear
search over the dataset. We also explicitly maintain this con-
straint, but at the same time, greatly increasing the number of
representable buckets by imposing an efﬁcient hierarchical
structure on the hash code to unlock signiﬁcant improvement
in the speedup factor.

3. Problem formulation

Consider the following hash function

r(x) = argmin
h∈{0,1}d

−f (x; θ)⊺h

under the constraint that khk1 = ks. The idea is to optimize
the weights in the neural network f (·; θ) : X → Rd, take
ks highest activation dimensions, activate the corresponding
dimensions in the binary compound hash code h, and hash
the data x ∈ X into the corresponding active buckets of a
hash table H. During inference, a query xq is given, and
all the hashed items in the ks active bits set by the hash
function r(xq) are retrieved as the candidate nearest items.
Often times [27], these candidates are reranked based on
the euclidean distance in the base embedding representation
f (·; θ) space.

i

is Pi6=q Pr(h⊺

Given a query hq, the expected number of retrieved items
hq 6= 0). Then, the expected speedup factor
[11] (SUF) is the ratio between the total number of items
and the expected number of retrieved items. Concretely, it
becomes (Pr(h⊺
i
case d ≫ ks, this ratio approaches d/ks

hq 6= 0))−1 = (1 − (cid:0)d−ks

ks(cid:1))−1. In

ks (cid:1)/(cid:0) d

2.

Now, suppose we design a hash function r(x) so that
the function has total dim(r(x)) = dk (i.e. exponential in
some integer parameter k > 1) indexable buckets. The ex-
pected speedup factor [11] approaches dk/k2
s which means
the query time speedup increases linearly with the number
of buckets. However, naively increasing the bucket size

for higher speedup has several major downsides. First, the
hashing network has to output and hold dk activations in the
memory at the ﬁnal layer which can be unpractical in terms
of the space efﬁciency for large scale applications. Also, this
could also lead to dead buckets which are under-utilized and
degrade the search speedup. On the other hand, hashing the
items uniformly at random among the buckets could help
to alleviate the dead buckets but this could lead to a severe
drop in the search accuracy.

Our approach to this problem of maintaining a large num-
ber of representable buckets while preserving the accuracy
and keeping the computational complexity manageable is to
enforce a hierarchy among the optimal hash codes in an efﬁ-
cient tree structure. First, we use dim(f (x)) = dk number
of activations instead of dk activations in the last layer of the
hash network. Then, we deﬁne the unique mapping between
the dk activations to dk buckets by the following procedure.

Denote the hash code as eh = [h1, . . . , hk] ∈ {0, 1}d×k

where khvk1 = 1 ∀v 6= k and khkk1 = ks. The superscript
denotes the level index in the hierarchy. Now, suppose we
construct a tree T with branching factor d, depth k where
the root node has the level index of 0. Let each dk leaf
node in T represent a bucket indexed by the hash function
r(x). Then, we can interpret each hv vector to indicate the
branching from depth v − 1 to depth v in T . Note, from

the construction of eh, the branching is unique until level

k − 1, but the last branching to the leaf nodes is multi-way
because ks bits are set due to the sparsity constraint at level
k. Figure 1 illustrates an example translation from the given
hash activation to the tree bucket index for k = 2 and ks = 2.
Concretely, the hash function r(x) : Rd×k → {0, 1}dk
can
be expressed compactly as Equation (1).

k

Ov=1

r(x) =

argmin

hv

− (f (x; θ)v)⊺ hv

(1)

subject to khvk1 =(1

ks

∀v 6= k

v = k

and hv ∈ {0, 1}d

two vectors. The following section discusses how to ﬁnd

whereN denotes the tensor multiplication operator between
the optimal hash code eh and the corresponding activation

f (x; θ) = [f (x; θ)1, . . . , f (x; θ)k] ∈ Rd×k respecting the
hierarchical structure of the code.

4. Methods

To compute the optimal set of embedding representations
and the corresponding hash code, the embedding representa-
tions are ﬁrst required in order to infer which ks activations
to set in the hash code, but to learn the embedding repre-
sentations, it requires the hash code to determine which
dimensions of the activations to adjust so that similar items
would get hashed to the same buckets and vice versa. We
take the alternating minimization approach iterating over
computing the sparse hash codes respecting the hierarchical
quantization structure and updating the network parameters

211380

elements as possible for a pair of hash codes from different
classes in the current level v. The last term also makes sure,
in the event that the second term becomes zero, the hash code
still respects orthogonality among dissimilar items. This can
occur when the hash code for all the previous levels was
computed perfectly splitting dissimilar pairs into different
branches and the second term becomes zero.

minimize
h1 ,...,hn

n

X

−(f (xi; θ)v)⊺ hi

+ X

i Q′ hj
h⊺

+ X

i P ′ hj
h⊺

|

}

}

|

i=1

{z

{z

unary term

orthogonality

(i,j)∈N

sibling penalty

i = hw

(i,j)∈Sv
{z
}
|
(3)
j , ∀w = 1, . . . , v − 1(cid:9)
where S v = (cid:8)(i, j) ∈ N | hw

denotes the set of pairs of siblings at level v in T , and
Q′, P ′ encodes the pairwise cost for the sibling and the
orthogonality terms respectively. However, optimizing Equa-
tion (3) is NP-hard in general even in the simpler case of
ks = 1, k = 1, d > 2 [3, 11]. Inspired by [11], we use
the average embedding of each class within the minibatch
p = 1
cv

m Pi:yi=p f (xi; θ)v ∈ Rd as shown in Equation (4).

zp

⊺Qzq +Xp6=q

zp

⊺P zq

minimize
z1,...,znc

−(cv

nc

Xp=1
|

p)⊺zp + X(p,q)∈Sv
{z

∀v 6= k

p6=q

z

v = k

:=ˆg(z1,...,znc )

, zp ∈ {0, 1}d, ∀p,

subject to kzpk =(1
z = (cid:8)(p, q) | zw

ks

p = zw

q , ∀w = 1, . . . , v − 1(cid:9), nc

where S v
is the number of unique classes in the minibatch, and
we assume each class has m examples in the minibatch
in
(i.e.
accordance with the deep metric learning problem setting
[21, 23, 11], we assume we are given access to the label
adjacency information only within the minibatch.

npairs [23] minibatch construction). Note,

}

(4)

The objective in Equation (4) upperbounds the objective
in Equation (3) (denote as g(·; θ)) by a gap M (θ) which
depends only on θ. Concretely, rewriting the summation in
the unary term in g, we get

nc

h⊺

Xp Xi:yi=p
+ X(i,j)∈Sv
p)⊺ hi + X(i,j)∈Sv
Xp Xi:yi=p
{z

:=M (θ)

(cv

nc

≤

nc

Xp Xi:yi=p

−(cv

+ maximize

ˆh1,...,ˆhn

|

(5)

i P ′hj
h⊺

i Q′hj + X(i,j)∈N

h⊺

i Q′hj + X(i,j)∈N

i P ′hj
h⊺

p − f (xi; θ)v)⊺ ˆhi

.

Minimizing the upperbound in Equation (5) over h1, . . . , hn
is identical to minimizing the objective ˆg(z1, . . . , znc ) in

}

311381

Figure 1: Example hierarchical structure for k = 2 and
ks = 2. (Left) The hash code for each embedding represen-
tation [f (xi; θ)1, f (xi; θ)2] ∈ R2d. (Right) Corresponding
activated hash buckets out of total d2 buckets.

indexed at the given hash codes per each mini-batch. Sec-
tion 4.1 and Section 4.3 formalize the subproblems in detail.

4.1. Learning the hierarchical hash code

Given a set of continuous embedding representation
{f (xi; θ)}n
i=1, we wish to compute the optimal binary hash
code {h1, . . . , hn} so as to hash similar items to the same
buckets and dissimilar items to different buckets. Further-
more, we seek to constrain the hash code to simultaneously
maintain the hierarchical structure and the hard sparsity con-
ditions throughout the optimization process. Suppose items
xi and xj are dissimilar items, in order to hash the two items
to different buckets, at each level of T , we seek to encourage
the hash code for each item at level v, hv
j to differ.
To achieve this, we optimize the hash code for all items per
each level sequentially in cascading fashion starting from
1, . . . , h1
the ﬁrst level {h1
1, . . . , hk
n}
as shown in Equation (2).

n} to the leaf nodes {hk

i and hv

−(f (xi; θ)v)⊺ hv
i

(2)

n

Xi=1

k

Xv=1
|

hv
i

minimize
1:n,...,h1
hk
1:n

+

k

Xv=2 X(i,j)∈N
|

subject to khv

unary term

v−1

{z
Yw=1

sibling penalty

{z
i k =(1

ks

∀v 6= k

v = k

⊺Q′hv
j

1(hw

i = hw
j )

+

hv
i

⊺P ′hv
j

}

}

k

Xv=1 X(i,j)∈N
|
{z

orthogonality

}

where N denotes the set of dissimilar pairs of data and 1(·)
denotes the indicator function. Concretely, given the hash
codes from all the previous levels, we seek to minimize the
following discrete optimization problem in Equation (3),
subject to the same constraints as in Equation (2), sequen-
tially for all levels1 v ∈ {1, . . . , k}. The unary term in the
objective encourages selecting as large elements of each em-
bedding vector as possible while the second term loops over
all pairs of dissimilar siblings and penalizes for their orthog-
onality. The last term encourages selecting as orthogonal

1In Equation (3), we omit the dependence of v for all h1, . . . , hn to

avoid the notation clutter.

, hv

i ∈ {0, 1}d, ∀i,

g(h1, . . . , hn; θ) =

−(f (xi; θ)v)⊺ hi

0𝑎ℎ1ℎ2𝑏𝑐𝑑−100⋮⋮10⋮⋮01⋮⋮01⋮⋮00ℎ0ℎ1[0]⋯⋯⋯⋯⋯𝑟(𝑥)0⋯⋯0⋯1⋯1⋯0⋯0⋯0𝑎𝑑𝑎𝑑+𝑏𝑎𝑑+𝑐𝑎𝑑+𝑑−1𝑑2−1ℎ1[𝑎]ℎ1[𝑑−1]ℎ2[0]ℎ2[𝑏]ℎ2[𝑐]ℎ2[𝑑−1]⋯⋯Equation (4) since each example j in class i shares the same
class mean embedding vector ci. Absorbing the factor m
into the cost matrices i.e. Q = mQ′ and P = mP ′, we
arrive at the upperbound minimization problem deﬁned in
Equation (4). In the upperbound problem Equation (4), we
consider the case where the pairwise cost matrices are di-
agonal matrices of non-negative values. Theorem 1 in the
following subsection proves that ﬁnding the optimal solution
of the upperbound problem in Equation (4) is equivalent to
ﬁnding the minimum cost ﬂow solution of the ﬂow network
G′ illustrated in Figure 2. Section B in the supplementary
material shows the running time to compute the minimum
cost ﬂow (MCF) solution is approximately linear in nc and
d. On average, it takes 24 ms and 53 ms to compute the MCF
solution (discrete update) and to take a gradient descent step
with npairs embedding [23] (network update), respectively
on a machine with 1 TITAN-XP GPU and Xeon E5-2650.

4.2. Equivalence of the optimization problem to

minimum cost ﬂow

Theorem 1. The optimization problem in Equation (4) can
be solved exactly in polynomial time by ﬁnding the minimum
cost ﬂow solution on the ﬂow network G’.

Proof. Suppose we construct a vertex set A =
{a1, . . . , anc} and partition A into {Ar}l
r=0 with the par-
tition of {1, . . . , nc} from equivalence relation S v
2. Here,
z
we will deﬁne A0 as a union of subsets of size 1 (i.e. each el-
ement in A0 is a singleton without a sibling), and A1, . . . , Al
as the rest of the subsets (of size greater than or equal to2).

Concretely, |A| = nc and A = Sl

r=0 Ar.

Then, we construct l + 1 set of complete bipartite graphs
{Gr = (Ar ∪ Br, Er)}l
r=0 where we deﬁne gr = |Ar| and
|Br| = d ∀r. Now suppose we construct a directed graph G′
by directing all edges Er from Ar to Br, attaching source
s to all vertices in Ar, and attaching sink t to all vertices in

B0. Formally, G′ = (cid:16)Sl

r=0 (Ar ∪ Br) ∪ {s, t}, E′(cid:17). The

edges in E′ inherit all directed edges from source to vertices
in Ar, edges from vertices in B0 to sink, and {Er}l
r=0. We
also attach gr number of edges for each vertex br,q ∈ Br to
b0,q ∈ B0 and attach nc number of edges from each vertex
b0,q ∈ B0 to t. Concretely, E′ is

{(s, ap)|ap ∈ A} ∪

l

[

r=0

Er ∪

l

[

r=1

{(br,q , b0,q)i}gr −1

i=0 ∪ {(b0,q , t)j }nc−1
j=0 .

Edges incident to s have capacity u(s, ap) = ks and
cost v(s, ap) = 0 for all ap ∈ A. The edges between
ap ∈ Ar and br,q ∈ Br have capacity u(ap, br,q) = 1 and
cost v(ap, br,q) = −cp[q]. Each edge i ∈ {0, . . . , gr − 1}
between br,q ∈ Br and b0,q ∈ B0 has capacity

u(cid:0)(br,q, b0,q)i(cid:1) = 1 and cost u(cid:0)(br,q, b0,q)i(cid:1) = 2αi.
capacity u(cid:16)(b0,q, t)j(cid:17) = 1 and cost v(cid:16)(b0,q, t)j(cid:17) = 2βj.

Each edge j ∈ {0, . . . , nc − 1} between b0,q ∈ B0 and t has

2Deﬁne (p, q) ∈ S v

z ⇐⇒ ap, aq ∈ Ar, ∀r ≥ 1

Figure 2 illustrates the ﬂow network G′. The amount of ﬂow
from source to sink is ncks. The ﬁgure omits the vertices in
A0 and the corresponding edges to B0 to avoid the clutter.

Now we deﬁne the ﬂow {fz(e)}e∈E′ for each edge in-
dexed both by ﬂow conﬁguration zp ∈ z1:nc where zp ∈
{0, 1}d, kzpk1 = ks ∀p and e ∈ E′ below in Equation (6).

(i) fz(s, ap) = ks, (ii) fz(ap, br,q) = zp[q]

(iii) fz(cid:0)(br,q, b0,q)i(cid:1) =(1
(iv) fz(cid:16)(b0,q, t)j(cid:17) =(1

0 otherwise

∀i <Pp:ap∈Ar
∀j <Pnc

zp[q]

p=1

0 otherwise

zp[q]

(6)

To prove the equivalence of computing the minimum cost
ﬂow solution and ﬁnding the minimum binary assignment
in Equation (4), we need to show (1) that the ﬂow deﬁned
in Equation (6) is feasible in G′ and (2) that the minimum
cost ﬂow solution of the network G′ and translating the
computed ﬂows to {zp} in Equation (4) indeed minimizes
the discrete optimization problem. We ﬁrst proceed with the
ﬂow feasibility proof.

It is easy to see the capacity constraints are satisﬁed by
construction in Equation (6) so we prove that the ﬂow conser-
vation conditions are met at each vertices. First, the output
p=1 ks = ncks
is equal to the input ﬂow. For each vertex ap ∈ A, the
amount of input ﬂow is ks and the output ﬂow is the same

ﬂow from the source Pap∈A fz(s, ap) = Pnc
fz(ap, br,q) = Pd
Pbr,q∈Br
ﬂow as yr,q = Pap∈Ar
Pp:ap∈Ar

For r > 0, for each vertex br,q ∈ Br, denote the input
zp[q].
=
i=0 fz((br,q, b0,q)i)
The second term vanishes

fz(ap, br,q) = Pp:ap∈Ar

ﬂow is Pgr−1

zp[q] = kzk1 = ks.

zp[q] = yr,q.

output

The

q=1

because of Equation (6) (iii).

The last ﬂow conservation condition is to check
the connections from each vertex b0,q ∈ B0 to the
sink. Denote the input ﬂow at the vertex as y0,q =
zp[q]. The output
zp[q] = y0,q which is
identical to the input ﬂow. Therefore, the ﬂow construction
in Equation (6) is feasible in G′.

zp[q] + Pl
r=1 yr,q = Pnc
j=0 fz((b0,q, t)j) = Pnc

Pp:ap∈A0
ﬂow isPnc−1

p=1

p=1

The second part of the proof is to check the optimal-
ity conditions and show the minimum cost ﬂow ﬁnds
the minimizer of Equation (4). Denote, {fo(e)}e∈E′
as the minimum cost ﬂow solution of the network G′

denote the optimal ﬂow from ap ∈ Ar to br,q ∈
Br, fo(ap, bq) as z′
By optimality of the ﬂow,

which minimizes the total cost Pe∈E′ v(e)fo(e). Also
{fo(e)}e∈E′ , Pe∈E′ v(e)fo(e) ≤ Pe∈E′ v(e)fz(e) ∀z.
Pnc
p=1 −cp

the lhs of the inequality is equal
T z′

r=1Pp16=p2∈{p|ap∈Ar} αz′

p + Pl

By Lemma 1,

to
p2 +

p[q].

T z′

p1

411382

Ar−1

Ar

...

...

Input flow

ncks

s

ks, 0

ap

...

...

Ar+1

Br−1

Br

1, −cp[0]

1, −cp[q]

1, −cp[d]

Br+1

...

br,1
...
br,q
...
br,d

...

...

...

1, 2(gr − 1)α

...
1, 0
...

...

B0

b0,1
...
b0,q
...
b0,d

1, 2(nc − 1)β

...
...
...
1, 0
...
...

unary term

sibling penalty

orthogonality

Output flow

t

ncks

Figure 2: Equivalent ﬂow network diagram G′ corresponding to the discrete optimization Equation (4). Edge labels show the
capacity and the cost respectively.

p1

p2 .

T z′

βz′

Pp16=p2
the rhs of the inequality is equal to Pnc
T zp2 + Pp16=p2
Pl
r=1Pp16=p2∈{p|ap∈Ar} αzp1

Additionally, Lemma 2 shows
T zp +
T zp2 .

p=1 −cp
βzp1

Finally, ∀{z}

nc

X

p=1

−cp

T z′

p +

l

X

r=1

X

p16=p2∈{p|ap∈Ar }

αz′

p1

T

z′

p2 + X
p16=p2

βz′

p1

T

z′

p2

≤

nc

X

p=1

−cp

T zp +

l

X

r=1

X

p16=p2∈{p|ap∈Ar }

αzp1

T zp2 + X
p16=p2

βzp1

T zp2 .

This shows computing the minimum cost ﬂow solution on
G′ and converting the ﬂows to z’s, we can ﬁnd the minimizer
of the objective in Equation (4).

Lemma 1. Given the minimum cost ﬂow {fo(e)}e∈E′ of the

network G′, the total cost of the ﬂow is Pe∈E′ v(e)fo(e) =
Pnc
p=1 −cp
Pp16=p2
βz′

r=1Pp16=p2∈{p|ap∈Ar} αz′

p + Pl

T z′

T z′

T z′

p2 .

p2 +

p1

p1

Proof. Proof in section A.2 of the supplementary material.

Lemma 2. Given a feasible ﬂow {fz(e)}e∈E′ of the net-

work G′, the total cost of the ﬂow is Pe∈E′ v(e)fz(e) =
Pnc
p=1 −cp
Pp16=p2

r=1Pp16=p2∈{p|ap∈Ar} αzp1

T zp + Pl

T zp2 +

T zp2 .

βzp1

Proof. Proof in section A.2 of the supplementary material.

4.3. Learning the embedding representation given

the hierarchical hash codes

1, . . . , zv

Given a set of binary hash codes for the mean embed-
dings {zv
nc}, ∀v = 1, . . . , k computed from Equa-
tion (4), we can derive the hash codes for all n examples
in the minibatch, hv
p ∀i : yi = p and update the
i
network weights θ given the hierarchical hash codes in
turn. The task is to update the embedding representations,
{f (xi; θ)v}n
i=1, ∀v = 1, . . . , k, so that similar pairs of data

:= zv

have similar embedding representations indexed at the acti-
vated hash code dimensions and vice versa. Note, In terms
of the hash code optimization in Equation (4) and the bound
in Equation (5), this embedding update has the effect of
tightening the bound gap M (θ).

i ∨ hv

k(cid:0)hv

We employ the state of the art deep metric learning
algorithms (denote as ℓmetric(·)) such as triplet loss with
semi-hard negative mining [21] and npairs loss [23] for
this subproblem where the distance between two exam-
ples xi and xj at hierarchy level v is deﬁned as dv
ij =

j(cid:1)⊙(f (xi; θ)v − f (xj; θ)v) k1. Utilizing the log-

ical OR of the two binary masks, in contrast to independently
indexing the representation with respective masks, to index
the embedding representations helps prevent the pairwise
distances frequently becoming zero due to the sparsity of the
code. Note, this formulation in turn accommodates the back-
propagation gradients to ﬂow more easily. In our embedding
representation learning subproblem, we need to learn the rep-
resentations which respect the tree structural constraint on
the corresponding hash code h = [h1, . . . , hk] ∈ {0, 1}d×k
where khvk1 = 1 ∀v 6= k and khkk1 = ks. To this end, we
decompose the problem and compute the embedding loss
per each hierarchy level v separately.

Furthermore, naively using the similarity labels to deﬁne
similar pairs versus dissimilar pairs during the embedding
learning subproblem could create a discrepancy between the
hash code discrete optimization subproblem and the embed-
ding learning subproblem leading to contradicting updates.
Suppose two examples xi and xj are dissimilar and both had
the highest activation at the same dimension o and the hash
code for some level v was identical i.e. hv
j [o] = 1.
Enforcing the metric learning loss with the class labels, in
this case, would lead to increasing the highest activation for
one example and decreasing the highest activation for the
other example. This can be problematic for the example with
decreased activation because it might get hashed to another
occupied bucket after the gradient update and this can repeat

i [o] = hv

511383

causing instability in the optimization process.

However, if we relabel the two examples so that they are
treated as the same class as long as they have the same hash
code at the level, the update wouldn’t decrease the activations
for any example, and the sibling term (the second term) in
Equation (4) would automatically take care of splitting the
two examples in the next subsequent levels.

To this extent, we apply label remapping as follows.
yv
i = remap(hv
i ), where remap(·) assigns arbitrary unique
labels to each unique conﬁguration of hv
i . Concretely,
remap(hv
i = yv
j . Finally, the
embedding representation learning subproblem aims to solve
Equation (7) given the hash codes and the remapped labels.
Section C in the supplementary material includes the abla-
tion study of label remapping.

i ) = remap(hv

j ) ⇐⇒ yv

minimize

θ

k

Xv=1

ℓmetric ({f (xi; θ)v}n

i=1; {hv

i }n

i=1, {yv

i }n

i=1)

(7)

Following the protocol in [11], we use the Tensorﬂow
implementation of deep metric learning algorithms in
tf.contrib.losses.metric_learning.

5. Implementation details

Algorithm 1 Learning algorithm

input θemb
initialize θf = [θb, θd]

b

(pretrained metric learning base model); θd, k

for t = 1, . . . , MAXITER do

Sample a minibatch {xi} and initialize S1
for v = 1, · · · , k do

z = ∅

Update the ﬂow network G′ by computing class cost vectors

p = 1
cv

mPi:yi=p f (xi; θf )v

Compute the hash codes {hv
Update S v+1
z and {hv
i }
Remap the label to compute yv

given S v

z

i } via minimum cost ﬂow on G′

end for
Update the network parameter given the hash codes

θf ← θf − η(t)∂θf

ℓmetric(θf ; hv

1:nc

, yv

1:nc )

k

Xv=1

Update stepsize η(t) ← ADAM rule [12]

end for

output θf (ﬁnal estimate);

Network architecture For fair comparison, we follow
the protocol in [11] and use the NIN [15] architecture (de-
note the parameters θb) with leaky relu [30] with τ = 5.5
as activation function and train Triplet embedding network
with semi-hard negative mining [21], Npairs network [23]
from scratch as the base model, and snapshot the network
weights (θemb
) of the learned base model. Then we replace
the last layer in (θemb
) with a randomly initialized dk di-
mensional fully connected projection layer (θd) and ﬁnetune
the hash network (denote the parameters as θf = [θb, θd]).
Algorithm 1 summarizes the learning procedure.

b

b

b

b

Hash table construction and query We use the learned
hash network θf and apply Equation (1) to convert xi into
the hash code h(xi; θf ) and use the base embedding net-
work θemb
to convert the data into the embedding represen-
tation f (xi; θemb
). Then, the embedding representation is
hashed to buckets corresponding to the ks set bits in the hash
code. During inference, we convert a query data xq into the
hash code h(xq; θf ) and into the embedding representation
f (xq; θemb
). Once we retrieve the union of all bucket items
indexed at the ks set bits in the hash code, we apply a rerank-
ing procedure [27] based on the euclidean distance in the
embedding space.

b

Evaluation metrics Following the evaluation protocol
in [11], we report our accuracy results using precision@k
(Pr@k) and normalized mutual information (NMI) [17] met-
rics. Precision@k is computed based on the reranked order-
ing (described above) of the retrieved items from the hash
table. We evaluate NMI, when the code sparsity is set to
ks = 1, treating each bucket as an individual cluster. We
report the speedup results by comparing the number of re-
trieved items versus the total number of data (exhaustive
linear search) and denote this metric as SUF.

6. Experiments

We report our results on Cifar-100 [13] and ImageNet
[20] datasets and compare against several baseline methods.
First baseline methods are the state of the art deep metric
learning models [21, 23] performing an exhaustive linear
search over the whole dataset given a query data (denote
as ‘Metric’). Next baseline is the Binarization transform
[1, 32] where the dimensions of the hash code corresponding
to the top ks dimensions of the embedding representation
are set (denote as ‘Th’). Then we perform vector quantiza-
tion [27] on the learned embedding representation from the
deep metric learning methods above on the entire dataset
and compute the hash code based on the indices of the ks
nearest centroids (denote as ‘VQ’). Another baseline is the
quantizable representation in [11](denote as [11]). In both
Cfar-100 and ImageNet, we follow the data augmentation
and preprocessing steps in [11] and train the metric learn-
ing base model with the same settings in [11] for fair com-
parison. In Cifar-100 experiment, we set (d, k) = (32, 2)
and (d, k) = (128, 2) for the npairs network and the triplet
network, respectively.
In ImageNet experiment, we set
(d, k) = (512, 2) and (d, k) = (256, 2) for the npairs net-
work and the triplet network, respectively. In ImageNetSplit
experiment, we set (d, k) = (64, 2). We also perform LSH
hashing [10] baseline and Deep Cauchy Hashing [6] baseline
which both generate n-bit binary hash codes with 2n buckets
and compare against other methods when ks = 1 (denote as
‘LSH’ and ‘DCH’, respectively). For the fair comparison,
we set the number of buckets, 2n = dk.

611384

test

train

test

train

Triplet

Npairs

ks

1

2

3

4

Method

Metric

LSH
DCH

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

SUF

1.00

138.83
96.13
41.21
22.78
97.67
97.67

14.82
5.63
76.12
98.38

7.84
2.83
42.12
94.55

4.90
1.91
16.19
92.18

Pr@1

Pr@4

Pr@16

56.78

55.99

53.95

52.52
56.26
54.82
56.74
57.63
58.42

56.55
56.78
57.30
58.39

56.78
56.78
56.97
58.19

56.84
56.77
57.11
58.52

48.67
55.65
52.88
55.94
57.16
57.88

55.62
56.00
56.70
57.51

55.91
55.99
56.25
57.42

56.01
55.99
56.21
57.79

39.71
54.26
48.03
53.77
55.76
56.58

52.90
53.99
55.19
56.09

53.64
53.95
54.40
56.02

53.86
53.94
54.20
56.22

SUF

1.00

135.64
89.60
43.19
40.35
97.77
97.28

15.34
6.94
78.28
97.20

8.04
2.96
44.36
93.69

5.00
1.97
16.52
91.27

Pr@1

Pr@4

Pr@16

62.64

61.91

61.22

60.45
61.06
61.56
62.54
63.85
64.73

62.41
62.66
63.60
64.35

62.66
62.62
62.87
63.60

62.66
62.62
62.81
64.20

58.10
60.80
60.24
61.78
63.40
64.63

61.68
61.92
63.19
63.91

61.88
61.92
62.22
63.35

61.94
61.91
62.14
63.95

54.00
60.81
58.23
60.98
63.39
64.69

60.89
61.26
63.09
63.81

61.16
61.22
61.84
63.32

61.24
61.22
61.58
63.63

SUF

1.00

29.74
41.59
12.72
34.86
54.85
101.1

5.09
6.08
16.20
69.48

3.10
2.66
7.25
57.09

2.25
1.66
4.51
49.43

Pr@1

Pr@4

Pr@16

57.05

55.70

53.91

53.55
57.23
54.95
56.76
58.19
58.28

56.52
57.13
57.27
57.60

56.97
57.01
57.15
57.56

57.02
57.03
57.15
57.75

50.75
56.25
52.60
55.35
57.22
57.79

55.28
55.74
55.98
56.98

55.56
55.69
55.81
56.70

55.64
55.70
55.77
56.79

43.03
54.45
47.16
53.75
55.87
56.92

53.04
53.90
54.42
55.82

53.76
53.90
54.10
55.41

53.88
53.91
54.01
55.50

SUF

1.00

30.75
40.49
13.65
31.35
54.90
97.47

5.36
5.44
16.51
69.91

3.21
2.36
7.32
58.62

2.30
1.55
4.52
50.80

Pr@1

Pr@4

Pr@16

61.78

60.63

59.73

59.87
61.59
60.80
61.22
63.11
63.06

61.65
61.82
61.98
62.19

61.75
61.78
61.90
62.30

61.78
61.78
61.81
62.43

58.34
60.77
59.49
60.24
62.29
62.62

60.50
60.56
60.93
61.71

60.66
60.62
60.80
61.44

60.66
60.62
60.69
61.65

55.35
60.12
57.27
59.34
61.94
62.44

59.50
59.70
60.15
61.27

59.73
59.73
59.96
60.91

59.75
59.73
59.77
61.01

Table 1: Results with Triplet network with hard negative mining and Npairs network. Querying test data against a hash table
built on test set and a hash table built on train set on Cifar-100.

Triplet

Npairs

Triplet

Npairs

Method

SUF

Pr@1 Pr@4 Pr@16

SUF

Pr@1 Pr@4 Pr@16

ks Metric

1.00

10.90

9.39

1

2

3

8.86
LSH
164.25
9.82
DCH 140.77
18.81
10.20
146.26 10.37
221.49 11.00
590.41 10.91

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

10.82
6.33
10.88
32.83
60.25
11.10
533.86 11.14

10.87
3.64
10.90
13.85
27.16
11.20
477.86 11.21

7.23
8.43
8.58
8.84
9.59
9.58

9.30
9.33
9.64
9.72

9.38
9.38
9.55
9.72

7.45

5.04
6.44
6.50
6.90
7.83
7.85

7.32
7.39
7.73
7.96

7.42
7.44
7.60
7.94

1.00

15.73 13.75

11.08

112.31
220.52

1.74

451.42
478.46
952.49

11.71
8.98
13.87 11.77
15.06 12.92
15.20 13.27
16.95 15.27
17.00 15.53

1.18

15.70 13.69
15.62 13.68
116.26
116.61
16.40 14.49
1174.35 17.22 15.57

1.07
55.80
53.98

15.73 13.74
15.74 13.74
16.24 14.32
1297.98 17.09 15.37

5.56
8.99
9.92
10.96
13.06
13.54

10.96
11.15
12.00
13.63

11.07
11.12
11.73
13.39

Table 2: Results with Triplet network with hard negative
mining and Npairs [23] Network. Querying ImageNet val
data against hash table built on val set.

6.1. Cifar 100

Cifar-100 [13] dataset has 100 classes. Each class has
500 images for train and 100 images for test. Given a query
image from test, we experiment the search performance
both when the hash table is constructed from train and from
test. The batch size is set to 128 in Cifar-100 experiment.
We ﬁnetune the base model for 70k iterations and decayed
the learning rate to 0.3 of previous learning rate after 20k
iterations when we optimize our methods. Table 1 shows
the results from the triplet network and the npairs network
respectively. The results show that our method not only
outperforms search accuracies of the state of the art deep
metric learning base models but also provides the superior
speedup over other baselines.

Cifar-100

train

test

62.94
86.11
68.20
76.85
89.11
89.95

53.11
68.88
54.95
62.68
68.95
69.64

ImageNet

val

37.90
45.55
31.62
45.47
48.52
61.21

Cifar-100

train

test

43.80
80.74
51.46
80.25
84.90
86.80

37.45
65.62
44.32
66.69
68.56
71.30

ImageNet

val

36.00
50.01
15.20
53.74
55.09
65.49

LSH
DCH

Th
VQ
[11]
Ours

Table 3: Hash table NMI for Cifar-100 and Imagenet.

ks

1

2

3

Method

Metric

LSH
Th

VQ-train
VQ-test

[11]
Ours

Th

VQ-train
VQ-test

[11]
Ours

Th

VQ-train
VQ-test

[11]
Ours

SUF

1.00

33.75
10.98
54.30
57.44
56.35
78.23

4.55
15.29
16.43
15.99
71.14

2.79
7.80
8.20
7.24
84.04

Pr@1

Pr@4

Pr@16

21.55

18.49
20.25
20.15
20.59
21.35
21.46

21.27
21.51
21.58
22.12
22.12

21.53
21.56
21.58
22.18
21.97

19.11

15.50
17.22
18.10
18.31
18.49
18.88

18.86
19.03
18.93
19.21
18.63

19.11
19.11
19.09
19.40
18.87

16.06

11.14
13.66
14.85
15.32
15.32
15.67

15.68
15.88
15.94
15.95
15.34

15.99
16.03
16.06
16.10
15.56

Table 4: Results with Triplet network with hard negative
mining. Querying ImageNet val set in Ctest against hash
table built on val set in Ctest.

6.2. ImageNet

ImageNet ILSVRC-2012 [20] dataset has 1, 000 classes
and comes with train (1, 281, 167 images) and val set
(50, 000 images). We use the ﬁrst nine splits of train set

711385

ℎ([8]

ℎ([14]

ℎ([12]

ℎ([6]

ℎ([21]

]

0

[

"

ℎ

ℎ"

[2]

ℎ([9]

(cid:1)

ℎ([6]

Root

depth 1

(cid:1)

depth 2

ℎ"

[11]

]

0

1

[

"

ℎ

ℎ([29]

ℎ([2]

ℎ([5]

ℎ([20]

ℎ([11]

Figure 3: Visualization of the examples mapped by our
trained three level hash codes [h(1), h(2)] on Cifar-100. Each
parent node (denoted as depth 1) is color coded in red, yellow,
blue, and green in cw order. Each color coded box (denoted
as depth 2) shows examples of the hashed items in each child
node.

to train our model, the last split of train set for validation,
and use validation dataset to test the query performance.
We use the images downsampled to 32 × 32 from [8]. We
ﬁnetune npairs base model and triplet base model as in [11]
and add a randomly initialized fully connected layer to learn
hierarchical representation. Then, we train the parameters
in the newly added layer with other parameters ﬁxed. When
we train with npairs loss, we set the batch size to 1024 and
train for 15k iterations decaying the learning rate to 0.3 of
previous learning rate after each 6k iterations. Also, when
we train with triplet loss, we set the batch size to 512 and
train for 30k iterations decaying the learning rate of 0.3 of
previous learning rate after each 10k iterations. Our results
in Table 2 show that our method outperforms the state of
the art deep metric learning base models in search accuracy
while providing up to 1298× speedup over exhaustive linear
search. Table 3 compares the NMI metric and shows that
the hash table constructed from our representation yields
buckets with signiﬁcantly better class purity on both datasets
and on both the base metric learning methods.

ageNet ILSVRC-2012 dataset. Then, we split the two sub-
classes of each 119 super-class into Ctrain and Ctest, where
Ctrain ∩ Ctest = ∅. Section D in the supplementary material
shows the class names in Ctrain and Ctest. We use the images
downsampled to 32 × 32 from [8]. We train the models
with triplet embedding on Ctrain and test the models on Ctest.
The batch size is set to 200 in ImageNetSplit dataset. We
ﬁnetune the base model for 50k iterations and decayed the
learning rate to 0.3 of previous learning rate after 40k iter-
ations when we optimize our methods. We also perform
vector quantization with the centroids obtained from Ctrain
(denote as ‘VQ-train’) and Ctest (denote as ‘VQ-test’), re-
spectively. Table 4 shows our method preserves the accuracy
without compromising the speedup factor.

Note, in all our experiments in Tables 1 to 4, while all the
baseline methods show severe degradation in the speedup
over the code compound parameter ks, the results show
that the proposed method robustly withstands the speedup
degradation over ks. This is because our method 1) greatly
increases the quantization granularity beyond other base-
line methods and 2) hashes the items more uniformly over
the buckets.
In effect, indexing multiple buckets in our
quantized representation does not as adversarially effect
the search speedup as other baselines. Figure 3 shows a
qualitative result with npairs network on Cifar-100, where
d = 32, k = 2, ks = 1. As an interesting side effect, our
qualitative result indicates that even though our method does
not use any super/sub-class labels or the entire label infor-
mation during training, optimizing for the objective in Equa-
tion (2) naturally discovers and organizes the data exhibiting
a meaningful hierarchy where similar subclasses share com-
mon parent nodes.

7. Conclusion

We have shown a novel end-to-end learning algorithm
where the quantization granularity is signiﬁcantly increased
via hierarchically quantized representations while preserv-
ing the search accuracy and maintaining the computational
complexity practical for the mini-batch stochastic gradient
descent setting. This not only provides the state of the art
accuracy results but also unlocks signiﬁcant improvement in
inference speedup providing the highest reported inference
speedup on Cifar100 and ImageNet datasets respectively.

6.3. ImageNetSplit

Acknowledgements

In order to test the generalization performance of our
learned representation against previously unseen classes,
we performed an experiment on ImageNet where the set of
classes for training and testing are completely disjoint. Each
class in ImageNet ILSVRC-2012 [20] dataset has super-
class based on WordNet [18]. We select 119 super-classes
which have exactly two sub-classes in 1000 classes of Im-

This work was partially supported by Kakao, Kakao
Brain and Basic Science Research Program through
the National Research Foundation of Korea (NRF)
(2017R1E1A1A01077431). Hyun Oh Song is the corre-
sponding author.

811386

[22] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learn-
ing transferrable representations for unsupervised domain
adaptation. In NIPS, 2016. 2

[23] K. Sohn. Improved deep metric learning with multi-class

n-pair loss objective. In NIPS, 2016. 1, 3, 4, 5, 6, 7

[24] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep

metric learning via facility location. In CVPR, 2017. 1

[25] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 1

[26] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014. 1

[27] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen. A survey
on learning to hash. arXiv preprint arXiv:1606.00185, 2016.
1, 2, 6

[28] X. Wang and A. Gupta. Unsupervised learning of visual

representations using videos. In ICCV, 2015. 2

[29] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hashing
for image retrieval via image representation learning. In AAAI,
2014. 2

[30] B. Xu, N. Wang, T. Chen, and M. Li. Empirical evaluation of
rectiﬁed activations in convolutional network. arXiv preprint
arXiv:1505.00853, 2015. 6

[31] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cascaded

embedding. In ICCV, 2017. 1, 2

[32] A. Zhai, D. Kislyuk, Y. Jing, M. Feng, E. Tzeng, J. Donahue,
Y. L. Du, and T. Darrell. Visual discovery at pinterest. In
Proceedings of the 26th International Conference on World
Wide Web Companion, 2017. 1, 6

[33] F. Zhao, Y. Huang, L. Wang, and T. Tan. Deep semantic
In

ranking based hashing for multi-label image retrieval.
CVPR, 2015. 2

References

[1] P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor-
mance of multilayer neural networks for object recognition.
In ECCV, 2014. 1, 6

[2] S. Bell and K. Bala. Learning visual similarity for product
design with convolutional neural networks. In SIGGRAPH,
2015. 1, 2

[3] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy
minimization via graph cuts. IEEE Transactions on pattern
analysis and machine intelligence, 2001. 3

[4] J. Bromley, I. Guyon, Y. Lecun, E. Sackinger, and R. Shah.
Signature veriﬁcation using a "siamese" time delay neural
network. In NIPS, 1994. 1

[5] M. Bucher, S. Herbin, and F. Jurie.

Improving semantic
embedding consistency by metric learning for zero-shot clas-
sifﬁcation. In ECCV, 2016. 1, 2

[6] Y. Cao, M. Long, B. Liu, and J. Wang. Deep cauchy hashing
for hamming space retrieval. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
6

[7] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen. Deep
quantization network for efﬁcient image retrieval. In AAAI,
2016. 2

[8] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled
variant of imagenet as an alternative to the cifar datasets.
arXiv preprint arXiv:1707.08819, 2017. 8

[9] R. Hadsell, S. Chopra, and Y. Lecun. Dimensionality re-
duction by learning an invariant mapping. In CVPR, 2006.
1

[10] P. Jain, B. Kulis, and K. Grauman. Fast image search for

learned metrics. In CVPR, 2008. 6

[11] Y. Jeong and H. O. Song. Efﬁcient end-to-end learning for

quantizable representations. In ICML, 2018. 1, 2, 3, 6, 7, 8

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[13] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 (canadian

institute for advanced research). 2009. 1, 6, 7

[14] Q. Li, Z. Sun, R. He, and T. Tan. Deep supervised discrete

hashing. In NIPS, 2017. 2

[15] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,

abs/1312.4400, 2013. 6

[16] S. Liu and H. Lu. Learning deep representations with diode
loss for quantization-based similarity search. In IJCNN, 2017.
2

[17] C. D. Manning, P. Raghavan, and H. Schutze. Introduction to
Information Retrieval. Cambridge university press, 2008. 6
[18] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 1995. 8

[19] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov. Hamming

distance metric learning. In NIPS, 2012. 2

[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. IJCV, 2015. 1, 6, 7, 8

[21] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering. In
CVPR, 2015. 1, 3, 5, 6

911387

