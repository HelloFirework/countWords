Defending against adversarial attacks by randomized diversiﬁcation

Olga Taran, Shideh Rezaeifar, Taras Holotyak, Slava Voloshynovskiy∗

Department of Computer Science, University of Geneva

7, route de Drize, 1227 Carouge, Switzerland

{olga.taran, shideh.rezaeifar, taras.holotyak, svolos}@unige.ch

Abstract

The vulnerability of machine learning systems to adver-
sarial attacks questions their usage in many applications.
In this paper, we propose a randomized diversiﬁcation as a
defense strategy. We introduce a multi-channel architecture
in a gray-box scenario, which assumes that the architecture
of the classiﬁer and the training data set are known to the
attacker. The attacker does not only have access to a se-
cret key and to the internal states of the system at the test
time. The defender processes an input in multiple chan-
nels. Each channel introduces its own randomization in
a special transform domain based on a secret key shared
between the training and testing stages. Such a transform
based randomization with a shared key preserves the gra-
dients in key-deﬁned sub-spaces for the defender but it pre-
vents gradient back propagation and the creation of vari-
ous bypass systems for the attacker. An additional beneﬁt of
multi-channel randomization is the aggregation that fuses
soft-outputs from all channels, thus increasing the reliabil-
ity of the ﬁnal score. The sharing of a secret key creates an
information advantage to the defender. Experimental evalu-
ation demonstrates an increased robustness of the proposed
method to a number of known state-of-the-art attacks.

1. Introduction

Besides remarkable and impressive achievements, many
machine learning systems are vulnerable to adversarial at-
tacks [6]. The adversarial attacks attempt at tricking a de-
cision of a classiﬁer by introducing bounded and invisible
perturbations to a chosen target image. This weakness seri-
ously questions the usage of the machine learning in many
security- and trust-sensitive domains.

Many researchers have proposed various defense strate-
gies and countermeasures to defeat adversarial attacks.
However, the growing number of defenses naturally stimu-
lates the invention of new and even more universal attacks.

∗S. Voloshynovskiy is a corresponding author. The research was sup-

ported by the SNF project No. 200021 182063.

X
1

1

·

·

·

· · ·

·

·

·

n
x 1

n
xN

Training

φθ

Shared secret key k

Soft-max

1

1

·

·

·

· · ·

·

·

·

y

M

1

y

M

N

x

x′

Known
training
data

Known
architecture

Adversarial
examples
modulator

x′ = x + wc′

Target host (x)

Target class (c′)

Testing

1

φθ

·

·

·

ˆc

M

y

Figure 1: Setup under investigation: the attacker knows the
labeled training data set X and the system architecture but
he does not have access to secret key k of the defender
shared between the training and testing.

An overview and classiﬁcation of the most efﬁcient attacks
and defenses are given in [15, 10].

In this paper, we consider a ”game” between the de-
fender and the attacker according to the diagram presented
in Figure 1.

The defender has access to the classiﬁer φθ and the train-
ing data set X. The defender shares a secret key k between
training and testing. The classiﬁer outputs a soft-max vector
y of length M , where M corresponds to the total number
of classes, and each yc, 1 ≤ c ≤ M is treated as a proba-
bility that a given input x belongs to a class c. The trained
classiﬁer φθ is used during testing.

The attacker in the white-box scenario has full knowl-
edge about the classiﬁer architecture, defense mechanisms,
training data and, quite often, can access the trained param-
eters of the classiﬁer. In the gray-box scenario, considered
in this paper, the attacker knows the architecture of the clas-
siﬁer, the general defense mechanism and has access to the
same training data X [3, 15]. Using the above available
knowledge, the attacker can generate a non-targeted or tar-
geted, with respect to a speciﬁed class c′, adversarial pertur-
bation wc′ . The attacker produces an adversarial example
by adding this perturbation to the target host sample x as

11226

W1

k
1

.
.
.

.
.
.

.
.
.

WJ

k
J

W−1

1

W−1

1

W−1

J

W−1

J

P11

k11

.
.
.

P1I

k1I

PJ1

kJ1

.
.
.

PJ I

kJ I

Classiﬁer 11

φθ11

1

·

·

·

M

y

11

Classiﬁer 1I

φθ

1I

Classiﬁer J1

φθJ1

Classiﬁer J I

φθJ I

1

·

·

·

M

y

1I

1

·

·

·

M

y
J1

1

·

·

·

M

y
J I

Aggregator

ˆc

1

·

·

·

y

M

B

Figure 2: Generalized diagram of the proposed multi-channel classiﬁer.

x′ = x + wc′ . The adversarial example is presented to the
classiﬁer at test time in an attempt to trick the classiﬁer φθ
decision.

Without pretending to be exhaustive in our overview, we

group existing defense strategies into three major groups:

1. Non key-based defenses: This group includes the ma-
jority of state-of-the-art defense mechanisms based on
detection and rejection, adversarial retraining, ﬁltering
and regeneration, etc. [10]. Besides the broad diversity
of these methods, a common feature and the main dis-
advantage of these approaches is an absence of ”cryp-
tographic” elements, like for example a secret key, that
would allow to create an information advantage of the
defender over the attacker.

2. Defense via randomization and obfuscation: The
defense mechanisms of this group are mainly based on
the ideas of randomization avoiding the reproducible
and repeatable use of parameters of the trained system.
This includes gradient masking [1] and introducing an
ambiguity via different types of key-free randomiza-
tion. The example of such randomization can be noise
addition at different levels of the system [14], injection
of different types of randomization like, for example,
random image resizing or padding [13] or randomized
lossy compression [5], etc.

The main disadvantage of this group of defense strate-
gies consists in the fact that the attacker can bypass the
defense blocks or take this ambiguity into account dur-
ing the generation of the adversarial perturbations [1].
Additionally, the classiﬁcation accuracy is degraded
since the classiﬁer is only trained on average for dif-

ferent sets of randomization parameters unless special
ensembling or aggregation is properly applied to com-
pensate this loss. However, even in this case the mis-
match between the training and testing stages can only
ensure the performance on average whereas one is in-
terested to have the guaranteed performance for each
realization of randomized parameters. Unfortunately,
this is not achievable without the common secret shar-
ing between the training and testing.

3. Key-based defenses: The third group generalizes the
defense mechanisms, which include a randomization
explicitly based on a secret key that is shared between
training and testing stages. For example, one can men-
tion the use of random projections [11], the random
feature sampling [4] and the key-based transformation
[10], etc.

the main disadvantage of the known
Nevertheless,
methods in this group consists of the loss of perfor-
mance due to the reduction of useful data that should
be compensated by a proper diversiﬁcation and corre-
sponding aggregation.

In this paper, we target further extension of the key-based
defense strategies based on the cryptographic principles to
create an information advantage of the defender over the at-
tacker yet maximally preserving the information in the clas-
siﬁcation system. The generalized diagram of the proposed
system is shown in Figure 2. It has two levels of randomiza-
tion, each of which can be based on unique secret keys. An
additional robustiﬁcation is achieved via the aggregation of
the soft outputs of the multi-channel classiﬁers trained for
their own randomizations. As it will be shown throughout

11227

the paper, usage of multi-channel architecture diminishes
the efﬁciency of attacks.

The main contribution of this paper is twofold:
• A new multi-channel classiﬁcation architecture with
defense strategy against gray-box attacks based on the
cryptographic principle.

• An investigation of the efﬁciency of the proposed ap-
proach on three standard data sets for several classes
of well-known adversarial attacks.

The remainder of this paper is organized as follows: Sec-
tion 2 introduces a new multi-channel classiﬁcation archi-
tecture. Section 3 provides an extension of the defense strat-
egy based on the data independent permutation proposed in
[10] to multi-channel architecture. The efﬁcient key-based
data independent transformation is investigated in Section
4. The ﬁltering by a hard-thresholding in the secret domain
is analyzed in Section 5. Section 6 concludes the paper.

2. Multi-channel classiﬁcation algorithm

A multi-channel classiﬁer, which forms the core of the
proposed architecture, is shown in Figure 2. It consists of
four main building blocks:

1. Pre-processing of the input data in a transform do-
main via a mapping Wj , 1 ≤ j ≤ J .
In general,
the transform Wj can be any linear mapper. For ex-
ample it can be a random projection or belong to the
family of orthonormal transformations (WjWT
j = I)
like DFT (discrete Fourier transform), DCT (discrete
cosines transform), DWT (discrete wavelet transform),
etc. Moreover, Wj can also be a learnable transform.
However, it should be pointed out that from the point
of view of the robustness to adversarial attacks, the
data independent transform Wj is of interest to avoid
key-leakage from the training data. Furthermore, Wj
can be based on a secret key kj .

2. Data independent processing Pji, 1 ≤ i ≤ I presents
the second level of randomization and serves as a de-
fense against gradient back propagation to the direct
domain.

One can envision several cases. As shown in Figure
3a, Pji ∈ {0, 1}l×n, l < n, presents a lossy sam-
pling of the input signal of length n, as considered
In Figure 3b, Pji ∈ {0, 1}n×n is a lossless
in [4].
permutation, similar to [10]. Finally, in Figure 3c,
Pji ∈ {−1, 0, +1}n×n corresponds to sub-block sign
ﬂipping. The yellow color highlights the key deﬁned
region of key-based sign ﬂipping. This operation is
reversible and thus lossless for an authorized party.
Moreover, to make the data independent processing ir-
reversible for the attacker, it is preferable to use a Pji
based on secret key kji.

n

Pji =

1 0 0 · · · 0 0
0 0 1 · · · 0 0
· · · · · · · · · · · ·
0 0 0 · · · 1 0

l

l < n

n
1 0 0 · · · 0 0
0 0 1 · · · 0 0
· · · · · · · · · · · ·
0 0 0 · · · 1 0

Pji =

n

(a) randomized sampling

(b) randomized permutation

n
1 0 0 ... .. ... 0 0 0

0 1 0 ... .. ... 0 0 0
... ... .. .. ... ... ...

0 ... -1 0 0 ... ... 0
0 ... 0 1 0 ... ... 0
0 ... 0 0 -1 ... ... 0
... ... .. .. ... ... ...

0 0 0 ... .. ... 0 1 0
0 0 0 ... .. ... 0 0 1

Pji =

n

(c) randomized sign ﬂipping in the
sub-block deﬁned in orange

Figure 3: Randomized transformation Pji, 1 ≤ j ≤ J, 1 ≤
i ≤ I examples. All transforms are key-based.

3. Classiﬁcation block can be represented by any family
of classiﬁers. However, if the classiﬁer is designed
for classiﬁcation of data in the direct domain then it
is preferable that it is preceded by W−1

.

j

4. Aggregation block can be represented by any operation
ranging from a simple summation to learnable opera-
tors adapted to the data or to a particular adversarial
attack.

As it can be seen from Figure 2, the chain of the ﬁrst 3
blocks can be organized in a parallel multi-channel struc-
ture that is followed by one or several aggregation blocks.
The ﬁnal decision about the class is made based on the ag-
gregated result. The rejection option can be also naturally
envisioned.

The training of the described algorithm can be repre-

sented as:

( ˆϑ, { ˆθji}) = arg min
ϑ,{θji}

T

J

Ij

X

X

X

t=1

j=1

i=1

L(yt, Aϑ(φθji (f (xt)))),

(1)

with:

f (xt) = W−1

j

PjiWj xt,

where L is a classiﬁcation loss, yt is a vectorized class label
of the sample xt, Aϑ corresponds to the aggregation oper-
ator with parameters ϑ, φθji is the ith classiﬁer of the jth
channel, θ denotes the parameters of the classiﬁer, T equals
to the number of training samples, J is the total number
of channels and Ij equals to the number of classiﬁers per
channel that we will keep ﬁxed and equals to I.

The attacker might discover the secret keys kj and/or
kji or make the full system end-to-end differentiable using
the Backward Pass Differentiable Approximation technique
proposed in [1] or via replacing the key-based blocks by

11228

the bypass mappers. To avoid such a possibility, we restrict
the access of the attacker to the internal results within the
block B. This assumption corresponds to our deﬁnition of
the gray-box setup.

In the proposed system, we will consider several prac-
tical simpliﬁcations leading to information and complexity
advantages for the defender over the attacker:

• The defender training can be performed per channel in-
dependently until the aggregation block. At the same,
the attacker should train and back propagate the gradi-
ents in all channels simultaneously or at least to guar-
antee the majority of wrong scores after aggregation.

• The blocks of data independent processing Pji aim at
preventing gradient back propagation into the direct
domain but the classiﬁer training is adapted to a par-
ticular Pji in each channel.

• It will be shown further by the numerical results that
the usage of the multi-channel architecture with the
following aggregation stabilizes the results’ deviation
due to the use of randomizing or lossy transformations
Pji, if such are used.

• The right choice of the aggregation operator Aϑ pro-
vides an additional degree of freedom and increases
the security of the system through the possibility to
adapt to speciﬁc types of attacks.

• Moreover, the overall security level considerably in-
creases due to the independent randomization in each
channel. The main advantage of the multi-channel sys-
tem consists in the fact that each channel can have an
adjustable amount of randomness, that allows to obtain
the required level of defense against the attacks.
In
a one-channel system the amount of randomness can
be either insufﬁcient to prevent the attacks or too high
which leads to classiﬁcation accuracy loss. Therefore,
having a channel-wise distributed randomness is more
ﬂexible and efﬁcient for the above trade-off.

The described generalized multi-channel architecture
provides a variety of choices for the transform operators W
and data independent processing Pji. In Section 3, we will
consider a variant with multiple Pji in the form of the con-
sidered permutation in the direct domain Wj = I. In Sec-
tion 4, we will investigate a sign ﬂipping operator Pji for
the common DCT operator W. Section 5 will be dedicated
to the investigation of a denoising version of Pji based on
hard-thresholding in a secret sub-space of the DCT domain
Wj .

3. Classiﬁcation with multi-channel permuta-

tions in the direct domain

P1

k1

.
.
.

.
.
.

PI

kI

Classiﬁer 1

φθ1

Classiﬁer I

φθ

I

1

·

·

·

y

M

1

1

·

·

·

M

y
I

+

1

·

·

·

y

M

ˆc

B

Figure 4: Classiﬁcation via multi-channel permutations in
the direct domain.

this idea for a single channel. However, despite the reported
efﬁciency of the proposed defense strategy, a single channel
architecture is subject to a drop in classiﬁcation accuracy,
even for the original, i.e., non-adversarial, data.

Therefore, this paper investigates the performance of a

permutation-based defense in a multi-channel setting.

3.1. Problem formulation

The generalized diagram of the corresponding extended
multi-channel approach is illustrated in Figure 4. The per-
mutation in the direct domain implies that Wj = I with
J = 1 and I permutation channels. Therefore, each chan-
nel 1 ≤ i ≤ I has only one data independent permutation
block Pi represented by a lossless permutation of the input
signal x ∈ Rn×n×m in the direct domain, where n corre-
sponds to the size of the input image and m is the number
of the channels (colors) in this image. Thus, the permuta-
tion matrix Pi is a matrix of size n × n, generated from a
secret key ki, whose entries are all zeros except for a single
element of each row, which is equal to one. In addition, as
illustrated in Figure 3b, all non-zero entries are located in
different columns. For our experiments, we assume that Pi
is the same for each input image color channel but it can be
a different one in the general case to increase the security of
the system. As an aggregation operator A, we use a sum-
mation for the sake of simplicity and interpretability. The
aggregated result represents a M -dimensional vector y of
real non-negative values, where M equals to the number of
classes and each entry yc is treated as a probability that a
given input x belongs to the class c.

Under the above assumptions, the optimization problem

(2) reduces to 1:

{ ˆθi} = arg min

{θi}

T

I

X

X

t=1

i=1

3.2. Numerical results

L(yt, A(φθi (Pixt))).

(2)

The simplest case of randomized diversiﬁcation can be
constructed for the direct domain with the permutation of
input pixels. In fact, the algorithm proposed in [10] reﬂects

To reveal the impact of multi-channel processing, we
compare our results with an identical single channel system

1https://github.com/taranO/defending-adversarial-attacks-by-RD

11229

Data type

I

1

5

10

15

20

25

MNIST: original classiﬁer error is 1%

Original

CW ℓ2

CW ℓ0

CW ℓ∞

2.83

8.85

13.87

11.67

1.73

4.56

5.98

4.72

1.37

3.82

4.98

4.03

1.43

3.53

4.69

3.87

1.57

3.55

4.47

3.59

Fasion-MNIST: original classiﬁer error is 7.5%

Original

CW ℓ2

CW ℓ0

CW ℓ∞

Original

CW ℓ2

CW ℓ0

CW ℓ∞

11.40

12.16

13.45

11.99

9.4

10.15

10.15

9.72

9.27

9.78

9.62

9.69

9.2

9.41

9.56

9.24

CIFAR: original classiﬁer error is 21%

47.03

47.76

48.39

47.41

41.47

41.82

42.27

42.12

40.2

39.83

40.87

40.53

39.8

39.59

39.73

39.58

9.23

9.49

9.82

9.26

39.2

39.4

39.85

39.62

1.4

3.51

4.4

3.69

9.2

9.4

9.63

9.32

39

39.04

39.76

39.21

Table 1: Classiﬁcation error (%) on the ﬁrst 1000 test sam-
ples for I-channel system with the direct domain permuta-
tion.

reported in [10]. For each classiﬁer φθi we use exactly the
same architecture as mentioned in Table 2 in [10]2. More-
over, taking into account that the generation of adversar-
ial examples is quite a slow process, as well as in [10], we
verify our approach on the ﬁrst 1000 test samples of the
MNIST [8] and Fashion-MNIST [12] data sets. Addition-
ally, we investigate the CIFAR-10 data set [7].

The obtained results are given in Table 1. For all data
sets, a single channel set up with I = 1 corresponds to the
results of the approach proposed in [10] and CW denotes
the attacks proposed by Carlity and Wagner in [2].

As one can note from Table 1, increasing the number
of channels leads to a decrease of the classiﬁcation error. In
the case of the MNIST data set, our multi-channel algorithm
allows to reduce the error on the original non-attacked data
in 2 times, from 2.8% to 1.4%. For the attacked data, the
classiﬁcation error decreases 2.5 times from almost 9-14%
to 3.5-4.5%. In case of the Fashion-MNIST data set, one
can observe a similar dynamic, namely, the classiﬁcation
error decreases from 11.5-13.5% to 9-9.5. For the CIFAR-
10 data set using the multi-channel architecture allows to
reduce the error from 47-48% to only about 39.5%. The
CIFAR-10 natural images are more complex in compari-
son to the MNIST and Fasion-MNIST and the introduced
permutation destroys local correlations. This has a direct
impact on classiﬁer performance.

2The Python code for generating adversarial examples is available at

https://github.com/carlini/nn robust attacks

P

k

DCT

P DCT−1

k

(a) coordinate domain

(b) DCT domain

Figure 5: Global permutation in the coordinate domain (a)
and DCT based encoding using key-based sign ﬂipping (b).

4. Classiﬁcation with multi-channel sign per-

mutation in the DCT domain

The results obtained in Section 3 for the CIFAR-10 data
set show a high sensitivity to the gradient perturbations that
degrade the performance of the classiﬁer. In this Section we
investigate the other data independent processing functions
Pji based on a secret key kji preserving the gradient in a
special way that is more suitable for the classiﬁcation of
complex natural images. We will consider a general scheme
for demonstrative purposes to justify that the permutations
should be localized rather than global.

4.1. Global permutation

We will consider sign ﬂipping in the DCT domain as a
basis for the multi-channel randomization. For the visual
comparison of the effect of global permutation in the coor-
dinate domain versus global sign ﬂipping in the DCT do-
main, we show an example in Figure 5. From this Figure
one can note that the permutation in the coordinate domain
disturbs the local correlation in the image that will impact
the local gradients.
In turn, this might impact the train-
ing of modern classiﬁers that are mostly based on gradient
techniques. At the same time, preservation of the gradi-
ents makes the data more vulnerable to adversarial attacks.
Keeping this in mind, we can conclude that the global DCT
sign permutation also ”randomizes” the images but, in con-
trast to the permutation in direct domain, it keeps the local
correlation.

To answer the question whether the preservation of local
correlation at the randomization can help preserve the loss
of the gradients, we investigate the global DCT sign permu-
tation for the classiﬁcation architecture shown in Figure 4
with Wj is DCT and Pi ∈ {−1, 1}n×n. It should be noted
that the transform Wj is ﬁxed for all channels. Therefore,
the secrecy part consists in the key-based ﬂipping of DCT
coefﬁcients’ signs.

In the experimental results we obtained the classiﬁcation
accuracy to be very close to the results represented in Ta-
ble 1. For the sake of space, we do not present this table
in the paper. Nevertheless, we can conclude that the global
sign permutation in the DCT domain does not improve the
previous situation with the global permutation in direct do-
main.

11230

H

V D

(a) sub-bands

(b) original

(c) V

(d) H

(e) D

(f) V+H+D

Figure 6: Local randomization in the DCT sub-bands by
key-based sign ﬂipping.

4.2. Local permutation

Taking into account the above observation, we investi-
gate the behaviour of the local DCT sign permutations, i.e.,
we will use a global DCT transform but will ﬂip the signs
only for the selected number of coefﬁcients as shown in Fig-
ure 6c.

The general idea, illustrated in Figure 6, consists in the
fact that the DCT domain can be split into overlapping or
non-overlapping sub-bands of different size. In our case, for
the simplicity and interpretability, we split the DCT domain
into 4 sub-bands, namely, (1) top left that represents the low
frequencies of the image, (2) vertical, (3) horizontal and (4)
diagonal sub-bands. After that we apply the DCT sign ﬂip-
ping as randomization in each sub-band keeping all other
sub-bands unchanged and apply the inverse DCT transform.
The corresponding illustrative examples are shown in Fig-
ures 6c - 6e. Finally, we apply the DCT sign permutation
in 3 sub-bands. The corresponding result is shown in Fig-
ure 6f. It is easy to see that local DCT sign ﬂipping applied
in one individual sub-band creates a speciﬁc oriented dis-
tortion due to the speciﬁcity of chosen sub-bands but pre-
serves the local image content quite well. The simultaneous
permutation of 3 sub-bands creates more degradation which
might be undesirable and can have a negative inﬂuence on
the classiﬁcation accuracy.

To investigate the behaviour of the local DCT sign per-
mutations we use the multi-channel architecture shown
It is a three-channel model with I sub-
in Figure 7.
channels. As a Wj we use a standard DCT transform. The
sub-channels’ data independent processing blocks Pji ∈
{−1, 0, 1}n×n are based on the individual secret keys kji
and are represented by the matrices that allow to change
the elements’ signs only in the sub-band of interest, like il-
lustrated in Figure 3c. In general case, the sub-bands can be
overlapping or non-overlapping and have different positions
and sizes. As discussed, we use only 3 non-overlapping
sub-bands of equal size as illustrated in Figure 6a. The ar-
chitecture of the classiﬁers φθji is identical to the ones used
in Section 2. As an aggregation operator we use a simple

DCT

Classiﬁer 11

1

P11

DCT−1

φθ11

k11

.
.
.

Classiﬁer 1I

P1I

DCT−1

φθ

1I

k

1I

·

·

·

M

y

11
1

·

·

·

M

y

1I

Classiﬁer 31

1

H

.
.
.

.
.
.

.
.
.

+

1

·

·

·

y

M

ˆc

P31

DCT−1

φθ31

V

k31

.
.
.

Classiﬁer 3I

P3I

DCT−1

φθ

3I

k3I

·

·

·

M

y

31
1

·

·

·

M

y

3I

B

Figure 7: Classiﬁcation with local DCT sign permutations.

summation.

The corresponding optimization problem becomes:

{ ˆθji} = arg min

{θji}

T

3

I

X

X

X

t=1

j=1

i=1

L(yt, A(φθji (Pjixt))). (3)

4.3. Numerical results

The results obtained for the architecture proposed in Fig-
ure 7 are shown in Table 2. The column ”Classical” corre-
sponds to the results of the one-channel classical classiﬁer
for the original non-permuted data, that is referred to as the
classical scenario.

It should be pointed out that in the previous experiments
we observed a drop in the classiﬁcation accuracy even for
the original non-attacked data. In the proposed scheme with
the 12 and 15 sub-channels, the obtained classiﬁcation er-
ror on the adversarial examples corresponds to those of the
original data and, in some cases, is ever lower. For example,
we obtained a 2 times decrease in the classiﬁcation error on
the MNIST for the original data in comparison to the clas-
sical architecture.

The CIFAR-10 data set presents a particular interest for
us as a data set with natural images. For CW ℓ2 and CW
ℓ∞ attacks the classiﬁcation error is the same as in the case
of the classical scenario on the original data. This demon-
strates that the proposed method does not cause a degrada-
tion in performance due to the introduced defense mecha-
nism. In case of CW ℓ0 attack there is only about 2% of
successful attacks.

In the case of the Fashion-MNIST data set, the obtained
results are better than the results for the permutation in
the direct domain given in Table 1. For the original non-
attacked data the classical scenario accuracy is achieved.

11231

Data type

Classical

J · I

3

6

9

12

15

Original

CW ℓ2

CW ℓ0

1

100

100

CW ℓ∞

99.99

Original

CW ℓ2

CW ℓ0

CW ℓ∞

Original

CW ℓ2

CW ℓ0

CW ℓ∞

7.5

100

100

99.9

21

100

100

100

MNIST

0.5

6.28

19.3

2.81

0.5

5.34

18.48

2.37

Fashion-MNIST

8.1

9.27

10.62

9.2

CIFAR-10

21.2

22.42

25.72

22.8

7.4

8.67

9.99

8.41

19.6

21.3

24.52

21.39

0.5

4.66

17.6

2.22

7.6

8.87

10.13

8.66

19.5

21.04

23.84

21.21

0.5

4.44

16.7

2.12

7.2

8.62

9.87

8.47

18.6

20.79

23.43

20.81

0.5

4.73

17.42

2.06

7.4

8.62

9.86

8.49

19.2

20.92

23.28

20.92

Attack MNIST

Fashion-MNIST

CW ℓ2

CW ℓ0

CW ℓ∞

Table 2: Classiﬁcation error (%) on the ﬁrst 1000 test sam-
ples for the DCT domain with the local sign ﬂipping in 3
sub-bands (J = 3).

Table 3: Adversarial examples.

For the attacked data the classiﬁcation error exceeds the
level of those on the original data only with 1-2%.

The situation with the MNIST data set is even more in-
teresting. First of all, we would like to point out that we
decrease in 2 times the classiﬁcation error in comparison
to the classical scenario. However, for the CW ℓ0 the re-
sults are surprisingly worse. To investigate the reasons of
the performed degradation we visualize the adversarial ex-
amples. The results are shown in Table 3.
It is easy to
see that, in general, the CW ℓ∞ noise manifests itself as a
background distortion and doesn’t affect considerably the
regions of useful information. The CW ℓ2 noise affects the
regions of interest but the intensity of the noise is much
lower than the intensity of the meaningful information. As
well as in CW ℓ2, the CW ℓ0 noise is concentrated in the
region near the edges but its intensity is as strong as the in-
formative image parts. Thus, it becomes evident why local
DCT sign permutation is not capable to withstand such kind
of noise. In general, such a strong noise is easy detectable
and the corresponding adversarial examples can be rejected
by many detection mechanisms, like for example an aux-
iliary ”detector” sub-network [9]. Moreover, as it can be
seen from the Fashion-MNIST examples, the inﬂuence of
such noise and successful attacks drastically decreases with
increasing image complexity. As it has been shown by the
CIFAR-10 results, the local DCT sign permutation produces
a high level of defense against such an attack for natural im-
ages.

5. Classiﬁcation with multi-channel hard
thresholding in the sub-bands of the DCT
domain

As it can be seen from Figure 6, the local DCT sign
permutation creates sufﬁciently high image distortions. As
a simple strategy to avoid this effect, we investigate hard
thresholding of the DCT coefﬁcients in the deﬁned sub-
bands.
In this case the matrix Pji contains zeros for the
coefﬁcients of key-deﬁned sub-bands. Alternatively, one
can consider this strategy as a random sampling as illus-
trated in Figure 3a, where one retains only the coefﬁcients
used by the classiﬁer. In this sense, the considered strategy
is close to the randomization in a single channel without the
aggregation considered in [4].

Note that the considered processing is a data indepen-
dent transform. The secret keys can be used for choosing
the sub-bands positions. Thus, the attacker can not predict
in advance, which DCT coefﬁcients will be used or sup-
pressed.

For simplicity and to be comparable with the previously
obtained results, we use the multi-channel architecture that
is shown in Figure 7, the DCT sub-band division as illus-
trated in Figure 6a with ﬁxed 3 sub-band sizes and posi-
tions. Instead of applying the sign permutation, the corre-
sponding DCT frequencies are set to zero and the result is
transformed back to the direct domain. The visualization of
the results of such a transformation is shown in Figure 8.
The resulting images are slightly blurry but less noisy than
in the case of the DCT sign permutation.

The obtained numerical results for the MNIST, Fashion-
MNIST and CIFAR-10 data sets are given in Table 4. In

11232

(a) original

(b) sub-band V (c) sub-band H (d) sub-band D

Figure 8: Local zero ﬁlling in the DCT domain.

Original CW ℓ2

CW ℓ0

CW ℓ∞

MNIST

Fashion-MNIST

0.6

8.8

7.59

9.6

CIFAR

21.1

23.28

21.3

11.23

27.08

3.03

9.58

23.27

Table 4: Classiﬁcation error (%) for the DCT based hard
thresholding over the ﬁrst 1000 test samples (J = 3, I = 1).

general, the results are very close to the results of using the
DCT sign permutation represented in Table 2 with the num-
ber of classiﬁers equals to 3. For the original non-attacked
data the classiﬁcation error is almost the same. In case of
the attacked data, the classiﬁcation error is about 0.5-1 %
higher. This is related to the fact that the zero replacement
of DCT coefﬁcients leads to a loss of information and, con-
sequently, to a decrease in classiﬁcation accuracy.

Hence, replacing the DCT coefﬁcients by zeros might

also serve as a defense strategy.

6. Conclusions

In this paper, we address a problem of protection against
adversarial attacks in classiﬁcation systems. We propose the
randomized diversiﬁcation mechanism as a defense strat-
egy in the multi-channel architecture with the aggregation
of classiﬁers’ scores. The randomized diversiﬁcation is a
secret key-based randomization in a deﬁned domain. The
goal of this randomization is to prevent the gradient back
propagation or use of bypass systems by the attacker. We
evaluate the efﬁciency of the proposed defense and the per-
formance of several variations of a new architecture on three
standard data sets against a number of known state-of-the-
art attacks. The numerical results demonstrate the robust-
ness of the proposed defense mechanism against adversarial
attacks and show that using the multi-channel architecture
with the following aggregation stabilizes the results and in-
creases the classiﬁcation accuracy.

For the future work we aim at investigating the proposed
defense strategy against the gradient based sparse attacks
and non-gradient based attacks.

References

[1] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients
give a false sense of security: Circumventing defenses to ad-

versarial examples. In J. Dy and A. Krause, editors, Proceed-
ings of the 35th International Conference on Machine Learn-
ing, volume 80 of Proceedings of Machine Learning Re-
search, pages 274–283, Stockholmsmssan, Stockholm Swe-
den, 10–15 Jul 2018. PMLR. 2, 3

[2] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In 2017 IEEE Symposium on Security
and Privacy (SP), pages 39–57. IEEE, 2017. 5

[3] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo:
Zeroth order optimization based black-box attacks to deep
neural networks without training substitute models. In Pro-
ceedings of the 10th ACM Workshop on Artiﬁcial Intelligence
and Security, pages 15–26. ACM, 2017. 1

[4] Z. Chen, B. Tondi, X. Li, R. Ni, Y. Zhao, and M. Barni.
Secure detection of image manipulation by means of random
feature selection. CoRR, abs/1802.00573, 2018. 2, 3, 7

[5] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li,
L. Chen, M. E. Kounavis, and D. H. Chau. Shield: Fast, prac-
tical defense and vaccination for deep learning using jpeg
compression. In Proceedings of the 24nd ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data
Mining. ACM, 2018. 2

[6] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
In International Confer-

harnessing adversarial examples.
ence on Learning Representations (ICLR), 2015. 1

[7] A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset.
online: http://www. cs. toronto. edu/kriz/cifar. html, 2014. 5
[8] Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit
database. AT&T Labs [Online]. Available: http://yann. le-
cun. com/exdb/mnist, 2, 2010. 5

[9] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On
detecting adversarial perturbations. In International Confer-
ence on Learning Representations (ICLR), 2017. 7

[10] O. Taran, S. Rezaeifar, and S. Voloshynovskiy. Bridging ma-
chine learning and cryptography in defence against adver-
sarial attacks.
In Workshop on Objectionable Content and
Misinformation (WOCM), ECCV2018, Munich, Germany,
September 2018. 1, 2, 3, 4, 5

[11] N. X. Vinh, S. Erfani, S. Paisitkriangkrai, J. Bailey,
C. Leckie, and K. Ramamohanarao. Training robust mod-
els using random projection. In Pattern Recognition (ICPR),
2016 23rd International Conference on, pages 531–536.
IEEE, 2016. 2

[12] H. Xiao, K. Rasul, and R. Vollgraf.

Fashion-mnist: a
novel image dataset for benchmarking machine learning al-
gorithms. arXiv preprint arXiv:1708.07747, 2017. 5

[13] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating
adversarial effects through randomization. In International
Conference on Learning Representations (ICLR), 2018. 2

[14] Z. You, J. Ye, K. Li, and P. Wang. Adversarial noise layer:
Regularize neural network by adding noise. arXiv preprint
arXiv:1805.08000, 2018. 2

[15] X. Yuan, P. He, Q. Zhu, R. R. Bhat, and X. Li. Adversarial
examples: Attacks and defenses for deep learning. arXiv
preprint arXiv:1712.07107, 2017. 1

11233

