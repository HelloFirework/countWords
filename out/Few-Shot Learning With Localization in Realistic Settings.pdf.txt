Few-Shot Learning with Localization in Realistic Settings

Davis Wertheimer
Cornell University
dww78@cornell.edu

Bharath Hariharan
Cornell University
bh497@cornell.edu

Abstract

Traditional recognition methods typically require large,
artiﬁcially-balanced training classes, while few-shot learn-
ing methods are tested on artiﬁcially small ones. In con-
trast to both extremes, real world recognition problems ex-
hibit heavy-tailed class distributions, with cluttered scenes
and a mix of coarse and ﬁne-grained class distinctions.
We show that prior methods designed for few-shot learn-
ing do not work out of the box in these challenging condi-
tions, based on a new “meta-iNat” benchmark. We intro-
duce three parameter-free improvements: (a) better train-
ing procedures based on adapting cross-validation to meta-
learning, (b) novel architectures that localize objects using
limited bounding box annotations before classiﬁcation, and
(c) simple parameter-free expansions of the feature space
based on bilinear pooling. Together, these improvements
double the accuracy of state-of-the-art models on meta-iNat
while generalizing to prior benchmarks, complex neural ar-
chitectures, and settings with substantial domain shift.

1. Introduction

Image recognition models have purportedly reached hu-
man performance on benchmarks such as ImageNet, but de-
pend critically on large, balanced, labeled training sets with
hundreds of examples per class. This requirement is im-
practical in many realistic scenarios, where concepts may
be rare or have very few labeled training examples. Fur-
thermore, acquiring more labeled examples might require
expert annotators and thus be too expensive. This problem
is exacerbated in applications (e.g., robotics) that require
learning new concepts on the ﬂy in deployment, and cannot
wait for a costly ofﬂine data collection process.

These considerations have prompted research on the
problem of “few-shot” learning: recognizing concepts from
small labeled sets [14, 18, 37, 40, 43]. This past work builds
“learners” that can learn to distinguish between a small
number of unseen classes (often fewer than 20) based on
an extremely small number of training examples (e.g. 5 per
class). However, multiple challenges plague these methods

Figure 1. Discrepancies between existing benchmarks and real
world problems. Top: Traditional recognition benchmarks use
many, equally large classes, while few-shot benchmarks use few,
equally small classes. Natural problems tend to be heavy-tailed.
Bottom: Clockwise from top left: relevant objects may be over-
lapping, tiny, occluded, underemphasized (bird is on the feeder),
blurry, or simply hard to delineate [39].

when they are applied to real-world recognition problems.

First,

few-shot methods typically assume balanced
datasets, and optimize the learner for an exact, often unre-
alistically small number of training examples per class. In
contrast, real-world problems may have highly imbalanced,
heavy-tailed class distributions, with orders of magnitude
more data in some classes than in others. A practical learner
must therefore work equally well for all classes irrespective
of the number of training examples. It is unclear how or
even if few-shot methods can handle such an imbalance.

Second, few-shot learning methods often assume the
number of relevant concepts to be small, and as such highly
distinct from each other. In contrast, real world applications

6558

often involve thousands of classes with subtle distinctions.
These distinctions can be particularly hard to detect when
natural images are cluttered or difﬁcult to parse (Figure 1,
bottom). Thus, the learner must also be able to make ﬁne-
grained class distinctions on cluttered natural images.

We ﬁrst evaluate prototypical networks [37], a simple
yet state-of-the-art few-shot learning method, on a realistic
benchmark based on the heavy-tailed class distribution and
subtle class distinctions of the iNaturalist dataset [39]. We
show that prototypical networks can struggle on this chal-
lenging benchmark, conﬁrming the intuitions above.

We next present ways to address the challenge of heavy-
tailed, ﬁne-grained, cluttered recognition. We introduce
modiﬁcations to prototypical networks that signiﬁcantly im-
prove accuracy without increasing model complexity.

First,

to deal with heavy class imbalance, we pro-
pose a new training method based on leave-one-out cross-
validation. This approach makes optimization easier and
the learner more resilient to wider distributions of class
sizes. This technique yields a 4 point gain in accuracy.

Second, we posit that when objects are small or scenes
are cluttered, the learner may ﬁnd it difﬁcult to identify rel-
evant objects from image-level labels alone. To tackle this
problem, we explore new learner architectures that localize
each object of interest before classifying it. These learners
use bounding box annotations for a tiny subset of the la-
belled images. Localization improves accuracy by 6 points,
more so when objects occupy under 40% of the image.

Even after localizing the object, the learner may need
to look for subtle distinctions between concepts. Existing
few-shot methods rely on the learning process alone to build
informative feature representations. We show that straight-
forward, parameter-free adjustments can signiﬁcantly im-
prove performance. In particular, we ﬁnd that the represen-
tational power of the learner can be signiﬁcantly increased
by leveraging bilinear pooling [7, 26]. While in its origi-
nal formulation, bilinear pooling signiﬁcantly increases the
model parameter count, we show that it can be applied to
prototypical networks with zero increase. This modiﬁcation
signiﬁcantly improves accuracy by up to 9 points.

Together, these contributions double the accuracy of
prototypical networks and other strong baselines on our
challenging heavy-tailed benchmark, with negligible im-
pact on model complexity. Our results suggest that our
proposed approach provides signiﬁcant beneﬁts over prior
techniques for realistic recognition problems in the wild.

2. Related Work

The ideas behind our proposed techniques have broad
prior support, but appear in mostly disjoint or incompatible
problem settings. We adapt these concepts into a uniﬁed
framework for recognition in real-world scenarios.
Meta-learning: Prior work on few-shot

learning has

mainly focused on optimizing a learner: a function that
takes a small labeled training set and an unlabeled test set as
inputs, and outputs predictions on the test set. This learner
can be expressed as a parametric function and trained on a
dataset of “training” concepts so that it generalizes to new
ones. Because these methods train a learner, this class of ap-
proaches is often called “meta-learning”. Optimization may
focus on the learner’s parameterization [6, 15, 29, 36], its
update schedule [28, 34, 35], the generalizability of a built-
in feature extractor [12, 37, 40], or a learned distance metric
in feature space [13, 21, 38]. An orthogonal approach is to
generate additional, synthetic data [18, 46].

In most cases, however, few-shot classiﬁers [6, 15, 21,
28, 34, 35, 37, 38] are evaluated on one or both of only
two datasets: mini-ImageNet [40] and Omniglot [24]. The
former presents only ﬁve classes at a time, with one or
ﬁve training images per class. The latter is a handwrit-
ten character dataset, on which accuracy regularly surpasses
98% [12, 28, 29, 38, 40]. Some recent work has expanded
the number of classes dramatically [18, 43], but still as-
sumes that novel classes have the same number of examples.
These benchmarks are therefore divorced from real-world
conditions, which involve difﬁcult problems, natural im-
ages, many concepts, and varying amounts of training data
[27, 39, 44]. Many prior meta-learning approaches are in-
compatible with these settings. Notably, Wang et al [42] de-
sign an approach to heavy-tailed problems based on knowl-
edge transfer from common to rare classes. Their approach
is orthogonal to our improvements.

Heavy-tailed datasets: Heavy-tailed class distributions are
common in the real world. MS-COCO [25], the SUN
database [44], DeepFashion [27], MINC [5], and Places
[49] are all examples where an order of magnitude separates
the number of images in the most versus the least common
classes. MINC and Places are especially noteworthy be-
cause they are explicitly designed to narrow this gap in data
availability [5, 49], yet display heavy class imbalance any-
way. Despite this trend, standard recognition benchmarks
like ImageNet [11], CIFAR-10, and CIFAR-100 [22] heav-
ily curate their data to ensure that classes remain nicely bal-
anced and easily separable. The mini-ImageNet and Om-
niglot few-shot benchmarks encode class balance explicitly,
as do other proposed few-shot benchmarks [18, 43, 46].

Improving feature space: It is well known that higher-
order expansions of feature space can raise the expressive
power of hand-designed feature extractors [19, 33]. Re-
cent work has shown that similar techniques [7, 26], learn-
able generalizations of these techniques [8, 47], and efﬁ-
cient approximations to these techniques [16, 20] also im-
prove the performance of convolutional networks. The im-
provement is especially large in ﬁne-grained classiﬁcation
settings, such as facial recognition [4, 9, 26]. However, us-
ing the resulting expanded feature space requires parameter-

6559

Figure 2. Our real-world learning benchmark. Initially, many im-
ages are available with bounding box annotations. The learner
must then adapt to new classes using varying but limited amounts
of data, with very few bounding boxes. At test time in the wild,
there are no annotations.

heavy models. We adapt bilinear pooling [26] to the few-
shot setting as a truly parameter-free expansion, which no
longer risks overﬁtting to small datasets.
Localization: A close relationship exists between localiza-
tion and recognition. Networks trained solely on image-
level, classiﬁcation-based losses nevertheless learn to local-
ize objects of interest [30, 48]. These learned localizations
can act as useful data annotation, including for the origi-
nal recognition task [41, 45, 48]. Very difﬁcult problems,
however, may require expensive ground truth annotations
to begin bootstrapping. Fortunately, a very small set of an-
notations can be sufﬁcient to predict the rest [36]. Semi-
supervised localization further improves when image-level
category labels are provided [17, 23]. Since each can boot-
strap from the other, combining recognition and localization
may prove a particularly effective remedy for data scarcity.

3. Problem Setup and Benchmark

Our goal is to build learners, systems that can automati-
cally learn new concepts under challenging real-world con-
ditions, with heavy-tailed distributions of classes and subtle
class distinctions. Each learner may have tunable parame-
ters or hyperparameters. As in prior work, these parame-
ters are learned on a “representation set” of concepts (“base
classes” in [18]) with many training examples (see Fig. 2).
Once trained, the learner must generalize to a disjoint
“evaluation set” of novel categories. The evaluation set is
split into a small collection of labeled “reference images”
and a larger set of unlabeled “query images”. The learner
may use the reference images to deﬁne the new set of cate-
gories, estimate new parameters for those categories (e.g. a
linear classiﬁer) and/or ﬁne-tune its feature representations.
Final accuracy is reported on the unannotated query im-
ages. We report top-1 and top-5 accuracy, both as a mean
over images and over the categories of the evaluation set.
The latter metric penalizes classiﬁers that focus on large
categories while ignoring smaller ones.

Two approaches to the above problem act as illustra-
tive examples. A traditional transfer learning approach is
to train a softmax classiﬁer on the representation set. On
the evaluation set, the fully-connected layer is replaced by
a new version with the appropriate number of categories,
and ﬁne-tuned on reference images. Query images form the
test set. Meta-learning approaches, such as prototypical net-
works, train a parametric learner on tiny datasets sampled
from the representation set, teaching the learner to adapt to
novel tiny datasets. The learner processes the evaluation set
in a single pass, with reference images forming the training
set and query images forming the test set.
Object location annotations: As discussed in Section 1, a
key challenge in real-world recognition problems is ﬁnding
relevant objects in cluttered scenes. Small sets of image-
level class labels may be insufﬁcient. We therefore provide
bounding boxes for a small fraction (≤ 10%) of the refer-
ence images in the evaluation set. Note that with extremal
point clicks, these annotations are cheap to acquire in prac-
tice [32]. We fully annotate the representation set, as such
datasets tend to be heavily curated in the real world (Fig. 2).

3.1. Benchmark Implementation

We now convert this problem setup into a benchmark
that accurately evaluates learners on real-world heavy-tailed
problems. For this, the evaluation set must satisfy three key
properties. First, as in many real-world problems, training
sets should be heavily imbalanced, with orders of magni-
tude difference between rare and common classes. Yet the
number of examples per class must be neither unnecessar-
ily small (e.g. fewer than 10), nor unrealistically large (e.g.
more than 200). Second, in contrast to past few-shot learn-
ing benchmarks that use ﬁve classes at a time [24, 40], there
should be many (e.g. at least 20) categories in the evalua-
tion set, with coarse- and ﬁne-grained distinctions, as in the
real world. Third, images must be realistically challenging,
with clutter and small regions of interest.

We implement our benchmark using the iNat2017
dataset [39], an organically collected, crowdsourced com-
pendium of living organisms, with ﬁne- and coarse-grained
species distinctions, a heavy-tailed class size distribution,
and bounding box annotations for a signiﬁcant subset. Of
the appropriately-sized categories with bounding boxes,
80% are randomly assigned to the representation set, and
the rest to the evaluation set. Within the evaluation set, 20%
of images are reference images and the rest are query im-
ages, for an overall split of 80/4/16% representation, ref-
erence, and query. We propose this “meta-iNat” dataset
as a realistic, heavy-tailed, ﬁne-grained benchmark for
meta-learning algorithms. Meta-iNat contains 1,135 animal
species, the distribution for which can be found in Fig. 3.

While all images in meta-iNat have bounding box anno-
tations, only 10% are allowed during evaluation (see Sec-

6560

image, but never both. While reference images learn to
form good class centroids, conditioned on the other contrib-
utors, query images gravitate toward the correct centroid
and away from others. Gradients for both are necessary
for learning, but every image gets only one, so prototypi-
cal weight updates are noisy.

This reference/query distinction also limits the number
of reference images a network can handle. For a proto-
type network to work on common classes as well as rare
ones, it must be trained with a larger number of reference
images [37].
Increasing the reference images per batch,
however, requires either increasing the batch size, which
runs into memory constraints, or decreasing the number of
queries, producing noisier query gradients. Thus the origi-
nal prototypical networks are designed for rare classes.

As an alternative, we propose to use leave-one-out cross-
validation within each batch, abandoning the hard refer-
ence/query split. The entire batch is treated as reference
images, and the contribution of each image is subtracted
(“folded”) out from its corresponding prototype whenever
it acts as a query. Each image thus gets a combined, cleaner
gradient, acting as both a reference and a query. Further-
more, the number of query / reference images can be as high
as the batch size / one less. The result is stable training with
large reference sets without violating memory constraints.
We call this approach batch folding.
Procedure: Let n be the number of classes and p the num-
ber of images per class in a batch. Denote by vi,j the
feature vector of the i-th image in the j-th category. Let
cj = Pi vi,j
be the centroid of the j-th class. To make pre-
dictions for the i-th image in the j-th category, the network
uses the following class prototypes:

p

c1, c2, ... cj−1,

p

p − 1

(cj −

vi,j
p

), cj+1, ... cn

(1)

Overhead: Batch folding is efﬁciently parallelizable using
tensor broadcasting. The necessary broadcast operations
are built-in to most machine learning libraries, including
NumPy [1], PyTorch [2], and TensorFlow [3].

Note also that standard prototypical network prediction
already involves calculating the L2 distance between every
centroid and every query image embedding. This has the
same asymptotic cost as calculating (1) for every image, so
long as query set size nquery ≈ ntotal. Generally this is
true [37]. The overhead of batch folding also tends to be
dominated by earlier convolutional layers.

4.3. Localization

Image-level labels are less informative when the object
of interest is small and the scene cluttered, since it is un-
clear what part of the image the label refers to. Given many,
sufﬁciently different training images, the machine eventu-
ally ﬁgures out the region of interest [48]. But with only a

6561

Figure 3. Category sizes in meta-iNat

tion 3). We run ten trials on the evaluation set with a differ-
ent collection of annotated reference images in each trial.

4. Approach

We build upon prototypical networks[37] (Section 4.1)
by introducing three light-weight and parameter-free im-
provements to prototypical networks [37]. Batch folding
(Section 4.2) improves gradients during training and helps
the learner generalize to large classes. Few-shot localization
(Section 4.3) teaches the learner to localize an object before
classifying it. Covariance pooling (Section 4.4) greatly in-
creases the expressive power of prototype vectors without
affecting the underlying network architecture. In addition
to being parameter-free, these techniques are mutually com-
patible and mutually beneﬁcial.

4.1. Prototypical Networks

We brieﬂy review prototypical networks [37]. Prototyp-
ical networks are a learner architecture designed to learn
novel classes using few training examples. The learner uses
a feature extractor to embed labeled reference and unlabeled
query images in feature space. Reference image embed-
dings are averaged within each class to generate a “proto-
type” vector for that class. Predictions are made on query
embeddings based on L2 proximity to each class prototype.
Training a prototypical network amounts to setting the
parameters of the feature extractor, since classiﬁcation is
non-parametric. The prototypical network is trained on the
representation set by sampling small datasets of reference
and query images. These are passed through the network
to get class probabilities for the query images. Cross en-
tropy loss on query images is then minimized. Through this
training, the network learns a feature extractor that produces
good prototypes from limited reference images.

4.2. Batch Folding

Batch folding is motivated by the fact that during train-
ing, every image in a batch is either a reference or a query

few images and image-level labels, distinguishing relevant
features from distractors becomes highly difﬁcult.

For these reasons, isolating the region of interest (on both
reference and query images) should make classiﬁcation sig-
niﬁcantly easier. We consider two possible approaches. In
unsupervised localization, the learner internally develops
a category-agnostic “foregroundness” model on the repre-
sentation set. Few-shot localization uses reference image
bounding boxes on the evaluation set for such localization.
Procedure:
In both approaches, the localizer is a sub-
module that classiﬁes each location in the ﬁnal 10 × 10
feature map as “foreground” or “background”. This predic-
tion is calculated as a softmax over each pixel embedding’s
negative L2 proximity to a foreground vector and to a back-
ground vector. In unsupervised localization, these vectors
are learned parameters optimized on the representation set.
In few-shot localization, the localizer gets a few reference
images annotated with bounding boxes. We use these boxes
as ﬁgure/ground masks, and average all the foreground pixel
embeddings to produce the foreground vector. The back-
ground vector is computed similarly.

The output of the localizer is a soft foreground / back-
ground mask. Multiplying the feature map with its mask
(and inverse mask) produces foreground and background
maps, which are average-pooled then concatenated. This
double-length feature vector is used to form prototypes and
perform classiﬁcation. Fig. 4 provides a visual explanation.
Training: Both localization approaches are trainable end-
to-end, so we train them within the classiﬁcation problem.
We use no additional supervisory loss; localizers are trained
only to be useful for classiﬁcation. Despite this, the outputs
are visually quite good. Examples are given in Fig. 5.

When a few-shot localizer is trained with batch fold-
ing, an additional round of folding during localization is
required. Each image’s contribution is removed from the
foreground and background vectors. Otherwise, each im-
age ‘sees’ its own ground truth bounding box during local-
ization, preventing generalization to unannotated images.

4.4. Covariance Pooling

For hard classiﬁcation problems, methods such as bilin-
ear pooling [26], ﬁsher vectors [33] and others [4, 16] can
be used to expand the feature space and increase expressive
power. Unfortunately, a traditional learning framework uses
these expanded representations as input to linear classiﬁers
or fully-connected softmax layers [9, 20, 26], dramatically
increasing parameters and making the model prone to catas-
trophic overﬁtting on small classes.

However, these techniques can be adapted to prototypi-
cal networks without any parameter increase. We use bilin-
ear pooling [26],1 which improves ﬁne-grained classiﬁca-

1 Similar techniques have been called second-order pooling [7] and co-

variance descriptors [31] in the literature.

Top-1 Accuracy

Top-5 Accuracy

Model

Softmax
Reweighted Softmax
Resampled Softmax
Transfer Learning
PN

PN+BF
PN+BF+fsL*
PN+BF+usL
PN+BF+usL+CP
PN+BF+fsL+CP*

Mean

13.35±.24
6.92±.19
1.54±.06
17.39±.24
16.07±.19
20.04±.04
26.25±.05
28.75±.13
32.74±.13
35.52±.05

Per-Class
6.55±.19
7.88±.16
.99±.02

17.61±.10
17.55±.19
20.81±.08
26.29±.04
28.39±.15
30.52±.13
31.69±.06

Mean

34.46±.30
21.94±.31
3.77±.01
41.03±.25
42.1±.21
47.86±.31
55.43±.09
57.90±.24
61.32±.14
63.76±.09

Per-Class
20.05±.30
22.53±.29
2.75±.03
40.81±.27
41.98±.18
46.57±.23
53.01±.08
55.27±.37
56.62±.16
57.33±.10

Table 1. Results on the meta-iNat benchmark, with 95% conﬁ-
dence intervals from 4 trials. PN is a prototypical network, BF is
batch folding, fsL and usL are few-shot and unsupervised local-
ization, and CP is covariance pooling. *Results are averaged over
10 runs of 4 trials, annotations randomly sampled per-run.

tion performance and generalizes many hand-designed fea-
ture descriptors such as VLAD [19], Fisher vectors [33],
and Bag-of-Visual-Words [10]. This approach takes two
feature maps (from e.g. a two-stream convolutional net-
work) and computes the cross-covariance between them,
by performing a pixel-wise outer product before average-
pooling.
In our localization models, the predicted fore-
ground and background maps act as the two streams. Other-
wise, we use the outer product of the feature map with itself.
Both versions perform signed square-root normalization, as
in bilinear pooling, but do not project to the unit sphere, as
this heavily constrains the prototype prediction space.

It is worth emphasizing that this expansion adds no pa-
rameters. Unlike prior models, all improvement in perfor-
mance comes from increased feature expressiveness, not
from increased network capacity. To emphasize this dis-
tinction, we call this version covariance pooling.

5. Experiments

We ﬁrst present overall results on the meta-iNat bench-
mark (Table 1). We analyze localizer behavior, and then
generalization to larger networks, tasks with domain shift,
and the original mini-ImageNet. We use 4-layer convo-
lutional learners closely mimicking prototypical networks
[37], plus average-pooling (see supplementary).

5.1. Meta iNat

Baseline results: Standard softmax classiﬁers trained from
scratch on the evaluation set’s reference images perform
poorly, especially on rare classes. Upweighting rare
classes during training improves the per-class accuracy only
slightly. Oversampling the rare classes causes catastrophic
overﬁtting. A second baseline is transfer learning: we train
the same network on the representation set, but replace and
re-train the ﬁnal linear layer on the evaluation set, using
class weights. This approach works signiﬁcantly better than
training from scratch, attaining 17.6% per-class accuracy.

6562

Figure 4. Few-shot localization. Provided bounding boxes mask off foreground and background regions (1), which are averaged to produce
foreground and background feature vectors (2). Pixel features on new feature maps (3) are classiﬁed as foreground or background based
on distance from those vectors (4). The predicted mask separates foreground and background regions (5), which are average pooled
independently and concatenated (6). Unsupervised localization learns the foreground/background vectors as parameters, and begins at (3).

Figure 5. Example outputs of the few-shot localizer. The leftmost
image provides the foreground and background centroids for each
row. The network learns without supervision or dedicated param-
eters to isolate (mostly) appropriate regions of interest.

As our third baseline, prototypical networks trained on
the meta-iNat representation set easily outperform the mod-
els trained from scratch, and are comparable to transfer
learning without any need for label reweighting. This
suggests that prototypical networks are inherently class-
balanced, but provide no additional advantages over transfer
learning in this heavy-tailed setting.

Batch folding: A prototypical network trained with batch
folding outperforms all baselines by almost a 3-point mar-
gin. The per-class accuracy gain as a function of class size
is plotted in Fig. 6. We see gains across the board, suggest-
ing that batch folding does provide higher-quality gradients.
At the same time, by incorporating more reference images
during training, batch folding helps models generalize to
larger classes: the positive slope of the best-ﬁt line suggests
that large classes beneﬁt more from batch folding, though
not at the expense of small ones.

Localization: Incorporating few-shot localization leads to
another signiﬁcant boost in performance, about 6 percent-
age points. Note that 10% of reference images are anno-
tated, only 1 to 20 images per category. This relatively
cheap annotation has an outsized impact on performance.

Figure 6. Batch folding improves accuracy for all class sizes in
expectation, but particularly helps with large ones (r

2 = .05)

Interestingly, unsupervised localization provides a larger
gain, about 8 percentage points. We posit that few-
shot localization underperforms its counterpart because it
uses bounding boxes, a very coarse form of segmentation.
Bounding boxes may include a signiﬁcant amount of back-
ground, hurting the separation of foreground from back-
ground. Indeed, we ﬁnd that when provided bounding boxes
are large (e.g. occupying the full image), the few-shot lo-
calizer is unable to localize correctly.

As hypothesized, localization particularly helps when
objects are small, and bounding boxes cover less than half
the image (Fig. 7). The decrease in gain for tiny objects
is not entirely surprising - classiﬁcation is inherently harder
when the relevant object contains only a few pixels.

Covariance pooling: Accuracy improves yet again with
covariance pooling, yielding a 4 point gain over unsuper-
vised localization and 9 points over few-shot localization.
Notably, covariance pooling causes class balance to break:
large categories beneﬁt disproportionately (Fig. 8). We hy-
pothesize that the high dimensionality of covariance space

6563

Localizer
Untrained
No Gradient

Jointly trained

10%
10%

Supercategory

% annotation Mean acc.
19.74±.03
22.77±.23
23.67±.79
25.85±.11
26.17±.08
26.28±.05
26.21±.04

1%
4%
16%
64%

Per-Class acc.

20.42±.06
22.86±.18
24.08±.66
25.96±.09
26.22±.06
26.3±.04
26.25±.03

Figure 7. Few-shot localization is more helpful on images with
small regions of interest

Table 2. Top-1 accuracy with 95% conﬁdence intervals for few-
shot localization models as annotations increase, with comparison
to baseline localizers. All models use batch folding.

further: categories in meta-iNat are grouped into nine super-
categories, so we also try using one box per supercategory,
nine total. Accuracy does drop signiﬁcantly, but is still bet-
ter than models that do not localize. Thus localization can
lead to real accuracy gains using hardly any annotations at
all, to our knowledge a ﬁrst-of-its-kind ﬁnding.
Joint training: Although the few-shot localizer never re-
ceives direct training supervision, it must still be learned
jointly with the classiﬁer. Table 2 also compares localizers
that are not jointly trained. Applying few-shot localization
to a network trained without it leads to a drop in perfor-
mance (“Untrained”). Training the network to use localiza-
tion, but preventing backpropagation through the localizer
itself, also leads to a drop in performance (“No Gradient”).
Localization thus provides a useful training signal, but must
itself be trained with the classiﬁer for maximum beneﬁt.

Figure 8. Covariance pooling improves performance on large
classes, at the expense of some small ones (r

2 = .50)

5.3. Generalization

is responsible. Small categories do not have enough refer-
ence images to span the space, so centroid quality suffers.

Unsupervised localization does not interact well with co-
variance pooling, perhaps because covariance space is too
high-dimensional for reference images to span during train-
ing. Thus the learned foreground and background vectors
may overﬁt to a particular manifold on the representation
set. Few-shot localization, which calculates these vectors
dynamically, does not have this problem. We conclude that
both localization techniques are useful for different settings.
Using all three techniques, top-1 accuracy is doubled
from the baseline prototypical network. The best performer
uses batch folding, few-shot localization and covariance
pooling. An ablation study is provided in supplementary.

5.2. Analyzing Few Shot Localizer Behavior

We next evaluate the number of bounding box annota-
tions needed for good classiﬁcation accuracy. As shown in
Table 2, performance saturates at 16% bounding box avail-
ability, but even at 1% (amounting to one box per class),
performance decreases only slightly. This scarcity can go

We evaluate our models on three new settings. To test
generalization over domain shift, we create a second split of
meta-iNat based on supercategories. To test generalization
to other network architectures, we evaluate our techniques
on meta-iNat using more powerful, pretrained ResNet ar-
chitectures. Finally, these techniques are tested on mini-
ImageNet, using evaluation methods from prior literature.
With some expected caveats for mini-ImageNet, our results
generalize extremely well to all settings.
Supercategory meta-iNat: We wish to evaluate our results
in settings where transfer learning is more difﬁcult, and
switching from the representation set to the evaluation set
involves substantial domain shift. To that end we construct
a new version of meta-iNat, which we call Supercategory
meta-iNat. Rather than assign categories to the representa-
tion and evaluation sets randomly, we instead split by super-
category. Insects and arachnids (354 total) form the evalua-
tion set, and everything else (birds, ﬁsh, mammals, reptiles,
etc.) is the representation set. Training and evaluation are
performed as before, with results in Table 3.

Transfer learning on Supercategory meta-iNat is much
harder than in the original setting. Scores are uniformly
lower across the board. However, overall trends remain ex-

6564

Top-1 Accuracy

Top-5 Accuracy

Model

Reweighted Softmax
Transfer Learning
PN

PN+BF
PN+BF+fsL*
PN+BF+usL
PN+BF+usL+CP
PN+BF+fsL+CP*

Mean

4.59±.21
6.34±.23
5.33±.18
7.29±.11
11.69±.06
12.46±.59
17.65±.21
20.02±.13

Per-Class
5.38±.22
6.19±.14
6.31±.18
8.24±.13
12.38±.07
12.95±.51
16.72±.18
17.32±.09

Mean

15.95±.58
18.89±.48
17.41±.45
22.09±.35
30.64±.11
32.28±1.1
40.16±.26
43.45±.20

Per-Class
16.57±.53
17.86±.49
18.32±.27
22.53±.37
29.86±.09
31.18±.95
36.19±.48
36.65±.15

Table 3. Results on the Supercategory meta-iNat benchmark, with
95% conﬁdence intervals. Models are as in Table 1.

Top-1 Accuracy

Top-5 Accuracy

Model

Transfer Learn (top)
Transfer Learn (full)
PN

PN+BF
PN+BF+fsL*
PN+BF+fsL+CP*

Mean

19.27±.17
22.52±.58
35.35±.24
37.36±.15
46.2±.04
51.25±.13

Per-Class
18.72±.20
18.22±.40
35.59±.11
36.73±.12
44.43±.08
46.04±.13

Mean

44.02±.30
48.16±.60
67.82±.13
69.25±.16
75.87±.04
77.5±.06

Per-Class
41.2±.36
40.38±.48
66.33±.19
67.03±.15
73.26±.06
72.14±.05

Table 4. Results on meta-iNat using ResNet50 features, with 95%
conﬁdence intervals. Transfer Learning (top) adjusts unfrozen up-
per layers on reference images, while (full) ﬁne-tunes the entire
network. Other models are as in Table 1.

actly the same. Batch folding outperforms standard proto-
typical networks and transfer learning baselines by 2 points.
Few-shot and unsupervised localization lead to similar, sub-
stantial accuracy gains (4 points). Covariance pooling also
improves (5 points), but again causes mean accuracy to out-
strip per-class accuracy. Unsupervised localization under-
performs few-shot localization when using covariance pool-
ing, so we remove it from future tests.
ResNet-50: While batch folding, few-shot localization,
and covariance pooling lead to substantial improvement on
meta-iNat, accuracy is still low. For more powerful mod-
els, these improvements might disappear. To test this, we
replace the bottom two prototypical network layers with a
frozen ResNet-50 pretrained on ImageNet. Details can be
found in supplementary. Results are presented in Table 4.

Using the pretrained ResNet-50 model, it is possible to
perform transfer learning directly from ImageNet to the
meta-iNat evaluation set. Freezing the ResNet, and training
just the top two layers on reference images, works poorly
given the power of the model. Fine-tuning the entire net-
work on reference images works slightly better, but de-
creases per-class accuracy. Freezing the ResNet and train-
ing the top layers as a prototypical network improves top-
1 accuracy by 13 percentage points. Batch folding, few-
shot localization, and covariance pooling provide another
16 points. We conclude that these techniques are helpful
for large neural architectures as well as small ones.
Mini-ImageNet: Batch folding, few-shot localization, and
covariance pooling improve accuracy on large evaluation
sets with long-tailed class distributions. To see if these tech-

Model

PN
PN+BF
PN+fsL
PN+fsL+CP

5-shot Accuracy

1-shot Accuracy

65.76±.29
65.2±.29
67.85±.29
69.45±.28

49.97±.30
47.67±.31

51.1±.3

49.64±.31

Table 5. Five-class accuracy on mini-ImageNet with 95% conﬁ-
dence intervals over 10 dataset passes. “Shot” refers to number of
reference images. Models are as in Table 1.

Figure 9. Batch folding causes overﬁtting on the smaller mini-
ImageNet representation set. Models are trained on twenty cat-
egories but tested on ﬁve, so test loss is lower than training loss.

niques still help with the original, smaller few-shot learning
problem, we construct a mock mini-ImageNet dataset with
similar statistics but annotated with bounding boxes. Per-
formance of prototypical networks on our dataset is similar
to the published ﬁgures [37]. Table 5 shows the results.

An immediate departure from prior results is the fact that
batch folding hurts performance. Batch folding does indeed
result in better training and lower training loss, but overﬁts
because the representation set is smaller (Fig. 9).

Few-shot

localization and covariance pooling make
modest but real improvements when ﬁve reference images
are provided (“ﬁve shot”). There is little discernible effect
on single-reference (“one-shot”) performance, perhaps be-
cause with few reference images and a small set of classes, a
more expressive feature space is unnecessary. Nevertheless,
the small improvements do suggest that few-shot localiza-
tion and covariance pooling generalize to few-shot learning.

6. Conclusion

In this paper, we have shown that past work on clas-
sical or few-shot balanced benchmarks fails to generalize
to realistic heavy-tailed classiﬁcation problems. We show
that parameter-free localization from limited bounding box
annotations, and improvements to training and representa-
tion, provide large gains beyond those previously observed
in data abundant settings. Ours is but a ﬁrst step addressing
broader questions of class balance and data scarcity.
Acknowledgements This work was partly funded by a
grant from Aricent.

6565

References

[17] M. Guillaumin, D. K¨uttel, and V. Ferrari.

[1] Broadcasting - numpy v1.15 manual.

https:

//docs.scipy.org/doc/numpy-1.15.0/user/
basics.broadcasting.html. Accessed: 2018-11-
16.

[2] Broadcasting semantics - pytorch master documentation.

https://pytorch.org/docs/stable/notes/
broadcasting.html. Accessed: 2018-11-16.

[3] Broadcasting semantics — xla — tensorﬂow. https://
www.tensorflow.org/xla/broadcasting. Ac-
cessed: 2018-11-16.

[4] D. Acharya, Z. Huang, D. Pani Paudel, and L. Van Gool. Co-
variance pooling for facial expression recognition. In IEEE
Conference on Computer Vision and Pattern Recognition,
June 2018.

[5] S. Bell, P. Upchurch, N. Snavely, and K. Bala. Material
recognition in the wild with the materials in context database.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2015.

[6] L. Bertinetto, J. a. F. Henriques, J. Valmadre, P. H. S. Torr,
and A. Vedaldi. Learning feed-forward one-shot learners. In
Neural Information Processing Systems, 2016.

[7] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu.
Semantic segmentation with second-order pooling.
In Eu-
ropean Conference on Computer Vision, pages 430–443.
Springer, 2012.

[8] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy in
deep networks with circulant projections. In IEEE Interna-
tional Conference on Computer Vision, 2015.

[9] A. R. Chowdhury, T. Lin, S. Maji, and E. Learned-Miller.
One-to-many face recognition with bilinear cnns. In IEEE
Winter Conference on Applications of Computer Vision
(WACV), March 2016.

[10] G. Csurka, C. R. Dance, L. Fan, J. Willamowski, and
In

C. Bray. Visual categorization with bags of keypoints.
European Conference on Computer Vision, 2004.

[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2009.

[12] H. Edwards and A. Storkey. Towards a neural statistician.
In International Conference on Learning Representations,
2017.

[13] M. Fink. Object classiﬁcation from a single example utiliz-
ing class relevance metrics. In Neural Information Process-
ing Systems, 2004.

[14] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In Interna-

learning for fast adaptation of deep networks.
tional Conference on Machine Learning, 2017.

[15] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In Interna-

learning for fast adaptation of deep networks.
tional Conference on Machine Learning, 2017.

[16] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact
bilinear pooling. In IEEE Conference on Computer Vision
and Pattern Recognition, June 2016.

annotation with segmentation propagation.
Journal of Computer Vision, 110(3):328–348, Dec 2014.

Imagenet auto-
International

[18] B. Hariharan and R. Girshick. Low-shot learning by shrink-
ing and hallucinating features. In IEEE International Con-
ference on Computer Vision, 2017.

[19] H. Jgou, M. Douze, C. Schmid, and P. Prez. Aggregating
local descriptors into a compact image representation.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, June 2010.

[20] J. Kim, K. W. On, W. Lim, J. Kim, J. Ha, and B. Zhang.
Interna-

Hadamard product for low-rank bilinear pooling.
tional Conference on Learning Representations, 2017.

[21] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural

networks for one-shot image recognition. 2015.

[22] A. Krizhevsky. Learning multiple layers of features from tiny
images. Master’s thesis, Department of Computer Science,
University of Toronto, 2009.

[23] D. Kuettel, M. Guillaumin, and V. Ferrari. Segmentation
propagation in imagenet.
In A. Fitzgibbon, S. Lazebnik,
P. Perona, Y. Sato, and C. Schmid, editors, European Con-
ference on Computer Vision, 2012.

[24] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-
level concept learning through probabilistic program induc-
tion. Science, 2015.

[25] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Gir-
shick, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L.
Zitnick. Microsoft COCO: common objects in context. In
European Conference on Computer Vision, 2014.

[26] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-

els for ﬁne-grained visual recognition. 2017.

[27] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion:
Powering robust clothes recognition and retrieval with rich
annotations. In IEEE Conference on Computer Vision and
Pattern Recognition, 2016.

[28] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A sim-
In International Confer-

ple neural attentive meta-learner.
ence on Learning Representations, 2018.

[29] T. Munkhdalai and H. Yu. Meta networks. In International

Conference on Machine Learning, 2017.

[30] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object local-
ization for free? - weakly-supervised learning with convo-
lutional neural networks. In IEEE Conference on Computer
Vision and Pattern Recognition, June 2015.

[31] N. Otberdout, A. Kacem, M. Daoudi, L. Ballihi, and
S. Berretti. Deep covariance descriptors for facial expression
recognition. In British Machine Vision Conference, 2018.

[32] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Fer-
rari. Extreme clicking for efﬁcient object annotation. 2017.
Improving the
In Euro-

[33] F. Perronnin, J. S´anchez, and T. Mensink.

ﬁsher kernel for large-scale image classiﬁcation.
pean Conference on Computer Vision, 2010.

[34] S. Ravi and H. Larochelle. Optimization as a model for few-
shot learning. In International Conference on Learning Rep-
resentations, 2017.

[35] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. Lillicrap. Meta-learning with memory-augmented neural

6566

networks. In International Conference on Machine Learn-
ing, 2016.

[36] A. Shaban, S. Bansal, Z. Liu, I. Essa, and B. Boots. One-
shot learning for semantic segmentation. In British Machine
Vision Conference, 2017.

[37] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks
for few-shot learning. In Neural Information Processing Sys-
tems, 2017.

[38] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M.
Hospedales. Learning to compare: Relation network for few-
shot learning. In IEEE Conference on Computer Vision and
Pattern Recognition, 2018.

[39] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun,
A. Shepard, H. Adam, P. Perona, and S. Belongie. The inat-
uralist species classiﬁcation and detection dataset. In IEEE
Conference on Computer Vision and Pattern Recognition,
2018.

[40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
Neural Information Processing Systems, 2016.

[41] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,
X. Wang, and X. Tang. Residual attention network for image
classiﬁcation. In IEEE Conference on Computer Vision and
Pattern Recognition, July 2017.

[42] Y. Wang, D. K. Ramanan, and M. Hebert. Learning to model
the tail. In Neural Information Processing Systems, Decem-
ber 2017.

[43] Y.-X. Wang and M. Hebert. Learning to learn: Model regres-
sion networks for easy small sample learning. In European
Conference on Computer Vision, 2016.

[44] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
Sun database: Large-scale scene recognition from abbey to
zoo. In IEEE Conference on Computer Vision and Pattern
Recognition, 2010.

[45] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang.
The application of two-level attention models in deep convo-
lutional neural network for ﬁne-grained image classiﬁcation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, June 2015.

[46] Y. xiong Wang, R. Girshick, M. Herbert, and B. Hariharan.
Low-shot learning from imaginary data. In IEEE Conference
on Computer Vision and Pattern Recognition, 2018.

[47] Z. Yang, M. Moczulski, M. Denil, N. d. Freitas, A. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, 2015.
[48] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, June 2016.

[49] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Neural Informa-
tion Processing Systems, pages 487–495. Curran Associates,
Inc., 2014.

6567

