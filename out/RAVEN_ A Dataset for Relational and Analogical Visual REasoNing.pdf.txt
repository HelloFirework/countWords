RAVEN: A Dataset for Relational and Analogical Visual rEasoNing

Chi Zhang⋆,1,2

Feng Gao⋆,1,2

Baoxiong Jia1

Yixin Zhu1,2

Song-Chun Zhu1,2

1 UCLA Center for Vision, Cognition, Learning and Autonomy

2 International Center for AI and Robot Autonomy (CARA)

{chi.zhang,f.gao,baoxiongjia,yixin.zhu}@ucla.edu, sczhu@stat.ucla.edu

Abstract

(a)

 

x
i
r
t
a
M
m
e
b
o
r
P

l

(b)

Center

?

(c)

Dramatic progress has been witnessed in basic vision
tasks involving low-level perception, such as object recog-
nition, detection, and tracking. Unfortunately, there is still
an enormous performance gap between artiﬁcial vision sys-
tems and human intelligence in terms of higher-level vi-
sion problems, especially ones involving reasoning. Earlier
attempts in equipping machines with high-level reasoning
have hovered around Visual Question Answering (VQA),
one typical task associating vision and language under-
standing. In this work, we propose a new dataset, built in the
context of Raven’s Progressive Matrices (RPM) and aimed
at lifting machine intelligence by associating vision with
structural, relational, and analogical reasoning in a hierar-
chical representation. Unlike previous works in measuring
abstract reasoning using RPM, we establish a semantic link
between vision and reasoning by providing structure repre-
sentation. This addition enables a new type of abstract rea-
soning by jointly operating on the structure representation.
Machine reasoning ability using modern computer vision is
evaluated in this newly proposed dataset. Additionally, we
also provide human performance as a reference. Finally, we
show consistent improvement across all models by incorpo-
rating a simple neural module that combines visual under-
standing and structure reasoning.

1. Introduction

The study of vision must therefore include not
only the study of how to extract from images . . . ,
but also an inquiry into the nature of the internal
representations by which we capture this infor-
mation and thus make it available as a basis for
decisions about our thoughts and actions.

— David Marr, 1982 [35]

Computer vision has a wide spectrum of tasks. Some
computer vision problems are clearly purely visual, “cap-

⋆ indicates equal contribution.

t
e
S
 
r
e
w
s
n
A

1

5

2

6

3

4

7

8

Figure 1. (a) An example RPM. One is asked to select an image
that best completes the problem matrix, following the structural
and analogical relations. Each image has an underlying structure.
(b) Speciﬁcally in this problem, it is an inside-outside structure
in which the outside component is a layout with a single centered
object and the inside component is a 2 × 2 grid layout. Details in
Figure 2. (c) lists the rules for (a). The compositional nature of the
rules makes this problem a difﬁcult one. The correct answer is 7.

turing” the visual information process; for instance, ﬁlters
in early vision [5], primal sketch [13] as the intermediate
representation, and Gestalt laws [24] as the perceptual orga-
nization. In contrast, some other vision problems have triv-
ialized requirements for perceiving the image, but engage
more generalized problem-solving in terms of relational
and/or analogical visual reasoning [16]. In such cases, the
vision component becomes the “basis for decisions about
our thoughts and actions”.

Currently, the majority of the computer vision tasks fo-
cus on “capturing” the visual information process; few lines
of work focus on the later part—the relational and/or ana-
logical visual reasoning. One existing line of work in equip-
ping artiﬁcial systems with reasoning ability hovers around
Visual Question Answering (VQA) [2, 22, 48, 58, 62].
However, the reasoning skills required in VQA lie only
at the periphery of the cognitive ability test circle [7]. To

5317

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
push the limit of computer vision or more broadly speak-
ing, Artiﬁcial Intelligence (AI), towards the center of cog-
nitive ability test circle, we need a test originally designed
for measuring human’s intelligence to challenge, debug, and
improve the current artiﬁcial systems.

A surprisingly effective ability test of human visual rea-
soning has been developed and identiﬁed as the Raven’s
Progressive Matrices (RPM) [28, 47, 52], which is widely
accepted and believed to be highly correlated with real intel-
ligence [7]. Unlike VQA, RPM lies directly at the center of
human intelligence [7], is diagnostic of abstract and struc-
tural reasoning ability [9], and characterizes the deﬁning
feature of high-level intelligence, i.e., ﬂuid intelligence [21].
Figure 1 shows an example of RPM problem together
with its structure representation. Provided two rows of ﬁg-
ures consisting of visually simple elements, one must efﬁ-
ciently derive the correct image structure (Figure 1(b)) and
the underlying rules (Figure 1(c)) to jointly reason about
a candidate image that best completes the problem matrix.
In terms of levels of reasoning required, RPM is arguably
harder compared to VQA:
• Unlike VQA where natural language questions usually
imply what to pay attention to in the image, RPM relies
merely on visual clues provided in the matrix and the cor-
respondence problem itself, i.e., ﬁnding the correct level
of attributes to encode, is already a major factor distin-
guishing populations of different intelligence [7].

• While VQA only requires spatial and semantic under-
standing, RPM needs joint spatial-temporal reasoning in
the problem matrix and the answer set. The limit of short-
term memory, the ability of analogy, and the discovery of
the structure have to be taken into consideration.

• Structures in RPM make the compositions of rules much
more complicated. Unlike VQA whose questions only
encode relatively simple ﬁrst-order reasoning, RPM usu-
ally includes more sophisticated logic, even with recur-
sions. By composing different rules at various levels, the
reasoning progress can be extremely difﬁcult.
To push the limit of current vision systems’ reasoning
ability, we generate a new dataset to promote further re-
search in this area. We refer to this dataset as the Rela-
tional and Analogical Visual rEasoNing dataset (RAVEN)
in homage to John Raven for the pioneering work in the
creation of the original RPM [47]. In summary:
• RAVEN consists of 1, 120, 000 images and 70, 000 RPM
problems, equally distributed in 7 distinct ﬁgure conﬁgu-
rations.

• Each problem has 16 tree-structure annotations, totaling

up to 1, 120, 000 structural labels in the entire dataset.

• We design 5 rule-governing attributes and 2 noise at-
tributes. Each rule-governing attribute goes over one of 4
rules, and objects in the same component share the same
set of rules, making in total 440, 000 rule annotations and
an average of 6.29 rules per problem.
The RAVEN dataset is designed inherently to be light

in visual recognition and heavy in reasoning. Each image
only contains a limited set of simple gray-scale objects with
clear-cut boundaries and no occlusion. In the meantime,
rules are applied row-wise, and there could be one rule for
each attribute, attacking visual systems’ major weaknesses
in short-term memory and compositional reasoning [22].

An obvious paradox is: in this innately compositional
and structured RPM problem, no annotations of structures
are available in previous works (e.g., [3, 55]). Hence, we set
out to establish a semantic link between visual reasoning
and structure reasoning in RPM. We ground each problem
instance to a sentence derived from an Attributed Stochas-
tic Image Grammar (A-SIG) [12, 30, 43, 56, 60, 61] and
decompose the data generation process into two stages: the
ﬁrst stage samples a sentence from a pre-deﬁned A-SIG
and the second stage renders an image based on the sen-
tence. This structured design makes the dataset very diverse
and easily extendable, enabling generalization tests in dif-
ferent ﬁgure conﬁgurations. More importantly, the data gen-
eration pipeline naturally provides us with abundant dense
annotations, especially the structure in the image space.
This semantic link between vision and structure represen-
tation opens new possibilities by breaking down the prob-
lem into image understanding and tree- or graph-level rea-
soning [26, 53]. As shown in Section 6, we empirically
demonstrate that models with a simple structure reasoning
module to incorporate both vision-level understanding and
structure-level reasoning would notably improve their per-
formance in RPM.

The organization of the paper is as follows. In Section 2,
we discuss related work in visual reasoning and computa-
tional efforts in RPM. Section 3 is devoted to a detailed
description of the RAVEN dataset generation process, with
Section 4 benchmarking human performance and compar-
ing RAVEN with a previous RPM dataset. In Section 5,
we propose a simple extension to existing models that in-
corporates vision understanding and structure reasoning.
All baseline models and the proposed extensions are evalu-
ated in Section 6. The notable gap between human subjects
(84%) and vision systems (59%) calls for further research
into this problem. We hope RAVEN could contribute to the
long-standing effort in human-level reasoning AI.

2. Related Work

Visual Reasoning Early attempts were made in 1940s-
1970s in the ﬁeld of logic-based AI. Newell argued that one
of the potential solutions to AI was “to construct a single
program that would take a standard intelligence test” [42].
There are two important trials: (i) Evans presented an AI
algorithm that solved a type of geometric analogy tasks in
the Wechsler Adult Intelligence Scale (WAIS) test [10, 11],
and (ii) Simon and Kotovsky devised a program that solved
Thurstone letter series completion problems [54]. However,
these early attempts were heuristic-based with hand-crafted
rules, making it difﬁcult to apply to other problems.

5318

(a)

Rules

(b)

Center

Center

(c)

…

…

…

…

…

…

…

…

(d)

(e)

Scene

Structure

Component

Layout

Entity

Noise Attributes

Modify constrained attributes to generate an answer set 

Figure 2. RAVEN creation process. A graphical illustration of the grammar production rules used in A-SIG is shown in (b). Note that
Layout and Entity have associated attributes (c). Given a randomly sampled rule combination (a), we ﬁrst prune the grammar tree (the
transparent branch is pruned). We then sample an image structure together with the values of the attributes from (b), denoted by black, and
apply the rule set (a) to generate a single row. Repeating the process three times yields the entire problem matrix in (d). (e) Finally, we
sample constrained attributes and vary them in the correct answer to break the rules and obtain the candidate answer set.

The reasoning ability of modern vision systems was
ﬁrst systematically analyzed in the CLEVR dataset [22].
By carefully controlling inductive bias and slicing the vi-
sion systems’ reasoning ability into several axes, Johnson
et al. successfully identiﬁed major drawbacks of existing
models. A subsequent work [23] on this dataset achieved
good performance by introducing a program generator in a
structured space and combining it with a program execu-
tion engine. A similar work that also leveraged language-
guided structured reasoning was proposed in [18]. Mod-
ules with special attention mechanism were latter proposed
in an end-to-end manner to solve this visual reasoning
task [19, 49, 59]. However, superior performance gain was
observed in very recent works [6, 36, 58] that fell back to
structured representations by using primitives, dependency
trees, or logic. These works also inspire us to incorporate
structure information into solving the RPM problem.

More generally, Bisk et al. [4] studied visual reasoning
in a 3D block world. Perez et al. [46] introduced a condi-
tional layer for visual reasoning. Aditya et al. [1] proposed
a probabilistic soft logic in an attention module to increase
model interpretability. And Barrett et al. [3] measured ab-
stract reasoning in neural networks.

Computational Efforts in RPM The research com-
munity of cognitive science has tried to attack the prob-
lem of RPM with computational models earlier than the
computer science community. However, an oversimpliﬁed
assumption was usually made in the experiments that the
computer programs had access to a symbolic representation
of the image and the operations of rules [7, 32, 33, 34].
As reported in Section 4.4, we show that giving this crit-
ical information essentially turns it into a searching prob-

lem. Combining it with a simple heuristics provides us an
optimal solver, easily surpassing human performance. An-
other stream of AI research [31, 37, 38, 39, 50] tries to solve
RPM by various measurements of image similarity. To pro-
mote fair comparison between computer programs and hu-
man subjects in a data-driven manner, Wang and Su [55]
ﬁrst proposed a systematic way of automatically generating
RPM using ﬁrst-order logic. Barrett et al. [3] extended their
work and introduced the Procedurally Generating Matri-
ces (PGM) dataset by instantiating each rule with a relation-
object-attribute tuple. Hoshen and Werman [17] ﬁrst trained
a CNN to complete the rows in a simplistic evaluation en-
vironment, while Barrett et al. [3] used an advanced Wild
Relational Network (WReN) and studied its generalization.

3. Creating RAVEN

Our work is built on prior work aforementioned. We im-
plement all relations in Advanced Raven’s Progressive Ma-
trices identiﬁed by Carpenter et al. [7] and generate the an-
swer set following the monotonicity of RPM’s constraints
proposed by Wang and Su [55].

Figure 2 shows the major components of the generation
process. Speciﬁcally, we use the A-SIG as the representa-
tion of RPM; each RPM is a parse tree that instantiates from
the A-SIG. After rules are sampled, we prune the grammar
to make sure the relations could be applied on any sentence
sampled from it. We then sample a sentence from the pruned
grammar, where rules are applied to produce a valid row.
Repeating such a process three times yields a problem ma-
trix. To generate the answer set, we modify attributes on
the correct answer such that the relationships are broken.

5319

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
v
n
n
R
d
2
R
4
g
z
8
x
X
U
x
0
W
q
9
q
5
n
K
T
C
a
k
=
"
>
A
A
A
B
9
X
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
E
V
o
K
Q
5
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
k
x
0
f
m
y
T
k
4
5
n
6
2
7
N
R
B
Z
+
Q
8
a
C
h
C
i
5
V
/
o
+
B
s
u
i
Q
t
I
G
G
m
l
0
c
y
u
d
n
e
C
R
A
q
D
r
v
v
t
r
K
y
u
r
W
9
s
F
r
a
K
2
z
u
7
e
/
u
l
g
8
O
m
i
V
P
N
o
c
F
j
G
e
t
2
w
A
x
I
o
a
C
B
A
i
W
0
E
w
0
s
C
i
S
0
g
t
H
V
1
G
8
9
g
D
Y
i
V
n
c
4
T
s
C
P
2
E
C
J
U
H
C
G
V
r
r
v
I
j
w
h
Y
n
b
L
Q
c
G
k
V
y
q
7
F
X
c
G
u
k
y
8
n
J
R
J
j
n
q
v
9
N
X
t
x
z
y
N
Q
C
G
X
z
J
i
O
5
y
b
o
Z
0
y
j
4
B
I
m
x
W
5
q
I
G
F
8
x
A
b
Q
s
V
S
x
C
I
y
f
z
a
6
e
0
F
O
r
9
G
k
Y
a
1
s
K
6
U
z
9
P
Z
G
x
y
J
h
x
F
N
j
O
i
O
H
Q
L
H
p
T
8
T
+
v
k
2
J
4
4
W
d
C
J
S
m
C
4
v
N
F
Y
S
o
p
x
n
Q
a
A
e
0
L
D
R
z
l
2
B
L
G
t
b
C
3
U
j
5
k
m
n
G
0
Q
R
V
t
C
N
7
i
y
8
u
k
W
a
1
4
b
s
W
7
q
Z
Z
r
l
3
k
c
B
X
J
M
T
s
g
Z
8
c
g
5
q
Z
F
r
U
i
c
N
w
o
k
m
z
+
S
V
v
D
m
P
z
o
v
z
7
n
z
M
W
1
e
c
f
O
a
I
/
I
H
z
+
Q
M
1
S
p
L
z
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
v
n
n
R
d
2
R
4
g
z
8
x
X
U
x
0
W
q
9
q
5
n
K
T
C
a
k
=
"
>
A
A
A
B
9
X
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
E
V
o
K
Q
5
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
k
x
0
f
m
y
T
k
4
5
n
6
2
7
N
R
B
Z
+
Q
8
a
C
h
C
i
5
V
/
o
+
B
s
u
i
Q
t
I
G
G
m
l
0
c
y
u
d
n
e
C
R
A
q
D
r
v
v
t
r
K
y
u
r
W
9
s
F
r
a
K
2
z
u
7
e
/
u
l
g
8
O
m
i
V
P
N
o
c
F
j
G
e
t
2
w
A
x
I
o
a
C
B
A
i
W
0
E
w
0
s
C
i
S
0
g
t
H
V
1
G
8
9
g
D
Y
i
V
n
c
4
T
s
C
P
2
E
C
J
U
H
C
G
V
r
r
v
I
j
w
h
Y
n
b
L
Q
c
G
k
V
y
q
7
F
X
c
G
u
k
y
8
n
J
R
J
j
n
q
v
9
N
X
t
x
z
y
N
Q
C
G
X
z
J
i
O
5
y
b
o
Z
0
y
j
4
B
I
m
x
W
5
q
I
G
F
8
x
A
b
Q
s
V
S
x
C
I
y
f
z
a
6
e
0
F
O
r
9
G
k
Y
a
1
s
K
6
U
z
9
P
Z
G
x
y
J
h
x
F
N
j
O
i
O
H
Q
L
H
p
T
8
T
+
v
k
2
J
4
4
W
d
C
J
S
m
C
4
v
N
F
Y
S
o
p
x
n
Q
a
A
e
0
L
D
R
z
l
2
B
L
G
t
b
C
3
U
j
5
k
m
n
G
0
Q
R
V
t
C
N
7
i
y
8
u
k
W
a
1
4
b
s
W
7
q
Z
Z
r
l
3
k
c
B
X
J
M
T
s
g
Z
8
c
g
5
q
Z
F
r
U
i
c
N
w
o
k
m
z
+
S
V
v
D
m
P
z
o
v
z
7
n
z
M
W
1
e
c
f
O
a
I
/
I
H
z
+
Q
M
1
S
p
L
z
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
v
n
n
R
d
2
R
4
g
z
8
x
X
U
x
0
W
q
9
q
5
n
K
T
C
a
k
=
"
>
A
A
A
B
9
X
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
E
V
o
K
Q
5
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
k
x
0
f
m
y
T
k
4
5
n
6
2
7
N
R
B
Z
+
Q
8
a
C
h
C
i
5
V
/
o
+
B
s
u
i
Q
t
I
G
G
m
l
0
c
y
u
d
n
e
C
R
A
q
D
r
v
v
t
r
K
y
u
r
W
9
s
F
r
a
K
2
z
u
7
e
/
u
l
g
8
O
m
i
V
P
N
o
c
F
j
G
e
t
2
w
A
x
I
o
a
C
B
A
i
W
0
E
w
0
s
C
i
S
0
g
t
H
V
1
G
8
9
g
D
Y
i
V
n
c
4
T
s
C
P
2
E
C
J
U
H
C
G
V
r
r
v
I
j
w
h
Y
n
b
L
Q
c
G
k
V
y
q
7
F
X
c
G
u
k
y
8
n
J
R
J
j
n
q
v
9
N
X
t
x
z
y
N
Q
C
G
X
z
J
i
O
5
y
b
o
Z
0
y
j
4
B
I
m
x
W
5
q
I
G
F
8
x
A
b
Q
s
V
S
x
C
I
y
f
z
a
6
e
0
F
O
r
9
G
k
Y
a
1
s
K
6
U
z
9
P
Z
G
x
y
J
h
x
F
N
j
O
i
O
H
Q
L
H
p
T
8
T
+
v
k
2
J
4
4
W
d
C
J
S
m
C
4
v
N
F
Y
S
o
p
x
n
Q
a
A
e
0
L
D
R
z
l
2
B
L
G
t
b
C
3
U
j
5
k
m
n
G
0
Q
R
V
t
C
N
7
i
y
8
u
k
W
a
1
4
b
s
W
7
q
Z
Z
r
l
3
k
c
B
X
J
M
T
s
g
Z
8
c
g
5
q
Z
F
r
U
i
c
N
w
o
k
m
z
+
S
V
v
D
m
P
z
o
v
z
7
n
z
M
W
1
e
c
f
O
a
I
/
I
H
z
+
Q
M
1
S
p
L
z
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
v
n
n
R
d
2
R
4
g
z
8
x
X
U
x
0
W
q
9
q
5
n
K
T
C
a
k
=
"
>
A
A
A
B
9
X
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
E
V
o
K
Q
5
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
k
x
0
f
m
y
T
k
4
5
n
6
2
7
N
R
B
Z
+
Q
8
a
C
h
C
i
5
V
/
o
+
B
s
u
i
Q
t
I
G
G
m
l
0
c
y
u
d
n
e
C
R
A
q
D
r
v
v
t
r
K
y
u
r
W
9
s
F
r
a
K
2
z
u
7
e
/
u
l
g
8
O
m
i
V
P
N
o
c
F
j
G
e
t
2
w
A
x
I
o
a
C
B
A
i
W
0
E
w
0
s
C
i
S
0
g
t
H
V
1
G
8
9
g
D
Y
i
V
n
c
4
T
s
C
P
2
E
C
J
U
H
C
G
V
r
r
v
I
j
w
h
Y
n
b
L
Q
c
G
k
V
y
q
7
F
X
c
G
u
k
y
8
n
J
R
J
j
n
q
v
9
N
X
t
x
z
y
N
Q
C
G
X
z
J
i
O
5
y
b
o
Z
0
y
j
4
B
I
m
x
W
5
q
I
G
F
8
x
A
b
Q
s
V
S
x
C
I
y
f
z
a
6
e
0
F
O
r
9
G
k
Y
a
1
s
K
6
U
z
9
P
Z
G
x
y
J
h
x
F
N
j
O
i
O
H
Q
L
H
p
T
8
T
+
v
k
2
J
4
4
W
d
C
J
S
m
C
4
v
N
F
Y
S
o
p
x
n
Q
a
A
e
0
L
D
R
z
l
2
B
L
G
t
b
C
3
U
j
5
k
m
n
G
0
Q
R
V
t
C
N
7
i
y
8
u
k
W
a
1
4
b
s
W
7
q
Z
Z
r
l
3
k
c
B
X
J
M
T
s
g
Z
8
c
g
5
q
Z
F
r
U
i
c
N
w
o
k
m
z
+
S
V
v
D
m
P
z
o
v
z
7
n
z
M
W
1
e
c
f
O
a
I
/
I
H
z
+
Q
M
1
S
p
L
z
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
a
d
/
a
n
D
l
Q
6
U
U
S
8
3
m
w
0
O
c
/
O
x
6
u
F
w
=
"
>
A
A
A
B
+
3
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
F
l
Q
k
l
j
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
m
x
o
v
N
l
n
Z
x
y
f
u
h
u
D
y
W
y
/
C
s
0
F
C
B
E
y
4
/
Q
8
T
d
c
E
h
e
Q
M
N
J
K
o
5
l
d
7
e
4
E
q
e
A
K
X
f
f
b
2
t
j
c
2
t
7
Z
L
e
2
V
9
w
8
O
j
4
7
t
k
0
p
b
J
V
o
y
a
L
F
E
J
L
I
b
U
A
W
C
x
9
B
C
j
g
K
6
q
Q
Q
a
B
Q
I
6
w
e
R
2
7
n
e
e
Q
C
q
e
x
I
8
4
S
8
G
P
6
C
j
m
I
W
c
U
j
T
S
w
K
3
2
E
K
S
J
m
D
y
g
1
Q
y
0
h
H
9
h
V
t
+
Y
u
4
K
w
T
r
y
B
V
U
q
A
5
s
L
/
6
w
4
T
p
C
G
J
k
g
i
r
V
8
9
w
U
/
Y
x
K
5
E
x
A
X
u
5
r
B
S
l
l
E
z
q
C
n
q
E
x
j
U
D
5
2
e
L
2
3
L
k
w
y
t
A
J
E
2
k
q
R
m
e
h
/
p
7
I
a
K
T
U
L
A
p
M
Z
0
R
x
r
F
a
9
u
f
i
f
1
9
M
Y
X
v
s
Z
j
1
O
N
E
L
P
l
o
l
A
L
B
x
N
n
H
o
Q
z
5
B
I
Y
i
p
k
h
l
E
l
u
b
n
X
Y
m
E
r
K
0
M
R
V
N
i
F
4
q
y
+
v
k
3
a
9
5
r
k
1
7
7
5
e
b
d
w
U
c
Z
T
I
G
T
k
n
l
8
Q
j
V
6
R
B
7
k
i
T
t
A
g
j
U
/
J
M
X
s
m
b
l
V
s
v
1
r
v
1
s
W
z
d
s
I
q
Z
U
/
I
H
1
u
c
P
O
O
u
V
L
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
a
d
/
a
n
D
l
Q
6
U
U
S
8
3
m
w
0
O
c
/
O
x
6
u
F
w
=
"
>
A
A
A
B
+
3
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
F
l
Q
k
l
j
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
m
x
o
v
N
l
n
Z
x
y
f
u
h
u
D
y
W
y
/
C
s
0
F
C
B
E
y
4
/
Q
8
T
d
c
E
h
e
Q
M
N
J
K
o
5
l
d
7
e
4
E
q
e
A
K
X
f
f
b
2
t
j
c
2
t
7
Z
L
e
2
V
9
w
8
O
j
4
7
t
k
0
p
b
J
V
o
y
a
L
F
E
J
L
I
b
U
A
W
C
x
9
B
C
j
g
K
6
q
Q
Q
a
B
Q
I
6
w
e
R
2
7
n
e
e
Q
C
q
e
x
I
8
4
S
8
G
P
6
C
j
m
I
W
c
U
j
T
S
w
K
3
2
E
K
S
J
m
D
y
g
1
Q
y
0
h
H
9
h
V
t
+
Y
u
4
K
w
T
r
y
B
V
U
q
A
5
s
L
/
6
w
4
T
p
C
G
J
k
g
i
r
V
8
9
w
U
/
Y
x
K
5
E
x
A
X
u
5
r
B
S
l
l
E
z
q
C
n
q
E
x
j
U
D
5
2
e
L
2
3
L
k
w
y
t
A
J
E
2
k
q
R
m
e
h
/
p
7
I
a
K
T
U
L
A
p
M
Z
0
R
x
r
F
a
9
u
f
i
f
1
9
M
Y
X
v
s
Z
j
1
O
N
E
L
P
l
o
l
A
L
B
x
N
n
H
o
Q
z
5
B
I
Y
i
p
k
h
l
E
l
u
b
n
X
Y
m
E
r
K
0
M
R
V
N
i
F
4
q
y
+
v
k
3
a
9
5
r
k
1
7
7
5
e
b
d
w
U
c
Z
T
I
G
T
k
n
l
8
Q
j
V
6
R
B
7
k
i
T
t
A
g
j
U
/
J
M
X
s
m
b
l
V
s
v
1
r
v
1
s
W
z
d
s
I
q
Z
U
/
I
H
1
u
c
P
O
O
u
V
L
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
a
d
/
a
n
D
l
Q
6
U
U
S
8
3
m
w
0
O
c
/
O
x
6
u
F
w
=
"
>
A
A
A
B
+
3
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
F
l
Q
k
l
j
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
m
x
o
v
N
l
n
Z
x
y
f
u
h
u
D
y
W
y
/
C
s
0
F
C
B
E
y
4
/
Q
8
T
d
c
E
h
e
Q
M
N
J
K
o
5
l
d
7
e
4
E
q
e
A
K
X
f
f
b
2
t
j
c
2
t
7
Z
L
e
2
V
9
w
8
O
j
4
7
t
k
0
p
b
J
V
o
y
a
L
F
E
J
L
I
b
U
A
W
C
x
9
B
C
j
g
K
6
q
Q
Q
a
B
Q
I
6
w
e
R
2
7
n
e
e
Q
C
q
e
x
I
8
4
S
8
G
P
6
C
j
m
I
W
c
U
j
T
S
w
K
3
2
E
K
S
J
m
D
y
g
1
Q
y
0
h
H
9
h
V
t
+
Y
u
4
K
w
T
r
y
B
V
U
q
A
5
s
L
/
6
w
4
T
p
C
G
J
k
g
i
r
V
8
9
w
U
/
Y
x
K
5
E
x
A
X
u
5
r
B
S
l
l
E
z
q
C
n
q
E
x
j
U
D
5
2
e
L
2
3
L
k
w
y
t
A
J
E
2
k
q
R
m
e
h
/
p
7
I
a
K
T
U
L
A
p
M
Z
0
R
x
r
F
a
9
u
f
i
f
1
9
M
Y
X
v
s
Z
j
1
O
N
E
L
P
l
o
l
A
L
B
x
N
n
H
o
Q
z
5
B
I
Y
i
p
k
h
l
E
l
u
b
n
X
Y
m
E
r
K
0
M
R
V
N
i
F
4
q
y
+
v
k
3
a
9
5
r
k
1
7
7
5
e
b
d
w
U
c
Z
T
I
G
T
k
n
l
8
Q
j
V
6
R
B
7
k
i
T
t
A
g
j
U
/
J
M
X
s
m
b
l
V
s
v
1
r
v
1
s
W
z
d
s
I
q
Z
U
/
I
H
1
u
c
P
O
O
u
V
L
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
a
d
/
a
n
D
l
Q
6
U
U
S
8
3
m
w
0
O
c
/
O
x
6
u
F
w
=
"
>
A
A
A
B
+
3
i
c
b
V
C
7
T
s
N
A
E
D
z
z
D
O
F
l
Q
k
l
j
E
S
F
R
R
X
Y
a
K
C
N
o
K
I
M
g
D
y
m
x
o
v
N
l
n
Z
x
y
f
u
h
u
D
y
W
y
/
C
s
0
F
C
B
E
y
4
/
Q
8
T
d
c
E
h
e
Q
M
N
J
K
o
5
l
d
7
e
4
E
q
e
A
K
X
f
f
b
2
t
j
c
2
t
7
Z
L
e
2
V
9
w
8
O
j
4
7
t
k
0
p
b
J
V
o
y
a
L
F
E
J
L
I
b
U
A
W
C
x
9
B
C
j
g
K
6
q
Q
Q
a
B
Q
I
6
w
e
R
2
7
n
e
e
Q
C
q
e
x
I
8
4
S
8
G
P
6
C
j
m
I
W
c
U
j
T
S
w
K
3
2
E
K
S
J
m
D
y
g
1
Q
y
0
h
H
9
h
V
t
+
Y
u
4
K
w
T
r
y
B
V
U
q
A
5
s
L
/
6
w
4
T
p
C
G
J
k
g
i
r
V
8
9
w
U
/
Y
x
K
5
E
x
A
X
u
5
r
B
S
l
l
E
z
q
C
n
q
E
x
j
U
D
5
2
e
L
2
3
L
k
w
y
t
A
J
E
2
k
q
R
m
e
h
/
p
7
I
a
K
T
U
L
A
p
M
Z
0
R
x
r
F
a
9
u
f
i
f
1
9
M
Y
X
v
s
Z
j
1
O
N
E
L
P
l
o
l
A
L
B
x
N
n
H
o
Q
z
5
B
I
Y
i
p
k
h
l
E
l
u
b
n
X
Y
m
E
r
K
0
M
R
V
N
i
F
4
q
y
+
v
k
3
a
9
5
r
k
1
7
7
5
e
b
d
w
U
c
Z
T
I
G
T
k
n
l
8
Q
j
V
6
R
B
7
k
i
T
t
A
g
j
U
/
J
M
X
s
m
b
l
V
s
v
1
r
v
1
s
W
z
d
s
I
q
Z
U
/
I
H
1
u
c
P
O
O
u
V
L
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
o
3
f
u
2
F
y
9
z
h
Z
/
Q
2
k
5
w
e
b
g
Q
6
o
6
e
5
4
=
"
>
A
A
A
B
+
3
i
c
b
V
D
L
S
s
N
A
F
J
3
4
r
P
U
V
6
9
J
N
s
A
i
u
S
t
K
N
L
o
v
d
u
K
x
g
H
9
C
G
M
p
l
O
2
q
H
z
C
D
M
3
0
h
L
y
K
2
5
c
K
O
L
W
H
3
H
n
3
z
h
t
s
9
D
W
A
w
O
H
c
+
7
h
3
j
l
R
w
p
k
B
3
/
9
2
t
r
Z
3
d
v
f
2
S
w
f
l
w
6
P
j
k
1
P
3
r
N
I
x
K
t
W
E
t
o
n
i
S
v
c
i
b
C
h
n
k
r
a
B
A
a
e
9
R
F
M
s
I
k
6
7
0
b
S
5
8
L
t
P
V
B
u
m
5
C
P
M
E
x
o
K
P
J
Y
s
Z
g
S
D
l
Y
Z
u
Z
Q
B
0
B
g
B
Z
U
4
l
E
S
S
o
h
H
7
p
V
v
+
Y
v
4
W
2
S
o
C
B
V
V
K
A
1
d
L
8
G
I
0
V
S
Y
c
O
E
Y
2
P
6
g
Z
9
A
m
G
E
N
j
H
C
a
l
w
e
p
o
Q
k
m
U
z
y
m
f
U
s
l
F
t
S
E
2
f
L
2
3
L
u
y
y
s
i
L
l
b
Z
P
g
r
d
U
f
y
c
y
L
I
y
Z
i
8
h
O
C
g
w
T
s
+
4
t
x
P
+
8
f
g
r
x
b
Z
g
x
m
a
R
A
J
V
k
t
i
l
P
u
g
f
I
W
R
X
g
j
p
i
k
B
P
r
c
E
E
8
3
s
r
R
6
Z
Y
I
0
J
2
L
r
K
t
o
R
g
/
c
u
b
p
F
O
v
B
X
4
t
e
K
h
X
G
3
d
F
H
S
V
0
g
S
7
R
N
Q
r
Q
D
W
q
g
e
9
R
C
b
U
T
Q
D
D
2
j
V
/
T
m
5
M
6
L
8
+
5
8
r
E
a
3
n
C
J
z
j
v
7
A
+
f
w
B
C
n
W
V
E
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
o
3
f
u
2
F
y
9
z
h
Z
/
Q
2
k
5
w
e
b
g
Q
6
o
6
e
5
4
=
"
>
A
A
A
B
+
3
i
c
b
V
D
L
S
s
N
A
F
J
3
4
r
P
U
V
6
9
J
N
s
A
i
u
S
t
K
N
L
o
v
d
u
K
x
g
H
9
C
G
M
p
l
O
2
q
H
z
C
D
M
3
0
h
L
y
K
2
5
c
K
O
L
W
H
3
H
n
3
z
h
t
s
9
D
W
A
w
O
H
c
+
7
h
3
j
l
R
w
p
k
B
3
/
9
2
t
r
Z
3
d
v
f
2
S
w
f
l
w
6
P
j
k
1
P
3
r
N
I
x
K
t
W
E
t
o
n
i
S
v
c
i
b
C
h
n
k
r
a
B
A
a
e
9
R
F
M
s
I
k
6
7
0
b
S
5
8
L
t
P
V
B
u
m
5
C
P
M
E
x
o
K
P
J
Y
s
Z
g
S
D
l
Y
Z
u
Z
Q
B
0
B
g
B
Z
U
4
l
E
S
S
o
h
H
7
p
V
v
+
Y
v
4
W
2
S
o
C
B
V
V
K
A
1
d
L
8
G
I
0
V
S
Y
c
O
E
Y
2
P
6
g
Z
9
A
m
G
E
N
j
H
C
a
l
w
e
p
o
Q
k
m
U
z
y
m
f
U
s
l
F
t
S
E
2
f
L
2
3
L
u
y
y
s
i
L
l
b
Z
P
g
r
d
U
f
y
c
y
L
I
y
Z
i
8
h
O
C
g
w
T
s
+
4
t
x
P
+
8
f
g
r
x
b
Z
g
x
m
a
R
A
J
V
k
t
i
l
P
u
g
f
I
W
R
X
g
j
p
i
k
B
P
r
c
E
E
8
3
s
r
R
6
Z
Y
I
0
J
2
L
r
K
t
o
R
g
/
c
u
b
p
F
O
v
B
X
4
t
e
K
h
X
G
3
d
F
H
S
V
0
g
S
7
R
N
Q
r
Q
D
W
q
g
e
9
R
C
b
U
T
Q
D
D
2
j
V
/
T
m
5
M
6
L
8
+
5
8
r
E
a
3
n
C
J
z
j
v
7
A
+
f
w
B
C
n
W
V
E
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
o
3
f
u
2
F
y
9
z
h
Z
/
Q
2
k
5
w
e
b
g
Q
6
o
6
e
5
4
=
"
>
A
A
A
B
+
3
i
c
b
V
D
L
S
s
N
A
F
J
3
4
r
P
U
V
6
9
J
N
s
A
i
u
S
t
K
N
L
o
v
d
u
K
x
g
H
9
C
G
M
p
l
O
2
q
H
z
C
D
M
3
0
h
L
y
K
2
5
c
K
O
L
W
H
3
H
n
3
z
h
t
s
9
D
W
A
w
O
H
c
+
7
h
3
j
l
R
w
p
k
B
3
/
9
2
t
r
Z
3
d
v
f
2
S
w
f
l
w
6
P
j
k
1
P
3
r
N
I
x
K
t
W
E
t
o
n
i
S
v
c
i
b
C
h
n
k
r
a
B
A
a
e
9
R
F
M
s
I
k
6
7
0
b
S
5
8
L
t
P
V
B
u
m
5
C
P
M
E
x
o
K
P
J
Y
s
Z
g
S
D
l
Y
Z
u
Z
Q
B
0
B
g
B
Z
U
4
l
E
S
S
o
h
H
7
p
V
v
+
Y
v
4
W
2
S
o
C
B
V
V
K
A
1
d
L
8
G
I
0
V
S
Y
c
O
E
Y
2
P
6
g
Z
9
A
m
G
E
N
j
H
C
a
l
w
e
p
o
Q
k
m
U
z
y
m
f
U
s
l
F
t
S
E
2
f
L
2
3
L
u
y
y
s
i
L
l
b
Z
P
g
r
d
U
f
y
c
y
L
I
y
Z
i
8
h
O
C
g
w
T
s
+
4
t
x
P
+
8
f
g
r
x
b
Z
g
x
m
a
R
A
J
V
k
t
i
l
P
u
g
f
I
W
R
X
g
j
p
i
k
B
P
r
c
E
E
8
3
s
r
R
6
Z
Y
I
0
J
2
L
r
K
t
o
R
g
/
c
u
b
p
F
O
v
B
X
4
t
e
K
h
X
G
3
d
F
H
S
V
0
g
S
7
R
N
Q
r
Q
D
W
q
g
e
9
R
C
b
U
T
Q
D
D
2
j
V
/
T
m
5
M
6
L
8
+
5
8
r
E
a
3
n
C
J
z
j
v
7
A
+
f
w
B
C
n
W
V
E
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
o
3
f
u
2
F
y
9
z
h
Z
/
Q
2
k
5
w
e
b
g
Q
6
o
6
e
5
4
=
"
>
A
A
A
B
+
3
i
c
b
V
D
L
S
s
N
A
F
J
3
4
r
P
U
V
6
9
J
N
s
A
i
u
S
t
K
N
L
o
v
d
u
K
x
g
H
9
C
G
M
p
l
O
2
q
H
z
C
D
M
3
0
h
L
y
K
2
5
c
K
O
L
W
H
3
H
n
3
z
h
t
s
9
D
W
A
w
O
H
c
+
7
h
3
j
l
R
w
p
k
B
3
/
9
2
t
r
Z
3
d
v
f
2
S
w
f
l
w
6
P
j
k
1
P
3
r
N
I
x
K
t
W
E
t
o
n
i
S
v
c
i
b
C
h
n
k
r
a
B
A
a
e
9
R
F
M
s
I
k
6
7
0
b
S
5
8
L
t
P
V
B
u
m
5
C
P
M
E
x
o
K
P
J
Y
s
Z
g
S
D
l
Y
Z
u
Z
Q
B
0
B
g
B
Z
U
4
l
E
S
S
o
h
H
7
p
V
v
+
Y
v
4
W
2
S
o
C
B
V
V
K
A
1
d
L
8
G
I
0
V
S
Y
c
O
E
Y
2
P
6
g
Z
9
A
m
G
E
N
j
H
C
a
l
w
e
p
o
Q
k
m
U
z
y
m
f
U
s
l
F
t
S
E
2
f
L
2
3
L
u
y
y
s
i
L
l
b
Z
P
g
r
d
U
f
y
c
y
L
I
y
Z
i
8
h
O
C
g
w
T
s
+
4
t
x
P
+
8
f
g
r
x
b
Z
g
x
m
a
R
A
J
V
k
t
i
l
P
u
g
f
I
W
R
X
g
j
p
i
k
B
P
r
c
E
E
8
3
s
r
R
6
Z
Y
I
0
J
2
L
r
K
t
o
R
g
/
c
u
b
p
F
O
v
B
X
4
t
e
K
h
X
G
3
d
F
H
S
V
0
g
S
7
R
N
Q
r
Q
D
W
q
g
e
9
R
C
b
U
T
Q
D
D
2
j
V
/
T
m
5
M
6
L
8
+
5
8
r
E
a
3
n
C
J
z
j
v
7
A
+
f
w
B
C
n
W
V
E
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
h
P
s
F
u
C
q
o
x
3
t
h
q
Z
2
m
6
Y
H
N
J
K
R
6
G
0
=
"
>
A
A
A
B
+
H
i
c
b
V
A
9
T
8
M
w
E
H
X
4
L
O
W
j
A
U
a
W
i
A
q
J
q
U
q
6
w
F
j
B
w
s
B
Q
J
P
o
h
t
V
H
l
u
E
5
r
1
b
E
j
+
4
w
I
U
X
8
J
C
w
M
I
s
f
J
T
2
P
g
3
u
G
0
G
a
H
n
S
S
U
/
v
3
e
n
u
X
p
R
y
p
s
H
3
v
5
2
1
9
Y
3
N
r
e
3
S
T
n
l
3
b
/
+
g
4
h
4
e
t
b
U
0
i
t
A
W
k
V
y
q
b
o
Q
1
5
U
z
Q
F
j
D
g
t
J
s
q
i
p
O
I
0
0
4
0
u
Z
7
5
n
Q
e
q
N
J
P
i
H
r
K
U
h
g
k
e
C
R
Y
z
g
s
F
K
A
7
f
S
B
/
o
I
A
P
k
t
z
q
S
B
6
c
C
t
+
j
V
/
D
m
+
V
B
A
W
p
o
g
L
N
g
f
v
V
H
0
p
i
E
i
q
A
c
K
x
1
L
/
B
T
C
H
O
s
g
B
F
O
p
+
W
+
0
T
T
F
Z
I
J
H
t
G
e
p
w
A
n
V
Y
T
4
/
f
O
q
d
W
W
X
o
x
V
L
Z
E
u
D
N
1
d
8
T
O
U
6
0
z
p
L
I
d
i
Y
Y
x
n
r
Z
m
4
n
/
e
T
0
D
8
W
W
Y
M
5
E
a
o
I
I
s
F
s
W
G
e
y
C
9
W
Q
r
e
k
C
l
K
g
G
e
W
Y
K
K
Y
v
d
U
j
Y
6
w
w
A
Z
t
V
2
Y
Y
Q
L
L
+
8
S
t
r
1
W
u
D
X
g
r
t
6
t
X
F
V
x
F
F
C
J
+
g
U
n
a
M
A
X
a
A
G
u
k
F
N
1
E
I
E
G
f
S
M
X
t
G
b
8
+
S
8
O
O
/
O
x
6
J
1
z
S
l
m
j
t
E
f
O
J
8
/
s
n
G
T
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
h
P
s
F
u
C
q
o
x
3
t
h
q
Z
2
m
6
Y
H
N
J
K
R
6
G
0
=
"
>
A
A
A
B
+
H
i
c
b
V
A
9
T
8
M
w
E
H
X
4
L
O
W
j
A
U
a
W
i
A
q
J
q
U
q
6
w
F
j
B
w
s
B
Q
J
P
o
h
t
V
H
l
u
E
5
r
1
b
E
j
+
4
w
I
U
X
8
J
C
w
M
I
s
f
J
T
2
P
g
3
u
G
0
G
a
H
n
S
S
U
/
v
3
e
n
u
X
p
R
y
p
s
H
3
v
5
2
1
9
Y
3
N
r
e
3
S
T
n
l
3
b
/
+
g
4
h
4
e
t
b
U
0
i
t
A
W
k
V
y
q
b
o
Q
1
5
U
z
Q
F
j
D
g
t
J
s
q
i
p
O
I
0
0
4
0
u
Z
7
5
n
Q
e
q
N
J
P
i
H
r
K
U
h
g
k
e
C
R
Y
z
g
s
F
K
A
7
f
S
B
/
o
I
A
P
k
t
z
q
S
B
6
c
C
t
+
j
V
/
D
m
+
V
B
A
W
p
o
g
L
N
g
f
v
V
H
0
p
i
E
i
q
A
c
K
x
1
L
/
B
T
C
H
O
s
g
B
F
O
p
+
W
+
0
T
T
F
Z
I
J
H
t
G
e
p
w
A
n
V
Y
T
4
/
f
O
q
d
W
W
X
o
x
V
L
Z
E
u
D
N
1
d
8
T
O
U
6
0
z
p
L
I
d
i
Y
Y
x
n
r
Z
m
4
n
/
e
T
0
D
8
W
W
Y
M
5
E
a
o
I
I
s
F
s
W
G
e
y
C
9
W
Q
r
e
k
C
l
K
g
G
e
W
Y
K
K
Y
v
d
U
j
Y
6
w
w
A
Z
t
V
2
Y
Y
Q
L
L
+
8
S
t
r
1
W
u
D
X
g
r
t
6
t
X
F
V
x
F
F
C
J
+
g
U
n
a
M
A
X
a
A
G
u
k
F
N
1
E
I
E
G
f
S
M
X
t
G
b
8
+
S
8
O
O
/
O
x
6
J
1
z
S
l
m
j
t
E
f
O
J
8
/
s
n
G
T
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
h
P
s
F
u
C
q
o
x
3
t
h
q
Z
2
m
6
Y
H
N
J
K
R
6
G
0
=
"
>
A
A
A
B
+
H
i
c
b
V
A
9
T
8
M
w
E
H
X
4
L
O
W
j
A
U
a
W
i
A
q
J
q
U
q
6
w
F
j
B
w
s
B
Q
J
P
o
h
t
V
H
l
u
E
5
r
1
b
E
j
+
4
w
I
U
X
8
J
C
w
M
I
s
f
J
T
2
P
g
3
u
G
0
G
a
H
n
S
S
U
/
v
3
e
n
u
X
p
R
y
p
s
H
3
v
5
2
1
9
Y
3
N
r
e
3
S
T
n
l
3
b
/
+
g
4
h
4
e
t
b
U
0
i
t
A
W
k
V
y
q
b
o
Q
1
5
U
z
Q
F
j
D
g
t
J
s
q
i
p
O
I
0
0
4
0
u
Z
7
5
n
Q
e
q
N
J
P
i
H
r
K
U
h
g
k
e
C
R
Y
z
g
s
F
K
A
7
f
S
B
/
o
I
A
P
k
t
z
q
S
B
6
c
C
t
+
j
V
/
D
m
+
V
B
A
W
p
o
g
L
N
g
f
v
V
H
0
p
i
E
i
q
A
c
K
x
1
L
/
B
T
C
H
O
s
g
B
F
O
p
+
W
+
0
T
T
F
Z
I
J
H
t
G
e
p
w
A
n
V
Y
T
4
/
f
O
q
d
W
W
X
o
x
V
L
Z
E
u
D
N
1
d
8
T
O
U
6
0
z
p
L
I
d
i
Y
Y
x
n
r
Z
m
4
n
/
e
T
0
D
8
W
W
Y
M
5
E
a
o
I
I
s
F
s
W
G
e
y
C
9
W
Q
r
e
k
C
l
K
g
G
e
W
Y
K
K
Y
v
d
U
j
Y
6
w
w
A
Z
t
V
2
Y
Y
Q
L
L
+
8
S
t
r
1
W
u
D
X
g
r
t
6
t
X
F
V
x
F
F
C
J
+
g
U
n
a
M
A
X
a
A
G
u
k
F
N
1
E
I
E
G
f
S
M
X
t
G
b
8
+
S
8
O
O
/
O
x
6
J
1
z
S
l
m
j
t
E
f
O
J
8
/
s
n
G
T
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
h
P
s
F
u
C
q
o
x
3
t
h
q
Z
2
m
6
Y
H
N
J
K
R
6
G
0
=
"
>
A
A
A
B
+
H
i
c
b
V
A
9
T
8
M
w
E
H
X
4
L
O
W
j
A
U
a
W
i
A
q
J
q
U
q
6
w
F
j
B
w
s
B
Q
J
P
o
h
t
V
H
l
u
E
5
r
1
b
E
j
+
4
w
I
U
X
8
J
C
w
M
I
s
f
J
T
2
P
g
3
u
G
0
G
a
H
n
S
S
U
/
v
3
e
n
u
X
p
R
y
p
s
H
3
v
5
2
1
9
Y
3
N
r
e
3
S
T
n
l
3
b
/
+
g
4
h
4
e
t
b
U
0
i
t
A
W
k
V
y
q
b
o
Q
1
5
U
z
Q
F
j
D
g
t
J
s
q
i
p
O
I
0
0
4
0
u
Z
7
5
n
Q
e
q
N
J
P
i
H
r
K
U
h
g
k
e
C
R
Y
z
g
s
F
K
A
7
f
S
B
/
o
I
A
P
k
t
z
q
S
B
6
c
C
t
+
j
V
/
D
m
+
V
B
A
W
p
o
g
L
N
g
f
v
V
H
0
p
i
E
i
q
A
c
K
x
1
L
/
B
T
C
H
O
s
g
B
F
O
p
+
W
+
0
T
T
F
Z
I
J
H
t
G
e
p
w
A
n
V
Y
T
4
/
f
O
q
d
W
W
X
o
x
V
L
Z
E
u
D
N
1
d
8
T
O
U
6
0
z
p
L
I
d
i
Y
Y
x
n
r
Z
m
4
n
/
e
T
0
D
8
W
W
Y
M
5
E
a
o
I
I
s
F
s
W
G
e
y
C
9
W
Q
r
e
k
C
l
K
g
G
e
W
Y
K
K
Y
v
d
U
j
Y
6
w
w
A
Z
t
V
2
Y
Y
Q
L
L
+
8
S
t
r
1
W
u
D
X
g
r
t
6
t
X
F
V
x
F
F
C
J
+
g
U
n
a
M
A
X
a
A
G
u
k
F
N
1
E
I
E
G
f
S
M
X
t
G
b
8
+
S
8
O
O
/
O
x
6
J
1
z
S
l
m
j
t
E
f
O
J
8
/
s
n
G
T
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
h
e
W
/
X
W
8
2
P
g
a
P
G
W
4
c
h
7
o
K
o
r
p
m
Y
4
=
"
>
A
A
A
B
+
H
i
c
b
V
D
L
S
s
N
A
F
L
3
x
W
e
u
j
U
Z
d
u
g
k
V
w
V
Z
J
u
d
F
k
U
w
W
U
F
+
4
A
2
l
M
l
0
0
g
6
d
T
M
L
M
j
R
h
D
v
8
S
N
C
0
X
c
+
i
n
u
/
B
u
n
b
R
b
a
e
m
D
g
c
M
4
9
3
D
s
n
S
A
T
X
6
L
r
f
1
t
r
6
x
u
b
W
d
m
m
n
v
L
u
3
f
1
C
x
D
4
/
a
O
k
4
V
Z
S
0
a
i
1
h
1
A
6
K
Z
4
J
K
1
k
K
N
g
3
U
Q
x
E
g
W
C
d
Y
L
J
9
c
z
v
P
D
C
l
e
S
z
v
M
U
u
Y
H
5
G
R
5
C
G
n
B
I
0
0
s
C
t
9
Z
I
+
I
m
N
9
I
k
8
6
m
A
7
v
q
1
t
w
5
n
F
X
i
F
a
Q
K
B
Z
o
D
+
6
s
/
j
G
k
a
M
Y
l
U
E
K
1
7
n
p
u
g
n
x
O
F
n
A
o
2
L
f
d
T
z
R
J
C
J
2
T
E
e
o
Z
K
E
j
H
t
5
/
P
D
p
8
6
Z
U
Y
Z
O
G
C
v
z
J
D
p
z
9
X
c
i
J
5
H
W
W
R
S
Y
y
Y
j
g
W
C
9
7
M
/
E
/
r
5
d
i
e
O
n
n
X
C
Y
p
M
k
k
X
i
8
J
U
O
B
g
7
s
x
a
c
I
V
e
M
o
s
g
M
I
V
R
x
c
6
t
D
x
0
Q
R
i
q
a
r
s
i
n
B
W
/
7
y
K
m
n
X
a
5
5
b
8
+
7
q
1
c
Z
V
U
U
c
J
T
u
A
U
z
s
G
D
C
2
j
A
L
T
S
h
B
R
R
S
e
I
Z
X
e
L
O
e
r
B
f
r
3
f
p
Y
j
K
5
Z
R
e
Y
Y
/
s
D
6
/
A
G
w
4
Z
O
9
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
h
e
W
/
X
W
8
2
P
g
a
P
G
W
4
c
h
7
o
K
o
r
p
m
Y
4
=
"
>
A
A
A
B
+
H
i
c
b
V
D
L
S
s
N
A
F
L
3
x
W
e
u
j
U
Z
d
u
g
k
V
w
V
Z
J
u
d
F
k
U
w
W
U
F
+
4
A
2
l
M
l
0
0
g
6
d
T
M
L
M
j
R
h
D
v
8
S
N
C
0
X
c
+
i
n
u
/
B
u
n
b
R
b
a
e
m
D
g
c
M
4
9
3
D
s
n
S
A
T
X
6
L
r
f
1
t
r
6
x
u
b
W
d
m
m
n
v
L
u
3
f
1
C
x
D
4
/
a
O
k
4
V
Z
S
0
a
i
1
h
1
A
6
K
Z
4
J
K
1
k
K
N
g
3
U
Q
x
E
g
W
C
d
Y
L
J
9
c
z
v
P
D
C
l
e
S
z
v
M
U
u
Y
H
5
G
R
5
C
G
n
B
I
0
0
s
C
t
9
Z
I
+
I
m
N
9
I
k
8
6
m
A
7
v
q
1
t
w
5
n
F
X
i
F
a
Q
K
B
Z
o
D
+
6
s
/
j
G
k
a
M
Y
l
U
E
K
1
7
n
p
u
g
n
x
O
F
n
A
o
2
L
f
d
T
z
R
J
C
J
2
T
E
e
o
Z
K
E
j
H
t
5
/
P
D
p
8
6
Z
U
Y
Z
O
G
C
v
z
J
D
p
z
9
X
c
i
J
5
H
W
W
R
S
Y
y
Y
j
g
W
C
9
7
M
/
E
/
r
5
d
i
e
O
n
n
X
C
Y
p
M
k
k
X
i
8
J
U
O
B
g
7
s
x
a
c
I
V
e
M
o
s
g
M
I
V
R
x
c
6
t
D
x
0
Q
R
i
q
a
r
s
i
n
B
W
/
7
y
K
m
n
X
a
5
5
b
8
+
7
q
1
c
Z
V
U
U
c
J
T
u
A
U
z
s
G
D
C
2
j
A
L
T
S
h
B
R
R
S
e
I
Z
X
e
L
O
e
r
B
f
r
3
f
p
Y
j
K
5
Z
R
e
Y
Y
/
s
D
6
/
A
G
w
4
Z
O
9
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
h
e
W
/
X
W
8
2
P
g
a
P
G
W
4
c
h
7
o
K
o
r
p
m
Y
4
=
"
>
A
A
A
B
+
H
i
c
b
V
D
L
S
s
N
A
F
L
3
x
W
e
u
j
U
Z
d
u
g
k
V
w
V
Z
J
u
d
F
k
U
w
W
U
F
+
4
A
2
l
M
l
0
0
g
6
d
T
M
L
M
j
R
h
D
v
8
S
N
C
0
X
c
+
i
n
u
/
B
u
n
b
R
b
a
e
m
D
g
c
M
4
9
3
D
s
n
S
A
T
X
6
L
r
f
1
t
r
6
x
u
b
W
d
m
m
n
v
L
u
3
f
1
C
x
D
4
/
a
O
k
4
V
Z
S
0
a
i
1
h
1
A
6
K
Z
4
J
K
1
k
K
N
g
3
U
Q
x
E
g
W
C
d
Y
L
J
9
c
z
v
P
D
C
l
e
S
z
v
M
U
u
Y
H
5
G
R
5
C
G
n
B
I
0
0
s
C
t
9
Z
I
+
I
m
N
9
I
k
8
6
m
A
7
v
q
1
t
w
5
n
F
X
i
F
a
Q
K
B
Z
o
D
+
6
s
/
j
G
k
a
M
Y
l
U
E
K
1
7
n
p
u
g
n
x
O
F
n
A
o
2
L
f
d
T
z
R
J
C
J
2
T
E
e
o
Z
K
E
j
H
t
5
/
P
D
p
8
6
Z
U
Y
Z
O
G
C
v
z
J
D
p
z
9
X
c
i
J
5
H
W
W
R
S
Y
y
Y
j
g
W
C
9
7
M
/
E
/
r
5
d
i
e
O
n
n
X
C
Y
p
M
k
k
X
i
8
J
U
O
B
g
7
s
x
a
c
I
V
e
M
o
s
g
M
I
V
R
x
c
6
t
D
x
0
Q
R
i
q
a
r
s
i
n
B
W
/
7
y
K
m
n
X
a
5
5
b
8
+
7
q
1
c
Z
V
U
U
c
J
T
u
A
U
z
s
G
D
C
2
j
A
L
T
S
h
B
R
R
S
e
I
Z
X
e
L
O
e
r
B
f
r
3
f
p
Y
j
K
5
Z
R
e
Y
Y
/
s
D
6
/
A
G
w
4
Z
O
9
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
h
e
W
/
X
W
8
2
P
g
a
P
G
W
4
c
h
7
o
K
o
r
p
m
Y
4
=
"
>
A
A
A
B
+
H
i
c
b
V
D
L
S
s
N
A
F
L
3
x
W
e
u
j
U
Z
d
u
g
k
V
w
V
Z
J
u
d
F
k
U
w
W
U
F
+
4
A
2
l
M
l
0
0
g
6
d
T
M
L
M
j
R
h
D
v
8
S
N
C
0
X
c
+
i
n
u
/
B
u
n
b
R
b
a
e
m
D
g
c
M
4
9
3
D
s
n
S
A
T
X
6
L
r
f
1
t
r
6
x
u
b
W
d
m
m
n
v
L
u
3
f
1
C
x
D
4
/
a
O
k
4
V
Z
S
0
a
i
1
h
1
A
6
K
Z
4
J
K
1
k
K
N
g
3
U
Q
x
E
g
W
C
d
Y
L
J
9
c
z
v
P
D
C
l
e
S
z
v
M
U
u
Y
H
5
G
R
5
C
G
n
B
I
0
0
s
C
t
9
Z
I
+
I
m
N
9
I
k
8
6
m
A
7
v
q
1
t
w
5
n
F
X
i
F
a
Q
K
B
Z
o
D
+
6
s
/
j
G
k
a
M
Y
l
U
E
K
1
7
n
p
u
g
n
x
O
F
n
A
o
2
L
f
d
T
z
R
J
C
J
2
T
E
e
o
Z
K
E
j
H
t
5
/
P
D
p
8
6
Z
U
Y
Z
O
G
C
v
z
J
D
p
z
9
X
c
i
J
5
H
W
W
R
S
Y
y
Y
j
g
W
C
9
7
M
/
E
/
r
5
d
i
e
O
n
n
X
C
Y
p
M
k
k
X
i
8
J
U
O
B
g
7
s
x
a
c
I
V
e
M
o
s
g
M
I
V
R
x
c
6
t
D
x
0
Q
R
i
q
a
r
s
i
n
B
W
/
7
y
K
m
n
X
a
5
5
b
8
+
7
q
1
c
Z
V
U
U
c
J
T
u
A
U
z
s
G
D
C
2
j
A
L
T
S
h
B
R
R
S
e
I
Z
X
e
L
O
e
r
B
f
r
3
f
p
Y
j
K
5
Z
R
e
Y
Y
/
s
D
6
/
A
G
w
4
Z
O
9
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
i
4
/
Q
s
a
t
C
t
X
e
b
I
K
L
M
Z
f
1
6
b
e
r
M
W
g
=
"
>
A
A
A
B
+
H
i
c
b
V
B
N
T
w
I
x
F
O
z
i
F
+
I
H
q
E
c
v
j
c
T
E
E
9
k
1
J
n
o
k
c
v
G
I
i
Y
A
J
b
E
i
3
v
I
W
G
b
n
f
T
v
j
X
i
h
l
/
i
x
Y
P
G
e
P
W
n
e
P
P
f
W
G
A
P
C
k
7
S
Z
D
L
z
J
u
9
1
g
k
Q
K
g
6
7
7
7
R
T
W
1
j
c
2
t
4
r
b
p
Z
3
d
v
f
1
y
5
e
C
w
b
e
J
U
c
2
j
x
W
M
b
6
P
m
A
G
p
F
D
Q
Q
o
E
S
7
h
M
N
L
A
o
k
d
I
J
x
Y
+
Z
3
H
k
A
b
E
a
s
7
n
C
T
g
R
2
y
o
R
C
g
4
Q
y
v
1
K
+
U
e
w
i
M
i
Z
g
1
Q
C
H
r
a
r
1
T
d
m
j
s
H
X
S
V
e
T
q
o
k
R
7
N
f
+
e
o
N
Y
p
5
G
N
s
4
l
M
6
b
r
u
Q
n
6
G
d
M
o
u
I
R
p
q
Z
c
a
S
B
g
f
s
y
F
0
L
V
U
s
A
u
N
n
8
8
O
n
9
N
Q
q
A
x
r
G
2
j
6
F
d
K
7
+
T
m
Q
s
M
m
Y
S
B
X
Y
y
Y
j
g
y
y
9
5
M
/
M
/
r
p
h
h
e
+
Z
l
Q
S
Y
q
g
+
G
J
R
m
E
q
K
M
Z
2
1
Q
A
d
C
A
0
c
5
s
Y
R
x
L
e
y
t
l
I
+
Y
Z
t
x
2
Y
E
q
2
B
G
/
5
y
6
u
k
f
V
7
z
3
J
p
3
e
1
G
t
X
+
d
1
F
M
k
x
O
S
F
n
x
C
O
X
p
E
5
u
S
J
O
0
C
C
c
p
e
S
a
v
5
M
1
5
c
l
6
c
d
+
d
j
M
V
p
w
8
s
w
R
+
Q
P
n
8
w
e
G
v
J
O
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
E
4
r
s
E
2
O
w
D
t
t
f
I
G
V
7
1
o
E
w
0
8
O
G
p
g
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
8
N
A
E
N
3
4
W
e
t
X
1
a
O
X
x
S
J
4
K
o
k
I
e
i
x
6
8
V
j
F
f
k
A
b
y
2
Y
7
a
Z
d
u
N
m
F
3
o
p
b
Q
/
+
H
F
g
y
J
e
/
S
/
e
/
D
d
u
2
x
y
0
9
c
H
A
4
7
0
Z
Z
u
Y
F
i
R
Q
G
X
f
f
b
W
V
p
e
W
V
1
b
L
2
w
U
N
7
e
2
d
3
Z
L
e
/
s
N
E
6
e
a
Q
5
3
H
M
t
a
t
g
B
m
Q
Q
k
E
d
B
U
p
o
J
R
p
Y
F
E
h
o
B
s
O
r
i
d
9
8
A
G
1
E
r
O
5
w
l
I
A
f
s
b
4
S
o
e
A
M
r
X
T
f
Q
X
h
C
x
O
w
2
l
W
D
G
3
V
L
Z
r
b
h
T
0
E
X
i
5
a
R
M
c
t
S
6
p
a
9
O
L
+
Z
p
B
A
q
5
Z
M
a
0
P
T
d
B
P
2
M
a
B
Z
c
w
L
n
Z
S
A
w
n
j
Q
9
a
H
t
q
W
K
R
W
D
8
b
H
r
1
m
B
5
b
p
U
f
D
W
N
t
S
S
K
f
q
7
4
m
M
R
c
a
M
o
s
B
2
R
g
w
H
Z
t
6
b
i
P
9
5
7
R
T
D
C
z
8
T
K
k
k
R
F
J
8
t
C
l
N
J
M
a
a
T
C
G
h
P
a
O
A
o
R
5
Y
w
r
o
W
9
l
f
I
B
0
4
y
j
D
a
p
o
Q
/
D
m
X
1
4
k
j
d
O
K
5
1
a
8
m
7
N
y
9
T
K
P
o
0
A
O
y
R
E
5
I
R
4
5
J
1
V
y
T
W
q
k
T
j
j
R
5
J
m
8
k
j
f
n
0
X
l
x
3
p
2
P
W
e
u
S
k
8
8
c
k
D
9
w
P
n
8
A
Y
j
K
T
E
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
E
4
r
s
E
2
O
w
D
t
t
f
I
G
V
7
1
o
E
w
0
8
O
G
p
g
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
8
N
A
E
N
3
4
W
e
t
X
1
a
O
X
x
S
J
4
K
o
k
I
e
i
x
6
8
V
j
F
f
k
A
b
y
2
Y
7
a
Z
d
u
N
m
F
3
o
p
b
Q
/
+
H
F
g
y
J
e
/
S
/
e
/
D
d
u
2
x
y
0
9
c
H
A
4
7
0
Z
Z
u
Y
F
i
R
Q
G
X
f
f
b
W
V
p
e
W
V
1
b
L
2
w
U
N
7
e
2
d
3
Z
L
e
/
s
N
E
6
e
a
Q
5
3
H
M
t
a
t
g
B
m
Q
Q
k
E
d
B
U
p
o
J
R
p
Y
F
E
h
o
B
s
O
r
i
d
9
8
A
G
1
E
r
O
5
w
l
I
A
f
s
b
4
S
o
e
A
M
r
X
T
f
Q
X
h
C
x
O
w
2
l
W
D
G
3
V
L
Z
r
b
h
T
0
E
X
i
5
a
R
M
c
t
S
6
p
a
9
O
L
+
Z
p
B
A
q
5
Z
M
a
0
P
T
d
B
P
2
M
a
B
Z
c
w
L
n
Z
S
A
w
n
j
Q
9
a
H
t
q
W
K
R
W
D
8
b
H
r
1
m
B
5
b
p
U
f
D
W
N
t
S
S
K
f
q
7
4
m
M
R
c
a
M
o
s
B
2
R
g
w
H
Z
t
6
b
i
P
9
5
7
R
T
D
C
z
8
T
K
k
k
R
F
J
8
t
C
l
N
J
M
a
a
T
C
G
h
P
a
O
A
o
R
5
Y
w
r
o
W
9
l
f
I
B
0
4
y
j
D
a
p
o
Q
/
D
m
X
1
4
k
j
d
O
K
5
1
a
8
m
7
N
y
9
T
K
P
o
0
A
O
y
R
E
5
I
R
4
5
J
1
V
y
T
W
q
k
T
j
j
R
5
J
m
8
k
j
f
n
0
X
l
x
3
p
2
P
W
e
u
S
k
8
8
c
k
D
9
w
P
n
8
A
Y
j
K
T
E
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
E
4
r
s
E
2
O
w
D
t
t
f
I
G
V
7
1
o
E
w
0
8
O
G
p
g
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
8
N
A
E
N
3
4
W
e
t
X
1
a
O
X
x
S
J
4
K
o
k
I
e
i
x
6
8
V
j
F
f
k
A
b
y
2
Y
7
a
Z
d
u
N
m
F
3
o
p
b
Q
/
+
H
F
g
y
J
e
/
S
/
e
/
D
d
u
2
x
y
0
9
c
H
A
4
7
0
Z
Z
u
Y
F
i
R
Q
G
X
f
f
b
W
V
p
e
W
V
1
b
L
2
w
U
N
7
e
2
d
3
Z
L
e
/
s
N
E
6
e
a
Q
5
3
H
M
t
a
t
g
B
m
Q
Q
k
E
d
B
U
p
o
J
R
p
Y
F
E
h
o
B
s
O
r
i
d
9
8
A
G
1
E
r
O
5
w
l
I
A
f
s
b
4
S
o
e
A
M
r
X
T
f
Q
X
h
C
x
O
w
2
l
W
D
G
3
V
L
Z
r
b
h
T
0
E
X
i
5
a
R
M
c
t
S
6
p
a
9
O
L
+
Z
p
B
A
q
5
Z
M
a
0
P
T
d
B
P
2
M
a
B
Z
c
w
L
n
Z
S
A
w
n
j
Q
9
a
H
t
q
W
K
R
W
D
8
b
H
r
1
m
B
5
b
p
U
f
D
W
N
t
S
S
K
f
q
7
4
m
M
R
c
a
M
o
s
B
2
R
g
w
H
Z
t
6
b
i
P
9
5
7
R
T
D
C
z
8
T
K
k
k
R
F
J
8
t
C
l
N
J
M
a
a
T
C
G
h
P
a
O
A
o
R
5
Y
w
r
o
W
9
l
f
I
B
0
4
y
j
D
a
p
o
Q
/
D
m
X
1
4
k
j
d
O
K
5
1
a
8
m
7
N
y
9
T
K
P
o
0
A
O
y
R
E
5
I
R
4
5
J
1
V
y
T
W
q
k
T
j
j
R
5
J
m
8
k
j
f
n
0
X
l
x
3
p
2
P
W
e
u
S
k
8
8
c
k
D
9
w
P
n
8
A
Y
j
K
T
E
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
E
4
r
s
E
2
O
w
D
t
t
f
I
G
V
7
1
o
E
w
0
8
O
G
p
g
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
8
N
A
E
N
3
4
W
e
t
X
1
a
O
X
x
S
J
4
K
o
k
I
e
i
x
6
8
V
j
F
f
k
A
b
y
2
Y
7
a
Z
d
u
N
m
F
3
o
p
b
Q
/
+
H
F
g
y
J
e
/
S
/
e
/
D
d
u
2
x
y
0
9
c
H
A
4
7
0
Z
Z
u
Y
F
i
R
Q
G
X
f
f
b
W
V
p
e
W
V
1
b
L
2
w
U
N
7
e
2
d
3
Z
L
e
/
s
N
E
6
e
a
Q
5
3
H
M
t
a
t
g
B
m
Q
Q
k
E
d
B
U
p
o
J
R
p
Y
F
E
h
o
B
s
O
r
i
d
9
8
A
G
1
E
r
O
5
w
l
I
A
f
s
b
4
S
o
e
A
M
r
X
T
f
Q
X
h
C
x
O
w
2
l
W
D
G
3
V
L
Z
r
b
h
T
0
E
X
i
5
a
R
M
c
t
S
6
p
a
9
O
L
+
Z
p
B
A
q
5
Z
M
a
0
P
T
d
B
P
2
M
a
B
Z
c
w
L
n
Z
S
A
w
n
j
Q
9
a
H
t
q
W
K
R
W
D
8
b
H
r
1
m
B
5
b
p
U
f
D
W
N
t
S
S
K
f
q
7
4
m
M
R
c
a
M
o
s
B
2
R
g
w
H
Z
t
6
b
i
P
9
5
7
R
T
D
C
z
8
T
K
k
k
R
F
J
8
t
C
l
N
J
M
a
a
T
C
G
h
P
a
O
A
o
R
5
Y
w
r
o
W
9
l
f
I
B
0
4
y
j
D
a
p
o
Q
/
D
m
X
1
4
k
j
d
O
K
5
1
a
8
m
7
N
y
9
T
K
P
o
0
A
O
y
R
E
5
I
R
4
5
J
1
V
y
T
W
q
k
T
j
j
R
5
J
m
8
k
j
f
n
0
X
l
x
3
p
2
P
W
e
u
S
k
8
8
c
k
D
9
w
P
n
8
A
Y
j
K
T
E
g
=
=
<
/
l
a
t
e
x
i
t
>
3.2. Applying Rules

Carpenter et al. [7] summarized that in the advanced
RPM, rules were applied row-wise and could be grouped
into 5 types. Unlike Berrett et al. [3], we strictly follow
Carpenter et al.’s description of RPM and implement all
the rules, except that we merge Distribute Two into
Distribute Three, as the former is essentially the lat-
ter with a null value in one of the attributes.

rules

Speciﬁcally, we implement 4 types of

in
RAVEN: Constant, Progression, Arithmetic,
and Distribute Three. Different from [3], we add
internal parameters to certain rules (e.g., Progression
could have increments or decrements of 1 or 2), resulting in
a total of 8 distinct rule instantiations. Rules do not operate
on the 2 noise attributes. As shown in Figure 1 and 2, they
are denoted as [attribute:rule] pairs.

To make the image space even more structured, we re-
quire each attribute to go over one rule and all Entities
in the same Component to share the same set of rules,
while different Components could vary.

Given the tree representation and the rules, we ﬁrst prune
the grammar tree such that all sub-trees satisfy the con-
straints imposed by the relations. We then sample from the
tree and apply the rules to compose a row. Iterating the pro-
cess three times yields a problem matrix.

3.3. Generating the Answer Set

To generate the answer set, we ﬁrst derive the correct
representation of the solution and then leverage the mono-
tonicity of RPM constraints proposed by Wang and Su [55].
To break the correct relationships, we ﬁnd an attribute that
is constrained by a rule as described in Section 3.2 and vary
it. By modifying only one attribute, we could greatly reduce
the computation. Such modiﬁcation also increases the difﬁ-
culty of the problem, as it requires attention to subtle differ-
ence to tell an incorrect candidate from the correct one.

4. Comparison and Analysis

In this section, we compare RAVEN with the existing
PGM, presenting its key features and some statistics in Sec-
tion 4.1. In addition, we ﬁll in two missing pieces in a
desirable RPM dataset, i.e., structure and hierarchy (Sec-
tion 4.2), as well as the human performance (Section 4.3).
We also show that RPM becomes trivial and could be solved
instantly using a heuristics-based searching method (Sec-
tion 4.4), given a symbolic representation of images and
operations of rules.

4.1. Comparison with PGM

Table 1 summarizes several essential metrics of RAVEN
and PGM. Although PGM is larger than RAVEN in terms
of size, it is very limited in the average number of rules
(AvgRule), rule instantiations (RuleIns), number of struc-

5320

Figure 3. Examples of RPM that show the effects of adding
noise attributes. (Left) Position, Type, Size, and Color
could vary freely as long as Number follows the rule. (Right)
Position and Type in the inside group could vary freely.

Finally, the structured presentation is fed into a rendering
engine to generate images. We elaborate the details below1.

3.1. Deﬁning the Attributed Grammar

illustration of

See Figure 2 for a graphical

We adopt an A-SIG as the hierarchical and structured im-
age grammar to represent the RPM problem. Such represen-
tation is advanced compared with prior work (e.g., [3, 55])
which, at best, only maintains a ﬂat representation of rules.
the
grammar production rules. Speciﬁcally,
the A-SIG for
RPM has 5 levels—Scene, Structure, Component,
Layout, and Entity. Note that each grammar level
could have multiple instantiations,
i.e., different cate-
gories or types. The Scene level could choose any
available Structure, which consists of possibly mul-
tiple Components. Each Component branches into
Layouts that links Entities. Attributes are appended
to certain levels; for instance, (i) Number and Position
are associated with Layout, and (ii) Type, Size, and
Color are associated with Entity. Each attribute could
take a value from a ﬁnite set. During sampling, both image
structure and attribute values are sampled.

To increase the challenges and difﬁculties in the RAVEN
dataset, we further append 2 types of noise attributes—
Uniformity and Orientation—to Layout and
Entity, respectively. Uniformity, set false, will not
constrain Entities in a Layout to look the same, while
Orientation allows an Entity to self-rotate. See Fig-
ure 3 for the effects of the noise attributes.

This grammatical design of the image space allows the
dataset to be very diverse and easily extendable. In this
dataset, we manage to derive 7 conﬁgurations by combining
different Structures, Components, and Layouts.
Figure 4 shows examples in each ﬁgure conﬁguration.

1See the supplementary material for production rules, semantic mean-

ings of rules and nodes, and more examples.

Figure 4. Examples of 7 different ﬁgure conﬁgurations in the proposed RAVEN dataset.

tures (Struct), and ﬁgure conﬁgurations (FigConﬁg). This
contrast in PGM’s gigantic size and limited diversity might
disguise model ﬁtting as a misleading reasoning ability,
which is unlikely to generalize to other scenarios.

Table 1. Comparison with the PGM dataset.

AvgRule
RuleIns
Struct

FigConﬁg
StructAnno
HumanPerf

PGM [3] RAVEN (Ours)

1.37

6.29

5
1
3
0

8
4
7

1,120,000

X

To avoid such an undesirable effect, we refrain from gen-
erating a dataset too large, even though our structured rep-
resentation allows generation of a combinatorial number of
problems. Rather, we set out to incorporate more rule in-
stantiations (8), structures (4), and ﬁgure conﬁgurations (7)
to make the dataset diverse (see Figure 4 for examples).
Note that an equal number of images for each ﬁgure con-
ﬁguration is generated in the RAVEN dataset.

4.2. Introduction of Structure

A distinctive feature of RAVEN is the introduction of
the structural representation of the image space. Wang and
Su [55] and Barrett et al. [3] used plain logic and ﬂat rule
representations, respectively, resulting in no base of the
structure to perform reasoning on. In contrast, we have in
total 1, 120, 000 structure annotations (StructAnno) in the
form of parsed sentences in the dataset, pairing each prob-
lem instance with 16 sentences for both the matrix and the
answer set. These representations derived from the A-SIG
allow a new form of reasoning, i.e., one that combines
visual understanding and structure reasoning. As shown
in [32, 33, 34] and our experiments in Section 6, incorpo-
rating structure into RPM problem solving could result in
further performance improvement across different models.

4.3. Human Performance Analysis

Another missing point in the previous work [3] is the
evaluation of human performance. To ﬁll in the missing
piece, we recruit human subjects consisting of college stu-
dents from a subject pool maintained by the Department of
Psychology to test their performance on a subset of repre-
sentative samples in the dataset. In the experiments, human
subjects were familiarized by solving problems with only

one non-Constant rule in a ﬁxed conﬁguration. After the
familiarization, subjects were asked to answer RPM prob-
lems with complex rule combinations, and their answers
were recorded. Note that we deliberately included all ﬁg-
ure conﬁgurations to measure generalization in the human
performance and only “easily perceptible” examples were
used in case certain subjects might have impaired percep-
tion. The results are reported in Table 2. The notable perfor-
mance gap calls for further research into this problem. See
Section 6 for detailed analysis and comparisons with vision
models.

4.4. Heuristics based Solver using Searching

We also ﬁnd that the RPM could be essentially turned
into a searching problem, given the symbolic representation
of images and the access to rule operations as in [32, 33, 34].
Under such a setting, we could treat this problem as con-
straint satisfaction and develop a heuristics-based solver.
The solver checks the number of satisﬁed constraints in
each candidate answer and selects one with the highest
score, resulting in perfect performance. Results are reported
in Table 2. The optimality of the heuristic-based solver also
veriﬁes the well-formedness of RAVEN in the sense that
there exists only one candidate that satisﬁes all constraints.

5. Dynamic Residual Tree for RPM

The image space of RPM is inherently structured and
could be described using a symbolic language, as shown
in [7, 32, 33, 34, 47]. To capture this characteristic and fur-
ther improve the model performance on RPM, we propose a
simple tree-structure neural module called Dynamic Resid-
ual Tree (DRT) that operates on the joint space of image
understanding and structure reasoning. An example of DRT
is shown in Figure 5.

In the DRT, given a sentence S sampled from the A-SIG,
usually represented as a serialized n-ary tree, we could
ﬁrst recover the tree structure. Note that the tree is dy-
namically generated following the sentence S, and each
node in the tree comes with a label. With a structured tree
representation ready, we could now consider assigning a
neural computation operator to each tree node, similar to
Tree-LSTM [53]. To further simplify computation, we re-
place the LSTM cell [15] with a ReLU-activated [41] fully-
connected layer f . In this way, nodes with a single child
(leaf nodes or OR-production nodes) update the input fea-

5321

Figure 5. An example computation graph of DRT. (a) Given the
serialized n-ary tree representation (pre-order traversal with / de-
noting end-of-branch), (b) a tree-structured computation graph is
dynamically built. The input features are wired from bottom-up
following the tree structure. The ﬁnal output is the sum with the
input, forming a residual module.

tures by

I = ReLU(f ([I, wn])),

(1)

where [·, ·] is the concatenation operation, I denotes the in-
put features, and wn the distributed representations of the
node’s label [40, 45]. Nodes with multiple children (AND-
production nodes) update input features by

I = ReLU f "Xc

Ic, wn#!! ,

(2)

where Ic denotes the features from its child c.

In summary, features from the lower layers are fed into
the leaf nodes of DRT, gradually updated by Equation 1 and
Equation 2 from bottom-up following the tree structure, and
output to higher-level layers.

Inspired by [14], we make DRT a residual module by
adding the input and output of DRT together, hence the
name Dynamic Residual Tree (DRT)

I = DRT(I, S) + I.

(3)

6. Experiments

6.1. Computer Vision Models

We adopt several representative models suitable for RPM
and test their performances on RAVEN [3, 14, 27, 57].
In summary, we test a simple sequential learning model
(LSTM), a CNN backbone with an MLP head (CNN), a
ResNet-based [14] image classiﬁer (ResNet), the recent re-
lational WReN [3], and all these models augmented with
the proposed DRT.

LSTM The partially sequential nature of the RPM
problem inspires us to borrow the power of sequential learn-
ing. Similar to ConvLSTM [57], we feed each image feature
extracted by a CNN into an LSTM network sequentially and
pass the last hidden feature into a two-layer MLP to predict
the ﬁnal answer. In the DRT-augmented LSTM, i.e., LSTM-
DRT, we feed features of each image to a shared DRT be-
fore the ﬁnal LSTM.

CNN We test a neural network model used in Hoshen
and Werman [17]. In this model, a four-layer CNN for im-
age feature extraction is connected to a two-layer MLP
with a softmax layer to classify the answer. The CNN is
interleaved with batch normalization [20] and ReLU non-
linearity [41]. Random dropout [51] is applied at the penul-
timate layer of MLP. In CNN-DRT, image features are
passed to DRT before MLP.

ResNet Due to its surprising effectiveness in image
feature extraction, we replace the feature extraction back-
bone in CNN with a ResNet [14] in this model. We use a
publicly available ResNet implementation, and the model is
randomly initialized without pre-training. After testing sev-
eral ResNet variants, we choose ResNet-18 for its good per-
formance. The DRT extension and the training strategy are
similar to those used in the CNN model.

WReN We follow the original paper [3] in implement-
ing the WReN. In this model, we ﬁrst extract image features
by a CNN. Each answer feature is then composed with each
context image feature to form a set of ordered pairs. The
order pairs are further fed to an MLP and summed. Finally,
a softmax layer takes features from each candidate answer
and makes a prediction. In WReN-DRT, we apply DRT on
the extracted image features before the relational module.

For all DRT extensions, nodes in the same level share
parameters and the representations for nodes’ labels are
ﬁxed after initialization from corresponding 300-dimension
GloVe vectors [45]. Sentences used for assembling DRT
could be either retrieved or learned by an encoder-decoder.
Here we report results using retrieval.

6.2. Experimental Setup

We split the RAVEN dataset into three parts, 6 folds for
training, 2 folds for validation, and 2 folds for testing. We
tune hyper-parameters on the validation set and report the
model accuracy on the test set. For loss design, we treat the
problem as a classiﬁcation task and train all models with the
cross-entropy loss. All the models are implemented in Py-
Torch [44] and trained with ADAM [25] before early stop-
ping or a maximum number of epochs is reached.

6.3. Performance Analysis

Table 2 shows the testing accuracy of each model
trained on RAVEN, against the human performance and
the heuristics-based solver. Neither human subjects nor the
solver experiences an intensive training session, and the
solver has access to the rule operations and searches the an-
swer based on a symbolic representation of the problem. In
contrast, all the computer vision models go over an exten-
sive training session, but only on the training set.

In general, human subjects produce better testing accu-
racy on problems with simple ﬁgure conﬁgurations such
as Center, while human performance reasonably dete-
riorates on problem instances with more objects such as

5322

Table 2. Testing accuracy of each model against human subjects and the solver. Acc denotes the mean accuracy of each model, while other
columns show model accuracy on different ﬁgure conﬁgurations. L-R denotes Left-Right, U-D denotes Up-Down, O-IC denotes
Out-InCenter, and O-IG denotes Out-InGrid. ⋆Note that the perfect solver has access to rule operations and searches on the
symbolic problem representation.
Acc

Center 2x2Grid

3x3Grid

Method

O-IC

O-IG

U-D

L-R

13.07% 13.19%
14.69% 13.09%
36.97% 33.58%
53.43% 52.82%
13.96% 14.29%
15.02% 15.38%
39.42% 37.30%

14.13%
LSTM
28.62%
WReN
30.30%
CNN
41.86%
ResNet
15.08%
LSTM+DRT
23.26%
WReN+DRT
CNN+DRT
30.06%
ResNet+DRT 59.56% 58.08% 46.53%
Human
81.82%
100%
Solver⋆

84.41% 95.45%
100%
100%

6.34%

12.84% 12.35% 12.15% 12.99%
13.69%
8.38% 10.56%
7.49%
28.27%
39.43% 41.26% 43.20% 37.54%
33.53%
58.77% 60.16% 63.19% 53.12%
44.29%
13.79% 13.24% 13.99% 13.29%
14.09%
8.93% 12.35%
6.99%
29.51%
34.57%
45.49% 45.54% 45.93% 37.54%
50.40% 65.82% 67.11% 69.09% 60.11%
79.55%
86.36% 81.81% 86.36% 81.81%
100%
100%
100%

8.43%

100%

100%

2x2Grid and 3x3Grid. Two interesting observations:
1. For ﬁgure conﬁgurations with multiple components, al-
though each component in Left-Right, Up-Down,
and Out-InCenter has only one object, making the
reasoning similar to Center except that the two compo-
nents are independent, human subjects become less ac-
curate in selecting the correct answer.

2. Even if Up-Down could be regarded as a simple trans-
pose of Left-Right, there exists some notable dif-
ference. Such effect is also implied by the “inversion
effects” in cognition; for instance, inversion disrupts
face perception, particularly sensitivity to spatial rela-
tions [8, 29].
In terms of model performance, a counter-intuitive result
is: computer vision systems do not achieve the best accuracy
across all other conﬁgurations in the seemingly easiest ﬁg-
ure conﬁguration for human subjects (Center). We further
realize that the LSTM model and the WReN model perform
only slightly better than random guess (12.5%). Such results
contradicting to [3] might be attributed to the diverse ﬁg-
ure conﬁgurations in RAVEN. Unlike LSTM whose accu-
racy across different conﬁgurations is more or less uniform,
WReN achieves higher accuracy on conﬁgurations consist-
ing of multiple randomly distributed objects (2x2Grid and
3x3Grid), with drastically degrading performance in con-
ﬁgurations consisting of independent image components.
This suggests WReN is biased to grid-like conﬁgurations
(majority of PGM) but not others that require compositional
reasoning (as in RAVEN). In contrast, a simple CNN model
with MLP doubles the performance of WReN on RAVEN,
with a tripled performance if the backbone is ResNet-18.

We observe a consistent performance improvement
across different models after incorporating DRT, suggest-
ing the effectiveness of the structure information in this
visual reasoning problem. While the performance boost is
only marginal in LSTM and WReN, we notice a marked
accuracy increase in the CNN- and ResNet-based models
(6.63% and 16.58% relative increase respectively). How-

ever, the performance gap between artiﬁcial vision systems
and humans are still signiﬁcant (up to 37% in 2x2Grid),
calling for further research to bridge the gap.

6.4. Effects of Auxiliary Training

Barrett et al. [3] mentioned that training WReN with
a ﬁne-tuned auxiliary task could further give the model a
10% performance improvement. We also test the inﬂuence
of auxiliary training on RAVEN. First, we test the effects
of an auxiliary task to classify the rules and attributes on
WReN and our best performing model ResNet+DRT. The
setting is similar to [3], where we perform an OR operation
on a set of multi-hot vectors describing the rules and the
attributes they apply to. The model is then tasked to both
correctly ﬁnd the answer and classify the rule set with its
governing attributes. The ﬁnal loss becomes

Ltotal = Ltarget + βLrule,

(4)

where Ltarget denotes the cross-entropy loss for the an-
swer, Lrule the multi-label classiﬁcation loss for the rule
set, and β the balancing factor. We observe no performance
change on WReN but a serious performance downgrade on
ResNet+DRT (from 59.56% to 20.71%).

Since RAVEN comes with structure annotations, we fur-
ther ask whether adding a structure prediction loss could
help the model improve performance. To this end, we cast
the experiment in a similar setting where we design a multi-
hot vector describing the structure of each problem instance
and train the model to minimize

Ltotal = Ltarget + αLstruct,

(5)

where Lstruct denotes the multi-label classiﬁcation loss for
the problem structure, and α the balancing factor. In this
experiment, we observe a slight performance decrease in
ResNet+DRT (from 59.56% to 56.86%). A similar effect is
noticed on WReN (from 14.69% to 12.58%).

5323

6.5. Test on Generalization

One interesting question we would like to ask is how a
model trained well on one ﬁgure conﬁguration performs on
another similar ﬁgure conﬁguration. This could be a mea-
sure of models’ generalizability and compositional reason-
ing ability. Fortunately, RAVEN naturally provides us with
a test bed. To do this, we ﬁrst identify several related con-
ﬁguration regimes:
• Train on Center and test on Left-Right, Up-Down,
and Out-InCenter. This setting directly challenges
the compositional reasoning ability of the model as it
requires the model to generalize the rules learned in a
single-component conﬁguration to conﬁgurations with
multiple independent but similar components.

• Train on Left-Right and test on Up-Down, and vice-
versa. Note that for Left-Right and Up-Down, one
could be regarded as a transpose of another. Thus, the
test could measure whether the model simply memorizes
the pattern in one conﬁguration.

• Train on 2x2Grid and test on 3x3Grid, and vice-
versa. Both conﬁgurations involve multi-object interac-
tions. Therefore the test could measure the generalization
when the number of objects changes.

The following results are all reported using the best per-
forming model, i.e., ResNet+DRT.

Table 3. Generalization test. The model is trained on Center and
tested on three other conﬁgurations.

Center

Left-Right

Up-Down

Out-InCenter

51.87%

40.03%

35.46%

38.84%

Table 4. Generalization test. The row shows conﬁgurations the
model is trained on and the column the model is tested on.
Up-Down

Left-Right

Left-Right
Up-Down

41.07%
39.48%

38.10%
43.60%

Table 5. Generalization test. The row shows conﬁgurations the
model is trained on and the column the model is tested on.

2x2Grid 3x3Grid

2x2Grid
3x3Grid

40.93%
39.14%

38.69%
43.72%

Table 3, 4 and 5 show the result of our model generaliza-

tion test. We observe:
• The model dedicated to a single ﬁgure conﬁguration does
not achieve better test accuracy than one trained on all
conﬁgurations together. This effect justiﬁes the impor-
tance of the diversity of RAVEN, showing that increasing
the number of ﬁgure conﬁgurations could actually im-
prove the model performance.

• Table 3 also implies that a certain level of composi-
tional reasoning, though weak, exists in the model, as the
three other conﬁgurations could be regarded as a multi-
component composition of Center.

• In Table 4, we observe no major differences in terms of
test accuracy. This suggests that the model could success-
fully transfer the knowledge learned in a scenario to a
very similar counterpart, when one conﬁguration is the
transpose of another.

• From Table 5, we notice that the model trained on
3x3Grid could generalize to 2x2Grid with only mi-
nor difference from the one dedicated to 2x2Grid. This
could be attributed to the fact that in the 3x3Grid con-
ﬁguration, there could be instances with object distribu-
tion similar to that in 2x2Grid, but not vice versa.

7. Conclusion

We present a new dataset for Relational and Analogi-
cal Visual Reasoning in the context of Raven’s Progres-
sive Matrices (RPM), called RAVEN. Unlike previous
work, we apply a systematic and structured tool, i.e., At-
tributed Stochastic Image Grammar (A-SIG), to generate
the dataset, such that every problem instance comes with
rich annotations. This tool also makes RAVEN diverse and
easily extendable. One distinguishing feature that tells apart
RAVEN from other work is the introduction of the struc-
ture. We also recruit quality human subjects to benchmark
human performance on the RAVEN dataset. These aspects
ﬁll two important missing points in previous works.

We further propose a novel neural module called Dy-
namic Residual Tree (DRT) that leverages the structure an-
notations for each problem. Extensive experiments show
that models augmented with DRT enjoy consistent perfor-
mance improvement, suggesting the effectiveness of using
structure information in solving RPM. However, the differ-
ence between machine algorithms and humans clearly man-
ifests itself in the notable performance gap, even in an unfair
situation where machines experience an intensive training
session while humans do not. We also realize that auxiliary
tasks do not help performance on RAVEN. The generaliza-
tion test shows the importance of diversity of the dataset,
and also indicates current computer vision methods do ex-
hibit a certain level of reasoning ability, though weak.

The entire work still leaves us many mysteries. Humans
seem to apply a combination of the top-down and bottom-
up method in solving RPM. How could we incorporate this
into a model? What is the correct way of formulating visual
reasoning? Is it model ﬁtting? Is deep learning the ultimate
way to visual reasoning? If not, how could we revise the
models? If yes, how could we improve the models?

Finally, we hope these unresolved questions would call

for attention into this challenging problem.

Acknowledgement: The authors thank Yuxin Chi and
Prof. Ying Nian Wu and Prof. Hongjing Lu at UCLA Statis-
tics Department for helpful discussions. The work reported
herein was supported by DARPA XAI grant N66001-17-
2-4029, ONR MURI grant N00014-16-1-2007, ARO grant
W911NF-18-1-0296, and a NVIDIA GPU donation.

5324

References

[1] S. Aditya, Y. Yang, and C. Baral. Explicit reasoning
over end-to-end neural architectures for visual ques-
tion answering. Proceedings of AAAI Conference on
Artiﬁcial Intelligence (AAAI), 2018. 3

[2] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. Lawrence Zitnick, and D. Parikh. Vqa: Visual
question answering. In Proceedings of International
Conference on Computer Vision (ICCV), pages 2425–
2433, 2015. 1

[3] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lil-
licrap. Measuring abstract reasoning in neural net-
works. In Proceedings of International Conference on
Machine Learning (ICML), pages 511–520, 2018. 2,
3, 4, 5, 6, 7

[4] Y. Bisk, K. J. Shih, Y. Choi, and D. Marcu. Learn-
ing interpretable spatial operations in a rich 3d blocks
world. Proceedings of AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2018. 3

[5] F. W. Campbell and J. Robson. Application of fourier
analysis to the visibility of gratings. The Journal of
physiology, 197(3):551–566, 1968. 1

[6] Q. Cao, X. Liang, B. Li, G. Li, and L. Lin. Visual ques-
tion reasoning on general dependency tree.
In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 3

[7] P. A. Carpenter, M. A. Just, and P. Shell. What one
intelligence test measures: a theoretical account of the
processing in the raven progressive matrices test. Psy-
chological review, 97(3):404, 1990. 1, 2, 3, 4, 5

[8] K. Crookes and E. McKone. Early maturity of face
recognition: No childhood development of holistic
processing, novel face encoding, or face-space. Cog-
nition, 111(2):219–247, 2009. 7

[9] R. E Snow, P. Kyllonen, and B. Marshalek. The topog-
raphy of ability and learning correlations. Advances in
the psychology of human intelligence, pages 47–103,
1984. 2

[10] T. Evans. A Heuristic Program to Solve Geometric

Analogy Problems. PhD thesis, MIT, 1962. 2

[11] T. G. Evans. A heuristic program to solve geometric-
analogy problems. In Proceedings of the April 21-23,
1964, spring joint computer conference, 1964. 2

[12] K. S. Fu. Syntactic methods in pattern recognition,

volume 112. Elsevier, 1974. 2

[13] C.-e. Guo, S.-C. Zhu, and Y. N. Wu. Primal sketch:
Integrating structure and texture. Computer Vision and
Image Understanding (CVIU), 106(1):5–19, 2007. 1

the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016. 6

[15] S. Hochreiter and J. Schmidhuber. Long short-term

memory. Neural computation, 1997. 5

[16] K. J. Holyoak, K. J. Holyoak, and P. Thagard. Mental
leaps: Analogy in creative thought. MIT press, 1996.
1

[17] D. Hoshen and M. Werman.

Iq of neural networks.

arXiv preprint arXiv:1710.01692, 2017. 3, 6

[18] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and
K. Saenko. Learning to reason: End-to-end module
networks for visual question answering. In Proceed-
ings of International Conference on Computer Vision
(ICCV), 2017. 3

[19] D. A. Hudson and C. D. Manning. Compositional
arXiv

attention networks for machine reasoning.
preprint arXiv:1803.03067, 2018. 3

[20] S. Ioffe and C. Szegedy. Batch normalization: Ac-
celerating deep network training by reducing internal
covariate shift. In Proceedings of International Con-
ference on Machine Learning (ICML), 2015. 6

[21] S. M. Jaeggi, M. Buschkuehl, J. Jonides, and W. J.
Perrig.
Improving ﬂuid intelligence with training
on working memory. Proceedings of the National
Academy of Sciences, 105(19):6829–6833, 2008. 2

[22] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-
Fei, C. L. Zitnick, and R. Girshick. Clevr: A diagnos-
tic dataset for compositional language and elementary
visual reasoning.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 2, 3

[23] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoff-
man, L. Fei-Fei, C. L. Zitnick, and R. B. Girshick. In-
ferring and executing programs for visual reasoning.
In Proceedings of International Conference on Com-
puter Vision (ICCV), 2017. 3

[24] G. Kanizsa and G. Kanizsa. Organization in vision:
Essays on Gestalt perception, volume 49. Praeger
New York, 1979. 1

[25] D. P. Kingma and J. Ba. Adam: A method for stochas-
tic optimization. International Conference on Learn-
ing Representations (ICLR), 2014. 6

[26] T. N. Kipf and M. Welling. Semi-supervised clas-
siﬁcation with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016. 2

[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Im-
agenet classiﬁcation with deep convolutional neural
networks. In Proceedings of Advances in Neural In-
formation Processing Systems (NIPS), 2012. 6

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-
ual learning for image recognition. In Proceedings of

[28] M. Kunda, K. McGreggor, and A. K. Goel. A compu-
tational model for solving problems from the raven’s

5325

progressive matrices intelligence test using iconic vi-
sual representations. Cognitive Systems Research,
22:47–66, 2013. 2

[29] R. Le Grand, C. J. Mondloch, D. Maurer, and H. P.
Brent. Neuroperception: Early visual experience and
face processing. Nature, 410(6831):890, 2001. 7

[30] L. Lin, T. Wu, J. Porway, and Z. Xu. A stochas-
tic graph grammar for compositional object rep-
resentation and recognition.
Pattern Recognition,
42(7):1297–1307, 2009. 2

[42] A. Newell. You can’t play 20 questions with nature
and win: Projective comments on the papers of this
symposium.
In W. G. Chase, editor, Visual Infor-
mation Processing: Proceedings of the Eighth Annual
Carnegie Symposium on Cognition. Academic Press,
1973. 2

[43] S. Park and S.-C. Zhu. Attributed grammars for joint
estimation of human attributes, part and pose. In Pro-
ceedings of International Conference on Computer Vi-
sion (ICCV), 2015. 2

[31] D. R. Little, S. Lewandowsky, and T. L. Grifﬁths. A
bayesian model of rule induction in raven’s progres-
sive matrices. In Annual Meeting of the Cognitive Sci-
ence Society (CogSci), 2012. 3

[44] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer. Automatic differentiation in pytorch.
In
NIPS-W, 2017. 6

[32] A. Lovett and K. Forbus. Modeling visual problem
solving as analogical reasoning. Psychological Re-
view, 124(1):60, 2017. 3, 5

[33] A. Lovett, K. Forbus, and J. Usher. A structure-
mapping model of raven’s progressive matrices.
In
Proceedings of the Annual Meeting of the Cognitive
Science Society, 2010. 3, 5

[34] A. Lovett, E. Tomai, K. Forbus, and J. Usher. Solving
geometric analogy problems through two-stage ana-
logical mapping. Cognitive science, 33(7):1192–1231,
2009. 3, 5

[35] D. Marr. Vision: A computational investigation into.

WH Freeman, 1982. 1

[36] D. Mascharka, P. Tran, R. Soklaski, and A. Majum-
dar. Transparency by design: Closing the gap between
performance and interpretability in visual reasoning.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 3

[37] K. McGreggor and A. K. Goel. Conﬁdent reasoning
on raven’s progressive matrices tests. In Proceedings
of AAAI Conference on Artiﬁcial Intelligence (AAAI),
pages 380–386, 2014. 3

[38] K. McGreggor, M. Kunda, and A. Goel. Fractals and

ravens. Artiﬁcial Intelligence, 215:1–23, 2014. 3

[39] C. S. Mekik, R. Sun, and D. Y. Dai. Similarity-
based reasoning, raven’s matrices, and general intel-
ligence. In Proceedings of International Joint Confer-
ence on Artiﬁcial Intelligence (IJCAI), pages 1576–
1582, 2018. 3

[40] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and
phrases and their compositionality. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS), 2013. 6

[41] V. Nair and G. E. Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines.
In Proceed-
ings of International Conference on Machine Learn-
ing (ICML), 2010. 5, 6

[45] J. Pennington, R. Socher, and C. Manning. Glove:
Global vectors for word representation. In Proceed-
ings of the conference on Empirical Methods in Natu-
ral Language Processing (EMNLP, 2014. 6

[46] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and
A. Courville. Film: Visual reasoning with a general
conditioning layer.
In Proceedings of AAAI Confer-
ence on Artiﬁcial Intelligence (AAAI), 2018. 3

[47] J. C. e. a. Raven. Raven’s progressive matrices. West-

ern Psychological Services, 1938. 2, 5

[48] M. Ren, R. Kiros, and R. Zemel. Exploring models
and data for image question answering. In Proceed-
ings of Advances in Neural Information Processing
Systems (NIPS), 2015. 1

[49] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. Lillicrap. A simple
neural network module for relational reasoning.
In
Proceedings of Advances in Neural Information Pro-
cessing Systems (NIPS), 2017. 3

[50] S. Shegheva and A. K. Goel. The structural afﬁnity
method for solving the raven’s progressive matrices
test for intelligence. In Proceedings of AAAI Confer-
ence on Artiﬁcial Intelligence (AAAI), 2018. 3

[51] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
and R. Salakhutdinov. Dropout: a simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 2014. 6

[52] C. Stranneg˚ard, S. Cirillo, and V. Str¨om. An anthro-
pomorphic method for progressive matrix problems.
Cognitive Systems Research, 22:35–46, 2013. 2

[53] K. S. Tai, R. Socher, and C. D. Manning.

Im-
proved semantic representations from tree-structured
long short-term memory networks. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2015. 2, 5

[54] L. L. Thurstone and T. G. Thurstone. Factorial studies

of intelligence. Psychometric monographs, 1941. 2

5326

[55] K. Wang and Z. Su. Automatic generation of raven’s
progressive matrices. In Proceedings of International
Joint Conference on Artiﬁcial Intelligence (IJCAI),
2015. 2, 3, 4, 5

[56] T.-F. Wu, G.-S. Xia, and S.-C. Zhu. Compositional
boosting for computing hierarchical image structures.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2007. 2

[57] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K.
Wong, and W.-c. Woo. Convolutional lstm network:
A machine learning approach for precipitation now-
casting. In Proceedings of Advances in Neural Infor-
mation Processing Systems (NIPS), 2015. 6

[58] K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B.
Tenenbaum. Neural-symbolic vqa: Disentangling rea-
soning from vision and language understanding. arXiv
preprint arXiv:1810.02338, 2018. 1, 3

[59] C. Zhu, Y. Zhao, S. Huang, K. Tu, and Y. Ma. Struc-
tured attentions for visual question answering. In Pro-
ceedings of International Conference on Computer Vi-
sion (ICCV), 2017. 3

[60] J. Zhu, T. Wu, S.-C. Zhu, X. Yang, and W. Zhang.
A reconﬁgurable tangram model for scene representa-
tion and categorization. IEEE Transactions on Image
Processing, 25(1):150–166, 2016. 2

[61] S.-C. Zhu, D. Mumford, et al. A stochastic grammar
of images. Foundations and Trends R(cid:13) in Computer
Graphics and Vision, 2(4):259–362, 2007. 2

[62] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Vi-
sual7w: Grounded question answering in images. In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2016. 1

5327

