Structured Binary Neural Networks for Accurate Image Classiﬁcation and

Semantic Segmentation

Bohan Zhuang1

Chunhua Shen1∗ Mingkui Tan2

Lingqiao Liu1

Ian Reid1

1Australian Centre for Robotic Vision, The University of Adelaide

2South China University of Technology

Abstract

In this paper, we propose to train convolutional neural
networks (CNNs) with both binarized weights and activa-
tions, leading to quantized models speciﬁcally for mobile
devices with limited power capacity and computation re-
sources. Previous works on quantizing CNNs seek to ap-
proximate the ﬂoating-point information using a set of dis-
crete values, which we call value approximation, but typ-
ically assume the same architecture as the full-precision
networks. However, we take a novel “structure approxi-
mation” view for quantization—it is very likely that a dif-
ferent architecture may be better for best performance. In
particular, we propose a “network decomposition” strat-
egy, termed Group-Net, in which we divide the network into
groups. In this way, each full-precision group can be effec-
tively reconstructed by aggregating a set of homogeneous
binary branches. In addition, we learn effective connections
among groups to improve the representational capability.
Moreover, the proposed Group-Net shows strong general-
ization to other tasks. For instance, we extend Group-Net
for highly accurate semantic segmentation by embedding
rich context into the binary structure. Experiments on both
classiﬁcation and semantic segmentation tasks demonstrate
the superior performance of the proposed methods over var-
ious popular architectures. In particular, we outperform the
previous best binary neural networks in terms of accuracy
and major computation savings.

1. Introduction

Designing deeper and wider convolutional neural networks
has led to signiﬁcant breakthroughs in many machine learn-
ing tasks, such as image classiﬁcation [17,26], object detec-
tion [40, 41] and object segmentation [7, 34]. However, ac-
curate deep models often require billions of FLOPs, which
makes it infeasible for deep models to run many real-time
applications on resource constrained mobile platforms. To

∗C. Shen is the corresponding author.

solve this problem, many existing works focus on network
pruning [18, 28, 54], low-bit quantization [24, 53] and/or ef-
ﬁcient architecture design [8, 21]. Among them, the quan-
tization approaches represent weights and activations with
low bitwidth ﬁxed-point integers, and thus the dot product
can be computed by several XNOR-popcount bitwise oper-
ations. The XNOR of two bits requires only a single logic
gate instead of using hundreds units for ﬂoating point multi-
plication [10,14]. Binarization [22,39] is an extreme quanti-
zation approach where both the weights and activations are
represented by a single bit, either +1 or −1. In this paper,
we aim to design highly accurate binary neural networks
(BNNs) from both the quantization and efﬁcient architec-
ture design perspectives.

Existing quantization methods can be mainly divided
into two categories. The ﬁrst category methods seek to de-
sign more effective optimization algorithms to ﬁnd better
local minima for quantized weights. These works either in-
troduce knowledge distillation [35,38,53] or use loss-aware
objectives [19, 20]. The second category approaches fo-
cus on improving the quantization function [4, 48, 51]. To
maintain good performance, it is essential to learn suitable
mappings between discrete values and their ﬂoating-point
counterparts . However, designing quantization function is
highly non-trivial especially for BNNs, since the quantiza-
tion functions are often non-differentiable and gradients can
only be roughly approximated.

The above two categories of methods belong to value
approximation, which seeks to quantize weights and/or ac-
tivations by preserving most of the representational ability
of the original network. However, the value approximation
approaches have a natural limitation that it is merely a lo-
cal approximation. Moreover, these methods often lacks of
adaptive ability to general tasks. Given a pretrained model
on a speciﬁc task, the quantization error will inevitably oc-
cur and the ﬁnal performance may be affected.

In this paper, we seek to explore a third category called
structure approximation . The main objective is to redesign
a binary architecture that can directly match the capability
of a ﬂoating-point model. In particular, we propose a Struc-

413

tured Binary Neural Network called Group-Net to partition
the full-precision model into groups and use a set of paral-
lel binary bases to approximate its ﬂoating-point structure
counterpart. In this way, higher-level structural information
can be better preserved than the value approximation ap-
proaches.

Furthermore, relying on the proposed structured model,
we are able to design ﬂexible binary structures according
to different tasks and exploit task-speciﬁc information or
structures to compensate the quantization loss and facilitate
training. For example, when transferring Group-Net from
image classiﬁcation to semantic segmentation, we are mo-
tivated by the structure of Atrous Spatial Pyramid Pooling
(ASPP) [5]. In DeepLab v3 [6] and v3+ [7], ASPP is merely
applied on the top of extracted features while each block in
the backbone network can employ one atrous rate only. In
contrast, we propose to directly apply different atrous rates
on parallel binary bases in the backbone network, which
is equivalent to absorbing ASPP into the feature extraction
stage. Thus, we signiﬁcantly boost the performance on se-
mantic segmentation, without increasing the computation
complexity of the binary convolutions.

In general, it is nontrivial to extend previous value ap-
proximation based quantization approaches to more chal-
lenging tasks such as semantic segmentation (or other gen-
eral computer vision tasks). However, as will be shown, our
Group-Net can be easily extended to other tasks. Never-
theless, it is worth mentioning that value and structure ap-
proximation are complementary rather than contradictory.
In other words, both are important and should be exploited
to obtain highly accurate BNNs.

Our methods are also motivated by those energy-efﬁcient
architecture design approaches [8, 21, 23, 49] which seek to
replace the traditional expensive convolution with compu-
tational efﬁcient convolutional operations (i.e., depthwise
separable convolution, 1 × 1 convolution). Nevertheless,
we propose to design binary network architectures for dedi-
cated hardware from the quantization view. We highlight
that while most existing quantization works focus on di-
rectly quantizing the full-precision architecture, at this point
in time we do begin to explore alternative architectures that
shall be better suited for dealing with binary weights and ac-
tivations. In particular, apart from decomposing each group
into several binary bases, we also propose to learn the con-
nections between each group by introducing a fusion gate.
Moreover, Group-Net can be possibly further improved
with Neural Architecture Search methods [37, 55, 56] .

Our contributions are summarized as follows:

• We propose to design accurate BNNs structures from
the structure approximation perspective. Speciﬁcally,
we divide the networks into groups and approximate
each group using a set of binary bases. We also pro-
pose to automatically learn the decomposition by in-

troducing soft connections.

• The proposed Group-Net has strong ﬂexibility and can
be easily extended to other tasks. For instance, in
this paper we propose Binary Parallel Atrous Convo-
lution (BPAC), which embeds rich multi-scale con-
text into BNNs for accurate semantic segmentation.
Group-Net with BPAC signiﬁcantly improves the per-
formance while maintaining the complexity compared
to employ Group-Net only.

• We evaluate our models on ImageNet and PASCAL
VOC datasets based on ResNet. Extensive exper-
iments show the proposed Group-Net achieves the
state-of-the-art trade-off between accuracy and com-
putational complexity.

We review some relevant works in the sequel.
Network quantization: The recent increasing demand for
implementing ﬁxed point deep neural networks on embed-
ded devices motivates the study of network quantization.
Fixed-point approaches that use low-bitwidth discrete val-
ues to approximate real ones have been extensively explored
in the literature [4, 22, 29, 39, 51, 53]. BNNs [22, 39] pro-
pose to constrain both weights and activations to binary val-
ues (i.e., +1 and −1), where the multiply-accumulations
can be replaced by purely xnor(·) and popcount(·) opera-
tions. To make a trade-off between accuracy and complex-
ity, [13, 15, 27, 47] propose to recursively perform residual
quantization and yield a series of binary tensors with de-
creasing magnitude scales. However, multiple binarizations
are sequential process which cannot be paralleled. In [29],
Lin et al. propose to use a linear combination of binary
bases to approximate the ﬂoating point tensor during for-
ward propagation. This inspires aspects of our approach,
but unlike all of these local tensor approximations, we addi-
tionally directly design BNNs from a structure approxima-
tion perspective.
Efﬁcient architecture design: There has been a rising in-
terest in designing efﬁcient architecture in the recent liter-
ature. Efﬁcient model designs like GoogLeNet [45] and
SqueezeNet [23] propose to replace 3×3 convolutional ker-
nels with 1×1 size to reduce the complexity while in-
creasing the depth and accuracy. Additionally, separable
convolutions are also proved to be effective in Inception
approaches [44, 46]. This idea is further generalized as
depthwise separable convolutions by Xception [8], Mo-
bileNet [21, 43] and ShufﬂeNet [49] to generate energy-
efﬁcient network structure. To avoid handcrafted heuristics,
neural architecture search [30, 31, 37, 55, 56] has been ex-
plored for automatic model design.

2. Method

Most previous literature has focused on value approxi-
mation by designing accurate binarization functions for

414

weights and activations (e.g., multiple binarizations [13, 15,
27, 29, 47]). In this paper, we seek to binarize both weights
and activations of CNNs from a “structure approximation”
view. In the following, we ﬁrst give the problem deﬁnition
and some basics about binarization in Sec. 2.1. Then, in
Sec. 2.2, we explain our binary architecture design strategy.
Finally, in Sec. 2.3, we describe how to utilize task-speciﬁc
attributes to generalize our approach to semantic segmenta-
tion.

2.1. Problem deﬁnition

For a convolutional

layer, we deﬁne the input x ∈
Rcin×win×hin , weight ﬁlter w ∈ Rc×w×h and the output
y ∈ Rcout×wout×hout , respectively.
Binarization of weights: Following [39], we approximate
the ﬂoating-point weight w by a binary weight ﬁlter bw and
a scaling factor α ∈ R+ such that w ≈ αbw, where bw is
the sign of w and α calculates the mean of absolute values
of w. In general, sign(·) is non-differentiable and so we
adopt the straight-through estimator [1] (STE) to approx-
imate the gradient calculation. Formally, the forward and
backward processes can be given as follows:

Forward : bw = sign(w),

Backward :

∂ℓ
∂w

=

∂ℓ
∂bw ·

∂bw
∂w ≈

∂ℓ
∂bw ,

(1)

where ℓ is the loss.
Binarization of activations: For activation binarization,
we utilize the piecewise polynomial function to approxi-
mate the sign function as in [33]. The forward and back-
ward can be written as:

Forward : ba = sign(x),
Backward : ∂ℓ

∂ba

∂x = ∂ℓ
∂x = 


∂ba · ∂ba
∂x ,
2 + 2x : −1 ≤ x < 0
2 − 2x : 0 ≤ x < 1
0 : otherwise

where

(2)

.

2.2. Structured Binary Network Decomposition

In this paper, we seek to design a new structural representa-
tion of a network for quantization. First of all, note that a
ﬂoat number in computer is represented by a ﬁxed-number
of binary digits. Motivated by this, rather than directly do-
ing the quantization via “value decomposition”, we propose
to decompose a network into binary structures while pre-
serving its representability.

Speciﬁcally, given a ﬂoating-point residual network Φ
with N blocks, we decompose Φ into P binary fragments
[F1, ..., FP ], where Fi(·) can be any binary structure. Note
that each Fi(·) can be different. A natural question arises:
can we ﬁnd some simple methods to decompose the net-
work with binary structures so that the representability can
be exactly preserved? To answer this question, we here ex-
plore two kinds of architectures for F(·), namely layer-wise

decomposition and group-wise decomposition in Sec. 2.2.1
and Sec. 2.2.2, respectively. After that, we will present a
novel strategy for automatic decomposition in Sec. 2.2.3.

2.2.1 Layer-wise binary decomposition

The key challenge of binary decomposition is how to re-
construct or approximate the ﬂoating-point structure. The
simplest way is to approximate in a layer-wise manner. Let
B(·) be a binary convolutional layer and bw
i be the bina-
rized weights for the i-th base. In Fig. 1 (c), we illustrate the
layer-wise feature reconstruction for a single block. Speciﬁ-
cally, for each layer, we aim to ﬁt the full-precision structure
using a set of binarized homogeneous branches F(·) given
a ﬂoating-point input tensor x:

F(x) =

K

X

i=1

λiBi(x) =

K

X

i=1

λi(bw

i ⊕ sign(x)),

(3)

where ⊕ is bitwise operations xnor(·) and popcount(·), K
is the number of branches and λi is the combination co-
efﬁcient to be determined. During the training, the struc-
ture is ﬁxed and each binary convolutional kernel bw
i as
well as λi are directly updated with end-to-end optimiza-
tion. The scale scalar can be absorbed into batch normal-
ization when doing inference. Note that all Bi’s in Eq. (3)
have the same topology as the original ﬂoating-point coun-
terpart. Each binary branch gives a rough approximation
and all the approximations are aggregated to achieve more
accurate reconstruction to the original full precision convo-
lutional layer. Note that when K = 1, it corresponds to di-
rectly binarize the ﬂoating-point convolutional layer (Fig. 1
(b)). However, with more branches (a larger K), we are ex-
pected to achieve more accurate approximation with more
complex transformations.

During the inference, the homogeneous K bases can be
parallelizable and thus the structure is hardware friendly.
This will bring signiﬁcant gain in speed-up of the inference.
Speciﬁcally, the bitwise XNOR operation and bit-counting
can be performed in a parallel of 64 by the current gener-
ation of CPUs [33, 39]. And we just need to calculate K
binary convolutions and K full-precision additions. As a
result, the speed-up ratio σ for a convolutional layer can be
calculated as:

σ =

=

cincoutwhwinhin

1

64 (Kcincoutwhwinhin) + Kcoutwouthout
64
K ·

cinwhwinhin + 64wouthout

cinwhwinhin

.

,

(4)

We take one layer in ResNet for example. If we set cin =
256, w × h = 3 × 3, win = hin = wout = hout = 28,
K = 5, then it can reach 12.45× speedup. But in practice,
each branch can be implemented in parallel. And the actual
speedup ratio is also inﬂuenced by the process of memory
read and thread communication.

415

conv

conv

⊕

B(⋅)

B(⋅)

⊕

ℱ

(⋅)

B(⋅)
…

B(⋅)

⊕

λ
1

B(⋅)
⊕…

B(⋅)

λ
K

⊕

(a)

(b)

(c)

Figure 1: Overview of the baseline binarization method and the proposed layer-wise binary decomposition. We take one residual block with two con-
volutional layers for illustration. For convenience, we omit batch normalization and nonlinearities. (a): The ﬂoating-point residual block. (b): Direct
binarization of a full-precision block. (c): Layer-wise binary decomposition in Eq. (3), where we use a set of binary convolutional layers B(·) to approxi-
mate a ﬂoating-point convolutional layer.

(a)

conv

conv

⊕

conv

conv

⊕

B(⋅)

B(⋅) ⊕ λ

1

B(⋅)

B(⋅) ⊕ λ

1

(b)

…

⊕

…

⊕

B(⋅)

B(⋅) ⊕

λ
K

B(⋅)

B(⋅) ⊕

λ
K

G(⋅)

B(⋅)

B(⋅)

⊕

B(⋅)

B(⋅)

(c)

…

B(⋅)

B(⋅)

⊕

B(⋅)

B(⋅)

H (⋅)

⊕ λ

1

⊕

⊕

λ
K

Figure 2: Illustration of the proposed group-wise binary decomposition
strategy. We take two residual blocks for description. (a): The ﬂoating-
point residual blocks.
(b): Basic group-wise binary decomposition in
Eq. (5), where we approximate a whole block with a linear combination
of binary blocks G(·). (c): We approximate a whole group with homo-
geneous binary bases H(·), where each group consists of several blocks.
This corresponds to Eq. (6).

set Gi(·) as the basic residual block [17] which is shown
in Fig. 2 (a). Considering the residual architecture, we can
decompose F(x) by extending Eq. (3) as:

F(x) =

K

X

i=1

λiGi(x),

(5)

where λi is the combination coefﬁcient to be learned. In
Eq. (5), we use a linear combination of homogeneous bi-
nary bases to approximate one group, where each base Gi
is a binarized block. In this way, we effectively keep the
original residual structure in each base to preserve the net-
work capacity. As shown in Sec. 4.3.1, the group-wise de-
composition strategy performs much better than the simple
layer-wise approximation.

Furthermore, the group-wise approximation is ﬂexible.
We now analyze the case where each group may contain
different number of blocks. Suppose we partition the net-
work into P groups and it follows a simple rule that each
group must include one or multiple complete residual build-
ing blocks. For the p-th group, we consider the blocks set
T ∈ {Tp−1 +1, ..., Tp}, where the index Tp−1 = 0 if p = 1.
And we can extend Eq. (5) into multiple blocks format:

2.2.2 Group-wise binary decomposition

K

In the layer-wise approach, we approximate each layer with
multiple branches of binary layers. Note each branch will
introduce a certain amount of error and the error may accu-
mulate due to the aggregation of multi-branches. As a re-
sult, this strategy may incur severe quantization errors and
bring large deviation for gradients during backpropagation.
To alleviate the above issue, we further propose a more ﬂex-
ible decomposition strategy called group-wise binary de-
composition, to preserve more structural information during
approximation.

To explore the group-structure decomposition, we ﬁrst
consider a simple case where each group consists of only
one block. Then, the layer-wise approximation strategy can
be easily extended to the group-wise case. As shown in
Fig. 2 (b), similar to the layer-wise case, each ﬂoating-point
group is decomposed into multiple binary groups. How-
ever, each group Gi(·) is a binary block which consists of
several binary convolutions and ﬂoating-point element-wise
operations (i.e., ReLU, AddTensor). For example, we can

F(xTp−1+1) =

λiHi(x),

Pi=1
i (GTp−1

i

λiGTp

=

K

Pi=1

(...(GTp−1+1

i

(xTp−1+1))...)),

(6)

where H(·) is a cascade of consequent blocks which is
shown in Fig. 2 (c). Based on F(·), we can efﬁciently con-
struct a network by stacking these groups and each group
may consist of one or multiple blocks. Different from
Eq. (5), we further expose a new dimension on each base,
which is the number of blocks. This greatly increases the
structure space and the ﬂexibility of decomposition. We il-
lustrate several possible connections in Sec. S1 in the sup-
plementary ﬁle and further describe how to learn the de-
composition in Sec. 2.2.3.

2.2.3 Learning for dynamic decomposition

There is a big challenge involved in Eq. (6). Note that the
network has N blocks and the possible number of connec-
tions is 2N . Clearly, it is not practical to enumerate all pos-
sible structures during the training. Here, we propose to

416

solve this problem by learning the structures for decompo-
sition dynamically. We introduce in a fusion gate as the soft
connection between blocks G(·). To this end, we ﬁrst deﬁne
the input of the i-th branch for the n-th block as:

C n
i = sigmoid(θn
i ),
i ⊙ Gn−1
xn
i = C n
i ) ⊙

+ (1 − C n

i

i

(xn−1
K

X

j=1

)

(7)

(a): The conventional ﬂoating-point dilated convolution.

Floating-point feature map

3x3 Conv, dilation rate=2

a11

a21

a31

a41

a51

a61

a71

a12

a22

a32

a42

a52

a62

a72

a13

a23

a33

a43

a53

a63

a73

a14

a24

a34

a44

a54

a64

a74

a15

a25

a35

a45

a55

a65

a75

a16

a26

a36

a46

a56

a66

a76

a17

a27

a37

a47

a57

a67

a77

w11

w12

w13

Output

⊛

w21

w22

w23

w31

w32

w33

λjGn−1

j

(xn−1

j

),

Sign

Multi-dilations decompose

where θ ∈ RK is a learnable parameter vector, C n
scalar and ⊙ is the Hadamard product.

i

is a gate

n−1

G
1

Fusion gate

1 − C
1

C
1

n

G
1

B(⋅)

B(⋅)

⊕

…

B(⋅)

B(⋅)

⊕

λ
1

⊕
λ
K

⊗

⊗ ⊕

B(⋅)

B(⋅)

⊕

…

B(⋅)

B(⋅)

⊕

λ
1

⊕
λ
K

Figure 3: Illustration of the soft connection between two neighbouring
blocks. For convenience, we only illustrate the fusion strategy for one
branch.

Here, the branch input xn

i is a weighted combination of
two paths. The ﬁrst path is the output of the corresponding
i-th branch in the (n − 1)-th block, which is a straight con-
nection. The second path is the aggregation output of the
(n − 1)-th block. The detailed structure is shown in Fig. 3.
In this way, we make more information ﬂow into the branch
and increase the gradient paths for improving the conver-
gence of BNNs.

Remarks: For the extreme case when

K

P

i=1

C n

i = 0,

K

Eq. (7) will be reduced to Eq. (5) which means we approxi-
mate the (n−1)-th and the n-th block independently. When
i = K, Eq. (7) is equivalent to Eq. (6) and we set H(·)
P
to be two consequent blocks and approximate the group as

C n

i=1

N

K

i = N K, it corre-
a whole. Interestingly, when
sponds to set H(·) in Eq. (6) to be a whole network and
directly ensemble K binary models.

P

P

n=1

i=1

C n

2.3. Extension to semantic segmentation

The key message conveyed in the proposed method is that
although each binary branch has a limited modeling capa-
bility, aggregating them together leads to a powerful model.
In this section, we show that this principle can be applied
to tasks other than image classiﬁcation. In particular, we
consider semantic segmentation which can be deemed as a
dense pixel-wise classiﬁcation problem. In the state-of-the-
art semantic segmentation network, the atrous convolutional
layer [6] is an important building block, which performs
convolution with a certain dilation rate. To directly apply

Binary feature map

-1

1

1

1

1

-1

-1

-1 -1

1

1

1

-1 -1

1

-1

1

1

-1 -1

-1

1

-1

1

1

1

-1

1

1

-1

-1

1

-1

1

1

-1 -1

-1

1

1

-1

1

1

-1 -1 -1

-1

1

1

-1 1 -1

⊕

1 -1 1

-1 1 -1

3x3 Conv
rate=1

-1

-1

1

3x3 Conv
rate=2

⊕

-1

1

1

-1

-1

-1

…

.
.

Output

Sum

(b):  The proposed Binary Parallel Atrous Convolution (BPAC). 

Figure 4: The comparison between conventional dilated convolution and
BPAC. For expression convenience, the group only has one convolutional
layer. ⊛ is the convolution operation and ⊕ indicates the XNOR-popcount
operations. (a): The original ﬂoating-point dilated convolution. (b): We
decompose the ﬂoating-point atrous convolution into a combination of bi-
nary bases, where each base uses a different dilated rate. We sum the
output features of each binary branch as the ﬁnal representation.

the proposed method to such a layer, one can construct mul-
tiple binary atrous convolutional branches with the same
structure and aggregate results from them. However, we
choose not to do this but propose an alternative strategy: we
use different dilation rates for each branch. In this way, the
model can leverage multiscale information as a by-product
of the network branch decomposition. It should be noted
that this scheme does not incur any additional model pa-
rameters and computational complexity compared with the
naive binary branch decomposition. The idea is illustrated
in Fig. 4 and we call this strategy Binary Parallel Atrous
Convolution (BPAC).

In this paper, we use the same ResNet backbone in [6,
7] with output stride=8, where the last two blocks em-
In BPAC, we keep rates =
ploy atrous convolution.
{2, 3, ..., K, K +1} and rates = {6, 7, ..., K +4, K +5} for
K bases in the last two blocks, respectively. Intriguingly, as
will be shown in Sec. 4.4, our strategy brings so much ben-
eﬁt that using ﬁve binary bases with BPAC achieves similar
performance as the original full precision network despite
the fact that it saves considerable computational cost.

3. Discussions

Complexity analysis: A comprehensive comparison of var-
ious quantization approaches over complexity and storage
is shown in Table 1. For example, in the previous state-

417

Table 1: Computational complexity and storage comparison of different quantization approaches. F : full-precision, B: binary, QK : K-bit quantization.

Model

Weights Activations

Operations

Memory saving Computation Saving

Full-precision DNN

[22, 39]
[9, 20]
[50, 52]

[35, 48, 51, 53]

[13, 15, 27, 29, 47]

Group-Net

F
B
B
QK
QK
K × B

F
B
F
F
QK
K × B

K × (B, B)

XNOR-popcount

+, -, ×
+, -
+, -, ×
+, -, ×

+, -, XNOR-popcount
+, -, XNOR-popcount

1

∼ 32×
∼ 32×
∼ 32
K ×
∼ 32
K ×
∼ 32
K ×
∼ 32
K ×

1

∼ 64×
∼ 2×
< 2×
< 64
K 2×
< 64
K 2×
< 64
K ×

of-the-art binary model ABC-Net [29], each convolutional
layer is approximated using K weight bases and K activa-
tion bases, which needs to calculate K 2 times binary con-
volution. In contrast, we just need to approximate several
groups with K structural bases. As reported in Sec. 4.2 , we
save approximate K times computational complexity while
still achieving comparable Top-1 accuracy. Since we use K
structural bases, the number of parameters increases by K
times in comparison to the full-precision counterpart. But
we still save memory bandwidth by 32/K times since all
the weights are binary in our paper. For our approach, there
exists element-wise operations between each group, so the
computational complexity saving is slightly less than 64
K ×.
Differences of Group-net from ﬁxed-point methods: The
proposed Group-net with K bases is different from the K-
bit ﬁxed-point approaches [35, 48, 51, 53].

We ﬁrst show how the inner product between ﬁxed-point
weights and activations can be computed by bitwise opera-

tions. Let a weight vector w ∈ RM be encoded by a vector
i ∈ {−1, 1}M , i = 1, ..., K. Assume we also quan-
bw
tize activations to K-bit. Similarly, the activations x can
j ∈ {−1, 1}M , j = 1, ..., K. Then, the
be encoded by ba
convolution can be written as

QK(wT )QK(x) =

K−1

K−1

X

i=0

X

j=0

2i+j(bw

i ⊕ ba
j ),

(8)

Then,

where QK(·) is any quantization function1.
During the inference, it needs to ﬁrst get the encod-
ing ba
j for each bit by looking up the quantization in-
it calculates and sums over K 2 times
tervals.
xnor(·) and popcount(·). The complexity is about O(K 2).
Note that the output range for a single output shall be
[−(2K − 1)2M, (2K − 1)2M ].
j via sign(x). More-
over, since we just need to calculate K times xnor(·) and
popcount(·) (see Eq. (3)), and then sum over the outputs,
the computational complexity is O(K). For binary convo-
lution, its output range is {-1, 1}. So the value range for
each element after summation is [−KM, KM ], in which
the number of distinct values is much less than that in ﬁxed-
point methods.

In contract, we directly obtain ba

1For simplicity, we only consider uniform quantization in this paper.

In summary, compared with K-bit ﬁxed-point meth-
ods, Group-Net with K bases just needs √K compu-
tational complexity and saves (2K − 1)2/K accumulator
bandwidth. Even √K-bit ﬁxed-point quantization requires

more memory bandwidth to feed signal in SRAM or in reg-
isters.
Differences of Group-net from multiple binarizations
methods:
In ABC-Net [29], a linear combination of
binary weight/activations bases are obtained from the
full-precision weights/activations without being directly
learned. In contrast, we directly design the binary network
structure, where binary weights are end-to-end optimized.
[13,15,27,47] propose to recursively approximate the resid-
ual error and obtain a series of binary maps corresponding
to different quantization scales. However, it is a sequential
process which cannot be paralleled. And all multiple bina-
rizations methods belong to local tensor approximation. In
contrast to value approximation, we propose a structure ap-
proximation approach to mimic the full-precision network.
Moreover, tensor-based methods are tightly designed to lo-
cal value approximation and are hardly generalized to other
tasks accordingly. In addition, our structure decomposition
strategy achieves much better performance than tensor-level
approximation as shown in Sec. 4.3.1. More discussions are
provided in Sec. S2 in the supplementary ﬁle.

4. Experiment

We deﬁne several methods for comparison as follows:
LBD: It implements the layer-wise binary decomposition
strategy described in Sec. 2.2.1. Group-Net:
It imple-
ments the full model with learnt soft connections described
in Sec. 2.2.3. Following Bi-Real Net [32, 33], we apply
shortcut bypassing every binary convolution to improve the
convergence. Group-Net**: Based on Group-Net, we keep
the 1 × 1 downsampling convolution to full-precision simi-
lar to [2, 33].

4.1. Implementation details

As in [4, 39, 51, 53], we quantize the weights and activa-
tions of all convolutional layers except that the ﬁrst and the
last layer have full-precision. In all ImageNet experiments,
training images are resized to 256 × 256, and a 224 × 224
crop is randomly sampled from an image or its horizontal

418

ﬂip, with the per-pixel mean subtracted. We do not use any
further data augmentation in our implementation. We use a
simple single-crop testing for standard evaluation. No bias
term is utilized. We ﬁrst pretrain the full-precision model
as initialization with Tanh(·) as nonlinearity and ﬁne-tune
the binary counterpart. We use Adam [25] for optimiza-
tion. For training all binary networks, the mini-batch size
and weight decay are set to 256 and 0, respectively. The
learning rate starts at 5e-4 and is decayed twice by multi-
plying 0.1 at the 30th and 40th epoch. We train 50 epochs
in total. Following [4, 53], no dropout is used due to bina-
rization itself can be treated as a regularization. We apply
layer-reordering to the networks as: Sign → Conv → ReLU
→ BN. Inserting ReLU(·) after convolution is important for
convergence. Our simulation implementation is based on
Pytorch [36].

4.2. Evaluation on ImageNet

on

evaluated

ImageNet

proposed method

is
[42] dataset.

The
ImageNet
(ILSVRC2012)
is a large-
scale dataset which has ∼1.2M training images from 1K
categories and 50K validation images. Several representa-
tive networks are tested: ResNet-18 [17], ResNet-34 and
ResNet-50. As discussed in Sec. 3, binary approaches
and ﬁxed-point approaches differ a lot in computational
complexity as well as storage consumption. So we compare
the proposed approach with binary neural networks in
Table 2 and ﬁxed-point approaches in Table 3, respectively.

4.2.1 Comparison with binary neural networks

Since we employ binary weights and binary activations, we
directly compare to the previous state-of-the-art binary ap-
proaches, including BNN [22], XNOR-Net [39], Bi-Real
Net [33] and ABC-Net [29]. We report the results in Ta-
ble 2 and summarize the following points. 1): The most
comparable baseline for Group-Net is ABC-Net. As dis-
cussed in Sec. 3, we save considerable computational com-
plexity while still achieving better performance compared
to ABC-Net. In comparison to directly binarizing networks,
Group-Net achieves much better performance but needs K
times more storage and complexity. However, the K homo-
geneous bases can be easily parallelized on the real chip.
In summary, our approach achieves the best trade-off be-
tween computational complexity and prediction accuracy.
2): By comparing Group-Net** (5 bases) and Group-Net
(8 bases), we can observe comparable performance. It jus-
tiﬁes keeping 1 × 1 downsampling layers to full-precision
is crucial for preserving the performance. 3): For Bottle-
neck structure in ResNet-50, we ﬁnd larger quantization
error than the counterparts using basic blocks with 3 × 3
convolutions in ResNet-18 and ResNet-34. The similar ob-
servation is also claimed by [3]. We assume that this is
mainly attributable to the 1 × 1 convolutions in Bottleneck.
The reason is 1 × 1 ﬁlters are limited to two states only

(either 1 or -1) and they have very limited learning power.
What’s more, the bottleneck structure reduces the number
of ﬁlters signiﬁcantly, which means the gradient paths are
greatly reduced. In other words, it blocks the gradient ﬂow
through BNNs. Even though the bottleneck structure can
beneﬁt full-precision training, it is really needed to be re-
designed in BNNs. To increase gradient paths, the 1 × 1
convolutions should be removed.

4.2.2 Comparison with ﬁx-point approaches

Since we use K binary group bases, we compare our ap-
proach with at least √K-bit ﬁx-point approaches. In Ta-

ble 3, we compare our approach with the state-of-the-art
ﬁxed-point approaches DoReFa-Net [51], SYQ [12] and
LQ-Nets [48]. As described in Sec. 3, K binarizations
are more superior than the √K-bit width quantization with
respect to the resource consumption. Here, we set K=4.
DOREFA-Net and LQ-Nets use 2-bit weights and 2-bit acti-
vations. SYQ employs binary weights and 8-bit activations.
All the comparison results are directly cited from the cor-
responding papers. LQ-Nets is the current best-performing
ﬁxed-point approach and its activations have a long-tail dis-
tribution. We can observe that Group-Net requires less
memory bandwidth while still achieving comparable accu-
racy with LQ-Nets.

4.3. Ablation study

Due to the limited space, we provide more experiments in
Sec. S1 in the supplementary material.

4.3.1 Layer-wise vs. group-wise binary decomposition

We explore the difference between layer-wise and group-
wise design strategies in Table 4. By comparing the results,
we ﬁnd Group-Net outperforms LBD by 7.2% on the Top-
1 accuracy. Note that LBD approach can be treated as a
kind of tensor approximation which has similarities with
multiple binarizations methods in [13, 15, 27, 29, 47] and
the differences are described in Sec. 3. It strongly shows
the necessity for employing the group-wise decomposition
strategy to get promising results. We speculate that this sig-
niﬁcant gain is partly due to the preserved block structure
in binary bases.
It also proves that apart from designing
accurate binarization function, it is also essential to design
appropriate structure for BNNs.

4.4. Evaluation on PASCAL VOC

We evaluate the proposed methods on the PASCAL VOC
2012 semantic segmentation benchmark [11] which con-
tains 20 foreground object classes and one background
class. The original dataset contains 1,464 (train), 1,449
(val) and 1,456 (test) images. The dataset is augmented by
the extra annotations from [16], resulting in 10,582 training
images. The performance is measured in terms of averaged

419

Table 2: Comparison with the state-of-the-art binary models using ResNet-18, ResNet-34 and ResNet-50 on ImageNet. All the comparing results are
directly cited from the original papers. The metrics are Top-1 and Top-5 accuracy.

Model

Full BNN XNOR Bi-Real Net ABC-Net (25 bases) Group-Net (5 bases) Group-Net** (5 bases) Group-Net (8 bases)

ResNet-18

ResNet-34

ResNet-50

Top-1 % 69.7
Top-5 % 89.4
Top-1 % 73.2
Top-5 % 91.4
Top-1 % 76.0
Top-5 % 92.9

42.2
67.1

51.2
73.2

-
-
-
-

-
-
-
-

56.4
79.5
62.2
83.9

-
-

65.0
85.9
68.4
88.2
70.1
89.7

64.8
85.7
68.5
88.0
69.5
88.2

67.0
87.5
70.5
89.3
71.2
90.0

67.5
88.0
71.8
90.4
72.8
90.5

Table 3: Comparison with the state-of-the-art ﬁxed-point models with
ResNet-18 on ImageNet. The metrics are Top-1 and Top-5 accuracy.

Model

Full-precision

Group-Net** (4 bases)

Group-Net (4 bases)

LQ-Net [48]

DOREFA-Net [51]

SYQ [12]

W A Top-1 (%) Top-5 (%)
32
1
1
2
2
1

69.7
66.3
64.2
64.9
62.6
62.9

89.4
86.6
85.6
85.9
84.4
84.6

32
1
1
2
2
8

Table 4: Comparison with Group-Net and LBD using ResNet-18 on Ima-
geNet. The metrics are Top-1 and Top-5 accuracy.

Model

Bases Top-1 % Top-5 %

Full-precision

Group-Net

LBD

1
5
5

69.7
64.8
57.6

89.4
85.7
79.7

Table 5: Performance on PASCAL VOC 2012 validation set.

Model

Full-precision
LQ-Net (3-bit)

ResNet-18, FCN-32s

Group-Net

Group-Net + BPAC

Group-Net** + BPAC

Full-precision
LQ-Net (3-bit)

ResNet-18, FCN-16s

Group-Net

Group-Net + BPAC

Group-Net** + BPAC

Full-precision
LQ-Net (3-bit)

ResNet-34, FCN-32s

Group-Net

Group-Net + BPAC

Group-Net** + BPAC

Full-precision
LQ-Net (3-bit)

ResNet-50, FCN-32s

Group-Net

Group-Net + BPAC

Group-Net** + BPAC

-

2.4
4.4
1.1
-0.2

2.2
4.6
1.0
-0.4

mIOU ∆
64.9
-
62.5
60.5
63.8
65.1
67.3
65.1
62.7
66.3
67.7
72.7
70.4
68.2
71.2
72.8
73.1
70.7
67.2
70.4
71.0

2.4
5.9
2.7
2.1

2.3
4.5
1.5
-0.1

-

-

pixel intersection-over-union (mIOU) over 21 classes. Our
experiments are based on the original FCN [34]. For both
FCN-32s and FCN-16s, we adjust the dilation rates of the
last 2 blocks in ResNet with atrous convolution to make the
output stride equal to 8. We ﬁrst pretrain the binary back-
bone network on ImageNet dataset and ﬁne-tune it on PAS-
CAL VOC. During ﬁne-tuning, we use Adam with initial
learning rate=1e-4, weight decay=1e-5 and batch size=16.
We set the number of bases K = 5 in experiments. We
train 40 epochs in total and decay the learning rate by a fac-

tor of 10 at 20 and 30 epochs. We do not add any auxiliary
loss and ASPP. We empirically observe full-precision FCN
under dilation rates (4, 8) in last two blocks achieves the
best performance. The main results are in Table 5.

From the results, we can observe that when all bases us-
ing the same dilation rates, there is a large performance gap
with the full-precision counterpart. This performance drop
is consistent with the classiﬁcation results on ImageNet
dataset in Table 2.
It proves that the quality of extracted
features have a great impact on the segmentation perfor-
mance. What’s more, by utilizing task-speciﬁc BPAC, we
ﬁnd signiﬁcant performance increase with no computational
complexity added, which strongly justiﬁes the ﬂexibility of
Group-Net. Moreover, we also quantize the backbone net-
work using ﬁxed-point LQ-Nets with 3-bit weights and 3-
bit activations. Compared with LQ-Nets, we can achieve
comparable performance while saving considerable com-
plexity.
In addition, we can observe Group-Net + BPAC
based on ResNet-34 even outperform the counterpart on
ResNet-50. This shows the widely used bottleneck struc-
ture is not suited to BNNs as explained in Sec. 4.2.1. We
provide more analysis in Sec. S3 in the supplementary ﬁle.

5. Conclusion

In this paper, we have begun to explore highly efﬁcient and
accurate CNN architectures with binary weights and activa-
tions. Speciﬁcally, we have proposed to directly decompose
the full-precision network into multiple groups and each
group is approximated using a set of binary bases which
can be optimized in an end-to-end manner. We also pro-
pose to learn the decomposition automatically. Experimen-
tal results have proved the effectiveness of the proposed ap-
proach on the ImageNet classiﬁcation task. Moreover, we
have generalized Group-Net from image classiﬁcation task
to semantic segmentation and achieved promising perfor-
mance on PASCAL VOC. We have implemented the ho-
mogeneous multi-branch structure on CPU and achieved
promising acceleration on test-time inference.

Acknowledgement L. Liu was in part supported by
ARC DECRA Fellowship DE170101259. M. Tan was
partially supported by National Natural Science Founda-
tion of China (NSFC) 61602185, Program for Guang-
dong Introducing Innovative and Enterpreneurial Teams
2017ZT07X183.

420

References

[1] Y. Bengio, N. L´eonard, and A. Courville. Estimating or prop-
agating gradients through stochastic neurons for conditional
computation. arXiv preprint arXiv:1308.3432, 2013. 3

[2] J. Bethge, M. Bornstein, A. Loy, H. Yang, and C. Meinel.
Training competitive binary neural networks from scratch.
arXiv preprint arXiv:1812.01965, 2018. 6

[3] J. Bethge, H. Yang, C. Bartz, and C. Meinel.

Learn-
arXiv preprint

ing to train a binary neural network.
arXiv:1809.10463, 2018. 7

[4] Z. Cai, X. He, J. Sun, and N. Vasconcelos. Deep learn-
ing with low precision by half-wave gaussian quantization.
In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 5918–
5926, 2017. 1, 2, 6, 7

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE transactions on pattern analysis and ma-
chine intelligence, 40(4):834–848, 2018. 2

[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-
thinking atrous convolution for semantic image segmenta-
tion. arXiv preprint arXiv:1706.05587, 2017. 2, 5

[7] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam.
Encoder-decoder with atrous separable convolution for se-
mantic image segmentation. Proc. Eur. Conf. Comp. Vis.,
2018. 1, 2, 5

[8] F. Chollet. Xception: Deep learning with depthwise sepa-
In Proc. IEEE Conf. Comp. Vis. Patt.

rable convolutions.
Recogn., pages 1251–1258, 2017. 1, 2

[9] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In Proc. Adv. Neural Inf. Process. Syst., pages
3123–3131, 2015. 6

[10] A. Ehliar. Area efﬁcient ﬂoating-point adder and multiplier
with ieee-754 compatible semantics. In Field-Programmable
Technology (FPT), 2014 International Conference on, pages
131–138. IEEE. 1

[11] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. Int. J. Comp. Vis., 88(2):303–338, 2010. 7

[12] J. Faraone, N. Fraser, M. Blott, and P. H. Leong. Syq:
Learning symmetric quantization for efﬁcient deep neural
networks.
In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,
2018. 7, 8

[13] J. Fromm, S. Patel, and M. Philipose. Heterogeneous
In

bitwidth binarization in convolutional neural networks.
Proc. Adv. Neural Inf. Process. Syst., 2018. 2, 3, 6, 7

[14] G. Govindu, L. Zhuo, S. Choi, and V. Prasanna. Analysis
of high-performance ﬂoating-point arithmetic on fpgas. In
Parallel and Distributed Processing Symposium, 2004. Pro-
ceedings. 18th International, page 149. IEEE, 2004. 1

[15] Y. Guo, A. Yao, H. Zhao, and Y. Chen. Network sketching:
Exploiting binary structure in deep cnns. In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., pages 5955–5963, 2017. 2, 3, 6, 7
[16] B. Hariharan, P. Arbel´aez, L. Bourdev, S. Maji, and J. Ma-
lik. Semantic contours from inverse detectors. In Proc. Eur.
Conf. Comp. Vis., 2011. 7

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., pages 770–778, 2016. 1, 4, 7

[18] Y. He, X. Zhang, and J. Sun. Channel pruning for acceler-
ating very deep neural networks. In Proc. IEEE Int. Conf.
Comp. Vis., volume 2, page 6, 2017. 1

[19] L. Hou and J. T. Kwok. Loss-aware weight quantization of

deep networks. In Proc. Int. Conf. Learn. Repren., 2018. 1

[20] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of
deep networks. In Proc. Int. Conf. Learn. Repren., 2017. 1,
6

[21] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017. 1, 2

[22] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks. In Proc. Adv. Neu-
ral Inf. Process. Syst., pages 4107–4115, 2016. 1, 2, 6, 7

[23] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016. 2

[24] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
H. Adam, and D. Kalenichenko. Quantization and training
of neural networks for efﬁcient integer-arithmetic-only infer-
ence. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018.
1

[25] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In Proc. Int. Conf. Learn. Repren., 2015. 7

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In Proc.
Adv. Neural Inf. Process. Syst., pages 1097–1105, 2012. 1

[27] Z. Li, B. Ni, W. Zhang, X. Yang, and W. Gao. Perfor-
mance guaranteed network acceleration via high-order resid-
ual quantization. In Proc. IEEE Int. Conf. Comp. Vis., pages
2584–2592, 2017. 2, 3, 6, 7

[28] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and
D. Doermann. Towards optimal structured cnn pruning via
generative adversarial learning. In Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., 2019. 1

[29] X. Lin, C. Zhao, and W. Pan. Towards accurate binary con-
volutional neural network. In Proc. Adv. Neural Inf. Process.
Syst., pages 344–352, 2017. 2, 3, 6, 7

[30] C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, and K. Murphy. Progressive neural ar-
chitecture search. In Proc. Eur. Conf. Comp. Vis., 2018. 2

[31] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and
K. Kavukcuoglu. Hierarchical representations for efﬁcient
architecture search. In Proc. Int. Conf. Learn. Repren., 2018.
2

[32] Z. Liu, W. Luo, B. Wu, X. Yang, W. Liu, and K.-T. Cheng.
Bi-real net: Binarizing deep network towards real-network
performance. arXiv preprint arXiv:1811.01335, 2018. 6

[33] Z. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng.
Bi-real net: Enhancing the performance of 1-bit cnns with
improved representational capability and advanced training
algorithm. In Proc. Eur. Conf. Comp. Vis., 2018. 3, 6, 7

421

[50] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. Incremen-
tal network quantization: Towards lossless cnns with low-
precision weights. Proc. Int. Conf. Learn. Repren., 2017. 6
[51] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.
Dorefa-net: Training low bitwidth convolutional neural
networks with low bitwidth gradients.
arXiv preprint
arXiv:1606.06160, 2016. 1, 2, 6, 7, 8

[52] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. Proc. Int. Conf. Learn. Repren., 2017. 6

[53] B. Zhuang, C. Shen, M. Tan, L. Liu, and I. Reid. Towards ef-
fective low-bitwidth convolutional neural networks. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2018. 1, 2, 6, 7

[54] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu,
J. Huang, and J. Zhu. Discrimination-aware channel pruning
for deep neural networks. In Proc. Adv. Neural Inf. Process.
Syst., pages 883–894, 2018. 1

[55] B. Zoph and Q. V. Le. Neural architecture search with rein-
forcement learning. In Proc. Int. Conf. Learn. Repren., 2017.
2

[56] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning
transferable architectures for scalable image recognition. In
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018. 2

[34] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
In Proc. IEEE Conf.

networks for semantic segmentation.
Comp. Vis. Patt. Recogn., pages 3431–3440, 2015. 1, 8

[35] A. Mishra and D. Marr. Apprentice: Using knowledge dis-
tillation techniques to improve low-precision network accu-
racy. In Proc. Int. Conf. Learn. Repren., 2018. 1, 6

[36] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Au-
tomatic differentiation in pytorch. In Proc. Adv. Neural Inf.
Process. Syst. Workshops, 2017. 7

[37] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Ef-
ﬁcient neural architecture search via parameter sharing. In
Proc. Int. Conf. Mach. Learn., 2018. 2

[38] A. Polino, R. Pascanu, and D. Alistarh. Model compression
via distillation and quantization. In Proc. Int. Conf. Learn.
Repren., 2018. 1

[39] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In Proc. Eur. Conf. Comp. Vis., pages 525–542,
2016. 1, 2, 3, 6, 7

[40] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., pages 779–788, 2016.
1

[41] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Proc. Adv. Neural Inf. Process. Syst., pages 91–99, 2015. 1

[42] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. Int.
J. Comp. Vis., 115(3):211–252, 2015. 7

[43] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages
4510–4520, 2018. 2

[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning.
In Proc. AAAI Conf. on Arti. In-
tel., volume 4, page 12, 2017. 2

[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., pages 1–9, 2015. 2

[46] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision.
In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 2818–
2826, 2016. 2

[47] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In Proc. AAAI
Conf. on Arti. Intel., pages 2625–2631, 2017. 2, 3, 6, 7

[48] D. Zhang, J. Yang, D. Ye, and G. Hua. Lq-nets: Learned
quantization for highly accurate and compact deep neural
In Proc. Eur. Conf. Comp. Vis., 2018. 1, 6, 7,
networks.
8

[49] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mobile
devices. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018.
2

422

