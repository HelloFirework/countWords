Taking A Closer Look at Domain Shift:

Category-level Adversaries for Semantics Consistent Domain Adaptation

Yawei Luo1

,

2, Liang Zheng5, Tao Guan1

,

6, Junqing Yu1

,

4 ∗, Yi Yang2

3

,

1School of Computer Science & Technology, Huazhong University of Science & Technology

2CAI, University of Technology Sydney 3Baidu Research

4Center of Network and Computation, Huazhong University of Science & Technology

5Research School of Computer Science, Australian National University 6Farsee2 Tech. Co.

Abstract

We consider the problem of unsupervised domain adap-
tation in semantic segmentation. A key in this campaign
consists in reducing the domain shift, i.e., enforcing the
data distributions of the two domains to be similar. One of
the common strategies is to align the marginal distribution
in the feature space through adversarial learning. How-
ever, this global alignment strategy does not consider the
category-level joint distribution. A possible consequence
of such global movement is that some categories which are
originally well aligned between the source and target may be
incorrectly mapped, thus leading to worse segmentation re-
sults in target domain. To address this problem, we introduce
a category-level adversarial network, aiming to enforce local
semantic consistency during the trend of global alignment.
Our idea is to take a close look at the category-level joint
distribution and align each class with an adaptive adversar-
ial loss. Speciﬁcally, we reduce the weight of the adversarial
loss for category-level aligned features while increasing the
adversarial force for those poorly aligned. In this process,
we decide how well a feature is category-level aligned be-
tween source and target by a co-training approach. In two
domain adaptation tasks, i.e., GTA5 → Cityscapes and SYN-
THIA → Cityscapes, we validate that the proposed method
matches the state of the art in segmentation accuracy.

1. Introduction

Semantic segmentation aims to assign each pixel of a
photograph to a semantic class label. Currently, the achieve-
ment is at the price of large amount of dense pixel-level

∗Corresponding author (yjqing@hust.edu.cn).
This work was done when Yawei Luo (royalvane@hust.edu.cn) was a
visiting student at University of Technology Sydney. Part of this work was
done when Yi Yang (yee.i.yang@gmail.com) was visiting Baidu Research
during his Professional Experience Program. The code is publicly available
at https://github.com/RoyalVane/CLAN.

Figure 1. (Best viewed in color.) Illustration of traditional and the
proposed adversarial learning. The size of the solid gray arrow
represents the weight of the adversarial loss. (a) Traditional adver-
sarial learning ignores the semantic consistency when pursuing the
marginal distribution alignment. As a result, the global movement
might cause the well-aligned features (class A) to be mapped onto
different joint distributions (negative transfer). (b) The proposed
self-adaptive adversarial learning reweights the adversarial loss for
each feature by a local alignment score. Our method reduces the
inﬂuence of the adversaries when discovers a high semantic align-
ment score on a feature, and vice versa. As is shown, the proposed
strategy encourages a category-level joint distribution alignment
for both class A and class B.

annotations obtained by expensive human labor [4, 23, 27].
An alternative would be resorting to simulated data, such
as computer generated scenes [31, 32], so that unlimited
amount of labels are made available. However, models
trained with the simulated images do not generalize well
to realistic domains. The reason lies in the different data
distributions of the two domains, typically known as do-

2507

Source sample, class ATarget sample, class ASource sample, class BTarget sample, class B(a) Classical adversarial loss(b) Self-adaptive adversarial lossClassifier boundaryAdversarial lossSourceSourceSourceSourceTargetTargetTargetTargetCCC1C2C1C2main shift [37]. To address this issue, domain adaptation
approaches [35, 41, 14, 46, 17, 16, 13, 48] are proposed to
bridge the gap between the source and target domains. A
majority of recent methods [26, 24, 40, 43, 42] aim to align
the feature distributions of different domains. Works along
this line are based on the theoretical insights in [1] that min-
imizing the divergence between domains lowers the upper
bound of error on the target domain. Among this cohort of
domain adaptation methods, a common and pivotal step is
minimizing some distance metric between the source and
target feature distributions [24, 40]. Another popular choice,
which borrows the idea from adversarial learning [10], is to
minimize the accuracy of domain prediction. Through a min-
imax game between two adversarial networks, the generator
is trained to produce features that confuse the discriminator
while the latter is required to correctly classify which domain
the features are generated from.

Although the works along the path of adversarial learn-
ing have led to impressive results [39, 15, 22, 19, 43, 36],
they suffer from a major limitation: when the generator net-
work can perfectly fool the discriminator, it merely aligns
the global marginal distribution of the features in the two
domains ( i.e., P (Fs) ≈ P (Ft), where Fs and Ft denote the
features of source and target domain in latent space) while
ignores the local joint distribution shift, which is closely
related to the semantic consistency of each category (i.e.,
P (Fs, Ys) 6= P (Ft, Yt), where Ys and Yt denote the cat-
egories of the features). As a result, the de facto use of
the adversarial loss may cause those target domain features,
which are already well aligned to their semantic counterpart
in source domain, to be mapped to an incorrect semantic
category (negative transfer). This side effect becomes more
severe when utilize a larger weight on the adversarial loss.

To address the limitation of the global adversarial
learning, we propose a category-level adversarial network
(CLAN), prioritizing category-level alignment which will
naturally lead to global distribution alignment. The cartoon
comparison of traditional adversarial learning and the pro-
posed one is shown in Fig. 1. The key idea of CLAN is
two-fold. First, we identify those classes whose features
are already well aligned between the source and target do-
mains, and protect this category-level alignment from the
side effect of adversarial learning. Second, we identify the
classes whose features are distributed differently between
the two domains and increase the weight of the adversarial
loss during training. In this process, we utilize co-training
[47], which enables high-conﬁdence predictions with two
diverse classiﬁers, to predict how well each feature is se-
mantically aligned between the source and target domains.
Speciﬁcally, if the two classiﬁers give consistent predictions,
it indicates that the feature is predictive and achieves good
semantic alignment. In such case, we reduce the inﬂuence of
the adversarial loss in order to encourage the network to gen-

erate invariant features that can keep semantic consistency
between domains. On the contrary, if the predictions dis-
agree with each other, which indicates that the target feature
is far from being correctly mapped, we increase the weight
of the adversarial loss on that feature so as to accelerate the
alignment. Note that 1) Our adversarial learning scheme acts
directly on the output space. By regarding the output predic-
tions as features, the proposed method jointly promotes the
optimization for both classiﬁer and extractor; 2) Our method
does not guarantee rigorous joint distribution alignment be-
tween domains. Yet, compared with marginal distribution
alignment, our method can map the target features closer
(or no negative transfer at worst) to the source features of
the same categories. The main contributions are summarized
below.

• By proposing to adaptively weight the adversarial loss
for different features, we emphasize the importance of
category-level feature alignment in reducing domain
shift.

• Our results are on par with the state-of-the-art UDA
methods on two transfer learning tasks, i.e., GTA5 [31]
→ Cityscapes [8] and SYNTHIA [32] → Cityscapes.

2. Related Works

This section will focus on adversarial learning and co-
training techniques for unsupervised domain adaptation,
which form the two main motivations of our method.

Adversarial learning. Ben-David et al. [1] had proven
that the adaptation loss is bounded by three terms, e.g., the
expect loss on source domain, the domain divergence, and
the shared error of the ideal joint hypothesis on the source
and target domain. Because the ﬁrst term corresponds to
the well-studied supervised learning problems and the third
term is considered sufﬁciently low to achieve an accurate
adaptation, the majority of recent works lay emphasis on
the second term. Adversarial adaptation methods are good
examples of this type of approaches and can be investigated
on different levels. Some methods focus on the distribution
shift in the latent feature space [26, 39, 15, 22, 19, 43, 36]. In
an example, Hoffman et al. [15] appended category statistic
constraints to the adversarial model, aiming to improve se-
mantic consistency in target domain. Other methods address
the adaption problem on the pixel level [21, 3], which relate
to the style transfer approaches [49, 7] to make images indis-
tinguishable across domains. A joint consideration of pixel
and feature level domain adaptation is studied in [14]. Be-
sides alignment in the bottom feature layers, Tsai et al. [41]
found that aligning directly the output space is more effective
in semantic segmentation. Domain adaptation in the output
space enables the joint optimization for both prediction and
representation, so our method utilizes this advantage.

2508

Co-training. Co-training [47] belongs to multi-view
learning in which learners are trained alternately on two
distinct views with conﬁdent labels from the unlabeled data.
In UDA, this line of methods [44, 5, 33, 25] are able to as-
sign pseudo labels to unlabeled samples in the target domain,
which enables direct measurement and minimization the
classiﬁcation loss on target domain. In general, co-training
enforces the two classiﬁers to be diverse in the learned pa-
rameters, which can be achieved via dropout [34], consensus
regularization [35] or parameter diverse [44], etc. Similar to
co-training, tri-training keeps the two classiﬁers producing
pseudo labels and uses these pseudo labels to train an extra
classiﬁer [33, 44]. Apart from assigning pseudo labels to un-
labeled data, Saiko et al. [34, 35] maximized the consensus
of two classiﬁers for domain adaptation.

Our work does not follow the strategy of global feature
alignment [41, 15, 39] or classiﬁers consensus maximiza-
tion [34, 35]. Instead, category-level feature alignment is
enforced through co-training. To our knowledge, we are
making an early attempt to adaptively weight the adversarial
loss for features in segmentation task according to the local
alignment situation.

3. Method

3.1. Problem Settings

We focus on the problem of unsupervised domain adapta-
tion (UDA) in semantic segmentation, where we have access
to the source data XS with pixel-level labels YS, and the
target data XT without labels. The goal is to learn a model
G that can correctly predict the pixel-level labels for the tar-
get data XT . Traditional adversaries-based networks (TAN)
consider two aspects for domain adaptation. First, these
methods train a model G that distills knowledge from la-
beled data in order to minimize the segmentation loss in the
source domain, formalized as a fully supervised problem:

Lseg(G) = E[ℓ(G(XS), YS)] ,

(1)

where E[·] denotes statistical expectation and ℓ(·, ·) is an
appropriate loss function, such as multi-class cross entropy.
Second, adversaries-based UDA methods also train G to
learn domain-invariant features by confusing a domain dis-
criminator D which is able to distinguish between samples
of the source and target domains. This property is achieved
by minimaxing an adversarial loss:

Ladv(G, D) = − E[log(D(G(XS)))]

− E[log(1 − D(G(XT )))] .

(2)

the negative transfer that causes the samples from different
domains but of the same class label to be mapped farther
away in the feature space. In some cases, some classes are
already aligned between domains, but the adversarial loss
might deconstruct the existing local alignment when pursu-
ing the global marginal distribution alignment. In this paper,
we call this phenomenon “lack of semantic consistency”,
which is a critical cause of performance degradation.

3.2. Network Architecture

Our network architecture is illustrated in Fig. 2. It is
composed of a generator G and a discriminator D. G can
be any FCN-based segmentation network [38, 23, 4] and D
is a CNN-based binary classiﬁer with a fully-convolutional
output [10]. As suggested in the standard co-training algo-
rithm [47], generator G is divided into feature extractor E
and two classiﬁers C1 and C2. E extracts features from
input images; C1 and C2 classify features generated from
E into one of the pre-deﬁned semantic classes, such as car,
tree and road. Following the co-training practice, we enforce
the weights of C1 and C2 to be diverse through a cosine
distance loss. This will provide us with the distinct views
/ classiﬁers to make semantic predictions for each feature.
The ﬁnal prediction map p is obtained by summing up the
two diverse prediction tensors p(1) and p(2) and we call p an
ensemble prediction.

Given a source domain image xs ∈ XS, feature extractor
E outputs a feature map, which is input to classiﬁers C1 and
C2 to yield the pixel-level ensemble prediction p. On the
one hand, p is used to calculate a segmentation loss under
the supervision of the ground-truth label ys ∈ YS. On the
other hand, p is input to D to generate an adversarial loss.

Given a target domain image xt ∈ XT , we also forward
it to G and obtain an ensemble prediction p. Different from
the source data ﬂow, we additionally generate a discrepancy
map out of p(1) and p(2), denoted as M(p(1), p(2)), where
M(·, ·) denotes some proper distance metric to measure the
element-wise discrepancy between p(1) and p(2). When us-
ing the cosine distance as an example, M(p(1), p(2)) forms
a 1 × H × W shaped tensor with the (ith ∈ H, jth ∈ W )
element equaling to (1 − cos(p(1)
i,j )). Once D produces
an adversarial loss map Ladv, an element-wise multiplica-
tion is performed between Ladv and M(p(1), p(2)). As a
result, the ﬁnal adaptive adversarial loss on a target sample
takes the form as PH
i,j ))×Ladvi,j ,
where {i, j} traverses over all the pixels on the map. In this
manner, each pixel on the segmentation map is differently
weighted w.r.t the adversarial loss.

j=1(1−cos(p(1)

i=1 PW

i,j , p(2)

i,j , p(2)

3.3. Training Objective

However, as mentioned above, there is a major limita-
tion for traditional adversarial learning methods: even under
perfect alignment in marginal distribution, there might be

The proposed network is featured by three loss functions,
i.e., the segmentation loss, the weight discrepancy loss and
the self-adaptive adversarial loss. Given an image x ∈ XS

2509

Figure 2. Overview of the proposed category-level adversarial network. It consists of a feature extractor E, two classiﬁers C1 and C2, and a
discriminator D. C1 and C2 are fed with the deep feature map extracted from E and predict semantic labels for each pixel from diverse
views. In source ﬂow, the sum of the two prediction maps is used to calculate a segmentation loss as well as an adversarial loss from D.
In target ﬂow, the sum of the two prediction maps is forwarded to D to produce a raw adversarial loss map. Additionally, we adopt the
discrepancy of the two prediction maps to produce a local alignment score map. This map evaluates the category-level alignment degree of
each feature and is used to adaptively weight the raw adversarial loss map.

of shape 3 × H × W and a label map y ∈ YS of shape
C × H × W where C is the number of semantic classes,
the segmentation loss (multi-class cross-entropy loss) can be
concretized from Eq. 1 as

Lseg(G) =

H×W

C

X

i=1

X

c=1

−yic log pic ,

(3)

where pic denotes the predicted probability of class c on
pixel i. yic denotes the ground truth probability of class c on
the pixel i. If pixel i belongs to class c, yic = 1, otherwise
yic = 0.

For the second loss, as suggested in the standard co-
training algorithm [47], the two classiﬁers C1 and C2 should
have possibly diverse parameters in order to provide two
different views on a feature. Otherwise, the training degener-
ates to self-training. Speciﬁcally, we enforce divergence of
the weights of the convolutional layers of the two classiﬁers
by minimizing their cosine similarity. Therefore, we have
the following weight discrepancy loss:

the traditional adversarial loss (Eq. 2) as

Ladv(G, D) = −E[log(D(G(XS)))] −
E[(λlocalM(p(1), p(2)) + ǫ) log(1 − D(G(XT )))] ,

(5)

where p(1) and p(2) are predictions made by C1 and C2,
respectively, M(·, ·) denotes the cosine distance, and λlocal
controls the adaptive weight for adversarial loss. Note that
in Eq. 5, to stabilize the training process, we add a small
number ǫ to the self-adaptive weight.

With the above loss terms, the overall loss function of our

approach can be written as

LCLAN (G, D) =Lseg(G) + λweightLweight(G) +

λadvLadv(G, D) ,

(6)

where λweight and λadv denote the hyper parameters that
control the relative importance of the three losses. The
training objective of CLAN is

G∗, D∗ = arg min

G

max

D

LCLAN (G, D).

(7)

Lweight(G) =

~w1 · ~w2

k ~w1k k ~w2k

,

(4)

We solve Eq. 7 by alternating between optimizing G and

D until LCLAN (G, D) converges.

where ~w1 and ~w2 are obtained by ﬂattening and concatenat-
ing the weights of the convolution ﬁlters of C1 and C2.

Third, we adopt the discrepancy between the two predic-
tions p(1) and p(2) as an indicator to weight the adversarial
loss. The self-adaptive adversarial loss can be extended from

3.4. Analysis

The major difference between the proposed framework
and traditional adversarial learning consists in two aspects:
the discrepancy loss and the category-level adversarial loss.
Accordingly, analysis will focus on the two differences.

2510

Feature extractorClassifiersDiscriminatorTarget imageSource imageLocal alignment score mapSegmentation resultSegmentation lossCategory-level adversarial lossSource flowTarget flowTensor sumDistance metricElement-wise productWeight discrepancyWeight discrepancy lossM𝜮𝜮MFigure 3. A contrastive analysis of CLAN and traditional adversarial network (TAN). (a): A target image, and we focus on the poles and
trafﬁc signs in orange boxes. (b): A non-adapted segmentation result. Although the global segmentation result is poor, the poles and trafﬁc
signs can be correctly segmented. It indicates that some classes are originally aligned between domains, even without any domain adaptation.
(c): Adapted result of TAN, in which a decent segmentation map is produced but poles and trafﬁc signs are poorly segmented. The reason is
that the global alignment strategy tends to assign a conservative prediction to a feature and would lead some features to be predicted to other
prevalent classes [11, 18], thus causing those infrequent features being negatively transferred. (d): Adapted result from CLAN. CLAN
reduces the weight of adversarial loss for those aligned features. As a result, the original well-segmented class are well preserved. We then
map the high-dimensional features of (b), (c) and (d) to a 2-D space with t-SNE [29] shown in (e), (f) and (g). The comparison of feature
distributions further proves that CLAN can enforce category-level alignment during the trend of global alignment. (For a clear illustration,
we only show 4 related classes, i.e., building in blue, trafﬁc sign in orange, pole in red and vegetation in green.)

First, the discrepancy (co-training) loss encourages E
to learn domain-invariant semantics instead of the domain
speciﬁc elements such as illumination. In our network, clas-
siﬁers C1 and C2 1) are encouraged to capture possibly
different characteristics of a feature, which is ensured by
the discrepancy loss, and 2) are enforced to make the same
prediction of any E output (no matter the source or target),
which is required by the segmentation loss and the adver-
sarial loss. The two forces actually require that E should
capture the essential aspect of a pixel across the source and
target domains, which, as we are aware of, is the pure se-
mantics of a pixel, i.e., the domain-invariant aspect of a
pixel. Without the discrepancy loss (co-training), force 1)
is missing, and there is a weaker requirement for E to learn
domain-invariant information. On the other side, in our sim-
ulated → real task, the two domains vary a lot at visual level,
but overlap at semantic level. If C1 and C2 are input with
visual-level features from E, their predictions should be in-
accurate in target domain and tend to be different, which will
be punished by large adversarial losses. As a result, once our
algorithm converges, C1 and C2 will be input with semantic-
level features instead of visual-level features. That is, E is
encouraged to learn domain-invariant semantics. Therefore,
the discrepancy loss serves as an implicit contributing factor
for the improved adaptation ability.

Second,

in our major contribution, we extend the
loss with an adaptive weight
traditional adversarial
[λlocalM(p(1), p(2)) + ǫ].
On the one hand, when
M(p(1), p(2)) is large, feature maps of the same class do

not have similar joint distributions between two domains:
they suffer from the semantic inconsistency. Therefore, the
weights are such assigned as to encourage G to fool D
mainly on features that suffer from domain shift. On the
other hand, when M(p(1), p(2)) is small, the joint distribu-
tion would have a large overlap across domains, indicating
that the semantic inconsistency problem is not severe. Under
this circumstance, G tends to ignore the adversarial punish-
ment from D. From the view of D, the introduction of the
adaptive weight encourages D to distill more knowledge
from examples suffering from semantic inconsistency rather
than those well-aligned classes. As a result, CLAN is able to
improve category-level alignment degree in adversarial train-
ing. This could be regarded as an explicit contributing factor
for the adaptation ability. We additionally give a contrastive
analysis between traditional adversarial network (TAN) and
CLAN on their adaptation result in Fig. 3.

4. Experiment

4.1. Datasets

We evaluate CLAN together with several state-of-the-art
algorithms on two adaptation tasks, e.g., SYNTHIA [32] →
Cityscapes [8] and GTA5 [31] → Cityscapes. Cityscapes
is a real-world dataset with 5,000 street scenes. We use
Cityscapes as the target domain. GTA5 contains 24,966 high-
resolution images compatible with the Cityscapes annotated
classes. SYNTHIA contains 9400 synthetic images. We use
SYNTHIA or GTA5 as the source domain.

2511

(a) Target image(b) Non-adapted(c) Adapted (TAN)(d) Adapted (CLAN)(e) Non-adapted features(f) Adapted features (TAN) (g) Adapted features (CLAN) Table 1. Adaptation from GTA5 [31] to Cityscapes [8]. We present per-class IoU and mean IoU. “V” and “R” represent the VGG16-FCN8s
and ResNet101 backbones, respectively. “ST” and “AT” represent two lines of method, i.e., self training- and adversarial learning-based
DA. We highlight the best result in each column in bold. To clearly showcase the effect of CLAN on infrequent classes, we highlight these
classes in blue. Gain indicates the mIoU improvement over using the source only.

.
h
c
r
A

.
h
t
e

M

d
a
o
r

.
e
d
i
s

.
l
i
u
b

l
l
a
w

e
c
n
e
f

e
l
o
p

t
h
g
i
l

n
g
i
s

.
e
g
e
v

.
r
r
e
t

y
k
s

.
s
r
e
p

r
e
d
i
r

r
a
c

k
c
u
r
t

s
u
b

n
i
a
r
t

r
o
t
o
m

e
k
i
b

U
o
I
m

n

i
a
g

GTA5 → Cityscapes

Source only
CBST [50]

Source only
MCD [35]

Source only
CDA [45]

V -
64.0 22.1 68.6 13.3
V ST 90.4 50.8 72.0 18.3

8.7
9.5

19.9 15.5
74.9 13.4 37.0 37.7 10.3 48.2
27.2 28.6 14.1 82.4 25.1 70.8 42.6 14.5 76.9

5.9

6.1
5.9

1.8 10.8

1.2
24.3 —
12.5 1.2 14.0 28.6 36.1 11.8

2.9

V -
V AT 86.4

8.5

25.9 10.9 50.5

3.3
76.1 18.6

12.2 25.4 28.6 13.0 78.3
9.7

63.9 52.1
82.8 32.7 71.4 25.2

14.9

0.6

7.3

7.8

7.9
1.1

5.2

66.3
76.3 16.1 17.1 1.4

0.9 13.7
0.2

7.8

0.7
0.0

24.9 —
28.8
3.9

V -
64.1
V AT 74.9 22.0 71.7

18.1

6.8

7.3
6.0

8.7
11.9

21.0 14.9 16.8 45.9
8.4

16.3 11.1 75.7 13.3 66.5 38.0

2.4

64.4 41.6 17.5 55.3

8.4

5.0

6.9

4.3

55.2 18.8 18.9 0.0 16.8 14.6 28.9

13.8 22.3 —
6.6

8.9
6.0
Source only
FCNs in the wild [15]
10.9 14.2
CyCADA (feature) [14] V AT 85.6 30.7 74.7 14.4 13.0 17.6 13.7
Baseline (TAN) [41]
CLAN

70.0
47.0 24.5
79.2 21.3 64.6 44.1
74.6 15.8 69.9 38.2
V AT 87.3 29.8 78.6 21.1 18.2 22.5 21.5 11.0 79.7 29.6 71.3 46.8
V AT 88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8

V -
5.5
V AT 70.4 32.4 62.1 14.9

26.0 14.9 65.1

2.5
2.7
5.8

12.9
5.4

2.9

1.5
7.3
5.0

0.0
40.0 12.1
3.5
70.4
8.0
72.3 16.0
3.6
80.1 23.0 26.9 0.0 10.6
80.5 26.6 29.9 0.0 10.7

0.0
0.0
0.1

0.0
0.0
0.0
0.3
0.0

17.9 —
27.1
9.2
29.2 11.3
35.0 17.1
36.6 18.7

9.3

0.0
4.2
3.5
6.5
7.9

Source only
Baseline (TAN) [41]
CLAN

-

R
R AT 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4
R AT 87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2

75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6 —
4.8
6.6

Table 2. Adaptation from SYNTHIA [32] to Cityscapes [8]. We present per-class IoU and mean IoU for evaluation. CLAN and state-of-the-
art domain adaptation methods are compared. For each backbone, the best accuracy is highlighted in bold. To clearly showcase the effect of
CLAN on infrequent classes, we highlight these classes in blue. Gain indicates the mIoU improvement over using the source only.

SYNTHIA → Cityscapes

.

h
c
r
A

V
V

V
V
V
V
V

R
R
R

.

h
t
e

M

-
ST

-
AT
AT
AT
AT

-
AT
AT

d
a
o
r

17.2
69.6

6.4
11.5
62.7
78.9
80.4

55.6
79.2
81.3

.
e
d
i
s

19.7
28.7

17.7
19.6
25.6
29.2
30.7

23.8
37.2
37.0

.
l
i
u
b

47.3
69.5

29.7
30.8
78.3
75.5
74.7

74.6
78.8
80.1

t
h
g
i
l

3.0
11.9

0.0
0.1
1.2
0.1
1.4

6.1
9.9
16.1

n
g
i
s

9.1
13.6

7.2
11.7
5.4
4.8
8.0

12.1
10.5
13.7

.
e
g
e
v

71.8
82.0

30.3
42.3
81.3
72.6
77.1

74.8
78.2
78.2

y
k
s

78.3
81.9

66.8
68.7
81.0
76.7
79.0

79.0
80.5
81.5

.
s
r
e
p

37.6
49.1

51.1
51.2
37.4
43.4
46.5

55.3
53.5
53.4

r
e
d
i
r

4.7
14.5

1.5
3.8
6.4
8.8
8.9

19.1
19.6
21.2

r
a
c

42.2
66.0

47.3
54.0
63.5
71.1
73.8

39.6
67.0
73.0

s
u
b

9.0
6.6

3.9
3.2
16.1
16.0
18.2

23.3
29.5
32.9

r
o
t
o
m

0.1
3.7

0.1
0.2
1.2
3.6
2.2

13.7
21.6
22.6

e
k
i
b

0.9
32.4

0.0
0.6
4.6
8.4
9.9

25.0
31.3
30.7

U
o
I
m

26.2
36.1

20.2
22.9
35.7
37.6
39.3

38.6
45.9
47.8

n

i
a
g

—
9.9

—
2.7
15.2
17.4
19.1

—
7.3
9.2

Source only
CBST [50]

Source only
FCNs in the wild [15]
Cross-city [6]
Baseline (TAN) [41]
CLAN

Source only
Baseline (TAN) [41]
CLAN

4.2. Implementation Details

We use PyTorch for implementation. We utilize the
DeepLab-v2 [4] framework with ResNet-101 [12] pre-
trained on ImageNet [9] as our source-only backbone for
network G. We use the single layer adversarial DA method
proposed in [41] as the TAN baseline. For co-training, we
duplicate two copies of the last classiﬁcation module and
arrange them in parallel after the feature extractor, as illus-
trated in Fig. 2. For a fair comparison to those methods
with the VGG backbone, we also apply CLAN on VGG-16
based FCN8s [23]. For network D, we adopt a similar struc-
ture with [30], which consists of 5 convolution layers with
kernel 4 × 4 with channel numbers {64, 128, 256, 512, 1}
and stride of 2. Each convolution layer is followed by a

Leaky-ReLU [28] parameterized by 0.2 except the last layer.
Finally, we add an up-sampling layer to the last layer to
rescale the output to the size of the input map, in order
to match the size of local alignment score map. During
training, we use SGD [2] as the optimizer for G with a mo-
mentum of 0.9, while using Adam [20] to optimize D with
β1 = 0.9, β2 = 0.99. We set both optimizers a weight
decay of 5e − 4. For SGD, the initial learning rate is set to
2.5e − 4 and decayed by a poly learning rate policy, where
the initial learning rate is multiplied by (1 − iter
max iter )power
with power = 0.9. For Adam, we initialize the learning rate
to 5e − 5 and ﬁx it during the training. We train the network
for a total of 100k iterations. We use a crop of 512 × 1024
during training, and for evaluation we up-sample the predic-
tion map by a factor of 2 and then evaluate mIoU. In our best

2512

model, the hyper-parameters λweight, λadv, λlocal and ǫ are
set to 0.01, 0.001, 40 and 0.4 respectively.

4.3. Comparative Studies

We present the adaptation results on task GTA5 →
Cityscapes in Table 1 with comparisons to the state-of-the-art
domain adaptation methods [35, 45, 15, 14, 41, 50]. We ob-
serve that CLAN signiﬁcantly outperforms the source-only
segmentation method by +18.7% on VGG-16 and +6.6%
on ResNet-101. Besides, CLAN also outperforms the state-
of-the-art methods, which improves the mIOU by over +7%
compared with MCD [35], CDA [45] and CyCADA [14].
Compared to traditional adversarial network (TAN) in the
output space [41], CLAN brings over +1.6% improvement
in mIOU in both architectures of VGG-16 and ResNet-101.
In some infrequent classes which are prone to suffer from the
side effect of global alignment, e.g., fence, trafﬁc light and
pole, CLAN can signiﬁcantly outperform TAN. Besides, we
also compare CLAN with the self training-based methods,
among which CBST [50] is the current state-of-the-art one.
This series of explicit methods usually achieve higher mIoU
then the implicit feature alignment. While in our experiment,
we ﬁnd that CLAN is on par with CBST. Some qualitative
segmentation examples can be viewed in Fig. 5.

Table 2 provides the comparative results on the task SYN-
THIA → Cityscapes. On VGG-16, our ﬁnal model yields
39.3% in terms of mIOU, which signiﬁcantly improves the
non-adaptive segmentation result by 19.1%. Besides, CLAN
outperforms the current state-of-art method [15] by 16.4%
and [6] by 3.6%. On ResNet-101, CLAN brings 9.2% im-
provement to source only segmentation model. Compare to
TAN [41], the use of adaptive adversarial loss also brings
1.9% gain in terms of mIOU. Likewise, CLAN is more ef-
fective on those infrequent classes which are prone to be
negatively transferred, such as trafﬁc light and sign, bring-
ing over 3.2% improvement respectively. While on some
prevalent classes, CLAN can also be par on with the baseline
method. Note that on the “train” class, the improvement
is not stable. This is due to the training samples that con-
tain the “train” are very few. Finally, comparing with the
self training-based method, CLAN outperforms CBST by
3.2% in terms of mIOU. These observations are in consistent
with our t-SNE analysis in Fig. 3, which further veriﬁes that
CLAN can actually boost the category-level alignment in
segmentation-based DA task.

4.4. Feature Distribution

To further verify that CLAN is able to decrease the neg-
ative transfer effect for those well-aligned features, we de-
signed an experiment to take a closer look at the category-
level alignment degree of each class. Speciﬁcally, we ran-
domly select 1K source and 1K target images and calculate
the cluster center distance (CCD) {de
n} of features of

1...de

1

0.8

0.6

0.4

i

e
c
n
a
t
s
d
 
r
e
t
n
e
C

0.2

0

TAN
CLAN

44

42

U
o
m

I

40

38

mIoU

D loss

1

0.8

0.6

0.4

D

 
f

o
 
s
s
o
L

0.2

0

5

Epochs

10

0   /   0 . 4

2

0   /   0 . 4

4

0   /   0 . 4

8

0   /   0 . 4

4

0   /   0 . 1

4

0   /   0 . 2

4

0   /   0 . 8

1

 / 

local

Figure 4. Left: Cluster center distance variation as training goes
on. Right: Mean IoU (see bars & left y axis) and convergence
performance (see lines & right y axis) variation when training with
different λlocal and ǫ.

i /d0

i is normalized by de

the same class between two domains, where n = #class
and e is training epoch. de
i (In this
way, the CCD from the pre-trained model without any ﬁne-
tuning would be always normalized to 1). We report de
i in
Fig. 4 (left subﬁgure, taking the class “wall” as an example).
First, we observe as training goes on, de
i is monotonically
decreasing in CLAN while not being monotone in TAN,
suggesting CLAN prevents the well-aligned features from
being incorrectly mapped. Second, de
i converges to a smaller
value in CLAN than TAN, suggesting CLAN achieves better
feature alignment at semantic level.

We further report the ﬁnal CCD of each class in Fig. 6.
We can observe that CLAN can achieve a smaller CCD in
most cases, especially in those infrequent classes which are
prone to be negatively transferred. These quantitative results,
together with the qualitative t-SNE [29] analysis in Fig. 3,
indicate that CLAN can preferably align the two domains in
semantic level. Such category-aligned feature distribution
usually makes the subsequent classiﬁcation easier.

4.5. Parameter Studies

In this experiment, we aim to study two problems: 1)
whether the adaptive adversarial loss would cause instabil-
ity (vanishing gradient) during adversarial training and 2)
how much the adaptive adversarial loss would effect the per-
formance. For the problem 1), we utilize the loss of D to
indicate the convergence performance and a stable adversar-
ial training is achieved if D loss converges around 0.5. First,
we test our model using λlocal = 40, with varying ǫ over a
range {0.1, 0.2, 0.4, 0.8}. We do not use any ǫ larger than
0.8 since CLAN would degrade into TAN in that case. In
the experiment, our model suffers from poor convergence
when utilize a very small ǫ, e.g., 0.1 or 0.2. It indicates that
a proper choice of ǫ is between 0.2 and 0.8. Motivated by
this observation, we then test our model using ǫ = 0.4 with
varying λlocal over a range {10, 20, 40, 80}. We observe that
the convergence performance is not very sensitive to λlocal
since the loss of D converges to proper values in all the cases.
The best performance is achieved when using λlocal = 40
and ǫ = 0.4. Besides, we observe that the adaptation perfor-
mance of CLAN can steadily outperform TAN when using

2513

Figure 5. Qualitative results of UDA segmentation for GTA5 → Cityscapes. For each target image, we show the non-adapted (source only)
result, adapted result with CLAN and the ground truth label map, respectively.

e
c
n
a

t
s
d

i

 
r
e

t

n
e
C

1

0.8

0.6

0.4

0.2

0

Pre-trained
Source only
TAN
CLAN

ro a d

sid e.

b uil.

w all

fe nce

p ole

lig ht

sig n

ve g e.

terr.

sky

p ers.

rid er

car

truck

b us

train

m otor

bike

Figure 6. Quantitative analysis of the feature joint distributions. For each class, we show the distance of the feature cluster centers between
source domain and target domain. These results are from 1) the model pre-trained on ImageNet [9] without any ﬁne-tuning, 2) the model
ﬁne-tuned with source images only, 3) the adapted model using TAN and 4) the adapted model using CLAN, respectively.

parameters near the best value. We present the detailed per-
formance variation in Fig. 4 (right subﬁgure). By comparing
both the convergence and segmentation results under these
different parameter settings, we can conclude that our pro-
posed adaptive adversarial weight can signiﬁcantly effect
and improve the adaptation performance.

5. Conclusion

In this paper, we introduce the category-level adversar-
ial network (CLAN), aiming to address the problem of se-
mantic inconsistency incurred by global feature alignment
during unsupervised domain adaptation (UDA). By taking
a close look at the category-level data distribution, CLAN

adaptively weight the adversarial loss for each feature ac-
cording to how well their category-level alignment is. In this
spirit, each class is aligned with an adaptive adversarial loss.
Our method effectively prevents the well-aligned features
from being incorrectly mapped by the side effect of pure
global distribution alignment. Experimental results validate
the effectiveness of CLAN, which yields very competitive
segmentation accuracy compared with state-of-the-art UDA
approaches.

Acknowledgment. This work is partially supported by
the National Natural Science Foundation of China (No.
61572211).

2514

Target ImageGround TruthNon-adaptedAdapted (CLAN)References

[1] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1-2):151–175, 2010.

[2] L. Bottou. Large-scale machine learning with stochastic
gradient descent. In Proceedings of COMPSTAT’2010, pages
177–186. Springer, 2010.

[3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), volume 1,
page 7, 2017.

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs.
IEEE transactions on pattern analysis and machine
intelligence, 40(4):834–848, 2018.

[5] M. Chen, K. Q. Weinberger, and J. Blitzer. Co-training for
domain adaptation. In Advances in neural information pro-
cessing systems, pages 2456–2464, 2011.

[6] Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F.
Wang, and M. Sun. No more discrimination: Cross city adap-
tation of road scene segmenters. In 2017 IEEE International
Conference on Computer Vision (ICCV), pages 2011–2020.
IEEE, 2017.

[7] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and
J. Choo. Stargan: Uniﬁed generative adversarial networks
for multi-domain image-to-image translation. arXiv preprint
arXiv:1711.09020, 2017.

[8] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 3213–3223, 2016.

[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pages 248–255. IEEE, 2009.

[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio. Generative
adversarial nets. In Advances in neural information process-
ing systems, pages 2672–2680, 2014.

[11] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.
Courville. Improved training of wasserstein gans. In Advances
in Neural Information Processing Systems, pages 5767–5777,
2017.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 770–778,
2016.

[13] Y. He, P. Liu, Z. Wang, Z. Hu, and Y. Yang. Filter pruning
via geometric median for deep convolutional neural networks
acceleration.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2019.

[14] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adver-

sarial domain adaptation. arXiv preprint arXiv:1711.03213,
2017.

[15] J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild:
Pixel-level adversarial and constraint-based adaptation. arXiv
preprint arXiv:1612.02649, 2016.

[16] G. Kang, L. Jiang, Y. Yang, and A. G. Hauptmann. Con-
trastive adaptation network for unsupervised domain adap-
tation.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019.

[17] G. Kang, L. Zheng, Y. Yan, and Y. Yang. Deep adversarial
attention alignment for unsupervised domain adaptation: the
beneﬁt of target expectation maximization. In Proceedings of
the European Conference on Computer Vision (ECCV), pages
401–416, 2018.

[18] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In International Conference on Learning Representations
(ICLR), 2018.

[19] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. arXiv preprint arXiv:1703.05192, 2017.

[20] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[21] P. Li, X. Liang, D. Jia, and E. P. Xing. Semantic-aware grad-
gan for virtual-to-real urban scene adaption. arXiv preprint
arXiv:1801.01726, 2018.

[22] M.-Y. Liu and O. Tuzel. Coupled generative adversarial
In Advances in neural information processing

networks.
systems, pages 469–477, 2016.

[23] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recognition,
pages 3431–3440, 2015.

[24] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning transfer-
able features with deep adaptation networks. arXiv preprint
arXiv:1502.02791, 2015.

[25] P. Luo, F. Zhuang, H. Xiong, Y. Xiong, and Q. He. Transfer
learning from multiple source domains via consensus regu-
larization. In Proceedings of the 17th ACM conference on
Information and knowledge management, pages 103–112.
ACM, 2008.

[26] Y. Luo, P. Liu, T. Guan, J. Yu, and Y. Yang. Signiﬁcance-
aware information bottleneck for domain adaptive semantic
segmentation. arXiv preprint arXiv:1904.00876, 2019.

[27] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang.
Macro-micro adversarial network for human parsing. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 418–434, 2018.

[28] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlinear-
ities improve neural network acoustic models. In Proc. icml,
volume 30, page 3, 2013.

[29] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.
Journal of machine learning research, 9(Nov):2579–2605,
2008.

[30] A. Radford, L. Metz, and S. Chintala. Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks. arXiv preprint arXiv:1511.06434, 2015.

2515

[47] Z.-H. Zhou and M. Li. Tri-training: Exploiting unlabeled
data using three classiﬁers. IEEE Transactions on knowledge
and Data Engineering, 17(11):1529–1541, 2005.

[48] F. Zhu, L. Zhu, and Y. Yang. Sim-real joint reinforcement
transfer for 3d indoor navigation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2019.

[49] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. arXiv preprint arXiv:1703.10593, 2017.

[50] Y. Zou, Z. Yu, B. Vijaya Kumar, and J. Wang. Unsuper-
vised domain adaptation for semantic segmentation via class-
balanced self-training. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 289–305, 2018.

[31] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision, pages 102–118. Springer,
2016.

[32] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3234–3243, 2016.

[33] K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-
training for unsupervised domain adaptation. arXiv preprint
arXiv:1702.08400, 2017.

[34] K. Saito, Y. Ushiku, T. Harada, and K. Saenko. Adversarial
dropout regularization. arXiv preprint arXiv:1711.01575,
2017.

[35] K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum
classiﬁer discrepancy for unsupervised domain adaptation.
arXiv preprint arXiv:1712.02560, 2017.

[36] S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and
R. Chellappa. Unsupervised domain adaptation for semantic
segmentation with gans. arXiv preprint arXiv:1711.06969,
2017.

[37] H. Shimodaira. Improving predictive inference under covari-
ate shift by weighting the log-likelihood function. Journal of
statistical planning and inference, 90(2):227–244, 2000.

[38] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[39] J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi, A. Butscher,
A. Nguyen, T. Du, and L. Guibas. Convolutional wasser-
stein distances: Efﬁcient optimal transportation on geometric
domains. ACM Transactions on Graphics (TOG), 34(4):66,
2015.

[40] B. Sun and K. Saenko. Deep coral: Correlation alignment
In European Conference on

for deep domain adaptation.
Computer Vision, pages 443–450. Springer, 2016.

[41] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang,
and M. Chandraker. Learning to adapt structured output space
for semantic segmentation. arXiv preprint arXiv:1802.10349,
2018.

[42] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simulta-
neous deep transfer across domains and tasks. In Computer
Vision (ICCV), 2015 IEEE International Conference on, pages
4068–4076. IEEE, 2015.

[43] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In Computer Vision and
Pattern Recognition (CVPR), volume 1, page 4, 2017.

[44] J. Zhang, C. Liang, and C.-C. J. Kuo. A fully convolutional tri-
branch network (fctn) for domain adaptation. arXiv preprint
arXiv:1711.03694, 2017.

[45] Y. Zhang, P. David, and B. Gong. Curriculum domain adap-
tation for semantic segmentation of urban scenes. In The
IEEE International Conference on Computer Vision (ICCV),
volume 2, page 6, 2017.

[46] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang. Invariance
matters: Exemplar memory for domain adaptive person re-
identication. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE, 2019.

2516

