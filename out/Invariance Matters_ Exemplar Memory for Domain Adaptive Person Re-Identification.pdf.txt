Invariance Matters: Exemplar Memory for Domain Adaptive

Person Re-identiÔ¨Åcation

Zhun Zhong1

,

2, Liang Zheng3, Zhiming Luo5, Shaozi Li1 ‚àó, Yi Yang2
1 Cognitive Science Department, Xiamen University

4

,

2 Centre for ArtiÔ¨Åcial Intelligence, University of Technology Sydney

3 Research School of Computer Science, Australian National University

4 Baidu Research 5 Postdoc Center of Information and Communication Engineering, Xiamen University

Abstract

This paper considers the domain adaptive person re-
identiÔ¨Åcation (re-ID) problem: learning a re-ID model from
a labeled source domain and an unlabeled target domain.
Conventional methods are mainly to reduce feature distri-
bution gap between the source and target domains. How-
ever, these studies largely neglect the intra-domain vari-
ations in the target domain, which contain critical fac-
tors inÔ¨Çuencing the testing performance on the target do-
main.
In this work, we comprehensively investigate into
the intra-domain variations of the target domain and pro-
pose to generalize the re-ID model w.r.t three types of the
underlying invariance, i.e., exemplar-invariance, camera-
invariance and neighborhood-invariance. To achieve this
goal, an exemplar memory is introduced to store features
of the target domain and accommodate the three invariance
properties. The memory allows us to enforce the invari-
ance constraints over global training batch without signif-
icantly increasing computation cost. Experiment demon-
strates that the three invariance properties and the pro-
posed memory are indispensable towards an effective do-
main adaptation system. Results on three re-ID domains
show that our domain adaptation accuracy outperforms the
state of the art by a large margin. Code is available at:
https://github.com/zhunzhong07/ECN

1. Introduction

Person re-identiÔ¨Åcation (re-ID) [38, 41, 31, 14] is a
task, which aims to Ô¨Ånd
In

cross-camera image retrieval
matched persons of a given query from the database.

‚àóCorresponding author (szlig@xmu.edu.cn).

This work was done when Zhun Zhong (zhunzhong007@gmail.com)
was a visiting student at University of Technology Sydney. Part of this
work was done when Yi Yang (yee.i.yang@gmail.com) was visiting Baidu
Research during his Professional Experience Program.

spite of the impressive achievement of supervised learning
in the re-ID community, learning a re-ID model that gener-
alizes well on a target domain remains a challenge [7, 29].
Obtaining sufÔ¨Åcient unlabeled data in the target domain is
relatively easy, and yet it is difÔ¨Åcult to learn a deep re-ID
model without annotations. This work considers the prob-
lem of unsupervised domain adaptation (UDA), where we
are provided with labeled source domain and unlabeled tar-
get domain. Our goal is to learn a discriminative represen-
tation for the target set.

In the traditional setting of UDA, most methods are de-
veloped under the closed-set scenario, assuming that the
source and target domains share entirely the same classes
[27, 10]. However, this assumption cannot be applied to
UDA in person re-ID, because the classes from the two do-
mains are completely different. UDA in person re-ID is an
open set problem [3] which is more challenging than closed-
set one. During UDA in person re-ID, it is improper to di-
rectly align the distributions of the source and target do-
mains as in existing closed-set UDA methods. Instead, we
should learn to well separate the unseen classes from the
target domain.

Recent advanced methods address the UDA problem
in person re-ID mostly by reducing the gap between the
source and target domains on the image-level [7, 30, 1] or
the attribute feature-level [29, 16]. These methods only
consider the overall inter-domain variations between the
source and target domains, but ignore the intra-domain vari-
ations of the target domain.
In fact, the target variations
are critically inÔ¨Çuencing factors for person re-ID. In this
study, we explicitly take into account the intra-domain vari-
ations of target domain and investigate three underlying in-
variances, i.e., exemplar-invariance, camera-invariance, and
neighborhood-invariance, as described below.

First, given a deep re-ID model trained on a labeled set,
we observe that the top-ranked retrieval results always are
more likely to be visually correlated to the query. A similar
phenomenon is observed in image classiÔ¨Åcation [33]. This

598

push

pull

Individual exemplars

CamStyle images of the third exemplar

Neighbors of the first exemplar

(a) Exemplar-invariance

(b) Camera-invariance

(c) Neighborhood-invariance

Figure 1. Examples of three underlying invariances. (a) Exemplar-invariance: an exemplar is enforced to be apart from others. (b) Camera-
invariance: an exemplar and its camera-style transferred (CamStyle) images are encouraged to be close to each other, as well as CamStyle
images should be far away from others. (c) Neighborhood-invariance: an exemplar and its neighbors are forced to be close to each other.

indicates that the deep re-ID model has learned the appar-
ent similarity instead of semantic information from visual
data. In reality, each person exemplar could differ signif-
icantly from other exemplars even belonged to the same
identity. Thus, it is possible to enable the re-ID model to
capture the apparent representation of person by learning to
discriminate individual exemplars. Based on this, we intro-
duce the exemplar-invariance to learn apparent similarity on
unlabeled target data by enforcing each person exemplar to
be close to itself and far away from others. Second, as a
key inÔ¨Çuencing factor in person re-ID, camera-style varia-
tions [44] might signiÔ¨Åcantly change the appearance of per-
son. Nevertheless, a person image generated by camera-
style transfer still belongs to the original identity. Taking
this into account, we enforce the camera-invariance [43] un-
der the assumption that a person image and the correspond-
ing camera-style transferred images should be close to each
other. Third, suppose we are provided an appropriate re-
ID model trained on the source and target domains. A tar-
get exemplar and its nearest-neighbors in the target set may
probably have the same identity. Considering this trait, we
present the neighborhood-invariance by encouraging an ex-
emplar and its corresponding reliable neighbors to be close
to each other. This helps us to learn a model that is more ro-
bust to overcome the image variations of the target domain,
such as pose, view and background changes. Examples of
these three invariances are shown in Fig. 1.

Based on the above aspects, we propose a novel unsu-
pervised domain adaptation method for person re-ID. Dur-
ing the training process, an exemplar memory is introduced
into the network to memorize the up-to-date representation
of each exemplar of the target set. The memory enables us
to enforce the invariance constraints over whole/global tar-
get training batch instead of the mini-batch. This helps us
to effectively perform the invariance learning of the target
domain during the network optimizing procedure.

In summary, the contribution of this work is three-fold:

‚Ä¢ We comprehensively study three underlying invari-
ances of the target domain. Experiments show that
these properties are indispensable for improving the
transferable ability of re-ID models.

‚Ä¢ We propose a memory module to effectively enforce
the three invariance properties into the system. The
memory helps us to take advantage of sample simi-
larity over the global training set. With the memory,
accuracy can be signiÔ¨Åcantly improved, requiring very
limited extra computation cost and GPU memory.

‚Ä¢ Our method outperforms the state-of-the-art UDA
approaches by a large margin on three large-
scale datasets: Market-1501, DukeMTMC-reID and
MSMT17.

2. Related Work

Unsupervised domain adaptation. An effective ap-
proach for addressing UDA is by aligning the feature dis-
tributions between the two domains. This alignment can
be achieved by reducing the Maximum Mean Discrepancy
(MMD) [11] between domains [17, 35], or training an ad-
versarial domain-classiÔ¨Åer [2, 27] to encourage the features
of the source and target domains to be indistinguishable.
The above mentioned methods are designed under the as-
sumption of the closed-set scenario, where the classes of
the source and target domains are entirely identical. How-
ever, in practice, there are many scenarios that exist un-
known classes in the target domain. The unknown-class
samples from the target domain should not be aligned with
the source domain. This task is introduced by Busto and
Grall [3], referred as open set domain adaptation. To tackle
this problem, Busto and Grall [3] develop a method to learn
a mapping from the source domain to the target domain by
discarding unknown-class target samples. Recently, an ad-
versarial learning framework [22] is proposed to separate
target samples into known and unknown classes, and reject
unknown classes during feature alignment.
In this paper,
we study the problem of UDA in person re-ID, where the
classes are totally different between the source and target
domains. This is a more challenging open set problem.

Unsupervised person re-identiÔ¨Åcation. The art super-
vised methods have made great achievement in person re-ID
[14, 26, 39, 25], relying on rich-labeled data and the success
of deep networks [18, 12, 8]. However, the performance

599

Input

Labeled source data

‚Ä¶

id 1

id 7

id i

Unlabeled target data

‚Ä¶

img 1 img 2

img j

Deep re-ID Network

Classification

Softmax

ùêø"#$

Exemplar Memory

Key memory

ùë£( ùë£)

‚Ä¶

ùë£*

‚Ä¶

ùë£+

(a) exemplar-invariance

(b) camera-invariance

ùêø%&%

e
c
n
a
i
r
a
v
n
I

i

g
n
n
r
a
e
L

Source flow

Target flow

Source + Target flow

1 2

‚Ä¶

j

‚Ä¶

n

Value memory

(c) neighborhood-invariance

l
l
u
p

h
s
u
p

C
g
m

i

B
g
m

i

A
g
m

i

A

f
o
r
o
b
h
g
i
e
n

A

f
o

e
l
y
t
S
m
a
C

Figure 2. The framework of the proposed approach. During training, labeled source data and unlabeled target data are fed-forward into the
deep re-ID network to obtain up-to-date representations. Subsequently, two components are designed to optimize the network with source
data and target data, respectively. The Ô¨Årst component is a classiÔ¨Åcation module that calculates the cross-entropy loss for labeled source
data. The second component is an exemplar memory module that saves the up-to-date features for target data and computes the invariance
learning loss for unlabeled target data.

may drop signiÔ¨Åcantly when tested on an unseen dataset.
To address this problem, several methods use the labeled
source domain to learn a deep re-ID model as an initial-
ized feature extractor. Then, these methods learn a metric
[36] or reÔ¨Åne the re-ID model by unsupervised clustering
[9] on the target domain. However, these methods do not
take advantage of the labeled source data as a beneÔ¨Åcial su-
pervision during adapting procedure. To overcome previous
drawbacks, many domain adaptation approaches are devel-
oped to adapt the model with both labeled source domain
and unlabeled target domain. These methods are mainly to
reduce the domain shifts between datasets on image-level
[7, 30, 1] and attribute feature-level [29, 16]. Despite their
effectiveness, these methods largely ignore the intra-domain
variations in target domain. Recently, Zhong et al. [43] Ô¨Årst
propose a HHL method to learn camera-invariant network
for the target domain. However, HHL overlooks the latent
positive pairs in the target domain. This might lead the re-
ID model to be sensitive to other variations in the target
domain, such as pose and background variations.

Difference from previous works. Indeed, the three in-
variance properties and the memory module have been sep-
arately presented in existing works. However, our work is
different from them. The exemplar-invariance and mem-
ory module have been presented in self-supervised learning
[33], few-shot learning [23, 28, 32] and supervised learning
[34]. Yet, we explore the feasibility of this idea in unsuper-
vised domain adaptation and overcoming the variations in
the target domain. The neighborhood-invariance is similar
to deep association learning (DAL) [4]. A difference from
DAL is that we design a soft classiÔ¨Åcation loss to align the
top-k neighbors instead of calculating the triplet loss be-
tween the mutual top-1 neighbors. Importantly, comparing
with HHL [43] and DAL [4], we comprehensively consider
three invariance constraints. It is worthy of discovering the
mutual beneÔ¨Åt among the three invariance properties.

3. The Proposed Method

Preliminary.

In the context of unsupervised domain
adaptation (UDA) in person re-ID, we are provided with
a fully labeled source domain {Xs, Ys}, including Ns per-
son images. Each person image xs,i is associated with an
identity ys,i. The number of identities is M for source do-
main. In addition, we are provided with an unlabeled target
domain Xt, containing Nt person images. The identity an-
notation of the target domain is not available. Our goal is
to learn a transferable deep re-ID model using both labeled
source domain and unlabeled target domain, which gener-
alizes well on the target testing set.

3.1. Overview of Framework

The framework of our method is shown in Fig. 2. In our
model, the ResNet-50 [12] pre-trained on ImageNet [6] is
utilized as the backbone. SpeciÔ¨Åcally, we keep the layers of
ResNet-50 till the Pooling-5 layer as the base network and
add a 4096-dimensional fully convolutional (FC) layer af-
ter Pooling-5 layer. The new FC layer is named FC-4096,
followed by batch normalization [13], ReLU [19], Dropout
[24] and two components. The Ô¨Årst component is a clas-
siÔ¨Åcation module for supervised learning with the labeled
It has an M -dimensional FC layer (named
source data.
as FC-#id) and a softmax activation function. We use the
cross-entropy loss to calculate the loss for the source do-
main. The other component is an exemplar memory mod-
ule for invariance learning with the unlabeled target data.
The exemplar memory is served as a feature-storage that
saves the up-to-date output of FC-4096 layer for each target
image. We calculate the invariance learning loss of the tar-
get domain by estimating the similarities between the target
samples within mini-batch and whole target samples saved
in the exemplar memory.

600

3.2. Supervised Learning for Source Domain

3.4. Invariance Learning for Target Domain

Due to the identities of source images are available, we
treat the training process of the source domain as a clas-
siÔ¨Åcation problem [38]. The cross-entropy loss is used to
optimize the network, formulated as,

Lsrc = ‚àí 1
ns

log p(ys,i|xs,i),

(1)

ns

Pi=1

where ns is the number of source images in a training batch.
p(ys,i|xs,i) is the predicted probability that the source im-
age xs,i belongs to identity ys,i, which is obtained by the
classiÔ¨Åcation module.

The model trained using labeled source data produces
a high accuracy on the same distributed testing set. How-
ever, the performance will deteriorate seriously when the
testing set has a different distribution to the source domain.
Next, we will introduce an exemplar memory based method
to overcome this problem by considering the intra-domain
variations of target domain in the training of network.

3.3. Exemplar Memory

In order to improve the generalization ability of the net-
work on the target testing set, we propose to enforce in-
variance learning into the network by estimating the sim-
ilarities between target images. To achieve this goal, we
Ô¨Årst construct an exemplar memory for storing the up-to-
date features of all target images. The exemplar memory is
a key-value structure [34], which has the key memory (K)
and the value memory (V). In the exemplar memory, each
slot stores the L2-normalized feature of FC-4096 in the key
part, while storing the label in the value part. Given an un-
labeled target data including Nt images, we regard each im-
age instance as an individual category. Thus, the exemplar
memory contains Nt slots, in which each slot storing the
feature and label of a target image. In the initialization, we
initialize the values of all the features in the key memory
to zeros. For simplicity, we assign the corresponding in-
dexes as the labels of target samples and store them in the
value memory. For example, the class of i-th target image
in value memory is assigned to V[i] = i. The labels in the
value memory are Ô¨Åxed throughout training process. Dur-
ing each training iteration, for a target training sample xt,i,
we forward it through the deep reID network and obtain
the L2-normalized feature of FC-4096, f (xt,i). During the
back-propagation, we update the feature in the key memory
for the training sample xt,i through,

K[i] ‚Üê Œ±K[i] + (1 ‚àí Œ±)f (xt,i),

(2)

where K[i] is the key memory of image xt,i in i-th slot. The
hyper-parameter Œ± ‚àà [0, 1] controls the updating rate. K[i]
is then L2-normalized via K[i] ‚Üê kK[i]k2.

The deep re-ID model trained with only source domain
is usually sensitive to the intra-domain variations of the tar-
get domain. The variations are critical inÔ¨Çuencing factors
for the performance. Therefore, it is necessary to consider
the image variations of the target domain during transferring
the knowledge from source domain to target domain. In this
study, we investigate three underlying invariances of tar-
get data for UDA in person re-ID, i.e., exemplar-invariance,
camera-invariance and neighborhood-invariance.

Exemplar-invariance. The appearance of each person
image may be very different from others even shared the
same identity. In other words, each person image can be
close to itself while far away from others. Therefore, we
enforce exemplar-invariance into the re-ID model by learn-
ing to distinguish individual person images. This allows the
re-ID model to capture the apparent representation of per-
son. To achieve this goal, we regard the Nt target images
as Nt different classes, and classify each image into its own
class. Given a target image xt,i, we Ô¨Årst compute the cosine
similarities between the feature of xt,i and features saved in
the key memory. Then, the predicted probability that xt,i
belongs to class i is calculated using softmax function,

p(i|xt,i) = exp(K[i]Tf (xt,i)/Œ≤)

PNt

j=1 exp(K[j]Tf (xt,i)/Œ≤)

,

(3)

where Œ≤ ‚àà (0, 1] is temperature fact that balances the scale
of distribution.

The objective of exemplar-invariance is to minimize the

negative log-likelihood over target training image, as

Lei = ‚àí log p(i|xt,i).

(4)

Camera-invariance. Camera style variation is an im-
portant factor in person re-ID. A person image may en-
counter with signiÔ¨Åcant changes in appearance under differ-
ent cameras. The re-ID model trained using labeled source
data can capture the camera-invariance for source domain,
but may suffer from the image variations caused by target
cameras. Since the camera settings of the two domains will
be very different. To overcome this problem, we propose
to equip the network with camera-invariance [43] of tar-
get domain, based on the assumption that an image and
its camera-style transferred counterparts should be close
to each other.
In this paper, we suppose the camera-ID
of each image is known, since the camera-ID can be eas-
ily obtained when collecting person images from video se-
quences. Given the unlabeled target data, we consider each
camera as a style domain and adopt StarGAN [5] to train a
camera style (CamStyle) transfer model [44] for the target
domain. With the learned CamStyle transfer model, each
real target image collected from camera c is augmented with
C ‚àí 1 images in the styles of other cameras while remain-

601

ing the original identity. C is the number of cameras in the
target domain.

To introduce the camera-invariance into the model, we
regard that each real image and its style-transferred coun-
terparts share the same identity. Thus, the loss function of
camera-invariance is explained as,

Lci = ‚àí log p(i|ÀÜxt,i),

(5)

where ÀÜxt,i is a target sample randomly selected from the
style-transferred images of xt,i. In this way, images in dif-
ferent camera styles of the same sample are forced to be
close to each other.

Neighborhood-invariance. For each target image, there
may exist a number of positive samples in the target data.
If we could exploit these positive samples in the training
process, we are able to further improve the robustness of
re-ID model in overcoming the variations of target domain.
To achieve this objective, we Ô¨Årst calculate the cosine sim-
ilarities between f (xt,i) and the features stored in the key
memory K. Then, we Ô¨Ånd the k-nearest neighbors of xt,i
in K and deÔ¨Åne the indexes of them as M(xt,i, k). k is the
size of M(xt,i, k). The nearest one in M(xt,i, k) is i.

We endow the neighborhood-invariance into the network
under the assumption that the target image xt,i should be-
long to the classes of candidates in M(xt,i, k). Thus, we
assign the weight of the probability that xt,i belongs to the
class j as,

wi,j =( 1

k ,
1,

j 6= i
j = i

, ‚àÄj ‚àà M(xt,i, k).

(6)

The objective of neighborhood-invariance is formulated as
a soft-label loss,

wi,j log p(j|xt,i), ‚àÄj ‚àà M(xt,i, k).

(7)

Lni = ‚àíPj6=i

Note that, to distinguish between exemplar-invariance and
neighborhood-invariance, xt,i is not classiÔ¨Åed to its own
class in Eq. 7.

Overall loss of invariance learning. By jointly con-
sidering the exemplar-invariance, camera-invariance and
neighborhood-invariance,
loss of invariance
learning over target training images can be written as,

the overall

(8)

nt

Ltgt = ‚àí 1
nt

Pi=1Pj wi,j log p(j|x‚àó

t,i),

t,i, k). x‚àó

where j ‚àà M(x‚àó
t,i is an image randomly sam-
pled from the union set of xt,i and its camera style-
transferred images. nt is the number of target images
In Eq. 8, when i = j, we op-
in the training batch.
timize the network with the exemplar-invariance learning
and camera-invariance learning by classifying x‚àó
t,i into its
own class. When i 6= j, the network is optimized with
the neighborhood-invariance learning by leading x‚àó
t,i to be
close to its neighbors in M(x‚àó

t,i, k).

3.5. Final Loss for Network

By combining the losses of source and target domains,

the Ô¨Ånal loss for the network is formulated as,

L = (1 ‚àí Œª)Lsrc + ŒªLtgt,

(9)

where Œª ‚àà [0, 1] controls the importance of the source loss
and the target loss. To this end, we introduce a loss function
for UDA person re-ID, in which, the loss of source domain
aims to maintain a basic representation for person. As well
as, the loss of target domain attempts to take the knowledge
from labeled source domain and incorporate the invariance
properties of target domain into the network.

3.6. Discussion on the Three Invariance Properties

We analyze the advantage and disadvantage for each in-
variance. The exemplar-invariance enforces each exemplar
away from each other. It is beneÔ¨Åcial to enlarge the distance
between exemplars from different identities. However, ex-
emplars of the same identity will also be far apart, which
is harmful to the system. On the contrast, neighborhood-
invariance encourages each exemplar and its neighbors to
be close to each other.
It is beneÔ¨Åcial to reduce the dis-
tance between exemplars of the same identity. However,
neighborhood-invariance might also pull closer images of
different identities, because we could not guarantee that
each neighbor shares the same identity with the query exem-
plar. Therefore, there exists a trade off between exemplar-
invariance and neighborhood-invariance, where the former
aims to lead the exemplars from different identities to be far
away while the latter attempts to encourage exemplars of the
same identity to be close to each other. Camera-invariance
has the similar effect as the exemplar-invariance and also
leads the exemplar and its camera-style transferred samples
to share the same representation.

4. Experiment

4.1. Dataset

We evaluate the proposed method on three large-scale
person re-identiÔ¨Åcation (re-ID) benchmarks: Market-1501
[37], DukeMTMC-reID [21, 40] and MSMT17 [30]. Per-
formance is evaluated by the cumulative matching charac-
teristic (CMC) and mean Average Precision (mAP).

4.2. Experiment Setting

Deep re-ID model. We adopt ResNet-50 [12] as the
backbone of our model and initialize the model with the pa-
rameters pre-trained on ImageNet [6]. We Ô¨Åx the Ô¨Årst two
residual layers to save GPU memory. The input image is
resized to 256 √ó128. During training, we perform random
Ô¨Çipping, random cropping and random erasing [42] for data
augmentation. The probability of dropout is set to 0.5. We

602

Œ≤

0.01
0.03
0.05
0.1
0.5
1.0

Duke ‚Üí Market-1501 Market-1501 ‚Üí Duke
Rank-1

Rank-1

mAP

mAP

47.3
72.3
75.1
71.4
52.3
47.8

20.0
40.3
43.0
36.8
23.1
20.8

29.1
59.7
63.3
59.3
45.4
40.2

13.2
35.7
40.4
35.8
24.2
19.3

Table 1. Evaluation with different values of Œ≤ in Eq. 3.

train the model with a learning rate of 0.01 for ResNet-50
base layers and of 0.1 for the others in the Ô¨Årst 40 epochs.
The learning rate is divided by 10 for the next 20 epochs.
The SGD optimizer is used to train the model. We set the
mini-batch size to 128 for both source images and target
images. We initialize the updating rate of key memory Œ±
to 0.01 and increase Œ± linearly with the number of epochs,
i.e., Œ± = 0.01 √ó epoch. Without speciÔ¨Åcation, we set the
temperature fact Œ≤ = 0.05, number of candidate positive
samples k = 6 and weight of losses Œª = 0.3. We train
the model with exemplar-invariance and camera-invariance
learning at the Ô¨Årst 5 epochs and add the neighborhood-
invariance learning for the rest epochs. In testing, we ex-
tract the L2-normalized output of Pooling-5 layer as the im-
age feature and adopt the Euclidean distance to measure the
similarities between query and gallery images.

Baseline. We set the model as the baseline when trained

the network using only the classiÔ¨Åcation component.

4.3. Parameter Analysis

We Ô¨Årst analyze the sensitivities of our approach to three
important hyper-parameters, i.e., the temperature fact Œ≤, the
weight of losses Œª, and the number of candidate positive
samples k. By default, we vary the value of one parameter
and keep the others Ô¨Åxed.

Temperature fact Œ≤. In Table 1, we investigate the ef-
fect of the temperature fact Œ≤ in Eq. 3. Using a lower value
for Œ≤ leads to a lower entropy, which commonly achieves
better results. However, the network does not converge if
the temperature fact is too low, e.g., Œ≤ = 0.01. The best
results are produced when Œ≤ is around 0.05.

The weight of source and target losses Œª. In Fig. 3 we
compare different values of Œª in Eq. 9. When Œª = 0, our
method reduces to the baseline that trained the model only
with labeled source data. It is clearly shown that, when con-
sidering invariance learning for target domain (Œª > 0), our
approach signiÔ¨Åcantly improves the baseline at all values.
It is worth noting that our approach outperforms the base-
line by a large margin even trained the model using only
unlabeled target data (Œª = 1). This demonstrates the effec-
tiveness of our approach and the importance of overcoming
the variations in target domain. When Œª is between 0.3 to

)

%

(
P
A
m

45

40

35

30

25

20

15

10

Duke->Market

Market->Duke

Duke->Market

Market->Duke

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

ùúÜ

ùúÜ

Figure 3. Evaluation with different values of Œª in Eq. 9.

Duke->Market Market->Duke

73.9

75.1

74.3

73.5

72.1

71.3

70.1

60.6

61.1

63.3

63

62.1

61.4

60.6

63.1

53.9

68.9

68.1

67.3

58.1

57.8

56

1

2

4

6

8

10
k

12

14

16

18

20

43

43.8

40.4

40.3

41.6

39.1

42.7

42.3

39.1

38.8

41.3

40.1

37.5

36

38.1

37.8

34.7

33.5

37.3

36.8

29.7

28.4

)

%

(
y
c
a
r
u
c
c
a

1
-
k
n
a
R

80

70

60

50

40

30

20

80

70

60

50

)

%

(
y
c
a
r
u
c
c
a

1
-
k
n
a
R

)

%

(
P
A
m

45

40

35

30

25

1

2

4

6

8

10
k

12

14

16

18

20

Figure 4. Evaluation with different number of candidate positive
samples in neighborhood-invariance learning.

0.8, our result is impacted just marginally and the best re-
sults are obtained. This shows that our method is insensitive
to Œª in an appropriate range.

Number of positive samples k.

In Fig. 4, we show
the results of using different number of positive sam-
ples in neighborhood-invariance learning. When k = 1,
our approach reduces to the model trained with exemplar-
invariance and camera-invariance learning. When adding
neighborhood-invariance learning into the system (k > 1),
our results achieve consistent improvement. The rank-1 ac-
curacy and mAP Ô¨Årst improve with the increase of k and
achieve best results when k is between 6 to 8. Assigning
a too large value to k reduces the results. This is because
an excess of false positive samples may include during
neighborhood-invariance learning, which could have dele-
terious effects on performance.

According to the analysis above, we set Œ≤ = 0.05, Œª =

0.3 and k = 6 in the following experiment.

4.4. Evaluation

Performance of baseline. Table 2 reports the results of
the baseline. When trained with labeled target training set
and tested on the target testing set, the baseline (called Su-
pervised Learning) achieves high accuracy. However, we
observe a serious drop in performance when the baseline is
trained using labeled source set only (called Source Only)
and directly applied to the target testing set. For exam-
ple, when tested on Market-1501, the baseline trained on

603

Methods

Supervised Learning
Source Only
Ours w/ E
Ours w/ E+C
Ours w/ E+N
Ours w/ E+C+N

Src.

N/A
C
M
T
M
e
k
u
D

R-1

87.6
43.1
48.7
63.1
58.0
75.1

Market-1501
R-5

R-10 R-20 mAP

95.5
58.8
67.4
79.1
69.9
87.6

97.2
67.3
74.0
84.6
75.6
91.6

98.3
74.3
80.2
89.1
80.4
94.5

69.4
17.7
21.0
28.4
27.7
43.0

Src.

N/A
1
0
5
1
-
t
e
k
r
a

M

DukeMTMC-reID

R-1

75.6
28.9
34.2
53.9
39.7
63.3

R-5

87.3
44.0
51.3
70.8
53.0
75.8

R-10 R-20 mAP

90.6
50.9
58
76.1
58.1
80.4

92.9
57.5
64.2
80.7
62.9
84.2

57.8
14.8
18.7
29.7
23.6
40.4

Table 2. Methods comparison when tested on Market-1501 and DukeMTMC-reID. Supervised Learning: Baseline model trained with
labeled target data. Source Only: Baseline model trained with only labeled source data. E: Exemplar-invariance. C: Camera-invariance.
N: Neighborhood-invariance. Src.: Source domain.

labeled Market-1501 training set achieves a rank-1 accu-
racy of 87.6%. However, the rank-1 accuracy declines to
43.1% when trained the baseline on labeled DukeMTMC-
reID training set. A similar drop can be observed when
tested on DukeMTMC-reID. This decline in accuracy is
mainly caused by the domain shifts between datasets.

Ablation experiment on invariance learning. To inves-
tigate the effectiveness of the proposed invariance learning
for target domain, we conduct ablation studies in Table 2.
First, we show the effect of exemplar-invariance learning
by adding exemplar-invariance learning into the baseline.
As shown in Table 2, ‚ÄúOurs w/ E‚Äù consistently improves the
results over baseline (Source Only). SpeciÔ¨Åcally, the rank-
1 accuracy improves from 43.1% to 48.7% and 28.9% to
34.2% when tested on Market-1501 and DukeMTMC-reID,
respectively. This demonstrates that exemplar-invariance
learning is an effective way to improve the discrimination
of person descriptors for the target domain.

Next, we validate the effectiveness of camera-invariance
learning over the model trained with exemplar-invariance
learning (Ours w/ E). In Table 2, we observe signiÔ¨Åcant
improvement when adding camera-invariance learning into
the system. For example, ‚ÄúOurs w/ E+C‚Äù achieves a rank-
1 accuracy of 63.1% when regarding DukeMTMC-reID as
source domain and tested on Market-1501. This is higher
than ‚ÄúOurs w/ E‚Äù by 14.4% in rank-1 accuracy. The im-
provement demonstrates that the image variations caused
by target cameras severely impact the performance in test-
ing set. Injecting camera-invariance learning into the model
could effectively improve the robustness of the system to
camera style variations.

We also evaluate the effect of neighborhood-invariance
learning. As reported in Table 2, ‚ÄúOurs w/ E+N‚Äù consis-
tently improves the results of ‚ÄúOurs w/ E‚Äù. Using exemplar-
invariance and neighborhood-invariance during training, the
model (Ours w/ E+N) has 39.7% rank-1 accuracy and
23.6% mAP when using Market-1501 as source domain
and tested on DukeMTMC-reID. This increases the re-
sults of ‚ÄúOurs w/ E‚Äù by 5.5% in rank-1 accuracy and by
4.9% in mAP, respectively. Furthermore, when integrating

Method

Ours w/ mini-batch
Ours w/ memory

DukeMTMC-reID ‚Üí Market-1501

R-1
67.2
75.1

Time (mins) Memory (MB)

‚âà 59.3
‚âà 60.6

‚âà5,000
‚âà5,260

Table 3. Computational cost analysis of the exemplar memory.

neighborhood-invariance learning into a better model (Ours
w/ E+C), our approach would gain more improvement in
performance. For example, ‚ÄúOurs w/ E+C+N‚Äù achieves
rank-1 accuracy of 75.1% when regarding DukeMTMC-
reID as source domain and tested on Market-1501, improv-
ing the rank-1 accuracy of ‚ÄúOurs w/ E+C‚Äù by 12%. Simi-
lar improvement is observed when tested on DukeMTMC-
reID. This is because that more reliable positive samples
would be mined from unlabeled target set by integrating
neighborhood-invariance learning into a more discrimina-
tive model.
The beneÔ¨Åt of the exemplar memory. We use the pro-
posed exemplar memory and the mini-batch to implement
the proposed invariance learning, respectively. For mini-
batch based method, input samples are composed of the
target samples, corresponding CamStyle samples and cor-
responding k-nearest neighbors. As shown in Table 3, the
exemplar memory based method clearly outperforms the
mini-batch based method. It is noteworthy that using the ex-
emplar memory introduces limited additional training time
(‚âà + 1.3 mins) and GPU memory (‚âà + 260 MB) compared
to using the mini-batch.

4.5. Comparison with State of the art Methods

We compare our approach with state-of-the-art unsu-
pervised learning methods when tested on Market-1501,
DukeMTMC-reID and MSMT17.

Table 4 reports the comparisons when tested on Market-
1501 and DukeMTMC-reID. We use DukeMTMC-reID as
the source set when tested on Market-1501 and vice versa.
We compare with two hand-crafted feature based methods
without transfer learning: LOMO [15] and BOW [37], three
unsupervised methods that use a labeled source data to ini-
tialize the model but ignore the labeled source data during

604

Methods

LOMO [15]
Bow [37]
UMDL [20]
PTGAN [30]
PUL [9]
SPGAN [7]
CAMEL [36]
MMFA [16]
SPGAN+LMP [7]
TJ-AIDL [29]
CamStyle [45]
HHL [43]
Ours (ECN)

R-1

27.2
35.8
34.5
38.6
45.5
51.5
54.5
56.7
57.7
58.2
58.8
62.2
75.1

Market-1501
R-5

R-10

41.6
52.4
52.6

-

60.7
70.1

-

75.0
75.8
74.8
78.2
78.8
87.6

49.1
60.3
59.6
66.1
66.7
76.8

-

81.8
82.4
81.1
84.3
84.0
91.6

mAP

8.0
14.8
12.4

-

20.5
22.8
26.3
27.4
26.7
26.5
27.4
31.4
43.0

R-1

12.3
17.1
18.5
27.4
30.0
41.1

-

45.3
46.4
44.3
48.4
46.9
63.3

DukeMTMC-reID
R-10

R-5

21.3
28.8
31.4

-

43.4
56.6

-

59.8
62.3
59.6
62.5
61.0
75.8

26.6
34.9
37.6
50.7
48.5
63.0

-

66.3
68.0
65.0
68.9
66.7
80.4

mAP

4.8
8.3
7.3

-

16.4
22.3

-

24.7
26.2
23.0
25.1
27.2
40.4

Table 4. Unsupervised person re-ID performance comparison with state-of-the-art methods on Market-1501 and DukeMTMC-reID.

learning feature for target domain: CAMEL [36], UMDL
[20] and PUL [9], and six unsupervised domain adaptation
approaches: PTGAN [30], SPGAN [7], MMFA [16], TJ-
AIDL [29], CamStyle [45], HHL [43]. We Ô¨Årst compare
with hand-crafted feature based methods which do not re-
quire learning on neither labeled source set nor unlabeled
target set. These two hand-crated features have demon-
strated the effectiveness on small datasets, but fail to pro-
duce competitive results on large-scale datasets. For exam-
ple, the rank-1 accuracy of LOMO is 12.3% when tested
on DukeMTMC-reID. This is much lower than transferring
learning based methods. Next, we compare with three unsu-
pervised methods. BeneÔ¨Åt from initializing model from the
labeled source data and learning with unlabeled target data,
the results of these three unsupervised approaches are com-
monly superior to hand-crafted methods. For example, PUL
obtains rank-1 accuracy of 45.5% when using DukeMTMC-
reID as source set and tested on Market-1501, surpassing
BOW by 9.7% in rank-1 accuracy. Compare to state-of-the-
art domain adaptation approaches, our approach clearly out-
performs them by a large margin on both datasets. Specif-
ically, our method achieves rank-1 accuracy = 75.1% and
mAP = 43.0% when using DukeMTMC-reID as source
set and tested on Market-1501, and, obtains rank-1 accu-
racy = 63.3% and mAP = 40.4% when using Market-1501
as source set and tested on DukeMTMC-reID. The rank-
1 accuracy is 12.9% higher and 16.4% higher than current
best results (HHL [43]) when tested on Market-1501 and
DukeMTMC-reID, respectively.

We also evaluate our approach on a larger and more chal-
lenging dataset, i.e., MSMT17. Since it is a newly released
dataset, there is only one unsupervised method (PTGAN
[30]) reported on MSMT17. As shown in Table 5, our ap-
proach clearly surpasses PTGAN when using Market-1501
and DukeMTMC-reID as source domains. For example, our

Methods

Src.

PTGAN [30] Market
Ours (ECN) Market
Duke
PTGAN [30]
Ours (ECN)
Duke

R-1

10.2
25.3
11.8
30.2

MSMT17
R-5

R-10

-

36.3

-

41.5

24.4
42.1
27.4
46.8

mAP

2.9
8.5
3.3
10.2

Table 5. Performance evaluation when tested on MSMT17.

method produces rank-1 accuracy = 30.2% and mAP =
10.2% when using DukeMTMC-reID as source set. This
is higher than PTGAN by 18.4% in rank-1 accuracy and by
6.9% in mAP.

5. Conclusion

In this paper, we propose an exemplar memory based
unsupervised domain adaptation (UDA) method for person
re-ID task. With the exemplar memory, we can directly
evaluate the relationships between target samples. And
thus we could effectively enforce the underlying invariance
constraints of the target domain into the network training
process. Experiment demonstrates the effectiveness of the
invariance learning for improving the transferable ability of
deep re-ID model. Our approach produces a new state of
the art in UDA accuracy on three large-scale domains.

Acknowledgements This work is supported by the National
Nature Science Foundation of China (No. 61876159, No.
61806172, No. 61572409, No. U1705286 & 61571188),
Fujian Province 2011Collaborative Innovation Center of
TCM Health Management, Collaborative Innovation Cen-
ter of Chinese Oolong Tea Industry-Collaborative Innova-
tion Center (2011) of Fujian Province. Zhun Zhong thanks
Wenjing Li for encouragement.

605

References

[1] Slawomir Bak, Peter Carr, and Jean-Francois Lalonde. Do-
main adaptation through synthesis for unsupervised person
re-identiÔ¨Åcation. In Proc. ECCV, 2018.

[2] Konstantinos Bousmalis, George Trigeorgis, Nathan Silber-
man, Dilip Krishnan, and Dumitru Erhan. Domain separa-
tion networks. In Proc. NIPS, 2016.

[3] Pau Panareda Busto and Juergen Gall. Open set domain

adaptation. In Proc. ICCV, 2017.

[4] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Deep
association learning for unsupervised video person re-
identiÔ¨Åcation. In Proc. BMVC, 2018.

[5] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo
Ha2 Sunghun Kim, and Jaegul Choo. Stargan: UniÔ¨Åed
generative adversarial networks for multi-domain image-to-
image translation. In Proc. CVPR, 2018.

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proc. CVPR, 2009.

[7] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi
Yang, and Jianbin Jiao. Image-image domain adaptation with
preserved self-similarity and domain-dissimilarity for person
re-identiÔ¨Åcation. In Proc. CVPR, 2018.

[8] Xuanyi Dong and Yi Yang. Searching for a robust neural

architecture in four gpu hours. In Proc. CVPR, 2019.

[9] Hehe Fan, Liang Zheng, Chenggang Yan, and Yi Yang.
Unsupervised person re-identiÔ¨Åcation: Clustering and Ô¨Åne-
tuning. ACM TOMM, 2018.

[10] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain

adaptation by backpropagation. In Proc. ICML, 2015.

[11] Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bern-
hard Sch¬®olkopf, and Alex J Smola. A kernel method for the
two-sample-problem. In Proc. NIPS, 2007.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In Proc.

Deep residual learning for image recognition.
CVPR, 2016.

[13] Sergey Ioffe and Christian Szegedy. Batch normalization:
accelerating deep network training by reducing internal co-
variate shift. In Proc. ICML, 2015.

[14] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious at-
tention network for person re-identiÔ¨Åcation. In Proc. CVPR,
2018.

[15] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Per-
son re-identiÔ¨Åcation by local maximal occurrence represen-
tation and metric learning. In Proc. CVPR, 2015.

[16] Shan Lin, Haoliang Li, Chang-Tsun Li, and Alex Chichung
Kot. Multi-task mid-level feature alignment network for un-
supervised cross-dataset person re-identiÔ¨Åcation.
In Prco.
BMVC, 2018.

[17] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I
Jordan. Learning transferable features with deep adaptation
networks. In Proc. ICML, 2015.

[18] Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi
Yang. Taking a closer look at domain shift: Category-level
adversaries for semantics consistent domain adaptation. In
Proc. CVPR, 2019.

[19] Vinod Nair and Geoffrey E Hinton. RectiÔ¨Åed linear units im-
prove restricted boltzmann machines. In Proc. ICML, 2010.

[20] Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pon-
til, Shaogang Gong, Tiejun Huang, and Yonghong Tian.
Unsupervised cross-dataset transfer learning for person re-
identiÔ¨Åcation. In Proc. CVPR, 2016.

[21] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,
and Carlo Tomasi. Performance measures and a data set for
multi-target, multi-camera tracking. In Proc. ECCVW, 2016.

[22] Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and
Tatsuya Harada. Open set domain adaptation by backpropa-
gation. In Proc. ECCV, 2018.

[23] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan
Wierstra, and Timothy Lillicrap. Meta-learning with
memory-augmented neural networks. In Proc. ICML, 2016.

[24] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overÔ¨Åtting. JMLR, 2014.

[25] Xiaoxiao Sun and Liang Zheng. Dissecting person re-
In Proc.

identiÔ¨Åcation from the viewpoint of viewpoint.
CVPR, 2019.

[26] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin
Wang. Beyond part models: Person retrieval with reÔ¨Åned
part pooling (and a strong convolutional baseline). In Proc.
ECCV, 2018.

[27] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Dar-
rell. Adversarial discriminative domain adaptation. In Proc.
CVPR, 2017.

[28] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wier-
stra, et al. Matching networks for one shot learning. In Proc.
NIPS, 2016.

[29] Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.
Transferable joint attribute-identity deep learning for unsu-
pervised person re-identiÔ¨Åcation. In Proc. CVPR, 2018.

[30] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identiÔ¨Åcation. In Proc. CVPR, 2018.

[31] Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wei Bian, and Yi
Yang. Progressive learning for person re-identiÔ¨Åcation with
one example. IEEE TIP, 2019.

[32] Zhirong Wu, Alexei A Efros, and Stella X Yu. Improving
generalization via scalable neighborhood component analy-
sis. In Proc. ECCV, 2018.

[33] Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In Proc. CVPR, 2018.

[34] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiao-
gang Wang. Joint detection and identiÔ¨Åcation feature learn-
ing for person search. In Proc. CVPR, 2017.

[35] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang,
Yong Xu, and Wangmeng Zuo. Mind the class weight bias:
Weighted maximum mean discrepancy for unsupervised do-
main adaptation. In Proc. CVPR, 2017.

[36] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Cross-
view asymmetric metric learning for unsupervised person re-
identiÔ¨Åcation. In Proc. ICCV, 2017.

606

[37] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiÔ¨Åcation:
A benchmark. In Proc. ICCV, 2015.

[38] Liang Zheng, Yi Yang, and Alexander G Hauptmann. Person

re-identiÔ¨Åcation: Past, present and future. arXiv, 2016.

[39] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng,
Yi Yang, and Jan Kautz. Joint discriminative and generative
learning for person re-identiÔ¨Åcation. In Proc. CVPR, 2019.

[40] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identiÔ¨Åcation
baseline in vitro. In Proc. ICCV, 2017.

[41] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-
ranking person re-identiÔ¨Åcation with k-reciprocal encoding.
In CVPR, 2017.

[42] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and

Yi Yang. Random erasing data augmentation. arXiv, 2017.

[43] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-
alizing a person retrieval model hetero- and homogeneously.
In Proc. ECCV, 2018.

[44] Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li,
Camera style adaptation for person re-

and Yi Yang.
identiÔ¨Åcation. In Proc. CVPR, 2018.

[45] Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, and
Yi Yang. Camstyle: A novel data augmentation method for
person re-identiÔ¨Åcation. IEEE TIP, 2019.

607

