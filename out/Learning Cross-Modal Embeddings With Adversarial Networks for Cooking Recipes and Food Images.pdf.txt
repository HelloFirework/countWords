Learning Cross-Modal Embeddings with Adversarial Networks

for Cooking Recipes and Food Images

Wang Hao‚Ä†,‚àó Doyen Sahoo‚Ä†,‚àó Chenghao Liu‚Ä†

Ee-peng Lim‚Ä† Steven C. H. Hoi‚Ä†,‚Ä°

‚Ä†Singapore Management University ‚Ä°Salesforce Research Asia

{hwang, doyens, chliu, eplim, chhoi}@smu.edu.sg

Abstract

Food computing is playing an increasingly important
role in human daily life, and has found tremendous applica-
tions in guiding human behavior towards smart food con-
sumption and healthy lifestyle. An important task under the
food-computing umbrella is retrieval, which is particularly
helpful for health related applications, where we are inter-
ested in retrieving important information about food (e.g.,
ingredients, nutrition, etc.). In this paper, we investigate an
open research task of cross-modal retrieval between cooking
recipes and food images, and propose a novel framework
Adversarial Cross-Modal Embedding (ACME) to resolve the
cross-modal retrieval task in food domains. SpeciÔ¨Åcally, the
goal is to learn a common embedding feature space between
the two modalities, in which our approach consists of several
novel ideas: (i) learning by using a new triplet loss scheme
together with an effective sampling strategy, (ii) imposing
modality alignment using an adversarial learning strategy,
and (iii) imposing cross-modal translation consistency such
that the embedding of one modality is able to recover some
important information of corresponding instances in the
other modality. ACME achieves the state-of-the-art perfor-
mance on the benchmark Recipe1M dataset, validating the
efÔ¨Åcacy of the proposed technique.

1. Introduction

With the rapid development of social networks, Internet
of Things (IoT), and smart-phones equipped with cameras,
there has been an increasing trend towards sharing food im-
ages, recipes, cooking videos and food diaries. For example,
the social media platform ‚ÄúAll Recipes" 1 allows chefs to
share their created recipes and relevant food images. Their
followers or fans follow the cooking instructions, upload

‚àódenotes equal contribution
1https://www.allrecipes.com

Modality 
Alignment

Triplet Loss
Retrieval
Learning

Cross-modal
Translation 
Consistency

Ingredients: sugar, 
cornstarch, buttermilk, 
baking powder, eggs, 
lemon juice 

Instructions: Combine 
sugar and cornstarch in 
a large saucepan; stir in 
rhubarb‚Ä¶

recipes

ùê∏ùëõùëêùëüùëíùëê

generated images

ùê∏ùëõùëêùëñùëöùëî

images

sugar                 garlic cloves
salt
buttermilk
baking powder

eggs     

predicted ingredients

Figure 1: Example of illustrating the idea of Adversarial Cross-
Model Embedding (ACME), where the embeddings of cooking
recipes and food images are aligned. These embeddings are useful
for several health applications from the perspective of understand-
ing characteristics about food, nutrition and calorie intake.

their pictures for reproducing the same dishes and share their
experiences for peer comments. As a result, the community
has access to rich, heterogeneous sources of information on
food. In recent years, food-computing [32] has become a
popular research topic due to its far-reaching impact on hu-
man life, health and well being. Analyzing food data could
support a lot of human-centric applications in medicine, biol-
ogy, gastronomy, and agronomy [32]. One of the important
tasks under the food-computing umbrella is Food Retrieval,
i.e., we are interested in retrieving relevant information about
a speciÔ¨Åc food. For example, given a food image, we are
interested in knowing its recipe, nutrition content, or calo-
rie information. In this paper, we investigate the problem
of cross-modal retrieval between cooking recipes and food
images, where our goal is to Ô¨Ånd an effective latent space to
map recipes to their corresponding food images.

The idea of cross-modal retrieval in the food domain is
to align matching pairs in a common space, so that given

11572

a recipe, the appropriate relevant images can be retrieved,
or given a food image, the corresponding recipe can be re-
trieved. Recent efforts addressing the problem for large-scale
cross-modal retrieval between cooking recipes and food im-
ages [40, 6, 9] use CNNs [18] for encoding the food images,
and LSTM [21] to encode recipes, and align the feature vec-
tor using common retrieval losses (e.g., pairwise cosine loss
and triplet loss). Despite achieving promising performance,
there are several critical shortcomings: (i) there can be high
variation in images corresponding to one recipe, making it
difÔ¨Åcult for a triplet loss using a naive sampling strategy
to converge quickly; (ii) They do not consider aligning the
feature distributions of the two heterogeneous modalities:
cooking recipes and food images, which can have very differ-
ent distributions; and (iii) Cross-Modal mapping using Deep
Neural Networks can lead to possible loss of information in
the embedding process, and for the existing approaches it is
unclear if the embedding of one modality is able to capture
relevant semantic information in the other modality.

To address the above limitations, we propose a novel
end-to-end framework named Adversarial Cross-Modal
Embedding (ACME). SpeciÔ¨Åcally, we propose an improved
triplet loss scheme empowered with hard sample mining,
which considerably improves the performance and conver-
gence over a traditional triplet loss. We propose to align the
embeddings from the two modalities using an adversarial
loss, in an attempt to making the feature distribution of the
cooking recipes and food images indistinguishable. We also
enforce the cross-modal translation consistency by recover-
ing the relevant information of one modality from the feature
embedding of another modality, i.e., we train the model to
generate an appropriate image given a recipe embedding, and
to predict the ingredients given a food image embedding.

Figure 1 shows an overview of ACME framework, which
is end-to-end trainable, and signiÔ¨Åcantly outperforms state-
of-the-art baselines for cross-modal retrieval on Recipe1M
[40] dataset. We conduct extensive ablation studies to evalu-
ate performance of various components of ACME. Finally,
we do qualitative analysis of the proposed method, and visu-
alize the retrieval results. The code is publicly available2.

2. Related Work

2.1. Food Computing

Food Computing has evolved as a popular research topic
in recent years [32, 2, 1]. With rapid growth of IoT and the in-
Ô¨Åltration of social media into our daily lives, people regularly
share food and image recipes online. This has given rise to
using this rich source of heterogeneous information for sev-
eral important tasks, and an immense opportunity to analyze
food related information. This has far reaching impact to our
daily lives, behaviour, health [35, 13] and culture[39]. Such

2https://github.com/LARC-CMU-SMU/ACME

analysis could have implications in medicine[15], biology[5],
gastronomy [33], agronomy [20] etc.

There are many ways to analyze food, and many tasks
that can be addressed using food computing. One commonly
studied task is recognition of food from images. This prob-
lem has been studied for years, where large datasets are
collected, and prediction models are trained on them. Early
approaches used kernel-based models [25], while more re-
cent approaches have exploited deep CNNs [27, 45, 30].
Another task that has received substantial attention is food
recommendation. This is a more difÔ¨Åcult task than tradi-
tional recommendation systems, as a variety of contextual
heterogeneous information needs to be integrated to make
recommendations. There have been several efforts in litera-
ture for food recommendation, including chatbot-based[14],
and dietary recommendation for diabetics[38].

Our focus is on another common task: Food Retrieval.
We aim to retrieve relevant information about a food item
given a query. For example, it can be a difÔ¨Åcult task to
estimate calorie and nutrition from a food image, but if we
could retrieve the recipe and ingredients from an image,
the task of nutrition and calorie estimation becomes much
simpler. There are several types of retrieval within food
computing: image-to-image retrieval [10], recipe-to-recipe
retrieval [7], and cross-modal image-to-recipe and recipe-
to-image retrieval[40]. Our work is in the domain of cross-
modal retrieval between cooking recipes and food images.

2.2. Cross Modal Retrieval

The goal of cross-modal retrieval is to retrieve relevant
instances from a different modality, e.g. retrieving an im-
age using text. The main challenge lies in the media gap
[36], which means features from different modalities are
inconsistent, making it difÔ¨Åcult to measure the similarity.

To solve this issue, many efforts focus on using pairs to
learn a similarity or distance metric to correlate cross-modal
data [40, 42, 11, 44]. Apart from metric learning meth-
ods, some alignment ideas are also used for cross-modal
retrieval: like global alignment [23, 4, 3] and local align-
ment [26, 24, 34]. Canonical Correlation Analysis (CCA)
[23] utilizes global alignment to allow the mapping of differ-
ent modalities which are semantically similar by maximizing
the correlation between cross-modal (similar) pairs. In [26],
local alignment is used to embed the images and sentences
into a common space. In order to enhance both global and
local alignment, [24] learns a multi-modal embedding by
optimizing pairwise ranking. Another line of work uses ad-
versarial loss [16] which has often been used for alignment
in domain adaptation[22] and cross-modal retrieval [43].

Existing work for large-scale cross-modal retrieval for
cooking recipes and food images include JE [40], which
uses a pairwise cosine loss to learn a joint embedding be-
tween the two modalities. This method was extended to

11573

use a simple triplet loss by AdaMine [6]. Another approach
tried to improve the embedding of the recipes using hier-
archical attention [9]. Unlike these efforts, our work uses
a superior sampling strategy for an improved triplet loss,
imposes cross-modal alignment, and enforces cross-modal
translation consistency towards achieving more effective and
robust cross-modal embedding.

3. Adversarial Cross-Modal Embedding

We now present our proposed Adversarial Cross-Modal

Embedding (ACME) between recipes and food images.

3.1. Overview

Given a set of image-recipe pairs (it, rt) for t = 1, . . . , T ,
where a food image it ‚àà I and a recipe rt ‚àà R (where I and
R correspond to the image and recipe domains respectively),
our goal is to learn embedding functions EV : I ‚Üí Rd and
ER : R ‚Üí Rd which encode the image and the recipe into a
d‚àídimensional visual vector and recipe vector, respectively.
The embedding functions should be learned so that the em-
bedding of a food image and its corresponding recipe should
be close to each other (for it1 and rt2 where t1 = t2), and
should be distant from embedding of other images or recipes
(for it1 and rt2 where t1 6= t2). Such an embedding is well
suited for retrieval tasks. Note that in our work, the paired
instances may have a many-to-one relationship from images
to recipes, i.e., we may have many images for a given recipe.
Accordingly, we want the embeddings of all the images for a
given recipe to be close to the embedding of this recipe, and
distant from the embedding of other recipes.

A simple way to learn this embedding is to use a pair-
wise cosine loss at the level of feature representation, and
use back-propagation to learn the embedding functions [40].
As this loss function may not be ideal for retrieval tasks,
triplet-loss was also considered [6]. However, we hypothe-
size that both of these approaches suffer from several critical
limitations: (i) They give equal importance to all the sam-
ples during the optimization of the triplet loss, which may
signiÔ¨Åcantly hinder the convergence and generalization, as
there could be a high variance among many images corre-
sponding to the same recipe; (ii) Being from heterogeneous
modalities, the feature distributions can be very dissimilar,
and the existing approaches do not try to align these dis-
tributions; and (iii) The embedding functions possibly lose
important information during the embedding process, as a
result, the embedding of one modality may not effectively
capture semantic information in the other modality.

To address these issues, we propose a novel end-to-
end framework for Adversarial Cross-Modal Embedding
(ACME) between Cooking Recipes and Food Images. To ad-
dress the Ô¨Årst challenge, we propose a new retrieval learning
component that leverages a hard sample mining [19] strategy
to improve model training and performance of the existing

triplet loss. To address the modality-distribution alignment,
we use an adversarial loss [16] to ensure that features of the
embedding functions across different modalities follow the
same distribution. Finally, we introduce a novel cross-modal
translation consistency component, wherein food images are
generated using the embedded recipe features, and the in-
gredients of a recipe are predicted using the image features.
The entire pipeline can be trained in an end-to-end manner
with a joint optimization objective. The overall proposed
architecture is shown in Figure 2.

More formally, during the feed-forward Ô¨Çow through the
pipeline, the food images i and recipes r are encoded using
a CNN (parameterized by EV ) and an LSTM (parameter-
ized by ER), respectively. The CNN gives us high-level
visual features Vm ‚àà Rd, and the LSTM gives us high-level
recipe features Rm ‚àà Rd. These high-level features are
then passed through a fully-connected layer (parameterized
by FC), where both modalities share the same weight [37],
with the purpose of correlating the common representation
of each modality, and give us the Ô¨Ånal representation V and
R for the visual features and recipe features, respectively.
The recipe features R are then used to generate food images,
and the visual features V are used to predict the ingredients
in the particular instance. This framework is then optimized
over three objectives: to achieve a feature representation that
is good at retrieval tasks; to obtain a feature representation
that aligns the distribution of the two modalities in order to
make them modality-invariant; and the features achieve the
cross-modal translation consistency. The overall objective
of the proposed ACME is given as:

L = LRet + Œª1LM A + Œª2LT rans,

(1)

where Œª1 and Œª2 are trade-off parameters. The retrieval
learning component LRet(V, R) receives the two high-level
feature vectors: V ‚àà Rd for the image and R ‚àà Rd for
the recipes, and computes the retrieval loss. The modal-
ity alignment component LM A(Vm, Rm) operates on the
penultimate layer features Vm ‚àà Rd and Rm ‚àà Rd, and
aims to achieve modality-invariance using an adversarial
loss to align the two distributions. The cross-modal transla-
tion consistency component LT rans is further divided into
two sub components: recipe2image (generates food image
from R) and image2recipe (predicts ingredients based on
V ). recipe2image and image2recipe are optimized using an
adversarial loss and classiÔ¨Åcation loss respectively. At the
end of the training procedure, the feature representations V
and R are used for retrieval tasks.

3.2. Cross Modal Retrieval Learning

After images and recipes are passed through the encoder
functions and the weight sharing layers, visual representa-
tions V and recipe representations R are obtained. Our goal
is to have feature representation V t1 and Rt2 be similar for a

11574

Modality Alignment

Hard Sample Mining

Translation Consistency

Input

images

ùë¨ùëΩ (CNN)

ùë≥ùë¥ùë®

instructions

‚Ä¶

ingredients

‚Ä¶

ùë¨ùëπ (RNN)

ùëΩùíé

ùëπùíé

ùëΩ

ùë≥ùëπùíÜùíï

ùë≠ùë™

ùëπ

Retrieval Learning

Weight Sharing

G

Real

Fake

ùë≥ùë™ùíäùüêùíì
ùë≥ùíäùüêùíì
ùë≥ùë™ùíìùüêùíä
ùë≥ùíìùüêùíä

C

C

C

D

ùë≥ùëªùíìùíÇùíèùíî

Figure 2: ACME: Adversarial Cross-Modal Embedding: Our proposed framework to achieve effective cross-modal embedding. The
food images and recipes are encoded using a CNN and LSTM respectively to give feature embeddings. The feature embeddings are aligned
using an adversarial loss LM A to achieve Modality Alignment. Both image and text embeddings are then passed through a shared FC
layer to give the Ô¨Ånal embedding. This embedding is learned by optimizing LRet, which uses triplet loss with hard sample mining. This
embedding is also optimized to achieve cross-modal translation consistency (LT rans), where the recipe embedding is used to generate a
corresponding real food image, and the the food image embedding is used to predict the ingredients in the food item.

given pair (t1 = t2) and dissimilar for t1 6= t2. This is done
via the usage of the triplet loss [41]. A triplet comprises
one feature embedding as an anchor point in one modality,
and a positive and negative feature embedding from another
modality. The positive instance corresponds to the one we
want it to be similar to the anchor point, and the negative
instance should be dissimilar to the anchor point. In our case,
we have two types of triplets: one where the visual feature V
behaves as the anchor, and another where the recipe feature
R behaves as the anchor. The objective is then given as:

LRet = X

[d(Va, Rp) ‚àí d(Va, Rn) + Œ±]+

V

+ X

R

[d(Ra, Vp) ‚àí d(Ra, Vn) + Œ±]+ ,

(2)

Our goal is to minimize this objective as:

min

EV ,ER,FC

LRet

Here d(‚Ä¢) is the Euclidean distance, subscripts a, p and n
refer to anchor, positive and negative samples, respectively,
and Œ± is the margin of error. To improve the convergence
of learning, we integrate the hard sample mining strategy
into the process of learning with triplet loss. In particular,
unlike the traditional approach that simply treats all instances
equally important for the triplet construction for a given
anchor, the proposed approach gives preference to the most

distant positive instance and the closest negative instance
during the training procedure.

3.3. Modality Alignment

A major challenge in using encoded features from differ-
ent modalities is that the distributions of the encoded features
can be very different, resulting in poor generalization and
slower convergence. A more desirable solution is to align
the distribution of the encoded features. We aim to align
the distributions of the penultimate layer features Vm and
Rm. To do so, we use an adversarial loss, where we try to
achieve a feature representation such that a discriminator
DM cannot distinguish whether the feature representation
was obtained from the image or the recipe. We empirically
adopt WGAN-GP [17] in our experiment. The objective
LM A is given as:

LM A =Ei‚àºpimage[log DM (EV (i)]+

Er‚àºprecipe[log(1 ‚àí DM (ER(r)))],

(3)

and solved by a min-max optimization as:

min
EV ,ER

max
DM

LM A

3.4. Translation Consistency

In order to have better generalization, we want the learned
embedding of one modality to be able to recover the corre-
sponding information in the other modality, leading to better

11575

semantic alignment. This would enforce a cross-modal trans-
lation consistency, ensuring that the learned feature represen-
tation preserve information across modalities. SpeciÔ¨Åcally,
we aim to use recipe features R to generate a food image,
and visual features V to predict the ingredients of the recipe.
The objective is given as:

LT rans = Lr2i + Li2r

Below we present the details of each of these losses, which
help achieve the cross-modal translation consistency.

3.4.1 Recipe2Image

For the recipe2image generation, we have a two-fold goal:
the generated images must be realistic, and their food-class
identity is preserved. Taking the recipe encoding as input,
we use a generator Gr2i to generate an image. A discrimi-
nator Dr2i is then used to distinguish whether the generated
image is real or fake. At the same time, a Food-ClassiÔ¨Åer
Cr2i is used to predict which food category the generated
image belongs to. During training, the discriminator Dr2i
and classiÔ¨Åer Cr2i receive both real images and the gen-
erated fake images as input. SpeciÔ¨Åcally, we borrow the
idea of StackGAN [46] to guarantee the quality of generated
food images, and the generator Gr2i is conditioned on the
recipe feature R = FC(ER(r)). The adversarial objective
is formulated as:

LGr2i =Ei‚àºpimage[log Dr2i(i)]+

Er‚àºprecipe[log(1 ‚àí Dr2i(Gr2i(FC(ER(r)))))]

(4)
While the adversarial loss is able to generate realistic images,
it does not guarantee translation consistency. Therefore, we
use a food-category classiÔ¨Åer which encourages the gener-
ator to use the recipe features to generate food items in the
appropriate corresponding food-category. The objective is a
cross-entropy loss denoted by LCr2i. Combining these two
objectives, our optimization is formulated as:

min

Gr2i,Cr2i,ER,FC

max
Dr2i

Lr2i = LGr2i + LCr2i

3.4.2

Image2Recipe

To achieve the image2recipe translation, we aim to recover
the ingredients in the food image. This is done by applying
a multi-label classiÔ¨Åer on the visual features V , which will
predict the ingredients in the food image. The multi-label
objective is denoted as LGi2r (as it is in some sense generat-
ing the ingredients from a given image). Like in the case of
recipe2image, we also maintain translation consistency by
ensuring the features can be classiÔ¨Åed into the correct food
category, using cross-entropy loss LCi2r . The optimization
is formulated as:

min

Gi2r ,Ci2r ,EI ,FC

Li2r = LGi2r + LCi2r

4. Experiments

4.1. Dataset and Evaluation Metrics

We evaluate the effectiveness of our proposed method
[40], one of the largest collection
on Recipe1M dataset
of public cooking recipe data along with food images. It
comprises over 1m cooking recipes and 800k food images.
The authors assigned a class label to each recipe based on
the recipe titles. This resulted in 1,047 food-classes (which
provide relevant information for the cross-modal translation
consistency). The authors also identiÔ¨Åed 4,102 frequently
occurring unique ingredients. We adopt the the original data
splits [40] using 238,999 image-recipe pairs for training,
51,119 pairs for validation and 51,303 pairs for testing.

We evaluate the model using the same metrics as prior
work [40, 6]. SpeciÔ¨Åcally, we Ô¨Årst sample 10 different sub-
sets of 1,000 pairs (1k setup), and 10 different subsets of
10,000 (10k setup) pairs in the testing set. Then, we consider
each item in a modality as a query (for example, an im-
age), and we rank instances in the other modality (example
recipes) according to the L2 distance between the embed-
ding of query and that of candidate. Using the L2 distance
for retrieval, we evaluate the performance using standard
metrics in cross-modal retrieval task. For each test-subset
we sampled before (1k and 10k), we compute the median re-
trieval rank (MedR). We also evaluate the Recall Percentage
at top K (R@K), i.e., the percentage of queries for which the
matching item is ranked among the top K results.

4.2. Implementation Details

Our image encoder EV is initialized as a ResNet-50 [18]
pretrained on ImageNet [12], and gives a 1024-dimensional
feature vector. A recipe comprises a set of instructions and a
set of ingredients. The recipe encoder ER is a hierarchical
LSTM [21] to encode the instructions (where the word-
level embedding is obtained from a pretrained skip-thought
algorithm [29]), and a bidirectional-embedding to encode
the ingredients (where the ingredient embeddings are ob-
tained from word2vec)
[31]. Both embeddings are then
concatenated and go through a fully-connected layer to give
a 1024-dimensional feature vector. During image generation,
from recipe2image, our generator Gr2i generated images of
size 128 √ó 128 √ó 3. We set Œª1 and Œª2 in Eq. (1) to be 0.005
and 0.002 respectively. The model was trained using Adam
[28] with the batch size of 64 in all our experiments. Initial
learning rate is set as 0.0001, and momentum is set as 0.999.

4.3. Baselines

We compare against several state-of-the-art baselines:
CCA [23]: Canonical Correlation Analysis is one of the
most widely-used models for learning a common embedding
from different feature spaces. CCA learns two linear pro-
jections for mapping text and image features to a common

11576

Table 1: Main Results. Evaluation of performance of ACME compared against the baselines. The models are evaluated on the basis of
MedR (lower is better), and R@K (higher is better).

Size of test-set

Image to recipe retrieval

Recipe to image retrieval

Methods

medR ‚Üì R@1 ‚Üë R@5 ‚Üë R@10 ‚Üë medR ‚Üì R@1 ‚Üë R@5 ‚Üë R@10 ‚Üë

CCA [23]
SAN [8]
JE [40]
AM [9]
AdaMine [6]
ACME (ours)

JE [40]
AM [9]
AdaMine [6]
ACME (ours)

15.7
16.1
5.2
4.6
1.0
1.0

41.9
39.8
13.2
6.7

14.0
12.5
25.6
25.6
39.8
51.8

-
7.2
14.9
22.9

32.0
31.1
51.0
53.7
69.0
80.2

-
19.2
35.3
46.8

43.0
42.3
65.0
66.9
77.4
87.5

-
27.6
45.2
57.9

24.8
-
5.1
4.6
1.0
1.0

39.2
38.1
12.2
6.0

9.0
-
25.0
25.7
40.2
52.8

-
7.0
14.8
24.4

24.0
-
52.0
53.9
68.1
80.2

-
19.4
34.6
47.9

35.0
-
65.0
67.1
78.7
87.6

-
27.8
46.1
59.0

1k

10k

space that maximizes feature correlation.

SAN [8]: Stacked Attention Network (SAN) considers
ingredients only (and ignores recipe instructions), and learns
the feature space between ingredients and image features
through a two-layer deep attention mechanism.

JE [40]: uses pairwise cosine loss to Ô¨Ånd a joint embed-
ding between different modalities. They attach a classiÔ¨Åer to
the embedding and predict the food category (1,047 classes).
AM [9]: In addition to triplet loss, AM uses an attention
mechanism over the recipe, which is applied at different
levels (title, ingredients and instructions). Compared with
JE, AM uses additional information from food title.

AdaMine [6]: uses a double triplet loss where the triplet
loss is applied to both learning a joint embedding and clas-
siÔ¨Åcation of the embedding into appropriate food category.
They also integrate the adaptive learning schema which per-
forms adaptive mining for signiÔ¨Åcant triplets.

4.4. Quantitative Results

4.4.1 Main Results

We show the performance of ACME for cross-modal re-
trieval against the baselines in Table 1. In retrieval tasks on
both 1k, and 10k test datasets, our proposed method ACME
outperforms all the baselines across all metrics. On the 1k
test dataset, ACME achieves a perfect median rank of 1.0
in both image to recipe and recipe to image retrieval tasks,
matching the performance of the current state-of-the-art best
method. When we shift to the larger 10k test dataset, where
retrieval becomes much more difÔ¨Åcult, ACME achieves a
Median Rank of less than 6.7 and 6.0 respectively for the
two retrieval tasks, which is about 50% lower than the cur-
rent state-of-the-art method. The performance of ACME
for Recall@K is signiÔ¨Åcantly superior to all the baselines
by a substantial margin. On the whole, the performance

of ACME is shown to be very promising, beating all the
state-of-the-art methods across all the metrics consistently.

4.4.2 Ablation Studies

We also conduct extensive ablation studies to evaluate the
contributions of each of our proposed components in the
ACME framework. SpeciÔ¨Åcally, we Ô¨Årst evaluate the perfor-
mance of most basic version of the model with a basic triplet
loss (TL). We incrementally add more components: Ô¨Årst,
we evaluate the gains from introducing Hard-sample Mining
(HM). Based on this model, we then add the Modality Align-
ment component (MA). Based on the resultant model, we
then add the cross-modal translation components to measure
their usefulness. In particular we want to see the effect of
each cross-modal translation R2I and I2R, and their com-
bined effect. We also measure the effect of the full model
with and without MA, and Ô¨Ånally combine all the compo-
nents in the ACME framework. We evaluate these compo-
nents on image-to-recipe retrieval, and the results are shown
in Table 2. In general, we observe that every of the proposed
component adds positive value to the embedding model by
improving the performance, and all of them work in a col-
laborative manner to give an overall improved performance.

4.5. Qualitative Results

Here, we visualize some results of ACME. We use the
trained ACME model, and perform the two retrieval tasks
(Recipe to Image and Image to Recipe) on the full 50k test
dataset. Note that this is much harder than the 1k dataset
presented in the previous section, and retrieving the correct
instance (out of 50k possibilities) is difÔ¨Åcult. Our main
goal is to show, that despite this difÔ¨Åculty, the top retrieved
items appear to be (qualitatively) good matches for the query.
We Ô¨Årst show the query instance in one modality, and the

11577

Food class

Recipe query

Top 5 retrieved images

coffee cake

sauce chicken

fruit salad

Ingredients: sugar,  eggs, 
cornstarch, lemon juice, baking 
powder, buttermilk 

Instructions : Combine sugar and 
cornstarch in a large saucepan; 
stir in rhubarb and strawberries 
and br. Preheat oven to 350 
degrees F (175 degrees C).‚Ä¶

Ingredients: Chicken, cooked 
rice, soy sauce, pepper, 
sesame oil, garlic powder 

Instructions: Preheat oven to 350 
and prepare a 13x9 pan with foil 
and spray foil with non stick spray 
in mins. the soy sauce will turn the 
chicken brown slightly‚Ä¶

Ingredients: honey, water, 
ground ginger, lime zest, fresh 
lime juice, sugar, orange zest

Instructions: Combine first 6 
ingredients in a small saucepan. 
Bring to a boil over medium heat; 
cook 5 minutes, whisking 
constantly. Remove from heat‚Ä¶

Figure 3: Analysis of Recipe to Image Retrieval on the full 50k test dataset. The Ô¨Årst column denotes the food category, and the second
column is the query recipe. The top 5 results retrieved by ACME are shown. The best match (i.e., the ground truth) is boxed in red. In most
cases, the top retrieved images display similar concepts as the ground truth.

Chicken soup

Sugar cookies

Image query

Ingredients: chicken broth, 
lemongrass, coconut milk, fish sauce, 
boneless chicken breasts, gingerroot 

Ingredients: chicken breasts, 
garlic, chicken broth, noodles, 
carrots, celery 

Ingredients: pears, celery, onion, 
red pepper, chicken stock, fresh 
ginger, dry white wine 

Ingredients: sugar, baking 
powder, eggs, powdered sugar, 
milk

Ingredients: egg yolks, baking 
powder, eggs, vanilla, sugar, 
Unsalted butter

Ingredients: sugar, vanilla, 
butter, flour, baking powder, 
eggs, salt

True recipes

Instructions: Combine the coconut 
milk, chicken broth, lemongrass 
and ginger in a large pot. Bring 
to a simmer, stirring frequently or 
the coconut milk will curdle‚Ä¶

Instructions: Cut chicken into 
small cubes and boil in broth for 
about 30 minutes. Cut chicken 
into small cubes and boil in broth 
for about 30 minutes‚Ä¶

Instructions: Drain the pears, 
chop into large chunks, discard 
the juice (or drink it!) and set 
aside. Heat the oil in a large 
saute pan with a lid‚Ä¶

Instructions: Cream together sugar 
and shortening. Add and mix in 
eggs, almond extract, milk, flour, 
salt and baking powder‚Ä¶

Instructions: preheat oven to 375, 
start by creaming your butter and 
sugar till a fluffy lighter 
consistency, start adding your 
eggs 1 at a time‚Ä¶

Instructions: In a large bowl, 
cream together the sugar and 
butter. Beat in the eggs and 
vanilla until smooth. Stir in the 
flour , baking powder and salt ‚Ä¶

Retrieved recipes

Ingredients: chicken thighs, 
powder, coconut milk, galangal, 
chicken stock, sugar, mushrooms 

Ingredients: carrot, water, chicken
broth, turnip, garlic cloves, salt

Ingredients: onions, garlic cloves, 
olive oil, bay leaves, chicken broth
cumin

Ingredients: milk, water, eggs, 
vanilla, oil, Dream Whip topping 
mix

Ingredients: shortening, water, 
almond extract, confectioners' 
sugar, vanilla

Ingredients: unsalted butter, 
bread flour, granulated sugar, 
cookie butter, egg, lemon juice

Instructions: Pour coconut milk in 
the pot add galanga and lemon 
grass bring to boil. Add chicken 
and chilies bring to boil again till 
chicken is cook‚Ä¶

Instructions: Combine: chicken & 
next 10 ingredients in a large 
dutch oven, and bring mixture to a 
boil over high heat. REmove
chicken from borth ‚Ä¶

Instructions: Saute onions in oil til
soft. Add garlic, celery, and carrots 
& saute lightly. Add everything else 
& bring to boil. Turn down heat & 
simmer 1 1/2-2 hrs ‚Ä¶

Instructions: Prepare cake 
according to boxed instructions in 
a 9 x 13 cake pan. Remove when 
done and poke holes with the end 
of a wooden spoon ‚Ä¶

Instructions: In a large mixing 
bowl beat the shortening creamer 
and extracts. Gradually beat in 
the confectioners sugar. Add in 
enough water‚Ä¶

Instructions: Bring the butter to 
room-temperature to soften. Cream 
with an electric whisk, then whisk 
in the sugar, egg, and lemon‚Ä¶

Figure 4: Analysis of Image to Recipe Retrieval on the full 50k test dataset. We consider two food-categories: chicken soup and sugar
cookies. We show 3 different image-queries in each of these categories. Below each image is the true recipe, and below that is the top
retrieved recipe by ACME. We can observe that our retrieved recipe has several common ingredients with the true recipe.

corresponding ground truth in the other modality. We then
retrieve the top results in the second modality, which could
show the comparison with the true instance. Even though,
often we are not able to obtain the ground truth, ACME
retrieves instances that are very similar to this ground truth.

4.5.1 Recipe to Image Retrieval Results

We show some examples of recipe to image retrieval in
Figure 3. We consider 3 food-classes: coffee cake, sauce

chicken, and fruit salad, and pick up a recipe to query from
each of these classes. ACME is then used to rank the images,
and then we look at the top 5 retrieved results. In all cases,
the retrieved images are visually very similar to the ground
truth image. For example, the images retrieved in case of
coffee cake, all resemble cakes; images retrieved for sauce
chicken, all have some form of cooked chicken, couple of
them with rice (the ground truth does not have rice, but the
ingredients in the query recipe use cooked rice) ; images
retrieved for fruit salad appear to have fruits in all of them.

11578

Table 2: Ablation Studies. Evaluation of beneÔ¨Åts of different
components of the ACME framework. The models are evaluated
on the basis of MedR (lower is better), and R@K (higher is better).

Component
TL
TL+HM
TL+HM+MA
TL+HM+MA+R2I
TL+HM+MA+I2R
TL+HM+R2I+I2R
All

MedR R@1 R@5 R@10
70.1
85.1
85.5
85.7
86.1
85.5
87.5

25.9
47.5
48.0
50.0
49.3
49.5
51.8

56.4
76.2
77.3
77.8
78.4
77.9
80.2

4.1
2.0
1.9
1.4
1.8
1.6
1.0

This suggests that the common space learned by our structure
has done a good job of capturing semantic information across
the two modalities.

4.5.2

Image to Recipe Retrieval Results

We show the image to recipe retrieval results in Figure 4.
We consider two food categories: chicken soup, and sugar
cookies, and use 3 images in each category as a query image.
We retrieve the top recipe for each image query, and compare
it with the true recipe corresponding to that image. In the
case of chicken soup, it is often hard to view the chicken in
the food image (as it maybe submerged in the soup), yet the
ingredients in the recipe retrieved based on the ACME em-
bedding contains chicken. We also observe several common
ingredients between the true recipe and the retrieved recipe
for the sugar cookies images. Such a performance can have
several interesting applications for food-computing. For ex-
ample, a user can take a food image, retrieve the recipe, and
estimate the nutrients and the calories for that food image.

4.5.3 Cross-Modal Translation Consistency

An interesting by-product of the proposed ACME framework
is the cross-modal translation. These translation components
were primarily used to add constraints to obtain a better se-
mantic embedding for our primary retrieval task. We do not
claim that these components are robust and generate very
good translation (as the food categories are noisy, generating
real images is difÔ¨Åcult, classiÔ¨Åcation of food into 1,047 cate-
gories and multi-label classiÔ¨Åcation of ingredients into over
4,102 categories is very difÔ¨Åcult). However, the cross-modal
translation can sometimes obtain very interesting results.

For example, in Figure 5 we see a recipe2image trans-
lation. Given a recipe query, we retrieve a real food image
from the dataset, and compare it with the image generated
by the r2i component of ACME. For two instances of ice
cream, we can see that the generated images show incredible
resemblance to the real food images. Similarly, we look at
the ingredient prediction from image to see the results of

the i2r component in Figure 6. The query image is a pork
chop. We can see several correctly predicted ingredients for
the query image. In fact, it is wholly plausible that using the
ingredients predicted, it would be possible to cook the dish
to obtain a very similar image as the query image.

Recipe Query (chocolate chip)
Ingredients: all - purpose flour, sugar, butter, egg, 
almond extract, seedless raspberry jam, miniature 
semisweet chocolate chips
Instructions: In a large bowl, combine flour and 
sugar. Cut in butter until mixture resembles course 
crumbs. Stir in egg and extract just until moistened, 
set aside 1 C of crumb mixture for topping‚Ä¶

Recipe Query (ice cream)
Ingredients: sugar, lemon verbena, buttermilk, 
white chocolate, whole milk
Instructions: Take one cup of buttermilk and heat it 
slowly in a pan with 8 pieces of the white 
chocolate. Stir until the chocolate has completely 
dissolved. Take the mix off the stove and stir in the 
rest of the buttermilk and the whole milk‚Ä¶

Retrieved Images

Generated Images

Retrieved Images

Generated Images

Figure 5: Visualization of the retrieved food images and generated
images (generated by r2i translation component) for a query recipe.

True Ingredients:
Tomatoes, cayenne pepper, pork chop, garlic 
clove, salt
Retrieved Ingredients:
boneless sirloin pork chops, black pepper, chili 
sauce, garlic powder, salt
Predicted Ingredients:
browning sauce, pork rib chops, cracked 
pepper, dried parsley flakes, garlic scapes

Image query

Figure 6: Visualization of the retrieved ingredients and predicted
results (predicted by i2r translation component) for query image
pork chops.

5. Conclusion

In this paper, we have proposed an end-to-end framework
ACME for learning a joint embedding between cooking
recipes and food images, where we are the Ô¨Årst to use adver-
sarial networks to guide the learning procedure. SpeciÔ¨Åcally,
we proposed the usage of hard sample mining, used an ad-
versarial loss to do modality alignment, and introduced a
concept of cross-modal translation consistency, where we
use the recipe embedding to generate an appropriate food
image, and use the food image embedding to recover the in-
gredients in the food. We conducted extensive experiments,
and ablation studies, and achieved state-of-the-art results in
the Recipe1M dataset for cross-modal retrieval.

Acknowledgments

This research is supported by the National Research Foun-
dation, Prime Minister‚Äôs OfÔ¨Åce, Singapore under its Interna-
tional Research Centres in Singapore Funding Initiative.

11579

References

[1] Palakorn Achananuparp, Ee-Peng Lim, and Vibhanshu Ab-
hishek. Does journaling encourage healthier choices?: An-
alyzing healthy eating behaviors of food journalers. In Pro-
ceedings of the 2018 International Conference on Digital
Health, pages 35‚Äì44. ACM, 2018.

[2] Palakorn Achananuparp, Ee-Peng Lim, Vibhanshu Abhishek,
and Tianjiao Yun. Eat & tell: A randomized trial of random-
loss incentive to increase dietary self-tracking compliance. In
Proceedings of the 2018 International Conference on Digital
Health, pages 45‚Äì54. ACM, 2018.

[3] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu.
Deep canonical correlation analysis. In International Confer-
ence on Machine Learning, pages 1247‚Äì1255, 2013.

[4] Francis R Bach and Michael I Jordan. Kernel independent
component analysis. Journal of machine learning research,
3(Jul):1‚Äì48, 2002.

[5] Carl A Batt.

Food pathogen detection.

Science,

316(5831):1579‚Äì1580, 2007.

[6] Micael Carvalho, R√©mi Cad√®ne, David Picard, Laure Soulier,
Nicolas Thome, and Matthieu Cord. Cross-modal retrieval in
the cooking context: Learning semantic text-image embed-
dings. In ACM SIGIR, 2018.

[7] Minsuk Chang, L√©onore V Guillain, Hyeungshik Jung, Vi-
vian M Hare, Juho Kim, and Maneesh Agrawala. Recipescape:
An interactive tool for analyzing cooking instructions at scale.
In Proceedings of the 2018 CHI Conference on Human Fac-
tors in Computing Systems, page 451. ACM, 2018.

[8] Jingjing Chen, Lei Pang, and Chong-Wah Ngo. Cross-modal
recipe retrieval: How to cook this dish? In International Con-
ference on Multimedia Modeling, pages 588‚Äì600. Springer,
2017.

[9] Jing-Jing Chen, Chong-Wah Ngo, Fu-Li Feng, and Tat-Seng
Chua. Deep understanding of cooking procedure for cross-
modal recipe retrieval. In 2018 ACM Multimedia Conference
on Multimedia Conference, pages 1020‚Äì1028. ACM, 2018.

[10] Gianluigi Ciocca, Paolo Napoletano, and Raimondo Schettini.
Learning cnn-based features for retrieval of food images. In
International Conference on Image Analysis and Processing,
pages 426‚Äì434. Springer, 2017.

[11] Antonia Creswell and Anil Anthony Bharath. Adversarial
In European Conference on

training for sketch retrieval.
Computer Vision, pages 798‚Äì809. Springer, 2016.

[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
2009.

[13] Takumi Ege and Keiji Yanai. Image-based food calorie es-
timation using knowledge on food categories, ingredients
and cooking directions. In Proceedings of the on Thematic
Workshops of ACM Multimedia 2017, pages 367‚Äì375. ACM,
2017.

[14] Ahmed Fadhil. Can a chatbot determine my diet?: Addressing
challenges of chatbot application for meal recommendation.
arXiv preprint arXiv:1802.09100, 2018.

[15] Giovanni Maria Farinella, Dario Allegra, Marco Moltisanti,
Filippo Stanco, and Sebastiano Battiato. Retrieval and classi-

Ô¨Åcation of food images. Computers in biology and medicine,
77:23‚Äì39, 2016.

[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances
in neural information processing systems, pages 2672‚Äì2680,
2014.

[17] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville.
Improved training of
wasserstein gans. In Advances in Neural Information Process-
ing Systems, pages 5767‚Äì5777, 2017.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770‚Äì778, 2016.

[19] Alexander Hermans, Lucas Beyer, and Bastian Leibe.

In
defense of the triplet loss for person re-identiÔ¨Åcation. arXiv
preprint arXiv:1703.07737, 2017.

[20] Jos√© Luis Hern√°ndez-Hern√°ndez, Mario Hern√°ndez-
Hern√°ndez, Severino Feliciano-Morales, Valent√≠n √Ålvarez-
Hilario, and Israel Herrera-Miranda. Search for optimum
color space for the recognition of oranges in agricultural
Ô¨Åelds.
In International Conference on Technologies and
Innovation, pages 296‚Äì307. Springer, 2017.

[21] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term

memory. Neural computation, 9(8):1735‚Äì1780, 1997.

[22] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adaptation.
ICML, 2018.

[23] Harold Hotelling. Relations between two sets of variates.

Biometrika, 28(3/4):321‚Äì377, 1936.

[24] Xinyang Jiang, Fei Wu, Xi Li, Zhou Zhao, Weiming Lu,
Siliang Tang, and Yueting Zhuang. Deep compositional cross-
modal learning to rank via local-global alignment. In Proceed-
ings of the 23rd ACM international conference on Multimedia,
pages 69‚Äì78. ACM, 2015.

[25] Taichi Joutou and Keiji Yanai. A food image recognition
system with multiple kernel learning. In Image Processing
(ICIP), 2009 16th IEEE International Conference on, pages
285‚Äì288. IEEE, 2009.

[26] Andrej Karpathy, Armand Joulin, and Li F Fei-Fei. Deep
fragment embeddings for bidirectional image sentence map-
ping. In Advances in neural information processing systems,
pages 1889‚Äì1897, 2014.

[27] Yoshiyuki Kawano and Keiji Yanai. Food image recogni-
tion with deep convolutional features. In Proceedings of the
2014 ACM International Joint Conference on Pervasive and
Ubiquitous Computing: Adjunct Publication, pages 589‚Äì593.
ACM, 2014.

[28] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

[29] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard
Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
Skip-thought vectors.
In Advances in neural information
processing systems, pages 3294‚Äì3302, 2015.

11580

[43] Bokun Wang, Yang Yang, Xing Xu, Alan Hanjalic, and
Heng Tao Shen. Adversarial cross-modal retrieval. In Pro-
ceedings of the 2017 ACM on Multimedia Conference, pages
154‚Äì162. ACM, 2017.

[44] Wei Wu, Jun Xu, and Hang Li. Learning similarity function
between objects in heterogeneous spaces. Microsoft Research
Technique Report, 2010.

[45] Keiji Yanai and Yoshiyuki Kawano. Food image recognition
using deep convolutional network with pre-training and Ô¨Åne-
tuning. In Multimedia & Expo Workshops (ICMEW), 2015
IEEE International Conference on, pages 1‚Äì6. IEEE, 2015.

[46] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In Proceedings of the ICCV,
pages 5907‚Äì5915, 2017.

[30] Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod
Vokkarane, and Yunsheng Ma. Deepfood: Deep learning-
based food image recognition for computer-aided dietary as-
sessment. In International Conference on Smart Homes and
Health Telematics, pages 37‚Äì48. Springer, 2016.

[31] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In Advances in neural
information processing systems, pages 3111‚Äì3119, 2013.

[32] Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and
Ramesh Jain. A survey on food computing. arXiv preprint
arXiv:1808.07202, 2018.

[33] Ole G Mouritsen, Lars Duelund, Ghislaine Calleja, and
Michael Bom Fr√∏st. Flavour of fermented Ô¨Åsh, insect, game,
and pea sauces: garum revisited. International journal of
gastronomy and food science, 9:16‚Äì28, 2017.

[34] Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang
Hua. Hierarchical multimodal lstm for dense visual-semantic
embedding. In Computer Vision (ICCV), 2017 IEEE Interna-
tional Conference on, pages 1899‚Äì1907. IEEE, 2017.

[35] Ferda OÔ¨Çi, Yusuf Aytar, Ingmar Weber, Raggi al Hammouri,
and Antonio Torralba. Is saki# delicious?: The food percep-
tion gap on instagram and its relation to health. In Proceed-
ings of the 26th International Conference on World Wide Web,
pages 509‚Äì518. International World Wide Web Conferences
Steering Committee, 2017.

[36] Yuxin Peng, Xin Huang, and Yunzhen Zhao. An overview of
cross-media retrieval: Concepts, methodologies, benchmarks,
and challenges. IEEE Transactions on Circuits and Systems
for Video Technology, 28(9):2372‚Äì2385, 2018.

[37] Yuxin Peng, Jinwei Qi, and Yuxin Yuan. Cm-gans: Cross-
modal generative adversarial networks for common represen-
tation learning. arXiv preprint arXiv:1710.05106, 2017.

[38] Faisal Rehman, Osman Khalid, Kashif Bilal, Sajjad A Madani,
et al. Diet-right: A smart food recommendation system. KSII
Transactions on Internet & Information Systems, 11(6), 2017.
[39] Sina Sajadmanesh, Sina Jafarzadeh, Seyed Ali Ossia,
Hamid R Rabiee, Hamed Haddadi, Yelena Mejova, Mirco
Musolesi, Emiliano De Cristofaro, and Gianluca Stringhini.
Kissing cuisines: Exploring worldwide culinary habits on the
web. In Proceedings of the 26th International Conference on
World Wide Web Companion, pages 1013‚Äì1021. International
World Wide Web Conferences Steering Committee, 2017.

[40] Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin,
Ferda OÔ¨Çi, Ingmar Weber, and Antonio Torralba. Learning
cross-modal embeddings for cooking recipes and food images.
Training, 720(619-508):2, 2017.

[41] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniÔ¨Åed embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 815‚Äì823, 2015.

[42] Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D
Manning, and Andrew Y Ng. Grounded compositional se-
mantics for Ô¨Ånding and describing images with sentences.
Transactions of the Association of Computational Linguistics,
2(1):207‚Äì218, 2014.

11581

