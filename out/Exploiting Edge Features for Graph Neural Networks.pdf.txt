Exploiting Edge Features for Graph Neural Networks

Liyu Gong1 Qiang Cheng ∗ 1,2

1 Institute for Biomedical Informatics, University of Kentucky, Lexington, USA
2 Department of Computer Science, University of Kentucky, Lexington, USA

{liyu.gong, Qiang.Cheng}@uky.edu

Abstract

Edge features contain important

information about
graphs. However, current state-of-the-art neural network
models designed for graph learning, e.g., graph convo-
lutional networks (GCN) and graph attention networks
(GAT), inadequately utilize edge features, especially multi-
dimensional edge features. In this paper, we build a new
framework for a family of new graph neural network mod-
els that can more sufﬁciently exploit edge features, includ-
ing those of undirected or multi-dimensional edges. The
proposed framework can consolidate current graph neural
network models, e.g., GCN and GAT. The proposed frame-
work and new models have the following novelties: First,
we propose to use doubly stochastic normalization of graph
edge features instead of the commonly used row or symmet-
ric normalization approaches used in current graph neural
networks. Second, we construct new formulas for the op-
erations in each individual layer so that they can handle
multi-dimensional edge features. Third, for the proposed
new framework, edge features are adaptive across network
layers. Fourth, we propose to encode edge directions us-
ing multi-dimensional edge features. As a result, our pro-
posed new framework and new models are able to exploit a
rich source of graph edge information. We apply our new
models to graph node classiﬁcation on several citation net-
works, whole graph classiﬁcation, and regression on sev-
eral molecular datasets. Compared with the current state-
of-the-art methods, i.e., GCNs and GAT, our models obtain
better performance, which testify to the importance of ex-
ploiting edge features in graph neural networks.

1. Introduction

Deep neural networks have become one of the most suc-
cessful machine learning techniques in recent years.
In
many important problems, they achieve state-of-the-art per-
formance, e.g., convolutional neural networks (CNN) [19]

∗Corresponding author.

Figure 1: Schematic illustration of the proposed edge en-
hanced graph neural network (EGNN) architecture (right),
compared with the original graph neural network (GNN) ar-
chitecture (left). A GNN layer could be a GCN layer, or a
GAT layer, while an EGNN layer is an edge enhanced coun-
terpart of it. EGNN differs from GNN structurally in two
folds. Firstly, the adjacency matrix A in GNN is either a bi-
nary matrix that indicates merely the neighborhood of each
node and is used in GAT layers, or a nonnegative-valued
matrix that has one dimensional edge features and is used in
GCN layers; in contrast, EGNN uses the multi-dimensional
nonnegative-valued edge features represented as a tensor E
which may exploit multiple attributes associated with each
edge. Secondly, in GNN the same original adjacency ma-
trix A is fed to every layer; in contrast, the edge features
in EGNN are adapted at each layer before being fed to next
layer.

in image recognition, and recurrent neural networks (RNN)
[12] and Long Short Term Memory (LSTM) [14] in natu-
ral language processing. In real world, many problems can
be naturally modeled with graphs rather than conventional
tables, grid type images, or time sequences. Generally, a

9211

GNN LayerEGNN Layer𝑋𝑋𝑁𝑁×𝐹𝐹0𝐴𝐴𝑁𝑁×𝑁𝑁GNN LayerGNN Layer𝑋𝑋𝑁𝑁×𝐹𝐹𝐿𝐿𝐿𝐿⋮𝑋𝑋𝑁𝑁×𝐹𝐹11EGNN LayerEGNN Layer𝑋𝑋𝑁𝑁×𝐹𝐹0𝐸𝐸𝑁𝑁×𝑁𝑁×𝑃𝑃0𝑋𝑋𝑁𝑁×𝐹𝐹11𝐸𝐸𝑁𝑁×𝑁𝑁×𝑃𝑃1𝑋𝑋𝑁𝑁×𝐹𝐹𝐿𝐿𝐿𝐿𝐸𝐸𝑁𝑁×𝑁𝑁×𝑃𝑃𝐿𝐿⋮graph contains nodes and edges, where nodes represent en-
tities in real world, and edges represent interactions or re-
lationships between entities. For example, a social network
naturally models users as nodes and friendship relationships
as edges. For each node, there is often an associated feature
vector describing it, e.g., a user’s proﬁle in a social network.
Similarly, each edge is also often associated with features
depicting relationship strengths or other properties. Due to
their complex structures, a challenge in learning on graphs
is to ﬁnd effective ways to incorporate different sources of
information contained in graphs into computational models
such as neural networks.

techniques.

Recently, several neural network models have been de-
veloped for graph learning, which obtain better perfor-
mance than traditional
Inspired by graph
Fourier transform, Defferrard et al. [11] propose a graph
convolution operation as an analogue to standard convolu-
tions used in CNN. Just like the convolution operation in
image spatial domain is equivalent to multiplication in the
frequency domain, convolution operators deﬁned by poly-
nomials of a graph Laplacian is equivalent to ﬁltering in the
graph spectral domain. Particularly, by applying Cheby-
shev polynomials to the graph Laplacian, spatially local-
ized ﬁltering is obtained. Kipf et al. [18] approximate
the polynomials using a re-normalized ﬁrst-order adjacency
matrix to obtain comparable results on graph node classiﬁ-
cation tasks. Those graph convolutional networks (GCNs)
[11][18] combine graph node features and graph topolog-
ical structural information to make predictions. Velick-
ovic et al. [27] adopt attention mechanism into graph learn-
ing, and propose a graph attention network (GAT). Unlike
GCNs, which use a ﬁxed or learnable polynomial of Lapla-
cian or adjacency matrix to aggregate (ﬁlter) node infor-
mation, GAT aggregates node information by using an at-
tention mechanism on graph neighborhoods. The essential
difference between GAT and GCNs is stark: In GCNs the
weights for aggregating (ﬁltering) neighbor nodes are de-
ﬁned by the graph topological structure, which is indepen-
dent of node contents; in contrast, weights in GAT are a
function of node contents due to the attention mechanism.
Empirical results on graph node classiﬁcation show that the
adaptiveness of GAT makes it more effective to fuse infor-
mation from node features and graph topological structures.

One major problem in the current GNN models, such as
GAT and GCNs, is that edge features are not fully incor-
porated. In GAT, graph topological information is injected
into the model by forcing the attention coefﬁcient between
two nodes to zero if they are not connected. Therefore, the
edge information used in GAT is only the indication about
whether there is an edge or not, i.e., connectivities. How-
ever, graph edges are often in possession of rich information
like strengths, types, etc. Instead of being a binary indicator
variable, edge features could be continuous, e.g., strengths,

or multi-dimensional. GCNs can utilize one-dimensional
real-valued edge features, e.g., edge weights, but the edge
features are restricted to be one-dimensional. Properly ad-
dressing this problem is likely to beneﬁt many graph learn-
ing problems. Another problem of GAT and GCNs is that
each GAT or GCN layer ﬁlters node features based on the
original adjacency matrix that is given as an input. The orig-
inal adjacency matrix is likely to be noisy and not optimal,
which will limit the effectiveness of the ﬁltering operation.
In this paper, we address the above problems by propos-
ing new GNN models to more adequately exploit edge in-
formation, which naturally enhance current GCNs and GAT
models. Our models construct different formulas from those
of GCNs and GAT, so that they are capable of exploiting
multi-dimensional edge features. Also our new models can
exploit one-dimensional edge features more effectively by
making them adaptive across network layers. Moreover, our
models leverage doubly stochastic normalization to aug-
ment the GCNs and GAT models that use ordinary row or
symmetric edge normalization. Doubly stochastic matrices
have nice properties that can facilitate the use of edges.

We conduct experiments on several citation network
datasets and molecular datasets. For citation networks,
we encode directed edges as three dimensional edge fea-
ture vectors. For molecular datasets, different atom bond
types are naturally encoded as multi-dimensional edge at-
tributes. By leveraging those multi-dimensional edge fea-
tures our methods outperform current state-of-the-art ap-
proaches. The results conﬁrm that edge features are im-
portant for graph learning, and our proposed EGNN models
are effective incorporating edge features.

As a summary, the novelties of our proposed EGNN

model include the following:

• A new framework for adequately exploiting multi-
dimensional edge features. Our new framework is able
to incorporate multi-dimensional positive-valued edge
features. It eliminates the limitation of GAT which can
handle only binary edge indicators and the limitation
of GCNs which can handle only one dimensional edge
features.

• Doubly stochastic edge normalization. We propose to
normalize edge feature matrices into doubly stochastic
matrices which show improved performance in denois-
ing [29].

• Attention based edge adaptiveness across neural net-
work layers. We design a new graph network archi-
tecture which can not only ﬁlter node features but also
adapt edge features across layers. Leveraging this new
architecture, in our models the edge features are adap-
tive to both local contents and the global layers when
passing through the layers of the neural network.

9212

• Multi-dimensional edge features for directed edges.
We propose a method to encode edge directions
as multi-dimensional edge features. Therefore, our
EGNN can effectively learn on directed graphs.

The rest of this paper is organized as follows: Section 2
brieﬂy reviews the related works. Details of the proposed
EGNN architecture and two types of proposed EGNN lay-
ers are described in Section 3. Section 4 presents the exper-
imental results, and Section 5 concludes the paper.

2. Related works

A critical challenge in graph learning is the complex
non-Euclidean structure of graph data. To address this
challenge, traditional machine learning approaches extract
graph statistics (e.g., degrees) [5], kernel functions [28][24]
or other hand-crafted features which measure local neigh-
borhood structures. Those methods lack ﬂexibility in that
designing sensible hand-crafted features is time consuming
and extensive experiments are needed to generalize to dif-
ferent tasks or settings. Instead of extracting structural in-
formation or using hand-engineered statistics as features of
the graph, graph representation learning attempts to embed
graphs or graph nodes in a low-dimensional vector space
using a data-driven approach. One kind of embedding ap-
proaches are based on matrix-factorization, e.g., Laplacian
Eigenmap (LE) [4], Graph Factorization (GF) algorithm [2],
GraRep [7], and HOPE [21]. Another class of approaches
focus on employing a ﬂexible, stochastic measure of node
similarity based on random walks, e.g., DeepWalk [22],
node2vec [2], LINE [26], and HARP [9]. There are several
limitations in matrix factorization-based and random walk-
based graph learning approaches. First, the embedding
function which maps to a low-dimensional vector space is
linear or overly simple so that complex patterns cannot be
captured; Second, they typically do not incorporate node
features; Finally, they are inherently transductive, for the
whole graph structure is required in the training phase.

Recently these limitations in graph learning have been
addressed by adopting new advances in deep learning. Deep
learning with neural networks can represent complex map-
ping functions and be efﬁciently optimized by gradient-
descent methods. To embed graph nodes to a Euclidean
space, deep autoencoders are adopted to extract connectiv-
ity patterns from the node similarity matrix or adjacency
matrix, e.g., Deep Neural Graph Representations (DNGR)
[8] and Structural Deep Network Embeddings (SDNE) [30].
Although autoencoder-based approaches are able to capture
more complex patterns than matrix factorization based and
random walk based methods, they are still unable to lever-
age node features.

With celebrated successes of CNN in image recognition,
recently, there has been an increased interest in adapting

convolutions to graph learning. In [6], the convolution op-
eration is deﬁned in the Fourier domain, that is, the spec-
tral space, of the graph Laplacian. The method is afﬂicted
by two major problems: Firstly, the eigen decomposition is
computationally intensive; secondly, ﬁltering in the Fourier
domain may result in non-spatially localized effects.
In
[13], a parameterization of the Fourier ﬁlter with smooth
coefﬁcients is introduced to make the ﬁlter spatially local-
ized.
[11] proposes to approximate the ﬁlters by using a
Chebyshev expansion of the graph Laplacian, which pro-
duces spatially localized ﬁlters, and also avoids computing
the eigenvectors of the Laplacian.

Attention mechanisms have been widely employed in
many sequence-based tasks [3][33][16]. Compared with
convolution operators, attention mechanisms enjoy two
beneﬁts: Firstly, they are able to aggregate any variable
sized neighborhood or sequence; further, the weights for ag-
gregation are functions of the contents of a neighborhood or
sequence. Therefore, they are adaptive to the contents. [27]
adapts an attention mechanism to graph learning and pro-
poses a graph attention network (GAT), achieving current
state-of-the-art performance on several graph node classiﬁ-
cation problems.

3. Edge feature enhanced graph neural net-

works

3.1. Architecture overview

Given a graph with N nodes, let X be an N × F matrix
representation of the node features of the whole graph. We
denote an element of a matrix or a tensor by indices in the
subscript. Speciﬁcally, the “·” notation in the subscript is
used to select the whole range (slice) of a dimension. There-
fore, Xij will represent the value of the jth feature of the
ith node. Xi· ∈ RF , i = 1, 2, . . . , N , represents the F -
dimensional feature vector of the ith node. Similarly, let E
be an N ×N ×P tensor representing the edge features of the
graph. Then Eij · ∈ RP , i = 1, 2, . . . , N, j = 1, 2, . . . , N ,
represents the P -dimensional feature vector of the edge
connecting the ith and jth nodes, and Eijp denotes the pth
channel of the edge feature in Eij ·. We use the notation
Eij · = 0 to mean that there is no edge between the ith and
jth nodes. Let Ni, i = 1, 2, . . . , N , denote the index set of
neighboring nodes of node i.

Our proposed network has a multi-layer feed-forward ar-
chitecture. We use superscript l to denote the output of the
lth layer. The inputs to the network are denoted by X 0 and
E 0. After passing through the ﬁrst EGNN layer, X 0 is ﬁl-
tered to produce an N × F 1 new node feature matrix X 1.
In the mean time, edge features are adapted to E 1 that pre-
serves the dimensionality of E 0. The adapted E 1 is fed to
the next layer as edge features. This procedure is repeated
for every subsequent layer. Within each hidden layer, non-

9213

linear activations can be applied to the ﬁltered node features
X l. The node features X L can be considered as an embed-
ding of the graph nodes in an F L-dimensional space. For
a node classiﬁcation problem, a soft-max operator will be
applied to each node embedding vector X L
i· along the last
dimension. For a whole-graph prediction (classiﬁcation or
regression) problem, a pooling layer is applied to the ﬁrst
dimension of X L so that the feature matrix is reduced to a
single vector embedding for the whole graph. Then a fully
connected layer is applied to the vector, whose output could
be used as predictions for regression, or logits for classiﬁca-
tion. The weights of the network will be trained with super-
vision from ground truth labels. Figure 1 gives a schematic
illustration of the EGNN architecture with a comparison to
the existing GNN architectures. Note that the input edge
features in E 0 are already pre-normalized. The normaliza-
tion method will be described in the next subsection. Two
types of EGNN layers, attention based EGNN (EGNN(A))
layer and convolution based EGNN (EGNN(C)) layer will
also be presented in the following subsections.

3.2. Doubly stochastic normalization of edges

In graph convolution operations, the edge feature ma-
trices will be used as ﬁlters to multiply the node feature
matrix. To avoid increasing the scale of output features by
multiplication, the edge features need to be normalized. Let
ˆE be the raw edge features, our normalized features E is
produced as follows:

˜Eijp =

Eijp =

N

PN
Xk=1

ˆEijp

ˆEikp

k=1

˜Eikp ˜Ejkp
˜Evkp

v=1

PN

(1)

(2)

Note that all elements in ˆE are nonnegative. It can be easily
veriﬁed that such kind of normalized edge feature tensor E
satisﬁes the following properties:

Eijp ≥ 0,

N

Xi=1

Eijp =

N

Xj=1

Eijp = 1.

(3)

(4)

In other words, the edge feature matrices E··p for p =
1, 2, · · · , P are square nonnegative real matrices with rows
and columns summing to 1. Thus, they are doubly stochas-
tic matrices, i.e., they are both left stochastic and right
stochastic. Doubly stochastic matrices (DSMs) have sev-
eral nice properties, e.g., they are symmetric, positive semi-
deﬁnite and having the largest eigen-value 1. The graph
convolution has an effect similar to passing information
through edges in a diffusion process by iteratively multiply-
ing the previous result with the edge matrix. Since taking

the power of a DSM preserves the three mentioned proper-
ties, it prevents the edge matrix from exploding or shrinking
to zero during diffusion, thus can help stabilize the process,
compared with the previously used row normalization as in
GAT [27]:

(5)

(6)

or symmetric normalization as in GCN [18]:

Eijp =

ˆEijp

ˆEijp

j=1

PN

ˆEijp

Eijp =

qPN

i=1

ˆEijpqPN

j=1

ˆEijp

Further, the powering (or diffusion) has an effect of increas-
ing the gaps between large egien-values, which denoises the
edges. The effectiveness of doubly stochastic matrix has
been recently demonstrated for graph edges denoising [29].

3.3. EGNN(A): Attention based EGNN layer

We describe the attention based EGNN layer. The origi-
nal GAT model [27] is only able to handle one dimensional
binary edge features, i.e., the attention mechanism is de-
ﬁned on the node features of the neighborhood, which does
not take the real valued edge features, e.g., weights, into
account. To exploit multi-dimensional nonnegative-valued
edge features, we propose a new attention mechanism. In
our new mechanism, feature vector X l
i· will be aggregated
from the feature vectors of the neighboring nodes of the
ith node, i.e., {Xj, j ∈ Ni}, by simultaneously incorpo-
rating the corresponding edge features. Utilizing the ten-
sor and the notation that zero valued edge features mean no
edge connections, the aggregation operation is deﬁned as
follows:

X l = σ


P

p=1(cid:16)αl
n

··p(X l−1, El−1

.

(7)

··p )gl(X l−1)(cid:17)


Here, f is the concatenation operator; σ is a non-linear ac-
tivation; α is a function which produces an N × N × P
tensor; g is a transformation which maps the node features
from the input space to the output space, and usually a linear
mapping is used:

gl(X l−1) = X l−1W l,

(8)

where W l is an F l−1 × F l weight matrix.

i·

, X l−1

ijp is a function of X l−1

In Eq. (7), αl contains the attention coefﬁcients, whose
speciﬁc entry αl
and Eijp.
In existing attention mechanisms [27], the attention coefﬁ-
cient depends on Xi· and Xj · only. Our mechanism allows
the attention operation to be guided by edge features. For
multiple dimensional edge features, we consider them as
multi-channel signals, and each channel will guide a sepa-
rate attention operation. The results from different channels

j ·

9214

are combined by the concatenation operation. For a speciﬁc
channel of edge features, our attention function is chosen to
be the following:

ijp = f l(X l−1
ˆαl
i·
αl
··p = DS(ˆαl
··p),

, X l−1

j ·

)El−1
ijp ,

(9)

(10)

where DS is the doubly stochastic normalization operator
deﬁned in Eqs. (1) and (2). In principle, f l can be any ordi-
nary attention function which produces a scalar value from
two input vectors. In this paper, we use a linear function as
the attention function for simplicity:

f l(X l−1

i·

, X l−1

j ·

) = exp(cid:26)L(cid:16)aT [X l−1

i· W lkX l−1

j · W l](cid:17)(cid:27) ,

(11)
where L is the LeakyReLU activation function; W l is the
same mapping as in (8).

The attention coefﬁcients will be used as new edge fea-

tures for the next layer:

are simply treated as undirected graphs. In this paper, we
show in the experiment part that discarding edge directions
will lose important information. By viewing directions of
edges as a kind of edge features, we encode a directed edge
channel Eijp to be

h ˆEijp

ˆEjip

ˆEijp + ˆEjipi .

Therefore, each directed channel is augmented to three
channels. Note that the three channels deﬁne three types
of neighborhoods: forward, backward, and undirected. As
a result, EGNN will aggregate node information from these
three different types of neighborhoods, which contains the
direction information. Taking the citation network for in-
stance, EGNN will apply the attention mechanism or con-
volution operation on the papers that a speciﬁc paper cited,
the papers cited this paper, and the union of the former two.
With such edge features, discriminative patterns in various
types of neighborhoods can be effectively captured.

El = αl.

(12)

4. Experimental results

By doing so, EGNN adapts the edge features across the net-
work layers, which helps capture essential edge features as
determined by our novel attention mechanism.

For all the experiments, we implement the algorithms in
Python on the TensorFlow platform [1]. In all the experi-
ments, models are trained with a Nvidia Tesla K40 graphics
card with 12 Gigabyte graphics memory.

3.4. EGNN(C): Convolution based EGNN layer

4.1. Citation networks

By regarding the graph convolution operation as a spe-
cial case of the graph attention operation, we derive our
EGNN(C) layer from the formula of EGNN(A) layer. In-
deed, the essential difference between GCN [18] and GAT
[27] is whether we use the attention coefﬁcients (i.e., ma-
trix α) or the adjacency matrix to aggregate node features.
With this view, we derive EGNN(C) by replacing the atten-
tion coefﬁcient matrices α··p with the corresponding edge
feature matrices E··p. The resulting formula for EGNN(C)
is given as follows:

X l = σ


P

n

p=1(cid:16)E··pX l−1W l(cid:17)


,

(13)

To benchmark the effectiveness of our proposed mod-
els, we apply them to the network node classiﬁcation prob-
lem. Three datasets are tested: Cora [23], Citeseer [23],
and Pubmed [20]. Some basic statistics about these datasets
are listed in Table 1. All three datasets are directed graphs,

Table 1: Summary of citation network datasets

Cora Citeseer

Pubmed

# Nodes
# Edges
# Node Features
# Classes

2708
5429
1433

7

3327
4732
3703

6

19717
44338

500

3

where the notations are the same as in Section 3.3.

3.5. Edge features for directed graph

In real world, many graphs are directed. Often times,
edge direction contains important information about the
graph. For example, in a citation network, machine learning
papers sometimes cite mathematics papers or other theoret-
ical papers. However, mathematics papers may seldom cite
machine learning papers. In many previous studies includ-
ing GCNs and GAT, edge directions are not considered. In
their experiments, directed graphs such as citation networks

where edge directions represent the directions of citations.
For Cora and Citeseer, node features contain binary indi-
cators representing the occurrences of predeﬁned keywords
in a paper. For Pubmed, term frequency-inverse document
frequency (TF-IDF) features are employed to describe the
network nodes (i.e., papers).

The three citation network datasets are also used in [32]
[18] [27]. However, they all use a pre-processed version
which discards the edge directions. Since our EGNN mod-
els require the edge directions to construct edge features, we
use the original version from [23] and [20]. For each of the

9215

Table 2: Classiﬁcation accuracies on citation networks. Doubly stochastic normalization, multi-dimensional edge features,
edge adaptiveness and weighted loss components are denoted by “D”, “M”, “A” and “W”, respectively, in square brackets.

Dataset
Splitting

GCN
GAT

Cora

CiteSeer

Pubmed

Sparse

Dense

Sparse

Dense

Sparse

Dense

72.9 ± 0.8% 72.0 ± 1.2% 69.2 ± 0.7% 75.3 ± 0.4% 83.3 ± 0.4% 83.4 ± 0.2%
75.5 ± 1.1% 79.0 ± 1.0% 69.5 ± 0.5% 74.9 ± 0.5% 83.4 ± 0.1% 83.4 ± 0.2%

EGNN(C)[W]
EGNN(A)[W]

82.7 ± 0.6% 87.6 ± 0.6% 69.3 ± 0.6% 76.0 ± 0.5% 84.5 ± 0.2% 84.3 ± 0.4%
82.7 ± 0.6% 86.6 ± 0.6% 69.4 ± 0.5% 74.9 ± 0.8% 83.1 ± 0.2% 82.7 ± 0.2%

EGNN(C)[D]
EGNN(C)[DW]
EGNN(C)[M]
EGNN(C)[MW]
EGNN(C)[DM]
EGNN(C)[DMW]
EGNN(A)[A]
EGNN(A)[AW]
EGNN(A)[D]
EGNN(A)[DW]
EGNN(A)[M]
EGNN(A)[MW]
EGNN(A)[ADM]
EGNN(A)[ADMW]

81.8 ± 0.5% 85.1 ± 0.5% 70.6 ± 0.3% 75.0 ± 0.3% 84.3 ± 0.1% 84.1 ± 0.1%
83.2 ± 0.3% 87.4 ± 0.4% 70.3 ± 0.3% 75.4 ± 0.5% 84.1 ± 0.1% 84.1 ± 0.1%
80.2 ± 0.4% 86.1 ± 0.5% 69.4 ± 0.3% 76.8 ± 0.4% 86.2 ± 0.2% 86.7 ± 0.1%
82.3 ± 0.4% 87.2 ± 0.4% 69.4 ± 0.3% 77.1 ± 0.4% 86.2 ± 0.1% 86.4 ± 0.3%
83.0 ± 0.3% 88.8 ± 0.3% 69.5 ± 0.3% 76.7 ± 0.4% 86.0 ± 0.1% 86.0 ± 0.1%
83.4 ± 0.3% 88.5 ± 0.4% 69.5 ± 0.3% 76.6 ± 0.4% 85.8 ± 0.1% 85.6 ± 0.2%
76.0 ± 1.0% 79.1 ± 1.0% 69.5 ± 0.4% 74.6 ± 0.3% 83.4 ± 0.1% 83.6 ± 0.2%
82.6 ± 0.6% 86.3 ± 0.9% 69.4 ± 0.4% 74.9 ± 0.4% 83.7 ± 0.2% 82.8 ± 0.3%
80.1 ± 1.0% 85.4 ± 0.5% 70.1 ± 0.4% 74.7 ± 0.4% 84.3 ± 0.2% 84.2 ± 0.1%
82.7 ± 0.4% 87.2 ± 0.5% 69.5 ± 0.3% 74.5 ± 0.5% 83.9 ± 0.2% 83.3 ± 0.2%
81.7 ± 0.4% 87.9 ± 0.4% 69.4 ± 0.3% 75.7 ± 0.3% 85.5 ± 0.1% 86.0 ± 0.1%
82.8 ± 0.3% 87.0 ± 0.6% 69.1 ± 0.3% 76.3 ± 0.5% 85.2 ± 0.2% 85.3 ± 0.3%
82.5 ± 0.3% 88.4 ± 0.3% 69.4 ± 0.4% 76.5 ± 0.3% 85.7 ± 0.1% 86.7 ± 0.1%
83.1 ± 0.4% 88.4 ± 0.3% 69.3 ± 0.3% 76.3 ± 0.5% 85.6 ± 0.2% 85.7 ± 0.2%

three datasets, we split nodes into 3 subsets for training, val-
idation and testing. Two splittings are tested. One splitting
has 5%, 15% and 80% sized subsets for training, validation
and test, respectively. Since it has a small training set, we
call it “sparse” splitting. Another splitting has 60%, 20%
and 20% sized subsets, which is called “dense” splitting.

Following the experiment settings of [18][27], we use
two layers of EGNN in all of our experiments for fair com-
parison. Throughout the experiments in this subsection, we
use the Adam optimizer [17] with learning rate 0.005. An
early stopping strategy with a window size of 100 is adopted
for the citation networks; i.e., we stop training if the vali-
dation loss does not decrease for 100 consecutive epochs.
We set the output dimension of W to 64 for hidden layers.
We apply dropout [25] with dropout rate 0.6 to both input
features and normalized attention coefﬁcients. L2 regular-
ization with weight decay 0.0005 is applied to the weights
W and a. Moreover, exponential linear unit (ELU) [10] is
employed as nonlinear activations for hidden layers.

We notice that the class distributions of the training sub-
sets of the three datasets are not balanced. To test the effects
of dataset imbalance, we train each algorithm with two dif-
ferent loss functions, i.e., unweighted and weighted losses.
The weight of a node belonging to class k is calculated as

where K and nk are the numbers of classes and nodes be-
longing to the kth class in the training subset, respectively.
Thus, nodes in a minority class are given larger weights and
are penalized more in the loss than a majority class.

The baseline methods we used are GCN [18] and GAT
[27]. To investigate the effectivenesses of each components,
we perform ablation study of EGNN(A) and EGNN(C).
We denote a speciﬁc version of EGNN(A) or EGNN(C)
by EGNN(A)[·] or EGNN(C)[·], where the letters in the
square bracket represent different combinations of compo-
nents. We denote doubly stochastic normalization, multi-
dimensional edge features, edge adaptiveness, and weighted
loss by “D”, “M”, “A” and “W”, respectively. For example,
EGNN(C)[M] means the EGNN(C) model with the compo-
nent of the multi-dimensional edge features only. Totally,
18 models are tested, including the two baselines. We run
each model 20 times, and record the mean and standard de-
viation of the classiﬁcation accuracies, which are listed in
Table 2. We can observe several interesting phenomena,
some of which warrant further investigations:

• Overall, almost all EGNN variants outperform their
corresponding baselines, which indicates that all the
three components are able to incorporate useful in-
formation for classiﬁcation.
Particularly, multi-
dimensional edge features and doubly stochastic nor-
malization improve more than edge adaptiveness.

PK

k=1 nk
Knk

,

(14)

9216

(a) Cora

(b) Citeseer

(c) Pubmed

Figure 2: Node class distribution of the training subsets of the three citation networks. The Cora dataset is more imbalanced
than the other two.

• The two baselines underperform on both the sparse and
dense splittings of the Cora dataset. This is caused
by the class imbalance of the Cora dataset. We il-
lustrate the class distributions of the three datasets in
Figure 2, from which we can see that Cora is more
imbalanced than Citeseer and Pubmed. On the Cora
dataset, EGNN(A)[W] and EGNN(C)[W], which add
the weighted loss component only, perform much bet-
ter than GAT and GCN. This veriﬁes that the un-
derperformance of GAT and GCN are caused by the
class imbalance. Our proposed models are highly re-
silient to class imbalance. Without weighted training,
our framework obtains high accuracies on the Cora
dataset. Weighted training does not always improve
performance, especially on less imbalanced datasets,
e.g., Pubmed. This indicates that simply weighting the
classes is not sufﬁcient to fully solve the class imbal-
ance problem. More sophisticated methods need to be
designed to address this problem in the future.

• Performances on dense splittings are consistently
higher than on sparse splitting.
It is not unexpected
because more training data gives an algorithm more
information to tune parameters.

• We noticed that the Pubmed graph is much bigger than
Cora and Citeseer. Also, its node features (TF-IDF)
are more sophisticated than the bag-of-words features
of the other two datasets. Finally, Pubmed is the least
imbalanced. These characteristics may be the reason
that the sparse and dense splittings of Pubmed get close
accuracies as the sparse splitting already provides suf-
ﬁcient information for training.

• Another characteristic we noticed is that the average
degree of Citeseer is the smallest among the citation
networks. Therefore, a very limited number of edges
can be utilized in a sparse splitting. This restricts
the ability of our models to incorporate edge features;
thus, we do not observe much performance gain on the
sparse splitting of Citeseer.

• Either EGNN(C)[DW] or EGNN(C)[MW] is not as

good as EGNN(C)[W] on the dense splitting of the
Cora dataset. However, EGNN(C)[DMW] is better
than EGNN(C)[W]. This interesting phenomenon in-
dicates that doubly stochastic normalization and multi-
dimensional edge feature might not work well individ-
ually on some datasets, but can improve performance
considerably if combined.

To compare the computational efﬁciency of the models,
we record the average training time on the Cora dataset
with dense splitting in Table 3. Note that EGNN(C) and
EGNN(A) are full models with all components imple-
mented. According to Table 3, the proposed EGNN(C) and
EGNN(A) are a little higher but still essentially comparable
to GCN and GAT in terms of time complexity.

Table 3: Average training time (in milliseconds) per epoch
on the Cora dataset with dense splitting.

Model
Time (ms)

GCN GAT EGNN(C) EGNN(A)

22

49

48

159

4.2. Molecular analysis

One promising application of graph learning is molec-
ular analysis. A molecule can be represented as a graph,
where each atom is a node and chemical bonds are edges.
Unlike citation network analysis in Section 4.1, the prob-
lem here is whole-graph prediction, either classiﬁcation or
regression. For example, given a graph representation of a
molecule, the goal may be to classify it as toxic or not, or to
predict the solubility (regression). In other words, we need
to predict one value for the whole graph, rather than one
value for a graph node. Usually, for each chemical bond,
there are several attributes associated with it, e.g., Atom Pair
Type, Bond Order, and Ring Status. Therefore, the graphs
intrinsically contain multi-dimensional edge features.

Three datasets, Tox21, Lipophilicity and Freesolv, are
used to test our algorithms. Tox21 contains 7831 environ-
mental compounds and drugs. Each compound is associ-
ated with 12 labels, e.g., androgen receptor, estrogen recep-
tor, and mitochondrial membrane potential, which deﬁnes

9217

0%5%10%15%20%25%30%35%NNRuleRLProbTheoryGeneAlgoCaseBasedPercentage0%5%10%15%20%25%AgentsIRDBAIHCIMLPercentage0%5%10%15%20%25%30%35%40%45%123PercentageTable 4: Performance on molecular datasets

Dataset

Validation

Test

Validation

Test

Tox21 (AUC)

Lipo (RMSE)

Freesolv (RMSE)
Test

Validation

RF
Weave

0.78 ± 0.01
0.79 ± 0.02

0.75 ± 0.03
0.80 ± 0.02

0.87 ± 0.02
0.88 ± 0.06

0.86 ± 0.04
0.89 ± 0.04

1.98 ± 0.07
1.35 ± 0.22

1.62 ± 0.14
1.37 ± 0.14

EGNN(C)
EGNN(A)

0.82 ± 0.01
0.82 ± 0.01

0.82 ± 0.01
0.81 ± 0.01

0.80 ± 0.02
0.79 ± 0.02

0.75 ± 0.01
0.75 ± 0.01

1.07 ± 0.08
1.09 ± 0.12

1.09 ± 0.08
1.01 ± 0.12

a multi-label classiﬁcation problem. Lipophilicity contains
4200 compounds. The goal is to predict compound solubil-
ity, which is a regression task. Freesolv includes a set of
642 neutral molecules, which similarly deﬁnes a regression
task. For all the three datasets, compounds are converted
to graphs. For all the three datasets, nodes are described
by 25-dimensional feature vectors. The dimensionality of
edge feature vectors are 42, 21, and 25 for Tox21, Lipo, and
Freesolv, respectively.

For both EGNN(A) and EGNN(C), we implement a net-
work consisting of 2 graph processing layers, a global max-
pooling layer, and a fully connected layer. For each graph
processing layer, the output dimensions of the linear map-
ping g are ﬁxed to be 16. For Tox21, the sigmoid cross
entropy loss is applied to the output logits of the fully con-
nected layer. For Lipo and Freesolv, the mean squared error
loss is employed. The networks are trained by Adam op-
timizer [17] with learning rate 0.0005. An early stopping
strategy with a window size of 200 is adopted. L2 regular-
ization with weight decay 0.0001 is applied to parameters
of the models except for bias parameters. Moreover, ex-
ponential linear unit (ELU) [10] is employed as nonlinear
activations for hidden layers.

Our models are compared with two baseline models
which are shown in MoleculeNet [31]: Random Forest
and Weave. Random Forest is a traditional learning model
which is widely applied to various problems. Weave model
[15] is similar to graph convolution but speciﬁcally de-
signed for molecular analysis.

All the three datasets are split into training, validation
and test subsets with a ratio of 8:1:1. We run our models 5
times, and record the means and standard deviations of per-
formance scores. For classiﬁcation task (i.e., Tox21), Area
Under Curve (AUC) scores of the receiver operating charac-
teristic (ROC) curve are recorded. Since it is a multi-label
classiﬁcation problem, we record the AUCs of each class
and take the averaged value as the ﬁnal score. For regression
(i.e., Lipo and Freesolv), root mean square error (RMSE) is
used as the evaluation metric. The scores are given in Table
4. The results show that EGNN(C) and EGNN(A) outper-
form the two baselines with considerable margins. On the
Tox21 dataset, the AUC scores are improved by more than

0.2 compared with the Weave model. For the two regres-
sion tasks, RMSEs are improved by about 0.1 and 0.3 on
the Lipo and Freesolv datasets, respectively. The scores of
EGNN(C) and EGNN(A) are close on the three datasets.

5. Conclusions

In this paper, we propose a new framework to ad-
dress the existing problems in the current state-of-the-art
graph neural network models. Speciﬁcally, we propose a
new attention mechanism to incorporate multi-dimensional
nonnegative-valued edge features. Then, we propose a new
graph neural network architecture that adapts edge features
across neural network layers. Our framework admits a
formula that allows for extending convolutions to multi-
dimensional edge features. Further, we propose to use dou-
bly stochastic normalization, as opposed to the ordinary row
normalization or symmetric normalization used in the ex-
isting graph neural network models. Finally, we propose
a method to design multi-dimensional edge features for di-
rected edges to effectively handle directed graphs. Exten-
sive experiments are conducted on three citation network
datasets for graph node classiﬁcation evaluation, and on
three molecular datasets to test the performance on whole
graph classiﬁcation and regression tasks. Experimental re-
sults show that our new framework outperforms current
state-of-the-art models such as GCN and GAT signiﬁcantly
and consistently on all the datasets. Detailed ablation study
also shows the effectiveness of each individual component
in our framework.

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur,
J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,
P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and
X. Zheng. TensorFlow: A System for Large-scale Machine
Learning. In Proceedings of the 12th USENIX Conference on
Operating Systems Design and Implementation, OSDI’16,
pages 265–283, Berkeley, CA, USA, 2016. USENIX Asso-
ciation.

[2] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josi-
fovski, and A. J. Smola. Distributed Large-scale Natural

9218

In Proceedings of the 22Nd Interna-
Graph Factorization.
tional Conference on World Wide Web, WWW ’13, pages
37–48, New York, NY, USA, 2013. ACM.

[19] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, Nov. 1998.

[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine
Translation by Jointly Learning to Align and Translate.
arXiv:1409.0473 [cs, stat], Sept. 2014.

[4] M. Belkin and P. Niyogi. Laplacian Eigenmaps and Spectral
Techniques for Embedding and Clustering. In Advances in
Neural Information Processing Systems, page 7, 2001.

[5] S. Bhagat, G. Cormode, and S. Muthukrishnan. Node Clas-
siﬁcation in Social Networks. arXiv:1101.3291 [physics],
pages 115–148, 2011.

[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spec-
tral Networks and Locally Connected Networks on Graphs.
arXiv:1312.6203 [cs], Dec. 2013.

[7] S. Cao, W. Lu, and Q. Xu. GraRep: Learning Graph Repre-
sentations with Global Structural Information. In Proceed-
ings of the 24th ACM International on Conference on In-
formation and Knowledge Management, CIKM ’15, pages
891–900, New York, NY, USA, 2015. ACM.

[8] S. Cao, W. Lu, and Q. Xu. Deep Neural Networks for Learn-
ing Graph Representations. In AAAI Conference on Artiﬁcial
Intelligence, 2016.

[9] H. Chen, B. Perozzi, Y. Hu, and S. Skiena. HARP: Hierar-
chical Representation Learning for Networks. In AAAI Con-
ference on Artiﬁcial Intelligence, 2018.

[10] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
Accurate Deep Network Learning by Exponential Linear
Units (ELUs). In International Conference on Learning Rep-
resentations, 2016.

[11] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-
tional Neural Networks on Graphs with Fast Localized Spec-
tral Filtering. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems, pages 3844–3852. Curran Asso-
ciates, Inc., 2016.

[12] J. L. Elman. Finding Structure in Time. Cognitive Science,

14(2):179–211, Mar. 1990.

[13] M. Henaff, J. Bruna, and Y. LeCun. Deep Convolutional Net-
works on Graph-Structured Data. arXiv:1506.05163 [cs],
June 2015.

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural Computation, 9(8):1735, Nov. 1997.

[15] S. Kearnes, K. Mccloskey, M. Berndl, V. Pande, and P. Riley.
Molecular graph convolutions: Moving beyond ﬁngerprints.
Journal of Computer - Aided Molecular Design; Dordrecht,
30(8):595–608, Aug. 2016.

[16] S. Kim, J.-H. Hong, I. Kang, and N. Kwak. Semantic Sen-
tence Matching with Densely-connected Recurrent and Co-
attentive Information. arXiv:1805.11360 [cs], May 2018.

[17] D. P. Kingma and J. Ba. Adam: A Method for Stochas-
tic Optimization. In International Conference on Learning
Representations, 2015.

[18] T. N. Kipf and M. Welling. Semi-Supervised Classiﬁcation
with Graph Convolutional Networks. In International Con-
ference on Learning Representations, 2017.

[20] G. Namata, B. London, L. Getoor, and B. Huang. Query-
In

driven Active Surveying for Collective Classiﬁcation.
Workshop on Mining and Learning with Graphs, 2012.

[21] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu. Asymmet-
ric Transitivity Preserving Graph Embedding. In Proceed-
ings of the 22Nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’16, pages
1105–1114, New York, NY, USA, 2016. ACM.

[22] B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online
Learning of Social Representations.
In Proceedings of the
20th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, pages 701–710, New
York, NY, USA, 2014. ACM.

[23] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and
T. Eliassi-Rad. Collective Classiﬁcation in Network Data. AI
Magazine; La Canada, 29(3):93–106, 2008.

[24] N. Shervashidze, P. Schweitzer, E. J. van Leeuwen,
K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-Lehman
Graph Kernels.
Journal of Machine Learning Research,
12(Sep):2539–2561, 2011.

[25] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A Simple Way to Prevent
Neural Networks from Overﬁtting. J. Mach. Learn. Res.,
15(1):1929–1958, Jan. 2014.

[26] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei.
LINE: Large-scale Information Network Embedding.
In
Proceedings of the 24th International Conference on World
Wide Web, WWW ’15, pages 1067–1077, 2015.

[27] P. Velickovic, G. Cucurull, A. Casanova, and A. Romero.
Graph Attention Networks. In International Conference on
Learning Representations, 2018.

[28] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and
K. M. Borgwardt. Graph Kernels. Journal of Machine Learn-
ing Research, 11(Apr):1201–1242, 2010.

[29] B. Wang, A. Pourshafeie, M. Zitnik, J. Zhu, C. D. Busta-
mante, S. Batzoglou, and J. Leskovec. Network Enhance-
ment: A general method to denoise weighted biological net-
works. arXiv:1805.03327 [cs, q-bio], May 2018.

[30] D. Wang, P. Cui, and W. Zhu. Structural Deep Network
Embedding. In Proceedings of the 22nd ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data
Mining, KDD ’16, pages 1225–1234, New York, NY, USA,
2016. ACM.

[31] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Ge-
niesse, A. S. Pappu, K. Leswing, and V. Pande. MoleculeNet:
A benchmark for molecular machine learning. Chemical Sci-
ence, 9(2):513–530, 2018.

[32] Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting Semi-
Supervised Learning with Graph Embeddings.
In Interna-
tional Conference on Machine Learning, pages 40–48, June
2016.

[33] G. Zhou, C. Song, X. Zhu, X. Ma, Y. Yan, X. Dai, H. Zhu,
J. Jin, H. Li, and K. Gai. Deep Interest Network for Click-
Through Rate Prediction. arXiv:1706.06978 [cs, stat], June
2017.

9219

