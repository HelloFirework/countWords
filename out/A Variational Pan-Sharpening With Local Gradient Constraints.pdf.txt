A Variational Pan-Sharpening with Local Gradient Constraints

Xueyang Fu12, Zihuang Lin1, Yue Huang1, Xinghao Ding1∗

1Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China

2School of Information Science and Technology, University of Science and Technology of China, China

∗Corresponding author: dxh@xmu.edu.cn

Abstract

Pan-sharpening aims at fusing spectral and spatial in-
formation, which are respectively contained in the multi-
spectral (MS) image and panchromatic (PAN) image, to
produce a high resolution multi-spectral (HRMS) image. In
this paper, a new variational model based on a local gra-
dient constraint for pan-sharpening is proposed. Differ-
ent with previous methods that only use global constraints
to preserve spatial information, we ﬁrst consider gradien-
t difference of PAN and HRMS images in different local
patches and bands. Then a more accurate spatial preser-
vation based on local gradient constraints is incorporated
into the objective to fully utilize spatial information con-
tained in the PAN image. The objective is formulated as
a convex optimization problem which minimizes two least-
squares terms and thus very simple and easy to implement.
A fast algorithm is also designed to improve efﬁciency. Ex-
periments show that our method outperforms previous vari-
ational algorithms and achieves better generalization than
recent deep learning methods.

1. Introduction

Remote sensing images have become widely used in
many practical applications, such as environmental moni-
toring, object positioning and classiﬁcation. Due to phys-
ical constraints, satellites such as IKONOS, QuickBird-
2, WorldView-2 and WorldView-3 capture two images of
the same scene at the same time, where one image called
panchromatic (PAN) image is of high spatial resolution, and
the other called multi-spectral (MS) image is of low spatial
resolution but it contains good spectral content. In order to
obtain the high resolution multi-spectral (HRMS) images,

This work was supported in part by the National Natural Science Foun-
dation of China under Grants 61571382, 81671766, 61571005, 81671674,
61671309 and U1605252, in part by the Fundamental Research Funds for
the Central Universities under Grants 20720160075 and 20720180059, in
part by the CCF-Tencent open fund, and the Natural Science Foundation
of Fujian Province of China (No.2017J01126).

(a) PAN image

(b) MS image

(c) Fused result

Figure 1: An example of our proposed method. The fused
result has rich details with promising spectral preservation.

pan-sharpening techniques which refer to fuse the low res-
olution spectral information with the spatial structure in the
PAN image have been developed.

1.1. Related works

In the past decades, many pan-sharpening methods have
been proposed. Among these existing methods, the most
common methods include the intensity hue-saturation tech-
nique (IHS) [8], the principal component analysis (PCA)
[21] and Brovey transform [15]. These methods are popu-
lar due to their relatively fast computation. But they usually
suffer from spectral distortion while increasing spatial res-
olution of fused results.

Beside component substitution,

the multi-resolution
analysis (MRA) method is another popular pan-sharpening
method in which the PAN image and MS image are de-
composed into other planes by using some multi-resolution
tools, e.g., decimated wavelet transform (DWT) [22], a
trous wavelet transform (ATWT) [26] and Laplacian pyra-
mid (LP) [7]. The MRA method can sharpen MS image
effectively. However, this may cause some local dissimilar-
ities because the high frequencies extracted from the PAN
image are not exactly to those of the HRMS images.

Recently, in the light of the strong nonlinear mapping
ability of deep learning, researchers have begun explor-
ing the deep convolutional neuron network based methods.
Although these methods [17, 23, 30] obtain excellent per-

10265

formance, they require substantial computational resources
and training data, of which the latter is not easy obtained
in the pan-sharpening area since there is no true ground-
truth. Since all deep learning methods use synthetic data
for training, their generalization performance for real-world
data and new satellite is limited.

So from a practical perspective, variational methods
are reconsidered to pan-sharpening ﬁeld. These method-
s [4, 5, 10, 13, 18, 32] achieve pan-sharpening by modeling
the relationship between PAN, MS and HRMS images in-
to an objective function with some prior knowledge, which
is universal and independent on speciﬁc training data. The
ﬁrst variational pan-sharpening method P+XS technique [5]
preserves spectral well, but produces blurring effects. To
struggle against the blurred edges, a large part of methods
introduce a high-pass ﬁlter to describe structural similarity
while minimizing spectral distortion, such as guided ﬁlter-
based fusion (GDF) [13], Bayesian nonparametric dictio-
nary learning (BNDL) [12] and satellite image registration
and fusion (SIRF) [10]. However, they still suffer some
degradation due to indecent structural constraints.

1.2. Our contributions

In this paper, for the pan-sharpening problem, we focus
on spatial improvement by considering local gradient con-
straints while keeping the spectral information as undistort-
ed as possible. To improve spatial resolution, recent varia-
tional methods make assumptions based on the gradient dif-
ference of PAN and HRMS images. For example, MBF [4]
assumes this relationship follows the Gaussian distribution
while PHLP [18] considers it obeys the Laplacian distribu-
tion through statistical experiments.

All the previous methods assume that the gradient dif-
ference of PAN and HRMS is global linear. However, we
ﬁnd that the relationship is not consistent in different local
image patches. To verify this viewpoint, we randomly scan
one line from a 8-band image and present the difference
of gradient values among the PAN image and each band
of HRMS image in Figure 2(d). Obviously, it is unreason-
able to model the gradient relationships between PAN and
HRMS images with only a global linear function. There-
fore, to avoid global constraints from limiting the modeling
ﬂexibility, a new variational model based on local gradient
constraints is proposed. We formulate our objective func-
tion to consider the following two aspects:

• Spectral preservation: we assume that

the down-
sampled HRMS image should be close to the original
MS image, which aims at preserving the exact spectral
information without introducing false information.

• Spatial improvement: a simple yet effective local lin-
ear regression model is proposed to constraint the gra-
dient difference of PAN and HRMS images, so as to

(a) HRMS

(b) PAN

(c) Intensity scanning

(d) Gradient scanning

Figure 2: 1D signals of intensity and gradient values in one
line of HRMS and PAN images.

effectively utilize the spatial information of PAN im-
age. To the best of our knowledge, this is the ﬁrst vari-
ational model for the speciﬁc pan-sharpening problem
that based on the local constraint.

We show that by using our local gradient constraints, a
simple least-square term, which is easy to optimize, is suf-
ﬁcient to model spatial preservation. To optimize the ob-
jective function, a fast iterative shrinkage-thresholding al-
gorithm (FISTA) is designed. Experiments show that our
method has a great advantage over non deep learning meth-
ods, both subjectively and objectively. Moreover, since we
adopt the universal local constraint, our proposed method
has a better generalization ability than deep learning based
methods that adopt the supervised learning strategy.

2. Motivation

At ﬁrst, some auxiliary notations and deﬁnitions are in-
troduced to simplify our analysis, which will be used in the
following paper. The satellite typically captures two kinds
of images including a PAN image and a corresponding MS
image which has B bands (e.g., B = 8 for WorldView-
2 satellite). We denote the observed PAN image as P ∈
RM ×N and P ∈ RM ×N ×B represents P that expanded
to B bands. The corresponding MS image is denoted by
c ×B and the pan-sharpened HRMS image is
M ∈ R
denoted by X ∈ RM ×N ×B, where c is a reduction ratio.

c × N

M

Since both the PAN and MS images are taken from the
same scene, the spatial structure of them should have a
strong similarity. The PAN image contains abundant spatial
information, which makes it play a signiﬁcant role in im-

10266

proving the spatial resolution of the MS image. The ﬁrst P
+ XS method [5] assumes that the PAN image can be mod-
eled as a global linear combination among all bands of the
HRMS image, i.e.,

XB

b=1

wbXb = P + ε.

(1)

However, even for the same object, different sensor has dif-
ferent response. In other words, differences in intensity of
the HRMS image and the PAN image may be very large,
as shown in Figure 2(c). To avoid this drawback, recent
approaches ensure the consistency of the high-pass ﬁltered
components of PAN image and HRMS image. This require-
ment, which enforces structure similarity rather than inten-
sity similarity, is based on the following assumption:

XB

b=1

wb∇Xb = ∇P + ε,

(2)

where ∇ represents the gradient. To enforce spatial resolu-
tion, previous variational pan-sharpening methods often use
the ℓ2 norm [4] to enforce spatial resolution, or switch to
ℓ1 [25] when sparsity is desired. In SIRF [10], group spar-
sity is encouraged by introducing the ℓ2,1 norm. However,
according to empirical image statistics, assuming the error
ε obeys Gaussian (ℓ2) or Laplacian (ℓ1) assumptions are not
as appropriate as heavy-tailed distribution such as a hyper-
Laplacian [20]. Thus, the PHLP method [18] which adopts
the ℓ1/2 penalty on the gradients of the reconstruction error
is introduced to enforce structural preservation.

Although spatial improvement is achieved by using d-
ifferent sparse assumptions, we argue that modeling based
on equation (2) is not appropriate. First, equation (2) is
built from a global perspective, which is a relative rough as-
sumption. As shown in Figure 2(d), setting the weight w as
a global parameter cannot well model the local relationship
between ∇Xb and ∇P . Second, most previous method-
s simply set w as 1/B, which further reduce the modeling
ﬂexibility. Thus, based on the above analysis, we propose
a new local linear model to better describe the relationship
between ∇X and ∇P at each band:

Let the derivative of the equation (4) be zero, we can get:

ak =

1

|ω| Pi∈ωk

∇xi∇pi − µ(∇xk)µ(∇pk)

σ2(∇pk) + ε

ck = µ(∇xk) − akµ(∇pk),

,

(5)

(6)

where µ and σ2 are the mean and variance, respectively. ε
is a very small parameter to prevent the denominator from
being zero. Note that when ε → 0, ak can be rewritten to:

ak =

cov(∇xk, ∇pk)

σ2(∇pk)

=

cov(∇xk, ∇pk)
σ(∇pk)σ(∇xk)

·

σ(∇xk)
σ(∇pk)

= ρ(∇pk, ∇xk) ·

σ(∇xk)
σ(∇pk)

,

(7)

where the cov() is the covariance and ρ is the correlation
coefﬁcient. Equation (7) means when pk contains structures
that do not exist in xk, ρ(∇pk, ∇xk) is very small and ak
tends to zero, and our model can greatly reduce the effect of
∇pk on ∇xk and vice versa. Moreover, shown in Figure 3,
both a and c can be positive or negative. This implies that
our assumption is much more robust than the equation (2)
which hypothesizes the coefﬁcients are global constants.

However, a pixel i is involved in all the overlapping win-
dows ωk that covers i, so the value of ∇xi in equation (3)
is not identical when it is computed in different windows.
Thus, we follow the strategy of the guided ﬁlter [16] to av-
erage coefﬁcients of all windows overlapping i by

∇xi = ai∇pi + ci,

(8)

where ai = 1

|ω| Pk∈ωi

ak and ci = 1

|ω| Pk∈ωi

ck.

To clearly illustrate different value of a and c, we calcu-
late the value of a and c on 65,536 pixels by using equations
(5) and (6), the results are shown in Figure 3. We found a
and c have different values at different local patches, which
demonstrates that the previous global assumption is inaccu-
rate by simply setting w as a positive value that equals 1/B.
We therefore believe that our local linear model (3) is more
reasonable than the global one (2) for the spatial preserva-
tion.

∇xi = ak∇pi + ck, ∀i ∈ ωk,

(3)

3. Modeling

where ωk represents an image block centered at location k.
For a random pixel i ∈ ωk, ∇xi and ∇pi are the intensity
of ∇X and ∇P at location i, ak and ck are the linear coef-
ﬁcients which are constants in the local area ωk. We easily
ﬁnd that equation (2) is a special form of our model with
ak = 1/wb and ck = ε/wb. Thus, our model equation (3)
can be seen as a general form of previous methods.

To get ak and ck, we minimize this objective function:

min
ak,ck

Xi∈ωk

(∇xi − ak∇pi − ck)2.

(4)

Most previous pan-sharpening methods ﬁrstly up-sample
the multi-spectral image to obtain low resolution multi-
spectral (LRMS) image M to the same size with the PAN
image P, and then propose a spectral preservation based on
the LRMS image to obtain the HRMS image X. However,
on the one hand, the up-sampling approach will introduce
incorrect information, and different up-sampling approach-
es may also affect the results. On the other hand, the objec-
tive function should be added more prior regularization ac-
cording to the speciﬁc up-sampling way. Therefore, instead

10267

Figure 3: According to the equation (3), we show statistical values of calculated a and c of all 65,536 pixels of each band
from a WorldView-2 image of size 256 × 256. As can be seen, both a and c have various sign and magnitudes.

of up-sampling MS image, we argue that the down-sampled
HRMS image should be consistent with the MS image. The
following equation is presented for spectral preservation:

f1(X, M) =

1
2

kψX − Mk2
2 ,

(9)

where ψ denotes a down-sampling operator.

Based on the above analysis in section 2, the spatial p-

reservation term can be written as follows:

f2(X, A, C, P)

B

=

X

X

b=1

k

X
i∈ωk

(∇xb,i − ab,k∇pi − cb,k)2,

(10)

where A and C are matrix form of a and c. Note that since
each pixel of ∇x at location i has its own coefﬁcients ai
and ci, for the entire image, equation (10) can be written in
matrix form via the ℓ2 regularization:

f2(X, A, C, P) =

1
2

k∇X − A · ∇P − Ck2
2 ,

(11)

where · is the element-wise multiplication. Thus, the ﬁnal
objective function composed of the two energy functions
can be rewritten to

min
X,A,C

L = min
X,A,C

f1(X, M) + λf2(X, A, C, P),

(12)

where λ is a regularization parameter.

Compared with previous model, the proposed one has
several advantages. First, the down-sampled way can make
better use of the spectral information of the observed LRM-
S image, which reduces the spectral distortion. Second, the
local regularization term preserves the differences in differ-
ent bands and patches simultaneously, making the gradient
constraints more reﬁned. Last but not least, both f1 and f2
are least-square terms, which make it easy to optimize.

4. Optimization

In this section, our goal is to minimize the objective func-
tion (12). We ﬁrst use Bregman iteration to solve the model.

By decomposing the problem into three sub-problems, each
problem can be solved in a closed form. We summarize our
optimization for pan-sharpening in Algorithm 1.

Update for X: ﬁrst, we introduce an auxiliary variable
Xg = A·∇P+ C, then the objective function (12) becomes:

L =

1
2

kψX − Mk2

2 +

λ
2

k∇X − Xgk2
2 .

(13)

This is a simple least-square optimization problem. Since ψ
can not be written in matrix form, the FISTA framework [6]
is applied to optimize the model to separate ψ. Under the
FISTA framework, the objective function is split into the
following iterative procedure:

Y = Yj − ψ−1(ψX − M)/L,

(14)

where ψ−1 denotes the inverse operator of ψ, j is the jth
iteration. L is the Lipschitz constant for ψ−1(ψX − M).
Then X can be obtained by solving the following function:

Xj = argmin

X

= F −1 


2

λ

2 +

2 (cid:13)(cid:13)(cid:13)

∇X − Xj

kX − Yk2

1
2
F (Y) + λ (cid:16)F (∇x)∗F (Xj
g)(cid:17)
F (1) + λ (F (∇x)∗F (∇x) + F (∇y)∗F (∇x))

g) + F (∇y)∗F (Xj

g(cid:13)(cid:13)(cid:13)

2


 ,

(15)

where F is the FFT operator and F()∗ denotes the com-
plex conjugate. ∇x and ∇y denote the horizontal and verti-
cal differential operators, respectively. F(1) is the Fourier
Transform of the delta function. All operations in equation
(15) are component-wise. Then the step size t and auxiliary
variable Y is updated:

tj+1 = (1 + p1 + 4(tj)2)/2,
Yj+1 = Xj +

tj − 1
tj+1 (Xj − Xj−1).

(16)

(17)

Update for a and c: with X, we update a and c accord-

ing to equations (5) and (6):

aj
b,i =

cj
b,i =

1
|ω| Xk∈ωi
1
|ω| Xk∈ωi

aj
b,k,

cj
b,k,

(18)

(19)

10268

Update for Xg:

the Xg can be directly updated with

ﬁxed a and c:

Xj+1

g = Aj · ∇P + Cj.

(20)

2 + λ (cid:13)(cid:13)∇X − Xj
g(cid:13)(cid:13)

2

2

Algorithm 1
Input: L, λ, t1 = 1, P, Y0, M.
for j = 1 to Max-Iteration do
Y = Yj − ψT (ψX − M)/L
2 kX − Yk2
Xj = argmin

1

X

cov(∇xj

tj+1 (Xj − Xj−1)
d,k,∇pd,k)

tj+1 = (1 + p1 + 4(tj)2)/2
Yj+1 = Xj + tj −1
aj
d,k =
σ2(∇pd,k)+ε
d,k = µ(∇xj
cj
d,k) − aj
g = Aj · ∇P + Cj
Xj+1
end for
Output: the HRMS image X.

d,kµ(∇pk)

5. Experiments

To demonstrate the effectiveness of proposed algorith-
m, we compare our method with ﬁve conventional pan-
sharpening methods: AWLP [24], BDSD [14], Indusion
[19], MTF-GLP [3], PRACS [11], as well as two variation-
al pan-sharpening methods: SIRF [10], PHLP [18]. For fair
comparison, we adjust parameters of each approach to get
their best performances. For visual convenience, we only
present the RGB bands of fused images but conduct exper-
iments in all spectral bands.

5.1. Evaluation at lower scale

Due to the lack of HRMS images of the same scene,
Walds synthesis protocol [28] is used in the simulated ex-
periments. On the basis of this protocol, pan-sharpening is
conducted on the degraded data, and the original MS image
is regarded as a ground truth which is used to compare with
the pan-sharpened image.

To evaluate different methods at lower scale, we intro-
duce both qualitative results and quantitative metrics for as-
sessing the fused images. Quantitative metrics including
spectral angle mapper (SAM) [31], universal image quali-
ty index averaged over the bands (QAVE) [29] and 8-band
extension of Q8 [2], relative dimensionless global error in
synthesis (ERGAS) [27] and the spatial correlation coefﬁ-
cient (SCC) [33]. These metrics are used to measure the
distortion of the spectral information and spatial structures.
For quantitative evaluation, we list the mean and stan-
dard deviation across 225 images with different methods in
Table 1. The best results are boldfaced and the last row
of the table indicate the ideal value.
It can be seen that
our proposed approach signiﬁcantly exceeds all convention-
al and variation methods, which we believe that our local
constraint is more reasonable than others before.

For qualitative analysis, Figure 4 presents the visual re-
sults of each methods while the corresponding residuals
are shown in Figure 5. Even though all the fused images
provide clear versions of the target image by visually, we
can still ﬁnd several subtle discrepancies from residuals. It
can be seen that BDSD suffers severe spectral and spatial
distortion, followed by PRACS. In the case of Indusion,
strong artifacts introduced by the decimation can be no-
ticed. AWLP and MTF-GLP have different levels of spatial
distortion. SIRF performs poorly in keeping some spectral
features and PHLP leads to over-blurred result. Our method
achieves proper trade-off between spectral information and
sharp edges preservation.

5.2. Evaluation at the original scale

Since the PAN images are down-sampled in the simulat-
ed experiment, we apply these methods at the original scale
of PAN images as a complement. Since there are unavail-
able ground truth images, we adopt LRMS images as spec-
tral reference and PAN images as spatial reference. Further-
more, we use the reference-free measurement QNR [1] to
assess the pan-sharpened images. The QNR index is com-
posed by two components: spectral distortion index Dλ and
spatial distortion index Ds.

Performance indexes in Table 2 are obtained by calculat-
ing means and standard deviations over 200 test images. We
highlight the optimal value and ﬁnd that PRACS presents
the best Dλ index while our method has the best perfor-
mance in terms of Ds and QNR metrics.

From a qualitative point of view, we scale up the small
region of parking lot in Figure 6. There is obvious spec-
tral distortion in bright area produced by Indusion, PRAC-
S, MTF-GLP, PHLP and SIRF. BDSD and AWLP exist a
slight degree of color variation compared to the LRMS im-
age. Only our proposed method not only makes full use
of the position information provided by PAN image, but al-
so prevents the spectral content from distorting. Since we
lack of the ground truth, the residuals to the LRMS image
are shown in Figure 7. The LRMS image loses many high-
resolution spatial details but contains abundant spectral in-
formation. Thus, the smooth regions of residuals should
tend to be gray while edges of structures should show ap-
parently. Again, we observe that our model also performs
well in dealing with original scale images.

5.3. Comparison with deep learning based methods

Due to the powerful non-linear modeling ability, deep
learning technology has been explored to handle pan-
sharpening. Therefore, we also compare our model with
two recent deep learning based methods, i.e., PNN [23] and
PanNet [30]. These two methods are designed in a super-
vised fashion to learn the mapping function from labeled da-
ta. To evaluate both pan-sharpening performance and gen-

10269

Table 1: Quality metrics of different methods on 225 satellite images from WorldView-3.

Algorithm

Q8

QAVE

SAM

ERGAS

SCC

Indusion [19]
PRACS [11]
BDSD [14]
AWLP [24]
MTF-GLP [3]
PHLP [18]
SIRF [9, 10]
Proposed

0.799 ± 0.017
0.836 ± 0.023
0.871 ± 0.010
0.849 ± 0.028
0.871 ± 0.023
0.859 ± 0.013
0.863 ± 0.013
0.891 ± 0.023

0.799 ± 0.015
0.822 ± 0.025
0.867 ± 0.013
0.844 ± 0.029
0.858 ± 0.030
0.835 ± 0.011
0.859 ± 0.002
0.890 ± 0.023

6.385 ± 1.544
6.675 ± 1.628
7.158 ± 1.909
6.219 ± 1.487
6.639 ± 1.723
5.748 ± 0.926
6.140 ± 1.416
5.460 ± 1.309

4.340 ± 0.699
3.834 ± 0.718
3.631 ± 0.621
3.697 ± 0.697
3.494 ± 0.723
3.747 ± 0.590
3.564 ± 0.553
3.172 ± 0.603

0.825 ± 0.026
0.835 ± 0.040
0.856 ± 0.032
0.865 ± 0.029
0.857 ± 0.047
0.845 ± 0.024
0.866 ± 0.019
0.891 ± 0.027

ideal value

1

1

0

0

1

(a) LRMS

(b) Indusion

(c) PRACS

(d) BDSD

(e) AWLP

(f) MTF-GLP

(g) PHLP

(h) SIRF

(i) Proposed

(j) Ground truth

Figure 4: Comparison with different methods (source: WorldView-3). The size of PAN is 400 × 400.

(a) Indusion

(b) PRACS

(c) BDSD

(d) AWLP

(e) MTF-GLP

(f) PHLP

(g) SIRF

(h) Proposed

Figure 5: The residuals between the HRMS image reconstructions and the ground truth from Figure 4.

eralization ability, the compared models of both PNN and
PanNet are only trained on WorldView-3. While for testing,
we use the data from both WorldView-2 and WorldView-3.
Since PNN and PanNet are trained on WorldView-3, they
have good visual quality on the testing image that from the
same satellite, as shown in Figure 8. However, the general-
ization ability of PNN and PanNet is limited due to the su-
pervised learning strategy. As shown in Figure 9, the resid-

uals of PNN and PanNet contain more detail and spectral
information. This is because once the training is ﬁnished,
the network parameters of PNN and PanNet will be ﬁxed
and cannot adapt to the new type of data. On the contrary,
our model adopts the local constraint, which is a universal
regularization and is independent of data. This makes our
model has a better generalization ability than PNN and Pan-
Net. This advantage is further proved in Table 3.

10270

(a) LRMS

(b) PAN

(c) Indusion

(d) BDSD

(e) PRACS

(f) MTF-GLP

(g) AWLP

(h) PHLP

(i) SIRF

(j) Proposed

Figure 6: The fusion results at the original scale (source: WorldView-3). The size of PAN is 400 × 400.

(a) Indusion

(b) BDSD

(c) PRACS

(d) MTF-GLP

(e) AWLP

(f) PHLP

(g) SIRF

(h) Proposed

Figure 7: The residuals to the LRMS image from Figure 6. Note that ideal residuals should have smooth regions close to
gray while the edges of structures should be apparent.

Table 2: Quality metrics evaluated at original scales
on 200 satellite images from WorldView-3.

Algorithm

Dλ

Ds

QNR

BDSD [14]
Indusion [19]
PRACS [14]
AWLP [24]
MTF-GLP [3]
SIRF [9, 10]
PHLP [18]
Proposed

0.079 ± 0.035
0.055 ± 0.023
0.019 ± 0.006
0.065 ± 0.026
0.049 ± 0.018
0.070 ± 0.027
0.029 ± 0.020
0.030 ± 0.012

0.128 ± 0.034
0.073 ± 0.018
0.103 ± 0.021
0.108 ± 0.018
0.072 ± 0.018
0.088 ± 0.027
0.077 ± 0.019
0.050 ± 0.015

0.803 ± 0.048
0.876 ± 0.034
0.880 ± 0.021
0.835 ± 0.037
0.883 ± 0.032
0.849 ± 0.047
0.896 ± 0.035
0.922 ± 0.024

ideal value

0

0

1

(a) Ground truth

(b) PNN

(c) PanNet

(d) Proposed

We also test on data at original scales. As shown in Fig-
ure 10, both PNN and PanNet have spectral distortion even
though their models are trained on the same data source,
i.e., WorldView-3. This spectral distortion is more obvi-
ous when PNN and PanNet are directly tested on new satel-
lite, i.e., WorldView-2, as shown in Figure 11. On the con-
trary, our model achieves a good trade-off between spatial
and spectral preservation. The corresponding quantitative

(e) |(a) - (b)|

(f) |(a) - (c)|

(g) |(a) - (d)|

Figure 8: Visual comparison with deep learning. PNN and
PanNet are trained and tested on WorldView–3 images.

results are shown in Table 4, which further proves the gen-
eralization ability of our model to original scale images.

10271

Table 3: Quality metrics of different methods on WorldView-3 and WorldView-2 satellites. “-WV3” and
“-WV2” indicates testing on WorldView-3 and WorldView-2, respectively.

Algorithm

Q8

QAVE

SAM

ERGAS

SCC

PNN-WV3 [23]
PanNet-WV3 [30]
Proposed-WV3
PNN-WV2 [23]
PanNet-WV2 [30]
Proposed-WV2

0.882 ± 0.005
0.925 ± 0.005
0.891 ± 0.023
0.694 ± 0.251
0.723 ± 0.179
0.775 ± 0.189

0.891 ± 0.003
0.928 ± 0.010
0.890 ± 0.023
0.710 ± 0.246
0.728 ± 0.180
0.774 ± 0.196

4.752 ± 0.870
4.128 ± 0.787
5.460 ± 1.309
4.696 ± 1.535
4.091 ± 2.090
2.940 ± 1.585

3.277 ± 0.473
2.469 ± 0.347
3.172 ± 0.603
4.720 ± 0.751
5.569 ± 0.876
3.598± 0.587

0.915 ± 0.009
0.943 ± 0.018
0.891 ± 0.027
0.904 ± 0.015
0.845 ± 0.032
0.915± 0.019

ideal value

1

1

0

0

1

(a) Ground truth

(b) PNN

(c) PanNet

(d) Proposed

(a) LRMS

(b) PNN

(c) PanNet

(d) Proposed

(e) |(a) - (b)|

(f) |(a) - (c)|

(g) |(a) - (d)|

(e) PAN

(f) |(a) - (b)|

(g) |(a) - (c)|

(h) |(a) - (d)|

Figure 9: Visual comparison with deep learning. PNN and
PanNet are trained on WorldView–3 images and tested on
WorldView–2 images to evaluate generalization ability.

Figure 10: Comparison with deep learning on a original s-
cale WorldView-3 image.

Table 4: Quality metrics evaluated at original scales on
WorldView-3 and WorldView-2 satellites.

Algorithm

Dλ

Ds

QNR

PNN-WV3 [23]
PanNet-WV3 [30]
Proposed-WV3
PNN-WV2 [23]
PanNet-WV2 [30]
Proposed-WV2

0.036 ± 0.008
0.023 ± 0.008
0.030 ± 0.012
0.054 ± 0.055
0.091 ± 0.079
0.011 ± 0.005

0.087 ± 0.021
0.071 ± 0.013
0.050 ± 0.015
0.035 ± 0.033
0.125 ± 0.113
0.035 ± 0.015

0.880 ± 0.022
0.908 ± 0.015
0.922 ± 0.024
0.915 ± 0.080
0.803 ± 0.161
0.954 ± 0.018

ideal value

0

0

1

(a) LRMS

(b) PNN

(c) PanNet

(d) Proposed

6. Conclusion

We propose a pan-sharpening method that incorporates
a local constraint for image spatial preservation. First-
ly, we show our local penalty can outperform global one
through statistical veriﬁcation. Secondly, based on this lo-
cal constraint, we build a new variational model for pan-
sharpening. We also derive an simple optimization algorith-
m to efﬁciently solve the proposed model. The experiment
proves that our model can achieve better spectral and spatial
preservation compared with other methods. Moreover, due

(e) PAN

(f) |(a) - (b)|

(g) |(a) - (c)|

(h) |(a) - (d)|

Figure 11: Comparison with deep learning on a original s-
cale WorldView-2 image.

to the proposed universal local constraint, our model has a
better generalization ability than recent deep learning based
method. Since our method does not require training phase,
it has enough ﬂexibility to directly deal with new satellites
and achieve satisfactory performance.

10272

References

[1] L. Alparone, B. Aiazzi, S. Baronti, A. Garzelli, F. Nencini,
and M. Selva. Multispectral and panchromatic data fusion
assessment without reference. Photogrammetric Engineer-
ing & Remote Sensing, 74(2):193–200, 2008. 5

[2] L. Alparone, S. Baronti, A. Garzelli, and F. Nencini. A glob-
al quality measurement of pan-sharpened multispectral im-
agery. IEEE Geoscience Remote Sensing Letters, 1(4):313–
317, 2004. 5

[3] L. Alparone, L. Wald, J. Chanussot, C. Thomas, P. Gamba,
and L. M. Bruce. Comparison of pansharpening algorithms:
Outcome of the 2006 grs-s data-fusion contest. IEEE Trans-
actions on Geoscience and Remote Sensing, 45(10):3012–
3021, 2007. 5, 6, 7

[4] H. A. Aly and G. Sharma. A regularized model-based opti-
mization framework for pan-sharpening. IEEE Transactions
on Image Processing, 23(6):2596–2608, 2014. 2, 3

[5] C. Ballester, V. Caselles, L. Igual, J. Verdera, and B. Roug´e.
A variational model for p+ xs image fusion. International
Journal of Computer Vision, 69(1):43–58, 2006. 2, 3

[6] A. Beck and M. Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
journal on imaging sciences, 2(1):183–202, 2009. 4

[7] P. Burt and E. Adelson. The laplacian pyramid as a com-
IEEE Transactions on communications,

pact image code.
31(4):532–540, 1983. 1

[8] W. J. Carper. The use of intensity-hue-saturation transforma-
tions for merging spot panchromatic and multispectral im-
age data. Photogramm. Eng. Remote Sens., 56(4):457–467,
1990. 1

[9] C. Chen, Y. Li, W. Liu, and J. Huang.

Image fusion with
local spectral consistency and dynamic gradient sparsity. In
CVPR, pages 2760–2765, 2014. 6, 7

[10] C. Chen, Y. Li, W. Liu, and J. Huang.

simul-
taneous satellite image registration and fusion in a uni-
ﬁed framework.
IEEE Transactions on Image Processing,
24(11):4213–4224, 2015. 2, 3, 5, 6, 7

Sirf:

[11] J. Choi, K. Yu, and Y. Kim. A new adaptive component-
substitution-based satellite image fusion by using partial re-
placement. IEEE Transactions on Geoscience and Remote
Sensing, 49(1):295–309, 2011. 5, 6

[12] X. Ding, Y. Jiang, Y. Huang, and J. Paisley. Pan-sharpening
with a bayesian nonparametric dictionary learning model. In
AISTATS, pages 176–184, 2014. 2

[13] F. Fang, F. Li, C. Shen, and G. Zhang. A variational approach
for pan-sharpening. IEEE Transactions on Image Process-
ing, 22(7):2822–2834, 2013. 2

[14] A. Garzelli, F. Nencini, and L. Capobianco. Optimal mmse
pan sharpening of very high resolution multispectral im-
ages. IEEE Transactions on Geoscience and Remote Sens-
ing, 46(1):228–236, 2008. 5, 6, 7

[15] A. R. Gillespie, A. B. Kahle, and R. E. Walker. Color en-
hancement of highly correlated images. ii. channel ratio and
chromaticity transformation techniques. Remote Sensing of
Environment, 22(3):343–365, 1987. 1

[16] K. He, J. Sun, and X. Tang. Guided image ﬁltering. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
35(6):1397–1409, 2013. 3

[17] W. Huang, L. Xiao, Z. Wei, H. Liu, and S. Tang. A new pan-
sharpening method with deep neural networks. IEEE Geo-
science Remote Sensing Letters, 12(5):1037–1041, 2015. 1

[18] Y. Jiang, X. Ding, D. Zeng, Y. Huang, and J. Paisley. Pan-
sharpening with a hyper-laplacian penalty. In ICCV, pages
540–548, 2015. 2, 3, 5, 6, 7

[19] M. M. Khan, J. Chanussot, L. Condat, and A. Montanvert.
Indusion: Fusion of multispectral and panchromatic images
using the induction scaling technique. IEEE Geoscience Re-
mote Sensing Letters, 5(1):98–102, 2008. 5, 6, 7

[20] D. Krishnan and R. Fergus. Fast image deconvolution using
hyper-laplacian priors. In NIPS, pages 1033–1041, 2009. 3
[21] P. Kwarteng and A. Chavez. Extracting spectral contrast in
landsat thematic mapper image data using selective princi-
pal component analysis. Photogramm. Eng. Remote Sens,
55:339–348, 1989. 1

[22] S. G. Mallat. A theory for multiresolution signal decom-
position: the wavelet representation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 11(7):674–693,
1989. 1

[23] G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa. Pan-
sharpening by convolutional neural networks. Remote Sens-
ing, 8(7):594, 2016. 1, 5, 8

[24] X. Otazu, M. Gonz´alez-Aud´ıcana, O. Fors, and J. N´u˜nez.
Introduction of sensor spectral response into image fusion
methods. application to wavelet-based methods. IEEE Trans-
actions on Geoscience and Remote Sensing, 43(10):2376–
2385, 2005. 5, 6, 7

[25] F. Palsson, J. R. Sveinsson, and M. O. Ulfarsson. A new
pansharpening algorithm based on total variation. IEEE Geo-
science Remote Sensing Letters, 11(1):318–322, 2014. 3

[26] M. J. Shensa. The discrete wavelet transform: wedding the
a trous and mallat algorithms. IEEE Transactions on signal
processing, 40(10):2464–2482, 1992. 1

[27] L. Wald. Data fusion: deﬁnitions and architectures: fusion of
images of different spatial resolutions. Presses des MINES,
2002. 5

[28] L. Wald, T. Ranchin, and M. Mangolini. Fusion of satellite
images of different spatial resolutions: Assessing the quality
of resulting images. Photogrammetric Engineering & Re-
mote Sensing, 63(6):691–699, 1997. 5

[29] Z. Wang and A. C. Bovik. A universal image quality index.

IEEE Signal Processing Letters, 9(3):81–84, 2002. 5

[30] J. Yang, X. Fu, Y. Hu, Y. Huang, X. Ding, and J. Paisley.
PanNet: A deep network architecture for pan-sharpening. In
ICCV, 2017. 1, 5

[31] R. H. Yuhas, A. F. Goetz, and J. W. Boardman. Discrim-
ination among semi-arid landscape endmembers using the
spectral angle mapper (sam) algorithm. In Summaries of the
Third Annual JPL Airborne Geoscience Workshop; AVIRIS
Workshop: Pasadena, CA, USA, pages 147–149, 1992. 5

[32] D. Zeng, Y. Hu, Y. Huang, Z. Xu, and X. Ding. Pan-
sharpening with structural consistency and ℓ1/2 gradient pri-
or. Remote Sensing Letters, 7(12):1170–1179, 2016. 2

10273

[33] J. Zhou, D. Civco, and J. Silander. A wavelet transfor-
m method to merge landsat tm and spot panchromatic da-
ta. International Journal of Remote Sensing, 19(4):743–757,
1998. 5

10274

