TACNet: Transition-Aware Context Network for

Spatio-Temporal Action Detection

Lin Song1∗ Shiwei Zhang2∗ Gang Yu3 Hongbin Sun1†

1 Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong Univeristy.

2 Artiﬁcial Intelligence and Automation, Huazhong University of Science and Technology.

3 Megvii Inc. (Face++).

{stevengrove@stu, hsun@mail}.xjtu.edu.cn, swzhang@hust.edu.cn, yugang@megvii.com

Negative Samples

Target Samples

Negative Samples

Transitional state

Groundtruth

Time T

Figure 1: Diagram of transitional state. There are some ambiguous states around but not belong to the target actions, and it
is hard to distinguish them. We deﬁne the states as “transitional state” (red boxes). If we can effectively distinguish these
states, we can improve the ability of temporal extent detection.

Abstract

the state-of-the-art methods on the untrimmed UCF101-24
in terms of both frame-mAP and video-mAP.

Current state-of-the-art approaches for spatio-temporal
action detection have achieved impressive results but re-
main unsatisfactory for temporal extent detection. The main
reason comes from that, there are some ambiguous states
similar to the real actions which may be treated as target
actions even by a well-trained network. In this paper, we
deﬁne these ambiguous samples as “transitional states”,
and propose a Transition-Aware Context Network (TACNet)
to distinguish transitional states. The proposed TACNet in-
cludes two main components, i.e., temporal context detector
and transition-aware classiﬁer. The temporal context de-
tector can extract long-term context information with con-
stant time complexity by constructing a recurrent network.
The transition-aware classiﬁer can further distinguish tran-
sitional states by classifying action and transitional states
simultaneously. Therefore, the proposed TACNet can sub-
stantially improve the performance of spatio-temporal ac-
tion detection. We extensively evaluate the proposed TAC-
Net on UCF101-24 and J-HMDB datasets. The experi-
mental results demonstrate that TACNet obtains competi-
tive performance on JHMDB and signiﬁcantly outperforms

∗indicates equal contribution.
†indicates corresponding author.

1. Introduction

Action detection focuses both on classifying the ac-
tions present in a video and on localizing them in space
and time.
It has been receiving more and more attention
from researchers because of its various applications. Ac-
tion detection has already served as a critical technology
in anomaly detection, human-machine interaction, video
monitoring, etc. Currently, most of action detection ap-
proaches [8, 15, 19, 23] separate the spatio-temporal de-
tection into two stages, i.e., spatial detection and temporal
detection. These approaches adopt the detectors based on
deep neural networks [4, 12] to spatially detect action in
the frame level. Then, they construct temporal detection
by linking frame-level detections and applying some objec-
tive functions, such as maximum subarray method [15], to
create spatio-temporal action tubes. Because these methods
treat video frames as a set of independent images, they can
not exploit the temporal continuity of videos. Thus, their
detection results are actually unsatisfactory.

To address this issue, ACT [10] employs a stacked strat-
egy to exploit the short-term temporal continuity for clip-

11987

level detection and signiﬁcantly improves the performance
of spatio-temporal action detection. However, ACT still can
not extract long-term temporal context information, which
is critical to the detection of many action instances, such as
“long jump”. Moreover, due to the two separate stages in
action detection, ACT can not thoroughly address the time
error induced by ambiguous samples, which are illustrated
as red boxes in Figure 1. In this paper, the ambiguous sam-
ple is deﬁned as “transitional state”, which is close to the
action duration but does not belong to the action. Accord-
ing to the error analysis of ACT detector, 35%-40% of the
total errors [10] are time errors, which are mainly caused
by transitional states. Therefore, to further improve the per-
formance of spatio-temporal action detection, it is critical
to extract long-term context information and distinguish the
transitional states in a video sequence.

The above observations motivate this work. In particu-
lar, we propose a Transition-Aware Context Network (TAC-
Net) to improve the performance of spatio-temporal ac-
tion detection. The proposed TACNet includes two main
components, i.e., temporal context detector and transition-
aware classiﬁer. The temporal context detector is designed
based on standard SSD framework, but can encode the long-
term context information by embedding several multi-scale
Bi-directional Conv-LSTM [11] units. To the best of our
knowledge, this is the ﬁrst work to combine Conv-LSTM
with SSD to construct a recurrent detector for action detec-
tion. The transition-aware classiﬁer can distinguish transi-
tional states by classifying action and action states simul-
taneously. More importantly, we further propose a com-
mon and differential mode scheme to accelerate the con-
vergence of TACNet. Therefore, the proposed TACNet can
not only extract long-term temporal context information but
also distinguish the transitional states. We test the proposed
TACNet on UCF101-24 [21] and J-HMDB [9] datasets and
achieve a remarkable improvement in terms of both frame-
and video-level metrics on both datasets.

In summary, we make the following three contributions:

• we propose a temporal context detector to extract long-
information efﬁciently with

term temporal context
constant time complexity;

• we design a transition-aware classiﬁer, which can dis-
tinguish transitional states and alleviate the temporal
error of spatio-temporal action detection;

• we extensively evaluate our TACNet in untrimmed
videos from the UCF-24 dataset and achieve state-of-
the-art performance.

methods. Although we concentrate on fully supervised
methods in this paper, weakly supervised methods have also
achieved signicant improvement in recent years. The pur-
pose of these methods is to detect actions only with video-
level labels but without frame-level bounding box annota-
tions. These methods can signiﬁcantly reduce annotation
costs and are more suitable to process large unannotated
video data. Multi-Instance Learning (MIL) is one of the
frequently-used approaches for weakly supervised spatio-
temporal action detection. In [20], Siva et al. transforms
the weakly supervised action detection as a MIL problem.
They globally optimize both inter- and intra-class distances
to locate interested actions. Multi-fold MIL scheme is then
proposed in [6] to prevent training from prematurely lock-
ing onto erroneous object detection. Recently, deep model
and attention mechanisms are also employed in deep model
based weakly supervised methods. The methods in [18, 11]
apply attention mechanism to focus on key volumes for ac-
tion detection. Besides, Mettes et al.
[14, 13] proposes to
apply point annotations to perform action detection.

Compared with weakly supervised methods, fully super-
vised methods can leverage bounding box level annotations
to achieve remarkable performance for spatio-temporal ac-
tion detection. Many approaches are proposed to construct
action tubes. Gkioxari et al. [5] ﬁrstly proposes to apply the
linking algorithm on frame-level detection to generate ac-
tion tubes. Peng et al. [15] improves frame-level action de-
tection by stacking optical ﬂow over several frames and also
proposes a maximum subarray method for temporal detec-
tion. Weinzaepfel et al. [22] improves the linking algorithm
by using a tracking-by-detection method. Singh et al. [19]
designs online algorithm to incrementally generate action
tubes for real-time action detection. However, these meth-
ods do not explore the temporal information of actions, and
the performance is still unsatisfactory. To encode the tem-
poral information, Saha et al. [16] and Hou et al. [8] extend
classical region proposal network (RPN) to 3D RPN, which
generates 3D region proposals spanned by several succes-
sive video frames. Becattini et al. [1] adopts LSTM to pre-
dict action progress. Zhu et al. [25] proposes a two-stream
regression network to generate temporal proposals. In con-
trast, Kalogeiton et al. [10] stacks the feature maps of mul-
tiple successive frames by SSD detector to predict score and
regression on anchor cuboid and achieves the state-of-the-
art performance. Therefore, this paper uses ACT [10] as
the baseline to compare and evaluate the action detection
performance of TACNet.

3. Transition-Aware Context Network

2. Related Work

3.1. TACNet framework

Spatio-temporal action detection methods can generally
be classied into two categories: weakly and fully supervised

Figure 2 illustrates the overall framework of TACNet,
which mainly consists of two parts, i.e., two-stream tem-

11988

Figure 2. Overall framework of the proposed TACNet. TACNet mainly contains two modules: temporal context detector and transition-
aware classiﬁer. In the temporal context detector, we embed several multi-scale Conv-LSTM [11] units in the standard SSD detector [12]
to extract temporal context. Based on the recurrent action detector, the transition-aware classiﬁer is designed to simultaneously detect the
action categories and states. Then, we can correctly localize the temporal boundaries for the target actions.

poral context detection and transition-aware classiﬁcation
& regression. Although the framework is similar to most
of the previous methods, especially ACT detector, tempo-
ral context detector and transition-aware classiﬁer are pro-
posed to signiﬁcantly improve the capability of extracting
long-term temporal context information and distinguish-
ing transitional states respectively. For the temporal con-
text detector, we adopt two-stream SSD to construct ac-
tion detection as the ACT detector does.
In addition, to
extract long-term temporal context information, we embed
several Bi-directional Conv-LSTM (Bi-ConvLSTM) [11]
unites into different feature maps with different scales. The
Bi-directional Conv-LSTM architecture can keep the spa-
tial layout of the feature maps, and is of beneﬁt to perform-
ing spatial localization. In the transition-aware classiﬁer, to
distinguish transitional states, we propose two branches to
classify the actions and action states simultaneously. More-
over, we further design a common and differential mode
scheme, inspired by the basic concepts in the signal pro-
cessing domain [2], to accelerate the convergence of the
overall TACNet. Associating with the regression mod-
ule, transition-aware classiﬁer can spatially detect the ac-
tions and temporally predict the temporal boundaries in the
meantime.
In addition, the proposed method can be em-
bedded into various detection frameworks, and this work is
based on the SSD due to its effectiveness and efﬁciency.

3.2. Temporal Context Detector

Long-term temporal context information is critical to
spatio-temporal action detection. The standard SSD per-
forms action detection from multiple feature maps with dif-
ferent scales in the spatial level, but it does not consider
temporal context information. To extract temporal context,
we embed Bi-ConvLSTM unit into SSD framework to de-
sign a recurrent detector. As a kind of LSTM, ConvLSTM

not only can encode long-term information, but also is more
suitable to handle spatio-temporal data like video. Because
the ConvLSTM unit can preserve the spatial structure of
frames over time by replacing the fully connected multi-
plicative operations in an LSTM unit with convolutional op-
erations. Therefore, it is reasonable to employ ConvLSTM
units into our framework to extract long-term temporal in-
formation.

In particular, we embed a Bi-ConvLSTM unit between
every two layers of adjacent scales in SSD to construct
the proposed temporal context detector, as shown in Fig-
ure 2. The proposed module considers the input sequences
in both forward and backward directions which adopts a
pair of temporal-symmetric ConvLSTM for these two di-
rections. The Bi-ConvLSTM can extract a pair of features
for each scale in a frame. These features are concatenated
and transformed by a 1 × 1 convolutional layer to elimi-
nate the redundancy of channels. By this means, the pro-
posed temporal context detector can leverage the advantage
of SSD and extract long-term temporal context information
as well. Moreover, we also make two modiﬁcations to the
Bi-ConvLSTM unit: (i) we replace activation function tanh
by ReLU , which can improve the performance slightly ac-
cording to experimental results; (ii) we apply 2D dropout
between inputs and hidden states to avoid over-ﬁtting.

Compared with the ACT, our method is also efﬁcient in
terms of computational cost. ACT applies a sliding win-
dow with stride 1, and takes n stacked frames as input for
the processing of each frame. Therefore, the computational
complexity of ACT is O(n). On the contrary, we constantly
process each frame twice. Therefore, supposing n is the
number of stacked frames, the computational cost of ACT
and the proposed temporal context detector is O(n) and
O(1) respectively. We can ﬁnd that the computational cost
gap increases when n grows, especially n can be vast when

11989

considering long-term temporal information.

3.3. Transition Aware Classiﬁer

Proposals in the transitional state have a similar appear-
ance to target actions and may easily confuse the detectors.
Most of the existing methods do not provide explicit deﬁ-
nition on these proposals but rely on post-processing algo-
rithms to prune them or simply treat them as background.
However, since these proposals are much different from
background (e.g., scenes and other objects), treating them as
background enlarges intra-class variance and limits the de-
tection performance. In this paper, we propose a transition-
aware classiﬁer to perform the action category classiﬁcation
and the prediction of transitional state simultaneously.

i.e.

0 , c+

1 , ..., c+

2 , ..., c−

To simultaneously predict action categories and action
c+ =
states, we ﬁrstly deﬁne a pair of scores,
1 , c−
K] and c− = [c−
[c+
K], where K is the
number of categories and c+
0 is the score of background.
The scores c+ and c− denote the action classiﬁcation scores
and transitional state classiﬁcation scores respectively. We
should note that the transitional state scores have no back-
ground category. In the transition-aware classiﬁer, we apply
two classiﬁers to predict these two scores, as shown in Fig-
ure 3. Based on these deﬁnitions, we formulate the training
targets of target actions and transitional states as following:
For an active sample of category i, the training target

should meet Eq. 1:

c+
i > c−

i and c+

i > c+

j , ∀j 6= i

(1)

where i, j ∈ [1, 2, 3 . . . , K].

While for a transitional sample of category i, the training

target should meet Eq. 2:

i < c−
c+
i .

(2)

In general, we can directly train TACNet based on these
targets. However, we ﬁnd that it is hard to converge when
learning these two targets. The reason may come from that
the c+ and c− are inter-coupled, which leads to the inter-
action between them. For example, a transitional sample
of the category i tries to simultaneously minimize c+
i and
maximize c−
to meet Eq.2, which will hurt the distribution
i
of category prediction c+.

To solve this issue, we take advantage of the knowledge
in signal processing domain [2] that inter-coupled signal
can be decoupled by a pair of individual branches, i.e.,
common mode and differential mode branches.
Inspired
by these concepts, we design a common and differential
scheme to train our network.
In particular, the proposed
transition-aware classiﬁer still outputs c+ and c−, but the
difference is that we use c+ + c− to predict action category
(upper branch in Figure 3) and c+ − c− to predict action

Figure 3. Diagram of the transition-aware classiﬁer. We propose
a common and differential mode scheme to decouple the inter-
coupled features into two branches, which predict action category
and action states respectively.

state (lower branch in Figure 3). We formulate the decou-
pled targets as following:

pi =

ti =

ec+

i +c−
i

Pj∈[0,K] ec+
ec+
i −c−

i −c−
i

i + 1

ec+

, ∀i ∈ [0, K]

j +c−
j

, ∀i ∈ [1, K].

(3)

(4)

where pi denotes the probability of being classiﬁed as cate-
gory i (Eq.3), which is independent to action states. How-
ever, we should note that ti denotes the probability of active
state but not transitional state (Eq.4). The probability of a
transitional state is calculated as 1 − ti.

When a sample in transitional state tries to minimize
c+ − c−, we suppose that c+ changes to c+ − λ, and c−
will change to c− + λ (since these two branches share the
same gradient magnitude). The category prediction c++c−
will have no change. By this means, the predictions of the
action category and action state are decoupled. In our ex-
periments, we ﬁnd that the network can converge easily and
the prediction has a very slight effect on each other.

3.4. Training and Inference Procedures

In the training phase of TACNet, we denote P as the pos-
itive samples which have more than 0.5 IoU with at least one
ground-truth bounding box. We apply T and v to denote
the positive sample set of transitional states and the cor-
responding predicted action categories respectively. How-
ever, there are no annotations for transitional states in the
existing untrimmed datasets so far. According to the deﬁ-
nition of transitional states, we propose a “simple-mining”
strategy to label the positive samples of the transition states.
In detail, the detected boxes are treated as transitional state
samples when their scores meet c+
0 , but their corre-
sponding frames have no ground-truth annotations. These
transitional state samples are further applied to train TAC-
Net.

i > c+

11990

We employ the same loss function as SSD for regression
Lreg. Besides, we introduce the classiﬁcation loss Lcls and
transitional loss Ltrans based on the classiﬁcation scores p
and action state scores t illustrated in Eq.5:

Lcls = − X

j∈P

log pj

y − X

j∈G\P

log pj
0,

Ltrans = − X

j∈P

log tj

y − X

j∈T

log (cid:16)1 − tj

vj(cid:17) ,

(5)

vj = arg max
i∈[1,K]

(pj

i ), T = {j |vj > 0 , j ∈ U \G} ,

where U and G refers to the set of all available anchors and
anchors in the images which have groudtruth annotations
respectively, pj
i is the predicted probability of j-th anchor
with category i, and tj
i is the probability of j-th transitional
anchor with predicted category i.

We optimize the proposed TACNet with the combined

loss:

L =

1
Np

(Lcls + Lreg) +

1
Nt

Ltrans,

(6)

where Np and Nt denote the number of positive samples
P and transitional samples T respectively. We train each
stream of our network in an end-to-end manner. The exper-
iments demonstrate that the branches of classiﬁcation and
transition can be optimized simultaneously.

In the inference phase, TACNet takes video clips as in-
put and output three items: spatial detection boxes, clas-
siﬁcation scores, and action state scores. To construct
spatio-temporal action tubes, we ﬁrst apply the greedy al-
gorithm [19] on the frame-level detections using category
scores to construct candidate tubes. Secondly, we apply ac-
tion state predictions to perform temporal detection. In the
experiments, we ﬁnd that the action state scores are discon-
tinuous, and hence apply the watershed segmentation algo-
rithm [24] to trim candidate tubes for temporal detection.
Besides, inspired by ACT [10], we introduce a micro-tube
reﬁnement (MR) procedure which constructs a micro-tube
for proposals in the same spatial position of adjacent frames
to average the predicted category scores. The score of each
box is set to the product of category score and action state
score.

4. Experiments

4.1. Experimental setup

We evaluate TACNet on two datasets: UCF101-24 [21]
and J-HMDB [9]. The UCF101-24 dataset contains 3207
videos for 24 classes. Approximate 25% of all the videos
are untrimmed. The J-HMDB dataset contains 928 videos
with 33183 frames for 21 classes. Videos in this dataset are

Table 1. Performance Comparison on J-HMDB.

Method

Frame-mAP

SSD
TCD1
TS2+SSD

TS+TCD
TS+TCD+MR3

49.5

54.1

56.4

61.5

65.5

1 TCD: Temporal Context Detector;
2 TS: Two-Stream;
3 MR: Micro-tube reﬁnement.

Video-mAP

0.5

60.1

64.5

70.3

73.4

73.4

0.75

41.5

45.3

48.3

52.5

52.5

0.5:0.95

33.8

35.1

42.2

44.8

44.8

0.2

60.9

65.0

70.9

74.1

74.1

trimmed to actions. Hence we only use it to evaluate the
spatial detection of the proposed TACNet.

We apply the metrics of frame-mAP and video-mAP [5]
to evaluate TACNet at frame-level and video-level respec-
tively. The frame-mAP and video-mAP measure the area
under the precision-recall curve of detections for each frame
and action tubes respectively. Therefore, frame-mAP mea-
sures the ability of classiﬁcation and spatial detection in
a single frame, and video-mAP can also evaluate the per-
formance of temporal detection. The prediction is correct
when its overlap with ground-truth box/tube above a certain
threshold and predicted category is correct. In this paper, we
apply constant IoU threshold (0.5) to evaluate frame-mAP
and variable IoU thresholds (i.e 0.2, 0.5, 0.75 and 0.5:0.95)
to evaluate video-mAP.

We provide some implementation details in TACNet as
follows. Input frames are resized to 300x300. The clip size
L is set as 16 for training and inference. The number of
ﬂow images S is set as 5 for each frame. The probability of
2D dropout is set as 0.3. In the training phase, we stack 32
clips as a mini-batch and apply data augmentation of color
jitter, cropping, rescaling, and horizontal ﬂipping. We use
the hard-negative-mining strategy that only the hardest neg-
atives up to the same quantity of positives are kept to cal-
culate the loss. Following the previous works, we separate
training the appearance stream from the motion stream. To
train the appearance branch, we set the learning rate as 0.01
for initial learning and decrease it by 10 times in every 20
epochs. We employ a warmup scheme [7] with the learn-
ing rate of 0.001 in the training phase. To train the motion
branch, we take the parameters from the appearance branch
as the initial parameters and set the initial learning rate as
0.001. Furthermore, we ﬁne-tune the fusion network with
a consistent learning rate of 0.0001. Besides, we optimize
TACNet by stochastic gradient descent (SGD) with momen-
tum of 0.9. In the inference phase, the number of micro-tube
frames is set as 8 for micro-tube reﬁnement procedure.

4.2. Analysis of Temporal Context Detector

We perform several experiments to evaluate the effec-
tiveness of the proposed temporal context detector un-

11991

Table 2. Performance Comparison on UCF101-24.

Method

SSD

TCD

TS+SSD

TS+TCD
TS+TCD+BG1
TS+SSD+TAC2

TS+TCD+TAC

TS+TCD+TAC+MR

F-mAP

65.3

67.5

66.5

68.7

68.3

67.1

69.7

72.1

Video-mAP

0.5

39.3

45.0

47.5

50.6

48.9

49.0

52.9

52.9

0.75

16.3

17.4

19.2

20.9

20.8

20.1

21.8

21.8

0.5:0.95

18.4

19.4

21.0

23.0

22.4

21.8

24.1

24.1

0.2

69.1

72.2

74.3

77.3

77.0

74.5

77.5

77.5

1 BG: This method simply treats transitional states as background, and

can be treat as hard-mining method;

2 TAC: Transition-Aware Classiﬁer;

der different conﬁgurations on J-HMDB and UCF101-24
dataset. We report the frame-level and video-level perfor-
mance in Table 1 - 2.

From the results, we can ﬁnd that temporal context detec-
tor signiﬁcantly outperforms standard SSD on both datasets.
On the dataset of J-HMDB, the proposed temporal context
detector with a two-stream conﬁguration obtains the im-
provements of 5.1% and 3.1% the in terms of frame-mAP
and video-mAP (IoU threshold is 0.5) respectively. The im-
provements on UCF101-24 are 2.2% and 3.1% respectively.
These results clearly demonstrate that temporal context in-
formation can effectively improve performance.

4.3. Analysis of Transition Aware Classiﬁer

We perform several experiments

to evaluate the
transition-aware classiﬁer on the untrimmed UCF101-24
dataset. We report the experimental results with different
conﬁgurations in Table 2, and the per-class performance of
untrimmed categories in Table 4. We present some visual-
ized analysis of transition-aware classiﬁer in Figure 4.

In Table 2, the results of improvement prove its effec-
tiveness of the proposed transition-aware classiﬁer with dif-
ferent settings.
It should be noted that the hard-mining
method (“BG” in the table) leads to a slight performance
drop, which means that simply treating transition sates as
the background is unreasonable. In contrast, the transition-
aware classiﬁer achieves signiﬁcant improvement in terms
of both metrics. In particular, it improvements performance
by 2.3% in terms of video-mAP when IoU is 0.5. There-
fore, the results can clearly demonstrate that it is crucial to
deﬁne transitional states and the proposed transition-aware
classiﬁer can well distinguish these states.

In Table 4, we can ﬁnd that transition-aware classi-
ﬁer can obtain obvious improvement in the untrimmed
videos. Especially, we outperform the baseline without a
transitional-aware classiﬁer and ACT by 1.7% and 7.8%
in terms of video-mAP respectively. The transition-aware
classiﬁer achieves 8% improvement in terms of temporal

detection when only considering temporal IoU (“Temporal”
in the table),. Therefore, these results demonstrate the ca-
pability of a transition-aware classiﬁer for temporal extent
detection.

In Figure 4, we take a “Volleyball” action instance as
an example to intuitively show the reason for the perfor-
mance improvement of a transition-aware classiﬁer. We
can ﬁnd that it is difﬁcult to distinguish transitional states
by only considering classiﬁcation scores. However, action
state scores can help to trim the actions well temporally.
More results can be found in Figure 5. In Figure 5, we can
ﬁnd that the proposed classiﬁer can also distinguish the ac-
tion states for multiple instances.

Table 4. Per-class performance of untrimmed categories on
UCF101-24.

Video-mAP

Frame-mAP

Dunk

Diving

Category

Basketball

CliffDiving
CricketBowl

Temporal2
To
Tw
9.3
25.8
76.6
88.2
84.4
84.4
25.5
14.0
84.7
84.7
70.4
70.4
66.2
68.0
77.4
77.4
9.6
8.0
12.0
48.8
58.3
50.3
1 Spatio-temporal: evaluating the performance in terms of standard

Spatio-temporal1
ACT
0.0
1.2
39.9
1.1
26.1
51.0
71.1
44.6
0.5
0.0
23.5

To4
34.8
53.0
74.4
42.4
82.3
55.8
59.0
63.8
40.2
44.1
55.0

Tw3
44.0
57.1
74.9
39.7
85.9
58.9
58.6
64.0
46.4
50.6
58.0

To
0.3
5.3
45
0.9
43.2
49.5
46.2
60.7
0.1
0.9
25.2

Tw
5.5
18.9
42.9
3.6
52.1
65.9
50.9
57.2
2.2
13.4
31.3

GolfSwing
LongJump
PoleVault

Volleyball
Mean-AP

TennisSwing

video-mAP;

2 Temporal: evaluating the performance again ground-truth by only con-

sidering temporal IoU while without spatial IoU;

3 Tw: results with transition-ware classiﬁer;
4 To: results without a transition-aware classiﬁer.

Table 5. Performance of different detector and base model on
UCF101-24 dataset without micro-tube reﬁnement.

Method

Model

Frame-mAP

SSD

DSSD

SSD

VGG16

VGG16

Resnet50

DSSD

Resnet50

69.7

70.1

72.0

74.6

Video-mAP

0.2

0.5

0.75

0.5:0.95

77.5

52.9

21.8

77.5

53.0

22.1

78.9

54.4

23.0

79.2

54.6

23.3

24.1

24.5

24.8

25.4

4.4. Exploration on advanced backbone

In this section, we explore different detectors and mod-
els using our method. We respectively replace the detector
and base model with Deconvlution-SSD [3] and Resnet-50
respectively, and show the results in Table 5. All the mod-
els are pretrained on the ImageNet. Two streams, temporal
context detector, and transition-aware classiﬁer are also ap-

11992

Figure 4. Visualized analysis of transition-aware classiﬁer when taking “Basketball pitch” action as an example. Top row: detection boxes
with their corresponding action state scores; bottom row: the procedure with different predictions including Coarse action state score (CA),
Reﬁned action state score (FA), Classiﬁcation score (CLS), and compared to Ground-truth (GT). Based on these predictions, we temporally
trim the detection whose FA score is larger than 0.5, while the others are treated as transitional samples.

Table 3. Comparison with the state-of-the-art on J-HMDB (trimmed) and UCF101 (untrimmed)

Method

F-mAP

Saha [17]

Peng [15]

Singh [19]

Hou [8]

Becattini [1]

Kalogeiton [10]

Ours

-

58.5

-

61.3

-

65.7

65.5

J-HMDB

Video-mAP

0.2

72.6

74.3

73.8

78.4

-

74.2

74.1

0.5

71.5

73.1

72.0

76.9

-

73.7

73.4

0.75

43.3

-

0.5:0.95

40.04

-

44.5

41.6

-

-

52.1

52.5

-

-

44.8

44.8

UCF101-24 (Full)1

UCF101-24 (Untrimmed)1

F-mAP

-

65.7

-

41.4

-

69.5

72.1

0.2

66.7

73.5

73.5

47.1

67.0

76.5

77.5

Video-mAP

0.5

0.75

0.5:0.95

F-mAP

Video-mAP

0.5

35.9

32.1

46.3

-

35.7

49.2

52.9

7.9

2.7

15.0

-

-

19.7

21.8

14.4

7.3

20.4

-

-

23.4

24.1

-

-

-

-

-

-

-

-

-

-

52.1

58.0

23.5

31.3

1 UCF101-24 is a mixture dataset which is made up of untrimmed categories and trimmed categories, thus we evaluate our approaches in two

criterions to fully illustrate the performance gain on untrimmed videos.

plied. We can ﬁnd that the employment of advanced models
can further improve performance.

4.5. Comparison with the state of the art

We compare TACNet with state-of-the-art methods in
terms of both frame-mAP and video-mAP, and the results
are shown in Table 3. From the results, we see that TAC-
Net outperforms all these previous methods on temporally
untrimmed UCF101-24 dataset in terms of both metrics.
In particular, we surpass ACT, which is the current state-
of-the-art, by 3.7% in terms of video-level when the IoU
threshold is 0.5. On the trimmed J-HMDB dataset, ACT and
T-CNN outperform TACNet in terms of frame-mAP and
video-mAP respectively. We think there are two reasons:
(i) these two methods directly generate action tubes, which
are suitable for the trimmed dataset; (ii) J-HMDB dataset
is relatively simple, especially for localization since only
one instance in each frame. However, for action detection,
video-mAP is more suitable to evaluate the performance
than frame-mAP in both spatial and temporal domain, and
in terms of video-mAP TACNet obtains a competitive per-
formance with ACT. On the more challenging untrimmed

UCF101-24 dataset, we can ﬁnd that TACNet signiﬁcantly
outperforms T-CNN and ACT. It even improves the per-
formance against T-CNN by 28.5% and 30.4% in terms of
frame-mAP and video-mAP when IoU is 0.5. Therefore, the
superior performance of TACNet demonstrates the impor-
tance of long-term temporal context information and transi-
tional state detection.

5. Conclusions

This paper aims to improve the performance of action
In particular, we ﬁnd that it is critical to ex-
detection.
tract long-term temporal context information and distin-
guish transitional states. Based on these observations, we
propose a TACNet which consists of a temporal context
detector and a transitional-aware classiﬁerWe extensively
explore TACNet on two public datasets. From the ex-
perimental results, we ﬁnd TACNet can signiﬁcantly im-
prove the performance and surpass the state-of-the-art on
the challenging untrimmed dataset. The performance im-
provements of TACNet come from both temporal detection
and transition-aware method. In future work, we will con-
tinue our exploration on how to further improve temporal

11993

00.51CAFACLSGTFigure 5. Predicted results of the proposed TACNet in four states: (a) Background (w/o box), (b) Transitional state (blue box), (c) Active
state (green box) and (d) Ground-truth (yellow box).

detection by considering the relation between actors and
their surrounding peoples or objects.

6. Acknowledgment

This research was supported by the National Key R&D

Program of China (No. 2017YFA0700800).

References

[1] F. Becattini, T. Uricchio, L. Ballan, L. Seidenari, and
A. Del Bimbo. Am i done? predicting action progress in
videos. arXiv preprint arXiv:1705.01781, 2017.

[2] D. E. Bockelman and W. R. Eisenstadt. Combined differen-
tial and common-mode scattering parameters: Theory and
simulation.
IEEE transactions on microwave theory and
techniques, 43(7):1530–1539, 1995.

[3] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
DSSD: Deconvolutional single shot detector. arXiv preprint
arXiv:1701.06659, 2017.

[4] R. Girshick. Fast r-cnn. In ICCV, pages 1440–1448, 2015.
[5] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

pages 759–768, 2015.

[6] R. Gokberk Cinbis, J. Verbeek, and C. Schmid. Multi-fold
In

mil training for weakly supervised object localization.
CVPR, pages 2409–2416, 2014.

[7] P. Goyal,

P. Doll´ar, R. Girshick,

P. Noordhuis,
L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.
Accurate, large minibatch sgd: training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677, 2017.

[8] R. Hou, C. Chen, and M. Shah. Tube convolutional neu-
ral network (t-cnn) for action detection in videos. In ICCV,
2017.

[9] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black.
Towards understanding action recognition. In ICCV, pages
3192–3199, 2013.

[10] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid.
Action tubelet detector for spatio-temporal action localiza-
tion. In ICCV, 2017.

[11] Z. Li, K. Gavrilyuk, E. Gavves, M. Jain, and C. G. Snoek.
Videolstm convolves, attends and ﬂows for action recogni-
tion. Computer Vision and Image Understanding, 166:41–
50, 2018.

[12] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. Ssd: Single shot multibox detector. In
ECCV, pages 21–37. Springer, 2016.

[13] P. Mettes, C. G. Snoek, and S.-F. Chang. Localizing actions
from video labels and pseudo-annotations. arXiv preprint
arXiv:1707.09143, 2017.

[14] P. Mettes, J. C. van Gemert, and C. G. Snoek. Spot on:
In

Action localization from pointly-supervised proposals.

11994

ECCV, pages 437–453. Springer, 2016.

[15] X. Peng and C. Schmid. Multi-region two-stream r-cnn for

action detection. In ECCV, pages 744–759, 2016.

[16] S. Saha, G. Singh, and F. Cuzzolin. Amtnet: Action-micro-
tube regression by end-to-end trainable deep architecture.
ICCV, Oct, 2, 2017.

[17] S. Saha, G. Singh, M. Sapienza, P. H. Torr, and F. Cuzzolin.
Deep learning for detecting multiple space-time action tubes
in videos. arXiv preprint arXiv:1608.01529, 2016.

[18] S. Sharma, R. Kiros, and R. Salakhutdinov. Action recogni-
tion using visual attention. arXiv preprint arXiv:1511.04119,
2015.

[19] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin.
Online real-time multiple spatiotemporal action localisation
and prediction. In CVPR, pages 3637–3646, 2017.

[20] P. Siva and T. Xiang. Weakly supervised action detection. In

BMVC, volume 2, page 6, 2011.

[21] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[22] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, pages
3164–3172, 2015.

[23] G. Yu and J. Yuan. Fast action proposals for human action

detection and search. In CVPR, pages 1302–1311, 2015.

[24] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin.
Temporal action detection with structured segment networks.
In ICCV, volume 8, 2017.

[25] H. Zhu, R. Vial, and S. Lu. Tornado: A spatio-temporal
convolutional regression network for video action proposal.
In CVPR, pages 5813–5821, 2017.

11995

