Part-regularized Near-duplicate Vehicle Re-identiﬁcation

Bing He1

Jia Li1,3,4 ∗

Yifan Zhao1

Yonghong Tian2,3

1State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University
2National Engineering Laboratory for Video Technology, School of EE&CS, Peking University

3Peng Cheng Laboratory, Shenzhen, China

4Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China

1{bing, jiali, zhaoyf}@buaa.edu.cn 2yhtian@pku.edu.cn

Abstract

Vehicle re-identiﬁcation (Re-ID) has been attracting
more interests in computer vision owing to its great con-
tributions in urban surveillance and intelligent transporta-
tion. With the development of deep learning approaches,
vehicle Re-ID still faces a near-duplicate challenge, which
is to distinguish different instances with nearly identical ap-
pearances. Previous methods simply rely on the global vi-
sual features to handle this problem. In this paper, we pro-
posed a simple but efﬁcient part-regularized discriminative
feature preserving method which enhances the perceptive
ability of subtle discrepancies. We further develop a nov-
el framework to integrate part constrains with the global
Re-ID modules by introducing an detection branch. Our
framework is trained end-to-end with combined local and
global constrains. Specially, without the part-regularized
local constrains in inference step, our Re-ID network out-
performs the state-of-the-art method by a large margin on
large benchmark datasets VehicleID and VeRi-776.

1. Introduction

Given a query image of a vehicle identity, vehicle re-
identiﬁcation task aims to retrieve all the images of this i-
dentity from a large image database which typically cap-
tured from a large camera network. With the proposals of
large dataset [14, 12, 27]and the development of deep learn-
ing algorithms [24, 36], recent models have gain remarkable
success in the past decade. The re-identiﬁcation of vehi-
cles has a great potential to contribute to the urban security
surveillance and intelligent transportation.

Considering the inconspicuous divergences among dif-
ferent instances, vehicle re-identiﬁcation is still a very chal-
lenging task, especially with the large amount of dataset. To
address this Re-ID task, many deep learning models [27, 1]

∗Jia Li is the corresponding author. URL: http://cvteam.net

Figure 1. Near-duplicate problem. (a) Images in each row all come
from different vehicle identities with similar appearance. Appar-
ently it is difﬁcult to distinguish them especially the ﬁrst row since
all three vehicle identities come from a same vehicle model. Sub-
tle discriminative vehicle parts are crucial for the near-duplicate
vehicle re-identiﬁcation. (b) As shown in the right side of the im-
age those similar vehicles are easy to distinguish using the local
part feature.

relied on global information have been proposed in the past
decades. One intuitive solution is to reduce the distances
of identical vehicle images and enlarge the distance of d-
ifferent ones with learning approaches. To better measure
the distance, previous works [12] mainly use deep metric
learning to directly embed the raw image into an Euclidean
space where the distance can be directly used as similarity
scores between two vehicles. Weinberger et al. [25] explore
the topic of metric learning to perform k-nearest neighbor
classiﬁcation and propose the Large Margin Nearest Neigh-
bor loss (LMNN). FaceNet [20] improved the LMNN loss
into a modiﬁed triplet loss which directly optimize the ﬁnal
distance metric and can be applied in re-identiﬁcation and
face recognition tasks. Although these works reach remark-
able success in vehicle re-identiﬁcation tasks, they usually
get confused when these vehicle have inconspicuous differ-
ences. e.g., see Fig. 1 (a).

3997

To handle this problem, recent works resort to addition-
al license plate and spatial-temporal information. Liu et
al. [14] introduce license plate recognition into Re-ID task.
The license plate recognition usually fails in unconstrained
environment due to the various viewpoints and changeable
illuminations. However, owing to the privacy and securi-
ty considerations in vehicle re-identiﬁcation task, the plate
information is inaccessible in the public benchmarks. Be-
sides, Some other methods [21, 24] rely on extra spatial-
temporal information to explore the ﬁnal retrieval results.

In this paper, we explore the near-duplicate phenomenon
in vehicle re-identiﬁcation. As illustrated in Fig. 1 a), dif-
ferent vehicles usually share similar geometric shapes and
appearances which can be hard to distinguish by deep mod-
els. While the details from these near-duplicate vehicles
have arresting variances in local features such as brands
and tags in windows which are easily recognized by hu-
man beings, see Fig. 1 b). To handle the near duplicated
phenomenon in vehicle re-identiﬁcation task, we propose
a part-regularized approach which integrates the local and
no-local features into a uniﬁed architecture. To avoid the
vanish of local features, we enhance the perception of local
information of regularized parts in deep learning network-
s. Inspired by ROI (region of interest) in object detection,
we adopt ROI receptive module to capture the local infor-
mation. We develop a simple but effective ROI projection
approach to combine detection branch with our Re-ID task.
After combining these features, we further developed a lo-
cal and no-local classiﬁcation loss. To summarize, the con-
tribution of our work is three-fold:

• We design an effective representation learning frame-
work by jointly considering local and global represen-
tations.

• We propose a part-regularized approach to enhance the
discriminative capability of global features for vehicle
re-identiﬁcation.

• We conduct extensive experiments to show that the
proposed approach outperforms state-of-the-art: Vehi-
cleID [12] by 57% in rank-1, 23% in rank-5, VeRi-
776 [14] by 48% in mAP, 2.1% in HIT@1 and 9.6%
in HIT@5.

The rest of this paper is organized as follows: Sec. 2 re-
views the related works, Sec. 3 gives the problem statement
of vehicle re-identiﬁcation and explains the details of our
part-regularized model. Qualitative and quantitative exper-
iments are presented in Sec. 4 and we ﬁnally conclude our
paper in Sec. 5.

2. Related Work

Vehicle Re-ID. The vehicle re-identiﬁcation task has
Li-

gained more and more attention in recent years.

u et al. [12] proposed a benchmark dataset VehicleID and
a pipeline which use Deep Relative Distance Learning
(DRDL) to project vehicle images into an Euclidean space,
where the distance can directly measure the similarity of t-
wo vehicle images. Liu et al. [14]proposed another dataset,
which called VeRi-776, and build a coarse-to-ﬁne progres-
sive search framework through utilizing the visual appear-
ance, license plate and spatial-temporal information. VeRi-
776 contains rich annotations including vehicle types, col-
ors, brands, license plate and spatio-temporal information.
Wang et al. [24] explored vehicle viewpoint attribute and
proposed orientation invariant feature embedding module.
The orientations information are extracted by 20 vehicle key
points locations. Shen et al. [21] pushed spatial-temporal
idea further and proposed Visual-spatial-temporal Path Pro-
posals method. Yan et al. [27] model the relationships of
vehicle images as multi-grain list and proposes two rank-
ing methods, generalized pairwise ranking and multi-grain
based list ranking to address this problem, and contributed
two high-quality and well-annotated vehicle datasets VD1
and VD2, which are collected from two different cities with
diverse annotated attributes. While Lou et al. [15] resort to
adversarial learning to generate cross views of new exam-
ples.

Person Re-ID. Person re-identiﬁcation aims to retrieve
all the images of the query individual from a large scale
image database. The person re-id methods can be rough-
ly categorized into two groups, classiﬁcation methods and
Siamese methods based on triplet comparisons. Li et al. [7]
proposed a multi-scale context aware network that can cap-
ture knowledge of the local context. Xiao et al. [26] pro-
posed a model to learn deep feature representations from
multiple dataset with Convolutional Neural Networks.Their
experiment shows that some neurons learn representations
shared across all datasets, while some others are effective
only for a speciﬁc domain. Su et al. [22] proposed a pose-
driven convolutional neural network to address the large
pose deformations and the complex view variations prob-
lem. AlignedReID [31] learns a global feature but performs
part alignment during training. local feature is extracted by
horizontal pooling from each row, without requiring addi-
tional supervision or pose estimation.

Discriminative part localization. Discriminative part
localization has been studied for a long time by many com-
munity such as ﬁne-grained recognition [5, 10, 18, 29,
30], face recognition [37, 16, 17, 33, 23] and person re-
identiﬁcation [26]. After deep learning dominate computer
vision community, hand-craft part features for ﬁne-grained
recognition has been drooped. Many works [34, 8] in per-
son re-identiﬁcation exploited human body parts to learn
robust representations. Li et al. [8] proposed to learn and
localize deformable pedestrian parts using Spatial Trans-
former Networks(STN). Using semantic segmentation’ s a-

3998

Figure 2. The pipeline of our framework. Our framework consist of two modules, a local module which focuses on the part features to
distinguish the subtle discrepancy in visual features and a global module which is regularized by the part attentions in the local module.
A part-localization network and a new objective is introduced to encourage correct classiﬁcation of the identiﬁed parts. The LocalNet is a
common object detection network which generates the ROI of each vehicle part. After that, in every local part branch, we project the ROIs
generated by the part localization module into the global feature map. Specially, we only use the global module (in green) to conduct our
inference which is already regularized by the part features in back propagation process.

bility of localizing the various human part precisely under
severe pose variations, Kalayeh et al. [6] exploited human
semantic parsing to harness local visual cues for vehicle
re-identiﬁcation. Fu et al. [3] proposed a weakly super-
vised recurrent attention convolutional neural network to
recursively learn discriminative region attention and region-
based feature representation. Picking deep ﬁlter respons-
es [32] proposed to learn part detectors in an unsupervised
way by analyzing ﬁlter response from deep convolutional
neural network.

3. Methodology

3.1. Problem Statement

Given a query image,

the target of vehicle re-
identiﬁcation is to compute the similarity score between this
query image and all the other images in the gallery. Deﬁne
the training set as {xi, yi}N
i=1. Each vehicle image xi is la-
beled with identiﬁcation label yi with the total number of
N training images. The training Images and identiﬁcation
labels are denoted as x and y respectively. The desired sim-
ilarity between probe p and gallery image g is deﬁned as
M (φ(p; θ), φ(g; θ)), where φ(·; θ) is the feature extraction
function which usually denotes a common deep encoder,
and M (·) is a metric deﬁned in the feature space. The most
important question is how to learn the feature extraction
function φ(·; θ). Previous works use classiﬁcation method

to learn parameters θ in function φ(·; θ), from which the
optimization target can be deﬁned as

arg min

θ

E(cid:0)φ(x; θ)⊤w, y(cid:1) ,

(1)

where φ(x; θ)is the feature extracted by deep neural net-
work with parameter θ, w is the parameter to project the
features into predicted labels. E(·) is the cross entropy
loss. As discussed before, the equations above only opti-
mize the global feature and become easy to ignore subtle
visual cues. To handle this problem, We introduce part in-
formation and propose a novel local feature based optimiza-
tion target which is deﬁned as

arg min

θ

E(cid:0)φ(x; θ)⊤wg, y(cid:1) +

λpE(cid:16)(φ(x; θ) ⊛ Mp)⊤

wl, yp(cid:17) ,

(2)

X

p∈P

where wg is the parameter to project the global feature into
predicted identiﬁcation label. wl is the local parameter that
project the local part feature into predicted part label. Mp
is the part location that can be used to extract local feature
from global feature. ⊛ is the local feature extraction oper-
ation. This formulation introduce the part constrain to the
re-id task and force the network preserve the local part cue
to recognize parts. Details will be explained in section 3.2.

3999

There are still some unsolved problems in Eq. (2). First,
the part set P is not deﬁned which means we don’t know
which part should be used. Second, The part location Mp
need to be extracted. Third, yp, which is the part label,
should be determined. In the next subsection, we will ex-
plain our network structure to address these problems.

3.2. Part Regularized Re ID

In this section, we introduce part regularized (PR) con-
strains into the vehicle re-identiﬁcation task. Our frame-
work consists of two components, a global module to con-
duct Re-ID categorization and a local part-regularized mod-
ule to encourage correct classiﬁcation of the identiﬁed parts.
To preserve better context information, which is very cru-
cial for the near-duplicate problem, we adopt bounding box
detection network for part localization. We will explain the
details of the two main components in this section and de-
scribe training scheme in section 3.3.

Part deﬁnition. We select three vehicle parts for our
part detection module, lights, including front light and back
light, window, including front window and back window,
and vehicle brand. The vehicle head area is crucial to dis-
tinguish different vehicle model. we use the front lights to
inference the vehicle head area including the brand. Differ-
ent model may have extremely difﬁdence lights, we deﬁne
bounding box of the light as tight bounding box contains the
light but extend it to the bottom of the vehicle. This deﬁni-
tion can preserve more context information which we ﬁnd
more stable in experiment. The deﬁnition of the three parts
in our model is shown in Fig. 3. We draw N local branches
in Fig. 2 since our framework is ﬂexible to various deﬁni-
tions of vehicle parts, and we only test N = 3 parts (window,
light, brand) to validate the effectiveness of this framework.

Part detection. To solve the second problem, we need
to ﬁnd the parts location of the training images. There are
many off the shelf object part localization algorithms, which
can mainly categorize into two classes, detection and seg-
mentation. Segmentation method need pixel level annota-
tion which is difﬁcult to get. In this paper we use a detection
branch to detect the predeﬁned vehicle parts. As shown in
the Fig. 2, raw vehicle image is fed into the LocalN et (use
YOLO in experiments), which has 24 convolutional layer-
s, to get raw part detection results. A desired result is that
every image get three bounding box for window, left light
and right light respectively. During the training process, we
ﬁnd that in some rare cases the vehicle part detection model
may fail due to occlusion. To handle these invisible parts
in a speciﬁc vehicle image, we refer to the rest images of
the same vehicle and compute the average locations of the
missing parts. After that, these average part locations are
used as the pseudo detection results of this speciﬁc image
to facilitate the subsequent training process.

Figure 3. The part deﬁnition of our model. The ﬁrst row shows the
vehicle window part in both front and back view. Vehicle lights are
shown in the second row. We extend the bounding box of the lights
to the bottom of the vehicle to preserve more context information.
The head and rear area of the vehicle containing the vehicle brand
is deﬁned as vehicle brand part.

Part-based Feature Extraction and Aggregation. Our
part-based feature extraction and aggregation module has
one global branch and three local part branch. All four
branches share the same backbone network, any convolu-
tional backbone can be used here, we use ResNet-50 [4] in
this paper. All input image were re-sized into H×W to gen-
erate feature map with shape S × S × C. The global branch
simply use global average pooling to generate the global
feature vector. In every local part branch, we project the
ROI generated by the part localization module into the glob-
al feature map. We divided the input image into an S × S
grid where S ×S equals the spatial size of the global feature
map(S ×S). Every grid cell overlapped with the ROI would
be marked as that part corresponding to the ROI. After that
local part feature vector will be extracted use local average
pooling.

Now we have part set P and part localization Mp in E-
q. (2), we need to deﬁne the part label yp to train the net-
work. However part label is very difﬁcult to obtained. For
example, the brand part label may be set as the name of ve-
hicle manufacturer, since all the brand from the same man-
ufacturer should be same. The window part, on the other
hand, contains personalize cue of a speciﬁc vehicle, so it
should labeled with that speciﬁc vehicle identity. Consid-
ering vehicle model and vehicle make information is not
available in some scenario, we propose to use vehicle iden-
tiﬁcation label to approximate the part label, Eq. (2) can be

4000

(a)

(b)

Figure 4. Visualization of the part detection module. (a) shows the
detection result of the light in different viewpoint, notice that in
some image light is invisible and can not be detect. (b) shows the
detection result of the vehicle window.

modiﬁed as

arg min

θ

E(cid:0)φ(x; θ)⊤wg, y(cid:1) +

λpE(cid:16)(φ(x; θ) ⊛ Mp)⊤

wl, y(cid:17) ,

(3)

X

p∈P

where y ≈ yp, now we can use vehicle identiﬁcation label
to optimize our model.

3.3. Training Scheme

Both of our part localization module and part feature
extraction and aggregation module can be trained end to
end using backpropagation. We adopt the successful Y-
OLO network [19] as our backbone of LocalN et. In train-
ing steps, ﬁrst we train the part detection module and ex-
tract all the part locations of the training images. Part in-
formation of the test images was not extracted since we
don’t use local feature branch at test stage. For VehicleI-
D and VeRi-776, we adopt the transfer learning scheme
and use the ImageNet pretrained weights for backbone net-
work GlobalN et(ResNet-50). Then we use the optimiza-
tion function deﬁned in Eq. (3) with a initial learning rate
lr = 0.01 with exponential learning rate schedule to ﬁne-
tune the whole feature extraction module, including global
and local branch.

4. Experiment

4.1. Datasets and Evaluation Metric

We evaluate our proposed model on two public large-
scale vehicle re-identiﬁcation datasets, VehicleID and VeRi-
776.

VeRi-776 is a benchmark dataset for vehicle re-id task. It
contains about 50,000 images of 776 vehicles labeled with
rich attributes, e.g. types, colors, brands, license plate anno-
tation and spatiotemporal relation annotation. Each vehicle
was captured by various cameras with different view points.
The short coming of this dataset is that the number of iden-
tities is relatively small, in test stage it is very easy to dis-
tinguish each vehicle just based on model information. We
use the ofﬁcial dataset settings and adopt mAP, HIT-1 and
HIT-5 to evaluate our proposed model.

VehicleID is another benchmark with larger data volume.
VehicleID is captured by multiple non-overlapping cameras
and there are 221,763 images of 26,267 vehicles in total.
Each image is either captured from the front view or back
view. In VehicleID, only 250 vehicle models are included,
which means many different identities share same vehicle
model, near-duplicate problem appears. We use mAP to
evaluate our method on three subset(i.e. small, medium and
large) of the testset.

There are no bounding box annotations of the vehicle
parts in both the VehicleID and VeRi-776 dataset. There-
fore, we randomly select 500 vehicle images from the Ve-
hicleID dataset and label three vehicle parts with bounding
boxes (window, light and brand), and these images are used
to train the YOLO model. The trained model shows im-
pressive detection results on both VehicleID and VeRi-776
dataset, implying a good generalization ability. The annota-
tion process is also quite efﬁcient and costs only 4 hours of
one person in annotating all the 500 images.

The mean average precision (mAP) and cumulative
match curve (CMC) are adopted in our experiments. For
VeRi-776, the image-to-track metric HIT@1 and HIT@5 is
also reported. The CMC curve shows the probability that
the image of the probe identity appears in different-sized
retrieved list. CMC can be calculated as

CM C@k = PN

i=1 m(qi, k)

N

,

(4)

where N is the number of queries and m(qi, k) equals to 1
if qi appears in the top-k of the rank list. The number of
ground truth image of a probe should be exactly 1 in order
to use the cumulative match curve. The precision measures
the accurate of the prediction, the average precision for each
query q can be calculated as

AP (q) =

N

X

k=1

P (k)∆r(k),

(5)

4001

where P (k) is the precision at a cutoff of k images, N is
the total number of images in the gallery, and ∆r(k)is the
change in recall that happened between cutoff k − 1 and
cutoff k. The mean average precision for all query images
is determined by

mAP = PN

q=1 AP (q)

Q

,

(6)

where Q is the total number of queries.

4.2. Experiment Setup

We use ResNet-50 as the backbone network for feature
extraction. We apply average global pooling[11] on the
global feature map followed with a 1 × 1 convolutional lay-
er to extract the ﬁnal 256-d global feature vector. Euclidean
distance (L2) was adopted to compute similarity score be-
tween query and gallery images at both training and testing
stage. It is worth mentioning that we only use global branch
at test stage because during the experiment we found that
fusing global and local part features yields similar perfor-
mance compared to just using global branch, Which mean
our model does not need part detection at test stage.

4.3. Comparison with State of the art

The proposed method is compared with state-of-the-art

vehicle re-identiﬁcation methods on two datasets.

VehicleID. For VehicleID dataset, testing data is split in-
to three subsets ordered by their size. For each test dataset
split, one image of each vehicle identity is selected and
putted into the gallery set. The rest images are all probe
queries. In this setting each vehicle identity has many query
images but have only one gallery images, so the cumula-
tive match curve (CMC) metric is adopted for evaluation.
Table 1 and Table 2 show performance comparisons on
VehicleID. Our model outperform all the existing method.
OIFE [24] and VAMI [36] exploit the vehicle view infor-
mation use the view invariant feature to roughly alight the
vehicle image. Those view align methods are useful when
distinguish different vehicles from difference vehicle mod-
el, but they can’t address the near-duplicate problem since
appearance of same viewpoint of the near-duplicate vehi-
cles are still fairly similar.
It need more detail cues than
vehicle view informations to distinguish the near-duplicate
vehicle.

VeRi-776. The cross-camera search is performed fol-
lowed the ofﬁcial settings in [14]. At test stage, each im-
age of a vehicle from every camera is selected as probe
image and used to search for tracks of the same vehi-
cle in other cameras. That means evaluation for VeRi-
776 is conducted in an image-to-track fashion, in which
the probe is an image, while the targets are images in
the track. The problem is how to deﬁne the similari-

Figure 5. Class activation map (CAM) generated by identiﬁcation
classiﬁcation model. CAMs with (part-regularized) PR method
are show in the ﬁrst and third rows while NoPR are in the second
and forth rows. Activation maps with PR can easily distinguish
different cars by the accurate part information for near-duplicated
vehicles while NoPR models are usually get confused. It is worth
mentioning that the activation map without RP can attend to vehi-
cle light or brand parts originally.

Table 1. Result of CMC@1 in VehicleID Dataset.

Method

Small Medium Large

VGG+Triplet Loss [2]
VGG+CCL [12]
Mixed Diff+CCL [12]
OIFE [24]
VAMI [36]

0.404
0.436
0.490

-

0.354
0.370
0.428

-

0.631

0.529

0.319
0.329
0.382
0.670
0.473

Ours

0.784

0.750

0.742

ty between a query image and a gallery track. Follow-
ing the settings in [14], the similarity is deﬁned as maxi-
mum similarity between a query image and all images in
the track. The image-to-track evaluation results is shown
in Table 3. Fact+Plate+STR [14], Siamese+Path [21] and
OIFE+ST [24] relies on the spatil-temporal information in
Veri-776 Dataset. Fact+Plate+STR [14] uses the addition-
al license plate informations. Other methods only rely on
the visual information including ours. Our Part-regularized
model outperform all the existing method on mAP metric

4002

(a)

(b)

(c)

Figure 6. Rank list visualization. The ﬁrst image with green border in each row is the query image , the rest images are retrieved from the
gallery and sorted by similarity score (L2 distance). The ground-truth is marked with red border. (a) rank list result of our full model with
all three part branch. The ﬁrst image in each raw is the query, and the rest ten images are the top ten retrieval results. (b) rank list result
after removing the window branch. (c) rank list result after removing the lights and brand branches.

Table 2. Result of CMC@5 in VehicleID Dataset.

Table 3. Results of mAP and HIT@1 HIT@5 in VeRi-776 Dataset.

Method

Small Medium Large

Method

mAP HIT@1 HIT@5

VGG+Triplet Loss [2]
VGG+CCL [12]
Mixed Diff+CCL [12]
OIFE [24]
VAMI [36]

0.617
0.642
0.735

-

0.546
0.571
0.668

-

0.833

0.751

0.503
0.533
0.616
0.829
0.703

Ours

0.923

0.883

0.864

including those who use extra none-visual cues.

4.4. Ablation Study

We conduct ablation study on VehicleID dataset to in-
vestigate the effeteness of each part branch in our model.
There are three local part branch in our framework, win-
dow branch, light branch and brand branch. We remove one
branch at a time and retrain the whole network to evaluate
the performance. Rank list visualization is also performed

BOW-CN [35]
LOMO [9]
GoogLeNet [28]
FACT [13]
Plate-SNN [14]
FACT+Plate-REC [14]
FACT+Plate-SNN [14]
FACT+Plate+STR [14]
Siamese+Path [21]
OIFE [24]
OIFE+ST [24]
VAMI [36]

0.122
0.096
0.170
0.185
0.157
0.186
0.259
0.278
0.583
0.480
0.514
0.501

0.339
0.253
0.498
0.510
0.363
0.512
0.611
0.614
0.835
0.894
0.924

-

0.537
0.465
0.712
0.735
0.466
0.736
0.774
0.788
0.900

-
-
-

Ours

0.743

0.943

0.987

as shown in Fig. 6.

4003

Table 4. Results of Match Rate of ablation experiment.

Table 6. Inﬂuences of different resolutions in VeRi-776 dataset.

Method

CMC@1 CMC@5

Input size

mAP

HIT@1

HIT@5

Global+Light+Brand+Window
Global+Light+Brand
Global+Light+Window
Global+Window+Brand
Global+Window
Window+Light+Brand

Baseline (w/o parts)

0.742
0.675
0.710
0.726
0.707
0.687

0.645

0.864
0.830
0.887
0.851
0.832
0.829

0.800

Table 5. Inﬂuences of different resolutions in VehicleID dataset.

VehicleSet

Input size

CMC@1 CMC@5

Small

Medium

Large

128 × 128
256 × 256
128 × 128
256 × 256
128 × 128
256 × 256

0.726
0.784
0.685
0.750
0.661
0.742

0.886
0.923
0.838
0.883
0.819
0.864

Vehicle window. As shown in Table 4, cutting off the
vehicle window branch depress the re-id performance by 7
percent. Vehicle window contains the personality feature
which is crucial to distinguish difference vehicle identities
from same vehicle model. The visualization result conﬁrm-
s this point. As shown in Fig 6, almost all of the top 10
retrieval results are come from the same vehicle model. In
this scenario visual cues from vehicle window become ex-
tremely important since others vehicle parts are almost the
same.

Vehicle brand and light. Removing the vehicle brand
or vehicle light branch also depress the performance of our
model. Compared to cutting off the vehicle window branch,
removing the vehicle light and brand only yields a smaller
performance drop. This is because the global feature can
learn some of the vehicle light and brand information orig-
inally as discussed before in Fig 5. The performance drop
shows that putting explicit constrains to the neural network
makes the learning process more efﬁcient.

Global branch. We cutting off the global branch and
only use three part branch to train the network. During test-
ing three part feature vector is extracted and fusing together
to computer the similarity score. The performance drops
a lot unsurprisingly. The other part like vehicle body and
wheels are useful when distinguish two vehicle identities.
The global branch is response to extract those descrimina-
tive information.

Inﬂuences of resolution. We conduct experiments on
different resolutions of input size, as shown in Tab. 5 and
6. For VehicleID dataset, we conduct experiments on three
testset with different image resolutions. One intuitive obser-
vation is that images with higher resolution performs better

128 × 128
256 × 256
512 × 512

0.653
0.702
0.743

0.878
0.922
0.943

0.959
0.979
0.987

but with a higher computation cost. Interestingly, we ﬁnd
that images with size 128 × 128 exhibit large performance
drop especially for CMC@1 indicator, while for CMC@5
and HIT@5, images with low resolution yield feasible re-
sults.

5. Conclusions

In this paper, we explore the near-duplicate challenge
which causes one of the most remarkable confusions in ve-
hicle re-identiﬁcation tasks. To enlarge the divergences be-
tween nearly identical instances, we proposed a simple but
efﬁcient part-regularized approach which enhances the lo-
cal features in the original Re-ID task. Our model intro-
duces part level constrains to the typical Re-ID framework
to enhance the perceptive of subtle discrepancies, which is
crucial for the near-duplicate vehicle Re-ID, not being ig-
nored during forward propagation and the detection ROIs
on feature maps is the best practice to facilitate the local
visual cues. We also conduct qualities and quantities exper-
iments to demonstrate the effectiveness of each branch in
our framework.

Acknowledgments

This work was supported in part by the National Basic
Research Program of China under Grant 2015CB351806,
in part by the National Natural Science Foundation of Chi-
na under Grant 61672072, Grant 61532003, and Grant
61825101, and in part by Beijing Nova Program under
Grant Z181100006218063.

References

[1] Y. Bai, Y. Lou, F. Gao, S. Wang, Y. Wu, and L. Duan. Group
sensitive triplet embedding for vehicle re-identiﬁcation. TM-
M, 2018.

[2] S. Ding, L. Lin, G. Wang, and H. Chao. Deep feature
learning with relative distance comparison for person re-
identiﬁcation. PR, 48(10), 2015.

[3] J. Fu, H. Zheng, and T. Mei. Look closer to see better: Recur-
rent attention convolutional neural network for ﬁne-grained
image recognition. In CVPR, volume 2, 2017.

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016.

[5] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn for

ﬁne-grained visual categorization. In CVPR, 2016.

4004

[6] M. M. Kalayeh, E. Basaran, M. G¨okmen, M. E. Kamasak,
and M. Shah. Human semantic parsing for person re-
identiﬁcation. In CVPR, 2018.

[25] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 10(Feb), 2009.

[7] D. Li, X. Chen, Z. Zhang, and K. Huang. Learning deep
context-aware features over body and latent parts for person
re-identiﬁcation. In CVPR, 2017.

[26] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep fea-
ture representations with domain guided dropout for person
re-identiﬁcation. In CVPR, 2016.

[8] D. Li, X. Chen, Z. Zhang, and K. Huang. Learning deep
context-aware features over body and latent parts for person
re-identiﬁcation. In CVPR, 2017.

[27] K. Yan, Y. Tian, Y. Wang, W. Zeng, and T. Huang. Exploit-
ing multi-grain ranking constraints for precisely searching
visually-similar vehicles. In ICCV, 2017.

[9] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identiﬁcation
by local maximal occurrence representation and metric
learning. In CVPR, 2015.

[28] L. Yang, P. Luo, C. Change Loy, and X. Tang. A large-scale
car dataset for ﬁne-grained categorization and veriﬁcation.
In CVPR, 2015.

[10] D. Lin, X. Shen, C. Lu, and J. Jia. Deep lac: Deep local-
ization, alignment and classiﬁcation for ﬁne-grained recog-
nition. In CVPR, 2015.

[11] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv

preprint arXiv:1312.4400, 2013.

[12] H. Liu, Y. Tian, Y. Yang, L. Pang, and T. Huang. Deep rel-
ative distance learning: Tell the difference between similar
vehicles. In CVPR, 2016.

[13] X. Liu, W. Liu, H. Ma, and H. Fu. Large-scale vehicle re-
identiﬁcation in urban surveillance videos.
In Multimedia
and Expo (ICME), 2016 IEEE International Conference on.
IEEE, 2016.

[14] X. Liu, W. Liu, T. Mei, and H. Ma. A deep learning-based
approach to progressive vehicle re-identiﬁcation for urban
surveillance. In ECCV. Springer, 2016.

[15] Y. Lou, Y. Bai, J. Liu, S. Wang, and L.-Y. Duan. Embed-
ding adversarial learning for vehicle re-identiﬁcation. IEEE
Transactions on Image Processing, 2019.

[16] P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via

deep learning. In CVPR. IEEE, 2012.

[17] J.-J. Lv, X. Shao, J. Xing, C. Cheng, X. Zhou, et al. A
deep regression architecture with two-stage re-initialization
for high performance facial landmark detection. In CVPR,
volume 1, 2017.

[29] H. Zhang, T. Xu, M. Elhoseiny, X. Huang, S. Zhang, A. El-
gammal, and D. Metaxas. Spda-cnn: Unifying semantic part
detection and abstraction for ﬁne-grained recognition.
In
CVPR, 2016.

[30] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-
based r-cnns for ﬁne-grained category detection. In ECCV.
Springer, 2014.

[31] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao,
W. Jiang, C. Zhang, and J. Sun. Alignedreid: Surpassing
human-level performance in person re-identiﬁcation. arXiv
preprint arXiv:1711.08184, 2017.

[32] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian. Picking
deep ﬁlter responses for ﬁne-grained image recognition. In
CVPR, 2016.

[33] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Facial landmark
detection by deep multi-task learning. In ECCV. Springer,
2014.

[34] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang,
and X. Tang. Spindle net: Person re-identiﬁcation with hu-
man body region guided feature decomposition and fusion.
In CVPR, 2017.

[35] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.
Scalable person re-identiﬁcation: A benchmark. In CVPR,
2015.

[18] O. M. Parkhi, A. Vedaldi, C. Jawahar, and A. Zisserman. The

[36] Y. Zhou and L. Shao. Aware attentive multi-view inference

truth about cats and dogs. In ICCV. IEEE, 2011.

for vehicle re-identiﬁcation. In CVPR, 2018.

[37] S. Zhu, C. Li, C. Change Loy, and X. Tang. Face alignment

by coarse-to-ﬁne shape searching. In CVPR, 2015.

[19] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You on-
ly look once: Uniﬁed, real-time object detection. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 779–788, 2016.

[20] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
2015.

[21] Y. Shen, T. Xiao, H. Li, S. Yi, and X. Wang. Learning deep
neural networks for vehicle re-id with visual-spatio-temporal
path proposals. In ICCV. IEEE, 2017.

[22] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, and Q. Tian. Pose-
driven deep convolutional model for person re-identiﬁcation.
In ICCV. IEEE, 2017.

[23] Y. Sun, X. Wang, and X. Tang. Deep convolutional network

cascade for facial point detection. In CVPR, 2013.

[24] Z. Wang, L. Tang, X. Liu, Z. Yao, S. Yi, J. Shao, J. Yan,
S. Wang, H. Li, and X. Wang. Orientation invariant feature
embedding and spatial temporal regularization for vehicle re-
identiﬁcation. In CVPR, 2017.

4005

