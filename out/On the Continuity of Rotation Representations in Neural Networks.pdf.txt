On the Continuity of Rotation Representations in Neural Networks

Yi Zhou∗

University of Southern California

Connelly Barnes∗
Adobe Research

zhou859@usc.edu

connellybarnes@yahoo.com

Jingwan Lu

Adobe Research
jlu@adobe.com

Jimei Yang

Hao Li

Adobe Research

University of Southern California, Pinscreen

jimyang@adobe.com

USC Institute for Creative Technologies

hao@hao-li.com

Abstract

In neural networks, it is often desirable to work with var-
ious representations of the same space. For example, 3D
rotations can be represented with quaternions or Euler an-
gles. In this paper, we advance a deﬁnition of a continuous
representation, which can be helpful for training deep neu-
ral networks. We relate this to topological concepts such as
homeomorphism and embedding. We then investigate what
are continuous and discontinuous representations for 2D,
3D, and n-dimensional rotations. We demonstrate that for
3D rotations, all representations are discontinuous in the
real Euclidean spaces of four or fewer dimensions. Thus,
widely used representations such as quaternions and Eu-
ler angles are discontinuous and difﬁcult for neural net-
works to learn. We show that the 3D rotations have con-
tinuous representations in 5D and 6D, which are more suit-
able for learning. We also present continuous represen-
tations for the general case of the n dimensional rotation
group SO(n). While our main focus is on rotations, we also
show that our constructions apply to other groups such as
the orthogonal group and similarity transforms. We ﬁnally
present empirical results, which show that our continuous
rotation representations outperform discontinuous ones for
several practical problems in graphics and vision, includ-
ing a simple autoencoder sanity test, a rotation estimator
for 3D point clouds, and an inverse kinematics solver for
3D human poses.

1. Introduction

Recently, there has been an increasing number of appli-
cations in graphics and vision, where deep neural networks
are used to perform regressions on rotations. This has been
done for tasks such as pose estimation from images [13, 31]
and from point clouds [15], structure from motion [29], and
skeleton motion synthesis, which generates the rotations of

∗Authors have equal contribution.

joints in skeletons [30]. Many of these works represent 3D
rotations using 3D or 4D representations such as quater-
nions, axis-angles, or Euler angles.

However, for 3D rotations, we found that 3D and 4D rep-
resentations are not ideal for network regression, when the
full rotation space is required. Empirically, the converged
networks still produce large errors at certain rotation an-
gles. We believe that this actually points to deeper topolog-
ical problems related to the continuity in the rotation rep-
resentations. Informally, all else being equal, discontinuous
representations should in many cases be “harder” to approx-
imate by neural networks than continuous ones. Theoreti-
cal results suggest that functions that are smoother [33] or
have stronger continuity properties such as in the modulus
of continuity [32, 10] have lower approximation error for a
given number of neurons.

Based on this insight, we ﬁrst present in Section 3 our
deﬁnition of the continuity of representation in neural net-
works. We illustrate this deﬁnition based on a simple exam-
ple of 2D rotations. We then connect it to key topological
concepts such as homeomorphism and embedding.

Next, we present in Section 4 a theoretical analysis of the
continuity of rotation representations. We ﬁrst investigate in
Section 4.1 some discontinuous representations, such as Eu-
ler angle and quaternion representations. We show that for
3D rotations, all representations are discontinuous in four or
fewer dimensional real Euclidean space with the Euclidean
topology. We then investigate in Section 4.2 some continu-
ous rotation representations. For the n dimensional rotation
group SO(n), we present a continuous n2 − n dimensional
representation. We additionally present an option to reduce
the dimensionality of this representation by an additional 1
to n − 2 dimensions in a continuous way. We show that
these allow us to represent 3D rotations continuously in 6D
and 5D. While we focus on rotations, we show how our con-
tinuous representations can also apply to other groups such
as orthogonal groups O(n) and similarity transforms.

Finally, in Section 5 we test our ideas empirically. We
conduct experiments on 3D rotations and show that our
6D and 5D continuous representations always outperform

15745

the discontinuous ones for several tasks, including a rota-
tion autoencoder “sanity test,” rotation estimation for 3D
point clouds, and 3D human pose inverse kinematics learn-
ing. We note that in our rotation autoencoder experiments,
discontinuous representations can have up to 6 to 14 times
higher mean errors than continuous representations. Fur-
thermore they tend to converge much slower while still pro-
ducing large errors over 170◦ at certain rotation angles even
after convergence, which we believe are due to the discon-
tinuities being harder to ﬁt. This phenomenon can also
be observed in the experiments on different rotation repre-
sentations for homeomorphic variational auto-encoding in
Falorsi et al. [14], and in practical applications, such as 6D
object pose estimation in Xiang et al. [31].

We also show that one can perform direct regression on
3x3 rotation matrices. Empirically this approach introduces
larger errors than our 6D representation as shown in Sec-
tion 5.2. Additionally, for some applications such as inverse
and forward kinematics, it may be important for the network
itself to produce orthogonal matrices. We therefore require
an orthogonalization procedure in the network. In particu-
lar, if we use a Gram-Schmidt orthogonalization, we then
effectively end up with our 6D representation.

Our contributions are: 1) a deﬁnition of continuity for
rotation representations, which is suitable for neural net-
works; 2) an analysis of discontinuous and continuous rep-
resentations for 2D, 3D, and n-D rotations; 3) new formulas
for continuous representations of SO(3) and SO(n); 4) em-
pirical results supporting our theoretical views and that our
continuous representations are more suitable for learning.

2. Related Work

In this section, we will ﬁrst establish some context for
our work in terms of neural network approximation theory.
Next, we discuss related works that investigate the continu-
ity properties of different rotation representations. Finally,
we will report the types of rotation representations used in
previous learning tasks and their performance.

Neural network approximation theory. We review a
brief sampling of results from neural network approxima-
tion theory. Hornik [17] showed that neural networks can
approximate functions in the Lp space to arbitrary accuracy
if the Lp norm is used. Barron et al. [6] showed that if a
function has certain properties in its Fourier transform, then
at most O(ǫ−2) neurons are needed to obtain an order of ap-
proximation ǫ. Chapter 6.4.1 of LeCun et al. [23] provides
a more thorough overview of such results. We note that
results for continuous functions indicate that functions that
have better smoothness properties can have lower approxi-
mation error for a particular number of neurons [32, 10, 33].
For discontinuous functions, Llanas et al. [24] showed that
a real and piecewise continuous function can be approx-
imated in an almost uniform way. However, Llanas et
al. [24] also noted that piecewise continuous functions when
trained with gradient descent methods require many neu-
rons and training iterations, and yet do not give very good
results. These results suggest that continuous rotation rep-
resentations might perform better in practice.

Continuity for rotations. Grassia et al. [16] pointed out
that Euler angles and quaternions are not suitable for ori-
entation differentiation and integration operations and pro-
posed exponential map as a more robust rotation represen-
tation. Saxena et al. [28] observed that the Euler angles and
quaternions cause learning problems due to discontinuities.
However, they did not propose general rotation representa-
tions other than direct regression of 3x3 matrices, since they
focus on learning representations for objects with speciﬁc
symmetries.

Neural networks for 3D shape pose estimation. Deep
networks have been applied to estimate the 6D poses of ob-
ject instances from RGB images, depth maps or scanned
point clouds.
Instead of directly predicting 3x3 matri-
ces that may not correspond to valid rotations, they typ-
ically use more compact rotation representations such as
quaternion [31, 21, 20] or axis-angle [29, 15, 13].
In
PoseCNN [31], the authors reported a high percentage of
errors between 90◦ and 180◦, and suggested that this is
mainly caused by the rotation ambiguity for some symmet-
ric shapes in the test set. However, as illustrated in their
paper, the proportion of errors between 90◦ to 180◦ is still
high even for non-symmetric shapes. In this paper, we ar-
gue that discontinuity in these representations could be one
cause of such errors.

Neural networks for inverse kinematics. Recently, re-
searchers have been interested in training neural networks
to solve inverse kinematics equations. This is because such
networks are faster than traditional methods and differen-
tiable so that they can be used in more complex learning
tasks such as motion re-targeting [30] and video-based hu-
man pose estimation [19]. Most of these works represented
rotations using quaternions or axis-angle [18, 19]. Some
works also used other 3D representations such as Euler an-
gles and Lie algebra [19, 34], and penalized the joint posi-
tion errors. Csiszar et al. [11] designed networks to output
the sine and cosine of the Euler angles for solving the in-
verse kinematics problems in robotic control. Euler angle
representations are discontinuous for SO(3) and can result
in large regression errors as shown in the empirical test in
Section 5. However, those authors limited the rotation an-
gles to be within a certain range, which avoided the discon-
tinuity points and thus achieved very low joint alignment
errors in their test. However, many real-world tasks require
the networks to be able to output the full range of rotations.
In such cases, continuous rotation representations will be a
better choice.

3. Deﬁnition of Continuous Representation

In this section, we begin by deﬁning the terminology we
will use in the paper. Next, we analyze a simple motivat-
ing example of 2D rotations. This allows us to develop our
general deﬁnition of continuity of representation in neural
networks. We then explain how this deﬁnition of continuity
is related to concepts in topology.

Terminology. To denote a matrix, we typically use M ,
and Mij refers to its (i, j) entry. We use the term SO(n) to
denote the special orthogonal group, the space of n dimen-

25746

Representation Space

Original Space

Mapping g

0

2π

Disconnected Set of Angular 

Representations in [0, 2π]

Connected Set

of Rotations in S1

Figure 1. A simple 2D example, which motivates our deﬁnition of
continuity of representation. See Section 3 for details.

Figure 2. Our deﬁnition of continuous representation, as well as
how it can apply in a neural network. See the body for details.

sional rotations. This group is deﬁned on the set of n × n
real matrices with M M T = M T M = I and det(M ) = 1.
The group operation is multiplication, which results in the
concatenation of rotations. We denote the n dimensional
unit sphere as Sn = {x ∈ Rn+1 : ||x|| = 1}.

Motivating example: 2D rotations. We now consider
the representation of 2D rotations. For any 2D rotation M ∈
SO(2), we can also express the matrix as:

cos(θ) (cid:21)
M = (cid:20) cos(θ) − sin(θ)

sin(θ)

(1)

We can represent any rotation matrix M ∈ SO(2) by
choosing θ ∈ R, where R is a suitable set of angles, for ex-
ample, R = [0, 2π]. However, this particular representation
intuitively has a problem with continuity. The problem is
that if we deﬁne a mapping g from the original space SO(2)
to the angular representation space R, then this mapping is
discontinuous. In particular, the limit of g at the identity ma-
trix, which represents zero rotation, is undeﬁned: one direc-
tional limit gives an angle of 0 and the other gives 2π. We
depict this problem visually in Figure 1. On the right, we
visualize a connected set of rotations C ⊂ SO(2) by visual-
izing their ﬁrst column vector [cos(θ), sin(θ)]T on the unit
sphere S1. On the left, after mapping them through g, we
see that the angles are disconnected. In particular, we say
that this representation is discontinuous because the map-
ping g from the original space to the representation space
is discontinuous. We argue that these kind of discontinu-
ous representations can be harder for neural networks to ﬁt.
Contrarily, if we represent the 2D rotation M ∈ SO(2) by
its ﬁrst column vector [cos(θ), sin(θ)]T , then the represen-
tation would be continuous.

Continuous representation: We can now deﬁne what
we consider a continuous representation. We illustrate our
deﬁnitions graphically in Figure 2. Let R be a subset of
a real vector space equipped with the Euclidean topology.
We call R the representation space: in our context, a neu-
ral network produces an intermediate representation in R.
This neural network is depicted on the left side of Figure 2.
We will come back to this neural network shortly. Let X
be a compact topological space. We call X the original

In our context, any intermediate representation in
space.
R produced by the network can be mapped into the orig-
inal space X. Deﬁne the mapping to the original space
f : R → X, and the mapping to the representation space
g : X → R. We say (f, g) is a representation if for every
x ∈ X, f (g(x)) = x, that is, f is a left inverse of g. We say
the representation is continuous if g is continuous.

Connection with neural networks: We now return to
the neural network on the left side of Figure 2. We imag-
ine that inference runs from left to right. Thus, the neural
network accepts some input signals on its left hand side,
outputs a representation in R, and then passes this repre-
sentation through the mapping f to get an element of the
original space X. Note that in our context, the mapping f
is implemented as a mathematical function that is used as
part of the forward pass of the network at both training and
inference time. Typically, at training time, we might im-
pose losses on the original space X. We now describe the
intuition behind why we ask that g be continuous. Suppose
that we have some connected set C in the original space,
such as the one shown on the right side of Figure 1. Then if
we map C into representation space R, and g is continuous,
then the set g(C) will remain connected. Thus, if we have
continuous training data, then this will effectively create a
continuous training signal for the neural network. Contrar-
ily, if g is not continuous, as shown in Figure 1, then a con-
nected set in the original space may become disconnected
in the representation space. This could create a discontinu-
ous training signal for the network. We note that the units
in the neural network are typically continuous, as deﬁned
on Euclidean topology spaces. Thus, we require the repre-
sentation space R to have Euclidean topology because this
is consistent with the continuity of the network units.

Domain of the mapping f : We additionally note that
for neural networks, it is speciﬁcally beneﬁcial for the map-
ping f to be deﬁned almost everywhere on a set where the
neural network outputs are expected to lie. This enables f
to map arbitrary representations produced by the network
back to the original space X.

Connection with topology: Suppose that (f, g) is a con-
tinuous representation. Note that g is a continuous one-to-
one function from a compact topological space to a Haus-
dorff space. From a theorem in topology [22], this implies
that if we restrict the codomain of g to g(X) (and use the
subspace topology for g(X)) then the resulting mapping is
a homeomorphism. A homeomorphism is a continuous bi-
jection with a continuous inverse. For geometric intuition,
a homeomorphism is often described as a continuous and
invertible stretching and bending of one space to another,
with also a ﬁnite number of cuts allowed if one later glues
back together the same points. One says that two spaces are
topologically equivalent if there is a homeomorphism be-
tween them. Additionally, g is a topological embedding of
the original space X into the representation space R. Note
that we also have the inverse of g: if we restrict f to the
domain g(X) then the resulting function f |g(X) is simply
the inverse of g. Conversely, if the original space X is not
homeomorphic to any subset of the representation space R

35747

Representation Space RInput signalOriginal SpaceXMapping fNeural NetworkMapping gthen there is no possible continuous representation (f, g) on
these spaces. We will return to this later when we show that
there is no continuous representation for the 3D rotations in
four or fewer dimensions.

4. Rotation Representation Analysis

Here we provide examples of rotation representations
that could be used in networks. We start by looking in
Section 4.1 at some discontinuous representations for 3D
rotations, then look in Section 4.2 at continuous rotation
representations in n dimensions, and show that how for the
3D rotations, these become 6D and 5D continuous rotation
representations. We believe this analysis can help one to
choose suitable rotation representations for learning tasks.

4.1. Discontinuous Representations

Case 1: Euler angle representation for the 3D rota-
tions. Let the original space X = SO(3), the set of 3D
rotations. Then we can easily show discontinuity in an Eu-
ler angle representation by considering the azimuth angle
θ and reducing this to the motivating example for 2D rota-
tions shown in Section 3. In particular, the identity rotation
I occurs at a discontinuity, where one directional limit gives
θ = 0 and the other directional limit gives θ = 2π. We vi-
sualize the discontinuities in this representation, and all the
other representations, in the supplemental Section F.

Case 2: Quaternion representation for the 3D rota-
tions. Deﬁne the original space X = SO(3), and the rep-
resentation space Y = R4, which we use to represent the
quaternions. We can now deﬁne the mapping to the repre-
sentation space gq(M ) =




h M32 − M23, M13 − M31, M21 − M12, tiT
h√M11 + 1, c2√M22 + 1, c3√M33 + 1, 0iT
t = Tr(M ) + 1, ci =(1
−1

if Mi,1 + Mi,2 > 0
otherwise

if t 6= 0
if t = 0

(2)

(3)

Likewise, one can deﬁne the mapping to the original

space SO(3) as in [2]:

fq ([x0, y0, z0, w0]) =

1 − 2y2 − 2z2,
2xy + 2zw,
2xz − 2yw,




2xy − 2zw,
1 − 2x2 − 2z2,
2yz + 2xw,

2xz + 2yw
2yz − 2xw

1 − 2x2 − 2y2 
 ,

(x, y, z, w) = N ([x0, y0, z0, w0])

(4)

Here the normalization function is deﬁned as N (q) =
q/||q||. By expanding in terms of the axis-angle repre-
sentation for the matrix M , one can verify that for every
M ∈ SO(3), f (g(M )) = M.

However, we ﬁnd that the representation is not contin-
uous. Geometrically, this can be seen by taking differ-
ent directional limits around the matrices with 180 degree
rotations, which are deﬁned by Rπ = {M ∈ SO(3) :
Tr(M ) = −1}. Speciﬁcally, in the top case of Equation (2),
where t 6= 0, the limit of gq as we approach a 180 degree
rotation is [ 0, 0, 0, 0 ], and meanwhile, the ﬁrst three coor-

dinates of gq(r) for r ∈ Rπ are nonzero. Note that our
deﬁnition of continuous representation from Section 3 re-
quires a Euclidean topology for the representation space Y ,
in contrast to the usual topology for the quaternions of the
real projective space RP 3, which we discuss in the next
paragraph. In a similar way, we can show that other popular
representations for the 3D rotations such as axis-angle have
discontinuities, e.g.
the axis in axis-angle has discontinu-
ities at the 180 degree rotations.

Representations for the 3D rotations are discontin-
uous in four or fewer dimensions. The 3D rotation
group SO(3) is homeomorphic to the real projective space
RP 3. The space RP n is deﬁned as the quotient space of
Rn+1 \ {0} under the equivalence relation that x ∼ λx for
all λ 6= 0. In a graphics and vision context, it may be most
intuitive to think of RP 3 as being the homogeneous coor-
dinates in R4 with the previous equivalence relation used to
construct an appropriate topology via the quotient space.

Based on standard embedding and non-embedding re-
sults in topology [12], we know that RP 3 (and thus SO(3))
embeds in R5 with the Euclidean topology, but does not em-
bed in Rd for any d < 5. By the deﬁnition of embedding,
there is no homeomorphism from SO(3) to any subset of
Rd for any d < 5, but a continuous representation requires
this. Thus, there is no such continuous representation.

4.2. Continuous Representations

In this section, we develop two continuous representa-
tions for the n dimensional rotations SO(n). We then ex-
plain how for the 3D rotations SO(3) these become 6D and
5D continuous rotation representations.

Case 3: Continuous representation with n2 − n di-
mensions for the n dimensional rotations. The rotation
representations we have considered thus far are all not con-
tinuous. One possibility to make a rotation representation
continuous would be to just use the identity mapping, but
this would result in matrices of size n × n for the rep-
resentation, which can be excessive, and would still re-
quire orthogonalization, such as a Gram-Schmidt process
in the mapping f to the original space, if we want to en-
sure that network outputs end up back in SO(n). Based
on this observation, we propose to perform an orthogonal-
ization process in the representation itself. Let the orig-
inal space X = SO(n), and the representation space be
R = Rn×(n−1) \ D (D will be deﬁned shortly). Then we
can deﬁne a mapping gGS to the representation space that
simply drops the last column vector of the input matrix:

gGS





a1

. . .

a1

. . .

(5)

an 


 =



an−1 


, where ai, i = 1, 2, ..., n are column vectors. We note that
the set gGS(X) is a Stiefel manifold [3]. Now for the map-
ping fGS to the original space, we can deﬁne the following
Gram-Schmidt-like process:

fGS





a1

. . .

an−1 


 =



b1

. . .

bn 


(6)

45748

bi =

N (a1)

N (ai −Pi−1
det


b1

. . .








j=1(bj · ai)bj)

bn−1

e1
...
en




if i=1
if 2 ≤ i < n

if i = n.





Here N (·) denotes a normalization function, the same as
before, and e1, . . . , en are the n canonical basis vectors of
the Euclidean space. The only difference of fGS from an
ordinary Gram-Schmidt process is that the last column is
computed by a generalization of the cross product to n di-
mensions. Now clearly, gGS is continuous. To check that
for every M ∈ SO(n), fGS(gGS(M )) = M , we can use
induction and the properties of the orthonormal basis vec-
tors in the columns of M to show that the Gram-Schmidt
process does not modify the ﬁrst n − 1 components. Lastly,
we can use theorems for the generalized cross product such
as Theorem 5.14.7 of Bloom [8], to show that the last com-
ponent of fGS(gGS(M )) agrees with M . Finally, we can
deﬁne the set D as that where the above Gram-Schmidt-
like process does not map back to SO(n): speciﬁcally, this
is where the dimension of the span of the n−1 vectors input
to g is less than n − 1.

6D representation for the 3D rotations: For the 3D
rotations, Case 3 gives us a 6D representation. The gener-
alized cross product for bn in Equation (7) simply reduces
to the ordinary cross product b1 × b2. We give the detailed
equations in Section B in the supplemental document. We
speciﬁcally note that using our 6D representation in a net-
work can be beneﬁcial because the mapping fGS in Equa-
tion (7) ensures that the resulting 3x3 matrix is orthogonal.
In contrast, suppose a direct prediction for 3x3 matrices is
used. Then either the orthogonalization can be done in-
network or as a postprocess. If orthogonalization is done
in network, the last 3 components of the matrix will be dis-
carded by the Gram-Schmidt process in Equation (7), so the
3x3 matrix representation is effectively our 6D representa-
tion plus 3 useless parameters. If orthogonalization is done
as a postprocess, then this prevents certain applications such
as forward kinematics, and the error is also higher as shown
in Section 5.

Group operations such as multiplication: Suppose
that the original space is a group such as the rotation group,
and we want to multiply two representations r1, r2 ∈ R.
In general, we can do this by ﬁrst mapping to the origi-
nal space, multiplying the two elements, and then mapping
back: r1r2 = g(f (r1)f (r2)). However, for the proposed
representation here, we can gain some computational efﬁ-
ciency as follows. Since the mapping to the representation
space in Equation (5) drops the last column, when comput-
ing f (r2), we can simply drop the last column and compute
the product representation as the product of an n × n and an
n × (n − 1) matrix.

Case 4: Further reducing the dimensionality for the
n dimensional rotations. For n ≥ 3 dimensions, we can
reduce the dimension for the representation in the previous
case, while still keeping a continuous representation. Intu-
itively, a lower dimensional representation that is less re-

T

(7)

y

N0

p

p′

x

Figure 3. An illustration of stereographic projection in 2D. We are
given as input a point p on the unit sphere S1. We construct a ray
from a ﬁxed projection point N0 = (0, 1) through p and ﬁnd the
intersection of this ray with the plane y = 0. The resulting point
p′ is the stereographic projection of p.

dundant could be easier to learn. However, we found in
our experiments that the dimension reduced representation
does not outperform the Gram-Schmidt-like representation
from Case 3. However, we still develop this representation
because it allows us to show that continuous rotation repre-
sentations can outperform discontinuous ones.

We show that we can perform such dimension reduc-
tion using one or more stereographic projections combined
with normalization. We show an illustration of a 2D stereo-
graphic projection in Figure 3, which can be easily general-
ized to higher dimensions. Let us ﬁrst normalize the input
point, so it projects to a sphere, and then stereographically
project the result using a projection point of (1, 0, . . . , 0).
We call this combined operation a normalized projection,
and deﬁne it as P : Rm → Rm−1:

,

v2

v3

1−v1

P (u) = (cid:2)

, v = u/||u||.
(8)
Now deﬁne a function Q : Rm−1 → Rm, which does a
stereographic un-projection:

1−v1 (cid:3)T

. . . ,

1−v1

vm

,

Q(u) =

2 (||u||2 − 1), u1,

1

||u|| (cid:2) 1

. . . , um−1 (cid:3)T

(9)

Note that the un-projection is not actually back to the
sphere, but in a way that coordinates 2 through m are a
unit vector. Now we can use between 1 and n − 2 nor-
malized projections on the representation from the previous
case, while still preserving continuity and one-to-one be-
havior.

For simplicity, we will ﬁrst demonstrate the case of one
stereographic projection. The idea is that we can ﬂatten the
representation from Case 3 to a vector and then stereograph-
ically project the last n + 1 components of that vector. Note
that we intentionally project as few components as possible,
since we found that nonlinearities introduced by the projec-
tion can make the learning process more difﬁcult. These
nonlinearities are due to the square terms and the division
in Equation (9). If u is a vector of length m, deﬁne the slic-
ing notation ui:j = (ui, ui+1, . . . , uj), and ui: = ui:m. Let
M(i) be the ith column of matrix M. Deﬁne a vectorized
representation γ(M ) by dropping the last column of M like
in Equation (5): γ(M ) = [M T
(n−1)]. Now we can
deﬁne the mapping to the representation space as:

(1), . . . , M T

gP(M ) = [γ1:n2−2n−1, P (γn2−2n:)]

(10)

55749

n = 3: 1

2 3

4 5 6

n = 4:

1 2 3 4

5 6 7 8

9 10 11 12

n = 5: 1 2 3 4 5

6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

Legend

No projection
Projection #1
Projection #2
Projection #3

Figure 4. An illustration of how n − 2 normalized projections
can be made to reduce the dimensionality for the representation
of SO(n) from Case 3 by n − 2. In each row we show the dimen-
sion n, and the elements of the vectorized representation γ(M )
containing the ﬁrst n − 1 columns of M ∈ SO(n). Each column
is length n: the columns are grouped by the thick black rectangles.
Each unique color speciﬁes a group of inputs for the “normalized
projection” of Equation (8). The white regions are not projected.

Here we have dropped the implicit argument M to γ for
brevity. Deﬁne the mapping to the original space as:

fP(u) = fGS(cid:16)[u1:n2−2n−1, Q(un2−2n:)](n×(n−1))(cid:17)(11)

Here the superscript (n × (n − 1)) indicates that the vec-
tor is reshaped to a matrix of the speciﬁed size before going
through the Gram-Schmidt function fGS. We can now see
why Equation (9) is normalized the way it is. This is so that
projection followed by un-projection can preserve the unit
length property of the basis vector that is a column of M ,
and also correctly recover the ﬁrst component of Q(·), so
we get fP(gP(M )) = M for all M ∈ SO(3). We can show
that gP is deﬁned on its domain and continuous by using
properties of the orthonormal basis produced by the Gram-
Schmidt process gGS. For example, we can show that γ 6= 0
because components 2 through n + 1 of γ are an orthonor-
mal basis vector, and N (γ) will never be equal to the projec-
tion point [1, 0, . . . , 0, 0] that the stereographic projection is
made from. It can also be shown that for all M ∈ SO(3),
fP(gP(M )) = M . We show some of these details in the
supplemental material.

As a special case, for the 3D rotations, this gives us a
5D representation. This representation is made by using the
6D representation from Case 3, ﬂattening it to a vector, and
then using a normalized projection on the last 4 dimensions.
We can actually make up to n − 2 projections in a similar
manner, while maintaining continuity of the representation,
as follows. As a reminder, the length of γ, the vectorized
result of the Gram-Schmidt process, is n(n − 1): it contains
n−1 basis vectors each of dimension n. Thus, we can make
n − 2 projections, where each projection i = 1, . . . , n − 2
selects the basis vector i + 1 from γ(M ), prepends to it an
appropriately selected element from the ﬁrst basis vector of
γ(M ), such as γn+1−i, and then projects the result. The
resulting projections are then concatenated as a row vector
along with the two unprojected entries to form the represen-
tation. Thus, after doing n − 2 projections, we can obtain
a continuous representation for SO(3) in n2 − 2n + 2 di-
mensions. See Figure 4 for a visualization of the grouping
of the elements that can be projected.

Other groups: O(n), similarity transforms, quater-
nions. In this paper, we focus mainly on the representation
of rotations. However, we note that the preceding repre-
sentations can easily be generalized to O(n), the group of

orthogonal n × n matrices M with M M T = M T M = I.
We can also generalize to the similarity transforms, which
we denote as Sim(n), deﬁned as the afﬁne maps ρ(x) on
Rn, ρ(x) = αRx + u, where α > 0, R is an n × n or-
thogonal matrix, and u ∈ Rn [4]. For the orthogonal group
O(n), we can use any of the representations in Case 3 or
4, but with an additional component in the representation
that indicates whether the determinant is +1 or -1. Then the
Gram-Schmidt process in Equation (7) needs to be modiﬁed
slightly: if the determinant is -1 then the last vector bn needs
to be negated. Meanwhile, for the similarity transforms,
the translation component u can easily be represented as-
is. The matrix component αR of the similarity transform
can be represented using any of the options in Case 3 or
4. The only needed change is that the Gram-Schmidt pro-
cess in Equations 7 or 11 should multiply the ﬁnal resulting
matrix by α. The term α is simply the norm of any of the
basis vectors input to the Gram-Schmidt process, e.g. ||a1||
in Equation (7). Clearly, if the projections of Case 4 are
used, at least one basis vector must remain not projected,
so α can be determined. In the supplemental material, we
also explain how one might adapt an existing network that
outputs 3D or 4D rotation representations so that it can use
our 6D or 5D representations.

5. Empirical Results

We investigated different rotation representations and
found that those with better continuity properties work bet-
ter for learning. We ﬁrst performed a sanity test and then
experimented on two real world applications to show how
continuity properties of rotation representations inﬂuence
the learning process.

5.1. Sanity Test

We ﬁrst perform a sanity test using an auto-encoder
structure. We use a multi-layer perceptron (MLP) network
as an encoder to map SO(3) to the chosen representation R.
We test our proposed 6D and 5D representations, quater-
nions, axis-angle, and Euler angles. The encoder network
contains four fully-connected layers, where hidden layers
have 128 neurons and Leaky ReLU activations. The ﬁxed
“decoder” mapping f : R 7→ SO(3) is deﬁned in Section 4.
For training, we compute the loss using the L2 distance
between the input SO(3) matrix M and the output SO(3)
matrix M ′: note that this is invariant to the particular rep-
resentation used, such as quaternions, axis-angle, etc. We
use Adam optimization with batch size 64 and learning rate
10−5 for the ﬁrst 104 iterations and 10−6 for the remain-
ing iterations. For sampling the input rotation matrices dur-
ing training, we uniformly sample axes and angles. We test
the networks using 105 rotation matrices generated by ran-
domly sampling axes and angles and calculate geodesic er-
rors between the input and the output rotation matrices. The
geodesic error is deﬁned as the minimal angular difference
between two rotations, written as

Langle = cos−1((M ′′

00 + M ′′

11 + M ′′

22 − 1)/2)

M ′′ = M M ′−1

65750

(12)

(13)

Sanity Test

0.1

Mean(°)

Max(°)

Std(°)

6D

5D

0.49

0.49

1.98

0.27

1.99

0.27

Quat

3.32

179.93

5.97

AxisA

3.69

179.22

5.99

Euler

6.98

179.95

17.31

a. Mean errors during iterations.

b. Percentile of errors at 500k iteration.

c. Errors at 500k iteration.

3D Point Cloud Pose Estimation Test

Mean(°)

Max(°)

Std(°)

6D

5D

Quat

2.85

4.78

9.03

179.83

9.16

179.87

12.25

179.66

16.33

AxisA

11.93

179.7

21.35

Euler

14.13

179.67

23.8

Matrix

4.21

180.0

9.44

0.1

d. Mean errors during iterations.

e. Percentile of errors at 2600k iteration.

f. Errors at 2600k iteration.

Human Body Inverse Kinematics Test

Mean(cm) Max(cm)

Std(cm)

6D

5D

Quat

AxisA

Euler

1.9

2.0

3.3

3.0

2.7

Matrix

22.9

28.7

33.3

87.1

120.0

48.7

53.6

1.2

1.4

3.1

2.3

2.1

4.0

g. Mean errors during iterations.

h. Percentile of errors at 1960k iterations.

i. Errors at 1960k iteration.

Figure 5. Empirical results. In (b), (e), (h) we plot on the x axis a percentile p and on the y axis the error at the given percentile p.

Figure 5(a) illustrates the mean geodesic errors for differ-
ent representations as training progresses. Figure 5(b) illus-
trates the percentiles of the errors at 500k iterations. The re-
sults show that the 6D and 5D representations have similar
performance with each other. They converge much faster
than the other representations and produce smallest mean,
maximum and standard deviation of errors. The Euler an-
gle representation performs the worst, as shown in Table (c)
in Figure 5. For the quaternion, axis angle and Euler angle
representations, the majority of the errors fall under 25◦,
but certain test samples still produce errors up to 180◦. The
proposed 6D and 5D representations do not produce errors
higher than 2◦. We conclude that using continuous rotation
representations for network training leads to lower errors
and faster convergence.

In Appendix G.2, we report additional results, where we
trained using a geodesic loss, uniformly sampled SO(3),
and compared to 3D Rodriguez vectors and quaternions
constrained to one hemisphere [20]. Again, our continuous
representations outperform common discontinuous ones.

5.2. Pose Estimation for 3D Point Clouds

In this experiment, we test different rotation representa-
tions on the task of estimating the rotation of a target point
cloud from a reference point cloud. The inputs of the net-
works are the reference and target point clouds Pr, Pt ∈
RN ×3, where N is the number of points. The network out-
put is the estimated rotation R ∈ RD between Pr and Pt,
where D is the dimension of the chosen representation.

We employ a weight-sharing Siamese network where
each half is a simpliﬁed PointNet structure [27], Φ :
RN ×3 7→ R1024. The simpliﬁed PointNet uses a 4-layer
MLP of size 3 × 64 × 128 × 1024 to extract features for
each point and then applies max pooling across all points to
produce a single feature vector z. One half of the Siamese
network maps the reference point cloud to a feature vector
zr = Φ(Pr) and the other half maps the target point cloud
to zt = Φ(Pt). Then we concatenate zr and zt and pass this
through another MLP of size 2048 × 512 × 512 × D to pro-
duce the D dimensional rotation representation. Finally, we
transform the rotation representations to SO(3) with one of
the mapping functions f deﬁned in Section 4.

75751

We train the network with 2,290 airplane point clouds
from ShapeNet [9], and test it with 400 held-out point
clouds augmented with 100 random rotations. At each train-
ing iteration, we randomly select a reference point cloud
and transform it with 10 randomly-sampled rotation ma-
trices to get 10 target point clouds. We feed the paired
reference-target point clouds into the Siamese network and
minimize the L2 loss between the output and the ground-
truth rotation matrices.

We trained the network with 2.6×106 iterations. Plot (d)
in Figure 5 shows the mean geodesic errors as training pro-
gresses. Plot (e) and Table (f) show the percentile, mean,
max and standard deviation of errors. Again, the 6D rep-
resentation has the lowest mean and standard deviation of
errors with around 95% of errors lower than 5◦, while Euler
representation is the worst with around 10% of errors higher
than 25◦. Unlike the sanity test, the 5D representation here
performs worse than the 6D representation, but outperforms
the 3D and 4D representations. We hypothesize that the dis-
tortion in the gradients caused by the stereographic projec-
tion makes it harder for the network to do the regression.
Since the ground-truth rotation matrices are available, we
can directly regress the 3 × 3 matrix using L2 loss. During
testing, we use the Gram-Schmidt process to transform the
predicted matrix into SO(3) and then report the geodesic
error (see the bottom row of Table (f) in Figure 5). We hy-
pothesize that the reason for the worse performance of the
3 × 3 matrix compared to the 6D representation is due to the
orthogonalization post-process, which introduces errors.

5.3. Inverse Kinematics for Human Poses

In this experiment, we train a neural network to solve
human pose inverse kinematics (IK) problems. Similar to
the method of Villegas et al. [30] and Hsu et al. [18], our
network takes the joint positions of the current pose as in-
puts and predicts the rotations from the T-pose to the current
pose. We use a ﬁxed forward kinematic function to trans-
form predicted rotations back to joint positions and penalize
their L2 distance from the ground truth. Previous work for
this task used quaternions. We instead test on different ro-
tation representations and compare their performance.

The input contains the 3D positions of the N joints
on the skeleton marked as P = (p1, p2, p3, ..., pN ),
pi = (x, y, z)⊺. The output of the network are the ro-
tations of the joints in the chosen representation R =
(r1, r2, r3, ..., rN ), ri ∈ RD, where D is the dimension of
the representation.

We train a four-layer MLP network that has 1024 neu-
rons in hidden layers with the L2 reconstruction loss L =
||P − P ′||2
2, where P ′ = Π(T, R). Here Π is the forward
kinematics function which takes as inputs the “T” pose of
the skeleton and the predicted joints rotations, and outputs
the 3D positions of the joints. Due to the recursive com-
putational structure of forward kinematics, the accuracy of
the hip orientation is critical for the overall skeleton pose
prediction and thus the joints adjacent to the hip contribute
more weight to the loss (10 times higher than other joints).
We use the CMU Motion Capture Database [25] for

training and testing because it contains complex motions
like dancing and martial arts, which cover a wide range of
joint rotations. We picked in total 865 motion clips from 37
motion categories. We randomly chose 73 clips for test-
ing and the rest for training. We ﬁx the global position
of the hip so that we do not need to worry about predict-
ing the global translation. The whole training set contains
1.14 × 106 frames of human poses and the test set contains
1.07 × 105 frames of human poses. We train the networks
with 1,960k iterations with batch size 64. During training,
we augmented the poses with random rotation along the y-
axis. We augmented each instance in the test set with three
random rotations along the y-axis as well. The results, as
displayed in subplots (g), (h) and (i) in Figure 5, show that
the 6D representation performs the best with the lowest er-
rors and fastest convergence. The 5D representation has
similar performance as the 6D one. On the contrary, the
4D and 3D representations have higher average errors and
higher percentages of big errors that exceed 10 cm.

We also perform the test of using the 3×3 matrix with-
out orthogonalization during training and using the Gram-
Schmidt process to transform the predicted matrix into
SO(3) during testing. We ﬁnd this method creates huge er-
rors as reported in the bottom line of Table (i) in Figure
5. One possible reason for this bad performance is that the
3×3 matrix may cause the bone lengths to scale during the
forward kinematics process. In Appendix G.1, we addition-
ally visualize some human body poses for quaternions and
our 6D representation.

6. Conclusion

We investigated the use of neural networks to approx-
imate the mappings between various rotation representa-
tions. We found empirically that neural networks can better
ﬁt continuous representations. For 3D rotations, the com-
monly used quaternion and Euler angle representations have
discontinuities and can cause problems during learning. We
present continuous 5D and 6D rotation representations and
demonstrate their advantages using an auto-encoder sanity
test, as well as real world applications, such as 3D pose es-
timation and human inverse kinematics.

7. Acknowledgements

We thank Noam Aigerman, Kee Yuen Lam, and Sitao
Xiang for fruitful discussions; Fangjian Guo, Xinchen Yan,
and Haoqi Li for helping with the presentation. This re-
search was conducted at USC and Adobe and was funded
by in part by the ONR YIP grant N00014-17-S-FO14, the
CONIX Research Center, one of six centers in JUMP, a
Semiconductor Research Corporation (SRC) program spon-
sored by DARPA, the Andrew and Erna Viterbi Early Ca-
reer Chair,
the U.S. Army Research Laboratory (ARL)
under contract number W911NF-14-D-0005, Adobe, and
Sony. This project was not funded by Pinscreen, nor has
it been conducted at Pinscreen or by anyone else afﬁliated
with Pinscreen. The content of the information does not
necessarily reﬂect the position or the policy of the Govern-
ment, and no ofﬁcial endorsement should be inferred.

85752

References

[1] Cayley transform.

https://en.wikipedia.org/

wiki/Cayley_transform#Matrix_map.

[2] Rotation matrix.

https://en.wikipedia.org/

wiki/Rotation_matrix#Quaternion.

[3] Stiefel manifold.

https://en.wikipedia.org/

wiki/Stiefel_manifold.

[4] C. Allen-Blanchette, S. Leonardos, and J. Gallier. Motion

interpolation in sim (3). 2014.

[5] M. J. Baker. Maths: Conversion matrix to quater-
http://www.euclideanspace.com/

nion.
maths/geometry/rotations/conversions/
matrixToQuaternion/. Accessed: 2018-11-21.

[6] A. R. Barron. Universal approximation bounds for superpo-
sitions of a sigmoidal function. IEEE Transactions on Infor-
mation theory, 39(3):930–945, 1993.
Rodrigues’

Belongie.

formula.

rotation

[7] S.

http://mathworld.wolfram.com/
RodriguesRotationFormula.html.
2019-04-04.

Accessed:

[8] D. M. Bloom. Linear algebra and geometry. CUP Archive,

1979.

[9] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.

[10] Z. Chen and F. Cao. The construction and approximation of
neural networks operators with gaussian activation function.
Mathematical Communications, 18(1):185–207, 2013.

[11] A. Csiszar, J. Eilers, and A. Verl. On solving the inverse
kinematics problem using neural networks. In Mechatronics
and Machine Vision in Practice (M2VIP), 2017 24th Inter-
national Conference on, pages 1–6. IEEE, 2017.

[12] D. M. Davis. Embeddings of real projective spaces. Bol. Soc.

Mat. Mexicana (3), 4:115–122, 1998.

[13] T.-T. Do, M. Cai, T. Pham, and I. Reid. Deep-6dpose: Re-
covering 6d object pose from a single rgb image. arXiv
preprint arXiv:1802.10367, 2018.

[14] L. Falorsi, P. de Haan, T. R. Davidson, N. De Cao, M. Weiler,
P. Forr´e, and T. S. Cohen. Explorations in homeomorphic
variational auto-encoding. arXiv preprint arXiv:1807.04689,
2018.

[15] G. Gao, M. Lauri, J. Zhang, and S. Frintrop. Occlusion re-
sistant object rotation regression from point cloud segments.
arXiv preprint arXiv:1808.05498, 2018.

[16] F. S. Grassia. Practical parameterization of rotations using
the exponential map. Journal of graphics tools, 3(3):29–48,
1998.

[17] K. Hornik. Approximation capabilities of multilayer feed-

forward networks. Neural networks, 4(2):251–257, 1991.

[18] H.-W. Hsu, T.-Y. Wu, S. Wan, W. H. Wong, and C.-Y. Lee.
Quatnet: Quaternion-based head pose estimation with multi-
regression loss. IEEE Transactions on Multimedia, 2018.

[19] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.

[20] A. Kendall, R. Cipolla, et al. Geometric loss functions for
camera pose regression with deep learning. In Proc. CVPR,
volume 3, page 8, 2017.

[21] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
Proceedings of the IEEE international conference on com-
puter vision, pages 2938–2946, 2015.

[22] C. Kosniowski. A ﬁrst course in algebraic topology. CUP

Archive, page 53, 1980.

[23] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,

521(7553):436, 2015.

[24] B. Llanas, S. Lantar´on, and F. J. S´ainz. Constructive ap-
proximation of discontinuous functions by neural networks.
Neural Processing Letters, 27(3):209–226, 2008.

[25] M. LLC. Cmu graphics lab motion capture database. http:

//mocap.cs.cmu.edu/.

[26] X. Perez-Sala, L. Igual, S. Escalera, and C. Angulo. Uniform
sampling of rotations for discrete and continuous learning of
2d shape models. In Robotic Vision: Technologies for Ma-
chine Learning and Vision Applications, pages 23–42. IGI
Global, 2013.

[27] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE, 1(2):4, 2017.

[28] A. Saxena, J. Driemeyer, and A. Y. Ng. Learning 3-d ob-
ject orientation from images. In Robotics and Automation,
2009. ICRA’09. IEEE International Conference on, pages
794–800. IEEE, 2009.

[29] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg,
A. Dosovitskiy, and T. Brox. Demon: Depth and motion net-
work for learning monocular stereo. In IEEE Conference on
computer vision and pattern recognition (CVPR), volume 5,
page 6, 2017.

[30] R. Villegas, J. Yang, D. Ceylan, and H. Lee. Neural kine-
matic networks for unsupervised motion retargetting. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 8639–8648, 2018.

[31] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox. Posecnn: A
convolutional neural network for 6d object pose estimation
in cluttered scenes. Robotics: Science and Systems (RSS),
2018.

[32] Z. Xu and F. Cao. The essential order of approximation for
neural networks. Science in China Series F: Information Sci-
ences, 47(1):97–112, 2004.

[33] Z.-B. Xu and F.-L. Cao. Simultaneous lp-approximation or-
der for neural networks. Neural Networks, 18(7):914–923,
2005.

[34] X. Zhou, X. Sun, W. Zhang, S. Liang, and Y. Wei. Deep kine-
matic pose regression. In European Conference on Computer
Vision, pages 186–201. Springer, 2016.

95753

