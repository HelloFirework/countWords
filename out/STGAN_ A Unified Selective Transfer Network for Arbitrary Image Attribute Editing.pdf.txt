STGAN: A Uniﬁed Selective Transfer Network

for Arbitrary Image Attribute Editing

Ming Liu1⋆
1Harbin Institute of Technology, 2Department of Computer Vision Technology (VIS), Baidu Inc., 3Peng Cheng Laboratory, Shenzhen

, Yukang Ding2, Min Xia1, Xiao Liu2, Errui Ding2, Wangmeng Zuo()1,3, Shilei Wen2

csmliu@outlook.com, csmxia@gmail.com, wmzuo@hit.edu.cn,

{dingyukang, liuxiao12, dingerrui, wenshilei}@baidu.com

Figure 1: High resolution (384 × 384) results of STGAN for facial attribute editing, and more results are given in the suppl.

Abstract

1. Introduction

Arbitrary attribute editing generally can be tackled by
incorporating encoder-decoder and generative adversar-
ial networks. However, the bottleneck layer in encoder-
decoder usually gives rise to blurry and low quality editing
result. And adding skip connections improves image qual-
ity at the cost of weakened attribute manipulation ability.
Moreover, existing methods exploit target attribute vector
to guide the ﬂexible translation to desired target domain. In
this work, we suggest to address these issues from selective
transfer perspective. Considering that speciﬁc editing task
is certainly only related to the changed attributes instead of
all target attributes, our model selectively takes the differ-
ence between target and source attribute vectors as input.
Furthermore, selective transfer units are incorporated with
encoder-decoder to adaptively select and modify encoder
feature for enhanced attribute editing. Experiments show
that our method (i.e., STGAN) simultaneously improves at-
tribute manipulation accuracy as well as perception qual-
ity, and performs favorably against state-of-the-arts in ar-
bitrary facial attribute editing and season translation.

⋆Work done during an internship at Baidu.

Image attribute editing, aiming at manipulating an image
to possess desired attributes, is an interesting but challeng-
ing problem with many real-world vision applications. On
one hand, it is impracticable to collect paired images with
and without desirable attributes (e.g., female and male face
images of the same person). Thus, unsupervised genera-
tive learning models, e.g., generative adversarial networks
(GANs) [9], have attracted upsurging attention in attribute
editing. On the other hand, arbitrary attribute editing ac-
tually is a multi-domain image-to-image translation task.
Learning single translation model for each speciﬁc attribute
editing task may achieve limited success [20, 29, 34]. But it
is ineffective in exploiting the entire training data, and the
learned models grows exponentially along with the number
of attributes. To handle this issue, several arbitrary attribute
editing approaches [7, 11, 26] have been developed, which
usually (i) use encoder-decoder architecture, and (ii) take
both source image and target attribute vector as input.

Albeit their extensive deployment, encoder-decoder net-
works remain insufﬁcient for high quality attribute editing.
Attribute can be of either local, global, or abstract charac-
teristic of the image. In order to properly manipulate im-
age attribute, spatial pooling or downsampling generally are
required to obtain high-level abstraction of image content

13673

Raw ImageTo FemaleAdd EyeglassesTo Brown HairAdd BeardTo Mouth ClosedTo OldRemove BangsReconstructionRaw ImageReconstructionAdd BangsTo Mouth OpenTo Pale SkinTo Blond HairTo OldAdd EyeglassesAdd MustacheTo Light EyebrowsTo BaldAdd BeardTo Bushy EyebrowsInput

AttGAN

StarGAN

STGAN

Figure 2: Reconstruction results of AttGAN [11], StarGAN [7]
and our STGAN.

and attributes. For example, auto-encoder architecture is
adopted in [11, 17, 26], and shallow encoder-decoder with
residual blocks is used in [7, 34]. However, the introduc-
tion of bottleneck layer, i.e., the innermost feature map with
minimal spatial size, gives rise to blurry and low quality
editing result. As a remedy, some researchers suggest to add
one [11] or multiple [14] skip connections between encoder
and decoder layers. Unfortunately, as further shown in Sec.
3.1, the deployment of skip connections improves image
quality of editing result but is harmful to attribute manipula-
tion ability of learned model. Another possible solution is to
employ spatial attention network to allow attribute-speciﬁc
region editing [32], which, however, is effective only for lo-
cal attributes and not designed for arbitrary attribute editing.
Moreover, most existing methods exploit both source im-
age and target attribute vector for arbitrary attribute editing.
In particular, the encoders in [11, 17] only take source im-
age as input to produce latent code, and then the decoders
utilize both latent code and target attribute vector to gener-
ate editing result. In contrast, StarGAN [7] directly takes
source image and target attribute vector as input. How-
ever, for arbitrary attribute editing only the attributes to be
changed are required, taking full target attribute vector as
input may even have adverse effect on editing result. As
shown in Fig. 2, although all attributes keep unchanged, un-
wanted changes and visual degradation can be observed in
the results by AttGAN [11] and StarGAN [7], mainly as-
cribing to the limitation of encoder-decoder and the use of
target attribute vector as input.

To address the above issues, this work investigates ar-
bitrary attribute editing from selective transfer perspective
and presents a STGAN model. In terms of selective, our
STGAN is suggested to (i) only consider the attributes to
be changed, and (ii) selectively concatenate encoder feature
in editing attribute irrelevant regions with decoder feature.
In terms of transfer, our STGAN is expected to adaptively
modify encoder feature to match the requirement of varying
editing task, thereby providing a uniﬁed model for handling
both local and global attributes.

To this end, instead of full target attribute vector, our
STGAN takes the difference between target and source at-

tribute vectors as input to encoder-decoder. Subsequently,
selective transfer units (STUs) are proposed to adaptively
select and modify encoder feature, which is further concate-
nated with decoder feature for enhancing both image qual-
ity and attribute manipulation ability. In particular, STU is
added to each pair of encoder and decoder layers, and takes
both encoder feature, inner state, and difference attribute
vector into consideration for exploiting cross-layer consis-
tency and task speciﬁcity. From Figs. 1 and 2, our STGAN
can generate high quality and photo-realistic results for ar-
bitrary attribute editing, and obtain near-ideal reconstruc-
tion when the target and source attributes are the same. To
sum up, the contribution of this work involves:

• Instead of all target attributes, difference attribute vec-
tor is taken as input to enhance the ﬂexible translation
of attributes and ease the training procedure.

• Selective transfer units are presented and incorporated
with encoder-decoder for simultaneously improving
attribute manipulation ability and image quality.

• Experimental results show that our STGAN performs
favorably against state-of-the-arts in arbitrary facial at-
tribute editing and season translation.

2. Related Work

Encoder-Decoder Architecture.
In their pioneer
work [12], Hinton and Zemel proposed an autoencoder
network, which consists of an encoder to map the input
into latent code and a decoder to recover from the latent
code. Subsequently, denoising autoencoders [30] are pre-
sented to learn representation robust to partial corruption.
Kingma and Welling [16] suggested a Variational Autoen-
coder (VAE), which validates the feasibility of encoder-
decoder architecture to generate unseen images. Recent
studies show that skip connections [14,28] between encoder
and decoder layers usually beneﬁt the training stability and
visual quality of generated images. However, as discussed
in Sec. 3.1, skip connections actually improves image qual-
ity at the cost of weakened attribute manipulation ability,
and should be carefully used in arbitrary attribute editing.

Generative Adversarial Networks. GAN [9, 27] is orig-
inally proposed to generate images from random noise,
and generally consists of a generator and a discriminator
which are trained in an adversarial manner and suffer from
the mode collapse problem. Recently, enormous efforts
have been devoted to improving the stability of learning.
In [3, 10], Wasserstein-1 distance and gradient penalty are
suggested to improve stability of the optimization process.
In [18], the VAE decoder and GAN generator are collapsed
into one model and optimized by both reconstruction and
adversarial loss. Conditional GAN (cGAN) [14, 24] takes
conditional variable as input to the generator and discrim-
inator to generate image with desired properties. As a re-

3674

Input

AttGAN-ED

AttGAN

AttGAN-2s AttGAN-UNet

Figure 3: Results of AttGAN [11] variants for reconstructing input
image. Please zoom in for better observation.

sult, GAN has become one of the most prominent models
for versatile image generation [9, 27], translation [14, 34],
restoration [19, 21] and editing [25] tasks.

Image-to-Image Translation. Image-to-image translation
aims at learning cross-domain mapping in supervised or
unsupervised settings.
Isola et al. [14] presented a uni-
ﬁed pix2pix framework for learning image-to-image trans-
lation from paired data.
Improved network architectures,
e.g., cascaded reﬁnement networks [4] and pix2pixHD [31],
are then developed to improve the visual quality of syn-
thesized images. As for unpaired image-to-image transla-
tion, additional constraints, e.g., cycle consistency [34] and
shared latent space [22], are suggested to alleviate the in-
herent ill-posedness of the task. Nonetheless, arbitrary at-
tribute editing actually is a multi-domain image-to-image
translation problem, and cannot be solved with scalabil-
ity by aforementioned methods. To address this issue, [2]
and [13] decouple generators by learning domain-speciﬁc
encoders/decoders with shared latent space, but are still lim-
ited in scaling to change multiple attributes of an image.

Facial Attribute Editing. Facial attribute editing is an in-
teresting multi-domain image-to-image translation problem
and has received considerable recent attention. While sev-
eral methods have been proposed to learn single translation
model for each speciﬁc attribute editing task [5, 20, 29, 32],
they suffer from the limitation of image-to-image trans-
lation and cannot well scale to arbitrary attribute editing.
Therefore, researchers resort to learning a single model for
arbitrary attribute editing. IcGAN [26] adopts an encoder
to generate latent code of an image, and a cGAN to de-
code latent code conditioned on target attributes. However,
IcGAN ﬁrst trains the cGAN model followed by the en-
coders, greatly restricting its reconstruction ability. Lam-
ple et al. [17] trained the FaderNet in an end-to-end man-
ner by imposing adversarial constraint to enforce the in-
dependence between latent code and attributes. Modu-
larGAN [33] presents a feasible solution to connect speciﬁc
attribute editing to arbitrary attribute editing, but its compu-
tation time gradually increases along with the number of at-
tributes to be changed. StarGAN [7] and AttGAN [11] elab-
orately tackle arbitrary attribute editing by taking target at-
tribute vector as input to the transform model. In this work,
we analyze the limitation of StarGAN [7] and AttGAN [11],
and further develop a STGAN for simultaneously enhanc-
ing the attribute manipulation ability and image quality.

Method
AttGAN-ED
PSNR/SSIM 22.68/0.758

AttGAN

24.07/0.841

AttGAN-2s AttGAN-UNet
26.13/0.897

29.66/0.929

Table 1: Reconstruction evaluation of AttGAN [11] variants.

Figure 4: Attribute generation accuracy of AttGAN [11] variants.

3. Proposed Method

This section presents our proposed STGAN for arbitrary
attribute editing. To begin with, we use AttGAN as an ex-
ample to analyze the limitation of skip connections. Then,
we formulate STGAN by taking difference attribute vec-
tor as input and incorporating selective transfer units into
encoder-decoder structure. Finally, network architecture
(see Fig. 5) and model objective of STGAN are provided.

3.1. Limitation of Skip Connections in AttGAN

StarGAN [7] and AttGAN [11] adopt encoder-decoder
structure, where spatial pooling or downsampling are es-
sential to obtain high level abstract representation for at-
tribute manipulation. Unfortunately, downsampling irre-
versibly diminishes spatial resolution and ﬁne details of fea-
ture map, which cannot be completely recovered by trans-
posed convolutions and the results are prone to blurring or
missing details. To enhance image quality of editing result,
AttGAN [11] applies one skip connection between encoder
and decoder, but we will show that it is still limited.

To analyze the effect and limitation of skip connections,
we test four variants of AttGAN on the test set: (i) AttGAN
w/o skip connection (AttGAN-ED), (ii) AttGAN model re-
leased by He et al. [11] with one skip connection (AttGAN),
(iii) AttGAN with two skip connections (AttGAN-2s), and
(iv) AttGAN with all symmetric skip connections [28]
(AttGAN-UNet). Table 1 lists the PSNR/SSIM results of
reconstruction by keeping target attribute vector the same as
the source one, and Fig. 3 shows the reconstruction results
of an image. It can be seen that adding skip connections
does beneﬁt the reconstruction of ﬁne details, and better re-
sult can be obtained with the increase of skip connections.
By setting target attribute vector different from source one,
Fig. 4 further assesses the facial attribute generation ac-
curacy via a facial attribute classiﬁcation model1. While

1We train the model on CelebA [23] dataset which can achieve 94.5%

mean accuracy on the 13 attributes we use.

3675

,/,38,.,743/,7743,70-7480,880803/074:9 503:89,.040,7/!,0$30;07,0997-:900307,943..:7,.99
9999
899
&09adding one skip connection, i.e., AttGAN, only slightly
decreases generation accuracy for most attributes, notable
degradation can be observed by adding multiple skip con-
nections. Thus, the deployment of skip connections im-
proves reconstruction image quality at the cost of weakened
attribute manipulation ability, mainly attributing to that skip
connection directly concatenates encoder and decoder fea-
tures. To circumvent this dilemma, we present our STGAN
to employ selective transfer units to adaptively transform
encoder features guided by attributes to be changed.

3.2. Taking Difference Attribute Vector as Input

Both StarGAN [7] and AttGAN [11] take target attribu-
tion vector attt and source image x as input to the gener-
ator. Actually, the use of full target attribution vector is re-
dundant and may be harmful to editing result. In Fig. 2, the
target attribution vector attt is exactly the same as on the
source one atts, but StarGAN [7] and AttGAN [11] may
manipulate some unchanged attributes by mistake. From
Fig. 2, after editing the face image with blond hair becomes
more blond. Moreover, they even incorrectly adjust hair
length of a source image with the attribute female.

For arbitrary image attribute editing, instead of full target
attribute vector, only the attributes to be changed should be
considered to preserve more information of source image.
So we deﬁne the difference attribute vector as the difference
between target and source attribute vectors,

attributes, and be consistent among different encoder layers.
Thus, we modify the structure of GRU [6, 8] to build STUs
for passing information from inner layers to outer layers.

Without loss of generality, we use the l -th encoder layer
as an example. Denote by f l
enc the encoder feature of the
l -th layer, and sl+1 the hidden state from the l + 1-th layer.
For convenience, the difference attribute vector attdiﬀ is
stretched to have the same spatial size of sl+1. Different
from sequence modeling, feature maps across layers are of
different spatial size. So we ﬁrst use transposed convolution
to upsample hidden state sl+1,

ˆsl+1 = Wt ∗T [sl+1, attdiﬀ ],

(2)

where [·, ·] denotes the concatenation operation, and ∗T de-
notes transposed convolution. Then, STU adopts the math-
ematical model of GRU to update the hidden state sl and
transformed encoder feature f l
t ,

rl = σ(Wr ∗ [f l

enc, ˆsl+1]),

zl = σ(Wz ∗ [f l

enc, ˆsl+1]),

sl = rl ◦ ˆsl+1,

ˆf l
t = tanh(Wh ∗ [f l

enc, sl ]),

(3)

(4)

(5)

(6)

(7)

attdiﬀ = attt − atts .

(1)

t = (1 − zl ) ◦ ˆsl+1 + zl ◦ ˆf l
f l
t ,

Taking attdiﬀ as input can bring several distinctive mer-
its. First, the attributes to be changed are only a small set
of attribute vector, and the use of attdiﬀ usually makes
the model easier to train. Second, in comparison to attt ,
attdiﬀ can provide more valuable information for guid-
ing image attribute editing, including whether an attribute
is required to edit or not, toward what direction an attribute
should be changed. The information can then be utilized
to design proper model to transform and concatenate en-
coder feature with decoder feature, and improve image re-
construction quality without sacriﬁce of attribute manipula-
tion accuracy. Finally, in practice attdiﬀ actually is more
convenient to be provided by user. When taking attt as
input, the user is required to either manually supply all tar-
get attributes, or modify source attributes provided by some
attribute prediction method.

3.3. Selective Transfer Units

Fig. 5 shows the overall architecture of our STGAN. In-
stead of directly concatenating encoder with decoder fea-
tures via skip connection, we present selective transfer unit
(STU) to selectively transform encoder feature, making it
compatible and complementary to decoder feature. Natu-
rally, the transform is required to be adaptive to the changed

where ∗ denotes the convolution operation, ◦ denotes entry-
wise product, and σ(·) stands for the sigmoid function.

The introduction of the reset gate rl and update gate zl
allows us control the contribution of hidden state, difference
attribute vector, and encoder feature in a selective manner.
Moreover, the convolution transform and linear interpola-
tion in Eqns. (6) and (7) provide an adaptive means for the
transfer of encoder feature and its combination with hidden
state. In comparison to GRU where f l
t is adopted as the out-
put of hidden state, we take sl as the output of hidden state
and f l
t as the output of transformed encoder feature. And
experiments empirically validate that such modiﬁcation can
bring moderate gains on attribute generation accuracy.

3.4. Network Architecture

Our STGAN is comprised of two components, i.e., a
generator G and a discriminator D . Fig. 5 illustrates the
network structure of G consisting of an encoder Genc for
abstract latent representation and a decoder Gdec for target
image generation. The encoder Genc contains ﬁve convolu-
tion layers with kernel size 4 and stride 2, while the decoder
Gdec has ﬁve transposed convolution layers. Besides, STU
is applied right after each of the ﬁrst four encoder layers,
denoted by (f l

enc, sl+1, attdiﬀ ).

t , sl ) = G l

st (f l

3676

Figure 5: The overall structure of STGAN. On the left is the generator. The top-right ﬁgure shows detailed STU structure, and all variables
marked in this ﬁgure share same dimension (e.g., 64 × 64). The difference attribute vector of adding Eyeglasses and removing Mouth Open
attributes is shown on the bottom-right.

The discriminator D has two branches Dadv and Datt .
Dadv consists of ﬁve convolution layers and two fully-
connected layers to distinguish whether an image is a fake
image or a real one. Datt shares the convolution layers with
Dadv , but predicts an attribute vector by another two fully-
connected layers. Please refer to the suppl. for more details
on the network architecture.

3.5. Loss Functions

Given an input image x, the encoder features can be ob-

tained by,

f = Genc(x),

(8)

where f = {f 1
are deployed to transform encoder features for each layer,

enc}. Then, guided by attdiﬀ , STUs

enc, ..., f 5

Lrec = kx − G(x, 0)k1,

(12)

where the ℓ1-norm k·k1 is adopted for preserving the sharp-
ness of reconstruction result.

Adversarial loss. When the target attributes are different
from source ones, i.e., attdiﬀ 6= 0, the ground-truth of
editing result will be unavailable. Therefore, adversarial
loss [9] is employed for constraining the editing result to
be indistinguishable from real images. In particular, we fol-
low Wasserstein GAN (WGAN) [3] and WGAN-GP [10],
and deﬁne the losses for training Dadv and G as,
LDadv = ExDadv (x) − EˆyDadv (ˆy)+

max
Dadv

λEˆx (cid:2)(k∇ˆxDadv (ˆx)k2 − 1)2(cid:3) ,
Dadv (G(x, attdiﬀ )),

LGadv = Ex,attdiﬀ

(13)

(14)

(f l

t , sl ) = G l

st (f l

enc, sl+1, attdiﬀ ),

(9)

max

G

Note that we adopt four STUs, and directly pass f 5
enc to
Gdec. The STUs deployed in different layers do not share
parameters due to that (i) the dimensions are different and
(ii) the features of inner layers are more abstract than those
of the outer layers.

Let ft = {f 1
can be given by,

t , ..., f 4

t }. Thus, the editing result of Gdec

ˆy = Gdec(f 5

enc, ft ),

and can be written by,

ˆy = G(x, attdiﬀ ).

(10)

(11)

In the following, we detail the reconstruction, adversarial,
and attribute manipulation losses which are collaborated to
train our STGAN.

Reconstruction loss. When the target attributes are exactly
the same as source ones, i.e., attdiﬀ = 0, it is natural to re-
quire that the editing result approximates the source image.
Thus the reconstruction loss is deﬁned as,

where ˆx is sampled along lines between pairs of real and
generated images.

Attribute manipulation loss. Even the ground-truth is
missing, we can require the editing result to possess the de-
sired target attributes. Thus, we introduce an attribute clas-
siﬁer Datt which shares the convolution layers with Dadv ,
and deﬁne the following attribute manipulation losses for
training Datt and generator G,

c

LDatt = −

X

[att(i)

s

log D (i)

att (x)+

i=1

c

(1 − att(i)

s ) log (1 − D (i)

att (x))],

LGatt = −

X

[att(i)

t

log D (i)

att (ˆy)+

i=1

(1 − att(i)

t ) log (1 − D (i)

att (ˆy))],

(15)

(16)

where att(i)
atts/t (Datt (x)).

s/t (D (i)

att (x)) denotes the i -th attribute value of

3677

σσˆ+ˆσσˆ+ˆσσˆ+ˆσσˆ+ˆσσˆ+ˆσσˆ+ˆσσˆ+ˆσσˆ+ˆTanhσσ1-lencfltf1ˆl+slrlzˆltflsTanhσσ1-lencfltf1ˆl+slrlzˆltfls001011001001011001000011101000011101001000-100001000-100tattsattdiffattEncoder/Decoder FeatureSelective Transfer UnitSTUTransferred FeatureDifference Attribute TensorSTUSTUSTUSTUMethod
PSNR/SSIM 15.28/0.430

IcGAN

FaderNet

AttGAN

StarGAN

STGAN

30.62/0.908

24.07/0.841

22.80/0.819

31.67/0.948

Method

Bald

Bangs

Eyebrows Glasses

Hair
Color

Male

Table 2: Reconstruction quality of the comparison methods on fa-
cial attribute editing task.

Model Objective. Taking the above losses into account, the
objective to train the discriminator D can be formulated as,

AttGAN
12.76%
StarGAN 11.28%
STGAN

30.04% 11.52% 15.68%
19.20% 32.28% 13.52%
75.96% 47.60% 69.96% 50.76% 56.20% 70.80%

34.28%
18.12%

10.64%
19.40%

Method

Mouth
Open

Mustache

Pale
Skin

Young

Average

AttGAN
20.40%
StarGAN 23.40%
STGAN

21.08% 15.16% 19.15%
16.52% 27.92% 19.27%
56.20% 69.76% 60.72% 62.40% 56.92% 61.58%

20.20%
10.04%

No

Beard
18.92%
20.36%

min
D

LD = −LDadv + λ1LDatt ,

(17)

Table 3: Results of user study for ranking the models on facial
attribute editing task.

and that for the generator G is,

min
G

LG = −LGadv + λ2LGatt + λ3Lrec,

(18)

Method
summer→winter
winter→summer

AttGAN StarGAN CycleGAN STGAN
60.5%
50.5%

24.9%
24.6%

4.7%
17.0%

9.9%
7.9%

where λ1, λ2, and λ3 are the model tradeoff parameters.

Table 4: Results of user study for ranking the models on season
conversion task.

4. Experiments

We train the model by the ADAM [15] optimizer with
β1 = 0.5 and β2 = 0.999. The learning rate is initial-
ized as 2 × 10−4 and decays to 2 × 10−5 for ﬁne-tuning
after 100 epochs. In all experiments, the tradeoff param-
eters in Eqns. (17) and (18) are set to λ1 = 1, λ2 = 10
and λ3 = 100. All the experiments are conducted in the
TensorFlow [1] environment with cuDNN 7.1 running on
a PC with Intel(R) Xeon(R) E3-1230v5 CPU 3.40GHz and
Nvidia GTX1080Ti GPU. The source code can be found at
https://github.com/csmliu/STGAN.git.

4.1. Facial Attribute Editing

Following [7, 11], we ﬁrst evaluate our STGAN for ar-
bitrary facial attribute editing on the CelebA dataset [23]
which has been adopted by most relevant works [7, 11, 17,
26].

Dataset and preprocessing. The CelebA dataset [23] con-
tains 202,599 aligned facial images cropped to 178 × 218,
with 40 with/without attribute labels for each image. The
images are divided into training set, validation set and test
set. We take 1,000 images from the validation set to assess
the training process, use the rest of the validation set and the
training set to train our STGAN model, and utilize the test
set for performance evaluation. We consider 13 attributes,
including Bald, Bangs, Black Hair, Blond Hair, Brown Hair,
Bushy Eyebrows, Eyeglasses, Male, Mouth Slightly Open,
Mustache, No Beard, Pale Skin and Young, due to that they
are more distinctive in appearance and cover most attributes
used by the relevant works. In our experiment, the central
170 × 170 region of each image is cropped and resized to
128 × 128 by bicubic interpolation. Training and inference
time please refer to the suppl.

Qualitative results. We compare STGAN with four
IcGAN [26], FaderNet [17],
competing methods,
AttGAN [11] and StarGAN [7]. The qualitative results are

i.e.,

Figure 7: Attribute generation accuracy of IcGAN [26], Fader-
Net [17], AttGAN [11], StarGAN [7] and STGAN.

shown in Fig. 6. The results of AttGAN are generated by
the released model, and we retrain other models for a fair
comparison. It can be observed from Fig. 6, all the com-
peting methods are still limited in manipulating complex
attributes, e.g., Bald, Hair, and Age, and are prone to over-
smoothing results. Besides, their results are more likely
to be insufﬁciently modiﬁed and photo non-realistic when
dealing with complex and/or multiple attributes. In com-
parison, our STGAN is effective in correctly manipulating
the desired attributes, and can produce results with high im-
age quality. More editing results are given in the suppl.

Quantitative evaluation. The performance of attribute
editing can be evaluated from two aspects, i.e., image qual-
ity and attribute generation accuracy. Due to the unavail-
ability of editing result, we resort to two alternative mea-
sures for quantitative evaluation of our STGAN. First, we
use the training set of STGAN to train a deep attribute clas-
siﬁcation model which can attain an accuracy of 94.5% for
the 13 attributes on the test set. Then Fig. 7 shows the at-
tribute generation accuracy, i.e., classiﬁcation accuracy on
the changed attributes of editing results. It can be seen that

3678

,/,38,.,743/,7743,70-7480,880803/074:9 503:89,.040,7/!,0$30;07,0997-:900307,943..:7,..,/070999$9,7$%Figure 6: Facial attribute editing results on the CelebA dataset. The rows from top to down are results of IcGAN [26], FaderNet [17],
AttGAN [11], StarGAN [7] and STGAN.

our STGAN outperforms all the competing methods with a
large margin. For the attributes Bald, Black Hair, Brown
Hair, and Eyebrows, STGAN achieves 20% accuracy gains
against the competing methods.

As for image quality, we keep target attribute vector the
same as the source one, and give the the PSNR/SSIM re-
sults of reconstruction in Table 2. Beneﬁted from the STUs
and difference attribute vector, our STGAN achieves much
better reconstruction (> 7 dB by PSNR) in comparison to
AttGAN and STGAN. The result is consistent with Fig. 2.
The reconstruction ability of IcGAN is very limited due to
the training procedure. FaderNet obtains better reconstruc-
tion results, mainly ascribing to that each FaderNet model
is trained to deal with only one attribute.

User study. User study on a crowdsourcing platform
is conducted to evaluate the generation quality of three
top-performance methods,
i.e., AttGAN, StarGAN and
STGAN. We consider 11 tasks for 13 attributes, as the trans-
fer among Blond Hair, Black Hair and Brown Hair are
merged into Hair Color. For each task, 50 validated peo-
ple participate in and each of them is given 50 questions. In
each question, people are given a source image randomly
selected from test set and the editing results by AttGAN,
StarGAN and STGAN. For a fair comparison, the results
are shown in a random order. The users are instructed to
choose the best result which changes the attribute more suc-
cessfully, is of higher image quality and better preserves the
identity and ﬁne details of source image. The results are
shown in Table 3, and STGAN has higher probability to be
selected as the best method on all the 11 tasks.

4.2. Season Translation

We further train our STGAN for image-to-image trans-
lation between summer and winter using the dataset re-
leased by CycleGAN [34]. The dataset contains photos of
Yosemite, including 1,231 summer and 962 winter images
in the training set, and 309 summer and 238 winter images
for testing. We also randomly select 100 images from the
training set to validation. All images are used as the original
size of 256 × 256.

We compare our STGAN with AttGAN [11], Star-
GAN [7], and CycleGAN released by Zhu et al. [34]. Note
that CycleGAN uses two generators respectively for sum-
mer→winter and winter→summer translation, while the
other three methods conduct the two tasks with a single
model. Fig. 8 shows several examples of translation results.
It can be seen that STGAN performs favorably against the
competing methods. We also conduct a user study using the
same setting for facial attribute editing. From Table 4, our
STGAN has a probability of more than 50% to win among
the four competing methods.

5. Ablation Study

Using facial attribute editing, we implement several vari-
ants of STGAN, and evaluate them on CelebA [23] to as-
sess the role of difference attribute vector and STUs. Con-
cretely, we consider six variants, i.e., (i) STGAN: original
STGAN, (ii) STGAN-dst: substituting difference attribute
vector with target attribute vector, (iii) STGAN-conv: in-
stead of STU, applying a convolution operator by taking
encoder feature and difference attribute vector as input to
modify encoder feature, (iv) STGAN-conv-res: adopting

3679

Raw ImageReconstructionTo BaldAdd EyeglassesTo FemaleAdd BangsTo Mouth OpenTo OldTo Light EyebrowsAdd BeardTo Pale SkinAdd MustacheTo OldTo FemaleTo Brown HairTo Blond HairDifference attribute vector vs. target attribute vector.
In Fig. 9, we present the comparison results of AttGAN,
StarGAN and STGAN-dst with their counterparts (i.e.,
AttGAN-diff, StarGAN-diff and STGAN) by using differ-
ence attribute vector. One can see that difference attribute
vector generally beneﬁt attribute generation accuracy for all
the three models. Moreover, empirical studies show that the
use of difference attribute vector gives rise to training sta-
bility as well as image reconstruction performance. Note
that while AttGAN-diff and StarGAN-diff perform better
than AttGAN and StarGAN, they still suffer from the poor
image quality.

Selective Transfer Unit vs. its variants. Fig. 10 reports
the attribute generation accuracy of several STGAN vari-
ants for transforming encoder feature conditioned on differ-
ence attribute vector. The two convolutional methods, i.e.,
STGAN-conv and STGAN-conv-res, are signiﬁcantly infe-
rior to STGAN, indicating that they are limited in selective
transfer of encoder feature. In comparison to STGAN-conv,
STGAN-conv-res achieves relatively higher attribute gener-
ation accuracy. So we also compare STGAN with STGAN-
res to check whether STU can be improved via residual
learning. However, due to the selective ability of STUs,
further deployment of residual learning cannot bring any
gains for most attributes, and performs worse for several
global (e.g., Gender, Age) and ﬁne (e.g., Mustache, Beard)
attributes. Finally, STGAN is compared with STGAN-gru
by using transformed feature as hidden state. Although
STGAN-gru performs better on Bald, STGAN is slightly
superior to STGAN-gru for most attributes and the gain is
notable for attributes Gender and Mustache.

6. Conclusion

In this paper, we study the problem of arbitrary image at-
tribute editing for selective transfer perspective, and present
a STGAN model by incorporating difference attribute vec-
tor and selective transfer units (STUs) in encoder-decoder
network. By taking difference attribute vector rather than
target attribute vector as model input, our STGAN can fo-
cus on editing the attributes to be changed, which greatly
improves the image reconstruction quality and enhances the
ﬂexible translation of attributes. Furthermore, STUs are
presented to adaptively select and modify encoder feature
tailored to speciﬁc attribute editing task, thereby improving
attribute manipulation ability and image quality simultane-
ously. Experiments on arbitrary facial attribute editing and
season translation show that our STGAN performs favor-
ably against state-of-the-arts in terms of attribute generation
accuracy and image quality of editing results.

Acknowledgement. This work was supported in part by the
National Natural Science Foundation of China under grant
No. 61671182 and 61872118.

3680

Input

AttGAN

StarGAN

CycleGAN

STGAN

Figure 8: Results of season translation, the top two rows are sum-
mer→winter, and the bottom two rows are winter→summer.

Figure 9: Effect of difference attribute vector on AttGAN, Star-
GAN and STGAN.

Figure 10: Attribute generation accuracy of STGAN variants.

the residual learning formulation to learn the convolution
operator in STGAN-conv, (v) STGAN-gru: replacing STU
with GRU in STGAN, (vi) STGAN-res: adopting the resid-
ual learning formulation to learn the STU in STGAN. We
also train AttGAN and StarGAN models with difference at-
tribute vector, denoted by AttGAN-diff and StarGAN-diff.
Figs. 9 and 10 show their results on attribute manipulation.
Please refer to the suppl. for qualitative results.

,/,38,.,743/,7743,70-7480,880803/074:9 503:89,.040,7/!,0$30;07,0997-:900307,943..:7,.9999
/11$9,7$9,7
/11$%
/89$%,/,38,.,743/,7743,70-7480,880803/074:9 503:89,.040,7/!,0$30;07,0997-:900307,943..:7,.$%
.43;$%
.43;
708$%
7:$%
708$%References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning. In USENIX con-
ference on Operating Systems Design and Implementation,
pages 265–283, 2016. 6

[2] Asha Anoosheh, Eirikur Agustsson, Radu Timofte, and Luc
Van Gool. Combogan: Unrestrained scalability for image
domain translation. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 783–790, 2018. 3
[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
2, 5

[4] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks.
In IEEE Inter-
national Conference on Computer Vision, pages 1511–1520,
2017. 3

[5] Ying-Cong Chen, Huaijia Lin, Michelle Shu, Ruiyu Li, Xin
Tao, Xiaoyong Shen, Yangang Ye, and Jiaya Jia. Facelet-
bank for fast portrait manipulation.
In IEEE Conference
on Computer Vision and Pattern Recognition, pages 3541–
3549, 2018. 3

[6] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. In Con-
ference on Empirical Methods in Natural Language Process-
ing, pages 1724–1734, 2014. 4

[7] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation.
In IEEE Conference on Computer Vision and
Pattern Recognition, pages 8789–8797, 2018. 1, 2, 3, 4, 6, 7
[8] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and
Yoshua Bengio. Empirical evaluation of gated recurrent
neural networks on sequence modeling.
arXiv preprint
arXiv:1412.3555, 2014. 4

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
Neural Information Processing Systems, pages 2672–2680,
2014. 1, 2, 3, 5

[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville.
Improved training of
wasserstein gans. In Advances in Neural Information Pro-
cessing Systems, pages 5767–5777, 2017. 2, 5

[11] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan,
and Xilin Chen. Arbitrary facial attribute editing: Only
change what you want. arXiv preprint arXiv:1711.10678,
2017. 1, 2, 3, 4, 6, 7

[12] Geoffrey E Hinton and Richard S Zemel. Autoencoders,
minimum description length and helmholtz free energy. In
Advances in Neural Information Processing Systems, pages
3–10, 1994. 2

[13] Le Hui, Xiang Li,

Jiaxin Chen, Hongliang He, and
Jian Yang. Unsupervised multi-domain image translation

with domain-speciﬁc encoders/decoders.
arXiv:1712.02050, 2017. 3

arXiv preprint

[14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-
In IEEE Conference on Computer Vision

Efros.
sarial networks.
and Pattern Recognition, pages 5967–5976, 2017. 2, 3

[15] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 6

[16] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2

[17] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, An-
Fader networks:
toine Bordes, Ludovic Denoyer, et al.
Manipulating images by sliding attributes.
In Advances in
Neural Information Processing Systems, pages 5967–5976,
2017. 2, 3, 6, 7

[18] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo
Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Confer-
ence on Machine Learning, pages 1558–1566, 2016. 2

[19] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wen-
zhe Shi. Photo-realistic single image super-resolution us-
ing a generative adversarial network.
In IEEE Conference
on Computer Vision and Pattern Recognition, pages 4681–
4690, 2017. 3

[20] Mu Li, Wangmeng Zuo, and David Zhang.

Deep
identity-aware transfer of facial attributes. arXiv preprint
arXiv:1610.05586, 2016. 1, 3

[21] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang
Lin, and Ruigang Yang. Learning warped guidance for blind
face restoration. In European Conference on Computer Vi-
sion, pages 272–289, 2018. 3

[22] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In Advances in Neural
Information Processing Systems, pages 700–708, 2017. 3

[23] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild.
In IEEE Inter-
national Conference on Computer Vision, pages 3730–3738,
2015. 3, 6, 7

[24] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2

[25] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting.
In IEEE Conference on Computer
Vision and Pattern Recognition, pages 2536–2544, 2016. 3

[26] Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, and
Jose M ´Alvarez. Invertible conditional gans for image edit-
ing. arXiv preprint arXiv:1611.06355, 2016. 1, 2, 3, 6, 7

[27] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
tional generative adversarial networks.
arXiv preprint
arXiv:1511.06434, 2015. 2, 3

[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In International Conference on Medical Image Computing

3681

and Computer-Assisted Intervention, pages 234–241, 2015.
2, 3

[29] Wei Shen and Rujie Liu. Learning residual images for face
attribute manipulation.
In IEEE Conference on Computer
Vision and Pattern Recognition, pages 4030–4038, 2017. 1,
3

[30] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre Antoine Manzagol. Extracting and composing robust
features with denoising autoencoders. In International Con-
ference on Machine Learning, pages 1096–1103, 2008. 2

[31] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 8798–8807, 2018. 3

[32] Gang Zhang, Meina Kan, Shiguang Shan, and Xilin Chen.
Generative adversarial network with spatial attention for face
attribute editing. In European Conference on Computer Vi-
sion, pages 417–432, 2018. 2, 3

[33] Bo Zhao, Bo Chang, Zequn Jie, and Leonid Sigal. Modular
generative adversarial networks. In European Conference on
Computer Vision, pages 150–165, 2018. 3

[34] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In IEEE International Con-
ference on Computer Vision, pages 2223–2232, 2017. 1, 2,
3, 7

3682

