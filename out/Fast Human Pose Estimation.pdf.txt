Fast Human Pose Estimation

Feng Zhang1

Xiatian Zhu2

Mao Ye1

1

{zhangfengwcy, cvlab.uestc}@gmail.com, School of Computer Science and Engineering,

University of Electronic Science and Technology of China

2

eddy@visionsemantics.com, Vision Semantics Limited

Abstract

Existing human pose estimation approaches often only
consider how to improve the model generalisation perfor-
mance, but putting aside the signiﬁcant efﬁciency problem.
This leads to the development of heavy models with poor
scalability and cost-effectiveness in practical use. In this
work, we investigate the under-studied but practically crit-
ical pose model efﬁciency problem. To this end, we present
a new Fast Pose Distillation (FPD) model learning strat-
egy. Speciﬁcally, the FPD trains a lightweight pose neural
network architecture capable of executing rapidly with low
computational cost. It is achieved by effectively transferring
the pose structure knowledge of a strong teacher network.
Extensive evaluations demonstrate the advantages of our
FPD method over a broad range of state-of-the-art pose es-
timation approaches in terms of model cost-effectiveness on
two standard benchmark datasets, MPII Human Pose and
Leeds Sports Pose.

1. Introduction

Human pose estimation has gained remarkable progress
from the rapid development of various deep CNN mod-
els [30, 8, 10]. This is because deep neural networks are
strong at approximating complex and non-linear mapping
functions from arbitrary person images to the joint locations
even at the presence of unconstrained human body appear-
ance, viewing conditions and background noises.

Nevertheless, the model performance advantages come
with the cost of training and deploying resource-intensive
networks with large depth and width. This causes inefﬁcient
model inference, requiring per-image computing cost at
tens of FLoating point OPerations (FLOPs) therefore poor
scalability particularly on resource-limited devices such as
smart phones and robots. There is a recent attempt that bina-
rises the network parameters for model execution speedup
[7], which however suffers signiﬁcantly weak model gener-
alisation capacity.

In this study, we consider the problem of improving

the pose estimation efﬁciency without model performance
degradation but preserving comparable accuracy results.
We observe that the basic CNN building blocks for state-of-
the-art human pose networks such as Hourglass [19] are not
cost-effective in establishing small networks due to a high
number of channels per layer and being more difﬁcult to
train. To overcome these barriers, we design a lightweight
variant of Hourglass network and propose a more effective
training method of small pose networks in a knowledge dis-
tillation fashion [13]. We call the proposed method Fast
Pose Distillation (FPD). Compared with the top-performing
alternative pose approaches [32, 10], the proposed FPD ap-
proach enables much faster and more cost-effective model
inference with extremely smaller model size while simulta-
neously reaching the same level of human pose prediction
performance.

We summarise our contributions in follows:

(i) We investigate the under-studied human pose model
efﬁciency problem, opposite to the existing attempts
mostly focusing on improving the accuracy perfor-
mance alone at high costs of model inference at de-
ployment. This is a critical problem to be addressed
for scaling up the existing deep pose estimation meth-
ods to real applications.

(ii) We propose a Fast Pose Distillation (FPD) model
training method enabling to more effectively train ex-
tremely small human pose CNN networks. This is
based on an idea of knowledge distillation that have
been successfully exploited in inducing object image
categorisation deep models.
In particular, we de-
rive a pose knowledge distillation learning objective to
transfer the latent knowledge from a pre-trained larger
teacher model to a tiny target pose model (to be de-
ployed in test time). This aims to pursue the best model
performance given very limited computational budgets
using only a small fraction (less than 20%) of cost re-
quired by similarly strong alternatives.

(iii) We design a lightweight Hourglass network capable of
constructing more cost-effective pose estimation CNN

3517

models while retaining sufﬁcient learning capacity for
allowing satisfactory accuracy rates. This is achieved
by extensively examining the redundancy degree of ex-
isting state-of-the-art pose CNN architecture designs.

In the evaluations, we have conducted extensive empir-
ical comparisons to validate the efﬁcacy and superiority of
the proposed FPD method over a wide variety of state-of-
the-art human pose estimation approaches in the balance of
model inference efﬁciency and prediction performance on
two commonly adopted benchmark datasets, MPII Human
Pose [1] and Leeds Sports Pose [15].

2. Related Work

Human Pose Estimation The past ﬁve years have wit-
nessed a huge progress of human pose estimation in the
deep learning regime [30, 28, 6, 31, 19, 11, 32, 20, 22]. De-
spite the clear performance increases, these prior works fo-
cus only on improving the pose estimation accuracy by us-
ing complex and computationally expensive models whilst
largely ignoring the model inference cost issue. This sig-
niﬁcantly restricts their scalability and deployability in real-
world applications particularly with very limited computing
budgets available.

In the literature, there are a few recent works designed
to improve model efﬁciency. For example, Bulat and Tz-
imiropoulos built parameter binarised CNN models to ac-
commodate resource-limited platforms [7]. But this method
leads to dramatic performance drop therefore not satisﬁed
for reliable utilisation. In most cases, high accuracy rates
are required. Raﬁ et al. exploited good general purpose
practices to improve model efﬁciency without presenting a
novel algorithm [24]. Further, this method does not pro-
vide quantitative evaluation on the trade-off between model
efﬁciency and effectiveness.

In contrast to these previous methods, we systematically
study the pose estimation efﬁciency problem under the con-
dition of preserving the model performance rate so that the
resulted model is more usable and reliable in real-world ap-
plication scenarios.

Knowledge Distillation The objective of knowledge
distillation is concerned with information transfer between
different neural networks with distinct capacities [5, 13, 3].
For instance, Hinton et al. successfully employed a well
trained large network to help train a small network [13].
The rationale is an exploitation of extra supervision from
a teacher model, represented in form of class probabilities
[13], feature representations [3, 25], or an inter-layer ﬂow
[35]. This principle has also been recently applied to accel-
erate the model training process of large scale distributed
neural networks [2], to transfer knowledge between multi-
ple layers [17] or between multiple training states [18]. Be-
yond the conventional two stage training based ofﬂine dis-

tillation, one stage online knowledge distillation has been
attempted with added merits of more efﬁcient optimisation
[37, 16] and more effective learning [16]. Besides, knowl-
edge distillation has been exploited to distil easy-to-train
large networks into harder-to-train small networks [25].

While these past works above transfer category-level dis-
criminative knowledge, our method transfers richer struc-
tured information of dense joint conﬁdence maps. A more
similar work is the latest radio signals based pose model that
also adopts the idea of knowledge distillation [38]. How-
ever, this method targets at using wireless sensors to tackle
the occlusion problem, rather than the model efﬁciency is-
sue as we conﬁder here.

3. Fast Human Pose Estimation

Human pose estimation aims to predict the spatial coor-
dinates of human joints in a given image. To train a model
in a supervised manner, we often have access to a training
dataset {I i, Gi}N
i=1 of N person images each labelled with
K joints deﬁned in the image space as:

Gi = {gi

1, .., gi

K} ∈ RK×2,

(1)

where H and W denotes the image height and width, re-
spectively. Generally, this is a regression problem at the
imagery pixel level.

Objective Loss Function For pose model training, we
often use the Mean-Squared Error (MSE) based loss func-
tion [29, 19]. To represent the ground-truth joint labels,
we generate a conﬁdence map mk for each single joint k
(k ∈ {1, · · · , K}) by centring a Gaussian kernel around the
labelled position zk = (xk, yk).

More speciﬁcally, a Gaussian conﬁdence map mk for

the k-th joint label is written as:

1

mk(x, y) =

2πσ2 exp(cid:16) −[(x − xk)2 + (y − yk)2]

(cid:17) (2)
where (x, y) speciﬁes a pixel
location and the hyper-
parameter σ denotes a pre-ﬁxed spatial variance. The MSE
loss function is then obtained as:

2σ2

Lmse =

1
K

K

X

k=1

kmk − ˆmkk2

2

(3)

where ˆmk refers to the predicted conﬁdence map for the
k-th joint. The standard SGD algorithm can then be used
to optimise a deep CNN pose model by back-propagating
MSE errors on training data in a mini-batch incrementally.
Existing pose methods rely heavily on large deep neural
networks for maximising the model performance, whilst ne-
glecting the inference efﬁciency. We address this limitation
for higher scalability by establishing lightweight CNN ar-
chitectures and proposing an effective model learning strat-
egy detailed below.

3518

Figure 1. An overview of the proposed Fast Pose Distillation model learning strategy. To establish a highly cost-effective human pose
estimation model, We need to build a compact backbone such as (a) a lightweight Hourglass network. To more effectively train a small
target network, we adopt the principle of knowledge distillation in the pose estimation context. This requires to (b) pre-train a strong
teacher pose model, such as the state-of-the-art Hourglass network or other existing alternatives. The teacher model is used to provide
extra supervision guidance in the (c) pose knowledge distillation procedure via the proposed mimicry loss function. At test time, the small
target pose model enables a fast and cost-effective deployment. The computationally expensive teacher model is abandoned ﬁnally, since
its discriminative knowledge transferred already into the target model therefore used in deployment (rather than wasted).

Stage

Building Block

1, 2, 3, 4 Hourglass with 128 channels per layer

Table 1. The structure of a small pose CNN model.

3.1. Compact Pose Network Architecture

Human pose CNN models typically consist of multi-
ple repeated building blocks with the identical structure
[8, 31, 19, 11, 32, 20, 22]. Among these, Hourglass is one
of the most common building block units [19]. However,
we observe that existing designs are not cost-effective, due
to deploying a large number of both channels and blocks
in the entire architecture therefore leading to a suboptimal
trade-off between the representation capability and the com-
putational cost. For example, [19] suggested a CNN archi-
tecture of 8 Hourglass stages each having 9 Residual blocks
with 256 channels within every layer.

We therefore want to minimise the expense of exist-
ing CNN architectures for enabling faster model inference.
With careful empirical examination, we surprisingly re-
vealed that a half number of stages (i.e. 4 Hourglass mod-
ules) sufﬁce to achieve over 95% model generalisation ca-
pacity on the large scale MPII benchmark. Moreover, the
per-layer channels are also found highly redundant and re-
ducing a half number (128) only results in less than 1%
performance drop (Table 5). Based on these analysis, we
construct a very light CNN architecture for pose estimation
with only one sixth computational cost of the original de-
sign. See Table 1 and Figure 1 for the target CNN architec-
ture speciﬁcations.

Remarks Whilst it is attractive to deploy tiny pose net-
works that run cheaply and fast, it is empirically non-trivial
to train them although theoretically shallow networks have

the similar representation capacities to approximate the tar-
get functions as learned by deeper counterparts [3, 26]. A
similar problem has been occurred and investigated in ob-
ject image classiﬁcation through the knowledge distillation
strategy, i.e. let the target small network mimic the predic-
tion of a larger teacher model [13]. However, it remains
unclear how well such a similar method will work in ad-
dressing structured human pose estimation in dense pixel
space. To answer this question, in the following we present
a pose structure knowledge distillation method.

3.2. Supervision Enhancement by Pose Distillation

Model Training Pipeline We adopt the generic model

training strategy of knowledge distillation:

1. We ﬁrst train a large teacher pose model. In our ex-
periments, by default we select the original Hourglass
model [19] due to its clean design and easy model
training. Other stronger models can be considered
without any restrictions.

2. We then train a target student model with the assistance
of knowledge learned by the teacher model. Knowl-
edge distillation happens in this step. The structure of
the student model is presented in Table 1.

An overview of the whole training procedure is depicted
in Figure 1. The key to distilling knowledge is to design a
proper mimicry loss function that is able to effectively ex-
tract and transfer the teacher’s knowledge to the training of
the student model. The previous distillation function is de-
signed for single-label based softmax cross-entropy loss in
the context of object categorisation [3, 13] and unsuitable to
transfer the structured pose knowledge in 2D image space.

3519

To address this aforementioned problem, we design a
joint conﬁdence map dedicated pose distillation loss func-
tion formulated as:

Lpd =

1
K

K

X

k=1

kms

k − mt

kk2

2

(4)

k and mt

where ms
k specify the conﬁdence maps for the k-
th joint predicted by the pre-trained teacher model and the
in-training student target model, respectively. We choose
the MSE function as the distillation quantity to measure the
divergence between the student and teacher models in or-
der to maximise the comparability with the pose supervised
learning loss (Eqn (3)).

Overall Loss Function We formulate the overall FPD
loss function for pose structure knowledge distillation dur-
ing training as:

Lfpd = αLpd + (1 − α)Lmse

(5)

where α is the balancing weight between the two loss terms,
estimated by cross-validation. As such, the target network
learns both to predict the labelled ground-truth annotations
of training samples by Lmse and to match the prediction
structure of the stronger teacher model by Lpd.

Further Remarks Why does the proposed pose distil-
lation loss function probably help to train a more gener-
alisable target model, as compared to training only on the
labelled data? A number of reason may explain this in the
context of pose estimation.

1. The body joint labels are likely to be erroneous due to
the high difﬁculty of locating the true positions in the
manual annotation process. In such cases, the teacher
model may be able to mitigate some errors through sta-
tistical learning and reasoning therefore reducing the
misleading effect of wrongly labelled training samples
(Figure 3 Row (A)).

2. Given difﬁcult

training cases

say with confus-
ing/cluttered background and random occlusion situ-
ations, the teacher prediction may provide softened
learning tasks by explained away these hard samples
with model inference (Figure 3 Row (B)).

3. The teacher model may provide more complete joint
labels than the original annotation therefore not only
providing additional more accurate supervision but
also mitigating the misleading of missing joint labels
(Figure 3 Row (C)).

4. Learning to match the ground-truth conﬁdence map
can be harder in comparison to aligning the teacher’s
prediction. This is because the teacher model has
spread some reasoning uncertainty for each training
sample either hard or easy to process.

5. On the other hand, the teacher’s conﬁdence map en-
codes the abstract knowledge learned from the entire
training dataset in advance, which may be beneﬁcial
to be considered in learning every individual training
sample during knowledge distillation.

In summary, the proposed model is capable of handling
wrong pose joint annotations, e.g. when the pre-trained
teacher predicts more accurate joints than manual wrong
and missing labels. Due to a joint use of the ground-truth
labels and the teacher model’s prediction, our model is tol-
erant to either error but not co-occurring ones. This allevi-
ates the harm of label errors in the training data, in contrast
to existing methods that often blindly trust all given labels.

3.3. Model Training and Deployment

The proposed FPD model training method consists of
two stages: (i) We train a teacher pose model by the con-
ventional MSE loss (Eqn (3)), and (ii) train a target student
model by the proposed loss (Eqn (5)), with the knowledge
distillation from the teacher model to the target model be-
ing conducted in each mini-batch and throughout the entire
training process. At test time, we only use the small tar-
get model for efﬁcient and cost-effective deployment whilst
throwing away the heavy teacher network. The target model
already extracts the teacher’s knowledge.

4. Experiments

4.1. Experiment Setup

Datasets We utilised two human pose benchmark
datasets, MPII [1] and Leeds Sports Pose (LSP) [15]. The
MPII dataset is collected from YouTube videos with a wide
range of human activities and events. It has 25K scene im-
ages and 40K annotated persons (29K for training and 11K
for test). Each person has 16 labelled body joints. We
adopted the standard train/valid/test data split [28]. Fol-
lowing [29], we randomly sampled 3K samples from the
training set for model validation.

The LSP benchmark contains natural person images
from many different sports scenes. Its extended version pro-
vides 11K training samples and 1K test samples. Each per-
son in LSP has 14 labelled joints.

Performance Metrics We used the standard Percentage
of Correct Keypoints (PCK) measurement that quantiﬁes
the fraction of correct predictions within an error thresh-
old τ [34]. Speciﬁcally, the quantity τ is normalised against
the size of either torso (τ = 0.2 for LSP, i.e. PCK@0.2)
or head (τ = 0.5 for MPII, i.e. PCKh@0.5). We measured
each individual joint respectively and took their average as
an overall metric. Using different τ values, we yielded a
PCK curve. Therefore, the Area Under Curve (AUC) can
be obtained as a holistic measurement across different de-

3520

Method

Head Sho. Elbo. Wri. Hip Knee Ank. Mean AUC # Param Deployment Cost

Wei et al., CVPR’16[31]

Raﬁ et al., BMVC’16[24]

Belagiannis&Zisserman, FG’17[4]
Insafutdinov et al., ECCV’16[14]

97.2 93.9 86.4 81.3 86.8 80.6 73.4
97.7 95.0 88.2 83.0 87.9 82.6 78.4
96.8 95.2 89.3 84.4 88.4 83.4 78.0
97.8 95.0 88.7 84.0 88.4 82.8 79.4
Bulat&Tzimiropoulos, ECCV’16[6] 97.9 95.1 89.9 85.3 89.4 85.7 81.7
98.2 96.3 91.2 87.1 90.1 87.4 83.6
98.1 96.3 92.2 87.8 90.6 87.6 82.7
98.5 96.3 91.9 88.1 90.6 88.0 85.0
98.1 96.6 92.5 88.4 90.7 87.7 83.5
98.5 96.7 92.5 88.7 91.1 88.6 86.0
98.6 96.9 93.0 89.1 91.7 89.0 86.2

Ning et al., TMM’17[21]
Chu et al., CVPR’17[11]
Peng et al., CVPR’18[22]
Yang et al., ICCV’17[32]
Nie et al., CVPR’18[20]

Newell et al., ECCV’16[19]

Sekii, ECCV18[27]

-

-

-

-

-

-

-

86.3 57.3
88.1 58.8
88.5 60.8
88.5 61.4
89.7 59.6
90.9 62.9
91.2 63.6
91.5 63.8
91.5
92.0 64.2
92.4 65.9
88.1

-

-

FPD

98.3 96.4 91.5 87.4 90.9 87.1 83.7

91.1 63.5

56M
17M
66M
31M
76M
26M
74M
58M
26M
28M
26M
16M

3M

28G
95G
286G
351G
67G
55G
124G
128G
55G
46G
63G
6G

9G

Table 2. PCKh@0.5 and AUC (%) rates on the MPII test dataset. M/G: 106/109.

Method

Head Sho. Elbo. Wri. Hip Knee Ank. Mean AUC # Param Deployment Cost

Fan et al., CVPR’15[12]

Tompson et al., NIPS’14[29] 90.6 79.2 67.9 63.4 69.5 71.0 64.2
92.4 75.2 65.3 64.0 75.7 68.3 70.4
90.5 81.8 65.8 59.8 81.6 70.6 62.0
91.8 78.2 71.8 65.5 73.3 70.2 63.4
90.6 78.1 73.8 68.8 74.8 69.9 58.9
95.8 86.2 79.3 75.0 86.6 83.8 79.8
87.2 88.2 82.4 76.3 91.4 85.8 78.7

Carreira et al., CVPR’16[8]
Chen&Yuille, NIPS’14[9]
Yang et al., CVPR’16[33]
Raﬁ et al., BMVC’16[24]
Yu et al., ECCV’16[36]

72.3 47.3
73.0 43.2
73.1 41.5
73.4 40.1
73.6 39.3
83.8 56.9
84.3 55.2

Peng et al., CVPR’18[22]

98.6 95.3 92.8 90.0 94.8 95.3 94.5

94.5

-

FPD

97.3 92.3 86.8 84.2 91.9 92.2 90.9

90.8 64.3

-
-
-
-
-

56M

-

26M

3M

-
-
-
-
-

28G

-

55G

9G

Table 3. PCK@0.2 and AUC (%) rates on the LSP test dataset. M/G: 106/109.

cision thresholds. To measure the model efﬁciency both in
training and test, we used the FLOPs.

Training Details We implemented all the following ex-
periments in Torch. We cropped all the training and test
images according to the provided positions and scales, and
resized them to 256×256 in pixels. As typical, random
scaling (0.75-1.25), rotating (±30 degrees) and horizontal
ﬂipping were performed to augment the training data. We
adopted the RMSProp optimisation algorithm. We set the
learning rate to 2.5 × 10−4, the mini-batch size to 4, and
the epoch number to 130 and 70 for MPII and LSP bench-
marks, respectively. For the network architecture, we used
the original Hourglass as the teacher model and the cus-
tomised Hourglass with less depth and width (Table 1) as
the target model.

4.2. Comparisons to State Of The Art Methods

Results on MPII Table 2 compares the PCKh@0.5 ac-
curacy results of state-of-the-art methods and the proposed
FPD on the test dataset of MPII. It is clearly observed that
the proposed FPD model is signiﬁcantly efﬁcient and com-
pact therefore achieving a much cheaper deployment cost.
Importantly, this advantage is obtained without clearly com-
promising the model generalisation capability, e.g. achiev-
ing as high as 91.1%.

Speciﬁcally, compared with the best performer [20], the
FPD model only requires 14.3% (9/63) computational cost
but gaining 96.4% (63.5/65.9) performance in mean PCKh
accuracy. This leads to a 6.7%× (96.4/14.3) cost-effective
advantage. When compared to the most efﬁcient alterna-
tive competitor [24], our model is 2.9× (26/9) more efﬁ-
cient whilst simultaneously achieving a mean PCKh gain
of 4.8% (91.1-86.3). These evidences clearly suggest the
cost-effectiveness advantages of our method over other al-
ternative approaches.

We evaluated the proposed FPD method by extensively
comparing against recent human pose estimation deep
methods on MPII and LSP.

In pose estimation, an improvement of 0.8% indicates a
signiﬁcant gain particularly on the challenging MPII with
varying poses against cluttered background. This boost

3521

P
S
L

I
I
P
M

Figure 2. Example of human pose estimation on LSP and MPII.

Figure 3. Pose estimation examples on MPII by the proposed FPD model. Column (1): The input images. Column (2): Ground-truth joint
conﬁdence maps. Column (3): Joint conﬁdence maps predicted by the teacher model. Column (4): The difference between ground-truth
and teacher’s conﬁdence map. Each row represents a type of pose knowledge transfer. Row (A): Error labelling of the right leg ankle in the
“ground-truth” annotations, which is corrected by the teacher model. Row (B): A softened teacher conﬁdence map with larger uncertainty
than the ground-truth due to the highly complex human posture. Row (C): Missing joint labels are discovered by the teacher model.

3522

FPD

✗

✓

Head

97.4
97.5

Sho.

96.0
96.3

Elbo.

90.2
91.4

Wri.

85.8
87.3

Hip

88.2
89.4

Knee

84.3
85.6

Ank.

80.6
82.0

Mean

89.4
90.4

AUC

61.4
62.4

Table 4. Generalisation evaluation of the proposed FPD approach. Metric: Mean PCKh@0.5 and AUC.

# Stage

# Channel

Mean

AUC

# Param

Deployment Cost

8
4
2
1

4
4
4
4

256
256
256
256

256
128
64
32

91.9
91.4
90.5
86.4

91.4
90.1
87.9
83.4

63.7
63.9
63.0
58.3

63.9
62.4
59.5
54.9

26M
13M
7M
3M

13M
3M

0.95M
0.34M

55G
30G
17G
10G

30G
9G
4.5G
3.1G

Table 5. Cost-effectiveness analysis of the Hourglass model. Metric: PCKh@0.5 and AUC. M/G: 106/109.

Pose Distillation

✗

✓

Mean

90.1
90.9

AUC

62.4
63.3

Table 6. Effect of the proposed pose knowledge distillation. Met-
ric: Mean PCKh@0.5 and AUC (%).

is bigger than other state-of-the-art gains, e.g. +0.3% in
91.2% [21] vs 90.9% [19]; further +0.3% in 91.5% [23].
More speciﬁcally, given all 163,814 test joints, each 0.1%
gain means correcting 163 joints.

Results on LSP Table 3 compares the PCK@0.2 rates
of our FPD model and existing methods with top reported
performances on the LSP test data. Compared to MPII, this
benchmark has been less evaluated by deep learning mod-
els, partly due to a smaller size of training data. Overall, we
observed the similar comparisons. For example, our FPD
runs more efﬁciently than the most competitive alternative
[24] and consumes much less training energy, in addition to
achieving the best pose prediction accuracy rate among all
compared methods.

Qualitative Examination To provide visual test, Figure
2 shows qualitative pose estimation evaluations on LSP and
MPII. It is observed that such a small FPD model can still
achieve reliable and robust pose estimation in arbitrary in-
the-wild images with various background clutters, different
human poses and viewing conditions.

4.3. Ablation Study

We carried out detailed component analysis and discus-

sion on the validation set of MPII.

FPD generalisation evaluation Besides using the state-
of-the-art Hourglass as the backbone network, we also

tested the more recent model [32] when integrated into
the proposed FPD framework.
In particular, we adopted
the original network as the teacher model and constructed
a lightweight variant as the student (target) model. The
lightweight model was constructed similarly as in Table 1
because it is based on the Hourglass design too: reducing
the number of stages to 4 and the number of channels in
each module to 128. The results in Table 4 show that our
FPD approach achieves 1.0% mean PCKh@0.5 gain, simi-
lar to the Hourglass case. This suggests the good generali-
sation capability of the proposed approach in yielding cost-
effective pose estimation deep models.

Cost-effectiveness analysis of Hourglass We exten-
sively examined the architecture design of the state-of-the-
art Hourglass neural network model [19] in terms of cost-
effectiveness. To this end, we tested two dimensions in
design: depth (the layer number) and width (the channel
number).
Interestingly, we revealed in Table 5 that re-
moving half stages (layers) and half channels only leads
to quite limited performance degradation. This indicates
that the original Hourglass design is highly redundant with
poor cost-effectiveness. However, this is largely ignored in
previous works due to their typical focus on pursuing the
model accuracy performance alone whilst overlooking the
important model efﬁciency problem. This series of CNN
architecture examinations helps us to properly formulate a
lightweight pose CNN architecture with only 16% (9/55)
computational cost but obtaining 98% (90.1/91.9) model
performance as compared to the state-of-the-art design, lay-
ing a good foundation towards building compact yet strong
human pose deep models.

Effect of pose knowledge distillation We tested the
effect of using our pose knowledge distillation on the
lightweight Hourglass network.
In contrast to all other

3523

Loss Function

MSE

Cross-Entropy

Head

97.7
97.6

Sho.

96.4
96.2

Elbo.

91.8
91.5

Wri.

87.6
87.6

Hip

89.7
89.0

Knee

86.6
86.5

Ank.

83.9
83.6

Mean

90.9
90.7

AUC

63.3
63.0

Table 7. Pose knowledge distillation by different types of loss function. Metric: Mean PCKh@0.5 and AUC.

α

Mean
AUC

0

90.1
62.4

0.05

90.8
63.2

0.1

90.8
63.2

0.5

90.9
63.3

0.95

90.7
63.0

0.99

90.7
63.0

Table 8. Performance analysis of the learning importance parame-
ter of pose distillation. Metric: Mean PCKh@0.5 and AUC (%).

methods, the model [23] additionally beneﬁts from an aux-
iliary dataset MPII in model training. Table 6 shows that
teacher knowledge transfer brings in 0.8% (90.9-90.1) mean
PCKh accuracy boost. This suggests that the generic prin-
ciple of knowledge distillation is also effective in the struc-
tured pose estimation context, beyond object categorisation.

To further validate how on earth this happens, we visu-
alise three pose structure transfer examples in Figure 3. It
is shown that the proposed mimicry loss against the teacher
prediction is likely to pose extra information in cases of er-
ror labelling, hard training images, and missing annotation.

Pose distillation loss function We ﬁnally evaluated the
effect of loss function choice for pose knowledge distilla-
tion. To that end, we further tested a Cross-Entropy mea-
surement based loss. Speciﬁcally, we ﬁrst normalise the en-
tire conﬁdence map so that the sum of all pixel conﬁdence
scores is equal to 1, i.e. L1 normalisation. We then measure
the divergence between the predicted and ground-truth con-
ﬁdence maps using the Cross-Entropy criterion. The results
in Table 7 show that the MSE is a better choice in compar-
ison to Cross-Entropy. The plausible reason is that MSE
is also the formulation of the conventional supervision loss
(Eqn (3)) therefore more compatible.

Parameter analysis of loss balance We evaluated the
balance importance between the conventional MSE loss and
the proposed pose knowledge distillation loss, as controlled
by α in Eqn (5). Table 8 shows that equal importance (when
α = 0.5) is the optimal setting. This suggests that the two
loss terms are similarly signiﬁcant with the same numerical
scale. On the other hand, we found that this parameter set-
ting is not sensitive with a wide range of satisfactory values.
This indicates that the teacher signal is not far away from
the ground-truth labels (see Figure 3 Column (4)), possibly
providing an alternative supervision as a replacement of the
original joint conﬁdence map labels.

5. Conclusion

In this work, we present a novel Fast Pose Distillation
(FPD) learning strategy.
In contrast to most existing hu-
man pose estimation methods, the FPD aims to address
the under-studied and practically signiﬁcant model cost-
effectiveness quality in order to scale the human pose esti-
mation models to large deployments in reality. This is made
possible by developing a lightweight human pose CNN ar-
chitecture and designing an effective pose structure knowl-
edge distillation method from a large teacher model to a
lightweight student model. Compared with existing model
compression techniques such as network parameter binari-
sation, the proposed method achieves highly efﬁcient hu-
man pose models without accuracy performance compro-
mise. We have carried out extensive comparative evalu-
ations on two human pose benchmarking datasets. The
results suggests the superiority of our FPD approach in
comparison to a wide spectrum of state-of-the-art alterna-
tive methods. Moreover, we have also conducted a se-
quence of ablation study on model components to provide
detailed analysis and insight about the gains in model cost-
effectiveness.

6. Acknowledgement

This work was supported in part by the National
Natural Science Foundation of China (61773093), Na-
tional Key R&D Program of China (2018YFC0831800),
Important Science and Technology Innovation Projects
in Chengdu (2018-YF08-00039-GX) and Research Pro-
grams of Sichuan Science and Technology Department
(2016JY0088, 17ZDYF3184). Mao Ye is the major corre-
sponding author.

References

[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and
Bernt Schiele. 2d human pose estimation: New benchmark
and state of the art analysis. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2014.

[2] Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Or-
mandi, George E Dahl, and Geoffrey E Hinton. Large scale
distributed neural network training through online distilla-
tion. In International Conference on Learning Representa-
tions, 2018.

[3] Jimmy Ba and Rich Caruana. Do deep nets really need to
In Advances in Neural Information Processing

be deep?
Systems, 2014.

3524

[4] Vasileios Belagiannis and Andrew Zisserman. Recurrent hu-

man pose estimation. 2017.

[5] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-
Mizil. Model compression.
In ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Min-
ing, 2006.

[6] Adrian Bulat and Georgios Tzimiropoulos. Human pose es-
timation via convolutional part heatmap regression. In Euro-
pean Conference on Computer Vision, 2016.

[7] Adrian Bulat and Georgios Tzimiropoulos. Binarized convo-
lutional landmark localizers for human pose estimation and
face alignment with limited resources. In IEEE International
Conference on Computer Vision, 2017.

[8] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Ji-
tendra Malik. Human pose estimation with iterative error
feedback. In IEEE Conference on Computer Vision and Pat-
tern Recognition, 2016.

[9] Xianjie Chen and Alan L Yuille. Articulated pose estimation
by a graphical model with image dependent pairwise rela-
tions.
In Advances in Neural Information Processing Sys-
tems, 2014.

[10] Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and
Jian Yang. Adversarial posenet: A structure-aware convolu-
tional network for human pose estimation. In IEEE Interna-
tional Conference on Computer Vision, 2017.

[11] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L.
Yuille, and Xiaogang Wang. Multi-context attention for hu-
man pose estimation. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, 2017.

[12] Xiaochuan Fan, Kang Zheng, Yuewei Lin, and Song Wang.
Combining local appearance and holistic view: Dual-source
deep neural networks for human pose estimation. In IEEE
Conference on Computer Vision and Pattern Recognition,
2015.

[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the

knowledge in a neural network. arXiv, 2015.

[14] Eldar

Insafutdinov, Leonid Pishchulin, Bjoern Andres,
Mykhaylo Andriluka, and Bernt Schiele. Deepercut: A
deeper, stronger, and faster multi-person pose estimation
model. European Conference on Computer Vision, 2016.

[15] Sam Johnson and Mark Everingham. Clustered pose and
nonlinear appearance models for human pose estimation. In
British Machine Vision Conference, 2010.

[16] Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge
In Advances in
distillation by on-the-ﬂy native ensemble.
Neural Information Processing Systems, pages 7528–7538,
2018.

[17] Xu Lan, Xiatian Zhu, and Shaogang Gong. Person search by
multi-scale matching. In European Conference on Computer
Vision, 2018.

[18] Xu Lan, Xiatian Zhu, and Shaogang Gong. Self-referenced
In Asian Conference on Computer Vision,

deep learning.
2018.

[19] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In European Con-
ference on Computer Vision, 2016.

[20] Xuecheng Nie, Jiashi Feng, Yiming Zuo, and Shuicheng
Yan. Human pose estimation with parsing induced learner.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2018.

[21] G. Ning, Z. Zhang, and Z. He. Knowledge-guided deep frac-
tal neural networks for human pose estimation. IEEE Trans-
actions on Multimedia, PP(99):1–1, 2017.

[22] Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S. Feris, and
Dimitris Metaxas. Jointly optimize data augmentation and
network training: Adversarial data augmentation in human
pose estimation.
In IEEE Conference on Computer Vision
and Pattern Recognition, 2018.

[23] Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and
Dimitris Metaxas. Jointly optimize data augmentation and
network training: Adversarial data augmentation in human
pose estimation.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2226–
2234, 2018.

[24] Umer Raﬁ, Bastian Leibe, Juergen Gall, and Ilya Kostrikov.
An efﬁcient convolutional network for human pose estima-
tion. In British Machine Vision Conference, 2016.

[25] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv e-print, 2014.

[26] Frank Seide, Gang Li, and Dong Yu. Conversational speech
transcription using context-dependent deep neural networks.
In Twelfth annual conference of the international speech
communication association, 2011.

[27] Taiki Sekii. Pose proposal networks. In The European Con-

ference on Computer Vision (ECCV), September 2018.

[28] Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun,
and Christoph Bregler. Efﬁcient object localization using
convolutional networks. In IEEE Conference on Computer
Vision and Pattern Recognition, 2015.

[29] Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph
Joint training of a convolutional network and a
In Advances

Bregler.
graphical model for human pose estimation.
in Neural Information Processing Systems, 2014.

[30] Alexander Toshev and Christian Szegedy. Deeppose: Human
pose estimation via deep neural networks. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2014.

[31] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser
Sheikh. Convolutional pose machines. In IEEE Conference
on Computer Vision and Pattern Recognition, 2016.

[32] Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, and
Xiaogang Wang. Learning feature pyramids for human pose
estimation. In IEEE International Conference on Computer
Vision, 2017.

[33] Wei Yang, Wanli Ouyang, Hongsheng Li, and Xiaogang
Wang. End-to-end learning of deformable mixture of parts
and deep convolutional neural networks for human pose esti-
mation. In IEEE Conference on Computer Vision and Pattern
Recognition, 2016.

[34] Yi Yang and Deva Ramanan. Articulated human detection
with ﬂexible mixtures of parts. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 35(12):2878–2890,
2013.

3525

[35] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In IEEE Conference on
Computer Vision and Pattern Recognition, 2017.

[36] Xiang Yu, Feng Zhou, and Manmohan Chandraker. Deep
deformation network for object landmark localization.
In
European Conference on Computer Vision, pages 52–70.
Springer, 2016.

[37] Ying Zhang, Tao Xiang, Timothy M Hospedales, and
Huchuan Lu. Deep mutual learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4320–4328, 2018.

[38] Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh,
Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina
Katabi. Through-wall human pose estimation using radio
signals. In IEEE Conference on Computer Vision and Pat-
tern Recognition, 2018.

3526

