Variational Bayesian Dropout with a Hierarchical Prior

Yuhang Liu1

,

2, Wenyong Dong1, Lei Zhang3

,

2, Dong Gong2, and Qinfeng Shi2

1School of Computer Science, Wuhan University, Hubei, China

2The University of Adelaide, Australia

3Inception Institute of Artiﬁcial Intelligence, UAE

https://sites.google.com/view/yuhangliu/pro

Abstract

Variational dropout (VD) is a generalization of Gaus-
sian dropout, which aims at inferring the posterior of net-
work weights based on a log-uniform prior on them to learn
these weights as well as dropout rate simultaneously. The
log-uniform prior not only interprets the regularization ca-
pacity of Gaussian dropout in network training, but also
underpins the inference of such posterior. However, the
log-uniform prior is an improper prior (i.e., its integral is
inﬁnite), which causes the inference of posterior to be ill-
posed, thus restricting the regularization performance of
VD. To address this problem, we present a new general-
ization of Gaussian dropout, termed variational Bayesian
dropout (VBD), which turns to exploit a hierarchical pri-
or on the network weights and infer a new joint posterior.
Speciﬁcally, we implement the hierarchical prior as a zero-
mean Gaussian distribution with variance sampled from a
uniform hyper-prior. Then, we incorporate such a prior in-
to inferring the joint posterior over network weights and the
variance in the hierarchical prior, with which both the net-
work training and dropout rate estimation can be cast into
a joint optimization problem. More importantly, the hierar-
chical prior is a proper prior which enables the inference
of posterior to be well-posed. In addition, we further show
that the proposed VBD can be seamlessly applied to net-
work compression. Experiments on classiﬁcation and net-
work compression demonstrate the superior performance of
the proposed VBD in regularizing network training.

1. Introduction

Deep neural networks have gained great success in vari-
ous artiﬁcial intelligence research areas, e.g., computer vi-
sion [12, 8], natural language processing [4], etc. Never-
theless, due to the limited samples with annotation in prac-
tice, training deep neural networks with extensive parame-
ters often suffers from over-ﬁtting problem [35]. Dropout
proves to be a practical technique to alleviate this prob-
lem, which stochastically regularizes network weights by

(cid:55)(cid:75)(cid:72)(cid:3)(cid:79)(cid:82)(cid:74)(cid:3)(cid:88)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)

(cid:42)(cid:68)(cid:88)(cid:86)(cid:86)(cid:76)(cid:68)(cid:81)

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)

(cid:28)

(cid:58)

c

(cid:28)

(cid:58)

(cid:72)

(cid:57)(cid:68)(cid:85)(cid:76)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:71)(cid:85)(cid:82)(cid:83)(cid:82)(cid:88)(cid:87)
(cid:58) (cid:28)
(cid:12)
(cid:12)
(cid:76)
(cid:80)

p(cid:58)

(cid:11) (cid:11)
KLD q

(cid:12) (cid:95)(cid:95)

(cid:81)

(cid:11)

(cid:95)

(cid:57)(cid:68)(cid:85)(cid:76)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:37)(cid:68)(cid:92)(cid:72)(cid:86)(cid:76)(cid:68)(cid:81)(cid:3)(cid:71)(cid:85)(cid:82)(cid:83)(cid:82)(cid:88)(cid:87)

(cid:80)(cid:76)(cid:81)

(cid:11) (cid:11)
KLD q

(cid:58)

(cid:15)

(cid:72)

(cid:12) (cid:95)(cid:95)

(cid:11)

p

(cid:58) (cid:28)
(cid:12)(cid:12)

(cid:72)

(cid:15)

(cid:95)

a

b

Figure 1: Variational dropout v.s. the proposed variational
Bayesian dropout.

randomly enforcing multiplicative noise on input features
during training [13]. Over the past several years, various
dropout methods have been proposed [13, 29, 33]. Among
them, Gaussian dropout [33] provides a general framework,
which introduces the distribution of network weights into
model training and thus can well approximate the conven-
tional dropout with different types of noise, such as binary
noise [13] or Gaussian noise [29]. While these methods
have shown promising regularization performance in vari-
ous deep network architectures [20, 28, 16, 18, 9], the rea-
son behind such success is not clear, and their performance
heavily depends on a predeﬁned dropout rate, for which tra-
ditional grid-search based methods is a prohibitive opera-
tion for large network models.

Variational dropout (VD) [19] is a generalization of
Gaussian dropout, which focuses on inferring the posteri-
or of network weights based on a log-uniform prior. By
doing this, VD can address these two aspects of the prob-
lems mentioned above. 1) Bayesian Interpretation. It has
been proved [19] that VD can be consistent with Gaus-
sian dropout for a ﬁxed dropout rate by enforcing the log-
uniform prior on network weights. This implies that incor-
porating Gaussian dropout into network training amounts to
variational inference on the network weights, where these
weights are regularized by the Kullback-Leibler (KL) di-
vergence between the variational posterior, i.e., the distri-
bution of network weights introduced by Gaussian dropout,
and the log-uniform prior. In other words, the log-uniform
prior endows Gaussian dropout with the regularization ca-
pacity. 2) Adaptive dropout rate. Based on the log-uniform
prior, VD [19] can simultaneously learn network weights
as well as dropout rate via inferring the posterior on these

17124

weights. To sum up, the log-uniform prior as the footstone
of VD underpins these two advantages above. However, re-
cent theoretical progress in [14, 15, 26] demonstrates that
the log-uniform prior is an improper prior (i.e., its integral
is inﬁnite), which causes inferring the posterior of network
weights to be ill-posed. Such ill-posed inference can degen-
erate variational inference on these weights into penalized
maximum likelihood estimation [14]. Thus, the interpreta-
tion of the regularization capacity of Gaussian dropout is
not in a full Bayesian way. And more importantly, the reg-
ularization capacity of VD is still limited.

To address this problem, we propose a variational
Bayesian dropout (VBD) framework, which is a new gen-
eralization of Gaussian dropout. A visual comparison be-
tween the proposed VBD and VD can be seen in Figure 1.
In VBD, we assume network weights to come from a two-
level hierarchical prior. Instead of only inferring the poste-
rior over network weights, we propose to infer the joint pos-
terior over both network weights and their hyper-parameters
deﬁned in their ﬁrst-level prior. Through implementing the
hierarchical prior as a zero-mean Gaussian distribution with
variance sampled from a uniform distribution, we can the-
oretically prove that the proposed VBD can be consisten-
t with Gaussian dropout [33] for a ﬁxed dropout rate as
VD. Thus, VBD also can interpret the regularization ca-
pacity of Gaussian dropout.
In contrast to the improper
log-uniform prior, the proposed hierarchical prior is a prop-
er prior, which enables the inference of posterior in VBD
well-posed. This not only leads to a full Bayesian justi-
ﬁcation for Gaussian dropout, but also improves the regu-
larization capacity obviously. In addition, we further ﬁnd
that the proposed VBD can be seamlessly applied to neu-
ral network compression as [25, 26]. Experimental results
on classiﬁcation as well as network compression tasks show
the effectiveness of VBD in handling over-ﬁtting.

2. Related work

Dropout. Dropout plays an important role in improv-
ing the generalization capacity of deep neural networks. At
ﬁrst, dropout is employed to randomly drop input features
with Bernoulli distribution during training to prevent feature
co-adaptation [13]. This amounts to training an exponential
number of different networks with shared parameters.
In
the test phase, the prediction is determined by averaging the
outputs of all these different networks. The idea of dropout
is then generalized by multiplying the input features with
random noise drawn from other distributions, e.g., Gaus-
sian [29]. While these early methods have shown effective-
ness in some cases, repeatedly dropping a random subset
of input features makes training a network much slower.
To address this problem, Gaussian dropout [33] proposes
to sample the output features from a Gaussian distribution
instead of input features in dropout training and shows vir-

tually identical regularization performance but faster con-
vergence. This is inspired by the observation that enforcing
multiplicative noise on input features, whatever the noise
is generated from a Bernoulli distribution or a Gaussian,
making use of the central limit theorem, makes the corre-
sponding outputs to be approximately Gaussian [33]. How-
ever, these conventional dropout methods fail to clarify the
intrinsic principle for their regularization capacity. In ad-
dition, their performance depends a lot on the pre-deﬁned
dropout rate. In contrast, the proposed VBD can provide a
Bayesian interpretation for dropout as well as automatically
estimating the dropout rate.

Variational Dropout. VD is a generalization of Gaus-
sian dropout, which can interpret the regularization capacity
of dropout as well as automatically estimating the dropout
rate via inferring the posterior of network weights. For ex-
ample, literature in [19] proves that training network with
variational dropout framework implicitly imposes the log-
uniform prior on weights for preventing over-ﬁtting. S-
ince the dropout rate can be automatically determined, some
works of literature [25, 26] further apply VD to compress
neural networks. However, the log-uniform prior is an im-
proper prior which causes the inference of posterior over
network weights in VD to be ill-posed [15, 26], thus limit-
ing its performance in preventing over-ﬁtting. In this study,
the proposed VBD imposes a proper hierarchical prior on
network weights, which induces a well-posed Bayesian in-
ference over network weights and thus improves the regu-
larization capacity.

Concrete Dropout and Adversarial Dropout. In addi-
tion, recent works have proposed another two dropout meth-
ods, e.g., concrete dropout [6] and adversarial dropout [27].
They are different from the proposed VBD. Speciﬁcal-
ly, concrete dropout provides Bayesian generalization for
dropout with Bernoulli distribution [13], while the pro-
posed VBD provides Bayesian generalization for Gaussian
dropout [33]. Besides, adversarial dropout [27] proposes to
handle over-ﬁtting by training the network in an adversarial
way. In contrast, the proposed method focuses on introduc-
ing hierarchical prior on network weights to regularize the
network training.

3. Preliminaries

Consider a supervised learning problem on a dataset
𝒟 = {(x𝑖, y𝑖)}𝑁
𝑖=1 of observation-label pairs. We train a
fully connected neural network with 𝐿 hidden layers. For
each layer, we have:

B = A𝜃,

(1)

where A denotes the 𝑀 × 𝐾 matrix of input features for
current minibatch, 𝜃 is the 𝐾 × 𝐷 weight matrix, B is the
𝑀 × 𝐷 output matrix before activation function.

27125

3.1. Gaussian Dropout

To prevent over-ﬁtting, dropout applies multiplicative
noise on the input of each layer of neural networks during
training as follows:

B = (A ∘ 𝜉)𝜃,

(2)

where 𝜉 is the 𝑀 × 𝐾 noisy matrix, and ∘ denotes the
element-wise (Hadamard) product. In conventional dropout
methods, the elements of the noise 𝜉 are either sampled
from a Bernoulli distribution with probability 1 ࢤ 𝑝 of being
1, with the dropout rate 𝑝 [13], or sampled from a Gaussian
distribution with mean 1 and variance 𝛼 = 𝑝/(1 ࢤ 𝑝) [29].
Regardless which strategy of the above two is used, accord-
ing to the central limit theorem and equation Eq. (2), one
can directly produce 𝐵𝑚,𝑑 by sampling from the following
Gaussian distribution:

𝑞(𝐵𝑚,𝑑∣A) = 𝒩 (𝐵𝑚,𝑑∣𝜇𝑚,𝑑, 𝛿2

𝑚,𝑑),

(3)

where mean 𝜇𝑚,𝑑 = ∑𝐾
𝛼 ∑𝐾
This means 𝑞(W) can be factorized as follows:

𝑚,𝑑 =
𝑘,𝑑. Here 𝐴𝑚,𝑘 denotes an element in A.

𝑘=1 𝐴𝑚,𝑘𝜃𝑘,𝑑 and variance 𝛿2

𝑘=1 𝐴2

𝑚,𝑘𝜃2

𝑞(W) =

𝐾

𝐷

∏

∏

𝑘=1

𝑑=1

𝑞(𝑊𝑘,𝑑) =

𝐾

𝐷

∏

∏

𝑘=1

𝑑=1

𝒩 (𝑊𝑘,𝑑∣𝜃𝑘,𝑑, 𝛼𝜃2

𝑘,𝑑),

(4)
where each element 𝑊𝑘,𝑑 in W can be sampled from
𝑞(W𝑘,𝑑) in Eq. (4). Finally, the objective function for net-
work training with Gaussian dropout becomes:

max

𝜃

𝐿𝒟(𝜃) ≃ max

𝜃

𝑁

∑

𝑖=1

𝔼

𝑞(W)log 𝑝(y𝑖∣x𝑖, W).

(5)

The viewpoint in Eq. (5) provides a opportunity to bridge
the gap between Bayesian inference and dropout, if we use
Eq. (4) as an approximate posterior distribution for a net-
work model with a special prior on the weights. The chal-
lenge in this gap is what is the special prior.

3.2. Variational Dropout

VD uses 𝑞(W) in Eq. (4) as a variational posterior to
approximate the true posterior 𝑝(W∣𝒟) in terms of minimal
KL divergence,

min
𝜃,𝛼

𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝒟)).

(6)

Here 𝐷𝐾𝐿() denotes the KL divergence. Given W
from a prior 𝑝(W), according to the Bayesian rule, i.e.,
𝑝(W∣𝒟) ∝ 𝑝(𝒟∣W)𝑝(W), minimizing the KL divergence
in Eq. (6) is equivalent to maximizing the variational lower
bound of the marginal likelihood of data as:

max
𝜃,𝛼

𝐿𝒟(𝜃, 𝛼) ࢤ 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W)),

(7)

where 𝐿𝒟(𝜃, 𝛼) with ﬁxed parameter 𝛼 is the same as one
in Eq. (5) and known as the expected log-likelihood term.

Firstly, in VD, the dropout rate 𝛼 can be automatically
determined by data characteristics. Secondly, VD can pro-
vide a Bayesian interpretation for the success of dropout
in preventing over-ﬁtting. To clarify this, VD requires that
the optimization of 𝜃 in Eq. (7) is consistent with that in
Gaussian dropout in Eq. (5), i.e., maximizing the expect-
ed log-likelihood. To this end, the prior 𝑝(W) has to be
such that 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W)) in Eq. (7) does not depend
on weight parameters 𝜃. With such a requirement, we have
the following proposition.

Proposition 1 ([19]). The only prior in VD, which enables
𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W)) not depending on weight parameters
𝜃, is the log-uniform prior:

𝑝(∣𝑊𝑘,𝑑∣) =

𝑐

∣𝑊𝑘,𝑑∣

⇔ 𝑝(log(∣𝑊𝑘,𝑑∣)) = 𝑐.

(8)

The above discussion demonstrates that training net-
work with VD implicitly imposes the log-uniform prior on
weights. With such a prior, the KL term in Eq. (7) is able
to regularize the number of signiﬁcant digits stored for the
weights W in the ﬂoating-point format, thus being able
to mitigate over-ﬁtting at some extent. However, the log-
uniform prior is an improper prior, which reaches ill-posed
variational inference, e.g., the KL divergence between the
variational posterior in Eq. (4) and the log uniform prior
in Eq. (8) is inﬁnite. Although leveraging truncated tech-
niques relieves this problem at some extent [19, 26], patho-
logical behaviour still remains [14], e.g., the resultant inﬁ-
nite KL divergence theoretically degenerate VD into a max-
imum likelihood estimation that fails to avoid over-ﬁtting.
More details for the theoretical justiﬁcation can be found
in [15, 26, 14]. Therefore, the performance of VD in pre-
venting over-ﬁtting need to be further improved.

In this section, we will introduce the details of the pro-
posed VBD. In the following, we ﬁrst introduce the pro-
posed VBD framework. Then, we show a speciﬁcally
designed hierarchical prior in VBD framework and prove
that VBD with this prior can be consistent with Gaussian
dropout for a ﬁxed dropout rate.

3.3. Variational Bayesian Dropout

In contrast to a one-level prior in VD, we turn to propose
a two-level hierarchical prior 𝑝(W, 𝛾) = 𝑝(W∣𝛾)𝑝(𝛾).
This brings two aspects of advantages. Firstly, two kind-
s of very simple distributions in hierarchical structure can
produce a much more complicated distribution, e.g., a hi-
erarchical sparse prior [36], a zero-mean Gaussian distri-
bution with variance depicted by Gamma distribution [37],
and the super-Gaussian scale mixture model [22]. Thus the
two-level structure increase the possible solution spaces for
the proper and feasible prior to interpret Gaussian dropout.

37126

Secondly, the hierarchical structure enables the two-level
prior separable in the involved Bayesian inference and thus
is possible to simplify the Bayesian inference or makes the
intractable inference tractable, which will be further clari-
ﬁed in the following subsections.

Similar to VD, the proposed VBD aims at optimizing a
variational posterior to approximate the true posterior [17,
23]. Unlike that VD only considers the posterior of network
weights, we propose to model the joint posterior of both the
network weights (e.g., W) and the hyper-parameters (e.g.,
𝛾) in their prior as illustrated in Figure 1. We thus arrive the
objective of Bayesian inference in the proposed VBD:

min
𝜃,𝛼,𝛾

𝐷𝐾𝐿(𝑞(W, 𝛾)∣∣𝑝(W, 𝛾∣𝒟)),

(9)

where 𝑞(W, 𝛾) denotes a corresponding variational join-
t posterior for 𝑝(W, 𝛾∣𝒟). Note that when the hyper-
parameter 𝛾 is ﬁxed, the proposed VBD will reduce to VD.
Thus, the proposed VBD is a more general version of VD.
According to variational Bayesian inference technique [17],
we use the variational posterior 𝑞(W, 𝛾) = 𝑞(W)𝑞(𝛾) to
approximate the true posterior 𝑝(W, 𝛾∣𝒟), and then the ob-
jective in Eq. (9) can be reformulated as the variational low-
er bound of the marginal likelihood of data as:

max
𝛼,𝜃,𝛾

𝐿𝒟(𝛼, 𝜃)ࢤ𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝛾))ࢤ𝐷𝐾𝐿(𝑞(𝛾)∣∣𝑝(𝛾)),

(10)
where 𝐿𝒟 is the expected log-likelihood term in Eq. (7).
Derivations can be found in supplementary material. For
the proposed VBD, the key is to exploit a proper hierarchi-
cal prior 𝑝(W, 𝛾) for supporting Gaussian dropout. In the
following, we will provide such prior and discuss its advan-
tages.

3.4. The Proposed Hierarchical Prior

Inspired by the hierarchical prior in sparse Bayesian
learning [30], we assume the network weights W come
from a zero-mean Gaussian distribution. Then, a uniform
hyper-prior is imposed on the variance of the Gaussian dis-
tribution to adjust the shape of the ultimate prior. When
each element 𝑊𝑘,𝑑 in W is independent identically dis-
tributed, the proposed two-level hierarchical prior 𝑝(W, 𝛾)
is formulated as:

Proposition 2. With the prior 𝑝(𝛾) in Eq. (11), we em-
ploy mean-ﬁeld variational approximation, viz., 𝑞(𝛾) =
∏𝐾
𝑑=1 𝑞(𝛾𝑘,𝑑), and assume that 𝑞(𝛾𝑘,𝑑) comes from
a delta distribution. Then the proposed VBD framework in
Eq. (10) reduces to

𝑘=1 ∏𝐷

max
𝛼,𝜃,𝛾

𝐿𝒟(𝛼, 𝜃) ࢤ 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝛾)).

(12)

Given the prior in Eq. (11) and the variational posteri-
or 𝑞(𝛾𝑘,𝑑), 𝐷𝐾𝐿(𝑞(𝛾)∣∣𝑝(𝛾)) in Eq. (10) will collapse to a
constant and thus can be neglected in optimization. Sim-
ilar trick can be found in [2, 1]. Speciﬁcally, to simplify
the representation, we assume 𝛾𝑘,𝑑 as a one-dimensional s-
calar. As deﬁned in Eq. (11), we have 𝑝(𝛾𝑘,𝑑) = 1/(𝑏 ࢤ 𝑎).
Note that the delta distribution 𝑞(𝛾𝑘,𝑑) either lies in or out of
[𝑎, 𝑏], e.g., 𝛿(𝛾𝑘,𝑑 ࢤ 𝛾′
𝑘,𝑑 lies in or out of [𝑎, 𝑏]. If
𝑞(𝛾𝑘,𝑑) is out of [𝑎, 𝑏], there is 𝐷𝐾𝐿(𝑞(𝛾𝑘,𝑑)∣∣𝑝(𝛾𝑘,𝑑)) =
+∞. To avoid this case, [𝑎, 𝑏] is generally regarded as
a large enough interval [2, 1]. As a result, we arrive
𝐷𝐾𝐿(𝑞(𝛾𝑘,𝑑)∣∣𝑝(𝛾𝑘,𝑑)) = log(𝑏 ࢤ 𝑎), which is indepen-
dent to the unknown variables 𝛼, 𝜃 and 𝛾 and thus can be
neglected. Therefore, we do not need to set speciﬁc values
for the hyper-parameters 𝑎 and 𝑏 in practice. The detailed
proof can be found in supplementary material.

𝑘,𝑑) and 𝛾′

According to [19], the key property of the log-uniform
prior is to enable the KL divergence 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W))
in Eq. (7) not depending on weight parameters 𝜃 as men-
tioned in Proposition 1. With this property, learning 𝜃 in
VD will be consistent with that in conventional Gaussian
dropout for a ﬁxed dropout rate 𝛼. In the following, we will
demonstrate that the proposed hierarchical prior Eq. (11) al-
so shows a similar property in VBD framework Eq. (12). To
this end, we give the following theoretical result.

Proposition 3. Given the objective Eq. (12), together with
the prior Eq. (11) and the variational posterior Eq. (4),
𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝛾)) = ∑𝐾
𝑑=1 0.5 log(1 + 𝛼ࢤ1),
which does not depend on weight parameters 𝜃.

𝑘=1 ∑𝐷

Proof. Since the variational posterior in Eq. (4) and the
prior 𝑝(W∣𝛾) in Eq. (11) are fully factorized, the KL-
divergence 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝛾)) in (12) can be decom-
posed into a sum as:

𝑝(W∣𝛾) =

𝑝(𝛾) =

𝐾

𝐷

∏

∏

𝑘=1

𝑑=1

𝐾

𝐷

∏

∏

𝑘=1

𝑑=1

𝑝(𝑊𝑘,𝑑∣𝛾𝑘,𝑑) =

𝐾

𝐷

∏

∏

𝑘=1

𝑑=1

𝒩 (𝑊𝑘,𝑑∣0, 𝛾𝑘,𝑑),

𝐾

𝐷

∑

∑

𝑘=1

𝑑=1

𝐷𝐾𝐿(𝑞(𝑊𝑘,𝑑)∣∣𝑝(𝑊𝑘,𝑑∣𝛾𝑘,𝑑)).

(13)

𝑝(𝛾𝑘,𝑑) =

𝐾

𝐷

∏

∏

𝑘=1

𝑑=1

𝒰 (𝛾𝑘,𝑑∣𝑎, 𝑏),

Since both the prior 𝑞(𝑊𝑘,𝑑) and the posterior 𝑝(𝑊𝑘,𝑑∣𝛾𝑘,𝑑)
the KL divergence
follow Gaussian
𝐷𝐾𝐿(𝑞(𝑊𝑘,𝑑)∣∣𝑝(𝑊𝑘,𝑑∣𝛾)) in Eq.
(13) can be calcu-
lated as:

distributions,

(11)

where 𝒰 (𝛾𝑘,𝑑∣𝑎, 𝑏) denotes an uniform distribution with
range [𝑎, 𝑏]. By imbedding this prior into the proposed VB-
D framework in Eq. (10), we give the following theoretical
result.

47127

𝐷𝐾𝐿(𝑞(𝑊𝑘,𝑑)∣∣𝑝(𝑊𝑘,𝑑∣𝛾)) =
𝛼𝜃2

0.5 log(

) +

𝛾𝑘,𝑑
𝛼𝜃2

𝑘,𝑑

𝑘,𝑑 + 𝜃2
2𝛾𝑘,𝑑

𝑘,𝑑

(14)

ࢤ 0.5.

By introducing Eq. (14) into Eq. (12), we arrive:

max
𝛼,𝜃,𝛾

𝐿𝒟(𝛼, 𝜃)−

𝐾

𝐷

∑

∑

𝑘=1

𝑑=1

0.5 log(

𝛾𝑘,𝑑
𝛼𝜃2

𝑘,𝑑

) +

𝛼𝜃2

𝑘,𝑑 + 𝜃2
2𝛾𝑘,𝑑

𝑘,𝑑

. (15)

To ﬁnd the optimal 𝛾𝑘,𝑑, referred to as 𝛾∗
𝑘,𝑑, by setting the
partial differential of Eq. (15) with respect to 𝛾𝑘,𝑑 to zero,
we have:

claims that the improper log-uniform prior interprets Gaus-
sian dropout, which fails to give full Bayesian interpretation
for Gaussian dropout and suffers limited regularization ca-
pacity, while VBD proposes a proper prior to handle those
drawbacks. In the following, we will show that these two
main differences further lead to the apparent advantage of
VBD in network compression compared with VD.

𝛾∗
𝑘,𝑑 = 𝛼𝜃2

𝑘,𝑑 + 𝜃2

𝑘,𝑑.

(16)

4. Extension to Neural Network Compression

Replacing 𝛾𝑘,𝑑 in Eq. (14) by Eq. (16) completes the proof.

In summary, with Propositions 2 and 3, the ﬁnal objec-
tive for the proposed VBD with the hierarchical prior can
be given as:

max
𝛼,𝜃

𝐿𝒟(𝛼, 𝜃) ࢤ

𝐾

𝐷

∑

∑

𝑘=1

𝑑=1

0.5 log(1 + 𝛼ࢤ1).

(17)

Built on this loss function, we can see that: 1) The pro-
posed hierarchical prior also meets the requirement such
that 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝛾)) does not depend on weight pa-
rameters 𝜃.
In other words, the proposed VBD with the
hierarchical prior is consistent with Gaussian dropout when
𝛼 is ﬁxed. Hence, the proposed VBD can give Bayesian
interpretation for Gaussian dropout. 2) The dropout rate pa-
rameter 𝛼 also can be automatically learned as VD.

Note that we apply a uniform prior on 𝛾 to be able to
update 𝛾. If 𝛾 was treated as the variance hyper-parameter
of the Gaussian prior, we could not update it in the proposed
framework. This is because the prior could not see any data.
Then, if we cannot update 𝛾, we cannot produce Eq. (16),
which means that we could not produce Propositions 3. To
allow 𝛾 to be updated, we impose a uniform prior on it and
utilize delta variational posterior leading to Propositions 2.
This is also why many variational approximation methods
(e.g., [2, 1]) employ priors on hyper-parameters.

More importantly, the improper log-uniform prior in tra-
ditional VD induces ill-posed Bayesian inference, since
it leads to inﬁnite 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W)) as mentioned in
[14, 15, 26]. By contrast, the proposed hierarchical pri-
or in VBD gives well-posed Bayesian inference, since it
produces reasonable and tractable 𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W∣𝛾))
as shown in Proposition 3. Hence, the proposed hierar-
chical prior in VBD shows obvious superiority over the
log-uniform prior in VD, and we argue that the proposed
VBD framework with the hierarchical prior provides a full
Bayesian interpretation for the success of Gaussian dropout
in preventing over-ﬁtting.

According to the discussion above, we can conclude as
follows. 1) The proposed VBD is a more general VBD.
VD focuses on incorporating one-level priors, while VBD
can contain two-level priors as shown in Figure 1. 2) VD

Since VD can adaptively learn the dropout rate 𝑝 (or 𝛼)
from training dataset, it can be utilized for neural network-
s compression [25, 26]. Inspired by this, in this section we
turn to exploit the ability to apply the proposed VBD to neu-
ral networks compression under the frameworks proposed
in [25, 26]. In addition, we will also discuss the advantage
of the proposed framework on networks compression.

4.1. Compressing Weights

We ﬁrst extend the proposed VBD to compressing
weights under the framework in [25]. Further details about
the framework can be found in that paper. To this end,
the proposed hierarchical prior Eq. (11) is used to model
weights in a neural network. For convenience, we replace
the original 𝛼 in Eq. (4) to learn speciﬁc 𝛼 for each weight,
in which Proposition 3 holds. The distribution Eq. (4) as a
variational posterior, 𝒩 (𝜃𝑘,𝑑, 𝛼𝑘,𝑑𝜃2
𝑘,𝑑), is used to approxi-
mate the true posterior. In this way, to learn 𝜃𝑘,𝑑 and 𝛼𝑘,𝑑,
the objective function in Eq. (17) is rewritten as:

max
𝛼,𝜃

𝐾

𝐷

𝐿𝒟(𝛼, 𝜃) ࢤ

∑

∑

0.5 log(1 + 𝛼ࢤ1

𝑘,𝑑).

(18)

𝑘=1

𝑑=1

Furthermore, since the natural gradient of 𝜃𝑘,𝑑 faces with
high variance, we follow the re-parameterization trick in
[25], viz, 𝜎2

𝑘,𝑑 = 𝛼𝑘,𝑑𝜃2
difference
the method

𝑘,𝑑.
between
in

It

is

to

the

The

[25]

while

the
is

∑𝐾

proposed

𝑘=1 ∑𝐷
VBD,

term ࢤ𝐷𝐾𝐿(𝑞(W)∣∣𝑝(W)).

proposed method
regular-
on
e-
𝑑=1 ࢤ0.5 log(1 + 𝛼ࢤ1
𝑘,𝑑)
is

it
𝑑=1 𝑘1𝑆(𝑘2 + 𝑘3 log(𝛼𝑘,𝑑)) − 0.5 log(1 + 𝛼ࢤ1
𝑘,𝑑)

and
ization
quivalent
the
in
∑𝐾
𝑘=1 ∑𝐷
in the method proposed in [25], where 𝑘1, 𝑘2 and 𝑘3 are
constant, and 𝑆() denotes the sigmoid function. The
term ࢤ0.5 log(1 + 𝛼ࢤ1
𝑘,𝑑) in the latter is heuristically
designed in [25] to model the behaviour that the negative
KL-divergence goes to a constant as log 𝛼𝑘,𝑑 goes to
In contrast, the term ࢤ0.5 log(1 + 𝛼ࢤ1
𝑘,𝑑)
minus inﬁnity.
in the proposed VBD is naturally derived from Bayesian
inference as mentioned above. Further discussion on this
term will be given in section 4.3.

57128

4.2. Structured Compressing

Although the method in [25] can be employed to com-
press weights in a neural network, it fails to accelerate neu-
ral networks in the testing phase, since resultant compres-
sion is unstructured. Recently, structured Bayesian pruning
in [26] employs VD to remove neurons and/or convolution-
al channels in convolutional neural networks for structured
compression, resulting in satisfactory performance. Similar
to VD, the proposed VBD also can be employed for struc-
tured pruning by constructing a dropout-like layer under the
framework in [26].

Speciﬁcally, we construct a single dropout-like layer

with an input matrix 𝑓 (B) as follows:

B′ = 𝑓 (B) ∘ W′,

(19)

where W′ denotes the dropout noise and 𝑓 () denotes the
activation function. The output of this layer B′ is of the
same size as the input B, and would serve as an input
matrix for the following layer. Similar to that in the pre-
vious section, we enforce the proposed hierarchical pri-
or Eq. (11) on W′, and imposes the variational posteri-
or, 𝒩 (𝜃𝑚,𝑑, 𝛼𝑚,𝑑𝜃2
𝑚,𝑑) (For convenience, we re-utilize the
same symbol 𝛼𝑚,𝑑 and 𝜃𝑚,𝑑 that is originally used to mod-
el weights in the previous section, however, they are served
to W′ in this section). Again, the objective function E-
q. (18) is used for learning 𝜃𝑚,𝑑 and 𝛼𝑚,𝑑, and the re-
parameterization [25] is adopted.

4.3. Analysis

In these two kinds of compression schemes above, ef-
fective compression depends on high dropout rate, e.g.,
𝛼𝑘,𝑑 → ∞ or 𝛼ࢤ1
𝑘,𝑑 → 0, which corresponds to a bina-
ry dropout rate that approaches 𝑝 = 1. This effectively
means that the corresponding weight or neuron is always ig-
nored and can be removed [25]. In this subsection, we will
show that the proposed variational Bayesian dropout explic-
itly imposes a sparse regularization for optimizing 𝛼ࢤ1
𝑘,𝑑, and
thus is able to effectively compress the deep neural network-
s. To this end, we ﬁrstly rewrite the objective Eq. (18) as:

min
𝛼,𝜃

𝐾

𝐷

∑

∑

𝑘=1

𝑑=1

0.5 log(1 + 𝛼ࢤ1

𝑘,𝑑) ࢤ 𝐿𝒟(𝛼, 𝜃).

(20)

The expected log-likelihood term 𝐿𝒟 can be viewed as the
data ﬁt-term for 𝛼ࢤ1
𝑘,𝑑 and the remainder derived from KL
divergence works as the regularization term. For such a reg-
ularization term, we have the following theoretical results.

Proposition 4. The regularization term 0.5 log(1 + 𝛼ࢤ1
𝑘,𝑑)
in Eq. (20) is a concave, non-decreasing function on the
domain [0, +∞), with respect to 𝛼ࢤ1
𝑘,𝑑.

According to [3], introducing such a regularization term
0.5 log(1 + 𝛼ࢤ1
𝑘,𝑑) into the objective is beneﬁcial to promote

the sparsity of the solution. Therefore, with optimizing
𝛼ࢤ1
𝑘,𝑑, we can obtain sparse 𝛼ࢤ1
𝑘,𝑑. Note that this regulariza-
tion term coincides with that in [5] which is motivated by
information bottleneck principle. In contrast, the regular-
ization term in this study stems from variational Bayesian
inference. Besides, this sparsity-promoting regularization
comes from a result of the particular variational approxima-
tion, which is different from previous methods with sparse
priors to compress network, e.g., [24, 7].

5. Experiments

In this section, we conduct experiments on classiﬁcation
task to demonstrate the effectiveness of the proposed vari-
ational Bayesian dropout in preventing over-ﬁtting. Then,
we further evaluate its performance in neural network com-
pression including weight compression and structured com-
pression. Note that neural network compression can also
imply the ability of preventing over-ﬁtting in term of the
ﬁnal test error.

5.1. Classiﬁcation

MNIST Dataset Following the settings in [19], we ﬁrst
take the hand-written digit classiﬁcation task on MNIST
dataset as a standard benchmark to evaluate the perfor-
mance of dropout methods in preventing over-ﬁtting. On
this task, we compare the proposed variational Bayesian
dropout with other ﬁve existing dropout methods, name-
ly no dropout, standard dropout with Bernoulli noise [13],
dropout with Gaussian noise [29], Gaussian dropout [33]
and VD [19], and concrete dropout [6] which is able to learn
adaptive dropout rate. We follow the network architecture
in [29], which adopt a fully connected neural network con-
sisting of 3 hidden layers with different number of units and
rectiﬁed linear units. For experimental setting, all networks
are trained for 50 epochs. More details about the network
architecture can be found in supplementary material.

The number of units

No Dropout

Dropout, Bernoulli
Dropout, Gaussian
Gaussian Dropout
Concrete Dropout

VD
Ours

100

1.80
1.69
1.70
1.66
2.39
1.69
1.56

340

1.77
1.68
1.66
1.64
1.61
1.62
1.53

580

1.78
1.67
1.68
1.71
1.55
1.67
1.53

820

1.78
1.67
1.69
1.65
1.54
1.62
1.52

1060

1.71
1.63
1.62
1.64
1.51
1.63
1.45

Table 1: Test error (%) on the MNIST dataset.

Table 1 shows the test error for all methods with various
choices of the number of units per layer. We observe that
although VD can adaptively learn dropout rate during train-
ing, it only obtains slightly better performance compared
with conventional dropout methods with ﬁxed dropout rate,

67129

including dropout with Bernoulli noise and dropout with
Gaussian noise. This is because the Bayesian inference in
VD with the improper log-uniform prior is ill-posed, which
hence ultimately restricts the capacity in preventing over-
ﬁtting as discussed before. Conversely, the proposed VBD
with adaptive dropout rate gains impressive performance,
which is better than that of the standard dropout as well as
VD. This proﬁts from that the proposed hierarchical prior
is a proper prior and thus it can appropriately regularize the
network weights in the Bayesian inference. Besides, the
proposed VBD is superior to concrete dropout.

CIFAR-10 Dataset We further compare the proposed
method with dropout with Bernoulli noise [13], dropout
with Gaussian noise [29], VD [19], concrete dropout [6]
on CIFAR-10 dataset. We also compare with adversarial
dropout recently proposed in [27]. In this case, we follow
the network architecture with different 𝑠𝑐𝑎𝑙𝑒 in [19] for ex-
perimental setting, all networks are trained for 100 epochs.
More details for the network architecture can be found in
supplementary material.

𝑠𝑐𝑎𝑙𝑒

No Dropout

Dropout, Bernoulli
Dropout, Gaussian
Adversarial Dropout

Concrete Dropout

VD
Ours

1

48.34
45.55
46.52
45.50
43.47
44.12
39.50

1.5

48.13
43.38
43.50
42.35
42.68
42.99
38.88

2

47.56
42.69
42.79
42.50
42.19
42.60
38.52

Table 2: Test error (%) on the CIFAR-10 dataset.

Figure 2 shows the test error for the all methods with
different 𝑠𝑐𝑎𝑙𝑒. We can see that concrete dropout only per-
forms on par with traditional dropout methods and VD. In
addition, we found that due to its negative effect of the im-
proper log-uniform prior, VD only provides comparable re-
sults to those methods with ﬁxed dropout rate. In contrast,
proﬁting from the proper hierarchical prior, the proposed
variational Bayesian dropout performs impressively well in
preventing over-ﬁtting. For example, when 𝑠𝑐𝑎𝑙𝑒 = 2,
compared with no dropout method, the proposed method
reduces the test error by 9.04%. The improvement is even
up to 4.08% when compared with VD.

SVHN Dataset In this case, we follow the network ar-
chitecture with 𝑠𝑐𝑎𝑙𝑒 = 1 used in experiments on CIFAR-
10 Dataset, and all networks are trained for 100 epochs Ta-
ble 3 shows the results of test error (%) in SVHN dataset.
Since traditional methods for setting dropout rate, such as
Grid-search based methods, are computationally expensive.
We set the dropout rate to be 0.5 for all layers of the net-
work in our experiments for simplicity. Under this simple
setting, traditional dropouts, e.g., Dropout with Bernoulli

Methods

Error (%)

No dropout

Dropout, Bernoulli
Dropout, Gaussian
Concrete Dropout

Adversarial Dropout

VD
Ours

22.01
20.31
20.22
18.95
18.66
19.74
17.46

Table 3: Test error on the SVHN dataset.

noise and Dropout with Gaussian noise, are slightly superi-
or to no dropout. Again, due to the improper log-uniform
prior, VD only provides comparable results to tradition-
al dropout with ﬁxed dropout rate. Conversely, compared
with these ﬁxed dropout rate based methods, the proposed
method with adaptive dropout rate gets the best test error,
17.46%.

5.2. Network compression

Compressing Weights In this part, we turn to evaluate
the effectiveness of the proposed method in weight com-
pression in neural networks for classiﬁcation on the MNIST
dataset. Here we adopt two kinds of basic neural networks,
the fully-connected LeNet-300-100 [21] and a convolution-
al LeNet-5-Caffe 1. We compare the proposed method with
four network compression methods, including Pruning [11],
Dynamic Network Surgery (DNS) [10], Soft Weight Shar-
ing (SWS) [32] and VD [25]. In the experiments, we strictly
follow the settings in [25].

Table 4 shows the results of compressing weights in
LeNet-300-100. Compared with traditional VD, with the
similar sparsity ∣W∣
∣W∕=0∣ = 66, the proposed method gets
1.67% test error that is better than the result of tradition-
al VD 1.94%. Further, although traditional VD has better
results in compression ratio ∣W∣
∣W∕=0∣ = 68, compared with
Pruning, DNS and SWS, it also reports higher test error
1.94% due to its limited ability to avoid over-ﬁtting. On the
contrary, since the proposed VBD provides a real Bayesian
interpretation for dropout, it can effectively prevent over-
ﬁtting and gains the better test error 1.76% as well as higher
compression ratio ∣W∣

∣W∕=0∣ = 81.

Table 4 also shows the results of compressing weights
in LeNet-5-Caffe. We can see that compared with Pruning,
DNS and SWS, traditional VD reports better compression
ratio ∣W∣
∣W∕=0∣ = 280 and test error 0.75%. Further, compared
with the all methods, the proposed method obtains the best
compression ratio ∣W∣

∣W∕=0∣ , without the loss of test error.

Structured Compressing We here test the performances
of the proposed method on structured compressing for neu-

1https://github.com/BVLC/caffe/tree/master/examples/mnist

77130

Methods

Original
Pruning

DNS
SWS
VD

Ours (low test error)
Ours (high sparsity)

LeNet-300-100

LeNet-5-Caffe

Error % Sparsity per Layer %

∣W∣

∣W∕=0∣

Error % Sparsity per Layer %

1.64
1.59
1.99
1.94
1.94
1.67
1.76

92.0-91.0-74.0
98.2-98.2-94.5

98.9-97.2-62.0
98.7-97.4-87.4
98.9-98.1-90.9

1
12
56
23
68
66
81

0.8
0.77
0.91
0.97
0.75
0.74
0.81

34-88-92.0-81
86-97-99.3-96

67-98-99.8-95
69-98-99.7-95
65-98-99.8-97

∣W∣

∣W∕=0∣

1
12
111
200
280
198
290

Table 4: Compressing weights in LeNet-300-100 and LeNet-5-Caffe. ∣W∣ and ∣W∕=0∣ denote the number of weights, the
number of weights with nonzero value, respectively. The sparsity per layer is computed by the rate between the number of
weights with zero value and the number of weights in each layer.

ral networks. The used architecture of neural networks is a
fully-connected LeNet-500-300 and a convolutional LeNet-
5-Caffe. We compare VD [25], SSL [34] and SBP [26].

Table 5: Compressing neurons.

Models

Methods

Error %

Neurons per Layer

LeNet-500-300

LeNet-5-Caffe

Original

VD
SSL
SBP
Ours

Original

VD
SSL
SBP
Ours

1.54
1.57
1.49
1.55
1.35

0.80
0.75
1.00
0.86
0.66

784 - 500 - 300 - 10
537 - 217 - 130 - 10
434 - 174 - 78 - 10
245 - 160 - 55 - 10
179 - 160 - 60 - 10

20 - 50 - 800 - 500
17 - 32 - 329 - 75
3 - 12 - 800 - 500
3 - 18 - 284 - 283
16 - 34 - 123 - 62

Table 5 shows the results of compressing neurons in
LeNet-500-300. As discussed in [31, 14], VD based S-
BP corresponds to maximum likelihood estimation, which
leads to overly pruning neurons, and hence SBP gets the
higher test error 1.55%. The proposed method not only
prunes the most neurons but also gains the lowest test er-
ror. By contrast, due to the improper log-uniform prior, VD
produces the highest test error and only slightly compresses
neurons.

Table 5 also shows the results of neurons compression in
LeNet-5-Caffe for all methods. We ﬁnd that the proposed
method gains the lowest test error as well as the least neu-
rons, e.g., the test error of the proposed method is even up
to 0.66%. In addition, in LeNet-5-Caffe, the ﬁrst two layers
are convolutional layers, and the following two layers are
fully-connected layers. Differing from SSL and SBP that
mainly focus on pruning neurons in convolutional layers,
the result shows that the proposed method prefers to prun-
ing neurons in the fully-connected layers. This means that
the proposed method emphasizes feature extraction.

6. Conclusion

In this study, we propose a new generalization (i.e., VB-
D) for Gaussian dropout to address the drawback of VD
brought by the improper log-uniform prior, e.g., the ill-
posed inference of posterior over network weights. To-
wards this goal, we exploit a hierarchical prior to the net-
work weights and propose to infer the joint posterior over
both these weights and the hyper-parameters deﬁned in their
ﬁrst-level prior. Through implementing the hierarchical pri-
or as a zero-mean Gaussian distribution with variance sam-
pled from a uniform hyper-prior, the proposed VDB can
cast the network training and the dropout rate estimation
into a joint optimization problem. In VBD, the hierarchi-
cal prior is a proper prior which enables the inference of
posterior to be well-posed, thus not only leading to a ful-
l Bayesian justiﬁcation for Gaussian dropout but also im-
proving regularization capacity. In addition, we also show
that the proposed VBD can be seamlessly applied to net-
work compression. In the experiments on both classiﬁca-
tion and network compression tasks, the proposed VBD
shows superior performance in terms of regularizing net-
work training.

VBD is a general dropout framework, which exploits a
promising direction for dropout, i.e., investigating hierar-
chical priors of network weights. The Gaussian-uniform
prior in this study is one feasible choice, but no means the
only option. In the future, more effort will be made to ex-
ploit other possible choices to regularize the training pro-
cess of deep neural networks better.

Acknowledgements: This work was partially support-
ed by National Key R&D Program of China (2018YF-
B0904200), National Natural Science Foundation of China
(No. 61672024, 61170305 and 60873114) and Australian
Research Council
(DP140102270 and DP160100703).
Yuhang was supported by a scholarship from the China
Scholarship Council.

87131

References

[1] S Derin Babacan, Rafael Molina, Minh N Do, and
Aggelos K Katsaggelos. Bayesian blind deconvolu-
tion with general sparse image priors.
In European
Conference on Computer Vision (ECCV), pages 341–
355. Springer, 2012. 4, 5

[2] Kwokleung Chan, Te-Won Lee, and Terrence J Se-
jnowski. Variational learning of clusters of undercom-
plete nonsymmetric independent components. Journal
of Machine Learning Research (JMLR), 3(Aug):99–
114, 2002. 4, 5

[3] Yichen Chen, Dongdong Ge, Mengdi Wang, Zizhuo
Wang, Yinyu Ye, and Hao Yin. Strong NP-hardness
for sparse optimization with concave penalty function-
s. In Proceedings of the 34th International Conference
on Machine Learning (ICML), pages 740–747, 2017.
6

[4] Ronan Collobert and Jason Weston. A uniﬁed archi-
tecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
the 25th international conference on Machine learn-
ing (ICML), pages 160–167, 2008. 1

[5] Bin Dai, Chen Zhu, Baining Guo, and David Wipf.
Compressing neural networks using the variational in-
formation bottleneck. In Proceedings of the 35th In-
ternational Conference on Machine Learning (ICML),
pages 1135–1144, 2018. 6

[6] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete
dropout. In Advances in Neural Information Process-
ing Systems (NIPS), pages 3581–3590, 2017. 2, 6, 7

[7] Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez.
Structured variational learning of bayesian neural net-
works with horseshoe priors.
arXiv preprint arX-
iv:1806.05975, 2018. 6

[8] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang,
Ian Reid, Chunhua Shen, Anton Van Den Hengel, and
Qinfeng Shi. From motion blur to motion ﬂow: a deep
learning solution for removing heterogeneous motion
blur. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages
2319–2328, 2017. 1

[9] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and
Yoshua Bengio. Deep learning, volume 1. MIT press
Cambridge, 2016. 1

[10] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynam-
ic network surgery for efﬁcient dnns. In Advances In
Neural Information Processing Systems (NIPS), pages
1379–1387, 2016. 7

[11] Song Han, Jeff Pool, John Tran, and William Dally.
Learning both weights and connections for efﬁcien-
t neural network. In Advances in Neural Information

Processing Systems (NIPS), pages 1135–1143, 2015.
7

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition (CVPR), pages 770–778,
2016. 1

[13] Geoffrey E Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-
Improving neural networks by preventing
dinov.
co-adaptation of feature detectors.
arXiv preprint
arXiv:1207.0580, 2012. 1, 2, 3, 6, 7

[14] Jiri Hron, Alexander GG Matthews, and Zoubin
Ghahramani. Variational bayesian dropout: pitfall-
s and ﬁxes.
In Proceedings of the 35th Internation-
al Conference on Machine Learning (ICML), pages
2024–2033, 2018. 2, 3, 5, 8

[15] Jiri Hron, Alexander G de G Matthews, and Zoubin
Ghahramani. Variational gaussian dropout is not
bayesian. arXiv preprint arXiv:1711.02989, 2017. 2,
3, 5

[16] G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 2261–
2269, 2017. 1

[17] Michael I Jordan, Zoubin Ghahramani, Tommi S
Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine
learning, 37(2):183–233, 1999. 4

[18] Alex Kendall, Vijay Badrinarayanan, and Roberto
Cipolla. Bayesian segnet: Model uncertainty in deep
convolutional encoder-decoder architectures for scene
understanding.
arXiv preprint arXiv:1511.02680,
2015. 1

[19] Diederik P Kingma, Tim Salimans, and Max Welling.
Variational dropout and the local reparameterization
trick. In Advances in Neural Information Processing
Systems (NIPS), pages 2575–2583, 2015. 1, 2, 3, 4, 6,
7

[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information
Processing Systems (NIPS), pages 1097–1105, 2012.
1

[21] Yann LeCun, L´eon Bottou, Yoshua Bengio, and
Patrick Haffner. Gradient-based learning applied to
document recognition.
Proceedings of the IEEE,
86(11):2278–2324, 1998. 7

[22] Yuhang Liu, Wenyong Dong, Dong Gong, Lei Zhang,
and Qinfeng Shi. Deblurring natural image using

97132

[35] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-
jamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization.
arXiv
preprint arXiv:1611.03530, 2016. 1

[36] Lei Zhang, Wei Wei, Yanning Zhang, Chunhua Shen,
Anton van den Hengel, and Qinfeng Shi. Cluster s-
parsity ﬁeld: An internal hyperspectral imagery prior
for reconstruction. International Journal of Computer
Vision (IJCV), 126(8):797–821, 2018. 3

[37] Lei Zhang, Wei Wei, Yanning Zhang, Chunna Tian,
and Fei Li. Reweighted laplace prior based hyperspec-
tral compressive sensing for unknown sparsity. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2274–2281,
2015. 3

super-gaussian ﬁelds. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 452–
468, 2018. 3

[23] Yuhang Liu, Wenyong Dong, and Mengchu Zhou.
Frame-based variational bayesian learning for inde-
pendent or dependent source separation. IEEE Trans-
actions on Neural Networks and Learning Systems
(TNNLS), 29(10):4983–4996, 2018. 4

[24] Christos Louizos, Karen Ullrich, and Max Welling.
Bayesian compression for deep learning. In Advances
in Neural Information Processing Systems (NIPS),
pages 3288–3298, 2017. 6

[25] Dmitry Molchanov, Arsenii Ashukha, and Dmitry
Vetrov. Variational dropout sparsiﬁes deep neural net-
works. In Proceedings of the 34th International Con-
ference on Machine Learning (ICML), pages 2498–
2507, 2017. 2, 5, 6, 7, 8

[26] Kirill Neklyudov, Dmitry Molchanov, Arsenii
Ashukha, and Dmitry P Vetrov. Structured bayesian
pruning via log-normal multiplicative noise.
In
Advances in Neural Information Processing Systems
(NIPS), pages 6775–6784, 2017. 2, 3, 5, 6, 8

[27] Sungrae Park, JunKeon Park, Su-Jin Shin, and Il-Chul
Moon. Adversarial dropout for supervised and semi-
supervised learning. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2018. 2, 7

[28] Karen Simonyan and Andrew Zisserman. Very deep
convolutional networks for large-scale image recogni-
tion. arXiv preprint arXiv:1409.1556, 2014. 1

[29] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
a simple way to prevent neural networks from overﬁt-
ting. Journal of Machine Learning Research (JMLR),
15(1):1929–1958, 2014. 1, 2, 3, 6, 7

[30] Michael E Tipping. Sparse bayesian learning and the
relevance vector machine. Journal of machine learn-
ing research (JMLR), 1(Jun):211–244, 2001. 4

[31] Brian Trippe and Richard Turner. Overpruning in vari-
ational bayesian neural networks. arXiv preprint arX-
iv:1801.06230, 2018. 8

[32] Karen Ullrich, Edward Meeds, and Max Welling. Soft
weight-sharing for neural network compression. arXiv
preprint arXiv:1702.04008, 2017. 7

[33] Sida Wang and Christopher Manning. Fast dropout
training.
In Proceedings of the 30th International
Conference on Machine Learning (ICML), pages 118–
126, 2013. 1, 2, 6

[34] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen,
and Hai Li. Learning structured sparsity in deep neu-
ral networks. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 2074–2082, 2016. 8

107133

