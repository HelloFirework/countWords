MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors

Lile Cai1

Bin Zhao2

Zhe Wang1

Jie Lin1

Chuan Sheng Foo1

Mohamed Sabry Aly3

Vijay Chandrasekhar1

{1Institute for Infocomm Research, 2Institute of Microelectronics}, A*STAR, Singapore

3Nanyang Technological University, Singapore

{caill,wang zhe,lin-j,foo chuan sheng,vijay}@i2r.a-star.edu.sg

zhaobin@ime.a-star.edu.sg, msabry@ntu.edu.sg

Abstract

cantly slower [18].

Modern convolutional object detectors have improved
the detection accuracy signiﬁcantly, which in turn inspired
the development of dedicated hardware accelerators to
achieve real-time performance by exploiting inherent paral-
lelism in the algorithm. Non-maximum suppression (NMS)
is an indispensable operation in object detection. In stark
contrast to most operations, the commonly-adopted Gree-
dyNMS algorithm does not foster parallelism, which can be
a major performance bottleneck. In this paper, we introduce
MaxpoolNMS, a parallelizable alternative to the NMS al-
gorithm, which is based on max-pooling classiﬁcation score
maps. By employing a novel multi-scale multi-channel max-
pooling strategy, our method is 20× faster than GreedyNMS
while simultaneously achieves comparable accuracy, when
quantiﬁed across various benchmarking datasets, i.e., MS
COCO, KITTI and PASCAL VOC. Furthermore, our method
is better suited for hardware-based acceleration than Gree-
dyNMS.

Two-stage object detectors consist of a (ﬁrst stage) re-
gion proposal network (RPN) that hypothesizes candidate
object locations and a (second stage) detection network that
reﬁnes region proposals. The RPN and detection network
share the same feature extractor network. Figure 1 displays
the typical blocks of two-stage object detectors. Typically
a signiﬁcant portion of execution time is consumed in the
feature extractor, region proposal and object detection net-
works. These layers contain convolution and pooling opera-
tions, which in principle can be mapped to a highly-parallel
hardware accelerator (e.g., Google TPU [19]) – this is not
the case with remaining blocks, i.e., Non-Maximum Sup-
pression (NMS).

1. Introduction

Deep neural networks (DNNs) have caused a major leap
in object detection accuracy. State-of-the-art DNN-based
object detection algorithms can be broadly classiﬁed into
one-stage and two-stage methods. One-stage object detec-
tors (e.g., YOLO [24] and SSD [22]) operate in a sliding-
window manner that makes prediction for densely sampled
locations in the input image. Alternatively, two-stage ap-
proaches, such as Faster R-CNN [25] and R-FCN [5], ﬁrst
generate a sparse set of region proposals and then perform
a second stage prediction to classify each proposal and re-
ﬁne its location. Two-stage methods consistently achieve
higher accuracy than one-stage methods, but are signiﬁ-

Figure 1. System diagram of two-stage object detectors. This work
tackles the NMS in region proposal network (denoted in red box).

NMS is an essential block as it removes duplicate detec-
tions, hence reducing false positives. Both the region pro-
posal network and object detection network employ NMS
as a post-processing step. The commonly-adopted Gree-
dyNMS [7] is a simple hand-crafted method. When ap-
plied in region proposal network, it ﬁrst sorts all the can-
didate detection boxes according to their objectness scores,
followed by two nested loops to greedily select high score
boxes and delete other boxes that overlap signiﬁcantly with
the selected ones. The inner loop is parallelizable, but the
outer loop is sequential in nature – without examining the
preceding box ﬁrst, it cannot be decided whether the follow-

9356

Input ImageRegion ProposalsFinal DetectionsFeature ExtractorRegion Proposal NetworkObject Detection NetworkNon-Maximum SuppressionNon-Maximum Suppressioning box should be selected. When applying GreedyNMS in
object detection network, the number of processed boxes is
much smaller than that for RPN (e.g., 300 vs. 6000), and it
is conducted separately for each object class.

NMS can induce a performance bottleneck as it cannot
be easily parallelized. This is illustrated in Fig. 2. As GPUs
become increasingly powerful, the time spent on convolu-
tion operations reduces signiﬁcantly, while the time spent
on NMS is not affected and gradually occupies an increas-
ing portion of the total execution time.

Figure 2. Execution time of convolution and GreedyNMS on dif-
ferent GPU platforms. Time was measured when the GPU was
launched to run a Faster R-CNN detection network with ResNet-
152-V2 backbone [14]. The last two bars are projected values
since 16-bit ﬁxed-point Faster R-CNN is not available1. The con-
volution includes all the operations in ResNet block 0–4, and the
GreedyNMS is the one in RPN.

In this paper, we introduce a scalable and paralleliz-
able approach to perform NMS in region proposal network.
The key insight is that an object proposal corresponds to a
peak in the objectness score map, hence we leverage max-
pooling to obtain this peak. The method is thus called Max-
poolNMS. By employing a novel multi-scale multi-channel
max-pooling strategy, our method simultaneously obtains
comparable accuracy and up to 20× speed-up versus Gree-
dyNMS. Our method avoids computing intersection over
union (IoU) and solely relies on max-pooling operations,
and thus is highly parallelizable.

2. Related Work

Convolutional object detectors Modern convolutional
object detectors follow either a one-stage, proposal-free
paradigm, or a two-stage, proposal-based paradigm. One-
stage detectors apply the object classiﬁer and location re-
gressor to a densely sampled set of local windows in dif-
ferent locations, scales and aspect ratios. YOLO [24], SSD
[22] and the recently proposed RetinaNet [20] are repre-
sentative architectures for one-stage object detectors. Two-
stage paradigm was popularized by the R-CNN framework
[11], and was further improved in terms of speed by Fast

1Projected execution time is derived using the total number of convo-
lution operations in the entire Faster R-CNN network, total memory ac-
cesses for weights and activations, and reported computing performance
and memory-access bandwidth of the examined GPU.

R-CNN [10]. Faster R-CNN [25] incorporates the region
proposal function into deep learning framework by intro-
ducing a region proposal network that shares the same fea-
ture extraction network with Fast R-CNN. Mask R-CNN
[12] adds a mask prediction branch to Faster R-CNN to ob-
tain pixel level object detections. Cascade R-CNN [2] adds
more stages to Faster R-CNN and uses the output of pre-
vious stages to train the next stage detector of higher qual-
ity. R-FCN [5] is another line of work for two-stage de-
tectors, which aims to share more computation among the
proposal boxes in the second stage by employing position-
[18] conduct an
sensitive prediction maps. Huang et al.
extensive study to evaluate the speed/accuracy trade-offs
for SSD, Faster R-CNN and R-FCN, and conclude that the
most accurate models are consistently achieved by two-
stage methods. Recent work on single shot instance seg-
mentation shows that the proposal-based approach can not
only achieve better mAP but also runs signiﬁcantly faster
than sliding window-based approach for mask prediction
[3].

NMS in object detection NMS has been employed as
a post-processing step in several generations of detectors.
The de facto algorithm, GreedyNMS, was ﬁrst demon-
strated in [7] to surpass other approaches for human detec-
tion. Since then, it has been a standard component in object
detection and widely used in one-stage and two-stage detec-
tors. Soft-NMS [1] is a variant of GreedyNMS that decays
the score of neighboring detections instead of totally remov-
ing it. It has been shown to improve the mAP of object de-
tectors by around 1-2%. However, it was only applied to
replace GreedyNMS in the second stage detection network
in the paper, and it is not clear whether it can also replace the
one in region proposal network. Fitness NMS [26] weights
the original classiﬁcation score by a predicted ﬁtness value
so that boxes with high IoU with ground truth boxes can
have higher scores. Another line of research is to replace
GreedyNMS with learnable network architectures so that
the model can be trained fully end-to-end. The idea is to
predict only one high scoring detection for an object and
the key to achieve this is to design features that condition on
multiple detections on the same object. Tnet [15] is a con-
vnet for NMS, where the IoU values between a detection
and its neighboring boxes are used together with score val-
ues to make sparse prediction. Gnet[16] computes pairwise
context features between a detection and its neighbors to
generate the feature representation. Relation network [17]
computes the relation feature for a detection by a weighted
sum of appearance features from other detections. Due to
pairwise computation and additional network architectures,
these methods are more suited to deploy in the second stage
detection network where the number of processed boxes is
small. Also, all these work focused on improving the ac-

9357

120TOPs240TOPs015304.49TFLOPS10.6TFLOPS15TFLOPS120TOPs240TOPs02004006004.49 TFLOPs32-bit floatK40 GPU(measured)10.6 TFLOPs32-bit float1080Ti GPU(measured)15 TFLOPs32-bit floatV100 GPU(measured)120 TOPs16-bit fixedV100 GPU(projected)240 TOPs16-bit fixed2× V100 GPU(projected)Execution time (ms)ConvolutionGreedyNMScuracy of GreedyNMS, while the speed and parallelism as-
pects have been left unexplored.

3. Method

Modern convolutional object detector utilizes multi-
scale “anchors” to realize scale-invariant object detection.
In RPN, a binary classiﬁer (i.e., object/background) and a
location regressor are trained for each anchor. Applying
the trained classiﬁer to densely sampled locations of the in-
put image (typically in a stride of 16) produces the object-
ness score maps. Figure 3 displays the 12 objectness score
maps for an input image from the KITTI dataset. We use 4
scales {642, 1282, 2562, 5122} and 3 aspect ratios (width to
height) {1 : 2, 1 : 1, 2 : 1} for anchor generation. We make
the following observations on these maps.

1. Objects correspond to peaks on the map. This is due to
the fact that during training, only anchors with a high
IoU (e.g., above 0.7) with ground truth boxes are con-
sidered as positive samples, and thus only anchors con-
taining objects can have high objectness scores during
testing. Anchors in the neighborhood of the peak an-
chor can also have high response since similar input
should produce similar output for a continuous classi-
ﬁcation function.

2. The score map is scale and aspect-ratio speciﬁc and
only responds to objects of around that size. This is be-
cause small (large) anchor boxes can only be matched
to small (large) ground truth objects, and thus can only
be trained to detect small (large) objects.

3. An object can have high response on more than one
score maps. This is due to the fact that the actual object
scales and aspect ratios are continuous, yet the prede-
ﬁned anchor sizes are discretized. An object can have
a size that falls between two neighboring anchor sizes
and thus has a strong response on both maps. Take the
image shown in Fig. 3 as an example. The green car
on the right has high response in two neighboring as-
pect ratios (scores maps (e) and (i)), as well as in two
neighboring scales (score maps (e) and (f)).

These observations suggest that max-pooling operations
could be sufﬁcient to obtain a meaningful set of object pro-
posals. As an object is a local maximum on the score map,
it can be picked up by max-pooling, whose function is to se-
lect the maximum value in a local window. The key param-
eters here are the kernel sizes and strides for max-pooling.
Observation 2 provides insights into how to set the parame-
ters effectively. As each score map is focused on detecting
objects of a speciﬁc size, we set the kernel size and stride of
max-pooling to be proportional to the anchor box size. To
further reduce false positives, Observation 3 suggests that
we can perform multi-channel max-pooling across neigh-
boring score maps to remove duplicate responses for the

Figure 3. The 12 objectness score maps for an input image from
the KITTI dataset.

same object. After obtaining the proposals returned by max-
pooling, we sort the proposals by their scores and output a
ﬁxed number of proposals for the second stage prediction.
We provide details on how to conduct multi-scale and multi-
channel max-pooling in the following.

3.1. Multi scale max pooling

As score maps are generated by multi-scale anchors, it
is natural to use multi-scale kernel sizes for different score
maps when conducting max-pooling. Given a score map
for an anchor of size h × w, an object of size h × w will be
roughly projected to a h
s area on the map, where s is
the stride of the map. Suppose the centers of two objects are
αw apart on the image, they will correspond to two peaks
that are αw
s apart on the score map. In order to not miss
either peak for object proposal, the kernel size and stride of
the max-pooling operation along the x dimension can be set
as:

s × w

ksizex, stridex = max(1, round(

αw
s

)).

Similarly for y dimension, we set the parameters as:

ksizey, stridey = max(1, round(

αh
s

)).

(1)

(2)

Table 1 presents the kernel sizes and strides for the 12
score maps computed by Eq. 1 and Eq. 2 with α = 0.25 and
s = 16. The algorithm to perform multi-scale max-pooling
is summarized in Algorithm 1.

Table 1. The kernel sizes and strides for the max-pooling operation
on the 12 score maps when α = 0.25 and s = 16.
5122
6x11
8x8
11x6

1282
1x3
2x2
3x1

2562
3x6
4x4
6x3

642
1x1
1x1
1x1

1:2
1:1
2:1

3.2. Multi channel max pooling

An object can produce multiple peaks on neighboring
score maps. This property can be utilized to reduce false

9358

  ScaleAspect ratio1:21:12:1 (a)(b)(c)(d)(e)(i)(f)(g)(h)(j)(k)(l)128225625122642Algorithm 1 Multi-Scale Max-Pooling
1: input:
2: M score: array of nar × nscl score maps, where nar is the number

of aspect ratios and nscl is the number of scales

3: output:
4: M max: array of nar × nscl maps of the same size as M score that

record the maximum value within a pooling window

5: parameters:
6: kx, ky: array of nar × nscl kernel sizes computed by Eq. 1 and Eq. 2
7: sx, sy: array of nar × nscl strides computed by Eq. 1 and Eq. 2
8: functions:
9: maxpool(x, ksizes, strides): perform max-pooling on input x with
the given pooling sizes and strides. ksizes and strides are 3-D vec-
tors specifying the pooling parameters along the x, y and channel di-
mension.

10: procedure
11: M max ← M score
12:
13:
14:
15:
16:
17: end procedure

end for

end for

for i ∈ [0, 1, · · · , nar − 1] do

for j ∈ [0, 1, · · · , nscl − 1] do
i,j , ky

maxpool(M max

, [kx

i,j

i,j , 1], [sx

i,j , sy

i,j , 1])

positives. Speciﬁcally, we apply a 2-channel max-pooling
on the two neighboring score maps. Two maps are consid-
ered to be “neighbors” if they are horizontally or vertically
connected in the scale-aspect ratio grid shown in Fig. 3.
To perform 2-channel max-pooling across aspect ratios (Al-
gorithm 2), we ﬁrst perform max-pooling on M score
, then
concatenate the max-pooling results of M score
i,j with M score
i+1,j
in the channel dimension, and then perform 2-channel max-
pooling on the concatenated map. Max-pooling across
scales (Algorithm 3) is conducted in a similar manner.
The difference between Algorithm 2 and Algorithm 3 is
the choice of kernel sizes and strides for 2-channel max-
pooling. For max-pooling across scales, we ﬁnd it neces-
sary to use the smaller one of the two maps’ kernel sizes
and strides, otherwise peaks on the smaller scale score map
are incorrectly removed.

i,j

Algorithm 2 Multi-Channel Max-Pooling Across Aspect
Ratios
1: procedure
2: M max ← M score
3:
4:
5:
6:
7:
8:

for j ∈ [0, 1, · · · , nscl − 1] do

for i ∈ [0, 1, · · · , nar − 2] do

maxpool(M max

i,j , 1], [sx

if i == 0 then

i,j , ky

i,j , sy

i,j , 1])

end if
M concat ← concatenate(M max
i+1,j , ky
maxpool(M concat, [kx

, M max
i+1,j )
i+1,j , 2], [sx

i+1,j , sy

, [kx

i,j

i,j

i+1,j , 2])

9:
10:
11:
12: end procedure

end for

end for

Algorithm 3 Multi-Channel Max-Pooling Across Scales
1: procedure
2: M max ← M score
3:
4:
5:

for j ∈ [0, 1, · · · , nscl − 2] do

for i ∈ [0, 1, · · · , nar − 1] do

M concat ← concatenate(M max
maxpool(M concat, [kx
if j == nscl − 2 then

i,j , ky

i,j

i,j , 2], [sx

, M max
i,j+1)
i,j , sy

i,j , 2])

maxpool(M max

i,j+1, [kx

i,j+1, ky

i,j+1, 1], [sx

i,j+1, sy

i,j+1, 1])

6:
7:
8:
9:
10:
11:
12: end procedure

end for

end for

end if

4. Experimental Results

4.1. Accuracy benchmarking

In this section, we report the detection accuracy of Max-
poolNMS on three benchmarking datasets, i.e., MS COCO
[21], KITTI [9] and PASCAL VOC [8]. The pooling strat-
egy in Algorithm 1, 2 and 3 is denoted as MS, MC-AR and
MC-SCL, respectively. The α used in the experiments is
0.25 for all datasets. We will investigate the sensitivity of
α on accuracy in Section 5. Our experiment is based on the
Tensorﬂow object detection API [18]. For anchor genera-
tion, we use 4 scales {642, 1282, 2562, 5122} and 3 aspect
ratios (width to height) {1 : 2, 1 : 1, 2 : 1} in all the experi-
ments. The number of proposals is 300.

COCO For COCO dataset, we directly use the pretrained
detection models with ResNet-101-V1 [13] backbone pro-
vided in the Tensorﬂow detection model zoo and replace
GreedyNMS with MaxpoolNMS in inference. Image sizes
are scaled to lie within [600, 1024]. The model is evalu-
ated on COCO test-dev2017 that contains 20,288 images
and the results are reported in Table 2. It can be seen that
MaxpoolNMS is able to match the accuracy of GreedyNMS
for different matching IoUs and object sizes.

KITTI For KITTI dataset, we randomly split 1870 im-
ages from the training set as the validation subset and train
on the rest 5611 images. We use the ResNet-50-V2 [14]
as the backbone network. The optimizer is SGD with mo-
mentum 0.9 and random horizontal ﬂips is used for data
augmentation. GreedyNMS is still employed in ﬁnetuning
and is replaced with MaxpoolNMS in inference. For Faster
R-CNN, the network was trained for 100k iterations with a
batch size of 1. The initial learning rate is 0.001, and de-
cayed by a factor of 0.1 at 75k iterations. For R-FCN, we
trained for 150k iterations and other settings are the same as
training Faster R-CNN. Input images are resized so that the
maximum dimension is 1024. The results are reported in
Table 3. MaxpoolNMS can achieve comparable mean aver-
age precision (mAP) as GreedyNMS for testing samples at
various difﬁculty levels from easy to hard.

9359

Table 2. Evaluation results on COCO test-dev2017.

mAP@IoU
0.50

0.50:0.95

mAP@area

0.75

S

M

L

Recall@maxDets
1

10

100

Faster R-CNN+ResNet-101-V1
GreedyNMS
MS
MC-AR
MC-SCL

30.1
30.1
30.1
29.7

48.0
48.2
48.1
47.6

R-FCN+ResNet-101-V1
GreedyNMS
MS
MC-AR
MC-SCL

27.0
27.3
27.2
26.9

45.2
45.5
45.3
44.9

32.3
32.3
32.3
31.9

28.7
28.9
28.8
28.5

10.4
10.5
10.5
10.5

9.3
9.6
9.1
9.2

32.8
32.9
32.8
32.5

30.0
30.3
30.1
29.8

46.2
46.1
45.8
44.6

39.2
39.4
39.3
38.2

26.5
26.5
26.4
26.2

23.8
24.0
23.8
23.7

37.3
37.4
37.1
36.6

33.3
33.5
33.2
33.0

37.8
37.9
37.5
37.1

33.9
34.0
33.7
33.4

Recall@area

S

M

L

12.9
12.9
12.9
12.9

11.3
11.5
11.4
11.5

40.5
40.8
40.7
40.3

36.8
37.0
36.9
36.6

59.7
59.5
58.7
56.9

51.3
51.1
50.5
49.4

Table 3. Evaluation results on KITTI val.

mAP

Easy Moderate

Hard

Car

Pedestrian
Easy Moderate

Hard

Easy Moderate

Hard

Cyclist

Faster R-CNN+ResNet-50-V2
GreedyNMS
98.68
98.83
MS
MC-AR
98.67
98.84
MC-SCL

81.29
81.35
81.27
81.78

R-FCN+ResNet-50-V2
GreedyNMS
MS
MC-AR
MC-SCL

80.51
80.42
80.29
81.03

96.39
96.85
96.80
96.97

94.56
95.17
95.06
94.51

93.26
93.49
93.40
93.65

88.43
88.51
88.65
88.63

86.76
87.21
86.80
87.39

86.92
86.96
87.21
87.51

88.38
88.28
88.17
88.81

81.09
80.88
81.14
81.72

81.43
81.16
81.36
82.02

75.36
75.11
75.61
76.14

76.01
75.58
75.68
76.65

86.92
87.27
86.25
87.12

86.33
86.28
85.69
85.79

82.57
82.74
81.98
83.09

81.42
81.36
81.19
81.82

80.09
80.42
79.56
80.58

78.76
78.48
78.40
79.05

VOC We trained the network on the union set of
VOC2007 trainval and VOC2012 trainval (07+12), which
contains a total of 16,551 (5011 + 11540) images. The net-
work was trained with a batch size of 4 on 4 GTX 1080Ti
GPUs. For Faster R-CNN, the number of iterations and
learning rate setting are the same as KITTI. For R-FCN, we
trained for 150k iterations. The initial learning rate is 0.003,
decayed by 0.1 at 80k and 120k iterations. Images are re-
sized so that the minimum dimension is 600. The model is
then evaluated on the VOC2007 test that contains 4952 im-
ages. The results are shown in Table 4 . It can be seen that
MC-AR and MS both achieve comparable mAP as Gree-
dyNMS, yet there is some noticeable drop (2-5%) in mAP
for MC-SCL. We will investigate why MC-SCL caused the
mAP drop in Section 5.

4.2. Speed benchmarking

This section compares the execution time of Maxpool-
NMS with GreedyNMS. We implemented the entire Faster
R-CNN pipeline in C++, and plugged in the GreedyNMS or
MaxpoolNMS for the ﬁrst stage NMS. We trained Faster R-
CNN with ResNet-152-V2 backbone network on KITTI and
the C++ model loaded weights from the Tensorﬂow-trained
model. The input image size is 1920 ∗ 580 and timing is
averaged over 100 images randomly selected from KITTI.
Letting n denote the average number of boxes before
NMS, and m the number of boxes after NMS, we con-
duct speed benchmarking with different n and m, and pro-
vide the timing breakdown in Table 5. It can be seen that
our method can achieve signiﬁcant speed-up (5×–20×) re-
gardless of different n and m values. The time complex-
ity of the two algorithms can be analyzed as follows. For

GreedyNMS, the complexity is O(n log(n)) (step 1: sort-
ing) + O(nm) (step 2: 2 nested loops). For Maxpool-
NMS, the time complexity is O(n) (step 1: max-pooling)
+ O(n log(n)) (step 2: sorting). Our method is more efﬁ-
cient than GreedyNMS in two aspects : (1) sorting is per-
formed after max-pooling in our method, and thus there are
far less elements to sort; (2) max-pooling only involves sim-
ple comparison operation, while the 2 nested loops requires
intensive computation of IoU which is a much more expen-
sive operation involving division.

5. Discussions

Why does MaxpoolNMS work? GreedyNMS can be
viewed as a special type of max-pooling, where the pooling
window size and stride are adaptively determined by IoU
and objectness scores, while MaxpoolNMS is max-pooling
with ﬁxed pooling parameters. Figure 4 compares the boxes
selected by the two methods. Comparing Fig. 4 (c) with
Fig. 4 (d) and (e), it can be seen that the two methods select
an overlapping but not exactly the same set of boxes. Quan-
titatively, the overlapping percentage (i.e., the percentage of
boxes selected by both methods) is around 40% – 50% as
shown in Table 6. It may be surprising that given such a
low overlap, the two methods can actually achieve compa-
rable mAP. To explain this, we draw the precision vs. recall
curves for region proposals and ﬁnal detections in Fig. 5. It
can be seen that the region proposals selected by Maxpool-
NMS has lower precision than GreedyNMS for different
recall values. However, the precision gap between Gree-
dyNMS and MaxpoolNMS is eliminated after the second
stage prediction. We thus conclude that the effectiveness

9360

mAP

aero

bike

brid

boat

Table 4. Evaluation results on VOC2007 test.
horse
bottle

chair

table

cow

dog

bus

car

cat

mbike

person

plant

sheep

sofa

train

tv

Faster R-CNN+ResNet-50-V2
GreedyNMS
MS
MC-AR
MC-SCL

74.28
73.85
74.38
69.16
R-FCN+ResNet-50-V2
70.93
GreedyNMS
MS
71.01
71.31
MC-AR
MC-SCL
69.03

75.32
74.54
75.79
70.87

78.84
77.96
78.14
77.32

78.92
79.13
80.21
71.02

77.93
79.15
79.30
76.66

77.13
76.68
76.70
69.77

71.41
72.18
72.67
67.69

68.11
67.31
67.44
57.51

62.08
60.83
60.70
58.40

61.60
59.36
62.50
56.32

55.85
55.95
56.75
52.64

81.21
80.81
81.52
75.09

77.58
78.99
79.72
75.31

84.18
83.96
84.05
80.26

82.17
82.21
82.77
81.50

84.15
84.16
84.23
78.37

80.97
81.56
82.25
79.44

56.85
57.13
56.11
50.67

52.53
52.76
51.58
51.08

82.02
82.12
82.07
79.30

77.74
78.31
78.01
78.07

62.75
61.99
63.32
59.45

57.77
57.32
59.05
57.38

82.29
81.11
81.49
77.39

77.94
78.45
78.36
76.53

83.66
83.69
83.62
84.25

81.85
81.27
81.30
81.63

76.76
76.96
77.18
71.44

75.56
76.38
75.76
74.12

80.60
80.57
80.71
78.06

77.79
77.83
77.52
76.58

45.80
46.28
46.81
41.17

42.82
42.81
41.96
41.07

77.91
78.07
79.16
75.81

73.56
73.53
74.99
70.77

71.69
70.77
70.67
67.40

66.41
65.39
66.35
63.24

78.85
77.11
77.44
68.17

74.86
74.74
77.35
71.55

75.85
75.24
76.53
70.79

73.02
72.69
71.57
69.66

Table 5. Timing breakdown (in ms) for GreedyNMS (step 1: sorting, step 2: 2 nested loops) and MaxpoolNMS (step 1: max-pooling, step
2: sorting). Timing is reported as total timing (sorting/the other step). We use CPU implementation for both methods.

Score Threshold
GreedyNMS (m=300)
GreedyNMS (m=100)
MS
MC-AR
MC-SCL

0 (n=53280)
31.69(18.19/13.50)
22.33(18.93/3.40)
6.67(6.22/0.45)
3.44(2.75/0.69)
4.27(3.30/0.97)

0.001 (n=9745)
16.30(2.83/13.46)
6.13(2.87/3.26)
0.82(0.39/0.43)
0.86(0.23/0.64)
1.10(0.26/0.84)

0.05 (n=5408)
13.19(1.47/11.72)
4.87(1.56/3.31)
0.51(0.07/0.44)
0.66(0.04/0.64)
0.86(0.06/0.80)

of MaxpoolNMS is attributed to the following two factors:
1) MaxpoolNMS can select a different region proposal than
GreedyNMS that is equivalently effective to detect an ob-
ject, as illustrated by an example in Fig. 6; 2) The second
stage GreedyNMS (which we do not replace with Maxpool-
NMS) can remove duplicates for ﬁnal detections.

Figure 4. Comparison of the bounding boxes (i.e., region propos-
als) selected by GreedyNMS and MaxpoolNMS. (a) An input im-
age from KITTI. (b) The objectness score maps for scale 642 and
1282 (from left to right) and aspect ratios {1 : 2, 1 : 1, 2 : 1}
(from top to bottom). (c) Bounding boxes selected by GreedyNMS
(a white pixel indicates that the box corresponding to the anchor at
that location is selected). (d) Bounding boxes selected by MS. (e)
Bounding boxes selected by MC-SCL. The red boxes in (d) and
(e) are to demonstrate how cross-scale max-pooling can help to
remove duplicates for the same object.

We have found that MaxpoolNMS is not working well
for one-stage object detectors and second stage NMS. The
reason is that, instead of selecting 50 – 300 boxes as region
proposals, we need to select few boxes (5 – 10) as the ﬁnal
detections in such cases, which will require more sophis-
ticated methods to design the pooling sizes and strides in

Table 6.
Overlapping percentage of the boxes selected by
GreedyNMS and MaxpoolNMS on KITTI val with Faster R-
CNN+ResNet-50-V2 detector.

No. of proposals

MS

300
200
100
50

49.61% 43.14%
41.30% 37.67%
39.01% 46.45%
39.60% 50.84%

MC-AR MC-SCL
50.84%
41.70%
39.29%
40.02%

order to accurately remove duplicates. Fine-tuning is not
helping here, as MaxpoolNMS itself does not have learn-
able parameters and the way it affects training is by select-
ing different boxes (vs. GreedyNMS) for second stage pre-
diction. However, different selecting strategies will not af-
fect second stage training signiﬁcantly [4]. One solution
may be to borrow the idea of deformable convolutional net-
works [6] to design a deformable max-pooling, with addi-
tional modules to learn an offset ﬁeld for the max-pooling
kernel sizes and strides.

Sensitivity of hyper parameter α MaxpoolNMS has one
hyper-parameter, i.e., α in Eq. 1 and Eq. 2, which deter-
mines the pooling kernel sizes and strides and thus control
the trade-off between precision and recall. A large kernel
size can help to remove more duplicates (high precision) at
the risk of missing detections (low recall). To investigate its
effects on detection accuracy, we vary α from 0.1 to 1.0 and
run evaluation on COCO val2017 with Faster R-CNN de-
tector. Also, we vary the proposal number from 50 to 300.
Results are presented in Fig. 7. It can be seen that for MS
and MC-SCL, mAP is stable and can match GreedyNMS
when α is in the range of [0.2, 0.8] for proposal number
of 200 and 300, and in the range of [0.5, 1.0] for proposal
number of 50 and 100, i.e., a small proposal number re-
quires larger pooling size and strides. This is intuitive – in
order to select a more compact set of proposals, we need to
consider a larger area for comparison. MC-AR tends to be
more sensitive to α, implying that proposals in neighboring

9361

  (a)(b)(d)(c)(e)Figure 5. Precision vs. recall curves for region proposals (top row) and ﬁnal detections (bottom row) at different matching IoUs. The
curves are draw on KITTI val with Faster R-CNN+ResNet-50-V2 detector. The precision gap in region proposals between MaxpoolNMS
and GreedyNMS is eliminated in ﬁnal detections.

Figure 6. Two correct detections of a car that are predicted from
two different proposals for a VOC2007 test image. (a) Final de-
tection (in yellow box) and its corresponding proposal (in dashed
red box) selected by GreedyNMS. (b) Final detection and its cor-
responding proposal selected by MaxpoolNMS with MS scheme.
Red arrows indicate the anchors that are responsible to generate
the proposals. The detector is Faster R-CNN+ResNet-50-V2.

aspect ratios are more likely to contain different objects (so
considering them as duplicates and removing them using
MC-AR can cause mAP drop).

Inﬂuence of large anchor scales We observe that the ob-
jectness score maps corresponding to large anchor scales
typically have high response in a relatively large area. This
can be attributed to several factors: 1) if there are few
ground truth objects that are matched to this anchor size,
the corresponding score map can not train properly and thus
can not make good predictions. This explains for the almost
homogeneous score maps (c), (d) , (h) and (l) in Fig. 3; 2)
As the effective receptive ﬁeld is much smaller than the the-
oretical one [23], the classiﬁer neuron is probably learning
to predict an object based on part of it and thus produce a
dense prediction for large objects; 3) During training, any
anchor with an IoU above some threshold with a ground
truth object is considered as positive sample and for large
objects, anchors surrounding the object are easier to have
high IoUs. These anchors have different receptive ﬁelds and
thus the classiﬁer is trained to give high response to differ-
ent areas of the object. Figure 8 displays the mean score
value for the 12 score maps shown in Fig. 3. As expected,

score maps for small anchors are sparse (very small mean
score value) while score maps for large anchors are dense
(large mean score value). The presence of dense score maps
can cause problem for cross-channel pooling, especially for
MC-SCL, as the dense maps may suppress the correctly-
selected peaks in neighboring score maps of smaller an-
chor scales. This is demonstrated by an example in Fig. 9.
One solution is to set a threshold to separate sparse maps
from dense maps and only conduct cross channel pooling
on the sparse maps (with MS for dense maps). As shown
in Fig. 10, with proper threshold (i.e., 0 – 0.01), MC-SCL
can also match the accuracy of GreedyNMS on VOC2007
test. Note that though KITTI also have dense score maps in
the last scale (i.e., 5122), it is not affected by them as most
of its objects are on scale 642 and 1282 (see Fig. 11 for the
object size distribution for the three datasets).

Which max-pooling scheme works the best? Based on
the above discussions, we conclude that each scheme has
its own strength and weakness, which are summarized in
Table 7. The best scheme is thus a trade-off between ac-
curacy, implementation complexity and sensitivity given a
speciﬁc problem.

Table 7. Strength and weakness of the three pooling schemes.

MS

MC-AR

MC-SCL

Strength
easiest to implement and
can match GreedyNMS
with properly selected pa-
rameters
can further remove dupli-
cates in neighboring as-
pect ratios
can further remove dupli-
cates in neighboring scales

Weakness
the best obtainable accu-
racy is lower than MC-AR
and MC-SCL

more sensitive to α

easier to be affected by
dense score maps

9362

  Matching IoU=0.5Matching IoU=0.6Matching IoU=0.7Matching IoU=0.8Matching IoU=0.9Precision vs. Recall curve for region proposalsPrecision vs. Recall curve for final detections  (a)(b)Figure 7. Sensitivity of α on COCO val2017 for different max-pooling schemes and proposal numbers. (a) MS; (b) MC-AR; (c) MC-SCL.
The detector is Faster R-CNN+ResNet-101-V1.

Figure 8. The mean score values for the 12 maps. Vertical black
lines indicate standard deviations.

Figure 10. Effects of sparse map threshold on VOC2007 test. (a)
Faster R-CNN detector; (b) R-FCN detector.

Figure 9. Dense maps may suppress the correctly-selected peaks
in neighboring maps. (a) Score maps corresponding to scale 2562
(top row) and 5122 (bottom row) for the image shown in Fig. 6. (b)
Max-pooling results of MS. (c) Max-pooling results of MC-SCL.
Red arrow indicates the correct anchor that should be selected to
predict the car in Fig. 6, which is incorrectly removed by cross-
scale pooling in (c).

6. Conclusions

GreedyNMS can be a performance bottleneck, particu-
larly with improved computing capabilities for parallel op-
erations. In this paper, we introduced MaxpoolNMS, a par-
allelizable alternative to NMS for region proposal network.
We utilized the fact that an object proposal should corre-
spond to a peak in the objectness score maps and thus can
employ multi-scale max-pooling to obtain this peak. We
also exploited the fact that an object can produce multi-

Figure 11. The distribution of object size for COCO, KITTI and
VOC.

ple peaks on neighboring score maps and proposed to use
multi-channel pooling across aspect ratios and scales to re-
move duplicates. Extensive experiments on COCO, KITTI
and VOC2007 with popular two-stage object detectors, i.e.,
Faster R-CNN and R-FCN, demonstrated the effectiveness
and robustness of our method. Benchmarking on accuracy
and speed shows that MaxpoolNMS can achieve compa-
rable accuracy as GreedyNMS with up to 20× speed-up.
Our approach is scalable and parallelizable, making NMS
no longer a performance bottleneck in the region proposal
network. This paves the way towards high-performance and
real-time realization of two-stage object detectors.

Acknowledgements

This research is supported by A*STAR under

its
Hardware-Software Co-optimisation for Deep Learning
(Project No.A1892b0026).

9363

  (b)(a)(c)  (a)(b)(c)  (a)(b)[16] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning

non-maximum suppression. arXiv preprint, 2017. 2

[17] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen
Wei. Relation networks for object detection. arXiv preprint
arXiv:1711.11575, 2017. 2

[18] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,
Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-
jna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy
trade-offs for modern convolutional object detectors.
In
IEEE CVPR, 2017. 1, 2, 4

[19] Norman P Jouppi, Cliff Young, Nishant Patil, David Patter-
son, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh
Bhatia, Nan Boden, Al Borchers, et al. In-datacenter perfor-
mance analysis of a tensor processing unit. In Proceedings
of the 44th Annual International Symposium on Computer
Architecture, pages 1–12. ACM, 2017. 1

[20] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection. arXiv
preprint arXiv:1708.02002, 2017. 2

[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014. 4

[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. SSD: Single shot multibox detector. In European con-
ference on computer vision, pages 21–37. Springer, 2016. 1,
2

[23] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel.
Understanding the effective receptive ﬁeld in deep convolu-
tional neural networks. In Advances in neural information
processing systems, pages 4898–4906, 2016. 7

[24] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016. 1, 2

[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In Advances in neural information
processing systems, pages 91–99, 2015. 1, 2

[26] Lachlan Tychsen-Smith and Lars Petersson. Improving ob-
ject localization with ﬁtness nms and bounded iou loss. arXiv
preprint arXiv:1711.00164, 2017. 2

References

[1] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and
Larry S Davis. Improving object detection with one line of
code. arXiv preprint arXiv:1704.04503, 2017. 2

[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-
arXiv preprint

ing into high quality object detection.
arXiv:1712.00726, 2017. 2

[3] Xinlei Chen, Ross Girshick, Kaiming He, and Piotr Doll´ar.
Tensormask: A foundation for dense object segmentation.
arXiv preprint arXiv:1903.12174, 2019. 2

[4] Xinlei Chen and Abhinav Gupta. An implementation of
faster rcnn with study for region sampling. arXiv preprint
arXiv:1702.02138, 2017. 6

[5] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: Object
detection via region-based fully convolutional networks. In
Advances in neural information processing systems, pages
379–387, 2016. 1, 2

[6] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In Proceedings of the IEEE international confer-
ence on computer vision, pages 764–773, 2017. 6

[7] Navneet Dalal and Bill Triggs. Histograms of oriented gra-
dients for human detection.
In Computer Vision and Pat-
tern Recognition, 2005. CVPR 2005. IEEE Computer Soci-
ety Conference on, volume 1, pages 886–893. IEEE, 2005.
1, 2

[8] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge.
International journal of computer
vision, 88(2):303–338, 2010. 4

[9] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2012. 4

[10] Ross Girshick.

Fast R-CNN.

arXiv preprint

arXiv:1504.08083, 2015. 2

[11] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
580–587, 2014. 2

[12] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 2980–2988. IEEE, 2017.
2

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 4

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In European
Conference on Computer Vision, pages 630–645. Springer,
2016. 2, 4

[15] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. A con-
vnet for non-maximum suppression. In German Conference
on Pattern Recognition, pages 192–204. Springer, 2016. 2

9364

