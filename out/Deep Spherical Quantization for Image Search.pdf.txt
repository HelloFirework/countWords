Deep Spherical Quantization for Image Search

Sepehr Eghbali and Ladan Tahvildari

University of Waterloo

{sepehr.eghbali,ladan.tahvildari}@uwaterloo.ca

Abstract

Hashing methods, which encode high-dimensional im-
ages with compact discrete codes, have been widely applied
to enhance large-scale image retrieval. In this paper, we
put forward Deep Spherical Quantization (DSQ), a novel
method to make deep convolutional neural networks gen-
erate supervised and compact binary codes for efﬁcient im-
age search. Our approach simultaneously learns a mapping
that transforms the input images into a low-dimensional
discriminative space, and quantizes the transformed data
points using multi-codebook quantization. To eliminate the
negative effect of norm variance on codebook learning, we
force the network to L2 normalize the extracted features and
then quantize the resulting vectors using a new supervised
quantization technique speciﬁcally designed for points lying
on a unit hypersphere. Furthermore, we introduce an easy-
to-implement extension of our quantization technique that
enforces sparsity on the codebooks. Extensive experiments
demonstrate that DSQ and its sparse variant can generate
semantically separable compact binary codes outperform-
ing many state-of-the-art image retrieval methods on three
benchmarks.

1. Introduction

Nearest neighbor search is one of the fundamental prob-
lems in multimedia systems. Given a query point, the
goal entails ﬁnding the most similar item to the query in a
dataset. Accuracy and speed are two key aspects in retrieval
systems, however, with explosive growth of high dimen-
sional items such as images, videos and documents on the
Internet, most of traditional branch and bound indexing data
structures are deemed impractical, mainly because of their
query time or memory cost that grow exponentially with
the number of dimensions. This has led to a burgeoning
ﬁeld of research, Approximate Nearest Neighbor (ANN),
that focuses on reducing storage and computational costs
with minimal accuracy loss.

ANN problem has witnessed a great amount of research
over the past two decades. The state-of-the-art in ANN is

mainly focused on hashing (compact coding), which aims
at encoding high-dimensional media data into short binary
codes subject to preserving a given notation of similarity.
Binary-valued representation has several advantages, such
as being compact to store and faster to compare, making it
a suitable ﬁt for large-scale nearest neighbor search. More-
over, for binary strings, one can achieve sublinear query
time using hash tables [13, 34] or tree-based indexing data
structures [11, 12]. Finding compact binary codes that bet-
ter respect the given notion of similarity has been the topic
of much work over the last two decades during which a
rich set of hashing techniques has been proposed. Com-
pact coding techniques are roughly in two streams, catego-
rized by the way they compute distance between encoded
items: 1) Binary hashing maps high-dimensional input vec-
tors into Hamming space where the distance between two
codes can be computed extremely fast using bitwise opera-
tors. 2) Multi-Codebook Quantization (MCQ) which, anal-
ogous to k-means algorithm, partitions the input space into
non-overlapping cells and then approximates the distance
between two points with the distance between the centers
of cell they belong to. The search speed enhancement of
MCQ stems from the fact that the distance between cells
can be pre-computed and stored in lookup tables.

Not surprisingly, with the dawn of deep learning, most of
recent research effort in compact coding has been directed
towards using deep networks for producing compact and
functional binary codes. Deep hashing methods simulta-
neously learn the representation and hash coding from raw
images. Similarly, deep MCQ has been the topic of study in
recent years [4, 17]. Surprisingly enough, although MCQ is
a more powerful model as it enables producing many more
possible distinct distances, due to the lack of research, its
performance in the context of deep supervised compact cod-
ing is inferior to state-of-the-art in supervised binary hash-
ing [22].

Most of existing deep supervised MCQ techniques incor-
porate an unsupervised quantization (usually product quan-
tization (PQ) [18]) on top of the features generated by a
deep architecture. Nevertheless, the adopted networks often
produce deep features with relatively high norm variance

11690

which adversely affects the quality of quantization [44]. To
address this shortcoming, we reformulate the quantization
problem by L2 normalizing deep features to remove norm
variance. By exploiting the fact that resulting features lie
on a hypersphere, we propose a novel MCQ algorithm that
drops the hard orthogonality constraint of product quanti-
zation to achieve lower quantization error. Furthemore, to
encourage better discriminating performance, inspired by
the recently proposed center loss [43], we add a supervised
quantization loss term to the ﬁnal objective function to in-
crease inter-class variance. Finaly, we propose a sparse ex-
tension of our quantization algorithm which is necessary for
dealing with large codebooks [47]. Comprehensive empir-
ical studies on three standard image retrieval benchmarks
testify that DSQ generates compact binary codes which out-
perform many state-of-the-art methods.

2. Related Work

Existing hashing methods consist of supervised and un-
supervised hashing. We refer interested readers to [41] for
a comprehensive survey.

Unsupervised hashing methods learn hash functions that
map data to binary codes using unlabeled data. Typical
learning criteria are reconstruction error minimization [20],
preserving local neighborhood [26] and quantization error
minimization [15]. Supervised hashing, on the other hand,
aims at learning binary codes that are faithful to a given
notion of semantic information such as point-wise (class la-
bels) [22, 37, 42], pairwise [2, 3, 6] or triplet labels [24, 33].
Multi-Codebook Quantization. A subclass of unsuper-
vised hashing methods, called Multi-Codebook Quantiza-
tion (MCQ), is formulated as a quantization problem which
aims at approximating vectors with summation of multiple

codewords. Formally, let X = [x1, . . . , xn] ∈ Rd×n de-
note the set of n points to be quantized, MCQ is the problem
of ﬁnding 1) m codebooks (dictionaries) Cj ∈ Rd×h, j ∈
{1 . . . , m} each containing h codewords, and 2) the encod-
im]T ∈ {0, 1}mh×1,
ing binary vectors bi = [bT
that minimize the quantization error:

i1, . . . , bT

n

X

i=1

kxi − [C1, . . . , Cm]bik2

2

(1)

where each subcode bij is limited to having only one non-
zero entry, kbijk1 = 1, to ensure only one codeword per
codebook is selected. During the query phase, MCQ uses
the approximation of each point to estimate the distance be-

tween the query q ∈ Rd and each data point:

m

kq − xik2

X

2 ≈
+ X

i=1

t6=j

kq − Cbik2
(Ctbit)T Cj bij

2 − (m − 1)kqk2

2

(2)

Given the query, the ﬁrst term can be efﬁciently com-
puted using lookup tables that store the distance between the
query and each codeword. The second term can be ignored
during search time as it is constant for a given ﬁxed query.
One of the key features that differentiates MCQ techniques
is how they handle the third term. Product quantization
(PQ) [18], Cartesian K-means (CKM) [32] and Optimized
Product Quantization (OPQ) [14] restrict the codebooks to
be mutually orthogonal making the third term equal to zero.
Composite Quantization [40], on the other hand, forces it to
be a constant which in turn makes the resulting optimization
problem hard to solve.

Additive quantization (AQ) [1] and its enhanced exten-
sion Local Search Quantization (LSQ) [29] expand kq −
xik2
2 based on the inner product of query and codewords but
their formulation requires not only approximating the input
vector but also its L2 norm kxik2
2. AQ provides two solu-
tions to estimate the norm. The ﬁrst is to separately quan-
tize the scalar value kxik2
2, which results in an additional
memory cost that grows linearly with the database size.
The other way is to estimate the norm with the codewords
which makes the cost of distance computation quadratic in
the number of codebooks.

Supervised MCQ. While most of the research in super-
vised hashing is focused on supervised binary hashing, a
handful of studies have been recently proposed on using
MCQ in the supervised setting. Supervised MCQ tech-
niques can be, for the most part, described as a combina-
tion of supervised loss function and one of the unsupervised
MCQ techniques described above. Supervised Quantiza-
tion (SQ) [42] combines supervised L2 loss with CQ, how-
ever, the resulting optimization problem is hard to solve as it
inherits the constant inter-dictionary-element-product con-
straint from CQ. Deep Quantization Network (DQN) [4]
combines a deep architecture and PQ. One shortcoming of
DQN is that during codebook optimization, it ignores the
supervisory information. SUBIC [17] integrates the one
hot-encoding layer in deep neural network which encodes
each image with concatenation of one-hot block similar to
PQ. However, its sparse property limits its representation
capability.

Effect of Norm Variance on MCQ. Recently, Wu et
al. [44] have shown that norm variance adversely affects the
quantization error of unsupervised MCQ techniques, even
when the variance is relatively moderate. To address this
issue, authors propose to separately scalar quantize the data
point norms and then unit normalize the data points before
applying PQ. Nevertheless, it is not clear how the quantiza-
tion budget should be split between the two the quantizers.
Also, PQ imposes strong orthogonality on the codebooks
which reduces the ﬁdelity of leaned codebooks [1].

We conclude this section by empirically showing the ef-
fect of norm variance on the performance of supervised

11691

Unnormalized

Normalized

0.75

0.7

0.65

P
A
M

DQN [4]

DTQ [24]

Figure 1: Performance of two supervised MCQ models with
and without feature normalization on CIFAR-10 for 64-bit
codes.

quantization. To this aim, we run two state-of-the-art super-
vised MCQ techniques with and without feature normaliza-
tion on CIFAR-10 dataset (settings of the experiment will
be discussed in Section 4). Figure 1 plots the MAP perfor-
mance of two supervised MCQ techniques and it demon-
strates that one can achieve marginal performance gain by
simply normalizing the features during training without in-
curring any additional cost.

3. Proposed Approach

In similarity retrieval, we are given a training set of n
points, X = {xi ∈ Rd}n
i=1, with each point associated with
a class label, yi ∈ {1, . . . , l}. The goal, given query point
q ∈ Rd, entails (approximately) ﬁnding items in X that are
semantically closest to q so that the found neighbors share
the same class label as q. This paper follows the idea of
compact coding techniques that is converting database vec-
tors into compact code and then performing the similarity
search in the resulting space which has the advantage of
lower memory cost and fast distance computation.

In this paper, we propose to use a deep network that maps
the input points into a discriminative space, and simultane-
ously perform a form of a supervised MCQ on the embed-
ded points to achieve fast retrieval with low computational
and storage overhead. To this aim, we deﬁne a loss func-
tion comprising four terms, softmax loss, center loss, quan-
tization loss, and discriminative loss each of which will be
discussed in the following.

3.1. Softmax and Center loss

In deep retrieval systems, obtaining a robust and discrim-
inative representation is crucial for achieving good perfor-
mance. Usually, this is achieved by applying the softmax
loss to the representation layer of the network. However, the
resulting features optimized with the supervision of softmax
loss are often not discriminative enough as the softmax loss
only focuses on ﬁnding a decision boundary that separates

different classes without considering the intra-class com-
pactness which is crucial to the accuracy of nearest neighbor
search [16, 43].

To increase the intra-class variations while keeping the
features of different classes separable, we adopt the state-
of-the-art center loss [43] on top of the softmax loss.

Let f (·; θ) : Rd → Rp, with p ≪ d, denote the
feed-forward network that embed the input vectors into p-
dimensional deep features, also let zi denote deep feature
representation of input xi, zi = f (xi; θ), then, the center
loss is deﬁned as:

LC =

n

X

i=1

kzi − φyik2

2

(3)

where yi is the classes label associated with zi and φyi de-
notes the yi-th class center of deep features.
Intuitively,
center loss learns a center for the features of each class
and meanwhile aims at pulling the deep features of the
same class close to its corresponding center.
It has been
shown that joint supervision of softmax loss and center loss
can produce signiﬁcantly better discriminative deep fea-
tures [43].

3.2. Quantization loss

We constrain the deep features to live on a p-dimensional
unit hypersphere, i.e. kf (x; θ)k2 = 1. Other than decreas-
ing the intra-class variability of deep features [39], there
are two advantages in normalizing feature vectors: 1) norm
variance is strictly zero, and 2) Euclidean nearest neigh-
bor search is equivalent to Maximum Inner Product Search
2 =

(MIPS) as for unit norm vectors we have kq − xk2
2 − 2qT x.

The main beneﬁt of dealing with MIPS is that, unlike Eu-
clidean distance (see (2)), inner product naturally satisﬁes
the distributive law, that is hq,Pj tji = Pjhq, tji. MCQ
works well in large part due to the fact that it permits the dis-
tance between query and a quantized point to be computed
as the summation of partial distances between query and se-
lected codewords. Given the query, the distances between
query and all codewords are stored in query-speciﬁc lookup
tables and then used to calculate the distance between query
and all quantized points. However, to make Euclidean dis-
tance satisfy the distributive law, we either need to enforce
strong [18, 32]/weak [46, 47] orthogonality constraints over
the codewords of different dictionaries which reduces the ﬁ-
delity of model and often leads to non-convex optimization,
or we have to store the inner product between the all code-
words in lookup table [1, 29] which increases storage cost
and distance computation time.

To reduce the approximation error of MIPS, we need to
minimize the distance reconstruction error of MCQ. Since
the Euclidean distance on the unit sphere is equal to the

11692

negative dot product plus a constant, distance reconstruction
error can be rewritten as:

problem with each iteration updating one set of parameters
while ﬁxing others.

E

q∼P (q)h

E

q∼P (q)h

n

X

i=1

n

X

i=1
n

|hzq, zii − hzq, ¯zii|i =

hzq, zi − ¯ziii ≤

(4)

X

i=1

kzi − ¯zik2

where ¯zi denotes the approximation of zi using MCQ and
zq = f (q; θ).

This suggests that the search accuracy directly depends
on the quantization error; low quantization error leads to
high search accuracy.

Therefore, the cost function we aim to optimize is the

quantization loss:

n

LQ({Cj},{bi}) =

2

kzi − [C1, . . . , Cm]bik2

X
i=1
bi = [bT
bij ∈ {0, 1}h,kbijk1 = 1
j = 1, . . . m

i1, . . . , bT

im]T

(5)

The beneﬁt of such a simple formulation, in comparison
to those that enforce multiple constraints on the code-
words [18, 32, 46] are multi-fold; it causes a straightfor-
ward optimization procedure and also less implementation
overhead.

3.3. Discriminative Dictionary Learning

Updating θ. With C, φyi s, and B ﬁxed, the parameters
of the network are updated through back-propagation as all
of the terms in the loss are differentiable.

Updating Φ. We follow a similar procedure to [43] for
updating the centers. In particular, to avoid large perturba-
tion caused by few mislabelled instances, we use a learning
rate parameter ζ for training the centers:

φt+1
yi = φt

yi − ζ∆φyj

(8)

i=1

i=1

✶(yi = j)

∆φyj = Pn

1 + Pn

✶(yi = j) · [λ(φyi − zi) + γ(φyi − Cbi)]
(9)
where ✶(condition) equals 1 if the condition is satisﬁed
and 0 otherwise. Ideally, the centers should be updated in
each iteration based on the whole training set which would
be extremely costly. To reduce the cost, the update is per-
formed on the mini-batches.

Updating C. Given B, φyi s and θ ﬁxed, the resulting

optimization problem is:

αkZ − CBk2

2 + γkΦ − CBk2

2

(10)

where Z = [z1, . . . , zn], B = [b1, . . . , bn], and Φ =
[φy1 , . . . , φyn ]. This is a quadratic function in C and there-
fore a closed-form solution exists:

C =

1

α + γ

(αZ + γΦ)BT (BBT )−1

(11)

It is easy to observe that the optimization problem de-
composes over each of the p dimensions. Thus, we can re-
duce the computational cost by solving p least square prob-
lem each with mh variables.

Finally, we also incorporate the supervisory information
during quantization procedure. In particular, we encourage
the quantized points to be closer to their centers. To achieve
this goal, we use the following loss:

min
C(t)

LD =

n

X

i=1

kφyi − Cbik2

2

(6)

Intuitively, (6) penalizes the cases where the point ¯zi is not
assigned to the clusters that are close to φyi .

The overall loss for training model takes the form:

L = Lsof tmax + αLQ + λLC + γLD

(7)

where α, λ and γ are the hyper-parameters that control the
effect of each term.

3.4. Optimization

The objective function composes of four sets of learnable
parameters, the parameters of the deep network θ, the cen-
ters φyi s, the codewords in matrix C, the codeword assign-
ment matrix B. We use alternative optimization to solve the

αkZ (t) − C (t)B(t)k2

2 + γkΦ(t) − C (t)B(t)k2

2

(12)

∀ t = 1, . . . , p

Each of the p problems is a least squares problem with a
closed form solution. Online learning algorithms can also
be leveraged for acceleration [28].

Updating B. Given θ, φyi and C ﬁxed, optimizing bi-
nary matrix B, known as encoding phase, has been histori-
cally identiﬁed as the bottleneck of MCQ [1, 29].

It can be seen that the composition indicator vector bi
is independent of all other vectors {bt}t6=i. Thus, the op-
timization problem with respect to B can be decomposed
into n independent subproblems:

2

2 + γkφyi − Cbik2

αkzi − Cbik2
bi = [bT
bij ∈ {0, 1}h,kbijk1 = 1
i = 1, . . . , n j = 1, . . . , m

i1, . . . , bT

im]T

(13)

min
bi

11693

The problem is essentially a high-order Markov Random
Field (MRF) problem which is NP-hard. Following [29],
we use Stochastic Local Search (SLS) method to optimize
bi. The idea of SLS for escaping local minima is to it-
eratively alternate between a local search procedure, and
a randomized pertubation to the current solution. For the
local search, we again use alternative optimization tech-
nique. Given {bij}j6=t ﬁxed, bit is updated by exhaustively
checking all codewords of Cj and ﬁnding the element that
minimizes the objective function in (13). For the pertur-
bation procedure of SLS, we randomly choose k codes by
sampling from the uniform distribution U (1, m). The se-
lected codes are perturbed by setting each of them to a uni-
formly selected random value between 1 and h. The result-
ing perturbed solution is then accepted as the starting point
of the next local search procedure. Although this procedure
is computationally demanding, it can be accelerated using
GPU implementation [30, 31], making encoding even faster
than codebook learning.

3.5. Asymmetric Distance Computation

Given the query, the search process starts by embedding
the query using the trained network, zq = f (q; θ). Then,
the inner product between zq and all codewords are stored
in m × h query-speciﬁc lookup table. Finally, inner prod-
uct between query and all database vector is approximated
with:

hzq, zii ≈

m

X

j=1

hzq, Cj biji

(14)

Therefore, computing the inner product between query and
each database item takes O(m) lookups and O(m) addition
operations (same as PQ), plus the time required to embed
the query into the deep feature space.

3.6. Sparse Codebook Learning

In sparse codebook learning, the optimization problem is
augmented with sparsity constraint on the codewords. The
key advantage of sparse coodebooks is that the distance be-
tween the query and every codeword can be computed ef-
ﬁciently using sparse vector manipulations. This is prac-
tically important as for large codebooks, with many code-
words, the time required for online construction of lookup
tables become non-negligible. Zhang et al. [47] have shown
that sparse codewords can increase the search speed up to
30%. As the name suggests, the Sparse Composite Quanti-
zation (SCQ) technique proposed in [47] adds sparsity con-
straint to the CQ [46] formulation and uses coordinate de-
scent to solve the optimization problem. However, CQ itself
involves a hard optimization problem and adding the spar-
sity constraint makes the problem even harder.

In contrast, in our formulation, codebook optimization
reduces to a linear regression problem, thus adding the

sparsity constraint changes the objective to a regularized
quadratic problem. In particular, using straightforward al-
gebraic manipulations (10) can be rewritten as:

(α + γ)k

αZ + γΦ
α + γ − CBk2

2

2 − kαZ + γΦk2
+ αkZk2

α + γ
2 + γkΦk2

2

(15)

Since only the ﬁrst term depends on C, we can write the
objective function of sparse quantization as:

min
C k

αZ + γΦ
α + γ − CBk2

2

s.t.

kCk0 ≤ ǫ

(16)

The resulting optimization is non-convex because of L0 reg-
ularization term. Commonly, such problems are relaxed by
replacing L0 norm with convex L1 norm. Therefore, our
ﬁnal objective function for learning sparse codebooks is de-
ﬁned as:

min
C k

αZ T + γΦT

α + γ

− BT C Tk2

2

s.t.

kCk1 ≤ ǫ

(17)

which is essentially a linear regression problem with L1
norm regularization on the coefﬁcients, known as Lasso in
the statistical literature. It can be efﬁciently solved using a
wide range of heavily-optimized off-the-shelf Lasso solvers
such as feature-sign search [21] or SPGL1 solver [38].

4. Experiments

In this section, we gauge the performance of the pro-
posed supervised quantization approach by comparing it
with the state-of-the-art against three different datasets..

4.1. Datasets and Evaluation

We conduct experiments on three standard datasets:

CIFAR-10 [19], NUS-WIDE [8] and ImageNet [9].

CIFAR-10 dataset consists of 60,000 32 × 32 color im-
ages evenly divided into 10 categories. We follow the ofﬁ-
cial split of the datasets and use 50K images as the training
set and 10k images as the query set.

NUS-WIDE is a set of 269,648 images collected from
Flickr. This is a multi-label dataset where each image is
associated with one or multiple labels from a given 81 con-
cepts. Following [37, 42], we collect 193,752 images that
are from the 21 most frequent labels for evaluation, includ-
ing sky, clouds, person, water, animal, grass, building, win-
dow, plants, lake, ocean, road, ﬂowers, sunset, relocation,
rocks, vehicles, snow, tree, beach, and mountain. For each
label, we randomly sample 100 images as query points and
the remaining images form the training set.

The dataset ILSVRC 2012, named as ImageNet in this
paper, contains over 1.2 million images covering 1,000 cat-
egories. Following the settings in [5, 7], we select 100 cate-
gories and use images associated with them in the provided

11694

training set and the validation set as the training and the
query sets, respectively.

Parameter setting. There are trade-off parameters in the
objective function (7): α for quantization loss, λ for center
loss and γ for discriminative loss. We select parameters via
validation. In particular, we choose a subset of the training
set (same size as the query set), and the best parameters are
chosen so that the average performance in terms of MAP is
maximized against the validation set. We ﬁx ζ to 0.5 and k
to 4.

Following almost all MCQ techniques [1, 32, 47], we
choose h = 256 to be the codebook size, so that each
subindex ﬁts into one byte of memory. This let us store
B as a m × n uint8 matrix. We vary m = {2, 4, 6, 8}
such that m log2 h is equal to the desired bit-rates which
are {16, 32, 48, 64}.

Experimental settings. We use the raw images as the
input for all deep methods, but the images are resized to ﬁt
the input of the adopted model. For fairness of comparison,
for all deep compact coding methods here, we use Alexnet
as the core architecture. To reduce the size of deep fea-
tures, we add a fully connected layer to the network which
transforms the output of the network into a 256-dimensional
feature space, thus p = 256. We do not tune the size of
feature space for saving time while we think that tuning it
might yield better performance. The L2 normalization is
performed on the 256-dimensional deep features using a L2
normalize layer [35].

We ﬁne-tune layers conv1–fc7 copied from the AlexNet
model pre-trained on ImageNet and train the last layer
which maps the feature layer via back-propagation. As the
last layer is trained from scratch, we set its learning rate
to be 10 times that of the other layers. We use mini-batch
stochastic gradient descent (SGD) with 0.9 momentum as
the solver, and cross-validate the learning rate from 10−5 to
10−2 with a multiplicative step-size √10. We also ﬁx the

mini-batch size of images as 128 and the weight decay pa-
rameter as 0.0005. Following [29], we use SPGL1 as the
lasso solver for the sparse extension of our algorithm [38].
For non-deep methods, we extract the outputs of the layer
‘fc7’ in the deep model [10] as input features.

Methods. We compare DSQ with a wide range of su-
pervised compact coding methods including binary hash-
ing methods: KSH [27], ITQ [15], SDH [37], CNNH [45],
DPSH [23], DSH [25], HashNet [5], and supervised quan-
tization techniques: SQ [42], SUBIC [17], DQN [4] and
DTQ [24]. We implemented SQ in Python as its source
code is not available at the time of writing this paper. We
tried our best to be faithful to the experimental settings of
the paper [42]. Other techniques are executed using the im-
plementation generously provided by the authors.

4.2. Results

Single domain retrieval. Single-domain retrieval is
the main experimental benchmark in the supervised binary
hashing literature in which the query and training items be-
long to the same set of class labels. To evaluate perfor-
mance of different techniques, we adopt the widely used
Mean Average Precision (MAP). We report the results of
MAP@5000 and MAP@1000 for NUS-WIDE and Ima-
geNet datasets respectively. Table 1 shows the single-
domain retrieval performance of DSQ against a wide-range
of techniques. The observation is that our proposed method
consistently delivers the best performance for different
length of codes. We attribute the performance improvement
to the proposed loss that aims at jointly preserving similar-
ity information and controlling the quantization error. Also,
dropping the orthogonality constraint increases the ﬁdelity
of codebooks which in turn reduces the approximation error
of nearest neighbor search. Finally, back-proping the pro-
posed supervised quantization loss can remarkably enhance
the quantizibilty of the deep representation.

Figure 2 also shows the performance of different tech-
niques in terms of the precision-recall curves for 64-bit
codes. From the curves, we can observe that DSQ deliv-
ers higher precision than the state-of-the-art compact cod-
ing methods at the same recall rate. This shows that DSQ is
also favourable for precision-oriented retrieval systems. Al-
though the query time comparison is not presented here due
to space limit, we observed that all deep MCQ techniques
in this study exhibit similar query time mainly because they
adopt the same core architecture (AlexNet). However, bi-
nary hashing techniques are often faster than deep MCQ
as they incorporate Hamming distance to compare binary
codes.

Sparse coding. We also show the performance of sparse
extension of DSQ. To the best of our knowledge, sparse
DSQ is the ﬁrst attempt to explore supervised sparse multi-
codebook quantization for semantic similarity search. Nev-
ertheless, we compare our technique with two unsupervised
sparse quantization techniques, SCQ [47] and SLSQ [29]
applied to the deep features of the ‘fc7’ layer of the deep
model in [10].

Following [46], we evaluate the sparse version of our
algorithm using two degrees of sparsity: SDSQ1 with

kCk0 ≤ ǫ = h · p and SDSQ2 with kCk0 ≤ ǫ = h · p + p2.

Since the former criterion imposes a harder sparsity con-
straint on the codebooks, we would naturally expect to
achieve lower search accuracy but better query time. We
compare against SCQ1 and SCQ2 from [47] and SLSQ1
and SLSQ2 from [29].

Figure 3 shows the performance of different techniques
against three different datasets. Again, in this scenario,
we observe that sparse DSQ comfortably outperforms the
baselines with a large margin mainly because sparse DSQ

11695

Method

16

CIFAR-10
48
32

KSQ
ITQ
SDH
CNNH
DPSH
DSH

HashNet

DTQ
SUBIC

SQ
DQN
DSQ

0.3216
0.2412
0.4199
0.5373
0.6367
0.6192
0.6857
0.7037
0.6555
0.6212
0.5979
0.7212

0.3285
0.2432
0.4301
0.5421
0.6412
0.6565
0.6923
0.7191
0.6789
0.6438
0.6097
0.7346

0.3371
0.2482
0.4392
0.5765
0.6573
0.6624
0.7183
0.7319
0.6854
0.6545
0.6099
0.7418

64

0.3384
0.2531
0.4465
0.5780
0.6676
0.6713
0.7187
0.7373
0.7014
0.6578
0.6133
0.7589

16

0.4061
0.5573
0.5342
0.6221
0.7015
0.7181
0.7331
0.7511
0.7021
0.7126
0.6913
0.7785

NUS-WIDE
48
32

0.4182
0.5932
0.6282
0.6233
0.7126
0.7221
0.7551
0.7812
0.7131
0.7138
0.7121
0.7899

0.4264
0.6128
0.6298
0.6321
0.7418
0.7521
0.7622
0.7886
0.7555
0.7303
0.7471
0.7918

ImageNet

64

0.4436
0.6166
0.6335
0.6372
0.7423
0.7531
0.7762
0.7892
0.7568
0.7423
0.7562
0.7988

16

0.1620
0.3115
0.2729
0.2888
0.3226
0.3428
0.5016
0.5128
0.5547
0.3865
0.5065
0.5769

32

0.2818
0.4632
0.4521
0.4472
0.5436
0.5500
0.6219
0.6123
0.5597
0.5586
0.6205
0.6541

48

0.3422
0.5223
0.5329
0.5328
0.6217
0.6329
0.6613
0.6727
0.6462
0.6279
0.6669
0.6800

64

0.3934
0.5446
0.5893
0.5436
0.6534
0.6645
0.6824
0.6916
0.6622
0.6618
0.6912
0.6940

Table 1: Single-domain category retrieval performance of DSQ versus the state-of-the-art with 16, 32, 48 and 64 bit codes.

CIFAR-10

NUS-WIDE

ImageNet

0.8

0.6

0.4

0.2

P
A
M

1

0.8

0.6

0.4

0.2

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

KSH [27]

ITQ [15]

SDH [37]

CNNH [45]

DPSH [23]

DSH [25]

HashNet [5]

DMDH [7]

SUBIC [17]

SQ [42]

DQN [4]

DSQ

Figure 2: Precision-recall curves on the CIFAR-10, NUS-WIDE and ImageNet datasets for 64-bit codes.

jointly optimizes the quantization error while preserving
the semantic similarity and satisfying the sparsity con-
straint, whereas the other benchmarks separately apply un-
supervised sparse quantization, which merely minimizes
the quantization error.

Cross-domain retrieval. To further evaluate our super-
vised quantization method, we follow an alternative eval-
uation protocol from [36] wherein the model learned on a
given set of training classes is tested on a new, disjoint set of
test classes. This protocol is used to show how each method
is capable of preserving the semantic information of certain
classes implicitly even if the class samples are not included
in the training set.

Toward this aim, we partition the samples based on their
class labels such that 70% of the labels belong to the train-
ing and the remaining labels are used to form the base and
query set. Note that in this scenario the training set is used
to optimize the parameters of the model. Once learning is
completed, the training set is removed and the items of base
set are mapped into compact codes using the trained model.

Finally, the average performance over the query set is re-
ported. We use 80% of the samples with unseen classes
as the training set and the rest as the query set. This pro-
cess is repeated 5 times with random class splits and the
average results is reported. For this setting, during the en-
coding phase, we drop the LC term from loss because the
trained centers do not correspond to any of the labels in the
base set. Similarly, the regression loss term in SQ [42] is
dropped during encoding as it directly depends on the class
labels of training set.

Table 2 demonstrates the results of this experiment
which shows the superiority of DSQ for different lengths
of code. We also observe that the MAP performance of
methods are generally higher than that of the previous pro-
tocol since there is less variation in the base set consisting
of only 3 classes and fewer samples to retrieve from. Also
the rank of techniques is different from the single domain
experiments. For example, SUBIC exhibits the closest per-
formance to DSQ whereas in single-domain setting DTQ is
the closest.

11696

CIFAR-10

NUS-WIDE

ImageNet

0.74

0.72

0.7

0.68

0.66

SDSQ1

SCQ1 [47]

SLSQ1 [29]

SDSQ2

SCQ2 [47]

SLSQ2 [29]

0.7

0.65

0.6

0.55

32

48

64

16

32

48

64

16

32

48

64

0.7

0.6

P
A
M

0.5

16

Figure 3: Mean Average Precision performance of different sparse quantization techniques against three datasets.

Method
CNNH
DPSH

HashNet
SUBIC

DSH
SQ
DQN
DTQ
DSQ

16

0.6241
0.6894
0.7826
0.7832
0.7316
0.7112
0.7562
0.7525
0.7944

32

0.6456
0.7134
0.7941
0.7931
0.7388
0.7126
0.7612
0.7685
0.8165

48

0.6478
0.7198
0.7989
0.8032
0.7437
0.7319
0.7649
0.7700
0.8195

64

0.6491
0.7256
0.8010
0.8077
0.7456
0.7389
0.7655
0.7895
0.8218

Table 2: Mean Average Precision performance of differ-
ent techniques for the task of cross domain performance on
CIFAR-10.

4.3. Ablation Study

We also perform an ablation study to showcase the con-
tribution and importance of loss function components on
the ﬁnal performance of the model by empirically com-
paring different variants of DSQ. We evaluate this exper-
iment across different models to understand the sensitiv-
ity of DSQ to different terms: 1) Lsof tmax + LQ, 2)
Lsof tmax + LQ + LC , 3) Lsof tmax + LQ + LD, and 4)
LC + LD. For each model, the coefﬁcients of different
terms are again tuned using cross validation and the aver-
age performance of model for 64-bit codes against CIFAR-
10 dataset is reported in Figure 4.

The ﬁrst observation is that all of the loss components
contribute in improving MAP. Also, the plot indicates the
importance of softmax loss. This is due to the fact that the
softmax loss is the only term in the objective function that
uses that class labels to force the deep features of differ-
ent classes staying apart, without it, the resulting loss func-
tion degrades all inputs points to be projected onto a single
point. The ﬁgure also demonstrates considerable contribu-
tion of discriminative loss, LD, showing the effectiveness of
our framework in incorporating semantic information dur-

0.8

0.6

0.4

0.2

P
A
M

D

L

+

Q

L

D

L

+

C

L

t o t a l

L

s o f t m

L

Q

L

+

a x

s o f t m

L

+

a x

C

L

+

Q

L

s o f t m

L

+

a x

Figure 4: Difference in MAP, when different loss com-
ponents are excluded from DSQ objective function. The
experiments are conducted on 64-bit codes of CIFAR-10
dataset.

ing quantization.

5. Conclusion

In this paper, we propose a deep supervised quantization
technique for efﬁcient and fast image retrieval. By incor-
porating L2 normalized features, we propose a simple yet
efﬁcient supervised MCQ algorithm for encoding unit nor-
malized data points with similarity preserving binary codes.
We also show that our algorithm can be easily extended to
accommodate sparsity constraint in the codebooks which is
necessary for learning large-scale codebooks. Comprehen-
sive experiments justify that DSQ and its sparse extension
generate compact binary codes that yield state-of-the-art re-
trieval performance on three standard benchmarks, namely
CIFAR-10, NUS-WIDE, and ImageNet.

11697

References

[1] A. Babenko and V. Lempitsky. Additive quantization for ex-
treme vector compression. In CVPR, pages 931–938, 2014.
[2] Y. Cao, B. Liu, M. Long, J. Wang, and M. KLiss. Hashgan:
Deep learning to hash with pair conditional wasserstein gan.
In CVPR, pages 1287–1296, 2018.

[3] Y. Cao, M. Long, B. Liu, and J. Wang. Deep cauchy hashing

for hamming space retrieval. In CVPR), 2018.

[4] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen. Deep quan-
tization network for efﬁcient image retrieval. In AAAI, pages
3457–3463, 2016.

[5] Z. Cao, M. Long, J. Wang, and S. Y. Philip. Hashnet: Deep
learning to hash by continuation. In ICCV, pages 5609–5618,
2017.

[6] Z. Chen, X. Yuan, J. Lu, Q. Tian, and J. Zhou. Deep hashing

via discrepancy minimization. In CVPR, 2018.

[7] Z. Chena, X. Yuana, J. Lua, Q. Tiand, and J. Zhoua. Deep
In CVPR, pages

hashing via discrepancy minimization.
6838–6847, 2018.

[8] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng.
Nus-wide: a real-world web image database from national
university of singapore. In CIVR, page 48, 2009.

[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255, 2009.

[10] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, pages
647–655, 2014.

[11] S. Eghbali, H. Ashtiani, and L. Tahvildari. Online nearest
neighbor search in binary space. In ICDM, pages 853–858,
2017.

[12] S. Eghbali, H. Ashtiani, and L. Tahvildari. Online nearest
neighbor search using hamming weight trees. IEEE Trans.
PAMI, 2019.

[13] S. Eghbali and L. Tahvildari. Fast cosine similarity search in
binary space with angular multi-index hashing. IEEE Trans.
on Knowledge and Data Engineering, 31(2):329–342, 2019.
[14] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quanti-

zation. IEEE Trans. PAMI, 36(4):744–755, 2014.

[15] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.

Itera-
tive quantization: A procrustean approach to learning binary
codes for large-scale image retrieval.
IEEE Trans. PAMI,
35(12):2916–2929, 2013.

[16] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-
center loss for multi-view 3d object retrieval. arXiv preprint
arXiv:1803.06189, 2018.

[17] H. Jain, J. Zepeda, P. Perez, and R. Gribonval. Subic: A su-
pervised, structured binary code for image search. In ICCV,
Oct 2017.

[18] H. Jegou, M. Douze, and C. Schmid. Product quantization
for nearest neighbor search. IEEE Trans. PAMI, 33(1):117–
128, 2011.

[19] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
[20] B. Kulis and T. Darrell. Learning to hash with binary recon-

structive embeddings. In NIPS, pages 1042–1050, 2009.

[21] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse

coding algorithms. In NIPS, pages 801–808, 2007.

[22] Q. Li, Z. Sun, R. He, and T. Tan. Deep supervised discrete

hashing. In NIPS, pages 2482–2491, 2017.

[23] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based
In IJCAI,

deep supervised hashing with pairwise labels.
pages 1711–1717, 2016.

[24] B. Liu, Y. Cao, M. Long, J. Wang, and J. Wang. Deep triplet

quantization. MM, 2018.

[25] H. Liu, R. Wang, S. Shan, and X. Chen. Deep supervised
hashing for fast image retrieval. In CVPR, pages 2064–2072,
2016.

[26] W. Liu, C. Mu, S. Kumar, and S.-F. Chang. Discrete graph

hashing. In NIPS, pages 3419–3427, 2014.

[27] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. Su-
pervised hashing with kernels. In CVPR, pages 2074–2081,
2012.

[28] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary
learning for sparse coding. In ICML, pages 689–696, 2009.

[29] J. Martinez, J. Clement, H. H. Hoos, and J. J. Little. Revisit-

ing additive quantization. In ECCV, pages 137–153, 2016.

[30] J. Martinez, H. H. Hoos, and J. J. Little. Solving multi-
codebook quantization in the gpu. In ECCV, pages 638–650,
2016.

[31] J. Martinez, S. Zakhmi, H. H. Hoos, and J. J. Little. Lsq++:
Lower running time and higher recall in multi-codebook
quantization. In ECCV, pages 491–506, 2018.

[32] M. Norouzi and D. J. Fleet. Cartesian k-means. In CVPR,

pages 3017–3024, 2013.

[33] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov. Hamming
distance metric learning. In NIPS, pages 1061–1069, 2012.

[34] M. Norouzi, A. Punjani, and D. J. Fleet. Fast exact search
IEEE Trans.

in hamming space with multi-index hashing.
PAMI, 36(6):1107–1119, 2014.

[35] R. Ranjan, C. D. Castillo, and R. Chellappa. L2-constrained
arXiv

softmax loss for discriminative face veriﬁcation.
preprint arXiv:1703.09507, 2017.

[36] A. Sablayrolles, M. Douze, N. Usunier, and H. J´egou. How
should we evaluate supervised hashing? In ICASSP, pages
1732–1736, 2017.

[37] F. Shen, C. Shen, W. Liu, and H. Tao Shen. Supervised dis-

crete hashing. In CVPR, pages 37–45, 2015.

[38] E. van den Berg and M. P. Friedlander. Probing the pareto
frontier for basis pursuit solutions. SIAM Journal on Scien-
tiﬁc Computing, 31(2):890–912, 2008.

[39] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li,
and W. Liu. Cosface: Large margin cosine loss for deep face
recognition. In CVPR, 2018.

[40] J. Wang and T. Zhang. Composite quantization. IEEE Trans.

PAMI, 2018.

[41] J. Wang, T. Zhang, N. Sebe, H. T. Shen, et al. A survey on

learning to hash. IEEE Trans. PAMI, 40(4):769–790, 2018.

[42] X. Wang, T. Zhang, G.-J. Qi, J. Tang, and J. Wang. Su-
pervised quantization for similarity search. In CVPR, pages
2018–2026, 2016.

11698

[43] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
pages 499–515, 2016.

[44] X. Wu, R. Guo, A. T. Suresh, S. Kumar, D. N. Holtmann-
Rice, D. Simcha, and F. Yu. Multiscale quantization for fast
similarity search. In NIPS, pages 5745–5755, 2017.

[45] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hash-
ing for image retrieval via image representation learning. In
AAAI, page 2, 2014.

[46] T. Zhang, C. Du, and J. Wang. Composite quantization for
approximate nearest neighbor search. In ICML, pages 838–
846, 2014.

[47] T. Zhang, G.-J. Qi, J. Tang, and J. Wang. Sparse composite

quantization. In CVPR, pages 4548–4556, 2015.

11699

