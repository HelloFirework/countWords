Frame-Consistent Recurrent Video Deraining with Dual-Level Flow

Wenhan Yang1, Jiaying Liu2 ∗, Jiashi Feng1

1 Department of ECE, National University of Singapore, Singapore, 119077

2 Institute of Computer Science and Technology, Peking University, Beijing, 100871

Abstract

In this paper, we address the problem of rain removal
from videos by proposing a more comprehensive framework
that considers the additional degradation factors in real
scenes neglected in previous works. The proposed frame-
work is built upon a two-stage recurrent network with dual-
level ﬂow regularizations to perform the inverse recovery
process of the rain synthesis model for video deraining. The
rain-free frame is estimated from the single rain frame at the
ﬁrst stage. It is then taken as guidance along with previous-
ly recovered clean frames to help obtain a more accurate
clean frame at the second stage. This two-step architecture
is capable of extracting more reliable motion information
from the initially estimated rain-free frame at the ﬁrst stage
for better frame alignment and motion modeling at the sec-
ond stage. Furthermore, to keep the motion consistency be-
tween frames that facilitates a frame-consistent deraining
model at the second stage, a dual-level ﬂow based regular-
ization is proposed at both coarse ﬂow and ﬁne pixel levels.
To better train and evaluate the proposed video deraining
network, a novel rain synthesis model is developed to pro-
duce more visually authentic paired training and evalua-
tion videos. Extensive experiments on a series of synthetic
and real videos verify not only the superiority of the pro-
posed method over state-of-the-art but also the effectiveness
of network design and its each component.

1. Introduction

Rain, as a most common bad weather condition, will
cause the visibility degradation in captured videos, e.g. con-
tent changes and detail loss, which may fail well-built out-
door computer vision systems by default taking clean video
frames as input. For example, rain streaks cause intensi-
ty ﬂuctuation of image content, obstruct the background
to some extent, and blur the scene. Rain will also result

∗Corresponding author. Email: liujiaying@pku.edu.cn. This work was
supported by National Natural Science Foundation of China under contract
No. 61772043, Beijing Natural Science Foundation under contract No.
L182002 and No. 4192025, and CCF-DiDi BigData Joint Lab.

(a) Rain Frame

(b) SpacCNN [7]

(c) FastDeRain [24]

(d) Proposed

Figure 1. Visual results of different deraining methods. Compared
with FastDeRain [24] and SpacCNN [7], our method is better at re-
moving rain accumulation and accumulation ﬂow (blue box) with
less detail loss (red box).

in occlusion, with no background signals through the rain-
drop. Another degradation factor due to rain is accumula-
tion, where distant streaks are overlapped to look like mis-
t or fog, which obscures the background and signiﬁcantly
reduces the visibility of distant scenes. When the amount
of rainfall changes rapidly in a local area, rain accumula-
tion ﬂuctuates and leads to visual degradation like a layer
of ﬂowing veils covering on the rain-free background as
shown in Fig. 2 (c) and the blue box in Fig. 1, which is
called accumulation ﬂow in this work. Compared to normal
accumulation, the behavior of accumulation ﬂow is more
dynamic. Besides static scene transmission, it is also affect-
ed by local rain density and atmospheric ﬂow, which makes
its estimation much challenging. Furthermore, the accumu-
lation ﬂow possesses complex local motion patterns, which
hinders both human perception as well as vision applica-
tions, and increases the difﬁculty to model and handle it.

A lot of research efforts have been dedicated to rain im-
age/video restoration. Some works [25, 21, 42, 36] take
a single rain image as input and separate rain streaks and

11661

rain-free images (non-rain images) based on texture appear-
ances. Frequency domain representation [25], sparse rep-
resentation [36], Gaussian mixture model [31] and deep
networks [53, 14] are adopted as basic models to dif-
ferentiate rain streaks and rain-free images. Besides the
above single image-based approaches, video-based method-
s [1, 2, 3, 8, 12, 16, 18, 19, 58] solve the problem by exploit-
ing both spatial and temporal context. Some [18, 16, 19]
leverage the physical aspects of rain, such as its directional
and chromatic properties. Others [8, 6, 27, 24] further uti-
lize temporal dynamics, including continuity of background
motions, random occurrence of streaks in video frames, and
explicit motion cues, to facilitate video rain removal.

Recently, the rapid development of deep networks also
leads to the blooming of deep learning-based and video im-
age processing, including denoising [57], JPEG artifacts re-
moval [9, 59], interpolation [52], super-resolution [10, 55,
51, 50], video compression [20, 48, 32], and single-image
rain removal [15, 37, 54], etc. Likewise, deep learning also
brings new progress to video rain removal. In [29], a mul-
tiscale convolutional sparse coding is proposed for video
rain streak removal. Chen et al. [7] propose to ﬁrst seg-
ment a rain image into superpixels, on which a consisten-
cy constraint is imposed, and then compensate for the lost
details in the aligned superpixels. In [33, 34], a recurrent
network is built to jointly perform rain degradation classiﬁ-
cation, rain removal and background detail reconstruction.

These previous methods achieve good performance in
some cases. However, to our best knowledge, they all fo-
cus on one or two of the rain degradation factors, and most
of them only consider rain streak removal. Some other rain
degradation factors for vision tasks in videos are rarely con-
sidered, such as rain accumulation ﬂow as shown in Fig. 2
(c) and the blue box in Fig. 1. Furthermore, it is not fully
explored how to utilize intra-frame and inter-frame context
to facilitate joint estimation of multiple rain-related factors.
Besides, in real rain scenes, the motions of objects and re-
gions are interweaved. Their inferences are disturbed by the
multiple degradation factors and their mixture effects. It re-
mains unclear how to describe motion information at both
pixel and region levels and model the motion patterns of the
rain-free frames robustly and accurately.

Based on the above observations, we solve the problem
of video rain removal more comprehensively by consider-
ing rain streak, accumulation, rain accumulation ﬂow and
occlusion. A two-stage recurrent network is designed and
a novel video rain synthesis model is built for synthesizing
visually authentic rain videos with various rain types. The
proposed recurrent network estimates the rain-free frame by
two stages. At the ﬁrst stage, the model recovers a rain-free
frame roughly from a single rain frame. Then, based on this
estimation and preceding recovered clean frames, a more
accurate estimation for the clean frame is inferred at the

second stage by using the temporal context information. We
also place two types of ﬂow-based orthogonal constraints at
the second stage to regularize the model learning at both
pixel and region-levels, which makes our results more tem-
porally continuous and regionally consistent, and facilitates
a frame-consistent method.

In summary, our contributions are as follows.
• We develop a new rain synthesis model

in-
cludes several visual degradation factors, namely rain
streak, accumulation, accumulation ﬂow and occlu-
sion. Based on this model, we synthesize a new rain
video dataset, corresponding to light and heavy rain
condition respectively, to support development and e-
valuation of data-driven video rain removal methods.

that

• We build a recurrent network (RNN) to predict the
rain-related variables in our novel rain synthesis mod-
el, and to estimate the rain-free frame to perform an in-
verse process based on these predicted variables. The
injection of the inverse recovery module makes our
network more effective.

• Our RNN has a two-stage architecture which fully
makes use of the potential of single-frame and multi-
frame context.
It is capable of extracting more reli-
able motion information based on the initially estimat-
ed rain-free frame as guidance for alignment and mo-
tion modeling at the second stage.

• To better capture the motion patterns and keep inter-
frame consistency, we employ two types of ﬂow-based
representations to regularize the learning of our video
deraining network at both coarse ﬂow and ﬁne levels.

2. Related Work

Single image deraining is a highly ill-posed problem.
To address it, many models and priors are used to perfor-
m signal separation and texture classiﬁcation. These mod-
els include sparse coding [25], generalized low rank mod-
el [8], nonlocal mean ﬁlter [26], discriminative sparse cod-
ing [36], Gaussian mixture model [31], rain direction pri-
or [56], transformed low rank model [5]. The presence of
deep learning has promoted the development of single im-
age deraining. In [14, 13], deep networks take the image de-
tail layer as their input. Yang et al. [53] propose a deep join-
t rain detection and removal method to remove heavy rain
streaks and accumulation. In [56], a novel density-aware
multi-stream densely connected CNN is proposed for joint
rain density estimation and removal. Wang et al. [45] de-
velop a perceptual generative adversarial network to apply
the translation from rainy images to clean ones.

Video rain removal can additionally make use of the
temporal context and motion information. Garg and Na-
yar are the ﬁrst to focus on rain modeling [18] and re-
moval [16, 19, 17]. Later works formulate rain streaks

1662

(a)

(b)

(c)

(d)

Figure 2. Different types of visibility degradation due to rain. (a)
Rain streaks. (b) Rain accumulation. (c) Rain accumulation ﬂow.
Due to the atmosphere ﬂow, the density of the veiling layers at the
same location of two frames changes. (d) Rain occlusion. The
occlusion regions present an identical intensity.

with more ﬂexible and intrinsic characteristics, including
temporal and chromatic properties of rain [58, 35], Fouri-
er domain feature [1], phase congruency features [40], the
size, shape and orientation of rain streaks [3, 2], spatio-
temporal correlation of local patches [8], and overall di-
rectional tendency of rain streaks [24]. The presence of
learning-based method, with improved modeling capacity,
brings new progress. Chen et al. [6] propose to embed mo-
tion segmentation by a Gaussian mixture model into rain
detection and removal. Tripathi et al. [43, 44] train a Bayes
rain detector based on spatial and temporal features. In [27],
Kim et al. train an SVM to reﬁne the roughly detected rain
maps. Wei et al. [47] encode rain streaks as patch-based
mixtures of Gaussian, which is capable of ﬁnely adapting a
wider range of rain variations.
In [39], a matrix decom-
position model is used to divide rain streaks into sparse
and dense ones. In [29], a multiscale convolutional sparse
coding method is proposed for video rain streak removal.
Chen at al. [7] propose to ﬁrst segment a rain image in-
to superpixel and then enforce the consistency constraints
and compensate for lost details on these aligned superpix-
els. In [33], a recurrent network is built to seamlessly inte-
grate rain degradation classiﬁcation, rain removal and back-
ground details reconstruction. Comparatively, in our work,
we aim to handle more types of visibility degradation via
our proposed rain synthesis model. To better jointly utilize
intra-frame and inter-frame context, we design a two-step
RNN for video deraining. To better model motion patterns
and keep inter-frame consistency at different granularities,
we propose to apply dual-level ﬂow constraints to regularize
the model learning.

3. Comprehensive Rain Synthesis Model

To address rain removal problem more comprehensive-
ly, we propose a novel comprehensive rain synthesis model
to facilitate the research on this topic. The rain images are
synthesized from clean ones considering four degradation
factors as below and tries to simulate the corresponding ef-
fects.
Rain Streaks. The fast falling raindrops in the focus of
a camera will obstruct the background to some extent, as

(a) Rain-Free Frame

(b) Synthesized Rain Frame

Figure 3. Examples of our synthesis data based on Eqn. (4).

shown in Fig. 2 (a). To synthesize similar effects, they are
combined with clean rain-free frames by linear addition-
s [53, 14, 31].
Rain Accumulation. The rain streaks in the distant are in-
terweaved to produce atmospheric veiling effects [53, 30],
as shown in Fig. 2 (b). In our synthesis model, the back-
ground signals are scattered out to degrade the visibility.
Rain Accumulation Flow. The out-of-focus raindrops (or
ﬂowing rain accumulation) form rain accumulation ﬂow as
proposed in this work, as shown in Fig. 2 (c). Its degree
of transparency is not correlated to the background depth,
and it can take any shape and make semi-transparent veiling
effects. Its presence in the temporal domain is continuous.
It will be added to rain streak contaminated images like a
veil.
Rain Occlusion. In moderate or heavy rain, the light trans-
mittance of raindrops becomes low and the rain region has
identical intensities [33]. In this case, the background infor-
mation is totally lost, as shown in Fig. 2 (d). It is generated
by an alpha matting process based on a binary mask, with
rain-contaminated images and given intensity maps.

We formulate our comprehensive rain synthesis model s-
tarting from the simplest case of a widely used single-frame
rain model [31, 36, 22]:

O = B + S,

(1)

where B is the rain-free frame without rain streaks, and S
is the rain streak frame. O is the captured image with rain
streaks. A video rain synthesis model is obtained with a
temporal indicator t added:

Ot = Bt + St, t = 1, 2, ..., N,

(2)

where t and N denote the current time-step and the total
number of video frames, respectively. Rain streaks St are
assumed to be independent identically distributed random
samples [41]. Their locations across the frames are assumed
uncorrelated. Considering rain accumulation and accumu-
lation ﬂow, Eqn. (2) is further extended as follows:

Ot = βtBt + (1 − βt)At + Ut + St, t = 1, 2, ..., N,

(3)

1663

S
VarL

SL
Rect

 M
tF

ML
Opt-Reg

ML
Opt-Fine

ML
Var

ML
Rect

tv
 S
tv
tB
 ,S F
tB

tu
 tu
tB
1tB
 ,
M F
tB
1

tv
M

tB

SF-DerainNet

Encoder Decoder

MF-AlignNet
 s
tB

M
 1
tB

 ,
M F
tB
1

MF-DerainNet

Encoder Decoder

 tO

…

…

…

…

64

 s
t





32

64

128

 s
tS

 s
tU

1
tA





Inverse Recovery Module

Convolutional Layer (Stride=1)
Convolutional Layer (Stride=2)

…

32

10

FlowNet

 s
tR

Warp

 tu

Warp

…

…

32

 tO

 1tO

F
 1
tO





 s
t v

[         
s
s
   
S U
R
,
t
t

s
t

,

,

s
t

]

…

…

256

64

 M
t v

…

…

…

64

 M
v
t

32

10

[        


S U
t





,

t



t

,

,




R

]

t

[            
M
M


 
S U
R
,
t
t

M
t

,

,

M
t

]

Physical Recovery

Module

 ,M F
tB

ResBlock
Deconvolutional Layer (Stride=2)

Convolutional LSTM

Forward

Skip Connection
Residual Connection

Figure 4. Framework of our two-stage recurrent network for video deraining. The SF-DerainNet takes single-frame rain input and outputs
four rain-related variables.
Inverse recovery module converts these predicted rain-related variables into the estimated rain-free frame
following the inverse recovery of Eqns. (3) and (4). Multi-frame alignment network estimates the optical-ﬂow among frames and aligns
previous frames to the current one. After that, MF-DerainNet takes the multi-frame rain input and their clean estimations to predict the
residual rain-related variables. These variables are further combined with the variables estimated from the single frame to eventually
produce the ﬁnal estimation of the rain-free frame. The whole model is trained end-to-end with the losses for variable prediction, rain-free
frame recovery, inter-frame consistency, and motion accuracy. Best viewed in color.

where At is the global atmospheric light, βt is the at-
mospheric transmission which is correlated with the scene
depth, and Ut is the rain accumulation ﬂow layer which
is decided by the local raindrop density and atmospheric
ﬂow. They are assumed temporally continuous. Given a
ﬁxed scene, {At} and {αt} are only affected by camera
motions. {Ut} has its own motion trajectory. Finally, we
make the rain model capable of describing occlusion by

eOt = (1 − αt) Ot + αtMt,

(4)

where Ot is deﬁned in Eqn. (3), Mt is the rain reliance map
and αt is an alpha matting map.

Based on this comprehensive rain synthesis model (4),
we can simulate realistic-looking rain videos. Two synthe-
sized examples are shown in Fig. 3. In our work, we build
a new video rain dataset with such a synthesis model. More
details are provided in Sec. 5.

4. Frame-Consistent Deraining Network

4.1. Design Methodology

Our framework is built with the following design
methodology. First, our method strictly follows the in-
verse recovery process of Eqns. (3) and (4), which is very
tractable. Second, our method adopts a two-step frame-
work, which fully utilizes the beneﬁts of single-frame and
multi-frame context. The single-frame derained results are
taken as guidance for multi-frame deraining, which fa-
cilitates more accurate motion estimation and alignment.
Third, two complementary constraints, pixel-level ﬁne ﬂow
and region-level regularized ﬂow constraints, are utilized

to regularize the learning of our video deraining network.
They jointly make the motion patterns of our results more
accurate and consistent across frames.

4.2. Network Architecture

Our framework consists of ﬁves modules: single-frame
deraining network (SF-DerainNet), multi-frame align-
ment network (MF-AlignNet), multi-frame deraining net-
work (MF-DerainNet), Inverse Recovery Module, and
Loss Function, as shown in Fig. 4.
It performs multi-
frame rain removal by two steps. First, the SF-DerainNet
is utilized to estimate the rain-related variables in Eqns. (3)
and (4). Then, the inverse recovery module takes the rain-
related variables as its input and estimates the rain-free
frame by calculating the inverse recovery process in Eqn-
s. (3) and (4). Then, based on the estimated rain-free frame
Bs
t−1 at time-step
(t − 1) from MF-DerainNet, we estimate the optical ﬂow at
time-step t. We use this ﬂow to warp Ot−1 and BM
t−1 to get
OF
t−1 by aligning them to Ot and Bt, respec-
tively. After that, MF-DerainNet extracts features from the

t at time-step t from SF-DerainNet, and BM

t−1 i andheOS

t , eOF

t−1i, and then concatenates

and transforms these features into the rain-related variables,
which are further fed into the inverse recovery module to
obtain the ﬁnal estimation BM,F
. Multiples losses are used
to jointly constrain the recovery of BM,F
to predict rain-
free frames accurately and keep the inter-frame consistency.

t

t

t−1 and BM,F
inputhBS

t , BM,F

Single-Frame Deraining Network. As shown in Fig. 4,
SF-DerainNet takes a U-Net-like architecture. It transforms

1664

ˆvS

t = hˆSS
t , ˆβS
t , ˆUS

the features by multiple convolutional layers progressive-
ly. In intermediate layers, the spatial resolutions of features
are ﬁrst down-sampled (encoder) and then up-sampled (de-
coder). There are residual connections (denoted by red) to
connect the features with the same spatial resolution within
the encoder (or decoder), which help local information con-
tained in the features generated by shallow layers reach the
output. There are also skip connections (denoted by blue)
to link the features with the same spatial resolution from en-
coder to decoder. The spatial resolution change is achieved
by stride convolution and deconvolution in SF-DerainNet.
The SF-DerainNet outputs four rain-related variables. We
use GSF(·) to represent the process of SF-DerainNet:

t , ˆUS

t , ˆβS

t , ˆRS

t i = GSF(cid:16) ˆOt(cid:17) ,

(5)

t , and ˆRS

where ˆSS
t are rain streak, rain accumu-
lation ﬂow, atmospheric transmission and residue estimated
from the single frame. The last term tries to remedy the
effects of rain occlusion and the estimation errors of other
terms.
Inverse Recovery Module. Given ˆSS
t , and ˆRS
t ,
we follow the inverse solution of Eqn. (3) to get the estima-
tion of the clean background frame ˆBS
t based on a single
frame rain input:

t , ˆUS

t , ˆβS

ˆBS

t =

ˆOt − ˆUS

t −(cid:16)1 − ˆβS

t (cid:17) × ˆAt

t − ˆSS
max(cid:16) ˆβS

t , ǫ(cid:17)

+ ˆRS
t ,

(6)

where ǫ is a threshold to guarantee the numerical reason-
ability, which is set to 0.1 [38].

Multi-Frame Alignment Network. At time-step t, we esti-
mate the (t − 1)-th rain-free frame ˆBM
t−1 from previous rain
frames. Then, we estimate the optical ﬂow ut = [ut,x, ut,y]
between ˆBM
t by FlowNet [11]. We use GFlow (·)
and GWarp (·) to denote the process of optical ﬂow calcula-
tion and warping operation based on the ﬂow:

t−1 and ˆBS

t (cid:17) ,
ut = GFlow(cid:16) ˆBM
t−1, ˆBS
t−1 = GWarp(cid:16) ˆBM
t−1, ˆut(cid:17) ,
t−1 = GWarp(cid:16) ˆOt−1, ˆut(cid:17) .

ˆOF

ˆBM,F

(7)

(8)

(9)

Then, the locations of the pixels in rain inputs are aligned,
and the successive MF-DerainNet is capable of removing
rain more effectively.

Multi-Frame Deraining Network. The MF-DerainNet

takes as input h ˆBS

t , ˆBM,F

t−1 i and heOS

t , eOF

t−1i and predicts

the rain-related variables. It uses a similar architecture to
SF-DerainNet. Differently, there are two branches at the

encoder side. Features are extracted fromh ˆBS

t−1 i and

t , ˆBM,F

M

B

t
x


M

B

t
y


,



,

M

B
t
t












B

t
x






,

t

B

y


,

t

B

t






ML
Opt-Reg

ML
Opt-Fine

1,M

B B
t



M
t









 tu

tu




B B
t

1,






t

Figure 5. The dual-level ﬂow constraint deduced from Eqn. (17).

region-level regularization. Best viewed in color.

∂x , ∂B(p)

h ∂B(p)
heOS
t , eOF

∂y , ∂B(p)

∂t i and u = (ux, uy) provide pixel and
t−1i, respectively, and concatenated before down-

sampling to the smallest spatial resolution. The skip con-
nections are built to connect the corresponding features
from the branch taking as input the estimated clean frames
to the decoder side. At the end of the convolutional layers
which have the smallest scales at the decoder side, we use
a convolutional LSTM to propagate the information at the
feature level across frames. Here, we utilize a residual task
learning method. We use GMF(·) to express the process of
MF-DerainNet:

t , ˆBM,F

t−1 , eOt, eOF

t−1(cid:17) ,

∆ˆvM

t = GMF(cid:16) ˆBS
t = ∆ˆvM
t + ˆvS
ˆvM
t .
t = hˆSM

t , ˆUM

After getting ˆvM
the inverse recovery module to obtain the rain-free frame
ˆBM
t .

t , ˆβM

, ˆRM

t

t i, we put it into

Loss Function. We train our network in an end-to-end man-
ner. The loss function consists of six terms:

Rect + LS

Var

Lall = LM
+ LM

Rect + LM
Var + LS
Opt-Reg + LM
Opt-Fine,
2

LS

Rect = (cid:13)(cid:13)(cid:13) ˆBS
Var = (cid:13)(cid:13)ˆvS

t − Bt(cid:13)(cid:13)(cid:13)
t − vt(cid:13)(cid:13)2

2

LS

,

2

.

The same applies to LM
Opt-Fine
are the losses measured by coarse ﬂow consistency and ﬁne
ﬂow orthogonal feature.

Opt-Reg and LM

Rect and LM

Var. LM

4.3. Dual Level Flow Constraints

To generate more temporally continuous and regionally
consistent videos, we employ two types of constraints for
network training, ﬁne ﬂow constraint and regularized ﬂow
constraint, which are ﬁrst proposed by this work.
Brightness Constant Constraint. We ﬁrst review the
brightness constant constraint used in the traditional opti-
cal ﬂow:

B(x, y, t) = B(x + ∆x, y + ∆y, t + ∆t),

(16)

where B(x, y, t) denotes the pixel at the location (x, y) of a
frame at time t. For frames t and (t + ∆t), ∆x and ∆y are

1665

(10)

(11)

(12)

(13)

(14)

(15)

Table 1. PSNR and SSIM results among different rain streak removal methods on RainSynLight25, RainSynComplex25, and NTURain.
Best results are denoted in red and the second best results are denoted in blue.

Dataset

RainSynLight25

RainSynHeavy25

NTURain

Metric
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM

DetailNet

TCLRM JORDER MS-CSC

25.72
0.8572
16.50
0.5441
30.13
0.9220

28.77
0.8693
17.31
0.4956
29.98
0.9199

30.37
0.9235
20.20
0.6335
32.61
0.9482

25.58
0.8089
16.96
0.5049
27.31
0.7870

DSC
25.63
0.8328
17.33
0.5036
29.20
0.9137

SE

FastDerain

26.56
0.8006
16.76
0.5293
25.73
0.7614

29.42
0.8683
19.25
0.5385
30.32
0.9262

J4RNet
32.96
0.9434
24.13
0.7163
32.14
0.9480

SpacCNN

Proposed

32.78
0.9239
21.21
0.5854
33.11
0.9474

35.80
0.9622
27.72
0.8239
36.05
0.9676

Table 2. PSNR and SSIM results among different rain removal methods on RainSynAll100. † and ‡ denote using ST-MRF and EVD-Net
as pre/post-processing, respectively. Best results are denoted in red and the second best results are denoted in blue.

Metric
PSNR
SSIM
Metric
PSNR
SSIM

†FastDeRain

FastDeRain†

SpacCNN†F

†SpacCNN

†MS-CSC MS-CSC†

19.46
0.6875

19.40
0.7322

18.39
0.7131

19.16
0.7214

17.81
0.6264

17.82
0.6211

‡FastDeRain

FastDeRain‡

SpacCNN‡F

‡SpacCNN

‡MS-CSC MS-CSC‡

18.55
0.7161

18.78
0.7351

17.93
0.7259

17.94
0.7270

16.92
0.6354

16.92
0.6346

†SE
17.41
0.6213
‡SE
17.67
0.6200

SE†
17.58
0.6245
SE‡
17.89
0.6278

J4RNet-E

Rain Input

20.31
0.6324

12.01
0.5739

J4RNet-P

Proposed

22.93
0.7746

25.72
0.8989

the spatial pixel displacements along x and y axes, respec-
tively. We can approximate Eqn. (16) with a Taylor series:

∂B(p)

∂x

ux +

∂B(p)

∂y

uy +

∂B(p)

∂t

= 0,

(17)

where p = (x, y, t) and u = (ux, uy) denote the two
dimensional velocity of point p, respectively.
and
∂B(p)
are the spatial gradients of ∂B(p) along x and y ax-

∂B(p)

∂x

∂y

es respectively. ∂B(p)
is the temporal gradient along the
time axis. From Eqn. (17), we get two types of ﬂow-related

∂t

representations: h ∂B(p)

∂x , ∂B(p)

∂y , ∂B(p)

∂t i and u = (ux, uy),

helping our network capture motion clues and keep tempo-
ral consistency at pixel and region-levels jointly.

Fine Flow Constraint. h ∂B(p)

∂x , ∂B(p)

∂y , ∂B(p)

∂t i contains the

full motion information among frames. It is a pixel-level
feature that is directly calculated from the input frames. The
feature may not be robust enough, e.g. disturbed by rain
streaks and small occlusion, and may fail to capture mo-
tion patterns at the region or object level. With these con-
siderations, following the process of extracting the feature
GOpt-Fine(·), we build a ﬁne ﬂow constraint to guide the net-
work to recover the accurate pixel motions:

LM

Opt-Fine = (cid:13)(cid:13)(cid:13)GOpt-Fine( ˆBM

t−1, ˆBM

t ) − GOpt-Fine (Bt−1, Bt)(cid:13)(cid:13)(cid:13)

Regularized Flow Constraint. The optical ﬂow u is a
more abstract feature. It cannot be directly calculated from
the inputs and needs to be solved with other imposed con-
straints. After estimated and reﬁned with priors, the optical
ﬂow is consistent within regions and objects, while some
motion details are smoothed. Hence, it is not a pixel-level
feature but can be used as an effective description for region
and object motions. In our work, we use optical ﬂow to reg-
ularize feature learning, making the motion patterns of the

estimated results more region-consistent:

LM

Opt-Reg = (cid:13)(cid:13)(cid:13)GFlow(cid:16) ˆBM

t−1, ˆBM

t (cid:17) − GFlow (Bt−1, Bt)(cid:13)(cid:13)(cid:13)

2

2

.

With ﬁne ﬂow constraint and regularized ﬂow constraint,
our network is capable of capturing motion trajectories and
patterns at both pixel-level and region-level.

5. Experimental Results

Datasets. We compare our model with state-of-the-arts
on ﬁve benchmark datasets. RainSynLight25 and RainSyn-
Complex25 are proposed in [33] with light and heavy rain
streaks respectively. NTURain [7] has two sub-groups: one
taken from a panning and unstable camera with slow move-
ments, and the other from a fast moving car-mount camera.
RainSynAll100 is synthesized by 1,000 non-rain sequences
with all degradation factors illustrated in Sec. 3. The non-
rain sequences are sampled from Vimeo-90K Dataset [49].
The whole dataset is split into training and testing dataset-
s, including 900 and 100 video sequences, respectively.
Practical rain video sequences are collected from practical
scenes from Youtube website1, GIPHY2 and movie clips.
More information about training data, implementation de-
tails, and video comparison results are provided in the on-
line supplementary material3.
Baselines. We compare D3R-Net with state-of-the-
art methods: discriminative sparse coding (DSC) [36],
[31],
rain detection and re-
layer priors (LP)
[53], deep detail network (Detail-
moval
(JORDER)
Net) [14], stochastic encoding (SE) [47], temporal correla-
tion and low-rank matrix completion (TCLRM) [27], Fast-
DeRain [24], joint recurrent rain removal and reconstruc-
tion (J4RNet) [33], superpixel alignment and compensation
CNN (SpacCNN) [7]. DSC, LP, JORDER and DetailNet

joint

1https://www.youtube.com/
2https://giphy.com/
3https://github.com/ﬂyywh/Dual-FLow-Video-Deraining-CVPR-2019

2

2

.

1666

(a) Rain Frame

(b) SE

(c) TCLRM

(d) FastDeRain

(e) J4R

(f) SpacCNN

(g) Proposed

Figure 6. Results of rain removal methods on synthesized datasets. Top panel: the results on RainSynComplex25. Bottom panel: the results
on RainSynAll100. Best viewed in color.

(a) Input

(b) SE

(c) MS-CSC

(d) TCLRM

(e) FastDeRain

(f) SpacCNN

(g) Proposed

Figure 7. Results of rain removal methods on real images. Best viewed in color.

are single frame deraining methods. SE, TCLRM, Fast-
Derain, J4RNet, and SpacCNN are multi-frame derainig
methods. JORDER, DetailNet, J4RNet and SpacCNN are
deep-learning based methods. When performing evalua-
tions on RainSynAll100, for the methods without rain ac-
cumulation removal, end-to-end united video dehazing and
detection network (EVD-Net) [28] and spatio-temporal M-
RF dehazing [4] are used to perform pre-processing or post-
processing. For the experiments on synthesized data, Peak
Signal-to-Noise Ratio (PSNR) [23] and Structure Similarity
Index (SSIM) [46] are used as comparison criteria. Follow-
ing previous works, we evaluate the results only in the lumi-
nance channel, since human visual system is more sensitive
to luminance than chrominance information.

Objective Evaluation. We compare our method on dataset-
s with only rain streak degradation in Table 1. Our method
shows signiﬁcant superiority to previous methods. Com-
pared to J4RNet and SpacCNN, our method achieves more

than 2.8 dB on 3.5 dB gains on RainSynLight25 and Rain-
SynHeavy25. We also evaluate all methods on our synthe-
sized rain dataset RainSynAll100. For a fair comparison,
FastDerain, SpacCNN and MS-CSC are evaluated with two
state-of-the-art dehazing methods, ST-MRF [4] and EVD-
Net [28], as pre/post-processing to remove rain accumu-
lation. In Table 2, †FastDerain denotes using the sequen-
tial combination of ST-MRF and FastDeRain for rain re-
moval. FastDerain† utilizes ST-MRF as post-processing.
‡FastDerain and FastDerain‡ utilize EVD-Net as pre/post-
processing, respectively. The same applies to other com-
pared methods. J4RNet-E applies the method in [33] to di-
rectly predict the rain-free frame based on the input rain
frame. J4RNet-P incorporates the inverse recovery module
to ﬁrst predict the rain-related variables and then infer the
rain-free frame based on these predicted variables. As is
observed in Table 2, our method achieves the best perfor-
mance. The performance gain is up to 2.5dB in PSNR and

1667

Table 3. Running time of different methods (in sec) to remove rain
in a video with the spatial resolution 832 × 512 per frame.

Table 4. Ablation analysis for network architecture. Best results
are denoted in red and the second best results are denoted in blue.

Baseline

DetailNet

JORDER

SE

FastDeRain

Time

Baseline

Time

1.4698

0.6329

SpacCNN MS-CSC
15.7957

9.5075

19.8516
J4RNet-E

0.8401

0.3962

J4RNet-P

0.8414

TCLRM
192.7007
Proposed

0.8974

0.12 in SSIM.
Subjective Evaluation We also compare visual results of
different deraining methods in Figs. 6 and 7. From the top
panel of Fig. 6, the results on synthesized data show our sig-
niﬁcant superiority in rain streak removal and detail preser-
vation. The bottom panel of Fig. 6 demonstrates our advan-
tages in removing rain streak, accumulation, accumulation
ﬂow, and even their combinations. Our method also per-
forms better than other methods on real images (Fig. 7). The
results in the top panel show that our method successful-
ly removes all sharp streaks, weak streaks and veil streaks,
while the results of other methods still display residual rain
to some extent. The results in the bottom panel demonstrate
our superiority of removing rain streak, rain accumulation
and accumulation ﬂow. Our method removes all streaks,
generates the sharp boundaries between trees and sky, and
better keeps regionally consistent.
Running Time. Table 3 compares the running time of sev-
eral state-of-the-art methods. J4RNet-E, J4RNet-P and the
proposed methods are implemented in Python and based
on Pytorch4. Other methods are implemented in MAT-
LAB. DetailNet and SpacCNN are based on MatConvNet5.
JORDER is implemented on the Caffes Matlab wrapper6.
TCLRM is a CPU-based method and others are GPU-based
approaches. The video resolution for testing is 832 × 512.
In general, the running time of our method is comparable to
other state-of-the-art methods.
Ablation Study of Network Architecture We evaluate our
methods with different components in Table 4 and Fig. 8.
It is clearly observed from Table 4 that SF-DerainNet and
LSTM memory among frames signiﬁcantly improve the ob-
jective results (v1 vs. v4 and v2 vs. v4). The inverse recov-
ery module beneﬁts rain streak removal, which leads to a
higher SSIM (v3 vs. v4). MF-Alignment further improves
PSNR and SSIM (v4 vs. v5). From the visual results in
Fig. 8, the absence of SF-DerainNet (v1) leads to obvious
detail loss. Though v4 may achieve very high PSNR in Ta-
ble 4 without the inverse recovery module, it fails to remove
rain streak and capture the intensity distribution. Compara-
tively, our full version achieves the best visual quality.
Visual Results with and without Dual-Level Flow Con-
straints We also compare the visual results generated by
the models with and without dual ﬂow constraints in Fig. 9,
which are used to facilitate generating more temporally con-
sistent and visually authentic results. Two very hard cases

4https://pytorch.org/
5http://www.vlfeat.org/matconvnet/
6http://caffe.berkeleyvision.org/

Baseline

SF-DerainNet

Inverse Recovery
LSTM Memory
MF-Alignment

v1
×

X

X

×

v2
X

X

×

×

v3
X

×

X

×

v4
X

X

X

×

v5
X

X

X

X

PSNR
SSIM

21.70
0.8120

22.87
0.8183

26.11
0.8683

25.89
0.9043

26.10
0.9125

Figure 8. Example results of different methods on RainSynAll100.
Crop results from left to right: ground truth, v1, v2, v3, v4, and v5.

Figure 9. Two examples of successive frame results generated by
our methods with and without dual ﬂow constraints on RainSynAl-
l100. (a) Input frames. (b) Results without dual ﬂow constraints.
(c) Results with dual ﬂow constraints.

with large sky regions, which are easily regarded as the ac-
cumulation or accumulation ﬂow are investigated. From the
results, it is clearly observed that, without dual ﬂow con-
straints, our results include less artifacts, and are more re-
gionally consistent and temporally smooth between frames.

6. Conclusion

In this paper, we address the problem of rain removal
from videos in a more comprehensive way. The degradation
factors of rain streak, accumulation, accumulation ﬂow and
occlusion, are considered and a two-stage recurrent network
with dual ﬂow constraints is constructed. This two-stage re-
current network extracts more reliable motion information
progressively. Rain-related variables (e.g. rain streak, at-
mospheric transmission) are estimated to infer the rain-free
frame by the inverse process of the rain synthesis model.
Furthermore, two types of ﬂow-based orthogonal represen-
tations are proposed to make the network better capture mo-
tion patterns and keep inter-frame consistency. Extensive
experiments have veriﬁed the superiority of our method and
the effectiveness of each component.

1668

References

[1] P. C. Barnum, S. Narasimhan, and T. Kanade. Analysis of
rain and snow in frequency space. Int’l Journal of Computer
Vision, 86(2-3):256–274, 2010. 2, 3

[2] J. Bossu, N. Hauti`ere, and J.-P. Tarel. Rain or snow detec-
tion in image sequences through use of a histogram of orien-
tation of streaks. International journal of computer vision,
93(3):348–367, 2011. 2, 3

[3] N. Brewer and N. Liu. Using the shape characteristics of
rain to identify and remove rain from video. In Joint IAPR
International Workshops on SPR and SSPR, pages 451–458,
2008. 2, 3

[4] B. Cai, X. Xu, and D. Tao. Real-time video dehazing based
on spatio-temporal mrf. In Paciﬁc Rim Conference on Mul-
timedia, pages 315–325. Springer, 2016. 7

[5] Y. Chang, L. Yan, and S. Zhong. Transformed low-rank mod-
el for line pattern noise removal. In Proc. IEEE Int’l Con-
f. Computer Vision, Oct 2017. 2

[6] J. Chen and L. P. Chau. A rain pixel recovery algorithm for
videos with highly dynamic scenes. IEEE Trans. on Image
Processing, 23(3):1097–1104, March 2014. 2, 3

[7] J. Chen, C.-H. Tan, J. Hou, L.-P. Chau, and H. Li. Robust
video content alignment and compensation for rain removal
in a cnn framework.
In Proc. IEEE Int’l Conf. Computer
Vision and Pattern Recognition, June 2018. 1, 2, 3, 6

[8] Y.-L. Chen and C.-T. Hsu. A generalized low-rank appear-
ance model for spatio-temporally correlated rain streaks. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1968–1975, 2013. 2, 3

[9] C. Dong, Y. Deng, C. C. Loy, and X. Tang. Compres-
sion artifacts reduction by a deep convolutional network.
In Proc. IEEE Int’l Conf. Computer Vision, pages 576–584,
Dec 2015. 2

[10] C. Dong, C. C. Loy, K. He, and X. Tang.

Image super-
resolution using deep convolutional networks. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 38(2):295–
307, Feb 2016. 2

[11] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazırbas¸,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks.
In
Proc. IEEE Int’l Conf. Computer Vision, 2015. 5

[12] D. Eigen, D. Krishnan, and R. Fergus. Restoring an im-
age taken through a window covered with dirt or rain.
In
Proc. IEEE Int’l Conf. Computer Vision, December 2013. 2

[13] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley. Clearing
the skies: A deep network architecture for single-image rain
removal.
IEEE Trans. on Image Processing, 26(6):2944–
2956, June 2017. 2

[14] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Pais-
ley. Removing rain from single images via a deep detail net-
work. In Proc. IEEE Int’l Conf. Computer Vision and Pattern
Recognition, July 2017. 2, 3, 6

[15] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Pais-
ley. Removing rain from single images via a deep detail net-
work. In Proc. IEEE Int’l Conf. Computer Vision and Pattern
Recognition, pages 1715–1723, July 2017. 2

[16] K. Garg and S. K. Nayar. Detection and removal of rain
from videos. In Proc. IEEE Int’l Conf. Computer Vision and
Pattern Recognition, volume 1, pages I–528, 2004. 2

[17] K. Garg and S. K. Nayar. When does a camera see rain?
In Proc. IEEE Int’l Conf. Computer Vision, volume 2, pages
1067–1074, 2005. 2

[18] K. Garg and S. K. Nayar. Photorealistic rendering of rain
In ACM Trans. Graphics, volume 25, pages 996–

streaks.
1002, 2006. 2

[19] K. Garg and S. K. Nayar. Vision and rain. Int’l Journal of

Computer Vision, 75(1):3–27, 2007. 2

[20] Y. Hu, W. Yang, S. Xia, W. Cheng, and J. Liu. Enhanced intra
prediction with recurrent neural network in video coding. In
Proc. Data Compression Conference, pages 413–413, March
2018. 2

[21] D.-A. Huang, L.-W. Kang, Y.-C. F. Wang, and C.-W. Lin.
Self-learning based image decomposition with applications
to single image denoising. IEEE Transactions on multimedi-
a, 16(1):83–93, 2014. 1

[22] D.-A. Huang, L.-W. Kang, M.-C. Yang, C.-W. Lin, and Y.-
C. F. Wang. Context-aware single image rain removal. In
Proc. IEEE Int’l Conf. Multimedia and Expo, pages 164–
169, 2012. 3

[23] Q. Huynh-Thu and M. Ghanbari. Scope of validity of p-
snr in image/video quality assessment. Electronics letters,
44(13):800–801, 2008. 7

[24] T.-X. Jiang, T.-Z. Huang, X.-L. Zhao, L.-J. Deng, and
Y. Wang. A novel tensor-based video rain streaks removal
approach via utilizing discriminatively intrinsic priors.
In
Proc. IEEE Int’l Conf. Computer Vision and Pattern Recog-
nition, July 2017. 1, 2, 3, 6

[25] L. W. Kang, C. W. Lin, and Y. H. Fu. Automatic single-
image-based rain streaks removal via image decomposition.
IEEE Trans. on Image Processing, 21(4):1742–1755, April
2012. 1, 2

[26] J. H. Kim, C. Lee, J. Y. Sim, and C. S. Kim. Single-
image deraining using an adaptive nonlocal means ﬁlter. In
Proc. IEEE Int’l Conf. Image Processing, pages 914–917,
Sept 2013. 2

[27] J. H. Kim, J. Y. Sim, and C. S. Kim. Video deraining and
desnowing using temporal correlation and low-rank matrix
completion. IEEE Trans. on Image Processing, 24(9):2658–
2670, Sept 2015. 2, 3, 6

[28] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng. End-to-end
united video dehazing and detectionc. In Proc. AAAI Conf.
on Artiﬁcial Intelligence, Feb. 2018. 7

[29] M. Li, Q. Xie, Q. Zhao, W. Wei, S. Gu, J. Tao, and D. Meng.
Video rain streak removal by multiscale convolutional sparse
coding. In Proc. IEEE Int’l Conf. Computer Vision and Pat-
tern Recognition, June 2018. 2, 3

[30] R. Li, L.-F. Cheong, and R. T. Tan. Single Image Deraining
using Scale-Aware Multi-Stage Recurrent Network. ArXiv
e-prints, December 2017. 3

[31] Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown. Rain
streak removal using layer priors. In Proc. IEEE Int’l Con-
f. Computer Vision and Pattern Recognition, pages 2736–
2744, 2016. 2, 3, 6

1669

[48] S. Xia, W. Yang, Y. Hu, S. Ma, and J. Liu. A group variation-
al transformation neural network for fractional interpolation
of video coding.
In Proc. Data Compression Conference,
pages 127–136, March 2018. 2

[49] T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman. Video
Enhancement with Task-Oriented Flow. ArXiv e-prints,
November 2017. 6

[50] W. Yang, J. Feng, G. Xie, J. Liu, Z. Guo, and S. Yan. Video
super-resolution based on spatial-temporal recurrent residu-
al networks. Computer Vision and Image Understanding,
168:79 – 92, 2018. Special Issue on Vision and Computa-
tional Photography and Graphics. 2

[51] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, and
S. Yan. Deep edge guided recurrent residual learning for
image super-resolution. IEEE Trans. on Image Processing,
26(12):5895–5907, Dec 2017. 2

[52] W. Yang, J. Liu, S. Xia, and Z. Guo. Variation learning guid-
ed convolutional network for image interpolation. In 2017
IEEE International Conference on Image Processing (ICIP),
pages 1652–1656, Sept 2017. 2

[53] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan.
Deep joint rain detection and removal from a single image. In
Proc. IEEE Int’l Conf. Computer Vision and Pattern Recog-
nition, July 2017. 2, 3, 6

[54] W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, and Z. Guo. Joint
rain detection and removal from a single image with contex-
tualized deep networks. IEEE Trans. on Pattern Analysis and
Machine Intelligence, pages 1–1, 2019. 2

[55] W. Yang, S. Xia, J. Liu, and Z. Guo. Reference guided deep
super-resolution via manifold localized external compensa-
tion. IEEE Trans. on Circuits and Systems for Video Tech-
nology, pages 1–1, 2018. 2

[56] H. Zhang and V. M. Patel. Density-aware single image de-
raining using a multi-stream dense network. In Proc. IEEE
Int’l Conf. Computer Vision and Pattern Recognition, June
2018. 2

[57] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-
yond a gaussian denoiser: Residual learning of deep cnn for
image denoising. IEEE Transactions on Image Processing,
26(7):3142–3155, July 2017. 2

[58] X. Zhang, H. Li, Y. Qi, W. K. Leow, and T. K. Ng. Rain re-
moval in video by combining temporal and chromatic prop-
erties. In Proc. IEEE Int’l Conf. Multimedia and Expo, pages
461–464, 2006. 2, 3

[59] X. Zhang, W. Yang, Y. Hu, and J. Liu. DMCNN: Dual-
domain multi-scale convolutional neural network for com-
pression artifacts removal. In Proc. IEEE Int’l Conf. Image
Processing, pages 390–394, Oct 2018. 2

[32] J. Liu, S. Xia, W. Yang, M. Li, and D. Liu. One-for-
all: Grouped variation network-based fractional interpola-
tion in video coding.
IEEE Trans. on Image Processing,
28(5):2140–2151, May 2019. 2

[33] J. Liu, W. Yang, S. Yang, and Z. Guo. Erase or ﬁll? deep
joint recurrent rain removal and reconstruction in videos. In
Proc. IEEE Int’l Conf. Computer Vision and Pattern Recog-
nition, June 2018. 2, 3, 6, 7

[34] J. Liu, W. Yang, S. Yang, and Z. Guo. D3r-net: Dynam-
ic routing residue recurrent network for video rain removal.
IEEE Trans. on Image Processing, 28(2):699–712, Feb 2019.
2

[35] P. Liu, J. Xu, J. Liu, and X. Tang. Pixel based temporal anal-
ysis using chromatic property for removing rain from videos.
In Computer and Information Science, 2009. 3

[36] Y. Luo, Y. Xu, and H. Ji. Removing rain from a single image
via discriminative sparse coding. In Proc. IEEE Int’l Con-
f. Computer Vision, pages 3397–3405, 2015. 1, 2, 3, 6

[37] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu. Attentive
generative adversarial network for raindrop removal from a
single image. In Proc. IEEE Int’l Conf. Computer Vision and
Pattern Recognition, pages 2482–2491, June 2018. 2

[38] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang.
Single image dehazing via multi-scale convolutional neural
networks. In Proc. IEEE European Conf. Computer Vision,
2016. 5

[39] W. Ren, J. Tian, Z. Han, A. Chan, and Y. Tang. Video
desnowing and deraining based on matrix decomposition. In
Proc. IEEE Int’l Conf. Computer Vision and Pattern Recog-
nition, July 2017. 3

[40] V. Santhaseelan and V. K. Asari. Utilizing local phase infor-
mation to remove rain from video. Int’l Journal of Computer
Vision, 112(1):71–89, March 2015. 3

[41] S. Starik and M. Werman. Simulation of rain in videos. In

Texture Workshop, ICCV, June 2003. 3

[42] S.-H. Sun, S.-P. Fan, and Y.-C. F. Wang. Exploiting im-
age structural similarity for single image rain removal.
In
Proc. IEEE Int’l Conf. Image Processing, pages 4482–4486,
2014. 1

[43] A. K. Tripathi and S. Mukhopadhyay. A probabilistic ap-
proach for detection and removal of rain from videos. IETE
Journal of Research, 57(1):82–91, 2011. 3

[44] A. K. Tripathi and S. Mukhopadhyay. Video post processing:
low-latency spatiotemporal approach for detection and re-
moval of rain. IET Image Processing, 6(2):181–196, March
2012. 3

[45] C. Wang, C. Xu, C. Wang, and D. Tao. Perceptual adversarial
IEEE Trans.

networks for image-to-image transformation.
on Image Processing, 27(8):4066–4079, Aug 2018. 2

[46] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity.
IEEE Trans. on Image Processing, 13(4):600–
612, 2004. 7

[47] W. Wei, L. Yi, Q. Xie, Q. Zhao, D. Meng, and Z. Xu. Should
we encode rain streaks in video as deterministic or stochas-
tic? In Proc. IEEE Int’l Conf. Computer Vision, Oct 2017.
3, 6

1670

