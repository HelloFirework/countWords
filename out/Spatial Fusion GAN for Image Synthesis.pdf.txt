Spatial Fusion GAN for Image Synthesis

Fangneng Zhan

Hongyuan Zhu

Nanyang Technological University

Institute for Infocomm, A*STAR, Singapore

50 Nanyang Avenue, Singapore 639798

1 Fusionopolis Way, Singapore 138632

fnzhan@ntu.edu.sg

zhuh@i2r.a-star.edu.sg

Shijian Lu

Nanyang Technological University

50 Nanyang Avenue, Singapore 639798

shijian.lu@ntu.edu.sg

Abstract

Recent advances in generative adversarial networks
(GANs) have shown great potentials in realistic image syn-
thesis whereas most existing works address synthesis real-
ism in either appearance space or geometry space but few
in both. This paper presents an innovative Spatial Fusion
GAN (SF-GAN) that combines a geometry synthesizer and
an appearance synthesizer to achieve synthesis realism in
both geometry and appearance spaces. The geometry syn-
thesizer learns contextual geometries of background images
and transforms and places foreground objects into the back-
ground images unanimously. The appearance synthesizer
adjusts the color, brightness and styles of the foreground
objects and embeds them into background images harmo-
niously, where a guided ﬁlter is introduced for detail pre-
serving. The two synthesizers are inter-connected as mu-
tual references which can be trained end-to-end without su-
pervision. The SF-GAN has been evaluated in two tasks:
(1) realistic scene text image synthesis for training better
recognition models; (2) glass and hat wearing for realis-
tic matching glasses and hats with real portraits. Qualita-
tive and quantitative comparisons with the state-of-the-art
demonstrate the superiority of the proposed SF-GAN.

1. Introduction

With the advances of deep neural networks (DNNs), im-
age synthesis has been attracting increasing attention as
a means of generating novel images and creating anno-
tated images for training DNN models, where the latter has
great potentials to replace the traditional manual annota-
tion which is usually costly, time-consuming and unscal-
able. The fast development of generative adversarial net-
works (GANs) [9] in recent years opens a new door of au-

Figure 1. The proposed SF-GAN is capable of synthesizing realis-
tic images concurrently in geometric and appearance spaces. Rows
1 and 2 show a few synthesized scene text images and row 3 shows
a few hat-wearing and glass-wearing images where the foreground
texts, glasses and hats as highlighted by red-color boxes are com-
posed with the background scene and face images harmoniously.

tomated image synthesis as GANs are capable of generat-
ing realistic images by concurrently implementing a gener-
ator and discriminator. Three typical approaches have been
explored for GAN-based image synthesis, namely, direct
image generation [27, 33, 1], image-to-image translation
[55, 16, 22, 14] and image composition [21, 2].

On the other hand, most existing GANs were designed
to achieve synthesis realism either from geometry space or
appearance space but few in both. Consequentially, most
GAN-synthesized images have little contribution (many
even harmful) when they are used in training deep network
models.
In particular, direct image generation still faces
difﬁculties in generating high-resolution images due to the
limited network capacity. GAN-based image composition
is capable of producing high-resolution images [21, 2] by
placing foreground objects into background images. But
most GAN-based image composition techniques focus on
geometric realism only (e.g. object alignment with con-

13653

textual background) which often produce various artifacts
due to appearance conﬂicts between the foreground objects
and the background images. As a comparison, GAN-based
image-to-image translation aims for appearance realism by
learning the style of images of the target domain whereas
the geometric realism is largely ignored.

We propose an innovative Spatial Fusion GAN (SF-
GAN) that achieves synthesis realism in both geometry and
appearance spaces concurrently, a very challenging task in
image synthesis due to a wide spectrum of conﬂicts be-
tween the foreground objects and background images with
respect to relative scaling, spatial alignment, appearance
style, etc. The SF-GAN address these challenges by de-
signing a geometry synthesizer and an appearance synthe-
sizer. The geometry synthesizer learns the local geome-
try of background images with which the foreground ob-
jects can be transformed and placed into the background
images unanimously. A discriminator is employed to train
a spatial transformation network, targeting to produce trans-
formed images that can mislead the discriminator. The ap-
pearance synthesizer learns to adjust the color, brightness
and styles of the foreground objects for proper matching
with the background images with minimum conﬂicts. A
guided ﬁlter is introduced to compensate the detail loss that
happens in most appearance-transfer GANs. The geometry
synthesizer and appearance synthesizer are inter-connected
as mutual references which can be trained end-to-end with
little supervision.

The contributions of this work are threefold. First, it de-
signs an innovative SF-GAN, an end-to-end trainable net-
work that concurrently achieves synthesis realism in both
geometry and appearance spaces. To the best of our knowl-
edge, this is the ﬁrst GAN that can achieve synthesis realism
in geometry and appearance spaces concurrently. Second,
it designs a fusion network that introduces guided ﬁlters
for detail preserving for appearance realism, whereas most
image-to-image-translation GANs tend to lose details while
performing appearance transfer. Third, it investigates and
demonstrates the effectiveness of GAN-synthesized images
in training deep recognition models, a very important issue
that was largely neglected in most existing GANs (except a
few GANs for domain adaptation [14, 16, 22, 55]).

2. Related Work

2.1. Image Synthesis

Realistic image synthesis has been studied for years,
from synthesis of single objects [29, 30, 40] to generation of
full-scene images [8, 34]. Among different image synthesis
approaches, image composition has been explored exten-
sively which synthesizes new images by placing foreground
objects into some existing background image. The target
is to achieve composition realism by controlling the object

size, orientation, and blending between foreground objects
and background images. For example, [10, 17, 50, 51] in-
vestigate synthesis of scene text images for training bet-
ter scene text detection [47] and recognition models [49].
They achieve the synthesis realism by controlling a series
of parameters such as text locations within the background
image, geometric transformation of the foreground texts,
blending between the foreground text and background im-
age, etc. Other image composition systems have also been
reported for DNN training [7], composition harmonization
[26, 42], image inpainting [54], etc.

Optimal image blending is critical for good appearance
consistency between the foreground object and background
image as well as minimal visual artifacts within the synthe-
sized images. One straightforward way is to apply dense
image matching at pixel level so that only the correspond-
ing pixels are copied and pasted, but this approach does not
work well when the foreground object and background im-
age have very different appearance. An alternative way is
to make the transition as smooth as possible so that artifacts
can be hidden/removed within the composed images, e.g.
alpha blending [43], but this approach tends to blur ﬁne de-
tails in the foreground object and background images. In ad-
dition, gradient-based techniques such as Poisson blending
[31] can edit the image gradient and adjust the inconsistency
in color and illumination to achieve seamlessly blending.

Most existing image synthesis techniques aim for geo-
metric realism by hand-crafted transformations that involve
complicated parameters and are prone to various unnatural
alignments. The appearance realism is handled by different
blending techniques where features are manually selected
and still susceptible to artifacts. Our proposed technique
instead adopts a GAN structure that learn geometry and ap-
pearance features from real images with little supervision,
minimizing various inconsistency and artifacts effectively.

2.2. GAN

GANs [9] have achieved great success in generating re-
alistic new images from either existing images or random
noises. The main idea is to have a continuing adversarial
learning between a generator and a discriminator, where the
generator tries to generate more realistic images while the
discriminator aims to distinguish the newly generated im-
ages from real images. Starting from generating MNIST
handwritten digits, the quality of GAN-synthesized images
has been improved greatly by the laplacian pyramid of ad-
versarial networks [6]. This is followed by various efforts
that employ a DNN architecture [33], stacking a pair of gen-
erators [52], learning more interpretable latent representa-
tions [4], adopting an alternative training method [1], etc.

Most existing GANs work towards synthesis realism in
the appearance space. For example, CycleGAN [55] uses
cycle-consistent adversarial networks for realistic image-

23654

to-image translation, and so other relevant GANs [16, 37].
LR-GAN [48] generates new images by applying additional
spatial transformation networks (STNs) to factorize shape
variations. GP-GAN [46] composes high-resolution images
by using Poisson blending [31]. A few GANs have been
reported in recent years for geometric realsim, e.g., [21]
presents a spatial transformer GAN (ST-GAN) by embed-
ding STNs in the generator for geometric realism, [2] de-
signs Compositional GAN that employs a self-consistent
composition-decomposition network.

Most existing GANs synthesize images in either geome-
try space (e.g. ST-GAN) or appearance space (e.g. Cycle-
GAN) but few in both spaces.
In addition, the GAN-
synthesized images are usually not suitable for training deep
network models due to the lack of annotation or synthesis
realism. Our proposed SF-GAN can achieve both appear-
ance and geometry realism by synthesizing images in ap-
pearance and geometry spaces concurrently. Its synthesized
images can be directly used to train more powerful deep
network models due to their high realism.

2.3. Guided Filter

Guided Filters [12, 13] use one image as guidance for
ﬁltering another image which has shown superior perfor-
mance in detail-preserving ﬁltering. The ﬁltering output is
a linear transform of the guidance image by considering its
structures, where the guidance image can be the input image
itself or another different image. Guided ﬁltering has been
used in various computer vision tasks, e.g., [20] uses it for
weighted averaging and image fusion, [53] uses a rolling
guidance for fully-controlled detail smoothing in an itera-
tive manner, [45] uses a fast guided ﬁlter for efﬁcient im-
age super-solution, [24] uses guided ﬁlters for high-quality
depth map restoration, [23] uses guided ﬁltering for toler-
ance to heavy noises and structure inconsistency, and [11]
puts Guided ﬁltering as a nonconvex optimization problem
and proposes solutions via majorize-minimization [15].

Most GANs for image-to-image-translation can synthe-
size high-resolution images but the appearance transfer of-
ten suppresses image details such as edges and texture. How
to keep the details of the original image while learning the
appearance of the target remain an active research area. The
proposed SF-GANs introduces guided ﬁlters into a cycle
network which is capable of achieving appearance transfer
and detail preserving concurrently.

3. The Proposed Method

The proposed SF-GAN consists of a geometry synthe-
sizer and an appearance synthesizer, and the whole network
is end-to-end trainable as illustrated in Fig. 2. Detailed net-
work structure and training strategy will be introduced in
the following subsections.

Table 1. The structure of the geometry estimation network within
the STN in Fig. 2

Layers Out Size
16 × 50
Block1
8 × 25
Block2
4 × 13
Block3

FC1
FC2

512
N

Conﬁgurations

3 × 3 conv, 32, 2 × 2 pool
3 × 3 conv, 64, 2 × 2 pool
3 × 3 conv, 128, 2 × 2 pool

-
-

3.1. Geometry Synthesizer

The geometry synthesizer has a local GAN structure as
highlighted by blue-color lines and boxes on the left of Fig.
2. It consists of a spatial transform network (STN), a com-
position module and a discriminator. The STN consists of
an estimation network as shown in Table 1 and a transfor-
mation matrix which has N parameters that control the ge-
ometric transformation of the foreground object.

The foreground object and background image are con-
catenated to act as the input of the STN, where the es-
timation network will predict a transformation matrix to
transform the foreground object. The transformation can
be afﬁne, homography, or thin plate spline [3] (We use thin
plate spline for the scene text synthesis task and homogra-
phy for the portrait wearing task). Each pixel in the trans-
formed image is computed by applying a sampling kernel
centered at a particular location in the original image. With
pixels in the original and transformed images denoted by
P s = (ps
N ), we use
a transformation matrix H to perform pixel-wise transfor-
mation as follows:

N ) and P t = (pt

2, . . . , ps

2, . . . , pt

1, ps

1, pt

xs
i
ys
i
1




(1)




xt
i
yt
i
1





 = H
j = (xt

i , ys

i = (xs

where ps
i ) denote the coordi-
nates of the i-th pixel within the original and transformed
image, respectively.

i ) and pt

i, yt

The transformed foreground object can thus be placed
into the background image to form an initially composed
image (Composed Image in Fig. 2). The discriminator D2
in Fig. 2 learns to distinguish whether the composed image
is realistic with respect to a set of Real Images. On the other
hand, our study shows that real images are not good refer-
ences for training geometry synthesizer. The reason is real
images are realistic in both geometry and appearance spaces
while the geometry can only achieve realism in geometry
space. The difference in appearance space between the syn-
thesized images and real images will mislead the training
of geometry synthesizer. For optimal training of geometry
synthesizer, the reference images should be realistic in the
geometry space only and concurrently have similar appear-
ance (e.g. colors and styles) with the initially composed

33655

Figure 2. The structure of the proposed SF-GAN: The geometry synthesizer is highlighted by blue-color lines and boxes on the left and the
appearance synthesizer is highlighted by orange-color lines and boxes on the right. STN denotes spatial transformation network, F denotes
guided ﬁlters, G1, G2, D1 and D2 denote the generators and discriminators. For clarity, cycle loss and identity loss are not included.

images. Such reference images are difﬁcult to create man-
ually. In the SF-GAN, we elegantly use images from the
appearance synthesizer (Adapted Real shown in Fig. 2) as
the reference to train the geometry synthesizer, more details
about the appearance synthesizer to be discussed in the fol-
lowing subsection.

3.2. Appearance Synthesizer

The appearance synthesizer is designed in a cycle struc-
ture as highlighted in orange-color lines and boxes on the
right of Fig. 2.
It aims to fuse the foreground object
and background image to achieve synthesis realism in the
appearance space. Image-to-image translation GANs also
strive for realistic appearance but they usually lose visual
details while performing the appearance transfer. Within
the proposed SF-GAN, guided ﬁlters are introduced which
help to preserve visual details effectively while working to-
wards synthesis realism within the appearance space.

3.2.1 Cycle Structure

The proposed SF-GAN adopts a cycle structure for map-
ping between two domains, namely, the composed image
domain and the real image domain. Two generators G1 and
G2 are designed to achieve image-to-image translation in
two reverse directions, G1 from Composed Image to Final
Synthesis and G2 from Real Images to Adapted Real as il-
lustrated in Fig. 2. Two discriminator D1 and D2 are de-
signed to discriminate real images and translated images.

In particular, D1 will strive to distinguish the adapted
composed images (i.e. the Composed Image after domain
adaptation by G1) and Real Images, forcing G1 to learn to
map from the Composed Image to Final Synthesis images
that are realistic in the appearance space G2 will learn to
map from Real Images to Adapted Real images, the images

that ideally are realistic in the geometry space only but have
similar appearance as the Composed Image. As discussed
in the previous subsection, the Adapted Real from G2 will
be used as references for training the geometry synthesizer
as it will better focus on synthesizing images with realistic
geometry (as the interfering appearance difference has been
compressed in the Adapted Real).

Image appearance transfer usually comes with detail
loss. We address this issue from two perspectives. The ﬁst
is by adaptive combination of cycle loss and identity loss.
Speciﬁcally, we adopt a weighted combination strategy that
assigns higher weight to the cycle-loss for interested im-
age regions while higher weight to the identify-loss for non-
interested regions. Take scene text image synthesis as an ex-
ample. By assigning a larger cycle-loss weight and smaller
identity-loss to text regions, it ensures a multi-mode map-
ping of the text style while keeping the background similar
to the original image. The second is by introducing guided
ﬁlters into the cycle structure for detail preserving, more
details to be described in the next subsection.

3.2.2 Guided Filter

Guided ﬁlter was designed to perform edge-preserving im-
age smoothing. It inﬂuences the ﬁltering by using structures
in a guidance image As appearance transfer in most image-
to-image-translation GANs tends to lose image details, we
introduce guided ﬁlters (F as shown in Fig. 2) into the SF-
GAN for detail preserving within the translated images. The
target is to perform appearance transfer on the foreground
object (within the Composed Image) only while keeping the
background image with minimum changes.

We introduce guided ﬁlters into the proposed SF-GAN
and formulate the detail-preserving appearance transfer as a
joint up-sampling problem as illustrated in Fig. 3. In par-

43656

By applying the linear model to all windows ωk in the
image and computing (ak, bk), the ﬁlter output can be de-
rived by averaging all possible values of Ti:

Ti =

1
|ω| X

k:i∈µk

(akIi + bk) = aiIi + bi

(6)

where ai = 1
bk. We
integrate the guide ﬁlter into the cycle structure network to
implement an end-to-end trainable system.

ak and bi = 1

|ω| Pk∈ωk

|ω| Pk∈ωi

Figure 3. Detailed structure of the guided ﬁlter F: Given an image
to be ﬁltered (Composed Image in Fig. 2), a translated image with
smoothed details (the output of G1 in Fig. 2 where details are lost
around background face and foreground hat areas) and the mask
of foreground object hat (provided), F produces a new image with
full details (Synthesized Image, the output of F at the bottom in
Fig. 2). It can be seen that the guided ﬁlter preserves details of
both background image (e.g.
the face area) and foreground hat
(e.g. the image areas highlighted by the red-color box).

ticular, the translated images from the output of G1 (image
details lost) is the input image I to be ﬁltered and the ini-
tially Composed Image (image details unchanged) shown in
Fig. 2 acts as the guidance image R to provide edge and
texture details. The detail-preserving image T (correspond-
ing to the Synthesized Image in Fig. 2) can thus be derived
by minimizing the reconstruction error between I and T ,
subjects to a linear model:

3.3. Adversarial Training

The proposed SF-GAN is designed to achieve synthe-
sis realism in both geometry and appearance spaces. The
SF-GAN training therefore has two adversarial objectives,
one is to learn the real geometry and the other is to learn
the real appearance The geometry synthesizer and appear-
ance synthesizer are actually two local GANs that are inter-
connected and need coordination during the training. For
presentation clarity, we denote the Foreground Object and
Background Image in Fig. 2 as the x, the Composed Image
as y and the Real Image as z which belongs to domains X,
Y and Z, respectively.

For the geometry synthesizer, the STN can actually be
viewed as a generator G0 which predicts transportation pa-
rameters for x. After the transformation of the Foreground
Object and Composition, the Composed Image becomes the
input of the discriminator D2 and the training reference z
comes from G2(z) of the appearance synthesizer. For the
geometry synthesizer, we adopt the Wasserstein GAN [1]
objective for training which can be denoted by:

′

Ti = akIi + bk, ∀i ∈ ωk

(2)

min
G0

max
D2

Ex∼X [D2(G0(x))] − Ez ′ ∼Z ′ [D2(z)]

(7)

where i is the index of a pixel and ωk is a local square win-
dow centered at pixel k.

To determine the coefﬁcients of the linear model ak and
bk, we seek a solution that minimizes the difference be-
tween T and the ﬁlter input R which can be derived by min-
imizing the following cost function in the local window:

E(ak, bk) = X

i∈ωk

((akIi + bk − Ri)2 + ǫa2
k)

(3)

where ǫ is a regularization parameter that prevents ak from
being too large. It can be solved via linear regression:

1

|ω| Pi∈ωk

ak =

Ii − µkRk

σk + ǫ

bk = Rk − akµk

(4)

(5)

where µk and σ2
is the number of pixels in ωk, and Rk = 1
mean of R in ωk.

k are the mean and variance of I in ωk, |ω|
is the

|ω| Pi∈ωk

′

′

denotes the domains for z

where Z
. Since G0 aims to
minimize this objective against an adversary D2 that tries to
maximize it, the loss functions of D2 and G0 can be deﬁned
by:

LD2 = Ex∼X [D2(G0(x)] − Ez ′ ∼Z ′ [D2(z

′

)]

LG0 = −Ex∼X [D2(G0(x))]

(8)

(9)

The appearance synthesizer adopts a cycle structure that
consists of two mappings G1 : Y → Z and G2 : Z → Y . It
has two adversarial discriminators D1 and D2. D2 is shared
between the geometry and appearance synthesizers, and it
aims to distinguish y from G2(z) within the appearance syn-
thesizer. The learning objectives thus consists of an adver-
sarial loss for the mapping between domains and a cycle
consistency loss for preventing the mode collapse. For the
adversarial loss, the objective of the mapping G1 : Y → Z
(and the same for the reverse mapping G2 : Z → Y ) can be
deﬁned by:

LD1 = Ey∼Y [D1(G1(y)] − Ez∼Z[D2(z)]

(10)

53657

LG1 = −Ey∼Y [D1(G1(y))]

(11)

As the adversarial

losses cannot guarantee that

the
learned function maps an individual input y to a desired out-
put z, we introduce cycle-consistency, aiming to ensure that
the image translation cycle will bring x back to the original
image, i.e. y → G1(y) → G2(G1(y)) = y. The cycle-
consistency can be achieved by a cycle-consistency loss:

LG1cyc = Ey∼p(y)[kG2(G1(y)) − yk]

LG2cyc = Ez∼p(z)[kG1(G2(z)) − zk]

(12)

(13)

We also introduce the identity loss to ensure that the trans-
lated image preserves features of the original image:

LG1idt = Ey∼Y [kG1(y) − yk]

LG2idt = Ez∼Z[kG2(z) − zk]

(14)

(15)

For each training step, the model needs to update the ge-
ometry synthesizer and appearance synthesizer separately.
In particular, LD2 and LG0 are optimized alternately while
updating the geometry synthesizer. While updating the ap-
pearance synthesizer, all weights of the geometry synthe-
In the mapping G1 : Y → Z, LD1
sizer are freezed.
and LG1 + λ1LG1cyc + λ2LG1idt are optimized alternately
where λ1 and λ2 controls the relative importance of the
cycle-consistency loss and the identity loss, respectively. In
the mapping G2 : Z → Y , LD2 and LG2 + λ1LG2cyc +
λ2LG2idt are optimized alternately.

It should be noted that the sequential updating is nec-
essary for end-to-end training of the proposed SF-GAN. If
discarding the geometry loss, we need update the geometry
synthesizer according to the loss function of the appearance
synthesizer. On the other hand, the appearance synthesizer
will generate blurry foreground objects regardless of the ge-
ometry synthesizer and this is similar to GANs for direct
image generation. As discussed before, the direct image
generation cannot provide accurate annotation information
and the directly generated images also have low quality and
are not suitable for training deep network models.

4. Experiments

4.1. Datasets

ICDAR2013 [19] is used in the Robust Reading Compe-
tition in the International Conference on Document Analy-
sis and Recognition (ICDAR) 2013. It contains 848 word
images for network training and 1095 for testing.

ICDAR2015 [18] is used in the Robust Reading Compe-
tition under ICDAR 2015. It contains incidental scene text
images that are captured without preparation before captur-
ing. 2077 text image patches are cropped from this dataset,

where a large amount of cropped scene texts suffer from
perspective and curvature distortions.

IIIT5K [28] has 2000 training images and 3000 test
images that are cropped from scene texts and born-digital
images. Each word in this dataset has a 50-word lexicon
and a 1000-word lexicon, where each lexicon consists of a
ground-truth word and a set of randomly picked words.

SVT [44] is collected from the Google Street View im-
ages that were used for scene text detection research. 647
words images are cropped from 249 street view images and
most cropped texts are almost horizontal.

SVTP [32] has 639 word images that are cropped from
the SVT images. Most images in this dataset suffer from
perspective distortion which are purposely selected for eval-
uation of scene text recognition under perspective views.

CUTE [35] has 288 word images mose of which are
curved. All words are cropped from the CUTE dataset
which contains 80 scene text images that are originally col-
lected for scene text detection research.

CelebA [25] is a face image dataset that consists of more
than 200k celebrity images with 40 attribute annotations.
This dataset is characterized by large quantities, large face
pose variations, complicated background clutters, rich an-
notations, and it is widely used for face attribute prediction.

4.2. Scene Text Synthesis

Data Preparation: The SF-GAN needs a set of Real Im-
ages to act at references as illustrated in Fig. 2. We create
the Real Images by cropping the text image patches from the
training images of ICDAR2013 [19], ICDAR2015 [18] and
SVT [44] by using the provided annotation boxes. While
cropping the text image patches, we extend the annotation
box (by an extra 1/4 of the width and height of the annota-
tion boxes) to include certain local geometric structures

Besides the Real Images, SF-GAN also needs a set
of Background Images as shown in Fig. 2. For scene
text image synthesis, we collect the background images by
smoothing out the text pixels of the cropped Real Images.
Further, the Foreground Object (text for scene text synthe-
sis) is computer-generated by using a 90k-lexicon. The cre-
ated Background Images, Foreground Texts and Real Images
are fed to the network to train the SF-GAN.

For the training of scene text recognition model, texts
need to be cropped out with tighter boxes (to exclude ex-
tra background). With the text maps as denoted by Trans-
formed Object in Fig. 2, scene text patches can be cropped
out accurately by detecting a minimal external rectangle.
Results Analysis: We use 1 million SF-GAN synthesized
scene text images to train scene text recognition models and
use the model recognition performance to evaluate the use-
fulness of the synthesized images. In addition, the SF-GAN
is benchmarked with a number of state-of-the-art synthe-

63658

Table 2. Scene text recognition accuracy over the datasets ICDAR2013, ICDAR2015, SVT, IIIT5K, SVTP and CUTE, where 1 million
synthesized text images are used for all comparison methods as as listed.

Methods

ICDAR2013

ICDAR2015

SVT

IIIT5K

SVTP

CUTE

AVERAGE

Jaderberg [17]

Gupta [10]

Zhan [50]

ST-GAN [21]

SF-GAN(BS)

SF-GAN(GS)

SF-GAN(AS)

SF-GAN

58.1

62.2

62.5

57.2

55.9

57.3

58.1

61.8

35.5

38.2

37.7

35.3

34.9

35.6

36.4

39.0

67.0

48.8

63.5

63.8

64.0

66.5

66.7

69.3

57.2

59.1

59.5

57.3

55.4

57.7

58.5

63.0

48.9

38.9

46.7

43.2

42.8

43.9

45.3

48.6

35.3

36.3

36.9

34.1

33.7

36.1

35.7

40.6

50.3

47.3

51.1

48.5

47.8

49.5

50.1

53.7

Foreground

Background

ST-GAN

CycleGAN

SF-GAN(GS)

SF-GAN

Figure 4. Illustration of scene text image synthesis by different
GANs: Rows 1-2 are foreground texts and background images
as labelled. Rows 3-4 show the images synthesized by ST-GAN
and CycleGAN, respectively. Row 5 shows images synthesized
by SF-GAN(GS), the output of the geometry synthesizer in SF-
GAN (Composed Image in Fig. 2). The last row shows images
synthesized by the proposed SF-GAN.

sis techniques by randomly selecting 1 million synthesized
scene text images from [17] and randomly cropping 1 mil-
lion scene text images from [10] and [50]. Beyond that,
we also synthesize 1 million scene text images with random
text appearance by using ST-GAN [21]. There are many
scene text recognition models [38, 39, 41, 36, 5], we design

an attentional scene text recognizer with a 50-layer ResNet
as the backbone network.

For ablation analysis, we evaluate SF-GAN(GS) which
denotes the output of the geometry synthesizer (Composed
Image as shown in Fig. 2) and SF-GAN(AS) which denotes
the output of the appearance synthesizer with random geo-
metric alignments. A baseline SF-GAN (BS) is also trained
where texts are placed with random alignment and appear-
ance. The three SF-GANs also synthesize 1 million im-
ages each for scene text recognition tests. The recognition
tests are performed over four regular scene text datasets IC-
DAR2013 [19], ICDAR2015 [18], SVT [44], IIIT5K [28]
and two irregular datasets SVTP [32] and CUTE [35] as de-
scribed in Datasets. Besides the scene text recognition, we
also perform user studies with Amazon Mechanical Turk
(AMT) where users are recruited to tell whether SF-GAN
synthesized images are real or synthesized.

Tables 2 and 3 show scene text recognition and AMT
user study results. As Table 2 shows, SF-GAN achieves the
highest recognition accuracy for most of the 6 datasets and
an up to 3% improvement in average recognition accuracy
(across the 6 datasets), demonstrating the superior useful-
ness of its synthesized images while used for training scene
text recognition models. The ablation study shows that the
proposed geometry synthesizer and appearance synthesizer
both help to synthesize more realistic and useful image in
recognition model training. In addition, they are comple-
mentary and their combination achieves a 6% improvement
in average recognition accuracy beyond the baseline SF-
GAN(BS). The AMT results in the second column of Table
3 also show that the SF-GAN synthesized scene text images
are much more realistic than state-of-the-art synthesis tech-
niques. Note the synthesized images by [17] are gray-scale
and not included in the AMT user study.

Fig. 4 shows a few synthesis images by using the pro-
posed SF-GAN and a few state-of-the-art GANs. As Fig.
4 shows, ST-GAN can achieve geometric alignment but the
appearance is clearly unrealistic within the synthesized im-

73659

Table 3. AMT user study to evaluate the realism of synthesized
images. Percentages represent the how often the images in each
category were classiﬁed as real by Turkers.

Methods

Gupta [10]

Zhan [50]

ST-GAN [21]

Real

SF-GAN

Text

38.0

41.5

31.6

74.1

57.7

Glass

Hat

-

-

41.7

78.6

62.0

-

-

42.6

78.2

67.3

ages. The CycleGAN can adapt the appearance of the fore-
ground texts to certain degrees but it ignores real geome-
try. This leads to not only unrealistic geometry but also
degraded appearance as the discriminator can easily distin-
guish generated images and real images according to the
geometry difference. The SF-GAN (GS) gives the output
of the geometry synthesizer, i.e.
the Composed Image as
shown in Fig. 2, which produces better alignment due to
good references from the appearance synthesizer. In addi-
tion, it can synthesize curve texts due to the use of a thin
plate spline transformation [3]. The fully implemented SF-
GAN can further learn text appearance from real images and
synthesize highly realistic scene text images. Besides, we
can see that the proposed SF-GAN can learn from neigh-
boring texts within the background images and adapt the
appearance of the foreground texts accordingly.

4.3. Portrait Wearing

Data preparation: We use the dataset CelebA [25] and fol-
low the provided training/test split for portrait wearing ex-
periment. The training set is divided into two groups by
using the annotation ‘glass’ and ‘hat’, respectively. For the
glass case, one group of people with glasses serve as the real
data for matching against in our adversarial settings and the
other group without glasses serves as the background. For
the foreground glasses, we crop 15 pairs of front-parallel
glasses and reuse them to randomly compose with the back-
ground images. According to our experiment, 15 pairs of
glasses as the foreground objects are sufﬁcient to train a ro-
bust model. The hat case has the similar setting, except that
we use 30 cropped hats as the foreground objects.
Results Analysis: Fig 5. shows a few SF-GAN synthe-
sized images and comparisons with ST-GAN synthesized
images. As Fig. 5 shows, ST-GAN achieves realism in the
geometry space by aligning the glasses and hats with the
background face images. On the other hand, the synthe-
sized images are unrealistic in the appearance space with
clear artifacts in color, contrast and brightness. As a com-
parison, the SF-GAN synthesized images are much more
realistic in both geometry and appearance spaces. In par-
ticular, the foreground glasses and hats within the SF-GAN

Objects

Faces

ST-GAN

SF-GAN

Figure 5. Illustration of portrait-wearing by different GANs:
Columns 1-2 show foreground hats and glasses and background
face images, respectively. Columns 3-4 show images synthesized
by by ST-GAN [21] and our proposed SF-GAN, respectively.

synthesized images have harmonious brightness, contrast,
and blending with the background face images. Addition-
ally, the proposed SF-GAN also achieve better geometric
alignment as compared with ST-GAN which focuses on ge-
ometric alignment only. We conjecture that the better ge-
ometric alignment is largely due to the reference from the
appearance synthesizer. The AMT results as shown in the
last two columns of Table 3 also show the superior synthesis
performance of our proposed SF-GAN.

5. Conclusions

This paper presents a SF-GAN, an end-to-end trainable
network that synthesize realistic images given foreground
objects and background images. The SF-GAN is capable of
achieving synthesis realism in both geometry and appear-
ance spaces concurrently. The ﬁrst scene text image syn-
thesis study shows that the proposed SF-GAN is capable of
synthesizing useful images to train better recognition mod-
els. The second portrait-wearing study shows the SF-GAN
is widely applicable and can be easily extend to other tasks.
We will continue to study SF-GAN for full-image synthesis
for training better detection models.

83660

References

[1] Martin Arjovsky, Soumith Chintala, and Lon Bottou.
Wasserstein generative adversarial networks. In ICML, 2017.
1, 2, 5

[2] Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, and
Trevor Darrell. Compositional gan: Learning conditional
image composition. arXiv:1807.07560, 2018. 1, 3

[3] Fred L. Bookstein. Principal warps: Thin-plate splines and
the decomposition of deformations. TPAMI, 11(6), 1989. 3,
8

[4] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel.
Infogan: Interpretable rep-
resentation learning by information maximizing generative
adversarial nets. In NIPS, 2016. 2

[5] Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu,
and Shuigeng Zhou. Aon: Towards arbitrarily-oriented text
recognition. In CVPR, 2018. 7

[6] Emily Denton, Soumith Chintala, Arthur Szlam, and Rob
Fergus. Deep generative image models using a laplacian
pyramid of adversarial networks. In NIPS, 2015. 2

[7] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut,
paste and learn: Surprisingly easy synthesis for instance de-
tection. In ICCV, 2017. 2

[8] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora
Vig. Virtual worlds as proxy for multi-object tracking anal-
ysis. In CVPR, 2016. 2

[9] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. In NIPS,
pages 2672–2680, 2014. 1, 2

[10] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.
In

Synthetic data for text localisation in natural images.
CVPR, 2016. 2, 7, 8

[11] Bumsub Ham, Minsu Cho, and Jean Ponce. Robust guided
image ﬁltering using nonconvex potentials. TPAMI, 2018. 3
Fast guided ﬁlter.

[12] Kaiming He

and Jian Sun.

arXiv:1505.00996, 2015. 3

[13] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image ﬁl-

tering. TPAMI, 2013. 3

[14] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Alexei A. Efros, and Trevor Darrell. Cycada:
Cycle-consistent adversarial domain adaptation.
In ICML,
2018. 1, 2

[15] David R. Hunter and Kenneth Lange. Quantile regression via
an mm algorithm. Journal of Computational and Graphical
Statistics, 2000. 3

[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017. 1, 2, 3

[17] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-
drew Zisserman. Synthetic data and artiﬁcial neural net-
works for natural scene text recognition.
In NIPS Deep
Learning Workshop, 2014. 2, 7

[18] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos
Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-
mura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-
drasekhar, Shijian Lu, Faisal Shafait, Seiichi Uchida, and

Ernest Valveny. Icdar 2015 competition on robust reading.
In ICDAR, pages 1156–1160, 2015. 6, 7

[19] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, S. R. Mestre,
J. Mas, D. F. Mota, J. A. Almazan, L. P. de las Heras, and et
al. Icdar 2013 robust reading competition. In ICDAR, pages
1484–1493, 2013. 6, 7

[20] Shutao Li, Xudong Kang, and Jianwen Hu.

Image fusion

with guided ﬁltering. TIP, 22(7), 2013. 3

[21] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,
and Simon Lucey. St-gan: Spatial transformer generative
adversarial networks for image compositing. In CVPR, 2018.
1, 3, 7, 8

[22] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised

image-to-image translation networks. In NIPS, 2017. 1, 2

[23] Wei Liu, Xiaogang Chen, Chunhua Shen, Jingyi Yu,
Qiang Wu, and Jie Yang. Robust guided image ﬁltering.
arXiv:1703.09379, 2017. 3

[24] Wei Liu, Yun Gu, Chunhua Shen, Xiaogang Chen, Qiang
Wu, and Jie Yang. Data driven robust image guided depth
map restoration. TIP, 2017. 3

[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
In ICCV, 2015.

Deep learning face attributes in the wild.
6, 8

[26] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala.

Deep painterly harmonization. arXiv:1804.03189, 2018. 2

[27] Simon Osindero Mehdi Mirza. Conditional generative ad-

versarial nets. arXiv:1411.1784, 2014. 1

[28] Anand Mishra, Karteek Alahari, and C.V. Jawahar. Scene
In

text recognition using higher order language priors.
BMVC, 2012. 6, 7

[29] Yair Movshovitz-Attias, Takeo Kanade, and Yaser Sheikh.
How useful is photo-realistic rendering for visual learning?
In ECCV, 2016. 2

[30] Dennis Park and Deva Ramanan. Articulated pose estimation

with tiny synthetic videos. In CVPR, 2015. 2

[31] Patrick P´erez, Michel Gangnet, and Andrew Blake. Poisson

image editing. TOG, 22(3), 2003. 2, 3

[32] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan
Tian, and Chew Lim Tan. Recognizing text with perspec-
tive distortion in natural scenes. In ICCV, 2013. 6, 7

[33] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016. 1, 2

[34] Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for data: Ground truth from computer
games. In ECCV, 2016. 2

[35] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng
Chan, and Chew Lim Tan. A robust arbitrary text detec-
tion system for natural scene images. Expert Syst. Appl.,
41(18):8027–8048, 2014. 6, 7

[36] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan
Lyu, Cong Yao, and Xiang Bai. Aster: An attentional scene
text recognizer with ﬂexible rectiﬁcation. TPAMI, 2018. 7

[37] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel,

Josh
Susskind, Wenda Wang, and Russ Webb. Learning from sim-
ulated and unsupervised images through adversarial training.
In CVPR, 2017. 3

93661

[38] Bolan Su and Shijian Lu. Accurate scene text recognition

based on recurrent neural network. In ACCV, 2014. 7

[39] Bolan Su and Shijian Lu. Accurate recognition of words in
scenes without character segmentation using recurrent neural
network. PR, pages 397–405, 2017. 7

[40] Hao Su, Charles R. Qi, Yangyan Li, and Leonidas Guibas.
Render for cnn: Viewpoint estimation in images using cnns
trained with rendered 3d model views. In ICCV, 2015. 2

[41] Shangxuan Tian, Ujjwal Bhattacharya, Shijian Lu, Bolan
Su, Qingqing Wang, Xiaohua Wei, Yue Lu, and Chew Lim
Tan. Multilingual scene character recognition with cooccur-
rence of histogram of oriented gradients. PR, pages 125–134,
2016. 7

[42] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli,
Xin Lu, and Ming-Hsuan Yang. Deep image harmonization.
In CVPR, 2017. 2

[43] Matthew Uyttendaele, Ashley Eden, and Richard Szeliski.
Eliminating ghosting and exposure artifacts in image mo-
saics. In CVPR, 2001. 2

[44] K. Wang, B. Babenko, and S. Belongie. End-to-end scene

text recognition. In ICCV, 2011. 6, 7

[45] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang.

Fast end-to-end trainable guided ﬁlter. In CVPR, 2017. 3

[46] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang.
Gp-gan: Towards realistic high-resolution image blending.
arXiv:1703.07195, 2017. 3

[47] Chuhui Xue, Shijian Lu, and Fangneng Zhan. Accurate
scene text detection through border semantics awareness and
bootstrapping. In ECCV, pages 355–372, 2018. 2

[48] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi
Parikh. Lr-gan: Layered recursive generative adversarial net-
works for image generation. In ICLR, 2017. 3

[49] Fangneng Zhan and Shijian Lu. Esir: End-to-end scene text
recognition via iterative image rectiﬁcation. In CVPR, 2019.
2

[50] Fangneng Zhan, Shijian Lu, and Chuhui Xue. Verisimilar
image synthesis for accurate detection and recognition of
texts in scenes. In ECCV, pages 249–266, 2018. 2, 7, 8

[51] Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Scene text
synthesis for efﬁcient and effective deep network training.
arXiv:1901.09193, 2019. 2

[52] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In ICCV, 2017. 2

[53] Qi Zhang, Xiaoyong Shen, Li Xu, and Jiaya Jia. Rolling

guidance ﬁlter. In ECCV, 2014. 3

[54] Yinan Zhao, Brian Price, Scott Cohen, and Danna Gurari.
Guided image inpainting: Replacing an image region by
pulling content from another image.
arXiv:1803.08435,
2018. 2

[55] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 1, 2

103662

