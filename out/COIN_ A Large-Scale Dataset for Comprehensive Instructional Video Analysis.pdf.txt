COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis

Yansong Tang1

Dajun Ding2

Yongming Rao1

Yu Zheng1

Danyang Zhang1

Lili Zhao2

Jiwen Lu1 ∗

1Department of Automation, Tsinghua University

Jie Zhou1
2 Meitu Inc.

https://coin-dataset.github.io/

Abstract

There are substantial instructional videos on the Inter-
net, which enables us to acquire knowledge for completing
various tasks. However, most existing datasets for instruc-
tional video analysis have the limitations in diversity and
scale, which makes them far from many real-world applica-
tions where more diverse activities occur. Moreover, it still
remains a great challenge to organize and harness such da-
ta. To address these problems, we introduce a large-scale
dataset called “COIN” for COmprehensive INstructional
video analysis. Organized with a hierarchical structure, the
COIN dataset contains 11,827 videos of 180 tasks in 12 do-
mains (e.g., vehicles, gadgets, etc.) related to our daily life.
With a new developed toolbox, all the videos are annotated
effectively with a series of step descriptions and the corre-
sponding temporal boundaries. Furthermore, we propose a
simple yet effective method to capture the dependencies a-
mong different steps, which can be easily plugged into con-
ventional proposal-based action detection methods for lo-
calizing important steps in instructional videos.
In order
to provide a benchmark for instructional video analysis, we
evaluate plenty of approaches on the COIN dataset under
different evaluation criteria. We expect the introduction of
the COIN dataset will promote the future in-depth research
on instructional video analysis for the community.

1. Introduction

Instructional videos provide intuitive visual examples for
learners to acquire knowledge to accomplish different tasks.
With the explosion of video data on the Internet, people
around the world have uploaded and watched substantial
instructional videos [10, 36], covering miscellaneous cate-
gories. According to the scientists in educational psycholo-
gy [34], novices often face difﬁculties in learning from the
whole realistic task, and it is necessary to divide the whole
task into smaller segments or steps as a form of simpliﬁca-

∗the corresponding author is Jiwen Lu.

tion. Actually, a variety of video analysis tasks in comput-
er vision (e.g., action temporal localization [45, 50], video
summarization [19, 30, 49] and video caption [27, 47, 53],
etc) have been developed recently. Also, growing efforts
have been devoted to exploiting different challenges of in-
structional video analysis [10, 24, 36, 52].

In recent years, a number of datasets for instructional
video analysis [10, 14, 28, 35, 40, 41, 52] have been collect-
ed in the community. Annotated with texts and temporal
boundaries of a series of steps to complete different tasks,
these datasets have provided good benchmarks for prelimi-
nary research. However, most existing datasets focus on a
speciﬁc domain like cooking, which makes them far from
many real-world applications where more diverse activities
occur. Moreover, the scales of these datasets are insufﬁcient
to satisfy the hunger of recent data-driven learning methods.

To tackle these problems, we introduce a new dataset
called “COIN” for COmprehensive INstructional video
analysis. The COIN dataset contains 11,827 videos of 180
different tasks, covering the daily activities related to ve-
hicles, gadgets, etc. Unlike existing instructional video
datasets, COIN is organized in a three-level semantic struc-
ture. Take the top row of Figure 1 as an example, the ﬁrst
level of this root-to-leaf branch is a domain named “vehi-
cles", under which there are numbers of video samples be-
longing to the second level tasks. For a speciﬁc task like
“change the car tire", It is comprised of a series of step-
s such as “unscrew the screws", “jack up the car", “put on
the tire", etc. These steps appear in different interval of a
video, which belongs to the third-level tags of COIN. We
also provide the temporal boundaries of all the steps, which
are effectively annotated based on a new developed toolbox.

As another contribution, we propose a new task-
consistency method to localize different steps in instruction-
al videos by considering their intrinsic dependencies. First,
as a bottom-up strategy, we infer the task label of the w-
hole video according to the prediction scores, which can be
obtained by existing proposal-based action detection meth-
ods. Then, as a top-down scheme, we reﬁne the proposal
scores based on the predicted task label.
In order to set

1207

(cid:42)(cid:85)(cid:83)(cid:71)(cid:79)(cid:84)

(cid:58)(cid:71)(cid:89)(cid:81)

(cid:57)(cid:90)(cid:75)(cid:86)

403

424

435

448

454

462

471

472

473

857

859

860

874

879

882

886

893

895

974

1010

1054

1058

1060

1061

1065

1068

1069

(cid:60)(cid:75)(cid:78)(cid:79)(cid:73)(cid:82)(cid:75)(cid:89)

(cid:41)(cid:78)(cid:71)(cid:84)(cid:77)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:41)(cid:71)(cid:88)(cid:3)(cid:58)(cid:79)(cid:88)(cid:75)

(cid:97)(cid:91)(cid:84)(cid:89)(cid:73)(cid:88)(cid:75)(cid:93)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:93)(cid:89)(cid:18)(cid:3)(cid:80)(cid:71)(cid:73)(cid:81)(cid:3)(cid:91)(cid:86)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:73)(cid:71)(cid:88)(cid:18)(cid:3)(cid:88)(cid:75)(cid:83)(cid:85)(cid:92)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:90)(cid:79)(cid:88)(cid:75)(cid:18)(cid:3)(cid:86)(cid:91)(cid:90)(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:90)(cid:79)(cid:88)(cid:75)(cid:18)(cid:3)(cid:90)(cid:79)(cid:77)(cid:78)(cid:90)(cid:75)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:93)(cid:89)(cid:3)(cid:99)

165

171

178

209

245

254

276

327

331

396

445

452

464

483

487

492

494

497

502

507

513

520

523

525

545

554

560

(cid:46)(cid:85)(cid:91)(cid:89)(cid:75)(cid:78)(cid:85)(cid:82)(cid:74)(cid:3)(cid:47)(cid:90)(cid:75)(cid:83)(cid:89)

(cid:56)(cid:75)(cid:86)(cid:82)(cid:71)(cid:73)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:42)(cid:85)(cid:85)(cid:88)(cid:3)(cid:49)(cid:84)(cid:85)(cid:72)

(cid:97)(cid:88)(cid:75)(cid:83)(cid:85)(cid:92)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:74)(cid:85)(cid:85)(cid:88)(cid:3)(cid:81)(cid:84)(cid:85)(cid:72)(cid:18)(cid:3)(cid:88)(cid:75)(cid:83)(cid:85)(cid:92)(cid:75)(cid:3)(cid:72)(cid:85)(cid:82)(cid:90)(cid:3)(cid:71)(cid:84)(cid:74)(cid:3)(cid:86)(cid:79)(cid:84)(cid:3)(cid:72)(cid:85)(cid:71)(cid:88)(cid:74)(cid:18)(cid:3)(cid:79)(cid:84)(cid:89)(cid:90)(cid:71)(cid:82)(cid:82)(cid:3)(cid:84)(cid:75)(cid:93)(cid:3)(cid:86)(cid:79)(cid:84)(cid:3)(cid:72)(cid:85)(cid:71)(cid:88)(cid:74)(cid:18)(cid:3)(cid:79)(cid:84)(cid:89)(cid:90)(cid:71)(cid:82)(cid:82)(cid:3)(cid:84)(cid:75)(cid:93)(cid:3)(cid:72)(cid:85)(cid:82)(cid:90)(cid:18)(cid:3)(cid:79)(cid:84)(cid:89)(cid:90)(cid:71)(cid:82)(cid:82)(cid:3)(cid:84)(cid:75)(cid:93)(cid:3)(cid:74)(cid:85)(cid:85)(cid:88)(cid:3)(cid:81)(cid:84)(cid:85)(cid:72)(cid:3)(cid:99)

Figure 1. Visualization of two root-to-leaf branches of the COIN. There are three levels of our dataset: domain, task and step. Take the
top row as an example, in the left box, we show a set of frames of 9 different tasks associated with the domain “vehicles”. In the middle
box, we present several images of 9 videos belonging to the task “change the car tire”. Based on this task, in the right box, we display
a sequence of frames sampled from a speciﬁc video, where the indices are presented at the left-top of each frame. The intervals in red,
blue and yellow indicate the step of “unscrew the screws”, “jack up the car” and “put on the tire”, which are described with the text in
corresponding color at the bottom of the right box. All ﬁgures are best viewed in color.

Table 1. Comparisons of existing instructional video datasets.

Dataset

Duration

Samples

Segments

Type of Task

Video Source Hierarchical

Classes

Year

MPII [35]
YouCook [14]
50Salads [40]
Breakfast [28]
"5 tasks" [10]
Ikea-FA [41]
YouCook2 [52]
EPIC-KITCHENS [13]

9h,48m
2h,20m
5h,20m

77h
5h

3h,50m

176h
55h

44
88
50

1,989
150
101
2,000
432

5,609

-

966
8,456

-

1,911
13,829
39,596

cooking activities
cooking activities
cooking activities
cooking activities

comprehensive tasks
assembling furniture

cooking activities
cooking activities

self-collected

YouTube

self-collected
self-collected

YouTube

self-collected

YouTube

self-collected

COIN (Ours)

476h,38m

11,827

46,354

comprehensive tasks

YouTube

✗

✗

✗

✗

✗

✗

✗

✗

✓

2012
2013
2013
2014
2016
2017
2018
2018

-
-
-
10
5
-
89
-

180

up a benchmark, we implement various approaches on the
COIN. The experimental results have shown the great chal-
lenges of our dataset and the effectiveness of the proposed
method for step localization.

2. Related Work

Tasks for Instructional Video Analysis: There are var-
ious tasks for instructional video analysis, e.g., step local-
ization, action segmentation, procedure segmentation [52],
dense video caption [53] and visual grounding [25, 51]. In
this paper, we focus on the ﬁrst two tasks, where step local-
ization aims to localize the start and end points of a series of
steps and recognizing their labels, and action segmentation
targets to parse a video into different actions at frame-level.
Datasets Related to Instructional Video Analysis:
There are mainly three types of related datasets.
(1)
The action detection datasets are comprised of untrimmed
video samples, and the goal is to recognize and local-
ize the action instances on temporal domain [23, 26] or

spatial-temporal domain [21].
(2) The video summariza-
tion datasets [15, 22, 30, 38] contain long videos arranging
from different domains. The tasks is to extract a set of in-
formative frames in order to brieﬂy summarize the video
content. (3) The video caption datasets are annotated with
descried sentences or phrases, which can be based on either
a trimmed video [46, 47] or different segments of a long
video [27]. Our COIN is relevant to the above mentioned
datasets, as it requires to localize the temporal boundaries
of important steps corresponding to a task. The main dif-
ferences lie in the following two aspects: (1) Consistency.
The steps belonging to different tasks shall not appear in the
same video. For example, it is unlikely for an instructional
video to contain the step “pour water to the tree” (belongs to
task “plant tree”) and the step “install the lampshade” (be-
longs to task “replace a bulb”). (2) Ordering. There may be
some intrinsic ordering constraints among a series of steps
for completing different tasks. For example, for the task of
“planting tree", the step “dig a hole" shall be ahead of the

1208

step “put the tree into the hole".

There have been a variety of instructional video dataset-
s proposed in recent years. Table I summarizes the com-
parison among some publicly relevant instructional datasets
and our proposed COIN. While the existing datasets present
various challenges to some extent, they still have some limi-
tations in the following two aspects. (1) Diversity: Most of
these datasets tend to be speciﬁc and contain certain types
of instructional activities, e.g., cooking. However, accord-
ing to some widely-used websites [7–9], people attempt to
acquire knowledge from various types of instructional video
across different domains. (2) Scale: Compared with the re-
cent datasets for image classiﬁcation (e.g., ImageNet [16]
with 1 million images) and action detection (e.g., Activ-
ityNet v1.3 [23] with 20k videos), most existing instruc-
tional video datasets are relatively smaller in scale. The
challenge to build such a large-scale dataset mainly stems
from the difﬁculty to organize enormous amount of video
and the heavy workload of annotation. To address these
two issues, we ﬁrst establish a rich semantic taxonomy cov-
ering 12 domains and collect 11,827 instructional videos to
construct COIN. With our new developed toolbox, we also
provide the temporal boundaries of steps that appear in all
the videos with effective and precise annotations.

Methods for Instructional Video Analysis: The ap-
proaches for instructional video analysis can be roughly di-
vided into three categories: unsupervised learning-based,
weakly-supervised learning-based and fully-supervised
learning-based. For the ﬁrst category, the step localization
task usually takes a video and the corresponding narration
or subtitle as multi-modal inputs 1. For example, Sener et
al. [36] developed a joint generative model to parse both
video frames and subtitles into activity steps. Alayrac et
al. [10] leveraged the complementary nature of the instruc-
tional video and its narration to discover and locate the main
steps of a certain task. Generally speaking, the advantages
to employ the narration or subtitle is to avoid human an-
notation, which may cost huge workload. However, these
narration or subtitles may be inaccurate [52] or even irrele-
vant to the video2.

For the second category, Kuehne et al. [28] develope-
d a hierarchical model based on HMMs and a context-free
grammar to parse the main steps in the cooking activities.
Richard et al. [32] [33] adopted Viterbi algorithm to solve
the probabilistic model of weakly supervised segmentation.
Ding et al. [17] proposed a temporal convolutional feature
pyramid network to predict frame-wise labels and use soft
boundary assignment to iteratively optimize the segmenta-
tion results. In this work, we also evaluate these three meth-

1The language signal should not be treated as supervision since the
steps are not directly given, but need to be further explored in an unsuper-
vised manner.

2For example, in a video with YouTube ID CRRiYji_K9Q, the instruc-

tor talks a lot about other things when she performs the task “injection”.

(cid:41)(cid:53)(cid:47)(cid:52)

(cid:43)(cid:82)(cid:75)(cid:73)(cid:90)(cid:88)(cid:79)(cid:73)(cid:71)(cid:82)(cid:3)(cid:39)(cid:86)(cid:86)(cid:82)(cid:79)(cid:71)(cid:84)(cid:73)(cid:75)(cid:89)

(cid:41)(cid:53)(cid:47)(cid:52)

Replace a Bulb

Install a Ceiling Fan

(cid:28615)(cid:28648)(cid:28633)(cid:28644)(cid:28564)(cid:28581)

(cid:28614)(cid:28633)(cid:28641)(cid:28643)(cid:28650)(cid:28633)(cid:28564)(cid:28648)(cid:28636)(cid:28633)(cid:28564)
(cid:28640)(cid:28629)(cid:28641)(cid:28644)(cid:28647)(cid:28636)(cid:28629)(cid:28632)(cid:28633)

(cid:28615)(cid:28648)(cid:28633)(cid:28644)(cid:28564)(cid:28582)

(cid:28616)(cid:28629)(cid:28639)(cid:28633)(cid:28564)(cid:28643)(cid:28649)(cid:28648)(cid:28564)(cid:28648)(cid:28636)(cid:28633)(cid:28564)

(cid:28643)(cid:28640)(cid:28632)(cid:28564)(cid:28630)(cid:28649)(cid:28640)(cid:28630)

(cid:28615)(cid:28648)(cid:28633)(cid:28644)(cid:28564)(cid:28583)

(cid:28605)(cid:28642)(cid:28647)(cid:28648)(cid:28629)(cid:28640)(cid:28640)(cid:28564)(cid:28648)(cid:28636)(cid:28633)(cid:28564)(cid:28642)(cid:28633)(cid:28651)(cid:28564)

(cid:28630)(cid:28649)(cid:28640)(cid:28630)

(cid:28615)(cid:28648)(cid:28633)(cid:28644)(cid:28564)(cid:28584)

(cid:28605)(cid:28642)(cid:28647)(cid:28648)(cid:28629)(cid:28640)(cid:28640)(cid:28564)(cid:28648)(cid:28636)(cid:28633)(cid:28564)
(cid:28640)(cid:28629)(cid:28641)(cid:28644)(cid:28647)(cid:28636)(cid:28629)(cid:28632)(cid:28633)

(cid:28648)(cid:28636)(cid:28633)(cid:28564)(cid:28634)(cid:28637)(cid:28646)(cid:28647)(cid:28648)(cid:28564)(cid:28640)(cid:28633)(cid:28650)(cid:28633)(cid:28640)(cid:28590)(cid:28564)(cid:28632)(cid:28643)(cid:28641)(cid:28629)(cid:28637)(cid:28642)(cid:28564)

(cid:28648)(cid:28636)(cid:28633)(cid:28564)(cid:28647)(cid:28633)(cid:28631)(cid:28643)(cid:28642)(cid:28632)(cid:28564)(cid:28640)(cid:28633)(cid:28650)(cid:28633)(cid:28640)(cid:28590)(cid:28564)(cid:28648)(cid:28629)(cid:28647)(cid:28639)

(cid:28648)(cid:28636)(cid:28633)(cid:28564)(cid:28648)(cid:28636)(cid:28637)(cid:28646)(cid:28632)(cid:28564)(cid:28640)(cid:28633)(cid:28650)(cid:28633)(cid:28640)(cid:28590)(cid:28564)(cid:28647)(cid:28648)(cid:28633)(cid:28644)

Figure 2. Illustration of the COIN lexicon. The left ﬁgure shows
the hierarchical structure, where the nodes of three different sizes
correspond to the domain, task and step respectively. For brevi-
ty, we do not draw all the tasks and steps here. The right ﬁgure
presents detailed steps of the task “replace a bulb", which belongs
to the domain “electrical appliances".

ods3 to provide a benchmark results on COIN.

For the third category, we focus on step localization.
This task is related to the area of action detection, where
promising progress has also been achieved recently. For ex-
ample, Zhao et al. [50] developed structured segment net-
works (SSN) to model the temporal structure of each ac-
tion instance with a structured temporal pyramid. Xu et
al. [45] introduced a Region Convolutional 3D Network (R-
C3D) architecture, which was built on C3D [42] and Faster
R-CNN [31], to explore the region information of video
frames. Compared with these methods, we attempt to fur-
ther explore the dependencies of different steps, which lies
in the intrinsic structure of instructional videos. Towards
this goal, we proposed a new method with a bottom-up s-
trategy and a top-down scheme. Our method can be easily
plugged into recent proposal-based action detection meth-
ods and enhance the performance of step localization in in-
structional videos.

3. The COIN Dataset

In this section we present COIN, a video-based dataset
which covers an extensive range of everyday tasks with ex-
plicit steps. To our best knowledge, it is currently the largest
dataset for comprehensive instructional video analysis. We
will introduce COIN from the following aspects: the estab-
lishment of lexicon, a new developed toolbox for efﬁcient
annotation, and the statistics of our dataset.

Lexicon: The purpose of COIN is to establish a rich se-
mantic taxonomy to organize comprehensive instructional
videos.
In previous literature, some representative large-
scale datasets were built upon existing structures. For ex-
ample, the ImageNet [16] database was constructed based
on a hierarchical structure of WordNet [20], while the Ac-
tivityNet dataset [23] adopted the activity taxonomy orga-

3The details of the weak supervisions are described in section 5.2.

1209

nized by American Time Use Survey (ATUS) [29]. In com-
parison, it remains great difﬁculty to deﬁne such a semantic
lexicon for instructional videos because of their high diver-
sity and complex temporal structure. Hence, most existing
instructional video datasets [52] focus on a speciﬁc domain
like cooking or furniture assembling, and [10] only consists
of ﬁve tasks.

Towards the goal of constructing a large-scale bench-
mark with high diversity, we proposed a hierarchical struc-
ture to organize our dataset. Figure 1 and Figure 2 present
the illustration of our lexicon, which contains three levels
from roots to leafs: domain, task and step.

(1) Domain. For the ﬁrst level, we bring the ideas from
the organization of several websites [7] [9] [8], which are
commonly-used for users to watch or upload instructional
videos. We choose 12 domains as: nursing & caring, ve-
hicles, leisure & performance, gadgets, electric appliances,
household items, science & craft, plants & fruits, snacks &
drinks, dishes, sports, and housework.

(2) Task. As the second level, the task is linked to the do-
main. For example, the tasks “replace a bulb” and “install
a ceiling fan” are associated with the domain “electrical ap-
pliances”. As most tasks on [7] [9] [8] may be too speciﬁc,
we further search different tasks of the 12 domains on Y-
ouTube. In order to ensure the tasks of COIN are commonly
used, we ﬁnally select 180 tasks, under which the searched
videos are often viewed 4.

(3) Step. The third level of the lexicon are various se-
ries of steps to complete different tasks. For example, steps
“remove the lampshade”, “take out the old bulb”, “instal-
l the new bulb” and “install the lampshade” are associated
with the tasks “replace a bulb”. We employed 6 expert-
s (e.g. driver, athlete, etc.) who have prior knowledge in
the 12 domains to deﬁne these steps. They were asked to
browse the corresponding videos as a preparation in order
to provide the high-quality deﬁnition, and each step phrase
will be double checked by another expert. In total, there are
778 deﬁned steps, where there are 4.84 words per phrase for
each step. Note that we do not directly adopt the narrated
information, which might have large variance for a speciﬁc
task, because we expect to obtain the simpliﬁcation of the
core steps, which are common in different videos of accom-
plishing a certain task.

Annotation Tool: Given an instructional video, the goal
of annotation is to label the step categories and the corre-
sponding segments. As the segments are variant in length
and content, it will cost huge workload to label the COIN
with conventional annotation tool. In order to improve the
annotation efﬁciency, we have developed a new toolbox
which has two modes: frame mode and video mode. Fig-
ure 4 shows an example interface of the frame mode, which
presents the frames extracted from a video under an ad-

4We present the statistics of browse times in supplementary material.

Figure 3. The interface of our new developed annotation tool under
the frame mode.

Distribution of Video Duration

Distribution of Segment Length

t
n
u
o
C
 
n
o
i
t
a
v
r
e
s
b
O

1200

1000

800

600

400

200

0

10000

8000

6000

4000

2000

t
n
u
o
C
 
n
o
i
t
a
v
r
e
s
b
O

0

100

200
400
Temporal Length (seconds)

300

500

600

0

0

20

40

60

80

100

Temporal Length (seconds)

Figure 4. The duration statistics of the videos (left) and segments
(right) in the COIN dataset.

justable frame rate (default is 2fps). Under the frame mode,
the annotator can directly select the start and end frame of
the segment as well as its label. However, due to the time
gap between two adjacent frames, some quick and consec-
utive actions might be missed. To address this problem, we
adopted another video mode. The video mode of the anno-
tation tool presents the online video and timeline, which is
frequently used in previous video annotation systems [27].
Though the video mode brings more continuous information
in the time scale, it is much more time-consuming than the
frame mode because of the process to locate a certain frame
and adjust the timeline5.

During the annotation process, each video is labelled by
three different workers with payments. To begin with, the
ﬁrst worker generated primary annotation under the frame
mode. Next, the second worker adjusted the annotation
based on the results of the ﬁrst worker. Ultimately, the third
worker switched to the video mode to check and reﬁne the
annotation. Under this pipeline, the total time of the anno-
tation process is about 600 hours.

Statistics: The COIN dataset consists of 11,827 videos
related to 180 different tasks, which were all collected from
YouTube. Figure 5 shows the sample distributions among
all the task categories.
In order to alleviate the effect of
long tails, we make sure that there are more than 39 videos

5 For a set of videos, the annotation time under the frame mode is only
26.8% of that under the video mode. Please see supplementary material for
details.

1210

l

s
e
p
m
a
S
#

120

100

80

60

40

20

0

l

s
e
p
m
a
S
#

60

40

20

0

Training
Testing

w
o
S

r
o
t
c
e
t
o
r
P
n
e
e
r
c
S
e

l
l
i

i

M
d
n
W
r
e
p
a
P
e
k
a
M

s
n
o
i
s
n
e
t
x
E
r
i
a
H
n
O
t
u
P

l
i

r
i
a
h
C
e
c
i
f
f

l

O
e
b
m
e
s
s
A

a
e
T
a
h
c
t
a
M
e
k
a
M

n
w
o
r
C
r
e
w
o
F
e
k
a
M

l

e
t
a
r
a
K
e
s
i
t
c
a
r
P

e
l
t
t
o
B
e
n
g
a
p
m
a
h
C
n
e
p
O

D
S
S
h
t
i

W
e
v
i
r
D
D
C
e
c
a
p
e
R

l

w
a
S
A
f
O
e
d
a
B
e
c
a
p
e
R

l

l

e
c
i
R
k
o
o
C
o
T
r
e
k
o
o
C
e
c
i
R
e
s
U

s
p

l

n
o
m
a
S
e
k
a
M

i
l
c
r
e
p
a
P
h
t
i

l

b
u
B
A
e
c
a
p
e
R

l

t
o
P
y
t
s
u
R
n
a
e
C

l

r
a
C
o
T
k
c
a
R
e
l
c
y
c
i
B

r
e
i
f
i
r
u
P
r
i
A
r
o
F
r
e
t
l
i

W
k
c
o
L
A
n
e
p
O

l
l

a
t
s
n
I

F
e
c
a
p
e
R

l

b
o
M
e
c
a
p
e
R

l

b
o
n
K
r
o
o
D
e
c
a
p
e
R

l

n
e
e
r
c
S
p
o
t
p
a
L
e
c
a
p
e
R

l

l

l

e
u
G
h
t
i

e
u
G
h
t
i

W
e
m

i
l

S
e
k
a
M

W
s
d
a
e
h
k
c
a
B
e
v
o
m
e
R

l

r
e
t
l
i

F
r
e
t
a
W
r
o
t
a
r
e
g
i
r
f
e
R
e
c
a
p
e
R

l

l

s
e
d
o
o
N

l
i

o
B

r
e
k
c
i
t
S
r
a
C
e
t
s
a
P

l

t
e
e
m
O
k
o
o
C

s
e
i
r
F
h
c
n
e
r
F
e
k
a
M

i

i

e
n
h
c
a
M
g
n
d
n
e
V
e
s
U

p
h
C
y
r
o
m
e
M
e
c
a
p
e
R

i

l

e
g
d
i
r
t
r
a
C
r
e
n
o
T
e
g
n
a
h
C

r
e
h
s
i
u
g
n
i
t
x
E
e
r
i
F
e
t
a
r
e
p
O

i

e
h
t
o
o
m
S
y
r
r
e
b
w
a
e
r
t
S
e
k
a
M

l

t
n
a
p
s
n
a
r
T

l

d
a
a
S
e
k
a
M

g
n
i
t
n
i
r
P
o
n
L
o
D

i

l

J

e
b
a
C
5
4
R
e
k
a
M

k
s
i
D
d
r
a
H
e
c
a
p
e
R

l

d
a
e
H
r
e
w
o
h
S

r
e
g
r
u
B
e
k
a
M

a
z
z
i
P
e
k
a
M

r
e
t
h
g
L
A

i

l

s
s
a
G
r
o
r
r
i

l
l
i
f
e
R

i

d
a
e
H
r
e
p
W
A
e
c
a
p
e
R

n
r
e
t
n
a
L
e
s
e
n
h
C
e
k
a
M

s
g
n
i
r
t
S
r
a
t
i
u
G
e
g
n
a
h
C

l

i

l
l

a
t
s
n
I

R
P
C
m
r
o
f
r
e
P

t
i
u
r
F
e
p
a
r
G
t
u
C

l

l

e
c
n
a
a
B
a
c
i
t
y
l
a
n
A
e
s
U

f

h
c
t
a
W
O
y
r
e
t
t
a
B
e
g
n
a
h
C

r
i
a
H
h
s
a
W

p
a
o
S
e
k
a
M

l
i

l

s
e
k
c
i
P
e
k
a
M

a
t
k
c
o
C
e
k
a
M

d
r
a
C
M
I
S
e
c
a
p
e
R

l

m
e
t
S
e
v
l
a
V
e
r
y
T
e
c
a
p
e
R

l

e
g
n
e

l
l

a
h
C
s
l
l
i

h
c
i
w
d
n
a
S
e
k
a
M

t
e
k
c
o
S
t
h
g
L
e
c
a
p
e
R

l

i

k
S
A
B
N
d
n
e
t
t
A

M
w
e
i
v
r
a
e
R
e
c
a
p
e
R

l

j

l

n
o
i
t
c
e
n
I
r
a
u
c
s
u
m
a
r
t
n
I
n
A
e
v
i
G

r
a
C
h
s
i
l

o
P

o
g
n
a
M
t
u
C

e

i

s
e
r
i
T
e
k
B
e
g
n
a
h
C

l
i

T
c
i
m
a
r
e
C

l
l

a
t
s
n
I

e
b
u
T
r
e
n
n
I
e
k
B
h
c
t
a
P

i

t
n
e
T
A
h
c
t
i
P

l

g
n
i
r
o
o
F
d
o
o
W

e
r
i
T
r
a
C
e
g
n
a
h
C

l
l

a
t
s
n
I

s
e
h
t
o
C
n
o
r
I

l

t
u
n
t
s
e
h
C
t
s
a
o
R

l

r
a
C
e
u
F

b
u
t
h
t
a
B
n
a
e
C

l

e
g
a
C
r
e
t
s
m
a
H
n
a
e
C

l

m
a
e
r
C
e
c
I
e
d
a
m
e
m
o
H
e
k
a
M

i

l

a
f
o
S
e
b
m
e
s
s
A

n
k
p
m
u
P
e
v
r
a
C

p
m

i
r
h
S
n
a
e
C

l

t
e
c
u
a
F
e
c
a
p
e
R

l

r
e
p
a
p

l
l

a
W
g
n
a
H

i

i

n
a
h
C
e
k
B
e
g
n
a
h
C

l

r
o
o
F
n
e
d
o
o
W
n
a
e
C

l

i

e
t
t
e
p
P
c
i
r
t
e
m
u
o
V
e
s
U

l

k
c
i
r
T
e
p
o
R
e
r
o
t
s
e
R
d
n
A
t
u
C

s
t
e
k
s
a
B
r
e
t
s
a
E
r
e
p
a
P
e
k
a
M

r
o
t
c
e
t
o
r
P
n
e
e
r
c
S
d
a
p
I
l
l

a
t
s
n
I

d
e
B
e
k
a
M

x
o
B
t
f
i

G
p
a
r
W

l

r
e
p
a
t
S
A

l
l
i
f
e
R

d
n
a
B
h
c
t
a
W
e
z
i
s
e
R

b
a
b
e
K
b
m
a
L
e
k
a
M

i

e
n
h
c
a
M
g
n
w
e
S
e
s
U

i

t
e

l
i

o
T
n
a
e
C

l

l

i

n
e
P
n
a
t
n
u
o
F

d
e
B
e
b
m
e
s
s
A

l
l
i
f
e
R

l

r
o
o
F
t
n
e
m
e
C
n
a
e
C

e
r
i
T
e
l
c
y
c
i
B
p
U
p
m
u
P

l

r
e
t
s
a
o
T
e
s
U

s
d
u
b
r
a
E
s
s
e
e
r
i

l

W
e
k
a
M

e
m
a
r
F
e
t
a
P
e
s
n
e
c
i
L

l

l
l

a
t
s
n
I

e
g
d
i
r
t
r
a
C

l
l
i
f
e
R

y
r
e
t
t
a
B
e
n
o
h
P
e

J

e
c
i
u
e
g
n
a
r
O
e
k
a
M

l
i

j

k
c
i
r
T
y
e
n
o
M
o
T
r
e
p
a
P
m
r
o
f
r
e
P

r
o
t
c
e
n
i
-
o
t
u
A
e
n
i
r
h
p
e
n
p
E
e
s
U

i

b
o
M
e
g
n
a
h
C

l

e
d
n
a
C
e
k
a
M

a
d
o
S
g
n
k
a
B
h
t
i

i

i

g
a
B
g
n
p
e
e
S
k
c
a
P

l

i

W
k
n
S
g
o
l
c
n
U

t
a
e
S
t
e

r
e
k
a
M
k

l
i

l
i

o
T
e
c
a
p
e
R

M
y
o
S
e
s
U

l

l

e
p
u
o
a
t
n
a
C
t
u
C

C
P
p
o
t
k
s
e
D
e
b
m
e
s
s
A

l

l

n
o
i
t
u
o
S
d
r
a
d
n
a
t
S
e
r
a
p
e
r
P

l

i

e
d
e
e
N
e
n
h
c
a
M
g
n
w
e
S
e
c
a
p
e
R

l

i

t
f
a
r
G

h
s
i
F
n
a
e
C

l

n
a
F
g
n

i
l
i

e
C

l

g
a
F
e
s
i
a
R

g
n

i
l
r
u
C
y
a
P

l

l
l

a
t
s
n
I

a
e
T
e
k
a
M

i
z
g
n
o
Z
p
a
r
W

e
e
f
f
o
C
e
k
a
M

l

s
g
u
p
r
a
E
e
s
U

k
c
o
D
o
T
t
a
o
B
e
T

i

d
a
e
H
e
g
a
d
n
a
B

t
a
e
S
r
e
h
t
a
e
L
n
a
e
C

l

l

k
s
a
F
c
i
r
t
e
m
u
o
V
e
s
U

l

e
e
r
T
t
n
a
P

r
a
g
u
S
w
o
B

l

l

l

e

l
l

a
r
a
P
k
r
a
P

J

k
c
a
e
s
U

l

d
o
o
B
w
a
r
D

l

d
e
W
c
r
A

g
o
D
h
s
a
W

l

e
o
H

l
l
i
r
D

d
a
e
h
m
u
r
D
e
c
a
p
e
R

l

d
r
a
e
B
e
v
a
h
S

l

e
t
a
o
c
o
h
C
e
k
a
M

i

e
k
o
o
C
e
k
a
M

s
u
m
m
u
H
e
k
a
M

l

t
e
l
t
u
O
a
c
i
r
t
c
e
E
e
c
a
p
e
R

l

l

t
o
P
i
t
e
N
e
s
U

i

t
e
n
b
a
C
e
b
m
e
s
s
A

l

l

s
l
i
c
n
e
P
a
c
i
n
a
h
c
e
M

l
l
i
f
e
R

r
a
C
o
T
y
e
K
n
O
y
r
e
t
t
a
B
e
c
a
p
e
R

l

o
a
i
t
u
o
Y
e
k
a
M

e
s
u
F
r
a
C
e
c
a
p
e
R

l

d
r
a
C
s
c
i
h
p
a
r
G
e
c
a
p
e
R

d
r
a
o
b
y
e
K
p
o
t
p
a
L
n
a
e
C

l

l

i

w
o
d
n
W
r
a
C
e
c
a
p
e
R

l

s
l
l

a
W
m
o
r
F
n
o
y
a
r
C
e
v
o
m
e
R

l

l

i

o
r
t
n
o
C
V
T
n
O
y
r
e
t
t
a
B
e
c
a
p
e
R

k
c
i
r
T
s
s
a
G
g
n
h
s
i
n
a
V
m
r
o
f
r
e
P

l

h
s
i
D
h
s
a
W

w
a
P
g
o
D
e
g
a
d
n
a
B

n
u
G
e
s
a
e
r
G
d
a
o
L

s
s
e
r
P
r
e
w
o
F
e
k
a
M

l

l

e
c
n
a
a
B
m
a
e
B
e
p
i
r
T
e
s
U

l

r
e
m
m
a
H
w
o
r
h
T

r
e
b
b
u
R
g
n
o
P
g
n
P
e
u
G

l

i

s
e
h
c
t
a
r
c
S
n
e
e
r
c
S
p
o
t
p
a
L
x
F

i

l

s
a
v
n
a
C
e
r
a
p
e
r
P

t
l
u
a
V
e
o
P
e
s
i
t
c
a
r
P

k
n
I
i

m
u
S
e
r
a
p
e
r
P

r
e
v
o
C
t
l
i

u
Q
n
O
t
u
P

l

r
e
n
o
i
t
i
d
n
o
C
r
i
A

l
l

o
o
t
S
e
s
o
C

l

l
l

a
t
s
n
I

i

n
a
t
r
u
C

l
l

a
t
s
n
I

k
c
o
L
A
e
t
a
c
i
r
b
u
L

e
c
i
D
r
e
p
a
P
e
k
a
M

a
t
s
n
I

g
o
D
A
h
t
i

W
e
e
b
s
i
r
F
y
a
P

l

s
w
a
H
d
e
t
a
o
C
r
a
g
u
S
e
k
a
M

y
r
e
t
t
a
B
p
a
r
w
e
R

e
g
a
C
r
e
t
s
m
a
H
A
p
U
t
e
S

s
l
a
i
r
e
A
g
n

i
i

c
i
l
r
a
G
h
s
a
m
S

k
S
e
s
i
t
c
a
r
P

i

i

l

J

n
u
G
g
n
p
p
a
T
e
s
U

p
m
u
e
p
i
r
T
e
s
i
t
c
a
r
P

s
e
s
n
e
L
t
c
a
t
n
o
C
r
a
e
W

s
d
r
a
u
G
n
h
S
r
a
e
W

t
f
i
L
t
h
g
e
W
e
s
i
t
c
a
r
P

i

l

i

i

d
e
h
s
d
n
W
m
o
r
F
s
e
h
c
t
a
r
c
S
e
v
o
m
e
R

Figure 5. The sample distributions of all the tasks in COIN. The blue bars and the grey bars indicate the number of training and testing
videos in each class respectively.

p, ..., sn

p , ..., sN

Figure 6. Flowchart of our proposed task-consistency method.
During the ﬁrst bottom-up aggregation stage, the inputs are a se-
ries of scores Sp = {s1
p } of an instructional video,
which denotes the probabilities of each step appearing in the cor-
responding proposal. We ﬁrst aggregate them into a video-based
score sv, and map it into another score st to predict the task label
L. At top-down reﬁnement stage, we generate a reﬁned mask
vector vr based on the task label. Then we alleviate the weights of
other bits in Sp by vr to ensure the task-consistency. The reﬁned
scores Sr are ﬁnally utilized to perform NMS process and output
the ﬁnal results.

for each task. We split the COIN into 9030 and 2797 video
samples for training and testing respectively. Figure 4 dis-
plays the duration distribution of videos and segments. The
averaged length of a video is 2.36 minutes. Each video is
labelled with 3.91 step segments, where each segment last-
s 14.91 seconds on average. In total, the dataset contains
videos of 476 hours, with 46,354 annotated segments.

4. Task-Consistency Analysis

Given an instructional video, one important real-world
application is to localize a series of steps to complete the
corresponding task.
In this section, we introduce a new
proposed task-consistency method for step localization in
instructional videos. Our method is motivated by the intrin-
sic dependencies of different steps which are associated to
a certain task. For example, it is unlikely for the steps of
“dig a pit of proper size” and “soak the strips into water”
to occur in the same video, because they belong to different
tasks of “plant tree” and “make french fries” respectively.
In another word, the steps in the same video should be task-
consistent to ensure that they belong to the same task. Fig-
ure 6 presents the ﬂowchart of our task-consistency method,
which contains two stages: (1) bottom-up aggregation and
(2) top-down reﬁnement.

Bottom-up aggregation: As our method is built up-
on the proposal-based action detection methods, we start
with training an existing action detector, e.g. SSN [50], on
our COIN dataset. During inference phase, given an input
video, we send it into the action detector to produce a series
of proposals with their corresponding locations and predict-
ed scores. These scores indicate the probabilities of each
step occuring in the corresponding proposal. We denote
p ∈ RK repre-
them as Sp = {s1
sents the score of the n − th proposal and K is the number
of the total steps. The goal of the bottom-up aggregation
stage is to predict the task labels based on these proposal

p }, where sn

p , ..., sN

p, ..., sn

1211

Table 2. Comparisons of the step localization accuracy (%) on the COIN dataset.

mAP @ α

mAR @ α

Method

0.1

0.2

0.3

0.4

Random
R-C3D [45]
SSN-RGB [50]
SSN-Flow [50]
SSN-Fusion [50]
R-C3D+TC
SSN+TC-RGB
SSN+TC-Flow
SSN+TC-Fusion

0.03
9.85
19.39
11.23
20.00
10.32
20.15
12.11
20.01

0.03
7.78
15.61
9.57
16.09
8.25
16.79
10.29
16.44

0.02
5.80
12.68
7.84
13.12
6.20
14.24
8.63
13.83

0.01
4.23
9.97
6.31
10.35
4.54
11.74
7.03
11.29

0.5

0.01
2.82
7.79
4.94
8.12
3.08
9.33
5.52
9.05

0.1

0.2

0.3

0.4

0.5

2.57
36.82
50.33
33.78
51.04
39.25
54.05
37.24
54.64

1.79
31.55
43.42
29.47
43.91
34.22
47.31
32.52
47.69

1.36
26.56
37.12
25.62
37.74
29.09
40.99
28.50
41.46

0.90
21.42
31.53
21.98
32.06
23.71
35.11
24.46
35.59

0.50
17.07
26.29
18.20
26.79
19.24
29.17
20.58
29.79

l the proposals as sv = PN

scores. To this end, we ﬁrst aggregate the scores along al-
p , where sv indicates the
probability of each step appearing in the video. Then we
construct a binary matrix W with the size of K × M to
model the relationship between the K steps and M tasks:

n=1 sn

wij =(1,

0,

if step i belongs to task j

otherwise

(1)

Having obtained the step-based score sv and the binary
matrix W , we calculate a task-based score as st = sv ∗ W .
This operation is essential to combine the scores of steps
belonging to same tasks. We choose the index L with the
max value in the st as the task label of the entire video.

Top-down reﬁnement: The target of the top-down re-
ﬁnement stage is to reﬁne the original proposal scores with
the guidance of the task label. We ﬁrst select the L − th
row in W as a mask vector v, based on which we deﬁne a
reﬁned vector as:

vr = v + γ(I − v).

(2)

Here I is an vector where all the elements equal to 1. γ
is an attenuation coefﬁcient to alleviate the weights of the
steps which do not belong to the task L. We empirically set
γ to be e−2 in this paper. Then, we employ the vr to mask
the original scores sn

p as follow:
sn
r = sn

p ⊙ vr,

(3)

r, ..., sn

where ⊙ is the element-wise Hadamard product. We com-
pute a sequence of scores as Sr = {s1
r }.
Based on these reﬁned scores and their locations, we em-
ploy a Non-Maximum Suppression (NMS) strategy to ob-
tain the results of step localization. In summary, we ﬁrst
predict the task label through the bottom-up scheme, and
reﬁne the proposal scores by the top-down strategy, hence
the task-consistency is guaranteed.

r , ..., sN

5. Experiments

In order to provide a benchmark for our COIN dataset,
we evaluate various approaches under two different set-

tings: step localization and action segmentation. We also
conduct experiments on our task-consistency method under
the ﬁrst setting. The following describes the details of our
experiments and results.

5.1. Evaluation on Step Localization

Implementation Details: In this task, we aim to localize
a series of steps and recognize their corresponding labels
given an instructional video. We mainly evaluate the fol-
lowing approaches: (1) Random. We uniformly segment-
ed the video into three intervals, and randomly assigned
the label to each interval. (2) R-C3D [45] and SSN [50].
These two methods are state-of-the-arts for action detection,
which output the same type of results (interval and label for
each action instance) with step localization. For R-C3D,
our implementation was built upon the codebase [3]. Fol-
lowing [45], we extracted the RGB frames of each video as
the inputs, and it took around 3.5 days to train the model
on a GTX 1080Ti GPU. For SSN, we used the PyTorch im-
plementation [4]. The reported results are based on the in-
puts of different modalities as: SSN−RGB, SSN−F low and
SSN−F usion. Here SSN−F low adopted the optical ﬂows
calculated by [48], and SSN−F usion combined the predict-
ed scores of SSN−RGB and SSN−F low.
(3) R-C3D+TC
and SSN+TC. In order to demonstrate the advantages of the
proposed method to explore the task-consistency in instruc-
tional videos, we further conducted experiments on apply-
ing our approach to R-C3D and SSN respectively.

Evaluation Metrics: As the results of step localization
contain time intervals, labels and conﬁdence scores, we em-
ployed Intersection over Union (IoU) as a basic metric to
determine whether a detected interval is positive or not. The
IoU is deﬁned as |G ∩ D|/|G ∪ D|, where G denotes the
ground truth action interval and D denotes the detected ac-
tion interval. We followed [12] to calculate Mean Average
Precision (mAP) and Mean Average Recall (mAR). The re-
sults are reported under the IoU threshold α ranging from
0.1 to 0.5.

Results: Table 2 presents the compared experimental re-

1212

(cid:82)(cid:79)(cid:84)(cid:75)(cid:3)(cid:91)(cid:86)(cid:3)(cid:71)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)(cid:3)

(cid:86)(cid:88)(cid:85)(cid:90)(cid:75)(cid:73)(cid:90)(cid:85)(cid:88)(cid:3)(cid:93)(cid:79)(cid:90)(cid:78)(cid:3)(cid:73)(cid:75)(cid:82)(cid:82)(cid:86)(cid:78)(cid:85)(cid:84)(cid:75)

(cid:86)(cid:71)(cid:89)(cid:90)(cid:75)(cid:3)(cid:86)(cid:88)(cid:85)(cid:90)(cid:75)(cid:73)(cid:90)(cid:85)(cid:88)
(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

(cid:85)(cid:86)(cid:75)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:82)(cid:85)(cid:90)(cid:3)(cid:85)(cid:76)

(cid:57)(cid:47)(cid:51)(cid:3)(cid:73)(cid:71)(cid:88)(cid:74)

(cid:86)(cid:71)(cid:89)(cid:90)(cid:75)(cid:3)(cid:86)(cid:88)(cid:85)(cid:90)(cid:75)(cid:73)(cid:90)(cid:85)(cid:88)
(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

M

(cid:86)(cid:71)(cid:89)(cid:90)(cid:75)(cid:3)(cid:86)(cid:88)(cid:85)(cid:90)(cid:75)(cid:73)(cid:90)(cid:85)(cid:88)
(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

(cid:86)(cid:82)(cid:71)(cid:73)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:82)(cid:71)(cid:72)(cid:75)(cid:82)

(cid:86)(cid:71)(cid:89)(cid:90)(cid:75)(cid:3)(cid:86)(cid:88)(cid:85)(cid:90)(cid:75)(cid:73)(cid:90)(cid:85)(cid:88)
(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

(cid:86)(cid:71)(cid:89)(cid:90)(cid:75)(cid:3)(cid:86)(cid:88)(cid:85)(cid:90)(cid:75)(cid:73)(cid:90)(cid:85)(cid:88)
(cid:44)(cid:79)(cid:82)(cid:83)(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)
(cid:3)(cid:85)(cid:84)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

(cid:93)(cid:79)(cid:86)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

(cid:93)(cid:79)(cid:86)(cid:75)(cid:3)(cid:90)(cid:78)(cid:75)(cid:3)(cid:89)(cid:73)(cid:88)(cid:75)(cid:75)(cid:84)

(cid:29411)(cid:29416)(cid:29417)

(cid:29412)(cid:29414)(cid:29419)

(cid:29415)(cid:29410)(cid:29417)

(cid:29415)(cid:29414)(cid:29415)

(cid:29416)(cid:29415)(cid:29413)

(cid:29416)(cid:29415)(cid:29415)

(cid:29417)(cid:29413)(cid:29410)

(cid:29418)(cid:29415)(cid:29411)

(cid:29418)(cid:29419)(cid:29415)

(cid:29411)(cid:29410)(cid:29412)(cid:29412)

(cid:29411)(cid:29410)(cid:29413)(cid:29415)

(cid:29411)(cid:29410)(cid:29417)(cid:29412)

(cid:29411)(cid:29410)(cid:29419)(cid:29410) (cid:29411)(cid:29411)(cid:29413)(cid:29415)

(cid:29411)(cid:29411)(cid:29417)(cid:29418)

(cid:29411)(cid:29411)(cid:29419)(cid:29411)

(cid:29411)(cid:29412)(cid:29413)(cid:29415)

(cid:29411)(cid:29415)(cid:29416)(cid:29418) (cid:29411)(cid:29416)(cid:29411)(cid:29415)

(cid:29411)(cid:29417)(cid:29416)(cid:29411)

SSN

-

Fusion

SSN+TC
-

Fusion

Groundtruth

(cid:44)(cid:88)(cid:71)(cid:83)(cid:75)

Figure 7. Visualization of step localization results. The video is associated with the task “paste screen protector on Pad”.

sults, which reveal great challenges to performing step lo-
calization on the COIN dataset. Even for the state-of-the-
art method SSN−F usion, it only attains the results of 8.12%
and 26.79% on mAP@0.5 and mAR@0.5 respectively. Be-
sides, we observe that R-C3D+TC and SSN+TC consistent-
ly improve the performance over the original models, which
illustrates the effectiveness of our proposed method to cap-
ture the dependencies among different steps.

We show the visualization results of different methods
and ground-truth in Figure 7. We analyze an instruction-
al video of the task “paste screen protector on Pad”. When
applying our task-consistency method, we can discard those
steps which do not belong to this task, e.g., “line up a screen
protector with cellphone” and “open the slot of SIM card”,
hence more accurate step labels can be obtained. More vi-
sualization results are presented in supplementary material.

5.2. Evaluation on Action Segmentation

Implementation Details: The goal of this task is to as-
sign each video frame with a step label. We present the re-
sults on three types of approaches as follows. (1) Random.
We randomly assigned a step label to each frame. (2) Fully-
supervised method. We used VGG16 network pretrained
on ImageNet, and ﬁnetuned it on the training set of COIN
to predict the frame-level label. (3) Weakly-supervised ap-
proaches.
In this setting, we evaluated recent proposed
Action-Sets [32], NN-Viterbi [33] and TCFPN-ISBA [17]
without temporal supervision. For Action-Sets, only a set
of steps within a video is given, while the occurring order of
steps are also provided for NN-Viterbi and TCFPN-ISBA.
We used frames or their representations sampled at 10fps as
input. We followed the default train and inference pipeline
of Action-Sets [1], NN-Viterbi [2] and TCFPN-ISBA [5].
However, these methods use frame-wise ﬁsher vector as
video representation, which comes with huge computation
and storage cost on the COIN dataset6. To address this, we
employed a bidirectional LSTM on the top of a VGG16 net-
work to extract dynamic feature of a video sequence [18].

Evaluation Metrics: We adopted frame-wise accuracy

6 The calculation of ﬁsher vector is based on the improved Dense Tra-
jectory (iDT) representation [43], which requires huge computation cost
and storage space.

Table 3. Comparisons of the action segmentation accuracy (%) on the
COIN dataset.

Method
Random
CNN [37]
Action-Sets [32]
NN-Viterbi [33]
TCFPN-ISBA [17]

Frame Acc.

0.13
25.79
4.94
21.17
34.30

Setting

-

fully-supervised

weakly-supervised
weakly-supervised
weakly-supervised

(FA), which is a common benchmarking metric for action
segmentation. It is computed by ﬁrst counting the number
of correctly predicted frames, and dividing it by the number
of total video frames.

Results: Table 3 shows the experimental results of ac-
tion segmentation on the COIN dataset. Given the weakest
supervision of video transcripts without ordering constraint,
Action-Sets [32] achieves the result of 4.94% frame accura-
cy. When taking into account the ordering information, NN-
Viterbi [33] and TCFPN-ISBA [17] outperform Action-Sets
with a large margin of 16.23% and 29.66% respectively. As
a fully-supervised method, CNN [37] reaches an accura-
cy 25.79%, which is much higher than Action-Sets. This
is because CNN utilizes the label of each frame to perfor-
m classiﬁcation and the supervision is much stronger than
Action-Sets. However, as the temporal information and or-
dering constraints are ignored, the result of CNN is inferior
to TCFPN-ISBA.

5.3. Discussion

What are the hardest and easiest domains for instruc-
tional video analysis? In order to provide a more in-depth
analysis of the COIN dataset, we report the performance of
SSN+TC−F usion among the 12 domains of COIN. Table 4
presents the comparison results, where the domain “sport-
s” achieves the highest mAP of 30.20%, This is because
the differences between the “sports” steps are more clear,
thus they are easier to be identiﬁed. In contrast, the results
of “gadgets” and “science & craft” are relatively low. The
reason is that the steps in these two domains usually have
higher similarity with each other. For example, the step “re-
move the tape of the old battery” is similar with the step
“take down the old battery”. Hence it is harder to localize

1213

Table 4. Comparisons of the step localization accuracy (%) over 12
domains on the COIN dataset. We report
the results obtained by
SSN+TC−F usion with α = 0.1.

Domain

nursing & caring
science & craft

leisure & performance

snacks & drinks
plants & fruits
household items

mAP
22.92
16.59
24.32
19.79
22.71
19.07

Domain
vehicles

electric appliances

gadgets
dishes
sports

housework

mAP
19.07
19.86
17.99
23.76
30.20
20.70

Table 5. Comparisons of the step localization accuracy (%) on the Break-
fast dataset. The results are all based on the combination scores of RGB
frames and optical ﬂows.

Metrics
Threshold
SSN [50]
SSN+TC

mAP
0.3

22.55
22.73

0.1

28.24
28.25

0.5

15.84
16.39

0.1

54.86
55.51

mAR

0.3

45.84
47.37

0.5

35.51
36.20

Table 6. Comparisons of the proposal localization accuracy (%) with Y-
ouCook2 dataset [52]. The results are obtained by temporal actionness
grouping (TAG) method [50] with α = 0.5.

YouCook2 COIN

mAP

40.16

39.67 mAR

YouCook2 COIN
56.16

54.12

the steps in these two domains. We also show the com-
pared performance across different tasks in the supplemen-
tary material.

Can the proposed task-consistency method be applied
to other instructional video datasets? In order to demon-
strate the effectiveness of our proposed method, we fur-
ther conduct experiments on another dataset called “Break-
fast" [28], which is also widely-used for instructional video
analysis. The Breakfast dataset contains over 1.9k videos
with 77 hours of 4 million frames. Each video is labelled
with a subset of 48 cooking-related action categories. Fol-
lowing the default setting, we set split 1 as testing set and
the other splits as training set. Similar to COIN, we employ
SSN [50], which is a state-of-the-art method for action de-
tection, as a baseline method under the setting of step local-
ization. As shown in Table 5, our proposed task-consistency
method improves the performance of the baseline model,
which further shows its advantages to model the dependen-
cies of different steps in instructional videos.

Comparison of state-of-the-art performance on exist-
ing datasets for video analysis. In order to assess the d-
ifﬁculty of COIN, we report the performance on different
tasks compared with other datasets. For proposal localiza-
tion, which is a task deﬁned in [52] for instructional video
analysis, we evaluated COIN and Youcook2 [52] based on
temporal actionness grouping (TAG) approach [50]. From
the results in Table 6, we observe that these two datasets are
almost equally challenging on this task. For video classi-
ﬁcation on COIN, we present the recognition accuracy of
180 tasks, which refer to the second level of the lexicon.

Table 7. Comparisons of the performance (%) on different datasets. The
video classiﬁcation task is evaluated by temporal segment networks (TSN)
model [44], while the action detection task is tested on stuctured segment
networks (SSN) method [50] with α = 0.5.

Video Classiﬁcation

Dataset
UCF101 [39]
ActivityNet v1.3 [23]
Kinectics [11]
COIN

Acc.
97.00
88.30
73.90
88.02

Action Detection / Step Localization
Dataset
THUMOS14 [26]
ActivityNet v1.3 [23]
Breakfast [28]
COIN

mAP
29.10
28.30
15.84
8.12

We employed the temporal segment network (TSN) mod-
el [6, 44], which is a state-of-the-art method for video clas-
siﬁcation. As shown in the Table 7, the classiﬁcation accu-
racy on COIN is 88.02%, suggesting its general difﬁculty
in comparison with other datasets. For action detection or
step localization, we display the compared performances of
structured segment networks (SSN) approach [50] on COIN
and the other three datasets. The THUMOS14 [26] and
ActivityNet [23] are conventional datasets for action detec-
tion, on which the detection accuracies are relatively higher.
The Breakfast [28] and COIN contain instructional videos
with more difﬁculty. Hence, the performance on these two
datasets are lower. Especially for our COIN, the results of
mAP@0.5 is only 8.12%. We attribute the low performance
to two aspects: (1) The step intervals are usually short-
er than action instances, which brings more challenges for
temporal localization; (2) Some steps in the same tasks are
similar, which carry ambiguous information for the recog-
nition process. These two phenomena are also common in
real-world scenarios, and future works are encouraged to
address these two issues.

6. Conclusions

In this paper we have introduced COIN, a new large-
scale dataset for comprehensive instructional video anal-
ysis. Organized in a rich semantic taxonomy, the COIN
dataset covers boarder domains and contains more tasks
than existing instructional video datasets. In addition, we
have proposed a task-consistency method to explore the re-
lationship among different steps of a speciﬁc task. In or-
der to establish a benchmark, we have evaluated various
approaches under different scenarios on the COIN. The ex-
perimental results have shown the great challenges on the
COIN and the effectiveness of our proposed method.

Acknowledgement

This work was supported in part by the National Nat-
ural Science Foundation of China under Grant U1813218,
Grant 61822603, Grant U1713214, Grant 61672306, Grant
61572271 and Meitu Cloud Vision Team. The authors
would like to thank Yongxiang Lian, Jiayun Wang, Yao Li,
Jiali Sun and Chang Liu for their generous help.

1214

References

[1] Codebase of Action-Sets.

https://github.com/

alexanderrichard/action-sets. 7

[2] Codebase of NN-Viterbi.

https://github.com/
alexanderrichard/NeuralNetwork-Viterbi. 7
https://github.com/

[3] Codebase

of R-C3D.

VisionLearningGroup/R-C3D. 6

[4] Codebase of SSN. https://github.com/yjxiong/

action-detection. 6

[5] Codebase of TCFPN-ISBA. https://github.com/

Zephyr-D/TCFPN-ISBA. 7

[6] Codebase of TSN. https://github.com/yjxiong/

temporal-segment-networks. 8

[7] Howcast. https://www.howcast.com. 3, 4
[8] Howdini. https://www.howdini.com. 3, 4
[9] Wikihow. https://www.wikihow.com. 3, 4

[10] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,
Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu-
pervised learning from narrated instruction videos. In CVPR,
pages 4575–4583, 2016. 1, 2, 3, 4

[11] João Carreira and Andrew Zisserman. Quo vadis, action
recognition? A new model and the kinetics dataset. In CVPR,
pages 4724–4733, 2017. 8

[12] Liu Chunhui, Hu Yueyu, Li Yanghao, Song Sijie, and Liu
Jiaying. PKU-MMD: A large scale benchmark for continu-
ous multi-modal human action understanding. In ACM MM
workshop, 2017. 6

[13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and
Michael Wray. Scaling egocentric vision: The epic-kitchens
dataset. In ECCV, 2018. 2

[14] Pradipto Das, Chenliang Xu, Richard F. Doell, and Jason J.
Corso. A thousand frames in just a few words: Lingual de-
scription of videos through latent topics and sparse object
stitching. In CVPR, pages 2634–2641, 2013. 1, 2

[15] Sandra Eliza Fontes de Avila, Ana Paula Brandão Lopes,
Antonio da Luz Jr., and Arnaldo de Albuquerque Araújo. V-
SUMM: A mechanism designed to produce static video sum-
maries and a novel evaluation method. Pattern Recognition
Letters, 32(1):56–68, 2011. 2

[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248–255, 2009. 3

[17] Li Ding and Chenliang Xu. Weakly-supervised action seg-
mentation with iterative soft boundary assignment. In CVPR,
pages 6508–6516, 2018. 3, 7

[18] Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Sub-
hashini Venugopalan, Sergio Guadarrama, Kate Saenko, and
Trevor Darrell. Long-term recurrent convolutional networks
for visual recognition and description. TPAMI, 39(4):677–
691, 2017. 7

[19] Ehsan Elhamifar, Guillermo Sapiro, and S. Shankar Sas-
try. Dissimilarity-based sparse subset selection. TPAMI,
38(11):2182–2197, 2016. 1

[20] C. Fellbaum. Wordnet: An electronic lexical database. Brad-

ford Books, 1998. 3

[21] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Car-
oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
George Toderici, Susanna Ricco, Rahul Sukthankar, Cordeli-
a Schmid, and Jitendra Malik. Ava: A video dataset of
spatio-temporally localized atomic visual actions. In CVPR,
pages 6047–6056, 2018. 2

[22] Michael Gygli, Helmut Grabner, Hayko Riemenschneider,
and Luc J. Van Gool. Creating summaries from user videos.
In ECCV, pages 505–520, 2014. 2

[23] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding.
In CVPR,
pages 961–970, 2015. 2, 3, 8

[24] De-An Huang, Joseph J. Lim, Li Fei-Fei, and Juan Carlos
Niebles. Unsupervised visual-linguistic reference resolution
in instructional videos. In CVPR, pages 1032–1041, 2017. 1
[25] De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg,
Li Fei-Fei, and Juan Carlos Niebles. Finding "it": Weakly-
supervised reference-aware visual grounding in instructional
videos. In CVPR, pages 5948–5957, 2018. 2

[26] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 2, 8

[27] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
ICCV, pages 706–715, 2017. 1, 2, 4

[28] Hilde Kuehne, Ali Bilgin Arslan, and Thomas Serre. The
language of actions: Recovering the syntax and semantics
of goal-directed human activities. In CVPR, pages 780–787,
2014. 1, 2, 3, 8

[29] U.S. Department of Labor. American time use survey. 2013.

4

[30] Rameswar Panda, Niluthpol Chowdhury Mithun,

and
Amit K. Roy-Chowdhury. Diversity-aware multi-video sum-
marization. TIP, 26(10):4712–4724, 2017. 1, 2

[31] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In NIPS, pages 91–99, 2015. 3

[32] Alexander Richard, Hilde Kuehne, and Juergen Gall. Action
sets: Weakly supervised action segmentation without order-
ing constraints. In CVPR, pages 5987–5996, 2018. 3, 7

[33] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juergen
Gall. Neuralnetwork-viterbi: A framework for weakly su-
pervised video learning. In CVPR, pages 7386–7395, 2018.
3, 7

[34] Nadolski RJ, Kirschner PA, and van Merriënboer JJ. Op-
timizing the number of steps in learning tasks for com-
plex skills. British Journal of Educational Psychology,
75(2):223– 237, 2005. 1

[35] Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka,
and Bernt Schiele. A database for ﬁne grained activity detec-
tion of cooking activities. In CVPR, pages 1194–1201, 2012.
1, 2

[36] Ozan Sener, Amir R. Zamir, Silvio Savarese, and Ashutosh
Saxena. Unsupervised semantic parsing of video collections.
In ICCV, pages 4480–4488, 2015. 1, 3

1215

[37] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, pages
1–14, 2015. 7

[38] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro
In

Jaimes. Tvsum: Summarizing web videos using titles.
CVPR, pages 5179–5187, 2015. 2

[39] K. Soomro, A. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. Techni-
cal Report CRCV-TR-12-01, University of Central Florida,
2012. 8

[40] Sebastian Stein and Stephen J. McKenna. Combining em-
bedded accelerometers with computer vision for recogniz-
ing food preparation activities. In UbiComp, pages 729–738,
2013. 1, 2

[41] Sam Toyer, Anoop Cherian, Tengda Han, and Stephen
Gould. Human pose forecasting via deep markov models.
In DICTA ,2017, pages 1–8, 2017. 1, 2

[42] Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre-
sani, and Manohar Paluri. Learning spatiotemporal features
with 3d convolutional networks. In ICCV, pages 4489–4497,
2015. 3

[43] Heng Wang and Cordelia Schmid. Action recognition with

improved trajectories. In ICCV, pages 3551–3558, 2013. 7

[44] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Val Gool. Temporal segment net-
works: Towards good practices for deep action recognition.
In ECCV, 2016. 8

[45] Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: region
convolutional 3d network for temporal activity detection. In
ICCV, pages 5794–5803, 2017. 1, 3, 6

[46] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A
large video description dataset for bridging video and lan-
guage. In CVPR, pages 5288–5296, 2016. 2

[47] Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian
Zhang, and Xiaokang Yang. Fine-grained video captioning
for sports narrative. In CVPR, pages 6066–6015, June 2018.
1, 2

[48] Christopher Zach, Thomas Pock, and Horst Bischof. A dual-
ity based approach for realtime tv-l1 optical ﬂow. In DAGM,
pages 214–223, 2007. 6

[49] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman.
Video summarization with long short-term memory. In EC-
CV, pages 766–782, 2016. 1

[50] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-
aoou Tang, and Dahua Lin. Temporal action detection with
structured segment networks.
In ICCV, pages 2933–2942,
2017. 1, 3, 5, 6, 8

[51] Luowei Zhou, Nathan Louis, and Jason J. Corso. Weakly-
supervised video object grounding from text by loss weight-
ing and object interaction. In BMVC, page 50, 2018. 2

[52] Luowei Zhou, Chenliang Xu, and Jason J. Corso. Toward-
s automatic learning of procedures from web instructional
videos. In AAAI, pages 7590–7598, 2018. 1, 2, 3, 4, 8

[53] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,
and Caiming Xiong. End-to-end dense video captioning with
masked transformer. In CVPR, pages 8739–8748, 2018. 1, 2

1216

