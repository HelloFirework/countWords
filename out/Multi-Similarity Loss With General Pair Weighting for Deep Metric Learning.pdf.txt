Multi-Similarity Loss with General Pair Weighting

for Deep Metric Learning

Xun Wang, Xintong Han, Weiling Huang∗, Dengke Dong, Matthew R. Scott

Malong Technologies, Shenzhen, China

Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China

{xunwang,xinhan,whuang,dongdk,mscott}@malong.com

Abstract

A family of loss functions built on pair-based computa-
tion have been proposed in the literature which provide a
myriad of solutions for deep metric learning.
In this pa-
per, we provide a general weighting framework for under-
standing recent pair-based loss functions. Our contribu-
tions are three-fold: (1) we establish a General Pair Weight-
ing (GPW) framework, which casts the sampling problem of
deep metric learning into a uniﬁed view of pair weighting
through gradient analysis, providing a powerful tool for un-
derstanding recent pair-based loss functions; (2) we show
that with GPW, various existing pair-based methods can be
compared and discussed comprehensively, with clear differ-
ences and key limitations identiﬁed; (3) we propose a new
loss called multi-similarity loss (MS loss) under the GPW,
which is implemented in two iterative steps (i.e., mining and
weighting). This allows it to fully consider three similarities
for pair weighting, providing a more principled approach
for collecting and weighting informative pairs. Finally, the
proposed MS loss obtains new state-of-the-art performance
on four image retrieval benchmarks, where it outperforms
the most recent approaches, such as ABE[14] and HTL
[4], by a large margin, e.g., 60.6% → 65.7% on CUB200,
and 80.9% → 88.0% on In-Shop Clothes Retrieval dataset
at Recall@1. Code is available at https://github.
com/MalongTech/research-ms-loss

1. Introduction

Metric learning aims to learn an embedding space, where
the embedded vectors of similar samples are encouraged to
be closer, while dissimilar ones are pushed apart from each
other [22, 23, 39]. With recent great success of deep neural
networks in computer vision, deep metric learning has at-
tracted increasing attention, and has been applied to various
tasks, including image retrieval [37, 8, 5], face recognition

Figure 1. Objective of the proposed multi-similarity loss, which
aims to collect informative pairs, and weight these pairs through
their own and relative similarities.

[36], zero-shot learning [42, 1, 15], visual tracking [19, 31]
and person re-identiﬁcation [41, 9].

Many recent deep metric learning approaches are built
on pairs of samples. Formally, their loss functions can be
expressed in terms of pairwise cosine similarities in the
embedding space1. We refer to this group of methods as
pair-based deep metric learning; and this family includes
contrastive loss [6], triplet loss [10], triplet-center loss [8],
quadruplet loss [18], lifted structure loss [25], N-pairs loss
[29], binomial deviance loss [40], histogram loss [32], an-
gular loss [34], distance weighted margin-based loss [38],
hierarchical triplet loss (HTL) [4], etc. For these pair-based
methods, training samples are constructed into pairs, triplets
or quadruplets, resulting a polynomial growth of training
pairs which are highly redundant and less informative. This
gives rise to a key issue for pair-based methods, where train-
ing with random sampling can be overwhelmed by redun-
dant pairs, leading to slow convergence and model degener-
ation with inferior performance.

Recent efforts have been devoted to improving sampling
schemes for pair-based metric learning techniques. For ex-

1For simplicity, we use a cosine similarity instead of Euclidean dis-

Corresponding author: whuang@malong.com

tance, by assuming an embedding vector is L2 normalized.

15022

ample, Chopra et al. [3] introduced a contrastive loss which
discards negative pairs whose similarities are smaller than a
given threshold. In triplet loss [10], a negative pair is sam-
pled by using a margin computed from the similarity of a
randomly selected positive pair. Alternatively, lifted struc-
ture loss [25] and N-pairs loss [29] introduced new weight-
ing schemes by designing a smooth weighting function to
assign a larger weight to a more informative pair. Though
driven by different motivations, these methods share a com-
mon goal of learning from more informative pairs. Thus,
sampling such informative pairs is the key to pair-based
deep metric learning, while precisely identifying these pairs
is particularly challenging, especially for the negative pairs
whose number is quadratic to the size of dataset.

In this work, we cast the sampling problem of deep met-
ric learning into a general pair weighting formulation. We
investigated various weighting schemes of recent pair-based
loss functions, attempted to understand their insights more
deeply, and identify key limitations of these approaches.
We observed that a key factor that impacts pair weight-
ing is to compute multiple types of similarities for a pair,
which can be deﬁned as self-similarity and relative similar-
ity, where the relative similarity is heavily dependent on the
other pairs. Furthermore, we found that most existing meth-
ods only explore this factor partially, which limits their ca-
pability considerably. For example, contrastive loss [6] and
binomial deviance loss [40] only consider the cosine sim-
ilarity of a pair, while triplet loss [10] and lifted structure
loss [25] mainly focus on the relative similarity. We pro-
pose a multi-similarity loss which fully considers multiple
similarities during sample weighting. The major contribu-
tions of this paper are summarized as follows.

– We establish a General Pair Weighting (GPW) frame-
work, which formulates deep metric learning into a
uniﬁed view of pair weighting. It provides a general
formulation for understanding and explaining various
pair-based loss functions through gradient analysis.

– We analyze the key factor that impacts pair weight-
ing with GPW, where various pair-based methods can
be analyzed comprehensively, with main differences
and key limitations clearly identiﬁed. This allows us
to deﬁne three types of similarities for a pair: a self-
similarity and two relative similarities. The relative
similarities are computed by comparing to other pairs,
which are of great importance to existing pair-based
methods.

– We propose a new multi-similarity (MS) loss, which is
implemented using two iterative steps with sampling
and weighting, as shown in Fig. 1. MS loss consid-
ers both self-similarity and relative similarities which
enables the model to collect and weight informative

pairs more efﬁciently and accurately, leading to boosts
in performance.

– MS loss is evaluated extensively on a number of bench-
marks for image retrieval, where it outperforms cur-
rent state-of-the-art approaches by a large margin, e.g.,
improving recent ABE [14] with +5.0% Recall@1 on
CUB200, and HTL [4] with +7.1% Recall@1 on In-
Shop Clothes Retrieval dataset.

2. Related Work

Classical pair-based loss functions. Siamese network
[6] is a representative pair-based method that learns an em-
bedding via contrastive loss. It encourages samples from a
positive pair to be closer, and pushes samples from a neg-
ative pair apart from each other, in the embedding space.
Triplet loss was introduced in [10] by using triplets as train-
ing samples. Each triplet consists of a positive pair and a
negative pair by sharing the same anchor point. Triplet loss
aims to learn an embedding space where the similarity of a
negative pair is lower than that of a positive one, by giving
a margin. Extended from triplet loss, quadruplets were also
applied in recent work, such as histogram loss [32].

Recently, Song et al. [25] argued that both contrastive
loss and triplet loss are difﬁcult to explore full pair-wise re-
lations between samples in a mini-batch. They proposed
a lifted structure loss attempted to fully utilize such pair-
wise relations. However, the lifted structure loss only sam-
ples approximately an equal number of negative pairs as
the positive ones randomly, and thus discards a large num-
ber of informative negative pairs arbitrarily. In [40], Dong
et al. proposed a binomial deviance loss by using a bino-
mial deviance to evaluate the cost between labels and sim-
ilarity, which emphasizes harder pairs.
In this work, we
propose a multi-similarity loss able to explore more mean-
ingful pair-wise relations by jointly considering both self-
similarity and the relative similarities.

Hard sample mining. Pair-based metric learning of-
ten generates a large amount of pair-wise samples, which
are highly redundant and include many uninformative sam-
ples. Training with random sampling can be overwhelmed
by these redundant samples, which signiﬁcantly degrade the
model capability and also slows the convergence. There-
fore, sampling plays a key role in pair-based metric learn-
ing.

The importance of hard negative mining has been dis-
cussed extensively [28, 7, 38, 4]. Schroff et al. [28] pro-
posed a semi-hard mining scheme by exploring semi-hard
triplets, which are deﬁned to have a negative pair farther
than the positive one. However, such semi-hard mining
method only generates a small number of valid semi-hard
triplets, so that it often requires a large batch-size to gener-
ate sufﬁcient semi-hard triplets, e.g., 1800 as suggested in

5023

[28]. Harwood et al. [7] provided a framework named smart
mining to collect hard samples from the whole dataset,
which suffers from off-line computation burden. Recently,
Ge et al. [4] proposed a hierarchical triplet loss (HTL)
which builds a hierarchical tree of all classes, where hard
negative pairs are collected via a dynamic margin. Sam-
pling matters in deep embedding learning was discussed in
[38], and a distance weighted sampling was proposed to col-
lect negative samples uniformly with respective to the pair-
wise distance. Unlike these methods which mainly focus on
sampling or hard sample mining, we provide a more gener-
alized formulation that casts sampling problem into general
pair weighting.

Instance weighting.

Instance weighting has been
widely applied to various tasks. For example, Lin et al.
[20] proposed a focal loss that allows the model to focus on
hard negative examples during training an object detector.
In [2], an active bias learning was developed to emphasize
high variance samples in training a neural network for clas-
siﬁcation. Self-paced learning [17], which pays more at-
tention on samples with a higher conﬁdence, was explored
to design noise-robust algorithms [12]. These approaches
[20, 13, 2, 17] were developed for weighting individual in-
stances that are only depended on itself (referred as self-
similarity), while our method aims to compute both self-
similarity and the relative similarities, which is a more com-
plicated problem that requires to measure multiple sample
correlations within a local data distribution.

3. General Pair Weighting (GPW)

In this section, we formulate the sampling problem of
metric learning into a uniﬁed weighting view, and provide
a general pair weighting (GPW) framework for analyzing
various pair-based loss functions.

3.1. GPW Framework

Let xi ∈ Rd be a real-value instance vector. Then we
have an instance matrix X ∈ Rm×d, and a label vector
y ∈ {1, 2, . . . , C}m for the m training samples respec-
tively. Then an instance xi is projected onto a unit sphere
in a l-dimension space by f (·; θ) : Rd → Sl, where f is a
neural network parameterized by θ. Formally, we deﬁne the
..=< f (xi; θ), f (xj; θ) >,
similarity of two samples as Sij
where < ·, · > denotes dot product, resulting in an m × m
similarity matrix S whose element at (i, j) is Sij .

Given a pair-based loss L, it can be formulated as a func-
tion in terms of S and y: L(S, y). The derivative with
respect to model parameters θ at the t-th iteration can be

calculated as:

∂L(S, y)

∂θ

(cid:12)(cid:12)(cid:12)(cid:12)t

=

=

(cid:12)(cid:12)(cid:12)(cid:12)t

∂L(S, y)

∂S
m

m

Xi=1

Xj=1

∂L(S, y)

∂Sij

∂S

∂θ(cid:12)(cid:12)(cid:12)(cid:12)t
(cid:12)(cid:12)(cid:12)(cid:12)t

(1)

.

∂Sij

∂θ (cid:12)(cid:12)(cid:12)(cid:12)t

Eq. 1 is computed for optimizing model parameters θ in
In fact, Eq. 1 can be reformulated
deep metric learning.
into a new form for pair weighting through a new function
F , whose gradient w.r.t. θ at the t-th iteration is computed
exactly the same as Eq. 1. F is formulated as below:

F(S, y) =

m

m

Xi=1

Xj=1

∂L(S, y)

∂Sij

Sij.

(2)

(cid:12)(cid:12)(cid:12)(cid:12)t

(cid:12)(cid:12)t

Note that ∂L(S,y)
not involved in the gradient of F w.r.t. θ.

∂Sij

is regarded as a constant scalar that

Since the central idea of deep metric learning is to en-
courage positive pairs to be closer, and push negatives apart
from each other. For a pair-based loss L, we can assume
∂L(S,y)
6 0 for a
positive pair. Thus, F in Eq. 2 can be transformed into the
form of pair weighting as follows:

> 0 for a negative pair, and ∂L(S,y)

(cid:12)(cid:12)t

∂Sij

∂L(S, y)

∂Sij

∂L(S, y)

∂Sij

wijSij −

Sij +

(cid:12)(cid:12)(cid:12)(cid:12)t
Xyj =yi

m

m

Xyj =yi
wijSij
 ,

Sij


(cid:12)(cid:12)(cid:12)(cid:12)t

(3)

m

Xyj 6=yi

m

Xyj 6=yi

m

∂Sij

F =

Xi=1

(cid:12)(cid:12)t




where wij = (cid:12)(cid:12)(cid:12)(cid:12)

Xi=1

=

m

∂L(S,y)

∂Sij

.

(cid:12)(cid:12)t(cid:12)(cid:12)(cid:12)(cid:12)

As indicted in Eq. 3, a pair-based method can be for-
mulated as weighting of pair-wise similarities, where the
weight for pair {xi, xj} is wij . Learning with a pair-based
loss function L is now transformed from Eq. 1 into comput-
ing the weight of pairs in Eq. 3. It is a general pair weighting
(GPW) formulation, and sampling is only one of its special
cases.

3.2. Revisit Pair based Loss Functions

To demonstrate the generalization ability of GPW frame-
work, we revisit four typical pair-based loss functions for
deep metric learning: contrastive loss [6], triplet loss [10],
binomial deviance loss [40] and lifted structure loss [25].

Contrastive loss. Hadsell et al. [6] proposed a Siamese
network, where a contrastive loss was designed to encour-
age positive pairs to be as close as possible, and negative
pairs to be apart from each other over a given threshold, λ:

Lcontrast

..= (1 − Iij)[Sij − λ]+ − IijSij,

(4)

5024

where Iij = 1 indicates a positive pair, and 0 for a neg-
ative one. By computing partial derivative with respect to
Sij in Eq. 4, we can ﬁnd that all positive pairs and hard
negative pairs with Sij > λ are assigned with an equal
weight. This is a simple and special case of our pair weight-
ing scheme, without considering any difference between the
selected pairs.

Triplet loss. In [10], a triplet loss was proposed to learn a
deep embedding, which enforces the similarity of a negative
pair to be smaller than that of a randomly selected positive
one over a given margin λ:

Ltriplet := [San − Sap + λ]+,

(5)

where San and Sap denote the similarity of a negative pair
{xa, xn}, and a positive pair {xa, xp}, with an anchor sam,
eplxa. According to the gradient computed for Eq. 5, a
triplet loss weights all pairs equally on the valid triplets
which are selected by San + λ > Sap, while the triplets
with San + λ 6 Sap are considered as less informative, and
are discarded. Triplet loss is different from contrastive loss
on pair selection scheme, but both methods consider all the
selected pairs equally, which limits their ability to identify
more informative pairs among the selected ones.

Lifted Structure Loss. Song et al. [25] designed a lifted
structure loss, which was further improved to a more gener-
alized version in [9]. It utilizes all the positive and negative
pairs among the mini-batch as follows:

Llif ted

..=

m

Xi=1

(cid:20)log Xyk=yi

eλ−Sik + log Xyk6=yi

eSik(cid:21)+

, (6)

where λ is a ﬁxed margin.

In Eq. 6, when the hinge function of an anchor xi returns
a non-zero value, we can have a weight value, wij , for the
pair {xi, xj}, by differentiating Llif ted on Sij , according
to Eq. 3. Then the weight for a positive pair is computed as:

w+

ij =

eλ−Sij

eλ−Sik

Pyk=yi

=

1
eSij −Sik

Pyk=yi

,

(7)

and the weight for a negative pair is:

w−

ij =

eSij

Pyk6=yi

=

eSik

1
eSik−Sij

Pyk6=yi

.

(8)

Eq. 7 shows that the weight for a positive pair is determined
by its relative similarity, measured by comparing it to the
remained positive pairs with the same anchor. The weight
for a negative pair is computed similarly based on Eq. 8.

Binomial Deviance Loss. Dong et al. introduced bino-
mial deviance loss in [40], which utilizes softplus function

instead of hinge function in contrastive loss:

Lbinomial =

m

Xi=1

(cid:26) 1
Pi Xyj =yi
Ni Xyj 6=yi

1

logh1 + eα(λ−Sij )i+
logh1 + eβ(Sij −λ)i(cid:27),

(9)

where Pi and Ni denote the numbers of positive pairs and
negative pairs with anchor xi, respectively. λ, α, β are ﬁxed
hyper-parameters.

The weight for pair {xi, xj} is wij in Eq. 1, which can

be derived from differentiating Lbinomial on Sij as:

w+

ij =

w−

ij =

1
Pi
1
Ni

αeα(λ−Sij )
1 + eα(λ−Sij )
βeβ(Sij −λ)
1 + eβ(Sij −λ)

,

,

yj = yi

yj 6= yi

(10)

As can be found, binomial deviance loss is a soft version
of contrastive loss. In Eq. 3, a negative pair with a higher
similarity is assigned with a larger weight, which means that
it is more informative, by distinguishing two similar sam-
ples from different classes (which form a negative pair).

4. Multi-Similarity Loss

In this section, we ﬁrst analyze three types of similarities
that impact sample selection and weighting in deep met-
ric learning. Then we propose a multi-similarity loss that
jointly considers all three similarities with iterative sample
mining and weighting.

4.1. Multiple Similarities

Though with diverse formulations, various pair-based
loss functions, which commonly focus on learning from
more informative pairs, can be cast into a pair weighting
problem within our GPW framework. Furthermore, we ob-
served that most pair-based approaches weight the pairs
based on either self cosine similarities or relative similar-
ities compared with other pairs. For simplicity, we take a
negative pair as an example to describe three different types
of similarities we deﬁned. Analysis of a positive pair is
similar. The three similarities for pair-based methods are
described as follows.

S: Self-similarity. A self-similarity is computed from
the pair itself, which is the most important similarity. A
negative pair with a larger cosine similarity means that it
is harder to distinguish two paired samples from differ-
ent classes. Such pairs are referred as hard negative pairs,
which are more informative and meaningful to learn dis-
criminative features. Contrastive loss [6] and binomial de-
viance loss [40] are based on this criterion. As shown in
case-1 of Fig. 2, the weights of three negative pairs are in-
creased, when the negative samples come closer.

5025

Figure 2. Violations of the three similarities for negative pairs. Case-1: The negative pairs’ cosine similarities increase as they come closer
to the anchor; case-2: Relative similarity compared with negatives decreases as the other negatives’ cosine similarities increase; case-3:
Relative similarity decrease as relevant positive pairs become closer.

N-pairs NCA Histogram contrastive Triplet LiftedStruct Binomial BinLifted MS
✓

✓

✓

✓

✗

✗

✗

✗

✗

✓

✗

✓

✓

✗

✓

✗

✗

✗

✓

✓

✗

✗

✗

✓

✗

✓

✓

S
N
P

Table 1. This table shows similarities that these pair-based methods utilize to assign weight on one negative pair. In the ﬁrst column of this
table is the three perspectives ‘S’ : the Similarity of itself; ‘N’: similarity compared with remaining Negative pairs shared the anchor point;
‘P’: similarity compared with Positive pairs of the same anchor.

Obviously, self-similarity is difﬁcult to fully describe
sample distribution in an embedding space, and correlations
of other pairs can make signiﬁcant impact to similarity mea-
surement. We introduce relative similarities by considering
all pairs which have a same anchor with current pair, and
deﬁne two type of relative similarities.

N: Negative relative similarity. It is computed by con-
sidering the relationship from neighboring negative pairs.
As described in case-2 of Fig. 2, the relative similarity
of a pair is decreased, even when its self-similarity is un-
changed. This is because its neighboring negative samples
move closer, which increases the self-similarities of these
neighboring pairs, and thus reduce the relative similarity.
Lifted structure loss [25] is based on this relative similarity,
as shown in Eq. 8.

P: Positive relative similarity. Similarly, the relative
similarity also considers the relationship from other positive
pairs (with a same anchor). As shown in case-3 of Fig. 2,
when these positive samples become closer to the anchor,
the relative similarity of current pair is decreased, and thus
the pair weight should be reduced accordingly. Triplet loss
is computed based on this similarity, as indicated in Eq. 5.

With our GPW framework, we analyze a number of ex-
isting pair-based loss functions based on the three similari-
ties deﬁned, and compare them in Table 1. Detailed formu-
lations of these functions are presented in Supplementary
Material. As can be found, lifted structure loss only con-
siders the negative relative similarity, by comparing with
the negative neighboring pairs for weighting. The weight
of current pair will be unchanged, when all positive sam-

pends on Pyk=yi

ples (in case-1 of Fig. 2) or all negative samples (in case-3
of Fig. 2) move synchronously to the anchor. This conclu-
sion can also be derived directly from Eq. 8, which only de-
eSij −Sik . Obviously, such relevant sam-
ples (positive or negative) often include meaningful infor-
mation, and are of great importance to learning discrimi-
native features, but are discarded arbitrarily, which may re-
duce model capability considerably. While a weighting or
sampling method based on each individual similarity has
been explored previously, to the best of our knowledge,
none of existing pair-based methods have assigned weights
on pairs making full use of all the three similarities.

4.2. Multi Similarity Loss

As discussed, unlike sampling or weighting schemes de-
veloped for classiﬁcation and detection tasks in [2, 20],
where the weight of an instance is computed individually
based on a cross entropy loss, it is difﬁcult to precisely mea-
sure the informativeness of a pair based on its individual co-
sine similarity. Pair-wise similarity between relevant sam-
ples or pairs should be considered, making the measurement
and weighting problems more complicated and challenging.
As shown in Table 1, each of the listed approaches can
consider one or two of the three similarities. To the best
our knowledge, none of the existing pair-based methods
can consider all of the three similarities simultaneously. To
this end, we propose a Multi-Similarity (MS) loss, which
consider all three perspectives by implementing a new pair
weighting scheme using two iterative steps: mining and
weighting. (i) informative pairs are ﬁrst sampled by mea-

5026

suring Similarity-P; and then (ii) the selected pairs are fur-
ther weighted using Similarity-S and Similarity-N jointly.
Details of the two steps are described as follows.

Pair mining. We ﬁrst select informative pairs by com-
puting Similarity-P, which measures the relative similar-
ity between negative↔positive pairs having a same anchor.
Speciﬁcally, a negative pair is compared to the hardest posi-
tive pair (with the lowest similarity), while a positive pair is
sampled by comparing to a negative one having the largest
similarity. Formally, assume xi is an anchor, a negative pair
{xi, xj} is selected if Sij satisﬁes the condition:

S−

ij > min
yk=yi

Sik − ǫ,

where ǫ is a given margin.

If {xi, xj} is a positive pair, the condition is:

S+

ij < max
yk6=yi

Sik + ǫ.

(11)

(12)

For an anchor xi, we denote the index set of its se-
lected positive and negative pairs as Pi and Ni respectively.
Our hard mining strategy is inspired by large margin near-
est neighbor (LMNN) [35], a traditional distance learning
approach which targets to seek an embedding space where
neighboring positive points are encouraged to have the same
class label with the anchor. The negative samples that sat-
isfy Eq. 11 are approximately identical to impostors deﬁned
in LMNN [35].

Pair weighting.

Pair mining with Similarity-P can
roughly select informative pairs, and discard the less in-
formative ones. We develop a soft weighting scheme that
further weights the selected pairs more accurately, by con-
sidering both Similarity-S and Similarity-N. Our weighting
mechanism is inspired by binomial deviance loss (consider-
ing similarity-S) and lifted structure loss (using Similarity-
N). Speciﬁcally, given a selected negative pair {xi, xj} ∈
Ni, its weight w−

ij can be computed as:

w−

ij =

=

1

eβ(λ−Sij ) + Pk∈Ni

eβ(Sij −λ)

eβ(Sik−λ)

1 + Pk∈Ni

eβ(Sik−Sij )

(13)

.

and the weight w+

ij of a positive pair {xi, xj} ∈ Pi is:

w+

ij =

1

e−α(λ−Sij ) + Pk∈Pi

e−α(Sik−Sij )

,

(14)

where α, β, λ are hyper-parameters as in Binomial deviance
loss (Eq. 9).

In Eq. 13, the weight of a negative pair is computed
jointly from its self-similarity by eβ(λ−Sij ) - Similarity-S,

and its relative similarity - Similarity-N, by comparing to its
negative pairs. Similar rules are applied for computing the
weight for a positive pair, as in Eq. 14. With these two con-
siderations, our weighting scheme updates the weights of
pairs dramatically to the violation of its self-similarity and
relative similarities.

The weight computed by Eq. 13 and Eq. 14 can be con-
sidered as a combination of the weight formulations of bi-
nomial deviance loss and lifted structure loss. However, it
is non-trivial to combine both functions in an elegant and
suitable manner. We will compare our MS weighting with
an average weighting scheme of binomial deviance (Eq.
10) and lifted structure (Eq. 8), denoted as BinLifted. We
demonstrate by experiments that direct combination of them
can not lead to performance improvement (as shown in ab-
lation study).

Finally, we integrate pair mining and weighting scheme
into a single framework, and provide a new pair-based loss
function - multi-similarity (MS) loss, whose partial deriva-
tive with respect to Sij is the weight deﬁned in Eq. 13 and
Eq. 14. Our MS loss is formulated as,

LM S =

1
m

m

Xi=1

(cid:26) 1

α

log(cid:2)1 + Xk∈Pi
log(cid:2)1 + Xk∈Ni

e−α(Sik−λ)(cid:3)
eβ(Sik−λ)(cid:3)(cid:27).

+

1
β

(15)

where LM S can be minimized with gradient descent op-
timization, by simply implementing the proposed iterative
pair mining and weighting.

5. Experiments

Our method was implemented by PyTorch. For network
architecture, we used the Inception network with batch nor-
malization [11] pre-trained on ILSVRC 2012-CLS [27], and
ﬁne-tuned it for our task. We add a FC layer on the top of
the network following the global pooling layer. All the input
images were cropped to 224 × 224. For data augmentation,
we used random crop with random horizontal mirroring for
training, and single center crop for testing. Adam optimizer
was used for all experiments.

We conduct experiments on four standard datasets:
CUB200 [33], Cars-196 [16], Stanford Online Products
(SOP) [25] and In-Shop Clothes Retrieval (In-Shop) [21].
We follow the data split protocol applied in [25]. For the
CUB200 dataset, we use the ﬁrst 100 classes with 5,864 im-
ages for training, and the remaining 100 classes with 5,924
images for testing. The Cars-196 dataset is composed of
16,185 images of cars from 196 model categories. The ﬁrst
98 model categories are used for training, with the rest for
testing. For the SOP dataset, we use 11,318 classes for
training, and 11,316 classes for testing. For the In-Shop

5027

Recall@K (%)
Binomial
LiftedStruct∗
MS mining
BinLifted
MS weighting
Binomialm
LiftedStruct∗
m
MS

1

71.9
69.7
67.0
70.4
73.2
74.6
72.2
77.3

2

80.0
79.3
77.4
79.5
81.5
83.8
81.7
85.3

4

86.4
86.2
84.7
86.2
87.6
89.7
88.0
90.5

8

91.0
91.0
90.0
91.1
92.6
94.1
92.4
94.2

S
N
P
SN
SN
SP
NP
SNP

Table 2. This table shows Recall@K of methods on Cars-196. The
ﬁrst column contains the methods and the similarities they utilize
to weight negative pairs. Embedding dimension is set to 64. The
subscript m denotes employing our MS mining step.

dataset, 3,997 classes with 25,882 images are used for train-
ing. The test set is partitioned to a query set with 14,218 im-
ages of 3,985 classes, and a gallery set having 3,985 classes
with 12,612 images.

For every mini-batch, we randomly choose a certain
number of classes, and then randomly sample M instances
from each class with M = 5 for all datasets in all experi-
ments. ǫ in Eq. 11 and Eq. 12 is set to 0.1 and the hyper-
parameters in Eq. 15 are: α = 2, λ = 1, β = 50, by follow-
ing [32]. Our method is evaluated on image retrieval task
by using the standard performance metric: Recall@K.

5.1. Ablation Study

To demonstrate the importance of weighting the pairs
from multi-similarities, we conduct an ablation study on
Cars-196 and results are shown in Table 2. We describe
LiftedStruct∗, MS mining and MS weighting here, other
methods have already been mentioned in section 3.

LiftedStruct∗. Lifted structure loss is easy to get stuck
in a local optima, resulting in poor performance. To eval-
uate the efﬁciency of three similarities more clearly and
fairly, we make a minor modiﬁcation to the lifted structure
loss, allowing it to employ Similarity-N more effectively:

Llif t∗ =

1
m

m

X

i=1(cid:26) 1

α

log X
yk =yi

e−αSik +

1
β

log X
yk 6=yi

eβSik(cid:27),

(16)
where α = 2, β = 50. This modiﬁcation is motivated to
make lift structure loss more focus on informative pairs, es-
pecially the hard negative pairs, and allows it to get rid of
the side effect of enormous easy negative pairs. We found
that this modiﬁcation can boost the performance of lifted-
structure loss empirically, e.g., with an over 20% improve-
ment of Recall@1 on the CUB200 .

MS mining. To investigate the impact of each compo-
nent of MS Loss, we evaluate the performance of MS min-
ing individually, where the pairs selected into Ni and Pi are
assigned with an equal weight.

MS weighting. Similarly, MS weighting scheme is also
evaluated individually without the mining step in the abla-

tion study, allowing us to analyze the effect of each similar-
ity more perspicaciously. In MS weighting, each pair in a
mini-batch is assigned with a non-zero weight, according to
the weighting strategy described in Eq. 13 and Eq. 14.

With the performance reported in Table 2, we analyze

the effect of each similarity as below:

Similarity-S: A cosine self-similarity is of the most
importance.
Binomial deviance loss, based on the
Similarity-S, achieves the best performance by using a
single similarity. Moreover, our MS weighting outper-
forms LiftedStruct∗
m by 69.7% → 73.2% at recall@1, and
Binomialm also improves the recall@1 with 67.0% →
74.6% over the MS mining, by adding the Similarity-S into
their weighting schemes.

Similarity-N: Relative similarities are also helpful to
measuring the importance of a pair more precisely. With
Similarity-N, our MS weighting increases the Recall@1 by
1.3% over Binomial (71.9% → 73.2%). Moreover, with
Similarity-N, LiftedStruct∗
m obtains a signiﬁcant perfor-
mance improvement over MS sampling (67% → 72.2%),
by considering both Similarity-P and Similarity-N.

Similarity-P: As shown in Table 2, by adding a
mining step based on Similarity-P, the performances of
LiftedStruct∗, Binomial and MS weighting are consistently
improved by a large margin. For instance, Recall@1 of Bi-
nomial is increased by nearly 3%: 71.9% → 74.6%.

Finally, the proposed MS loss achieves the best per-
formance among these methods, by exploring multi-
similarities for pair mining and weighting. However, it is
critical to integrate the three similarities effectively into a
single framework where the three similarities can be fully
explored and optimized jointly by using a single loss func-
tion. For example, BinLifted, which uses a weighting
scheme considering both similarities-S and similarities-N,
has lower performance than that of single Binomial, since
it implements a simple and straightforward combination of
Binomial and LiftedStruct∗
m. More discussions on the dif-
ference between our MS weighting and the direct combina-
tion are presented in Supplementary Material.

5.2. On Embedding Size

By following [28], we study the performance of MS loss
with varying embedding sizes {64, 128, 256, 512, 1024}.
As shown in Fig. 3, the performance is increased consis-
tently with the embedding dimension except at 1024. This
is different from lifted structure loss, which achieves its best
performance at 64 on the Cars-196 dataset [25].

5.3. Comparison with State of the Art

We further compare the performance of our method with
the state-of-the-art techniques on image retrieval task. As
shown in Table 3, our MS loss improves Recall@1 by 7%
on the CUB200, and 4% on the Cars-196 over Proxy-NCA

5028

CUB-200-2011

16
-
-
-

1

2

Cars-196
8
4

58.1 70.6 80.3 87.8
73.2 82.4 86.4 87.8
64.7 76.2 84.2 90.2
79.6 86.5 91.9 95.1 97.3

16
-
-
-

32
-
-
-
-

32
-
-
-
-

8

4

2

1

Recall@K (%)
Clustering 64[30]
48.2 61.4 71.8 81.9
ProxyNCA64 [24]
49.2 61.9 67.9 72.4
Smart Mining64 [7] 49.8 62.3 74.1 83.3
Margin128 [38]
HDC384 [30]
HTL512 [4]
ABIER512 [26]
ABE512 [14]
MS64
MS512

63.6 74.4 83.1 90.0 94.2
53.6 65.7 77.0 85.6 91.5 95.5 73.7 83.2 89.5 93.8 96.7 98.4
57.1 68.8 78.7 86.5 92.5 95.5 81.4 88.0 92.7 95.7 97.4 99.0
57.5 68.7 78.3 86.2 91.9 95.5 82.0 89.0 93.2 96.1 97.8 98.7
60.6 71.5 79.8 87.4
57.4 69.8 80.0 87.8 93.2 96.4 77.3 85.3 90.5 94.2 96.9 98.2
65.7 77.0 86.3 91.2 95.0 97.3 84.1 90.4 94.0 96.5 98.0 98.9

85.2 90.5 94.0 96.1

-

-

-

-

Table 3. Recall@K(%) performance on CUB200 and Cars-196. Superscript denotes embedding size. ABIER [26] and ABE [14] are
ensemble methods.

achieves a higher Recall@1 by +5% improvement at the
same dimension on the CUB200. On the Cars-196, our MS
loss obtains the second best performance in terms of Re-
call@1, while the best results are achieved by ABE, which
applies an ensemble method with a much heavier model.
Moreover, on the remaining three datasets, our MS loss is
of much stronger performance than ABE.

In Table 4 and 5, our MS loss outperforms Proxy-NCA
by 0.4% and Clustering by 7% at the same embedding size
of 64. Furthermore, when compared with ABE, MS loss
increases Recall@1 by 1.9% and 2.7% on the SOP and In-
Shop respectively. Moreover, even with much compact em-
bedding features of 128 dimension, our MS loss has already
surpassed all existing methods, which utilize much higher
dimensions of 384, 512 and 4096.

To summarize, on both ﬁne-grained datasets like Cars-
196 and CUB200, and large-scale datasets with enormous
categories like SOP and In-Shop, our method achieves
new state-of-the-art or comparable performance, even tak-
ing those methods with ensemble techniques like ABE and
BIER into consideration.

6. Conclusion

We have presented a new multi-similarity loss for deep
metric learning, and established a General Pair Weighting
(GPW) framework which, for the ﬁrst time, unify exist-
ing pair-based metric learning approaches into general pair
weighting through gradient analysis. GPW provides a pow-
erful tool for understanding and explaining various pair-
based loss functions, which allows us to clearly identify the
main differences and key limitations of existing methods.
Furthermore, we proposed a multi-similarity loss which
considers all three similarities simultaneously, and devel-
oped an iterative pair mining and weighting scheme for op-
timizing the multi-similarity loss efﬁciently. Our method
obtains new state-of-the-art performance on a number of
image retrieval benchmarks.

5029

Figure 3. The effect of embedding size on MS loss

1

40

30

20

10

Recall@K (%)
50
FashionNet4096 [21] 53.0 73.0 76.0 77.0 79.0 80.0
HDC384 [30]
62.1 84.9 89.0 91.2 92.3 93.1
HTL128 [4]
80.9 94.3 95.8 97.2 97.4 97.8
ABIER512 [26]
83.1 95.1 96.9 97.5 97.8 98.0
ABE512 [14]
87.3 96.7 97.9 98.2 98.5 98.7
MS128
88.0 97.2 98.1 98.5 98.7 98.8
MS512
89.7 97.9 98.5 98.8 99.1 99.2

-

-

1

-
-

10

100 1000

67.0 83.7 93.2

Table 4. Recall@K(%) performance on In-Shop.
Recall@K (%)
Clustering 64[30]
ProxyNCA64 [24] 73.7
Margin128 [38]
HDC384 [30]
HTL 512 [4]
ABIER512 [26]
ABE512 [14]
MS64
MS128
MS512

72.7 86.2 93.8 98.0
69.5 84.4 92.8 97.7
74.8 88.3 94.8 98.4
74.2 86.9 94.0 97.8
76.3 88.4 94.8 98.2
74.1 87.8 94.7 98.2
76.6 89.2 95.2 98.4
78.2 90.5 96.0 98.7

Table 5. Recall@K(%) performance on SOP.

at dimension 64. Compared with ABE employing an em-
bedding size of 512 and attention modules, our MS loss

641282565121024Embedding Dims757779818385Recall@1 (%)References

[1] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-
bedding consistency by metric learning for zero-shot classif-
ﬁcation. In ECCV, 2016. 1

[2] H.-S. Chang, E. Learned-Miller, and A. McCallum. Active
bias: Training more accurate neural networks by emphasiz-
ing high variance samples. In NIPS, 2017. 3, 5

[3] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 2

[4] W. Ge, W. Huang, D. Dong, and M. R. Scott. Deep metric
learning with hierarchical triplet loss. In ECCV, 2018. 1, 2,
3, 8

[5] A. Grabner, P. M. Roth, and V. Lepetit. 3d pose estimation
In CVPR,

and 3d model retrieval for objects in the wild.
2018. 1

[6] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In CVPR, 2006. 1, 2,
3, 4

[7] B. Harwood, V. K. B G, G. Carneiro, I. Reid, and T. Drum-
In ICCV,

mond. Smart mining for deep metric learning.
2017. 2, 3, 8

[8] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-center

loss for multi-view 3d object retrieval. In CVPR, 2018. 1

[9] A. Hermans*, L. Beyer*, and B. Leibe.

In defense of the
triplet loss for person re-identiﬁcation. arXiv:1703.07737v4,
2017. 1, 4

[10] E. Hoffer and N. Ailon. Deep metric learning using triplet

network. In SIMBAD, 2015. 1, 2, 3, 4

[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 6

[12] L. Jiang, D. Meng, S.-I. Yu, Z. Lan, S. Shan, and A. Haupt-
In NIPS. 2014.

mann. Self-paced learning with diversity.
3

[13] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Men-
tornet: Learning data-driven curriculum for very deep neural
networks on corrupted labels. In ICML, 2018. 3

[14] W. Kim, B. Goyal, K. Chawla, J. Lee, and K. Kwon.
In

Attention-based ensemble for deep metric learning.
ECCV, 2018. 1, 2, 8

[15] S. Kiran Yelamarthi, S. Krishna Reddy, A. Mishra, and
A. Mittal. A zero-shot framework for sketch based image
retrieval. In ECCV, 2018. 1

[16] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In ICCV Work-
shops, 2013. 6

[17] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning

for latent variable models. In NIPS, 2010. 3

[18] M. T. Law, N. Thome, and M. Cord. Quadruplet-wise image

similarity learning. In ICCV, 2013. 1

[19] L. Leal-Taix´e, C. Canton-Ferrer, and K. Schindler. Learning
In

by tracking: Siamese cnn for robust target association.
CVPR Workshops, 2016. 1

[20] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. Focal

loss for dense object detection. In ICCV, 2017. 3, 5

[21] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion:
Powering robust clothes recognition and retrieval with rich
annotations. In CVPR, 2016. 6, 8

[22] D. G. Lowe. Similarity metric learning for a variable-kernel

classiﬁer. Neural computation, 1995. 1

[23] S. Mika, G. Ratsch, J. Weston, B. Scholkopf, and K.-R.
Mullers. Fisher discriminant analysis with kernels. In Neural
networks for signal processing, 1999. 1

[24] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and
S. Singh. No fuss distance metric learning using proxies. In
ICCV, 2017. 8

[25] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 1, 2, 3, 4, 5, 6, 7

[26] M. Opitz, G. Waltner, H. Possegger, and H. Bischof. Deep
Metric Learning with BIER: Boosting Independent Embed-
dings Robustly. PAMI, 2018. 8

[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 6

[28] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
2015. 2, 3, 7

[29] K. Sohn.

Improved deep metric learning with multi-class

n-pair loss objective. In NIPS. 2016. 1, 2

[30] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep

metric learning via facility location. In CVPR, 2017. 8

[31] R. Tao, E. Gavves, and A. W. Smeulders. Siamese instance

search for tracking. In CVPR, 2016. 1

[32] E. Ustinova and V. Lempitsky. Learning deep embeddings

with histogram loss. In NIPS. 2016. 1, 2, 7

[33] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Master’s the-
sis, 2011. 6

[34] J. Wang, F. Zhou, S. Wen, X. Liu, and Y. Lin. Deep metric

learning with angular loss. ICCV, 2017. 1

[35] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric
learning for large margin nearest neighbor classiﬁcation. In
NIPS. 2006. 6

[36] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016. 1

[37] P. Wohlhart and V. Lepetit. Learning descriptors for object

recognition and 3d pose estimation. In CVPR, 2015. 1

[38] C. Wu, R. Manmatha, A. J. Smola, and P. Kr¨ahenb¨uhl. Sam-
pling matters in deep embedding learning. ICCV, 2017. 1, 2,
3, 8

[39] E. P. Xing, M. I. Jordan, S. J. Russell, and A. Y. Ng. Dis-
tance metric learning with application to clustering with
side-information. In NIPS, 2003. 1

[40] D. Yi, Z. Lei, and S. Z. Li. Deep metric learning for practical

person re-identiﬁcation. arXiv:1407.4979, 2014. 1, 2, 3, 4

[41] R. Yu, Z. Dou, S. Bai, Z. Zhang, Y. Xu, and X. Bai. Hard-
aware point-to-set deep metric for person re-identiﬁcation.
In ECCV, 2018. 1

[42] Z. Zhang and V. Saligrama. Zero-shot learning via joint la-

tent similarity embedding. In CVPR, 2016. 1

5030

