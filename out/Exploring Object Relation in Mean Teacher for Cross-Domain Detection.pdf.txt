Exploring Object Relation in Mean Teacher for Cross-Domain Detection∗

Qi Cai †, Yingwei Pan ‡, Chong-Wah Ngo §, Xinmei Tian †, Lingyu Duan ¶, and Ting Yao ‡

† University of Science and Technology of China, Hefei, China

‡ JD AI Research, Beijing, China

§ City University of Hong Kong, Kowloon, Hong Kong

¶ Peking University, Beijing, China

{cqcaiqi, panyw.ustc, tingyao.ustc}@gmail.com, cscwngo@cityu.edu.hk, xinmei@ustc.edu.cn, lingyu@pku.edu.cn

Abstract

Rendering synthetic data (e.g., 3D CAD-rendered im-
ages) to generate annotations for learning deep models
in vision tasks has attracted increasing attention in recent
years. However, simply applying the models learnt on syn-
thetic images may lead to high generalization error on re-
al images due to domain shift. To address this issue, re-
cent progress in cross-domain recognition has featured the
Mean Teacher, which directly simulates unsupervised do-
main adaptation as semi-supervised learning. The domain
gap is thus naturally bridged with consistency regulariza-
tion in a teacher-student scheme. In this work, we advance
this Mean Teacher paradigm to be applicable for cross-
domain detection. Speciﬁcally, we present Mean Teacher
with Object Relations (MTOR) that novelly remolds Mean
Teacher under the backbone of Faster R-CNN by integrat-
ing the object relations into the measure of consistency cost
between teacher and student modules. Technically, MTOR
ﬁrstly learns relational graphs that capture similarities be-
tween pairs of regions for teacher and student respectively.
The whole architecture is then optimized with three consis-
tency regularizations: 1) region-level consistency to align
the region-level predictions between teacher and student,
2) inter-graph consistency for matching the graph struc-
tures between teacher and student, and 3) intra-graph con-
sistency to enhance the similarity between regions of same
class within the graph of student. Extensive experiments
are conducted on the transfers across Cityscapes, Foggy C-
ityscapes, and SIM10k, and superior results are reported
when comparing to state-of-the-art approaches. More re-
markably, we obtain a new record of single model: 22.8%
of mAP on Syn2Real detection dataset.

1. Introduction

Deep Neural Networks have been proven to be highly
effective for learning vision models on large-scale dataset-

∗This work was performed at JD AI Research.

Synthetic Images from 3D 
CAD Models with Labels

(a) Faster R-CNN results

Faster R-CNN

car

Real Images

(b) Adaptation Results

Adaptation with
Mean Teacher

car

person

car

Figure 1. Object detection on one real image by (a) directly apply-
ing Faster R-CNN trained on images from 3D CAD models and
(b) domain adaptation of Mean Teacher in this work.
s. To date in the literature, there are various datasets (e.g.,
ImageNet [41] and COCO [25]) that include well-annotated
images useful for developing deep models across a variety
of vision tasks, e.g., recognition [15, 47], detection [13, 40],
and semantic segmentation [2, 27]. Nevertheless, given a
new dataset, the typical ﬁrst step is still to perform inten-
sive manual labeling, which is cost expensive and time con-
suming. An alternative is to utilize synthetic data which is
largely available from 3D CAD models [34], and the ground
truth could be freely and automatically generated. However,
many previous experiences have also shown that reapplying
a model learnt on synthetic data may hurt the performance
on real data due to a phenomenon known as “domain shift”
[50]. Take the object detection results shown in Figure 1
(a) as an example, the model trained on synthetic data from
3D CAD fails to accurately localize the objects such as per-
son and car. As a result, unsupervised domain adaptation,
which aims to utilize labeled examples from the source do-
main and numerous unlabeled examples in the target do-
main to reduce the prediction error on the target data, can
be a feasible solution for this challenge.

A recent pioneering practice [9] in unsupervised do-
main adaptation is to directly simulate this task as
semi-supervised learning. The basic idea is to develop
Mean Teacher [48], the state-of-the-art technique in semi-
supervised learning, to work in cross-domain recognition
task by pursuing the consistency of two predictions under
perturbations of inputs (e.g., different augmentations of im-
age). As such, the domain gap is naturally bridged via the
consistency regularization in Mean Teacher, which enforces
the predictions of two models (i.e., teacher and student) to

11457

Figure 2. A sketch of cross-domain binary classiﬁcation task with
two labeled examples/regions in source domain (large blue dots)
and three unlabeled examples/regions of one image in target do-
main (blue circle), demonstrating how the choice of the unlabeled
target samples affects the uniﬁed ﬁtted function across domains
(gray curve). (a) A model with no regularization is ﬂexible to ﬁt
any function that correctly classiﬁes only labeled source data. (b)
A model trained with augmented labeled source data (small blue
dots) learns to produce consistent results around labeled data. (c)
Mean Teacher [9] locally enforces the predictions to be consistent
to the noise around each individual target sample, pursuing addi-
tional local smoothing of ﬁtted function (gray curve). (d) Mean
Teacher with inter-graph consistency simultaneously adapts target
samples to make the holistic graph structure of them resistant to
the noise. (e) Mean Teacher with intra-graph consistency enforces
additional consistency across target samples of same class, further
improving ﬁtted function with long-range smoothing.

be consistent to the perturbations/noise around each unla-
beled target sample (Figure 2 (c)). Mean teacher aims for
learning a more smooth domain-invariant function than the
model trained with no regularization (Figure 2 (a)) or on-
ly augmented labeled source data (Figure 2 (b)).
In this
paper, we novelly consider the use of Mean Teacher for
cross-domain detection from the viewpoint of both region-
level and graph-structured consistencies. The objective of
region-level consistency is to align the region-level classiﬁ-
cation results of teacher and student models for the identical
teacher-generated region proposals, which in turn implicitly
enforces the consistency of object localization. The inspi-
ration of graph-structured consistency is from the rationale
that the inherent relations between objects within one image
should be invariant to different image augmentations.
In
the context of Mean Teacher, this kind of graph-structured
consistency (i.e., inter-graph consistency) is equivalent to
matching the graph structures between teacher and studen-
t models (Figure 2 (d)). Another kind of graph-structured
consistency, i.e., intra-graph consistency, is additionally ex-
ploited to reinforce the similarity between image regions of
same class within the graph of student model (Figure 2 (e)).
By consolidating the idea of region-level and graph-
structured consistencies into Mean Teacher for facilitating
cross-domain detection, we present a novel Mean Teacher
with Object Relations (MTOR), as shown in Figure 3. The
whole framework consists of teacher and student modules
under the same backbone of Faster R-CNN [40]. Specif-
ically, each labeled source sample is only passed through
student module to conduct supervised learning of detection,
while each unlabeled target sample will be fed into both
teacher and student with two random augmentations, en-
abling the measure of the consistency between them to the
induced noise. During training, with the same region pro-
posals generated by teacher, two relational graphs are con-

structed via calculating the feature similarity between each
pair of regions for teacher and student. The whole MTOR is
then trained by the supervised detection loss in student mod-
el plus three consistency regularizations, i.e., region-level
consistency to align the region-level predictions, inter-graph
consistency to match the graph structures between teacher
and student, and intra-graph consistency to enhance the sim-
ilarity between regions of same class in student. With both
region-level and graph-structured consistencies, our MTOR
could better build invariance across domains and thus obtain
encouraging detection results in Figure 1 (b).

2. Related Work

Object Detection. Recent years have witnessed remark-
able progress in object detection with deep learning. R-
CNN [14] is one of the early works that exploits a two-
stage paradigm for object detection by ﬁrstly generating re-
gion proposals with selective search and then classifying the
proposals into foreground classes/background. Later Fast
R-CNN [13] extends such paradigm by sharing convolution
features across region proposals to signiﬁcantly speed up
the detection process. Faster R-CNN [40] advances Fast R-
CNN by replacing selective search with an accurate and ef-
ﬁcient Region Proposal Networks (RPN). Next, a few sub-
sequent works [7, 8, 18, 22, 23, 33, 46] strive to improve the
accuracy and speed of two-stage detectors. Another line of
works builds detectors in one-stage manner by skipping re-
gion proposal stage. YOLO [37] jointly predicts bounding
boxes and conﬁdences of multiple categories as regression
problem. SSD [26] further improves it by utilizing multiple
feature maps at different scales. Numerous extensions to the
one-stage scheme have been proposed, e.g. [10, 24, 38, 39].
In this work, we adopt Faster R-CNN as the detection back-
bone for its robustness and ﬂexibility.

Domain Adaptation. As for the literature on domain
adaptation, while it is quite vast, the most relevant category
to our work is unsupervised domain adaptation in deep ar-
chitectures. Recent works have involved discrepancy-based
methods that guide the feature learning in DCNNs by min-
imizing the domain discrepancy with Maximum Mean Dis-
crepancy (MMD) [28, 29, 30]. Another branch is to exploit
the domain confusion by learning a domain discriminator
[11, 12, 44, 49]. Later, self-ensembling [9] extends Mean
Teacher [48] for domain adaptation and establishes new
records on several cross-domain recognition benchmarks.
All of the aforementioned works focus on the domain adap-
tation for recognition, and recently much attention has been
paid to domain adaptation in other tasks, e.g., object de-
tection [4, 35] and semantic segmentation [5, 16, 53]. For
domain adaptation on object detection, [45] uses transfer
component analysis to learn the common transfer compo-
nents across domains and [35] aligns the region features
with subspace alignment. More Recently, [4] constructs a

11458

(a)(b)(c)BeforeAfter(e)(d)Figure 3. The overview of Mean Teacher with Object Relations (MTOR) for cross-domain detection, with teacher and student models
under the same backbone of Faster R-CNN (better viewed in color). Each labeled source image is fed into student model to conduct
the supervised learning of detection. Each unlabeled target image xt is ﬁrstly transformed into two perturbed samples, i.e., xS
t and xT
t ,
with different augmentations and then we inject the two perturbed samples into student and teacher model separately. During training,
with the same set of teacher-generated region proposals RT
xt that shares between teacher and student, two relational graphs, i.e., GT
xt
and GS
xt , are constructed via calculating the feature similarity between each pair of regions for teacher and student, respectively. Next,
three consistency regularization are devised to facilitate cross-domain detection in Mean Teacher paradigm from region-level and graph-
structured perspectives: 1) Region-Level Consistency to align the region-level predictions between teacher and student; 2) Inter-Graph
consistency for matching the graph structures between teacher and student, and 3) Intra-Graph Consistency to enhance the similarity
between regions of same class within the graph of student. The whole MTOR is trained by minimizing the supervised loss on labeled
source data plus the three consistency losses on unlabeled target data in an end-to-end manner. Note that the student model is optimized
with stochastic gradient descent and the weights of teacher are the exponential moving average of student model weights.

domain adaptive Faster R-CNN by learning domain classi-
ﬁers on both image and instance levels.

Summary. Similar to previous work [4], our approach
aims to leverage additional unlabeled target data for learn-
ing domain-invariant detector for cross-domain detection.
The novelty is on the exploitation of Mean Teacher to bridge
domain gap with consistency regularization in the contex-
t of object detection, which has not been previously ex-
plored. Moreover, the object relation between image re-
gions is elegantly integrated into Mean Teacher paradigm
to boost cross-domain detection.

3. Mean Teacher in Semi-Supervised Learning

We brieﬂy review semi-supervised learning with Mean
Teacher [48]. Mean Teacher consists of two models with the
same network architecture: a student model fS parameter-
ized by wfS and a teacher model fT parameterized by wfT .
The main idea behind Mean Teacher is to encourage predic-
tions of teacher and student consistent under small perturba-
tions of inputs or network parameters. In other words, with
the inputs of two different augmentations for the same un-
labeled sample, teacher and student models should produce
similar predicted probabilities. Speciﬁcally, in the standard
setting of semi-supervised learning, we have access to la-
beled set XL = {(xl, yl)} and unlabeled set XU = {xu}.
Given two perturbed samples xS
u of the same unla-

u and xT

beled sample xu, the consistency loss penalizes the differ-
ence between the student’s prediction fS (xS
u ; wfS ) and the
teacher’s fT (xT
u ; wfT ), which is typically computed as the
Mean Squared Error:

Lcons(xu) = ||fS (xS

u ; wfS ) − fT (xT

u ; wfT )||2
2.

(1)

The student is trained using gradient descent, while the
weights of the teacher wfT at t-th iteration are the expo-
nential moving average of the student weights wfS : wt
fT =
α · wt−1
. α is a smoothing coefﬁcient
parameter that controls the updating of teacher weights.

fT + (1 − α) · wt−1

fS

Hence, the total training loss in Mean Teacher is com-
posed of supervised cross entropy loss on labeled samples
and consistency loss of unlabeled samples, balanced with
the tradeoff parameter λ:

L = X

LCE(xl, yl) + λ · X

Lcons(xu).

(2)

(xl,yl)∈XL

xu∈XU

4. Mean Teacher in Cross-Domain Detection

In this paper we remold Mean Teacher in the detection
backbone (e.g., Faster R-CNN) for cross-domain detection
by integrating the object relations into the measure of con-
sistency regularization between teacher and student. An
overview of our Mean Teacher with Object Relations (M-
TOR) framework is depicted in Figure 3. We begin this

11459

LabelsRegion-Level ConsistencyFeature mapInter-Graph ConsistencySupervised LossIntra-Graph ConsistencyRPNStudent ModelPredictionsRCNNTeacher ModelRandom AugmentationRandom AugmentationBase CNNRCNNBase CNNFeature mapFeature mapRPNTarget ImageSource Imagesection by elaborating the problem formulation. Then, a
region-level consistency, which is different from the gener-
ic consistency at image-level in primal Mean Teacher, is
provided to facilitate domain adaptation at region-level. In
addition, two kinds of graph-structured consistencies (inter-
graph and intra-graph consistencies) are introduced to ex-
plore object relation in Mean Teacher, enabling the interac-
tion between regions, which further enhance domain adap-
tation. Finally, the overall objective combining various con-
sistencies along with its optimization strategy are provided.

al graphs, we make the detection backbone—Faster R-CNN
transferable across domains in Mean Teacher paradigm with
three consistency regularization: 1) region-level consisten-
cy (Section 4.2) to align the region-level predictions of the
vertices in teacher and student graphs sharing the same s-
patial location, 2) inter-graph consistency (Section 4.3) for
matching the graph structures (i.e., the afﬁnity matrices) of
teacher and student graphs, and 3) intra-graph consistency
(Section 4.4) to enhance the similarity between regions be-
longing to the same class within the graph of student.

4.1. Problem Formulation

4.2. Region Level Consistency

In unsupervised domain adaptation, we are given Ns la-
beled images Ds = {(xs, Bs)} in source domain and Nt
unlabeled images Dt = {xt} in target domain, where Bs
denotes the bounding box annotation for source image xs.
The ultimate goal of cross-domain detection is to design
domain-invariant detectors depending on Ds and Dt.

Inspired by the recent success of consistency-based
methods in semi-supervised learning [1, 20, 48] and Mean
Teacher in cross-domain recognition [9], we formulate our
cross-domain detection model in a Mean Teacher paradigm
by enforcing the predictions of teacher and student mod-
els consistent under perturbations of input unlabeled tar-
get sample. Accordingly, each labeled source sample xs is
passed through student module to perform supervised learn-
ing of detection. Meanwhile, each unlabeled target sample
xt is ﬁrstly transformed into two perturbed samples (i.e.,
t and xS
xT
t ) with different augmentations, and then fed in-
to teacher and student models separately. This enables the
measure of consistency between student and teacher. Dur-
ing training, different from Mean Teacher in cross-domain
recognition [9] that solely encourages generic image-level
consistency, we consider the consistency at a ﬁner granular-
ity (i.e., region-level), which is tailored for object detection.
Moreover, two graph-structured consistencies are especially
designed to exploit object relations in the context of Mean
Teacher, which further boosts adaptation by aligning the re-
sults depending on the inherent relations between objects.

xt , GS

xt and GS

Speciﬁcally, given the identical set of region proposal-
s RT
xt = {rt} generated by teacher model F T , we con-
struct two relational graphs GT
xt to learn the afﬁni-
ty matrix that captures the relation between any pair of re-
gions in teacher and student, respectively. Note that we use
Gxt ∈ {GT
xt } for simplicity, i.e., Gxt denotes the graph
in either teacher GT
xt . More precisely, by
treating each region in teacher/student as one vertex, the re-
lational graph is constructed as Gxt = {Vxt , Ext } , where
Vxt denotes the set of predictions for all region proposals
in teacher/student and Ext is a (|Vxt | × |Vxt |) afﬁnity ma-
trix whose entry measures the similarities between every t-
wo regions. Ext is symmetric, and represents an undirected
weighted graph. On the basis of two constructed relation-

xt or student GS

Unlike [9] that pursues image-level consistency to per-
turbations of inputs in recognition, we facilitate Mean
Teacher in cross-domain detection by exploiting region-
level consistency under the identical region proposals be-
tween teacher and student. The design of region-level con-
sistency helps to reduce the local instance variances such as
scale, color jitter, random noise, etc, which in turn implicit-
ly enforces the consistency of object localization.

Technically, given the two perturbed samples xT

t and xS
t
of one unlabeled target sample xt, they are fed into teacher
and student detectors under the same backbone (i.e., Faster
R-CNN) separately. Faster R-CNN is a two-stage detec-
tor consisting of three major components: a Base Convo-
lution Neural Network (Base CNN) for feature extraction,
a Region Proposal Network (RPN) to generate candidate
region proposals, and a Region-based Convolution Neural
Network (RCNN) for classifying each region. Hence, with
the input of xT
Conv ﬁrstly pro-
duces output feature map fT
xt . Next, depending on the output
feature map fT
xt = {rt} are
generated via RPN in teacher F T

xt , a set of region proposals RT

t , the Base CNN of teacher F T

RP N :

xt = F T
fT

Conv(xT

t ), RT

xt = F T

RP N (fT

xt ).

(3)

For each region proposal rt ∈ RT
xt , a ROI pooling layer is
utilized to extract a ﬁxed-length vector fT
r from the feature
map fT
xt , which represents the region feature of rt in teach-
er. The RCNN in teacher F T
RCN N further takes each region
feature fT
r as input and classiﬁes it into one of the C fore-
ground categories and a catch-all background class. Here
the prediction of each region is the probability distribution
over background plus foreground categories, which is de-
noted as dT
r ). As such, by accumulating the
predicted results of all region proposals, the entire detection
output of xT
r }. Simi-
t
larly, for student model F S , another perturbed image xS
t is
fed into its Base CNN F S
Conv to produce the feature map
fS
xt . Note that instead of generating another set of region
proposals for xS
t via RPN in student, we directly take the
region proposals from teacher RT

in teacher is denoted as V T

xt as the ones in student:

RCN N (fT

xt = {dT

r = F T

fS
xt = F S

Conv(xS

t ), RS

xt = RT

xt .

(4)

11460

LRCL

xt =

1
|RT
xt |

· Xr∈RT

xt

4.3. Inter Graph Consistency

||dT

r − dS

r ||2
2.

(5)

LAGL

xt =

That is, we endow teacher and student with the same set of
region proposals, enabling the interaction between teacher
and student for measuring region-level consistency. Given
xt and feature map fS
region proposals RS
xt , we can acquire
the region feature fS
r for each region proposal and the corre-
sponding probability distribution dS
r ), lead-
ing to the entire detection results in student V S

RCN N (fS

r = F S

xt = {dS

r }.

As such, the region-level consistency is measured as the
distance between the prediction of teacher V T
xt and that of
student V S
xt . To focus more on foreground samples and sta-
bilize the training in the challenging cross-domain detec-
tion scenario, we follow [9] and adopt conﬁdence thresh-
olding to ﬁlter out background region proposals and low-
conﬁdence foreground region proposals with noise. For
each region proposal rt ∈ RT
xt of teacher model, we com-
r = maxj∈C(dT
pute the conﬁdence as qT
rj), where C is
the set of C foreground categories and dT
rj is the predict-
ed probability of j-th foreground category. If qT
r is below
the conﬁdence threshold ǫ, we eliminate the region propos-
al in RT
xt ), and the
corresponding region-level predictions of teacher and stu-
dent (V T
r }), the Region-level
Consistency Loss (RCL) is calculated as the average of
Mean Squared Error between the region-level predictions
of teacher and student for all region proposals:

xt . With the reﬁned region proposal (RT

r } and V S

xt = {dS

xt = {dT

The region-level consistency only individually aligns
the predictions of each region proposal in teacher and s-
tudent, while leaving the relations between regions unex-
ploited. Thus, inspired from graph structure exploitation
[31, 32, 51, 52] in computer vision tasks, we devise a nov-
el graph-structured regularization, i.e., inter-graph consis-
tency, to measure the consistency of graph structures under
perturbations of inputs by matching the afﬁnity matrices of
graphs constructed in teacher and student models. The ra-
tionale of inter-graph consistency is that the inherent rela-
tions between objects within each image should be invariant
to different image augmentations.

xt }, the afﬁnity matrix of teacher E T

In particular, for the graph constructed in teacher GT
xt , E T

xt =
{V T
xt is obtained by
deﬁning each entry as the similarity between two regions.
For instance, given two region proposals rm, rn ∈ RT
xt , the
entry (E T
xt is calculated as the cosine similarity
between the region representations (fT

xt )m,n in E T

rm and fT

rn ):

(E T

xt )m,n =

rm · fT
fT
rm ||2 · ||fT

rn

||fT

rn ||2

.

(6)

Similarly, we achieve the afﬁnity matrix of student ExS
by
measuring the cosine similarities between every two regions

t

in student. Accordingly, the IntEr-Graph Consistency Loss
(EGL) is deﬁned as the Mean Squared Error between the
afﬁnity matrices of graphs in teacher and student models:

LEGL

xt =

1
xt |2 · ||E S
|RT

xt − E T

xt ||2
2.

(7)

4.4. Intra Graph Consistency in Student

Inspired from self-labeling [21, 42] for domain adapta-
tion, the inter-graph consistency is devised to further rein-
force the similarity between regions of same class within the
graph of student with the supervision from teacher. Specif-
ically, since no label is provided for target samples in unsu-
pervised domain adaptation settings, we directly utilize the
teacher to assign each region proposal rt ∈ RT
xt a “pseu-
do” label: ˆlr = arg maxj∈C(dT
rj). Next, a (|RT
xt | × |RT
xt |)
supervision matrix M T
xt is naturally generated to indicate
whether two regions belong to the same category:

(M T

xt )(m,n) =(1 if ˆlrm = ˆlrn ,

0 otherwise.

,

(8)

where ˆlrm and ˆlrn denote the pseudo labels of two regions
rm, rn ∈ RT
xt , respectively. Thus, given the the afﬁnity
matrix of student E S
xt , the
intrA-Graph consistency Loss (AGL) is deﬁned as:

xt and the supervision matrix M T

P1≤m,n≤|RT

xt
max(1,

|

(M T

xt )(m,n) · (1 − (E S

xt )(m,n))

P1≤m,n≤|RT

xt

(M T

xt )(m,n))

|

.

(9)

xt

Note that LAGL
is triggered when at least two regions share
the same pseudo label in Rτ
xt . By minimizing the inter-
graph consistency loss, the similarity between regions with
the same pseudo label in student is enhanced, pursuing low-
er intra-class variation within the graph of student.

4.5. Optimization

Training Objective. The overall training objective of
our MTOR integrates the supervised loss Lsup on labeled
source data Ds and three consistency losses, i.e., region-
level consistency LRCL
in Eq.(5), inter-graph consistency
LEGL
in Eq.(9)
on unlabeled target data Dt:

in Eq.(7) and intra-graph consistency LAGL

xt

xt

xt

L = X(xs,Bs)∈Ds

Lsup(xs, Bs)+λ· Xxt∈Dt

(LRCL

xt +LEGL

xt +LAGL

xt

),

(10)

where λ is the tradeoff parameter.

Weights Update. The student network F S is opti-
mized with standard SGD algorithm by minimizing L. The
weights of teacher network F T at iteration t are updated as
the exponential moving average of student weights:

wt

F T = α · wt−1

F T + (1 − α) · wt−1
F S ,

(11)

where α denotes smoothing coefﬁcient parameter.

11461

5. Experiments

We conduct extensive evaluations of our MTOR for
cross-domain detection in two different domain shift s-
cenarios, including one normal-to-foggy weather transfer
in urban scene (Cityscapes [6] → Foggy Cityscapes [43])
and two synthetic-to-real transfers (i.e., SIM10k [19] → C-
ityscapes and 3D CAD-rendered images → real images in
Syn2Real detection dataset [34]).

5.1. Dataset and Experimental Settings

Dataset. The Cityscapes dataset (C) is a popular seman-
tic understanding benchmark in urban street scenes with
pixel-level annotation, containing 2,975 images for train-
ing and 500 images for validation. Since it is not dedicat-
ed for detection, we follow [4] and generate the bounding
box annotations by the tightest rectangles of each instance
segmentation mask for 8 categories (person plus 7 kinds of
transports). Foggy Cityscapes (F) is a recently proposed
synthetic foggy dataset which simulates fog on real scenes.
Each foggy image is rendered with clear image and depth
map from Cityscapes. Thus the annotations and data split
in Foggy Cityscapes are inherited from Cityscapes. SIM10k
(M) dataset contains 10k images rendered from computer
game—Grand Theft Auto V (GTA 5) with bounding box
annotations for cars. The Syn2Real detection dataset is
the largest synthetic-to-real object detection dataset to date
with over 70k images in the training, validation and test-
ing domains. The training domain consists of 8k synthet-
ic images (S) which are generated from 3D CAD models.
Each object is rendered independently and placed on a white
background. The validation domain includes 3,289 real im-
ages from COCO [25] (O) and the testing domain contains
60,863 images from video frames in YTBB [36] (Y).

Normal-to-Foggy Weather Transfer. We follow [4]
and evaluate C → F for transfer across different weather
conditions. The training set in Cityscapes is taken as source
domain. We use the training set in Foggy Cityscapes as tar-
get domain and results are reported on its validation set.

Synthetic-to-Real Image Transfer. We consider two
directions for synthetic-to-real transfers: M → C and S →
O/Y. For M → C, we utilize the entire SIM10k as source do-
main and leverage Cityscapes training set as target domain.
The results are reported on Cityscapes validation split. For
S → O/Y on Syn2Real detection dataset, we take the train-
ing set (synthetic images) as source domain and the valida-
tion set (COCO)/testing set (YTBB) as target domain. Since
the annotations of testing set are not publicly available, we
submit results to online testing server for evaluation.

Implementation Details. For C → F and M → C, we
adopt the 50-layer ResNet [15] pre-trained on ImageNet
[41] as the basic architecture of Faster R-CNN backbone.
For the more challenging S → O/Y, the Faster R-CNN
backbone is mainly constructed on 152-layer ResNet. For

all transfers, we utilize “image-centric” sampling strategy
[13]. Each input image is resized such that its scale (short-
er edge) is 600 pixels. Each mini-batch contains 2 images
per GPU, one from the source domain and the other from
the target domain. We train on 4 GPUs (so effective mini-
batch size is 8) and each image has 128 sampled anchors,
with a ratio of 1:3 of positive to negatives [13]. We im-
plement MTOR based on MXNet [3]. Speciﬁcally, the net-
work weights are trained by SGD optimizer with 0.0005
weight decay and 0.9 momentum. The learning rate and
maximum training epoch are set as 0.001 and 10 for all ex-
periments. The conﬁdence threshold ǫ is empirically set to
0.98 for C → F and M → C, and 0.99 for S → O/Y. The
tradeoff parameter λ in Eq.(10) and the smooth coefﬁcient
parameter α in Eq.(11) is set as 1.0 and 0.99, respective-
ly. Moreover, our MTOR is ﬁrstly pre-trained on labeled
source data. For data augmentations on target images, we
ﬁrstly augment each target image with the same spatial per-
turbation including random cropping, padding, or ﬂipping.
Next, we additionally perform two different kinds of image
augmentations with random color jittering (i.e., brightness,
contrast, hue and saturation augmentations) or PCA noise,
resulting in two perturbed target samples, one for studen-
t and the other for teacher. Following [4], we report mAP
with a IoU threshold of 0.5 for evaluation.

Compared Approaches. To empirically verify the mer-
it of our MTOR, we compare the following methods: (1)
Source-Only directly exploits the Faster R-CNN model
trained on source domain to detect objects in target sam-
(2) DA[4] designs two domain classiﬁers to allevi-
ples.
ate both image-level and region-level domain discrepancy,
which are further enforced with a consistency regularizer.
(3) MTOR is the proposal in this paper. Moreover, we de-
sign three degraded variants trained with region-level con-
sistency (MTORR), region-level plus inter-graph consis-
tency (MTORRE), and region-level plus intra-Graph con-
sistency (MTORRA). (4) Train-on-target is an oracle run
that trains Faster R-CNN on all the labeled target samples.

5.2. Performance Comparison and Analysis

Normal-to-Foggy Weather Transfer. Table 1 shows the
performance comparisons on Foggy Cityscapes validation
set for C → F transfer. Overall, the results with regard to
mAP score indicate that our proposed MTOR achieves su-
perior performance against state-of-the-art technique (DA).
In particular, the mAP of MTOR can achieve 35.1%, mak-
ing 3.1% absolute improvement over the best competitor
DA. The performances of Source-only which trains Faster
R-CNN only on the labeled source data can be regarded
as a lower bound without adaptation. By additionally in-
corporating the domain classiﬁer in both image and region
level, DA leads to a large performance boost over Source-
only, which basically indicates the advantage of alleviat-

11462

Table 1. The mean Average Precision (mAP) of different models on Foggy Cityscapes validation set for C → F transfer.

RCL

EGL

AGL

person

rider

Source-only
DA [4]
MTORR
MTORRE
MTORRA
MTOR
Train-on-target

X

X

X

X

X

X

X

X

25.7
29.2
30.8
28.7
29.6
30.6
31.4

35.9
40.4
41.5
40.1
41.2
41.4
42.6

Table 2. The Average Precision (AP) of car on Cityscapes valida-
tion set for M → C transfer.

RCL EGL AGL

car AP

Source-only
DA [4]
MTORR
MTORRE
MTORRA
MTOR
Train-on-target

X

X

X

X

X

X

X

X

39.4
41.9
45.9
46.1
46.3
46.6
58.6

ing the domain discrepancy over the source and target da-
ta. Note that for fair comparison, we re-implemented DA
based on the same 50-layer ResNet architecture. How-
ever,
the performances of DA are still lower than our
MTORR, which utilizes region-level consistency regular-
ization in Mean Teacher paradigm. This conﬁrms the ef-
fectiveness of enforcing region-level consistency under per-
turbations of unlabeled target samples for cross-domain de-
tection. In addition, by further integrating object relations
into Mean Teacher paradigm through graph-structured con-
sistency from inter-graph or intra-graph perspective, our
MTORRE and MTORRA improve MTORR. The result-
s demonstrate the advantage of inter-graph consistency to
match the graph structures between teacher and student, and
intra-graph consistency to enhance the similarity between
regions of same class in student. By simultaneously uti-
lizing region-level and two graph-structured consistencies,
MTOR further boosts up the performances, which indicates
the merit of jointly exploiting inter-graph and intra-graph
consistencies in Mean Teacher paradigm.

Synthetic-to-Real Image Transfer. The performance
comparisons for synthetic-to-real transfer task on M → C
are summarized in Table 2. Our MTOR exhibits better per-
formance than other runs. In particular, the AP of car for
MTOR can reach 46.6%, making the absolute improvement
over DA by 4.7%. Similar to the observations in normal-
to-foggy weather transfer, MTORR performs better than
DA by aligning region-level predictions in Mean Teacher
and the performance is further improved by incorporating
inter-graph and intra-graph consistency in MTORRE and
MTORRA. Combining all the three consistency regulariza-
tions, our MTOR achieves the best performance.

We further evaluate our approach for S → O/Y transfer
on the more challenging Syn2Real detection dataset. Ta-
ble 3 shows the performance comparisons on S → O trans-

car

36.0
43.4
44.1
45.9
43.7
44.0
51.7

truck

19.4
19.7
21.6
22.9
22.2
21.9
28.8

bus

30.8
38.3
37.8
38.0
38.4
38.6
43.4

train

mcycle

bicycle

mAP

9.7
28.5
35.1
38.6
40.9
40.6
40.2

29.0
23.7
26.7
26.9
27.8
28.3
31.7

28.9
32.7
35.8
34.9
35.3
35.6
33.2

26.9
32.0
34.2
34.5
34.9
35.1
37.9

Figure 4. Examples of detection results on COCO for S → O.
fer. A clear performance improvement is achieved by our
proposed MTOR over other baselines. Similar to the ob-
servations on the transfers across SIM10k, Cityscapes, and
Foggy Cityscapes, MTORR performs better than DA by
taking region-level consistency on target samples into ac-
count for cross-domain detection. Moreover, MTORRE and
MTORRA exhibit better performance than MTORR by ad-
ditionally pursuing inter-graph and intra-graph consistency
respectively, and further performance improvement is at-
tained when exploiting region-level consistency plus two
graph-structured consistencies by MTOR. We also submit-
ted our MTOR, Source-only, and DA to online evaluation
server and evaluated the performances on ofﬁcial testing set.
Table 3 summaries the performances on ofﬁcial testing set
YTBB for S → Y transfer. The results clearly show that our
MTOR outperforms two other baselines.

Qualitative Analysis. Figure 4 showcases four exam-
ples of detection results on COCO for S → O transfer by
three approaches, i.e., Source-only, DA and our MTOR. The
exemplar results clearly show that our MTOR can generate
more accurate detection results by exploring region-level
and graph-structured consistency in Mean Teacher paradig-
m to boost cross-domain detection. For instance, MTOR
correctly detects person in the fourth image which is missed
in Source-only and DA.

Effect of the Parameters λ and α. To clarify the ef-
fect of tradeoff parameter λ in Eq.(10) and smoothing co-
efﬁcient parameter α in Eq.(11), we show the performance
curves with different tradeoff/smoothing coefﬁcient param-
eters in Figure 5. As shown in the ﬁgure, we can see that
both mAP curves of λ and α are generally like the “∧”
shapes when λ varies in a range from 0.1 to 5.0 and α varies
in a range from 0.92 to 0.9999. The best performance is
achieved when λ is 1.0 and α is about 0.98.

Error Analysis of Highest Conﬁdent Detections. To

11463

(a) Source-only(b) DA(c) MTORbicyclebicyclehorsehorsebicycleplantmotorcarbicyclehorsemotorcarhorsebicyclehorsebicyclepersonmotorplantpersonbicyclemotorpersoncarTable 3. The mean Average Precision (mAP) of different models on Syn2Real detection dataset for S → O/Y transfers.

RCL EGL AGL plane bcycl bus

car horse knife mcycl person plant sktbd train truck mAP

mAP on validation set (COCO) for S → O transfer:
Source-only
DA [4]
MTORR
MTORRE
MTORRA
MTOR
Train-on-target

30.0
30.3
32.0
33.3
X 35.4
X 35.5
84.5

25.3 31.3 14.0 17.3
24.1 31.3 14.0 17.4
22.8 29.1 15.3 20.8
21.2 32.9 13.1 18.1
24.0 32.1 14.9 19.1
24.9 32.9 15.4 19.1
52.2 77.5 58.7 76.1

X

X

X

X

X

X

mAP on ofﬁcial testing set (YTBB) for S → Y transfer:
Source-only
DA [4]
MTOR (Ours)

28.4
38.0
X 42.8

18.4 23.8 28.4 35.8
16.1 23.3 30.7 33.0
21.0 31.3 33.3 42.9

X

X

1.9
1.3
0.6
3.1
1.8
1.8
28.9

3.6
4.7
10.2

25.6
27.4
32.4
32.2
31.6
31.4
65.4

35.7
34.8
38.5

18.5
18.9
22.2
24.0
24.2
21.8
71.9

8.6
6.1
7.2

2.2
14.7 14.7 21.1
3.1
17.5
14.5 21.8
18.2 36.9
0.6
0.5
20.5 34.4
0.6
1.4
2.0
3.7
18.9 31.7
14.4 18.9 30.4
1.7
49.2 70.5 83.8 52.5

14.8
8.4
15.7
14.0
12.9 18.0

6.4
9.8
7.2

5.2
9.5
8.2

18.1
18.5
19.3
19.6
20.0
20.7
64.3

18.1
19.6
22.8

Figure 5. Effect of parameters λ and α on C → F transfer.

Figure 6. Error analysis of highest conﬁdent detections on C → F.

further clarify the effect of the proposed region-level and
graph-structured consistencies in Mean Teacher paradigm,
we analyze the accuracies of Source-only, DA and MTOR
caused by the highest conﬁdent detections on Foggy C-
ityscapes for C → F transfer. We follow [4, 17] and catego-
rize the detections into 3 types: Correct (IoU with ground-
truth ≥ 0.5), Mis-Localized (0.5 > IoU with ground-truth
≥ 0.3) and Background (IoU with ground-truth < 0.3).
For each class, we select top-K predictions where K is
the number of ground-truth bounding boxes in this class.
We report the mean percentage of each type across all cat-
egories in Figure 6. Compared to Source-only, DA and our
MTOR clearly improve the number of correct detections (o-
range color) and reduce the number of false positives (other
colors). Moreover, by leveraging region-level and graph-
structured consistencies in Mean Teacher, MTOR leads to
both smaller mis-localized and background errors than DA.

Visualization of Relational Graph. Figure 7 further
shows the visualization of an exemplar relational graph (i.e.,
the afﬁnity matrix) learned by Source-only, DA and our M-
TOR on Foggy Cityscapes for C → F transfer. For each ap-
proach, we extract the region representation of each ground-
truth region and construct the relational graph by computing
cosine similarity between every two regions. Note that the
ﬁrst three regions belong to car class and the rest four re-
gions fall into person class. Thus we can clearly see that
most intra-class similarities of MTOR are higher than those
of Source-only and DA. The results demonstrate the advan-

Figure 7. Visualization of relational graph on Foggy Cityscapes.

tage of enforcing intra-graph consistency in MTOR, leading
to more discriminative region feature for object detection.

6. Conclusions

We have presented Mean Teacher with Object Relations
(MTOR), which explores domain adaptation for object de-
tection in an unsupervised manner. Particularly, we study
the problem from the viewpoint of both region-level and
graph-structured consistencies in Mean Teacher paradigm.
To verify our claim, we have built two relational graphs
that capture similarities between pairs of regions for teach-
er and student respectively. The region-level consisten-
cy is to align the region-level predictions between teacher
and student, which facilitates domain adaptation at region-
level. The inter-graph consistency further matches the graph
structures between teacher and student, pursuing a noise-
resistant holistic graph structure on target domain. In addi-
tion, intra-graph consistency is utilized to enhance the sim-
ilarity between regions of same class in student, which ide-
ally leads to graph with lower intra-class variation. Experi-
ments conducted on the transfers across Cityscapes, Foggy
Cityscapes, and SIM10k validate our proposal and analy-
sis. More remarkably, we achieve state-of-the-art perfor-
mance of single model on synthetic-to-real image transfer
in Syn2Real detection dataset.

Acknowledgments. This work was supported in part by
National Key R&D Program of China under contract No.
2017YFB1002203 and NSFC No. 61872329.

11464

34.2%34.4%34.6%34.8%35.0%35.2%mAPTradeoff parameter 0.10.20.51.02.05.034.2%34.4%34.6%34.8%35.0%35.2%35.4%mAPSmoothing coefficient parameter0.920.950.980.990.9990.9999(a) Source-only(b) DA(c) MTOR29%7%64%34%10%56%38%8%54%CorrectMis-localizedBackgroudcar1car2car3person1person2person3person4(a) Source-only(b) DA(c) MTORReferences

[1] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and An-
drew Gordon Wilson.
Improving consistency-based semi-
supervised learning with weight averaging. arXiv preprint
arXiv:1806.05594, 2018.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE Trans. on PAMI, 2018.
[3] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang,
Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and
Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learn-
ing library for heterogeneous distributed systems. In Work-
shop on Machine Learning Systems, NIPS, 2016.

[4] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and
Luc Van Gool. Domain adaptive faster r-cnn for object de-
tection in the wild. In CVPR, 2018.

[5] Yuhua Chen, Wen Li, and Luc Van Gool. Road: Reality ori-
ented adaptation for semantic segmentation of urban scenes.
In CVPR, 2018.

[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Tim-
o Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016.

[7] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object
detection via region-based fully convolutional networks. In
NIPS, 2016.

[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In ICCV, 2017.

[9] Geoffrey French, Michal Mackiewicz, and Mark Fisher.

Self-ensembling for domain adaptation. In ICLR, 2018.

[10] Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi,
and Alexander C Berg. Dssd: Deconvolutional single shot
detector. arXiv preprint arXiv:1701.06659, 2017.

[11] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain

adaptation by backpropagation. ICML, 2015.

[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. JMLR, 2016.

[13] Ross Girshick. Fast r-cnn. In ICCV, 2015.
[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, 2014.

[19] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,
Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.
Driving in the matrix: Can virtual worlds replace human-
generated annotations for real world tasks? ICRA, 2017.

[20] Samuli Laine and Timo Aila. Temporal ensembling for semi-

supervised learning. In ICLR, 2017.

[21] Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient
semi-supervised learning method for deep neural network-
s. In Workshop on Challenges in Representation Learning,
ICML, 2013.

[22] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yang-
dong Deng, and Jian Sun. Light-head r-cnn: In defense of
two-stage object detector. arXiv preprint arXiv:1711.07264,
2017.

[23] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017.

[24] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollar. Focal loss for dense object detection. In ICCV,
2017.

[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV, 2014.

[26] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In ECCV, 2016.

[27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Ful-
In

ly convolutional networks for semantic segmentation.
CVPR, 2015.

[28] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-
dan. Learning transferable features with deep adaptation net-
works. In ICML, 2015.

[29] Mingsheng Long, Jianmin Wang, and Michael I Jordan.
In

Deep transfer learning with joint adaptation networks.
ICML, 2017.

[30] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised domain adaptation with residual trans-
fer networks. In NIPS, 2016.

[31] Yingwei Pan, Yehao Li, Ting Yao, Tao Mei, Houqiang Li,
and Yong Rui. Learning deep intrinsic video representation
by exploring temporal coherence and graph structure. In IJ-
CAI, 2016.

[32] Yingwei Pan, Ting Yao, Tao Mei, Houqiang Li, Chong-Wah
Ngo, and Yong Rui. Click-through-based cross-view learn-
ing for image search. In SIGIR, 2014.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[33] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu
Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large
mini-batch object detector. In CVPR, 2018.

[16] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell.
Fcns in the wild: Pixel-level adversarial and constraint-based
adaptation. arXiv preprint arXiv:1612.02649, 2016.

[17] Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun

Dai. Diagnosing error in object detectors. In ECCV, 2012.

[18] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen
Wei. Relation networks for object detection. In CVPR, 2018.

[34] Xingchao Peng, Ben Usman, Kuniaki Saito, Neela Kaushik,
Judy Hoffman, and Kate Saenko. Syn2real: A new bench-
mark forsynthetic-to-real visual domain adaptation. arXiv
preprint arXiv:1806.09755, 2018.

[35] Anant Raj, Vinay P Namboodiri, and Tinne Tuytelaars. Sub-
space alignment based domain adaptation for rcnn detector.
BMVC, 2015.

11465

[36] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,
and Vincent Vanhoucke. Youtube-boundingboxes: A large
high-precision human-annotated data set for object detection
in video. In CVPR, 2017.

[37] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In CVPR, 2016.

[38] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster,

stronger. In CVPR, 2017.

[39] Joseph Redmon and Ali Farhadi. Yolov3: An incremental

improvement. arXiv preprint arXiv:1804.02767, 2018.

[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. IJCV, 2015.

[42] Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada.
Asymmetric tri-training for unsupervised domain adaptation.
In ICML, 2017.

[43] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Se-
mantic foggy scene understanding with synthetic data. IJCV,
2018.

[44] Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo,
and Rama Chellappa. Generate to adapt: Aligning domains
using generative adversarial networks. CVPR, 2018.

[45] Behjat Siddiquie, Vlad I Morariu, Fatemeh Mirrashed, Roge-
rio S Feris, and Larry S Davis. Domain adaptive object de-
tection. In WACV, 2013.

[46] Bharat Singh, Hengduo Li, Abhishek Sharma, and Larry S
Davis. R-fcn-3000 at 30fps: Decoupling detection and clas-
siﬁcation. In CVPR, 2018.

[47] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincen-
t Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, 2015.

[48] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In NIPS, 2017.

[49] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrel-
l. Adversarial discriminative domain adaptation. In CVPR,
2017.

[50] Ting Yao, Chong-Wah Ngo, and Shiai Zhu. Predicting do-

main adaptivity: redo or recycle? In ACM MM, 2012.

[51] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring

visual relationship for image captioning. In ECCV, 2018.

[52] Ting Yao, Yingwei Pan, Chong-Wah Ngo, Houqiang Li, and
Tao Mei. Semi-supervised domain adaptation with subspace
learning for visual recognition. In CVPR, 2015.

[53] Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao
Mei. Fully convolutional adaptation networks for semantic
segmentation. In CVPR, 2018.

11466

