Learning Linear Transformations for Fast Image and Video Style Transfer

Xueting Li∗1, Sifei Liu∗2, Jan Kautz2, and Ming-Hsuan Yang1,3

1University of California, Merced, 2NVIDIA, 3Google Cloud

Abstract

Given a random pair of images, a universal style trans-
fer method extracts the feel from a reference image to syn-
thesize an output based on the look of a content image.
Recent algorithms based on second-order statistics, how-
ever, are either computationally expensive or prone to gen-
erate artifacts due to the trade-off between image qual-
ity and run-time performance.
In this work, we present
an approach for universal style transfer that learns the
transformation matrix in a data-driven fashion. Our al-
gorithm is efﬁcient yet ﬂexible to transfer different levels
of styles with the same auto-encoder network. It also pro-
duces stable video style transfer results due to the preser-
vation of the content afﬁnity.
In addition, we propose a
linear propagation module to enable a feed-forward net-
work for photo-realistic style transfer. We demonstrate
the effectiveness of our approach on three tasks: artistic
style, photo-realistic and video style transfer, with com-
parisons to state-of-the-art methods. The project web-
site can be found at https://sites.google.com/
view/linear-style-transfer-cvpr19.

1. Introduction

A style transfer method takes a content image and a style
image as inputs to synthesize an image with the look from
the former and feel from the latter.
In recent years, nu-
merous style transfer methods have been developed. The
method by Gatys et al. [8] iteratively minimizes content and
style reconstruction losses between the target image and in-
put images. To reduce the computational cost, a few ap-
proaches have since been developed based on feed-forward
networks [13, 30]. However, these approaches do not gen-
eralize to universal style images with one single network.

For universal style transfer, a number of methods ex-
plore the second order statistical transformation from ref-
erence image onto content image via a linear multiplica-
tion between content image features and a transformation
matrix [12, 18, 19]. The AdaIn method [12] matches the

⇤equal contribution

(a) artistic

(b) photo-realistic

(c) video

Figure 1. Applications of the proposed algorithm. (a) Artistic style
transfer. (b) Photo-realistic style transfer. (c) Video style transfer
(click on the image to see animations using Adobe Reader). The
style thumbnails are shown in lower right corners.

means and variances of deep features between content and
style images. The WCT [18] algorithm further exploits the
feature covariance matrix instead of the variance, by em-
bedding both whitening and coloring processes within a
pre-trained encoder-decoder module. However, these ap-
proaches directly compute these matrices from deep fea-
ture vectors. As such, these methods do not present gen-
eral solutions to this problem. Furthermore, such matrix-
computation-based methods can be computationally expen-
sive due to the high dimensionality of deep feature vectors.
In this work, we propose a learnable linear transforma-
tion matrix which is conditioned on an arbitrary pair of con-
tent and style images. We derive a linear transform and
draw connections to the reconstruction objective (squared
Frobenius norm of the difference between Gram matrices)
widely used in style transfer [8, 13, 30, 12]. We learn the
transformation matrix with two light-weighted CNNs, and
show that this approach is signiﬁcantly faster than comput-
ing the transforms from features (e.g., [18, 19]). Speciﬁ-
cally, the learning-based transformation matrix can be con-
trolled by different levels of style losses and is computation-
ally efﬁcient. In addition, we present a linear propagation
module [22] that can correct distortions and artifacts in con-
tours and textures, which are commonly observed in style

13809

transfer results. We then integrate this module into our style
transfer network to generate undistorted results for photo-
realistic style transfer.

The contributions of this work are summarized as fol-
lows. First, we propose an efﬁcient (about 140 fps) and
ﬂexible style transfer model that preserves content afﬁnity
during the style transfer process. Second, we show that the
proposed method can be ﬂexibly applied to different tasks,
including but not limited to artistic style and video style
transfer by slightly modifying the network architecture (see
Fig. 1(a) and (c)). Third, we develop a linear propagation
module that can be equipped with a feed-forward neural net-
work to generate high-quality undistorted results for photo-
realistic style transfer (see Fig. 1(b)).

2. Related Work

Deep learning based style transfer has been intensively
studied [15, 32, 33, 9, 31, 17, 1, 7] to match statistical infor-
mation between content and style images based on features
extracted from pre-trained convolutional neural networks.
One main drawback with the method by Gatys et al. [8] is
the heavy computational cost due to the iterative optimiza-
tion process. Fast feed-forward approaches [13, 30, 16] ad-
dress this issue by training feed-forward neural networks
that minimize the same feature reconstruction loss and style
reconstruction loss as [8]. However, each feed-forward net-
work is trained to transfer one ﬁxed style. Dumoulin et
al. [4] introduce an instance normalization layer that allows
32 styles to be represented by one model, and Li et al. [17]
encode 1,000 styles by using a binary selection unit for im-
age synthesis. Nevertheless these models are not able to
transfer an arbitrary style onto a content image.

Universal style transfer. Several methods [10, 12, 27,
34] have been proposed to match mean and variance of
content and style feature vectors to transfer an arbitrary
style onto a content image. However, these methods do
not model the covariance of features and may not synthe-
size images well. Li et al. [18] resolve this problem by
applying both whitening and coloring transforms with pre-
trained image reconstruction auto-encoders. However, this
approach is computationally expensive due to the need of
large matrix decompositions at multiple levels. Shen et
al.[6] propose to train a meta network that generates a 14
layer network for each content and style image pair. How-
ever, it requires extra memory for each content/style pair
and does not explicitly model the second-order image statis-
tics.

Photo-realistic style transfer. Photo-realistic style trans-
fer methods [19, 23] aim to synthesize images without dis-
torting geometric structures. Luan et al. [23] introduce a
local-afﬁnity based energy term as an extra loss function of
the model by Gatys et al. [8], where stylized results are ob-

⊗

+

u

c

!,

transformation

r
e
d
o
c
e
d

 

G
G
V

r
e
d
o
c
n
e

!"

!#

-"

-*

!"

!#

CONVs

CONVs

)
!(

)
!*

) 	
⊗ %&' !(

)	
⊗ %&' !*

fc

fc

⊗

+

s
V
N
O
C

n
o
i
t
u
l
o
v
n
o
c

n
o
i
t
u
l
o
v
n
o
c

n
o
i
t
u
l
o
v
n
o
c

c : compress
u : uncompress
⊗: matrix multiplication

-
"

-
*

l
o
s
s

c
o
n
t
e
n
t
 

G
G
V

e
l
u
d
o
m

 
s
s
o
l

…
s
t
y
l
e

l
o
s
s

Figure 2. Overview of the proposed method. Our model contains a
pre-trained encoder and a decoder, a loss module, a transformation
module with the compress/uncompress blocks. Only the transfor-
mation module, as well as the pair of compress and uncompress
blocks are learnable, while all the others are ﬁxed (black). We
use orange arrows to denote the losses and “T” to denote the point
where a style is transformed (see Section 3.2 for technical details).

tained through time-consuming optimization. Li et al. [19]
replace the computationally expensive transforms with a
feed-forward network and solve the optimization problem
with a closed-form solution to synthesize images. In this
work, we introduce an efﬁcient linear propagation module
for photo-realistic style transfer. The whole pipeline can
be implemented by a single feed-forward network, which is
faster and GPU-friendly compared to existing methods.

3. Style Transfer by Linear Transformation

The proposed model contains two feed-forward net-
works, a symmetric encoder-decoder image reconstruction
module and a transformation learning module, as shown in
Fig. 2. The encoder-decoder is trained to reconstruct any
input image faithfully. It is then ﬁxed and serves as a re-
construction network in the remaining training procedures.
Instead of computing the transformation of the 2nd moment
statistic in the intermediate layers of the auto-encoder, as
being conducted in many previous work [18, 19, 29], we
make the transformation matrix learnable by outputting it
via a light-weighted CNN block. To learn the transfor-
mation via a feed-forward network, we need some super-
vision signal. We use a pre-trained VGG-19 network to
compute style losses at multiple levels and one content loss
in a way similar to the prior work [13, 12]. The proposed
feed-forward convolutional neural network, which is able to
transfer arbitrary styles efﬁciently at 140 fps, is much faster
than the computation-based methods.

3.1. Learning Universal Style Transfer

We denote the feature map of the top-most encoder layer
as F (I) 2 RN ⇥C , where I is an input image, N is the num-
ber of pixels, and C is the number of channels. For presen-
tation clarity, the feature maps of a content and style image
pair are denoted as row vectors Fc and Fs. We formulate
the image style transfer problem as a linear transformation
between the content feature Fc and learned matrix T , with
the transformed feature vector Fd. We use φs to denote a

23810

“virtual” feature map that provides the desired style. For
several style transfer methods based on matrix transforma-
tion [18, 12], φs = Fs. In this work, φs accommodates mul-
tiple forms based on different conﬁgurations of style losses.
Thus, φs can be described as a nonlinear mapping of Fs as
φs = φ(Fs). We denote ¯F as the vectorized feature map F
with zero mean.

Our goal is to learn the optimal Fd such that the statistic
of the transformed feature from the encoder matches that of
the desired style , which is determined by the style losses
in the loss network. It can be expressed as minimizing the
difference of centered covariance between Fd and φs:

F ⇤

d = arg minFd

1

N C k ¯Fd ¯Fd
s.t. ¯Fd = T ¯Fc.

>

  ¯φs ¯φs

>

k2

F

(1)

By substituting the linear constraint into Eq. (1), the minima
is obtained when

T ¯Fc ¯Fc

>

T > = ¯φs ¯φs

>

.

(2)

The centered covariance of ¯Fc is cov (Fc) = ¯Fc ¯Fc
VcDcV >
sition (SVD) is cov (φs) = ¯φs ¯φs
to show that

=
c , and the corresponding singular value decompo-
s . It is easy

= VsDsV >

>

>

T = ⇣VsD

1

2

s V >

s ⌘ U ⇣VcD

  1
c V >
2

c ⌘ ,

(3)

is one set of solutions to Eq. (2) where U 2 RC⇥C is a C-
dimensional orthogonal group. In other words, T is solely
determined by the covariance of the content and style im-
age feature vectors. Given T , the transformed feature is ob-
tained by ¯Fd + mean(Fs), which simultaneously aligns to
the mean and the covariance statistics of the target style, and
thus describes the style reconstruction used by most exist-
ing approaches. In the following, we discuss how to select
a proper model for learning T .

3.2. Learning Transformation T

Given that T is only conditioned on the content and style
images, one feasible approach is to use a network that takes
both images to directly output a C ⇥ C matrix. According
to Eq. (3), the terms of content and style are decoupled, so
we use two independent CNNs for the content/style inputs.
The formulation in Eq. (2) suggests three input options
to the CNNs: (i) images (c and s), (ii) feature maps (Fc and
Fs), and (iii) covariance matrices (cov (Fc) and cov (Fs)).
In this work, we use the third option where each CNN takes
the covariance of feature vectors and outputs a C ⇥ C inter-
mediate matrix. These two matrices are then multiplied to
formulate the T as shown in Fig. 2. We explain the design
options in the following paragraph.

First, we hope the module that outputs T should be ﬂex-
ibly adapted to both a full content image, and an arbitrarily-
shaped region, e.g., for stylization within a segmentation

(a) Content (b) Style (c) WCT (d) AdaIn (e) Ours

Figure 3. Style transfer using a shallow auto-encoder described in
Section 3.3. Our method faithfully captures styles even when a
shallow auto-encoder is used.

 1

mask, as in Section 5. This property does not hold when the
model input is the image or feature map. For example, it
is easy to show that T = ¯φsU ¯Fc
is one of the solutions
for Eq. (2), which is based on feature inputs. However, it
also requires that the content and style features inputing to
the transformation module have the same dimensions. Sec-
ond, since T describes the style transformation, it focus on
describing the global statistics shift, rather than any image
spatial information. Amazingly, using the covariance ma-
trices as the inputs addresses both challenges. The ablation
study in Section 5 shows that using a covariance matrix as
model input leads to better stylized images (see Fig. 5).

3.3. Learning Video Style Transfer

We show that since the transformation is learnable, the
model ﬂexibly accommodates numerous combinations of
the auto-encoder and the loss modules. Here we show
a practical solution for video style transfer. By using a
shallower auto-encoder, the network can generate stabilized
transferred videos as the proposed linear transformation
method is able to preserve afﬁnity across frames. Instead
of enforcing temporal consistency through any cross-frame
alignment strategy, e.g., optical ﬂow warping [11], we en-
sure a transferred video to have similar afﬁnity as the con-
tent video. Although the same principle could be applied
to both WCT and Adain methods, it is difﬁcult to preserve
afﬁnity as these algorithms are developed based on rela-
tively deep auto-encoders (see Fig. 10) and perform poorly
when shallower auto-encoders are used (see Fig. 3).

Afﬁnity describes the pairwise relation of pixels, fea-
tures, or other image elements. Preserving the afﬁnity of
content image/video, which indicates that the dense pair-
wise relations among pixels in the content image are pre-
served well in the stylized result, is a key solution for gener-
ating stabilized video. In principle, style transfer and afﬁn-
ity preservation belong to different forms of matrix multipli-
cations: the former is pre-multiplication while the latter is
post-multiplication to the feature vector (see Eq. (1)). Given
Eq. (1) and the normalized afﬁnity for a vectorized feature
F 2 RN ⇥C:

aﬀ (F ) = ¯F >cov (F ) 1 ¯F

(4)

It is straightforward to show that the afﬁnities of Fc and

Fd are equal to each other by the following equations:

33811

whitening

 

G
G
V

r
e
d
o
c
n
e

whitening

 

G
G
V

r
e
d
o
c
n
e

n
o
i
t
a
m
r
o
f
s
n
a
r
t

MSE

r
e
d
o
c
e
d

r
e
d
o
c
e
d

Training

Testing

SPN

SPN

Figure 4. Photo-realistic style transfer with the linear propagation
module. We ﬁrst ﬁx pretrained auto-encoder and train a SPN mod-
ule on reconstructed images to minimize distortions caused by
the auto-encoder using the whitened content image as guidance.
Given a test image, we directly apply it on style transferred image.

>

¯Fd

cov (φs) 1 ¯Fd = ¯Fd
T >⌘ 1

T > ⇣T ¯Fc ¯Fc

>

>

= ¯Fc

> ⇣T ¯Fc ¯Fc
T ¯Fc = ¯Fc

>

>

¯Fd

T >⌘ 1
cov (Fc) 1 ¯Fc.

(5)

Thus, the proposed linear transformation method is capable
of preserving the feature afﬁnity of the content image.

Although the afﬁnity is preserved via the proposed lin-
ear transformation model, there are three more factors can
cause temporal inconsistency: (i) the non-linear layers in
the decoder; (ii) the trace tr ⇣ ¯φs ¯φs
>⌘, determined by how
the loss is enforced (see similar analysis in [11]), that af-
fects Fd; and (iii) the spatial resolution of the top layer that
determines the scale at which the afﬁnity is preserved. In
this work, we use a shallower model up to the relu3 1 layer
of the VGG-19 model to better preserve afﬁnity. This shal-
lower model can still express rich stylization when a rela-
tively deep style loss network (e.g., using up to relu4 1 in
the loss module) is used. We show that the stylized results
contain fewer distortions and are more stable than existing
methods [8, 13] that are not based on the formulation in
Eq. (1) (see Fig. 8 and 10). Note that for a shallower en-
coder, the corresponding Fc (e.g., relu2 1) is not expressive
enough to transfer abstract styles.

3.4. Learning Photo-realistic Style Transfer

Photo-realistic style transfer is another important appli-
cation which aims to synthesize images without structural
distortions. Although using a shallower network described
in Section 3.3 can generate results that better align with
the content images, the stylized results still contain signiﬁ-
cant artifacts due to the distortion caused by the deep auto-
encoder. To address this issue, we exploit the spatial propa-
gation network (SPN) proposed by Liu et al. [22]. The SPN
is a generic framework that learns to model pixel pairwise
relations. We insert a SPN as a “anti-distortion ﬁlter” on top
of the auto-encoder, while the coefﬁcients of the ﬁlter are
learned through a CNN guidance network in a data-driven
manner (See [22] for more details). Speciﬁcally, we train

a SPN using the reconstructed and content images since no
photo-realistic stylized ground truth is available. The con-
tent image also serves as the input to the guidance network.
Then we apply the SPN on the stylized image to minimize
distortions caused by the auto-encoder. However, afﬁnity
learned directly from the original content image will include
color information and affect the stylized image. Instead of
using the original content image as guidance, we use the
whitened content image to remove color information and
encode only afﬁnity information. Fig. 4 illustrates the train-
ing and testing process of our method.

4. Model Analysis

In this section, we discuss the advantages of the linear
transformation method for style transfer in terms of run-
time efﬁciency and model size. The analysis of the propa-
gation module for photo-realistic style transfer is presented
in Section 5.
Run-time efﬁciency. Our method performs efﬁciently (see
Table 1) since it does not involve time-consuming matrix
computation, e.g., SVD in WCT [18]. On the other hand,
the AdaIn [12] method performs efﬁciently as only ﬁrst-
order statistics (i.e., variance) are involved but at the ex-
pense of image quality. We show that while the AdaIn
method can generate favorable results with a deeper auto-
encoder, the proposed model performs well since the co-
variance shift is accurately modeled with a much shallower
auto-encoder (which brings further speedup).
Model size. Our model can transfer rich styles using a sin-
gle auto-encoder, in contrast to the cascade networks in the
WCT method [18], thanks to the learnable transformation.
As analyzed in [8], Gram matrices of features from differ-
ent layers capture different details for style transfer, e.g.,
features from lower layers usually capture color and tex-
tures while those based on features from higher layers cap-
ture common patterns. Since the transformation in the WCT
method is not learnable, it has to present multi-level styles
by cascading several auto-encoders with different layers at
the expense of additional computational loads. In contrast,
a single transformation T in our model can express rich
styles by using a combination of multiple style reconstruc-
tion losses in the loss module, e.g., via {relu1 1, relu1 2,
relu1 3, relu1 4} in a way similar to [12, 13] as shown in
Fig. 6. Thus, the proposed method achieves the same goal
without additional computational overhead during the infer-
ence stage.

5. Experimental Results

We discuss the experimental settings and present abla-
tion studies to understand how the main modules of the pro-
posed algorithm contribute. We evaluate the proposed algo-
rithm against the state-of-the-art methods on artistic, video
and photo-realistic style transfer.

43812

(a) Content

(b) Style

(c) Image  CNN

(d) Feature CNN

(e) Covariance  CNN

Figure 5. Stylized images using different model inputs. The CNN
in (c) takes content/style images as input, the CNN in (d) takes
content/style features as input, and the CNN in (e) takes con-
tent/style features as input but feeds the covariance matrix of en-
coded features into the ﬁnal fully connected layer (see Fig. 2 and
Section 5 for details).

5.1. Experimental Settings

Encoder-decoder. Our method contains an encoder with
the ﬁrst few layers from the VGG-19 model [28] pre-trained
on the ImageNet dataset [3] and a symmetric decoder.
We train the decoder on the MS-COCO dataset [21] from
scratch to reconstruct images. This module is then ﬁxed
throughout the remaining network training procedure. We
show various design options of the encoder-decoder with
varying model depth for different applications.

Transformation module. Our transformation module con-
sists of two CNNs where each takes either content or style
features as input and outputs a transformation matrix, re-
spectively. These two transformation matrices are then mul-
tiplied to generate the ﬁnal transformation matrix T . We
compute the transferred feature vector Fd by multiplying a
content feature vector Fc with T , and feed Fd into the de-
coder to generate the stylized image. Within each CNN in
the transformation module, as discussed in Section 3, we
compute the transformation matrix from feature covariance
to handle images of any size during inference. In order to
learn a nonlinear mapping from feature covariance to the
transformation matrix, a number of fully-connected layers
are used. However, this leads to a large memory require-
ment and model size since the covariance matrix usually has
high dimensions, i.e., 512 ⇥ 512 in relu4 1. Thus, we fac-
torize the model by ﬁrst encoding the input features to the
reduced dimensions (e.g., 512 ! 32) via three consecutive
convolutional units (denoted as “CONVs” in Fig. 2), where
each is equipped with a 3 ⇥ 3 conv and a relu layer. The co-
variance matrix of the encoded feature is then fed into one
fc unit to compute the transformation matrix. We further
use a pair of convolutional layers to “compress” the con-
tent features, and “uncompress” the transformed features to
the corresponding dimensions. Overall, our transformation
module is compact, efﬁcient and can be easily trained for
any combination of styles.

Dataset and training settings. We use the MS-COCO
images and the WikiArt
dataset

[21] as our content

database [24] as our styles. Both datasets contain roughly
80,000 images. We keep the image ratio and re-scale the
smaller dimension of each training image to 300 pixels. We
then apply patch-based training by randomly cropping a re-
gion of 256⇥256 pixels from it as one training sample with
randomly ﬂip at the probability of 0.5. During the inference
stage, our model is able to handle any input size for both
content and style images, as discussed in Section 3. We
train our network using the Adam solver [14] with a learn-
ing rate of 10 4 and a batch size of 8 for 105 iterations. The
training roughly takes 5 hours on a single Titan XP GPU for
each model in our Pytorch [25] implementation.

The source code, trained models and real-time demos

will be made available to the public.

5.2. Ablation Studies for Transformation Modules

One single network? As discussed in Section 3.1, using
two separate CNNs for learning T is more suitable than
sharing a single CNN for content and style images. To ver-
ify this, we train a single CNN to learn the transformation
matrix, which takes both content and style images as in-
puts. However, this model does not converge during train-
ing, which is consistent with our discussion.

Inputs to the transformation module. We discuss in Sec-
tion 3 that the formulation in Eq. (2) suggests three input op-
tions to the CNNs in the transformation module. We imple-
ment three models with the same architecture except for the
image CNN, which uses ﬁve convolutional units instead of
three. Note that neither image nor feature CNN uses matrix
multiplication between the “CONVs” and “fc” modules, as
shown in Fig. 2. We show stylized images in Fig. 5.
In
general, the image CNN does not generate faithful stylized
results, e.g., the patches in the second row of Fig. 5(c) still
retain color pixels from the original content image. On the
other hand, the proposed covariance CNN generates results
more similar to the styles (e.g., abstract lines in the close-
ups of Fig. 5(e)) than the other methods.
Combining multi-level style losses. Features from dif-
ferent layers capture different style details. We show our
algorithm allows a ﬂexible combination of multiple lev-
els of styles within a single transformation matrix T by
making use of different style reconstruction losses in the
loss module. We apply two types of auto-encoders – en-
coders up to relu3 1 and relu4 1 in the VGG-19 model,
and use different style reconstruction losses – single loss
layer on relu1 1, relu2 1 or relu3 1, and multiple loss lay-
ers {relu1 1, · · · , relu4 1}, as shown in Fig. 6. The results
show that both transferring content features of lower layers
(relu3 1 in row (i)) and using one single style reconstruc-
tion loss from lower layers (relu1 1, relu2 1 in column (a)
and (b)) lead to more photo-realistic visual effects. On the
other hand, more stylized images can be generated using
the style reconstruction loss from higher layers (e.g., Fig. 6

53813

encoder
6789:_;

+

single-layer 
transform

decoder

VGG

…

/0_0, … , .#
.#

/4_0 	

multi-level 
style losses

(A)

r
e
d
o
c
n
e

;
_
<
9
8
7
6

(AA)

r
e
d
o
c
n
e

;
_
=
9
8
7
6

(C)

(D)

(%)

(E)

(7)

VGG

/0_0
.#

VGG

/>_0
.#

VGG

/?_0
.#

single encoder 

WCT

VGG

/0_0~/4_0
.#

Figure 6. Our model can ﬂexibly combine style losses at different levels. We use the same style loss (shown in the last row) to train models
in each column. Row 1 and 2 show stylized images by transferring content features from relu3 1 and relu4 1, respectively. The last column
shows results by one single layer WCT (relu3 1 on the top and relu4 1 at the bottom).

Image Size
Ulyanov et al. [30]
Gatys et al. [9]
Huang et al. [12]
Li et al. [18]
Ours (relu3 1)
Ours (relu4 1)

256
0.013
16.51
0.019
0.922
0.007
0.010

512
0.028
59.45
0.071
1.080
0.025
0.036

1024
0.092
N/A
N/A
N/A
0.100
0.146

Table 1. Runtime performance. “N/A” indicates the input cannot
ﬁt 12GB GPU. Runtime is measured in seconds using the source
code on a single Titan XP GPU. For WCT [18], we use the version
that cascades four different encoder-decoder modules because of
its best performance and faster speed.

column (c) and (d)). Speciﬁcally, the auto-encoder up to
relu3 1 (row (i) in Fig. 6) generates more undistorted re-
sults for all types of style losses than using auto-encoder up
to relu4 1 (row (ii) of Fig. 6).

We also show stylized results by the single-encoder
WCT in column (e) of Fig. 6 (relu3 1 on top and relu4 1
on bottom), where neither model performs well (e.g., nei-
ther color nor texture is well aligned, and edges are blurry).
In contrast, our method generates visually pleasing results
for richer styles, as shown in column (d) in Fig. 6, where no
cascade modules are required. Due to the ﬂexibility of our
algorithm, we are able to choose different settings for dif-
ferent tasks without any computational overheads. Speciﬁ-
cally, we transfer content image features from relu4 1 layer
for artistic style transfer. For video style transfer, we trans-
fer content frame features from relu3 1 to minimize distor-
tions. For all tasks, we compute style reconstruction losses
using features from relu1 1, relu2 1, relu3 1 and relu4 1 to
capture different style details.

5.3. Artistic Style Transfer

We evaluate the proposed algorithm with three state-
of-the-art methods for artistic style transfer: optimization
based method [8], fast feed-forward network [13] and fea-
ture transformation based approaches [12, 18].
Qualitative results. We present stylized results of the eval-
uated methods in Fig. 7 and more results in the appendix.

The proposed algorithm performs favorably against the
state-of-the-art methods. Although the optimization based
method [8] allows universal style transfer, it is computation-
ally expensive due to the adopted iterative optimization pro-
cess (see Table 1). The fast feed-forward methods [13] per-
forms more efﬁciently than the optimization based scheme,
but it requires training one network for each style and ad-
justing style weights for best performance.
The AdaIn method presents an efﬁcient solution for uni-
versal style transfer, but it generates less appealing results
(e.g., row 2, 3 in Fig. 7) as only the ﬁrst-order image statis-
tics are used. The WCT method performs well (see column
5 in Fig 7) by modeling the second-order image statistics
but at the expense of runtime (see Table 1).
In contrast,
our method learns the covariance-based transformation and
performs favorably for arbitrary style (see the last column
in Fig 7) and efﬁciently (see Table 1). Besides, it can be
applied to other applications ﬂexibly.

User study. We conduct a user study to evaluate the pro-
posed algorithm against the state-of-the-art style transfer
methods [8, 31, 12, 18]. We use 6 content and 40 style
images to synthesize 240 stylized results, and show 15 ran-
domly chosen content and style combinations to each sub-
ject. For each combination, we present 5 synthesized im-
ages by each method mentioned above in a random order
and ask the subject to select the most visually pleasant one.
We collect 540 votes from 36 users and present the percent-
age of votes for each method in Fig. 9(a). Overall, the pro-
posed algorithm is favored among all evaluated methods.

Efﬁciency. Table 1 shows the runtime performance of all
evaluated algorithms at three input image scales: 256⇥256,
512 ⇥ 512, 1024 ⇥ 1024. All methods listed in this table al-
low universal style transfer except the algorithm by Ulyanov
et al. [30] (row 1). Even the slower variant of the pro-
posed algorithm (trained to transfer content features from
relu4 1) runs at 100 FPS and 27 FPS for 256 ⇥ 256 and
512 ⇥ 512 images, thereby making real-time style trans-

63814

Content

Style

Gatys [8]

Huang [12]

Li [18]

Sheng [27]

Ours

Figure 7. Stylized images by the evaluated methods. Our model is trained to transfer content features from relu4 1 with style losses
computed on relu1 1, relu2 1, relu3 1, relu4 1 layer of VGG 19. All content images, as well as style images have never been seen by our
model during the training process. More examples are available in the appendix.

1
e
m
a
r
F

5
e
m
a
r
F

e
c
n
e
r
e
f
f
i

D

Frames

Gatys [8]

Johnson [13]

Li [18]

Ours

Figure 8. Video style transfer results. The ﬁrst row and second row
show the ﬁrst frame and ﬁfth frame with corresponding transferred
frames by evaluated methods, and the last row shows the heat map
of the difference between these two frames.

Gatys et al.

Ulyanov et al.

Huang et al.

Li et al.

Ours

12.7%

8.5%

35.3%

14.7%

28.8%

5.8%

7.5%

81.8%

(a) Stylization effects on images

(b) Stability on videos

Figure 9. User study of stylization effects on images and stability
on videos.

fer feasible. Our model performs more efﬁciently than the
optimization-based method [8] (by three orders of magni-
tude) as well as WCT [18] (by two orders of magnitude)
and comparably to the fast feed-forward schemes [30, 12].

5.4. Video Style Transfer

Video style transfer is conducted between a content
video and a style image in a frame-wise manner by using
a shallower auto-encoder up to the relu3 1 layer. This pro-
cess can be more efﬁcient than image based style transfer,
since the transformation output from the style branch can
be computed only once during initialization (see Fig. 2),
and directly applied to the remaining frames. Since our
approach can preserve the afﬁnity of the content images,
which is naturally consistent and stable, the stylized videos
are also visually stable without the need of any auxiliary
techniques such as optical ﬂow warping [11]. Fig. 8 shows
the direct frame-based style transfer results by the proposed
model compared with existing methods [8, 13, 18]. To visu-
alize stability of synthesized video clip, we show heat maps
of differences between two frames (i.e., row 3). The differ-
ences by the proposed algorithm are closest to that of the
original frames, which suggest that our algorithm is able to
preserve content afﬁnity during style transfer. To further
evaluate the stability of our algorithm in video style trans-
fer, we conduct a user study with 5 video clips synthesized
frame-wise by our algorithm and 4 state-of-the-art meth-
ods [8, 30, 12, 18]. For each group of videos, we ask each
subject to select the most stable video clip. We collect 106
votes from 27 subjects and present the preference of each
method in Fig. 9(b). Overall, the proposed algorithm per-
forms well against the other methods, which indicates our
approach is able to preserve afﬁnity during style transfer.

5.5. Photo-realistic Style Transfer

As discussed in Section 3, to achieve photo-realistic style
transfer, we combine our linear style transfer network with

73815

Content

Style

Gatys [8]

Johnson [13]

Li [18]

Huang [12]

Ours

Figure 10. Afﬁnity preserving comparison between our algorithm with [8, 13, 18, 12]. Our result has a more photo-realistic feel with the
content images. It faithfully preserves the contour of the dog (row 1) and the shadow of the face (row 2) in the stylized images.

Content

Style

Luan [21] (186.52s)

Li [18] (2.95s)

ours (0.17s)

ours + SPN (0.22s)

Figure 11. Photo-realistic style transfer results. The spatial mask is displayed at the right bottom corner of each content and style image. “+SPN” in the
last column means results ﬁltered by the SPN module after stylization. Inference time are tested and averaged over 60 512 × 256 images.

a SPN [22] to minimize distortions caused by the auto-
encoder. The SPN is similar as Liu et al. [22], which con-
tains a CNN with 8 convolution layers that outputs all local
weights that formulate a pixel-afﬁnity matrix, and a linear
propagation layer that outputs the ﬁltered images. At the
training stage, we ﬁx the style transfer encoder-decoder and
feed a reconstruction image produced by the auto-encoder
into the propagation module as the input. The propagation
module outputs a reﬁned reconstruction image under the
guidance of the corresponding whitened image. We then
compute the Euclidean loss between the reﬁned and origi-
nal images. Note that no style transfer process is involved
during the training stage.

At the inference stage, we ﬁrst transfer the correspond-
ing regions with respect to a pre-deﬁned mask for every
content/style pairs. This is carried out by separating the
masked regions only within the transformation module (see
Fig. 2) and combining the transformed features of differ-
ent regions, according to the mask. Since the inputs to the
transformation module are covariance matrices, our style
transfer network can take arbitrary masks. We apply pre-
trained propagation module directly onto the transferred im-
age while using its whitened content image as the input to
the guidance network. We show the qualitative comparison

with recent works [23, 19] in Fig. 11 along with their in-
ference time tested on 512 ⇥ 256 images. As shown in the
squared regions, thanks to our data-driven transformation
and propagation module, the proposed approach preserves
photo-realistic details better (e.g., the texture of the sky or
bottle) in content images when transferring color from style
images. Furthermore, our end-to-end approach is two or-
ders of magnitude faster than [23] and one order of magni-
tude faster than [19].

6. Conclusions

In this work, we propose a framework for analyzing uni-
versal style transfer methods and present an effective as
well as efﬁcient algorithm by learning linear transforma-
tions. In addition, we propose a linear propagation module
to enable a feed-forward network for photo-realistic style
transfer. Our algorithm is computationally efﬁcient, ﬂexi-
ble for numerous tasks, and effective for stylizing images
and videos. Experimental results demonstrate that the pro-
posed algorithm performs favorably against the state-of-the-
art methods on image and video style transfer.

Acknowledgement

This work is supported in part by the NSF CAREER

Grant #1149783, and gifts from Adobe, Verisk, and NEC.

83816

References

[1] A. J. Champandard.

ing two-bit doodles into ﬁne artworks.
arXiv:1603.01768, 2016. 2

Semantic style transfer and turn-
arXiv preprint

[2] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 10, 12

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 5

[4] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style. CoRR, abs/1610.07629, 2016. 2

[5] A. Dundar, M.-Y. Liu, T.-C. Wang, J. Zedlewski, and
J. Kautz. Domain stylization: A strong, simple baseline for
synthetic to real image domain adaptation. arXiv preprint
arXiv:1807.09384, 2018. 10

[6] S. Y. Falong Shen and G. Zeng. Neural style transfer via

meta networks. In CVPR, 2018. 2

[7] O. Frigo, N. Sabater, J. Delon, and P. Hellier. Split and
match: Example-based adaptive patch sampling for unsuper-
vised style transfer. In CVPR, 2016. 2

[8] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, 2016. 1, 2,
4, 6, 7, 8, 10, 13

[9] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In CVPR, 2017. 2, 6

[10] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
Exploring the structure of a real-time, arbitrary neural artistic
stylization network. In BMVC, 2017. 2, 10, 13

[11] A. Gupta, J. Johnson, A. Alahi, and L. Fei-Fei. Characteriz-
ing and improving stability in neural style transfer. In ICCV,
2017. 3, 4, 7

[20] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural

style transfer. In IJCAI, 2017. 10

[21] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick,
J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollr.
Microsoft coco: Common objects in context.
In ECCV,
2014. 5

[22] S. Liu, S. D. Mello, J. Gu, G. Zhong, M.-H. Yang, and
J. Kautz. Learning afﬁnity via spatial propagation networks.
In NIPS, 2017. 1, 4, 8

[23] F. Luan, S. Paris, E. Shechtman, and K. Bala. Deep photo

style transfer. In CVPR, 2017. 2, 8, 10, 11

[24] K. Nichol. Painter by numbers, wikiart. https://www.

kaggle.com/c/painter-by-numbers, 2016. 5

[25] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Au-
tomatic differentiation in pytorch. In NIPS Workshop, 2017.
5

[26] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for
data: Ground truth from computer games. In ECCV, 2016.
10

[27] L. Sheng, Z. Lin, J. Shao, and X. Wang. Avatar-net: Multi-
scale zero-shot style transfer by feature decoration. In CVPR,
2018. 2, 7

[28] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 5

[29] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy

domain adaptation. In AAAI, 2016. 2, 10

[30] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
Texture networks: Feed-forward synthesis of textures and
stylized images. In ICML, 2016. 1, 2, 6, 7, 10, 13

[31] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Improved texture
networks: Maximizing quality and diversity in feed-forward
stylization and texture synthesis. In CVPR, 2017. 2, 6

[12] X. Huang and S. Belongie. Arbitrary style transfer in real-
time with adaptive instance normalization. In ICCV, 2017.
1, 2, 3, 4, 6, 7, 8, 10, 13

[32] X. Wang, G. Oxholm, D. Zhang, and Y.-F. Wang. Multi-
modal transfer: A hierarchical deep convolutional neural net-
work for fast artistic style transfer. In CVPR, 2017. 2

[33] P. Wilmot, E. Risser, and C. Barnes. Stable and controllable
neural texture synthesis and style transfer using histogram
losses. arXiv preprint arXiv:1701.08893, 2017. 2

[34] H. Zhang and K. Dana. Multi-style generative network for
real-time transfer. arXiv preprint arXiv:1703.06953, 2017. 2
[35] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, 2017. 10, 12

[13] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, 2016.
1, 2, 4, 6, 7, 8

[14] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICML, 2015. 5

[15] C. Li and M. Wand. Combining markov random ﬁelds and
convolutional neural networks for image synthesis. In CVPR,
2016. 2

[16] C. Li and M. Wand. Precomputed real-time texture synthesis
with markovian generative adversarial networks. In ECCV,
2016. 2

[17] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
Diversiﬁed texture synthesis with feed-forward networks. In
CVPR, 2017. 2

[18] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
In NIPS,

Universal style transfer via feature transforms.
2017. 1, 2, 3, 4, 6, 7, 8, 10, 13

[19] Y. Li, M.-Y. Liu, X. Li, M.-H. Yang, and J. Kautz. A closed-
form solution to photorealistic image stylization. ECCV,
2018. 1, 2, 8, 10, 11

93817

