Blending-target Domain Adaptation by Adversarial Meta-Adaptation Networks

Ziliang Chen1, Jingyu Zhuang1, Xiaodan Liang1,2, Liang Lin1,2 ∗

1Sun Yat-sen University 2DarkMatter AI Research

c.ziliang@yahoo.com, zhuangjy6@mail2.sysu.edu.cn, xdliang328@gmail.com, linliang@ieee.org

Figure 1. The comparison of MTDA and BTDA (color orange and blue denote source and target) setups. In MTDA (a), target domains are
explicitly separated and we are informed by which target an unlabeled sample originates from. In BTDA (b), sub-target IDs are encrypted.
If we treat them as a combined single target, transfer learning will lead to the adaptation on a multi-target mixture instead of each hidden
target (gray distribution curves in (a), (b)). It implies category-shifted adaptation and negative transfer in practice. Best viewed in color.

Abstract

(Unsupervised) Domain Adaptation (DA) seeks for clas-
sifying target instances when solely provided with source
labeled and target unlabeled examples for training. Learn-
ing domain-invariant features helps to achieve this goal,
whereas it underpins unlabeled samples drawn from a sin-
gle or multiple explicit target domains (Multi-target DA). In
this paper, we consider a more realistic transfer scenario:
our target domain is comprised of multiple sub-targets im-
plicitly blended with each other, so that learners could not
identify which sub-target each unlabeled sample belongs to.
This Blending-target Domain Adaptation (BTDA) scenario
commonly appears in practice and threatens the validities
of most existing DA algorithms, due to the presence of do-
main gaps and categorical misalignments among these hid-
den sub-targets.

To reap the transfer performance gains in this new sce-
nario, we propose Adversarial Meta-Adaptation Network
(AMEAN). AMEAN entails two adversarial transfer learn-
ing processes. The ﬁrst is a conventional adversarial trans-
fer to bridge our source and mixed target domains. To cir-
cumvent the intra-target category misalignment, the second
process presents as “learning to adapt”: It deploys an un-
supervised meta-learner receiving target data and their on-
going feature-learning feedbacks, to discover target clus-

ters as our “meta-sub-target” domains. These meta-sub-
targets auto-design our meta-sub-target DA loss, which em-
pirically eliminates the implicit category mismatching in
our mixed target. We evaluate AMEAN and a variety of DA
algorithms in three benchmarks under the BTDA setup. Em-
pirical results show that BTDA is a quite challenging trans-
fer setup for most existing DA algorithms, yet AMEAN sig-
niﬁcantly outperforms these state-of-the-art baselines and
effectively restrains the negative transfer effects in BTDA.

1. Introduction

1 Despite achieving a growing number of successes, deep
supervised learning algorithms remain restrictive in a vari-
ety of new application scenarios due to their vulnerabili-
ties towards domain shifts [15]: when evaluated on newly-
emerged unlabeled target examples drawn from a distribu-
tion non-identical with the training source density, super-
vised learners inevitably perform inferior. In terms of vi-
sual data, this issue stems from diverse domain-speciﬁc ap-
pearance variabilities, e.g. differences in background and

1* Liang Lin is the corresponding author. This work was supported
in part by the National Key Research and Development Program of China
under Grant No. 2018YFC0830103 and Grant No.2016YFB1001004, in
part by National High Level Talents Special Support Plan (Ten Thousand
Talents Program), and in part by National Natural Science Foundation of
China (NSFC) under Grant No. 61622214, 61836012, and 61876224.

12248

camera poses, occlusions and volatile illumination condi-
tions, etc. Hence the variabilities are highly relevant in
most machine vision implementations and obstacle their
advancements. To address these problems, (unsupervised)
Domain Adaptations (DAs) choose suitable statistical mea-
sures between source and target domains, e.g., maximum
mean discrepancy (MMD) [22] and adversarial-network
distance [10, 35], to learn domain-invariant features in pur-
suit of consistent cross-domain model performances. The
related studies increasingly attract a large amount of inter-
ests from the areas of domain-adaptive perception [18, 7, 6],
autonomous steering [43, 42] and robotic vision [5, 3, 30].
Most existing DA approaches indeed have evidenced im-
pressive performances in a laboratory, whereas the domain
shifts that frequently occur in reality, are far from being set-
tled through these techniques. One explanation is the ideal
target-domain premise these DA techniques start from. Par-
ticularly, DAs are conventionally established on a “single-
target” preset, namely, all target examples are drawn from
an identical distribution. Some recent researches focus on
Multi-target DA (MTDA) [2, 44], where target examples
stem from multiple distributions, whereas we exactly know
which target they belong to (See Fig.1.(a)).

In this paper, we argue these target-domain preconditions
always taken for granted in most previous transfer learning
literatures. After revisiting widespread presences, we dis-
cover that target unlabeled examples are often too diverse
to well suit a “single-target” foresight. For instance, virtual-
to-real researches [5, 43] encourage robots and driver agents
trained on a simulation platform to adaptively perform in
the real-world environment. However, the target real-world
environment includes extensive arrays of scenarios and con-
tinuously changes as time goes by. Another case is an en-
crypted dataset stored in a cloud server [12], where the un-
labeled examples are derived from multiple origins whereas
due to a privacy protection, users have no access to iden-
tify these origins. These facts imply the existence of multi-
ple sub-target domains while unlike standard MTDA, these
sub-targets are blended with each other so that learners are
not able to identify which sub-target each unlabeled exam-
ple belongs to (See Fig.1.(b)). This so-called blending-
target domain adaptation (BTDA) scenario regularly oc-
curs in more other circumstances and during adaptation pro-
cess, it commonly arouses notorious negative transfer ef-
fects [29] by two reasons:

• Hidden sub-target domains are organized as a mixture
distribution, whereas without the knowledge of sub-
target ID, it is quite difﬁcult to align the category to
reduce their mismatching across the sub-targets.

• Regardless of the domain gaps among the hidden tar-
gets, existing DA approaches will suffer from the cat-
egory misalignments among these sub-targets.

. Due to the blending-target preset, BTDA can not be solved

by existing multi-target transfer learning methods.

To reap transfer performance gains and simultaneously
prevent the negative transfer effects in BTDA scenario, we
propose Adversarial Meta-Adaptation Network (AMEAN)
attempting to solve this problem under the context of vi-
sual recognition. AMEAN evolves from popular adversar-
ial adaptation frameworks [9, 17] and opts for minimiz-
ing the discrepancy between our source and mixed target
domains. But distinguished from these existing pipelines,
our AMEAN is inspired by meta-learning and AutoML
[1, 39], which concurrently deploys an unsupervised meta-
learner to learn deep target cluster embeddings by receiv-
ing the mixed target data and their ongoing-learned features
as feedbacks. The incurred clusters are treated as meta-
sub-target domains. Hence, our AMEAN auto-designs its
multi-target adversarial adaptation loss functions and to this
end, dynamically train itself to obtain domain-invariant fea-
tures from a source to a mixed target and among the multi-
ple meta-sub-target domains derived from the mixed target.
This bi-level optimization endows more diverse and ﬂexible
adaptation within the mixed target and effectively mitigates
its latent category mismatching.

Our contributions mainly present in three aspects:

1. On account of practical cross-domain applications, we
consider a new transfer scenario termed Blending-target
Domain Adaptation (BTDA), which is common in real-
ity and more difﬁcult to settle.

2. We propose AMEAN, a adversarial transfer framework
incorporating meta-learner dynamically inducing meta-
sub-targets to auto-design adversarial adaptation losses,
which effectively achieve transfers in BTDA.

3. Our experiments are conducted on three widely-applied
DA benchmarks. Our results show that BTDA setup def-
initely brings more transfer risks towards existing DA al-
gorithms, while AMEAN signiﬁcantly outperforms the
state-of-the-art and present more robust in BTDA setup.

2. Related Work

Before introducing BTDA problem setup, we would like
to brieﬂy revisit (unsupervised) Domain Adaptation (DA)
under the modern visual learning background.

Single-source-single-target DA. DAs are derived from
[31, 14], where shallow models are deployed to achieve data
transfer across visual domains. The development of deep
learning enlightens the tunnel to learn nonlinear transfer-
able feature mappings in DAs. Up-to-date deep DA meth-
ods have been branched into two mainstreams: explicit and
implicit statistical measure matching. The former employs
MMD [22, 24], CMD [45], JMMD [23, 25], etc, as the
domain regularizer to chase for consistent model perfor-
mances both on source and target datasets. The latter des-
ignates domain discriminators to perform adversarial learn-
ing, where the feature extractor is trained to optimize the

22249

transferable feature spaces. It includes amounts of avenues
to present diverse adversarial manners, e.g., CoGAN [21],
DDC [17], RevGred [9, 10], ADDA [35], VADA [34], GTA
[33], etc. Beside of the two branches, there are some ap-
proaches in virtue of other ideologies, e.g., reconstruction
[11], semi-supervised learning [32] to optimize a domain-
aligned feature space. It is worth noting that, these meth-
ods agree with the “single-source-single-target” precondi-
tion when they learns transferable features for DA .

Multi-source DA (MSDA). MSDA aims at boosting the
target-adaptive accuracy of a model by introducing mul-
tiple source domains in a transfer process. It is a histori-
cal topic [41] and refers to a part of DA theories [27] [4].
Some recent work place the problem under the deep visual
learning background. For instance, [40] invented a adver-
sarial reweighting strategy to infer a source-ensemble tar-
get classiﬁer; [46] developed an old-fashion theory to suit
deep MSDA and provided a target error upper bound; [26]
stacked multi-DA layers in a network to obtain robust multi-
source domain alignment.

Multi-target DA (MTDA). Similar to MSDA, the goal
of MTDA is to enhance data transfer efﬁcacy by bridging
the cross-target semantic. [2] used a semantic disentangler
to facilitate an adversarial MTDA approach; [44] consid-
ered the visible semantic gap between multiple targets and
propose a dictionary learning algorithm to suit this problem.
MTDA is still a fresh area and awaits more explorations.

3. Blending-target DA (BTDA): Problem Setup

, y

(s)
i

(s)
i

(s)
i )}ns

(s)
i ∈ Rd

i=1, where x

Preliminaries. Let’s consider an m-class visual recogni-
tion problem. Suppose a source dataset includes ns labeled
examples S = {(x
+ denotes
the ith source image lying on a d-dimensional data space
is an m-dimensional one-hot vector corresponding
and y
to its label. Besides, a target set includes nt unlabeled ex-
+ denotes the ith
amples T = {x
target image. S and T underly distributions PS (x, y) and
PT (x), in which PT (x) = R PT (x, y)dy indicates target

labels unobservable during training. DA seeks for learning
a classiﬁer along with a domain-invariant feature extractor
across S and T , which is capable to predict the correct la-
bels of given images sampled from PT (x, y). As of now
DA assumes all unlabeled images derived from a single tar-
get distribution PT .

i=1 where x

(t)
i ∈ Rd

(t)
i }nt

(t))}k

(t)) = Pk

setup: every unlabeled target instance x
tributions {PTj (x
ture PT (x

Here we turn to consider the multi-target DA (MTDA)
(t) underly k dis-
j=1, as they are drawn from the mix-
(t)) where ∀j ∈ [k], πj ∈
j πj = 1. However, The learning goal of MTDA
j=1 instead of their

is to simultaneously adapt k targets {Tj}k
mixed target T . Since the multi-target proportions {πj}k
j=1
are known in MTDA, target set Tj is explicitly provided by

[0, 1]&Pk

j=1 πjPTj (x

drawing from the posterior

πj PTj (x
πj PT

(t))
j′ (x(t))

Pk

j′=1

. Hence exist-

ing DA algorithms can address the problem by training k
target-speciﬁc DA models respectively, and using the jth-
target model to classify the examples from the jth target.

From MTDA to BTDA. Like MTDA, BTDA is also
established on a target mixture distribution and expected
to adapt k targets {Tj}k
j=1. However, multi-target propor-
tions {πj}k
j=1 in BTDA are unobservable. In other words,
BTDA learners are solely provided with a mixed target set
T drawn from a k-mixture density and required to classify
a mixed target test set drawn from the same mixture. If we
directly leverage existing DA techniques to transfer cate-
gory information from S to T , the learning objective will
guide domain-invariant features to adapt a mixed target set
T instead of k target {Tj}k
j=1. Since sub-targets from k dis-
tributions could be quite distinct in visual realism, adapting
to a mixed target probably would result in drastic category
mismatching and arouses serious negative transfer effects.

4. Adversarial MEta-Adaptation Network

To reap positive transfer performance in BTDA setup, we
propose Adversarial MEta-Adaptation Network (AMEAN).
AMEAN is coupled of two adversarial learning processes,
which are parallely executed to obtain domain-invariant fea-
tures by transferring data from source S to mixed target T ,
and among k “meta-sub-targets” { ˆTj}k
j=1 within the mixed
target T . The pipeline of AMEAN is concisely illustrated
in Fig.2 and we elaborate the methodology as follows.

4.1. Adaptation from a source to a mixed target

Suppose that C is a m-slot softmax classiﬁer proposed
on a transferable feature space and F denotes the feature ex-
tractor we long to optimize in BTDA. We employ a source-
target domain discriminator Dst to deploy an adversarial
learning scheme, where F is trained to match a source set S
and a mixed target set T (the uniﬁcation of {Tj}k
j=1) at the
feature level, while Dst is demanded to separate the source
and the target features.

min
F,C

λ(cid:16) E
|

max
Dst

Vst(F, Dst, C) =

x∼T log(cid:2)1 − Dst(cid:0)F (x)(cid:1)(cid:3)
x∼S log(cid:2)Dst(cid:0)F (x)(cid:1)(cid:3) + E
{z
}
T log C(cid:0)F (x)(cid:1)
}
{z

Adversarial DA loss
−E(x,y)∼S y

Classiﬁcation loss

|

(1)

(cid:17)

. This minimax objective can be optimized in two ways:

1. C, F , Dst are jointly trained by inducing a reversed
gradient layer [9] [10] across Dst and F , which in-
versely back-propagates the gradients from a source-
target domain discriminator Dst.

32250

Figure 2. The learning pipeline of our Adversarial MEta-Adaptation Network (AMEAN). AMEAN receives source samples with ground
truth and unlabeled samples drawn from an unknown target mixture to synchronously execute two transfer learning processes. In the ﬁrst
process, we propose an adversarial learning bridging our source S and the mixed target T , so as to learn a feature extractor F that maps
them into a common feature space along with a classiﬁer training. In the second stream, AMEAN uses an unsupervised meta-learner U
to accept target data and their representations (student feedbacks) and then, learns to separate the mixed target domain into k clusters as
meta-sub-targets { ˆTj}k
j=1. These meta-sub-targets are iteratively updated to auto-design the multi-target adaptation objectives (Eq.6 7),
which operates to progressively eliminate the category mismatching behide the mixed target. Best viewed in color.

2. C, F and Dst are updated by an alternative optimiza-

tion like GAN [13] [34].

Our experiments show them both well-suited in AMEAN.

4.2. Meta adaptation among meta sub targets

The pure source-to-target transfer is blind to the domain
gaps and category misalignment among {Tj}k
j=1, which the
negative transfer mainly ascribes to. Here we interpret the
key module of AMEAN to address this problem: an unsu-
pervised meta-learner trained to obtain k embedded clusters
[38] as our “meta-sub-targets”, which play the roles of auto-
matically and dynamically outputting multi-target adversar-
ial adaptation loss functions (Eq.6, 7) in pursuit of reducing
the category mismatching inside the mixed target T .

Meta-learning for dynamic loss design.. Meta-learning
(“learning to learn”) [1] become a thriving topic in machine
learning community. It refers to all learnable technique to
improve model generalization, which includes fast adapta-
tion to rare categories [37] and unseen tasks [8], as well as
auto-tuning the hyper-parameters accompanied with train-
ing, e.g., learning rate [1], architecture [47], loss function
[39], etc. Inspired by them, our meta-learner U is an un-
supervised net learning to ﬁnd deep clustering embeddings
that incurs k clusters based on the ongoing feature extrac-
tor feedbacks. It induces k meta-sub-targets { ˆTj(U )}k
j=1 to
take place of {Tj}k
j=1. Via this auxiliary, meta-learner has
access to auto-design the adversarial multi-target DA loss
with respect to the k meta-sub-targets and, dynamically al-
ter the losses according to the change of these meta-sub-
targets as our meta-learner updates the k clusters.

j=1 and {Tj}k

Two major concerns are probably raised to our method-
ology: 1). Are { ˆTj(U )}k
j=1 similar ? 2). Does
the clustering ﬁnally lead to target samples of the same
category staying together? Towards the ﬁrst concern, our
answer is not and unnecessary. The technical difﬁculty in
BTDA arises from the category misalignment instead of the
hidden sub-target domains. As long as a DA algorithm per-
forms appropriate category alignment in a mixed target, it is
unnecessary to explicitly discover these hidden sub-target
domains. AMEAN receives ongoing learned features that
implicitly conceive label information from Eq 1 , to adap-
tively organizes its meta-adaptation objective. In our abla-
tion, This manner shows powerful to overcome the category
misalignment in T . The second concern raises a dilemma
to our meta-transfer: as learned features become more clas-
siﬁable, target features of the same class will be closer and
closer, then clustering will more probably select them as a
newly-updated meta-target domain. It causes category shift
if we apply them to learn AMEAN. To remove this hid-
(t)
den threat, our meta-learner simultaneously receives x
(t)) to learn deep clustering embeddings, where
and F (x
(t) denotes the primitive state of a sample without DA and
x
(t)) implies the adaptation feedback from the ongoing
F (x
(t) is feature-agnostic, the induced
learned feature. Since x
clusters inherit the merit of meta-learning and concurrently
gets rid of this classiﬁable feature dilemma.

Discovering k meta-sub-targets via deep k-clustering.
Our unsupervised meta-learner U is derived from the base
model in DEC [38], a denoising auto-encoder (DAE) com-
posed of encoder U1 and decoder U2. Distinguished from

the DAE in DEC, U takes a target couple (cid:0)x

(t))(cid:1) as

t, F (x

42251

inputs and learns to satisfy the self-reconstruction as well as

obtain deep clustering embedding U1(cid:0)x

(t), F (x

(t))(cid:1). More

speciﬁcally, suppose {µj}k
we deﬁne a soft cluster assignment {qi,j}k

j=1 is the k cluster centroids, and
(t)
i ∈ T .

j=1 to x

qi,j =

(t)
i

(1 + ||U1(x
Pk

j ′ (1 +

||U1(x

,F (x
α
,F (x
α

(t)
i

(t)

i ))−µj ||2

)− α+1

2

(t)
i ))−µj′ ||2

)− α+1

2

(2)

where α indicates the degree of freedom in a Student’s t-
distribution. Eq.2 is aimed to learn U by iteratively infer-
ring deep cluster centroids {µj}k
j=1. This EM-like learning
operates with the help of auxiliary distributions {pi,j, ∀i ∈
[nt]}, which are computed by raising qi to the second power
and normalize it by the frequency in per cluster, i.e.,

pi,j =

q2
i,j/fj
Pk
j′ q2

i,j′ /fj′

, s.t. fj =

ntX

i

qi,j

(3)

where fj denotes the jth cluster frequency. Compared with
qi, pi endows more emphasis on data points with high con-
ﬁdence and thus, is more appropriate to supervise the soft
cluster inference. We employ KL divergence to restrict qi
and pi for the meta-learner clustering network learning:

min

U1,U2,{µj }k

j=1

E

xi∼T Lrec(xi; F ) −

kX

j=1

pi,j log

pi,j
qi,j

(4)

where Lrec(x; F ) denotes a l2 self-reconstruction w.r.t. a

target feedback pair(cid:0)x, F (x)(cid:1) and the second term denotes

a KL divergence term for clustering. Parameter learning and
k cluster centroid ({µj}k
j=1) update are facilitated by back-
propagation with a SGD solver. (See more in Appendix.A)
After meta-learner U converges, we apply the incurred
clustering assignments to separate T into k meta-sub-target
in a mixed target T will be classed into
domains, i.e., x
meta-sub-target ˆTj(U ) if qi,j is the maximum in {qi,j ′ }k
j ′=1:

(t)
i

∀j ∈ [k], ˆTj(U ) = {xi ∈ T &j = arg max

j ′

q∗
i,j ′ }

(5)

Meta-sub-target adaptation. Given k meta-sub-target do-
mains { ˆTj(U )}k
j=1, AMEAN auto-designs the k-sub-target
DA losses to re-align the features in T . More detailedly,
AMEAN designates a k-slot softmax classiﬁer Dmt sharing
parameters with Dst, as meta-sub-target domain discrimi-
nator. In order to obtain k meta-sub-target adaptations, fea-
tures extracted by F seek to “maximally confuse” the dis-
criminative decision of Dmt:
kX

j log(cid:2)Dmt(cid:0)F (x)(cid:1)(cid:3)

Vmt(F, Dmt) =

x∼ ˆTj (U )

min

1T

E

max
Dmt

F

j=1

(6)
where 1j indicates a k-dimensional one-hot vector imply-
ing that sample x belongs to ˆTj(U ). In the case of joint

Algorithm 1 AMEAN (Stochastic version)
Input: Source S; Mixed target T ; feature extractor F ; classiﬁer
C; domain discriminators Dst,Dmt; meta-learner U = {U1, U2}.
Output: well-trained F ∗, D∗
1: while not converged do
2:

mt, C ∗.

st,D∗

Meta-sub-target Discovery (Meta-update):
Initiate {µj}k
j=1 and U ;
while not converged do

Sample a mini-batch Xt from T ; Construct {qi,j}k by

Eq.2 and {pi,j}k by Eq.3 for each xi in the mini-batch.
j=1 in Eq.4 with SGD solver.

Update U and {µj}k

end while
Divide T into k meta-sub-targets { ˆTj(U )}k
Collaborative Advesarial Meta-Adaptation:
for 1:M do

j=1 by Eq.5.

{X (j)

t }k

Sample a mini-batch Xs from S and k mini-batches
j=1 from { ˆTj(U )}k
if alternating domain adaptation then
Update Dst and Dmt with Eq.9 .
Update F and C with Eq.10 .

j=1 respectively; Xt = ∪X (j)

.

t

else Update Dst, Dmt, F and C with Eq.8 .
end if

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

end for

17:
18: end while
19: return F ∗ = F ; C ∗ = C; D∗

st = Dst; D∗

mt = Dmt.

parameter learning, due to the mutual architectures of Dst
and Dmt, Eq.6 is implemented by the same reversed gradi-
ent layer originally for source-to-target transfer. However, if
subnets F and Dmt are alternatively trained, Eq.6 is solely
used to update Dmt while we prefer to optimize feature ex-

tractor F by maximizing the cross-entropy of Dmt(cid:0)F (x)(cid:1):
F eVmt(F, Dmt) =

x∼ ˆTj (U )Dmt(cid:0)F (x)(cid:1)T log(cid:2)Dmt(cid:0)F (x)(cid:1)(cid:3)

kX

min

E

j=1

(7)
It implies that F learns to “confuse” multi-target domain
discriminator Dmt, namely, Dmt could not identify which
meta-sub-target an unlabeled example belongs to.

4.3. Collaborative Advesarial Meta Adaptation

In order to learn domain-invariant features as well as re-
sist the negative transfer from a mixed target, the previous
transfer processes should be combined to battle the domain
shifts. In particular, we retrain our meta-learner to update

meta-DA losses Vmt(F, Dmt) and eVmt(F, Dmt) per M it-

eration during feature learning. After that, if we employ a
reverse gradient layer as the adversarial DA implementation
(Eq.1), the collaborative learning objective is formulated as

max

Dst,Dmt

min
F,C

Vjoint(Dst, Dmt, F, C)

(8)

= Vst(F, Dst, C) + γVmt(F, Dmt)

where γ indicates the balance factor between two transfer
processes. Eq.8 suits joint learning w.r.t. F , Dst, Dmt, C,

52252

Table 1. Classiﬁcation accuracy (ACC %)on Digit-ﬁve in BTDA setup. BLUE, RED indicate the baseline suffer from absolute negative
transfer (ANT%) or relative negative transfer (RNT%), respectively. Best viewed in color.

Models

Backbone-1:
Source only
ADDA
DAN
GTA
RevGrad
AMEAN
Backbone-2:
Source only
VADA
DIRT-T
AMEAN

mt→mm,sv,up,sy
ACCANT
RNT

mm→mt,sv,up,sy
ACCANT

RNT

sv→mm,mt,up,sy
ACCANT

RNT

sy→mm,mt,sv,up
ACCANT

RNT

up→mm,mt,sv,sy
ACCANT

RNT

Avg

ACCANT

RNT

26.9
43.7
31.3
44.6
52.4

56.2 (+3.8)

47.4
76.0
73.5

85.1 (+9.1)

0

-8.0
-7.5
-9.2
-8.9

-

0

-5.3
-7.1

-

56.0

55.9(−0.1)
53.1(−2.9)
54.5(−1.5)

64.0

65.2 (+1.2)

58.1
72.3
76.1

77.6 (+1.5)

0

-3.3
-3.1
-2.1
-4.1

-

0

-2.3
-1.5

-

67.2

40.4(−26.8)
48.7(−18.5)
60.3(−6.9)
65.3(−1.9)
67.3 (+0.1)

73.8
75.6
75.9

77.4 (+1.5)

0

-21.7
-9.5
-3.9
-4.1

-

0

-2.5
-5.5

-

73.8

66.1(−6.7)
63.3(−10.5)
74.5 (+0.7)
66.6(−6.2)
71.3(−2.5)

74.5
81.3
78.5

84.1 (+2.8)

0

-6.5
-3.9
-1.1
-7.5

-

0

-3.8
-3.1

-

36.9

34.8(−0.1)
27.0(−9.9)

41.3
44.3

47.5 (+3.2)

50.6
56.4
47.0

75.5 (+19.1)

0

-13.3
-11.0
-2.0
-6.3

-

0

-8.7
-7.5

-

52.2

48.2(−4.0)
44.7(−7.5)

55.0
58.5

61.5 (+3.0)

60.8
72.3
70.2

80.0 (+7.7)

0

-10.5
-7.0
-3.7
-6.2

-

0

-4.5
-5.0

-

while in an alternating adversarial manner, it would be more
appropriate to iteratively update {F, C} and {Dst, Dmt} by
switching the optimization objectives between

max

Dst,Dmt

min
F,C

Valter(Dst, Dmt) = Vst(F, Dst, C) + Vmt(F, Dmt)

(9)

Valter(F, C) = Vst(F, Dst, C) + γeVmt(F, Dmt)

(10)
In a summary, the stochastic learning pipeline of AMEAN
is described by Algorithm.1 .

5. Experiments

In this section, we elaborate comprehensive experiments
in the BTDA setup and compare AMEAN with state-of-the-
art DA baselines. Code is available at http:// URL .

5.1. Setup

Benchmarks. Digit-ﬁve [40] is composed of ﬁve domain
sets drawn from mt (MNIST) [20], mm (MNIST-M) [10],
sv(SVHN) [28], up (USPS) and sy (Synthetic Digits) [10],
respectively. There are 25000 for training and 9000 for test-
ing in mt, mm, sv, sy, while the entire USPS is chosen as
a domain set up. Ofﬁce-31 [31] is a famous visual recogni-
tion benchmark comprising 31 categories and totally 4652
images in three separated visual domains A (Amazon), D
(DSLR), W (Webcam), which indicate images taken by web
camera and digital camera in distinct environments. Ofﬁce-
Home [36] consists of four visual domain sets, i.e., Artistic
(Ar), Clip Art (Cl), Product (Pr) and Real-world (Rw) with
65 categories and around 15, 500 images in total.

Baselines. Since MSDA, MTDA approaches require do-
main remarks, they obviously do not suit the BTDA setup.
Therefore we compare our AMEAN with existing (single-
source-single-target) DA baselines and evaluate their clas-
siﬁcation accuracies by transferring class information from
source S to mixed target T . State-of-the-art DA baselines
include: Deep Adaptation Network (DAN) [22], Resid-
ual Transfer Network (RTN) [24], Joint Adaptation Net-
work (JAN) [25], Generate To Adapt (GTA) [33], Ad-
versarial Discriminative Domain Adaptation (ADDA) [35],
Reverse Gradient (RevGred) [9] [10], Virtual Adversarial

Domain Adaptation (VADA) [34] and its variant DIRT-T
[34]. DAN, RTN and JAN proposed MMD-based regu-
larizer to pursue cross-domain distribution matching in a
feature space; ADDA, RevGred, GTA and VADA are do-
main adversarial training paradigms encouraging domain-
invariant feature learning by “cheating” their domain dis-
criminators. DIRT-T is built upon VADA by introducing
a network to guide the dense target feature regions away
from the decision boundary. Beyond these approaches, we
also report the Source-only results based on F and C that
are merely trained on source labeled data to classify target
examples.

Implementation setting. In digit recognition, we eval-
uate AMEAN on two different backbones. The ﬁrst is de-
rived from a LeNet architecture with F , Dst, Dmt, C jointly
trained through a reversed gradient layer (Eq.8); the sec-
ond employs a GAN-based alternating learning scheme [13]
that switches the optimization between Eq.9 , 10 . For a
fair comparison, all baselines in Digit-ﬁve experiment are
based on these backbones. In Ofﬁce-31 and Ofﬁce-Home,
we evaluate all baselines with AlexNet[19] and ResNet-50
[16], where our AMEANs are trained by Eq.8. Our meta-
learner employ the same architecture in all experiments, i.e.,
a four-layered fully-connected DAE. More implementation
details are deferred in our Appendix.A.

5.2. Evaluation Criteria.

Our experimental evaluation is aimed to answer two fun-

damental questions in this paper:

1. Does BTDA bring more transfer learning risks to ex-

isting DA algorithms ?

2. Is our AMEAN able to reduce these transfer risks ?

As a primal metric, classiﬁcation accuracies on a mixed tar-
get (AccBTDA) are provided to evaluate DA baselines in the
BTDA setup, where their adaptations from S to a mixed T
are performed to cultivate a classiﬁer that predicts labels on
a mixed target test set. To answer the questions above, we
also consider two additional metrics, i.e., absolute negative
transfer (ANT) and relative negative transfer (RNT):

• Absolute negative transfer (ANT). Given a DA base-
line, if its performance is inferior to its Source-only, it

62253

Table 2. Classiﬁcation accuracy (ACC %) on Ofﬁce-31 in BTDA setup. BLUE, RED indicate the baseline suffer from absolute negative
transfer (ANT%) or relative negative transfer (RNT%), respectively. Best viewed in color.

Backbones

Models

A→D,W

ACCANT

AlexNet

ResNet-50

Source only

DAN
RTN
JAN

RevGrad

62.4
68.2
71.6
73.7
74.1

AMEAN (ours)

74.5 (+0.4)

Source only

DAN
RTN
JAN

RevGrad

68.6
78.0
84.3
84.2
78.2

AMEAN (ours)

90.1 (+5.8)

D→A,W

W→A,D

Avg

ACCANT

RNT

ACCANT

RNT

ACCANT

RNT

60.8

58.7(−2.1)
56.3(−4.5)

62.1

58.6(−2.2)
62.8 (+0.7)

70.0

64.4(−5.6)
67.5(−2.5)

74.4
72.2

77.0 (+2.6)

0

-5.0
-4.6
-4.9
-4.3

-
0

-6.8
-5.5
-0.8
-2.7

-

57.2

55.6(−1.6)
52.2(−5.0)

58.4

55.0(−2.2)
59.7 (+1.3)

66.5
66.7

64.8(−0.2)

72.0
69.8

73.4 (+1.4)

0

-4.9
-6.1
-3.6
-3.4

-
0

-1.8
-5.5
-2.8
-2.8

-

60.1
60.8

59.9(−0.2)

64.7
62.6

65.7 (+1.0)

68.4
69.7
72.2
76.9
73.4

80.2 (+3.4)

0

-3.4
-4.1
-3.0
-2.2

-
0

-3.6
-2.9
-1.6
-2.9

-

RNT

0

-0.2
-1.1
-0.3
+0.9

-
0

-2.1
+2.3
-1.2
-3.3

-

Table 3. Classiﬁcation accuracy (ACC %) on Ofﬁce-Home in BTDA setup. BLUE, RED indicate the baseline suffer from absolute
negative transfer (ANT%) or relative negative transfer (RNT%), respectively. Best viewed in color.

Backbones

Models

Ar→Cl,Pr,Rw

Cl→Ar,Pr,Rw

Pr→Ar,Cl,Rw

Rw→Ar,Cl,Pr

Avg

ACCANT

RNT

ACCANT

RNT

ACCANT

RNT

ACCANT

RNT

ACCANT

RNT

AlexNet

ResNet-50

Source only

DAN
RTN
JAN

RevGrad

33.4
39.7
42.8
43.5
42.1

AMEAN (ours)

44.6 (+1.1)

Source only

DAN
RTN
JAN

RevGrad

47.6
55.6
53.9
58.3
58.4

AMEAN (ours)

64.3 (+5.9)

0

-3.7
-2.0
-2.9
-3.3

-
0

-0.8
-1.8
-0.4
-3.1

-

37.6
43.2
45.2
46.5
45.1

47.6 (+1.1)

42.6
56.6
56.7
60.5
58.1

65.5 (+5.0)

0

-3.0
-2.5
-3.6
-4.4

-
0

+0.9
-1.3
+2.3
-2.2

-

32.4
39.4
40.6
40.9
41.1

42.8 (+1.7)

44.2
48.5
47.3
52.2
52.9

59.5 (+6.1)

0

-3.4
-2.4
-6.6
-4.5

-
0

-5.1
-3.8
-2.2
-4.5

-

39.3
47.8
49.6
49.1
48.4

50.2 (+1.1)

51.3
56.7
51.6
57.5
62.1

0

-2.2
-2.7
-2.1
-5.6

0

-6.3
-2.8
-7.0
-3.0

35.7
42.5
44.6
45.0
44.2

46.3 (+1.3 )

46.4
54.4
52.4
57.1
57.9

66.7 (+4.6)

64.0 (+6.1)

0

-3.1
-2.3
-4.6
-4.4

-
0

-2.6
-2.6
-1.9
-3.2

-

implies that this DA algorithm not only fails to beneﬁt
but also damages the classiﬁer, i.e., suffers from ANT.

• Relative negative transfer (RNT). RNT aims to mea-
sure how much performance drops if a DA baseline al-
ters from MTDA to BTDA setups. In MTDA, each DA
baseline performs adaptation from source S to each
explicit target Tj (∀j ∈ [k]).
It results in k target-
speciﬁc domain-adaptive models with their accuracies
{Accj}k
j=1 on the target test sets respectively. Towards
this end, we compute the MTDA weighted averaged
accuracy by AccMTDA = Pk
j=1 αj Accj where αj in-
dicates the ratio of sub-target Tj in a mixed target T .
Hence we have RNT = AccBTDA − AccMTDA.

More details of the metrics can be found in our Appendix.B.

5.3. Results.

The evaluations based on classiﬁcation accuracy (ACC,
i.e., AccBTDA), ANT and RNT have been conducted in Ta-
bles.1 - 3 (we highlight ANT, RNT in BLUE, RED).

In Digit-ﬁve (Table.1), ANT frequently occurs in BTDA
In sv→mm,mt,up,sy and sy→mm,mt,up,sv, few
setups.
DA algorithms are exempt for these performance degrada-
tion under backbone-1 (i.e., -26.8 for ADDA). This situa-
tion is ameliorated on backbone-2. However, all DA base-
lines suffer from RNT. Especially when an adaptation pro-
cess starts from a simple source to complex targets, i.e.,
mt→mm,sv,up,sy, the accuracy drop from MTDA to BTDA

is signiﬁcant. In Ofﬁce-31. (Table.2), due to D and W shar-
ing similar visual appearances, most of their categories are
aligned well in the mixed target. Therefore ANT and RNT
are suppressed in A→ D,W. But in the other transfer tasks,
ANT and RNT still haunt the performances of DA base-
lines. In Ofﬁce-Home (Table.3), all DA baselines get rid
of ANT, whereas they remain inferior to their performances
in MTDA setup (suffer RNT). Observe that, deeper models,
e.g., ResNet-50, may encourage the baselines to resist ANT.
But the deeper models do not help reduce RNTs across DA
algorithms. These evidences sufﬁciently verify the hardness
of BTDA and answer the ﬁrst question.

Though BTDA is a challenging transfer setup, AMEAN
presents as a ideal solver. As shown in Table.1 -3 , AMEAN
achieved the state-of-the-art in 29 out of 30 BTDA trans-
fer cases, and its average accuracy exceeds the second best
by 1.0 ∼ 7.7%. AMEAN almost achieve positive transfer
in all transfer cases, and has reaped huge transfer gains in
some of them, e.g., +37.7% in mt→mm,sv,up,sy, +21.5%
in A→ D,W, +31.3% in Ar→Cl,Pr,Rw, etc. More im-
portantly, AMEAN obtains more impressive performances
from deeper architectures, which demonstrates its superior-
ity to address BTDA problem.

5.4. Analysis.

Adaptation visualization. For the BTDA transfer task
mt → mm,sv,up,sy in Digit-ﬁve, we visualize the classiﬁ-

72254

Figure 3. t-SNE visualizations of the features learned by Source-only, RevGred, VADA and AMEAN on Digit-ﬁve in BTDA setup. Shapes
and colors indicate different domains and categories, respectively. Best viewed in color.

Table 4. The ablation of Vmt constructed by Explicit Sub-Target
(EST), deep k-Clustering (k-C) and AMEAN (ours).

mt→mm,sv,up,sy mm→mt,sv,up,sy D→A,W W→A,D

EST
k-C
ours

79.9
81.6
85.1

74.7
74.5
77.6

62.2
61.3
62.8

58.4
59.1
59.7

Figure 4. Ablation studies of our meta-learner across three transfer
tasks in Digit-ﬁve (Backbone-2) and in Ofﬁce-31 (AlexNet).

cation activations from Source-only, RevGred, VADA and
AMEAN in BTDA setup. As can be seen in Fig.3 , Source-
only barely captures any classiﬁcation patterns. In a com-
parison, RevGred and VADA show better classiﬁable visu-
alization patterns than Source-only’s. But their activations
remain pretty messy and most of them are misaligned in
their classes. It demonstrates that BTDA is a very challeng-
ing scenario for existing DA algorithms. Finally, the acti-
vations from AMEAN show clear classiﬁcation margins. It
illustrates the superior transferability of our AMEAN.

Ablation study. The crucial component of our AMEAN
is the meta-learner for auto-designing Vmt. Hence our
ablation focuses on this model-driven auto-learning tech-
nique. In particular, we evaluate three adaptation manners
derived from our AMEAN: adaptation without meta-learner
(w/o meta learner); adaptation without meta-learner but us-
ing explicit sub-target ({Tj}k
j=1) to guide the transfer in
BTDA (explicit sub-targets); adaptation with meta-learner
(w meta-learner, AMEAN). As illustrated in Fig.4 , explicit
sub-target information is not persistently helpful to BTDA.
In sy-to-others, explicit sub-target information even draws
back the source-to-target transfer gain. By contrast, meta-
learner plays a key role to enhance the adaptation towards
digit and real-world visual domains and obtain state-of-the-
art in BTDA. Surprisingly, the dynamical meta-sub-targets
even drive the adaptation model exceed those trained with
the explicit sub-target domains.

To further investigate the meta-adaptation dynamic pro-
vided by AMEAN, we ablate the multi-target DA by follow-
ing different target separation strategies: 1). explicit-sub-
target (EST); 2).static deep k-clustering (k-C); 3) AMEAN.
Note that, 2) ablates the auto-loss-design dynamic in Vmt,

Figure 5. The ablation of Vmt based on mt→mm,sv,up,sy in Digit-
ﬁve, where Vmt is constructed in different manners over iterations.
AMEAN performs a faster and more stable optimization process.

as it solely uses the initial clusters to divide the mixed target
and keep a static Vmt along model training. The comparison
of 2) and 3) helps to unveil whether our auto-loss strategy
facilitates AMEAN. As shown in Table 4 , 1) and 2) dis-
regard label information given by a source and their Vmt
still suffer from a risk of class mismatching. Our auto-loss
manner adaptively changes Vmt by receiving label informa-
tion from the features previously learned by Eq.1 and thus,
achieve better performance in BTDA. More importantly, it
also encourages a fast and more stable adaptation during the
minimax optimization process (see Fig 5).

6. Concluding Remarks

In this paper, we concern a realistic adaptation scenario,
where our target domain is comprised of multiple hidden
sub-targets and learners could not identify which sub-target
each unlabeled example comes from. This Blending-target
domain adaptation (BTDA) conceives category mismatch-
ing risk and if we apply existing DA algorithm in BTDA, it
will lead to negative transfer effect. To take on this uprising
challenge, we propose Adversarial MEta-Adaptation Net-
work (AMEAN). AMEAN starts from the popular adversar-
ial adaptation methods, while newly employs a meta-learner
network to dynamically devise a multi-target DA loss along
with learning domain-invariant features. The AutoML mer-
its are inherited by AMEAN to reduce the class mismatch-
ing in the mixed target domain. Our experiments focus on
verifying the threat of BTDA and the efﬁcacy of AMEAN
in BTDA. Our evaluations show the signiﬁcance of BTDA
problem as well as the superiority of our AMEAN.

82255

References

[1] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman,
D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learn-
ing to learn by gradient descent by gradient descent.
In
Advances in Neural Information Processing Systems, pages
3981–3989, 2016.

[2] Anonymous. Unsupervised multi-target domain adaptation:
An information theoretic approach. In Submitted to Interna-
tional Conference on Learning Representations, 2019. under
review.

[3] J. C. Balloch, V. Agrawal, I. Essa, and S. Chernova. Unbi-
asing semantic segmentation for robot perception using syn-
thetic data feature transfer. arXiv preprint arXiv:1809.03676,
2018.

[4] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different do-
mains. Machine learning, 79(1):151–175, 2010.

[5] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey,
M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Kono-
lige, et al. Using simulation and domain adaptation to im-
prove efﬁciency of deep robotic grasping. In 2018 IEEE In-
ternational Conference on Robotics and Automation (ICRA),
pages 4243–4250. IEEE, 2018.

[6] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and
D. Krishnan. Unsupervised pixel-level domain adapta-
tion with generative adversarial networks. arXiv preprint
arXiv:1612.05424, 2016.

[7] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Do-
main adaptive faster r-cnn for object detection in the wild.
2018.

[8] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-

learning for fast adaptation of deep networks. 2017.

[9] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. arXiv preprint arXiv:1409.7495, 2014.

[10] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
Adversarial Training of Neural Networks. 2017.

[11] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiﬁcation networks for un-
supervised domain adaptation. In European Conference on
Computer Vision, pages 597–613. Springer, 2016.

[12] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter,
M. Naehrig, and J. Wernsing. Cryptonets: Applying neu-
ral networks to encrypted data with high throughput and ac-
curacy. In International Conference on Machine Learning,
pages 201–210, 2016.

[13] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In International Conference on
Neural Information Processing Systems, pages 2672–2680,
2014.

[14] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation
for object recognition: An unsupervised approach. In Com-
puter Vision (ICCV), 2011 IEEE International Conference
on, pages 999–1006. IEEE, 2011.

[15] A. Gretton, A. J. Smola, J. Huang, M. Schmittfull, K. M.
Borgwardt, and B. Sch¨olkopf. Covariate shift by kernel mean
matching. 2009.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. 2015.

[17] J. Hoffman, E. Tzeng, T. Darrell, and K. Saenko. Simulta-
neous deep transfer across domains and tasks.
In Domain
Adaptation in Computer Vision Applications, pages 173–
187. Springer, 2017.

[18] J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the
wild: Pixel-level adversarial and constraint-based adapta-
tion. arXiv preprint arXiv:1612.02649, 2016.

[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012.

[20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[21] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-
In Advances in neural information processing sys-

works.
tems, pages 469–477, 2016.

[22] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer-
able features with deep adaptation networks. In International
Conference on Machine Learning, pages 97–105, 2015.

[23] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep trans-
fer learning with joint adaptation networks. arXiv preprint
arXiv:1605.06636, 2016.

[24] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsuper-
vised domain adaptation with residual transfer networks. In
Advances in Neural Information Processing Systems, pages
136–144, 2016.

[25] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep transfer

learning with joint adaptation networks. 2017.

[26] M. Mancini, L. Porzi, S. R. Bul`o, B. Caputo, and E. Ricci.
Boosting domain adaptation by discovering latent domains.
arXiv preprint arXiv:1805.01386, 2018.

[27] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-
tation with multiple sources. In Advances in neural informa-
tion processing systems, pages 1041–1048, 2009.

[28] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. Nips Workshop on Deep Learning and Unsu-
pervised Feature Learning, 2011.

[29] S. J. Pan and Q. Yang. A survey on transfer learning.
IEEE Transactions on knowledge and data engineering,
22(10):1345–1359, 2010.

[30] A. A. Rusu, M. Vecerik, T. Roth¨orl, N. Heess, R. Pascanu,
and R. Hadsell. Sim-to-real robot learning from pixels with
progressive nets. arXiv preprint arXiv:1610.04286, 2016.

[31] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. Computer Vision–
ECCV 2010, pages 213–226, 2010.

[32] K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-
training for unsupervised domain adaptation. arXiv preprint
arXiv:1702.08400, 2017.

92256

[33] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chel-
lappa. Generate to adapt: Aligning domains using generative
adversarial networks. ArXiv e-prints, abs/1704.01705, 2017.
[34] R. Shu, H. H. Bui, H. Narui, and S. Ermon. A dirt-t ap-
proach to unsupervised domain adaptation. arXiv preprint
arXiv:1802.08735, 2018.

[35] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Ad-
versarial discriminative domain adaptation. arXiv preprint
arXiv:1702.05464, 2017.

[36] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan-
chanathan. Deep hashing network for unsupervised domain
adaptation. In Proc. CVPR, pages 5018–5027, 2017.

[37] Y. X. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-

shot learning from imaginary data. 2018.

[38] J. Xie, R. Girshick, and A. Farhadi. Unsupervised deep em-
bedding for clustering analysis. In International conference
on machine learning, pages 478–487, 2016.

[39] H. Xu, H. Zhang, Z. Hu, X. Liang, R. Salakhutdinov, and
E. Xing. Autoloss: Learning discrete schedules for alternate
optimization. 2018.

[40] R. Xu, Z. Chen, W. Zuo, J. Yan, and L. Lin. Deep cock-
tail network: Multi-source unsupervised domain adaptation
with category shift. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3964–
3973, 2018.

[41] J. Yang, R. Yan, and A. G. Hauptmann. Cross-domain video
concept detection using adaptive svms. In Proceedings of the
15th ACM international conference on Multimedia, pages
188–197. ACM, 2007.

[42] L. Yang, X. Liang, T. Wang, and E. Xing. Real-to-virtual do-

main uniﬁcation for end-to-end autonomous driving. 2018.

[43] Y. You, X. Pan, Z. Wang, and C. Lu. Virtual to real rein-

forcement learning for autonomous driving. 2017.

[44] H. Yu, M. Hu, and S. Chen. Multi-target unsupervised do-

main adaptation without exactly shared categories. 2018.

[45] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschl¨ager,
and S. Saminger-Platz. Central moment discrepancy (cmd)
for domain-invariant representation learning. arXiv preprint
arXiv:1702.08811, 2017.

[46] H. Zhao, S. Zhang, G. Wu, J. ao P. Costeira, J. M. F. Moura,
and G. J. Gordon. Multiple source domain adaptation with
adversarial learning, 2018.

[47] B. Zoph and Q. V. Le. Neural architecture search with rein-
forcement learning. arXiv preprint arXiv:1611.01578, 2016.

102257

