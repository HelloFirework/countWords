Object-aware Aggregation with Bidirectional Temporal Graph

for Video Captioning

Junchao Zhang and Yuxin Peng∗

Institute of Computer Science and Technology,

Peking University, Beijing 100871, China

pengyuxin@pku.edu.cn

Abstract

Video captioning aims to automatically generate natural
language descriptions of video content, which has drawn a
lot of attention recent years. Generating accurate and ﬁne-
grained captions needs to not only understand the global
content of video, but also capture the detailed object in-
formation. Meanwhile, video representations have great
impact on the quality of generated captions. Thus, it is
important for video captioning to capture salient objects
with their detailed temporal dynamics, and represent them
using discriminative spatio-temporal representations.
In
this paper, we propose a new video captioning approach
based on object-aware aggregation with bidirectional tem-
poral graph (OA-BTG), which captures detailed tempo-
ral dynamics for salient objects in video, and learns dis-
criminative spatio-temporal representations by performing
object-aware local feature aggregation on detected object
regions. The main novelties and advantages are: (1) Bidi-
rectional temporal graph: A bidirectional temporal graph
is constructed along and reversely along the temporal order,
which provides complementary ways to capture the tempo-
(2) Object-aware
ral trajectories for each salient object.
aggregation: Learnable VLAD (Vector of Locally Aggre-
gated Descriptors) models are constructed on object tem-
poral trajectories and global frame sequence, which per-
forms object-aware aggregation to learn discriminative rep-
resentations. A hierarchical attention mechanism is also
developed to distinguish different contributions of multiple
objects. Experiments on two widely-used datasets demon-
strate our OA-BTG achieves state-of-the-art performance in
terms of BLEU@4, METEOR and CIDEr metrics.

1. Introduction

As a task of generating natural language descriptions for
video content automatically, video captioning takes a cru-

∗Corresponding author.

(cid:36) (cid:38)(cid:75)(cid:76)(cid:81)(cid:72)(cid:86)(cid:72)(cid:3)(cid:80)(cid:68)(cid:81)(cid:3)(cid:86)(cid:75)(cid:82)(cid:82)(cid:87)(cid:86)(cid:3)(cid:68)(cid:3)(cid:69)(cid:68)(cid:86)(cid:78)(cid:72)(cid:87)(cid:69)(cid:68)(cid:79)(cid:79)(cid:3)(cid:76)(cid:81)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:69)(cid:68)(cid:86)(cid:78)(cid:72)(cid:87)

Figure 1. Illustration for the salient objects marked by the colored
rectangle boxes and an example temporal trajectory indicated by
dashed curve, which are important for discriminative video under-
standing to generate accurate captioning description.

cial step forward to the high-level video understanding and
artiﬁcial intelligence.
It supports various potential appli-
cations, such as human-robot interaction or assisting the
visually-impaired. Recently, it has received increasing at-
tention in both computer vision and artiﬁcial intelligence
communities.

Previous works have explored to model the temporal in-
formation of video content by temporal attention mecha-
nism [1, 2] or hierarchical encoder-decoder structures [3, 4].
However, they mainly work on the global frame or salient
regions without discrimination on speciﬁc object instances,
which cannot well capture the detailed temporal dynamics
of each object. While for obtaining the accurate caption-
ing descriptions for complex video content, it plays a key
but challenging role to capture the salient objects with their
detailed temporal dynamics. As shown in Fig. 1, the ref-
erence captioning sentence “A Chinese man shoots a bas-
ketball into the basket” involves three salient objects in the
example video, namely the boy, the basketball and the bas-
ket, which needs the object-aware video understanding. Be-
sides, the reference sentence also describes the action that
the boy is performing, “shoots a basketball”, which needs to
understand the detailed temporal dynamics of the boy and
the basketball.

In addition, video representations have great impact
on the quality of generated captions. Therefore, how

18327

to describe the video content using discriminative spatio-
temporal representations is also important for video cap-
tioning. Many works directly extract global features on
video frames from the fully-connected layer or global pool-
ing layer in CNN, which may lose much ﬁne spatial infor-
mation. NetVLAD [5] shows its local information encod-
ing ability by embedding a trainable VLAD (vector of lo-
cally aggregated descriptors) encoding model into the CNN,
which aggregates the local features to encode local spatial
information. Following it, SeqVLAD [6] is proposed re-
cently to combine the trainable VLAD encoding model with
the sequence learning process, which explores both the lo-
cal spatial information and temporal information of video.
However, the above methods do not consider the object-
speciﬁc information, thus cannot distinguish speciﬁc ﬁne
spatio-temporal information corresponding to the speciﬁc
object instance.

For addressing the above two problems, in this paper, we
propose a novel video captioning approach based on object-
aware aggregation with bidirectional temporal graph (OA-
BTG), which captures detailed temporal dynamics for the
salient objects in video via a bidirectional temporal graph,
and learns discriminative spatio-temporal video representa-
tions by performing object-aware local feature aggregation
on object regions. Its main novelties and advantages are:

• Bidirectional temporal graph: The bidirectional
temporal graph is constructed on salient objects and
global frames to capture the detailed temporal dynam-
ics in video. The bidirectional temporal graph includes
a forward graph along the temporal order and a back-
ward graph reversely along the temporal order, which
provide different ways to construct the temporal trajec-
tories with complementary information for each salient
object instance.
In such way, detailed temporal dy-
namics for objects and global context are captured to
generate accurate and ﬁne-grained captions.

• Object-aware aggregation: For encoding the ﬁne
spatio-temporal information, we construct learnable
VLAD models on object temporal trajectories and
global frame temporal sequences, which perform
object-aware aggregation for each salient object in-
stance as well as the global frame to learn discrimi-
native representations. We also utilize a hierarchical
attention mechanism to distinguish different contribu-
tions of different object instances.
In such way, we
learn the discriminative spatio-temporal video repre-
sentations for boosting the captioning performance.

We conduct experiments on two widely-used datasets
MSVD and MSR-VTT, which demonstrate that our pro-
posed OA-BTG approach achieves the state-of-the-art per-
formance in terms of BLEU@4, METEOR and CIDEr met-
rics for video captioning.

2. Related Works

In the early stage, video captioning methods are mainly
template-based language models [7, 8, 9]. These methods
follow a bottom-up paradigm, which ﬁrst predicts semantic
concepts or words, like objects, scenes and activities, and
then generates sentences according to pre-deﬁned language
templates. These methods heavily rely on the template def-
inition and the predicted video concepts, which limits the
diversities of generated sentences. Recently, inspired by
the development of deep learning and neural machine trans-
lation (NMT) [10], many sequence learning based mod-
els [4, 11, 12, 13] are proposed to address video caption-
ing problem. Regarding video captioning as a “translating”
process, these methods construct the encoder-decoder struc-
tures to directly generate sentences from the video content.

Venugopalan et al. [14] make the early attempt to gener-
ate video descriptions using encoder-decoder structure, but
they simply apply mean pooling over individual frame fea-
tures to obtain video representation, which ignores the tem-
poral information of ordered video frames. For addressing
this issue, the following works [1, 4, 11] make advances
by using temporal attention mechanism, as well as taking
LSTM-based encoders to learn long-term temporal struc-
tures. Yang et al.
[2] achieve the progress by further
considering the different characteristics of video frames.
They propose to adaptively capture the regions-of-interests
in each frame, then learn discriminative features based on
these regions-of-interests for better video captioning. Xu
et al. [6] propose the SeqVLAD method, which performs
feature aggregation on frame features to exploit ﬁne spatial
information in video content.

However,

these methods mainly work on the global
frame or salient regions without discrimination on speciﬁc
object instances, which cannot well capture the temporal
evolution of each object in video. In this work, we propose
the OA-BTG approach, which constructs bidirectional tem-
poral graph on the objects across video frames to captures
their temporal trajectories. In addition, we also perform rep-
resentation learning on the temporal trajectories, which ex-
ploits the object-awareness to boost video captioning.

There are also some works [12, 15, 16, 17, 18] that ex-
ploit multi-modal features for video captioning. Besides
frame features extracted by popular 2D CNNs, they also
exploit motion features extracted by C3D [19] or audio fea-
tures [20], where they mine the complementarities among
multi-modal information to boost the video captioning per-
formance. Different from them, our OA-BTG approach
takes only visual features, which mainly focuses on captur-
ing detailed temporal evolutions of objects by bidirectional
temporal graph and learning discriminative features through
object-aware feature aggregation.

28328

3. Object-aware Aggregation with Bidirec-

tional Temporal Graph

two object regions ri and rj , we deﬁne their similarity score
s(i, j) as follows:

In this section, we present the proposed video caption-
ing approach based on object-aware aggregation with bidi-
rectional temporal graph (OA-BTG) in detail, which fol-
lows the encoder-decoder framework. As shown in Figure
(1) Bidi-
2, our OA-BTG consists of three components.
rectional Temporal Graph: For the input video, we ﬁrst
extract frames and multiple object regions. Then we con-
struct bidirectional temporal graph to capture detailed tem-
poral dynamics along and reversely along the temporal or-
(2) Object-aware Aggregation: Based on the bidi-
der.
rectional temporal graph, we perform object-aware aggre-
gation to aggregate the local features of object regions and
global frames into discriminative VLAD representations us-
(3) Decoder: Above two
ing learnable VLAD models.
components form the encoding stage. While in the decod-
ing stage, the learned object and frame VLAD representa-
tions are integrated and fed into the GRU units to generate
descriptions. Especially, hierarchical attention is applied to
distinguish the different contributions of multiple objects.
In the following subsections, we will introduce the bidi-
rectional temporal graph, object-aware aggregation and de-
coder respectively.

3.1. Bidirectional Temporal Graph

The bidirectional temporal graph (BTG) is constructed
based on detected object regions and global frames. For ob-
ject regions, bidirectional temporal graph is constructed to
group the same object instances or similar regions together
along and reversely along the temporal order.
It obtains
the forward and backward temporal trajectories of each de-
tected object instance, thus capture the detailed temporal
dynamics of salient objects, which are important for gener-
ating accurate and ﬁne-grained language descriptions. For
the global frames, we organize them into sequences along
and reversely along the temporal order to capture the for-
ward and backward temporal dynamics for global context.
The bidirectional temporal graph is constructed accord-
ing to the similarities among object regions in different
frames, including a forward graph and a backward graph to
obtain the object trajectories along and reversely along the
temporal order respectively. Speciﬁcally, for each frame vt
in the input video V , we extract N object regions, R(t) =
{r(t)
i }, where i = 1, 2, · · · , N , t = 1, 2, · · · , T , and T is
the number of sampled frames of V . For constructing the
forward graph, we take the object regions in frame v1 as
anchors to compute the similarities with object regions in
all other frames, where the similarity score is deﬁned to in-
dicate whether the two object regions belong to the same
object instance. With jointly considering the appearance in-
formation and relative spatial location information between

s(i, j) = (sapp(i, j) + siou(i, j) + sarea(i, j))/3

(1)

As for three terms in above equation, sapp indicates sim-
ilarity on visual appearance between ri and rj , which is
computed according to the Euclidean distance of their vi-
sual features:

sapp(i, j) = exp(cid:18)−

maxp,q(L2(gp, gq))(cid:19)

L2(gi, gj)

(2)

denotes

distance,

the Euclidean

where L2
and
maxp,q(L2(gp, gq)) computes the maximal Euclidean
distance of all object region pairs between the correspond-
ing two frames, which is utilized as a normalization factor.
g indicates the extracted visual feature for object region
using pretrained CNN model, taking the cropped object
region image as input. siou and sarea indicate the rates of
overlap area and area sizes between ri and rj , respectively,
which are computed as follows:

siou(i, j) =

area(ri) ∩ area(rj)
area(ri) ∪ area(rj)

sarea(i, j) = exp(cid:18)−(cid:12)(cid:12)(cid:12)(cid:12)

min(Ai, Aj)
max(Ai, Aj)

(3)

(4)

(cid:19)

− 1(cid:12)(cid:12)(cid:12)(cid:12)

where area denotes the spatial area of the object region and
A mean its area size.

The construction of the backward graph is similar to the
forward graph, while the anchors to compute the similar-
ity scores are composed of the object regions in frame vT .
Then the forward graph and the backward graph are com-
bined to compose the bidirectional temporal graph.

Aiming to group the object regions in different frames
but belonging to the same object instance together, we com-
pare object regions in all the other frames with the anchor
object regions, and then align them to the anchor object re-
gions with a nearest neighbor (NN) strategy according to
the bidirectional temporal graph. Speciﬁcally, for the ob-
ject region r(1)
in anchor frame v1 and N object regions
R(t) = {r(t)
j } in frame vt, t = 2, · · · , T , the object region
(s(i, j)) is aligned to the object region r(1)
argmaxr(t)
,
which means they are considered to belong to the same ob-
ject instance. We also align object regions in other frames
to the anchor object regions in frame vT using the same NN
strategy.

i

i

j

After above alignment process, we have obtained 2N
groups of aligned object regions. Then, N groups on the
forward graph are organized along the temporal order to
obtain the forward temporal trajectories of detected object
instances, while the other N groups on the backward graph
are organized reversely along the temporal order to obtain

38329

(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:57)(cid:76)(cid:71)(cid:72)(cid:82)

(cid:11)(cid:20)(cid:12)(cid:3)(cid:37)(cid:76)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:55)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3)(cid:42)(cid:85)(cid:68)(cid:83)(cid:75)

(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:53)(cid:72)(cid:74)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:55)
(cid:72)
(cid:80)
(cid:83)
(cid:82)
(cid:85)
(cid:68)
(cid:79)
(cid:3)

(cid:42)
(cid:85)
(cid:68)
(cid:83)
(cid:75)

(cid:3)

(cid:37)
(cid:71)

(cid:76)

(cid:76)
(cid:85)
(cid:72)
(cid:70)
(cid:87)
(cid:76)
(cid:82)
(cid:81)
(cid:68)
(cid:79)
(cid:3)

(cid:257)
(cid:257)
(cid:257)

(cid:257)
(cid:257)
(cid:257)

(cid:257)
(cid:257)
(cid:257)

(cid:42)(cid:79)(cid:82)(cid:69)(cid:68)(cid:79)(cid:3)(cid:41)(cid:85)(cid:68)(cid:80)(cid:72)(cid:86)

(cid:19)

(cid:55)

(cid:41)(cid:82)(cid:85)(cid:90)(cid:68)(cid:85)(cid:71)(cid:3)(cid:87)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3)(cid:87)(cid:85)(cid:68)(cid:77)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:92)
(cid:37)(cid:68)(cid:70)(cid:78)(cid:90)(cid:68)(cid:85)(cid:71)(cid:3)(cid:87)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3)(cid:87)(cid:85)(cid:68)(cid:77)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:92)

(cid:55)

(cid:19)

(cid:11)(cid:21)(cid:12)(cid:3)(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:16)(cid:68)(cid:90)(cid:68)(cid:85)(cid:72)(cid:3)(cid:36)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:57)(cid:47)(cid:36)(cid:39)(cid:3)
(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)
(cid:53)(cid:72)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:57)(cid:47)(cid:36)(cid:39)

(cid:38)(cid:16)(cid:42)(cid:53)(cid:56)

(cid:11)(cid:22)(cid:12)(cid:3)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:31)(cid:37)(cid:50)(cid:54)(cid:33)

(cid:42)(cid:53)(cid:56)

(cid:68)

(cid:68)

(cid:42)(cid:53)(cid:56)

(cid:80)(cid:68)(cid:81)

(cid:43)(cid:76)(cid:72)(cid:85)(cid:68)(cid:85)(cid:70)(cid:75)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)
(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)
(cid:55)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3)
(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)
(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:38)(cid:16)(cid:42)(cid:53)(cid:56)

(cid:257)
(cid:257)
(cid:257)

(cid:257)
(cid:257)
(cid:257)

(cid:257)
(cid:257)
(cid:257)

(cid:257)
(cid:257)
(cid:257)

(cid:257)
(cid:257)
(cid:257)

(cid:80)(cid:68)(cid:81)

(cid:42)(cid:53)(cid:56)

(cid:68)(cid:81)(cid:71)

(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)

(cid:57)(cid:47)(cid:36)(cid:39)
(cid:38)(cid:16)(cid:42)(cid:53)(cid:56)

(cid:38)(cid:16)(cid:42)(cid:53)(cid:56)

(cid:54)(cid:88)(cid:80)

(cid:68)(cid:81)(cid:71)

(cid:42)(cid:53)(cid:56)

(cid:68)

(cid:41)(cid:85)(cid:68)(cid:80)(cid:72)(cid:3)(cid:57)(cid:47)(cid:36)(cid:39)
(cid:53)(cid:72)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:55)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3)
(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:257)
(cid:257)
(cid:257)

(cid:80)(cid:82)(cid:87)(cid:82)(cid:85)(cid:70)(cid:92)(cid:70)(cid:79)(cid:72)

(cid:42)(cid:53)(cid:56)

(cid:31)(cid:40)(cid:50)(cid:54)(cid:33)

(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:87)(cid:68)(cid:74)(cid:72)

(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:87)(cid:68)(cid:74)(cid:72)

Figure 2. Overview of the proposed OA-BTG approach.

the backward temporal trajectories. The forward and back-
ward temporal trajectories are complement on capturing the
detailed temporal dynamics of salient objects in video for
the following two aspects: (1) Organizing the temporal se-
quence along and reversely along the temporal order pro-
vides two different ways to represent the temporal dynam-
ics of video content, and thus provides complementary in-
formation. (2) It usually cannot obtain the good temporal
trajectories for all the salient objects only on the forward
or backward graph, since not all the objects occur through-
out the whole video. Thus, we resort to both forward and
backward temporal trajectories on the bidirectional tempo-
ral graph for capturing the temporal dynamics better.

i = {of

it} and Ob

We denote the forward and backward temporal trajec-
tories as Of
it}, t = 1, · · · , T ,
i = 1, · · · , N , respectively. Then, we aslo organize the
global frames along and reversely along the temporal order
as V f = {vf
t }, t = 1, · · · , T , to capture
comprehensive temporal dynamics on global context.

t } and V b = {vb

i = {ob

3.2. Object aware Aggregation

For object-aware aggregation (OA), we devise two learn-
able VLAD models to learn the spatio-temporal correlations
for object region sequences and global frame sequence, as
well as aggregate the local features of object regions and
frames into discriminative VLAD representations.

Local features are ﬁrst extracted for global frames and
the detected objects of input video V . We feed the global
frames and cropped object region images into the pretrained
CNN model and take the feature maps from the convolu-
tional layer as local features. Each feature map has the size
as H × W × D, where H, W and D mean the height,
width and the number of channels. We organize the lo-
cal features of object regions and global frames according

to the forward and backward temporal sequences obtained
based on the BTG, and denote them as X of
it } and
it } for object sequences, as well as X f = {xf
X ob
t }
and X b = {xb

t} for frame sequences.

i = {xof

i = {xob

Inspired by NetVLAD [5] and SeqVLAD [6], we utilize
a convolutional gated recurrent unit (C-GRU) architecture
to construct the learnable VLAD model, where the C-GRU
aims to learn the soft assignments for VLAD encodings.

Taking the learnable VLAD model on forward tempo-
ral sequences of object regions as an example, the C-GRU
takes the local feature xof
(here the subscript i is omitted
t
for simplicity) at time t and the hidden state at−1 at time
t − 1 as inputs, and then updates its hidden state at as fol-
lows:

zt = σ(Wz ∗ xof
rt = σ(Wr ∗ xof

t + Uz ∗ at−1)

t + Ur ∗ at−1)

t + Ua ∗ (rt ⊙ at−1))

eat = tanh(Wa ∗ xof

at = (1 − zt) ⊙ at−1 + zt ⊙eat

where Wz, Wr, Wa and Uz, Ur, Ua denote the 2D-
convolutional kernels. Noted that all N groups of object
region sequences share the same C-GRU parameters. ∗ de-
notes the convolution operation, σ denotes the sigmoid ac-
tivation function, and ⊙ denotes the element-wise multipli-
cation. The output hidden state at ∈ RH×W ×K denotes
the learned assignments between local features X of and K
cluster centers, which are also learnable and introduced be-
low.

VLAD is a feature encoding method that learns K clus-
ter centers as visual words, denoted as C = {ck}, k =
1, · · · , K, and then maps each local feature to the nearest
ck ∈ RD.
Its key idea is to accumulate the differences
between local features and the corresponding cluster cen-
ter. Inspired by SeqVLAD [6], we set K cluster centers as

48330

(5)

(6)

(7)

(8)

learnable parameters and adopt “soft assignment” strategy
that assign the local features to the cluster centers accord-
ing to the learned assignment parameters at:

vlof

t =

HXh=1

WXw=1

at(h, w, k)(xof

t − ck)

(9)

βo
li = exp(eo

li)/

NXn=1

exp(eo

li)

φof
l =

NXi=1

liφof
βo

li

(14)

(15)

Then we obtain the VLAD representations for forward tem-
poral sequences of object regions as V Lof
it },
t = 1, · · · , T , and i = 1, · · · , N . Similarly, the VLAD
representations for backward sequences of object regions as
well as two temporal sequences of global frames can be ob-
tained as V Lob
t }.

t } and V Lb = {vlb

it }, V Lf = {vlf

i = {vlof

i = {vlob

3.3. Decoder

In the decoding stage, we process the forward and back-
ward temporal sequences respectively. Taking the forward
temporal sequences as example, the decoder is constructed
by GRU units with attention mechanism, which utilizes
VLAD representations of objects {V Lof
i } and frames V Lf
to generate words for captioning.

As shown in Fig. 2, the attention model for object VLAD
representations has a hierarchical structure including tem-
poral attention and object attention. The temporal attention
is applied to highlight object regions at discriminative time
steps when merging T object VLAD representations into
one representation, while the object attention is designed to
distinguish the different contributions of N different object
instances. The temporal attention mechanism is formulated
as follows:

elt = wT

atttanh(Watthl−1 + Uattvlof

it + batt)

(10)

where watt, Watt, Uatt and batt are parameters. elt com-
putes the relevant score between the visual feature vlof
it and
the hidden state hl−1 of GRU decoder at time l−1, l ≤ L in-
dicates the time step at decoding stage. Then the relevance
scores are normalized as attention scores:

βlt = exp(elt)/

TXn=1

exp(eln)

(11)

Then T object visual features are merged according to
above attention scores:

att, U o

att, W o

where wo
l de-
notes the discriminative spatio-temporal feature that indi-
cates the object information.

att are parameters, and φof

att and bo

For T frame VLAD representations {vlf

t }, the tempo-
ral attention mechanism is applied on them to obtain the
discriminative spatio-temporal feature φf
l that indicates the
global context information.

At time l, the obtained features φof
l

l are fed into
the GRU unit to update the hidden state and generate the
word:

and φf

(16)

(17)

(18)

(19)

l = σ(Wvzφf
zd

l + Wozφof

l + Wdzxw

l + Udzhl−1)

l = σ(Wvrφf
rd

l + Worφof

l + Wdrxw

l + Udrhl−1)

ehl = tanh(Wvhφf

l + Wohφof

l + Udh(rd

l ⊙ hl−1))

hl = (1 − zd

l ) ⊙ hl−1 + zd

l ⊙ehl

where σ denotes the sigmoid function and xw
the word embedding for
Wv∗, Wo∗, Wd∗ and Ud∗ denote the parameters to learn.

l denotes
the input word in time l.

After obtaining the hidden state hl, we apply a linear
layer and a softmax layer to compute the probability distri-
bution over all the vocabulary words. In the training stage,
we utilize the cross-entropy loss to optimize all the learn-
able parameters. While in the testing stage, we take beam
search method to generate the captioning descriptions.

We take a simple but effective way to exploit the comple-
mentarity of the forward and backward temporal sequences
(corresponding to the forward and backward graphs respec-
tively). In each time step, we fuse the obtained predicted
scores of words based on forward and backward graphs, and
then the word is generated according to the fused scores.
In such way, we mine the complementary between the for-
ward and backward temporal sequences to boost the video
captioning performance.

φof
li =

TXt=1

βltvlof
it

4. Experiments

(12)

The object attention takes the same mechanism as tem-
poral attention, which is applied on {φof
li }, i = 1, · · · , N to
distinguish the different contributions of N different object
instances:

eo
li = woT

atttanh(W o

atthl−1 + U o

attφof

li + bo

att)

(13)

4.1. Datasets and Evaluation Metrics

4.1.1 Datasets.

We evaluate our proposed OA-BTG approach on two
widely-used datasets, including Microsoft Video Descrip-
tion Corpus (MSVD) [21] and Microsoft Research-Video
to Text (MSR-VTT) [22].

58331

MSVD is an open-domain dataset for video captioning
that covers various topics including sports, animals and mu-
sic.
It contains 1,970 video clips from Youtube and col-
lects multi-lingual descriptions by Amzon Mechanical Turk
(AMT). There are totally about 8,000 English descriptions
with roughly 40 descriptions per video. Following [23, 16],
we only consider the English descriptions, taking 1,200,
100, 670 clips for training, validation, testing.

MSR-VTT is a large-scale benchmark used in the video-
to-language challenge1. It contains 10,000 video clips with
200,000 clip-sentence pairs in total, and covers comprehen-
sive video categories, diverse video content as well as lan-
guage descriptions. There are totally 20 categories, such
as music, sports, movie, etc. The descriptions are also col-
lected by AMT, and each video clip is annotated with 20
natural language sentences. Following the splits in [22],
there are 6,513 clips for training, 497 clips for validation,
and 2,990 clips for testing.

4.1.2 Evaluation Metrics.

For quantitative evaluation of our proposed approach,
we adopt the following common metrics in our experi-
ments: BLEU@4 [24], METEOR [25], and CIDEr [26].
BLEU@4 measures the fraction of n-grams (here n = 4)
between generated sentence and ground-truth descriptions.
METEOR measures uni-gram precision and recall between
generated sentence and ground-truth references, extending
exact word matching to including similar words. CIDEr is
a voting-based metric, which to some extent is robust to
incorrect ground-truth descriptions. Following [11, 4, 2],
all the metrics are computed by using the Microsoft COCO
evaluation server [27].

4.2. Experimental Settings

4.2.1 Video and sentence preprocessing.

We sample 40 (T = 40) frames for each input video and
extract 5 (N = 5) objet regions for each frame empiri-
cally. We utilize Mask R-CNN [28] to detect objects, which
is based on ResNet-101 [29] and pre-trained on Microsoft
COCO dataset [30]. All the object regions are cropped into
images and fed into ResNet-200 to extract local features.
The global frames are also fed into ResNet-200 to obtain
local features. We take the output of res5c layer in ResNet-
200 as local feature map with the size of 7 × 7 × 2048.

All the reference captioning sentences are tokenized and
converted to lower case. After removing the punctuations,
we collect 12, 593 word tokens for MSVD dataset and
27, 891 word tokens for MSR-VTT dataset.

1We adopt

the dataset of 2016’s challenge,

and the corre-
sponding competition results can be found in http://ms-multimedia-
challenge.com/2016/leaderboard.

4.2.2 Training details.

For the training video/sentence pairs, we ﬁlter out the sen-
tences with more than 16 words, and adopt zero padding
strategy to complement the sentences that has less than 16
words. During training, begin-of-sentence <BOS> tag and
end-of-sentence <EOS> tag are added at the beginning and
end of each sentence. Unseen words in the vocabulary will
be set to <UNK> tags. Each word is encoded as a one-hot
vector. The hidden units of encoder and decoder are set as
512. The word embedding size and attention size are set
as 512 and 100 respectively. For the trainable VLAD mod-
els, we set the cluster center number K as 64 for MSVD
dataset and 128 for MSR-VTT dataset. The reason is that
MSR-VTT is a large-scale dataset with diverse video con-
tent, thus a larger number of cluster centers are necessary to
fully represent the video content.

During training stage, all the parameters are randomly
initialized, and we utilize Adam algorithm to optimize cap-
tioning model. The learning rate is ﬁxed to be 1 × 10−4,
and the training batch size is 16. Dropout is applied on the
output of decoder GRU with the rate of 0.5 to avoid over-
ﬁtting. We also apply gradient clip of [−10, 10] to prevent
gradient explosion. In testing stage, we adopt beam search
to generate descriptions, where beam size is set as 5.

Table 1. Comparisons with state-of-the-art methods on MSVD
dataset. All the results are reported as percentage (%).

Methods

BLEU@4 METEOR CIDEr

Our OA-BTG

SeqVLAD [6]

LSTM-GAN [23]

MS-RNN [31]

MCNN+MCF [32]

RecNet [33]
TSA-ED [34]
aLSTMs [35]

STAT [36]

MA-LSTM [16]

DMRM [2]

hLSTMat [37]

mGRU [38]
TDDF [39]

56.9

51.0
42.9
53.3
46.5
52.3
51.7
50.8
51.1
52.3
51.1
53.0
53.8
45.8

36.2

35.2
30.4
33.8
33.7
34.1
34.0
33.3
32.7
33.6
33.6
33.6
34.5
33.3

90.6

86.0

-

74.8
75.5
80.3
74.9
74.8
67.5
70.4
74.8
73.8
81.2
73.0

4.3. Comparisons with State of the art Methods

Tables 1 and 2 show comparative results between OA-
BTG and the state-of-the-art methods on MSVD and MSR-
VTT datasets respectively. We can see that OA-BTG out-
performs all the compared methods on popular evaluation
metrics, which veriﬁes the effectiveness of bidirectional
temporal graph and the object-aware aggregation proposed
in our approach.

Among the compared methods, STAT [36] combines

68332

Table 2. Comparisons with state-of-the-art methods on MSR-VTT
dataset. All the results are reported as percentage (%).

Methods

BLEU@4 METEOR CIDEr

Our OA-BTG

LSTM-GAN [23]

MS-RNN [31]

MCNN+MCF [32]

M3 [12]

RecNet [33]
aLSTMs [35]

STAT [36]

MA-LSTM [16]
hLSTMat [37]

TDDF [39]

41.4

36.0
39.8
38.1
38.1
39.1
38.0
37.4
36.5
38.3
37.3

28.2

26.1
26.1
27.2
26.6
26.6
26.1
26.6
26.5
26.3
27.8

46.9

-

40.9
42.1

-

42.7
43.2
41.5
41.0

-

43.8

spatial and temporal attention, where the spatial attention
selects relevant objects while temporal attention selects im-
portant frames. hLSTMat [37] utilizes an adjusted temporal
attention mechanism to distinguish visual words and non-
visual words during sentence generation. LSTM-GAN [23]
introduces adversarial learning for video captioning. Dif-
ferent from them, our OA-BTG approach focus on captur-
ing detailed temporal trajectories for objects in video, as
well as learning discriminative visual representations. OA-
BTG constructs bidirectional temporal graph and performs
object-aware feature aggregation to achieve above goals,
which helps to generate accurate and ﬁne-grained captions
for better performance.

TSA-ED [34] also utilizes trajectory information, which
introduces a trajectory structured attentional encoder-
decoder network which explores the ﬁne-grained motion in-
formation. Although it extracts dense point trajectories, it
loss the object-aware information. While the trajectory ex-
traction in our OA-BTG approach is applied on the object
regions, thus captures the object semantics and its tempo-
ral dynamics, which play a key role for generating accurate
sentence and improve the video captioning performance.

Similarly, the recent work SeqVLAD [6] also incorpo-
rates a trainable VLAD process into the sequence learn-
ing framework to mine ﬁne motion information in succes-
sive video frames. Our OA-BTG approach obtains higher
performance for the following two reasons: (1) OA-BTG
applies aggregation process on object regions, which can
capture the object-aware semantic information.
(2) OA-
BTG also constructs bidirectional temporal graph to extract
the temporal trajectories for each object instance, which
captures the detailed temporal dynamics in video content.
Thus, OA-BTG achieves better captioning performance.

4.4. Ablation Study

In this subsection, we study in detail about the impact
of each component of our proposed OA-BTG. The corre-
sponding results are shown in Table 3. The baseline method

(denoted as BASELINE) only applies a learnable VLAD
model on global frame sequences. The methods in the sec-
ond row refer to methods with object-aware aggregation
with single-directional temporal graph constructed along or
reversely along the temporal order. It can be observed that
both methods with object-aware aggregation outperform the
baseline in popular metrics. For example, “OA with Back-
ward TG” improves the performance on BLEU@4, ME-
TEOR, CIDEr scores by 0.6%, 1.3%, 1.7% respectively
on MSVD dataset, and 1.2%, 1.0%, 3.0% respectively on
MSR-VTT dataset. These results verify the effectiveness of
object-aware aggregation in our proposed approach.

Comparing OA-BTG with single-directional baseline
(OA + Forward/Backward TG), it can be observed that OA-
BTG achieves better performance. Taking MSVD dataset
for example, OA-BTG obtains the average improvements
of 3.25%, 1.15%, 5.45% on BLEU@4, METEOR, CIDEr
scores, respectively, which indicates the effectiveness of
bidirectional temporal graph. Similarly, improvements can
also be found on MSR-VTT dataset.

Finally, OA-BTG facilitates the baseline method with
both innovations of object-aware aggregation and bidirec-
tional temporal graph, and the comparison between OA-
BTG and the baseline method deﬁnitely veriﬁes the overall
effectiveness of our proposed approach.

4.5. Qualitative Analysis

Figures 3 and 4 present some successful and failure cases
of the generated descriptions by our OA-BTG. From ﬁg-
ure 3, it can be seen that our approach indeed improves
the video captioning by capturing objects and their detailed
temporal information. For instance, the example in top-left
demonstrates that our approach can generate accurate de-
piction of actions by modeling the temporal trajectories of
each object. The example in top-right shows that our ap-
proach not only expresses the correct semantics, but also is
capable of capturing detailed actions so as to generate ﬁne-
grained description (“cutting a piece of bread”) rather than
a general one (“cooking”). Overall, all these comparisons
verify the effectiveness of our proposed method. Figure 4
shows two failure cases, where our OA-BTG approach fails
to describe “on a couch” and “chase”. It needs to not only
model salient objects with their trajectories, but also under-
stand interaction relationships among objects, which is very
challenging. However, our approach still successfully de-
scribes “playing with a dog”, “a group of people”, “playing
soccer” by modeling object-aware temporal information.

5. Conclusion

In this paper, we have proposed a novel video caption-
ing approach based on object-aware aggregation with bidi-
rectional temporal graph (OA-BTG), which captures the de-
tailed temporal dynamics on salient object instances, as well

78333

Table 3. Effectiveness of different components in our OA-BTG approach on MSVD and MSR-VTT datasets. All the results are reported
as percentage (%).

Methods

BASELINE

Our OA with Forward TG
Our OA with Backward TG

Our OA-BTG

MSVD

MSR-VTT

BLEU@4 METEOR CIDEr BLEU@4 METEOR CIDEr

52.7
54.0
53.3
56.9

34.1
34.7
35.4
36.2

83.7
84.9
85.4
90.6

39.6
40.8
40.8
41.4

26.3
26.9
27.3
28.2

42.3
45.1
45.3
46.9

GroundTruth: a man and woman are riding on a motorcycle
Baseline: a man and a woman are talking on the phone
Ours: a man and woman are riding a motorcycle

GroundTruth: a man is cutting bread in half
Baseline: a man is cooking
Ours: a man is cutting a piece of bread

GroundTruth: a man is doing exercise
Baseline: a woman is walking
Ours: a man is doing exercise

GroundTruth: a woman is styling her hair
Baseline: a woman is talking
Ours: a woman is styling her hair

Figure 3. Successful cases of the description examples generated by our OA-BTG approach. Examples of baseline method are presented
for comparison.

GroundTruth: a young girl petting a dog that is laying on a 
couch
Ours: a person is playing with a dog

GroundTruth: a group of soccer players chase the guy with 
the ball
Ours: a group of people are playing soccer 

Figure 4. Failure cases of the description examples generated by our OA-BTG approach.

as learns discriminative spatio-temporal representations for
complex video content by aggregating ﬁne local informa-
tion on object-aware regions and frames. First, a bidirec-
tional temporal graph is constructed to capture temporal
trajectories for each object instance in two complementary
directions, which exploits the detailed temporal dynamics
in video for generating accurate and ﬁne-grained captions.
Then, object-aware aggregation is performed to encoding
the ﬁne spatio-temporal information for salient objects and
global context. The integrity of our model, with contribu-
tions of the bidirectional temporal graph and object-aware
aggregation captures crucial spatial and temporal cues si-

multaneously, and thus boosting the performance.

In the future, we will explore how to construct more ef-
fective graph to model the relations among different object
instances, as well as explore their interactions between the
backward temporal sequences in an end-to-end model.

6. Acknowledgments

This work was supported by the National Natural Sci-
ence Foundation of China under Grant 61771025 and Grant
61532005.

88334

References

[1] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas,
Christopher Pal, Hugo Larochelle, and Aaron Courville. De-
scribing videos by exploiting temporal structure. In ICCV,
pages 4507–4515, 2015.

[2] Ziwei Yang, Yahong Han, and Zheng Wang. Catching the
In ACM

temporal regions-of-interest for video captioning.
MM, pages 146–153, 2017.

[3] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hi-
erarchical boundary-aware neural encoder for video caption-
ing. In CVPR, pages 3185–3194, 2017.

[4] Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yueting
Zhuang. Hierarchical recurrent neural encoder for video rep-
resentation with application to captioning. In CVPR, pages
1029–1038, 2016.

[5] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. Netvlad: Cnn architecture for weakly
supervised place recognition. In CVPR, pages 5297–5307,
2016.

[6] Youjiang Xu, Yahong Han, Richang Hong, and Qi Tian. Se-
quential video vlad: Training the aggregation locally and
temporally. IEEE Transactions on Image Processing (TIP),
27(10):4933–4944, 2018.

[7] Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkar-
nenkar, Subhashini Venugopalan, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Youtube2text: Recognizing and
describing arbitrary activities using semantic hierarchies and
zero-shot recognition. In ICCV, pages 2712–2719, 2013.

[8] Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Man-
fred Pinkal, and Bernt Schiele. Translating video content
to natural language descriptions. In ICCV, pages 433–440,
2013.

[9] Niveda Krishnamoorthy, Girish Malkarnenkar, Raymond J
Mooney, Kate Saenko, and Sergio Guadarrama. Generating
natural-language video descriptions using text-mined knowl-
edge. In AAAI, pages 541–547, 2013.

[10] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. In ICLR, pages 1–15, 2015.

[11] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Don-
ahue, Raymond Mooney, Trevor Darrell, and Kate Saenko.
Sequence to sequence-video to text. In ICCV, pages 4534–
4542, 2015.

[12] Junbo Wang, Wei Wang, Yan Huang, Liang Wang, and Tie-
niu Tan. M3: Multimodal memory modelling for video cap-
tioning. In CVPR, pages 7512–7520, 2018.

[13] Junchao Zhang and Yuxin Peng.

language alignment for video captioning.
42–54, 2019.

Hierarchical vision-
In MMM, pages

[15] Shizhe Chen, Jia Chen, Qin Jin, and Alexander Hauptmann.
Video captioning with guidance of multimodal latent topics.
In ACM MM, pages 1838–1846, 2017.

[16] Jun Xu, Ting Yao, Yongdong Zhang, and Tao Mei. Learning
multimodal attention lstm networks for video captioning. In
ACM MM, pages 537–545, 2017.

[17] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang,
Bret Harsham, John R Hershey, Tim K Marks, and Kazuhiko
Sumi. Attention-based multimodal fusion for video descrip-
tion. In ICCV, pages 4203–4212, 2017.

[18] Xin Wang, Wang Yuan-Fang, and William Yang Wang.
Watch, listen, and describe: Globally and locally aligned
cross-modal attentions for video captioning. In ACL, pages
795–801, 2018.

[19] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torre-
sani, and Manohar Paluri. Learning spatiotemporal features
with 3d convolutional networks. In ICCV, pages 4489–4497,
2015.

[20] Zhongwen Xu, Yi Yang, Ivor Tsang, Nicu Sebe, and Alexan-
der G Hauptmann. Feature weighting via optimal threshold-
ing for video analysis. In ICCV, pages 3440–3447, 2013.

[21] David L Chen and William B Dolan. Collecting highly par-
allel data for paraphrase evaluation. In ACL, pages 190–200,
2011.

[22] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
CVPR, pages 5288–5296, 2016.

[23] Yang Yang, Jie Zhou, Jiangbo Ai, Yi Bin, Alan Hanjalic,
Heng Tao Shen, and Yanli Ji. Video captioning by adver-
sarial lstm. IEEE Transactions on Image Processing (TIP),
27(11):5600–5611, 2018.

[24] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311–318, 2002.

[25] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
metric for mt evaluation with improved correlation with hu-
man judgments. In Proceedings of the acl workshop on in-
trinsic and extrinsic evaluation measures for machine trans-
lation and/or summarization, pages 65–72, 2005.

[26] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evalua-
tion. In CVPR, pages 4566–4575, 2015.

[27] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-
tam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence Zitnick.
Microsoft coco captions: Data collection and evaluation
server. arXiv preprint arXiv:1504.00325, 2015.

[28] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In ICCV, pages 2980–2988. IEEE, 2017.

[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[14] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Mar-
cus Rohrbach, Raymond Mooney, and Kate Saenko. Trans-
lating videos to natural language using deep recurrent neural
networks. In ACL, pages 1494–1504, 2015.

[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, pages 740–755. Springer, 2014.

98335

[31] Jingkuan Song, Yuyu Guo, Lianli Gao, Xuelong Li, Alan
Hanjalic, and Heng Tao Shen. From deterministic to gen-
erative: Multimodal stochastic rnns for video captioning.
IEEE transactions on neural networks and learning systems
(TNNLS), 2018.

[32] Aming Wu and Yahong Han. Multi-modal circulant fusion
for video-to-language and backward. In IJCAI, pages 1029–
1035, 2018.

[33] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruc-
tion network for video captioning. In CVPR, pages 7622–
7631, 2018.

[34] Xian Wu, Guanbin Li, Qingxing Cao, Qingge Ji, and Liang
Lin. Interpretable video captioning via trajectory structured
localization. In CVPR, pages 6829–6837, 2018.

[35] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and
Heng Tao Shen. Video captioning with attention-based lstm
and semantic consistency. IEEE Transactions on Multimedia
(TMM), 19(9):2045–2055, 2017.

[36] Yunbin Tu, Xishan Zhang, Bingtao Liu, and Chenggang Yan.
Video description with spatial-temporal attention. In ACM
MM, pages 1014–1022. ACM, 2017.

[37] Jingkuan Song, Lianli Gao, Zhao Guo, Wu Liu, Dongxiang
Zhang, and Heng Tao Shen. Hierarchical lstm with adjusted
temporal attention for video captioning.
In IJCAI, pages
2737–2743, 2017.

[38] Linchao Zhu, Zhongwen Xu, and Yi Yang. Bidirectional
multirate reconstruction for temporal modeling in videos. In
CVPR, pages 2653–2662, 2017.

[39] Xishan Zhang, Ke Gao, Yongdong Zhang, Dongming Zhang,
Jintao Li, and Qi Tian. Task-driven dynamic fusion: Reduc-
ing ambiguity in video description. In CVPR, pages 6250–
6258, 2017.

108336

