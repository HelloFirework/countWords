Towards Visual Feature Translation

Jie Hu1, Rongrong Ji12 ∗, Hong Liu1, Shengchuan Zhang1, Cheng Deng3, and Qi Tian4

1Fujian Key Laboratory of Sensing and Computing for Smart City, Department of Cognitive Science,

School of Information Science and Engineering, Xiamen University, Xiamen, China.

2Peng Cheng Laboratory, Shenzhen, China. 3Xidian University. 4Noah’s Ark Lab, Huawei.

{hujie.cpp,lynnliu.xmu,chdeng.xd}@gmail.com, {rrji,zsc 2016}@xmu.edu.cn, tian.qi1@huawei.com

Abstract

Cross-feature Retrieval

Most existing visual search systems are deployed based
upon ﬁxed kinds of visual features, which prohibits the fea-
ture reusing across different systems or when upgrading
systems with a new type of feature. Such a setting is ob-
viously inﬂexible and time/memory consuming, which is in-
deed mendable if visual features can be “translated” across
systems. In this paper, we make the ﬁrst attempt towards vi-
sual feature translation to break through the barrier of us-
ing features across different visual search systems. To this
end, we propose a Hybrid Auto-Encoder (HAE) to translate
visual features, which learns a mapping by minimizing the
translation and reconstruction errors. Based upon HAE, an
Undirected Afﬁnity Measurement (UAM) is further designed
to quantify the afﬁnity among different types of visual fea-
tures. Extensive experiments have been conducted on sev-
eral public datasets with sixteen different types of widely-
used features in visual search systems. Quantitative results
show the encouraging possibilities of feature translation.
For the ﬁrst time, the afﬁnity among widely-used features
like SIFT and DELF is reported.

1. Introduction

Visual features serve as the basis for most existing vi-
sual search systems.
In a typical setting, a visual search
system can only handle pre-deﬁned features extracted from
the image set ofﬂine. Such a setting prohibits the reusing
of a certain kind of visual feature across different systems.
Moreover, when upgrading a visual search system, a time-
consuming step is needed to extract new features and to
build the corresponding indexing, while the previous fea-
tures and indexing are simply discarded. Breaking through
such a setting, if possible, is by any means very beneﬁcial.
For instance, the existing features and indexing can be ef-
ﬁciently reused when updating old features with new ones,

∗Corresponding author.

r
o
t
a
l
s
n
a
r
T

 

•
•
•

 

 

•
•
•

 

Inverted Index

Feature A Feature B

Feature AB

Extractor

Expensive

Efficient

Translator

Merger of Retrieval Systems

Figure 1. Two potential applications of visual feature translation.
Top: In cross-feature retrieval, Feature A is translated to Feature
AB, which can be used to search images that are represented and
indexed by Feature B. Bottom: In the merger of retrieval systems,
Feature A used in System A is efﬁciently translated to Feature AB,
instead of the expensive process of re-extracting entire dataset in
System A with Feature B.

which can signiﬁcantly save the time and memory cost. For
another instance, images can be efﬁciently archived with
only respective features for cross-system retrieval. These
examples are detailedly depicted in Fig. 1.

However, feature reusing is not an easy task. Various di-
mensions and diverse distributions of different types of fea-
tures prohibit reusing features directly. Therefore, a feature
“translator” is needed to transform across different types of
features, which, to our best knowledge, remains untouched
in the literature. Intuitively, given a set of images extracted
with different types of features, one can leverage the feature
pairs to learn the corresponding feature translator.

In this paper, we make the ﬁrst attempt to investigate vi-

3004

Stage I: Feature Extraction

Stage II: Feature Translation

Stage III: Relation Mining

Compact Vector

Post Process

Encoding

Polling

Global Feature

Handcrafted

Learning-based

Local Descriptors

Translation

Reconstruction

Decoder $

Visualization

Minimum Spanning Tree

Undirected Graph

A

B

!"

Hybrid

!#

Undirected Affinity Measurement

Encoder %"

Encoder %#

Directed Graph

A

B

Directed Affinity Measurement

A

B

Input Image

Source Features

Target Features

Feature Translation

Figure 2. The overall ﬂowchart of the proposed visual feature translation. In Stage I, different handcrafted or learning-based features are
extracted from image set for training. In Stage II, the mappings from source features to target features are learned by our HAE with the
encoders Es, Et and the decoder D. Then the encoder Es and the decoder D are used in inference. In Stage III, the UAM is calculated to
quantify the afﬁnity among different types of visual features, which is further visualized by employing the Minimum Spanning Tree.

sual feature translation. Concretely, we propose a Hybrid
Auto-Encoder (HAE) that learns a mapping from source
features to target features by minimizing the translation and
reconstruction errors. HAE consists of two encoders and
one decoder. In training, the source and target features are
encoded into a latent space by corresponding encoders. Fea-
tures in this latent space are sent to a shared decoder to
produce the translated features and reconstructed features.
Then the reconstruction and translation errors are mini-
mized by optimizing the objective function. In inference,
the encoder of source features and the shared decoder are
used for translation. The proposed HAE further provides
a way to characterize the afﬁnity among different types of
visual features. Based upon HAE, an Undirected Afﬁnity
Measurement (UAM) is further proposed, which provides,
also for the ﬁrst time, a quantiﬁcation of the afﬁnity among
different types of visual features. We also discover that
UAM can predict the translation quality before the actual
translation happens.

We train HAE on the Google-Landmarks dataset [16]
and evaluate in total 16 different types of widely-used fea-
tures in visual search community [2, 4, 19, 21, 29, 36, 41,
44, 52]. The tests of feature translation are conducted on
three benchmark datasets, i.e., Oxford5k [40], Paris6k [37],
and Holidays [18]. Quantitative results show the encour-
aging possibility for feature translation. In particular, HAE

works relatively well for feature pairs such as V-CroW to V-
SPoC (e.g., 0.1 mAP decrease on the Oxford5k benchmark)
and R-rMAC to R-CroW (e.g., 1.8 mAP decrease on the
Holidays benchmark). Interestingly, visual feature transla-
tion provides some intriguing results (see Fig. 4 later in our
experiments). For example, when translating from SIFT to
DELF, characteristics like rotation or viewpoint invariance
can be highlighted, which provides a new way to absorb
merits of handcrafted features to learning-based ones.

In short, our contributions can be summarized as below:
• We are the ﬁrst to address the problem of visual feature
translation, which ﬁlls in the gaps between different
types of features.

• We are the ﬁrst to quantify the afﬁnity among different
types of visual features in retrieval, which can be used
to predict the quality of feature translation.

• The proposed scheme innovates in several detailed de-
signs, such as the HAE for training the translator and
the UAM for quantifying the afﬁnity. The source code
and meta-data are released online1.

The rest of this paper is organized as follows. Section
2 reviews the related work. The proposed feature transla-
tion and feature relation mining algorithms are introduced
in Section 3. Quantitative experiments are given in Section
4. Finally, we conclude this work in Section 5.

1https://github.com/hujiecpp/VisualFeatureTranslation

3005

2. Related Work

Visual Feature. Early endeavors mainly include holis-
tic features (e.g., color histogram [15] and shape [7]) and
handcrafted local descriptors [6, 20, 30, 31, 33, 39, 47, 49],
such as SIFT [29] and ORB [45]. Then, different aggre-
gation schemes (e.g., Fisher Vector [36] and VLAD [19])
are proposed to encode local descriptors. Along with the
proliferation of neural networks, deep visual features have
dominated visual search [1, 4, 5, 12, 16, 21, 32, 41, 43, 52],
for instance, the local feature DELF [16] and the global fea-
ture produced by GeM [41] pooling are both prominent for
representing images. Detailed surveys of visual features can
be found in [50, 55].

Transfer Learning. Transfer learning [35, 51] aims to
improve the learning of the target task using the knowledge
in source domain. It can be subdivided into: instance trans-
fer, feature transfer, parameter transfer, and relation trans-
fer. Our work relates to, but is not identical with, the feature
transfer. Feature transfer [3, 9, 11, 13, 24, 27, 34, 42, 53]
is usually based on the hypothesis that the source domain
and target domain have some shared characteristics. It aims
to ﬁnd a common feature space for both source and target
domains, which serves as a new representation to improve
the learning of the target task. For instance, the Structural
Corresponding Learning [8] uses pivot features to learn a
mapping from features of both domains to a shared fea-
ture space. For another instance, Joint Geometrical and Sta-
tistical Alignment [54] learns two coupled projections that
project features of both domains into subspaces where the
geometrical and distribution shifts are reduced. More re-
cently, deep learning has been introduced into feature trans-
fer [25, 26, 28, 46], in which neural networks are used to
ﬁnd the common feature spaces. In contrast, the visual fea-
ture translation aims to learn a mapping to translate features
from the source space to the target space, and the translated
features are used directly in the target space.

3. Visual Feature Translation

Fig. 2 shows the overall ﬂowchart of the proposed visual
feature translation. Firstly, source and target feature pairs
are extracted from image set for training in Stage I. Then,
feature translation based on HAE is learned in Stage II. Af-
ter translation, the afﬁnity among different types of features
is quantiﬁed and visualized in Stage III.

3.1. Preprocessing

As shown in Stage I of Fig. 2, we prepare the source and
target features for training the subsequent translator. For the
handcrafted features such as SIFT [29], the local descriptors
are extracted by the designed procedures ﬁrstly. These lo-
cal descriptors are then aggregated by encoding schemes to
produce the global features. For the learning-based features

Algorithm 1 The Training of HAE
Input: Feature sets Vs and Vt, decoders Es, Et and en-
coder D parameterized by θEs , θEt and θD.
Output: The learned translator Es and D.
1: while not convergence do
2:

3:

4:

5:

Get Zs by Zs = Es(Vs).
Get Zt by Zt = Et(Vt).
Get Vst by translation: Vst = D(Zs).
Get Vtt by reconstruction: Vtt = D(Zt).
Optimize the Eq. 1.

6:
7: end while
8: return Es and D.

such as V-MAC [44, 52], the feature maps are extracted by
neural networks ﬁrstly, followed by a pooling layer or en-
coding schemes to produce the feature vectors. In our set-
tings, we investigate in total 16 different types of features, a
detailed table of which can be found in Table 1. The feature
sets are arranged to form 16 × 16 feature set pairs (Vs, Vt),
where Vs denotes the set of source features and Vt denotes
the set of target features. The implementation is detailed in
Section 4.1.

3.2. Learning to Translate

To achieve the task of translating different types of fea-
tures, a Hybrid Auto-Encoder (HAE) is proposed, which is
shown in Stage II of Fig. 2. For training HAE, the source
features Vs and the target features Vt are input to the model
which outputs the translated features Vst and the recon-
structed features Vtt.

Formally, HAE consists of two encoders Es, Et and one
decoder D. In training, vs ∈ Vs is encoded into the latent
feature zs ∈ Zs by the encoder Es, and the same for vt ∈ Vt
into zt ∈ Zt by Et. The latent features zs and zt are then
decoded to obtain the translated feature vst ∈ Vst and the
reconstructed feature vtt ∈ Vtt by the shared decoder D.
We deﬁne the Euclidean distance as E(x, y) = kx − yk2.
The Es, Et and D are parameterized by θEs , θEt and θD,
which is learned by minimizing the following loss function:

L(θEs , θEt , θD) =Evst∈Vst,vt∈Vt [E(vst, vt)]
+Evtt∈Vtt,vt∈Vt [E(vtt, vt)],

(1)

where we deﬁne the ﬁrst item as the translation error and
the second item as the reconstruction error.

In the processing of the feature translation, only Es and
D are used to translate features from Vs to Vt. The algo-
rithm for training the HAE is summarized as Alg. 1.

We then get the following characteristics for our visual

feature translation:
Characteristic I: Saturation. The performance of trans-
lated features is difﬁcult to exceed that of the target features.

3006

This phenomenon is inherent in the feature translation pro-
cess. According to Eq. 1, the translation and reconstruc-
tion errors are minimized after optimizing. However, they
are difﬁcult to approach zero due to the information loss
brought by the architecture of Auto-Encoder.
Characteristic II: Asymmetry. The convertibility of trans-
lation is discrepancy between A2B and B2A (We abbrevi-
ate A2B for the translation from features A to features B,
etc.). The networks for translating different types of fea-
tures are by nature asymmetry. HAE relies on the transla-
tion error and reconstruction error, which is not the same
between A2B and B2A.
Characteristic III: Homology.
In general, homologous
features tend to have high convertibility.
In contrast, the
convertibility is not guaranteed for heterogenous features.
Homologous features refer to the features extracted by the
same extractor but encoded or pooled by different methods
(e.g., DELF-FV [16, 36] and DELF-VLAD [16, 19], or V-
CroW [21] and V-SPoC [4]), and the heterogenous features
refer to the features extracted by different extractor. This
characteristic is analyzed in details in Section 4.2.

3.3. Feature Relation Mining

HAE provides a way to quantify the afﬁnity between fea-
ture pairs. Therefore, the afﬁnity among different types of
features can be quantiﬁed as the Stage III shown in Fig. 2.
First, we use the difference between translation and recon-
struction errors as a Directed Afﬁnity Measurement (DAM)
and calculate the directed afﬁnity matrix M which forms
a directed graph for all feature pairs. Second, in order
to quantify the total afﬁnity among features, we design
an Undirected Afﬁnity Measurement (UAM) by employ-
ing M . The calculated undirected afﬁnity matrix U is sym-
metry, which forms a complete graph. Third, we visualize
the local similarity between features by using the Minimum
Spanning Tree (MST) of the complete graph.

Directed Afﬁnity Measurement. We assume that af-
ter optimizing, for Eq. 1, the reconstruction error is smaller
than the translation error. This intuitive assumption is veri-
ﬁed later in Section 4.3. Then, we can ﬁnd that:

L ≥Evst∈Vst,vt∈Vt [E(vst, vt)]

−Evtt∈Vtt,vt∈Vt [E(vtt, vt)] ≥ 0.

(2)

According to this inequation, when minimizing L, the trans-
lation error is forced to approximate the reconstruction er-
ror. If translation error is close to reconstruction error, we
think the translation between source and target features is
similar to the reconstruction of target features, which in-
dicates the source and target features have high afﬁnity.
Therefore, we regard the difference between the translation
and reconstruction errors as the afﬁnity measurement. We
use Ms→t to represent the DAM between Vs and Vt. The

Algorithm 2 Afﬁnity Calculation and Visualization

Input: The number of different types of features n, the
feature pairs (Vs, Vt) and the translator Es, D.
Output: The directed afﬁnity matrix M and the undirected
afﬁnity matrix U .

Calculate Mi→j by Eq. 3.

Calculate Ri→j and Ci→j by Eq. 4 and Eq. 5.

1: for i = 1 : n, j = 1 : n do
2:
3: end for
4: for i = 1 : n, j = 1 : n do
5:
6: end for
7: Calculate U by Eq. 6.
8: Generate the MST based on U by Kruskal’s algorithm.
9: Visualize the MST.
10: return M, U .

calculation of the element at row s and column t of M is
deﬁned as follows:

Ms→t =Evst∈Vst,vt∈Vt [E(vst, vt)]
−Evtt∈Vtt,vt∈Vt [E(vtt, vt)].

(3)

Undirected Afﬁnity Measurement. Due to the asym-
metry characteristic, M is asymmetric, which is unsuit-
able to be the total afﬁnity measurement of feature pairs.
We then resort to designing an Undirected Afﬁnity Mea-
surement (UAM) to quantify the overall afﬁnity among dif-
ferent types of features. Speciﬁcally, we treat A2B and B2A
as a uniﬁed whole, therefore the rows and columns of M are
considered consistently. For the rows of M , the element at
row i and column j of the matrix R with normalized rows
is deﬁned as:

Ri→j =

Mi→j − min(Mi→:)

max(Mi→:) − min(Mi→:)

,

(4)

where min(Mi→:) and max(Mi→:) are the minimum and
maximum of the row i, and Ri→j is normalized to [0, 1].

In a similar way, for the columns of M , the element
at row i and column j of the matrix C with normalized
columns is deﬁned as:

Ci→j =

Mi→j − min(M:→j)

max(M:→j) − min(M:→j)

,

(5)

where min(M:→j) and max(M:→j) are the minimum and
maximum of the column j, and Ci→j is normalized to [0, 1].

The undirected afﬁnity matrix U is deﬁned as follows:

U =

1
4

(R + RT + C + C T ).

(6)

If Uij has a small value, feature i and feature j are similar,
and vice versa.

3007

⑤④③②

②

⑤④③

Query

⑨⑧⑦⑥

②

⑤④③

0.14

V-GeM

V-rGeM

⑨⑧⑦⑥

0.68

R-GeM

0.10

R-rGeM

0.47

R-MAC

0.13

⑨⑧⑦⑥

DELF-VLAD

③②

④

⑤

0.28

⑨⑧⑦⑥

V-CroW

R-rMAC

0.65

DELF-FV

②

⑤④③

V-rMAC

0.38

0.06

0.65

0.13

R-CroW

0.25

V-SPoC

0.01

0.87

⑥

⑨⑧⑦

V-MAC

R-SPoC

SIFT-VLAD

0.37

SIFT-FV

⑤④③②

⑥

⑨⑧⑦

⑤④③②

⑤④③②

⑨⑧⑦⑥

⑨⑧⑦⑥

Figure 3. The visualization of the MST based on U with popular visual search features. The length of edges is the average value of
the results on Holidays, Oxford5k and Paris6k datasets. The images are the retrieval results for a query image of the Pantheon with
corresponding features in the main trunk of the MST. The close feature pairs such as R-SPoC and R-CroW have similar ranking lists.

The Visualization. We use the Minimum Spanning Tree
(MST) to visualize the relationship of features based on U .
The Kruskal’s algorithm [23] is used to ﬁnd MST. This al-
gorithm ﬁrstly creates a forest G, where each vertex is a
separate tree. Then the edge with minimum weight that con-
nects two different trees is recurrently added to the forest G,
which combines two trees into a single tree. The ﬁnal out-
put forms an MST for the complete graph. The MST helps
us to understand the most related feature pairs (connected
by an edge), as well as their afﬁnity score (the length of
the edge). The overall procedure is summarized as Alg. 2.
The visualization result of the afﬁnity among popular visual
features with a query example can be found in Fig. 3.

4. Experiments

We show the experiments in this section. First, we in-
troduce the experimental settings. Then, the translation per-
formance of our HAE is reported. Finally, we visualize and
analyze the results of relation mining.

4.1. Experimental Settings

Training Dataset. The Google-Landmarks dataset [16]
contains more than 1M images captured at various land-
marks all over the world. We randomly pick 40,000 images
from this dataset to train HAE, and pick 4,000 other images
to train PCA whitening [4, 17] and creating the codebooks
for local descriptors.

Test Dataset. We use the Holidays, Oxford5k and
Paris6k datasets for testing. The Holidays dataset [18] has
1,491 images with various scene types and 500 query im-
ages. The Oxford5k dataset [37] consists of 5,062 images
which have been manually annotated to generate a compre-
hensive ground truth for 55 query images. Similarly, the
Paris6k dataset [38] consists of 6,412 images with 55 query
images. Since the scalability of retrieval algorithms is not
our main concern, we do not use the disturbance dataset
Flickr100k [38]. Recently, the work in [40] revisited the la-
bels and queries on both Oxford5k and Paris6k. Because the
images remained the same, which does not affect the char-
acteristics of features, we do not use the revisited datasets
as our test datasets. The mean average precision (mAP) is
used to evaluate the retrieval performance. We translate the
source features of reference images to the target space, and
the target features of query images are used for testing.

Features. L1 normalization and square root [2] are
applied to SIFT [29]. The original extraction approach
(at most 1,000 local representations per image) is applied
to DELF [16]. The codebooks of FV [36] and VLAD
[19] are created for SIFT and DELF. We use 32 com-
ponents of Gaussian Mixture Model (GMM) to form the
codebooks of FV and the dimension of this feature is re-
duced to 2,048 by PCA whitening. The aggregated features
are termed as SIFT-FV and DELF-FV. We use 64 central
points to form the codebooks of VLAD and the dimension

3008

Holidays Oxford5k

Paris6k

DELF-FV [16, 36]

DELF-VLAD [16, 19]

R-CroW [21]
R-GeM [41]

R-MAC [44, 52]

R-rGeM [41]
R-rMAC [52]
R-SPoC [4]
V-CroW [21]
V-GeM [41]

V-MAC [44, 52]

V-rGeM [41]
V-rMAC [52]
V-SPoC [4]

SIFT-FV [2, 29, 36]

SIFT-VLAD [2, 29, 19]

83.42
84.61
86.38
89.08
88.53
89.32
89.08
86.57
83.17
84.57
74.18
85.06
83.50
83.38
61.77
63.92

73.38
75.31
61.73
84.47
60.82
84.60
68.46
62.36
68.38
82.71
60.97
82.30
70.84
66.43
36.25
40.49

83.06
82.54
75.46
91.87
77.74
91.90
83.00
76.75
79.79
86.85
72.65
87.33
83.54
78.47
36.91
41.49

Table 1. The mAP (%) of target features.

of this feature is also reduced to 2,048 by PCA whiten-
ing. The aggregated features are termed as SIFT-VLAD
and DELF-VLAD. For off-the-shelf deep features, we use
ImageNet [10] pre-trained VGG-16 (abbreviated as V) [48]
and ResNet101 (abbreviated as R) [14] to produce the fea-
ture maps. The max-pooling (MAC) [44, 52], average-
pooling (SPoC) [4], weighted sum-pooling (CroW) [21],
and regional max-pooling (rMAC) [52] are then used to
pool the feature maps. The extracted features are termed
as V-MAC, V-SPoC, V-CroW, V-rMAC, R-MAC, R-SPoC,
R-CroW and R-rMAC, respectively. For ﬁne-tuned deep
features, we consider the generalized mean-pooling (GeM)
and regional generalized mean-pooling (rGeM) [41]. The
extracted features are termed as V-GeM, V-rGeM, R-GeM
and R-rGeM, respectively.

Network Architecture. The task-speciﬁc network ar-
chitectures of HAE have a ﬁxed latent feature space of 510
dimension. The parameter settings of encoder which con-
sists of fully-connect layers with ReLU-based activation
function are 2048-2048-2048-510 or 512-512-510 for en-
coding the features with 2048 or 512 dimension. The pa-
rameter settings of the decoder are in reverse of that of en-
coder, depending on the dimension of the output features.
The output features are L2 normalized. We use Multi-Layer
Perceptron (MLP) as our baseline, whose architecture are
2048-2048-2048 or 512-512-512 for encoding the features
with 2048 or 512 dimension, and the encoders are in re-
verse. We used Adam [22] optimizer to minimize the ob-
jective function for all feature pairs, where the learning rate
is set as 0.00001.

4.2. Translation Results

Quantitative Evaluation. The performance of target
features is shown in Table 1. We use the mAP difference
between target and translated features to show the transla-
tion results. As shown in Table 2, we use a color map which

is normalized according to the minimum (white) and maxi-
mum (colored) values to show results of each dataset. From
the result, we ﬁnd although there are still few differences
between datasets, the trend of the colored values is almost
the same.

For further analyzing, the results can be divided into
three groups: high convertibility, inferior convertibility and
low convertibility. Firstly, the high convertibility results ap-
pear mostly in the translation between homologous features.
For example, when translating from V-CroW to V-SPoC,
the mAPs drop 3.8, 0.1, 0.3 on the Holidays, Oxford5k
and Paris6k datasets, respectively. Secondly, the inferior
results are found between heterogenous features such as R-
based features and V-based features. For example, when
translating from R-GeM to V-GeM, the mAPs decrease 5.7,
11.3, 2.3 on the three datasets, respectively. Another exam-
ple is the translation from V-rGeM to R-rMAC, the mAPs
decrease 12.4, 7.1, 5.8 on the three datasets, respectively.
Thirdly, the low convertibility results also emerge between
heterogenous features. For example, when translating from
SIFT-FV to DELF-FV, the performance is not high. An-
other example is the translation from DELF-VLAD to R-
GeM, in which the former is extracted by Resnet50 and the
latter is extracted by Resnet101. We explain it from the
different depth of network architectures, different training
procedures and different encoding/pooling schemes.

The average mAP difference of HAE compared with
MLP on three datasets is shown in Table 3. From the re-
sults, we can see MLP has a very unstable performance. In
contrast, HAE with appropriate dimension of latent feature
performs better than MLP, due to the regularization effect
brought by the “bottleneck” architecture (which enforces
encoder to learn the most valuable information for decoder).

Qualitative Evaluation. Some cross-feature retrieval
results are shown in Fig. 4. The ﬁrst column shows a suc-
cessful translation from V-CroW to V-SPoC, the ranking
lists are almost the same. The second column shows an infe-
rior translation from R-GeM to V-GeM. Interestingly, when
querying an image of the Arc de Triomphe at night, the im-
ages of the Arc de Triomphe during the day are retrieved
by the translated features and get high ranks, which inspires
the integration of feature translation to improve cross-modal
retrieval. The most exciting result lies in the third column:
although the translation from SIFT-FV to DELF-FV suffers
a low performance, the characteristics like rotation or view-
point invariance can be highlighted by translation, which
well bridges the merits of the handcrafted features to the
learning-based features. For example, the images from the
bottom view of the Eiffel Tower and the Arc de Triomphe
get high ranks (both at Rank@4). The rotated images of
them also have high ranks (at Rank@7 and Rank@3). Then,
in the fourth column, we show these characteristics do not
symmetrically exist in the reverse translation from DELF-

3009

L F -F

E

D

V

D

L F - V

E

D

A

L

R - C ro W

R - G e M

C

A

R -r G e M

R - M

R -r M

C

A

R -S P o C

V - C ro W

V - G e M

C

A

V -r G e M

C

A

V -S P o C

S IF T-F

V

S IF T- V

V -r M

V - M

D

A

L

DELF-FV

1.7
DELF-VLAD 4.0

DELF-FV

4.8
DELF-VLAD 5.2

4.9
5.5
4.1
6.0
4.3
5.1

8.6
1.8
8.1
2.1
8.6
9.1

3.2
3.0
1.8
2.7
2.0
2.9

7.4
2.1
7.7
1.7
8.0
8.0

16.0 19.3 20.8 17.9 13.0 16.9 11.4 14.1 20.9 13.0 11.0 13.6 40.0 42.5
4.4
15.9 19.1 21.3 17.7 14.0 16.1 10.5 14.0 21.0 11.2 11.3 12.2 40.7 42.9
3.0
1.2
32.6 38.4
8.5
13.1 17.9 10.5
2.6
12.9 16.2
33.5 38.1
6.4
8.8
12.6
5.3
5.7
5.0
10.2 13.4
38.5 41.7
2.8
7.3
10.3
8.8
12.3
2.7
12.7 15.1
35.8 40.0
4.5
9.9
4.7
6.7
13.7
5.5
11.3 12.8
37.2 40.6
1.8
9.5
10.6 11.6
8.9
3.3
11.6 14.8
31.5 36.7
1.4
8.2
13.0 18.7 11.0
12.6 15.7
2.6
35.2 38.1
3.8
9.8
9.8
18.8 20.0 15.1 17.7 14.8 18.4 12.1 15.3
9.8
34.8 38.4
2.8
17.8 19.6 18.3 14.0 21.0 15.2 13.5 20.1
3.5
15.2 46.9 50.5
20.9
33.5 36.7 33.7 34.6 31.1 35.3 22.2 35.8 11.4 18.9
10.4 35.4 37.4
1.4
2.4
18.0 19.9 17.2 15.0 20.2 12.7 12.4 17.5
42.8 45.1
9.9
12.6
12.8
23.3 26.1 21.5 25.9 21.5 23.3 14.1 22.9
30.9 36.6
17.2 18.0 13.6 16.8 14.7 16.5 11.1 13.4
2.2
10.3
8.1
55.9 63.8 61.2 68.5 69.5 66.6 57.1 59.3 59.5 60.8 63.4 60.3 59.4 54.9
3.7
4.9
5.9
1.6
57.9 63.6 61.4 69.7 70.7 67.3 56.0 60.9 60.5 59.7 64.8 60.4 60.4 55.9

8.2
7.1
4.6
5.8
5.2
7.8
2.2
5.9
7.3
5.8
3.6
3.6

8.8
6.7
6.2
9.3
7.6
8.1
2.6
6.8

3.0
6.7
6.7
9.9
4.7
5.7

8.9
6.6
1.8

8.2
7.9

5.2
0.9
5.8
1.1
1.3
4.4

24.4
2.6
26.4
3.8
21.0
22.8

21.3
3.1
25.8
2.9
19.1
20.6

15.5 30.8 22.2 28.8 18.5 16.3 11.7 22.1 18.4 20.2 21.2
10.4 27.0 11.8 25.5 13.7
19.6 22.8 17.4 17.9
8.3
2.1
1.5
5.1
9.8
8.0
4.8
1.4
4.9
0.9
2.1
7.0

30.3 33.2
9.5
8.4
26.8 30.1
9.7
4.2
16.8 27.1 21.1 21.3 20.3 15.3 27.9 30.6
2.8
27.2 27.2
12.8 11.3 18.6 11.2 14.9 12.5 31.6 33.9
3.4
19.3 15.8
17.6 27.9 20.6 26.8 23.9 18.3 30.6 33.3
6.9
30.5 28.0
11.4 11.8 19.2
14.6 13.2 29.7 32.2
1.5
17.8 16.5
15.3 22.4 17.1 18.9 16.4 15.3 28.2 31.8
1.5
26.5 24.6
15.3 24.5 21.7 20.8 19.5 13.6 27.0 29.9
25.9 24.6
1.8
17.6
26.7 31.2
1.0
23.5 23.2 13.6 32.3 19.0 33.8 14.6 14.8
1.0
3.2
29.3 34.0
17.1 11.9 11.3 17.3 15.9 16.1
12.3
32.4 10.5 14.4 30.9 36.3
9.9
40.0 40.4 33.2 46.8 29.1 52.0 30.2 33.6
25.8 29.7
1.9
3.8
7.1
18.1 13.4
10.8
27.6 32.9
16.9
3.7
5.1
31.3 32.9 21.4 38.4 20.3 39.0 18.4 22.0
24.6 30.6
2.1
17.3
24.7 22.5 14.8 38.3 17.9 36.4 17.0 16.0
0.5
9.5
13.5
65.3 67.9 55.4 80.9 56.2 79.5 61.0 57.2 61.0 77.1 56.4 75.3 64.1 59.2
9.8
63.4 67.3 57.0 81.2 56.5 79.7 61.4 57.2 59.9 76.6 56.5 75.2 63.2 57.6 10.2

19.2
2.2
26.1
4.1
17.1
19.1

0.3
4.7
5.4
6.8
0.9
3.5

21.6 16.0 17.0

6.3
1.3
6.6

6.2
6.5

0.1
4.6

9.6

9.1

7.1

9.7

7.6

4.0

5.7

2.3

6.3
6.5

6.0
1.9
7.9
1.1
3.4
5.1

8.9
0.6
9.0
1.1
4.9
8.0

9.5
8.9
4.0
0.2
4.7
2.7
0.2
3.3

15.1
2.9
14.2
3.2
10.4
14.3

13.4 17.9 13.5 13.7
12.3 19.1 12.9 13.7

20.1 14.1 18.8 13.4 13.4
6.0
18.0 13.0 16.0 11.5 10.7
3.3
4.9
17.4
16.8 17.3
3.5
1.1
10.7
8.3
6.5
15.9
18.9 18.8
3.8
0.4
3.5
9.3
9.4
7.1
0.9
13.7
14.2 13.6
9.7
15.2 15.2
17.1
3.6
0.3
18.1 20.0 10.4 22.9 13.9 23.2 13.9 13.5
2.3
10.6 12.2
10.0
9.2
29.6 33.0 24.9 31.4 24.7 34.6 23.6 29.3
1.7
10.9 12.8
6.5
1.8
21.4 24.2 12.9 25.0 19.5 22.3 15.4 14.9
16.9 21.8 11.6 23.8 14.2 25.6 14.1 13.8
1.6
59.8 64.0 59.3 82.2 67.3 78.9 62.3 61.7 63.0 71.6 63.0 68.9 66.9 60.1
58.7 60.7 60.7 80.5 67.4 78.2 63.5 61.4 62.3 70.9 63.6 68.7 65.8 59.1

14.5 16.5
7.2
5.5
15.0 19.0
11.0 14.4 18.4 13.5 14.9 11.1 17.6 22.7
2.7
19.4 20.5
12.3 14.2 18.0 13.8 18.1 12.2 20.6 25.2
17.1 20.7
6.6
3.4
18.2 22.0
6.9
9.8
16.8 22.0
10.0 13.6 15.8 12.7 13.2
19.3 21.6
5.0
1.0
1.8
4.8
13.5 17.7
26.8 30.4
9.7
8.7
12.6 16.0
4.8
1.2
20.0 24.2
2.7
1.7
15.4 19.7
2.5
6.5
10.0
8.8
5.9
10.2

10.9
1.9
15.1
3.3
9.7
13.2

9.6
2.0
16.3
1.2
8.5
12.7

1.7
1.4
7.3
3.1
1.0
2.7

7.0
11.9

3.7
9.6

3.9
8.9

11.3

12.3

11.4

12.3

3.9

6.7

5.8

7.4

8.0

6.2

6.8

R-CroW
R-GeM
R-MAC
R-rGeM
R-rMAC
R-SPoC
V-CroW
V-GeM
V-MAC
V-rGeM
V-rMAC
V-SPoC
SIFT-FV

SIFT-VLAD

R-CroW
R-GeM
R-MAC
R-rGeM
R-rMAC
R-SPoC
V-CroW
V-GeM
V-MAC
V-rGeM
V-rMAC
V-SPoC
SIFT-FV

SIFT-VLAD

R-CroW
R-GeM
R-MAC
R-rGeM
R-rMAC
R-SPoC
V-CroW
V-GeM
V-MAC
V-rGeM
V-rMAC
V-SPoC
SIFT-FV

SIFT-VLAD

DELF-FV

3.7
DELF-VLAD 6.4

Table 2. The mAP(%) difference between target and translated features on three public datasets: Holidays (Green), Oxford5k (Blue) and
Paris6k (Brown) in the ﬁrst, second and third blocks, respectively.

FV to SIFT-FV. We explain it from the limited representa-
tive ability of the SIFT-FV.

4.3. Relation Mining Results

After calculating the directed afﬁnity matrix M and the
undirected afﬁnity matrix U , we average the values of the
three datasets and draw the heat maps. As shown in Fig. 5
(left), the values of directed afﬁnity matrix M verify our
assumption that the reconstruction error is smaller than the
translation error as all the values are positive. As shown in

Fig. 5 (right), the positions of light and dark colors are al-
most the same as that of the translation results in Table 2,
which indicates the UAM can be used to predict the trans-
lation quality between two given features. To study the re-
lationship between features better, we visualize the MST
based on U as Fig. 3. The images are the ranking lists for
a query image with corresponding features. Since the re-
sults of leaf nodes connected in the MST (e.g. R-CroW and
R-SPoC) are very similar, we mainly show the results of
nodes in the trunk of the MST. The closer features return

3010

Query

Query

Query

Query

Query

Query

Query

Query

t
e
g
r
a
T

d
e
t
a
l
s
n
a
r
T

t
e
g
r
a
T

d
e
t
a
l
s
n
a
r
T

(a) V-CroW to V-SPoC

(b) R-GeM to V-GeM

(c) SIFT-FV to DELF-FV 

(d) DELF-FV to SIFT-FV

Figure 4. The retrieval results for querying images of the Eiffel Tower (up) and the Arc de Triomphe (down) with the target features and
the translated features. The images are resized for better view and the interesting results are colored by red bounding boxes.

D ELF-V L A D
R-Cro W

D ELF-F V

R-Ge M

R-M A C

R-rGe M

R-rM A C

R-SPoC

V-Cro W

V-Ge M

V-M A C

V-rGe M

V-rM A C

V-SPoC

SIFT-F V

SIFT-V L A D

5.4

7.0

9.7

5.9

6.6

15.3

12.7

9.7
9.0

DELF-FV

SIFT-VLAD

0.1
0.2
3.5
3.5
1.3
2.0

4.6
9.4
4.0
3.2
1.1
4.2

11.1
1.2
16.4
2.6
11.9
12.5

3.5
58.8
2.3
69.4
0.1
3.0

83.3
1.0
65.6
1.2
25.7
82.0

15.2
0.9
84.9
0.9
10.5
15.2

1.5
DELF-VLAD 4.8

17.4 18.5 15.3 16.1
14.5 20.1 13.9 16.9

R-CroW
R-GeM
R-MAC
R-rGeM
R-rMAC
R-SPoC
V-CroW
V-GeM
V-MAC
V-rGeM
V-rMAC
V-SPoC
SIFT-FV

43.1 46.9
9.8
72.0 85.8 73.6 86.0 77.9 72.7
5.9
9.4
43.3 47.1
71.8 85.7 73.1 86.0 77.4 72.4
1.7
11.2 18.3 19.7 14.9 13.6 10.9 42.8 47.0
1.6
76.7 77.3
1.1
7.0
43.2 47.0
72.7
76.9 77.2 71.2
12.7 18.6 15.8 16.9 14.8 12.5 43.2 47.2
77.0 77.5
2.9
3.6
43.2 46.8
7.3
9.2
7.5
7.7
22.9
4.0
77.7 76.8
43.1 47.0
9.9
15.7 14.1 12.3 10.0
9.4
0.2
0.6
76.1 76.7
42.9 46.9
10.3 17.3 18.8 14.1 12.3
9.7
76.8 77.3
0.7
1.4
43.1 46.5
3.2
0.9
0.3
21.3 25.1 14.8 24.7 17.1 27.8 15.5 15.0
2.8
3.4
1.8
43.1 46.2
16.6 23.2 18.7 15.3 27.7 15.2 12.6 22.0
10.6 43.3 46.9
6.9
8.7
44.6 66.4 53.1 71.8 51.5 69.9 37.0 62.7
43.0 46.3
3.2
3.4
3.0
20.7 24.3 14.2 18.7 20.3 13.6 10.3 12.9
42.2 46.1
3.6
1.5
2.8
29.3 42.1 22.9 34.1 24.3 32.7 17.8 22.3
1.9
4.9
0.9
41.1 45.8
19.6 22.6 13.3 26.9 16.3 25.3 15.8 14.4
12.6
4.1
78.3 79.1 72.9 87.0 74.2 87.0 78.6 73.8 59.8 74.6 67.8 77.1 76.2 57.3
8.6
78.0 78.8 72.9 87.0 73.9 86.8 78.5 73.6 59.6 77.1 67.9 80.6 77.6 57.9
4.3
28.2 30.7
9.4
3.4
13.7 23.4 19.0 21.8 15.0 15.5 10.1 16.5 19.1 15.6 15.3
6.6
8.1
DELF-VLAD 5.2
8.9
27.5 30.7
11.7 21.4 15.4 19.7 13.1 12.2
15.3 21.0 13.8 14.3
3.5
12.2 18.2 19.1 15.1 14.5 11.7 26.0 30.6
3.4
2.4
18.9 20.2
7.4
28.2 30.8
3.1
2.6
13.4 12.5
12.0 18.1 15.8 16.9 15.5 12.6 29.9 33.4
5.4
5.2
20.7 20.6
7.4
8.0
27.6 31.0
2.2
1.1
12.8 12.9
9.9
14.2 13.5 12.3 10.5 10.6 27.9 31.5
1.9
1.0
17.4 17.7
11.1 17.1 18.7 14.9 13.5 10.5 25.1 29.5
2.3
2.7
17.9 18.5
27.1 30.3
1.6
1.4
13.3
20.2 21.1 13.0 24.3 15.9 25.1 13.5 14.5
5.6
3.9
25.9 30.0
15.2 14.6 12.3 14.2 15.0 14.2
2.6
14.2
12.9 34.9 39.1
34.3 36.7 30.6 37.6 28.3 40.6 25.3 32.9 10.0 20.0
24.6 27.7
6.4
3.3
15.7 15.4 11.0 16.3 14.3 14.0
11.6
30.2 34.1
5.6
13.2
25.4 27.7 18.6 29.7 20.4 28.2 15.9 20.0
23.6 29.0
19.6 20.8 13.4 26.3 15.6 26.2 14.1 14.4
1.4
14.2
9.5
7.3
60.3 65.2 58.7 77.2 64.3 75.0 60.1 59.4 61.2 69.8 60.9 68.2 63.5 58.1
60.0 63.9 59.7 77.1 64.9 75.1 60.3 59.8 60.9 69.1 61.6 68.1 63.1 57.5
5.9
8.6

R-CroW
R-GeM
R-MAC
R-rGeM
R-rMAC
R-SPoC
V-CroW
V-GeM
V-MAC
V-rGeM
V-rMAC
V-SPoC
SIFT-FV

14.6
2.7
15.9
2.6
12.5
14.3

12.3
1.9
23.2
1.5
12.7
12.7

16.8
2.6
16.8
3.2
14.4
16.3

10.0
0.7
17.9
1.1
10.2
10.7

5.4
1.5
5.9
2.0
3.0
4.9

6.8
2.9
6.8
2.9
3.9
6.0

1.7
4.3
6.5
6.6
2.2
4.0

4.5
5.7
9.2
5.6
2.5
5.6

SIFT-VLAD

4.6
4.0
2.1

DELF-FV

12.3

13.3

6.4

6.9

9.8

8.4

6.1

9.0

9.0

9.8

8.4

Table 3. The average mAP difference (%) of MLP (Green) and
HAE (Blue) on three datasets.

more similar ranking lists, which indicates the rationality of
our afﬁnity measurement from the other perspective.

5. Conclusion

In this work, we present the ﬁrst attempt to investigate
visual feature translation, as well as the ﬁrst attempt at
quantifying the afﬁnity among different types of features
in visual search. In particular, we propose a Hybrid Auto-
Encoder (HAE) to translate visual features. Based on HAE,
we design an Undirected Afﬁnity Measurement (UAM) to

L F - V

E

D

E

D

1.68

0.00

3.66

3.76

3.92

3.67

3.75

3.63

3.95

4.07

4.54

3.94

4.30

V
3.87

D

A
5.02

5.08

A

A

A

A

A

A

D

A

D

D

D

C

C

C

E

L

E

L

E

E

E
A

D
L

3.43

3.45

R-GeM

R -r M

V
DELF-FV

D
DELF-VLAD
L

R-rMAC
C
R-SPoC

0.0 0
0.00
0.1 9
L F -F
V
L F -F
D
V
A
L F -F
V
1.32
0.5 8
A
L F -F
D
R - C ro W
D
E
L F - V
D
A
R - C ro W
D
E
L F - V
D
A
D
R - C ro W
R-CroW
0.5 9
L F - V
A
L
R - G e M
D
R - C ro W
L F - V
L
R - C ro W
E
R - G e M
L F - V
R - G e M
R - C ro W
0.6 2
3.58
R - G e M
C
R - G e M
C
R - M
A
C
R-MAC
3.72
0.5 8
A
R - M
R -r G e M
C
R - M
R -r G e M
C
R - M
A
R -r G e M
R-rGeM
0.5 0
3.48
R - M
C
R -r G e M
R -r G e M
C
R -r M
A
0.5 7
3.55
R -r M
R -S P o C
R -r M
R -S P o C
R -r M
R -S P o C
0.6 3
R -r M
R -S P o C
V - C ro W
V - C ro W
R -S P o C
V - C ro W
0.7 0
V - C ro W
V - G e M
V - C ro W
V - G e M
V - G e M
0.7 8
3.95
V - G e M
C
V - G e M
C
V - M
A
C
0.6 8
V-MAC
4.38
V - M
C
V -r G e M
V - M
C
V -r G e M
A
V - M
V -r G e M
0.6 5
V - M
V-rGeM
V -r G e M
V -r G e M
C
V -r M
0.6 2
V -r M
V -S P o C
V -r M
V -S P o C
V -r M
V -S P o C
0.8 5
V -r M
V -S P o C
3.67
V
S IF T -F
S IF T -F
V
V -S P o C
S IF T -F
0.8 6
S IF T -F
4.92
D
D
S IF T -F
S IF T - V
A
L
S IF T - V
4.93
S IF T - V
S IF T - V
F - F
L

V-rMAC
C
A
V-SPoC

L
SIFT-VLAD

V
SIFT-FV

V-CroW

V
F - V

V -r M

V-GeM

3.92
C

3.77

4.09

L

L

C

C

C

E

L

A

A

A

A

A

A

A

A

A

V

A

D

D

L

E

D

3.83

3.83

3.93

3.78

3.06

3.84

4.71

5.13

5.89

5.01

4.87

4.59

5.28

DELF-FV

5.38

3.82

3.79

3.94

3.74

3.06

3.83

4.70

5.06

5.92

4.95

4.90

4.57

DELF-VLAD

5.33

5.33

0.00

3.09

1.55

2.79

0.54

0.04

4.21

5.05

5.73

4.73

4.48

3.95

5.23

R-CroW

5.26

0.00

0.28

0.68

0.70

0.72

0.69

0.65

0.68

0.75

0.80

0.88

0.79

0.79

0.73

0.89

0.90

0.28

0.00

0.69

0.71

0.74

0.69

0.66

0.69

0.76

0.80

0.89

0.78

0.81

0.74

0.89

0.90

0.68

0.69

0.00

0.60

0.30

0.54

0.13

0.01

0.71

0.81

0.87

0.78

0.77

0.69

0.89

0.90

3.21

0.00

2.99

0.56

2.33

3.25

4.55

4.37

5.65

4.38

4.68

4.49

5.23

R-GeM
5.30

0.70

0.71

0.60

0.00

0.58

0.11

0.52

0.60

0.75

0.72

0.85

0.73

0.78

0.74

0.89

0.90

1.64

3.00

0.00

3.05

0.61

1.74

4.41

5.11

5.51

5.01

4.54

4.34

5.25

R-MAC

5.31

0.72

0.74

0.30

0.58

0.00

0.58

0.14

0.32

0.73

0.81

0.85

0.81

0.76

0.73

0.90

0.91

2.88

0.57

3.02

0.00

2.07

2.92

4.47

4.42

5.71

4.08

4.48

4.33

0.80

2.93

0.79

2.67

0.00

0.87

4.24

4.98

5.56

4.63

4.28

4.12

5.20

R-rGeM

5.26

5.22

R-rMAC

5.31

0.69

0.69

0.54

0.11

0.58

0.00

0.47

0.54

0.74

0.73

0.86

0.69

0.76

0.72

0.88

0.89

0.65

0.66

0.13

0.52

0.14

0.47

0.00

0.14

0.67

0.77

0.82

0.72

0.70

0.65

0.87

0.88

0.04

3.10

1.61

2.80

0.58

0.00

4.20

5.05

5.75

4.71

4.49

3.93

5.15

R-SPoC

5.21

0.68

0.69

0.01

0.60

0.32

0.54

0.14

0.00

0.71

0.82

0.87

0.78

0.77

0.68

0.88

0.89

3.80

3.77

3.70

3.76

2.99

3.82

0.01

4.10

3.07

3.99

2.07

0.52

5.22

V-CroW

5.24

0.75

0.76

0.71

0.75

0.73

0.74

0.67

0.71

0.00

0.69

0.51

0.68

0.38

0.09

0.97

0.98

4.09

3.69

3.97

3.73

3.34

4.19

4.06

0.01

4.93

0.83

4.13

4.19

5.19

V-GeM
5.25

0.80

0.80

0.81

0.72

0.81

0.73

0.77

0.82

0.69

0.00

0.81

0.14

0.74

0.72

0.97

0.98

4.35

4.17

4.16

4.25

3.59

4.42

3.15

4.89

0.01

5.03

1.48

4.00

4.01

3.68

3.96

3.59

3.18

4.02

3.97

0.84

5.05

0.01

3.89

4.01

4.11

4.05

3.92

4.00

3.26

4.11

2.42

4.54

1.62

4.29

0.02

3.10

3.68

3.72

3.66

3.68

2.91

3.70

0.49

4.18

3.80

3.99

2.60

0.00

4.95

4.83

4.88

4.81

4.21

4.96

6.69

6.68

7.11

6.59

6.51

6.60

5.48

V-MAC

5.44

5.18

V-rGeM

5.22

5.32

V-rMAC

5.35

5.13

V-SPoC

5.16

0.00

SIFT-FV

3.12

0.88

0.89

0.87

0.85

0.85

0.86

0.82

0.87

0.51

0.81

0.00

0.84

0.25

0.65

1.00

1.00

0.79

0.78

0.78

0.73

0.81

0.69

0.72

0.78

0.68

0.14

0.84

0.00

0.70

0.69

0.97

0.97

0.79

0.81

0.77

0.78

0.76

0.76

0.70

0.77

0.38

0.74

0.25

0.70

0.00

0.49

0.97

0.97

0.73

0.74

0.69

0.74

0.73

0.72

0.65

0.68

0.09

0.72

0.65

0.69

0.49

0.00

0.96

0.97

0.89

0.89

0.89

0.89

0.90

0.88

0.87

0.88

0.97

0.97

1.00

0.97

0.97

0.96

0.00

0.50

4.96

4.83

4.90

4.82

4.23

4.99

6.71

6.68

7.12

6.61

6.53

6.61

SIFT-VLAD
3.06

0.00

0.90

0.90

0.90

0.90

0.91

0.89

0.88

0.89

0.98

0.98

1.00

0.97

0.97

0.97

0.50

0.00

D

A

R - C r o

W

R - G e M

R - M

A

C

R -r G e M

R -r M

C

A

C

P o

W

V - G e M

V - M

V - C r o

R - S

A

C

V -r G e M

V -r M

C

A

P o

C
S I F

V - S

D

A

L

V

T - V

T - F
S I F

L
L

E
E

D
D

F - F
F - F
L
L

E
E

D
D

V
V
F - V
F - V

D
D

A
A

L
L

R - C r o
R - C r o

W
W

R - G e M
R - G e M

R - M
R - M

A
A

C
C

R -r G e M
R -r G e M

R -r M
R -r M

C
C

A
A

C
C

P o
P o

W
W

V - G e M
V - G e M

V - M
V - M

V - C r o
V - C r o

R - S
R - S

A
A

C
C

V -r G e M
V -r G e M

V -r M
V -r M

C
C

A
A

P o
P o

C
C
S I F
S I F

V - S
V - S

D
D

A
A

L
L

V
V

T - V
T - V

T - F
T - F
S I F
S I F

Figure 5. The heat maps of the directed afﬁnity matrix M (left)
and the undirected afﬁnity matrix U (right), the values are the av-
eraged results on Holidays, Oxford5k and Paris6k datasets.

quantify the afﬁnity. Extensive experiments have been con-
ducted on several public datasets with 16 different types of
widely-used features in visual search. Quantitative results
prove the encouraging possibility of feature translation.

Acknowledgments

This work is supported by the National Key R&D Pro-
gram (No.2017YFC0113000, and No.2016YFB1001503),
the Natural Science Foundation of China (No.U1705262,
No.61772443, No.61402388 and No.61572410), the Post
Doctoral Innovative Talent Support Program under Grant
BX201600094, the China Post-Doctoral Science Founda-
tion under Grant 2017M612134, Scientiﬁc Research Project
of National Language Committee of China (Grant No.
YB135-49), and Natural Science Foundation of Fujian
Province, China (No. 2017J01125 and No. 2018J01106).

3011

References

[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. Netvlad: Cnn architecture for weakly
supervised place recognition. In CVPR, 2016.

[2] Relja Arandjelovic and Andrew Zisserman. Three things ev-
eryone should know to improve object retrieval. In CVPR,
2012.

[3] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano

Pontil. Multi-task feature learning. In NIPS, 2007.

[4] Artem Babenko and Victor Lempitsky. Aggregating local

deep features for image retrieval. In ICCV, 2015.

[5] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and
In

Victor Lempitsky. Neural codes for image retrieval.
ECCV, 2014.

[6] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:

Speeded up robust features. In ECCV, 2006.

[7] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape
matching and object recognition using shape contexts. PAMI,
2002.

[8] John Blitzer, Ryan McDonald, and Fernando Pereira. Do-
main adaptation with structural correspondence learning. In
EMNLP, 2006.

[9] Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu.
Co-clustering based classiﬁcation for out-of-domain docu-
ments. In SIGKDD, 2007.

[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009.

[11] Lixin Duan, Ivor W Tsang, and Dong Xu. Domain transfer

multiple kernel learning. PAMI, 2012.

[12] Albert Gordo, Jon Almaz´an, Jerome Revaud, and Diane Lar-
lus. Deep image retrieval: Learning global representations
for image search. In ECCV, 2016.

[13] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivara-
man Balakrishnan, Massimiliano Pontil, Kenji Fukumizu,
and Bharath K Sriperumbudur. Optimal kernel choice for
large-scale two-sample tests. In NIPS, 2012.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[15] Jing Huang, S Ravi Kumar, Mandar Mitra, Wei-Jing Zhu,
and Ramin Zabih. Image indexing using color correlograms.
In CVPR, 1997.

[16] Noh Hyeonwoo, Araujo Andre, Sim Jack, Weyand Tobias,
and Han Bohyung. Large-scale image retrieval with attentive
deep local features. In ICCV, 2017.

[17] Herv´e J´egou and Ondˇrej Chum. Negative evidences and co-
occurences in image retrieval: The beneﬁt of pca and whiten-
ing. In ECCV, 2012.

[18] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Ham-
ming embedding and weak geometric consistency for large
scale image search. In ECCV, 2008.

[19] Herve Jegou, Matthijs Douze, Cordelia Schmid, and Patrick
P´erez. Aggregating local descriptors into a compact image
representation. In CVPR, 2010.

[20] Herve Jegou, Cordelia Schmid, Hedi Harzallah, and Jakob
Verbeek. Accurate image search using the contextual dis-
similarity measure. PAMI, 2010.

[21] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.
Cross-dimensional weighting for aggregated deep convolu-
tional features. In ECCV, 2016.

[22] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2014.

[23] Joseph B Kruskal. On the shortest spanning subtree of a
graph and the traveling salesman problem. AM MATH SOC,
1956.

[24] Jingen Liu, Mubarak Shah, Benjamin Kuipers, and Silvio
Savarese. Cross-view action recognition via view knowledge
transfer. In CVPR, 2011.

[25] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I
Jordan. Learning transferable features with deep adaptation
networks. In ICML, 2015.

[26] Mingsheng Long, Jianmin Wang, Yue Cao, Jiaguang Sun,
and S Yu Philip. Deep learning of transferable representation
for scalable domain adaptation. TKDE, 2016.

[27] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang
Sun, and Philip S Yu. Transfer joint matching for unsuper-
vised domain adaptation. In CVPR, 2014.

[28] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Deep transfer learning with joint adaptation net-
works. In ICML, 2017.

[29] David G Lowe. Distinctive image features from scale-

invariant keypoints. IJCV, 2004.

[30] Jiˇr´ı Matas Michal Perˇdoch, Ondˇrej Chum. Efﬁcient repre-
sentation of local geometry for large scale object retrieval.
In CVPR, 2009.

[31] Krystian Mikolajczyk, Tinne Tuytelaars, Cordelia Schmid,
Andrew Zisserman, Jiri Matas, Frederik Schaffalitzky, Timor
Kadir, and Luc Van Gool. A comparison of afﬁne region
detectors. IJCV, 2005.

[32] Joe Yue-Hei Ng, Fan Yang, and Larry S Davis. Exploiting
In

local features from deep networks for image retrieval.
CVPRW, 2015.

[33] David Nister and Henrik Stewenius. Scalable recognition

with a vocabulary tree. In CVPR, 2006.

[34] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang
Yang. Domain adaptation via transfer component analysis.
TNN, 2011.

[35] Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer

learning. TKDE, 2010.

[36] Florent Perronnin, Yan Liu, Jorge S´anchez, and Herv´e
Poirier. Large-scale image retrieval with compressed ﬁsher
vectors. In CVPR, 2010.

[37] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and
Andrew Zisserman. Object retrieval with large vocabularies
and fast spatial matching. In CVPR, 2007.

[38] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and
Andrew Zisserman. Lost in quantization: Improving particu-
lar object retrieval in large scale image databases. In CVPR,
2008.

3012

[39] Danfeng Qin, Stephan Gammeter, Lukas Bossard, Till
Quack, and Luc Van Gool. Hello neighbor: Accurate ob-
ject retrieval with k-reciprocal nearest neighbors. In CVPR,
2011.

[40] Filip Radenovi´c, Ahmet Iscen, Giorgos Tolias, Yannis
Avrithis, and Ondˇrej Chum. Revisiting oxford and paris:
Large-scale image retrieval benchmarking. In CVPR, 2018.
[41] Filip Radenovi´c, Giorgos Tolias, and Ondrej Chum. Fine-
tuning cnn image retrieval with no human annotation. PAMI,
2018.

[42] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,
and Andrew Y Ng. Self-taught learning: transfer learning
from unlabeled data. In ICML, 2007.

[43] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan,
and Stefan Carlsson. Cnn features off-the-shelf: an astound-
ing baseline for recognition. In CVPRW, 2014.

[44] Ali S Razavian, Josephine Sullivan, Stefan Carlsson, and At-
suto Maki. Visual instance retrieval with deep convolutional
networks. MTA, 2016.

[45] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary

Bradski. Orb: An efﬁcient alternative to sift or surf. 2011.

[46] Ozan Sener, Hyun Oh Song, Ashutosh Saxena, and Silvio
Savarese. Learning transferrable representations for unsu-
pervised domain adaptation. In NIPS, 2016.

[47] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Learning local feature descriptors using convex optimisation.
PAMI.

[48] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[49] Josef Sivic and Andrew Zisserman. Video google: A text
In ICCV,

retrieval approach to object matching in videos.
2003.

[50] Arnold WM Smeulders, Marcel Worring, Simone Santini,
Amarnath Gupta, and Ramesh Jain. Content-based image
retrieval at the end of the early years. PAMI, 2000.

[51] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang,
Chao Yang, and Chunfang Liu. A survey on deep transfer
learning. In ICANN, 2018.

[52] Giorgos Tolias, Ronan Sicre, and Herv´e J´egou. Particular
object retrieval with integral max-pooling of cnn activations.
In ICLR, 2016.

[53] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and
Trevor Darrell. Deep domain confusion: Maximizing for
domain invariance. arXiv preprint arXiv:1412.3474, 2014.

[54] Jing Zhang, Wanqing Li, and Philip Ogunbona. Joint geo-
metrical and statistical alignment for visual domain adapta-
tion. In CVPR, 2017.

[55] Liang Zheng, Yi Yang, and Qi Tian. Sift meets cnn: A

decade survey of instance retrieval. PAMI, 2017.

3013

