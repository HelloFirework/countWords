Unsupervised Person Image Generation with Semantic Parsing Transformation

Sijie Song1, Wei Zhang2, Jiaying Liu1 ∗, Tao Mei2

1 Institute of Computer Science and Technology, Peking University, Beijing, China

2 JD AI Research, Beijing, China

Abstract

In this paper, we address unsupervised pose-guided per-
son image generation, which is known challenging due to
non-rigid deformation. Unlike previous methods learning a
rock-hard direct mapping between human bodies, we pro-
pose a new pathway to decompose the hard mapping in-
to two more accessible subtasks, namely, semantic pars-
ing transformation and appearance generation. Firstly, a
semantic generative network is proposed to transform be-
tween semantic parsing maps, in order to simplify the non-
rigid deformation learning. Secondly, an appearance gen-
erative network learns to synthesize semantic-aware tex-
tures. Thirdly, we demonstrate that training our frame-
work in an end-to-end manner further reﬁnes the semantic
maps and ﬁnal results accordingly. Our method is gener-
alizable to other semantic-aware person image generation
tasks, e.g., clothing texture transfer and controlled image
manipulation. Experimental results demonstrate the supe-
riority of our method on DeepFashion and Market-1501
datasets, especially in keeping the clothing attributes and
better body shapes.

1. Introduction

Pose-guided image generation has attracted great atten-
tions recently, which is to change the pose of the person im-
age to a target pose, while keeping the appearance details.
This topic is of great importance in fashion and art domains
for a wide range of applications from image / video editing,
person re-identiﬁcation to movie production.

With the development of deep learning and generative
model [8], many researches have been devoted to pose-
guided image generation [19, 21, 5, 27, 26, 1, 20]. Initial-
ly, this problem is explored under the fully supervised set-
ting [19, 27, 26, 1]. Though promising results have been p-
resented, their training data has to be composed with paired
images (i.e., same person in the same clothing but in differ-
ent poses). To tackle this data limitation and enable more

∗Corresponding author. This work was done at JD AI Research.
is available at https://github.com/SijieSong/

Our project
person_generation_spt.git.

Figure 1: Visual results of different methods on DeepFash-
ion [18]. Compared with PG2 [19], Def-GAN [27], and
UPIS [21], our method successfully keeps the clothing at-
tributes (e.g., textures) and generates better body shapes
(e.g., arms).

ﬂexible generation, more recent efforts have been devot-
ed to learning the mapping with unpaired data [21, 5, 20].
However without “paired” supervision, results in [21] are
far from satisfactory due to the lack of supervision. Dis-
entangling image into multiple factors (e.g., background /
foreground, shape / appearance) is explored in [20, 5]. But
ignoring the non-rigid human-body deformation and cloth-
ing shapes leads to compromised generation quality.

Formally, the key challenges of this unsupervised task
are in three folds. First, due to the non-rigid nature of hu-
man body, transforming the spatially misaligned body-parts
is difﬁcult for current convolution-based networks. Sec-
ond, clothing attributes, e.g., sleeve lengths and textures,
are generally difﬁcult to preserve during generation. How-
ever, these clothing attributes are crucial for human visual
perception. Third, the lack of paired training data gives little
clue in establishing effective training objectives.

To address these aforementioned challenges, we propose
to seek a new pathway for unsupervised person image gen-
eration. Speciﬁcally, instead of directly transforming the
person image, we propose to transform the semantic parsing
between poses. On one hand, translating between person

12357

image and semantic parsing (in both directions) has been
extensively studied, where sophisticated models are avail-
able. On the other hand, semantic parsing transformation
is a much easier problem to handle spatial deformation, s-
ince the network does not care about the appearance and
textures.

As illustrated in Fig. 2, our model for unsupervised per-
son image generation consists of two modules: semantic
parsing transformation and appearance generation. In se-
mantic parsing transformation, a semantic generative net-
work is employed to transform the input semantic parsing
to the target parsing, according to the target pose. Then
an appearance generative network is designed to synthe-
size textures on the transformed parsing. Without paired
supervision, we create pseudo labels for semantic parsing
transformation and introduce cycle consistency for training.
Besides, a semantic-aware style loss is developed to help
the appearance generative network learn the essential map-
ping between corresponding semantic areas, where clothing
attributes can be well-preserved by rich semantic parsing.
Furthermore, we demonstrate that the two modules can be
trained in an end-to-end manner for ﬁner semantic parsing
as well as the ﬁnal results.

In addition, the mapping between corresponding seman-
tic areas inspires us to apply our appearance generative net-
work on applications of semantic-guided image generation.
Conditioning on the semantic map, we are able to achieve
clothing texture transfer of two person images. In the mean-
while, we are able to control the image generation by man-
ually modifying the semantic map.

The main contributions can be summarized as follows:

• We propose to address the unsupervised person image
generation problem. Consequently, the problem is de-
composed into semantic parsing transformation (HS)
and appearance generation (HA).

• We design a delicate training schema to carefully op-
timize HS and HA in an end-to-end manner, which
generates better semantic maps and further improves
the pose-guided image generation results.

• Our model is superior in rendering better body shape
and keeping clothing attributes. Also it is generaliz-
able to other conditional image generation tasks, e.g.,
clothing texture transfer and controlled image manip-
ulation.

2. Related Work

2.1. Image Generation

With the advances of generative adversarial network-
s (GANs) [8], image generation has received a lot of at-
tentions and been applied on many areas [15, 29, 4, 31].

There are mainly two branches in this research ﬁeld. One
lies in supervised methods and another lies in unsupervised
methods. Under the supervised setting, pix2pix [11] built a
conditional GAN for image to image translation, which is
essentially a domain transfer problem. Recently, more ef-
forts [15, 29] have been devoted to generating really high-
resolution photo-realistic images by progressively generat-
ing multi-scale images. For the unsupervised setting, re-
construction consistency is employed to learn cross-domain
mapping [34, 32, 16]. However, these unsupervised meth-
ods are developed and applied mostly for appearance gener-
ation of the spatially aligned tasks. With unpaired training
data, our work is more intractable to learn the mapping to
handle spatial non-rigid deformation and appearance gener-
ation simultaneously.

2.2. Pose Guided Person Image Generation

The early attempt on pose-guided image generation was
achieved by a two-stage network PG2 [19], in which the
output under the target pose is coarsely generated in the
ﬁrst stage, and then reﬁned in the second stage. To better
model shape and appearance, Siarohin et al. [27] utilized
deformable skips to transform high-level features of each
body part. Similarly, the work in [1] employs body part
segmentation masks to guide the image generation. How-
ever, [19, 27, 1] are trained with paired data. To relieve the
limitation, Pumarola et al. [21] proposed a fully unsuper-
vised GAN, borrowing the ideas from [34, 22]. On the other
hand, the works in [5, 20] solved the unsupervised problem
by sampling from feature spaces according to the data dis-
tribution. These sample based methods are less faithful to
the appearance of reference images, since they generate re-
sults from highly compressed features. Instead, we use se-
mantic information to help preserve body shape and texture
synthesis between corresponding semantic areas.

2.3. Semantic Parsing for Image Generation

The idea of inferring scene layout (semantic map) has
been explored in [10, 14] for text-to-image translation. Both
of the works illustrate that by conditioning on estimated lay-
out, more semantically meaningful images can be generat-
ed. The scene layout is predicted from texts [10] or scene
graphs [14] with the supervision from groundtruth. In con-
trast, our model learns the prediction for semantic map in an
unsupervised manner. We also show that the semantic map
prediction can be further reﬁned by end-to-end training.

3. The Proposed Method

Given a target pose pt and a reference image Ips under
pose ps, our goal is to generate an output image ˜Ipt , which
follows the clothing appearance of Ips but under the pose
pt. This generation can be formulated as: < Ips , pt >→
˜Ipt .

2358

Figure 2: Our framework for unsupervised person image generation.

i
t}N

the training set

During the training process, we are under an un-
supervised setting:
is composed with
{I i
i
, p
s, p
i=1, where the corresponding ground-truth im-
pi
s
age I i
is not available. For this challenging unpaired per-
pt
son image generation problem, our key idea is to introduce
human semantic parsing to decompose it into two modules:
semantic parsing transformation and appearance genera-
tion. Our overall framework can be viewed in Fig. 2(a). Se-
mantic parsing transformation module aims to ﬁrst generate
a semantic map under the target pose, which provides cru-
cial prior for the human body shape and clothing attributes.
Guided by the predicted semantic map and the reference
image, appearance generation module then synthesizes tex-
tures for the ﬁnal output image.

In the following, we ﬁrst introduce person representa-
tion, which is the input of our framework. We then describe
each module in details from the perspective of independent
training. Finally, we illustrate the joint learning of the two
modules in an end-to-end manner.

3.1. Person Representation

Besides the reference image Ips ∈ R3×H×W , the source
pose ps, and the target pose pt, our model also involves a
semantic map Sps extracted from Ips , pose masks Mps for
ps and Mpt for pt. In our work, we represent poses as prob-
ability heat maps, i.e., ps, pt ∈ Rk×H×W (k = 18). The
semantic map Sps is extracted with an off-the-shelf human
parser [7]. We represent Sps using a pixel-level one-hot en-
coding, i.e., Sps ∈ {0, 1}L×H×W , where L indicates the to-
tal number of semantic labels. For the pose masks Mps and
Mpt , we adopt the same deﬁnition in [19], which provide

prior on pose joint connection in the generation process.

3.2. Semantic Parsing Transformation (HS)

In this module, we aim to predict the semantic map
˜Spt ∈ [0, 1]L×H×W under the target pose pt, accord-
ing to the reference semantic map Sps . It is achieved by
the semantic generative network, which is based on U-
Net [23]. As shown in Fig. 2(b), our semantic genera-
tive network consists of a semantic map encoder ES, a
pose encoder EP and a semantic map generator GS. ES
takes Sps , ps and Mps as input to extract conditional se-
matic information, while EP takes pt and Mpt as input
to encode the target pose. GS then predicts ˜Spt based on
the encoded features. As [35], softmax activation func-
tion is employed at the end of GS to generate the seman-
tic label for each pixel. Formally, the predicted seman-
tic map ˜Spt conditioned on Sps and pt is formulated as
˜Spt = GS (ES(Sps , ps, Mps ), EP (pt, Mpt )). The intro-
duction of Mps and Mpt as input is to help generate contin-
uous semantic maps, especially for bending arms.

Pseudo label generation. The semantic generative net-
work is trained to model the spatial semantic deformation
under different poses. Since semantic maps do not associate
with clothing textures, people in different clothing appear-
ance may share similar semantic maps. Thus, we can search
similar semantic map pairs in the training set to facilitate
the training process. For a given Sps , we search a semantic
map Sp∗
which is under different poses but shares the same
clothing type as Sps . Then we use p
t as the target pose for
Sps , and regard Sp∗
as the pseudo ground truth. We deﬁne
a simple yet effective metric for the search problem. The

∗

t

t

2359

human body is decomposed into ten rigid body subparts as
in [27], which can be represented with a set of binary masks
{Bj}10

j=1(Bj ∈ RH×W ). Sp∗

is searched by solving

t

Adversarial loss. Discriminator DA is ﬁrst introduced
to distinguish between the realistic image and generated im-
age, which leads to adversarial loss Ladv

A

Sp∗

t = arg min
Sp

10

X

j=1

||Bj

p ⊗ Sp − fj(Bj

ps ⊗ Sps )||2
2,

(1)

A = Ladv(HA, DA, Ips , ˜Ipt ) + Ladv(HA, DA, Ips , ˜Ips ),
Ladv
(5)

where fj(·) is an afﬁne transformation to align the two
body parts according to four corners of corresponding bina-
ry masks, ⊗ denotes the element-wise multiplication. Note
that pairs sharing very similar poses are excluded.

Cross entropy loss.

The semantic generative net-
works can be trained under supervision with paired data
{Sps , ps, Sp∗
S to
constrain pixel-level accuracy of semantic parsing transfor-
mation, and we give the human body more weight than the
background with the pose mask Mp∗

t }. We use the cross-entropy loss Lce

, p

as

∗

t

t

Lce

S = −||Sp∗

t ⊗ log( ˜Sp∗

t ) ⊗ (1 + Mp∗

t )||1.

(2)

Adversarial loss. We also employ an adversarial loss
S with a discriminator DS to help GS generate semantic

Ladv
maps of visual style similar to the realistic ones.

Ladv

S = Ladv(HS, DS, Sp∗

t

, ˜Sp∗

t ),

(3)

where HS = GS ◦ (ES, EP ), Ladv(G, D, X, Y ) =
EX [log D(X))] + EY [log(1 − D(Y )] and Y is associated
with G.

The overall losses for our semantic generative network

are as follows,

where HA = GA ◦ (EA, E ′

S).

Pose loss. As in [21], we use pose loss Lpose

A with a pose

detector P to generate images faithful to the target pose

A = ||P( ˜Ipt ) − pt||2
Lpose

2 + ||P( ˜Ips ) − ps||2
2.

(6)

Content loss. Content loss Lcont

A

is also employed to

ensure the cycle consistency

A = ||Λ( ˜Ips ) − Λ(Ips )||2
Lcont
2,

(7)

where Λ(I) is the feature map of image I of conv2 1 layer
in VGG16 model [28] pretrained on ImageNet.

Style loss. It is challenging to correctly transfer the color
and textures from Ips to ˜Ipt without any constraints, since
they are spatially misaligned. [21] tried to tackle this issue
with patch-style loss, which enforces that texture around
corresponding pose joints in Ips and ˜Ipt are similar. We
argue that patch-style loss is not powerful enough in two-
folds: (1) textures around joints would change with differ-
ent poses, (2) textures of main body parts are ignored. An-
other alternative is to utilize body part masks. However,
they can not provide texture contour. Thanks to the guid-
ance provided by semantic maps, we are able to well re-
tain the style with a semantic-aware style loss to address
the above issues. By enforcing the style consistency among
Ips , ˜Ipt and ˜Ips , our semantic-aware style loss is deﬁned as

Ltotal

S = Ladv

S + λceLce
S .

(4)

A = Lsty(Ips , ˜Ipt , Sps , ˜Spt ) + Lsty( ˜Ipt , ˜Ips , ˜Spt , ˜Sps ),
Lsty
(8)

3.3. Appearance Generation (HA)

In this module, we utilize the appearance generative net-
work to synthesize textures for the output image ˜Ipt ∈
R3×H×W , guided by the reference image Sps and predict-
ed semantic map ˜Spt from semantic parsing transformation
module. The architecture of appearance generative network
consists of an appearance encoder EA to extract the appear-
ance of reference image Ips , a semantic map encoder E ′
S to
encode the predicted semantic map ˜Spt , and an appearance
generator GA. The architecture of appearance generative
network is similar to the semantic generative network, ex-
cept that we employ deformable skips in [27] to better mod-
el spatial deformations. The output image is obtained by
S( ˜Spt , pt)(cid:17), as in Fig. 2(c).
˜Ipt = GA (cid:16)EA(Ips , Sps , ps), E ′
Without the supervision of ground truth Ipt , we train the
appearance generative network using the cycle consistency
as [34, 21], in which GA should be able to map back Ips
with the generated ˜Ipt and ps. We denote the mapped-back
image as ˜Ips , and the predicted segmentation map as ˜Sps in
the process of mapping back.

where

Lsty(I1, I2, S1, S2)

=

L

X

l=1

||G(Λ(I1) ⊗ Ψl(S1)) − G(Λ(I2) ⊗ Ψl(S2)))||2
2.

And G(·) denotes the function for Gram matrix [6], Ψl(S)
denotes the downsampled binary map from S, indicating
pixels that belong to the l-th semantic label.

Face loss. Besides, we add a discriminator DF for gen-

erating more natural faces,

A = Ladv(HA, DF , F(Ips ), F( ˜Ipt ))
Lf ace
+ Ladv(HA, DF , F(Ips ), F( ˜Ips )),

(9)

where F(I) represents the face extraction guided by pose
joints on faces, which is achieved by a non-parametric spa-
tial transform network [12] in our experiments.

The overall losses for our appearance generative network

are as follows,

Ltotal

A = Ladv

A + λposeLpose
A + Lf ace
A .

+ λstyLsty

A + λcontLcont

A

(10)

2360

3.4. End to End Training

Since the shape and contour of our ﬁnal output is guided
by the semantic map, the visual results of appearance gener-
ation rely heavily on the quality of predicted semantic map
from semantic parsing transformation. However, if they are
independently trained, two reasons might lead to instability
for HS and HA.

• Searching error: the searched semantic maps are not

very accurate, as in Fig. 3(a).

• Parsing error: the semantic maps obtained from human
parser are not accurate, since we do not have labels to
ﬁnetune the human parser, as in Fig. 3(b).

Our training scheme is shown in Algorithm 1.

Algorithm 1 End-to-end training for our network.

Input: {Si
ps

i=1, {I i
ps
1: Initialize the network parameters.

i
s, Si
p∗
t

t )i}N ∗

, (p

, p

∗

, p

i
s, p

i
t}N

i=1.

, (p

∗

t )i}N ∗

i=1, train {HS, DS} to opti-

//Pre-train HS
i
s, Si
, p
p∗
t

2: With {Si
ps
mize Ltotal
//Train HA

S

.

3: With {I i
ps

, p

i

s, ˜Si

pt

, p

i
t}N

i=1 and {HS, DS} ﬁxed, train

{HA, DA, Df ace} to optimize Ltotal
A .
//Joint optimization

4: Train {HS, DS, HA, DA, Df ace} jointly with Ltotal
A ,

using {I i
ps

, p

i

s, ˜Si

pt

, p

i
t}N

i=1.

Output: HS, HA.

(a) Searching error

(b) Parsing error

Figure 3: Errors exist in the searched semantic map pairs,
which might cause the inaccuracy of semantic parsing trans-
formation.

4. Experiments

In this section, we evaluate our proposed framework with

both qualitative and quantitative results.

4.1. Datasets and Settings

DeepFashion [18]. We experiment with the In-shop
Clothes Retrieval Benchmark of the DeepFashion dataset.
It contains a large number of clothing images with various
appearance and poses, the resolution of which is 256 × 256.
Since our method does not require paired data, we random-
ly select 37, 258 images for training and 12, 000 images for
testing.

Market-1501 [33]. This dataset contains 32,668 images
from different viewpoints. The images are in the resolution

Figure 4: Example results by different methods (PG2 [19],
Def-GAN [27] and UPIS [21]) on DeepFashion. Our model
better keeps clothing attributes (e.g., textures, clothing type-
s).

of 128 × 64. We adopt the same protocol for data split as
in [33]. And we select 12,000 pairs for testing as in [27].

Implementation details. For the person representation,
the 2D poses are extracted using OpenPose [2], and the con-
dition semantic maps are extracted with the state-of-the-art
human parser [7]. We integrate the semantic labels original-
ly deﬁned in [7] and set L = 10 (i.e., background, face, hair,
upper clothes, pants, skirt, left/right arm, left/right leg). For
DeepFashion dataset, the joint learning to reﬁne semantic
map prediction is performed on the resolution of 128 × 128.
Then we upsample the predicted semantic maps to train im-
ages in 256 × 256 with progressive training strategies [15].
For Market-1501, we directly train and test on 128 × 64.
Besides, since the images in Market-1501 are in low resolu-
tion and the face regions are blurry. Lf ace
is not adopted on
Market-1501 for efﬁciency. For the hyper-parameters, we
set λpose, λcont as 700, 0.03 for DeepFashion and 1, 0.003
for Market-1501. λsty is 1 for all experiments. We adopt
ADAM optimizer [17] to train our network with a learning
rate 0.0002 (β1 = 0.5 and β2 = 0.999). The batch sizes for
DeepFashion and Market-1501 are set to 4 and 16, respec-
tively. For more detailed network architecture and training
scheme on each dataset, please refer to our supplementary.

A

4.2. Comparison with State of the Arts

Qualitative Comparison. In Fig. 1, Fig. 4 and Fig. 5,
we present the qualitative comparison with three state-of-
the-art methods: PG2 [19], Def-GAN [27] and UPIS [21]1.

1The results for PG2 and Def-GAN are obtained by public models re-

leased by their authors, and UPIS are based on our implementation.

2361

4.3. Ablation Study

We design the following experiments with different con-
ﬁgurations to ﬁrst evaluate the introduction of semantic in-
formation for unpaired person image generation:

• Baseline: our baseline model without the introduction
of semantic parsing, the architecture of which is the same as
appearance generative network, but without semantic map
as input. To keep the style on the output image, we use
mask-style loss, which replaces semantic maps with body
part masks in Eq. (8).

• TS-Pred: The semantic and appearance generative net-
works are trained independently in a two-stage manner.
And we feed the predicted semantic maps into appearance
generative network to get the output.

• TS-GT: The networks are trained in two-stage. We re-
gard semantic maps extracted from target images as ground
truth, and feed them into appearance generative network to
get the output.

• E2E (Ours): jointly training the networks in an end-to-

end manner.

Fig. 6 presents the intermediate semantic maps and the
corresponding generated images. Table 1 further shows the
quantitative comparisons. Without the guidance of seman-
tic maps, the network is difﬁcult to handle the shape and
appearance at the same time. The introduction of semantic
parsing transformation consistently outperforms our base-
line. When trained in two-stage, the errors in the predict-
ed semantic maps lead to direct image quality degradation.
With end-to-end training, our model is able to reﬁne the se-
mantic map prediction. For example, the haircut and sleeves
length in Fig. 6(a) are well preserved. For DeepFashion,
the end-to-end training strategy leads to comparable results
with that using GT semantic maps. For Market-1501, our
model (E2E) achieves even higher IS and SSIM values than
TS-GT. This is mainly because the human parser [7] does
not work very well on low-resolution images and many er-
rors exists in the parsing results, as the ﬁrst row in Fig. 6(b).
We then analyze the loss functions in the appearance
generation as shown in Fig. 7. We mainly explore the pro-
posed style loss and face adversarial loss, since other losses
are indispensable to ensure the cycle consistency. We adopt
TS-GT model here to avoid the inﬂuence of semantic map
prediction. In (a) and (b), we replace the semantic-aware
style loss Lsty
A with mask-style loss and patch-style loss, re-
spectively. Without semantic guidance, both of them lead to
dizzy contour. Besides, the adversarial loss for faces effec-
tively helps generate natural faces and improve the visual
quality of output images.

4.4. Applications

Since the appearance generative network essentially
learns the texture generation guided by semantic map, it can
also be applied on other conditional image generation tasks.

2362

Figure 5: Example results by different methods (PG2 [19],
Def-GAN [27] and UPIS [21]) on Market-1501. Our model
generates better body shapes.

PG2 [19] and Def-GAN [27] are supervised methods that re-
quire paired training data. UPIS [21] is under the unsuper-
vised setting, which essentially employs CycleGAN [34].
Our model generates more realistic images with higher vi-
sual quality and less artifacts. As shown in Fig. 4, our
method is especially superior in keeping the clothing at-
tributes, including textures and clothing type (the last row).
Similarly in Fig. 5, our method better shapes the legs and
arms. More generated results can be found in our supple-
mentary.

Quantitative Results. In Table 1, we use the Inception
Score (IS) [24] and Structural SIMilarity (SSIM) [30] for
quantitative evaluation. For Market-1501 dataset, to allevi-
ate the inﬂuence of background, mask-IS and mask-SSIM
are also employed as in [19], which exclude the background
area when computing IS and SSIM. For a fair comparison,
we mark the training data requirements for each method.
Overall, our proposed model achieves the best IS value
on both datasets, even compared with supervised methods,
which is in agreement with more realistic details and better
body shape in our results. Our SSIM score is slightly lower
than other methods, which can be explained by the fact that
blurry images always achieve higher SSIM but being less
photo-realistic, as observed in [20, 19, 13, 25]. Limited by
space, please refer to our supplementary for user study.

Table 1: Quantitative results on DeepFashion and Market-1501 datasets (*Based on implementation).

Models
PG2 [19]

Def-GAN [27]

V-Unet [5]

BodyROI7 [20]

UPIS [21]
Baseline
TS-Pred
TS-GT

E2E(Ours)

Paired data

Y
Y
N
N
N
N
N
N
N

DeepFashion
SSIM
IS
0.762
0.756
0.786
0.614
0.747
0.698
0.724
0.740
0.736

3.090
3.439
3.087
3.228
2.971
3.140
3.201
3.350
3.441

IS

3.460
3.185
3.214
3.483
3.431*
2.776
3.462
3.472
3.499

Market-1501

–

–

0.792
0.805

3.435
3.502

SSIM mask-IS mask-SSIM
0.253
0.290
0.353
0.099
0.151*
0.157
0.180
0.200
0.203

0.614
0.742*
0.714
0.740
0.749
0.758

3.491
3.485*
2.814
3.546
3.675
3.680

(a) Results on DeepFashion with different conﬁgurations. (Note E2E reﬁnes the haircut in the 1st row, sleeve length in the 2nd,
arms in the 3rd row, compared with TS-Pred.)

(b) Results on Market-1501 with different conﬁgurations. (Note E2E reﬁnes the body shape in the 1st and 3rd rows, pants length
in the 2nd row, compared with TS-Pred.)

Figure 6: Ablation studies on semantic parsing transformation.

2363

Figure 7: Analysis for the loss function in appearance gen-
eration. (a) Replace Lsty
A with mask-style loss. (b) Replace
Lsty
A with patch-style loss. (c) Without Lf ace
A . Results of
TS-GT with our full loss are in the right.

Figure 8: Application for clothing texture transfer. Left:
condition and target images. Middle: transfer from A to B.
Right: transfer from B to A. We compare our methods with
image analogy [9] and neural doodle [3].

Here we show two interesting applications to demonstrate
the versatility of our model.

Clothing Texture Transfer. Given the condition and tar-
get images and their semantic parsing results, our appear-
ance generative network is able to achieve clothing texture
transfer. The bidirectional transfer results can be viewed in
Fig. 8. Compared with image analogy [9] and neural doo-
dle [3], not only textures are well preserved and transferred
accordingly, but also photo-realistic faces are generated au-
tomatically.

Controlled Image Manipulation. By modifying the se-
mantic maps, we generate images in the desired layout. In
Fig. 9, we edit the sleeve lengths (top), and change the dress
to pants for the girl (bottom). We also compare with image
analogy [9] and neural doodle [3].

4.5. Discussions for Failure Cases

Though our model generates appealing results, we show
the examples of failure cases in Fig. 10. The example in the
ﬁrst row is mainly caused by the error in condition semantic
map extracted by the human parser. The semantic genera-

Figure 9: Application for controlled image manipulation.
By manually modifying the semantic maps, we can control
the image generation in the desired layout.

Figure 10: The failure cases in our model.

tive network is not able to predict the correct semantic map
where the arms should be parsed as sleeves. The transfor-
mation in the second example is very complicated due to the
rare pose, and the generated semantic map is less satisfac-
tory, which leads to unnatural generated images. However,
with groundtruth semantic maps, our model still achieves
pleasant results. Thus, such failure cases can be probably
solved with user interaction.

5. Conclusion

In this paper, we propose a framework for unsupervised
person image generation. To deal with the complexity of
learning a direct mapping under different poses, we decom-
pose the hard task into semantic parsing transformation and
appearance generation. We ﬁrst explicitly predict the se-
mantic map of the desired pose with semantic generative
network. Then the appearance generative network synthe-
sizes semantic-aware textures. It is found that end-to-end
training the model enables a better semantic map prediction
and further ﬁnal results. We also showed that our model
can be applied on clothing texture transfer and controlled
image manipulation. However, our model fails when errors
exist in the condition semantic map. It would be an interest-
ing future work to train the human parser and person image
generation model jointly.
Acknowledgements. This work was supported by Na-
tional Natural Science Foundation of China under contract
No. 61602463 and No. 61772043, Beijing Natural Science
Foundation under contract No. L182002 and No. 4192025.

2364

References

[1] Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Du-
rand, and John Guttag. Synthesizing images of humans in
unseen poses. In Proc. IEEE Conference on Computer Vi-
sion and Pattern Recognition, 2018. 1, 2

[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, 2017. 5

[3] Alex J. Champandard. Semantic style transfer and turn-
ing two-bit doodles into ﬁne artworks. arXiv preprint arX-
iv:1603.01768, 2016. 8

[4] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition, 2018. 2

[5] Patrick Esser, Ekaterina Sutter, and Bj¨orn Ommer. A varia-
tional u-net for conditional appearance and shape generation.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, 2018. 1, 2, 7

[6] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, 2016. 4

[7] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,
and Liang Lin. Look into person: Self-supervised structure-
sensitive learning and a new benchmark for human parsing.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 3, 5, 6

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing X-
u, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Proc. Ad-
vances in Neural Information Processing Systems, 2014. 1,
2

[9] Aaron Hertzmann. Image analogies. Proc Siggraph, 2001. 8

[10] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and
Honglak Lee. Inferring semantic layout for hierarchical text-
to-image synthesis. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, 2018. 2

[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A E-
fros. Image-to-image translation with conditional adversari-
al networks. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition, 2017. 2

[12] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Proc. Advances in Neural
Information Processing Systems, 2015. 4

[13] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
Proc. European Conference on Computer Vision, 2016. 6

[14] Justin Johnson, Agrim Gupta, and Li Fei-Fei.

Image gen-
In Proc. IEEE Conference on

eration from scene graphs.
Computer Vision and Pattern Recognition, 2018. 2

[15] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196, 2017. 2, 5

[16] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations
with generative adversarial networks. arXiv preprint arX-
iv:1703.05192, 2017. 2

[17] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

[18] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition and
retrieval with rich annotations. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, 2016. 1, 5

[19] Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuyte-
laars, and Luc Van Gool. Pose guided person image gener-
ation. In Proc. Advances in Neural Information Processing
Systems, 2017. 1, 2, 3, 5, 6, 7

[20] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc
Van Gool, Bernt Schiele, and Mario Fritz. Disentangled per-
son image generation. In Proc. IEEE Conference on Com-
puter Vision and Pattern Recognition, 2018. 1, 2, 6, 7

[21] Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, and
Francesc Moreno-Noguer. Unsupervised person image syn-
thesis in arbitrary poses. In Proc. IEEE Conference on Com-
puter Vision and Pattern Recognition, 2018. 1, 2, 4, 5, 6,
7

[22] Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka,
Bernt Schiele, and Honglak Lee. Learning what and where
to draw. In Proc. Advances in Neural Information Processing
Systems, 2016. 2

[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In Proc. Int’l Conference on Medical Image Computing and
Computer-Assisted Intervention, 2015. 3

[24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vick-
i Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Proc. Advances in Neural Information
Processing Systems, 2016. 6

[25] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,
Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, 2016. 6

[26] Chenyang Si, Wei Wang, Liang Wang, and Tieniu Tan. Mul-
tistage adversarial losses for pose-based human image syn-
thesis. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, 2018. 1

[27] Aliaksandr Siarohin, Enver Sangineto, St´ephane Lathuili`ere,
and Nicu Sebe. Deformable gans for pose-based human im-
age generation. In Proc. IEEE Conference on Computer Vi-
sion and Pattern Recognition, 2018. 1, 2, 4, 5, 6, 7

[28] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. 2015.
4

[29] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional gans.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, 2018. 2

2365

[30] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600–612, 2004. 6

[31] Wenqi Xian, Patsorn Sangkloy, Jingwan Lu, Chen Fang,
Fisher Yu, and James Hays. Texturegan: Controlling deep
image synthesis with texture patches. In Proc. IEEE Con-
ference on Computer Vision and Pattern Recognition, 2018.
2

[32] Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong.
Dualgan: Unsupervised dual learning for image-to-image
translation.
In Proc. IEEE Int’l Conference on Computer
Vision, 2017. 2

[33] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:
A benchmark. In Proc. IEEE Int’l Conference on Computer
Vision, 2015. 5

[34] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks.
In Proc. IEEE Int’l Con-
ference on Computer Vision, 2017. 2, 4, 6

[35] Shizhan Zhu, Sanja Fidler, Raquel Urtasun, Dahua Lin, and
Chen Change Loy. Be your own prada: Fashion synthesis
with structural coherence.
In Proc. IEEE Int’l Conference
on Computer Vision, 2017. 3

2366

