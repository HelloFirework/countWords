Generalized Zero-Shot Recognition based on Visually Semantic Embedding

Pengkai Zhu

Hanxiao Wang

Venkatesh Saligrama

Electrical and Computer Engineering Department, Boston University

Abstract

We propose a novel Generalized Zero-Shot learning
(GZSL) method that is agnostic to both unseen images and
unseen semantic vectors during training. Prior works in
this context propose to map high-dimensional visual fea-
tures to the semantic domain, which we believe contributes
to the semantic gap. To bridge the gap, we propose a novel
low-dimensional embedding of visual instances that is “vi-
sually semantic.” Analogous to semantic data that quanti-
ﬁes the existence of an attribute in the presented instance,
components of our visual embedding quantiﬁes existence of
a prototypical part-type in the presented instance. In paral-
lel, as a thought experiment, we quantify the impact of noisy
semantic data by utilizing a novel visual oracle to visually
supervise a learner. These factors, namely semantic noise,
visual-semantic gap and label noise lead us to propose a
new graphical model for inference with pairwise interac-
tions between label, semantic data, and inputs. We tabulate
results on a number of benchmark datasets demonstrating
signiﬁcant improvement in accuracy over state-of-art under
both semantic and visual supervision.

1. Introduction

Zero-shot learning (ZSL) is emerging as an important
tool for large-scale classiﬁcation [20], where one must ac-
count for challenges posed by non-uniform and sparse an-
notated classes [5], the prohibitive expense in labeling large
fractions of data [4], and the need to account for appearance
of novel objects for in-the-wild scenarios.

ZSL proposes to learn a model for classifying images
for “unseen” classes for which no training data is available
by leveraging semantic features, which are shared by both
seen and unseen classes. A few recent works [7, 24] point
out that, unseen image class recognition, while important,
overlooks real-world scenarios, where both seen and un-
seen instances appear. Consequently, generalized zero-shot
learning (GZSL) methods capable of recognizing both seen
and unseen instances at test time are required.

We propose to train a GZSL method, that takes labeled
seen class images and associated semantic side information
as input, while being agnostic to both unseen images and
unseen associated semantic vectors.

Challenges. We list challenges in this context:
Visual ! Semantic Gap. Visual feature representations
such as the ﬁnal-layer outputs of deep neural networks are
high-dimensional and not semantically meaningful. This
limits the learner in identifying robust associations between
visual patterns and semantic data.
Semantic!Visual Gap. A fundamental drawback of seman-
tic data is that they are often not visually meaningful and it
is difﬁcult for a learner to identify and suppress non-visual
semantic components during training.Additionally, seman-
tic information provided for some classes (ex. sofa-chair),
are nearly identical. This is challenging in a GZSL setting
particularly when one of such classes is among the unseen.

Novelty. At a conceptual level visual representation and su-
pervision fundamentally impacts accuracy. We re-examine
these concepts and propose novel methods in Sec. 3 to iden-
tify and bridge the visual-semantic gap.

Visually Semantic Embedding. By a visually semantic em-
bedding, we mean a mapping of visual instances to a rep-
resentation that mirrors how semantic data is presented for
an instance. In Sec. 3.1 we propose to train a model that
learns a ﬁnite list of parts based on a multi-attention model
and expresses the input as a ﬁnite probabilistic mixture of
part-types, which we then output as our representation. Our
intuition is informed by semantic data where for each in-
stance, an annotator could score existence of attributes from
a common vocabulary. Analogously, our embedding scores
existence of proto-typical part types in a presented instance.

3-Node Graphical Model. A key aspect of our setup, which
is presented in Sec. 3.1, is a graphical model[13] that has
semantic (S), input (X) and label (Y) variables in a 3-node
clique. This is based on the key insight that the labels are
not fully explained by either the input or the semantic in-
stance and thus we require a model that accounts for 3-way
connection (S $ X, X $ Y, Y $ S). This is a signiﬁ-
cant departure from existing works [15], where the semantic
signal is given paramount importance and the structure is a
chain X $ S $ Y .

Following convention, we conduct experiments with se-
mantic supervision on benchmark datasets in Sec. 4. While
we demonstrate signiﬁcant improvement over state-of-the-
art, we are driven to understand and quantify how semantic

12995

noise can explain GZSL performance loss.
Visually Semantic Supervision. As a thought experiment
we propose to train a novel visual oracle for GZSL super-
vision. Our intuition is that a visual oracle can reduce se-
mantic noise and provide more deﬁnitive feedback about
the presence/absence of prototypes. To ensure fair compar-
ison between semantic and visual feedback, we ﬁrst learn a
common vocabulary of protypical parts and part-types un-
beknownst to the learner. Like a semantic signal, our visual
oracle for each input instance provides the learner only a list
of similarity scores, with no other additional description of
what the components in the list mean.

Visual supervision naturally leads us to propose visual
evaluation, which involves evaluating predicted visually se-
mantic outputs against the ground-truth. In Sec. 4, we show
that GZSL performance improves not only for our method
but also for a well-known baseline method [10] when we
substitute visual in place of semantic supervision.
2. Related Work

Zero-shot learning as a topic has evolved rapidly over the
last decade and documenting the extensive literature here is
not possible. As summarized in [24] many existing meth-
ods can be grouped into attribute methods as exempliﬁed
in [15] that leverage attributes as an intermediate feature
space to link different classes, embedding methods [10] that
directly map visual domain to semantic space, and hybrid
methods [28], that map semantic and visual domain into a
shared feature space. Recent work [7, 24] introduces GZSL
problem and developed calibration and evaluation proto-
cols showing signiﬁcant drop in accuracy between ZSL and
GZSL. Many recent works are beginning to focus attention
on the GZSL setup.

In this context, we propose an embedding based method
and describe closely related setups and concepts that have
appeared in the literature. We ﬁrst categorize existing work
based on problem setup and different types of side infor-
mation utilized during training. There are a number of re-
cent works that propose approaches for both ZSL and GZSL
cases [17, 3, 30, 14, 25, 11, 8, 23, 16]. Among these, there
are works that leverage some form of unseen class informa-
tion during training [30, 14, 25, 11] to synthesize unseen
examples by means of GAN or VAE training. Other works
employ knowledge graphs [16, 23] incorporating both seen
and unseen classes during training to infer classiﬁers for un-
seen classes. Still others are transductive, namely, at test-
time [17] they leverage a batch of test examples to further
reﬁne their model. While these proposed approaches that
leverage unseen class information are interesting, we take
the view that for applications involving recognition in-the-
wild scenarios, novel classes may only appear at test-time,
and it is important to consider such situations. Ultimately,
as a subject of future work, it would be interesting to incor-
porate cases where some unseen class information is known

during training, while leaving open the possibility of exis-
tence of novel classes at test-time.

Like us, there are works

[8, 3] focusing on ZSL and
GZSL, while being agnostic to any unseen class informa-
In [3], authors propose an encoder-
tion during training.
decoder network with the goal of mirroring learnt seman-
tic relations between different classes in the visual domain.
While the goal is similar, our approach is signiﬁcantly dif-
ferent. We propose to mirror information provided by se-
mantic attributes visually by means of a low-dimensional
statistical embedding. Our embedding scores existence of
prototypical part types, where the prototypical part types
are learnt from training data.

In [8], the authors propose an approach that extends
methods of [12]. Their idea is to penalize approximation
error in reconstructing visual domain features from the se-
mantic domain, in addition to penalizing classiﬁcation loss.
Their claim is that by doing so they can overcome semantic
information loss suffered in methods that are based on vi-
sual to semantic embedding and prevent situations where at-
tributes possibly corresponding to unseen examples maybe
lost during training. In contrast, our claim is precisely that
many semantic attributes are not visual and conventional vi-
sual features are not represented in the presented semantic
vectors. Consequently, we propose approaches that on the
one hand produces semantically closer visual representa-
tions through low-dimensional graphical models, and on the
other hand suppress semantic components that are visually
unrepresentative by means of discriminative loss functions.
In general we do not require high-dimensional estimation,
which these methods require.

In this context our approach bears some similarities to
[18] and [30].
In particular, [18] propose zoom-net as a
means to ﬁlter-out redundant visual features such as delet-
ing background and focus attention on important locations
of an object. [30] further extend this insight and propose vi-
sual part detector (VPDE-Net) and utilize high-dimensional
part feature vectors as an input for semantic transfer. [30]’s
proposal is to incorporate the resulting reduced representa-
tion as a means to synthesize unseen examples leveraging
knowledge of unseen class attributes. Different from these
works we develop methods to learn a statistical representa-
tion of mixture proportions of latent parts. Apart from being
low-dimensional the mixture proportions intuitively capture
underlying similarity of a part-type to other such part-types
found in other classes. The focus of semantic mapping is
then to transfer knowledge between mixture proportion of
part types and semantic similarity.

3. Proposed Approach

3.1. A Probabilistic Perspective of GZSL

Let us motivate our approach from a probabilistic mod-
eling perspective. This will in turn provide a basis for our

2996

CNN

visual feature

large gap

small gap

input image

Semantic

Oracle

'metal', 'wheel', 'engine' ...
semantic attributes

visually semantic embedding
type-1    type-2    type-3 ...
0.1 ...

0.7

0.1

0.1

0.8 0.05 ...

part-1

part-2

part-3

...

Figure 1. Many existing works attempt to transfer high-dimensional visual features into semantic domain leading to signiﬁcant visual-
semantic gap. To bridge the gap we propose a new latent visual embedding that is visually semantic. As illustrated our new representation
is low dimensional and its components are likelihoods of a part-type relative to proto-typical part-types found across all instances. We posit
that our embedding mirrors how semantic components score similarity of an attribute found in an instance in the visual domain.

Figure 2. Graphical Model of label, semantic signal, and input as a
cycle. Input is transformed into a ﬁnite collection of feature vector
parts indexed by items in a part-list. Feature parts are mapped into
structured probability space, with component πx(k|m) denoting
the probability of k-th type in item m in the part-list. Πk(x) =
[πx(k|m)] denoting visual embedding for part k.

discriminative learning method. [N ] denotes integers from
1 to N . Overloading notation we denote [ηk,m] to mean the
matrix as k, m range over their values. Following conven-
tion we denote random variables with upper case letters and
a realization by lower case letters. Let x 2 X be inputs
taking values in an arbitrary feature space and Y the space
of objects or classes. The set Y is partitioned into o 2 O
and u 2 U denoting the collection of observed class labels
and unobserved labels respectively. Associated with each
observed and unobserved class labels are semantic signals,
so, su 2 S, taking values in a general space respectively.
We denote by p the joint density or marginal densities wher-
ever appropriate.

Given training data (x1, y1, sy1 ), . . . , (xn, yn, syn ) ⇢
X ⇥ O ⇥ S, the task of GZSL is to accurately predict a label
d⇠ pX (·). If we had knowledge of
with input drawn from x
joint probability density, the optimal predictor is the MAP
estimate, yMAP(x) = arg maxy maxs log(p(y, s | x)). Ex-
isting work [15] posits instead a chain X   S   Y , namely,
conditioned on semantic signals, the input and class la-
bels are independent or that the chain X   Y   S [27]

is true. Nevertheless, we take the view that, since seman-
tic information is not fully visual, visual features are not
fully semantic, and labels are not fully captured by either
visual or semantic signals, the three random variables form
a cycle in a graphical model. By means of potential func-
tions for graphical models we decompose log(p(y, s|x) /
φY X (y, x) + φXS(x, s)) + φSY (s, y).
Latent Graphical Model. Fig. 2 presents a detailed frame-
work of our model. The input X = x is mapped into a ﬁnite
set of feature vector parts, fm(x) 2 RC indexed by discrete
part-list, m 2 [M ]. Feature parts are derived from a multi-
attention model with different items m, m0 2 [M ], m 6= m0
focusing on different regions in the image. We then model
each feature part vector as a C-dimensional Gaussian Mix-
ture Model, fm(x) ⇡ Pi πx(k|m)N (θk,m, γ2I), with
isotropic components. Note that the parameters, θk,m 2
RC , are part and type dependent but shared among all in-
stances. We refer to θk,m’s as prototypical part-types. We
collect the parameters into a matrix Θm = [θk,m] 2 RC⇥K .
Each mixture component, πx(k|m) represents the prob-
ability of type k conditioned on part m. In this way, the in-
put x is embedded into a collection of mixture components
Πm(x) = [πx(k|m)] and Π(x) = [Πm(x)]. We decompose
the likelihood as:

log(p(y, s | x)) / φSX (s, Π(x)) + φXY ([fm(x)], y)

(1)

+ φY S(y, s)  

M

X

m=1

  Lprt([fm(x)])

Lmix(Θm, Πm(x), fm(x))

where, the last two terms respectively model mixture likeli-
hood and enforce diversity of multi-attention of parts. The
goal of training algorithm is to estimate the potential func-
tions, φ, the feature part backbone, fm(·), and the probabil-
ity maps, Πm(·) by leveraging training data.
Latent Visual Embedding Intuition. Note that, by design

2997

(see Fig. 2 and Eq. 1), the semantic random vector interacts
with the input only through the mixture component Π(x).
This is in contrast to existing works where the interaction
is high-dimensional. Our intuition is that, just like a com-
ponent of a semantic vector quantiﬁes existence of an at-
tribute, in an analogous fashion, πx(k|m) quantiﬁes exis-
tence of part-type k in part m. While prototypical parts
such as θk,m are high-dimensional, the corresponding mix-
ture component πx(k|m) is a scalar number. In this way
we propose to reduce the semantic-visual gap by removing
irrelevant visual features that are not transferable.

Visual Oracle Supervision (VOS). We consider visual or-
acles capable of providing feedback for learner predicted
visual embedding. We denote Πvo(x) as oracle feedback.
As to how to build a visual oracle will be discussed later.
We can consider structured and class-averaged VOS. In the
structured version, for each instance, x, VOS reveals the
probabilistic embedding Πvo(x); and in the class-averaged
case, only reveals ¯Πvo = E
X|Y [Πvo(x) | y]. The main dif-
ference in Eq. 1 is that we substitute oracle parameters for
the semantic signal, i.e., s with Πvo(x), ¯Πvo etc.
Justiﬁcation of Visual Oracle Supervision. In constructing
the visual oracle, our goal is driven by the need to quantify
semantic noise., To do so we need an oracle that provides
no more information than a “noiseless” semantic one. This
is not that hard since our visual oracle presents the learner
with mixture values Πvo(x) with out identifying what these
numbers mean or which classes, parts or locations they refer
to. All that a learner knows is that what the oracle is com-
municating information that has deﬁnitive visual meaning.

Visual and Semantic Test-Time Evaluation. At test-
time, following convention, for the semantic setting, we
assume that the codebook consisting of seen and unseen
semantic attribute vectors, {sy|y 2 Y} = {so|o 2
O} [ {su|u 2 U} are revealed to the learner. For a
test image, x,
the learner must identify the hidden la-
the learner computes Π(x), and esti-
bel. To do this,
mates the label by maximizing the visual-semantic potential
ˆy(x) = arg maxy2Y φSX (sy, Π(x)). In the visual evalua-
tion setting, the class-level { ¯Πy
vo|y 2 Y} is revealed dur-
ing test time and the learner make prediction by maximiz-
ing the visual-semantic potential in the Π space ˆy(x) =
arg maxy2Y φSX ( ¯Πy

vo, Π(x)).

3.2. Model and Loss Parameterization

Part Feature model fm(·):
Inspired by [29], we use a
multi-attention convolutional neural network (MA-CNN) to
map input images into a ﬁnite set of feature vector parts,
fm(x). Speciﬁcally, fm(x) = [fm,c] contains a feature ex-
tractor E and a channel grouping model G, where E(x) 2
RW ⇥H⇥C is a global feature map, and G(E(x)) 2 RM ⇥C
is a channel grouping weight matrix. We then calculate an

attention map Am(x) 2 RW ⇥H for the m-th part:

Am(x) = sigmoid  X

Gm,c(x) ⇥ Ec(x) 

(2)

c

The part feature fm(x) 2 RC is then calculated as:

fm,c(x) = X

w,h

[Am(x)   Ec(x)](w,h),

8c 2 [C]

(3)

where   is the element-wise multiplication. We parame-
terized E(·) by the ResNet-34 backbone (to conv5 x), and
G(·) by a fully-connected layer.
Mixture model Π(·): Note that our Gaussian mixture
model implies:

EZ[fm(x)|Πm, Θm] = ΘmΠm(x)

This sets up a matrix factorization problem, with positiv-
ity constraints on the components of Πm(x). Observe that,
at test-time, since the matrices Θm are known, the solu-
tion to Π(x) reduces to solving a linear system of equations
with positivity constraints. Alternatively, we can employ a
Bayesian perspective (which is what we do) and compute:

πx(k|m) / \π(k|m)N (fm(x); θk,m, γ2I)

(4)

where, \π(k|m) is the prior for prototype k in part m esti-
mated during training.
Part Feature Learning Loss Lprt: To encourage a part-
based representation fm(x) to be learned, we follow [29].
Since fm(x) can be decomposed into Am(x)   E(x), we
want to force the learned attention maps Am to be both com-
pact within the same part, and divergent among different
parts. We deﬁne Lprt([fm(x)]) to be:

Lprt([fm(x)]) = X

(Ldis(Am(x)) + λLdiv(Am(x)))

m

loss Ldis(Am) and divergent
where the compact
Ldiv(Am) are deﬁned as (x is dropped for simplicity):

Ldis(Am) = X

Aw,h

m [kw   w⇤k2 + kh   h⇤k2]

w,h

Ldiv(Am) = X

w,h

Aw,h

m [maxn,n6=mAw,h

n   ζ]

(5)
loss

(6)

(7)

where Aw,h
m is the amplitude of Am at coordinate (w, h),
and (w⇤, h⇤) is the coordinate of the peak value of Am, ζ is
a small margin to ensure the training robustness.
Mixture Model Learning Loss Lmix: We pose this as
a standard max-likelihood estimation problem, and learn
θk,m, γ parameters using the EM algorithm to ﬁt the feature
vectors fm,i , fm(xi), with xi being training examples.
We can write the negative log-likelihood for i-th sample:

Lmix(Θm, Πm, fm,i) =   log(Pk ¯π(k|m)N (fm,i; Θm, γ2I))

(8)

2998

where the parameters are optimized by the Expectation-
Maximization (EM) algorithm during training. Once they
are learned, the mixture component embedding Π(x) can
be inferred with Eq.(4).
Semantic-Label Potential φY S:
In the GZSL problem,
we usually assume a deterministic one-to-one mapping be-
tween the semantic signals to class labels provided by a
semantic oracle (human annotator). The Semantic-Label
potential function is thus simply modeled by an indicator
function:

φY S(y, sy0 ) = I(y = y0)

(9)

Visual-Label Potential φXY : To map visual representa-
tions to class labels, we construct a classiﬁcation model D
that takes the concatenated part features [fm(x)] as input
and outputs a classiﬁcation prediction, i.e. D([fm(x)]) 2
R|O|, where |O| refers to the number of observed classes.
In our implementation, D(·) is simply a fully-connected
layer followed by a softmax. Let ˆp(y) denote the one-hot
encoding of the ground-truth class label y for input image
x, the potential φXY is given by the negative cross-entropy
between label and prediction:

φXY ([f m(x)], y) =  CE(D([fm(x)]), ˆp(y))

(10)

Visual-Semantic Potential φSX : Existing works often take
the raw feature vectors as the visual representation, i.e.
E(x) or fm(x), and suffer from a large discrepancy be-
tween the visual and semantic domains. To mitigate such
a gap, we propose to adopt the latent mixture component
embedding Π(x).
Semantic Oracle. In the common GZSL setting where the
semantic signals are obtained from a human annotator, we
construct a Semantic Mapping model V (Π(x)) to project
Π(x) into S, where V (·) is further parameterized by a neu-
ral network. Given an imput image x and its semantic at-
tribute sy, the potential φSX is modeled as:

φSX (sy, Π(x)) =   Py02O[ηI(y0 = y) + s>

y0 V (Π(x))   s>

y V (Π(x))]+

(11)

where η is a margin parameter.
Visual Oracle. As discussed by Sec.3.1, we want to evalu-
ate the efﬁcacy of the proposed latent embedding. We thus
considers a visual oracle which directly provides Πvo(x) as
a visually semantic supervision, which is a list of visual part
similarity scores. In this case, we no longer need V (·) since
both Π(x) and Πvo(x) are already in the same space. The
potential φSX is thus:

φSX (Πvo(x), Π(x)) =  |Πvo(x)   Π(x)|2
F ,

(12)

where | · |F is the Frobenius norm.

3.3. Implementation Details

Our model takes input image size as [448 ⇥ 448] and
and the output of E(x) is in the size of 14 ⇥ 14 ⇥ 512.
There are 4 parts in the model and in each part, the number
of types M is set to 16. λ in Eq.(5) and ζ in Eq.(7) is em-
pirically set to 5 and 0.02. In the semantic-oracle scenario,
the smenatic mapping model V (·) is implemented by a two
fc-layer neural network with ReLU activation.

In the visual-oracle scenario, the visual oracle is built to
provide Πvo(x). It consists of the part feature model fm(·)
and a classiﬁer D, where the feature extractor E(·) in fm(·)
is parameterized by the VGG-19 convolutional layers. We
choose VGG instead of ResNet backbone to avoid that the
learner learns to recover the same parameters in the oracle
via the Πvo(x). The oracle is ﬁrst trained by maximizing
φXY   Lprt([fm(x)]) to learn a discriminating fm(·). Then
the EM optimization over PM
m=1 Lmix(ΘmΠm, fm(x)) is
done to generate Πov(x) for our model. During training, the
oracle provide instance level Πvo(x) and in the test time,
only class-averaged ¯Πvo is revealed to the learner to make
the prediction.

4. Experiments

Datasets. We evaluate the performance of our model
on three commonly used benchmark datasets for GZSL:
Caltech-UCSD Birds-200-2011 (CUB) [22], Animals with
Attributes 2 (AWA2) [24] and Attribute Pascal and Yahoo
(aPY) [9]. CUB is a ﬁne-grained dataset which contains
200 different types of birds. CUB has 11,788 images and
312-dim annotated semantic attributes. AWA2 is a coarse-
grained dataset which has 37,322 images from 50 differ-
ent animals. 85 binary and continuous class attributes are
provided. aPY is also a coarse-grained dataset with 64 se-
mantic attributes. It has 15,339 images of 20 Pascal classes
and 12 Yahoo classes. We did not choose SUN [26] dataset
for the reason that the scene images in SUN cannot be eas-
ily decomposed into visual parts which are compact and
consistent across different scenes, and consequently not ex-
pected to beneﬁt from our formulation. The statistics of the
datasets are summarized in Table 2.
Setting. We evaluate performance for both GZSL and ZSL
settings. Following the protocol in [24], in the GZSL set-
ting, the average-class Top-1 accuracy on unseen classes
(ts), seen classes (tr) and the harmonic mean (H) of ts and
tr are evaluated; In the ZSL setting, we report the average-
class Top-1 accuracy on both Standard Split (SS) and Pro-
posed Split (PS).

We examine impact of different concepts such as visual
representation, semantic vs. visual supervision on GZSL
performance. As summarized by Table 3, two variants of
the proposed model are evaluated: (1) Ours(S): We bench-
mark performance of proposed latent visual embedding in
the conventional GZSL setting. That is, we train a map-

2999

Methods

SJE[2]
SAE[12]
SSE[28]
GFZSL[21]
CONSE[19]
ALE[1]
SYNC[6]
DEVISE[10]
PSRZSL[3]
SP-AEN[8]
Ours(S)
Ours(Π)

ts

23.5
7.8
8.5
0.0
1.6
23.7
11.5
23.8
24.6
34.7
33.4
39.5

CUB

tr

59.2
54.0
46.9
45.7
72.2
62.8
70.9
53.0
54.3
70.6
87.5
68.9

H

33.6
13.6
14.4
0.0
3.1
34.4
19.8
32.8
33.9
46.6
48.4
50.2

AWA2

tr

73.9
82.2
82.5
80.1
90.6
81.8
90.5
74.7
73.8
90.9
91.3
88.7

H

14.4
2.2
14.8
4.8
1.0
23.9
18.0
27.8
32.3
37.1
57.2
60.2

ts
8.0
1.1
8.1
2.5
0.5
14.0
10.0
17.1
20.7
23.3
41.6
45.6

ts
3.7
0.4
0.2
0.0
0.0
4.6
7.4
4.9
13.5
13.7
24.5
43.6

aPY

tr

55.7
80.9
78.9
83.3
91.2
73.7
66.3
76.9
51.4
63.4
72.0
78.7

H
6.9
0.9
0.4
0.0
0.0
8.7
13.3
9.2
21.4
22.6
36.6
56.2

Table 1. gZSL learning results on CUB, AWA2 and aPY. ts = test classes (unseen classes), tr = train classes (seen classes), H = harmonical
mean. The accuracy is class-average Top-1 in %. The highest accuracy is in red color and the second is in blue (better viewed in color).

Dataset
CUB[22]
AWA2[24]
aPY[9]

Num Att

312
85
64

Y
200
50
32

O
150
40
20

U
50
10
12

Image
11788
37322
15339

Table 2. Statistics for CUB[22], AWA2[24] and aPY[9]. Number
of semantic attributes, number of class for all(Y), seen(O) and
unseen(U ), and the number of images are listed.

ping to project the learned latent representation Π into the
semantic space S (Eq.11) under the supervision provided
by the semantic oracle, and during test time the semantic
attributes for unobserved classes {so|o 2 O} are also re-
vealed. For this model, solely semantic supervision is lever-
aged, as same as all the competing methods. (2) Ours(Π):
We quantify the drawbacks of semantic information for
GZSL by replacing semantic signals with visual signals Πvo
generated by our visual oracle, which is then used for both
supervision and evaluation.
Training Details. To train our models, we take an alterna-
tive optimization approach where in each epoch, we update
the weights in two steps. In step 1, only the weights of G(·)
is updated by minimizing Lprt.
In step 2, we freeze the
weights of G(·) and update all the other modules. The se-
mantic model (Ours(S)) and the visual oracle is trained by
φXY while Ours(Π) model is trained by φSX + φXY in step
2. Adam optimizer is used to optimize the loss in each step.
The learning rate for step 1 and step 2 is set to 1e-6 and
1e-5, respectively.

Our models are trained for 80, 60 and 70 epochs on CUB,
AWA2 and aPY, respectively. As for the visual oracle, it
is trained to 70 epochs on AWA2 and 60 epochs on CUB
and aPY. The feature extractor E(·) is initialized with Ima-
geNet pretrained weights. The learning rate for V (·) and η
in Eq.(11) is selected via cross-validation. For the optimiza-
tion of Lmix, the EM algorithm is terminated if the loss did
not change or after 300 steps.
Competing Methods. To validate the beneﬁts of the pro-
posed latent visual embedding, we compare against other

Method
Others
Ours(S)
Ours(Π)

Supervision Evaluation Representation

SO
SO
Πvo

S
S
Π

F
Π

Π

Table 3. Comparison of the supervision, evaluation embedding and
feature representations for our model and others. S: semantic em-
bedding; Π: latent visual part similarity embedding; F : raw visual
feature embedding; SO: semantic oracle; Πvo: visual oracle.

state-of-the-art methods which also utilize semantic super-
vision and visual representation. Ten competitors are com-
pared: SJE[2], ALE[1], and DEVISE[10] which use struc-
tured loss to learn a linear compatibility between visual and
semantic space; SSE[28] learns the compatibility function
in a latent common space for visual and semantic embed-
ding; GFZSL[21] models the the class-conditional distribu-
tion as multi-variate Gaussian; CONSE[19] and SYNC[6]
learns maps the unseen image into semantic representa-
tion via combination of seen classes or phantom classes;
SAE[12] learns the mapping from semantic to visual em-
bedding; PSRZSL[3] and SP-AEN[8] try to preserve the
semantic relations in the mapping by encoder-decoder net-
work or adversarial training.

4.1. Generalized Zero Shot Learning Evaluation

The results for the GZSL setting are shown in Table. 1.
Observe that the proposed methods, Ours(S) and Ours(Π)
consistently outperforms state-of-the-art methods in the
GZSL setting. Speciﬁcally, the harmonic mean of the accu-
racy for seen (tr) and unseen (ts) classes with Ours(S) and
Ours(Π) reaches 48.4%, 50.2% on CUB, 57.2%, 60.2% on
AWA2, and 36.6%, 56.2% on aPY, which dominate other
competing methods and often surpass the third-best result
by a very large margin, e.g. a > 20% improvement on
AWA2, and a > 10% improvement on aPY. While several
competing methods (e.g. [21, 12, 19, 28]) only perform well
on the seen classes and obtain close-to-zero accuracy on un-
seen classes, we are able to classify both seen and unseen

3000

CUB

AWA2

aPY

Methods

SS
32.0
8.3
31.1
51.3
25.9
30.9
39.7
35.4

SS
69.5
80.7
67.5
79.3
67.9
80.3
71.2
68.6

SS
55.3
33.4
43.7
53.0
36.7
53.2
54.1
53.2

PS
53.9
33.3
43.9
49.3
34.3
54.9
55.6
52.0
56.0
55.4
66.7
71.9

SJE[2]
SAE[12]
SSE[28]
GFZSL[21]
CONSE[19]
ALE[1]
SYNC[6]
DEVISE[10]
PSRZSL[3]
SP-AEN[8]
Ours(S)
Ours(Π)

PS
32.9
8.3
34.0
38.4
26.9
39.7
23.9
39.8
38.4
24.1
50.1
65.4
Table 4. Zero shot learning results on CUB, AWA2 and aPY. SS =
standard split, PS = proposed split. The results are class-average
Top-1 accuracy in %. The highest accuracy is in red color and the
second is in blue (better viewed in color).

PS
61.9
54.1
61.0
63.8
44.5
62.5
46.6
59.7
63.8
58.5
69.1
84.4

63.7
68.8

90.7
92.4

52.1
54.4

-
-

-
-

-
-

improving upon existing works in the GZSL setting.

State-of-art comparison with Semantic Supervision. Note
that under identical conditions of semantic supervision,
the gain in our method (ours(S)) can be attributed primar-
ily to our latent visual embedding (Π(·)). Different from
the conventional visual representation, which is a high-
dimensional deep CNN feature vector, and not semantically
meaningful, Π(·) intrinsically describes the input image by
a common vocabulary of prototypical parts. These proto-
typical parts are estimated by the latent mixture model with
training images and the components of Π quantify the exis-
tence of a prototypical part in the instance. Such a represen-
tation resembles the semantic similarity of semantic vectors
and leads to mitigating the visual-semantic gap.

Semantic vs. Visual Supervision. Observe that Ours(Π)
consistently achieves better performance than Ours(S), e.g.
1.8%, 3.0% and 19.6% absolute improvement in the har-
monic mean on the three datasets. This comparison shows
that, although the proposed latent visual embedding Π is
able to reduce the Visual!Semantic gap, the semantic at-
tributes are noisy in that they contain information that are
difﬁcult to transfer from the visual domain (e.g. ’smelly’,
’agility’, ’weak’). Using semantic supervision and evalua-
tion for GZSL thus fundamentally limits attaining high ac-
curacy. By switching to visual supervision and evaluation
provided by our visual oracle, we see the potential to fur-
ther improve GZSL accuracy. This comparison is fair since
our visual oracle provides only a list of similarity scores
without any other identifying high-dimensional features to
the learner. This is similar to the case of a semantic oracle
providing attribute annotations.

Issue with aPY. Finally, observe that on aPY most existing
methods fail to recognize unseen classes achieving nearly
zero accuracy, while we get a signiﬁcant improvement from
Ours(S) and Ours(Π). The reason is that aPY attributes

Figure 3. Example of types in each part on CUB dataset. Each two
rows belong to a part and each two columns belong to a type. In m-
th part, the example can be labeled with a scalar π(k|m) indicating
its probability of belonging to the k-th type. The example in the
type has the largest π(k|m) among all types. Note that these types
are semantically meaningful and visually distinguishable.

are extremely noisy and are not visually representative (e.g.
bus-car attributes nearly identical). Thus, the semantic su-
pervision cannot provide useful information for training a
GZSL model.

4.2. Zero Shot Learning Evaluation

We next evaluate the results for the traditional ZSL set-
ting, where only unseen classes are included during test-
ing. The results are reported in Table 4. Observe that many
competing methods, which are ineffective in the GZSL set-
ting (e.g. [21, 12, 19, 28]), realize a huge performance gain
in the ZSL setting. However, Ours(S) and Ours(Π) is ro-
bust and still outperforms the competing methods. Again,
this can be attributed to our proposed latent visual embed-
ding. Ours(S) model consistently obtains superior perfor-
mance, improving the state-of-the-art accuracy from 55.3%
to 63.7% for SS on CUB, from 80.7% to 90.7% for SS on
AWA2, and from 51.3% to 52.1% for SS on aPY. A similar
improvement can also be observed on the PS split. This
observation shows that, with the same level of semantic
supervision, our low-dimensional latent visual embedding
is more semantically meaningful than the traditional high-
dimensional visual features, and thus effectively bridges the
Visual!Semantic gap. Additionally, with the visual super-
vision provided by our visual oracle, Our(Π) obtains even
better performance on all datasets and splits, which reveals
the drawbacks of leveraging primarily semantic attributes
for supervision and evaluation.

4.3. Analysis and Discussion

Visualization of discovered Latent Prototypical Part
Types. To verify that our parameterized Π is able to learn

3001

CUB

AWA2

aPY

Methods

ts

PS
Ours(Πf lat) 38.4 69.8 49.6 66.8 69.5 42.6 88.7 57.6 91.7 84.0 36.5 88.7 51.7 53.6 62.9
Ours(Π)
39.5 68.9 50.2 68.8 71.9 45.6 88.7 60.2 92.4 94.4 43.6 78.7 56.2 54.4 65.4

SS

PS

SS

PS

SS

H

H

H

ts

ts

tr

tr

tr

Table 5. Test accuracy for different visual representations on CUB, AWA2 and aPY. Ours(Πf lat): our model with ﬂatten visual represen-
tation. Ours(Π): our model with structured visual representation. The accuracy class-average Top-1 in %. Both gZSL (ts, tr, H) and ZSL
(SS, PS) performances are reported.

Methods

CUB

AWA2

ts

tr

H

ts

tr

H

DEVISE (S)
23.8 53.0 32.8 17.1 74.7 27.8
DEVISE (Πvo) 24.6 53.3 33.7 28.3 75.3 41.2

Table 6. gZSL performances for DEVISE[10] using semantic su-
pervision and visual oracle supervision (Πvo).

diverse parts and discriminating types, we visualize some
exemplar parts and types from CUB dataset in Figure 3. We
observe that the examples in each type are visually similar
to each other, but distinguishable to humans across differ-
ent classes. When provided with the examples in each type,
humans can score the existence of a type, i.e., πx(k|m),
thereby bypassing the proposed visual oracle. Noticeably,
the part feature model fm(·) is able to detect some semantic
parts. For example, in Figure 3, Part-1 detects the face (or
eye), Part-2 detects the beak and Part-3 tends to detect the
body texture representation of birds. These semantic parts
are easier to be linked to the semantic attributes, and hence
our visual semantic embedding is able to close the gap be-
tween the high-dimensional visual feature and the semantic
space.

It is worth noting that in Part-4, different semantic parts,
like head, chin, wins and legs, are discovered in differ-
ent types. We ﬁnd it reasonable since the same semantic
parts may not appear in different classes. The situation
could be even more common in coarse-grained recognition,
like a chair is not likely to have an engine. Moreover, our
model tries to learn the most discriminating part via the loss
φXY (fm(x), y). The same semantic part which is the most
discriminating to one class is possibly not important to an-
other class. This phenomenon also won’t cause any problem
for supervision because the visual oracle is based on visual
features, while for a human being this is unlikely to arise.

Structured vs. Flat Visual Supervision In our formula-
tion, both the feature model fm(x) and the mixture model
Π(x) are structured. That is, each part feature fm(x) is rep-
resented by a unique mixture model Πm conditioned on the
part m. The visual oracle’s supervision Πvo is also struc-
tured in a similar way. However, a different strategy is to
take a ﬂat representation and supervision: drop the part-
based representation by replacing fm(x) with the global
feature E(x), and collapse the structured Πvo into a sin-
gle list representation. Such a ﬂat supervision requires no
part-wise features and its result is reported in Table 5. We
observe that ﬂattening the latent structured visual embed-
ding as a single vector, which mirrors the common usage of

semantic attributes, suffers from a slight performance drop
from Ours(Π) since the rich part structure information is
lost. However, note that, compared to the noisy semantic
supervision (see competitors in Table 1 and 4), the ﬂat vi-
sual supervision still dominates competing methods.
Visual vs. Semantic Supervision. To further justify the
effectiveness of the proposed latent visual embedding for
GZSL supervision, we took an existing state-of-the-art ap-
proach, DEVISE[10], and re-trained it under the visual su-
pervision Πvo. As shown in Table 6, the proposed visual
supervision also boosts DEVISE’s GZSL performance, es-
pecially on AWA2, e.g. a 13.4% absolute improvement in
the harmonic mean. The result demonstrates that the pro-
posed latent visual embedding, as a supervision type, is ef-
fective and generalizable even to non-attention methods.

5. Conclusion

In this paper we proposed a novel Zero-Shot learning
(ZSL) method. Our method unlike many existing works
neither synthesizes unseen examples nor uses any unseen
semantic information during training. We claim that se-
mantic gap exists because visual features employed in prior
work is not semantic leading to signiﬁcant drop in accu-
racy. To bridge this semantic gap we proposed a new sta-
tistical model for embedding a visual instance into a low-
dimensional probability matrix. Our insight is based on the
fact that analogous to how a semantic component measures
the likeliness of the attribute arising in an object, so also,
our mixture component conveys visual likeliness by scor-
ing how similar a part type is relative to proto-typical part
types of other instances in the training set. To further reduce
semantic noise we propose a novel visual oracle for super-
vision in lieu of semantic supervision. We tabulate results
on a number of benchmark datasets demonstrating signiﬁ-
cant improvement in accuracy over state-of-art under both
semantic and visual supervision.

Acknowledgement

The authors would like to thank the Area Chair and the
reviewers for their constructive comments. This work was
supported by the Ofﬁce of Naval Research Grant N0014-18-
1-2257, NGA-NURI HM1582-09-1-0037 and the U.S. De-
partment of Homeland Security, Science and Technology
Directorate, Ofﬁce of University Programs, under Grant
2013-ST-061-ED0001.

3002

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-
embedding for image classiﬁcation.
IEEE transactions on
pattern analysis and machine intelligence, 38(7):1425–1438,
2016. 6, 7

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2927–2936, 2015. 6,
7

[3] Y. Annadani and S. Biswas. Preserving semantic relations
for zero-shot learning. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018. 2, 6, 7

[4] S. Antol, C. L. Zitnick, and D. Parikh. Zero-shot learning
via visual abstraction. In ECCV, pages 401–416. Springer,
2014. 1

[5] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain. Sparse
local embeddings for extreme multi-label classiﬁcation. In
NIPS, 2015. 1

[6] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-
sized classiﬁers for zero-shot learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5327–5336, 2016. 6, 7

[7] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-
ical study and analysis of generalized zero-shot learning for
object recognition in the wild. In ECCV, 2016. 1, 2

[8] L. Chen, H. Zhang, J. Xiao, W. Liu, and S.-F. Chang. Zero-
shot visual recognition using semantics-preserving adversar-
ial embedding network. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, volume 2,
2018. 2, 6, 7

[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing
objects by their attributes. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages
1778–1785. IEEE, 2009. 5, 6

[10] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In Advances in neural information processing
systems, pages 2121–2129, 2013. 2, 6, 7, 8

[11] H. Jiang, R. Wang, S. Shan, and X. Chen. Learning class
prototypes via structure alignment for zero-shot recognition.
In The European Conference on Computer Vision (ECCV),
September 2018. 2

[12] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder for
zero-shot learning. arXiv preprint arXiv:1704.08345, 2017.
2, 6, 7

[13] D. Koller and N. Friedman. Probabilistic Graphical Mod-
els: Principles and Techniques - Adaptive Computation and
Machine Learning. The MIT Press, 2009. 1

[14] V. Kumar Verma, G. Arora, A. Mishra, and P. Rai. Gener-
alized zero-shot learning via synthesized examples. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018. 2

[15] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. PAMI, 36(3):453–465, 2014. 1, 2, 3

[16] C.-W. Lee, W. Fang, C.-K. Yeh, and Y.-C. Frank Wang.
Multi-label zero-shot learning with structured knowledge
graphs.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 2

[17] Y. Li, D. Wang, H. Hu, Y. Lin, and Y. Zhuang. Zero-
shot recognition using dual visual-semantic mapping paths.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017. 2

[18] Y. Li, J. Zhang, J. Zhang, and K. Huang. Discriminative
learning of latent features for zero-shot recognition. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018. 2

[19] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning by
convex combination of semantic embeddings. arXiv preprint
arXiv:1312.5650, 2013. 6, 7

[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge, 2014. 1

[21] V. K. Verma and P. Rai. A simple exponential family frame-
work for zero-shot learning.
In Joint European Confer-
ence on Machine Learning and Knowledge Discovery in
Databases, pages 792–808. Springer, 2017. 6, 7

[22] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical re-
port, 2011. 5, 6

[23] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition via
semantic embeddings and knowledge graphs. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018. 2

[24] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-
shot learning-a comprehensive evaluation of the good, the
bad and the ugly. IEEE transactions on pattern analysis and
machine intelligence, 2018. 1, 2, 5, 6

[25] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata. Feature gener-
ating networks for zero-shot learning. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2018. 2

[26] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva.
Sun database: Exploring a large collection of scene cate-
gories. Int. J. Comput. Vision, 119(1):3–22, Aug. 2016. 5

[27] L. Zhang, T. Xiang, and S. Gong. Learning a deep embed-
ding model for zero-shot learning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017. 3

[28] Z. Zhang and V. Saligrama. Zero-shot learning via seman-
tic similarity embedding. In Proceedings of the IEEE inter-
national conference on computer vision, pages 4166–4174,
2015. 2, 6, 7

[29] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention
convolutional neural network for ﬁne-grained image recog-
nition. In Int. Conf. on Computer Vision, volume 6, 2017.
4

[30] Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal. A
generative adversarial approach for zero-shot learning from
noisy texts. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 2

3003

