AE2-Nets: Autoencoder in Autoencoder Networks

Changqing Zhang1∗, Yeqing Liu1 ∗, Huazhu Fu2

1College of Intelligence and Computing, Tianjin University, Tianjin, China

2Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE

{zhangchangqing; yeqing}@tju.edu.cn; hzfu@ieee.org

Abstract

Learning on data represented with multiple views (e.g.,
multiple types of descriptors or modalities) is a rapidly
growing direction in machine learning and computer vi-
sion. Although effectiveness achieved, most existing algo-
rithms usually focus on classiﬁcation or clustering tasks.
Differently, in this paper, we focus on unsupervised repre-
sentation learning and propose a novel framework termed
Autoencoder in Autoencoder Networks (AE2-Nets), which
integrates information from heterogeneous sources into an
intact representation by the nested autoencoder framework.
The proposed method has the following merits:
(1) our
model jointly performs view-speciﬁc representation learn-
ing (with the inner autoencoder networks) and multi-view
information encoding (with the outer autoencoder network-
s) in a uniﬁed framework; (2) due to the degradation pro-
cess from the latent representation to each single view, our
model ﬂexibly balances the complementarity and consis-
tence among multiple views. The proposed model is efﬁ-
ciently solved by the alternating direction method (ADM),
and demonstrates the effectiveness compared with state-of-
the-art algorithms.

1. Introduction

Real-world data are usually described with multiple
modalities or multiple types of descriptors that are consid-
ered as multiple views. Basically, due to the diversity of
sensors or feature extractors, these different views are usu-
ally highly heterogeneous. For example, an image may be
described with color (e.g., color histogram) and texture de-
scriptors (e.g., SIFT [18], GIST [21], HOG [7]). In social
networks, there usually exist both link graph describing re-
lationships between different subjects and subject-speciﬁc
attributes [31, 28]. In medical image analysis [10], a sub-
ject may be associated with different types of medical im-
ages used to capture different characteristics of anatomical

∗Changqing Zhang and Yeqing Liu contributed equally to this work.

structures. Accordingly, plenty of approaches have been
proposed to jointly exploit multiple types of features [9] or
multiple modalities of data [26, 20].

Most existing multi-view learning algorithms focus on
classiﬁcation [13, 4] or clustering [5, 16, 32]. Basically,
integrating different views into one comprehensive repre-
sentation is of vital importance for downstream tasks since
uniﬁed representation could be easily exploited by on-shelf
algorithms. Although it is important, jointly exploring mul-
tiple views is a long-standing challenge due to the complex
correlations underlying different views. The representative
way of learning a common representation is Canonical Cor-
relation Analysis (CCA) [14], which searches for two pro-
jections to map two views onto a low-dimensional com-
mon subspace where the linear correlation between the two
views is maximized. Then the learned representation can be
used for subsequent tasks (e.g., classiﬁcation or clustering).
To address more complex correlations beyond linear case,
kernelized CCA (KCCA) [1] introduces kernel techniques.
Furthermore, Deep Canonical Correlation Analysis (DC-
CA) [2] proposes learning highly nonlinear mappings with
deep neural networks to search for a common space that
could maximize the correlations between two views. Be-
yond CCA-based methods, Partial Least Squares (PLS) re-
gression [25] regresses the samples from one view to anoth-
er and the ﬂexible multi-view dimensionality co-reduction
algorithm (MDcR) [33] maximizes the correlations between
different views in kernel space.

Although effectiveness has been achieved on multi-view
learning, there are several main problems left for existing
algorithms. First, previous algorithms usually project dif-
ferent views onto a common space under the underlying
assumption that there exist sufﬁcient correlations between
different views. However, in practice, correlation (consis-
tence) and independence (complementarity) are co-existing
and it is challenging to automatically balance them. Ac-
cordingly, existing algorithms either maximize the correla-
tions [2, 16] for consistence or maximize the independence
for complementarity [5]. Second, existing algorithms usual-
ly project each view onto a low-dimensional space and then

2577

combine all of them for subsequent tasks rather than learn
a common low-dimensional representation, which makes it
a two-step manner in representation learning. Therefore,
in this paper, we propose the Autoencoder in Autoencoder
Networks (AE2-Nets), which aims to automatically encode
intrinsic information from heterogeneous views into a com-
prehensive representation and adaptively balance the com-
plementarity and consistence among different views.

The key advantage of the proposed model lies in the join-
t view-speciﬁc encoding and multi-view encoding with a
novel nested autoencoder networks. The view-speciﬁc rep-
resentation encoded by the inner-AE networks is responsi-
ble for reconstructing the raw input, while the multi-view
representation encoded by the outer-AE networks can re-
construct the encoded representation by inner-AE network
of each single view. The main contributions of this paper
are summarized as follows:
• We propose a novel unsupervised multi-view represen-
tation learning framework - Autoencoder in Autoencoder
Networks (AE2-Nets) for heterogeneous data, which can
ﬂexibly integrate multiple heterogeneous views into an in-
tact representation.
• The novel nested autoencoder networks could jointly per-
form view-speciﬁc representation learning and multi-view
representation learning - the inner autoencoder networks ef-
fectively extract information from each single view, while
the outer autoencoder networks model the degradation pro-
cess to encode intrinsic information from each single view
into a common intact representation.
• Extensive experimental results verify the effectiveness of
the proposed AE2-Nets on diverse benchmark datasets for
both classiﬁcation and clustering tasks.

The remainder of the paper is organized as follows. Re-
lated algorithms, including multi-view learning and multi-
view representation learning are brieﬂy reviewed in Section
2. Details of our proposed approach are presented in Sec-
tion 3. In Section 4, we present experimental results that
demonstrate the effectiveness of our model on a variety of
real-world datasets. Conclusions are drawn in Section 5.

2. Related Work

Learning based on data with multiple modalities or mul-
tiple types of features aims to conduct learning task by joint-
ly utilizing different views to exploit the complementarity,
and has attracted intensive attentions recently. For super-
vised learning, multimodal metric learning [34, 35] usual-
ly jointly learns multiple metrics for different modalities.
Hierarchical Multimodal Metric Learning (HM3L) [35] de-
composes the metric of each modality into a product of
two matrices: one is modality-speciﬁc, and the other is
shared by all the modalities. Beyond linear case, Fisher-
HSIC Multi-View Metric Learning (FISH-MML) [34] en-
forces the class separability with Fisher discriminant analy-

sis (FDA) within each view, and maximizes the consistence
in kernel space among multiple views by using Hilbert-
Schmidt Independence Criteria (HSIC). Under the proba-
bilistic framework, the method [30] learns latent represen-
tations and distance metric from multiple modalities with
the multi-wing harmonium (MWH) learning. There are also
some methods [22, 23] aggregating decisions from multiple
classiﬁers, where each classiﬁer is learned based upon one
single modality. Under speciﬁc assumptions, theoretical re-
sults [11, 6] have advocated the advantages of multi-view
integration for subsequent tasks. For clustering, based on
spectral clustering, co-regularized [16] and co-training [15]
based algorithms enforce clustering hypothesis of different
views to be consistent. Recently, the multi-view subspace
clustering methods [5, 12] relate different data points in a
self-representing manner on the original view and simulta-
neously constrain these subspace representations of differ-
ent views to exploit complementary information. There are
some multi-view methods focusing on other topics, e.g., di-
mensionality reduction [33].

Unsupervised multi-view representation learning is a
rather challenging problem since there is no class infor-
mation guiding the learning process. The main stream of
methods are CCA-based, which searches for projections to
maximize the correlation of two views. Due to the ability of
handling nonlinear correlations, the kernel extension of C-
CA has been widely used for integrating multi-view features
or dimensionality reduction. The Deep CCA [2] aims to
learn two deep neural networks (DNN) to maximize canon-
ical correlation across two views. Under the deep learn-
ing framework, the autoencoder based model [20] learns a
compact representation best reconstructing the input. Dif-
ferent from CCA, based on HSIC, a ﬂexible multi-view di-
mensionality co-reduction method [33] is proposed which
explores the correlations within each view independently,
and maximizes the dependence among different views with
kernel matching jointly. Inspired by deep learning, semi-
nonnegative matrix factorization is extended to obtain the
hierarchical semantics from multi-view data in a layer-wise
manner [36]. The learned representations of all views are
enforced to be the same in the ﬁnal layer.

3. Autoencoder in Autoencoder Networks

In this section, we present the AE2-Nets for learning
the intact representations with a set of multi-view samples
X = {X(1), ..., X(V )}, where X(v) ∈ Rdv×n is the feature
matrix of the vth view with V , n and dv being the number
of views, number of samples and dimensionality of feature
space for the vth view, respectively.

3.1. Proposed Approach

The key goal of AE2-Nets (as presented in Fig. 1) is to
recover an intact latent space which can well reveal the un-

2578

Figure 1: Overview of the Autoencoder in Autoencoder Networks (AE2-Nets). The key components are the nested autoen-
coder networks, which are composed of the inner AE networks (shown as the circle with green arrows) for view-speciﬁc
encoding and the outer AE networks (shown as the circle with red arrows) for multi-view encoding. View-speciﬁc encod-
ing automatically extracts features from each view while multi-view encoding ensures the intact latent representation can be
mapped back to each view with degradation process. Accordingly, the intrinsic information from multiple views are encoded
into the learned latent intact representation. The learned latent representation could be used for subsequent tasks, and the
task-speciﬁc goal could ﬂexibly be incorporated into our framework as well (shown in gray dash lines).

derlying structure of data across multiple views. The pro-
posed model jointly learns compact representation for each
single view and the intact multi-view representation which
can be mapped to reconstruct each single view. Then, the
intrinsic information of each view are automatically extract-
ed with the inner-AE networks, and the degradation pro-
cess involved in the outer-AE networks ensures the intrin-
sic information from each view are encoded into the latent
representation. Note that, due to the common intact repre-
sentation and associated non-linear networks, more general
correlations among different views are addressed.

For the inner networks, the reasons of using AE network-
s are: (1) since there is no supervised information guid-
ing the learning process, we employ AE networks instead
of general neural networks (e.g., for classiﬁcation) to en-
sure the intrinsic information to be preserved; (2) for con-
ventional multi-view representation learning models, learn-
ing processes are usually based on the pre-extracted fea-
tures, which is risky due to the high-dimensionality and
possible noise involved. The introduced encoding network-
s could extract intrinsic information to be encoded into
the latent multi-view representation instead of the original
high-dimensional/noisy features; (3) with variants of AE
(e.g., convolutional autoencoder for images), our model has
the potential of performing representation learning directly
based on raw data.

For simplicity,

the inner-AE network for

view is denoted as f (X(v); Θ(v)

ae ), where Θ(v)

the vth
ae =

ae

ae

, b(m,v)

{W(m,v)
}M
m=1 is the parameter set for all layers
with M + 1 being the number of layers of the inner-AE
network, i.e., consisting of M layers of nonlinear transfor-
mations. Speciﬁcally, the ﬁrst M/2 hidden layers encode
the input as a new representation, and the last M/2 lay-
ers decode the representation to reconstruct the input. Let
z(0,v)
i ∈ Rdv denote an input feature vector, then the
i
output of the mth layer is

= x(v)

= a(W(m,v)

z(m,v)
i
m = 1, 2, · · · , M,

ae

z(m−1,v)
i

+ b(m,v)

ae

),

(1)

i

where z(m,v)
∈ Rd(m,v) and d(m,v) is the number of
nodes at the mth layer for the vth view. W(m,v)
∈
Rd(m,v)×d(m−1,v) and b(m,v)
∈ Rd(m,v) denote the weights
and bias associated with the mth layer, respectively. a(·)
is a nonlinear activation function. Then, given the feature
matrix X(v) = [x(v)
n ] ∈ Rdv×n for the vth
view, the corresponding reconstruct representation is denot-
ed as

2 , · · · , x(v)

1 , x(v)

ae

ae

Z(M,v) = [z(M,v)

1

, z(M,v)

2

, · · · , z(M,v)

n

],

(2)

i

where z(M,v)
is the reconstructed representation for the ith
sample in the vth view. To obtain the low-dimensional rep-
resentation Z( M
2 ,v), we should minimize the following re-

2579

HGraphImageOuter AE: Multi-view encoding Inner AE:View-specific encoding (a) Heterogeneous inputTask-specific goal ...(b) AE networks(c) Degradation networks (d) Latent representation(e) Taskconstruction loss

min
(v)
ae }V

{Θ

v=1

1
2

V

Xv=1(cid:13)(cid:13)(cid:13)

X(v) − Z(M,v)(cid:13)(cid:13)(cid:13)

2

F

.

(3)

After obtaining the low-dimensional view-speciﬁc rep-
resentation Z( M
2 ,v), we focus on encoding them into one
intact common representation, H ∈ Rk×n, where k is the
dimensionality of the intact space, to preserve intrinsic in-
formation from different views. To this end, the degra-
dation networks involved in the outer-AE networks real-
ize the assumption that each single view could be recon-
structed from the comprehensive (or intact) common rep-
resentation. The fully connected neural networks (FC-
NN) are employed to model the degradation process as
shown in Fig. 1(c). Speciﬁcally, we map H onto the view-
speciﬁc representation Z( M
2 ,v) with degradation network
g(H; Θ(v)
l=1 with L + 1
being the number of layers of degradation network. Ac-
cordingly, we have G(0,v) = H as the input of the degra-
dation networks and G(l,v) = [g(l,v)
], with
+ b(l,v)
g(l,v)
). Then, the objective of
i
degradation networks is deﬁned as

dg ), where Θ(v)

dg = {W(l,v)

dg g(l−1,v)

, · · · , g(l,v)

= a(W(l)

dg }L

, b(l,v)

dg

dg

n

1

i

min
(v)
dg }V

v=1

{Θ

1
2

V

Xv=1(cid:13)(cid:13)(cid:13)

Z( M

2 ,v) − G(L,v)(cid:13)(cid:13)(cid:13)

2

F

.

(4)

In our model, we jointly learn new representation for
each view (with inner-AE networks) and seek the intact la-
tent representation (with outer-AE networks) in a uniﬁed
framework, and then the objective of our AE2-Nets is in-
duced as

min

(v)
ae ,Θ

(v)
dg }V

v=1,H

{Θ

1
2

V

Xv=1(cid:0)(cid:13)(cid:13)(cid:13)

2

F

X(v) − Z(M,v)(cid:13)(cid:13)(cid:13)
+ λ(cid:13)(cid:13)(cid:13)

Z( M

2 ,v) − G(L,v)(cid:13)(cid:13)(cid:13)

2

F (cid:1),

(5)
where λ > 0 is a tradeoff factor to balance the within-view
reconstruction and cross-view reconstruction (from the la-
tent representation to each single view). For all views,
G(L,v)s are derived from the common latent representa-
tion H. The proposed model automatically learns view-
speciﬁc representations and nonlinearly encodes them into
the multi-view intact representation. It is noteworthy that
although the proposed AE2-Nets is an unsupervised repre-
sentation learning model, it is easy to extend AE2-Nets to
meet speciﬁc tasks (e.g., classiﬁcation or clustering). More-
over, our model is applicable for the data with more than
two views.

3.2. Optimization

There are multiple blocks of variables in our problem,
and the objective function of our AE2-Nets is not jointly

convex for all these variables. Therefore, we optimize our
objective function by employing Alternating Direction Min-
imization (ADM) [17] strategy. To adopt the ADM strate-
gy, the optimization is cycled over the following three steps:
updating the view-speciﬁc auto-encoder networks, updating
the degradation networks and updating the latent represen-
tation H by ﬁxing the other blocks of variables. The opti-
mization for each step is as follows:
• Update View-Speciﬁc AE Networks. To update the
view-speciﬁc AE network for the vth view, we should min-
imize the following loss function

ae ({Θ(v)

ae }V

v=1) =

x(v)
i − z(M,v)

i

2 ,v)

( M
i

z

− g(L,v)

i

2

(cid:13)(cid:13)(cid:13)

+ λ(cid:13)(cid:13)(cid:13)

(6)

2

(cid:1).

(cid:13)(cid:13)(cid:13)

By applying the chain rule to calculate the gradient of E-
q. (6) w.r.t. W(m,v)

and b(m,v)

, we have

ae

ae

= (∆(m,v) + λΛ(m,v))(z(m−1,v)

i

)T ,

(7)

= ∆(m,v) + λΛ(m,v),

L(v)
n

1
2

Xi=1(cid:0)(cid:13)(cid:13)(cid:13)



ae

∂L(v)

ae

ae

∂W(m,v)
∂L(v)
∂b(m,v)

ae

where ∆(m,v) is deﬁned as

∆(m,v) =

(

−(x(v)
(W(m+1,v)

ae

i − z(m,v)

i

) ⊙ a′(y(m,v)

i

), m = M,

)T ∆(m+1,v) ⊙ a′(y(m,v)

i

), otherwise,

(8)

and Λ(m,v) is given by

Λ(m,v) =




(W(m+1,v)
2 ,v)

ae
(z

( m
i

)T Λ(m+1,v) ⊙ a′(y(m,v)
i
( m
2 ,v)
− g(L,v)
i

) ⊙ a′(y

i

), m ≤ M
2 − 1,
), m = M
2 ,

0, m ≥ M

2 + 1.

(9)
where a′(·) is the derivative of the activation function a(·),
⊙ denotes the element-wise multiplication, and y(m,v)
=
W(m,v)
. Then we can update the param-
eters {W(m,v)

M
}
m=1 with gradient descent as

z(m−1,v)
i

+ b(m,v)

, b(m,v)

ae

ae

ae

ae

i

W(m,v)

ae = W(m,v)

ae − µ

∂L(v)

ae

∂W(m,v)

ae

,

b(m,v)
ae = b(m,v)

ae − µ

ae

∂L(v)
∂b(m,v)

ae

,

(10)




where µ > 0 is the learning rate which is usually set to a
small positive value, e.g., 0.001.
•Update Degradation Networks. Similar to the update
strategy for the view-speciﬁc AE networks, we can obtain

2580

the gradient of Eq. (4) w.r.t. W(l,v)
view as

dg

and b(l,v)

dg

for the vth

dg

∂L(v)
∂W(l,v)

dg

= Υ(l,v)(g(l−1,v)

i

)T ,

where Υ(l,v) is deﬁned as

dg

∂L(v)
∂b(l,v)

dg

= Υ(l,v),

(11)

Υ(l,v) =(

−(z

2 ,v)

( M
i

(W(l+1,v)

dg

− g(l,v)

) ⊙ a′(q(l,v)
)T Υ(l+1,v) ⊙ a′(q(l,v)

i

i

i

), l = L
), otherwise
(12)
. Accordingly, we

where q(l,v)
can update the weights and bias with the following rule

dg g(l−1,v)

= W(l,v)

+ b(l,v)

dg

i

i

W(l,v)

dg = W(l,v)

dg − µ

dg

∂L(v)
∂W(l,v)

dg

dg = b(l,v)
b(l,v)

dg − µ

dg

∂L(v)
∂b(l,v)

dg

.

,

(13)




•Update Latent Representation H. To update the intact
latent representation H, we follow the similar way as up-
dating W(1,v)
. That is to say, we should optimize Eq. (4)
w.r.t. H. Accordingly, we can calculate the gradient as

dg

a′(q(l,v)

i

) ⊙ W(l,v)

dg

,

∂Lh
∂hi

=

α(v)(g(L,v)

i

− z

2 ,v)

( M
i

) ⊙

with Lh =

2 ,v)

( M
i

z

− g(L,v)

i

V

Xv=1
Xv=1

V

α(v)

2 (cid:13)(cid:13)(cid:13)

L

2

Yl=1
(cid:13)(cid:13)(cid:13)

(14)
where α(v) is a tradeoff factor to control the belief degree
for the vth view. In practice, we can set α(1) = · · · = α(V )
when there is no prior about the importance of each view.
For clariﬁcation, we summarize the optimization procedure
in Algorithm 1.

3.3. Connection with CCA/Matrix Factorization

ae ) = g(h; Θ(v)

CCA can be interpreted as a generative model [29, 3].
With a latent representation, h, the observations x(1) =
P(1)h + ǫ(1) and x(2) = P(2)h + ǫ(2), where P(1) and
P(2) are linear mappings, ǫ(1) and ǫ(2) are independent
Gaussian noise. For our AE2-Nets, the underlying model
is f (x(v); Θ(v)
dg ) + ε(v), where f (·) encodes
original features of each view into a compact representa-
tion and g(·) degrades the intact representation into each
single view. ε(v) is the error for the vth view. By ﬁxing
the features instead of learning by autoencoder networks,
and replacing g(h; Θ(v)
dg ) with linear projections, our mod-
i −
P(v)hi||2. This is similar to the generative model of CCA,
and is also equivalent to learning a common representation
under the matrix factorization framework.

el will be degraded into: min{P(v),H}PV

v=1Pn

i=1 ||x(v)

Algorithm 1: Optimization algorithm of AE2-Nets

Input: multi-view data X = {X(v)}V

v=1,

dimensionality k of latent representation H.

Initialize randomly {Θ(v)
while not converged do

ae , Θ(v)

dg }V

v=1 and H.

for each of V views do

update the parameters of view-speciﬁc AE
networks with Eq. (10);

end
for each of V views do

update the parameters of the degradation
networks with Eq. (13);

end
update H with Eq. (14);

end
Output: latent representation H.

4. Experiments

In the experiments, we compare the proposed AE2-
Nets with state-of-the-art multi-view representation learn-
ing methods on real-world datasets with multiple views,
and evaluate the results on both clustering and classiﬁcation
tasks with commonly used evaluation metrics.

4.1. Experimental Settings

Datasets. We conduct the comparisons on the following
datasets: handwritten1 contains 2000 images of 10 class-
es from number 0 to 9. Two different types of descriptors,
i.e., pix (240 pixel averages in 2 x 3 windows) and fac (216
proﬁle correlations), are used as two views. Caltech101-
72 contains a subset of images from Caltech101. There are
7 categories selected with 1474 images: faces, motorbikes,
dollar-bill, garﬁeld, snoopy, stop-sign, and windsor-chair.
The HOG and GIST descriptors are used. ORL3 contains
10 different images for each of 40 distinct subjects. COIL-
20 4 contains 1440 images of 20 object categories. Each
image is normalized to 32 × 32 with 256 gray levels per
pixel. For ORL and COIL-20, intensity of gray level and
Gabor descriptors are used. Caltech-UCSD Birds (CUB)
5 contains 11788 bird images associated with text descrip-
tions [24] from 200 different categories. We extract 1024-
dimensional features based on images with GoogLeNet, and
300-dimensional features based on text.

Compared methods. We compared the proposed AE2-

Nets with the following methods:
(1) FeatConcate: This method simply concatenates differ-

1https://archive.ics.uci.edu/ml/datasets/Multiple+Features
2http://www.vision.caltech.edu/Image Datasets/Caltech101/
3https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
4http://www.cs.columbia.edu/CAVE/software/softlib/
5http://www.vision.caltech.edu/visipedia/CUB-200.html

2581

Table 1: Performance comparison on clustering task.

Datasets

Methods

ACC

NMI

F score

RI

handwritten

Caltech101

ORL

COIL20

CUB

FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours

76.04 ± 2.28
66.43 ± 7.62
66.26 ± 0.16
69.17 ± 1.02
76.72 ± 2.77
71.86 ± 4.25
81.52 ± 1.62
47.23 ± 0.22
45.37 ± 0.09
56.71±10.50
62.11 ± 2.78
46.51 ± 0.67
55.75 ± 5.67
66.46 ± 4.55
61.10 ± 1.51
56.98 ± 2.06
59.68 ± 2.04
59.40 ± 2.20
61.70 ± 2.19
65.38 ± 2.86
68.85 ± 2.11
67.13 ± 4.09
58.68 ± 1.34
63.73 ± 0.78
62.72 ± 1.40
64.25 ± 2.98
53.92 ± 5.89
73.42 ± 1.90
73.80 ± 0.11
45.82 ± 1.58
54.50 ± 0.29
66.70 ± 1.52
73.68 ± 3.32
37.50 ± 2.45
77.75 ± 1.63

75.70 ± 1.44
69.62 ± 6.06
66.01 ± 0.45
66.96 ± 0.91
76.68 ± 0.93
73.09 ± 3.23
71.39 ± 1.50
57.19 ± 0.61
50.53 ± 0.03
57.61 ± 6.78
64.38 ± 4.11
56.43 ± 0.56
45.52 ± 2.28
60.60 ± 1.93
79.28 ± 0.70
76.03 ± 0.79
77.84 ± 0.83
77.52 ± 0.86
79.45 ± 1.20
82.87 ± 1.26
85.73 ± 0.78
79.94 ± 1.69
70.64 ± 0.47
76.02 ± 0.50
76.32 ± 0.66
79.44 ± 1.37
72.36 ± 2.11
82.55 ± 1.03
71.49 ± 0.24
46.59 ± 0.98
52.53 ± 0.19
65.76 ± 1.36
74.49 ± 0.75
37.82 ± 2.04
78.61 ± 1.62

70.96 ± 2.05
62.05 ± 7.70
59.05 ± 0.39
60.50 ± 1.10
71.93 ± 2.22
66.66 ± 4.69
68.57 ± 1.86
52.15 ± 0.28
52.15 ± 0.19
62.32±12.75
65.43 ± 4.24
51.55 ± 0.56
55.67 ± 5.50
73.42 ± 4.91
47.03 ± 2.21
45.13 ± 1.83
47.72 ± 2.05
46.71 ± 2.22
48.48 ± 2.59
52.01 ± 3.43
59.93 ± 1.31
64.81 ± 4.05
53.13 ± 0.90
58.76 ± 0.53
57.56 ± 1.15
63.60 ± 2.57
46.39 ± 4.97
69.38 ± 1.92
61.07 ± 0.18
39.93 ± 1.27
45.84 ± 0.31
58.22 ± 1.18
65.72 ± 1.37
28.95 ± 1.54
70.96 ± 2.63

93.93 ± 0.42
91.83 ± 1.79
91.39 ± 0.06
91.77 ± 0.21
94.11 ± 0.48
92.85 ± 1.13
93.68 ± 0.38
73.45 ± 0.16
73.27 ± 0.09
76.34 ± 6.86
79.31 ± 2.06
73.27 ± 0.30
73.43 ± 2.33
83.14 ± 2.33
97.10 ± 0.25
97.32 ± 0.09
97.42 ± 0.13
97.39 ± 0.14
97.28 ± 0.22
97.29 ± 0.30
97.94 ± 0.11
96.24 ± 0.60
95.18 ± 0.10
95.60 ± 0.06
95.27 ± 0.30
96.11 ± 0.29
92.56 ± 1.46
96.86 ± 0.22
91.98 ± 0.04
87.44 ± 0.31
88.61 ± 0.06
91.27 ± 0.24
92.75 ± 0.44
85.52 ± 0.26
93.92 ± 0.58

ent types of features from multiple views.
(2) CCA: Canonical Correlation Analysis (CCA) [14] maps
multiple types of features onto one common space by ﬁnd-
ing linear combinations of variables that maximally corre-
lation, and then combines these projected low-dimensional
features together.
(3) DCCA: Deep Canonical Correlation Analysis (DCCA)
[2] extends CCA using deep neural networks, and concate-
nates projected low-dimensional features of multiple views.
(4) DCCAE: Deep Canonically Correlated AutoEncoders
(DCCAE) [27] consists of two autoencoders and maximizes
the canonical correlation between the learned representa-
tions, and then combines these projected low-dimensional
features together.

(5) MDcR: Multi-view Dimensionality co-Reduction (MD-
cR) [33] applies the kernel matching to regularize the de-
pendence across multiple views and projects each view on-
to a low-dimensional space. Then these projected low-
dimensional features are concatenated together.
(6) DMF-MVC: Deep Semi-NMF for MVC (DMF-MVC)
[36] utilizes a deep structure through semi-nonnegative ma-
trix factorization to seek a common feature representation
with consistent knowledge for multi-view data.

Evaluation metrics. To comprehensively compare AE2-
Nets with others, we adopt four different metrics to evalu-
ate the clustering quality, i.e., Accuracy, Normalized Mutu-
al Information (NMI), F-score and Rand Index (RI), where
different metrics favor different properties of clustering.

2582

Table 2: Performance comparison on classiﬁcation task.

Datasets

Methods

G80%/P20%

G70%/P30%

G50%/P50%

G20%/P80%

handwritten

Caltech101

ORL

COIL20

CUB

FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours
FeatConcate
CCA [14]
DCCA [2]
DCCAE [27]
MDcR [33]
DMF-MVC [36]
Ours

89.60 ± 1.40
93.78 ± 0.82
95.18 ± 0.55
95.78 ± 0.46
92.33 ± 0.73
94.68 ± 0.71
96.93 ± 0.71
87.88 ± 0.67
91.10 ± 0.96
92.12 ± 0.58
91.58 ± 1.02
90.14 ± 0.74
85.51 ± 1.05
93.77 ± 1.35
79.13 ± 2.36
77.13 ± 3.96
83.25 ± 2.71
81.62 ± 2.95
92.00 ± 1.58
93.13 ± 1.21
97.88 ± 1.19
78.50 ± 2.30
90.50 ± 1.46
90.96 ± 1.24
92.54 ± 0.70
91.11 ± 0.80
95.25 ± 1.06
96.11 ± 1.10
82.50 ± 3.04
63.92 ± 3.14
65.67 ± 2.85
77.00 ± 2.94
83.08 ± 3.43
60.08 ± 2.79
85.83 ± 2.94

88.97 ± 0.73
93.47 ± 0.93
94.62 ± 0.64
95.10 ± 0.64
91.55 ± 0.39
93.72 ± 0.60
96.55 ± 0.66
87.47 ± 0.56
90.07 ± 1.03
91.46 ± 0.70
90.91 ± 0.75
89.45 ± 0.76
84.67 ± 0.82
92.98 ± 1.37
74.58 ± 1.32
73.83 ± 4.89
78.92 ± 1.93
80.00 ± 1.47
90.83 ± 2.08
91.75 ± 1.64
96.00 ± 2.18
76.42 ± 2.33
88.64 ± 0.95
90.48 ± 1.56
91.88 ± 1.44
90.29 ± 1.05
94.76 ± 0.77
95.55 ± 0.87
81.50 ± 3.13
61.39 ± 2.56
64.83 ± 1.83
74.56 ± 2.74
82.44 ± 3.08
58.56 ± 2.84
84.00 ± 1.41

88.87 ± 0.44
93.28 ± 0.66
94.35 ± 0.46
94.79 ± 0.58
91.41 ± 0.68
93.33 ± 0.46
95.88 ± 0.71
87.17 ± 0.49
89.82 ± 0.49
91.30 ± 0.48
90.54 ± 0.44
88.95 ± 0.41
81.88 ± 0.73
92.49 ± 0.72
68.00 ± 2.23
67.95 ± 2.77
71.15 ± 1.86
72.80 ± 2.04
83.35 ± 1.08
85.45 ± 1.85
92.20 ± 1.18
67.05 ± 2.33
86.86 ± 0.76
88.65 ± 0.84
90.35 ± 0.58
87.63 ± 1.12
92.07 ± 0.61
93.25 ± 0.73
80.80 ± 1.41
59.07 ± 2.32
62.37 ± 1.58
72.60 ± 2.52
81.53 ± 1.67
55.30 ± 1.90
82.67 ± 1.41

85.68 ± 0.53
91.12 ± 0.74
92.79 ± 0.51
92.63 ± 0.54
88.11 ± 0.61
88.23 ± 0.57
93.38 ± 0.49
87.10 ± 0.45
89.08 ± 0.71
90.73 ± 0.38
89.44 ± 0.43
88.46 ± 0.35
74.19 ± 0.99
91.36 ± 0.69
48.28 ± 2.27
49.00 ± 1.84
51.69 ± 1.75
51.25 ± 1.90
57.38 ± 2.08
56.44 ± 2.50
70.16 ± 2.54
48.69 ± 2.08
78.94 ± 0.87
83.35 ± 0.60
84.11 ± 1.10
79.46 ± 1.39
82.96 ± 1.03
88.85 ± 0.72
78.33 ± 0.99
53.06 ± 2.12
58.44 ± 2.92
67.35 ± 3.84
78.58 ± 1.65
49.60 ± 1.38
80.17 ± 1.83

There are different deﬁnitions for accuracy for evaluating
clustering, and the accuracy used in our experiments is de-
ﬁned as follows: given a sample xi, its cluster label and
class label (ground-truth) are denoted by ri and si, respec-
tively, then we have

ACC = Pn

i=1 δ(si, map(ri))

,

(15)

n

where δ(x, y) = 1 when x = y, otherwise δ(x, y) = 0.
map(ri) is the permutation map function, which maps the
cluster labels into class labels and the best map can be ob-
tained by Kuhn-Munkres algorithm. We employ the stan-
dard classiﬁcation accuracy and conduct experiments with
different partitions of gallery and probe sets. For each of
these metrics, a higher value indicates a better clustering
performance.

After obtaining the learned representation based on mul-
tiple views, we evaluate the learned representation of each
method on clustering and classiﬁcation tasks. For cluster-
ing, we employ k-means algorithm, while for classiﬁcation,
k-nearest neighbours (kNN) algorithm is used. The reason
for using k-means and kNN lies in the fact that these two
algorithms are both simple and can be used based on Eu-
clidean distance to reﬂect the quality of representation. For
all the compared methods, we tune all the parameters to the
best performance.

In our model, the fully connected layer with tanh(·) be-
ing the activation function is employed for the inner-AE
networks and degradation networks, where the numbers of
layers for them are empirically set as 5 and 3. We use
ℓ2-norm as regularization for parameters on all network-

2583

(a) handwritten

(b) Caltech101

Figure 2: Visualization of original features for each single
view and the latent representation with t-SNE [19].

0.75

0.70

I

M
N

0.65

0.60

0.55
0

1.0

0.8

0.6

0.4

0.2

n
o
i
t
c
n
u
f
 
e
v
i
t
c
e
j
b
o
 
f
o
 
e
u
l
a
V

0.2

0.4

0.6

0.8

1

0.0
0

20

(a)

40
60
Iteration

80

1
0
0

(b)

training and test sets, denoted as
t proportions of
Gtrain ratio/Ptest ratio, where G and P indicate “gallery
set” and “probe set”, respectively. Table 2 shows the com-
parison results for each Gtrain ratio/Ptest ratio. Accord-
ing to Table 2, the accuracy obtained from our AE2-Nets
is more promising than those of comparisons on different
partitions. It is observed that CCA-based methods do not
always outperform FeatConcate. One possible reason is
that overemphasizing the correlation (consistence) may har-
m the complementarity across different views. The superior
performance further validates the advantages of AE2-Nets.
To further investigate the improvement, we visualize o-
riginal features of each single view and our learned intact
representation with t-SNE [19]. As shown in Fig. 2, the
clustering structure is better reﬂected by the learned latent
representation.

Parameter tuning and convergence. The hyperparam-
eter λ is essential to control the fusion of multiple views. As
shown in Fig. 3(a), we present the parameter tuning on the
handwritten dataset and show the clustering performance of
our algorithm with different values for hyperparameter λ.
For each value, we repeat 5 times and plot the means and
standard deviations in terms of NMI. It is observed that the
promising performance could be expected when the value of
λ is within a wide range. To demonstrate the convergence
of our optimization algorithm, we conduct the convergence
experiment as shown in Fig. 3(b). Typically, the objective
value decreases fast in the beginning of iterations and our
optimization algorithm converges within 100 iterations on
these datasets in practice.

Figure 3: Parameter tuning (a) and convergence curve (b).

5. Conclusion

s and the weight decay is empirically set to 0.0001. We
select the dimensionality of latent representation H from
{50, 100, 150, 200, 250, 300} and tune the tradeoff param-
eter λ from {0.1, 0.2, · · · , 1.0}. For simplicity, we set
α1 = · · · = αV = α = 1 on all datasets. Due to random-
ness involved, we run all algorithms 30 times and report the
mean performances and standard deviations in terms of dif-
ferent metrics.

For clustering, the detailed results of different method-
s are shown in Table 1. Obviously, our algorithm basically
outperforms all the other methods on all datasets in terms of
ACC. Since CCA only seeks linear projections, it generally
performs rather unpromising. As expected, beneﬁtting from
nonlinearity, DCCA and DCCAE perform much better than
CCA, which also demonstrates the rationality of our algo-
rithm to model complex correlations based on neural net-
works instead of linear way. Moreover, although DCCAE
and MDcR perform favorably on Caltech101 and handwrit-
ten, respectively, it is not promising on other datasets.

For classiﬁcation, we divide data into differen-

In this paper, we have presented an unsupervised repre-
sentation learning model for heterogeneous data. Unlike ex-
isting multi-view representation learning models mapping
different views onto a common space, the proposed model
AE2-Nets jointly learns the representation of each view and
encodes them into an intact latent representation with a nov-
el nested autoencoder framework. In this way, our method
can ﬂexibly encode intrinsic information from each view.
Experimental results of AE2-Nets outperform the compared
state-of-the-art methods on real-world datasets. For future
directions, we will consider extending the current AE2-Nets
for end-to-end representation learning. For example, we
can design convolutional AE neural networks for images
or graphs [8] for the inner-AE networks to automatically
extract features for real-world heterogeneous data.

Acknowledgment

This work was partly supported by National Natu-
ral Science Foundation of China (61602337, 61732011,
61702358). Corresponding Author: Changqing Zhang.

2584

(1) View 1(2) View 2(3) Ours(1) View 1(2) View 2(3) Ours[21] A. Oliva and A. Torralba. Modeling the shape of the scene: A
holistic representation of the spatial envelope. International
Journal of Computer Vision, 42(3):145–175, 2001.

[22] N. C. Oza and K. Tumer. Classiﬁer ensembles: Select real-

world applications. Information Fusion, 9(1):4–20, 2008.

[23] Y. Peng, X. Zhou, D. Z. Wang, I. Patwa, D. Gong, and
C. Fang. Multimodal ensemble fusion for disambiguation
and retrieval. IEEE MultiMedia, 2016.

[24] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep
representations of ﬁne-grained visual descriptions. pages
49–58, 2016.

[25] A. Sharma and D. W. Jacobs. Bypassing synthesis: Pls for
In

face recognition with pose, low-resolution and sketch.
CVPR, 2011.

[26] N. Srivastava and R. R. Salakhutdinov. Multimodal learning
with deep boltzmann machines. In NIPS, pages 2222–2230,
2012.

[27] W. Wang, R. Arora, K. Livescu, and J. Bilmes. On deep
multi-view representation learning. pages 1083–1092, 2015.
[28] X. Wang, D. Jin, X. Cao, L. Yang, and W. Zhang. Seman-
tic community identiﬁcation in large attribute networks. In
AAAI, pages 265–271, 2016.

[29] M. White, X. Zhang, D. Schuurmans, and Y.-l. Yu. Convex
In NIPS, pages 1673–1681,

multi-view subspace learning.
2012.

[30] P. Xie and E. P. Xing. Multi-modal distance metric learning.

In IJCAI, pages 1806–1812. Citeseer, 2013.

[31] J. Yang, J. McAuley, and J. Leskovec. Community detection
In ICDM, pages 1151–

in networks with node attributes.
1156, 2013.

[32] C. Zhang, H. Fu, Q. Hu, X. Cao, Y. Xie, D. Tao, and D. X-
u. Generalized latent multi-view subspace clustering. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2018.

[33] C. Zhang, H. Fu, Q. Hu, P. Zhu, and X. Cao. Flexible multi-
view dimensionality co-reduction. IEEE Transactions on Im-
age Processing, 26(2):648–659, 2017.

[34] C. Zhang, Y. Liu, Y. Liu, Q. Hu, X. Liu, and P. Zhu. Fish-
In IJCAI,

mml: Fisher-hsic multi-view metric learning.
pages 3054–3060, 2018.

[35] H. Zhang, V. M. Patel, and R. Chellappa. Hierarchical mul-
In

timodal metric learning for multimodal classiﬁcation.
CVPR, pages 3057–3065, 2017.

[36] H. Zhao, Z. Ding, and Y. Fu. Multi-view clustering via deep

matrix factorization. In AAAI, pages 2921–2927, 2017.

References

[1] S. Akaho. A kernel method for canonical correlation analy-

sis. arXiv preprint cs/0609071, 2006.

[2] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep
canonical correlation analysis. In ICML, pages 1247–1255,
2013.

[3] F. R. Bach and M. I. Jordan. A probabilistic interpretation of

canonical correlation analysis. 2005.

[4] M. M. Bronstein, A. M. Bronstein, F. Michel, and N. Para-
gios. Data fusion through cross-modality metric learning us-
ing similarity-sensitive hashing. In CVPR, pages 3594–3601,
2010.

[5] X. Cao, C. Zhang, H. Fu, S. Liu, and H. Zhang. Diversity-
In CVPR, pages

induced multi-view subspace clustering.
586–594, 2015.

[6] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan.
Multi-view clustering via canonical correlation analysis. In
ICML, pages 129–136, 2009.

[7] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In CVPR, volume 1, pages 886–893, 2005.

[8] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-
tional neural networks on graphs with fast localized spectral
ﬁltering. In NIPS, pages 3844–3852, 2016.

[9] P. Dhillon, D. P. Foster, and L. H. Ungar. Multi-view learning
of word embeddings via cca. In NIPS, pages 199–207, 2011.

[10] J. S. Duncan and N. Ayache. Medical image analysis:
Progress over two decades and the challenges ahead. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
22(1):85–106, 2000.

[11] D. P. Foster, S. M. Kakade, and T. Zhang. Multi-view dimen-
sionality reduction via canonical correlation analysis. Tech
Report. Rutgers University, 2010.

[12] H. Gao, F. Nie, X. Li, and H. Huang. Multi-view subspace

clustering. In ICCV, pages 4238–4246, 2015.

[13] K. R. Gray, P. Aljabar, R. A. Heckemann, A. Hammers,
D. Rueckert, A. D. N. Initiative, et al. Random forest-
based similarity measures for multi-modal classiﬁcation of
alzheimer’s disease. NeuroImage, 65:167–175, 2013.

[14] H. Hotelling.

Relations between two sets of variates.

Biometrika, 28(3/4):321–377, 1936.

[15] A. Kumar and H. Daum´e. A co-training approach for multi-

view spectral clustering. In ICML, pages 393–400, 2011.

[16] A. Kumar, P. Rai, and H. Daume. Co-regularized multi-view

spectral clustering. In NIPS, pages 1413–1421, 2011.

[17] Z. Lin, R. Liu, and Z. Su. Linearized alternating direction
method with adaptive penalty for low-rank representation. In
NIPS, pages 612–620, 2011.

[18] D. G. Lowe. Distinctive image features from scale-invariant
International Journal of Computer Vision,

keypoints.
60(2):91–110, 2004.

[19] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.
Journal of Machine Learning Research, 9(Nov):2579–2605,
2008.

[20] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng.
Multimodal deep learning. In ICML, pages 689–696, 2011.

2585

