AANet: Attribute Attention Network for Person Re-Identiﬁcations

Chiat-Pin Tay1,2, Sharmili Roy2, and Kim-Hui Yap1,2

1School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore

2Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore

Abstract

This paper proposes Attribute Attention Network
(AANet), a new architecture that
integrates person at-
tributes and attribute attention maps into a classiﬁcation
framework to solve the person re-identiﬁcation (re-ID)
problem. Many person re-ID models typically employ se-
mantic cues such as body parts or human pose to improve
the re-ID performance. Attribute information, however, is
often not utilized. The proposed AANet leverages on a base-
line model that uses body parts and integrates the key at-
tribute information in an uniﬁed learning framework. The
AANet consists of a global person ID task, a part detection
task and a crucial attribute detection task. By estimating the
class responses of individual attributes and combining them
to form the attribute attention map (AAM), a very strong
discriminatory representation is constructed. The proposed
AANet outperforms the best state-of-the-art method [20] us-
ing ResNet-50 by 3.36% in mAP and 3.12% in Rank-1 accu-
racy on DukeMTMC-reID dataset. On Market1501 dataset,
AANet achieves 92.38% mAP and 95.10% Rank-1 accuracy
with re-ranking, outperforming [11], another state of the
art method using ResNet-152, by 1.42% in mAP and 0.47%
in Rank-1 accuracy. In addition, AANet can perform per-
son attribute prediction (e.g., gender, hair length, clothing
length etc.), and localize the attributes in the query image.

1. Introduction

Given a query image, person re-ID aims to retrieve im-
ages of a queried person from a collection of network-
camera images. The retrieval is typically attempted from a
collection of images taken within a short time interval with
respect to the queried image. This supports the underlying
assumption that the query person’s appearance and cloth-
ing attributes remain unchanged across the query and the
collection images. Person re-ID is a challenging problem
due to many factors such as partial/total occlusion of the

Figure 1. Class-aware heat maps are extracted and combined to
form a discriminatory Attribute Attention Map (AAM) at the im-
age level. The six heat maps shown here correspond to the six
attributes such as hair, upper clothing color, lower clothing color
etc. Best viewed in color.

subject, pose variation, ambient light changes, low image
resolution, etc. Recent deep learning based re-ID solutions
have demonstrated good retrieval performance.

The approaches used to solve the person re-ID problem
can be broadly divided into two categories. The ﬁrst cate-
gory comprises of metric learning methods that attempt to
learn an embedding space which brings images belonging to
a unique person close together and those belonging to dif-
ferent persons far away. Various approaches such as triplet
and quadruplet losses have been employed to learn such em-
bedding spaces [10, 2].

The second category of methods poses the re-ID problem
in a classiﬁcation set-up. Such methods learn by using Soft-
max normalization and computing cross-entropy loss, based
on person identity as ground truth, for back-propagation.
Research has shown that by integrating semantic informa-
tion such as body parts, human pose etc, the classiﬁca-

7134

X

GAP

ResNet50

Body Part

Search

1
2
… … …
…
c

z

v

c

Global ID
Classifier

GFN

PFN

GAP

Crop X

1
2
… … …
…
c

z

v

c

Part
Classifiers

Loss

l

n
o
i
t
u
o
v
n
o
C

z to v

Top

Middle

Bottom

Global

GAP

AFN

…

…

…

v

v

c

v

Attribute
Classifiers

1
1
2
…
2
…
1
…
…
2
9
8
c

c

CAM

AAM

GAP

1
2
… …
…
c

v

c

AAM
Classifier

Figure 2. Overview of AANet. The backbone network, which is based on the ResNet-50 architecture, outputs the feature map X. The
feature map X is forwarded to three tasks, namely the Global Feature Network(GFN), Part Feature Network (PFN) and Attribute Feature
Network (AFN). The output of these three tasks are combined using homoscedastic uncertainty learning to predict the person identiﬁcation.
Best viewed in color.

tion and recognition accuracy can be signiﬁcantly improved
[18, 22, 24]. Person attributes, such as clothing color, hair,
presence/absence of backpack, etc. are, however, not used
by the current state-of-the-art re-ID methods. Since, a typi-
cal re-ID model assumes that the physical appearance of the
person of interest would not signiﬁcantly change between
query image and the search images, physical appearance
becomes a key information that can be mined to achieve
higher re-ID performances. Such information is not utilized
by current research and the state of the art re-ID methods.

In this work, we propose to utilize the person attribute
information into the classiﬁcation framework. The re-
sulting framework, called the Attribute Attention Network
(AANet), brings together identity classiﬁcation, body part
detection and person attribute into an uniﬁed framework
that jointly learns a highly discriminatory feature space.
The resulting network outperforms existing state of the art
methods in multiple benchmark datasets.

Figure 2 gives an overview of the proposed architec-
ture.
The proposed framework consists of three sub-
networks. The ﬁrst network, called the Global Feature Net-
work (GFN), performs global identity (ID) classiﬁcation
based on the input query image. The second network, called
the Part Feature Network (PFN), focuses on body part de-
tection. The third network is the Attribute Feature Network
(AFN), which extracts class-aware regions from the persons
attributes to generate Attribute Attention Map (AAM). This
is shown in Figure 1. The three networks perform clas-

siﬁcation using person ID and attribute labels We use ho-
moscedastic uncertainty learning to optimize the weights of
the three sub-tasks for ﬁnal loss calculations.

Since AANet performs person attribute classiﬁcation as
part of network learning, it also output attribute predictions
for each query and gallery images. This enables attribute
matching of the gallery images, with or without retrieval by
query image.

Our key contributions can be summarized as follows:

1. We provide a new network architecture that integrates
attribute features with identity and body part classiﬁ-
cation in a uniﬁed learning framework.

2. We outperform the existing best state-of-the-art re-ID
method on multiple benchmark datasets and propose
the new state of the art solution for person re-ID.

The rest of the paper is organized as follows. Section 2
provides an overview of the related works. In section 3 we
describe the proposed AANet framework. Experimental re-
sults are provided in section 4 and 5. The paper is concluded
in section 6.

2. Related works

In recent years, deep learning was used to solve various
challenging computer vision tasks [14, 9, 10, 6, 7]. In this
section, we provide an overview of the recent re-ID deep

7135

architecture and large input image, but with classiﬁcation as
ﬁrst pass learning, followed by metric learning for accuracy
ﬁne-tuning.

Integrating semantic information such as body parts and
pose estimation have shown signiﬁcant improvement in re-
ID performance. Since a person’s attributes do not change
signiﬁcantly between the query image and the gallery im-
ages, we believe that attributes form a key information that
can signiﬁcantly impact person re-ID performance. This,
however, has not been utilized in the current re-ID methods.
In view of this, we propose to integrate physical attributes
to the identity classiﬁcation framework.

3. Proposed Attribute Attention Network

(AANet)

The proposed AANet is a multitask network with three
sub-networks, namely the GFN, PFN and AFN (Figure 2).
The sub-network, GFN, performs global image-level ID
classiﬁcation. The PFN detects and extracts localized body
parts before the classiﬁcation task. The AFN uses person
attributes for the classiﬁcation task and generates the At-
tribute Activation Map (AAM) that plays a crucial role in
identity classiﬁcation. Some examples of AAM are shown
in Figure 3.
In the ﬁgure, the generated AAMs provide
more discriminatory features than the ID heatmap. As a re-
sult, when GFN, PFN and AFN learn together, our AANet
becomes more generic and better at predicting person ID.
The various components of AANet are described in details
in the following sections.

The backbone network of AANet is based on ResNet ar-
chitecture (Figure 2) since ResNet is known to perform well
in re-ID problems. We removed the fully connected layer of
the backbone network so that AANet’s sub-networks can be
integrated. There are four classiﬁers within AANet. They
are the Global ID Classiﬁer, Part Classiﬁers, Attribute Clas-
siﬁers and AAM Classiﬁer. The Global ID and Part clas-
siﬁers belong to GFN and PFN respectively. The Attribute
and AAM Classiﬁers belong to AFN. All four classiﬁers
have rather similar network design. All of them utilize
global average pooling to reduce over-ﬁtting, and there is
a 3 layers (Z, V and C) architecture to increase network
depth for better feature learning. The classiﬁers learn using
Softmax normalization and Cross-entropy loss.

3.1. Global Feature Network

This network performs the identity (ID) classiﬁcation us-
ing the query image (Figure 2). The convolutional feature
map X ∈ RZ×H×W extracted by the backbone network is
provided as input to a global average pooling (GAP) layer.
This is followed by a 1x1 convolution layer that brings the
dimensionality down to V. BatchNorm and Relu are then ap-
plied to V before linear transformation to C, which is used

7136

Input
Image

AAM

ID

Heatmap

Heatmap

Input
Image

AAM

ID

Heatmap

Heatmap

Figure 3. Comparison between proposed AAM and class activa-
tion ID heatmap generated in GFN. The AAM captures person at-
tributes, therefore the activated areas lie mostly within the pedes-
trian body. The ID heatmap is inﬂuenced by the training dataset, is
less dense and may include background as part of its feature map.
Best viewed in color.

learning based methods that achieve close to state-of-the-
art performance.

Deep learning based re-ID solutions are often posed as
an identity classiﬁcation problem. Authors in [23] used
multi-domain datasets to achieve high re-ID performance
in a classiﬁcation set-up. Wang et al. [21] proposed to for-
mulate the re-ID problem as a joint learning framework that
learns feature representations using not only the query im-
age but also query and gallery image pairs. Many solutions
have used additional semantic cues such as human pose
or body parts to further improve the classiﬁcation perfor-
mance. Su et al. [18] proposed a Pose-driven Deep Con-
volutional (PDC) model to learn improved feature extrac-
tion and matching models from end-to-end. Wei et al. [22]
also adopted the human pose estimation, or key point de-
tection approach, in his Global-Local-Alignment Descriptor
(GLAD) algorithm. The local body parts are detected and
learned together with the global image by the four-stream
CNN model, which yields a discriminatory and robust rep-
resentation. Yao et al. [24] proposed the Part Loss Net-
works (PL-Net) to automatically detect human parts and
cross train them with the main identity task. Zhao et al.
[25] follows the concept of attention model and uses a part
map detector to extract multiple body regions in order to
compute their corresponding representations. The model is
learned through triplet loss function. Sun et al. [20] pro-
posed a strong Part-based Convolutional Baseline method,
with Reﬁned Part Pooling method to re-align parts for high
accuracy performance. Kalayeh et al. [11] used multiple
datasets, deep backbone architecture, large training images,
and human semantic parsing to achieve good accuracy re-
sults. Similarly, Jon et al. [1] proposed using deep backbone

X

Body 
part 
search

ResNet50

Z x H x W

X

Partitioned

Into 6 
parts

GAP

6 

detected

ROIs

Crop

X

Figure 4. The PFN divides the feature map X ∈ RZ×H×W into
six ROIs using peak activation detection and pooling. Features
from these 6 ROIs are further used for identity classiﬁcation. Best
viewed in color.

GAP

Global

…

Top

Middle

Bottom

…

…

…

V

9 attributes
(e.g. gender, hat,
hair,  backpack, etc.)

1 attribute
(age)

1 attribute
(upper 
clothing color)

1 attribute
(lower 
clothing color)

1
2

1
2
3
4

1
2
…
…
8

1
2
…
…
9

C

Figure 5. 12 attributes are generated from global, top, middle and
bottom vectors on Market1501 dataset.

by Softmax function. Cross-entropy loss is calculated on
Softmax output for learning using back-propagation.

3.2. Part Feature Network

This network performs ID classiﬁcation on body parts
using the same person ID labels used in GFN. The architec-
ture is shown in Figure 4. The body part detector partitions
the convolutional feature map X into six horizontal parts
and estimate the corresponding regions of interest (ROIs).
This is done by identifying peak activation regions in X.
Let (hz, wz) denote the peak activation location in each fea-
ture map z of X where z ∈ {1, . . . , Z}.

(hz, wz) = arg max

z

Xz(h, w)

(1)

where Xz(h, w) is the activation value at location (h, w) on
the z’th feature channel of X. These locations are then clus-
tered into 6 bins based on their vertical positions. These 6
bins constitute the 6 ROIs/ parts. The feature map X is now
divided into 6 parts using these ROIs. Figure 4 shows this
process. Once the 6 parts are computed, the subsequent pro-
cessing, which is shown in Figure 2 is performed similarly
as in GFN.

3.3. Attribute Feature Network

sub-tasks (i) attribute classiﬁcation and (ii) attribute atten-
tion map (AAM) generation. The ﬁrst sub-task performs
classiﬁcation on individual person attributes. The second
sub-task leverages on the output of ﬁrst sub-task and gen-
erates class activation map (CAM) [28] for each attribute.
CAM is a technique to localize the discriminatory image
regions even though the network is trained on image-level
labels only. Thus CAM ﬁts well for AANet use. The CAMs
generated from selected attribute classes are combined to
form a feature map that is forwarded to the AAM Classiﬁer
for learning. We describe these two sub-tasks in the follow-
ing paragraphs in detail.

(i) Attribute classiﬁcation The ﬁrst sub-task of AFN
is to perform attribute classiﬁcation. There are 10 and 12
annotated attributes on DuketMTMC-reID and Market1501
respectively. The ﬁrst layer of AFN is a 1x1 convolution
that downsized the channel depth of feature map X from Z
to V. Next, we partition the feature maps into three differ-
ent sets, namely the Top, Middle and Bottom feature maps,
each responsible for extracting features from their localized
regions. Part-based modeling is known to reduce back-
ground clutter and improve classiﬁcation accuracy. The
different parts focus on different attributes. The Top fea-
ture maps, for example, are used for capturing features such
as hat, hair, sleeves and upper clothing color etc. Features
from the lower half of the body are ignored in the Top fea-
ture map. As shown in Figure 5, the outputs of these feature
maps, together with global feature map, are average pooled
to generate 4 feature vectors at layer V. These 4 vectors are
the input to the fully connected layer C. On Market1501,
there are 4 classiﬁers at layer C, each generating their own
attribute predictions.

(ii) Attribute Attention Map The Attribute Attention
Map (AAM) is the input to the Attribute Classiﬁer (Fig-
ure 2, which performs person ID classiﬁcation. AAM com-
bines class sensitive activation regions from individual at-
tributes. These individual class-sensitive activation regions
are extracted using CAM from each person attribute. As
explained before, CAM uses GAP, with little tweak, to gen-
erate discriminatory image regions. Thus, CAM’s output
reveals image regions representing the attribute. Figure 6
shows some example of class sensitive activation regions
and the combined AAM. For qualitative comparison, the
second column in the ﬁgure shows the activation map gen-
erated by the global identiﬁcation task (GFN). The subse-
quent columns show the class speciﬁc activation regions of
various attributes such as gender, hair, sleeve, upper cloth-
ing color etc. The sixth column, for example, depicts the
class speciﬁc activation region for upper clothing color. We
can observe that the activation region corresponds to the up-
per clothing region in the input query image.

The AFN captures the key attribute information in the
AANet architecture (Figure 2). The AFN consists of two

Out of the 12 available attributes, the gender, hair, upper
and lower clothing colors, lower clothing type and length

7137

Query
Image

ID

Gender

Hair

Sleeve

Upper
clothing

Backpack Lower
clothing

Clothing

type

color

color

Lower
clothing
length

AAM

(attributes 

merged)

AAM
(after

thresholding)

Figure 6. On Market1501 dataset, a total of 12 attention maps are used. Eight most important attributes are shown. Only visual cues (in red
boxes) are selected for AAM generation. Global attention map obtained from the GFN is shown here for qualitative comparison with other
attributes. Backpack, handbag, bag and hat attributes, despite being important visual cues, do not appear in all images, and are therefore
dropped. Attention map for sleeves captures too much background information and thus is unsuitable for AAM. Best viewed in color.

are good choices for AAM generation. The AAM genera-
tion process involves merging the individual class speciﬁc
activation regions by maximum operation and performing
an adaptive thresholding. The thresholding process removes
some background regions that sometimes appear within the
class speciﬁc activation region. An example of this can be
observed in the second row of Figure 6 where the Lower
clothing activation map contains some background region
but on thresholding the region is removed from the gener-
ated AAM. When qualitatively comparing the class activa-
tion map generated by the global feature network on the
same query image, we can see that the AAM was more spe-
ciﬁc in localizing regions with distinct attribute information.
The Attribute Classiﬁer (Figure 2) takes the AAM and per-
form ID classiﬁcation, and shares the learning experience
with the GFN and PFN.

3.4. Loss calculation

The proposed AANet is formulated as a multitask net-
work. The multitask loss function of the AANet is deﬁned
as follows:

Ltotal(x, W, λ) =

T

X

i=0

λiLi(x, W )

(2)

Where x is a set of training images, W is the weights on in-
put x. T is the total number of task loss Li. λi are the task
loss weighting factors, and it plays an important role in op-

timizing the accuracy performance of AANet. If we assign
equal weighting to all λi, the retrieval accuracy will not be
optimal. In our work, we used homoscedastic uncertainty
learning [13, 8, 12] to obtain the task loss weighting. We
deﬁne the Bayesian probabilistic model classiﬁcation like-
lihood output as

p(y|f W (x), σ) = Sof tmax(

1
σ2 f W (x))

(3)

Where f W (x) is the output of the neural network and is
scaled by σ2. σ is the observation noise. The log likelihood
for this output is given by

log(p(y = c|f W (x), σ)) =

c (x)

1
σ2 f W
1
σ2 f W

i (x)))

−log(

C

X

i=0

exp(

(4)

Where C is the number of classes for the classiﬁca-
tion task. The task loss L(x, W, σ) can be formulated
as −log(p(y = c|f W (x), σ)). What we need is the
cross-entropy loss of the non-scaled y, which if deﬁned as
L(x, W ) = −log Softmax(y, fW (x)) [13], the loss function
can be simpliﬁed to

L(x, W, σ) ≈

1
σ2 L(x, W ) + log σ

(5)

7138

By applying the above loss function to our AANet, the

ﬁnal AANet loss function is now given as

DukeMTMC-reID

Methods

mAP

Rank-1

L(x, W, σg, σp, σa, σaa) ≈

Lg(x, W ) +

1
σ2
p

Lp(x, W )+

Laa(x, W ) + log σgσpσaσaa

1
σ2
g
1
σ2
aa

1
σ2
a

La(x, W ) +

(6)

Where Lg, Lp, La and Laa represent global, part, at-
tribute and attribute attention loss respectively. σg, σp, σa
and σaa represent observation noises for global, part, at-
tribute and attribute attention tasks respectively, and are in-
versely proportional to λi

3.5. Implementation

We implemented AANet with ResNet-50 and ResNet-
152 as backbone networks, and pre-trained them with the
ImageNet [4] dataset. Training images are enlarged to 384
x 128, with only random ﬂip as the data augmentation
method. Batch size is set to 32 for ResNet-50, and 24 for
ResNet-152. Using Stochastic Gradient Descent (SGD) as
the optimizer, we train the network for 40 epoch. Learning
rate starts at 0.1 for the newly added layers, and 0.01 for
the pretrained ResNet parameters, and follows the staircase
schedule at 20 epoch with a 0.1 reduction factor for all pa-
rameters. In all the three sub-networks, the value of Z is
2048 and that of V is 256. The value of C depends on the
dataset under evaluation. For DukeMTMC-reID [16], C is
702 and for Market1501 [26] it is 751.

During testing, we concatenate the outputs of the V layer
from all the classiﬁers, namely, the global identity classiﬁer,
the body part classiﬁer, the attribute classiﬁer and the AAM
classiﬁer (Figure 2) to form the representation of the query
image. For ranking, we use the l2 norm between these de-
scriptors of the query and the gallery images.

4. Experimental Results

In the following experiments, we used the DukeMTMC-
reID [16] and Market1501 [26] datasets to conduct our
training and testing. DukeMTMC-reID is a subset of
DukeMTMC dataset. The images are cropped from videos
taken from 8 cameras. The dataset consists of 16,522 train-
ing images and 17,661 gallery images, with 702 identities
for both training and testing. 408 distractor IDs are also
included in the dataset. There are a total of 23 attributes
annotated by Lin et al. [15]. We use all attributes, but with
modiﬁcation to the clothing color attributes. We merged all
8 upper clothing color attributes and 7 lower clothing color
attribute into a single upper clothing attribute and a single
lower clothing attribute respectively.

For Market1501, there are a total of 32,668 images for
both training and testing. There are 751 identities allocated

FMN [5]
SVDNet [19]
DPFL [3]
KPM (Res-50) [17]
PCB (Res-50)[20]
Proposed AANet-50
GP-reID (Res-101) [1]
SPReID (Res-152) [11]
Proposed AANet-152
GP-reID (Res-101)[1] + RR
SPReID (Res-152)[11] + RR
Proposed AANet-152 + RR

56.9
56.8
60.6
63.2
69.2
72.56
72.80
73.34
74.29
85.60
84.99
86.87

74.5
76.7
79.2
80.3
83.3
86.42
85.20
85.95
87.65
89.40
88.96
90.36

Table 1. Performance comparison with other state-of-the-art meth-
ods using DukeMTMC-reID dataset. AANet-50 denotes AANet
trained using ResNet-50. AANet-152 denotes AANet trained us-
ing ResNet-152. RR denotes Re-Ranking[27] .

for training and 750 identities for testing. Lin et al. [15]
also annotated this dataset, but with 27 person attributes.
We use the same clothing color strategy as in DukeMTMC-
reID, and use all attributes for training our model.

4.1. Comparison with existing methods

DukeMTMC-reID dataset We perform comparison
with the state-of-the-art methods in Table 1. The table
has three parts based on the backbone network being used.
First part compares models based on ResNet-50. The three
comparative networks are KPM (Res-50) [17], PCB (Res-
50)[20] and the proposed AANet-50. We outperform the
best state-of-the-art method [20] in this category by 3.36%
in mAP and 3.12% in Rank-1 accuracy.

The second comparison is based on networks using
larger backbone models, which include both ResNet-101
and ResNet-152. The three comparative networks are GP-
reID (Res-101) [1], SPReID (Res-152) [11] and the pro-
posed AANet-152. Here, we again outperformed the state-
of-the-art method [11] by 0.95% in mAP and 1.70% in
Rank-1 accuracy.

The third comparison is based on networks from the sec-
ond comparison, but this time with re-ranking [27]. We out-
performed the state-of-the-art method [1] by 1.27% in mAP
and 0.96% in Rank-1 accuracy.

Market1501 dataset We perform similar comparisons
as in previous section in Table 2 using the Market1501
dataset.
In the ﬁrst comparison, which uses ResNet-50,
the networks selected are KPM (Res-50) [17], PCB (Res-
50)[20] and the proposed AANet-50. We outperformed the
best state-of-the-art method [20] in this category by 0.85%
in mAP and 0.09% in Rank-1 accuracy.

7139

Methods

mAP

Rank1 Rank10

Market1501

63.4
PDC [18]
69.3
PL-Net [24]
73.1
DPFL [3]
73.9
GLAD [22]
75.3
KPM (Res-50)[17]
81.6
PCB (Res-50)[20]
82.45
Proposed AANet-50
81.20
GP-reID (Res-101) [1]
83.36
SPReID (Res-152) [11]
83.41
Proposed AANet-152
90.00
GP-reID (Res-101)[1]+RR
SPReID (Res-152)[11]+RR 90.96
Proposed AANet-152+RR 92.38

84.4
88.2
88.9
89.9
90.1
93.8
93.89
92.20
93.68
93.93
93.00
94.63
95.10

94.9

-
-
-

97.9
98.5
98.56

-

98.40
98.53

-

97.65
97.94

Table 2. Performance comparison with other state-of-the-art meth-
ods using Market1501 dataset. AANet-50 denotes AANet trained
using ResNet-50. AANet-152 denotes AANet
trained using
ResNet-152. RR denotes Re-Ranking[27].

The second comparison is made using GP-reID (Res-
101) [1], SPReID (Res-152) [11] and the proposed AANet,
with either ResNet-101 or ResNet152. Here, we again
outperformed the state-of-the-art method [11] by 0.05% in
mAP and 0.25% in Rank-1 accuracy.

The third comparison is made on networks from the pre-
vious section but with re-ranking [27]. We outperform the
state-of-the-art method [11] by 1.42% in mAP and 0.47% in
Rank-1 accuracy. We believe that the attribute information
is a key contributor in AANet’s person re-ID performance.

4.2. Network Analysis

In this section, we study the effect of task loss weights
and the size of the backbone network on the re-ID perfor-
mance. We also review various training parameters.

Ablation Study In Table 3, we show the impact of the
task loss weights on AANet accuracy performance using
DukeMTMC-reID dataset. The global ID task, the part task,
the attribute classiﬁcation task and the attribute attention
map task are denoted as Lg, Lp, La and Laa respectively.
As we add each of these relevant tasks to the network, the
accuracy improves, which justiﬁes the contribution of each
task on the overall performance. When we use homoscedas-
tic uncertainty learning to obtain the task loss weights Lg,
Lp and La, the performance improves to 70.47% mAP and
85.44% Rank-1 accuracy. This result alone is enough to
outperform the best state-of-the-art method using ResNet-
50. With the integration of AAM, which provides more dis-
criminatory features for learning, we improve the accuracy
results to 72.56% mAP and 86.42% Rank-1 accuracy.

Effect of Backbone Network The depth of the back-

AANet-50
Task Loss
Lg
Lg + Lp
Lg + Lp + La
Lg + Lp + La
Lg + Lp + La + Laa

Task

Weights

0
0
1

0
1
1

0
1
0
1
0
1
Uncertainty

Learning

mAP Rank 1

%

62.92
66.35
67.28
70.47
72.56

%

80.18
82.93
83.29
85.44
86.42

Table 3. Performance comparisons of different combination of task
losses using DukeMTMC-reID dataset, with and without uncer-
tainty learning. The top three rows are AANet accuracy with equal
weights to the tasks. Bottom two rows show the results with loss
weights obtained from uncertainty learning.

Figure 7. Three queries from the DukeMTMC-reID with eight re-
trieved images for each query.

bone network affects the accuracy performance of person
re-ID. Deeper networks yield better result, and this is clearly
shown in both Tables 1 and 2. Table 1 also shows that
the our proposed smaller AANet-50 outperformed deeper
SPReID (Res-152) [11] in Rank-1 accuracy by 0.47%, and
GP-reID (Res-101) [1] by 1.22%. We achieved similar
Rank-1 results on Market1501 dataset, with our AANet-50
outperforming those using deeper backbone networks.

Effect of Training Parameters Many tricks have been
used in the literature to enhance accuracy [11] and [1]. In
[11], the authors aggregate a total of 10 different datasets
to generate ∼ 111k images and ∼ 17k identities for train-
ing and testing. In addition, multiple image sizes are used
to train the network in different phases. In [1], authors use
techniques such as pre-training before regression learning,
large image size, hard triplet mining and deeper backbone
network for good person re-ID. These are good practices.
However, the proposed AANet uses smaller image size,
simpler training process, and a shallower ResNet-50 archi-
tecture to outperform existing state-of-the-art.

7140

Male and 

white lower clothing

Female, hat,

green and red clothing

Backpack, 

black and gray clothing

Query
images

Rank 1

Retrieved images 

Rank 1 to 10

Rank 10

Attributes
matching

Matched images

Rank 1 to 5

Figure 8. Three examples of how person attributes help in improving the image retrieval accuracy. These are challenging image queries
that return many falsely accepted images. Since AANet returns each query and gallery images with predicted attributes, it provides an
option for the user to use attribute matching to ﬁlter away the unwanted retrieved images. The useful attributes include gender. clothing
colors, backpack, etc. Green box denotes same ID as query image. Red box denotes different ID from query image. Best viewed in color

Methods
APR [15]
AANet-152

gender
86.45
92.31

hair

age
87.08 83.65 93.66 93.32 91.46
88.21 86.58 94.45 94.24 94.83

82.79
87.77

L.slv L.low S.clth B.pack H.bag

hat

bag

C.up C.low mean
88.98 75.07 97.13 73.40 69.91 85.33
89.61 79.72 98.01 77.08 70.81 87.80

Table 4. Performance comparisons of attribute accuracy on Market1501 dataset.

5. Experimental Results Using Attribute

are performed to return correct images up to rank 5.

In this section, we illustrate how person attributes help

5.2. Attribute Classiﬁcation Performance

in reﬁning the retrieved images for person re-ID.

5.1. Retrieval results

We show three retrieval examples using AANet in Figure
7. Though there are some occlusions on the query subjects,
for examples, cars and unwanted pedestrian, AANet has no
problem retrieving correct images from the gallery set.

In Figure 8, we show some examples of challenging
queries where the subjects are heavily occluded. This re-
sulted in poor retrieval accuracy. The ﬁgure demonstrates
how AANet provides an option for the user to ﬁlter away
the incorrect retrievals by using predicted attributes from
query and gallery images. Three examples are given, each
with their own retrieval difﬁculties. First example is given
in row one. More than half of the query subject is occluded
by another pedestrian. Most computer vision methods will
pick the unwanted pedestrian as subject of interest, and re-
turn wrong images. In this example, 9 out of 10 images are
wrongly retrieved, which results in poor mAP performance.
Through AANet’s attribute matching, those wrong images
can be ﬁltered out easily without laborious manual ﬁltering.
The ranking of theses attribute matched images were 1, 19,
38, 78, 172 during ﬁrst retrieval, indicating how difference
they are to query image. Same challenging queries are given
in row two and three. As in ﬁrst example, attribute ﬁltering

The accuracy of attribute classiﬁcation of the proposed
AANet is compared with APR [15] in Table 4. APR [15]
is provided by Lin et al., the author who annotated the
DukeMTMC-reID and Market1501 datasets with person at-
tributes. Since AANet employs localized attribute features
to enhance network learning, we obtained better represen-
tations and outperforms APR in every attribute prediction.

6. Conclusions

In this paper we propose a novel architecture to incorpo-
rate attributes based on physical appearance such as cloth-
ing color, hair, backpack etc. into a classication based per-
son re-ID framework. The proposed Attribute Attenion Net-
work (AANet) employs joint end-to-end learning and ho-
moscedastic uncertainty learning for multitask loss fusion.
The resulting network outperforms existing state-of-the-art
re-ID methods on multiple benchmark datasets.

Acknowledgement

This research was carried out at the Rapid-Rich Ob-
ject Search (ROSE) Lab at
the Nanyang Technologi-
cal University, Singapore. The ROSE Lab is supported
by the Infocomm Media Development Authority, Singa-
pore.

7141

References

[1] Jon Almazan, Bojana Gajic, Naila Murray, and Diane Lar-
lus. Re-id done right: towards good practices for person re-
identiﬁcation. arXiv preprint arXiv:1801.05339, 2018. 3, 6,
7

[2] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi
Huang. Beyond triplet loss: a deep quadruplet network for
person re-identiﬁcation. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), volume 2,
2017. 1

[3] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Person re-
identiﬁcation by deep learning multi-scale representations.
In 2017 IEEE International Conference on Computer Vision
Workshops, ICCV Workshops 2017, Venice, Italy, October
22-29, 2017, pages 2590–2600, 2017. 6, 7

[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical image
database.
In 2009 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 2009), 20-
25 June 2009, Miami, Florida, USA, pages 248–255, 2009.
6

[5] Guodong Ding, Salman Hameed Khan, Zhenmin Tang, and
Fatih Porikli. Let features decide for themselves: Fea-
ture mask network for person re-identiﬁcation. CoRR,
abs/1711.07155, 2017. 6

[6] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and
Gang Wang. Context contrasted feature and gated multi-
scale aggregation for scene segmentation. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018. 2

[7] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and
Gang Wang. Semantic correlation promoted shape-variant
context for segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2019.
2

[8] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian
approximation: Representing model uncertainty in deep
learning. CoRR, abs/1506.02142, 2015. 5

[9] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision, pages 2961–2969, 2017. 2

[10] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-
fense of the triplet loss for person re-identiﬁcation. CoRR,
abs/1703.07737, 2017. 1, 2

[11] Mahdi M Kalayeh, Emrah Basaran, Muhittin G¨okmen,
Mustafa E Kamasak, and Mubarak Shah. Human seman-
tic parsing for person re-identiﬁcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1062–1071, 2018. 1, 3, 6, 7

[12] Alex Kendall and Yarin Gal. What uncertainties do we need
in bayesian deep learning for computer vision? In Advances
in neural information processing systems, pages 5574–5584,
2017. 5

[13] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. CoRR, abs/1705.07115, 2017. 5

[14] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. nature, 521(7553):436, 2015. 2

[15] Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu, and Yi
Improving person re-identiﬁcation by attribute and

Yang.
identity learning. CoRR, abs/1703.07220, 2017. 6, 8

[16] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,
and Carlo Tomasi. Performance measures and a data set for
multi-target, multi-camera tracking. In European Conference
on Computer Vision, pages 17–35. Springer, 2016. 6

[17] Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi, and Xi-
aogang Wang. End-to-end deep kronecker-product matching
for person re-identiﬁcation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
6886–6895, 2018. 6, 7

[18] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao,
and Qi Tian. Pose-driven deep convolutional model for per-
son re-identiﬁcation. In IEEE International Conference on
Computer Vision, ICCV 2017, Venice, Italy, October 22-29,
2017, pages 3980–3989, 2017. 2, 3, 7

[19] Yifan Sun, Liang Zheng, Weijian Deng, and Shengjin Wang.
Svdnet for pedestrian retrieval. In IEEE International Con-
ference on Computer Vision, ICCV 2017, Venice, Italy, Oc-
tober 22-29, 2017, pages 3820–3828, 2017. 6

[20] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin
Wang. Beyond part models: Person retrieval with reﬁned
part pooling (and a strong convolutional baseline).
In The
European Conference on Computer Vision (ECCV), Septem-
ber 2018. 1, 3, 6, 7

[21] Faqiang Wang, Wangmeng Zuo, Liang Lin, David Zhang,
and Lei Zhang.
Joint learning of single-image and cross-
image representations for person re-identiﬁcation. In 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016,
pages 1288–1296, 2016. 3

[22] Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, and Qi
Tian. GLAD: global-local-alignment descriptor for pedes-
trian retrieval.
In Proceedings of the 2017 ACM on Mul-
timedia Conference, MM 2017, Mountain View, CA, USA,
October 23-27, 2017, pages 420–428, 2017. 2, 3, 7

[23] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang
Wang. Learning deep feature representations with domain
guided dropout for person re-identiﬁcation.
In 2016 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages
1249–1258, 2016. 3

[24] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li,
and Qi Tian. Deep representation learning with part loss for
person re-identiﬁcation. CoRR, abs/1707.00798, 2017. 2, 3,
7

[25] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang.
Deeply-learned part-aligned representations for person re-
identiﬁcation.
In IEEE International Conference on Com-
puter Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,
pages 3239–3248, 2017. 3

[26] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:
A benchmark.
In 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, December 7-
13, 2015, pages 1116–1124, 2015. 6

7142

[27] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-
ranking person re-identiﬁcation with k-reciprocal encoding.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,
2017, pages 3652–3661, 2017. 6, 7

[28] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2921–
2929, 2016. 4

7143

