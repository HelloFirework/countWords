Modulating Image Restoration with Continual Levels

via Adaptive Feature Modiﬁcation Layers

Jingwen He1

,∗ Chao Dong1

,∗ Yu Qiao1

2

,†

,

1ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab,

Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China

2The Chinese University of Hong Kong

Abstract

q30 DeJPEG

In image restoration tasks, like denoising and super-
resolution, continual modulation of restoration levels is
of great importance for real-world applications, but has
failed most of existing deep learning based image restora-
tion methods. Learning from discrete and ﬁxed restoration
levels, deep models cannot be easily generalized to data of
continuous and unseen levels. This topic is rarely touched
in literature, due to the difﬁculty of modulating well-trained
models with certain hyper-parameters. We make a step for-
ward by proposing a uniﬁed CNN framework that consists
of little additional parameters than a single-level model yet
could handle arbitrary restoration levels between a start
and an end level. The additional module, namely AdaFM
layer, performs channel-wise feature modiﬁcation, and can
adapt a model to another restoration level with high accu-
racy. By simply tweaking an interpolation coefﬁcient, the
intermediate model – AdaFM-Net could generate smooth
and continuous restoration effects without artifacts. Exten-
sive experiments on three image restoration tasks demon-
strate the effectiveness of both model training and modula-
tion testing. Besides, we carefully investigate the properties
of AdaFM layers, providing a detailed guidance on the us-
age of the proposed method.

1. Introduction

Deep learning methods have achieved great success in
image restoration tasks, such as denoising, super-resolution,
compression artifacts reduction, etc [12, 10, 5, 4, 22]. How-
ever, there still exists a large gap of restoration perfor-
mance between research environment mand real-world ap-
plications. In this work, we focus on two main issues that
prevent CNN based restoration methods from wide usages.
First, the degradation levels of real-world images are

∗The ﬁrst two authors are co-ﬁrst authors. (e-mail: jw.he@siat.ac.cn;

dong.chao@siat.ac.cn).

q80

model

q10 

model

over-sharpening

over-smoothed

Is it possible to achieve a compromise ?

Figure 1. Applying q10 or q80 DeJPEG model on images (LIVE1
[15]) with degradation q30 tends to produce either over-sharpening
(left) or over-smoothed (right) images.
generally continuous, such as JPEG quality q27 and q34.
On the other hand, the deep restoration models are usually
trained with discrete and ﬁx levels (e.g., q20, q30). Ap-
plying models with mismatched restoration levels tends to
produce either over-sharpening or over-smoothed images,
as shown in Figure 11. A straightforward solution is to train
a sufﬁciently large model to handle all degradation levels.
However, regardless of the computational burden, this gen-
eral model is not optimal for each individual level. When
we want to slightly adjust the output effects, we have to
retrain a new model by reﬁning the model structure, param-
eters or (and) loss functions, which is a tedious procedure
with unpredictable results.

Second, in industrial and commercial scenarios (e.g.,
human-interactive softwares), it is often necessary to con-
secutively modulate the restoration strength/effect to meet
different requirements. For example, the users always ex-
pect a tool bar to ﬂexibly adjust the restoration level, as

1DeJPEG is also known as JPEG deblocking and compression artifacts

†Corresponding author (e-mail: yu.qiao@siat.ac.cn).

reduction.

11056

q30 DeJPEG

Modulateλ to obtain continual restoration effects 

                                                            from DeJPEG  q80 to q10

tool bar

   λ=   0.0                     0.4                     0.7                    1.0  

                PSNR:   32.24                33.32                 33.04                 32.18  

×3.4 SR

from Super Resolution ×3 to ×4 

σ45 Denoising

 from Denoisingσ15 toσ75 

  

Figure 2. We can modulate the tool bar to obtain continual restora-
tion effect in DeJPEG, Super Resolution and Denoising.

shown in Figure 2. However, current deep models are
trained on ﬁxed degradation levels, and contain no hyper-
parameters for users to change the ﬁnal results.

To ﬁll in the gaps, our goal is to achieve arbitrary-level
image restoration and continual model modulation in a u-
niﬁed CNN framework. More formally, the task is to deal
with images of degradation levels between a “start” level
and an “end” level in a user controllable manner. To facili-
tate practical usages, we should avoid building a very large
model or model zoo, and prevent another training stage at
test time. In other words, the solution should contain a s-
mall amount of additional parameters and allow continual
tuning of parameters in testing.

This task is non-trivial and rarely studied in literature.
Perhaps the most relevant topic to modifying the network
outputs is arbitrary style transfer. Speciﬁcally, we can treat
different levels of degradation as different kinds of styles.
A representative approach is the Conditional Instance Nor-
malization (IN) [6], which allows users to mix up differ-
ent styles by tuning IN parameters. Nevertheless, image
restoration has higher and ﬁner request on the output image
quality. Directly applying Conditional IN in image restora-
tion could produce obvious and large-scale artifacts in the
output image (see Figure 6). Another similar concept is do-
main adaptation, which generally appears in high-level vi-
sion problems (e.g., image classiﬁcation and object detec-
tion).
It adapts/transfers the model trained on the source
domain to the target domain. However, domain adaptation
cannot easily generalize to unseen data, thus is not appro-

priate to address our problem.

In this work, we present a simple yet effective approach
that for the ﬁrst time enables consecutive modulation of the
restoration strength with little computation cost. This ap-
proach stems from the observation that ﬁlters among net-
works of different restoration levels are similar at patterns
while varying on scales and variances. Furthermore, the
model outputs could change continuously by modulating
the statistics of features/ﬁlters. The proposed framework is
built upon a novel Adaptive Feature Modiﬁcation (AdaFM)
layer that modiﬁes the middle-layer features with depth-
wise convolution ﬁlters. In practice, we ﬁrst train a standard
restoration CNN for the start level, and then insert AdaFM
layers and optimize it to the end level. After the training
stage, we ﬁx the CNN parameters, and interpolate the ﬁlters
of AdaFM layers according to testing restoration level. By
tuning a controlling coefﬁcient (ranging from 0 to 1), we
can interactively and consecutively manipulate the restora-
tion results/effects. Note that we only need to train the CNN
and AdaFM layers once, and no further training is required
in the test time.

To ensure the output quality, we demonstrate that the
model with AdaFM layers achieves comparable perfor-
mance to the single-level image restoration network in both
start and end level. Then, we show that the modulated-
network outputs are noise-free with consecutive restoration
effects (see Figure 2). Besides, we also examine the prop-
erties of AdaFM layers - complexity, range and direction,
providing a detailed instruction on the usage of the pro-
posed method. Notably, the added AdaFM layers contribute
to less than 4% parameters of the CNN model yet achieves
excellent modulation performance.

2. Related Work

The proposed Adaptive Feature Modiﬁcation (AdaFM)
layers are inspired by the recent normalization methods in
deep CNNs, thus we give a brief review of these works.
Normalization has been demonstrated effective in facilitat-
ing training very deep neural networks. The most represen-
tative method is batch normalization (BN) [8] that is pro-
posed to address the problem of Internal Covariate Shift
in the training process. In particular, BN layer normalizes
the output of each neuron using the mean and variance of
each batch calculated during the feed-forward process. Lat-
er on, Dmitry Ulyanov et al. [17] achieved signiﬁcant im-
provement in style transfer by replacing all the BN layers
with their proposed instance normalization (IN) layers. The
core idea is to normalize the features based on the statis-
tics across the spatial dimensions of each sample instead
of each batch. Recently, several alternative normalization
methods have been proposed, such as instance weight nor-
malization [14], layer normalization [2], group normaliza-
tion (GN) [20] and etc. The spatial feature transformation

11057

(SFT) layer proposed by Wang et al. [18] further extends the
normalization operation to a more general spatial-variant
transformation. Speciﬁcally, they apply a feature spatial-
wise transformation on the feature maps according to the
semantic segmentation priors. This approach indeed helps
generate more realistic textures compared with those pop-
ular GAN-based methods. We will compare the proposed
AdaFM layer with BN and SFT layers in Section 3.3.

Furthermore, recent works show that BN and IN have the
ability to adapt the model to a different domain with little
computation cost. Speciﬁcally, Li et al. [11] propose AdaB-
N (Adaptive Batch Normalization) to alleviate domain shift-
s, and show that AdaBN is effective for domain adaptation
task by re-computing the statistics of all BN layers across
the network. Huang et al. [7] show that instance normaliza-
tion (IN) can perform as style normalization by aligning the
mean and variance of content features with those style fea-
tures. In such way, they realize arbitrary style transfer at test
time. Moreover, Dumoulin et al. [6] extended IN to enable
multiple style transfer by learning different sets of param-
eters in normalization layers while the convolution param-
eters are shared. Our method is different from these works
in that 1) the proposed AdaFM layer is independent of ei-
ther batch or instance samples, 2) the ﬁlter size and position
of AdaFM layers are ﬂexible, indicating that AdaFM is be-
yond a normalization operation, 3) the interpolation proper-
ty of AdaFM layers could achieve continual modulation of
restoration levels, which has not been revealed before.

3. Method

3.1. Problem Formulation.

The problem of consecutive modulation of restoration
levels can be formulated as follows. Suppose we have a
“start” restoration level – La and an “end” restoration level
– Lb, the objective is to construct a deep network to han-
dle images with arbitrary degradation level Lc (La ≤ Lc ≤
Lb). Our solution pipeline consists of two stages – model
training and modulation testing. In model training, we train
a basic model and an adaptive model that could deal with
level La and Lb, respectively. While in modulation testing,
we propose a new network that can realize arbitrary restora-
tion effects between level La and Lb by modulating certain
hyper-parameters. In the following sections, we ﬁrst show
two important observations that inspire our method. Then
we propose the AdaFM layer and compare it with BN [8]
and SFT [18]. At last, we describe how to use AdaFM lay-
ers in model training and modulation testing.

3.2. Observation

Observation 1. We ﬁnd that the learned ﬁlters of restora-
tion models trained with different restoration levels are pret-
ty similar at visual patterns, but their weights have different

         
b) 

Figure 3. Filter visualization.

   

c)

   

a)

statistics (e.g., mean and variance). An example is shown
in Figure 3, the ﬁlter fa of level La is like a 2-D Gaussian
ﬁlter, then the corresponding ﬁlter fb ﬁnetuned from level
La to level Lb will also look like a Gaussian ﬁlter but with
different mean and variance. We use the Gaussian Denois-
ing problem for illustration. The start level is noise level
σ = 15, and the end level is σ = 50. We adopt a simple
and standard CNN structure ARCNN [4] to do the experi-
ments. We ﬁrst learn the model with noise level σ = 15 and
obtain ARCNN-15, then ﬁnetune the network on σ = 50
to obtain ARCNN-50. The ﬁrst layer ﬁlters of these two
models are visualized in Figure 3. In the ﬁrst glance, these
ﬁlters look similar with only slight differences. Their mean
cosine distance between the corresponding ﬁlters is 0.12,
indicating that they are very close to each other. To further
reveal their relationship, we use a ﬁlter to bridge the corre-
sponding ﬁlters. Speciﬁcally, each ﬁlter f15 in ARCNN-15
is convoluted with another ﬁlter g to approximate the corre-
sponding ﬁlter f50 in ARCNN-50. According to the com-
mutative law, we have (g ∗ f15) ∗ x = g ∗ (f15 ∗ x), where ∗
is convolution. Thus for each feature map x, the parameters
of g are optimized with

||f50 ∗ x − g ∗ (f15 ∗ x)||2.

(1)

min

g

The above operation is equivalent to adding a depth-wise
convolution layer after each layer of ARCNN-15, and ﬁne-
tuning the added parameters on the σ = 50 problem. When
g is of size 1 × 1, it is equal to a scaling and shift operation,
changing the mean and variance of the original ﬁlter. We
use the PSNR gap between their network outputs to show
the ﬁtting error. From Table 2, we can see that the value
of ﬁtting error decreases when the ﬁlter size of g increases.
The gap is already very small at 1 × 1, which demonstrates
our primal assumption. The 5 × 5 ﬁlters are also visual-
ized in Figure 3, where one can see the differences between
f15 and f50. Similar experiments for super resolution and
compression artifacts reduction are presented in the supple-
mentary ﬁle.

Observation 2. We ﬁnd that the network output could
change continuously by modulating the statistics of fea-
tures/ﬁlters. As the ﬁlter g is gradually updated by gradient
descent, what if we control the updating process by interpo-
lating the intermediate results? Speciﬁcally, we can obtain
the intermediate ﬁlter fmid by the following function:

fmid = f15 + λ(g − I) ∗ f15, 0 ≤ λ ≤ 1,

(2)

11058

Figure 4. The left part presents the basic model and the AdaFM-Net. The right part shows how AdaFM works in the adaptation process
and the modulation testing.

where λ is an interpolation coefﬁcient. When we modulate
λ gradually from 0 to 1, fmid will also change continuously
from f15 to g ∗ f15. After putting fmid back to the network,
we ﬁnd that the network output will also change continuous-
ly in visualization, as shown in Figure 2. Detailed analysis
can be found in Section 3.5 and 4.

modiﬁcation and ﬁnetune γ, β as gi, bi. Experiments show
that using BN achieves almost the same results as the 1 × 1
AdaFM ﬁlter.

Comparison with SFT layer. When the ﬁlter size of g
is as large as the feature map, it will perform spatial feature
transform as SFT layer [18]. The formulations are shown as

3.3. Adaptive Feature Modiﬁcation

AdaF M (xi) = gi ⊙ xi + bi, SF T (xi) = γ ⊙ xi + β, (5)

Inspired by the above observations, we propose a contin-
ual modulation method by introducing an Adaptive Feature
Modiﬁcation layer and the corresponding modulating strat-
egy. The overall framework is depicted in Figure 4.

Our aim is to add another layer to manipulate the statis-
tics of the ﬁlters, so that they could be adapted to another
restoration level. As indicated in Observation 1, we can add
a depth-wise convolution layer (or a group convolution lay-
er with the group number equal to the number of feature
maps) after each convolution layer and before the activa-
tion function (e.g., ReLU). We name the added layer as the
Adaptive Feature Modiﬁcation layer, which is formulated
as

AdaF M (xi) = gi ∗ xi + bi, 0 < i ≤ N,

(3)

where xi is the input feature map and N is the number of
feature maps. gi and bi are the corresponding ﬁlter and bias,
respectively. It is worth noting that gi depends on the degra-
dation level of input images. To further understand its be-
haviour, we compare the proposed layer with batch normal-
ization (BN) [8] and spatial feature transformation (SFT)
[18] layers.

Comparison with BN layer. When we set the ﬁlter size
of gi to 1 × 1, the feature modiﬁcation reduces to a normal-
ization operation. Note that BN [8] is also put directly after
the convolution layer. We compare it with BN as
xi − µ

AdaF M (xi) = gixi + bi, BN (xi) = γ(

) + β, (4)

σ

where µ, β are the mean and standard deviation of an input
batch, γ, β are afﬁne parameters. The 1 × 1 AdaFM ﬁlter
performs similar to BN without using the batch informa-
tion. As a special case, we can also use BN to do feature

where γ, β are afﬁne parameters. AdaFM and SFT layer
share the same function, but different on the parameters.
Speciﬁcally, γ, β are calculated from another sub-network
based on an additional prior, while gi, bi are directly learned
with the network.

3.4. Model Training

In this subsection, we discuss how to utilize the proposed
AdaFM layer for model training. The entire model, name-
ly AdaFM-Net, consists of a basic network and the AdaFM
layers. First, we train the basic network N a
bas, which can
be any standard CNN model, for the start restoration level
La. Then we insert AdaFM layers to N a
bas and form the
AdaFM-Net Nada. By ﬁxing the parameters of N a
bas, we
optimize the parameters of AdaFM layers on the end lev-
el Lb. Experiments demonstrate that by only ﬁnetuning the
AdaFM layers, the model N b
ada could achieve comparable
performance with a basic model N b
bas trained from scratch
on level Lb. As the AdaFM-Net is optimized from La to
Lb, we name this process as adaptation, and use adaptation
accuracy to denote its performance. Speciﬁcally, we can
use the PSNR distance between PSNR of N b
bas
as the measurement of adaptation accuracy. There are three
factors that affect the adaptation accuracy – ﬁlter size, di-
rection, and range.

ada and N b

(1) For ﬁlter size, a larger ﬁlter size or more parameter-
s will lead to better adaptation accuracy. We try ﬁlter size
from 1 × 1 to 7 × 7. From convergence curves shown in
Figure 5, we ﬁnd that 3 × 3 performs much better than 1 × 1
while 7 × 7 is only comparable to 5 × 5. Further increasing
the ﬁlter size could not continuously improve the perfor-

11059

stride 2ConvConvConvRelu+ConvReluAdaFMConvAdaFMConvReluAdaFMConvAdaFMResidual block...16 residual blocksConvAdaFM-NetupsamplingConvConvConvAdaFMUpscale: 2++basic model+Relumance. (2) For direction, different restoration levels have
different degrees of difﬁculty for the same network. Then
should we modulate the model from an easy level to a hard
level or the opposite direction? Experimentally, we ﬁnd that
from easy to hard is a better choice (see Section 4.2). (3)
For range, the smaller of the range/gap |Lb − La|, the better
the adaptation accuracy. For example, in super resolution
problem, transferring the ﬁlters from ×2 to ×3 is easier
than from ×2 to ×4. In Section 4, we conduct numerous
experiments to choose the best range for super-resolution,
denoising and compression artifacts reduction.

3.5. Modulation testing

After the training process, we discuss how to modulate
the AdaFM layers according to degradation level at test
time. As the features remain the same after convolution
with an identity ﬁlter, we initialize AdaFM layers with i-
dentity ﬁlters I and zero biases, which is regarded as the
start point of AdaFM layers. Based on Observation 2, we
can linearly interpolate the parameters of AdaFM layers as

g∗
i = I + λ(gi − I), b∗

i = λbi, 0 < i ≤ N,

(6)

i , b∗

where g∗
i are the ﬁlter and bias of the interpolated
AdaFM layers, λ(0 ≤ λ ≤ 1) is the interpolation coefﬁ-
cient determined by the degradation level of input image.
After adding the interpolated AdaFM layers back to the ba-
sic network N a
ada for a
middle level Lc(La ≤ Lc ≤ Lb). The effects of changing
the coefﬁcient λ from 0 to 1 are shown in Figure 2, 6, where
the output effects change continuously along with λ.

bas, we can get the AdaFM-Net N c

Interestingly, we ﬁnd that the interpolated network could
fairly deal with any restoration level Lc between level La
and Lb by adjusting the coefﬁcient λ, which behaves like a
strength controller in traditional methods. Experimentally,
we ﬁnd that the relationship between the coefﬁcient λ and
restoration level Lc can be formulated/approximated as a
polynomial function:

λ = f (Lc) =

M

X

j=0

wjLj
c,

(7)

c, λi}M

i=0. Specially, the start point is {L0

where M is the order and {wj}M
0 are coefﬁcients. To ﬁt
this polynomial function, we need to determine at least M
points {Li
c =
La, λ0 = 0} and the end point is {λM = 1, LM
c = Lb}.
Furthermore, we require a test set with degraded images
and ground truth to measure the adaptation accuracy. For
a middle level Li
c as in-
puts. By adjusting the coefﬁcient λ, the AdaFM-Net could
generate a series of outputs. We select the λ that achieves
the highest PSNR (evaluated on the test set) as the best co-
efﬁcient, recorded as λi for Li
c. It is worth noticing that the
modulation process and curve ﬁtting require no additional
training.

c , we use the test images of level Li

Extensive experiments show that the ﬁtting curve varies
a lot with ranges and problems. Take compression artifacts
reduction as an example. If the range is small, such as JPEG
quality from q80 to q50, then the ﬁtting function is linear
(order M = 1) as shown in Figure 7. On the other hand, if
the range is large, such as from q80 to q10, then we have to
use a curve (order M = 3) for approximation. Similar trend
is observed for denoising and super resolution (see details
in Section 4.3 and the supplementary ﬁle).

c}. For a given level Lc (Li

As an alternative choice, we can also use the piece-wise
linear function for approximation. Actually, when the range
is small enough, the relationship between λ and Lc is almost
linear. We can train a set of AdaFM-Nets on middle levels
{Li
), we can use
the coefﬁcient λ = (Lc−Li
c) to interpolate the
AdaFM-Nets between Li
. This strategy needs to
train and store more AdaFM-Nets on middle levels, but the
adaptation accuracy is comparably higher due to the small
range.

c)/(Li+1
c and Li+1

c −Li
c

c < Lc < Li+1

c

4. Experiments

4.1. Experimental Set up

Training settings. We use the DIV2K [1] dataset for all
the image restoration tasks. The training data is augmented
by horizontal ﬂipping and 90-degree rotations. Following
SRResNet [10], the mini-batch size is set to 16 and the HR
patch size is 96 × 96. The L1 loss [19] is adopted as the
loss function. For model training, the initial learning rate
is set to 1 × 10−4 and then decayed by a factor of 10 after
5 × 105 iterations. We adopt the Adam [9] optimizer with
β1 = 0.9, β2 = 0.999. All models are built on the PyTorch
framework and trained with NVIDIA 1080Ti GPUs.

The structure of basic model. Based on the widely used
SRResNet and DnCNN [22], the basic model Nbas adopts
a general CNN structure that consists of a pair of down-
sampling (convolution with stride 2) and up-sampling (pix-
elshufﬂe [16] with upscaling factor 2) layers, 16 residual
blocks, and several convolution layers. Speciﬁcally, the ﬁl-
ter number is 64 and the ﬁlter size is 3 × 3 for all convo-
lution layers. The residual block contains two convolution
layers and a ReLU activation layer. The middle features
are processed in a low-resolution (1/4 of the input size) s-
pace, while the output size remains the same as the input
size. For super-resolution, we can upsample the LR image
to the HR image size as SRCNN [5]. As shown in Table 1,
the basic model achieves better PSNR results than SRRes-
Net, DnCNN and ARCNN on super-resolution, denoising
and compression artifacts reduction, respectively. As stated
in Section 3.4 and 3.5, the basic model is also trained on d-
ifferent levels (as the baseline) to evaluate the performance
of AdaFM-Nets.

The position of AdaFM layers. As indicated in Sec-

11060

Super resolution
Set5×4
Denoising
CBSD68 σ15
DeJPEG
LIVE1 q10

SRResNet

basic model

32.05

32.13

DnCNN

basic model

33.89

34.10

ARCNN

basic model

29.13

29.55

Table 1. Comparisons with the state-of-the-art methods in PSNR.

Figure 5. The performances of adaptation with different ﬁlter sizes
of AdaFM layers in super resolution on Set5 dataset.

tion 3.3, we can insert the AdaFM layers after all convolu-
tion layers or just in the residual blocks (the same as BN and
IN). Moreover, an alternative choice is to add AdaFM layers
after all activation layers. To evaluate the above three ap-
proaches, we conduct experiments for super resolution task
×3 → ×4 with ﬁlter size 5 × 5. From the experimental re-
sults, we observe that adding AdaFM layers after activation
is inferior to that before activation (32.00 dB, 31.84 dB eval-
uated on Set5 [3]). The results of inserting AdaFM layers
after all convolution layers and in the residual blocks make
little difference (32.01 dB, 32.00 dB evaluated on Set5). To
save computation, we insert AdaFM layers just in residual
blocks before activation for all experiments.

Complexity analysis. We calculate the parameters of
the basic model and AdaFM layers. Following previous
works, we exclude the number of biases that perform add
operation in network. The total parameters in basic model
include the parameters of 16 residual blocks, 4 convolution
layers and a pixelshufﬂe layer. As we insert the AdaFM
layers in residual blocks, the number of AdaFM layers is
equal to the number of convolution layers in residual block-
s. Thus there are 16 × 2 × 64 = 2048 ﬁlters in AdaFM
layers. When the ﬁlter size is 1 × 1, 3 × 3, 5 × 5, the
number of parameters is 2048, 18432, 51200, respectively,
accounting for 0.15% 1.31% 3.65% of the total parameters
in the basic model. Note that these numbers are even s-
maller than the parameter number of a single residual block
(2 × 64 × 64 × 9 = 73728). Nevertheless, as AdaFM-Net is
comparably larger than the basic model, we still need to ver-
ify whether it signiﬁcantly improves the model capacity. In
super resolution ×4, we train an AdaFM-Net with AdaFM
layers of a large ﬁlter size 5 × 5 from scratch. The PSNR
value on DIV2K (30.39 dB) is almost the same as that of
the basic model (30.37 dB), indicating that the performance
is not inﬂuenced by AdaFM layers. We can safely use the

SR

PSNR(dB)
Set5
DIV2K100
LIVE1
DeJPEG
Denoising CBSD68

5×5

3×3

7×7
1×1
31.42 31.88 32.00 32.03
29.89 30.20 30.28 30.30
29.35 29.39 29.41 29.42
26.35 26.38 26.39 26.40

baseline
32.13
30.37
29.55
26.49

Table 2. The PSNR results of adaptation with different kernel sizes
of AdaFM layers in three tasks.

PSNR(dB) AdaBN Conditional IN AdaFM-Net

Set5 ×3
×4

LIVE1 q80
q10
CBSD68 σ15
σ75

34.04
28.70
38.29
27.61
33.83
19.68

33.53
31.30
36.99
28.89
31.33
24.15

34.34
32.00
38.81
29.35
34.10
26.35

Table 3. Comparisons with AdaBN [11] and conditional IN [6]

basic model as baseline to test the AdaFM-Nets. In anoth-
er perspective, this also demonstrates the effectiveness of
the proposed strategy, which adapts the model to different
restoration levels with little additional computation cost.

4.2. Evaluation of Model Training

In this section, we evaluate our proposed method on
three image restoration tasks, super resolution, denoising,
and compression artifacts reduction (JPEG Deblocking or
DeJPEG). The basic settings are shown below.

For super-resolution, we train our models in RGB chan-
nels and calculate the PSNR in y-channel on two widely
used benchmark datasets – Set5 [3] and the test set of DI-
V2K [1]. We evaluate our methods on upscaling factors
×2, ×3, ×4, ×5, ×6. All other settings remain the same
as SRCNN [5]. In denoising, we use Gaussian noise and
consider 5 noise levels, i.e., σ = 15, 25, 35, 50, 75. Fol-
lowing DnCNN [22], the models are trained with RGB
channels and evaluated in RGB channels on CSBD68 [13]
For DeJPEG, we use the JPEG quality q =
dataset.
80, 60, 40, 20, 10 in MATLAB JPEG encoder. Similar as
ARCNN [4], our models are trained and tested in y channel
only. LIVE1 [15] dataset is used for evaluation.

Filter Size. First, we need to determine the ﬁlter size
of AdaFM layers for different problems. We denote the
adaptation from the start level La to the end level Lb as
La → Lb. The basic model is trained on La, and AdaFM-
Net is tested on Lb.

For the super resolution task ×3 → ×4, we compare the
performance of AdaFM-Net with various ﬁlter sizes – 1×1,
3×3, 5×5 and 7×7. The convergence curves on Set5 are
plotted in Figure 5 , and the quantitative results are present-
ed in Table 2. In general, larger ﬁlters can achieve better
performance. Notably, the PSNR gap between 1 × 1 and
3 × 3 is larger than 0.4 dB. However, this trend does not
always hold when the ﬁlter size is expanded to 7×7. There-
fore, we use the ﬁlter size 5×5 to conduct the following
experiments for the super resolution tasks.

11061

051015202530iteration (104)29.0029.2029.4029.6029.8030.0030.20PSNRSuper Resolutionkernel size: 1kernel size: 3kernel size: 5kernel size: 7Adaptation in Super Resolution

range1

range2

direction

×2 → ×3 ×3 → ×4 ×4 → ×5 ×5 → ×6 ×2 ⇒ ×4 ×3 ⇒ ×5 ×4 ⇒ ×6 ×3 ← ×4 ×2 ⇐ ×4

Set5
AdaFM-Net

34.34
33.98

PSNR distance

0.36

DIV2K100
AdaFM-Net

32.35
32.07

PSNR distance

0.28

32.13
32.00

0.13

30.37
30.28

0.09

30.26
30.16

0.10

29.04
29.02

0.02

28.74
28.73

0.01

28.10
28.09

0.01

32.13
31.66

0.47

30.37
30.01

0.36

30.26
29.98

0.28

29.04
28,88

0.16

28.74
28.61

0.13

28.10
28.00

0.10

34.34
34.11

0.23

32.35
32.14

0.21

37.84
37.11

0.73

36.00
35.13

0.87

Table 4. Adaptation results. The PSNR distances within 0.2 dB are shown in bold.

range

direction

DeJPEG 80→60

80→40

80→20

80→10

80←10

LIVE1
AdaFM-Net

36.00
35.98

distance

0.02

34.34
34.29

0.05

31.93
31.81

29.55
29.35

0.12

0.20

38.81
37.77

1.04

Denoising

15→25

15→35

15→50

15→75

15←75

CBSD68
AdaFM-Net

31.44
31.43

distance

0.01

29.82
29.78

0.04

28.20
28.13

0.07

26.49
26.35

0.14

34.10
33.42

0.68

Table 5. Adaptation results of DeJPEG and Denoising. The PSNR
distances within 0.2 dB are shown in bold.

Similar as in super resolution, we compare the perfor-
mance with different ﬁlter sizes (1×1, 3×3, 5×5 and 7×7)
for denoising task σ15 → σ75 and DeJPEG task q80 →
q10. Results shown in Table 2 indicate that in both two
tasks, ﬁlter size 1×1 can already achieve excellent perfor-
mance. The PSNR gap between 1 × 1 and 7 × 7 is less than
0.1 dB. Considering the computation cost, we use ﬁlter size
1 × 1 for all denoising and DeJPEG experiments.

Direction. The second step is to ﬁnd the best adaptation
direction. Before experiments, it is essential to clarify the
way of measurement. For task La → Lb, the baseline is
the basic model trained on Lb with performance Pbas, and
the AdaFM-Net is ﬁnetuned on Lb with performance Pada.
Then the PSNR distance |Pbas−Pada| is used to evaluate the
adaptation accuracy of AdaFM-Net. In experience, 0.3 dB
is regarded as a signiﬁcant PSNR gap in image restoration.
In other words, if the distance |Pbas −Pada| exceeds 0.3 dB,
then the adaptation is NOT well-suited for applications.

We conduct three pairs of experiments – super resolution
task ×3 → ×4 and ×4 → ×3, denoising task σ15 → σ75
and σ75 → σ15, DeJPEG task q80 → q10 and q10 →
q80. Results are shown in Table 4, 5. In all three problems,
the tasks with direction from easy to hard (i.e., ×3 → ×4,
σ15 → σ75, q80 → q10) achieve better adaptation results.
Here, easy and hard refer to the difﬁculty of restoring the
input images. For example, in DeJPEG, the PSNR distance
of q80 → q10 is 0.2 dB, which is much lower than that of
the inverse direction q10 → q80 – 1.04 dB.

Range. In this subsection, we investigate the inﬂuence

of the adaptation range. Generally, by ﬁxing the start lev-
el La, we change the end level Lb and test the adaptation
accuracy.

Different from previous sections, we start discussion
with denoising and DeJPEG, where the trend of range is
more obvious. In denoising, we start with σ15 and change
the end level from σ25 to σ75. In DeJPEG, we start with
q80 and change the end level from q60 to q10. The adapta-
tion results are shown in Table 5. It is observed that better
adaptation accuracy is obtained with a smaller range. In ad-
dition, the proposed AdaFM can easily handle very large
range in either denoising or DeJPEG.

For super resolution, we ﬁnd it hard to adapt the model
across even 2 upscaling factors. For example, in Table 4,
the PSNR distance for the task ×2 → ×4 exceeds 0.3 dB
for all three test sets, indicating that we should not further
enlarge the range to 3. When ﬁxing the range to be 1 and 2
upscaling factors, we change both the start and end level to
see the change of results. From Table 4, we can conclude
that the adaptation is easier (lower PSNR distance) with a
harder start level (e.g., ×4 → ×5 is better than ×3 → ×4).

4.2.1 Comparison with AdaBN and Conditional IN

We compare with state-of-the-art methods on super res-
olution task ×3 → ×4, denoising task σ15 → σ75 and
DeJPEG task q80 → q10. To compare with AdaBN [11],
we train a network with batch normalization after all con-
volutional layers in the residual blocks, and then change all
the statistics in BN layers during testing. We also use con-
ditional IN [7] to handle different levels of restoration. The
results are shown in Table 3. It can be obviously observed
that neither of the two methods can obtain reasonable su-
per resolution, denoising and DeJPEG results. Therefore,
they are not suitable for image restoration tasks. Qualitative
comparisons are also shown in Figure 6, where we observe
clear artifacts on their output images.

4.3. Evaluation of Modulation Testing

In modulation testing, continuously manipulating the in-
terpolation coefﬁcient λ could gradually change the output
effect. If the input image is ﬁxed, then the output image
will become sharper or smoother with the increase of λ, as

11062

Denoising:σ15 andσ75

AdaFM-Net

λ= 0.0                        0.2                        0.4                        0.6                        0.8                        1.0              

Modulation Testing

Denoising:

σ45

DeJPEG:

q30

SR:
×3.4

AdaBN

Conditional IN

Figure 6. Left: Artifacts on the output images produced by AdaBN and conditional instance normalization. Right: Modulation testing in
Denoising (CBSD68), DeJEPG (LIVE1) and Super Resolution (Set14 [21]).

λ= 0.0                        0.2                        0.4                        0.6                        0.8                        1.0              

shown in Figure 6. On the other hand, we can choose d-
ifferent λ to deal with different kinds of degraded images.
As presented in Section 3.5, the coefﬁcient λ can be for-
mulated as a polynomial function of restoration level Lc
– λ = PM
In this subsection, we investigate
the curving ﬁtting with different ranges in DeJPEG prob-
lem. Similar investigations on super resolution and denois-
ing problems can be found in supplementary ﬁle.

j=0 wjLj
c.

We ﬁrst investigate the DeJPEG task q80 → q10. We
select 6 middle levels – Lc = q70, 60, 50, 40, 30, 20 – be-
tween q80 and q10. Then for a given level Lc, we use the
test images of Lc as inputs, and adjust λ to obtain different
outputs of AdaFM-Net. After calculating the PSNR val-
ues on LIVE1 test set, we select the λ that achieves the
best PSNR as the best coefﬁcient. For example, see the
blue line in Figure 7, the best coefﬁcient for level q60 and
q30 are 0.14, 0.40, respectively. After we have obtained al-
l middle points, we ﬁt the curve by a cubic function:λ =
1.51 − 6.24 × 10−2Lc + 1.01 × 10−3L2
c − 5.91 × 10−6L3
c .
Then for arbitrary levels between q80 and q10, we can use
this function to predict its corresponding interpolation coef-
ﬁcient. If we test a smaller range, such as q80 → q50, then
a simple straight line could fairly connect all middle points
(see the orange line in Figure 7). In other words, the poly-
nomial function is linear. This property holds for smaller
ranges such as q80 → q60.

To verify whether the interpolated image is of high qual-
ity, we use the PSNR distance on LIVE1 test set as the e-
valuation metric. Speciﬁcally, the basic model trained on
level Lc is used as the baseline, and the PSNR distance is
calculated between the PSNR of AdaFM-Net and that of
a well-trained baseline model. The smaller of the PSNR
distance the better of the adaptation/modulation accuracy.
Figure 7 illustrates the PSNR distances in two DeJPEG
tasks. It is observed that all PSNR distances are below 0.2
dB, indicating that the output quality is good enough for
practical usages. Further, the PNSR distances of the small-
range task q80 → q50 is much lower than the large-range
task q80 → q10. Thus modulation across smaller ranges
achieves better performance. For higher request of modu-
lation quality, we can decompose a large range to several

small ranges, and train AdaFM-Nets for each sub-task. We
can balance the performance and computation burden ac-
cording to different applications.

Figure 7. Top: the curve ﬁtting with different ranges in DeJPEG
problem; Bottom: the value of PSNR distance is annotated above
each bar.
5. Conclusion

We present a method that allows continual modulation of
restoration levels in a single CNN for versatile and ﬂexible
image restoration. The core idea of our method is to han-
dle images with arbitrary degradation levels with a single
model, which consists of a basic model and a modulation
layer – AdaFM layer. We further propose the learning and
modulating strategies of the AdaFM layers. In test time, the
model can be adapted to any restoration level by directly
adjusting the AdaFM layers without an additional training
stage.

Acknowledgements. This work is partially supported
by National Key Research and Development Program of
China (2016YFC1400704), Shenzhen Research Program
(JCYJ20170818164704758,
JCYJ20150925163005055,
CXB201104220032A), and Joint Lab of CAS-HK.

11063

1020304050607080JPEG compression level0.00.20.40.60.81.0coefficient: 0.070.140.200.280.400.591.000.000.250.500.711.00Rangefrom 80 to 10from 80 to 401020304050607080JPEG compression level303234363840PSNR0.00.030.060.080.120.150.160.20.00.010.020.020.05labelfrom 80 to 10baselinefrom 80 to 40using an efﬁcient sub-pixel convolutional neural network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1874–1883, 2016.

[17] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.
Instance normalization: The missing ingredient for fast styl-
ization. CoRR, abs/1607.08022, 2016.

[18] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 606–615, 2018.

[19] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Li-
u, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan:
Enhanced super-resolution generative adversarial networks.
In European Conference on Computer Vision, pages 63–79.
Springer, 2018.

[20] Yuxin Wu and Kaiming He. Group normalization. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 3–19, 2018.

[21] Roman Zeyde, Michael Elad, and Matan Protter. On sin-
gle image scale-up using sparse-representations. In Interna-
tional conference on curves and surfaces, pages 711–730.
Springer, 2010.

[22] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a Gaussian denoiser: Residual learning
of deep CNN for image denoising.
IEEE Transactions on
Image Processing, 26(7):3142–3155, 2017.

References

[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge

on single image super-resolution: Dataset and study.

[2] Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer nor-

malization. CoRR, abs/1607.06450, 2016.

[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie Line Alberi-Morel. Low-complexity single-image
super-resolution based on nonnegative neighbor embedding.
2012.

[4] Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou
Tang. Compression artifacts reduction by a deep convolu-
tional network.
In The IEEE International Conference on
Computer Vision (ICCV), December 2015.

[5] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution.
In European conference on computer vi-
sion, pages 184–199. Springer, 2014.

[6] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
A learned representation for artistic style. Proc. of ICLR,
2017.

[7] Xun Huang and Serge J Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. In ICCV,
pages 1510–1519, 2017.

[8] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015.

[9] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2015.

[10] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network.

[11] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and
Xiaodi Hou. Revisiting batch normalization for practical do-
main adaptation. CoRR, abs/1603.04779, 2017.

[12] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for s-
ingle image super-resolution.
In The IEEE conference on
computer vision and pattern recognition (CVPR) workshop-
s, volume 1, page 4, 2017.

[13] Stefan Roth and Michael J Black. Fields of experts: A frame-
work for learning image priors. In Computer Vision and Pat-
tern Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference on, volume 2, pages 860–867. IEEE, 2005.

[14] Tim Salimans and Diederik P Kingma. Weight normaliza-
tion: A simple reparameterization to accelerate training of
deep neural networks.
In Advances in Neural Information
Processing Systems, pages 901–909, 2016.

[15] H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A statisti-
cal evaluation of recent full reference image quality assess-
ment algorithms. IEEE Transactions on Image Processing,
15(11):3440–3451, Nov 2006.

[16] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution

11064

