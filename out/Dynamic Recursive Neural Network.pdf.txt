5147

structure, we analyze the gradient propagation of recursive
block and ﬁnd that the gradient explosion of a recursive net-
work is caused by BN layer. Therefore, we propose Loopy
Variable Batch Normalization to stabilize the gradients and
beneﬁt feature re-usage in a more general and easier way
than LSTM [10].
In particular, we employ different BN
layer for each looping. By applying LVBN, the recursive
network can be trained better with fewer parameters.

To incorporate the discrete decisions, we build upon re-
cent work [39] that introduces the Gumbel-Max trick [23]
and its differentiable approximations to allow for the propa-
gation of gradient through the discrete decision. Further, the
gate unit makes discrete decision deterministically by re-
moving the gumbel noise while maintains the performance.
To deal with the statistical bias, we make an improve-
ment on LVBN, which normalizes all the inputs and up-
dates the population statistics (moving averaged statistics
of means and variances across all training images) used
in testing time. Some inputs will jump out of loop in ad-
vance, while they should contribute to optimizing the gate
unit but not to population statistics. So our improved LVBN
(I-LVBN) corrects the population statistics by normalizing
inputs according to the decision made by the gate unit and
updates the population statistics across the inputs which are
allowed to pass.

DRNN, combined with the gate unit and I-LVBN,
achieves even better performance while accelerating the in-
ference of deep networks. To evaluate DRNN, we use Mo-
bileNetV2 [33] and ResNet [13] as the base models on clas-
siﬁcation and other visual tasks. We approve that, with the
progressive strategy designed for recursive network, Dy-
namic Recursive (DR) ResNet-53 outperforms ResNet-101
while reducing model parameters by 47.0% and computa-
tional cost by 35.2%. Further, we study the dynamic recur-
sive behavior of the learned model and reveal the relation
between the image saliency and the number of loop time.

the computational cost.

Our contributions are listed as follows:
• Presenting a Dynamic Recursive Mechanism to reduce
• Proposing LVBN to stabilize the gradients of recursive
networks and make full use of convolutional parame-
ters. Improving LVBN to deal with the statistical bias
caused by different loop time of a recursive block dur-
ing training.

• Model parameters and computational cost can be re-
duced while obtaining a universal improvement of ac-
curacy.

2. Related Work

Recursive network. Different from the way of shar-
ing weights along the sequence in Recurrent Neural Net-
works (RNN) [40], recursive network shares weights at ev-
ery node, which could be considered as a generalization of

RNN. [7] tries recursive layers on image recognition but
gets worse performance than a single convolution due to
overﬁtting. [36, 30] apply recursive blocks when the input
dimension is twice that of output. Studies, incorporating re-
current connections into CNNs, also show superiority in ob-
ject recognition [27], super-resolution [25] and some other
tasks. Recently, based on the studies about recurrent struc-
ture and iterative reﬁnement [24, 28], densely connected
structure is widely used to obtain more efﬁcient model like
DenseNet [18] and CliqueNet [43]. In contrast to these ap-
proaches, DRNN behaves as a strategy that can be generally
applied in networks with modern block structure. Requiring
no complicated connections from former inputs, very deep
DRNN performs well on robustness and convergence.

Adaptive computation. The main purpose of adap-
tive computation is providing “Customized Service” for
different inputs to reduce overall inference time, while
maintaining or even boosting accuracy. Cascade detec-
tors [8, 41] are early methods that exploit this idea in
computer vision, relying on extra prediction modules or
handcrafted control strategies. Early prediction models
like BranchyNet [38] and Adaptive Computation Time
(ACT) [11] adopt branches or halt units to decide whether
the model could stop early. Figurnov et al. [9] further ex-
tend this idea to the spatial domain in ResNet by apply-
ing ACT to each spatial position of multiple image blocks.
Our approach is closer to the works [39, 42] which add gate
unit on every block to determine the execution of block-
operation according to current input.
Inspired by above
methods, a reusable gate unit is designed to reconstruct net-
work structure on the ﬂy conditioned on the input during
execution. After every single forward of recursive block,
gate unit of recursive block is activated to make a routing
choice between going back to block or just passing by and
forwarding normally.

Model Compression. Besides critical need of accuracy
improvements, reducing storage and inference time also
plays an important role in deploying top-performing deep
neural networks. Related techniques focus on many ﬁelds
like distillation [3, 15], ﬁlter pruning [5, 31], low-rank fac-
torization [21], quantization [12], compression with struc-
tured matrices [4, 35] and network binarization [32]. These
works are applied after training the initial networks and usu-
ally used as post-processing. DRNN could be trained end-
to-end without well-designed training rules.

Other Compact deep nets like SqueezeNet [20] and Mo-
bileNet [17, 33] are also end-to-end trainable, but they ap-
ply the same computation to all images. Thanks to the loop
structure controlled by gate units, DRNN could reuse one
block dynamically.
In experiments, we prove even com-
pact model like MobileNetV2 could be further improved
by applying the dynamic recursive block. Theoretically, re-
cursively used blocks could be further pruned or quantized

5148

5149

5150

5151

Model

Error Params(106) FLOPs(109)

5.25
SD-ResNet 110 [19]
Pre-ResNet 110 [14]
6.37
AIG-ResNet 110 [39] 5.76
RCNN-160 [27]
7.09

ResNet-110 [13]
DR-Res 74 (l = 2)
DR-Res 66 (l = 3)
DR-Res 58 (l = 4)
DR-Res 54 (l = 5)

ResNet-56 [13]
DR-Res 40 (l = 2)
DR-Res 32 (l = 4)

ResNet-20 [13]
DR-Res 16 (l = 2)

6.61
5.21
5.66
5.79
5.97

6.97
6.51
6.73

8.75
8.14

1.7
1.7
1.78
1.86

1.7
0.92
0.73
0.55
0.45

0.85
0.50
0.310

0.27
0.18

0.255
0.255
0.215

-

0.255
0.220
0.214
0.197
0.192

0.127
0.110
0.099

0.041
0.032

Table 1: Error rate (%) on CIFAR-10. All of the DR-ResNets out-
perform their counterpart while using less parameters and com-
putation cost. Our proposed approach for dynamic recursive net-
works is applicable to both deep network architectures and shal-
lower ones.

of sizes {32, 16, 8} respectively, with n blocks for each fea-
ture map size. Our DR-ResNet has ⌊ (n−1)
⌋ + 1 blocks for
each feature map size where l is the number of loop. An ad-
ditional block is for downsampling. Such a design ensures
that the maximum computational cost of our network does
not exceed its counterpart.

l

We ﬁx the early blocks up to the ﬁrst downsampling be-
cause the low-level feature maps are not yet distinguish. For
the gates, we set the size of hidden state d to 16 and the tar-
get executed rate to 0.7.

We follow a similar training scheme as [13] with a
weight decay of 0.0005 and momentum of 0.9. The models
are trained with a mini-batch size of 256 for 350 epochs.
We start with learning rate of 0.1 and divide it by 10 after
150 and 250 epochs. All the images for training are padded
with 4 pixels on each side and a 32 × 32 crop is randomly
sampled from the padded image or its horizontal ﬂip.
Results Tab. 1 shows test error, the number of model
parameters and ﬂoating point operations(multiply-adds) on
CIFAR-10 [26]. The ﬁrst part in the table includes some
variant methods based on ResNet and a study that also in-
corporates recursive structure. The other parts compare our
DR-ResNets with ResNets. From the results, we observe
that all of the DR-ResNets outperform their counterpart.
The more times block loops, the less parameters and com-
putation are need. Whereas, the performances are similar.

Overall, DR-ResNet 54 is able to reduce parameters and
computation by 73.53% and 24.71% while outperform-
ing the ResNet-110. Since the ResNet-110 is overﬁtted
for CIFAR-10, stochastic depth ResNet-110 regularizes by

dropping layers and AIG-ResNet 110 applies adaptive infer-
ence graph. DR-ResNet 110 outperforms them by reusing
the convolutional layer conditioned on the input example.
Our proposed method can be applied not only to deep net-
works, but also to shallower networks like ResNet-20. DR-
ResNet 16 can also reduce parameters and computation by
33.3% and 22.0% while outperforming its counterpart.

4.2. Results on ImageNet

In experiments on ImageNet [6], we analyze the effec-
tiveness of our LVBN, dynamic recursive mechanism and
Improved LVBN based on ResNet-101 and evaluate a se-
ries of network on ImageNet.

Model conﬁgurations and training details We build
DR-ResNet and DR-MobileNetV2 by inheriting the resid-
ual bottleneck and inverted residual block.

Blocks in early stage or not repetitive are ﬁxed. The dy-
namic recursive block groups are marked bold in Tab. 2,
while the downsampling layers stay unchanged. The num-
ber of blocks is designed to ensure the maximum compu-
tational cost of our network does not exceed its counter-
part. The blocks in last group loop twice in DR-ResNets
because the last group comprises only three blocks. In DR-
MobileNetV2, the loop time of the fourth group is three and
the other dynamic recursive blocks loop twice.

For our counterparts of ResNet-101, ResNet-50 and Mo-
bileNetV2, the target rate is set to 0.7, 0.8, 0.9. All the
gates are initialized at a executed rate of 85% at the begin-
ning of training. The size of the hidden state is 16 for gates
in DR-ResNets. Moreover, each gate unit comprises only
one fully-connected layer for more compact structure.

We follow the ResNet training procedure, with learning
rate starting at 0.1 and decaying by 0.1 every 30 epochs.
The weight decay is 0.0001 and momentum is 0.9. All the
models are trained for 100 epochs. One exception is that for
DR-MobileNetV2 and its counterpart, we start the learning
rate at 0.01 and decay it by 0.1 at 200 and 300 epochs. Both
are optimized by stochastic gradient descent (SGD) for 400
epochs.

We apply the scale and aspect

ratio augmenta-
tion as GoogleNet [37] and the photometric distortions
method [16]. During test time, the images are rescaled to
256 × 256 follwed by a 224 × 224 center crop.
Quantitative comparison Tab. 2 shows top-1 accuracy
rate and models’ details on ImageNet. From the results
we can make the following key observations. DR-ResNet
with 65 and 53 layers have better performance than ResNet-
101 while using less computational resources. Shallower
DRNNs also outperform their counterpart.
In particular,
DR-ResNet 53 saves 35.2% of computation and 47.0% of
parameters. Similarly, DR-ResNet 35 outperforms ResNet-
50 while using 33.7% less parameters and 17.8% less av-
erage computational cost. For DR-ResNet, increasing loop

5152

Model

Top 1

Top 5

Params(106)

FLOPs(109)

Blocks

ResNet-101 [13]
Stochastic Depth ResNet-101 [19]
AIG-ResNet-101 [39]
DR-ResNet 65 (l = 2)
DR-ResNet 53 (l = 3, t = 0.7)
DR-ResNet 53 (l = 3, t = 0.65)
DR-ResNet 53 (l = 3, t = 0.6)
DR-ResNet 47 (l = 4)
DR-ResNet 44 (l = 5)

ResNet-50 [13]
Stochastic Depth ResNet-50 [19]
AIG-ResNet 50 [39]
DR-ResNet 35 (l = 2)

MobileNetV2 [33]
DR-MobileNetV2

77.95
77.20
77.93
78.12
77.96
77.14
76.91
77.41
77.27

76.45
72.25
76.42
76.48

71.8
71.8

93.86
93.56
93.85
93.90
93.86
93.52
93.51
93.54
93.53

92.90
90.86
93.17
92.92

90.27
90.28

44.54
44.54
46.23
28.12
23.60
23.60
23.60
21.34
20.21

25.56
25.56
26.56
17.61

3.40
2.96

7.6
7.6
5.11
5.49
4.92
4.62
4.35
4.56
4.25

3.8
3.8
3.15
3.12

(3,4,23,3)
(3,4,23,3)
(3,4,23,3)
(3,4,12,2)
(3,4,8,2)
(3,4,8,2)
(3,4,8,2)
(3,4,6,2)
(3,4,5,2)

(3,4,6,3)
(3,4,6,3)
(3,4,6,3)
(3,3,3,2)

0.300
0.275

(1,2,3,4,3,3,1)
(1,2,2,2,2,2,1)

Table 2: Top 1 and Top 5 accuracy rate (%) on ImageNet. Dynamic recursive block groups are bold in table. All the downsampling blocks
are not recursive in block groups. The last group blocks loop twice for computational equality in DR-ResNets. The result demonstrates
that DR-ResNet is more efﬁcient and also improves overall classiﬁcation quality. All the MobileNetV2, ResNets and AIG-ResNets are
reimplemented with the training procedure in Sec. 4.2.

Model

L G I

Top 1

Params FLOPs
(106)
(109)

ResNet-101 [13]
R-ResNet 53
R-ResNet 53
R-ResNet 53
R-ResNet 53
R-ResNet 53*

77.95
Fail
√
77.22
√ √
77.52
√ √ √ 77.96
√
78.38

44.54
23.39
23.39
23.60
23.60
23.39

7.6
7.6
7.6
4.92
4.92
7.6

Table 3: The comparative and ablative result of our dynamic re-
cursive network on ImageNet validation set. When Recursive
ResNet-53 is trained with smaller learning rate for more epochs,
it(R-ResNet 53*) also outperforms ResNet-101.

time performs better than reducing the execution rate. In
particular, DR-ResNet 53 with a target rate of 0.65 has a
larger expected total loop time than DR-ResNet 47.

For the deeper network, reusing the parameters of convo-
lutional layer ﬁrst improve accuracy, before increasing the
loop time further decrease accuracy insigniﬁcantly. This
demonstrates that reusing the parameters of convolutional
layer is efﬁcient to improve the capacity of network by dy-
namic recursive mechanism. Further, reusing blocks adap-
tively is often more effective to save computational resoures
compared to reducing blocks of identical structure directly.
As expected, decreasing the target rate reduces computa-
tion time. Interestingly, increasing loop times leads to bet-
ter result than reducing the execution rate. More loop time
means that more high-level information is used to reﬁne the
low-level ﬁlters to get a stronger ability for representation.

The recursive structure beneﬁts feature re-usage.

Due to our proposed approach for dynamic recursive net-
works is general, we also apply dynamic recursive block
on MobileNetV2. The result show that the training of dy-
namic recursive models can be applied to convolution and
depthwise-sparable convolution layers in different building
blocks.

These results indicate that the parameters of convolu-
tional layer is underused and DRNN is an effective means
to adaptively assemble network graph on the ﬂy.

Analysis of dynamic recursive network To understand
dynamic recursive network, we conduct ablation experi-
ments to examine how each proposed component affects
the ﬁnal performance. We use R- to indicate naive recur-
sive models. L indicates that we replace BN with LVBN,
G stands for gate units and I is the improved LVBN. Each
component is an improvement based on the previous one.

From results in Tab. 3, some promising conclusions can

be summed up as follows:

• LVBN is crucial to reuse convolution layers. Naive
recursive network (R-ResNet 53) fails to converge to
a good solution and becomes divergent after a few
epochs, as illustrated in Fig. 6. LVBN can not only
get correct population statistics for each loop but also
solve gradient explosion. Utilizing LVBN, we can
get a result similar to baseline without changing any
hyper-parameters of optimization algorithm. We train
a recursive network with LVBN starting with a lower
learning rate of 0.01 and weight decay of 0.0005 for
180 epochs. The learning rate is divided by 10 after

5153

70

60

50

40

30

20

10

%
 
y
c
a
r
u
c
c
a
 
1
-
p
o
t
 
t
e
N
e
g
a
m

I

Naive R-ResNet
R-ResNet with LVBN

0

0

20

40
60
Training Epochs

80

100

0.4

0.3

0.2

0.1

0.0

−0.1

−0.2

−0.3

−0.4

0

1.5

1.0

0.5

0.0

−0.5

−1.0

−1.5

0

0.12

0.10

0.08

0.06

0.04

0.02

50

100

150

200

250

3_1_bn1, running_mean

0.00

0

50

100
3_1_bn1, running_var

150

200

250

g=1
g=0

1.8

1.6

1.4

1.2

1.0

0.8

0.6

0.4

0.2

100

200

300

400

500

4_1_bn1, running_mean

0.0

0

100

200
4_1_bn1, running_var

300

400

500

y
c
n
e
u
q
e
r
F

0.30

0.25

0.20

0.15

0.10

0.05

0.00

4

6

8

10

12

14

16

18

20

Number of loops

Figure 6: Testing accuracy on validation
set of naive R-ResNet 53 and R-RseNet
53 with LVBN. For the naive network, we
start with a smaller learning rate of 0.01.

Figure 7: Statistics of the skipped and exe-
cuted feature maps in DR-ResNet 53. Both
shallow and deep blocks are shown. X-axis
represents channel index.

Figure 8: For DR-ResNet 53 on ImageNet,
11.74 out of 23 loops are executed on av-
erage. According to the target 0.7, the total
loop time has an expect of 11.921.

Figure 9: Visualization of easy and hard examples in ImageNet validation set with DR-ResNet 53. The images on the top are easy example
(loop less than 9 times) and the bottom ones are hard examples (loop more than 15 times).

100, 130 and 160 epochs. This trick leads to a better
top-1 accuracy of 78.38%, the last row of Tab. 3, with-
out adding FLOPs compared to ResNet-101. It shows
a trade-off between the size of model and the difﬁculty
of optimizing.

• Dynamic recursive mechanism is efﬁcient. Since
reusing lots of parameters in network poses extra opti-
mization contrast to unrolled networks, a dynamic re-
cursive network with LVBN is expected to have lower
performance. Howerer, the comparison between third
and fourth rows in Tab. 3 shows that our dynamic
recursive mechanism effectively improves the perfor-
mance. The prediction cost is reduced by 35.2% while
accuracy is increased by 0.4%. The main reason is that
our gate unit is efﬁcient and reduces the difﬁculty of
optimizing recursive network by jumping out of loop
in advance. The result also indicates that shallower
network is easy to optimize and able to tackle most
easy examples.

• Improved LVBN is essential. It deals with the devia-
tion of population statistics caused by the feature maps
which the gates forbid to pass. Each channel of the
features maps has different mean and variance. Fig. 7
shows the diverse mean and variance of the different
feature maps according to outputs of gates. The results
shows that the value discrepancy increases in the deep
layer, which indicates that the gate is more conﬁdent

in deeper layers.

Visualization of loop time Our primary interest lies in
understanding the learned gated pattern. Due to the dy-
namic recursive mechanism, loop time varies across im-
ages. Fig. 8 shows the distribution over the total loop times
in DR-ResNet 53 are executed on ImageNet validation set.
On average 11.74 loops are executed with a standard devia-
tion of 1.50. We collect the easy examples which skip most
loops and the hard examples which execute most loops in
Fig. 9 for ImageNet validation set. Images in the same col-
umn are in the same category. Interestingly, the easy exam-
ples are clear and iconic while the hard examples are blurry
and occluded, which are even hard for humans to recongize.

5. Conclusion

In this work, we introduce Dynamic Recursive Neu-
ral Network that reuses the identical blocks on the ﬂy
in a neat way. The usual gradient problem in recursive
networks is solved by our LVBN. DRNN also learns to
adaptively skip redundant loop based on the input. Fur-
ther, we correct the population statistics of LVBN which
is combined with dynamic recursive mechanism. Experi-
ments on ImageNet and CIFAR show that DRNNs reduce
model size and computational cost substantially while out-
performing. The proposed DRNN could be further ex-
tended to densely-connected or inception-based networks
and may help to optimize the learning of long-term depen-
dencies.

5154

References

[1] Y. Bengio. Estimating or propagating gradients through

stochastic neurons. Computer Science, 2013. 4

[2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term
dependencies with gradient descent is difﬁcult. Trans. Neur.
Netw., 5(2):157–166, Mar. 1994. 1

[3] G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learn-
ing efﬁcient object detection models with knowledge distil-
lation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems 30, pages
742–751. Curran Associates, Inc., 2017. 2

[4] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 2857–2865, 2015. 2

[5] Y. L. Cun, J. S. Denker, and S. A. Solla. Optimal brain dam-
age. In Advances in Neural Information Processing Systems,
pages 598–605. Morgan Kaufmann, 1990. 2

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09, 2009. 6

[7] D. Eigen, J. Rolfe, R. Fergus, and Y. Lecun. Understand-
ing deep architectures using a recursive convolutional net-
work. In International Conference on Learning Representa-
tions (ICLR2014), CBLS, April 2014, 2014. 2

[8] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cas-
cade object detection with deformable part models. In Com-
puter vision and pattern recognition (CVPR), 2010 IEEE
conference on, pages 2241–2248. IEEE, 2010. 2

[9] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang,
D. P. Vetrov, and R. Salakhutdinov. Spatially adaptive com-
putation time for residual networks.
In CVPR, volume 2,
page 7, 2017. 1, 2

[10] A. Graves. Long Short-Term Memory. Springer Berlin Hei-

delberg, 2012. 2

[11] A. Graves. Adaptive computation time for recurrent neural

networks. arXiv preprint arXiv:1603.08983, 2016. 1, 2

[12] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015. 2

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 1, 2, 3, 6, 7

[14] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In European Conference on Com-
puter Vision, pages 630–645, 2016. 6

[15] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. In NIPS Deep Learning and Represen-
tation Learning Workshop, 2015. 2

[16] A. G. Howard. Some improvements on deep convolutional
neural network based image classiﬁcation. arXiv preprint
arXiv:1312.5402, 2013. 6

[17] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017. 1, 2

[18] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, vol-
ume 1, page 3, 2017. 1, 2

[19] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.
Deep networks with stochastic depth. In European Confer-
ence on Computer Vision, pages 646–661. Springer, 2016. 6,
7

[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016. 2

[21] Y. A. Ioannou. Training CNNs with Low-Rank Filters for Ef-
ﬁcient Image Classiﬁcation: ICLR 2016 Poster, May 2016.
2

[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
pages 448–456, 2015. 3

[23] E. Jang, S. Gu, and B. Poole. Categorical reparameterization

with gumbel-softmax. 2016. 2, 4

[24] S. Jastrzebski, D. Arpit, N. Ballas, V. Verma, T. Che, and
Y. Bengio. Residual connections encourage iterative infer-
ence. arXiv preprint arXiv:1710.04773, 2017. 2

[25] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive
convolutional network for image super-resolution.
In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2016. 2

[26] A. Krizhevsky. Learning multiple layers of features from

tiny images. 2009. 6

[27] M. Liang and X. Hu. Recurrent convolutional neural network
for object recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2015. 2, 6

[28] Q. Liao and T. Poggio. Bridging the gaps between residual
learning, recurrent neural networks and visual cortex. arXiv
preprint arXiv:1604.03640, 2016. 2

[29] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete dis-
tribution: A continuous relaxation of discrete random vari-
ables. 2017. 4

[30] P. H. Pinheiro and R. Collobert. Recurrent convolutional
neural networks for scene labeling.
In 31st International
Conference on Machine Learning (ICML), number EPFL-
CONF-199822, 2014. 2

[31] A. Polyak and L. Wolf. Channel-level acceleration of deep

face representations. IEEE Access, 3:1–1, 10 2015. 2

[32] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In European Conference on Computer Vision,
pages 525–542. Springer, 2016. 2

[33] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4510–4520, 2018. 2,
7

5155

[34] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1

[35] V. Sindhwani, T. Sainath, and S. Kumar. Structured trans-
forms for small-footprint deep learning.
In Advances in
Neural Information Processing Systems, pages 3088–3096,
2015. 2

[36] R. Socher, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng.
Convolutional-recursive deep learning for 3d object classiﬁ-
cation. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger, editors, Advances in Neural Information Pro-
cessing Systems 25, pages 656–664. Curran Associates, Inc.,
2012. 2

[37] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1–9, 2015. 1, 6

[38] S. Teerapittayanon, B. McDanel, and H. Kung. Branchynet:
Fast inference via early exiting from deep neural networks.
In Pattern Recognition (ICPR), 2016 23rd International
Conference on, pages 2464–2469. IEEE, 2016. 1, 2

[39] A. Veit and S. Belongie. Convolutional networks with adap-

tive inference graphs. 2018. 1, 2, 4, 6, 7

[40] O. Vinyals, S. V. Ravuri, and D. Povey. Revisiting recur-
rent neural networks for robust asr. In Acoustics, Speech and
Signal Processing (ICASSP), 2012 IEEE International Con-
ference on, pages 4085–4088. IEEE, 2012. 2

[41] P. Viola and M. J. Jones. Robust real-time face detection.
International journal of computer vision, 57(2):137–154,
2004. 2

[42] X. Wang, F. Yu, Z.-Y. Dou, and J. E. Gonzalez. Skipnet:
Learning dynamic routing in convolutional networks. arXiv
preprint arXiv:1711.09485, 2017. 1, 2

[43] Y. Yang, Z. Zhong, T. Shen, and Z. Lin. Convolutional neu-
ral networks with alternately updated clique. arXiv preprint
arXiv:1802.10419, 2018. 1, 2

5156

