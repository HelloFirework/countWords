Im2Pencil: Controllable Pencil Illustration from Photographs

Yijun Li1, Chen Fang2, Aaron Hertzmann3, Eli Shechtman3, Ming-Hsuan Yang1

4

,

1UC Merced

2ByteDance AI

3Adobe Research 4Google Cloud

{yli62,mhyang}@ucmerced.edu

fangchen@bytedance.com {elishe,hertzman}@adobe.com

Abstract

ùêø":

Rough

ùêø$:

Clean

ùëÜ":

Hatching

ùëÜ%:

Blending

We propose a high-quality photo-to-pencil translation
method with Ô¨Åne-grained control over the drawing style.
This is a challenging task due to multiple stroke types (e.g.,
outline and shading), structural complexity of pencil shad-
ing (e.g., hatching), and the lack of aligned training data
pairs. To address these challenges, we develop a two-
branch model that learns separate Ô¨Ålters for generating
sketchy outlines and tonal shading from a collection of pen-
cil drawings. We create training data pairs by extracting
clean outlines and tonal illustrations from original pencil
drawings using image Ô¨Åltering techniques, and we manually
label the drawing styles. In addition, our model creates dif-
ferent pencil styles (e.g., line sketchiness and shading style)
in a user-controllable manner. Experimental results on dif-
ferent types of pencil drawings show that the proposed algo-
rithm performs favorably against existing methods in terms
of quality, diversity and user evaluations.

1. Introduction

Pencil is a popular drawing medium often used for quick
sketching or Ô¨Ånely-worked depiction. Notably, two main
components are the outlines that deÔ¨Åne region boundaries,
and shading that reÔ¨Çects differences in the amount of light
falling on a region as well as its intensity or tone and even
texture. Each of these may be applied in various different
styles. For example, pencil outlines may be more or less
‚Äúsketchy‚Äù (Figure 1(a)). Shading may be also more or less
sketchy and use different types of hatching strategies (Fig-
ure 1(b)). Hence, we seek to accurately reproduce these
drawing styles and allow users to select based on personal
preferences.

We split the task into generating the outlines and shading
separately, and express each as an image-to-image trans-
lation problem, learning the mapping from a collection of
pencil drawings. Unfortunately, gathering paired training
data for any artistic stylization task is challenging, due to
the cost, as well as the spatial distortions in drawings [6].
To avoid the burden of gathering ground-truth paired data,

ùëÜ$:

Crosshatching

ùëÜ&:

Stippling

(a) Pencil outlines

(b) Pencil shading

Figure 1. Examples of real pencil drawings in the outline (L1 ‚àº
L2) and shading (S1 ‚àº S4) styles that we train on.

we instead propose to create data pairs. In particular, we
Ô¨Ålter each pencil drawing with procedures that extract out-
lines, tone, and edges. This produces two sets of paired data
that can be used for our two subtasks, learning the outlines
and shading drawing. These Ô¨Ålters generate the same ab-
stractions (outlines, tone, and edges) when applied to input
photographs. Hence, at test-time, we Ô¨Ålter an input photo-
graph and then apply the trained model to produce pencil
illustrations in a user-selected style.

We achieve control over different pencil styles (e.g.,
sketchiness and shading) by training the model with sev-
eral distinct styles, where the style label is provided as an
input selection unit. Moreover, the Ô¨Åltering modules con-
tain additional controllable parameters to generate different
abstracted inputs, which are then mapped to pencil draw-
ings with different characteristics. On the other hand, we
Ô¨Ånd that image translation architectures can often produce
undesirable hatching patterns. We describe these issues and
show how careful design of network architectures can ad-
dress these problems.

A long-term goal of this research direction is to provide
more Ô¨Åne-grained control to neural-network stylization al-
gorithms. Existing learning-based methods do not provide
much control over style except by changing the training in-
put. In contrast, classical procedural stylization algorithms
can provide many different styles (e.g., [1, 35]) but without
the same quality and generality that can come from learn-
ing from examples. Our method allows Ô¨Åne-grained stylis-
tic control as we focus on separating outline and shading
style, and learning several stylistic options for each.

11525

(a) Input

(b) CycleGAN [48]

Style

(c) Gatys et al. [10]

(d) Ours: L1 + S2

(e) Ours: L1 + S4

(f) Ours: L2 + S3

Figure 2. Synthesis results of our algorithm in different combina-
tions of outline and shading styles, compared with existing meth-
ods (zoom in for Ô¨Åne pencil strokes). See Section 4.1 for experi-
mental details.

The main contributions of this work are summarized as

follows:

‚Ä¢ We propose a two-branch framework that learns one
model for generating sketchy outlines and one for tonal
shading, from a pencil drawing dataset.

‚Ä¢ We show how to use abstraction procedures to generate
paired training data for learning to draw with pencils.

‚Ä¢ We demonstrate the ability to synthesize images in
various different pencil drawing styles within a single
framework.

‚Ä¢ We present an architecture that captures hatching tex-
ture well in the shading drawing, unlike existing base-
lines.

2. Related Work

Procedural line drawing. There is a rich literature on
procedural (non-learning) stylization in Non-Photorealistic
Rendering (NPR) [35, 1]. Early work focuses on interactive
pen-and-ink drawing and hatching of 2D inputs [36, 37] and
3D models [22, 45, 46]. Pencil drawing is similar to pen-
and-ink drawing, but it has more degrees-of-freedom since
individual pencil strokes may have varying tone, width, and

texture. For 2D images, several procedural image styliza-
tion approaches have simulated pencil drawings [24, 34].
These methods use hand-crafted algorithms and features for
outlines and a pre-deÔ¨Åned set of pencil texture examples for
shading. While procedural approaches can be fast and in-
terpretable, accurately capturing a wide range of illustration
styles with purely procedural methods is still challenging.

Image-to-image translation. Due to the difÔ¨Åculty in au-
thoring stylization algorithms, numerous approaches have
been proposed to learn them from examples. The Image
Analogy approach uses texture synthesis applied to a sin-
gle training pair [15], requiring strict alignment between
the input photograph and the output drawing or painting.
For line drawing, the paired input images are created man-
ually by applying blurring and sharpening operators sepa-
rately to each input drawing. This method performs well
only for restricted classes of drawings, e.g., when most
hatching strokes have a consistent orientation. Although
neural image-translation methods have been recently devel-
oped [18, 49, 43], none of these has been demonstrated
for stylization, due to the difÔ¨Åculty of gathering aligned,
paired training data. Chen et al. [5] do learn stylization, but
trained on the results generated by an existing procedural
pencil rendering method [32]. However, the rendered draw-
ings exhibit limited sketching and shading styles. More re-
cently, several methods have been developed for learning
mappings from unpaired data [48, 23, 31, 17, 25] with two
conditional adversarial losses and cycle-consistency regu-
larization. However, as shown in Figure 2(b), these meth-
ods do not perform well at capturing hatching texture. Al-
though the Deep Image Analogy [30] method does not re-
quire paired data, it requires data where the exemplar and
target photo have very similar content. Several learning-
based algorithms have been developed solely for facial por-
traiture [44, 41, 2, 9].

Neural style transfer. A third approach is to transfer deep
texture statistics of a style exemplar, which does not em-
ploy paired training data. Since Gatys et al. [10] pro-
posed an algorithm for artistic stylization based on match-
ing the correlations (Gram matrix) between deep features,
numerous methods have been developed for improvements
in different aspects [20], e.g., efÔ¨Åciency [21, 42], general-
ity [5, 16, 28, 4, 27], quality [26, 30, 19], diversity [27],
high-resolution [38], and photorealism [33, 29]. However,
these methods do not perform well for pencil drawing. The
rendered results (Figure 2(c)) in the pencil style only cap-
ture the overall gray tones, but without capturing distinctive
hatching or outline styles well.

Style control.
Procedural methods often provide Ô¨Åne-
grained style control, e.g., [13], but at the cost of consider-
able effort and difÔ¨Åculty in mastering certain styles. Image-
to-image translation [17, 25] and neural style transfer meth-

1526

Created paired data

Outline abstraction

Pencil drawing

64

64

Outline style

256

128

Edge

XDoG

64

64
64

128

64

64

256

64

64

...

64

64

Residual blocks

Input photo

Outline

256

128

128

256

Shading style

64

64

Shading

256

128

128

256

64

64

64

64

64

64

64

64

...

64

64

Residual blocks

256

128

128

256

Upsample

256

64

64

256

16
16

Edge

Tone

Pencil outline

Combination

Pencil shading

{Edge map, tone illustration}

Pencil drawing

Input photo

Figure 3. Pipeline of the proposed algorithm. Left: the created paired training data generated by using an abstraction procedure on pencil
drawings for training. Right: the testing phase (including network details). Two branches will output an outline and shading drawing
result respectively, which can be combined together through pixel-wise multiplication as the third option of pencil drawing result. The
edge module in gray in the outline branch (top) is a boundary detector [7], which is optional at test-time. For highly-textured photos, it is
suggested to use this module to detect boundaries only. See Section 3 for technical details.

ods provide only high-level control, e.g., by selecting train-
ing inputs in a different style, interpolating between unre-
lated styles [8, 11], or selecting among high-level transfer
parameters [11].
In this work, we focus on developing a
method with Ô¨Åne-grained style control that allows subtle ad-
justments to pencil drawing.

3. Stylization Approach

Our approach is based on the observation that pencil
drawings can be separated into two components: outlines,
and shading. The outlines delineate object boundaries and
other boundaries in the scene, and shading or tone uses tonal
techniques such as hatching to depict reÔ¨Çected lighting, tex-
ture, and materials. Hence, our method includes a separate
outline branch and a shading branch. These two models are
trained separately but can be combined at test-time to gen-
erate different combinations of illustration styles. Figure 3
shows the main modules of the proposed method.

For each network branch, paired training data is unavail-
able, and thus we need to create the input-output pairs from
line drawings directly. We generate training data by using
an abstraction procedure on pencil drawings, where the ab-
straction estimates outlines or edges and tones. These Ô¨Ålters
are designed to produce similar abstractions from line draw-
ings as from photographs. Hence, at test-time, the same ab-
straction Ô¨Ålters can be applied to an input photograph, to
produce an input in the same domain as the training inputs.

3.1. Outline branch

The goal of the outline branch is to produce pencil-like
outlines from photos. Since there is no paired training data

for this task, we use an outline extraction algorithm, both to
process the training data and test images at run-time.

Outline extraction. We use the Extended Difference-of-
Gaussians (XDoG) Ô¨Ålter [47], which performs well whether
the input photo is a pencil drawing or a photograph. The
XDoG method takes an input image I, and convolves it
with two separate Gaussian Ô¨Ålters, with standard deviations
œÉ and k ¬∑ œÉ. A sigmoidal function is then applied to the
difference of these two images:

D(I; œÉ, k, œÑ ) = GœÉ(I) ‚àí œÑ ¬∑ Gk¬∑œÉ(I),

EX (D, «´, œï)=(cid:26) 1, if D ‚â• «´

1 + tanh(œï ¬∑ (D ‚àí «´)), otherwise

(1)

(2)

The behavior of the Ô¨Ålter is determined by Ô¨Åve parameters:
{œÉ, k, œÑ, «´, œï} for Ô¨Çexible control over detected edges. At
test-time, users can adjust any parameters to control the line
thickness and sketchiness as shown in Figure 10 and 11.

To demonstrate the effectiveness of XDoG, we compare
it with two alternative approaches for outline abstraction.
The Ô¨Årst one is a boundary detector based on structured
random forests [7]. The results in Figure 4(b) show that
although it detects outlines in photos well, it generally does
not handle thick lines in pencil drawings well and generates
two strokes. The second one is a method designed specif-
ically for sketch simpliÔ¨Åcation [39]. As shown on the top
of Figure 4(c), it obtains simpliÔ¨Åcation results on main con-
tours but does not handle smooth non-outline regions well
(e.g., eyes). More importantly, this sketch simpliÔ¨Åcation
method does not perform well on abstracting outlines of

1527

(a) Input

(b) [7]

(c) [39]

(d) XDoG [47]

Figure 4. Comparisons of different outline abstraction results on
pencil drawings (top) and photos (bottom).

photo inputs (see the bottom row of Figure 4(c)). In con-
trast, the XDoG Ô¨Ålter handles line thickness and smooth
non-outline regions well (Figure 4(d)). For some highly-
textured photos at test-time, the XDoG may produce far too
many edges (the second row of Figure 5(b)). In these cases,
we Ô¨Årst use the boundary detector [7] to extract their con-
tours, which are then Ô¨Åltered by the XDoG.

Paired training data. In order to generate paired training
data, we Ô¨Årst gather a set of pencil outline drawings with
very little shading, from online websites (e.g., Pinterest).
We annotate each drawing with one of two outline style
labels: ‚Äúrough‚Äù or ‚Äúclean‚Äù (Figure 1(a)). The data is col-
lected by searching the outline style as the main query on
web. We collected 30 images for each style. We use a 2-
bit one-hot vector as a selection unit to represent these two
styles, which serves as another network input to guide the
generation towards the selected style. Then for each draw-
ing, we manually select a set of XDoG parameters that pro-
duce good outlines. For example, for a sketchier input, we
use a bigger œÉ to produce one single thick line to cover all
sketchy lines along the same boundary. We crop patches of
size 256√ó256 on the created paired data and conduct vari-
ous augmentations (e.g., rotation, shift), resulting in about
1200 training pairs.

Translation model. As shown on the top row of Fig-
ure 3(right), the translation model is designed as an auto-
encoder with a few residual-based convolutional blocks in-
between. The selection unit is Ô¨Årst mapped from a 2-bit
vector to a 2-channel map, which is then encoded as fea-
ture maps (through convolutions) and concatenated with the
features of the outline input. Before the translation mod-
ule, an XDoG Ô¨Ålter is used to extract outlines from photos.
This module is not included during training, and the rest of
the model is trained on the outline/drawing pairs described
above. For highly-textured images, the boundary (edge) de-
tector may optionally be used prior to XDoG as well. As
described in Section 4.2, adjusting parameters to this XDoG
Ô¨Ålter can be used to vary outline drawing styles, such as line
thickness and sketchiness.

(a) Input

(b) XDoG [47] (c) Edge [7]

(d) Tone

Figure 5. Examples of extracted edge and tone results for highly-
textured inputs.

3.2. Shading branch

The goal of our shading branch is to generate textures in
non-outline regions according to the tonal values of the in-
put. As no paired data is available for learning the shading
branch network, we apply an abstraction procedure to gen-
erate the training data and preprocess inputs at test-time.
The abstraction aims at extracting edges and tones, and re-
moving detailed textures. Then the model learns to apply
pencil shading according to these maps.

Edge and tone extraction. For the edge map, we use the
boundary detector by Doll¬¥ar and Zitnick [7], which identi-
ties important edges, even in highly-textured images. We do
not use XDoG after boundary detection, because clean out-
lines are not necessary for shading generation. An example
comparison between XDoG and the boundary detector for
a highly-textured input is shown in Figure 5(b) and (c).

To extract the tone map, we apply the Guided Filter
(GF) [14] on the luminance channel of shading drawings or
photos to remove details and generate a smoothing output
as the tone extraction. Examples of extracted tone results
are shown in Figure 5(d).

Paired training data. We collect a set of pencil shading
drawings from online websites, and annotate each drawing
with one of our four style labels, i.e., hatching, crosshatch-
ing, blending, and stippling (Figure 1(b)). We searched the
data with each shading style as the main web query and col-
lected 20 shading drawings for each style. As in the outline
branch, we use a 4-bit one-hot vector as a selection unit.
For each drawing, we extract its edge map and tone map to
construct the paired data. We manually select the best pa-
rameters (e.g., the neighborhood size in GF [14]) for each
shading drawing to produce good abstraction results. By
cropping patches of size 256√ó256 and doing data augmen-
tations (e.g., rotation) on the paired data, we create about
3000 training pairs.

1528

(a) Tone (inset: photo patch)

(b) Edge tangent Ô¨Åeld

(a)

Lrec+Ladv

(c) Single stream
tone‚Üíshading

(d) Two-stream

{edge, tone}‚Üí shading

Figure 6. Comparisons of shading results on a photo patch (c)-
(d) obtained using different network architectures. The input is a
smooth photo patch (red inset). (a) Extracted tone map. (b) Edge
tangent Ô¨Åeld of (a). (c) Hatching result from single-stream archi-
tecture. ArtiÔ¨Åcial patterns appear that are unlike normal hatching.
(d) Hatching result from two-stream architecture. Hatching-like
textures are produced and artiÔ¨Åcial patterns are suppressed.

Translation model. We Ô¨Ånd that the direct translation from
the extracted tone to the shading drawing generates signiÔ¨Å-
cant artifacts when applied to photos, especially in smooth
regions. In such regions, artists draw pencil textures with
varying orientations; these strokes typically approximate
the tone but not the speciÔ¨Åc gradients in the image. How-
ever, naive training produces results that attempt to follow
the input gradients too closely. Figure 6(a) shows a smooth
photo patch and its extracted tone. We visualize its gradi-
ent Ô¨Åeld in terms of edge tangent Ô¨Åeld which is perpendicu-
lar to the image gradients using the linear integral convolu-
tions [3]. When simply relying on the tonal input, the shad-
ing result in Figure 6(c) shows that the generated hatching
lines looks quite unnatural by following these small gradi-
ents.

To address the above-mentioned issue, we design a
two-stream translation model to generate the shading (Fig-
ure 3(right-bottom)). The main stream is from the edge map
of the input, where there is no indication of small image
gradients. We employ the tonal abstraction for weak guid-
ance of tone in a secondary input stream that is fused into
the main stream at a deeper layer. Figure 6(d) shows that
the shading output of our two-stream network architecture
signiÔ¨Åcantly reduces the artifacts and exhibits more natural
and realistic strokes. In addition, the 4-bit selection unit is
fed to the network in the same way as in the outline branch
to guide the generation towards different shading styles.

3.3. Learning to draw

With the paired data, we train the model to learn to
translate from abstracted inputs to drawings. Our train-

(b)

(c)

Lper+Ladv Lrec+P Li

adv

(d)
Ours

Figure 7. Comparisons of pencil outline results obtained by models
trained with different loss functions. Inset image in red rectangle:
the XDoG input. Lrec: reconstruction loss. Ladv: adversarial loss
using single discriminator on patches of 256√ó256. Lper: percep-
tual loss. P Li
adv: adversarial loss using three discriminators on
patches of 256√ó256, 128√ó128 and 64√ó64. Our Ô¨Ånal loss is the
combination of Lper and P Li

adv.

ing is based on the existing translation frameworks, e.g.,
Pix2Pix [18]. However, we use different loss functions for
pencil drawing as the existing ones do not perform well for
our task. We now describe our loss functions used for both
branches of our model.

Perceptual loss. We use the perceptual loss [21] to min-
imize the difference between the network output and the
ground truth (GT) pencil drawing based on their deep fea-
tures:

Lper =

4

Xi=1

|| Œ¶i(G(x)) ‚àí Œ¶i(y) ||2

2 ,

(3)

where x, y are the input and the GT pencil drawing, G is the
translation model, and Œ¶i is the VGG-19 [40] network up to
the ReLU i 1 layer. The feature-based perceptual loss has
been shown to generate sharper results than the pixel-based
reconstruction loss Lrec.

Adversarial loss. In addition to the translation model G, we
use the discriminator network D to discriminate between
real samples from the pencil drawings and generated results.
The goal of G is to generate images that cannot be distin-
guished by D. This can be achieved by using an adversarial
loss [12]:

Ladv = min

G

max

D

Ey‚àºPY [log D(y)]

+ Ex‚àºPX [log(1 ‚àí D(G(x)))],

(4)

where PY and PX represent the distributions of pencil
drawing samples y and their abstracted samples x.

To better capture both the global style and local strokes
of pencil drawings, we propose to discriminate between
real data and fake generations at multiple scales during
the training. SpeciÔ¨Åcally, given the generated output of
size 256√ó256, we use three discriminators to discriminate
patches on three scales (256√ó256, 128√ó128, 64√ó64). Each
discriminator is designed as the PatchGAN used in [18].

1529

Style

Style

Style

(a) Input

(b) Gatys et al. [10]

(c) CycleGAN [48]

(d) Lu et al. [32]

(e) Ours

Figure 8. Visual comparisons of different methods for rendering pencil drawing effects (zoom in for details). The exemplars used for [10,
32] are shown in the pink rectangle. Top: to directly show pencil effects for the outline, we select a simple line input which is only Ô¨Åltered
by the XDoG (no need to detect boundaries Ô¨Årst). Bottom: pencil shading effects on a real photo example.

The overall loss function is deÔ¨Åned by:

L = Lper + Œ≤

3

Xi=1

Li

adv,

where Œ≤ is the weight to balance different losses, and Li
adv
is the adversarial loss of the ith discriminator. We set Œ≤ =
100 in all experiments. When learning with multiple styles,
in each iteration, all examples in the batch are limited to be
of the same style. Meanwhile, we set the corresponding bit
that represents the selected style as 1 and leave other bits as
0 in the selection unit.

Figure 7 shows the outline results from models trained
with different losses. It is observed that the perceptual loss
encourages better sharpness for a single line drawn along
the silhouette compared with the reconstruction loss. Mean-
while, using multiple discriminators helps synthesize better
pencil strokes with more sketchy details than employing just
one discriminator. Figure 7(d) shows that outline results ob-
tained by the proposed method using the loss function in (5)
look more like a real drawing.

4. Experimental Results

In this section, we present extensive experimental results
to demonstrate the effectiveness of our algorithm. We com-
pare with methods from both NPR and deep neural network-
based stylization. We experiment with synthesizing pencil
drawings in various outline and shading styles in a user-
controllable manner. More results and comparisons are
shown in the supplementary material1.

1http://bit.ly/cvpr19-im2pencil-supp

Table 1. User preference towards different methods (%). Each row
represents one user study, comparing three stylization algorithms.
The top row is applied to the input image directly, and the bottom
row uses the tonal adjustment of [32] as a preprocess.

(5)

Methods

CycleGAN [48] Lu et al. [32] Ours

Original tone
Adjusted tone

10.3
7.1

11.4
32.6

78.3
60.3

4.1. Comparisons

We compare with three algorithms [10, 48, 32] that rep-
resent neural style transfer, unpaired image-to-image trans-
lation, and NPR respectively. As the method of Gatys et
al. [10] is example-based, we select a representative pen-
cil outline example (the pink inset in Figure 8(b)) from
our dataset to obtain their style transfer results. For Cy-
cleGAN [48], in order to train a model for pencil drawing,
we collect a photo dataset that consists of 100 images from
online websites. Together with our pencil dataset, they con-
struct an unpaired dataset that is used to train CycleGAN for
translation between the two domains. Note that since Cycle-
GAN only supports transferring a certain kind of style, one
needs to train different CycleGAN model for each outline
and shading styles. The NPR method of Lu et al. [32] has
a two-phase design as well, treating the outline and shad-
ing drawing separately. The shading drawing phase also
requires a real pencil shading example. Since Lu et al. [32]
do not release the shading example used for their results, we
select a representative pencil shading example (the pink in-
set in Figure 8(d)) from our dataset to generate their results.
Figure 8 shows the visual comparison of three meth-

1530

(a) Input

(b) [32] w/ tone adjust

(a) Input

(b) Boundary [7]

(c) Clean: base

(c) Ours w/o tone adjust

(d) Ours w/ tone adjust

Figure 9. Comparisons of shading results between w/ and w/o ad-
justing the tone of input.

ods on outline drawing (top) and shading drawing (bottom).
The results by Gatys et al. [10] in (b) only exhibit some
global gray-like feel of pencil drawing. The lines are un-
natural and shading is not correctly positioned to reÔ¨Çect the
contrast in the input. CycleGAN [48] generates a few ran-
dom textures around outlines and inside regions, leading
to results that look like the gray image of the input with-
out clear pencil strokes, as shown in Figure 8(c). Without
a paired correspondence, simply relying on the cycle con-
straint and adversarial training is still limited in capturing
the real distribution of the pencil drawing domain, with re-
alistic strokes. The shading results of Lu et al. [32] in the
second row of Figure 8(d) show shading lines and crossings
that have no correlation with the underlying lines, struc-
tures and ‚Äúnatural‚Äù orientations of some content (e.g., the
water Ô¨Çow). The shading lines come from a real drawing
but the overall result does not look like one. In addition,
their gradient-based features also result in detecting the two
sides of thick lines in the Ô¨Årst row of Figure 8(d), which
is uncommon in drawing strokes. In contrast, our results
in Figure 8(e) present more realistic pencil strokes in rea-
sonable drawing directions and contain better shading that
corresponds to the contrast in the input.

User study. We resort to user studies for the quantitative
evaluation of [48, 32] and our method as pencil drawing
synthesis is originally a highly subjective task. The method
of Gatys et al. [10] is not included in the user study because
it was clearly inferior to the others in our early experiments
(Figure 8(b)). We use 30 natural images provided in [32]
and randomly select 15 images for each subject. We dis-
play the results by all three methods side-by-side in random
order and ask each subject to vote for one result that looks
the most like a pencil drawing. We Ô¨Ånally collect the feed-
back from 50 subjects of totally 750 votes and show the

(d) Clean: œÉ=3

(e) Rough: œÉ=3

(f) Clean: œÑ =0.97

Figure 10. Outline results for a highly-textured photo. (b) is the
boundary map of input (a), which is then Ô¨Åltered by the XDoG
with different parameters. We set œÉ = 2.0,œÑ = 0.99, k = 1.6,
«´ = 0.1, œï = 200 in XDoG and show the base pencil outlines
in (c). (d)-(f) show the results by adjusting one parameter while
keeping others Ô¨Åxed.

(a) Input

(b) Rough: base

(c) Rough: œÉ=4.5

(d) Clean: œÉ=4.5 (e) Rough: œÑ =0.98 (f) Rough: «´=1.6

Figure 11. Outline results for a simple cartoon image. The input
in (a) is directly Ô¨Åltered by the XDoG. We set œÉ = 2.5,œÑ = 0.96,
k = 1.6, «´ = 0.1, œï = 200 and show the base pencil outlines in
(b). (c)-(f) show diverse outline results by controlling parameters.

percentage of votes each method received in the top row of
Table 1. The study shows that our method receives the most
votes for better pencil effects, nearly seven times as much
as those of other two methods.

Lu et al. [32] observed that pencil drawings often ex-
hibit global tonal changes from the input, and described a
histogram-based tone adjustment step to model this obser-
vation. In order to fairly compare with this step, we per-
form a second user study where the input is preprocessed
by this step. The user study results with tone adjustment
are shown in the bottom row of Table 1. Again, our method
obtains substantially more votes than the previous methods.
We show our results with and without tone adjustment in

1531

Input

Hatching

Crosshatching

Blending

Stippling

Figure 12. Four types of shading results of the proposed algorithm by switching bits in the selection unit (zoom in for details).

Figure 9(c) and (d) as well as the corresponding result of
Lu et al. [32] in (b). The tone adjustment step provides an
additional user control for our method as well.

4.2. User control

Our translation model provides Ô¨Åne-grained control over
different pencil drawing styles. Figure 10 and 11 show
that users could either switch between clean and rough out-
line style through the selection unit, or adjust parameters
in XDoG to obtain different outline results. The photo ex-
ample in Figure 10(a) is highly-textured in the clothes and
background, so we Ô¨Årst use the boundary detector [7] to de-
tect its boundary map, which is then Ô¨Åltered by the XDoG.
As the most sensitive and important parameter in XDoG,
the œÉ deÔ¨Ånes the line thickness and sketchiness. Gener-
ally, when a clean style is selected, increasing the value of
œÉ leads to thicker lines (Figure 10(c)-(d)). When a rough
style is selected, increasing the value of œÉ results in increase
in repetitive lines (Figure 10(e)). In addition, by adjusting
other parameters (e.g., œÑ ), the XDoG Ô¨Ålter is able to con-
trol the sensitivity on detected edges, which allows users to
draw both strong and weak edges (Figure 10(f)). In Fig-
ure 11, we show the outline results for a simple cartoon im-
age without heavy textures. Figure 12 shows shading results
of two examples, i.e., a still life and a portrait ‚Äì two popular
reference choices for a pencil drawing. By controlling the
selection unit, users get results in different shading styles.

Color pencil drawings. The extension of our algorithm to
color pencil drawing is quite straightforward, following the
method of [15, 11]. We Ô¨Årst convert the input image from
RGB to LAB color space, then replace the L channel with
that of our generated gray-scale pencil drawing result, and
Ô¨Ånally map back to RGB space. Figure 13 shows two color

(a) Input

(b) Ours: L1 + S2 (c) Ours: L2 + S4

Figure 13. Extension of our algorithm to color pencils in differ-
ent outline and shading styles (zoom in for Ô¨Åne details). Pencil
outlines of all examples are generated by applying the XDoG and
learned model on their boundary maps.

pencil results in different outline and shading styles.

5. Conclusions

In this work, we propose a photo-to-pencil translation
method with Ô¨Çexible control over different drawing styles.
We design a two-branch network that learns separate Ô¨Ålters
for outline and shading generation respectively. To facili-
tate the network training, we introduce Ô¨Åltering/abstraction
techniques into deep models that avoid the heavy burden of
collecting paired data. Our model enables multi-style syn-
thesis in a single network to produce diverse results. We
demonstrate the effectiveness and Ô¨Çexibility of the proposed
algorithm on different pencil outline and shading styles.

Acknowledgment. This work is supported in part by the
NSF CAREER Grant #1149783, and gifts from Adobe,
Verisk, and NEC. YJL is supported by Adobe and Snap Inc.
Research Fellowship.

1532

References

[1] P. B¬¥enard and A. Hertzmann. Line drawings from 3d models.

arXiv preprint arXiv:1810.01175, 2018. 1, 2

[2] I. Berger, A. Shamir, M. Mahler, E. Carter, and J. Hodgins.
Style and abstraction in portrait sketching. ACM Transac-
tions on Graphics, 32(4):55, 2013. 2

[3] B. Cabral and L. C. Leedom. Imaging vector Ô¨Åelds using line

integral convolution. In SIGGRAPH, 1993. 5

[4] D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua. Stylebank:
An explicit representation for neural image style transfer. In
CVPR, 2017. 2

[5] Q. Chen, J. Xu, and V. Koltun. Fast image processing with

fully-convolutional networks. In ICCV, 2017. 2

[6] F. Cole, A. Golovinskiy, A. Limpaecher, H. S. Barros,
A. Finkelstein, T. Funkhouser, and S. Rusinkiewicz. Where
do people draw lines?
ACM Transactions on Graphics,
27(3):88, 2008. 1

[7] P. Doll¬¥ar and C. L. Zitnick. Structured forests for fast edge

detection. In ICCV, 2013. 3, 4, 7, 8

[8] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style. In ICLR, 2017. 3

[9] J. FiÀáser, O. JamriÀáska, M. Luk¬¥aÀác, E. Shechtman, P. Asente,
J. Lu, and D. S`ykora. Stylit: illumination-guided example-
based stylization of 3d renderings. ACM Transactions on
Graphics, 35(4):92, 2016. 2

[10] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, 2016. 2, 6, 7
[11] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In CVPR, 2017. 3, 8

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 5

[13] S. Grabli, E. Turquin, F. Durand, and F. X. Sillion. Pro-
grammable rendering of line drawing from 3d scenes. ACM
Transactions on Graphics, 29(2):18, 2010. 2

[14] K. He, J. Sun, and X. Tang. Guided image Ô¨Åltering. PAMI,

35(6):1397‚Äì1409, 2013. 4

[15] A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H.

Salesin. Image analogies. In SIGGRAPH, 2001. 2, 8

[16] X. Huang and S. Belongie. Arbitrary style transfer in real-
time with adaptive instance normalization. In ICCV, 2017.
2

[17] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. In ECCV, 2018. 2
[18] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 2, 5

[19] Y. Jing, Y. Liu, Y. Yang, Z. Feng, Y. Yu, D. Tao, and M. Song.
Stroke controllable fast style transfer with adaptive receptive
Ô¨Åelds. In ECCV, 2018. 2

[20] Y. Jing, Y. Yang, Z. Feng, J. Ye, Y. Yu, and M. Song. Neural
style transfer: A review. arXiv preprint arXiv:1705.04058,
2017. 2

[22] E. Kalogerakis, D. Nowrouzezahrai, S. Breslav, and
A. Hertzmann. Learning hatching for pen-and-ink illustra-
tion of surfaces. ACM Transactions on Graphics, 31(1):1,
2012. 2

[23] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. In ICML, 2017. 2

[24] H. Lee, S. Kwon, and S. Lee. Real-time pencil rendering. In

NPAR, 2006. 2

[25] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, and M.-H.
Yang. Diverse image-to-image translation via disentangled
representations. In ECCV, 2018. 2

[26] C. Li and M. Wand. Combining markov random Ô¨Åelds and
convolutional neural networks for image synthesis. In CVPR,
2016. 2

[27] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
DiversiÔ¨Åed texture synthesis with feed-forward networks. In
CVPR, 2017. 2

[28] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
In NIPS,

Universal style transfer via feature transforms.
2017. 2

[29] Y. Li, M.-Y. Liu, X. Li, M.-H. Yang, and J. Kautz. A closed-
form solution to photorealistic image stylization. In ECCV,
2018. 2

[30] J. Liao, Y. Yao, L. Yuan, G. Hua, and S. B. Kang. Visual
attribute transfer through deep image analogy. ACM Trans-
actions on Graphics, 36(4), 2017. 2

[31] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-

image translation networks. In NIPS, 2017. 2

[32] C. Lu, L. Xu, and J. Jia. Combining sketch and tone for

pencil drawing production. In NPAR, 2012. 2, 6, 7, 8

[33] F. Luan, S. Paris, E. Shechtman, and K. Bala. Deep photo

style transfer. In CVPR, 2017. 2

[34] X. Mao. Automatic generation of pencil drawing from 2d
images using line integral convolution. In CAD/GRAPHICS,
2001. 2

[35] P. Rosin and J. Collomosse. Image and Video-Based Artistic

Stylisation. Springer, 2013. 1, 2

[36] M. P. Salisbury, S. E. Anderson, R. Barzel, and D. H. Salesin.
Interactive pen-and-ink illustration. In SIGGRAPH, 1994. 2
[37] M. P. Salisbury, M. T. Wong, J. F. Hughes, and D. H. Salesin.
Orientable textures for image-based pen-and-ink illustration.
In SIGGRAPH, 1997. 2

[38] A. Sanakoyeu, D. Kotovenko, S. Lang, and B. Ommer. A
In

style-aware content loss for real-time hd style transfer.
ECCV, 2018. 2

[39] E. Simo-Serra, S. Iizuka, K. Sasaki, and H. Ishikawa. Learn-
ing to simplify: Fully convolutional networks for rough
sketch cleanup. ACM Transactions on Graphics, 35(4),
2016. 3, 4

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5

[41] Y. Song, L. Bao, Q. Yang, and M.-H. Yang. Real-time

exemplar-based face sketch synthesis. In ECCV, 2014. 2

[21] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, 2016.
2, 5

[42] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Tex-
ture networks: Feed-forward synthesis of textures and styl-
ized images. In ICML, 2016. 2

1533

[43] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional gans. In CVPR, 2018. 2

[44] X. Wang and X. Tang. Face photo-sketch synthesis and

recognition. PAMI, 31(11):1955‚Äì1967, 2009. 2

[45] G. Winkenbach and D. H. Salesin. Computer-generated pen-

and-ink illustration. In SIGGRAPH, 1994. 2

[46] G. Winkenbach and D. H. Salesin. Rendering parametric

surfaces in pen and ink. In SIGGRAPH, 1996. 2

[47] H. Winnem¬®oller, J. E. Kyprianidis, and S. C. Olsen. XDoG:
an extended difference-of-gaussians compendium includ-
ing advanced image stylization. Computers & Graphics,
36(6):740‚Äì753, 2012. 3, 4

[48] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In ICCV, 2017. 2, 6, 7

[49] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-
to-image translation. In NIPS, 2017. 2

1534

