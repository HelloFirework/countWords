LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking

Heng Fan1∗ Liting Lin2∗ Fan Yang1∗ Peng Chu1∗ Ge Deng1 Sijia Yu1 Hexin Bai1

Yong Xu2 Chunyuan Liao3 Haibin Ling1†

1Department of Computer and Information Sciences, Temple University, Philadelphia, USA

2School of Computer Science & Engineering, South China Univ. of Tech., Guangzhou,

Peng Cheng Laboratory, Shenzhen, China

3Meitu HiScene Lab, HiScene Information Technologies, Shanghai, China

https://cis.temple.edu/lasot/

Abstract

In this paper, we present LaSOT, a high-quality bench-
mark for Large-scale Single Object Tracking. LaSOT con-
sists of 1,400 sequences with more than 3.5M frames in to-
tal. Each frame in these sequences is carefully and man-
ually annotated with a bounding box, making LaSOT the
largest, to the best of our knowledge, densely annotated
tracking benchmark. The average video length of LaSOT
is more than 2,500 frames, and each sequence comprises
various challenges deriving from the wild where target ob-
jects may disappear and re-appear again in the view. By re-
leasing LaSOT, we expect to provide the community with a
large-scale dedicated benchmark with high quality for both
the training of deep trackers and the veritable evaluation of
tracking algorithms. Moreover, considering the close con-
nections of visual appearance and natural language, we en-
rich LaSOT by providing additional language speciﬁcation,
aiming at encouraging the exploration of natural linguistic
feature for tracking. A thorough experimental evaluation of
35 tracking algorithms on LaSOT is presented with detailed
analysis, and the results demonstrate that there is still a big
room for improvements.

1. Introduction

Visual tracking, aiming to locate an arbitrary target in
a video with an initial bounding box in the ﬁrst frame, has
been one of the most important problems in computer vision
with many applications such as video surveillance, robotics,
human-computer interaction and so forth [32, 47, 54]. With
considerable progresses in the tracking community, numer-
ous algorithms have been proposed. In this process, track-
ing benchmarks have played a vital role in objectively eval-

∗Authors make equal contributions to this work.
†Corresponding author.

UAV20L

UAV123

OTB-2013

OTB-2015

VOT-2014

VOT-2017

TC-128

NUS-PRO

LaSOT

Figure 1. Summaries of existing tracking benchmarks with high-
quality dense (per frame) annotations, including OTB-2013 [52],
OTB-2015 [53], TC-128 [35], NUS-PRO [28], UAV123 [39],
UAV20L [39], VOT-2014 [26], VOT-2017 [27] and LaSOT. The
circle diameter is in proportion to the number of frames of a bench-
mark. The proposed LaSOT is larger than all other benchmarks,
and focused on long-term tracking. Best viewed in color.

uating and comparing different trackers. Nevertheless, fur-
ther development and assessment of tracking algorithms are
restricted by existing benchmarks with several issues:
Small-scale. Deep representations have been popularly ap-
plied to modern object tracking algorithms, and demon-
strated state-of-the-art performances. However, it is difﬁ-
cult to train a deep tracker using tracking-speciﬁc videos
due to the scarcity of large-scale tracking datasets. As
shown in Fig. 1, existing datasets seldom have more than
400 sequences. As a result, researchers are restricted to
leverage either the pre-trained models (e.g., [46] and [18])
from image classiﬁcation for deep feature extraction or the
sequences from video object detection (e.g., [45] and [43])
for deep feature learning, which may result in suboptimal
tracking performance because of the intrinsic differences
among different tasks [55]. Moreover, large scale bench-
marks are desired for more reliable evaluation results.
Lack of high-quality dense annotations. For tracking,

5374

Benchmark

OTB-2013 [52]
OTB-2015 [53]
TC-128 [35]
VOT-2014 [26]
VOT-2017 [27]
NUS-PRO [28]
UAV123 [39]
UAV20L [39]
NfS [14]
GOT-10k [22]

Table 1. Comparison of LaSOT with the most popular dense benchmarks in the literatures.
Class
balance

Median
frames

Absent
labels

Object
classes

Mean
frames

Max
frames

Total
frames

frame
rate

duration

frames

Total

Min

Videos

Num. of
attributes

Lingual
feature

51
100
128
25
60
365
123
20
100
10,000

71
71
71
164
41
146
109
1,717
169

-

578
590
429
409
356
371
915
2,934
3,830

-

392
393
365
307
293
300
882
2,626
2,448

-

3,872
3,872
3,872
1,210
1,500
5,040
3,085
5,527
20,665

-

29K
59K
55K
10K
21K
135K
113K
59K
383K
1.5M

16.4 min
32.8 min
30.7 min
5.7 min
11.9 min
75.2 min
62.5 min
32.6 min
26.6 min

-

30 fps
30 fps
30 fps
30 fps
30 fps
30 fps
30 fps
30 fps
240 fps
10 fps

LaSOT

1,400

1,000

2,506

2,053

11,397

3.52M 32.5 hours

30 fps

✗

✗

✗

✗

✗

✗

✗

✗

✗

✓

✓

10
16
27
11
24
8
9
5
17
563

70

✗

✗

✗

✗

✗

✗

✗

✗

✗

✗

✓

11
11
11
n/a
n/a
n/a
12
12
9
6

14

✗

✗

✗

✗

✗

✗

✗

✗

✗

✗

✓

dense (i.e., per frame) annotations with high precision are
of importance for several reasons. (i) They ensure more ac-
curate and reliable evaluations; (ii) they offer desired train-
ing samples for the training of tracking algorithms; and
(iii) they provide rich temporal contexts among consecu-
tive frames that are important for tracking tasks. It is worth
noting that there are recently proposed benchmarks toward
large-scale and long-term tracking, such as [41] and [51],
their annotations are however either semi-automatic (e.g.,
generated by a tracking algorithm) or sparse (e.g., labeled
every 30 frames), limiting their usabilities.
Short-term tracking. A desired tracker is expected to be
capable of locating the target in a relative long period, in
which the target may disappear and re-enter the view. How-
ever, most existing benchmarks have been focused on short-
term tracking where the average sequence length is less than
600 frames (i.e., 20 seconds for 30 fps, see again Fig. 1) and
the target almost always appears in the video frame. The
evaluations on such short-term benchmarks may not reﬂect
the real performance of a tracker in real-world applications,
and thus restrain the deployment in practice.
Category bias. A robust tracking system should exhibit
stable performance insensitive to the category the target be-
longs to, which signiﬁes that the category bias (or class im-
balance) should be inhibited in both training and evaluating
tracking algorithms. However, existing benchmarks usually
comprise only a few categories (see Tab. 1) with unbalanced
numbers of videos.

In the literature, many datasets have been proposed to
deal with the issues above: e.g., [39, 51] for long-term
tracking, [41] for large-scale, [52, 35, 25] for precise dense
annotations. Nevertheless, none of them addresses all the
issues, which motivates the proposal of LaSOT.

1.1. Contribution

With the above motivations, we provide the community
a novel benchmark for Large-scale Single Object Tracking
(LaSOT) with multi-fold contributions:
1) LaSOT consists of 1,400 videos with average 2,512
frames per sequence. Each frame is carefully inspected
and manually labeled, and the result visually double-
checked and corrected when needed. This way, we gen-

erate around 3.52 million high-quality bounding box an-
notations. Moreover, LaSOT contains 70 categories with
each consisting of twenty sequences. To our knowledge,
LaSOT is the largest benchmark with high-quality man-
ual dense annotations for object tracking to date. By re-
leasing LaSOT, we aim to offer a dedicated platform for
the development and assessment of tracking algorithms.
2) Different from existing datasets, LaSOT provides both
visual bounding box annotations and rich natural lan-
guage speciﬁcation, which has recently been proven to
be beneﬁcial for various vision tasks (e.g., [21, 31]) in-
cluding visual tracking [34]. By doing so, we aim to en-
courage and facilitate explorations of integrating visual
and lingual features for robust tracking performance.

3) To assess existing trackers and provide extensive base-
lines for future comparisons on LaSOT, we evaluate 35
representative trackers under different protocols, and an-
alyze their performances using different metrics.

2. Related Work

With considerable progresses in the tracking community,
many trackers and benchmarks have been proposed in re-
cent decades. In this section, we mainly focus on the track-
ing benchmarks that are relevant to our work, and refer the
readers to surveys [32, 47, 54, 30] for tracking algorithms.
For a systematic review, we intentionally classify track-
ing benchmarks into two types: one with dense manual an-
notations (referred to as dense benchmark for short) and the
other one with sparse and/or (semi-)automatic annotations.
In the following, we review each of these two categories.

2.1. Dense Benchmarks

Dense tracking benchmark provides dense bounding box
annotations for each video sequence. To ensure high qual-
ity, the bounding boxes are usually manually labeled with
careful inspection. For the visual tracking task, these highly
precise annotations are desired for both training and assess-
ing trackers. Currently, the popular dense benchmarks con-
tain OTB [52, 53], TC-128 [35], VOT [25], NUS-PRO [28],
UAV [39], NfS [14] and GOT-10k [22].
OTB. OTB-2013 [52] ﬁrstly contributes a testing dataset

5375

by collecting 51 videos with manually annotated bounding
box in each frame. The sequences are labeled with 11 at-
tributes for further analysis of tracking performance. Later,
OTB-2013 is extended to the larger OTB-2015 [53] by in-
troducing extra 50 sequences.
TC-128. TC-128 [35] comprises 128 videos that are specif-
ically designated to evaluate color-enhanced trackers. The
videos are labeled with 11 similar attributes as in OTB [52].
VOT. VOT [25] introduces a series of tracking competitions
with up to 60 sequences in each of them, aiming to evalu-
ate the performance of a tracker in a relative short duration.
Each frame in the VOT datasets is annotated with a rotated
bounding box with several attributes.
NUS-PRO. NUS-PRO [28] contains 365 sequences with a
focus on human and rigid object tracking. Each sequence in
NUS-PRO is annotated with both target location and occlu-
sion level for evaluation.
UAV. UAV123 and UAV20L [39] are utilized for unmanned
aerial vehicle (UAV) tracking, comprising 123 short and 20
long sequences, respectively. Both UAV123 and UAV20L
are labeled with 12 attributes.
NfS. NfS [14] provides 100 sequences with a high framer-
ate of 240 fps, aiming to analyze the effects of appearance
variations on tracking performance.
GOT-10k. GOT-10k [22] consists of 10,000 videos, aim-
ing to provide rich motion trajectories for developing and
evaluating trackers.

LaSOT belongs to the category of dense tracking dataset.
Compared to others, LaSOT is the largest with 3.52 million
frames and an average sequence length of 2,512 frames. In
addition, LaSOT provides extra lingual description for each
video while others do not. Tab. 1 provides a detailed com-
parison of LaSOT with existing dense benchmarks.

2.2. Other Benchmarks

In addition to the dense tracking benchmarks, there ex-
ist other benchmarks which may not provide high-quality
annotations for each frame. Instead, these benchmarks are
either annotated sparsely (e.g., every 30 frames) or labeled
(semi-)automatically by tracking algorithms. Representa-
tives of this type of benchmarks include ALOV [47], Track-
ingNet [41] and OxUvA [51]. ALOV [47] consists of 314
sequences labeled in 14 attributes. Instead of densely an-
notating each frame, ALOV provides annotations every 5
frames. TrackingNet [41] is a subset of the video object
detection benchmark YT-BB [43] by selecting 30K videos,
each of which is annotated by a tracker. Though the tracker
used for annotation is proven to be reliable in a short period
(i.e., 1 second) on OTB 2015 [53], it is difﬁcult to guarantee
the same performance on a harder benchmark. Besides, the
average sequence length of TrackingNet does not exceed
500 frames, which may not demonstrate the performance of
a tracker in long-term scenarios. OxUvA [51] also comes

from YT-BB [43]. Unlike TrackingNet, OxUvA is focused
on long-term tracking. It contains 366 videos with an av-
erage length of around 4,200 frames. However, a problem
with OxUvA is that it does not provide dense annotations
in consecutive frames. Each video in OxUvA is annotated
every 30 frames, ignoring rich temporal context between
consecutive frames when developing a tracking algorithm.
Despite reduction of annotation cost, the evaluations on
these benchmarks may not faithfully reﬂect the true per-
formances of tracking algorithms. Moreover, it may cause
problems for some trackers that need to learn temporal mod-
els from annotations, since the temporal context in these
benchmarks may be either lost due to sparse annotation or
inaccurate due to potentially unreliable annotation. By con-
trast, LaSOT provides a large set of sequences with high-
quality dense bounding box annotations, which makes it
more suitable for developing deep trackers as well as evalu-
ating long-term tracking in practical application.

3. The Proposed LaSOT Benchmark

3.1. Design Principle

LaSOT aims to offer the community a dedicated dataset
for training and assessing trackers. To such purpose, we fol-
low ﬁve principles in constructing LaSOT, including large-
scale, high-quality dense annotations, long-term tracking,
category balance and comprehensive labeling.
1) Large-scale. One of the key motivations of LaSOT is to
provide a dataset for training data-hungry deep trackers,
which require a large set of annotated sequences. Ac-
cordingly, we expect such a dataset to contain at least a
thousand videos with at least a million frames.

2) High-quality dense annotations. As mentioned before,
a tracking dataset is desired to have high-quality dense
bounding box annotations, which are crucial for training
robust trackers as well as for faithful evaluation. For this
purpose, each sequence in LaSOT is manually annotated
with additional careful inspection and ﬁne-tuning.

3) Long-term tracking.

In comparison with short-term
tracking, long-term tracking can reﬂect more practical
performance of a tracker in the wild. We ensure that each
sequence comprises at least 1,000 frames, and the aver-
age sequence length in LaSOT is around 2,500 frames.

4) Category balance. A robust tracker is expected to per-
form consistently regardless of the category the target
object belongs to. For this purpose, in LaSOT we in-
clude a diverse set of objects from 70 classes and each
class contains equal number of videos.

5) Comprehensive labeling. As a complex task, tracking
has recently seen improvements from natural language
speciﬁcation. To stimulate more explorations, a princi-
ple of LaSOT is to provide comprehensive labeling for
videos, including both visual and lingual annotations.

5376

3.2. Data Collection

Our benchmark covers a wide range of object categories
in diverse contexts. Speciﬁcally, LaSOT consists of 70 ob-
ject categories. Most of the categories are selected from the
1,000 classes from ImageNet [12], with a few exceptions
(e.g., drone) that are carefully chosen for popular tracking
applications. Different from existing dense benchmarks that
have less than 30 categories and typically are unevenly dis-
tributed, LaSOT provides the same number of sequences for
each category to alleviate potential category bias. Details of
the dataset can be found in the supplementary material.

After determining the 70 object categories in LaSOT, we
have searched for the videos of each class from YouTube.
Initially, we collect over 5,000 videos. With a joint consid-
eration of the quality of videos for tracking and the design
principles of LaSOT, we pick out 1,400 videos. However,
these 1,400 sequences are not immediately available for the
tracking task because of a large amount of irrelevant con-
tents. For example, for the video of person category (e.g., a
sporter), it often contains some introduction content of each
sporter in the beginning, which is undesirable for tracking.
Therefore, we carefully ﬁlter out these unrelated contents in
each video and retain an usable clip for tracking. In addi-
tion, each category in LaSOT consists of 20 targets, reﬂect-
ing the category balance and varieties of natural scenes.

Eventually, we have compiled a large-scale dataset by
gathering 1,400 sequences with 3.52 million frames from
YouTube under Creative Commons licence. The average
video length of LaSOT is 2,512 frames (i.e., 84 seconds for
30 fps). The shortest video contains 1,000 frames (i.e., 33
seconds), while the longest one consists of 11,397 frames
(i.e., 378 seconds).

3.3. Annotation

In order to provide consistent bounding box annotation,
we deﬁne a deterministic annotation strategy. Given a video
with a speciﬁc tracking target, for each frame, if the target
object appears in the frame, a labeler manually draws/edits
its bounding box as the tightest up-right one to ﬁt any vis-
ible part of the target; otherwise, the labeler gives an ab-
sent label, either out-of-view or full occlusion, to the frame.
Note that, such strategy can not guarantee to minimize the
background area in the box, as observed in any other bench-
marks. However, the strategy does provide a consistent an-
notation that is relatively stable for learning the dynamics.
While the above strategy works great most of the time,
exceptions exist. Some objects, e.g. a mouse, may have long
and thin and highly deformable part, e.g. a tail, which not
only causes serious noise in object appearance and shape,
but also provides little information for localizing of the tar-
get object. We carefully identify such objects and associ-
ated videos in LaSOT, and design speciﬁc rules for their an-
notation (e.g., exclude the tails of mice when drawing their

Bear-12: “white bear walking on grass around the river bank”

Bus-19: “red bus running on the highway”

Horse-1: “brown horse running on the ground”

Person-14: “boy in black suit dancing in front of people”

Mouse-6: “white mouse moving on the ground around another white mouse”

Figure 2. Example sequences and annotations of our LaSOT. We
focus on long-term videos in which target objects may disappear,
and then re-enter the view again. In addition, we provide natural
language speciﬁcation for each sequence. Best viewed in color.

initial annotation

fine-tuned annotation

Figure 3. Examples of ﬁne-tuning initial annotations.

bounding boxes). An example of such cases is shown in the
last row of Fig. 2.

The natural language speciﬁcation of a sequence is repre-
sented by a sentence that describes the color, behavior and
surroundings of the target. For LaSOT, we provide 1,400
sentences for all videos. Note that the lingual description
aims to provide auxiliary help for tracking. For instance,
if a tracker generates proposals for further processing, the
lingual speciﬁcation can assist in reducing the ambiguity
among them by serving as a global semantic guidance.

The greatest effort for constructing a high-quality dense
tracking dataset is, apparently, the manual labeling, double-
checking, and error correcting. For this task, we have as-
sembled an annotation team containing several Ph.D. stu-
dents working on related areas and about 10 volunteers. To
guarantee high-quality annotation, each video is processed
by teams: a labeling team and a validation team. A labeling
team is composed of a volunteer and an expert (Ph.D. stu-
dent). The volunteer manually draws/edits the target bound-
ing box in each frame, and the expert inspects the results
and adjusts them if necessary. Then, the annotation results
are reviewed by the validation team containing several (typ-

5377

Attribute

Deﬁnition

Attribute

Deﬁnition

Table 2. Descriptions of 14 different attributes in LaSOT.

CM Abrupt motion of the camera
ROT
DEF
FOC
IV
OV
POC

The target rotates in the image
The target is deformable during tracking
The target is fully occluded in the sequence
The illumination in the target region changes
The target completely leaves the video frame
The target is partially occluded in the sequence

VC
SV
BC
MB
ARC
LR
FM The motion of the target is larger than the size of its bounding box

Viewpoint affects target appearance signiﬁcantly
The ratio of bounding box is outside the rage [0.5, 2]
The background has the similar appearance as the target
The target region is blurred due to target or camera motion
The ratio of bounding box aspect ratio is outside the rage [0.5, 2]
The target box is smaller than 1000 pixels in at least one frame

Figure 4. Distribution of sequences in each attribute on LaSOT and comparison with other benchmarks. Best viewed in color.

ically three) experts. If an annotation result is not unani-
mously agreed by the members of validation team, it will
be sent back to the original labeling team to revise.

To improve the annotation quality as much as possible,
our team checks the annotation results very carefully and
revises them frequently. Around 40% of the initial annota-
tions fail in the ﬁrst round of validation. And many frames
are revised more than three times. Some challenging exam-
ples of frames that are initially labeled incorrectly or inac-
curately are given in Fig. 3. With all these efforts, we ﬁnally
reach a benchmark with high-quality dense annotation, with
some examples shown in Fig. 2.

3.4. Attributes

To enable further performance analysis of trackers, we
label each sequence with 14 attributes, including illumi-
nation variation (IV), full occlusion (FOC), partial occlu-
sion (POC), deformation (DEF), motion blur (MB), fast mo-
tion (FM), scale variation (SV), camera motion (CM), rota-
tion (ROT), background clutter (BC), low resolution (LR),
viewpoint change (VC), out-of-view (OV) and aspect ratio
change (ARC). The attributions are deﬁned in Tab. 2, and
Fig. 4 (a) shows the distribution of videos in each attribute.
From Fig. 4 (a), we observe that the most common chal-
lenge factors in LaSOT are scale changes (SV and ARC),
occlusion (POC and FOC), deformation (DEF) and rota-
tion (ROT), which are well-known challenges for tracking
in real-world applications. Besides, Fig. 4 (b) demonstrates
the distribution of attributes of LaSOT compared to OTB-
2015 [53] and TC-128 [35] on overlapping attributes. From
the ﬁgure we observe that more than 1,300 videos in La-
SOT are involved with scale variations. Compared with

OTB-2015 and TC-128 with less than 70 videos with scale
changes, LaSOT is more challenging for scale changes. In
addition, on the out-of-view attribute, LaSOT comprises
477 sequences, much larger than existing benchmarks.

3.5. Evaluation Protocols

Though there is no restriction to use LaSOT, we suggest
two evaluation protocols for evaluating tracking algorithms,
and conduct evaluations accordingly.
Protocol I. In protocol I, we use all 1,400 sequences to eval-
uate tracking performance. Researchers are allowed to em-
ploy any sequences except for those in LaSOT to develop
tracking algorithms. Protocol I aims to provide large-scale
evaluation of trackers.
Protocol II. In protocol II, we split LaSOT into training and
testing subsets. According to the 80/20 principle (i.e., the
Pareto principle), we select 16 out of 20 videos in each cate-
gory for training, and the rest is for testing1. In speciﬁc, the
training subset contains 1,120 videos with 2.83M frames,
and the testing subset consists of 280 sequences with 690K
frames. The evaluation of trackers is performed on the test-
ing subset. Protocol II aims to provide a large set of videos
for training and assessing trackers in the mean time.

4. Evaluation

4.1. Evaluation Metric

Following popular protocols (e.g. OTB-2015 [53]), we
perform an One-Pass Evaluation (OPE) and measure the
precision, normalized precision and success of different
tracking algorithms under two protocols.

1The training/testing split is shown in the supplementary material.

5378

(a) Distribution of sequences in each attribute on LaSOT(b) Distribution comparison in common attributes on different benchmarksThe precision is computed by comparing the distance
between tracking result and groundtruth bounding box in
pixels. Different trackers are ranked with this metric on
a threshold (e.g., 20 pixels). Since the precision metric is
sensitive to target size and image resolution, we normalize
the precision as in [41]. With the normalized precision met-
ric, we rank tracking algorithms using the Area Under the
Curve (AUC) between 0 to 0.5. Please refer to [41] about
the normalized precision metric. The success is computed
as the Intersection over Union (IoU) between tracking result
and groundtruth bounding box. The tracking algorithms are
ranked using the AUC between 0 to 1.

4.2. Evaluated Trackers

We evaluate 35 algorithms on LaSOT to provide exten-
sive baselines, comprising deep trackers (e.g., MDNet [42],
TRACA [5], CFNet [50], SiamFC [4], StructSiam [59],
DSiam [16], SINT [49] and VITAL [48]), correlation ﬁl-
ter trackers with hand-crafted features (e.g., ECO HC [7],
DSST [8], CN [11], CSK [19], KCF [20], fDSST [9],
SAMF [33], SCT4 [6], STC [57] and Staple [3]) or deep
features (e.g., HCFT [37] and ECO [7]) and regularization
techniques (e.g., BACF [15], SRDCF [10], CSRDCF [36],
Staple CA [40] and STRCF [29]), ensemble trackers (e.g.,
PTAV [13], LCT [38], MEEM [56] and TLD [24]), sparse
trackers (e.g., L1APG [2] and ASLA [23]), other represen-
tatives (e.g., CT [58], IVT [44], MIL [1] and Struck [17]).
Tab. 3 summarizes these trackers with their representation
schemes and search strategies in a chronological order.

4.3. Evaluation Results with Protocol I
Overall performance. Protocol I aims at providing large-
scale evaluations on all 1,400 videos in LaSOT. Each tracker
is used as it is for evaluation, without any modiﬁcation. We
report the evaluation results in OPE using precision, nor-
malized precision and success, as shown in Fig. 5. MD-
Net achieves the best precision score of 0.374 and success
score of 0.413, and VITAL obtains the best normalized pre-
cision score of 0.484. Both MDNet and VITAL are trained
in an online fashion, resulting in expensive computation and
slow running speeds. SimaFC tracker, which learns off-line
a matching function from a large set of videos using deep
network, achieves competitive results with 0.341 precision
score, 0.449 normalized precision score and 0.358 success
score, respectively. Without time-consuming online model
adaption, SiamFC runs efﬁciently in real-time. The best
correlation ﬁlter tracker is ECO with 0.298 precision score,
0.358 normalized precision score and 0.34 success score.

Compared to the typical tracking performances on ex-
isting dense benchmarks (e.g., OTB-2015 [53]), the perfor-
mances on LaSOT are severely degraded because of a large
mount of non-rigid target objects and challenging factors in-
volved in LaSOT. An interesting observation from Fig. 5 is
that all the top seven trackers leverage deep feature, demon-

Table 3. Summary of evaluated trackers. Representation: Sparse -
Sparse Representation, Color - Color Names or Histograms, Pixel
- Pixel Intensity, HoG - Histogram of Oriented Gradients, H or B -
Haar or Binary, Deep - Deep Feature. Search: PF - Particle Filter,
RS - Random Sampling, DS - Dense Sampling.

Representation

Search

e
s
r
a
p
S

A
C
P

r
o
l
o
C

l
e
x
i

P

G
o
H

IJCV08 ✓

TLD [24]

IVT [44]
MIL [1] CVPR09
ICCV11
Struck [17]
L1APG [2] CVPR12
ASLA [23] CVPR12
CSK [19] ECCV12
CT [58] ECCV12
PAMI12
CN [11] CVPR14
DSST [8] BMVC14
MEEM [56] ECCV14
STC [57] ECCV14
SAMF [33] ECCVW14
LCT [38] CVPR15
SRDCF [10]
ICCV15
ICCV15
HCFT [37]
KCF [20]
PAMI15
Staple [3] CVPR16
SINT [49] CVPR16
SCT4 [6] CVPR16
MDNet [42] CVPR16
SiamFC [4] ECCVW16
Staple CA[40] CVPR17
ECO HC [7] CVPR17
ECO [7] CVPR17
CFNet [50] CVPR17
CSRDCF [36] CVPR17
ICCV17
ICCV17
ICCV17
PAMI17
VITAL [48] CVPR18
TRACA [5] CVPR18
STRCF [29] CVPR18
StructSiam [59] ECCV18

PTAV [13]
DSiam [16]
BACF [15]
fDSST [9]

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

B

r
o
H

H
H

H
B

p
e
e
D

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

F
P

✓

✓

✓

S
R

S
D

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

strating its advantages in handling appearance changes.
Attribute-based performance. To analyze different chal-
lenges faced by existing trackers, we evaluate all tracking
algorithms on 14 attributes. We show the results on three
most challenging attributes, i.e., fast motion, out-of-view
and full occlusion, in Fig. 6 and refer the readers to supple-
mentary material for detailed attribute-based evaluation.
Qualitative evaluation. To qualitatively analyze different
trackers and provide guidance for future research, we show
the qualitative evaluation results of six representative track-
ers, including MDNet, SiamFC, ECO, PTAV, Staple and
MEEM, in six typical hard challenges containing fast mo-
tion, full occlusion, low resolution, out-of-view, aspect ra-
tio change and background clutter in Fig. 7. From Fig. 7,
we observe that, for videos with fast motion, full occlusion
and out-of-view (e.g., Yoyo-3, Goldﬁsh-4 and Basketball-
15), the trackers are prone to lose the target because existing
trackers usually perform localization from a small local re-
gion. To handle these challenges, a potential solution is to
leverage an instance-speciﬁc detector to locate the target for
subsequent tracking. Trackers easily drift in video with low

5379

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

t

e
a
r
 
s
s
e
c
c
u
S

Precision plots of OPE on LaSOT

[0.374] MDNet
[0.372] VITAL
[0.341] SiamFC
[0.340] StructSiam
[0.329] DSiam
[0.299] SINT
[0.298] ECO
[0.292] STRCF
[0.272] ECO_HC
[0.265] CFNet
[0.250] HCFT
[0.243] PTAV
[0.239] BACF
[0.237] TRACA
[0.231] Staple
[0.231] CSRDCF
[0.231] Staple_CA
[0.227] SRDCF
[0.224] MEEM
[0.214] SAMF
[0.196] DSST
[0.193] LCT
[0.192] fDSST
[0.186] SCT4
[0.185] Struck
[0.184] KCF
[0.184] TLD
[0.170] ASLA
[0.158] CN
[0.154] L1APG
[0.142] STC
[0.136] CSK
[0.131] IVT
[0.102] CT
[0.078] MIL

5

10

15

20

25

30

35

40

45

50

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Normalized Precision plots of OPE on LaSOT

[0.484] VITAL
[0.481] MDNet
[0.449] SiamFC
[0.443] StructSiam
[0.432] DSiam
[0.383] SINT
[0.358] ECO
[0.353] STRCF
[0.330] CFNet
[0.327] ECO_HC
[0.311] HCFT
[0.307] BACF
[0.298] Staple_CA
[0.298] TRACA
[0.298] MEEM
[0.297] Staple
[0.283] PTAV
[0.279] CSRDCF
[0.279] SRDCF
[0.271] SAMF
[0.242] LCT
[0.239] DSST
[0.236] fDSST
[0.231] SCT4
[0.228] KCF
[0.228] Struck
[0.224] TLD
[0.213] ASLA
[0.194] L1APG
[0.194] CN
[0.176] STC
[0.174] CSK
[0.173] IVT
[0.145] CT
[0.117] MIL

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
s
s
e
c
c
u
S

Success plots of OPE on LaSOT

[0.413] MDNet
[0.412] VITAL
[0.358] SiamFC
[0.356] StructSiam
[0.353] DSiam
[0.340] ECO
[0.339] SINT
[0.315] STRCF
[0.311] ECO_HC
[0.296] CFNet
[0.285] TRACA
[0.280] MEEM
[0.277] BACF
[0.272] HCFT
[0.271] SRDCF
[0.269] PTAV
[0.266] Staple
[0.263] CSRDCF
[0.262] Staple_CA
[0.258] SAMF
[0.246] LCT
[0.234] Struck
[0.233] DSST
[0.232] fDSST
[0.228] TLD
[0.214] SCT4
[0.211] ASLA
[0.211] KCF
[0.186] CN
[0.178] CT
[0.172] CSK
[0.168] L1APG
[0.163] MIL
[0.151] STC
[0.136] IVT

0.5

0.4
0.6
Overlap threshold

0.7

0.8

0.9

1

Location error threshold

Location error threshold

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0

0

0.1

0.2

0.3

Figure 5. Evaluation results on LaSOT under protocol I using precision, normalized precision and success. Best viewed in color.

Success plots of OPE - Fast Motion (296)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

[0.250] MDNet
[0.245] VITAL
[0.217] ECO
[0.214] SINT
[0.208] DSiam
[0.205] StructSiam
[0.200] SiamFC
[0.183] ECO_HC
[0.180] STRCF
[0.168] MEEM
[0.157] CFNet
[0.150] TLD
[0.150] PTAV
[0.149] SRDCF
[0.146] TRACA
[0.145] BACF
[0.140] CSRDCF
[0.138] HCFT
[0.130] Staple
[0.129] LCT
[0.127] Staple_CA
[0.126] SAMF
[0.121] fDSST
[0.117] Struck
[0.109] DSST
[0.109] SCT4
[0.097] KCF
[0.092] ASLA
[0.085] CN
[0.078] CT
[0.074] CSK
[0.073] MIL
[0.069] STC
[0.069] L1APG
[0.056] IVT

Success plots of OPE - Full Occlusion (542)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

[0.304] MDNet
[0.304] VITAL
[0.260] StructSiam
[0.260] DSiam
[0.255] SINT
[0.253] SiamFC
[0.247] ECO
[0.218] ECO_HC
[0.215] STRCF
[0.213] MEEM
[0.197] TRACA
[0.195] CFNet
[0.189] SRDCF
[0.182] PTAV
[0.182] HCFT
[0.180] TLD
[0.176] BACF
[0.173] CSRDCF
[0.172] SAMF
[0.166] LCT
[0.164] Staple
[0.164] Staple_CA
[0.162] Struck
[0.151] fDSST
[0.151] SCT4
[0.142] ASLA
[0.139] KCF
[0.138] DSST
[0.119] CN
[0.117] CT
[0.108] MIL
[0.108] CSK
[0.105] L1APG
[0.097] STC
[0.079] IVT

Success plots of OPE - Low Resolution (661)

[0.308] VITAL
[0.308] MDNet
[0.263] ECO
[0.259] DSiam
[0.257] StructSiam
[0.254] SiamFC
[0.245] SINT
[0.224] STRCF
[0.220] ECO_HC
[0.197] MEEM
[0.192] CFNet
[0.189] PTAV
[0.187] TRACA
[0.186] SRDCF
[0.184] BACF
[0.180] HCFT
[0.176] CSRDCF
[0.174] TLD
[0.168] Staple
[0.162] Staple_CA
[0.159] LCT
[0.158] SAMF
[0.155] Struck
[0.151] fDSST
[0.137] DSST
[0.133] SCT4
[0.129] ASLA
[0.125] KCF
[0.114] CN
[0.101] CT
[0.101] L1APG
[0.096] CSK
[0.093] STC
[0.093] MIL
[0.070] IVT

0

0

0.1

0.2

0.3

0.5

0.4
0.6
Overlap threshold

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.5

0.4
0.6
Overlap threshold

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.5

0.4
0.6
Overlap threshold

0.7

0.8

0.9

1

Figure 6. Performances of trackers on three most challenging attributes under protocol I using success. Best viewed in color.

MEEM

Staple

PTAV

ECO

SiamFC

MDNet

GT

Figure 7. Qualitative evaluation in six typical hard challenges: Yoyo-3 with fast motion, Goldﬁsh-4 with full occlusion, Pool-4 with low-
resolution, Basketball-15 with out-of-view, Train-1 with aspect ration change and Person-2 with background clutter. Best viewed in color.

resolution (e.g., Pool-4) due to the ineffective representation
for small target. A solution for deep feature based trackers
is to combine features from multiple scales to incorporate
details into representation. Video with aspect ratio change
is difﬁcult as most existing trackers either ignore this issue
or adopt a simple method (e.g., random search or pyramid
strategy) to deal with it. Inspired from the success of deep
learning based object detection, a generic regressor can be
leveraged to reduce the effect of aspect ratio change (and
scale change) on tracking. For sequence with background

clutter, trackers drift due to less discriminative representa-
tion for target and background. A possible solution to alle-
viate this problem is to utilize the contextual information to
enhance the discriminability.

4.4. Evaluation Results with Protocol II

Under protocol II, we split LaSOT into training and test-
ing sets. Researchers are allowed to leverage the sequences
in the training set to develop their trackers and assess their
performances on the test set. In order to provide baselines

5380

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Precision plots of OPE on LaSOT Testing Set

[0.373] MDNet
[0.360] VITAL
[0.339] SiamFC
[0.333] StructSiam
[0.322] DSiam
[0.301] ECO
[0.298] STRCF
[0.295] SINT
[0.279] ECO_HC
[0.259] CFNet
[0.254] PTAV
[0.241] HCFT
[0.239] Staple
[0.239] BACF
[0.235] Staple_CA
[0.227] TRACA
[0.227] MEEM
[0.220] CSRDCF
[0.219] SRDCF
[0.203] SAMF
[0.190] LCT
[0.189] DSST
[0.184] fDSST
[0.181] Struck
[0.179] SCT4
[0.174] TLD
[0.168] ASLA
[0.166] KCF
[0.163] CN
[0.155] L1APG
[0.137] STC
[0.131] CSK
[0.122] IVT
[0.101] CT
[0.077] MIL

5

10

15

20

25

30

35

40

45

50

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Normalized Precision plots of OPE on LaSOT Testing Set

[0.460] MDNet
[0.453] VITAL
[0.420] SiamFC
[0.418] StructSiam
[0.405] DSiam
[0.354] SINT
[0.340] STRCF
[0.338] ECO
[0.320] ECO_HC
[0.312] CFNet
[0.286] HCFT
[0.283] BACF
[0.278] TRACA
[0.278] Staple
[0.274] PTAV
[0.270] Staple_CA
[0.265] MEEM
[0.254] CSRDCF
[0.248] SRDCF
[0.239] SAMF
[0.213] DSST
[0.209] LCT
[0.208] fDSST
[0.207] Struck
[0.200] SCT4
[0.193] TLD
[0.190] KCF
[0.189] ASLA
[0.177] CN
[0.174] L1APG
[0.156] STC
[0.152] IVT
[0.149] CSK
[0.122] CT
[0.097] MIL

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
s
s
e
c
c
u
S

Success plots of OPE on LaSOT Testing Set

[0.397] MDNet
[0.390] VITAL
[0.336] SiamFC
[0.335] StructSiam
[0.333] DSiam
[0.324] ECO
[0.314] SINT
[0.308] STRCF
[0.304] ECO_HC
[0.275] CFNet
[0.259] BACF
[0.257] TRACA
[0.257] MEEM
[0.250] HCFT
[0.250] PTAV
[0.245] SRDCF
[0.244] CSRDCF
[0.243] Staple
[0.238] Staple_CA
[0.233] SAMF
[0.221] LCT
[0.212] Struck
[0.210] TLD
[0.207] DSST
[0.203] fDSST
[0.194] ASLA
[0.191] SCT4
[0.178] KCF
[0.170] CN
[0.158] CT
[0.155] L1APG
[0.149] CSK
[0.139] MIL
[0.138] STC
[0.118] IVT

0.5

0.4
0.6
Overlap threshold

0.7

0.8

0.9

1

Location error threshold

Location error threshold

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0

0

0.1

0.2

0.3

Figure 8. Evaluation results on LaSOT under protocol II using precision, normalized precision and success. Best viewed in color.

and comparisons on the testing set, we evaluate the 35 track-
ing algorithms. Each tracker is used as it is for evalua-
tion without any modiﬁcation or re-training. The evalua-
tion results are shown in Fig. 8 using precision, normal-
ized precision and success. We observe consistent results as
in protocol I. MDNet and VITAL show top performances
with precision scores of 0.373 and 0.36, normalized preci-
sion scores of 0.46 and 0.453 and success scores of 0.397
and 0.39. Next, SiamFC achieves the third-ranked perfor-
mance with a 0.339 precision score, a 0.42 normalized pre-
cision score and a 0.336 success score, respectively. Despite
slightly lower scores in accuracy than MDNet and VITAL,
SiamFC runs much faster and achieves real-time running
speed, showing good balance between accuracy and efﬁ-
ciency. For attribute-based evaluation of trackers on LaSOT
testing set, we refer the readers to supplementary material
because of limited space.

In addition to evaluating each tracking algorithm as it is,
we conduct experiments by re-training two representative
deep trackers, MDNet [42] and SiamFC [4], on the training
set of LaSOT and assessing them. The evaluation results
show similar performances for these trackers as without re-
training. A potential reason is that our re-training may not
follow the same conﬁgurations used by the original authors.
Besides, since LaSOT are in general more challenging than
previous datasets (e.g., all sequences are long-term), dedi-
cated conﬁguration may be needed for training these track-
ers. We leave this part as a future work since it is beyond
the scope of this benchmark.

4.5. Retraining Experiment on LaSOT

We conduct the experiment by retraining SiamFC [4] on
the training set of LaSOT to demonstrate how deep learning
based tracker is improved using more data. Tab. 4 reports
the results on OTB-2013 [52] and OTB-2015 [53] and com-
parisons with the performance of original SiamFC trained
on ImageNet Video [45]. Note that, we utilize color images
for training, and apply a pyramid with 3 scales for track-
ing, i.e., SiamFC-3s (color). All parameters for training and

Table 4. Retraining of SiamFC [4] on LaSOT.

Training data

SiamFC-3s (color)

ImageNet
Video [45]

LaSOT

training set

OTB-2013 [52]

OTB-2015 [53]

Precision
Success
Precision
Success

0.803
0.588
0.756
0.565

0.816 (↑1.3%)
0.608 (↑2.0%)
0.777 (↑2.1%)
0.582 (↑1.7%)

tracking are kept the same in these two experiments. From
Tab. 4, we observe consistent performance gains on the two
benchmarks, showing the importance of speciﬁc large-scale
training set for deep trackers.

5. Conclusion

We present LaSOT with high-quality dense bounding
box annotations for visual object tracking. To the best of
our knowledge, LaSOT is the largest tracking benchmark
with high quality annotations to date. By releasing LaSOT,
we expect to provide the tracking community a dedicated
platform for training deep trackers and assessing long-term
tracking performance. Besides, LaSOT provides lingual an-
notations for each sequence, aiming to encourage the explo-
ration on integrating visual and lingual features for robust
tracking. By releasing LaSOT, we hope to narrow the gap
between the increasing number of deep trackers and the lack
of large dedicated datasets for training, and meanwhile pro-
vide more veritable evaluations for different trackers in the
wild. Extensive evaluations on LaSOT under two protocols
imply a large room to improvement for visual tracking.

Acknowledgement. We sincerely thank B. Huang, X. Li, Q.
Zhou, L. Chen, J. Liang, J. Wang and anonymous volunteers
for their help in constructing LaSOT. This work is supported in
part by the China National Key Research and Development Plan
(Grant No. 2016YFB1001200), and in part by US NSF Grants
1618398, 1407156 and 1350521, and Yong Xu thanks the sup-
ports by National Nature Science Foundation of China (U1611461
and 61672241), the Cultivation Project of Major Basic Research of
NSF-Guangdong Province (2016A030308013).

5381

References

[1] Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Vi-
In

sual tracking with online multiple instance learning.
CVPR, 2009. 6

[2] Chenglong Bao, Yi Wu, Haibin Ling, and Hui Ji. Real
time robust l1 tracker using accelerated proximal gradient
approach. In CVPR, 2012. 6

[3] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip HS Torr. Staple: Complementary learners
for real-time tracking. In CVPR, 2016. 6

[4] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
networks for object tracking. In ECCVW, 2016. 6, 8

[5] Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo
Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, and
Jin Young Choi. Context-aware deep feature compression
for high-speed visual tracking. In CVPR, 2018. 6

[6] Jongwon Choi, Hyung Jin Chang, Jiyeoup Jeong, Yian-
nis Demiris, and Jin Young Choi. Visual tracking using
attention-modulated disintegration and integration. In CVPR,
2016. 6

[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. Eco: Efﬁcient convolution operators for
tracking. In CVPR, 2017. 6

[8] Martin Danelljan, Gustav H¨ager, Fahad Khan, and Michael
Felsberg. Accurate scale estimation for robust visual track-
ing. In BMVC, 2014. 6

[9] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and
Michael Felsberg. Discriminative scale space tracking.
TPAMI, 39(8):1561–1575, 2017. 6

[10] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Learning spatially regularized correlation
ﬁlters for visual tracking. In ICCV, 2015. 6

[11] Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg,
and Joost Van de Weijer. Adaptive color attributes for real-
time visual tracking. In CVPR, 2014. 6

[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 4

[13] Heng Fan and Haibin Ling. Parallel tracking and verifying:
A framework for real-time and high accuracy visual tracking.
In ICCV, 2017. 6

[14] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva
Ramanan, and Simon Lucey. Need for speed: A benchmark
for higher frame rate object tracking. In ICCV, 2017. 2, 3

[15] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation ﬁlters for visual
tracking. In ICCV, 2017. 6

[16] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and
Song Wang. Learning dynamic siamese network for visual
object tracking. In ICCV, 2017. 6

[17] Sam Hare, Amir Saffari, and Philip H. S. Torr. Struck: Struc-

tured output tracking with kernels. In ICCV, 2011. 6

[19] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. Exploiting the circulant structure of tracking-by-
detection with kernels. In ECCV, 2012. 6

[20] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters. TPAMI, 37(3):583–596, 2015. 6

[21] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng,
Kate Saenko, and Trevor Darrell. Natural language object
retrieval. In CVPR, 2016. 2

[22] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A
large high-diversity benchmark for generic object tracking in
the wild. arXiv:1810.11981, 2018. 2, 3

[23] Xu Jia, Huchuan Lu, and Ming-Hsuan Yang. Visual track-
ing via adaptive structural local sparse appearance model. In
CVPR, 2012. 6

[24] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas.
TPAMI, 34(7):1409–1422,

Tracking-learning-detection.
2012. 6

[25] Matej Kristan, Jiri Matas, Aleˇs Leonardis, Tom´aˇs Voj´ıˇr,
Roman Pﬂugfelder, Gustavo Fernandez, Georg Nebehay,
Fatih Porikli, and Luka ˇCehovin. A novel performance
evaluation methodology for single-target trackers. TPAMI,
38(11):2137–2155, 2016. 2, 3

[26] Matej Kristan et al. The visual object tracking vot2014 chal-

lenge results. In ECCVW, 2014. 1, 2

[27] Matej Kristan et al. The visual object tracking vot2017 chal-

lenge results. In ICCVW, 2017. 1, 2

[28] Annan Li, Min Lin, Yi Wu, Ming-Hsuan Yang, and
Shuicheng Yan. Nus-pro: A new visual tracking challenge.
TPAMI, 38(2):335–349, 2016. 1, 2, 3

[29] Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, and Ming-
Hsuan Yang. Learning spatial-temporal regularized correla-
tion ﬁlters for visual tracking. In CVPR, 2018. 6

[30] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep
visual tracking: Review and experimental comparison. PR,
76:323–338, 2018. 2

[31] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, 2017. 2

[32] Xi Li, Weiming Hu, Chunhua Shen, Zhongfei Zhang, An-
thony Dick, and Anton Van Den Hengel. A survey of appear-
ance models in visual object tracking. ACM TIST, 4(4):58,
2013. 1, 2

[33] Yang Li and Jianke Zhu. A scale adaptive kernel correlation

ﬁlter tracker with feature integration. In ECCVW, 2014. 6

[34] Zhenyang Li, Ran Tao, Efstratios Gavves, Cees GM Snoek,
Arnold WM Smeulders, et al. Tracking by natural language
speciﬁcation. In CVPR, 2017. 2

[35] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding
color information for visual tracking: Algorithms and bench-
mark. TIP, 24(12):5630–5644, 2015. 1, 2, 3, 5

[36] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas,
and Matej Kristan. Discriminative correlation ﬁlter with
channel and spatial reliability. In CVPR, 2017. 6

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 1

[37] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual tracking.
In ICCV, 2015. 6

5382

[38] Chao Ma, Xiaokang Yang, Chongyang Zhang, and Ming-
In CVPR,

Hsuan Yang. Long-term correlation tracking.
2015. 6

[55] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
NIPS, 2014. 1

[56] Jianming Zhang, Shugao Ma, and Stan Sclaroff. Meem: ro-
bust tracking via multiple experts using entropy minimiza-
tion. In ECCV, 2014. 6

[57] Kaihua Zhang, Lei Zhang, Qingshan Liu, David Zhang, and
Ming-Hsuan Yang. Fast visual tracking via dense spatio-
temporal context learning. In ECCV, 2014. 6

[58] Kaihua Zhang, Lei Zhang, and Ming-Hsuan Yang. Real-time

compressive tracking. In ECCV, 2012. 6

[59] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,
Mengyang Feng, and Huchuan Lu. Structured siamese net-
work for real-time visual tracking. In ECCV, 2018. 6

[39] Matthias Mueller, Neil Smith, and Bernard Ghanem. A
benchmark and simulator for uav tracking. In ECCV, 2016.
1, 2, 3

[40] Matthias Mueller, Neil Smith, and Bernard Ghanem.
In CVPR, 2017.

Context-aware correlation ﬁlter tracking.
6

[41] Matthias M¨uller, Adel Bibi, Silvio Giancola, Salman Al-
Subaihi, and Bernard Ghanem. Trackingnet: A large-scale
dataset and benchmark for object tracking in the wild.
In
ECCV, 2018. 2, 3, 6

[42] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
2016. 6, 8

[43] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,
and Vincent Vanhoucke. Youtube-boundingboxes: A large
high-precision human-annotated data set for object detection
in video. In CVPR, 2017. 1, 3

[44] David A Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-
Hsuan Yang. Incremental learning for robust visual tracking.
IJCV, 77(1-3):125–141, 2008. 6

[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Imagenet large
Aditya Khosla, Michael Bernstein, et al.
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 1, 8

[46] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015. 1

[47] Arnold WM Smeulders, Dung M Chu, Rita Cucchiara, Si-
mone Calderara, Afshin Dehghan, and Mubarak Shah. Vi-
sual tracking: An experimental survey. TPAMI, 36(7):1442–
1468, 2014. 1, 2, 3

[48] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson Lau, and
Ming-Hsuan Yang. Vital: Visual tracking via adversarial
learning. In CVPR, 2018. 6

[49] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.

Siamese instance search for tracking. In CVPR, 2016. 6

[50] Jack Valmadre, Luca Bertinetto, Jo˜ao Henriques, Andrea
Vedaldi, and Philip HS Torr. End-to-end representation
learning for correlation ﬁlter based tracking. In CVPR, 2017.
6

[51] Jack Valmadre, Luca Bertinetto, Jo˜ao F Henriques, Ran Tao,
Andrea Vedaldi, Arnold Smeulders, Philip Torr, and Efstra-
tios Gavves. Long-term tracking in the wild: A benchmark.
In ECCV, 2018. 2, 3

[52] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object

tracking: A benchmark. In CVPR, 2013. 1, 2, 3, 8

[53] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-
ing benchmark. TPAMI, 37(9):1834–1848, 2015. 1, 2, 3, 5,
6, 8

[54] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object track-

ing: A survey. ACM CSUR, 38(4):13, 2006. 1, 2

5383

