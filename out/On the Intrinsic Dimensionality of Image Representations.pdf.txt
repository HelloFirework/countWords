On the Intrinsic Dimensionality of Image Representations

Sixue Gong

Vishnu Naresh Boddeti

Anil K. Jain

Michigan State University, East Lansing MI 48824

{gongsixu, vishnu, jain}@msu.edu

Abstract

This paper addresses the following questions pertaining
to the intrinsic dimensionality of any given image represen-
tation: (i) estimate its intrinsic dimensionality, (ii) develop
a deep neural network based non-linear mapping, dubbed
DeepMDS, that transforms the ambient representation to
the minimal intrinsic space, and (iii) validate the verac-
ity of the mapping through image matching in the intrinsic
space. Experiments on benchmark image datasets (LFW,
IJB-C and ImageNet-100) reveal that the intrinsic dimen-
sionality of deep neural network representations is signif-
icantly lower than the dimensionality of the ambient fea-
tures. For instance, SphereFace’s [26] 512-dim face repre-
sentation and ResNet’s [16] 512-dim image representation
have an intrinsic dimensionality of 16 and 19 respectively.
Further, the DeepMDS mapping is able to obtain a repre-
sentation of signiﬁcantly lower dimensionality while main-
taining discriminative ability to a large extent, 59.75% TAR
@ 0.1% FAR in 16-dim vs 71.26% TAR in 512-dim on IJB-
C [29] and a Top-1 accuracy of 77.0% at 19-dim vs 83.4%
at 512-dim on ImageNet-100.

1. Introduction

An image representation is an embedding function that
transforms the raw pixel representation of the image to a
point in a high-dimensional vector space. Learning or es-
timating such a mapping is motivated by two goals: (a)
the compactness of the representation, and (2) the effective-
ness of the mapping for the task at hand. While the latter
topic has received substantial attention, ranging from PCA
based Eigenfaces [42] to deep neural network (DNN) based
feature representations, there has been relatively little fo-
cus on the dimensionality of the representation itself. The
dimensionality of image representations has ranged from
hundreds to thousands of dimensions. For instance, cur-
rent state-of-the-art image representations have 128, 512,
1024 and 4096 dimensions for FaceNet [35], ResNet [16],
SphereFace [26] and VGG [36], respectively. The choice
of dimensionality is often determined by practical consid-

erations, such as, ease of learning the embedding function
[38], constraints on system memory, etc. instead of the ef-
fective dimensionality necessary for image representation.
This naturally raises the following fundamental but related
questions, How compact can the representation be without
any loss in recognition performance? In other words, what
is the intrinsic dimensionality of the representation? And,
how can one obtain such a compact representation? Ad-
dressing these questions is the primary goal of this paper.

The intrinsic dimensionality (ID) of a representation
refers to the minimum number of parameters (or degrees of
freedom) necessary to capture the entire information present
in the representation [4]. Equivalently, it refers to the di-
mensionality of the m-dimensional manifold M embedded
within the d-dimensional ambient (representation) space P
where m ≤ d. This notion of intrinsic dimensionality is
notably different from common linear dimensionality esti-
mates obtained through e.g., principal component analysis
(PCA). This linear dimension corresponds to the best linear
subspace necessary to retain a desired fraction of the vari-
ations in the data. In principle, linear dimensionality can
be as large as the ambient dimension if the variation fac-
tors are highly entangled with each other. An illustration of
these concepts is provided in Fig. 1.

The ability to estimate the intrinsic dimensionality of a
given image representation is useful in a number of ways.
At a fundamental level, the ID determines the true capac-
ity and complexity of variations in the data captured by the
representation, through the embedding function.
In fact,
the ID can be used to gauge the information content in the
representation, due to its linear relation with Shannon en-
tropy [41, 9]. Also, it provides an estimate of the amount
of redundancy built into the representation which relates to
its generalization capability. On a practical level, knowl-
edge of the ID is crucial for devising optimal unsupervised
strategies to obtain image features that are minimally re-
dundant, while retaining its full ability to categorize images
into different classes. Recognition in the intrinsic space can
provide signiﬁcant savings, both in memory requirements
as well as processing time, across downstream tasks like
large-scale face matching in the encrypted domain [5], im-

3987

Images (I)

Representation Model

Ambient Space (P ∈ Rd)

z

Intrinsic Space (M ∈ Rm)

DeepMDS

xx

yy

(a)

y = f (x, θ)

d23
G

x3

d12
G

x2

d13
E

x1

P ∈ Rd

(b)

y3

d23
E

y2

d12
E

y1

M ∈ Rm

(c)

Figure 1: Overview: This paper studies the manifold of feature vectors of images I obtained from a given representation model. (a) We estimate the
intrinsic dimensionality (ID) of the ambient space P and propose DeepMDS, an unsupervised method, to map P to a low-dimensional intrinsic space M. (b)
Illustration of the ambient space P and intrinsic manifold M of a face representation. Here, while the ambient and linear dimension of the representation is
three, its ID is only two. (b) Heatmaps of similarity scores between face pairs of 10 classes with 10 images per class for a representation with ID of 10-dim.
The similarity is computed in four different spaces, the 512-dim ambient space P, 10-dim space of linear dimensionality (PCA), 10-dim intrinsic space
M estimated by Isomap [40] and by our DeepMDS model. The class separability, as shown by the diagonal blocks, is better maintained by DeepMDS.

age matching and retrieval, etc. Lastly, gap between the am-
bient and intrinsic dimensionalities of a representation can
serve as a useful indicator to drive the development of algo-
rithms that can directly learn highly compact embeddings.

Estimating the ID of given data representation however
is a challenging task. Such estimates are crucially depen-
dent on the density variations in the representation, which
in itself is difﬁcult to estimate as images often lie on a topo-
logically complex curved manifold [39]. More importantly,
given an estimate of ID, how do we verify that it truly rep-
resents the dimensionality of the complex high-dimensional
representation space? An indirect validation of the ID is
possible through a mapping that transforms the ambient rep-
resentation space to the intrinsic representation space while
preserving its discriminative ability. However, there is no
certainty that such a mapping can be found efﬁciently. In
practice, ﬁnding such mappings can be considerably harder
than estimating the ID itself.

We overcome both of these challenges by (1) adopting
a topological dimensionality estimation technique based on
the geodesic distance between points on the manifold, and
(2) relying on the ability of DNNs to approximate the com-
plex mapping function from the ambient space to the intrin-
sic space. The latter enables validation of the ID estimates
through image matching experiments on the corresponding
low-dimensional intrinsic representation of feature vectors.

The key contributions and ﬁndings of this paper are:

– The ﬁrst attempt to estimate the intrinsic dimensionality
of DNN based image representations.
– An unsupervised DNN based dimensionality reduction
method under the framework of multidimensional scaling,
called DeepMDS.
– Numerical experiments yield an ID estimate of, 12 and
16 for FaceNet [35] and SphereFace [26] face representa-
tions, respectively, and 19 for ResNet-34 [16] image repre-
sentation. The estimates are signiﬁcantly lower than their
respective ambient dimensionalities, 128-dim for FaceNet
and 512-dim for the others.
– DeepMDS mapping is signiﬁcantly better than other di-
mensionality reduction approaches in terms of its discrimi-
native capability.

2. Related Work

Image Representation: The quest to develop image rep-
resentations that are simultaneously robust and discrimina-
tive have led to extensive research on this topic. Amongst
the earliest learning based approaches, Turk and Pentland
proposed Eigenfaces [42] that relied on principal compo-
nent analysis (PCA) of data. Later on, integrated and high-
dimensional spatially local features became prevalent for
image recognition, notable examples include local binary
patterns (LBP) [1], scale-invariant feature transform (SIFT)
[28] and histogram of oriented gradients (HoG) [10].
In
contrast to these hand-designed representations, the past

3988

Ambient SpacePCAIsomapDeepMDS0.40.00.40.8decade has witnessed the development of end-to-end rep-
resentation learning systems. Convolutional neural network
based features now typify the state-of-the-art image repre-
sentations [16, 37, 26]. All of these representations are how-
ever characterized by features that range from hundreds to
thousands of dimensions. While more compact representa-
tions are desirable, difﬁculties with optimizing DNNs with
narrow bottlenecks [38] have proven to be the primary bar-
rier towards realizing this goal.

Intrinsic Dimensionality: Existing approaches for esti-
mating intrinsic dimensionality can be broadly classiﬁed
into two groups: projection methods and geometric meth-
ods. The projection methods [11, 6, 43] determine the di-
mensionality by principal component analysis on local sub-
regions of the data and estimating the number of dominant
eigenvalues. These approaches have classically been used
in the context of modeling facial appearance under differ-
ent illumination conditions [12] and object recognition with
varying pose [30]. While they serve as an efﬁcient heuris-
tic, they do not provide reliable estimates of intrinsic di-
mension. Geometric methods [31, 14, 7, 21, 17, 24] on the
other hand model the intrinsic topological geometry of the
data and are based on the assumption that the volume of a
m-dimensional set scales with its size ǫ as ǫm and hence the
number of neighbors less than ǫ also behaves the same way.
Our approach in this paper is based on the topological no-
tion of correlation dimension [14, 7], the most popular type
of fractal dimensions. The correlation dimension implicitly
uses nearest-neighbor distance, typically based on the Eu-
clidean distance. However, Granata et.al. [13] observe that
leveraging the manifold structure of the data, in the form
of geodesic distances induced by a neighborhood graph of
the data, provides more realistic estimates of the ID. Build-
ing upon this observation we base our ID estimates on the
geodesic distance between points. We believe that estimat-
ing the intrinsic dimensionality would serve as the ﬁrst step
towards understanding the bound on the minimal required
dimensionality for representing images and aid in the de-
velopment of novel algorithms that can achieve this limit.

Dimensionality Reduction: There is a tremendous body
of work on the topic of estimating low-dimensional approx-
imations of data manifolds lying in high-dimensional space.
These include linear approaches such as Principal Compo-
nent Analysis [20], Multidimensional Scaling (MDS) [23]
and Laplacian Eigenmaps [2] and their corresponding non-
linear spectral extensions, Locally Linear Embedding [32],
Isomap [40] and Diffusion Maps [8]. Another class of di-
mensionality reduction algorithms leverage the ability of
deep neural networks to learn complex non-linear mappings
of data including deep autoencoders [18], denoising autoen-
coders [44, 45] and learning invariant mappings either with
the contrastive loss [15] or with the triplet loss [35]. While

the autoencoders can learn a compact representation of data,
such a representation is not explicitly designed to retain dis-
criminative ability. Both the contrastive loss and the triplet
loss have a number of limitations; (1) require similarity and
dissimilarity labels from some source and cannot be trained
in a purely unsupervised setting, (2) require an additional
hyper-parameter, maximum margin of separation, which is
difﬁcult to pre-determine, especially for an arbitrary repre-
sentation, and (3) do not maintain the manifold structure
in the low-dimensional space. In this paper, we too lever-
age DNNs to approximate the non-linear mapping from the
ambient to the intrinsic space. However, we consider an un-
supervised setting (i.e., no similarity or dissimilarity labels)
and cast the learning problem within the framework of MDS
i.e., preserving the ambient graph induced geodesic distance
between points in the intrinsic space.

3. Approach

Our goal in this paper is to compress a given image repre-
sentation space. We achieve this in two stages1: (1) estimate
the intrinsic dimensionality of the ambient image represen-
tation, and (2) learn the DeepMDS model to map the ambi-
ent representation space P ∈ Rd to the intrinsic representa-
tion space M ∈ Rm (m ≤ d). The ID estimates are based
on the one presented by [13] which relies on two key ideas,
(1) using graph induced geodesic distances to estimate the
correlation dimension of the image representation topology,
and (2) the similarity of the distribution of geodesic dis-
tances across different topological structures with the same
intrinsic dimensionality. The DeepMDS model is optimized
to preserve the interpoint geodesic distances between the
feature vectors in the ambient and intrinsic space, and is
trained in a stage-wise manner that progressively reduces
the dimensionality of the representation. Basing the pro-
jection method on DNNs, instead of spectral approaches
like Isomap, addresses the scalability and out-of-sample-
extension problems suffered by spectral methods. Specif-
ically, DeepMDS is trained in a stochastic fashion, which
allows it to scale. Furthermore, once trained, DeepMDS
provides a mapping function in the form of a feed-forward
network that maps the ambient feature vector to its corre-
sponding intrinsic feature vector. Such as map can easily be
applied to new test data.

3.1. Estimating Intrinsic Dimension

We deﬁne the notion of intrinsic dimension through the
classical concept of topological dimension of the support
of a distribution. This is a generalization of the concept

1Traditional single-stage dimensionality reduction methods use visual
aids to arrive at the ﬁnal ID and intrinsic space, e.g., plotting the projection
error against the ID values and looking for a “knee” in the curve.

3989

d23
G

x3

d12
G

x2

d13
E

x1

P ∈ Rd

p(r)

Geodesic Distance

2σ

rmax

r

(a) Graph Induced Geodesic Distance

(b) Topological Similarity

Figure 2: Intrinsic Dimension: Our approach is based on two observations: (a) Graph induced geodesic distance between images is able to capture the
topology of the image representation manifold more reliably. As an illustration, we show the graph edges for the surface of a unitary hypersphere and a face
manifold of ID two, embedded within a 3-dim space. (b) The distribution of the geodesic distances (for distance rmax − 2σ ≤ r ≤ rmax, where rmax is
the distance at the mode) has been empirically observed [13] to be similar across different topological structures with the same intrinsic dimensionality. The
plot shows the distance distribution for a face representation, unitary hypersphere and a Gaussian distribution of ID two embedded within 3-dim space.

of dimension of a linear space 2 to a non-linear manifold.
Methods for estimating the topological dimension are all
based on the assumption that the behavior of the number
of neighbors of a given point on an m-dimensional mani-
fold embedded within a d-dimensional space scales with its
size ǫ as ǫm. In other words, the density of points within
an ǫ-ball (ǫ → 0) in the ambient space is independent of
the ambient dimension d and varies only according to its
intrinsic dimensionality m. Given a collection of points
X = {x1, . . . , xn}, where xi ∈ Rd, the cumulative distri-
bution of the pairwise distances C(r) between the n points
can be estimated as,

C(r) =

2

n(n − 1)

n

Xi<j=1

H(r − kxi − xj k) = Z r

0

p(r)dr

(1)

where H(·) is the Heaviside function and p(r) is the prob-
ability distribution of the pairwise distances. In this paper,
we choose the correlation dimension [14], a particular type
of topological dimension, to represent the intrinsic dimen-
sion of the image representation. It is is deﬁned as,

m = lim
r→0

ln C(r)

ln r

=⇒ lim
r→0

C(r) ∝ rm

(2)

Therefore, the intrinsic dimension is crucially dependent on
the accuracy with which the probability distribution can be
estimated at very small length-scales (distances), i.e., r →
0. Signiﬁcant efforts have been devoted to estimating the
intrinsic dimension through line ﬁtting in the ln C(r) vs ln r
space around the region where r → 0 i.e.,

m =

lim

(r2−r1)→0

ln C(r2) − ln C(r1)

ln r2 − ln r1

(3)

= lim
r→0

d ln C(r)

d ln r

= lim
r→0

p(r)
C(r)

r = lim
r→0

m(r)

The main drawback with this approach is the need for re-

2Linear dimension is the minimum number of independent vectors nec-

essary to represent any given point in this space as a linear combination.

liable estimates of p(r) at very small length scales, which
is precisely where the estimates are most unreliable when
data is limited, especially in very high-dimensional spaces.
Granata et al. [13] present an elegant solution to this prob-
lem through three observations, (i) estimates of m(r) can
be stable even as r → 0 if the distance between points is
computed as the graph induced shortest path between points
instead of the euclidean distance, as is commonly the case,
(ii) the probability distribution p(r) at intermediate length-
scales around the mode of p(r) i.e., (rmax − 2σ) ≤ r ≤
rmax can be conveniently used to obtain reliable estimates
of ID, and (iii) the distributions p(r) of different topological
geometries are similar to each other as long as the intrinsic
dimensionality is the same, or in other words the distribu-
tion p(r) depends only on the intrinsic dimensionality and
not on the geometric support of the manifolds.

Figure 2 provides an illustration of these observations.
Consider two different manifolds, faces and the surface of
a (m + 1)-dimensional unitary hypersphere (henceforth re-
ferred to as m-hypersphere S m), with intrinsic dimension-
ality of m = 2 but embedded within 3-dim Euclidean
space. Beyond the nearest neighbor, the distance r be-
tween any pair of points in the manifold is computed as the
shortest path between the points as induced by the graph
connecting all the points in the representation. Figure 2b
shows the distribution of log
in the
range rmax −2σ ≤ r ≤ rmax, where σ is the standard devi-
p(r) corresponds to the
ation of p(r) and rmax = arg max

p(rmax) vs log

rmax

p(r)

r

r

radius of the mode of p(r). Interestingly, different topolog-
ical geometries, namely, a face representation of ID two, a
2-hypersphere and a 2-dim Gaussian, all embedded within
3-dim Euclidean space have almost identical distributions.
More generally, the distribution of log
rmax
in the range rmax −2σ ≤ r ≤ rmax is empirically observed
to depend only on the intrinsic dimensionality, rather than
the geometrical support of the manifold.

p(rmax) vs log

p(r)

r

The intrinsic dimensionality of the representation mani-

3990

0.650.700.750.800.850.900.951.00log(r/rmax)2.001.751.501.251.000.750.500.250.00log(p(r)/p(rmax))Representation (K=3)Gaussian (m=2)Hypersphere (m=2)Ambient Space

z

Parametric Non-Linear Mapping

Intrinsic Space

m
r
o
N
h
c
t
a
B

U
L
e
R
P

r
a
e
n
L

i

+

. . .

m
r
o
N
h
c
t
a
B

U
L
e
R
P

r
a
e
n
L

i

+

xx

yy

Figure 3: DeepMDS Mapping: A DNN based non-linear mapping is learned to transform the ambient space to a plausible intrinsic space. The network
is optimized to preserve distances between pairs of points in the ambient and intrinsic space.

fold can thus be estimated by comparing the empirical dis-
tribution of the pairwise distances ˆpM(r) on the manifold to
that of a known distribution, such as the m-hypersphere in
the range rmax − σ ≤ r ≤ rmax (see supplementary mate-
rial for Gaussian example). The distribution of the geodesic
distance pSm(r) of m-hypersphere can be analytically ex-
pressed as, pSm (r) = c sinm−1(r), where c is a constant
and m is the ID. Given ˆpM(r), we minimize the Root Mean
Squared Error (RMSE) between the distributions as,

c,m Z rmax

min

rmax−2σ

klog ˆpM(r) − log(c) − (m − 1) log (sin[r])k2

which upon simpliﬁcation yields,

min

m Z rmax

rmax−2σ (cid:13)(cid:13)(cid:13)(cid:13)

log

ˆpM(r)

ˆpM(rmax)

− (m − 1) log(cid:18)sin(cid:20) πr

2

2rmax(cid:21)(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)

The above optimization problem can be solved via a least-
squares ﬁt after estimating the standard deviation, σ, of p(r)
(see supplementary for details). Such a procedure could, in
principle, result in a fractional estimate of dimension. If one
only requires integer solutions, the optimal value of m can
be estimated by rounding-off the least squares ﬁt solution.

3.2. Estimating Intrinsic Space

The intrinsic dimensionality estimates obtained in the
previous subsection alludes to the existence of a mapping,
that can transform the ambient representation to the intrinsic
space, but does not provide any solutions to ﬁnd said map-
ping. The mapping itself could potentially be very complex
and our goal of estimating it is practically challenging.

We base our solution to estimate a mapping from the
ambient to the intrinsic space on Multidimensional scaling
(MDS) [23], a classical mapping technique that attempts
to preserve the distances (similarities) between points af-
ter embedding them in a low-dimensional space. Given
data points X = {x1, . . . , xn} in the ambient space and
Y = {y1, . . . , yn} the corresponding points in the intrinsic
low-dimensional space, the MDS problem is formulated as,

minX

i<j

(dH (xi, xj) − dL(yi, yj))2

(4)

where dH (·) and dL(·) are distance (similarity) metrics
in the ambient and intrinsic space, respectively. Differ-
ent choices of the metric, leads to different dimension-
ality reduction algorithms. For instance, classical metric
MDS is based on Euclidean distance between the points
while using the geodesic distance induced by a neighbor-
hood graph leads to Isomap [40]. Similarly, many different
distance metrics have been proposed corresponding to non-
linear mappings between the ambient space and the intrinsic
space. A majority of these approaches are based on spec-
tral decompositions and suffer many drawbacks, (i) compu-
tational complexity scales as O(n3) for n data points, (ii)
ambiguity in the choice of the correct non-linear function,
and (iii) collapsed embeddings on more complex data [15].
To overcome these limitations, we employ a DNN to ap-
proximate the non-linear mapping that transforms the am-
bient representation, x, to the intrinsic space, y by a para-
metric function y = f (x; θ) with parameters θ. We learn
the parameters of the mapping within the MDS framework,

min

θ

n

n

Xi=1

Xi=1

[dH (xi, xj ) − dL(f (xi; θ), f (xj ; θ))]2 + λkθk2

2

where the second term is a regularizer with a hyperparam-
eter λ. Figure 3 shows an illustration of the DNN based
mapping.

In practice, directly learning the mapping from the am-
bient to the intrinsic space is very challenging, especially
for disentangling a complex manifold under high levels of
compression. We adopt a curriculum learning [3] approach
to overcome this challenge and progressively reduce the di-
mensionality of the mapping in multiple stages. We start
with easier sub-tasks and progressively increase the difﬁ-
culty of the tasks. For example, a direct mapping from
R512 → R15 is instead decomposed into multiple mapping
functions R512 → R256 → R128 → R64 → R32 → R15.
We formulate the learning problem for L mapping functions
(cid:0)yl = fl(x; θ)(cid:1) as:
Xl=1

αl hdH (xi, xj ) − dL(yl

+ λkθlk2

j )i2

Xj=1

Xi=1

i, yl

θ1,...,θL

min

L

n

n

2

3991

where θl are the parameters of the l-th mapping. Appropri-
ately scheduling the αl weights enables us to set it up as a
curriculum learning problem.

4. Experiments

In this section, ﬁrst we will estimate the intrinsic di-
mensionality of multiple image representations over mul-
tiple datasets of varying complexity. Then, we will evalu-
ate the efﬁcacy of the proposed DeepMDS model in ﬁnding
the mapping from the ambient to the intrinsic space while
maintaining its discriminative ability.

4.1. Datasets

We choose two different domains of classiﬁcation prob-
lems for our experiments, face veriﬁcation and image clas-
siﬁcation. We consider two different face datasets for the
former and the ImageNet ILSVRC-2012 for the latter. Re-
call that DeepMDS is an unsupervised method, so category
information associated with the objects or faces is neither
used for intrinsic dimensionality estimation nor for learning
the mapping from the ambient to intrinsic space.

LFW [19]: 13,233 face images of 5,749 subjects, down-
loaded from the web. These images exhibit limited varia-
tions in pose, illumination, and expression, since only faces
that could be detected by the Viola-Jones face detector [46]
were included in the dataset.

IJB-C [29]: IARPA Janus Benchmark-C (IJB-C) dataset
consists of 3,531 subjects with a total of 31,334 (21,294
face and 10,040 non-face) still images and 11,779 videos
(117,542 frames), an average of 39 images per subject. This
dataset emphasizes faces with full pose variations, occlu-
sions and diversity of subject occupation and geographic
origin. Images in this dataset are labeled with ground truth
bounding boxes and other covariate meta-data such as oc-
clusions, facial hair and skin tone.

ImageNet [34]: The ImageNet ILSVRC-2012 classiﬁca-
tion dataset consists of 1000 classes, with 1.28 million im-
ages for training and 50K images for validation. We use
a subset of this dataset by randomly selecting 100 classes
with the largest number of images, for a total of 130K train-
ing images and 5K testing images.

4.2. Representation Models

For the face-veriﬁcation task, we consider multiple
publicly available state-of-the-art face embedding models,
namely, 128-dim FaceNet [35] representation and 512-dim
SphereFace [26] representation. In addition, we also evalu-
ate a 512-dim variant of FaceNet3 that outperforms the 128-
dim version. All of these representations are learned from
the CASIA WebFace [47] dataset, consisting of 494,414 im-
ages across 10,575 subjects. For image classiﬁcation on the

(a) Distance Distribution p(r)

(b) Least Squares Fitting

Figure 4: Intrinsic Dimensionality: (a) Geodesic distance
distribution, and (b) global minimum of RMSE.

ImageNet dataset, we choose a pre-trained 34 layer version
of the ResNet [16] architecture.

4.3. Baseline Methods

Intrinsic Dimensionality: We select two different algo-
rithms for estimating the intrinsic dimensionality of a given
representation, a classical k-nearest neighbor based esti-
mator [31] and “Intrinsic Dimensionality Estimation Algo-
rithm” (IDEA) [33].

Dimensionality Reduction: We compare DeepMDS
against three dimensionality reduction algorithms, principal
component analysis (PCA) for linear dimensionality reduc-
tion, Isomap [40] and denoising autoencoders [45] (DAE).

4.4. Intrinsic Dimensions

Implementation Details: The ID estimates for all the
methods we evaluate are dependent on the number of neigh-
bors k. For the baselines, k is used to compute the param-
eters of the probability density. For our method, k parame-
terizes the construction of the neighborhood graph. For the
latter, the choice of k is constrained by three factors; (1) k
should be small enough to avoid shortcuts between points
that are close to each other in the Euclidean space, but are
potentially far away in the corresponding intrinsic manifold
due to highly complicated local curvatures. (2) On the other
hand, k should also be large enough to result in a connected
graph i.e., there are no isolated data samples., and (3) k that
best matches the geodesic distance distribution of a hyper-
sphere of the same ID i.e., k that minimizes the RMSE.
Figure 4a shows the distance distributions for SphereFace
with k = 15, a 16-hypersphere and a 16-dim Gaussian.
The close similarity of the pairwise distance distributions
of these manifolds in the graph induced geodesic distance
space suggests that the ID of SphereFace (512-dim ambi-
ent space) is 16. Figure 4b shows the optimal RMSE for
SphereFace4 at different values of m. For all the approaches
we select the k-nearest neighbors using cosine similarity for
SphereFace, Euclidean distance for ResNet and arc-length,

4Similar curves for other representations and datasets can be found in

3https://github.com/davidsandberg/facenet

the supplementary material.

3992

0.650.700.750.800.850.900.951.00log(r/rmax)2.001.751.501.251.000.750.500.250.00log(p(r)/p(rmax))Representation (K=15)Gaussian (m=16)Hypersphere (m=16)020406080100120Dimension0246810Root Mean Squared ErrorLFW (m=10)IJB-C (m=16)Table 1: Intrinsic Dimensionality: Graph Distance [13]

Table 2: LFW Face Veriﬁcation for SphereFace Embedding

Representation

dataset

FaceNet-128

FaceNet-512

SphereFace

LFW
IJB-C

LFW
IJB-C

LFW
IJB-C

ResNet-34

ImageNet-100

4

10*
10

10*
11

10*
14

16

k

9

11
10

11
12

13
16

19*

7

13
10

11
11

11
14

18

15

18
11*

17
12*

9

16*

23

Dimension

Dimension Reduction method

PCA

Isomap

DAE

DeepMDS

512
256
128
64
32
16

10 (ID)

96.75%
96.80%
91.71%
66.38%
32.67%
16.04%

96.74%

92.88%
93.18%
95.00%
95.31%
89.47%
77.31%

77.80%
32.95%
32.04%
11.71%
27.53%
6.73%

96.73%
96.44%
96.50%
96.31%
95.95%
92.33%

T
1

x2

d(x1, x2) = cos−1 (cid:16) x
kx1kkx2k(cid:17), for FaceNet features, as
the latter are normalized to reside on the surface of a uni-
tary hypersphere. Finally, for simplicity, we round the ID
estimates to the nearest integer for all the methods.

Experimental Results: Table 1 reports the ID estimates
from the graph method for different values of k5 and for dif-
ferent representation models across different datasets. Due
to lack of space we report the ID estimates of the baselines
in the supplementary material. We make a number of obser-
vations from our results: (1) Surprisingly, the ID estimates
across all the datasets, feature representations and ID meth-
ods are signiﬁcantly lower than the dimensionality of the
ambient space, between 10 and 20, suggesting that image
representations could, in principle, be almost 10× to 50×
(2) Both6 the k-NN based estimator [31]
more compact.
and the IDEA estimator [33] are less sensitive to the num-
ber of nearest neighbors in comparison to the graph distance
based method [13], but are known to underestimate ID for
sets with high intrinsic dimensionality [43].

4.5. Dimensionality Reduction

Given the estimates of the dimensionality of the intrin-
sic space, we learn the mapping from the ambient space
to a plausible intrinsic space with the goal of retaining the
discriminative ability of the representation. The true intrin-
sic representation (ID and space) is unknown and therefore
not feasible to validate directly. However, verifying its dis-
criminate power can serve to indirectly validate both the ID
estimate and the learned intrinsic space.

Implementation Details: We ﬁrst extract image features
through the representations i.e., FaceNet-128, FaceNet-
512 and SphereFace for face images and ResNet-34 for
ImageNet-100. The architecture of the proposed DeepMDS
model is based on the idea of skip connection laden residual
units [16]. We train the mapping from the ambient to intrin-
sic space in multiple stages with each stage comprising of
two residual units. Once the individual stages are trained,
all the L projection models are jointly ﬁne-tuned to main-
tain the pairwise distances in the intrinsic space. We adopt a

5* denotes ﬁnal ID estimate that satisﬁes all constraints on k.
6Reported in supplementary material due to space constraints.

similar network structure (residual units) and training strat-
egy (stagewise training and ﬁne-tuning) for the stacked de-
noising autoencoder baseline. From an optimization per-
spective, training the autoencoder is more computationally
efﬁcient than the DeepMDS model, O(n) vs O(n2).

The parameters of the network are learned using the
Adam [22] optimizer with a learning rate of 3 × 10−4 and
the regularization parameter λ = 3 × 10−4. We observed
that using the cosine-annealing scheduler [27] was critical
to learning an effective mapping. To facilitate classiﬁcation
on ImageNet in the intrinsic space, after learning the pro-
jection, we separately learn a linear as well as a k-nearest
neighbor (k-NN) classiﬁer on the projected feature vectors
of the training set.
Experimental Results: We evaluate the efﬁcacy of the
learned projections, namely PCA, Isomap and DeepMDS,
in the learned intrinsic space and compare their respective
performance in the ambient space. Face representations are
evaluated in terms of veriﬁcation (TAR @ FAR) perfor-
mance and classiﬁcation on ImageNet-100 in terms of accu-
racy (Top-1 and Top-5). Given the ID estimate, designing
an appropriate scheme for mapping the intrinsic manifold
is much more challenging than the ID estimation itself. To
show how dimensionality of the intrinsic space inﬂuences
the performance of image representations, we evaluate and
compare their performance at multiple intermediate spaces.
Face veriﬁcation is performed on the IJB-C dataset fol-
lowing its veriﬁcation protocol and on the LFW dataset
following the BLUFR [25] protocol. Due to space con-
straints we only show results on the DeepMDS model here,
corresponding results for the baseline dimensionality re-
duction methods can be found in the supplementary mate-
rial. Figure 5 shows the ROC curves for the IJB-C dataset
and the precision-recall curves for a image retrieval task
on ImageNet-100. Table 2 reports the veriﬁcation rate at
FAR of 0.1% on the LFW dataset. Similarly, Table 3 shows
the Top-1 and Top-5 accuracy on ImageNet-100 for a pre-
trained ResNet-34 representation via a parametric (linear)
as well as a non-parametric (k-NN) classiﬁer.

We make the following observations from these results:
(1) for all the tasks the performance of the DeepMDS fea-
tures up to 32 dimensions (for faces) is comparable to the
original 128-dim and 512-dim features. The 10-dim space

3993

(a) FaceNet-128

(b) FaceNet-512

(c) SphereFace

(d) Image Retrieval

Figure 5: Face Veriﬁcation on IJB-C [29] (TAR @ 0.1% FAR in legend) for the (a) FaceNet-128, (b) FaceNet-512 and (c) SphereFace embeddings and
(d) Image retrieval on ImageNet-100 for the ambient 512-dim ResNet-34 representation, the intrinsic 19-dim space obtained from DAE and DeepMDS.

Table 3: ImageNet-100 Classiﬁcation (%) for ResNet-34

Table 4: DeepMDS Training Methods (TAR @ 0.1% FAR)

Classiﬁer Method

Dimension

512

256

128

64

32

19 (ID)

Top-1

Linear

k-NN

Top-5

Linear

DAE

DeepMDS

DAE

DeepMDS

DAE

DeepMDS

80.0
80.0
83.4
83.4

96.0
96.0

80.9
79.4
81.3
80.9

95.5
95.3

73.2
76.1
79.1
78.7

90.2
93.1

70.0
71.4
76.4
77.8

88.0
85.2

63.1
70.2
76.7
77.1

84.2
85.2

50.2
68.0
73.4
77.0

76.5
84.8

of DeepMDS on LFW, consisting largely of frontal face im-
ages with minimal pose variations and facial occlusions,
achieves a TAR of 92.33% at 0.1% FAR, a loss of about
4.5% compared to the ambient space. The 12-dim space
of DeepMDS on IJB-C, with full pose variations, occlu-
sions and diveristy of subject, achieves a TAR of 62.25%
at 0.1% FAR, compared to 69.32% in the ambient space.
(2) the proposed DeepMDS model is able to learn a low-
dimensional space up to the ID with a performance penalty
of 5%-10% for compression factors of 30× to 40× for 512-
dim representations, underscoring the fact that learning a
mapping from ambient to intrinsic space is more challeng-
ing than estimating the ID itself.
(3) In both tasks, we
observe that the DeepMDS model is able to retain signiﬁ-
cantly more discriminative ability compared to the baseline
approaches even at high levels of compression. Although
DAE achieves comparative results on ImageNet-100 classi-
ﬁcation, DeepMDS signiﬁcantly outperforms DAE for im-
age retrieval tasks. While Isomap is more competitive than
the other baselines it suffers from some drawbacks: (i) Due
to its iterative nature, it does not provide an explicit map-
ping function for new (unseen) data samples, while the au-
toencoder and DeepMDS models can map such data sam-
ples. Therefore, Isomap cannot be utilized to evaluate clas-
siﬁcation accuracy on the validation/test set of ImageNet-
100 dataset, and (ii) Computational complexity of Isomap is
O(n3) and hence does not scale well to large datasets (IJB-
C, ImageNet) and needs approximations, such as Nystr¨om
approximation [39], for tractability.

Ablation Study: Here we demonstrate the efﬁcacy of
the stagewise learning process for training the DeepMDS
model. All models have the same capacity. We con-

Method

Direct

Direct+IS

Stagewise + Finetune

Stagewise

TAR

80.25

86.15

90.42

92.33

(1) Direct mapping from the am-
sider four variants:
bient to intrinsic space, (2) Direct+IS: direct mapping
from ambient to intrinsic space with intermediate super-
vision at each stage i.e., optimize aggregate intermediate
losses, (3) Stagewise learning of the mapping, and (4)
Stagewise+Fine-Tune: the projection model trained stage-
wise and then ﬁne-tuned. Table 4 compares the results of
these variations on the LFW dataset (BLUFR protocol).
Our results suggest that stagewise learning of the non-linear
projection models is more effective at progressively disen-
tangling the ambient representation. Similar trend was ob-
served on larger datasets (IJB-C and ImageNet).
In fact,
stagewise training with ﬁne-tuning was critical in learning
an effective projection, both for DeepMDS as well as DAE.

5. Concluding Remarks

This paper addressed two questions, given a DNN based
image representation, what is the minimum degrees of free-
dom in the representation i.e., its intrinsic dimension and
can we ﬁnd a mapping between the ambient and intrin-
sic space while maintaining the discriminative capability of
the representation? Contributions of the paper include, (i)
a graph induced geodesic distance based approach to es-
timate the intrinsic dimension, and (ii) DeepMDS, a non-
linear projection to transform the ambient space to the in-
trinsic space. Experiments on multiple DNN based image
representations yielded ID estimates of 9 to 20, which are
signiﬁcantly lower than the ambient dimension (10× to 40
×). The DeepMDS model was able to learn a projection
from ambient to the intrinsic space while preserving its dis-
criminative ability, to a large extent, on the LFW, IJB-C and
ImageNet-100 datasets. Our ﬁndings in this paper suggest
that image representations could be signiﬁcantly more com-
pact and call for the development of algorithms that can di-
rectly learn more compact image representations.

3994

102101100101False Accept Rate (%)020406080100Verification Rate (%)128D: 42.57%64D: 42.16%32D: 40.74%16D: 37.89%12D: 32.69%102101100101False Accept Rate (%)020406080100Verification Rate (%)512D: 69.32%256D: 69.25%128D: 69.23%64D: 69.06%32D: 68.90%16D: 66.57%12D: 62.25%102101100101False Accept Rate (%)020406080100Verification Rate (%)512D: 71.26%256D: 71.13%128D: 70.63%64D: 68.36%32D: 64.02%16D: 59.75%References

[1] T. Ahonen, A. Hadid, and M. Pietik¨ainen. Face recognition
with local binary patterns. European Conference on Com-
puter Vision, 2004. 2

[2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimen-
sionality reduction and data representation. Neural Compu-
tation, 15(6):1373–1396, 2003. 3

[3] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-
riculum learning. In International Conference on Machine
Learning, pages 41–48. ACM, 2009. 5

[4] R. S. Bennett. Representation and analysis of signals part
xxi. the intrinsic dimensionality of signal collections. Tech-
nical report, Johns Hopkins University Baltimore MD, Dep-
tartment of Electrical Engineering and Computer Science,
1965. 1

[5] V. N. Boddeti. Secure face matching using fully homomor-
phic encryption. In IEEE International Conference on Bio-
metrics: Theory, Applications, and Systems (BTAS), 2018.
1

[6] J. Bruske and G. Sommer.

Intrinsic dimensionality esti-
mation with optimally topology preserving maps.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
20(5):572–575, 1998. 3

[7] F. Camastra and A. Vinciarelli.

Estimating the intrin-
sic dimension of data with a fractal-based method.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
24(10):1404–1407, 2002. 3

[8] R. R. Coifman and S. Lafon. Diffusion maps. Applied and

Computational Harmonic Analysis, 21(1):5–30, 2006. 3

[9] J. A. Costa and A. O. Hero. Geodesic entropic graphs for di-
mension and entropy estimation in manifold learning. IEEE
Transactions on Signal Processing, 52(8):2210–2221, 2004.
1

[10] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In IEEE Conference on Computer Vision
and Pattern Recognition, 2005. 2

[11] K. Fukunaga and D. R. Olsen. An algorithm for ﬁnding in-
trinsic dimensionality of data. IEEE Transactions on Com-
puters, 100(2):176–183, 1971. 3

[12] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman.
From few to many: Illumination cone models for face recog-
nition under variable lighting and pose. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 23(6):643–
660, 2001. 3

[13] D. Granata and V. Carnevale. Accurate estimation of the in-
trinsic dimension using graph distances: Unraveling the ge-
ometric complexity of datasets. Scientiﬁc Reports, 6:31377,
2016. 3, 4, 7

[14] P. Grassberger and I. Procaccia. Measuring the strangeness
of strange attractors. In The Theory of Chaotic Attractors,
pages 170–189. Springer, 2004. 3, 4

[15] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 1735–
1742, 2006. 3, 5

[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In European Conference on Com-
puter Vision, pages 630–645. Springer, 2016. 1, 2, 3, 6, 7

[17] M. Hein and J.-Y. Audibert. Intrinsic dimensionality estima-
tion of submanifolds in Rd. In International Conference on
Machine Learning, 2005. 3

[18] G. E. Hinton and R. R. Salakhutdinov.

dimensionality of data with neural networks.
313(5786):504–507, 2006. 3

Reducing the
Science,

[19] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face
recognition in unconstrained environments. Technical re-
port, Technical Report 07-49, University of Massachusetts,
Amherst, 2007. 6

[20] I. T. Jolliffe. Principal component analysis and factor anal-
In Principal Component Analysis, pages 115–128.

ysis.
Springer, 1986. 3

[21] B. K´egl. Intrinsic dimension estimation using packing num-
bers. In Advances in Neural Information Processing Systems,
2003. 3

[22] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 7

[23] J. B. Kruskal. Multidimensional scaling by optimizing
goodness of ﬁt to a nonmetric hypothesis. Psychometrika,
29(1):1–27, 1964. 3, 5

[24] E. Levina and P. J. Bickel. Maximum likelihood estimation
of intrinsic dimension. In Advances in Neural Information
Processing Systems, 2005. 3

[25] S. Liao, Z. Lei, D. Yi, and S. Z. Li. A benchmark study of
large-scale unconstrained face recognition. In IEEE Interna-
tional Joint Conference on Biometrics (IJCB), 2014. 7

[26] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 1, 2, 3, 6

[27] I. Loshchilov and F. Hutter. SGDR: stochastic gradient de-
scent with warm restarts. arXiv preprint arXiv:1608.03983,
2016. 7

[28] D. G. Lowe. Object recognition from local scale-invariant
features. In IEEE International Conference on Computer Vi-
sion, 1999. 2

[29] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller,
C. Otto, A. K. Jain, W. T. Niggel, J. Anderson, J. Cheney,
et al. Iarpa janus benchmark–c: Face dataset and protocol.
In International Conference on Biometrics, 2018. 1, 6, 8

[30] H. Murase and S. K. Nayar. Visual learning and recogni-
tion of 3-d objects from appearance. International Journal
of Computer Vision, 14(1):5–24, 1995. 3

[31] K. W. Pettis, T. A. Bailey, A. K. Jain, and R. C. Dubes. An
intrinsic dimensionality estimator from near-neighbor infor-
mation. IEEE Transactions on Pattern Analysis and Machine
Intelligence, (1):25–37, 1979. 3, 6, 7

[32] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduc-
tion by locally linear embedding. Science, 290(5500):2323–
2326, 2000. 3

[33] A. Rozza, G. Lombardi, C. Ceruti, E. Casiraghi, and P. Cam-
padelli. Novel high intrinsic dimensionality estimators. Ma-
chine Learning, 89(1-2):37–65, 2012. 6, 7

3995

[34] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015. 6

[35] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2015. 1, 2, 3, 6

[36] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1

[37] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI Conference on Artiﬁcial
Intelligence, volume 4, page 12, 2017. 3

[38] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Web-scale
training for face identiﬁcation. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015. 1, 3

[39] A. Talwalkar, S. Kumar, and H. Rowley. Large-scale mani-
fold learning. In IEEE Conference on Computer Vision and
Pattern Recognition. IEEE, 2008. 2, 8

[40] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global
geometric framework for nonlinear dimensionality reduc-
tion. Science, 290(5500):2319–2323, 2000. 2, 3, 5, 6

[41] J. Theiler. Estimating fractal dimension. JOSA A, 7(6):1055–

1073, 1990. 1

[42] M. A. Turk and A. P. Pentland. Face recognition using eigen-
faces. In IEEE Conference on Computer Vision and Pattern
Recognition, 1991. 1, 2

[43] P. J. Verveer and R. P. W. Duin. An evaluation of intrin-
sic dimensionality estimators. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 17(1):81–86, 1995. 3, 7
[44] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.
Extracting and composing robust features with denoising au-
toencoders. In International Conference on Machine Learn-
ing, pages 1096–1103. ACM, 2008. 3

[45] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-
A. Manzagol. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local de-
noising criterion. Journal of Machine Learning Research,
11(Dec):3371–3408, 2010. 3, 6

[46] P. Viola and M. J. Jones. Robust real-time face detection.
International Journal of Computer Vision, 57(2):137–154,
2004. 6

[47] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-

tation from scratch. arXiv:1411.7923, 2014. 6

3996

