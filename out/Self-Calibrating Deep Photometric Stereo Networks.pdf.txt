Self-calibrating Deep Photometric Stereo Networks

Guanying Chen1 Kai Han2 Boxin Shi3

,

4 Yasuyuki Matsushita5 Kwan-Yee K. Wong1

1The University of Hong Kong

2University of Oxford

3Peking University

4Peng Cheng Laboratory

5Osaka University

Abstract

This paper proposes an uncalibrated photometric stereo
method for non-Lambertian scenes based on deep learning.
Unlike previous approaches that heavily rely on assump-
tions of speciﬁc reﬂectances and light source distributions,
our method is able to determine both shape and light di-
rections of a scene with unknown arbitrary reﬂectances ob-
served under unknown varying light directions. To achieve
this goal, we propose a two-stage deep learning architec-
ture, called SDPS-Net, which can effectively take advantage
of intermediate supervision, resulting in reduced learning
difﬁculty compared to a single-stage model. Experiments
on both synthetic and real datasets show that our proposed
approach signiﬁcantly outperforms previous uncalibrated
photometric stereo methods.

1. Introduction

Photometric stereo aims at recovering the surface normal
of a static object from a set of images captured under differ-
ent light directions [34, 29]. Calibrated photometric stereo
methods assume known light directions, and promising re-
sults have been reported [28] at the cost of tedious light
source calibration. The problem of uncalibrated photomet-
ric stereo, where light directions are unknown, still remains
an open challenge, and its stable solution is wanted because
of the ease of setting. In this work, we study the problem
of uncalibrated photometric stereo for surfaces with general
and unknown isotropic reﬂectance.

Most of the existing methods for uncalibrated photomet-
ric stereo [2, 27, 23] assume a simpliﬁed reﬂectance model,
such as the Lambertian model, and focus on resolving the
shape-light ambiguity, such as the Generalized Bas-Relief
(GBR) ambiguity [3]. Although methods of [19, 20] can
handle surfaces with general bidirectional reﬂectance dis-
tribution functions (BRDFs), they rely on a uniform distri-
bution of light directions for deriving a solution.

Recently, with the great success of deep learning in var-
ious computer vision tasks, deep learning based methods
have been introduced to calibrated photometric stereo [25,

31, 15, 5].
Instead of explicitly modeling complex sur-
face reﬂectances, they directly learn the mapping from re-
ﬂectance observations to surface normals given light direc-
tions. Although they have obtained promising results in a
calibrated setting, they cannot handle the more challenging
problem of uncalibrated photometric stereo, where light di-
rections are unknown. One simple strategy to handle uncal-
ibrated photometric stereo with deep learning is to directly
learn the mapping from images to surface normals without
taking the light directions as input. However, as reported
in [5], the performance of such a model lags far behind
those which take both images and light directions as input.
In this paper, we propose a two-stage model named Self-
calibrating Deep Photometric Stereo Networks (SDPS-Net)
to tackle this problem. The ﬁrst stage of SDPS-Net, de-
noted as Lighting Calibration Network (LCNet), takes an
arbitrary number of images as input and estimates their
corresponding light directions and intensities. The second
stage of SDPS-Net, denoted as Normal Estimation Network
(NENet), estimates a surface normal map of a scene based
on the lighting conditions estimated by LCNet and the input
images. The rationales behind the design of our two-stage
model are as follows. First, lighting information is very im-
portant for normal estimation since lighting is the source of
various cues, such as shading and reﬂectance, and estimat-
ing the light directions (3-vectors) and intensities (scalars)
is in principle much easier than directly estimating the nor-
mal map (a 3-vector at each pixel location) together with the
lighting conditions. Second, by explicitly learning to esti-
mate light directions and intensities, the model can take ad-
vantage of the intermediate supervision, resulting in a more
interpretable behavior. Last, the proposed LCNet can be
seamlessly integrated with existing calibrated photometric
stereo methods, which enables them to deal with unknown
lighting conditions. Our code and model can be found at
https://guanyingc.github.io/SDPS-Net.

2. Related Work

In this section, we review learning based photometric
stereo and uncalibrated photometric stereo methods. We
also brieﬂy review the loosely related work on learning

18739

based lighting estimation. Readers are referred to [28] for
a comprehensive survey on calibrated photometric stereo
with Lambertian surfaces and general BRDFs using non-
learning based methods.

Learning based photometric stereo Recently, a few
deep learning based methods have been introduced to cali-
brated photometric stereo [25, 31, 15, 5]. Santo et al. [25]
proposed a fully-connected network to learn the mapping
from reﬂectance observations captured under a pre-deﬁned
set of light directions to surface normal in a pixel-wise
manner. Taniai and Maehara [31] introduced an unsuper-
vised learning framework that predicts both the surface nor-
mals and reﬂectance images of an object. Their model is
“trained” at test time for each test object by minimizing the
reconstruction loss between the input images and the ren-
dered images. Ikehata [15] introduced a ﬁxed shape rep-
resentation, called observation map, that is invariant to the
number and permutation of the images. For each surface
point of the object, all its observations are merged into an
observation map based on the given light directions, and
the observation map is then fed to a convolutional neural
network (CNN) to regress the normal vector. Chen et al. [5]
proposed a fully-convolutional network (FCN) to infer the
normal map from the input image-lighting pairs, and an
order-agnostic max-pooling operation was adopted to han-
dle an arbitrary number of inputs. All the above methods
assume known lighting conditions and cannot handle un-
calibrated photometric stereo, where the light directions and
intensities are not known a priori.

Uncalibrated photometric stereo When lighting is un-
known, the surface normals of a Lambertian object can
only be estimated up to a 3 × 3 linear ambiguity [12],
which can be reduced to a 3-parameter GBR ambigu-
ity [3, 36] using the surface integrability constraint. Pre-
vious work used additional clues like albedo priors [2, 27],
inter-reﬂections [4], specular spikes [7], Torrance and Spar-
row reﬂectance model [11], reﬂectance symmetry [30, 35],
multi-view images [9], and local diffuse maxima [23], to re-
solve the GBR ambiguity. Cho et al. [6] considered a semi-
calibrated case where the light directions are known but not
their intensities. There are few works that can handle non-
Lambertian surfaces under unknown lighting. Hertzmann
and Seitz [13] proposed an exemplar based method by in-
serting an additional reference object to the scene. Methods
based on cues like similarity in radiance changes [26, 19]
and attached shadow [22] were also introduced, but they re-
quire the light sources to be uniformly distributed on the
whole sphere. Recently, Lu et al. [18] introduced a method
based on the “constrained half-vector symmetry” to work
with non-uniform lightings. Different from these traditional
methods, our method can deal with surfaces with general
and unknown isotropic reﬂectance without the need of ex-
plicitly utilizing any additional clues or reference objects,

solving a complex optimization problem at test time, or
making assumptions on the light source distribution. The
work most related to ours is the UPS-FCN introduced in [5].
UPS-FCN is a single-stage model that directly regresses
surface normals from images that are normalized by the
known light intensities. Its performance lags far behind the
calibrated methods. In contrast, our method solves the prob-
lem in two stages. We ﬁrst tackles an easier problem of
estimating the light directions and intensities, and then esti-
mates the surface normals using the estimated lightings and
the input images.
Learning based lighting estimation Recently, learning
based single-image lighting estimation methods have at-
tracted considerable attention. Gardner et al. [10] intro-
duced a CNN for estimating HDR environment lighting
from an indoor scene image. Hold-Goeffroy et al. [14]
learned outdoor lighting using a physically-based sky
model. Weber et al. [32] estimated indoor environment
lighting from an image of an object with known shape.
Zhou et al. [37] estimated lighting, in the form of Spherical
Harmonics, from a human face image by assuming a Lam-
bertian reﬂectance model. Different from the above meth-
ods, our method can estimate accurate directional lightings
from multiple images of a static object with general shape
and non-Lambertian surface.

3. Image Formation Model

Following the conventional practice, we assume an or-
thographic camera with linear radiometric response, white
directional lightings coming from the upper-hemisphere,
and the viewing direction pointing towards the viewer. In
the rest of this paper, we refer to light direction and in-
tensity as “lighting”. Consider a non-Lambertian surface
whose appearance is described by a general isotropic BRDF
ρ. Given a surface point with normal n ∈ R3 being illumi-
nated by the j-th incoming lighting with direction lj ∈ R3
and intensity ej ∈ R, the image formation model can be
expressed as

mj = ejρ(n, lj) max(n⊤lj, 0) + ǫj,

(1)

where m represents the measured intensity, max(:, 0) ac-
counts for attached shadows, and ǫ accounts for the global
illumination effects (cast shadows and inter-reﬂections) and
noise.

Based on this model, given the observations of p sur-
face points under q different incoming lightings, the goal
of uncalibrated photometric stereo is to estimate the surface
normals for these p surface points given only the measured
intensities.
In this work, we tackle this problem using a
two-stage approach. In particular, we ﬁrst estimate light-
ings from the measured intensities, and then solve for the
surface normals using the estimated lightings and measured
intensities.

8740

Conv	+	LReLU

Conv	(stride-2)	+	LReLU

Fully-connected

Deconv

L2-Norm C Concat

N

Normalize

Local	Feature

C

Input	1

.

.

.

Input	$

64

128

128

128 256

256

Max

Pooling

Global	Feature

.

.

.

256

256

256

64

C

!"

#"

#%

!%

N

N

C

.

.

.

C

128 128 64

3

3

64 128

128 256

256 128

128

Max

Pooling

(a)	Lighting	Calibration	Network

(b)	Normal	Estimation	Network

Figure 1. The network architecture of SDPS-Net is composed of (a) Lighting Calibration Network and (b) Normal Estimation Network.
Kernel sizes for all convolutional layers are 3 × 3, and values above the layers indicate the number of feature channels.

4. Learning Uncalibrated Photometric Stereo

In this section, we introduce our two-stage framework,
called SDPS-Net, for uncalibrated photometric stereo (see
Fig. 1). The ﬁrst stage of SDPS-Net, denoted as Light-
ing Calibration Network (LCNet, Fig. 1 (a)), takes an ar-
bitrary number of images as input and estimates their cor-
responding light directions and intensities. The second
stage of SDPS-Net, denoted as Normal Estimation Network
(NENet, Fig. 1 (b)), estimates an accurate normal map of
the object based on the lightings estimated by LCNet and
the input images.

4.1. Lighting Calibration Network

To estimate lightings from the images, an intuitive ap-
proach would be directly regressing the light direction vec-
tors and intensity values. However, we propose that for-
mulating the lighting estimation as a classiﬁcation problem
is a superior choice, as will be veriﬁed by our experiments.
Our arguments are as follows. Fist, classifying a light direc-
tion into a certain range is easier than regressing the exact
value(s), and this will reduce the learning difﬁculty. Sec-
ond, taking discretized light directions as input may allow
NENet to better tolerate small errors in the estimated light
directions.

Discretization of lighting space Since we cast our light-
ing estimation as a classiﬁcation problem, we need to dis-
cretize the continuous lighting space. Note that a light di-
rection in the upper-hemisphere can be described by its az-
imuth φ ∈ [0◦, 180◦] and elevation θ ∈ [−90◦, 90◦] (see
Fig. 2 (a)). We can discretize the light directoin space by
evenly dividing both the azimuth and elevation into Kd
d classes (see Fig. 2 (b)). Solving a K 2
bins, resulting in K 2
d -
class classiﬁcation problem is not computationally efﬁcient,

y

P

θ

φ

y

x

xx

z

zz

(a)

(b)

Figure 2. (a) Illustration of the coordinate system (z axis is the
viewing direction). φ ∈ [0◦, 180◦] and θ ∈ [−90◦, 90◦] are the
azimuth and elevation of the light direction, respectively. (b) Ex-
ample discretization of the light direction space when Kd = 18.

as the softmax probability vector will have a very high di-
mension even when Kd is not large (e.g., K 2
d = 1, 296 when
Kd = 36). Instead, we estimate the azimuth and elevation
of a light direction separately, leading to two Kd-class clas-
siﬁcation problems. Similarly, we evenly divide the range
of possible light intensities into Ke classes (e.g., Ke = 20
for a possible light intensity range of [0.2, 2.0]).

Local-global feature fusion A straightforward approach
to estimate the lighting for each image is simply taking a
single image as input, encoding it into a feature map using
a CNN, and feeding the feature map to a lighting prediction
layer. It is not surprising that the result of such a simple so-
lution is far from satisfactory. Note that the appearance of
an object is determined by its surface geometry, reﬂectance
model and the lighting. The feature map extracted from a
single observation obviously does not provide sufﬁcient in-
formation for resolving the shape-light ambiguity. Thanks
to the nature of photometric stereo where multiple observa-
tions of an object are considered, we propose a local-global

8741

feature fusion strategy to extract more comprehensive infor-
mation from multiple observations.

Speciﬁcally, we separately feed each image into a
shared-weight feature extractor to extract a feature map,
which we call local feature as it only provides information
from a single observation. All local features of the input
images are then aggregated into a global feature through
a max-pooling operation, which has been proven to be efﬁ-
cient and robust on aggregating salient features from a vary-
ing number of unordered inputs [33, 5]. Such a global fea-
ture is expected to convey implicit surface geometry and
reﬂectance information of the object which help resolve the
ambiguity in lighting estimation. Each local feature is con-
catenated with the global feature, and fed to a shared-weight
lighting estimation sub-network to predict the lighting for
each individual image. By taking both local and global fea-
tures into account, our model can produce much more reli-
able results than using the local features alone. We empir-
ically found that additionally including the object mask as
input can effectively improve the performance of lighting
estimation, as will be seen in the experiment section.

Network architecture LCNet
is a multi-input-multi-
output (MIMO) network that consists of a shared-weight
feature extractor, an aggregation layer (i.e., max-pooling
layer), and a shared-weight lighting estimation sub-network
(see Fig. 1 (a)). It takes the observations of the object to-
gether with the object mask as input, and outputs the light
directions and intensities in the form of softmax probability
vectors of dimension Kd (azimuth), Kd (elevation) and Ke
(intensity), respectively. We convert the output of LCNet to
3-vector light directions and scalar intensity values by sim-
ply taking the middle value of the range with the highest
probability1.

Loss function Multi-class cross entropy loss is adopted
for both light direction and intensity estimation, and the
overall loss function is

LLight = λla Lla + λle Lle + λeLe,

(2)

where Lla and Lle are the loss terms for azimuth and eleva-
tion of the light direction, and Le is the loss term for light
intensity. During training, weights λla , λle and λe for the
loss terms are set to 1.

4.2. Normal Estimation Network

NENet is a multi-input-single-output (MISO) network.
The network architecture of NENet
is similar to PS-
FCN [5], consisting of a shared-weight feature extractor,
an aggregation layer, and a normal regression sub-network
(see Fig. 1 (b)). The key difference between NENet and

1We have experimentally veriﬁed that alternative ways like taking the
expectation of the probability vector or performing quadratic interpolation
in the neighborhood of the peak value do not improve the result.

PS-FCN is that PS-FCN requires accurate lightings as in-
put, whereas NENet is trained with discretized lightings es-
timated by the LCNet and shows a more robust behavior
over noise in the lightings.

NENet ﬁrst normalizes the input images using the light
intensities predicted by LCNet, and then concatenates the
light directions predicted by LCNet with the images to form
the input of the shared-weight feature extractor. Given an
image of size h × w, the loss function for NENet is

LNormal =

1
hw

hw

Xi (cid:0)1 − n⊤

i ˜ni(cid:1) ,

(3)

where ni and ˜ni denote the predicted normal and the
ground-truth normal, respectively, at pixel i.

4.3. Training Data

We adopted the publicly available synthetic Blobby and
Sculpture datasets [5] for training. Blobby and Sculpture
datasets provide surfaces with complex normal distributions
and diverse materials from MERL dataset [21]. Effects
of cast shadow and inter-reﬂection were considered during
rendering using the physically based raytracer Mitsuba [16].
There are 85, 212 samples in total. Each sample was ren-
dered under 64 distinct light directions sampled from the
upper-hemisphere with uniform light intensity, resulting in
5, 453, 568 images (85, 212 × 64). The rendered images
have a dimension of 128 × 128.

light

To simulate images under different

intensities,
we randomly generated light intensities in the range of
[0.2, 2.0] to scale the magnitude of the images (i.e., the ratio
of the highest light intensity to the lowest one is 10)2. Note
that this selected range contains a wider range of intensity
value than the public photometric stereo datasets like DiLi-
GenT benchmark [28] and Gourd&Apple dataset [1]. The
color intensities of the input images were normalized to the
range of [0, 1]. During training, we applied noise pertur-
bation in the range of [−0.025, 0.025] for data augmenta-
tion, and the input image size for LCNet and NENet was
128 × 128. At test time, NENet can take images of dif-
ferent dimensions, while the input for LCNet is rescaled to
128 × 128 as it contains fully-connected layers and requires
the input to have a ﬁxed spatial dimension. Trained only
on the synthetic dataset, we will show that our model can
generalize well on real datasets.

5. Experimental Results

We performed network analysis for our method, and
compared our method with the previous state-of-the-art
methods on both synthetic and real datasets.

2Note that the ratio (other than the exact value) matters, since light

intensity can only be estimated up to a scale factor.

8742

Implementation details Our
framework was imple-
mented in PyTorch [24] and Adam optimizer [17] was used
with default parameters. LCNet and NENet contain 4.4
million and 2.2 million parameters, respectively. We ﬁrst
trained LCNet using a batch size of 32 for 20 epochs un-
til convergence, and then trained NENet from scratch given
the lightings estimated by LCNet with a batch size of 16
for 10 epochs. We found that end-to-end ﬁne-tuning did not
improve the performance. The learning rate was initially set
to 0.0005 and halved every 5 and 2 epochs for LCNet and
NENet, respectively. It took about 22 hours to train LCNet
and 26 hours to train NENet on a single Titan X Pascal GPU
with a ﬁxed input image number of 32.

Evaluation metrics To measure the accuracy of the pre-
dicted light directions and surface normals, the widely used
mean angular error (MAE) in degree is adopted. Since the
light intensities among the testing images can only be esti-
mated up to a scale factor s, we introduce the scale-invariant
relative error

Eerr =

1
q

q

Xi

(cid:18) |sei − ˜ei|

˜ei

(cid:19) ,

(4)

where q is the number of images, ei and ˜ei are the esti-
mated and ground-truth light intensities, respectively, for
The scale factor s is computed by solving
image i.
i (sei − ˜ei)2 with least squares3.
argmin

s Pn

5.1. Network Analysis with Synthetic Data

MERLTest dataset To quantitatively perform network
analysis for our method, we rendered a synthetic dataset,
denoted as MERLTest, of sphere and bunny shapes, de-
noted as SPHERE and BUNNY hereafter, respectively, us-
ing the physically based raytracer Mitsuba [16]. Each
shape was rendered with 100 isotropic BRDFs from MERL
dataset [21] under 100 light directions sampled from
leading to 200 test objects (see
the upper-hemisphere,
Fig. 3). Cast shadows and inter-reﬂections are considered
for BUNNY. For all experiments on synthetic dataset in-
volving input with unknown light intensities, we randomly
generated light intensities in the range of [0.2, 2.0]. Each
experiment was repeated ﬁve times and the average results
were reported.
Discretization of lighting space For a given number of
bins Kd, the maximum deviation angle for azimuth and el-
evation of a light direction is δ = 180◦/(Kd × 2) after
discretization (e.g., δ = 2.5◦ when Kd = 36). To investi-
gate how light direction discretization affects the normal es-
timation accuracy, we adopted the state-of-the-art calibrated
method PS-FCN [5] and MERLTest dataset as the testbed.

3As the calibrated intensity in the real dataset is in the form of 3-
vector, we repeat the estimated intensity to be a 3-vector and calculate the
average result.

(b) SPHERE (pearl-paint)

(a) Light source
Figure 3. (a) Lighting distribution of MERLTest dataset. The light
direction is visualized by mapping a 3-d vector [x, y, z] to a point
[x, y]. (b) and (c) show a sample image and ground-truth normal
for SPHERE and BUNNY, respectively.

(c) BUNNY (pink-jasper)

y

A

B

δ

P

D

x

C

z

(a)

(b)

Figure 4. (a) Light directions A, B, C, and D have the maximum
deviation angles with the light direction P after discretization. (b)
Upper-bound of normal estimation error for PS-FCN [5] under dif-
ferent light direction space discretization levels (∞ indicates no
discretization).

We divided the azimuth and elevation of light directions
into different number of bins ranging from 2 to 180. For a
speciﬁc bin number, we replaced each ground-truth light di-
rection by each of the four light directions having the max-
imum possible angular deviations after discretization (see
Fig. 4 (a)), respectively. We then used those light direc-
tions as input for PS-FCN to infer surface normals. The
normal estimation error reported in Fig. 4 (b) is the upper-
bound error for PS-FCN caused by the discretization. We
can see that the increase in error caused by discretization
is marginal when Kd ≥ 30.
In our implementation, we
empirically set Kd and Ke to 36 and 20, respectively. We
experimentally found that the performance of LCNet is ro-
bust to different discretization levels. We chose a relatively
sparse discretization of lighting space in this paper as it may
allow NENet to learn to better tolerate small errors in the es-
timated lighting at test time.
Effectiveness of LCNet To validate the design of LC-
Net, we compared LCNet with three baseline models for
lighting estimation. The ﬁrst baseline model, denoted as
LCNetreg, is a regression based model that directly regresses
the light direction vectors and intensity values (please refer
to the supplementary for implementation details). The sec-
ond baseline model, denoted as LCNetw/o mask, is a classiﬁ-
cation based model that only takes the images as input with-
out the object mask input. The last baseline model, denoted
as LCNetlocal, is a classiﬁcation based model that indepen-
dently estimates lighting for each observation (i.e., without

8743

249183036456090180Number of Bins for Azimuth and Elevation010203040Angular Error in DegreeSphereBunnyTable 1. Lighting estimation results on the MERLTest dataset. The
results are averaged over samples rendered with 100 BRDFs.

ID Model

Direction

Intensity Direction

Intensity

SPHERE

BUNNY

A0
A1
A2
A3

LCNet
LCNetreg
LCNetw/o mask
LCNetlocal

3.47
4.10
5.46
6.87

0.082
0.104
0.104
0.198

5.38
5.46
8.85
9.98

0.089
0.094
0.144
0.255

local-global feature fusion). All models were trained under
the same setting, and the results are summarized in Table 1.
Experiments with IDs A0 & A1 in Table 1 show that
the proposed classiﬁcation based LCNet consistently out-
performed the regression based baseline on both light di-
rection and intensity estimation. This echoes our hypothesis
that classifying a light direction to a certain range is easier
than regressing an exact value. Thus, solving the classiﬁ-
cation problem reduces the learning difﬁculty and improves
the performance. Experiments with IDs A0 & A2 show that
taking the object mask as input can effectively improve the
lighting estimation results. This might be explained by the
fact that object mask provides strong information for oc-
cluding contours of the object, and helps the network dis-
tinguish the shadow region from the non-object region. Ex-
periments with IDs A0 & A3 show that the proposed local-
global feature fusion strategy can effectively make use of
information from multiple observations, and signiﬁcantly
improve the lighting estimation accuracy. Please refer to
our supplementary for detailed lighting estimation results
of LCNet on BUNNY from MERLTest dataset.

Effectiveness of NENet Experiments with IDs B1 & B2
in Table 2 show that after training with the discretized light-
ings estimated by LCNet, NENet performs better than PS-
FCN given possibly noisy lightings at test time, while ex-
periments with IDs B3 & B4 show that training NENet
with the light directions estimated by the regression based
baseline is not always helpful. This result further demon-
strates that the proposed framework is robust to noisy light-
ings. Experiments with IDs B0 & B1 show the proposed
method achieved results comparable to the fully calibrated
method PS-FCN [5], with average MAEs of 2.71 and 4.09
on SPHERE and BUNNY, respectively.

Figure 5 shows that the performances of LCNet and
NENet increased with the number of input images. This
is expected, since more useful information can be used to
infer the lightings and normals with more input images.

Comparison with single-stage models To validate the
effectiveness of the proposed two-stage framework, we
compared our method with ﬁve different single-stage base-
line models. We ﬁrst retrained UPS-FCN [5], denoted as
UPS-FCNretrain, with images scaled by randomly generated
light intensities to allow it adapt to unknown intensities at
test time. We then increased the model capacity of UPS-

Table 2. Normal estimation results on the MERLTest dataset. The
numbers are the average MAE over samples rendered with 100
BRDFs (value the lower the better). NENet† was trained given the
lightings estimated by LCNetreg.

ID Model

# Param

SPHERE

BUNNY

B0

B1
B2

B3
B4

PS-FCN [5]

LCNet + NENet
LCNet + PS-FCN

LCNetreg + NENet†
LCNetreg + PS-FCN

B5 UPS-FCNdeep+mask
B6 UPS-FCNdeep
B7 UPS-FCNwide
B8 UPS-FCNest light
B9 UPS-FCNretrain

2.2 M

6.6 M
6.6 M

6.6 M
6.6 M

6.1 M
6.1 M
6.4 M
5.7 M
2.2 M

2.66

2.71
3.19

3.22
3.73

3.65
4.30
5.61
6.80
7.44

3.80

4.09
4.67

4.99
4.96

6.41
7.29
8.85
10.62
12.34

Figure 5. Results of SDPS-Net on SPHERE from MERLTest dataset
with varying input image numbers.

FCN by introducing a wider network (i.e., more channels in
the convolutional layers) and a deeper network (i.e., more
convolutional layers), denoted as UPS-FCNwide and UPS-
FCNdeep, respectively. We also trained a deeper network,
denoted as UPS-FCNdeep+mask, that takes both the images
and object mask as input. We last investigated the effect
of having additional lighting supervision by training a vari-
ant model, denoted as UPS-FCNest light, to simultaneously
estimate lighting and surface normal. Please refer to our
supplementary for detailed network architectures.

Experiments with IDs B5-B9 in Table 2 show that uti-
lizing a wider or deeper network, taking the object mask as
input, or incorporating additional lighting supervision can
improve the performance of single-stage model in some ex-
tent. However, experiments with IDs B1 & B5 show that
the proposed method signiﬁcantly outperformed the best-
performing single-stage model, especially on surfaces with
complex geometry such as BUNNY, when the input as well
as the number of parameters are comparable. This result in-
dicates that simply increasing the layer numbers or channel
numbers of the network, or incorporating additional light-
ing supervision cannot produce optimal results.

Comparison with the non-learning method [23] To
further verify the effectiveness of our method over non-
learning method, we compared SDPS-Net with the existing
uncalibrated method PF14 [23], which achieved state-of-
the-art results on the DiLiGenT benchmark [28], on differ-

8744

8163264100Number of Input Images0246Angular Error in DegreeDirection0.000.050.100.15Relative ErrorIntensity8163264100Number of Input Images0246Angular Error in DegreeNormalNear uniform Biased

Lambertian

Fabric

Plastic

Phenolic

(a) DiLiGenT

(b) APPLE

(c) GOURD1

(d) GOURD2 (e) Light Stage 0

(a) Light sources

(b) Examples for the four typical types of BRDFs

Figure 7. Lighting distributions of real testing datasets. The light
direction is visualized by mapping a 3-d vector [x, y, z] to a point
[x, y]. The color of the point indicates the light intensity (value is
divided by the highest intensity to normalize to [0, 1]).

1

(c) Light direction estimation results

5.2. Evaluation on Real Datasets

(d) Light intensity estimation results

(e) Surface normal estimation results

Figure 6. Comparison between SDPS-Net and PF14 [23] on
BUNNY rendered with four different types of BRDFs under a near
uniform lighting distribution and a biased lighting distribution.

ent lighting distributions and types of BRDFs. Speciﬁcally,
we considered one near uniform and one biased lighting dis-
tribution (see Fig. 6 (a)). We rendered BUNNY using four
typical types of BRDFs, including the Lambertian model
and three other types from MERL dataset [21], namely, Fab-
ric, Plastic, and Phenolic. They contained 15, 12, 9, and 12
different BRDFs, respectively. We reported the average re-
sults for each type (see Fig. 6 (b) for an example of each
type.).

Figures 6 (c)-(e) compare SDPS-Net and PF14 on light-
ing estimation and normal estimation. The following obser-
vations are made: 1) PF14 performed well on light direction
and normal estimation for diffuse or near diffuse surfaces
(i.e., Lambertian and Fabric), but will quickly degenerate
when dealing with non-Lambertian surfaces. Besides, it
cannot reliably estimate light intensities for all the BRDFs.
2) SDPS-Net performed well on different types of BRDFs,
especially on surfaces exhibit specular highlights. This re-
sult suggests that specular highlight is an important clue for
uncalibrated photometric stereo [7]. 3) The performance of
light direction and normal estimation of both methods will
have a trend of decreasing when dealing with biased light-
ing distribution, while the performance of intensity estima-
tion will slightly improve.

Real
testing datasets We evaluated our method on
three publicly available non-Lambertian photometric
stereo datasets, namely the DiLiGenT benchmark [28],
Gourd&Apple dataset [1] and Light Stage Data Gallery [8].
Figure 7 visualizes the lighting distribution of these datasets
(note that for Light Stage Data Gallery, we only used 133
images with the front side of the object under illumination).
Since Gourd&Apple dataset and Light Stage Data Gallery
only provide calibrated lightings (without ground-truth nor-
mal maps), we quantitatively evaluated our method on light-
ing estimation while qualitatively evaluated it on normal es-
timation.

Evaluation on DiLiGenT benchmark Table 3 (a)-(b)
show that LCNet outperformed the regression based base-
line LCNetreg and achieved highly accurate results on both
light direction and intensity estimation on DiLiGenT bench-
mark, with an average MAE of 4.92 and an average rela-
tive error of 0.068, respectively. Table 3 (c) compares the
normal estimation results of SDPS-Net with previous state-
of-the-art methods on DiLiGenT benchmark. SDPS-Net
achieved state-of-the-art results on almost all objects with
an average MAE of 9.51, except for the BEAR object. Al-
though UPS-FCNdeep+mask achieved reasonably good results
on objects with smooth surface and uniform material (e.g.,
BALL), it had difﬁculties in handling surfaces with com-
plex geometry and spatially-varying BRDFs (e.g., READ-
ING and HARVEST). The normal estimation network cou-
pled with LCNet (i.e., SDPS-Net) outperforms that with
LCNetreg (i.e., LCNetreg+NENet†) with a clear improve-
ment of 1.52 in average MAE, demonstrating the effec-
tiveness of the proposed classiﬁcation based LCNet. It is
interesting to see that, coupled with our LCNet, the cali-
brated methods L2 baseline [34] and IS18 [15] can already
achieve results comparable to the previous state-of-the-art
methods. This result indicates that our proposed LCNet
can be integrated with existing calibrated methods to help
handle cases where lighting conditions are unknown. Fig-
ures 8 (a)-(b) show the qualitative results of SDPS-Net on
DiLiGenT benchmark.

Evaluation on other real datasets Table 4 shows that
SDPS-Net can estimate accurate light directions and inten-
sities for the challenging Gourd&Apple dataset and Light

8745

LambertianFabricPlasticPhenolic0102030Mean Angular Error7.25.84.85.29.18.17.010.28.19.711.411.16.810.218.018.5SDPS-Net (near-uniform lighting)SDPS-Net (biased lighting)PF14 (near-uniform lighting)PF14 (biased lighting)LambertianFabricPlasticPhenolic0.00.51.01.5Relative Error0.120.080.080.100.110.080.060.090.690.210.350.500.320.130.310.48SDPS-Net (near-uniform lighting)SDPS-Net (biased lighting)PF14 (near-uniform lighting)PF14 (biased lighting)LambertianFabricPlasticPhenolic010203040Mean Angular Error7.24.93.64.39.86.75.06.99.210.612.933.210.112.118.739.6SDPS-Net (near-uniform lighting)SDPS-Net (biased lighting)PF14 (near-uniform lighting)PF14 (biased lighting)(a) READING

(b) HARVEST

(c) APPLE

(d) GOURD1 (e) HELMET SIDE

(f) PLANT

(g) KNEELING K.

t
c
e
j
b
O

l
a
m
r
o
N

.
t
s
E

r
o
r
r
E

.
r
i

D

r
o
r
r
E

.
t
n
I

0◦

45◦

0

0.5

Figure 8. Qualitative results of SDPS-Net on the real testing datasets. The ﬁrst to the fourth rows show the object, estimated normal map,
error distribution of light direction and light intensity estimation, respectively.

Table 3. Results of SDPS-Net on the DiLiGenT benchmark.

(a) Results on light direction estimation.

Table 4.
Gourd&Apple dataset and the Light Stage Data Gallery.

Lighting estimation results of SDPS-Net on the

Method

BALL CAT POT1 BEAR POT2 BUDDHA GOBLET READING COW HARVEST Avg.

LCNetreg 4.94 5.82 5.62
3.27 4.08 5.44
LCNet

7.19
3.47

4.82
2.87

3.90
4.34

12.89
10.36

7.90
4.50

4.19
4.52

9.50
6.32

6.68
4.92

(b) Results on light intensity estimation.

Method

BALL CAT POT1 BEAR POT2 BUDDHA GOBLET READING COW HARVEST Avg.

LCNetreg 0.032 0.051 0.048 0.167 0.074
0.039 0.095 0.058 0.061 0.048
LCNet

0.080
0.048

0.075
0.067

0.141
0.105

0.044
0.073

0.085
0.082

0.080
0.068

(c) Results on normal estimation. (Best viewed in PDF with zoom.)

Method

BALL

CAT

POT1 BEAR POT2 BUDDHA GOBLET READING COW HARVEST Avg.

AM07 [2]
SM10 [27]
WT13 [35]
LM13 [19]
PF14 [23]
LC18 [18]
UPS-FCN [5]

9.39

31.45 18.37 16.81 49.16
7.27
19.84 16.68 11.98 50.68
8.90
4.39
36.55
14.52
22.43 25.01 32.82 15.44 20.57
9.54
4.77
15.90
12.60 12.40 10.90 15.70
9.30
6.62
14.68 13.98 11.23 14.19

9.07

9.51

6.42

4.90
LCNet + L2 [34]
LCNet + IS18 [15]
6.37
UPS-FCNdeep+mask 3.96
LCNetreg+NENet†
3.87
2.77
SDPS-Net

11.12
9.72
15.64 10.58
12.16 11.13
8.04
8.97
8.06
8.14

9.35
8.48
7.19
15.98
6.89

14.70
12.24
11.11
8.36
7.50

32.81
15.54
13.19
25.76
14.92
19.00
15.87

14.86
13.94
13.06
9.42
8.97

46.54
48.79
20.57
29.16
29.93
18.30
20.72

18.29
18.54
18.07
11.49
11.91

53.65
26.93
58.96
48.16
24.18
22.30
23.26

20.11
23.78
20.46
16.99
14.90

54.72
22.73
19.75
22.53
19.53
15.00
11.91

25.08
29.31
11.84
8.83
8.48

61.70
73.86
55.51
34.45
29.21
28.00
27.79

29.17
25.69
27.22
18.38
17.43

37.25
29.59
23.93
27.63
16.66
16.30
16.02

15.73
16.46
13.62
11.03
9.51

Stage Data Gallery. Our method can also reliably recover
visually pleasing surface normal of these two datasets (see
Fig. 8 (c)-(g)), clearly demonstrating the practicality of the
proposed methods in real world applications. Please refer
to our supplementary for more results.

6. Conclusion and Discussion

In this paper, we have proposed a two-stage deep learn-
ing framework, called SDPS-Net, for uncalibrated photo-
metric stereo. The ﬁrst stage of our framework takes an
arbitrary number of images as input and estimates their cor-
responding light directions and intensities, while the second
stage predicts the normal map of the object based on the
lightings estimated in the ﬁrst stage and the input images.
By explicitly learning to estimate lighting conditions, our
two-stage framework can take advantage of the intermedi-

(a) Results on the Gourd&Apple dataset.

APPLE GOURD1 GOURD2 Avg.

Direction
Intensity

9.31
0.106

4.07
0.048

7.11
0.186

6.83
0.113

(b) Results on the Light Stage Data Gallery.

HELMET

SIDE

6.57
0.212

Direction
Intensity

PLANT

FIGHTING
KNIGHT

KNEELING

STANDING

KNIGHT

KNIGHT

HELMET
FRONT

16.06
0.170

15.95
0.214

19.84
0.199

11.60
0.286

11.62
0.248

Avg.

13.61
0.221

ate supervision to reduce the learning difﬁculty and improve
the ﬁnal normal estimation results. Besides, the ﬁrst stage
of our framework can be seamlessly integrated with existing
calibrated methods, which enables them to handle uncali-
brated photometric stereo. Experiments on both synthetic
and real datasets showed that our method signiﬁcantly out-
performed existing state-of-the-art uncalibrated photomet-
ric stereo methods.

Since our framework is trained only on surfaces with
uniform material, it may not perform well in dealing with
steep color changes caused by multi-material surfaces (see
Fig. 8 (b) for an example). In the future, we will investi-
gate better training datasets and network architectures for
handling surfaces with spatially-varying BRDFs.

Acknowledgments We gratefully acknowledge the sup-
port of NVIDIA Corporation with the donation of the Titan
X Pascal GPU. Kai Han is supported by EPSRC Programme
Grant Seebibyte EP/M013774/1. Boxin Shi is supported in
part by National Science Foundation of China under Grant
No. 61872012. Yasuyuki Matsushita is supported by the
New Energy and Industrial Technology Development Orga-
nization (NEDO).

8746

References

[1] Neil Alldrin, Todd Zickler, and David Kriegman. Photo-
metric stereo with non-parametric and spatially-varying re-
ﬂectance. In CVPR, 2008. 4, 7

[2] Neil G Alldrin, Satya P Mallick, and David J Kriegman. Re-
solving the generalized bas-relief ambiguity by entropy min-
imization. In CVPR, 2007. 1, 2, 8

[3] Peter N Belhumeur, David J Kriegman, and Alan L Yuille.

The bas-relief ambiguity. IJCV, 1999. 1, 2

[4] Manmohan Krishna Chandraker, Fredrik Kahl, and David J
Kriegman. Reﬂections on the generalized bas-relief ambigu-
ity. In CVPR, 2005. 2

[5] Guanying Chen, Kai Han, and Kwan-Yee K. Wong. PS-
FCN: A ﬂexible learning framework for photometric stereo.
In ECCV, 2018. 1, 2, 4, 5, 6, 8

[6] Donghyeon Cho, Yasuyuki Matsushita, Yu-Wing Tai, and
Inso Kweon. Photometric stereo under non-uniform light
intensities and exposures. In ECCV, 2016. 2

[7] Ondrej Drbohlav and M Chaniler. Can two specular pixels

calibrate photometric stereo? In ICCV, 2005. 2, 7

[8] Per Einarsson, Charles-Felix Chabert, Andrew Jones, Wan-
Chun Ma, Bruce Lamond, Tim Hawkins, Mark Bolas, Se-
bastian Sylwan, and Paul Debevec. Relighting human loco-
motion with ﬂowed reﬂectance ﬁelds. In EGSR, 2006. 7

[9] Carlos Hernandez Esteban, George Vogiatzis, and Roberto
Cipolla. Multiview photometric stereo. IEEE TPAMI, 2008.
2

[10] Marc-Andr´e Gardner, Kalyan Sunkavalli, Ersin Yumer, Xi-
aohui Shen, Emiliano Gambaretto, Christian Gagn´e, and
Jean-Franc¸ois Lalonde. Learning to predict indoor illumi-
nation from a single image. ACM TOG, 2017. 2

[11] Athinodoros S Georghiades. Incorporating the torrance and
sparrow model of reﬂectance in uncalibrated photometric
stereo. In ICCV, 2003. 2

[12] Hideki Hayakawa. Photometric stereo under a light source

with arbitrary motion. JOSA A, 1994. 2

[13] Aaron Hertzmann and Steven M Seitz. Example-based pho-
tometric stereo: Shape reconstruction with general, varying
brdfs. IEEE TPAMI, 2005. 2

[14] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap,
Emiliano Gambaretto, and Jean-Franc¸ois Lalonde. Deep out-
door illumination estimation. In CVPR, 2017. 2

[15] Satoshi Ikehata. CNN-PS: CNN-based photometric stereo

for general non-convex surfaces. In ECCV, 2018. 1, 2, 7, 8

[16] Wenzel Jakob. Mitsuba renderer, 2010. 4, 5
[17] Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 5

[18] Feng Lu, Xiaowu Chen, Imari Sato, and Yoichi Sato. Symps:
BRDF symmetry guided photometric stereo for shape and
light source estimation. IEEE TPAMI, 2018. 2, 8

[19] Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe,
and Yoichi Sato. Uncalibrated photometric stereo for un-
known isotropic reﬂectances. In CVPR, 2013. 1, 2, 8

[20] Feng Lu, Imari Sato, and Yoichi Sato. Uncalibrated photo-
metric stereo based on elevation angle recovery from BRDF
symmetry of isotropic materials. In CVPR, 2015. 1

[21] Wojciech Matusik, Hanspeter Pﬁster, Matt Brand, and
Leonard McMillan. A data-driven reﬂectance model. In SIG-
GRAPH, 2003. 4, 5, 7

[22] Takahiro Okabe, Imari Sato, and Yoichi Sato. Attached
shadow coding: Estimating surface normals from shadows
under unknown reﬂectance and lighting conditions. In ICCV,
2009. 2

[23] Thoma Papadhimitri and Paolo Favaro. A closed-form, con-
sistent and robust solution to uncalibrated photometric stereo
via local diffuse reﬂectance maxima. IJCV, 2014. 1, 2, 6, 7,
8

[24] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory
Chanan. PyTorch: Tensors and dynamic neural networks in
python with strong gpu acceleration, 2017. 5

[25] Hiroaki Santo, Masaki Samejima, Yusuke Sugano, Boxin
Shi, and Yasuyuki Matsushita. Deep photometric stereo net-
work. In ICCV Workshops, 2017. 1, 2

[26] Imari Sato, Takahiro Okabe, Qiong Yu, and Yoichi Sato.
Shape reconstruction based on similarity in radiance changes
under varying illumination. In ICCV, 2007. 2

[27] Boxin Shi, Yasuyuki Matsushita, Yichen Wei, Chao Xu, and
In CVPR,

Ping Tan. Self-calibrating photometric stereo.
2010. 1, 2, 8

[28] Boxin Shi, Zhipeng Mo, Zhe Wu, Dinglong Duan, Sai-Kit
Yeung, and Ping Tan. A benchmark dataset and evaluation
for non-Lambertian and uncalibrated photometric stereo.
IEEE TPAMI, 2018. 1, 2, 4, 6, 7

[29] William M Silver. Determining shape and reﬂectance us-
ing multiple images. PhD thesis, Massachusetts Institute of
Technology, 1980. 1

[30] Ping Tan, Satya P Mallick, Long Quan, David J Kriegman,
and Todd Zickler. Isotropy, reciprocity and the generalized
bas-relief ambiguity. In CVPR, 2007. 2

[31] Tatsunori Taniai and Takanori Maehara. Neural inverse ren-
dering for general reﬂectance photometric stereo. In ICML,
2018. 1, 2

[32] Henrique Weber, Donald Pr´evost,

and Jean-Franc¸ois
Lalonde. Learning to estimate indoor lighting from 3d ob-
jects. In 3DV, 2018. 2

[33] Olivia Wiles and Andrew Zisserman. SilNet: Single-and
In

multi-view reconstruction by learning from silhouettes.
BMVC, 2017. 4

[34] Robert J Woodham. Photometric method for determining
surface orientation from multiple images. Optical engineer-
ing, 1980. 1, 7, 8

[35] Zhe Wu and Ping Tan. Calibrating photometric stereo by
holistic reﬂectance symmetry analysis. In CVPR, 2013. 2, 8
[36] Alan L Yuille, Daniel Snow, Russell Epstein, and Peter N
Belhumeur. Determining generative models of objects un-
der varying illumination: Shape and albedo from multiple
images using SVD and integrability. IJCV, 1999. 2

[37] Hao Zhou, Jin Sun, Yaser Yacoob, and David W. Jacobs. La-
bel denoising adversarial network (LDAN) for inverse light-
ing of faces. In CVPR, 2018. 2

8747

