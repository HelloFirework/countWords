Interpreting CNNs via Decision Trees

Quanshi Zhang†, Yu Yang‡, Haotian Ma§, and Ying Nian Wu‡

†Shanghai Jiao Tong University,

‡University of California, Los Angeles,

§South China University of Technology

Abstract

(a) Input

(b) grad-CAM

(c) Visualization of filters

This paper1 aims to quantitatively explain the rationales
of each prediction that is made by a pre-trained convolu-
tional neural network (CNN). We propose to learn a deci-
sion tree, which clariﬁes the speciﬁc reason for each predic-
tion made by the CNN at the semantic level. I.e. the decision
tree decomposes feature representations in high conv-layers
of the CNN into elementary concepts of object parts. In this
way, the decision tree tells people which object parts ac-
tivate which ﬁlters for the prediction and how much each
object part contributes to the prediction score. Such seman-
tic and quantitative explanations for CNN predictions have
speciﬁc values beyond the traditional pixel-level analysis of
CNNs. More speciﬁcally, our method mines all potential de-
cision modes of the CNN, where each mode represents a typ-
ical case of how the CNN uses object parts for prediction.
The decision tree organizes all potential decision modes in
a coarse-to-ﬁne manner to explain CNN predictions at dif-
ferent ﬁne-grained levels. Experiments have demonstrated
the effectiveness of the proposed method.

1. Introduction

Convolutional neural networks (CNNs) [20, 18, 14] have
achieved superior performance in various tasks. However,
besides the discrimination power, model interpretability is
still a signiﬁcant challenge for neural networks. Many s-
tudies have been proposed to visualize or analyze feature
representations hidden inside a CNN, in order to open the
black box of neural networks.

Motivation & objective: In the scope of network inter-
pretability, state-of-the-art algorithms are still far from the
ultimate goal of explaining why a CNN learns knowledge as
it is. Although some theories, such as the information bot-
tleneck [34], analyzed statistical characteristics of a neural

1Quanshi Zhang is the corresponding author with the John Hopcroft
Center and the MoE Key Lab of Artiﬁcial Intelligence, AI Institute, at the
Shanghai Jiao Tong University, China. Yu Yang and Ying Nian Wu are
with the University of California, Los Angeles, USA. Haotian Ma is with
the South China University of Technology, China.

y=0.87

...

Explanatory 

tree

Conv5-2

Conv3-3

T
a

i
l

Distribution of 
contributions of 
different filters

Feature maps 
of a nape filter

s
n
o
i
t
a
n
a
p
x
e
 
r
u
O

l

 
)
d
(

Figure 1. Different types of explanations for CNNs. We compare
(d) our task of quantitatively and semantically explaining CNN
predictions with previous studies of interpreting CNNs, such as
(b) the grad-CAM [26] and (c) CNN visualization [23]. Given an
input image (a), we infer a parse tree (green lines) within the deci-
sion tree to project neural activations onto clear concepts of object
parts. Our method quantitatively explains which ﬁlters/parts (in
the small/big round) are used for the prediction and how much
they contribute to the prediction. We visualize numerical contri-
butions from randomly selected 10% ﬁlters for clarity.

network, it is still a challenge to explain why a CNN en-
codes frontal-leg features, rather than rear-leg features, for
classiﬁcation during the end-to-end learning process.

Therefore, in this study, we limit our discussion to the
issue of explaining what knowledge a CNN learns. In this
direction, our research focuses on the following two new
perspectives of interpreting CNNs:
• How to explain features of a middle layer in a CNN at the
semantic level. I.e. we aim to transform chaotic features of
ﬁlters inside a CNN into semantically meaningful concepts,
such as object parts, so as to help people to understand the
knowledge in the CNN.
• How to quantitatively analyze the rationale of each C-
NN prediction. We need to ﬁgure out which ﬁlters/parts
pass their information through the CNN and contribute to
the prediction output. We also report the numerical contri-
bution of each ﬁlter (or object part) to the output score.

As shown in Fig. 1, the above two perspectives are cru-
cial in real applications and have essential differences from
traditional pixel-level visualization and diagnosis of CNN
features [37, 23, 10, 24, 22]. Our semantic and quantitative

16261

Mode of standing birds 
of various specieses

Mode of a species 
with a specific pose

y=0.9

 

C
F

s
r
e
y
a

l

...

Most generic 

rationales
(our target)

Most specific 

rationales

 

l

d
e
g
n
a
n
e
s
D

t

i

 

p
o

t
 

e
h

t
 

n

i
 
s
r
e

t
l
i
f

r
e
y
a
l
-
v
n
o
c

.
.
.

A head filter

contribute 2.32%

A feet filter

contribute 0.72%

A torso filter

contribute 1.21%

m
o
d
e

 

A
d
e
c
s
o
n

i

i

 

Figure 2. Decision tree that encodes all potential decision modes
of the CNN in a coarse-to-ﬁne manner. We learn a CNN for object
classiﬁcation with disentangled representations in the top conv-
layer, where each ﬁlter represents an object part. Given an input
image, we infer a parse tree (green lines) from the decision tree
to semantically and quantitatively explain which object parts (or
ﬁlters) are used for the prediction and how much an object part
(or ﬁlter) contributes to the prediction. We are more interested in
high-layer decision modes that summarize low-layer modes into
compact explanations of CNN predictions.

explanations for CNNs have potential values beyond pixel-
level visualization/analysis of CNNs. Semantic and quan-
titative explanations can help people better understand and
trust CNN predictions. E.g. in critical applications, such as
the recommendation for a surgery plan, people are usually
not simply satisﬁed by the plan itself, but expect a quantita-
tive explanation for the plan.

However, bridging the gap between a CNN’s middle-
layer features and semantic explanations has not been well
explored yet. We will introduce our task, clarify its chal-
lenges, and deﬁne relevant concepts from the following two
perspectives.
• Bridging middle-layer features with semantic concepts:
Given an input image, the ﬁrst issue is to learn more inter-
pretable feature representations inside a CNN and associate
each neural activation inside a CNN with a semantic con-
cept. This presents signiﬁcant challenges for state-of-the-
art algorithms.

Firstly, we need to force feature representations in mid-
dle conv-layers to be well disentangled during the learning
process. According to [2, 38]2, a ﬁlter in traditional CNNs
usually represents a mixture of parts and textures. Learning
semantically meaningful ﬁlters is difﬁcult but is the foun-
dation of semantic-level explanations. In this research, we
learn a CNN with disentangled ﬁlters in high conv-layers.
Each ﬁlter needs to be consistently activated by the same
object region over different input images. We do not use
any annotations of parts or textures to supervise the disen-
tanglement of ﬁlter features.

2Zhang et al. [40] summarized the six types of semantics deﬁned in [2]

as parts and textures.

Secondly, we also need to associate each disentangled
ﬁlter with an explicit semantic meaning (i.e. an object part
in this study). This association enables linguistic descrip-
tions of middle-layer knowledge, for example, how many
parts are memorized in the CNN and how the parts are or-
ganized.
• Bridging middle-layer features with ﬁnal CNN predic-
tions: When we have assigned middle-layer features with
speciﬁc part concepts, the next issue is to quantitatively ex-
plain how the CNN uses these middle-layer features to com-
pute prediction scores. In other words, given an input im-
age, we hope to clarify the speciﬁc rationale of the CNN
prediction.

Here, we deﬁne the rationale of a CNN prediction as
the set of object parts (or ﬁlters) that are activated and con-
tribute to the prediction. Given different input images, the
CNN uses different object parts to activate different sets of
ﬁlters to compute prediction scores, thereby having differ-
ent rationales. Let us take the bird classiﬁcation as an exam-
ple. The CNN may use several ﬁlters activated by the head
appearances as rationales to classify a standing bird, and the
CNN may take ﬁlters for wings to distinguish a ﬂying bird.
Given each input image, our task is to clarify ﬁlters of
which object parts are activated and to quantitatively mea-
sure the contribution of each object part to the prediction.
The concept of the contribution was also called the “impor-
tance” in [24, 22] and was termed “attribution” in [16]. As
shown in Fig. 2, we describe the contribution as “a head ﬁl-
ter contributes 2.32%, and a feet ﬁlter contributes 0.72%.”

Task: As shown in Fig. 1, given a pre-trained CNN, we
propose a method to construct a decision tree to explain CN-
N predictions semantically and quantitatively. We summa-
rize rationales of CNN predictions on all images into vari-
ous decision modes. Each tree node represents a decision
mode. Each decision mode describes common rationales
of predictions that are shared by multiple images. I.e. for
these images, the CNN usually activates similar ﬁlters (ob-
ject parts), and each part makes a similar contribution to the
prediction.

The decision tree hierarchically represents all potential
decision modes of a CNN in a coarse-to-ﬁne manner. Nodes
near to the tree root node mainly represent most common
decision modes (prediction rationales) shared by many im-
ages. Nodes near leaves correspond to ﬁne-grained modes
of minority images. In particular, each leaf node encodes
the speciﬁc decision mode of a certain image.

In order to build the decision tree, we learn ﬁlters to rep-
resent object parts (we do not label any parts or textures as
additional supervision3). Then, we assign each ﬁlter with
a speciﬁc part name. Finally, we mine decision modes to

3Part annotations are not used to learn the CNN and the decision tree.
Given the learned CNN, we label object parts for the ﬁlters to compute
part-level contributions in Equation (11).

6262

explain how the CNN use parts/ﬁlters for prediction, and
construct a decision tree.

Inference: When the CNN makes a prediction for an
input image, the decision tree determines a parse tree (see
green lines in Fig. 2) to encode a series of explanations.
Each node (decision mode) in the parse tree quantitatively
explains the prediction at a different abstraction level, i.e.
clarifying how much each object part/ﬁlter contributes to
the prediction score.

Compared to ﬁne-grained modes in leave nodes, we are
more interested in generic decision modes in high-level
nodes. Generic decision modes usually select signiﬁcan-
t object parts (ﬁlters) as the rationale of CNN predictions
and ignore insigniﬁcant ones. Thus, generic decision modes
reﬂect compact rationales for CNN predictions.

Contributions: In this paper, we aim to use semantic vi-
sual concepts to explain CNN predictions quantitatively and
semantically. We propose to learn the decision tree without
strong supervision for explanations. Our method is a gener-
ic approach and has been successfully applied to various
benchmark CNNs. Experiments have demonstrated the ef-
fectiveness of our method.

2. Related work

In this section, we limit our discussion to the literature
of opening the black box of CNN representations. [2, 21, 9,
4] discussed the deﬁnition of interpretability from different
perspectives concerning different tasks. Zhang et al. [43]
made a survey for the interpretability of deep visual models.
CNN visualization: Visualization of ﬁlters in a CNN is
the most direct way of exploring the pattern hidden inside a
neural unit. Gradient-based visualization [37, 23] estimates
the input image that maximizes the activation score of a
neural unit. Up-convolutional nets [10] invert feature maps
of conv-layers into images. Unlike gradient-based methods,
up-convolutional nets cannot mathematically ensure that the
visualization result reﬂects actual neural representations.

Zhou et al. [44] proposed a method to accurately com-
pute the image-resolution receptive ﬁeld of neural activa-
tions in a feature map. The estimated receptive ﬁeld of a
neural activation is smaller than the theoretical receptive
ﬁeld based on the ﬁlter size. The accurate estimation of the
receptive ﬁeld is crucial to understand a ﬁlter’s representa-
tions. Bau et al. [2] further deﬁned six types of semantics
for CNNs, i.e. objects, parts, scenes, textures, materials,
and colors. Zhang et al. [38] summarized the six types of
semantics into “parts” and “textures.” Nevertheless, each
ﬁlter in a CNN represents a mixture of semantics. [45] ex-
plained semantic reasons for visual recognition.

Network diagnosis: Beyond visualization, some meth-
ods diagnose a pre-trained CNN to obtain insight under-
standing of CNN representations.

Fong and Vedaldi [11] analyzed how multiple ﬁlters

jointly represented a speciﬁc semantic concept. Yosinski
et al. [36] evaluated the transferability of ﬁlters in interme-
diate conv-layers. Aubry et al. [1] computed feature dis-
tributions of different categories in the CNN feature space.
Selvaraju et al. [26] and Fong et al. [12] propagated gradi-
ents of feature maps w.r.t. the CNN loss back to the image,
in order to estimate image regions that directly contribute
the network output. The LIME [24] and SHAP [22] ex-
tracted image regions that were used by a CNN to predict
a label. Zhang et al. [41] used an explainer network to in-
terpret object-part representations in intermediate layers of
CNNs.

Network-attack methods [17, 30] diagnosed network
representations by computing adversarial samples for a C-
NN. In particular, inﬂuence functions [17] were proposed
to compute adversarial samples, provide plausible ways to
create training samples to attack the learning of CNNs, ﬁx
the training set, and further debug representations of a CN-
N. Lakkaraju et al. [19] discovered knowledge blind spot-
s (unknown patterns) of a pre-trained CNN in a weakly-
supervised manner. The study of [39] examined represen-
tations of conv-layers and automatically discover potential,
biased representations of a CNN due to the dataset bias.

CNN semanticization: Compared to the diagnosis of
CNN representations, some studies aim to learn more mean-
ingful CNN representations. Some studies extracted neural
units with clear semantics from CNNs for different applica-
tions. Given feature maps of conv-layers, Zhou et al. [44]
extracted scene semantics. Simon et al. mined objects from
feature maps of conv-layers [27], and learned object part-
s [28]. The capsule net [25] used a dynamic routing mech-
anism to parse the entire object into a parsing tree of cap-
sules. Each output dimension of a capsule in the network
may encode a speciﬁc meaning. Zhang et al. [40] proposed
to learn CNNs with disentangled intermediate-layer repre-
sentations. The infoGAN [6] and β-VAE [15] learned in-
terpretable input codes for generative models. Zhang et
al. [42] learned functionally interpretable, modular struc-
tures for neural networks via network transplanting.

Decision trees for neural networks: Distilling knowl-
edge from neural networks into tree structures is an emerg-
ing direction [13, 31, 5], but the trees did not explain the
network knowledge at a human-interpretable semantic lev-
el. Wu et al. [35] learned a decision tree via knowledge
distillation to represent the output feature space of an RN-
N, in order to regularize the RNN for better representation-
s. Vaughan et al. [32] distilled knowledge into an additive
model for explanation.

Despite the use of tree structures, there are two main d-
ifferences between the above two studies and our research.
Firstly, we focus on using a tree to explain each prediction
made by a pre-trained CNN semantically. In contrast, deci-
sion trees in the above studies are mainly learned for clas-

6263

Ordinary filters

Disentangled filters

mation between the distribution of feature maps and that of
part locations.

Lossf =Xxf ∈Xf
= − Xµ∈P

Lossf (xf ) = −M I(Xf ; P)

p(µ)Xxf ∈Xf

p(xf |µ) log

p(xf |µ)
p(xf )

(1)

Filter 1

Filter 2

Filter 3

Figure 3. Comparisons between ordinary CNN feature maps and
disentangled feature maps that are used in this study. We visualize
image regions corresponding to each feature map based on [44].

siﬁcation and cannot provide semantic-level explanation-
s. Secondly, we summarize decision modes from gradients
w.r.t. neural activations of object parts as rationales to ex-
plain CNN prediction. Compared to the above “distillation-
based” methods, our “gradient-based” decision tree reﬂects
CNN predictions more directly and strictly.

3. Image-speciﬁc rationale of a CNN prediction

In this section, we design a method to simplify the com-
plex feature processing inside a CNN into a linear form
(i.e. Equation (3)), as the speciﬁc rationale of the predic-
tion w.r.t. the input image. This linear equation clariﬁes (i)
which object parts activate which ﬁlters in the CNN and (ii)
how much these parts/ﬁlters contribute to the ﬁnal predic-
tion score.

In order to obtain semantic-level rationale of a CNN pre-
diction, we need (i) ﬁrst to ensure that the CNN’s middle-
layer features are semantically meaningful, and (ii) then to
extract explicit contributions of middle-layer features to the
prediction score.

In this study, we learn the CNN for object classiﬁcation.
Theoretically, we can interpret CNNs oriented to different
tasks. Nevertheless, in this paper, we limit our attention to
CNNs for classiﬁcation, in order to simplify the story.

3.1. Learning disentangled ﬁlters

The basic idea is to revise a benchmark CNN, in order
to make each ﬁlter in the top conv-layer represent a speciﬁc
object part. We expect the ﬁlter to be automatically con-
verged to the representation of a part, instead of using addi-
tional part annotations to supervise the learning process.

We apply the ﬁlter loss [40] to each ﬁlter in the top conv-
layer to push the ﬁlter towards the representation of an ob-
ject part. As shown in Fig. 3, the ﬁlter is learned to be acti-
vated by the same object part given different input images.
Theoretically, our method also supports other techniques of
mining interpretable features in middle layers [38, 27]. N-
evertheless, the ﬁlter loss usually ensures more meaningful
features than the other approaches.

Filter loss: Let xf ∈ RL×L denote the feature map of
a ﬁlter f . Without part annotations, the ﬁlter loss forces xf
to be exclusively activated by a speciﬁc part of a category.
We can summarize the ﬁlter loss as the minus mutual infor-

where M I(·) indicates the mutual information. Xf denotes
a set of feature maps of f extracted from different input
images. P = {µ|µ = [h, w], 1 ≤ h, w ≤ L} ∪ {∅} is referred
to as a set of all part-location candidates. Each location µ =
[h, w] corresponds to an activation unit in xf . Besides, ∅ ∈
P denotes the case that the target part does not appear in the
input image. In this case, all units in xf are expected to keep
inactivated. The joint probability p(xf , µ) to describe the
compatibility between xf and µ (please see [40] for details).
The ﬁlter loss ensures that given an input image, xf
should match only one of all L2+1 location candidates. It is
assumed that repetitive shapes on various regions are more
likely to describe low-level textures than high-level parts. If
the part appears, xf should have a single activation peak at
the part location; otherwise, xf should keep inactivated.

3.2. Quantitative rationales of CNN predictions

As analyzed in [2], ﬁlters in high conv-layers are more
prone to represent object parts, while those in low conv-
layers usually describe textures. Therefore, we choose ﬁl-
ters in the top conv-layer to represent object parts. Con-
sequently, we quantitatively analyze how fully-connected
(FC) layers use object-part features from the top conv-layer
to make ﬁnal predictions, as the rationale.

Given an input image I, let x ∈ RL×L×D denote the
feature map of the top conv-layer after a ReLU operation,
where L denotes the scale of the feature map, and D is the
ﬁlter number. Let y denote the scalar classiﬁcation score of
a certain category before the softmax operation (when the
CNN is learned for multiple categories, we may learn a spe-
ciﬁc decision tree to explain the output of each category).
Our task is to use x to represent the rationale of y.

As discussed in [22, 24], we can use a piecewise linear
representation to represent the function of cascaded FC lay-
ers and ReLU layers, as follows.
y = ffc-n(frelu(· · · ffc-1(x))) = X

g(h,w,d) · x(h,w,d) + b (2)

h,w,d

where x(h,w,d) ∈ R denotes the element at the location
(h, w) of the d-th channel; g(h,w,d) is a weight that describes
the importance of x(h,w,d) for the prediction on I. Theoreti-
cally, we can compute g = ∂y

∂x and b = y − g ⊗ x.

We use weights g to denote the speciﬁc rationale of the
prediction for the input image. g(h,w,d)x(h,w,d) measures
x(h,w,d)’s quantitative contribution to the prediction.

Different input images correspond to different weights
g, i.e. different rationales of their CNN predictions. It is

6264

0Q P
...

v v’

.
.
.

1P

...u

v v’

u’

2P

...

u

v’

u

’

v

Filter 1:
Filter 2:
…
Filter D-1:
Filter D:

head pattern
torso pattern

feet pattern
tail pattern

Figure 4. Learning a decision tree. Green lines in P3 indicate a
parse tree to explain the rationale of the prediction on an image.

because different images have various signal passing routes
through ReLU layers. Given an input images I, the CNN
uses certain weight values that are speciﬁc to I.

Because each interpretable ﬁlter only has a single activa-
tion peak [40], we can further compute vectors x, g ∈ RD
∂x ∈ RL×L×D to sim-
as an approximation to the tensors x, ∂y
sd Ph,w x(h,w,d) and
plify the computation. We get x(d) = 1
g(d) = sd
∂x(h,w,d) , where x(d) denotes the d-th ele-
ment of x. sd = EI Eh,w x(h,w,d) is used to normalize the
activation magnitude of the d-th ﬁlter.

L2 Ph,w

∂y

In this way, we can consider x and g to represent predic-
tion rationales4, i.e. using which ﬁlters/parts for prediction.

y ≈ gT x + b

(3)

Different dimensions of the vector x measure the scalar
signal strength of different object parts, since a ﬁlter poten-
tially represents a certain object part. g corresponds to the
selection of object parts for the CNN prediction.

4. Learning a decision tree

We learn a decision tree to interpret the classiﬁcation of
In the following two subsections, we ﬁrst
each category.
deﬁne basic concepts in a decision tree and then introduce
the learning algorithm.

4.1. Decision tree

Let us focus on the decision tree for a certain category.
We consider images of this category as positive images and
consider other images as negative images. Ω+ denotes im-
age indexes of the target category, i.e. positive images, and
Ω = Ω+ ∪ Ω− represents all training images. For an image
Ii (i ∈ Ω), yi denotes the classiﬁcation score of the target
category before the softmax layer.

As shown in Fig. 2, each node v in the decision tree en-
codes a decision mode that is hidden inside FC layers of the
CNN. A decision mode represents a common rationale of
the prediction shared by a group of positive training images
Ωv ⊂ Ω+. The decision tree organizes the hierarchy of deci-
sion modes in a coarse-to-ﬁne manner from the root node to

4Without loss of generality, we normalize g to a unit vector for more

convincing results: y ← y/kgk, g ← g/kgk, and b ← b/kgk.

Algorithm 1 Learning a decision tree for a category

Input: 1. A CNN with disentangled ﬁlters, 2. training
images Ω = Ω+ ∪ Ω−.
Output: A decision tree.
Initialize a tree Q = P0 and set t = 0
for each image Ii, i ∈ Ω+ do

Initialize a child of the root of the initial tree Q by
setting g = gi based on Equation (3) and α = 1.

end for
for t = t + 1 until ∆ log E ≤ 0 do

1. Choose (v, v′) in the second tree layer of Pt−1 that
maximize ∆ log E based on Equation (8)
2. Merge (v, v′) to generate a new node u based on
Equations (5) and (6), and obtain the tree Pt.

end for
Assign ﬁlters with semantic object parts to obtain A.

leaf nodes. Children nodes v′ ∈ Child(v) divides the paren-
t v’s decision mode into ﬁne-grained modes. Fine-grained
modes are shared by sub-groups of images.

Just like the rationale deﬁned in Equation (3), the deci-
sion mode in node v is parameterized with w and b, and
the mode explains predictions on a certain set of images Ωv.
For each image Ii, i ∈ Ωv, the decision mode is given as

hv(xi) = wT xi + b,
g Pi∈Ωv
max

cosine(gi, g),

w = α ◦ g
s.t. gT g = 1

minα,b

1

kΩv kPi∈Ωv

(wT xi + b − yi)2 + λkαk1

(4)

(5)

(6)

where w is referred to as the rationale of the decision mode.
g is a unit vector (kgk2 = 1) that reﬂects common rationales
that are shared by all images in Ωv. α ∈ {0, 1}D is given
as a binary selection of ﬁlters in the decision mode. ◦ de-
note element-wise multiplications. We compute sparse α to
obtain sparse explanations for the decision mode5.

In particular, when v is a leaf node, the decision mode is
formulated as the rationale of a speciﬁc image Ii. I.e. α =
[1, 1, . . . , 1]T and w = α ◦ gi = gi, where gi is computed in
Equation (3).

4.2. Learning decision trees

Just like hierarchical clustering, the basic idea of learn-
ing a decision tree is to summarize common generic deci-
sion modes from speciﬁc decision modes of different im-
ages. Algorithm 1 shows the pseudo-code of the learning
process. At the beginning, we initialize the decision mode
gi of each positive image Ii as a leaf node by setting g = gi
and α = 1. Thus, we build an initial tree Q as shown in
Fig. 4, in which the root node takes decision modes of all
positive images as children. Then, in each step, we select
and merge two nodes v, v′ ∈ V in the second tree layer (i.e.
children of the root node) to obtain a new node u, where V
denotes the children set of the root. u becomes a new child

6265

of the root node, and v and v′ are re-assigned as u’s chil-
dren. The image set of u is deﬁned as Ωu = Ωv ∪ Ωv′ and
we learn α, b, g for u based on Equations (5) and (6).

In this way, we gradually revise the initial tree P0 = Q

towards the ﬁnal tree after T merging operations as

Q = P0 → P1 → P2 → · · · → PT = ˆP

We formulate the objective for learning as follows.

max

P

E,

E = Qi∈Ω+ P (xi)
Qi∈Ω+ Q(xi)
}
|

Discrimination power

{z

· e−βkV k
| {z }

Sparsity of

decision modes

(7)

(8)

where P (xi) denotes the likelihood of xi being positive
that is estimated by the tree P . β is a scaling parameter5.
This objective penalizes the decrease of the discriminative
power and forces the system to summarize a few generic
decision modes for explanation. We compute the likelihood
of xi being positive as

P (xi) = eγˆh(xi)/Xj∈Ω

eγˆh(xj )

(9)

where ˆh(xi) = hˆv(xi) denotes the prediction on xi based on
best child ˆv ∈ V in the second tree layer. γ is a constant
scaling parameter5.

In the t-th step, we merge two nodes v, v′ ∈ V in the
second tree layer of Pt−1 to get a new node u, thereby
obtaining a new tree Pt. We can easily compute ∆ log E
w.r.t. each pair of (v, v′) based on Equation (8). Thus, we
learn the decision tree via a greedy strategy. In each step,
we select and merge the nodes v, v′ ∈ V that maximize
kΩv k+kΩv′ k . We normalize ∆ log E for reasonable cluster-
ing performance. Because each node merger operation only
affects ˆh(xi) values of a few examples in Ωv ∪ Ωv′ , we can
quickly estimate ∆ log E for each pair of nodes (v, v′).

∆ log E

4.3. Interpreting CNNs

Given a testing image Ii, the CNN makes a prediction
yi. The decision tree estimates quantitative decision modes
of the prediction at different ﬁne-grained levels. During the
inference procedure, we can infer a parse tree, which starts
from the root node, in a top-down manner. Green lines in
Fig. 4 show a parse tree. When we select the decision mode
in the node u as the rationale, we can further select its child
ˆv that maximizes the compatibility with the most speciﬁc
rationale gi as a more ﬁne-grained mode:

ˆv = argmaxv∈Child(u)cosine(gi, wv)

(10)

where we add the subscript v to differentiate the parameter
of v from parameters of other nodes.

A node v in the parse tree provides the rationale of the
prediction on image Ii at a certain ﬁne-grained level. We
compute the vector ρi and ˆ̺i to evaluate the contribution of
different ﬁlters and that of different object parts.

ρi = wv ◦ xi,

ˆ̺i = Aρi

(11)

5Please see the experiment section for settings of β, γ, and λ.

i

where the d-th element of ρi ∈ RD, ρ(d)
, denotes the contri-
bution to the CNN prediction that is made by the d-th ﬁlter.
If ρ(d)
i > 0, then the d-th object part makes a positive con-
tribution. If ρ(d)
i < 0, then the d-th ﬁlter makes a negative
contribution. Based on visualization results in Figs. 3 and
6, we label a matrix A ∈ {0, 1}M ×D to assign each ﬁlter in
the top conv-layer with a speciﬁc object part, where M is
the part number. Each ﬁlter is assigned to a certain part, and
the annotation cost is O(M ). Similarly, the m-th element of
ˆ̺i ∈ RM , ˆ̺(m)

i measures the contribution of the m-th part.

5. Experiments

Implementation details: We learned four types of dis-
entangled CNNs based on structures of four benchmark C-
NNs, including the AlexNet [18], the VGG-M network [29],
the VGG-S network [29], the VGG-16 network [29]. Note
that as discussed in [40], the ﬁlter loss in the explainer is
not compatible with skip connections in residual network-
s [14]. We followed the technique of [40] to modify an or-
dinary CNN to a disentangled CNN, which changed the top
conv-layer of the CNN to a disentangled conv-layer and fur-
ther added a disentangled conv-layer on the top conv-layer.
We used feature maps of the new top conv-layer as the in-
put of our decision tree. We loaded parameters of all old
conv-layers directly from the CNN that was pre-trained us-
ing images in the ImageNet ILSVRC 2012 dataset [8] with a
loss for 1000-category classiﬁcation. We initialized param-
eters of the new top conv-layer and all FC layers. Inspired
by previous studies of [40], we can weaken the problem of
multi-category classiﬁcation as a number of single-category
classiﬁcation to simplify the evaluation of interpretability.
Thus, we ﬁne-tuned the CNN for binary classiﬁcation of a
single category from random images with the log logistic
loss using three benchmark datasets. We simply set param-
i∈Ω+ [yi], and λ = 10−6pkΩvk in all
eters as β = 1, γ = 1/E
experiments for fair comparisons.

Datasets: Because the quantitative explanation of CNN
predictions requires us to assign each ﬁlter in the top conv-
layer with a speciﬁc object part, we used three benchmark
datasets with ground-truth art annotations to evaluate our
method. The selected datasets include the PASCAL-Part
Dataset [7], the CUB200-2011 dataset [33], and the ILSVR-
C 2013 DET Animal-Part dataset [38]. Just like in most
part-localization studies [7, 38], we used animal categories,
which prevalently contain non-rigid shape deformation, for
evaluation.
I.e. we selected six animal categories—bird,
cat, cow, dog, horse, and sheep—from the PASCAL Part
Dataset. The CUB200-2011 dataset contains 11.8K images
of 200 bird species. Like in [3, 28], we ignored species la-
bels and regarded all these images as a single bird category.
The ILSVRC 2013 DET Animal-Part dataset [38] consist-
s of 30 animal categories among all the 200 categories for
object detection in the ILSVRC 2013 DET dataset [8].

6266

1

2

3

1

2

3

1

2

3

1

2

3

1

2

3

1

2

3

1

2

3

1

2

3

Figure 5. Visualization of decision modes corresponding to nodes in the 2nd tree layer. We show typical images of each decision mode.

Input

Activation 
distribution

Contribution 
distribution

Input

Activation 
distribution

Contribution 
distribution

Image receptive fields of a filter on different images

Filter for 
bird nape

Filter for 
bird breast

Filter for 
cat torso

Filter for 
cat head

Figure 6. Object-part contributions for CNN prediction. Pie charts show contribution proportions of different parts, which are estimated
using nodes in the second tree layer. Heat maps indicate spatial distributions of neural activations in the top conv-layer (note that the
heat maps do not represent distributions of “contributions,” because neural activations are not weighted by gi). Right ﬁgures show image
receptive ﬁelds of different ﬁlters estimated by [44]. Based on these receptive ﬁlters, we assign the ﬁlters with different object parts to
compute the distribution of object-part contributions.

i

i

|/PM

m′=1 |ˆ̺(m′)

Analysis of object parts for prediction: We analyzed
the contribution of different object parts in the CNN pre-
diction, when we assigned each ﬁlter with a speciﬁc ob-
ject part. The vector ˆ̺i in Equation (11) speciﬁes the
contribution of different object parts in the prediction of
yi. For the m-th object part, we computed contrim =
|ˆ̺(m)
| as the ratio of the part’s contribution.
More speciﬁcally, for CNNs based on the ILSVRC 2013
DET Animal-Part dataset, we manually labeled the object
part for each ﬁlter in the top conv-layer. For CNNs based
on the Pascal VOC Part dataset [7], the study of [40] merged
tens of small parts into several major landmark parts for the
six animal categories. Given a CNN for a certain catego-
ry, we used [44] to estimate regions in different images that
corresponded to each ﬁlter’s neural activations, namely the
image receptive ﬁeld of the ﬁlter (please see Figs. 6 and
3). For each ﬁlter, we selected a part from all major land-
mark parts, which was closest to the ﬁlter’s image receptive
ﬁeld through all positive images. For the CNN based on the
CUB200-2011 dataset, we used ground-truth positions of
the breast, forehead, nape, tail of birds as major landmark
parts. Similarly, we assigned each ﬁlter in the top conv-
layer with the nearest landmark part.

Evaluation metrics: The evaluation has two aspects.
Firstly, we use two metrics to evaluate the accuracy of the
estimated rationale of a prediction. The ﬁrst metric eval-
uates errors of object-part contributions to the CNN pre-
diction that were estimated using nodes in the second tree
layer. Given an input image I, ˆ̺i in Equation (11) denotes
the quantitative contribution of the i-th part. Accordingly,
∗
i = y − ˆyi is referred to as the ground-truth contribution of
ˆ̺

∗

the part, where y denotes the original CNN prediction on I;
ˆyi is the output when we removed neural activations from
feature maps (ﬁlters) corresponding to the i-th part. In this
way, we used the deviation EI ∈I[ˆ̺i − ˆ̺
i ]/EI ∈I[y] to de-
note the error of the i-th part contributions. Another metric,
namely the ﬁtness of contribution distributions, compares
the ground-truth contribution distribution over different ﬁl-
ters in the top conv-layer with the estimated contribution of
these ﬁlters during the prediction process. When the deci-
sion tree uses node ˆv to explain the prediction for Ii, the
vector ρi in Equation (11) denotes the estimated contribu-
tion distribution of different ﬁlters. ti = gi ◦ xi correspond-
s to the ground-truth contribution distribution. We report-
ed the interaction-of-the-union value between ρi and ti to
measure the ﬁtness of the ground-truth and the estimated
ﬁlter contribution distributions. I.e. we computed the ﬁt-

ness as E

i∈Ω+ [

(d)
min( ˆρ
i
(d)
max( ˆρ
i

,|t

(d)
i
(d)
,|t
i

|)

|)

], where t(d)

i

denotes the d-th

element of ti and ˆρ(d)
non-negative values of ˆρ(d)
i
and ti may have negative elements.

i = max{ρ(d)

and |t(d)

i

i sign(t(d)

i

), 0}. We used
|, because vectors ρi

Secondly, in addition to the accuracy of the rationale, we
also measured the information loss of using the decision tree
to represent a CNN, as a supplementary evaluation. A met-
ric is the classiﬁcation accuracy. Because ˆh(xi) denotes
the prediction of yi based on the best child in the second
tree layer, we regarded ˆh(·) as the output of the tree and we
evaluated the discrimination power of ˆh(·). We used val-
ues of ˆh(xi) for classiﬁcation and compared its classiﬁca-
tion accuracy with the accuracy of the CNN. Another met-
ric, namely the prediction error, measures the error of the

6267

Dataset

ILSVRC Animal-Part

VOC Part

CUB200-2011

2nd
4.8
3.8
5.0

5th
31.6
25.7
32.0

10th
69.1
59.0
64.0

50th
236.5
219.5
230.0

100th
402.1
361.5
430.0

Table 1. Average number of nodes in the 2nd, 5th, 10th, 50th, and
100th layer of decision trees learned for VGG-16 nets.

2nd layer
5th layer
10th layer
50th layer
100th layer

breast
0.028
0.024
0.022
0.018
0.019

forehead

0.004
0.004
0.004
0.003
0.003

nape
0.013
0.010
0.010
0.008
0.008

tail

0.005
0.006
0.005
0.005
0.005

average
0.013
0.011
0.010
0.009
0.009

Table 2. Errors of object-part contributions that were estimated
using nodes in the 2nd/5th/10th/50th/100th layer of the decision
tree. The CNN was learned using the CUB200 dataset.

Dataset

ILSVRC Animal-Part

VGG-16

VGG-M

VGG-S

AlexNet

VOC Part

CUB200-2011

VOC Part

CUB200-2011

VOC Part

CUB200-2011

VOC Part

CUB200-2011

2nd
0.23
0.22
0.21
0.35
0.44
0.33
0.40
0.37
0.47

5th
0.30
0.30
0.26
0.38
0.44
0.35
0.40
0.38
0.47

10th
0.36
0.36
0.28
0.46
0.46
0.41
0.43
0.47
0.47

50th
0.52
0.53
0.33
0.63
0.59
0.63
0.48
0.66
0.58

100th
0.65
0.67
0.37
0.78
0.63
0.80
0.52
0.82
0.66

leaves
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

Table 3. Average ﬁtness of contribution distributions based on
nodes in the 2nd/5th/10th/50th/100th layer and leaf nodes, which
reﬂects the accuracy of the estimated rationale of a prediction.

y
c
a
r
u
c
c
a
n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

r
o
r
r
e

n
o
i
t
c
i
d
e
r
P

Dataset

ILSVRC Animal-Part

VGG-16

VOC Part

VGG-M

VGG-S

AlexNet

CUB200-2011

VOC Part

CUB200-2011

VOC Part

CUB200-2011

VOC Part

CUB200-2011

ILSVRC Animal-Part

VGG-16

VOC Part

VGG-M

VGG-S

AlexNet

CUB200-2011

VOC Part

CUB200-2011

VOC Part

CUB200-2011

VOC Part

CUB200-2011

CNN 2nd
94.4
96.7
94.2
95.4
96.5
91.5
95.7
94.2
97.2
96.0
92.7
95.5
95.4
95.8
90.7
93.9
95.4
94.9

–
–
–
–
–
–
–
–
–

0.052
0.052
0.075
0.053
0.036
0.047
0.045
0.055
0.044

5th
89.0
91.0
92.2
94.2
96.8
92.6
94.9
88.6
94.2

0.064
0.066
0.099
0.051
0.037
0.047
0.046
0.058
0.044

10th
88.7
90.1
88.3
93.1
96.0
91.3
93.1
88.6
94.3

0.063
0.070
0.101
0.051
0.038
0.045
0.050
0.055
0.045

50th
88.6
89.8
88.6
93.0
95.2
90.2
93.4
87.9
92.8

0.049
0.051
0.087
0.034
0.035
0.035
0.051
0.038
0.039

100th
88.7
89.4
88.9
92.6
94.9
88.8
93.6
86.2
92.0

0.034
0.035
0.083
0.019
0.030
0.019
0.038
0.020
0.033

leaves
87.8
88.2
85.3
90.8
93.5
86.1
88.8
84.1
90.0

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Table 4. Average classiﬁcation accuracy and average prediction
error based on nodes in the 2nd/5th/10th/50th/100th layer and leaf
nodes of the tree. The classiﬁcation accuracy and the prediction
error reﬂect the CNN knowledge not encoded by the decision tree.

estimated value ˆh(xi) w.r.t the true value yi. We comput-
i∈Ω+ [|ˆh(xi) − yi|]/(maxi∈Ω yi −
ed the prediction error as E
mini∈Ω yi), where we normalized the error using the value
range of yi.

Evaluation for nodes in different layers: The above three
metrics evaluate decision modes (nodes) in the second layer
of the decision tree. Because nodes in lower layers encode
more ﬁne-grained decision modes, we extended the three
metrics to evaluate nodes in low layers. When we evaluated
nodes in the k-th layer, we temporarily constructed a new
tree by removing all nodes above the k-th layer and directly
connecting the root node to nodes in the k-th layer. Thus,
we can apply the evaluation to the new tree.

Explanations based on the decision tree: Decision
modes in the decision tree objectively reﬂected the knowl-
edge hidden inside a CNN. Table 1 shows the structure of
the decision tree by listing numbers of nodes in different
layers of the decision tree. Fig. 5 visualizes decision modes
in the decision tree. Fig. 6 shows distributions of object-part
contributions to the CNN prediction, which were estimated
using nodes in the second layer of decision trees.

Table 4 evaluates the information loss when we use the
decision tree to represent a CNN. Metrics of the average
classiﬁcation accuracy, the average prediction error are used
for evaluation. Tables 2 and 3 use errors of object-part con-
tributions and the average ﬁtness of contribution distribu-
tions, respectively, to evaluate the accuracy of the estimated
rationales based on nodes in different tree layers. General-
ly speaking, because ﬁne-grained decision modes are close
to the image-speciﬁc rationale, ﬁne-grained decision modes
usually yielded lower error prediction rates. However, ﬁne-
grained decision modes did not exhibit higher accuracy in
classiﬁcation. It is because our method is designed to mine
common decision modes for objects of a certain catego-
ry, and ignores random/negative images, which is different
from the discriminative learning of classiﬁers.

6. Conclusion and discussions

In this study, we use a decision tree to explain CNN pre-
dictions at the semantic level. We have developed a method
to revise a CNN and built a tight coupling of the CNN and
a decision tree. The proposed decision tree encodes deci-
sion modes of the CNN as quantitative rationales for each
CNN prediction. Our method does not need any annota-
tions of object parts or textures in training images to guide
the learning the CNN. We have tested our method in differ-
ent benchmark datasets, and experiments have proved the
effectiveness of our approach.

Note that theoretically, the decision tree only provides
an approximate explanation for CNN predictions, instead
of an accurate reconstruction of CNN representation details.
There are two reasons. Firstly, without accurate object-part
annotations to supervised the learning of CNNs, the ﬁlter
loss can only roughly make each ﬁlter to represent an object
part. The ﬁlter may be activated by unrelated visual con-
cepts in a few challenging images. Secondly, the decision
mode in each node ignores insigniﬁcant object-part ﬁlters
to ensure a sparse representation of the decision mode.

Acknowledgements

This work is partially funded by Microsoft Research A-
sia, Huawei Key Laboratory of Shanghai Jiao Tong Uni-
versity, DARPA XAI Award N66001-17-2-4029, NSF IIS
1423305, and ARO project W911NF1810296.

6268

References

[1] M. Aubry and B. C. Russell. Understanding deep features

with computer-generated imagery. In ICCV, 2015. 3

[2] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Net-
work dissection: Quantifying interpretability of deep visual
representations. In CVPR, 2017. 2, 3, 4

[3] S. Branson, P. Perona, and S. Belongie. Strong supervision
Interactive training of deformable

from weak annotation:
part models. In ICCV, 2011. 6

[4] A. Chandrasekaran, V. Prabhu, D. Yadav, P. Chattopadhyay,
and D. Parikh. Do explanations make vqa models more pre-
dictable to a human? In EMNLP, 2018. 3

[5] Z. Che, S. Purushotham, R. Khemani, and Y. Liu.

Inter-
pretable deep models for icu outcome prediction. In Amer-
ican Medical Informatics Association (AMIA) Annual Sym-
posium, 2016. 3

[6] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. Infogan: Interpretable representation learning
by information maximizing generative adversarial nets.
In
NIPS, 2016. 3

[7] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and
A. Yuille. Detect what you can: Detecting and representing
objects using holistic models and body parts. In CVPR, 2014.
6, 7

[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 6

[9] F. Doshi-Velez and B. Kim. Towards a rigorous science of
interpretable machine learning. In arXiv:1702.08608, 2017.
3

[10] A. Dosovitskiy and T. Brox. Inverting visual representations

with convolutional networks. In CVPR, 2016. 1, 3

[11] R. Fong and A. Vedaldi. Net2vec: Quantifying and explain-
ing how concepts are encoded by ﬁlters in deep neural net-
works. In CVPR, 2018. 3

[12] R. C. Fong and A. Vedaldi.

s of black boxes by meaningful perturbation.
iv:1704.03296v1, 2017. 3

Interpretable explanation-
In arX-

[13] N. Frosst and G. Hinton. Distilling a neural network into a

soft decision tree. In arXiv:1711.09784, 2017. 3

[19] H. Lakkaraju, E. Kamar, R. Caruana, and E. Horvitz. Iden-
tifying unknown unknowns in the open world: Representa-
tions and policies for guided exploration. In AAAI, 2017. 3

[20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. In Proceed-
ings of the IEEE, 1998. 1

[21] Z. C. Lipton. The mythos of model interpretability. In Com-

munications of the ACM, 61:36–43, 2018. 3

[22] S. M. Lundberg and S.-I. Lee. A uniﬁed approach to inter-

preting model predictions. In NIPS, 2017. 1, 2, 3, 4

[23] A. Mahendran and A. Vedaldi. Understanding deep image

representations by inverting them. In CVPR, 2015. 1, 3

[24] M. T. Ribeiro, S. Singh, and C. Guestrin. “why should i trust
you?” explaining the predictions of any classiﬁer. In KDD,
2016. 1, 2, 3, 4

[25] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing

between capsules. In NIPS, 2017. 3

[26] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, and D. Batra. Grad-cam: Visual explanations
from deep networks via gradient-based localization. In arX-
iv:1610.02391v3, 2017. 1, 3

[27] M. Simon and E. Rodner. Neural activation constellations:
Unsupervised part model discovery with convolutional net-
works. In ICCV, 2015. 3, 4

[28] M. Simon, E. Rodner, and J. Denzler. Part detector discovery
in deep convolutional neural networks. In ACCV, 2014. 3, 6

[29] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
6

[30] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In arXiv:1312.6199v4, 2014. 3

[31] S. Tan, R. Caruana, G. Hooker, and A. Gordo. Transparent

model distillation. In arXiv:1801.08640, 2018. 3

[32] J. Vaughan, A. Sudjianto, E. Brahimi, J. Chen, and V. N.
Nair. Explainable neural networks based on additive index
models. In arXiv:1806.01933, 2018. 3

[33] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. Technical report,
In California Institute of Technology, 2011. 6

[34] N. Wolchover. New theory cracks open the black box of deep

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

learning. In Quanta Magazine, 2017. 1

for image recognition. In CVPR, 2016. 1, 6

[15] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
M. Botvinick, S. Mohamed, and A. Lerchner. β-vae: learn-
ing basic visual concepts with a constrained variational
framework. In ICLR, 2017. 3

[16] P.-J. Kindermans, K. T. Sch¨utt, M. Alber, K.-R. M¨uller,
D. Erhan, B. Kim, and S. D¨ahne. Learning how to explain
neural networks: Patternnet and patternattribution. In ICLR,
2018. 2

[17] P. Koh and P. Liang. Understanding black-box predictions

via inﬂuence functions. In ICML, 2017. 3

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 6

Imagenet
In

[35] M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, and
F. Doshi-Velez. Beyond sparsity: Tree regularization of deep
models for interpretability. In NIPS TIML Workshop, 2017.
3

[36] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In NIPS, 2014.
3

[37] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 3

[38] Q. Zhang, R. Cao, F. Shi, Y. Wu, and S.-C. Zhu. Interpreting
cnn knowledge via an explanatory graph. In AAAI, 2018. 2,
3, 4, 6

[39] Q. Zhang, W. Wang, and S.-C. Zhu. Examining cnn repre-

sentations with respect to dataset bias. In AAAI, 2018. 3

6269

[40] Q. Zhang, Y. N. Wu, and S.-C. Zhu. Interpretable convolu-

tional neural networks. In CVPR, 2018. 2, 3, 4, 5, 6, 7

[41] Q. Zhang, Y. Yang, Y. Liu, Y. N. Wu, and S.-C. Zhu. Un-
supervised learning of neural networks to explain neural net-
works. in arXiv:1805.07468, 2018. 3

[42] Q. Zhang, Y. Yang, Q. Yu, and Y. N. Wu. Network trans-

planting. in arXiv:1804.10272, 2018. 3

[43] Q. Zhang and S.-C. Zhu. Visual interpretability for deep
in Frontiers of Information Technology

learning: a survey.
& Electronic Engineering, 19(1):27–39, 2018. 3

[44] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.
Object detectors emerge in deep scene cnns. In ICLR, 2015.
3, 4, 7

[45] B. Zhou, Y. Sun, D. Bau, and A. Torralba. Interpretable basis

decomposition for visual explanation. In ECCV, 2018. 3

6270

