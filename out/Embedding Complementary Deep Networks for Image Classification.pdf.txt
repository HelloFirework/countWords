Embedding Complementary Deep Networks for Image Classiﬁcation

Qiuyu Chen1 ∗, Wei Zhang2 ∗, Jun Yu3, Jianping Fan1

1Department of Computer Science, University of North Carolina at Charlotte, USA

2Shanghai Key Laboratory of Intelligent Information Processing

2School of Computer Science, Fudan University, China

3School of Computer Science and Technology, Hangzhou Dianzi University, China

{qchen12,jfan}@uncc.edu,

weizh@fudan.edu.cn,

yujun@hdu.edu.cn

Abstract

rates on large-scale image classiﬁcation.

In this paper, a deep embedding algorithm is developed
to achieve higher accuracy rates on large-scale image clas-
siﬁcation. By adapting the importance of the object classes
to their error rates, our deep embedding algorithm can train
multiple complementary deep networks sequentially, where
each of them focuses on achieving higher accuracy rates for
different subsets of object classes in an easy-to-hard way.
By integrating such complementary deep networks to gen-
erate an ensemble network, our deep embedding algorithm
can improve the accuracy rates for the hard object classes
(which initially have higher error rates) at certain degrees
while effectively preserving high accuracy rates for the easy
object classes. Our deep embedding algorithm has achieved
higher overall accuracy rates on large-scale image classiﬁ-
cation.

1. Introduction

With the availability of massive training images and
the rapid growth of computational powers of GPUs, we
are now able to develop scalable learning algorithms to
support large-scale image classiﬁcation, and deep learn-
ing [17, 13, 5, 30, 31, 22, 10, 12] has demonstrated its out-
standing performance because it can learn more discrim-
inative representations in an end-to-end fashion. On the
other hand, boosting has demonstrated its strong capability
by embedding multiple complementary weak classiﬁers to
construct an ensemble one [26, 8, 34]. By assigning larger
weights (importance) to hard samples (which are misclas-
siﬁed by the previous weak classiﬁer), boosting can learn a
complementary weak classiﬁer at the current training round
by paying more attention on such hard samples. Thus it is
very attractive to invest whether boosting [26, 8, 34] can
be integrated with deep learning to achieve higher accuracy

∗The ﬁrst two authors have equal contributions on this work.

By using deep networks to replace the weak classiﬁers in
traditional boosting frameworks, boosting of deep networks
has recently received enough attention and some interesting
researches have been done [23, 28, 29, 36, 2, 18, 24, 6, 32].
All these existing deep boosting algorithms simply use the
weighted errors (proposed by Adaboost [26, 8, 34]) to re-
place the softmax errors (used in deep learning), and the
underlying deep networks treat the errors from the hard ob-
ject classes and the easy ones to be equally important. It
is worth noting that object classes may have signiﬁcant dif-
ferences on their learning complexities (i.e., some object
classes could be harder to be recognized than others), thus
the errors from the hard object classes and the easy ones
may have signiﬁcantly different effects on optimizing their
joint objective function. Therefore, learning a joint deep
network for the hard object classes and the easy ones may
not be an optimal solution for large-scale visual recogni-
tion because such joint deep network may not have strong
discrimination ability on the hard object classes which may
result in low accuracy rates.

To achieve higher accuracy rates on large-scale image
classiﬁcation, three types of solutions can be invested to
diversify the deep networks being combined and generate
more discriminative ensemble one: (a) weightingthe train-
ing samples according to their error rates and most existing
deep boosting algorithms [23, 28, 29, 36, 2, 18, 24, 6, 32]
belong to this direction; (b) learning multiple deep networks
by using different model parameters or using various sam-
ple subsets and some deep embedding algorithms [15, 25,
33, 9, 27, 14, 20, 4, 21, 1] belong to this direction; (c) train-
ing multiple complementary deep networks, e.g., such com-
plementary deep networks are trained sequentially by focus-
ing on achieving higher accuracy rates for different subsets
of object classes in an easy-to-hard way, so that they can
enhance each other. According to the best of our knowl-
edge, the third direction (i.e., training and embedding com-
plementary deep networks) has not been explored so far.

9238

Based on these observations, in this paper, a deep embed-
ding algorithm is developed to train and combine multiple
complementary deep networks to generate more discrimina-
tive ensemble network for achieving higher accuracy rates
on large-scale image classiﬁcation. The rest of the paper
is organized as: Section 2 brieﬂy reviews the related work;
Section 3 introduces our deep embedding algorithm; Sec-
tion 4 reports our experimental results over three datasets;
and we conclude this paper at Section 5.

Our code is public available at https://github.

com/qychen13/DifficultyAwareEmbedding.
2. Related Work

In this section, we brieﬂy review the most relevant re-
searches on deep learning, deep boosting, and deep embed-
ding.

Deep learning has demonstrated its outstanding abili-
ties on large-scale image classiﬁcation [17, 13, 5, 30, 31,
22, 10, 12], but most existing approaches completely ig-
nore that object classes may have signiﬁcant differences on
their learning complexities, e.g., some object classes may
be harder to be recognized than others and the gradients of
their joint objective function are not uniform for all of them.
As a result, learning a joint deep network for the hard object
classes and the easy ones may not be an optimal solution for
large-scale image classiﬁcation. Thus it is very attractive to
develop new approaches that can learn the deep networks
for the easy object classes and the hard ones sequentially in
an easy-to-hard way.

By assigning different weights (importance) to the train-
ing samples adaptively, boosting [26, 8, 34] has provided
an easy-to-hard approach to train multiple complementary
weak classiﬁers iteratively. Some deep boosting algorithms
have been developed by seamlessly integrating boosting
with deep networks [23, 28, 29, 36, 2, 18, 24, 6, 32]. All
these existing deep boosting algorithms use the weighted
errors to replace the softmax errors in traditional deep learn-
ing frameworks. Because the errors from all the object
classes (hard object classes and easy ones) are treated to
be equally important, such deep boosting algorithms may
still result in low accuracy rates for the hard object classes.
Some deep embedding algorithms have been devel-
oped [15, 25, 33, 9, 27, 14, 20, 4, 21, 1], where various sam-
ple sets or different model parameters are used to diversify
the deep networks but the errors from the hard object classes
and the easy ones are treated to be equally important. On the
other hand, our deep embedding algorithm focuses on com-
bining multiple complementary deep networks to generate
an ensemble network: (a) the errors from the hard object
classes are assigned with larger importance; (b) multiple
complementary deep networks are trained sequentially and
each of them focuses on achieving higher accuracy rates for
different subsets of object classes in an easy-to-hard way, so
that they can enhance each other.

Algorithm 1 Deep Embedding of Complementary Net-
works
Require: Training

classes:
Ini-
importance for N classes: φ1(C1) = ... =
N ; Number of complementary deep

{(xi, yi) | yi ∈ {C1, ..., CN }, i = 1, ..., R};
tial
φ1(CN ) = 1
networks or iterations: τ .

for

set

N

1: for t = 1, . . . , τ do

2:

3:

4:

5:

6:

Normalizing: ϕt(Cl) = φt(Cl)
j=1 φt(Cj )
Training the tth deep network ft(x);
Calculating the error rate εt(Cl) for each class;
Computing the weighted error

PN

rate for ft(x):

εt =PN

l=1 ϕt(Cl) εt(Cl);

Setting γt = µεt
1−µεt
Updating φt+1(Cl) = φt(Cl)γ1−µεt(Cl)

;

;

t

7:
8: end for
9: Output: F(x) = 1

Z Pτ

t=1 log(cid:16) 1

γt(cid:17) ft(x)

3. Embedding Complementary Deep Networks

As illustrated in Algorithm 1, our deep embedding algo-
rithm contains the following key components: (a) Training
the current tth deep network ft(x) by focusing on achiev-
ing higher accuracy rates for the hard object classes which
have higher error rates with the previous (t − 1)th deep net-
work ft−1(x); (b) Estimating the weighted error rate for
ft(x) according to the distribution of importance for N ob-
ject classes; (c) Updating the distribution of importance for
N object classes according to their error rates by ft(x), so
that the (t + 1)th deep network ft+1(x) can spend more
efforts on the hard object classes at the next training round;
(d) Such iterative process stops when the maximum number
of iterations is reached or a certain level of accuracy rates
is achieved. Our deep embedding algorithm uses the deep
CNNs as its weak learners and many well-designed deep
networks can be used.

3.1. Learning Complementary Deep Networks

To train the current tth deep network ft(x), a deep CNNs
is employed to obtain more discriminative representation
for the image x, followed by a fully connected discriminant
layer and a N -way softmax layer. The output of the tth deep
network ft(x) is a distribution of the prediction probabili-
ties for N object classes, denoted as ft(x; θt) = [pt(C1|x),
..., pt(CN |x)]⊤, where the lth probability score pt(Cl|x) is
for the image x to be assigned into the lth object class Cl,
and θt is the model parameter set for the tth deep network
ft(x). Ideally, the tth deep network ft(x) assigns the image
x into the object class with the maximum probability score:

ˆyt = arg max

l

pt(Cl|x),

l ∈ {1, · · · , N }

(1)

9239

The training set for N object classes is denoted as:
{(xi, yi) | yi ∈ {C1, ..., CN }, i = 1, ..., R}, where R is the
number of training samples. To train the tth deep network
ft(x), its model parameters can be obtained by maximizing
the objective function:

The importance over N object classes are initialized to
N , l = 1, ..., N , and they are updated

be equal: φ1(Cl) = 1
iteratively according to the error rates:

φt+1(Cl) = φt(Cl)γ1−µǫt(Cl)

t

Ωt(θt) =

N

Xl=1

ϕt(Cl)ξlt

(2)

and the normalization:

ϕt+1(Cl) =

where ϕt(Cl) =

φt(Cl)
j=1 φt(Cj )

PN

is the normalized importance

for the lth object class Cl, while φt(Cl) is the unnormalized
importance. ξlt is used to measure the margin between the
average conﬁdences for the correctly-classiﬁed images and
the misclassiﬁed images for the lth object class Cl:

(3)

1(yi 6= Cl) log pt(Cl|xi)

1(yi = Cl) log pt(Cl|xi)−

R

1
Rl

R

1

ξlt =

R − Rl

Xi=1
Xi=1
object class, and PN
Rl PR

i=1

where Rl is the number of training images from the lth
l=1 Rl = R. The indication function
1(yi = Cl) is equal to 1 if yi = Cl; otherwise zero. If the
second item in Eq.(3) is small enough and negligible, it ap-
proximates ξlt ≈ 1
1(yi = Cl) log pt(Cl|xi), then
maximizing the objective function in Eq.(2) is equivalent to
maximizing the weighted likelihood. By using the normal-
ized importance [ϕt(C1),...,ϕt(CN )] to estimate the learn-
ing complexities for N object classes, our deep embedding
algorithm can push the current deep network ft(x) to focus
on distinguishing the hard object classes which have higher
error rates and tend to be misclassiﬁed by the previous deep
network ft−1(x), thus it can support an easy-to-hard solu-
tion for large-scale image classiﬁcation.

For the tth deep network ft(x), the error rate ǫt(Cl) for

the lth object class Cl is deﬁned as:

ǫt(Cl) =

1
2

R

Xi=1(cid:26)1(yi = Cl)

1 − pt(Cl|xi)

Rl

+

(4)

1(yi 6= Cl)

pt(Cl|xi)

R − Rl (cid:27)

The error rate in Eq.(4) is calculated in a soft decision way
with probability; alternatively, we can also simply com-
pute the error rate in a hard decision way as ǫt(Cl) =
1

1(yi = Cl ∧ ˆyt

i 6= Cl).

The error rate εt for ft(x) is deﬁned as:

i=1

RPR

εt =

N

Xl=1

ϕt(Cl)ǫt(Cl)

(5)

(6)

(7)

φt+1(Cl)
j=1 φt+1(Cj)

PN

where µ in Eq.(6) is a hyper-parameter to be selected. γt in
Eq.(6) is an increasing function of εt and its range is 0 <
γt < 1, and as in Section 3.3, the optimal γt in Eq.(6) is set
to be:

γt =

µεt

1 − µεt

(8)

Thus updating the importance for N object classes accord-
ing to their error rates can push the (t + 1)th deep network
ft+1(x) to pay more attention on the hard object classes
(with larger error rates) at the next training round, so that
such sequential deep networks ft+1(x) and ft(x) are com-
plementary to each other.

3.2. Deep Embedding of Complementary Networks

After τ iterations, we can obtain τ complementary deep
networks {f1, · · · , ft, · · · , fτ }, where each of them focuses
on achieving higher accuracy rates on different subsets of N
object classes in an easy-to-hard way and they can enhance
each other. For recognizing N object classes accurately,
all these τ complementary deep networks are embedded to
generate an ensemble network F(x):

where Z = Pτ

τ

F(x) =

1
Z

log(cid:18) 1

γt(cid:19) ft(x)

Xt=1
t=1 log(cid:16) 1
γt(cid:17) is a normalization factor. The

(9)

output of F(x) is an N −dim vector for prediction probabil-
ity distribution. For a given test sample xtest, its lth prob-
ability score p(Cl|xtest) (for assigning it into the lth class
Cl) can be easily calculated by Eq.(9).

The given test sample xtest is ﬁnally be assigned into
top-1 object class with the maximum probability score or
top-k object classes with top-k scores. By training and com-
bining multiple complementary deep networks, our deep
embedding algorithm can generate more discriminative en-
semble network F(x) to achieve higher overall accuracy
rates on large-scale visual recognition, e.g., our deep em-
bedding algorithm can improve the accuracy rates for the
hard object classes at certain degrees while effectively pre-
serving high accuracy rates for the easy ones.

9240

3.3. Parameter Selection for Deep Embedding

According to Eq.(5) and Eq.(2), we can get:

Inspired by [7], we study the optimal parameter selec-
tion for deep embedding. In the proposed algorithm, γt in
the range [0, 1] is set to be an increasing function of the er-
ror rate εt. γt is employed in two folds: (i) As deﬁned in
Eq.(6), γt is used to update the distribution of importance
to pay more attention on the hard object classes with higher
error rates; (ii) As deﬁned in Eq.(9), the reciprocal of γt is
used to determine the weight or importance of the tth com-
plementary deep network ft(x) in the ensemble network.

The error rate is used as the criterion for the tth deep
network ft(x) to determine the hard object class: ǫt(Cl) >
1
2µ , l ∈ {1, · · · , N }, e.g., the error rate for the hard object
class is above a threshold 1
2µ . For the lth object class, by
assessing it over τ complementary deep networks, we can
further deﬁne ǫmin(Cl) as:

ǫmin(Cl) , min

t∈{1,··· ,τ }

{ǫt(Cl)}

If ǫmin(Cl) > 1
2µ , the lth object class Cl is always hard
to be recognized by all τ complementary deep networks.
The occurrence of such always-hard object classes may se-
riously affect the overall accuracy rates of our deep embed-
ding algorithm on large-scale image classiﬁcation.

We use ̺ to denote the number of such always-hard ob-

ject classes:

̺ =

N

Xl=1

1(cid:18)ǫmin(Cl) >

1

2µ(cid:19)

2µ ) = 1 if ǫmin(Cl) > 1

where 1(ǫmin(Cl) > 1
2µ is true,
otherwise, 1(ǫmin(Cl) > 1
2µ ) = 0. To achieve higher
overall accuracy rates on large-scale visual recognition, we
should select suitable γt in Eq.(8) to guarantee that ρ = ̺
N
is minimized (e.g., the number of such always-hard object
classes ̺ is minimized).

For 0 < η < 1, we have xη ≤ 1 − (1 − x)η. According

to Eq.(6), the importance for Cl is updated as:

φt+1(Cl) = φt(Cl)γ1−µǫt(Cl)

t

we can get:

N

φt+1(Cl) =

N

Xl=1

φt(Cl)γ1−µǫt(Cl)

t

≥

≤

φt(Cl)(1 − (1 − γt)(1 − µǫt(Cl)))

N

Xl=1
Xl=1

=

N

Xl=1

φt(Cl)(1 − (1 − γt)) + µ(1 − γt)

N

Xl=1

φt(Cl)ǫt(Cl)

(10)

9241

(11)

φt+1(Cl) ≤

N

Xl=1

N

Xl=1

φt(Cl)ǫt(Cl) =  N
Xl=1

φt(Cl)! εt

N

φt(Cl)(1 − (1 − γt))+

Xl=1
µ(1 − γt)  N
Xl=1

φt(Cl)! εt
φt(Cl)! [1 − (1 − γt)(1 − µεt)]

l=1 φ1(Cl) = 1, we can have:

φ2(Cl) ≤ 1 − (1 − γ1)(1 − µε1)

=  N
Xl=1
BecausePN
Xl=1

N

N

Xl=1

φT +1(Cl) ≤ Πτ

t=1[1 − (1 − γt)(1 − µεt)]

(12)

By substituting Eq.(6) into Eq.(12), we can get:

Πτ

t=1[1 − (1 − γt)(1 − µεt)] ≥

φt+1(Cl)

N

Xl=1

t=1γ1−µǫt(Cl)

t

t=1γ1−µǫt(Cl)

t

t=1γ1−µǫt(Cl)

t

(13)

(cid:17)
(cid:17)
(cid:17)

N

=

N

=

1
N

Xl=1(cid:16)φ1(Cl)Πτ
Xl=1(cid:16)ΠT
2µ(cid:16)Πτ

N Xǫmin(Cl)> 1

1

≥

When ǫmin(Cl) > 1
and 1 − µǫt(Cl) < 1
constraint 0 < γt < 1, we can have:

2µ holds, it guarantees that ǫt(Cl) > 1
2µ
2 for all N object classes. Recall the

1

N Xǫmin(Cl)> 1
2µ(cid:16)Πτ

2µ(cid:16)Πτ
t (cid:17) =

t=1γ

1

2

1

N Xǫmin(Cl)> 1

t=1γ1−µǫt(Cl)

t

(cid:17)

̺
N

Πτ

t=1γ

1

2
t

(14)

Combining Eq.(13) with Eq.(14), we can get:

ρ =

̺
N

≤

Πτ

t=1[1 − (1 − γt)(1 − µεt)]

1

Πτ

t=1γ

2
t

1 − (1 − γt)(1 − µεt)

(15)

1

γ

2
t

= Πτ

t=1

To minimize the right-side in Eq.(15), we set its partial
derivative over γt to be zero, and the optimal γt is deter-
mined as:

γt =

µεt

1 − µεt

We substitute γt = µεt
1−µεt

upper boundary for ρ as:

into Eq.(15), and obtain the

(a)

(b)

ρ =

̺
N

≤ 2τ Πτ

t=1pµεt(1 − µεt)

(16)

].

Now we study the range for the hyper-parameter µ. The
criterion for the tth deep network ft(x) to determine the
hard object classes (with higher error rates) is deﬁned as
ǫt(Cl) > 1
2µ , where µ is used to control the threshold of the
expected error rate (i.e., when we have more strict require-
ment on the expected error rate (i.e., smaller threshold), µ
should be larger), and we set the constraint for the hyper-
parameter µ as µ > 1
2 . On the other hand, the range of
γt = µεt
is 0 < γt < 1, and it is required that µεt < 1
2 ,
1−µεt
i.e., µ < 1
. As a result, µ should be selected between the
2εt
interval [ 1
2 , 1
2εt

2 , 1
2εt

], i.e., εt

, i.e., µεt > 1

2 < µεt < 1

From the relationship between µεt and µεt(1 − µεt), we
can observe the effect of µ on the upper boundary of ρ in
Eq.(16): (a) When µ ∈ [ 1
2 , the
condition 0 < γt < 1 is satisﬁed, and the upper bound-
ary for ρ in Eq.(16) increases when µ increases, the reason
is that when µ increases, the threshold for determining the
hard object classes is smaller, thus the number of always-
hard object classes may increase (i.e., the error rates for
more classes could be above such smaller threshold). (b)
When µ > 1
2 . In this case, the condition
2εt
0 < γt = µεt
< 1 is not satisﬁed, thus updating the dis-
1−µεt
tribution of importance in Eq.(6) can not effectively push
the next deep network to pay more attention on the hard ob-
ject classes. For such situation, large error rates εt tend to
result in µεt being larger than or approaching 1
2 , and γt be-
ing larger than or approaching 1. Thus the value of µ should
be smaller to alleviate large εt such that the following con-
straints can still exist: µεt < 1
2 and 0 < γt < 1. (c) When
µ < 1
2µ > 1, it can not be used as the criterion
for the tth deep network ft(x) to determine the hard object
classes that satisfy ǫt(Cl) > 1

2 , i.e., 1

2µ .

4. Experimental Results and Discussions

In this section, we report our evaluation results for
our deep embedding algorithm over three popular datasets:
MNIST [19], CIFAR-100 [16], and ImageNet1K [3].

(a) Experimental Results on MNIST: MNIST dataset
consists of 60,000 training handwritten digit samples and
10,000 test samples [19]. The research in [28] has demon-
strated the accuracy improvement on MNIST dataset by up-
dating the sample weights according to their error rates. For
fair comparison, we use two approaches to train the deep

Figure 1: Results on MNIST dataset: (a)The compar-
ison on top 1 error for MNIST dataset when different
weighting approaches are used; (b)The comparison on
AP (average precision) of the ensemble network when
different numbers of complementary deep networks are
combined.

networks: (1) our deep embedding algorithm updates the
class weights according to their error rates; (2) traditional
approach updates the sample weights like AdaBoost.
In
our experiments, we simply train the deep network with the
learning rate 0.01 throughout the whole 120 epochs.

With our deep embedding method, the top 1 error rate
on the test dataset decreases from 4.73% to 1.87% after
three iterations (as shown in Fig. 1a). After the ﬁrst iter-
ation, the top 1 error of our deep embedding method drops
more quickly than the traditional approach. Our deep em-
bedding method, which updates the class weights (i.e., the
distribution of importance), can exploit the idea that dif-
ferent classes may have different learning complexities and
they should be treated differentially in an easy-to-hard way.
From Fig. 1b, one can easily observe that our deep embed-
ding algorithm can signiﬁcantly improve the accuracy rates
for the hard classes while effectively preserving the high
accuracy rates for the easy ones.

(b) Experimental Results on CIFAR-100: We also
carry out our experiments on CIFAR-100 dataset [16].
CIFAR-100 dataset has 60,000 images for 100 object
classes. There are 500 training images and 100 testing
images for each class. In the training stage, we hold out
5,000 images for validation and use 45,000 images for train-
ing. We further adopt padding, mirroring, shifting for data
augmentation and normalization [10, 12]. After several it-
erations, the training error rate for each class is close to
zero [35], thus we update the distribution of importance ac-
cording to their error rates on the validation datasets. When
we train the deep networks on CIFAR-100, the initial learn-
ing rate is set to 0.1 and divided by 0.1 at epoch [150,225],
and we train the deep networks for 300 epoches. The com-
parison results are demonstrated in Tab. 1 when (1) different
types of deep networks (such as ResNet56(µ = 0.7) [11]
and DenseNet-BC(k=12) [12]) are used; (2) different num-
bers T of complementary deep networks are combined.

To train the ﬁrst deep network, we treat all 100 object
classes in CIFAR-100 with equal importance as shown in

9242

Iteration #1

Iteration #4(μ=0.7)

Iteration #1

Iteration #4(μ=0.5)

Iteration #1

Iteration #4(μ=0.3)

(a)

(b)

(c)

(d)

Figure 2: The comparison on CIFAR-100 dataset: (a) the distribution of weights (importance) for the deep networks
at different iterations; (b) the comparison on the accuracy rates for the complementary network on the training set
for CIFAR-100 dataset; (c) the effects of using different µ; (d) The comparison on the accuracy rates for the ensemble
network on the test set for CIFAR-100 dataset.

Fig. 2a, and the ﬁrst deep network is learned and its accu-
racy rates for all 100 object classes are shown in Fig. 2b.
One can easily observe that some easy object classes can
achieve acceptable accuracy rates at the ﬁrst iteration (i.e.,
the ﬁrst deep network) but some hard object classes may
have very low accuracy rates. By updating the distribution
of importance for 100 object classes (putting larger weights
for hard object classes and smaller weights for easy object
classes) as shown in Fig. 2a, from the second iteration, our
deep embedding algorithm can pay more attention on the
hard object classes. The effects of the hyper-parameter µ
on the performances of our deep embedding algorithm are
shown in Fig. 2c.

By combining multiple complementary deep networks,
our deep embedding algorithm can generate more discrim-
inative ensemble network to improve the overall accuracy
rates on large-scale image classiﬁcation. As shown in
Fig. 2b, one can observe that our complementary deep net-
works can achieve almost zero error rates on the training
image set, which has good correspondence with the obser-
vations in [35]. By increasing the importance of hard object
classes and pushing the next deep network to pay more at-
tention on them, one can easily observe that the accuracy
rates for such hard object classes may improve on the train-
ing set (as shown in Fig. 2b), however, the improvement of
their accuracy rates on the testing set may still be limited
as shown in Fig. 2d and Tab. 1. The reasons for this phe-
nomenon are: (1) such hard object classes may have huge
intra-class visual diversities, thus the test images and the
training images may have signiﬁcant differences on their
visual properties; (2) such hard object classes may have
huge inter-class visual similarities with others, thus they are
easily confused from other similar ones. Besides handling
the hard object classes and the easy ones sequentially in an
easy-to-hard way by weighting their importance according
to their error rates, we also need to look for more effective
solutions to deal with the issues of huge intra-class visual
diversities and huge inter-class visual similarities.

(c) Experimental Results on ImageNet1K:

Ima-
geNet1K dataset [3] consists 1,000 object classes, which

have 1.2 million images for training, and 50,000 for vali-
dation. When we train the deep networks on ImageNet1K
dataset, the initial learning rates are set to 0.1 and divided
by 0.1 at epoch [30, 60]. The performances of our ensem-
ble network are shown in Tab. 1 when: (1) different types of
complementary deep networks are used; (2) different num-
bers T of complementary deep networks are combined to
generate the ensemble network.

When we train the ﬁrst deep network, we treat all 1,000
object classes with equal importance as shown in Fig. 3(a),
and a deep network is learned and its accuracy rates for all
1,000 object classes on the training set are shown in Fig.
3(b). One can easily observe that some easy object classes
have achieved acceptable accuracy rates at the ﬁrst iteration
(i.e., by the ﬁrst deep network) but some hard object classes
may have very low accuracy rates. By updating the impor-
tance of 1,000 object classes according to their error rates
as shown in Fig. 3(a), from the second iteration, our deep
embedding algorithm can pay more attention on the hard
object classes and their accuracy rates can be improved dra-
matically on the training set.

Our deep embedding algorithm can generate more dis-
criminative ensemble network to achieve higher accuracy
rates on large-scale image classiﬁcation as shown in Fig.
3(c), e.g., our deep embedding algorithm can improve the
accuracy rates for the hard object classes at certain degrees
while effectively preserving high accuracy rates for the easy
ones. By comparing the performance improvement on the
test set (as shown in Fig. 3(c)) and that on the training set
(as shown in Fig. 3(b)), we have similar observations as
we have obtained on CIFAR-100 dataset: our deep embed-
ding algorithm can improve the accuracy rates for the hard
object classes at the training set while the improvement on
the testing set may not be so signiﬁcant, e.g., some hard ob-
ject classes are always hard for all τ complementary deep
networks and ρ is larger for ImageNet1K dataset.

Besides the two reasons (huge intra-class visual diver-
sities and huge inter-class visual similarities) which have
discussed above, another key reason for this phenomenon
in ImageNet1K dataset is that: (a) Some hard object classes

9243

020406080100Class ID0.51.01.52.02.53.03.54.0weightIteration #1Iteration #2Iteration #3Iteration #4020406080100Class ID0.9750.9800.9850.9900.9951.000Average PrecisionIteration #1Iteration #2Iteration #3Iteration #4Table 1: The comparisons on the top-1 average error rates (%), where the results in () are the top-5 average error
rates.

Datasets
MNIST

CIFAR-100

ImageNet1K

Network
MLP [19]
ResNet56 [11]
DenseNet-BC(k=12) [12]
ResNet50 [11]
DenseNet121 [12]
AlexNet [17]

T = 1
4.73
29.53
30.78

T = 2
2.22
26.65
28.95

T = 3
1.87
24.97
27.60

T = 4
1.86
24.15
26.64

24.18(7.49) 23.28(6.98) 22.96(6.81) 22.12(6.79)
25.88(8.38) 24.85(7.89) 23.67(7.25) 22.32(6.17)
43.71(21.24) 42.61(20.61) 40.83(19.32) 39.23(17.78)

Figure 3: The comparison on ImageNet1K: (a) the distribution of importance at different iterations; (b) the train-
ing accuracy rates for the complementary networks at different iterations; (c) the validation accuracy rates of the
embedding networks when different numbers T of complementary networks are trained and combined to form the
embedding networks.

are from the leaf nodes of the concept ontology with longer
depths,
there may have multiple visually-similar object
classes which are hard to be distinguished from each other,
e.g., ﬁne-grained hard object classes. As illustrated in Ta-
ble 1, the top-1 accuracy rates could be very low but the
top-5 accuracy rates could be much better because the mis-
takes on distinguishing among multiple ﬁne-grained hard
object classes are not counted in such top-5 error rates. (2)
Some hard object classes are from the leaf nodes of the con-
cept ontology with very short depths (i.e., coarse-grained
hard object classes), we may need larger numbers of train-
ing images to learn the deep networks for discriminating
such coarse-grained hard object classes effectively, as a re-
sult, using the same number of training images for all the
object classes (as it has been done by most existing deep
learning algorithms) may not be sufﬁcient to learn discrim-
inative deep networks for such coarse-grained hard object
classes.

To learn more discriminative deep networks for the hard
object classes, we may further invest: (1) Integrating ad-
ditional information (such as the inter-class semantic cor-
relations from the concept ontology [3]) to learn the deep
network for the ﬁne-grained hard object classes; (2) Us-
ing more training images for the coarse-grained hard ob-
ject classes and developing new deep learning algorithms
which can handle sample imbalances effectively; (3) Us-
ing heterogeneous embedding, e.g., using different types of

deep networks at different iterations such as AlexNet for the
ﬁrst one, ResNet50 for the second complementary one, and
ResNet152 for the third complementary one, et al..

(d) Comparison over Embedding Approaches: For
CIFAR-100 and ImageNet1K datasets, we have compared
three embedding approaches: (1) deep boosting [28, 29]; (2)
traditional deep embedding [9, 27]; (3) our deep embedding
algorithm. In this comparison experiment, the same type
of deep networks (ResNet56 for CIFAR-100 and ResNet50
for ImageNet1k) is used as the complementary network-
for three approaches. By combining the same numbers T
of deep networks, we have compared the performances of
the ensemble networks which are generated by three ap-
proaches. As shown in Table 2, one can easily observe that
our deep embedding algorithm can achieve higher overall
accuracy rates on large-scale image classiﬁcation.

Further Discussion

(a) Regularization: AdaBoost [7](weighting at the sam-
ple level) has achieved higher training accuracy by combin-
ing traditional weak learning models (i.e. small networks in
[28, 29]), but it is unsuitable for combining large networks:
(a) In deep learning, the training error could approach zero
and easily suffers from overﬁtting [35]; (b) When the train-
ing error rates are close to zeros, focusing on the hard sam-
ples could make the problem of overﬁtting even worse be-
cause the deep networks will be trained with a small portion

9244

(a) network#1

(b) epoch#1

(c) epoch#2

(d) epoch#3

Figure 4: Illustration for training convergence process of complementary networks dynamically/jointly optimized
on CIFAR100: the convergence process of one complementary network(a), the average precision among different
networks for the ﬁrst three epochs(b,c,d). It shows that the difﬁcult categories (small class ID) are similar even with
the randomization from the network initialization and optimization process.

Table 2: The comparisons on the top-1 average error rates (%) for multiple embedding approaches.

Datasets

CIFAR-100

ImageNet1K

Different Approaches for Network Embedding T = 1 T = 2 T = 3 T = 4
29.53 26.65 24.97 24.15
Our deep embedding
29.53 29.40 29.35 29.32
traditional deep embedding [9, 27]
29.53 27.60 26.82 26.53
deep boosting [28, 29]
Our deep embedding
24.18 23.28 22.96 22.12
24.18 23.65 23.15 23.08
traditional deep embedding [9, 27]
deep boosting [28, 29]
24.18 24.07 23.98 23.75

of samples. For example, in our experiments on CIFAR100
dataset, the training errors could easily approach zeros (Fig.
2a), which is in line with [35], thus we use the validation
error rates to weight the objective function in the subse-
quent iterations. At this scenario, weighting at the sample
level is not practical to use the validation results and would
only leave a small portion of the samples focused and may
worsen the overﬁtting.

Overall, weighting at the sample level may impair the
generation ability of strong complementary learning mod-
els (ResNet56/ResNet50 in Tab. 2). Weighting at the cate-
gory level provides an effective alternative which acts as a
regularization method and guides the optimization process
(SGD) to pay more attention on difﬁcult categories by op-
timizing the weighted objective function subsequently (Fig.
2 and Fig. 2d).

(b) Sequentially Guided Optimization: Combin-
ing multiple complementary deep networks dynami-
cally/jointly [9, 27] is indeed a reasonable alternative for
embedding and it has shown improved results on ﬁne-
grained tasks [9] and small neural networks [27], but the
improved margin is small with dynamic weighting on non-
ﬁne-grained task (CIFAR-100/ImageNet) with deep neural
networks (ResNet56/ResNet50) in our experiments (Tab.2).
The diversity of dynamic weighting is based on randomiza-
tion from network initialization and optimization process
(SGD) while our proposed method is based on learning dif-
ﬁculty of categories. For difﬁcult categories which are hard

to learn and converge slowly for all networks, i.e. small
ID categories in Fig.4a for all complementary networks (
Fig.4b,4c,4d), the weights/occupations could be distributed
almost equally for the complementary networks if opti-
mized jointly and behaves like average ensemble. How-
ever, such hard categories would be focused and learned by
guided objective function in the subsequent networks with
the proposed method (Fig.2&Fig.3). In addition, we have
proven the theoretical convergence of our proposed method
(Sec. 3.3&Eq. 16).

5. Conclusions

A deep embedding algorithm is developed to train and
combine multiple complementary deep networks, where
each of them focuses on achieving higher accuracy rates for
different subsets of object classes in an easy-to-hard way
and they can enhance each other. Our deep embedding al-
gorithm can improve the accuracy rates for the hard object
classes at certain degrees while effectively preserving high
accuracy rates for the easy ones, thus it can achieve higher
overall accuracy rates on large-scale image classiﬁcation.

Acknowledgment

We would like to thank the anonymous reviewers for
their helpful comments. This work was supported in part by
NSFC under Grant (No.61473091 and No.61572138) and
STCSM Project under Grant No.16JC1420400.

9245

020406080100Class ID0.00.20.40.60.81.0APEpoch #1(train)Epoch #2(train)Epoch #3(train)Epoch #40(train)020406080100Class ID0.00.10.20.30.40.5APnetwork #1(train)network #2(train)network #3(train)020406080100Class ID0.20.30.40.50.60.70.8APnetwork #1(train)network #2(train)network #3(train)020406080100Class ID0.40.50.60.70.8APnetwork #1(train)network #2(train)network #3(train)References

[1] Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boost-
ing dilated convolutional networks with mixed tensor de-
compositions. In 6th International Conference on Learning
Representations, ICLR, 2018.

[2] Corinna Cortes, Mehryar Mohri, and Umar Syed. Deep
In International Conference on Machine Learn-

boosting.
ing, pages 1179–1187, 2014.

[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009.

[4] Li Deng, Dong Yu, and John Platt. Scalable stacking and
learning for building deep architectures. In 2012 IEEE In-
ternational conference on Acoustics, speech and signal pro-
cessing (ICASSP), pages 2133–2136. IEEE, 2012.

[5] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,
Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep
convolutional activation feature for generic visual recogni-
tion. In International conference on machine learning, pages
647–655, 2014.

[6] Harris Drucker, Robert Schapire, and Patrice Simard.

Im-
proving performance in neural networks using a boosting al-
gorithm. In Advances in neural information processing sys-
tems, pages 42–49, 1993.

[7] Yoav Freund and Robert E Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting.
Journal of computer and system sciences,
55(1):119–139, 1997.

[8] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al.
Additive logistic regression: a statistical view of boosting
(with discussion and a rejoinder by the authors). The annals
of statistics, 28(2):337–407, 2000.

[9] ZongYuan Ge, Alex Bewley, Christopher McCool, Peter
Corke, Ben Upcroft, and Conrad Sanderson. Fine-grained
classiﬁcation via mixture of deep convolutional neural net-
works. In 2016 IEEE Winter Conference on Applications of
Computer Vision (WACV), pages 1–6. IEEE, 2016.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation.
In Proceedings of the
IEEE international conference on computer vision, pages
1026–1034, 2015.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[12] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017.

[13] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In Proceedings of the 22nd ACM inter-
national conference on Multimedia, pages 675–678. ACM,
2014.

[14] Tae-Kyun Kim, Ignas Budvytis, and Roberto Cipolla. Mak-
ing a shallow network deep: Conversion of a boosting clas-
siﬁer into a decision tree by boolean optimisation. Interna-
tional journal of computer vision, 100(2):203–215, 2012.

[15] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi,
and Samuel Rota Bulo. Deep neural decision forests. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 1467–1475, 2015.

[16] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer, 2009.

[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012.

[18] Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-
class deep boosting. In Advances in Neural Information Pro-
cessing Systems, pages 2501–2509, 2014.

[19] Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner,
et al. Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[20] Chen-Yu Lee, Patrick W Gallagher, and Zhuowen Tu. Gen-
eralizing pooling functions in convolutional neural networks:
Mixed, gated, and tree. In Artiﬁcial intelligence and statis-
tics, pages 464–472, 2016.

[21] Jun Li, Heyou Chang, and Jian Yang. Sparse deep stack-
ing network for image classiﬁcation. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015.

[22] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-
work. In International Conference on Learning Representa-
tions, ICLR, 2014.

[23] Mohammad Moghimi, Serge J Belongie, Mohammad J
Saberian, Jian Yang, Nuno Vasconcelos, and Li-Jia Li.
Boosted convolutional neural networks. In BMVC, pages 24–
1, 2016.

[24] Zhanglin Peng, Ya Li, Zhaoquan Cai, and Liang Lin. Deep
joint feature selection and analysis dictionary

boosting:
learning in hierarchy. Neurocomputing, 178:36–45, 2016.

[25] Samuel Rota Bulo and Peter Kontschieder. Neural decision
forests for semantic image labelling. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 81–88, 2014.

[26] Robert E Schapire and Yoram Singer. Improved boosting al-
gorithms using conﬁdence-rated predictions. Machine learn-
ing, 37(3):297–336, 1999.

[27] Jurgen Schmidhuber. Multi-column deep neural networks
for image classiﬁcation.
In Proceedings of the 2012 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3642–3649. IEEE Computer Society, 2012.

[28] Holger Schwenk and Yoshua Bengio. Adaboosting neu-
ral networks: Application to on-line character recognition.
In International Conference on Artiﬁcial Neural Networks,
pages 967–972. Springer, 1997.

[29] Holger Schwenk and Yoshua Bengio. Boosting neural net-

works. Neural computation, 12(8):1869–1887, 2000.

[30] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-

9246

ternational Conference on Learning Representations, ICLR,
2015.

[31] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1–9, 2015.

[32] Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-
Johann Simon-Gabriel, and Bernhard Sch¨olkopf. Adagan:
Boosting generative models. In Advances in Neural Infor-
mation Processing Systems, pages 5424–5433, 2017.

[33] Andreas Veit, Michael J Wilber, and Serge Belongie. Resid-
ual networks behave like ensembles of relatively shallow net-
works.
In Advances in neural information processing sys-
tems, pages 550–558, 2016.

[34] Paul Viola and Michael J Jones. Robust real-time face detec-
International journal of computer vision, 57(2):137–

tion.
154, 2004.

[35] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. Understanding deep learning re-
quires rethinking generalization. In 5th International Con-
ference on Learning Representations, ICLR, 2017.

[36] Ligang Zhou and Kin Keung Lai. Adaboosting neural net-
works for credit scoring.
In The Sixth International Sym-
posium on Neural Networks (ISNN 2009), pages 875–884.
Springer, 2009.

9247

