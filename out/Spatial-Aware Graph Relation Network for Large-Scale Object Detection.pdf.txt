Spatial-aware Graph Relation Network for Large-scale Object Detection

Hang Xu1∗ ChenHan Jiang2∗ Xiaodan Liang2† Zhenguo Li1

1Huawei Noah’s Ark Lab

2Sun Yat-sen University

Abstract

How to proper encode high-order object relation in the
detection system without any external knowledge? How to
leverage the information between co-occurrence and loca-
tions of objects for better reasoning? These questions are
key challenges towards large-scale object detection system
that aims to recognize thousands of objects entangled with
complex spatial and semantic relationships nowadays.

Distilling key relations that may affect object recogni-
tion is crucially important since treating each region sepa-
rately leads to a big performance drop when facing heavy
long-tail data distributions and plenty of confusing cate-
gories. Recent works try to encode relation by construct-
ing graphs, e.g. using handcraft linguistic knowledge be-
tween classes or implicitly learning a fully-connected graph
between regions. However, the handcraft linguistic knowl-
edge cannot be individualized for each image due to the se-
mantic gap between linguistic and visual context while the
fully-connected graph is inefﬁcient and noisy by incorpo-
rating redundant and distracted relations/edges from irrel-
evant objects and backgrounds. In this work, we introduce
a Spatial-aware Graph Relation Network (SGRN) to adap-
tive discover and incorporate key semantic and spatial rela-
tionships for reasoning over each object. Our method con-
siders the relative location layouts and interactions among
which can be easily injected into any detection pipelines to
boost the performance. Speciﬁcally, our SGRN integrates
a graph learner module for learning a interpatable sparse
graph structure to encode relevant contextual regions and
a spatial graph reasoning module with learnable spatial
Gaussian kernels to perform graph inference with spatial
awareness. Extensive experiments verify the effectiveness
of our method, e.g. achieving around 32% improvement on
VG(3000 classes) and 28% on ADE in terms of mAP.

1. Introduction

Most recent advancement of CNN detectors is focused
on the task with a limited number of categories (say, 20 for

∗Both authors contributed equally to this work.
†Corresponding Author: xdliang328@gmail.com

(e.g. “goat” is alone).

Figure 1. Different choice of constructing a graph to encode rela-
tion: (a) Using handcraft knowledge to build class-to-class graph.
Some spatial relations are ignored and the ﬁxed graph cannot
adapt to the image due to the gap between linguistic and visual
context.
(b) Implicitly learning a fully-
connected graph between regions. The learned edges are redun-
dant since background regions are also connected and the fully-
connected graph ignore the pairwise spatial information. (c) Our
proposed SGRN learns a spatial-aware sparse graph, leveraging
semantic and spatial layout relationship. (d) An example of prop-
agation with multiple learnable spatial Gaussian kernels about the
“driver” node. Different spatial kernels allows the propagation of
the graph behaves differently according to the pairwise spatial in-
formation(different thickness of the edges).

VOC [11] and 80 for MS-COCO dataset [32]). However,
there is an increasing need for recognizing more kinds of
objects (e.g. 3000 categories in VG [25]) so that large-scale
object detection [19, 23] has received a lot of attention be-
cause of its practical usefulness in the industry. Current de-
tection pipelines mostly treat the recognition of each region
separately and suffer from great performance drops when
facing heavy long-tail data distributions and plenty of con-
fusing categories. It has been well recognized in the com-
munity that relation between objects can help to improve
object recognition before the prevalence of deep learning
[12, 17, 47, 48, 49]. Adding more contextual information
by evolving the relation information will relieve the above

9298

kthpropagate weight1stpropagate weight(a) Handcraft Knowledge Graph(b) Fully Connected Graph(c) Spatial-aware Sparse Graph(d) Propagation with Gaussian Kernelsdistancedistance……11manmanhorsegoathorsecardrivermanmanhorsegoathorsecardriverproblems. Therefore, a crucial challenge for large-scale ob-
ject detection is how to capture and unify semantic and spa-
tial relationships and boost the performance.

With the advancement of geometric deep learning, us-
ing graph seems to be the most appropriate way to model
relation because of its ﬂexible structure of modeling pair-
wise interaction. Figure 1 gives an illustration of differ-
ent choice of design a graph to encode pairwise relation for
the task of detection. Figure 1a uses handcraft linguistic
knowledge [24, 36, 23, 6] to build a class-to-class graph.
For example, Jiang et al. [23] recently try to incorporate
semantic relation reasoning in large-scale detection by dif-
ferent kinds of knowledge forms. However, their method
heavily relied on the annotations of attribution and relation-
ship from VisualGenome data. Moreover, some spatial re-
lations may be ignored and the ﬁxed graph cannot adapt to
the image due to the semantic gap between linguistic and
visual context. (e.g. “goat” is alone). On the other hand,
some works [34, 5, 20, 51] try to implicitly learns a fully-
connected graph between regions from visual features as
shown in Figure 1b. For example, Hu et al. [20] introduced
the Relation Networks which use an adapted attention mod-
ule to allow interaction between the object’s visual features.
However, their fully-connected relation is inefﬁcient and
noisy by incorporating redundant and distracted relation-
ships/edges from irrelevant objects and backgrounds while
the pairwise spatial information is also not fully utilized.
Moreover, it is not clear what is learned in the module as
mentioned in their paper. Thus, our work aims to develop a
graph-based network which can model relation with aware-
ness of the spatial information as well as efﬁciently learn-
ing an interpretable sparse graph structure directly from the
training images. Figure 1c shows that a spatial-aware sparse
graph is learned by our method, leveraging both semantic
and spatial relationship.

In this paper, we propose a novel spatial-aware graph
relation network (SGRN) for large-scale object detection.
Our network simply consists of two modules: one sparse
graph learner module and one spatial-aware graph convolu-
tion module. Instead of building category-to-category graph
[7, 40], the proposal regions are deﬁned as graph nodes. A
sparse graph structure is learned via a relation learner mod-
ule. This not only identiﬁes the most relevant regions in the
image which can help to recognize the object in the image
but also avoiding unnecessary overhead with the negative
regions. Spatial-aware graph convolutions driven by learn-
able spatial Gaussian kernels is then performed to propagate
and enhance the regional context representation. The design
of Gaussian kernels in graph convolutions allows the graph
propagation to be aware of different spatial relationship as
shown in Figure 1c. Finally, each region’s new enhanced
context are concatenated to the original feature to improve
the performance of both classiﬁcation and localization in an

end-to-end style. Our method is in-place and easily plugged
into any existing detection pipeline for endowing its ability
to capture and unify semantic and spatial relationships.

Our SGRN thus enables adaptive graph reasoning over
regions with an interpretable learned graph (see Figure 4).
Both the contextual information and spatial information is
distilled and propagated through the graph efﬁciently. The
problem of imbalanced categories can then be alleviated by
sharing essential characteristics among frequent/rare cate-
gories. Also, the recognition of difﬁcult regions with heavy
occlusions, class ambiguities and tiny-size problems can be
thus remedied by the enhanced context information of re-
lated regions. Moreover, our method has shown great do-
main transferability by reusing the graph learner and rea-
soning module as shown in the Section 4.5.

The proposed SGRN outperforms current state-of-art de-
tection methods without adding any additional informa-
tion, i.e., [30], Faster R-CNN [45], Relation Network [20],
HKRM [23] and RetinaNets [31]. Consistent improvements
on the base detection network FPN and Faster R-CNN on
several object detection benchmarks have been observed,
i.e., VG [25] (1000/3000 categories), ADE [53](445 cat-
egories) , MS-COCO [32](80 categories).
In particular,
SGRN achieves around 32% of mAP improvement on VG
(3000 categories), 14% on VG (1000 categories), 28% on
ADE, 8% on MS-COCO.

2. Related Work

Object Detection.

Object detection is a core problem in computer vision. Big
progress has been made in recent years due to the usage of
CNN (backbone such as Resnet 101 [18]). Modern object
detection methods can usually be categorized in two groups:
1) two-stage detection methods: Faster R-CNN [45], R-
FCN [8], FPN [30]. They use a Region Proposal Network
to generate regions of interests in the ﬁrst stage and then
send the region proposals down the pipeline for object clas-
siﬁcation and bounding-box regression. 2) one-stage detec-
tion methods such as SSD [33] and YOLO [43]. They use
regression by taking an input image and learning the class
probabilities and bounding box coordinates. Such models
reach lower accuracy rates but are much faster than two-
stage object detectors. However, the number of categories
being considered usually is small: 20 for PASCAL VOC
[11] and 80 for COCO [32]. Also, those methods are usu-
ally performed on each proposal individually without con-
sidering the relationship between regions.

Visual Reasoning.

Visual reasoning aims to combine different information or
interactions between objects or scenes. Examples can be

9299

Figure 2. An overview of the proposed SGRN. Our method can be stacked on any modern detection network. SGRN encodes the relation
between regions as an undirected graph. The relation graph learner module ﬁrst learns a sparse adjacency matrix from the visual feature
which retains the most relevant connections. Then the weights of the previous classiﬁcation layer are collected and soft-mapped to the
regions to become visual embeddings of each regions. The pairwise spatial information between regions (distance, angle) is feed into
Gaussian kernels to determine the patterns of graph convolution.
In the spatial-aware graph reasoning module, visual embedding of
different regions are evolved and propagated according to the sparse adjacency matrix and the Gaussian kernels. The output of spatial
graph reasoning module is then concatenated to the original region features to improve both classiﬁcation and localization.

found in the task of classiﬁcation [36, 2], object detection
[6, 20, 23] and visual relationship detection [7]. Early works
usually involve handcraft relationships or shared attribute
among objects [1, 2, 26, 37]. For example, [14, 35, 44] rely
on ﬁnding similarity such as the attributes in the linguis-
tic space. [13, 15, 39] use object relationships as a post-
processing step. Recent works consider a graph structure
[6, 7, 24, 36] to incorporate external knowledge for various
[10] employed a label relation graph
tasks. Deng et al.
[6] leverage lo-
to guide the classiﬁcation. Chen et al.
cal region-based reasoning and global reasoning to facili-
tate object classiﬁcation. However, their method heavily re-
lied on external handcraft linguistic knowledge (word em-
bedding). Those handcraft graph may not be appropriate
because of the gap between linguistic and visual context.
Other works [20, 34, 41] encode the relation in an implicit
way. Liu et al. [34] proposed Structure Inference Network
(SIN) which learns a fully-connect graph implicitly with
stacked GRU cell to encode the message. However, the us-
age of fully-connected-graph allows redundant information
ﬂow and make the GRU cell less efﬁcient which leads to
a low reported performance (mAP: 23.2% on MSCOCO).
By contrast, our SGRN learns a sparse relation graph which
can be used to facilitate our spatial-aware GCN module.

Graph Convolutional Neural Networks

Graph CNNs (GCNs) aims to generalize Convolutional
Neural Networks (CNNs) to graph-structured data. Ad-
vances in this direction are often categorized as spec-
tral approaches and spatial approaches. Spectral GCNs
[9, 24] use analogies with the Euclidean domain to deﬁne a
graph Fourier transform, allowing to perform convolutions
in the spectral domain as multiplications. Spatial GCNs

[3, 38, 50] deﬁne convolutions with a patch operator di-
rectly on the graph, operating on groups of node neigh-
[38] presented mixture model CNNs
bors. Monti et al.
(MoNet), a spatial approach which provides a uniﬁed gener-
alization of CNN architectures to graphs. Graph Attention
Networks [50] model the convolution operator as an atten-
tion operation on node neighbors. Inspired by those work,
our model also deﬁned the graph convolution as a mixture
model. However, while these methods which learn a ﬁxed
graph structure, we aim to learn a dynamic adaptive graph
for each image which can leverage the information between
co-occurrence and locations of objects.

3. The Proposed Approach

3.1. Overview

An overview of SGRN can be found in Figure 2. We de-
velop a spatial-aware graph relation network which can be
implemented on any modern dominant detection system to
further improve their performance. In our network, the re-
lation is formulated as a region-to-region undirected graph
G : G =< N , E >. The relation graph learner module
ﬁrst learns a interpretable sparse adjacency matrix from the
visual feature which only retains the most relevant connec-
tions for recognition of the objects. Then the weights of the
previous classiﬁcation layer are collected and soft-mapped
to the regions to become visual embeddings of each region.
The pairwise spatial information between regions (distance,
angle) is calculated and feeds in Gaussian kernels to deter-
mine the patterns of graph convolution. In the spatial-aware
graph reasoning module, visual embeddings of different re-
gions are evolved and propagated according to the sparse
adjacency matrix and the Gaussian kernels. The output of
the spatial graph reasoning module is then concatenated to

9300

…ImageFC clsFC bboxClassifier Weights BackboneAdjProposalsFeatureRefined Proposals FeatureSpatial Graph ReasoningFeatureEncoderRelation Learner…FCLearnable Gaussian KernelWeighted Propagate EdgeRegion NodeEnhanced Proposals FeatureSparse Adjacency MatrixLatentvectorVisualEmbedding…RoIAlignthe original region features to improve both classiﬁcation
and localization.

3.2. Relation Learner Module

This module aims to produce a graphical representation
of the relationship between proposal regions which is rel-
evant to the object detection. We formulate the relation as
a region-to-region undirected graph G : G =< N , E >,
where each node in N corresponds to a region proposals
and each edge ei,j ∈ E encodes relationship between two
nodes.. We then seek to learn the E ∈ RNr×Nr thus the
node neighborhoods can be determined.

Formally, given the regional visual features of D di-
mension extracted from the backbone network for re-
gion proposals, we use the regional visual features f =
{f i}Nr
i=1, fi ∈ RD as the input of our module. We ﬁrst
transform the visual features to a latent space Z by non-
linear transformation denoted by

zi = φ(f ), i = 1, 2, ..., Nr

(1)

, where zi ∈ RL, L is the dimension of the latent space
and φ(.) is a non-linear function. In this paper, we consider
two fully-connected layers with ReLU activation as the non-
linear function φ(.). Let Z ∈ RNr×L be the collection of
{zi}Nr
i=1, zi ∈ RL, the adjacency matrix for the undirected
graph G with self loops can be then calculated by a matrix
multiplication as E = ZZT , so that ei,j = zizT
j .

Note that a lot of background (negative) samples ex-
ist among those Nr region proposals. Using a fully con-
nected adjacency matrix E will establish relationship be-
tween backgrounds(negative) samples. Those redundant
edges will lead to greater computation cost. Moreover,
the subsequent spatial-aware graph convolution will over-
propagate the information and the output of the graph con-
volution will be the same for all nodes. To solve this prob-
lem, we need to impose constraints on the graph sparsity.
For each region proposal i, we only retain the top t largest
value of each row of E.
In other words, most t relevant
nodes are picked as the neighbourhood of each region pro-
posal i:

Neighbour(Node i) = Top-tj=1,..,Nr (ei,j)

. This ensures a spare graph structure focusing on the most
relevant relationship for recognition of the objects.

3.3. Visual Embeddings of the Regions

Most existing graph-based approaches [24, 36, 7, 6]
propagate visual features locally among regions in each im-
age according to the edges. However, their methods will fail
when the regional visual features are poor thus the propaga-
tion is inefﬁcient or even wrong. Note that this situation
often happens in large-scale detection when heavy occlu-
sions and ambiguities exist in the image. To alleviate this

problem, our method tries to propagate information globally
over all the categories. In other words, our method needs to
create a high-level semantic visual embedding for each cat-
egory which can be regarded as an ideal prototype for one
particular object category.

In some zero/few-shot problems, they [47, 52, 16] use
the classiﬁer’s weights as the embedding or representation
of an unseen/unfamiliar category, we try to use the weights
as the visual embedding for each category. This is due to
the fact that the weights of the classiﬁer actually contains
high-level semantic information since they record the fea-
ture activation trained from all the images. Formally, let
W ∈ RC×(D+1) denotes the weights (parameters) of the
previous classiﬁers where C is the number of categories
and D is the dimension of the visual features. The category
visual embedding can be obtained by copying the parame-
ters W (including the bias) from the previous classiﬁcation
layer of the base detection networks. Note that the W are
updated during training so that our visual embeddings are
more accurate through time. Furthermore, our model can
be trained in an end-to-end fashion which avoids taking av-
erage or clustering across all the dataset [27].

Since our graph G is a region-to-region graph, we need
to ﬁnd most appropriate mappings from the category vi-
sual embedding w ∈ W to the regional representations of
nodes xi ∈ X (the input of our spatial graph reasoning
module). Chen et al. [6] suggest a hard-mapping which di-
rectly uses the one-to-one previous classiﬁcation results as
the category-to-region mapping. However, their mapping
will be wrong if previous classiﬁcation results is wrong. In-
stead, we use a soft-mapping which compute the mapping
weights mw→xi ∈ Ms as mw→xi = exp(sij )
Pj exp(sij ) , where sij
is the classiﬁcation score for the region i towards category
j from the previous classiﬁcation layer of the base detector.
Thus the input X ∈ RNr×(D+1) of our spatial-aware graph
reasoning module can be computed as X = MsW,where
Ms ∈ RNr×C is the soft-mapping matrix.

3.4. Spatial aware Graph Reasoning Module

Based on the regional input (nodes) X ∈ RNr×(D+1)
and the learned graph edges E ∈ RNr×Nr , the graph rea-
soning guided by the edges is employed to learn a new ob-
ject representations for further enhancement of classiﬁca-
tion and localization. As the locations of the regions in the
image are also crucial for the graph reasoning, spatial in-
formation should also be considered in our graph reason-
ing module. Here, we introduce our Spatial-aware Graph
Reasoning Module to use Graph Convolutional Neural Net-
works (GCN) for modeling the relation and interaction co-
herently with spatial information.

To capture the pairwise spatial information, we use
a pairwise pseudo-coordinate function u(a, b) which de-
ﬁnes, for each node a, u(a, b) will returns the coordi-

9301

3.5. SGRN for Multiple Domains

In recent years, a few open large detection datasets have
appeared with a different number of categories. For exam-
ple, MSCOCO has 80 categories and VisualGenome has
3000 categories. However, detectors are typically trained
under full supervision and have to be retrained when fac-
ing new dataset or new categories. This is tedious and very
time-consuming. Since our relation graph learner module
and spatial-aware graph reasoning module can be reused in
different datasets, we are particularly interested in the do-
main transferability of SGRN. Speciﬁcally, to train a new
model on a new dataset, we ﬁrst copy all the parameters
including SFRN from the model trained from the source
dataset except the bbox regression and classiﬁcation layer.
The weights Wsource of the bbox regression and classi-
ﬁcation layer can be transformed to the target dataset by
Wtarget = ΓWsource, where Γ ∈ RCtarget×Csource. The
Γ is a transform matrix which can be obtained by calcu-
lating the cosine distance between the category name word
embedding [2, 16]. Experiments of transferring from mul-
tiple datasets the can be found in Section 4.5. Our SGRN
shows great transfer capability which can be used to shorten
the training schedule.

4. Experiments

4.1. Datasets and Evaluation.

We ﬁrst conduct experiments on large-scale object de-
tection benchmarks with a large number of classes: Visual
Genome (VG) [25] and ADE [53]. Note that these two
datasets have long-tail distributions. The task is to local-
ize an object and classify it, which is different from the ex-
periments with given ground truth locations in Chen et al.
[6]. For VG, we use the synsets [46] instead of the raw
names of the categories due to inconsistent label annota-
tions, following [21, 6, 23]. We consider two set of target
classes: 1000 most frequent classes and 3000 most frequent
classes:VG1000 and VG3000. We split the remaining 92960
images with objects on these class sets into 87960 and 5,000
for training and testing, following [23]. For ADE dataset,
we use 20,197 images for training and 1,000 images for
testing with 445 categories, following [6, 23]. Since ADE
is a segmentation dataset, we convert segmentation masks
to bounding boxes [6] for all instances.

Moreover, we are curious about whether our SGRN also
works on a smaller scale dataset (fewer categories) so that
experiments are also conducted on common object detec-
tion datasets: MSCOCO [32] with 80 classes. MSCOCO
2017 contains 118k images for training, 5k for evaluation
(also denoted as minival) as common practice [20, 31, 29].
For all the evaluation, we adopt the metrics from COCO
detection evaluation criteria [32], that is, mean Average Pre-
cision (mAP) across different IoU thresholds (IoU= {0.5 :

9302

Figure 3. Flowchart of the Spatial-aware Graph Reasoning Mod-
ule. The relation learner module learns a sparse adjacency matrix
E from the visual feature f . The classiﬁer weights W is soft-
mapped to regions as the Visual embedding X. Then the pairwise
pseudo coordinates u(i, j) are calculated and determine wk(.)
from the K Gaussian kernels with learnable means and covari-
ances. Finally, E, X, and wk(.) are feed into Graph Convolutional
Neural Networks(GCN) deﬁned by Equation (2). The output of
our Spatial-aware Graph Reasoning Module is concatenated to the
f to improve both classiﬁcation and localization.

nated of node b in that system. Naturally, the relative po-
sitions of each region (node) in the image can be recog-
nized as the pseudo-coordinate system. In this paper, we
use a polar function u(a, b) = (d, θ) which returns a 2-
d vector that calculates the distance and the angle of two
centers([ca, ya],[cb, yb]) of the region proposals a and b, e.g.
d = q(ca − cb)2 + (ya − yb)2 and θ = arctan(cid:16) yb−ya
cb−ca (cid:17) .

Then we need to formulate our spatial-aware graph rea-
soning by deﬁning a patch operator to describe the inﬂuence
and propagation of each neighboring node in the graph.
Similar to MoNet [38], we deﬁne the patch operator by a
set of K Gaussian kernels of learnable means and covari-
ances. Formally, given the regional semantic input xj ∈ X
and the graph structure G =< N , E > , the patch operator
at each kernel k for node i is given by:

f′k(i) = X

j∈Neighbour(i)

wk(u(i, j))xjeij,

(2)

1
2

Σ−1

(u(i, j) − µk)T

where Neighbour(i) denotes the neighborhood of node i
and wk(.) is the kth Gaussian kernel:
k (u(i, j) − µk)(cid:19) ,
wk(u(i, j)) = exp(cid:18)−
and µk and Σk are learnable 2 × 1 mean vector and 2 × 2
covariance matrix, respectively. For each node i, f′k(i) is a
weighted sum of the neighbouring semantic representation
X and the Gaussian kernel wk(.) encodes the spatial in-
formations of regions. Then f′k(i) for each node is concate-
nated over K kernels and go through a linear transformation
L ∈ RE×(D+1): hi = L[f′(i)], and E is the dimension of
the output enhanced feature for each region. Finally, the hi
for each region is concatenated to the original region fea-
tures f i to improve both classiﬁcation and localization.

f𝑍=𝜙(.)ℰ=𝑍𝑍𝑇𝐺𝐶𝑁1(ℰ,X,𝑤1)…FCClassifier Weights WBboxPredictionPseudo Coordinate𝑢(𝑖,𝑗)Soft MappingVisual Embedding X𝑋𝐺𝐶𝑁𝐾(ℰ,X,𝑤𝐾)Gaussian kernel𝑤1(𝑢(.))Gaussian kernel𝑤1(𝑢(.))Gaussian kernel𝑤1(𝑢(.))𝜃𝑑Concat% Method

AP
6.2
6.5
7.8
5.7

AR1
14.6
15.3
18.1
13.8

APL
9.8
11.2
12.7
8.9

ARL
25.3
27.5
31.4
23.5

ARM
17.1
19.2
20.8
15.8

AR10
18.0
19.4
22.7
17.0

AR100
18.7
19.5
22.7
17.0

ARS
7.2
6.1
9.6
6.6

AP50
10.9
12.1
13.4
9.9

AP75
6.2
6.1
8.1
5.8

APS
2.8
2.4
4.1
2.7

APM
6.5
6.9
8.1
6.9

7.9

4.2

7.3

7.1

20.0

19.8

14.9

10.7

12.9

3.0
3.8
4.3
2.6

8.1+0.9 13.6+0.7 8.4+1.1 4.4+0.2 8.2+0.3 12.8+2.1 19.5+4.6 26.0+6.2 26.2+6.2 12.4+1.3 23.9+4.6 34.0+10.4

Light-head RCNN[28]
Cascade RCNN[4]
HKRM[23]
Faster-RCNN[45]
Faster-RCNN w SGRN 6.8+1.1 11.1+1.2 7.1+1.3 3.3+0.6 7.0+0.1 10.8+1.9 15.3+1.5 19.5+2.5 19.6+2.6 8.3+1.7 17.8+2.0 26.7+3.2
FPN[30]
FPN w SGRN
Light-head RCNN[28]
Cascade RCNN[4]
HKRM[23]
Faster-RCNN[45]
Faster-RCNN w SGRN 3.2+0.6 5.0+0.6 3.4+1.3 2.0+0.3 4.2+0.6 6.5+1.7 7.3+0.9 9.2+1.6 9.2+1.6 4.9+0.6 11.4+1.7 16.2+3.3
FPN[30]
FPN w SGRN
Light-head RCNN[28]
Cascade RCNN[4]
HKRM[23]
Faster-RCNN[45]
Faster-RCNN w SGRN 9.5+2.6 15.3+2.5 10.1+3.3 4.9+1.8 8.4+2.0 16.0+3.7 12.5+3.2 17.6+4.3 17.7+4.1 8.4+0.5 16.0+2.6 27.3+6.8
FPN[30]
FPN w SGRN

14.0+3.1 23.1+2.1 14.8+2.8 8.1+0.8 13.7+1.6 21.4+3.0 16.5+3.0 25.5+5.2 26.2+5.3 17.7+4.4 27.5+5.6 35.3+6.3

4.5+1.1 7.4+1.3 4.3+1.0 2.9+0.3 6.0+1.2 8.6+2.3 10.8+3.9 13.7+4.6 13.8+4.7 8.1+1.4 15.1+3.6 21.8+8.4

13.4
16.6
18.5
13.6

13.3
16.4
18.3
13.3

11.7
16.8
18.0
12.8

11.2
15.3
16.8
12.3

10.3
9.9
13.0
9.1

15.4
13.7
20.5
12.9

10.4
13.8
15.5
13.4

20.4
25.8
28.4
20.5

7.0
9.1
10.3
6.9

7.3
8.9
10.4
6.8

9.6
12.1
13.6
9.3

7.3
7.1
10.1
6.2

9.0
8.5
12.2
7.6

9.0
8.6
12.2
7.6

2.4
3.5
4.1
3.1

5.1
7.1
7.8
6.4

4.3
4.2
5.9
4.3

4.3
6.4
7.1
7.9

5.1
6.5
7.2
4.4

3.2
3.4
4.4
2.7

1.7
1.9
2.6
1.7

4.0
4.8
5.5
3.6

5.8
4.9
8.4
4.8

10.9

21.0

12.0

12.1

18.4

13.5

20.3

20.9

13.3

21.9

29.0

11.1

19.3

23.6

13.4

11.5

7.3

3.4

6.1

3.4

2.6

4.8

6.3

6.9

9.1

9.1

6.7

0
0
0
1
G
V

0
0
0
3
G
V

E
D
A

Table 1. Main results of test datasets on VG1000(Visual Genome), VG3000 and ADE445. “w SGRN” is the baseline model Faster-RCNN
[45] and FPN [30] adding the proposed SGRN method. Note that comparison of HKRM [23] is not fair since their method here used the
relation and attribute annotations of Visual Genome.

0.95, 0.5, 0.75}) and scales (small, medium, big). We also
use Average Recall (AR) with different number of given de-
tection per image ({1, 10, 100}) and different scales (small,
medium, big).

4.2. Implementation Details.

We conduct all experiments using Pytorch [42], 8 Tesla
V100 cards on a single server. ResNet-101 [18] pretrained
on ImageNet [46] is used as our backbone network. We use
two widely-adopted state-of-the-art detection methods i.e.
Faster-RCNN [45] and FPN [30] as our baselines.

Faster-RCNN [45]. The hyperparameters in training
mostly follow [45]. The parameters before conv2 are ﬁxed,
same with [28]. During training, we augment with ﬂipped
images and multi-scaling (pixel size={600 ∼ 1000}). Dur-
ing testing, pixel size= 600 is used. Following [45], RPN
is applied on the conv4 feature maps. The total number
of proposed regions after NMS is Nr = 128. Features in
conv5 are avg-pooled to become the feature vector for each
proposed regions and feed directly into the bbox regression
and classiﬁcation layers. We use the ﬁnal conv5 for 128
regions after avg-pool (D= 2048) as the proposals visual
features for our relation learner module’s inputs. Class ag-
nostic bbox regression is not adopted.

FPN[30]. The hyper-parameters in training mostly fol-
low [30]. During testing, pixel size= 800 is used. RPN
is applied to all the feature maps. The total number of
proposed regions after NMS is Nr = 512. Features in

conv5 are avg-pooled to become the proposals visual fea-
tures (D= 512) as our relation learner module’s inputs. In
the bbox-head, 2 shared FC layer is used for the propos-
als visual features and the output is a 1024-d vector feed
into the bbox regression and classiﬁcation layers. Class ag-
nostic bbox regression is adopted. Unless otherwise noted,
settings are same for all experiments.

SGRN. In the relation learner module, we use two linear
layers of size 256 to learns the latent Z (L = 256) in Equa-
tion (1) and most t = 32 relevant nodes is retained. Visual
embeddings of the regions are collected from the classiﬁ-
cation layer (2048-d for Faster RCNN, 1024-d for FPN).
For the spatial-aware graph reasoning module, we use two
spatial-aware graph convolution layers with dimensions 512
and 256 so that the output size of the module for each region
is E = 256. All linear layers are activated using Rectiﬁed
Linear Unit (ReLU) activation functions.

For all training, stochastic gradient descent(SGD) is per-
formed on 8 GPUs with 2 images on each. The initial learn-
ing rate is 0.02, reduce three times (×0.01) during ﬁne-
tuning; 10−4 as weight decay; 0.9 as momentum. For all
the datasets, we train 24 epochs for both baselines (Further
training after 12 epochs won’t increase the performance of
baseline). For our SGRN, we use 12 epochs of the base-
line as the pretrained model and train another 12 epochs
with same settings with baseline. We also implement and
compare the methods in Table 1 using the publicly released
code. For fair comparisons, we do not use soft-nms or on-

9303

Figure 4. Examples of the learned graph structures from our
SGRN. The centers of regions are plotted and connected by the
learned graph edges. Edges thickness correspond to the strength
of the graph edge weights.

% Method
SIN [34]
Relation Network[20]
Spatial Memory Network[5]
RetinaNet[31]
DetNet[29]
IoUNet[22]
HKRM[23]
FPN[30]
FPN w SGRN

O
C
O
C
S
M

mAP
23.2
38.8
31.6
39.1
40.2
40.6
37.8
38.3

AP50
44.5
60.3
52.2
59.1
62.1
59.0
58.0
60.5

AP75
22.0
42.9
33.2
42.3
43.8
49.0
41.3
42.0

41.7+3.4 62.3+1.8 45.5+3.5

Table 2. Comparison of mean Average Precision (mAP), AP50 and
AP75 on MSCOCO. “FPN w SGRN” is the the proposed SGRN
method based on FPN[30]. The accuracy number except FPN and
our method are directly from the original paper.

line hard example mining (OHEM) in all experiments.

4.3. Comparison with state of the art

Large-scale Detection Benchmarks. We ﬁrst evaluate
our SGRN method on large-scale detection benchmarks:
VG1000 (Visual Genome with 1000 categories), VG3000
with 3000 categories and ADE with 445 categories. Ta-
ble 1 shows the result of our method SGRN added on
the baseline model Faster-RCNN [45] and FPN [30]. Our
SGRN achieves signiﬁcant gains on both classiﬁcation and
localization accuracy than the baseline on all the large-
scale detection benchmarks. Our method achieve an over-
all AP of 8.1% compared to 7.2% by FPN on VG1000,
4.5% compared to 3.4% on VG3000, and 14.0% compared
to 10.9% on ADE, respectively. From Table 1, it can be
found that our method can boost the average precision of
0.6% to 3.0%. Our method works mostly on the large item
(APL:+2% ∼ 3%). Furthermore, our SGRN is able to im-
prove upon FPN by a margin of 3%~10% on average recall
(e.g. ARL:+10.4% for VG1000). This demonstrates that
our method can improve both the accuracy and false dis-
covery rate by the spatial-aware graph reasoning. We also
compare our method to Light-head RCNN [28], Cascade-

RCNN [4] and HKRM [23] in Table 1. Note that the method
of HKRM uses the relation and attribute annotation of Vi-
sual Genome so that the comparison is not fair. From the
table, the SGRN outperforms other competing methods by
a large margin (relatively 10%~80%).

Figure 4 shows visual examples of the learned graph
structures from our SGRN. The centers of regions are plot-
ted and connected by the learned graph edges. Edges thick-
ness correspond to the strength of the graph edge weights.
Our method learned interpretable edges between regions.
For example, objects with the same category are connected
such as “books”, “people”, “cars”. Their visual features
thus are shared and enhanced. Furthermore, objects with
semantic relationship or co-occurrence are connected e.g.
“mouse” and “laptop”; “umbrella” and “person”; “bicy-
cle” and “car” and “cellphone” and “people”. The correct
learned edges help the successful graph learning thus lead
to better detection performance. Figure 5 also shows the
qualitative comparisons of the baseline method FPN and
our SGRN on Visual Genome with 1000 categories. Our
method is more accurate than the baseline method due to the
help of the encoded relationship. For example, SGRN can
detect the “doughnut”, “catcher”, “food” and “sign” while
FPN cannot. FPN detects some false positive bbox such as
“paw” and “leg” in the cat image.

Common Detection Benchmark. We further investi-
gate the performance of our SGRN a smaller scale dataset
(fewer categories). Table 2 shows the performances on the
minival of the MSCOCO dataset with 80 categories. To
compare with the state-of-art methods, we also report the
accuracy numbers of SIN [34], Spatial Memory Network
[5], Relation Network [20], RetinaNet [31], DetNet [29],
IoUNet [22] and HKRM [23] directly from the original pa-
per. Note that our implementation of the baseline FPN has
higher accuracy than that in original works (38.3 vs 36.2
[30]). As can be seen, our method performs 3.4% better
than the baseline FPN and all the other competitors. This
demonstrates that our SGRN method can also work on the
dataset with a smaller number of categories by improving
the feature representation due to its ability of relation graph
reasoning.

4.4. Ablative Analysis

To perform a detailed ablative analysis, we have con-
ducted experiments with FPN baseline. Table 3 shows the
effectiveness of different components in our model on ADE:
1) The graph convolution of visual embedding is the most
important component of our method, accounting for 1.9%
improvement on FPN. 2) Reason with a fully connected
graph shows redundant edges lead performance drop, while
the sparse connection can improve the mAP by around
0.3%. 3) The design of spatial Gaussian Kernels can im-
prove the mAP by 0.5%. 4) Using soft-mapping Ms in-

9304

FPN

FPN with SGRN

Figure 5. Qualitative comparisons of the baseline method FPN and our SGRN on VG1000. Our method is more accurate than the baseline
method due to the help of the encoded relation.

stead of hard-mapping can further increase the overall AP
by 0.4%.

GCN-Visual

Sparse

Spatial

Soft-

Results on ADE

%

FPN

Embedding Connection Gaussian Mapping

AP

10.9

AP50

21.0

AR1

13.5

AR10

20.3

√
√
√
√
√
SGRN √

√
√
√

√

√

√
√

12.8+1.9 21.9+0.9 14.5+1.0 23.5+3.2
13.1+2.2 21.6+0.6 15.4+1.9 23.9+3.6
13.6+2.7 23.0+2.0 15.1+1.6 24.3+4.0
√ 13.4+2.5 21.8+0.8 16.0+2.5 23.7+3.4
√ 13.7+2.8 22.5+1.5 16.2+2.7 24.7+4.4
√ 14.0+3.1 24.1+2.1 16.5+3.0 25.5+5.2

Table 3. Ablation Study based on different components of our
model on ADE. Our ﬁnal model SGRN is reported in the last line.
The backbones are ResNet-101 with FPN.

4.5. Domain Transferability of SGRN

We are interested in the domain transferability of SGRN
e.g. transferring the trained SGRN model between multiple
datasets. Table 4 shows the comparison of our transferred
model and the baseline FPN. We transfer our trained model
from the source dataset to the target dataset by the method
mentioned in Section 4.5. We only train our SGRN for an-
other 5 epochs for adaption to the new dataset. From the
table, it can be found that our SGRN can perform better
than the baseline FPN even we only trained the model for 5
epochs. And we also try FPN transferred from MSCOCO
to ADE with the same setting of SGRN and trained for
5 epochs.
Its mAP is only 8.6% while our SGRN can
reach the same mAP with only 1 epoch training. Addi-
tional experiments in Table 4 shows the results of SGRN
with frozen all layers except the weight transfer layers, de-
noted as Frozen-SGRN. The performance only suffers a mi-
nor drop comparing to the original results. If we froze the
same layers for FPN based on ImageNet pretrained back-
bone (indicated as Frozen-FPN) the results deteriorate. This
demonstrates the effectiveness of the domain transferability
by reusing the graph learner and graph reasoning module.

Method

FPN

SGRN

SGRN

SGRN

SGRN

Target Source Training

Dataset Dataset Epochs

VG3000

-

12

VG3000 VG1000

VG3000 VG1000

VG3000 COCO

VG3000 COCO

1

5

1

5

AP AP50 AP75 AR1 AR10 AR100

3.4

3.4

4.6

2.1

3.4

6.1

5.7

7.5

3.7

5.7

3.4

3.5

4.8

2.2

3.5

6.9

7.4

9.1

9.6

9.1

9.6

10.2 13.5

13.5

4.7

6.6

7.8 10.7

Frozen-FPN

COCO

-

12

28.7 50.7 28.6 25.6 42.5

Frozen-SGRN COCO VG1000

Frozen-SGRN COCO VG1000

1

5

33.1 52.3 36.5 29.5 45.0

37.7 57.8 40.8 31.9 50.6

FPN

SGRN

SGRN

FPN

FPN

SGRN

SGRN

COCO

-

12

38.3 60.5 42.0 32.1 50.8

COCO VG1000

COCO VG1000

1

5

33.0 51.3 36.5 29.5 45.9

38.9 58.7 42.6 32.9 53.9

57.2

Frozen-FPN

ADE

-

12

7.8 16.1

Frozen-SGRN ADE VG1000

Frozen-SGRN ADE VG1000

1

5

7.7

7.0

10.0 15.5

8.4 12.4

6.7 11.3

10.9 17.4 11.8 13.5 20.1

-

12

10.9 21.0 12.0 13.5 20.3

ADE

ADE

COCO

ADE VG1000

ADE VG1000

5

1

5

8.6 16.5 10.1 10.7 16.6

8.6 15.3

9.3

11.2 16.0

12.9 20.4 13.8 16.1 22.8

23.2

6.7

10.8

45.4

46.4

35.1

35.4

47.3

16.0

12.5

20.5

20.9

17.1

16.2

5

23.1

ADE

COCO

11.9 19.3 13.0 14.3 22.4

SGRN
Table 4. Domain Transferability of our SGRN. FPN is the im-
plemented baseline method with backbone pretrained from Ima-
geNet. We transfer the trained SGRN model between multiple
datasets by the method in Section 4.5. We only train our SGRN
for another 5 epochs for adaption to the new dataset. The result of
training with frozen all layers except the weight transfer layers is
also reported denoted as Frozen-SGRN.

5. Conclusions

In this work, we proposed a new spatial-aware graph
relation network (SGRN) for encoding object relation in
the detection system without any external knowledge. Our
method is in-place and easily plugged into any existing de-
tection pipeline for endowing its ability to capture and unify
semantic and spatial relationships.

9305

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.
Label-embedding for attribute-based classiﬁcation. In
CVPR, 2013. 3

[2] J. Almazán, A. Gordo, A. Fornés, and E. Valveny.
Word spotting and recognition with embedded at-
tributes.
IEEE transactions on pattern analysis and
machine intelligence, 36(12):2552–2566, 2014. 3, 5

[3] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and
P. Vandergheynst. Geometric deep learning: going be-
yond euclidean data. IEEE Signal Processing Maga-
zine, 34(4):18–42, 2017. 3

[4] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving
into high quality object detection. In CVPR, 2018. 6,
7

[5] X. Chen and A. Gupta. Spatial memory for context

reasoning in object detection. In ICCV, 2017. 2, 7

[6] X. Chen, L.-J. Li, L. Fei-Fei, and A. Gupta.

Itera-
tive visual reasoning beyond convolutions. In CVPR,
2018. 2, 3, 4, 5

[7] B. Dai, Y. Zhang, and D. Lin. Detecting visual re-
lationships with deep relational networks. In CVPR,
2017. 2, 3, 4

[8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detec-
tion via region-based fully convolutional networks. In
NIPS, 2016. 2

[9] M. Defferrard, X. Bresson, and P. Vandergheynst.
Convolutional neural networks on graphs with fast lo-
calized spectral ﬁltering. In NIPS, 2016. 3

[10] J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy,
S. Bengio, Y. Li, H. Neven, and H. Adam. Large-
scale object classiﬁcation using label relation graphs.
In ECCV, 2014. 3

[11] M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. The pascal visual object
classes (voc) challenge. International Journal of Com-
puter Vision, 88(2):303–338, June 2010. 1, 2

[12] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. De-
In CVPR, 2009.

scribing objects by their attributes.
1

[13] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. Object detection with discriminatively
trained part-based models. IEEE transactions on pat-
tern analysis and machine intelligence, 32(9):1627–
1645, 2010. 3

[15] C. Galleguillos, A. Rabinovich, and S. Belongie. Ob-
ject categorization using co-occurrence, location and
appearance. In CVPR, 2008. 3

[16] C. Gong, D. He, X. Tan, T. Qin, L. Wang, and T.-Y.
Liu. Frage: Frequency-agnostic word representation.
In NIPS, 2018. 4, 5

[17] S. Gould, T. Gao, and D. Koller. Region-based seg-
mentation and object detection. In Advances in Neural
Information Processing Systems 22, 2009. 1

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In CVPR, 2016. 2, 6

[19] J. Hoffman, S. Guadarrama, E. S. Tzeng, R. Hu,
J. Donahue, R. Girshick, T. Darrell, and K. Saenko.
Lsda: Large scale detection through adaptation.
In
NIPS, 2014. 1

[20] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation
networks for object detection. In CVPR, 2018. 2, 3, 5,
7

[21] R. Hu, P. DollÃ¡r, K. He, T. Darrell, and R. Girshick.

Learning to segment every thing. In CVPR, 2018. 5

[22] B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang. Ac-
quisition of localization conﬁdence for accurate object
detection. In ECCV, 2018. 7

[23] C. Jiang, H. Xu, X. Liang, and L. Lin. Hybrid knowl-
edge routed modules for large-scale object detection.
In NIPS, 2018. 1, 2, 3, 5, 6, 7

[24] T. N. Kipf and M. Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In ICLR,
2017. 2, 3, 4

[25] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata,
J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A.
Shamma, M. Bernstein, and L. Fei-Fei.
Visual
genome: Connecting language and vision using
crowdsourced dense image annotations. International
Journal of Computer Vision, 2016. 1, 2, 5

[26] C. H. Lampert, H. Nickisch, and S. Harmeling. Learn-
ing to detect unseen object classes by between-class
attribute transfer. In CVPR, 2009. 3

[27] K.-H. Lee, X. He, L. Zhang, and L. Yang. Cleannet:
Transfer learning for scalable image classiﬁer training
with label noise. In CVPR, 2018. 4

[28] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun.
Light-head r-cnn: In defense of two-stage object de-
tector. In CVPR, 2017. 6, 7

[29] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun.
Detnet: A backbone network for object detection. In
ECCV, 2018. 5, 7

[14] A. Frome, G. S. Corrado, J. Shlens, S. Bengio,
J. Dean, T. Mikolov, et al. Devise: A deep visual-
semantic embedding model. In NIPS, 2013. 3

[30] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan,
and S. Belongie. Feature pyramid networks for object
detection. In CVPR, 2017. 2, 6, 7

9306

[31] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dol-
lár. Focal loss for dense object detection. IEEE trans-
actions on pattern analysis and machine intelligence,
2018. 2, 5, 7

[32] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft
coco: Common objects in context. In ECCV, 2014. 1,
2, 5

[33] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed,
C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox
detector. In ECCV, 2016. 2

[34] Y. Liu, R. Wang, S. Shan, and X. Chen. Structure
inference net: Object detection using scene-level con-
text and instance-level relationships. In CVPR, 2018.
2, 3, 7

[35] J. Mao, X. Wei, Y. Yang, J. Wang, Z. Huang, and A. L.
Yuille. Learning like a child: Fast novel visual con-
cept learning from sentence descriptions of images. In
ICCV, 2015. 3

[36] K. Marino, R. Salakhutdinov, and A. Gupta. The more
you know: Using knowledge graphs for image classi-
ﬁcation. In CVPR, 2017. 2, 3, 4

[37] I. Misra, A. Gupta, and M. Hebert. From red wine
to red tomato: Composition with context. In CVPR,
2017. 3

[38] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svo-
boda, and M. M. Bronstein. Geometric deep learning
on graphs and manifolds using mixture model cnns. In
CVPR, 2017. 3, 5

[39] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee,
S. Fidler, R. Urtasun, and A. Yuille. The role of con-
text for object detection and semantic segmentation in
the wild. In CVPR, 2014. 3

[40] M. Niepert, M. Ahmed, and K. Kutzkov. Learning
In ICML,

convolutional neural networks for graphs.
pages 2014–2023, 2016. 2

[41] W. Norcliffe-Brown, E. Vafeais, and S. Parisot. Learn-
ing conditioned graph structures for interpretable vi-
sual question answering. In NIPS, 2018. 3

[42] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer. Automatic differentiation in pytorch.
In
NIPS Workshop, 2017. 6

[43] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detec-
tion. In CVPR, 2016. 2

[44] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning
deep representations of ﬁne-grained visual descrip-
tions. In CVPR, 2016. 3

[45] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn:
Towards real-time object detection with region pro-
posal networks. In NIPS, 2015. 2, 6, 7

[46] O. Russakovsky,

J. Deng, H. Su,

J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, et al. Imagenet large scale visual recog-
nition challenge. International Journal of Computer
Vision, 115(3):211–252, 2015. 5, 6

[47] R. Salakhutdinov, A. Torralba, and J. Tenenbaum.
Learning to share visual appearance for multiclass ob-
ject detection.
In CVPR, pages 1481–1488. IEEE,
2011. 1, 4

[48] A. Torralba, K. P. Murphy, and W. T. Freeman. Shar-
ing features: efﬁcient boosting procedures for multi-
class object detection. In CVPR, 2004. 1

[49] A. Torralba, K. P. Murphy, W. T. Freeman, and M. A.
Rubin. Context-based vision system for place and ob-
ject recognition. In ICCV, 2003. 1

[50] P. Velickovic, G. Cucurull, A. Casanova, A. Romero,
P. Lio, and Y. Bengio. Graph attention networks. In
ICLR, 2017. 3

[51] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local

neural networks. In CVPR, 2018. 2

[52] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition
via semantic embeddings and knowledge graphs. In
CVPR, 2018. 4

[53] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and
A. Torralba. Scene parsing through ade20k dataset. In
CVPR, 2017. 2, 5

9307

