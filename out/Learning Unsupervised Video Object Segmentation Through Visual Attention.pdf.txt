Learning Unsupervised Video Object Segmentation through Visual Attention

Wenguan Wang ∗1

,

2, Hongmei Song ∗1, Shuyang Zhao 1,

Jianbing Shen †1

,

2, Sanyuan Zhao 1, Steven C. H. Hoi 3

,

4, Haibin Ling 5

1

Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China

2

Inception Institute of Artiﬁcial Intelligence, UAE

3Singapore Management University, Singapore

4

Salesforce Research Asia, Singapore

5Temple University, USA

wenguanwang.ai@gmail.com, shenjianbingcg@gmail.com

https://github.com/wenguanwang/AGS

Abstract

This paper conducts a systematic study on the role of
visual attention in the Unsupervised Video Object Segmen-
tation (UVOS) task. By elaborately annotating three popu-
lar video segmentation datasets (DAVIS16, Youtube-Objects
and SegTrackV 2) with dynamic eye-tracking data in the
UVOS setting, for the ﬁrst time, we quantitatively veriﬁed
the high consistency of visual attention behavior among hu-
man observers, and found strong correlation between hu-
man attention and explicit primary object judgements dur-
ing dynamic, task-driven viewing. Such novel observa-
tions provide an in-depth insight into the underlying ra-
tionale behind UVOS. Inspired by these ﬁndings, we de-
couple UVOS into two sub-tasks: UVOS-driven Dynamic
Visual Attention Prediction (DVAP) in spatiotemporal do-
main, and Attention-Guided Object Segmentation (AGOS)
in spatial domain. Our UVOS solution enjoys three ma-
jor merits: 1) modular training without using expensive
video segmentation annotations, instead, using more afford-
able dynamic ﬁxation data to train the initial video atten-
tion module and using existing ﬁxation-segmentation paired
static/image data to train the subsequent segmentation mod-
ule; 2) comprehensive foreground understanding through
multi-source learning; and 3) additional interpretability
from the biologically-inspired and assessable attention. Ex-
periments on popular benchmarks show that, even without
using expensive video object mask annotations, our model
achieves compelling performance in comparison with state-
of-the-arts.

∗Equal contribution.
†Corresponding author: Jianbing Shen. This work was supported in
part by the Beijing Natural Science Foundation under Grant 4182056, the
Fok Ying Tung Education Foundation under Grant 141067, and the Spe-
cialized Fund for Joint Building Program of Beijing Municipal Education
Commission.

Figure 1. Our UVOS solution has two key steps: Dynamic Visual
Attention Prediction (DVAP, §5.2) cascaded by Attention-Guided
Object Segmentation (AGOS, §5.3). The UVOS-aware attention
from DVAP acts as an intermediate video object representation,
freeing our method from the dependency of expensive video object
annotations and bringing better interpretability.

1. Introduction

Unsupervised Video Object Segmentation (UVOS), i.e.,
automatically segmenting primary object regions from the
background in videos, has been a long standing research
challenge in computer vision [29, 30, 12, 23], and has
shown potential beneﬁts for numerous applications, e.g., ac-
tion recognition [62] and object tracking [50]. Due to the
lack of user interactions in UVOS, it is very challenging
to automatically determine the primary foreground objects
from the complex background in real-world scenarios.

Deep learning has been actively explored for solving
UVOS recently. Despite having achieved promising results,
current deep learning based UVOS models [64, 45, 33, 67]
often rely on expensive pixel-wise video segmentation an-
notation data [86] to directly map input video frames into
corresponding segmentation masks, which are restricted
and generally lack of an explicit interpretation about the ra-

3064

tionale behind their choice of the foreground object(s). Sim-
ilar problems also has been experienced in a closely related
research area, video salient object detection (VSOD) [79],
which aims to extract a continuous saliency map for each
frame that highlights the most visually important area. An
biological interpretation for the choice of the salient object
regions is essential. The results from video salient object
detection are used as a vital cue or pre-processing step for
UVOS [64, 77].

In this paper, we emphasize the value of human visual
attention in UVOS (and its related task, video salient ob-
ject detection). According to studies in cognitive psychol-
ogy [39, 68, 82, 37], during visual perception, humans are
able to quickly orient attentions to the most important parts
of the visual stimuli, allowing them to achieve goals ef-
ﬁciently. We therefore argue that human visual attention
should be the underlying mechanism that drives UVOS. The
foreground in UVOS should be the object(s) that attracts hu-
man attention most, as the choice of the object(s) should be
consistent with human attention judgements.

To validate this novel hypothesis, we extend three pop-
ular video segmentation datasets, DAVIS16 [58], Youtube-
Objects [60] and SegTrackV 2 [44], with real human ﬁxation
annotation in the UVOS setting. The gaze data are collected
over a total of 190 video sequences with 25,049 frames from
20 human observers using professional eye-tracking instru-
ments (§3). To the best of our knowledge, this is the ﬁrst
attempt to collect UVOS-aware human attention data. Such
comprehensive datasets facilitate us to perform two essen-
tial experiments, i.e., quantifying the inter-subject consis-
tency and the correlation between human dynamic attention
and explicit object judgement (§4), in which two key obser-
vations are found from our quantitative analysis:
• There exist highly consistent attention behaviors among
human observers in the UVOS task, though the notion of
‘primary object(s)’ is sometimes viewed as ill-posed for
extremely-diverse dynamic scenes.

• There exists a strong correlation between human ﬁxation

and human explicit judgement of primary object(s).

These ﬁndings offer an insightful glimpse into the ratio-
nale behind UVOS from human attention perspective. In-
spired by this, we decompose UVOS into two sub-tasks:
dynamic visual attention prediction (DVAP) and attention-
guided object segmentation (AGOS). Accordingly, we de-
vise a novel UVOS model with two tightly coupled com-
ponents for DVAP and AGOS (see Fig. 1). One extra ad-
vantage of such task decomposition lies in modular train-
ing and data acquisition. Instead of using expensive video
segmentation annotation, the relatively easily-acquired dy-
namic ﬁxation data can be used to train DVAP, and ex-
isting large-scale ﬁxation-segmentation paired annotations
(e.g., [87, 47]) can be used to train the AGOS module.1 This

1Taking the DAVIS dataset as an example, it took several minutes per-

is because AGOS learns to map an individual input frame
and ﬁxation data to a segmentation mask, thus only needing
static image data. Roughly speaking, visual attention acts
as a middle-level representation that bridges dynamic fore-
ground characteristic modeling and static attention-aware
object segmentation. Such design naturally reﬂects real-
world human behavior, i.e., ﬁrst orienting rough attention
to important areas during dynamic viewing, and then focus-
ing on ﬁne-grained, pixel-wise object segmentation.

In our UVOS model, the DVAP module is built upon a
CNN-convLSTM architecture, where the convLSTM takes
static CNN feature sequence as input and learns to cap-
ture the dynamic visual attention, and the AGOS module
is based on an FCN architecture. Intuitively, DVAP informs
AGOS where the objects are located in each frame, then
AGOS performs ﬁne-grained object segmentation. Besides,
our model also enjoys several important characteristics:

• Fully-differentiable and supervised attention mechanism.
For AGOS, the attention from DVAP is used as a neu-
ral attention mechanism, thus the whole model is fully-
differentiable and end-to-end trainable. At high level,
DVAP can be viewed as an attention network, which pro-
vides an explicit spatiotemporal attention mechanism to
AGOS and is trained in a supervised manner.

• Comprehensive foreground understanding through learn-
ing on multi-source data and sharing weights. Our exper-
iments with dynamic gaze-tracking data conﬁrm a strong
correlation between eye movements and primary video
objects perception. Training with both ﬁxation and seg-
mentation data allows more comprehensive foreground
understanding. Moreover, by sharing several initial con-
volutional layers between DVAP and AGOS, information
can be exchanged efﬁciently.

• Learning from large-scale affordable data. Deep learn-
ing models are often hungry for large-scale data, but
a large video segmentation annotation data is very ex-
pensive. Our model leverages more affordable dynamic
gaze data and existing large-scale attention-segmentation
paired image data to achieve the same goal. Our experi-
ments show that our model yields promising segmenta-
tion results without training on the ground-truth video
segmentation data.

• Biologically-inspired and assessable interpretability.
The attention learned from DVAP not only enables our
model attend to the important object(s), but also offers
an extra dimension to interpret where our model focuses
on. Such interpretability is meaningful (biologically-
inspired) and assessable (w.r.t. human gaze records).

In summary, we propose a powerful, fully differentiable,
and biologically-inspired UVOS model that fully exploits

frame to annotation with 5 specialists, while with eye-tracker equipment,
annotating each frame only takes 1∼2 seconds.

3065

Dataset

Pub. Year #Videos #Viewers

Task

CRCNS [31]

TIP

2004

50

Hollywood-2 [52] TPAMI 2012 1,707
150
12

UCF sports [52] TPAMI 2012
2012

SFU [21]

TIP

DHF1K [75] CVPR 2018 1,000

DAVIS16 (Ours)
Youtube-Objects (Ours)
SegTrackV 2 (Ours)

-

2018
2018
2018

50
126
14

15
19
19
15
17
20
20
20

scene unders.
action recog.
action recog.

free-view
free-view

UVOS
UVOS
UVOS

Table 1. Statistics of dynamic eye-tracking datasets. Previous
datasets are either collected for bottom-up attention during free-
viewing or related to other tasks. By contrast, we extend exist-
ing DAVIS16 [58], Youtube-Objects [60], and SegTrackV 2 [44]
datasets with extra UVOS-aware gaze data.

the value of visual attention. The proposed model produces
state-of-the-art results on popular benchmarks. We expect
this work, together with our newly collected data, to pro-
vide a deeper insight into the underlying mechanism behind
UVOS and video salient object detection, and inspire more
research along this direction.

2. Related Work

Unsupervised Video Object Segmentation. Early UVOS
methods are typically based on handcrafted features and
heuristics such as long-term point trajectory [54, 5, 17,
53, 9], motion boundary [56], objectness [43, 51, 89, 18,
59, 83, 40, 45], and saliency [13, 77, 76, 34, 27]. Later,
with the renaissance of neural network, many deep learn-
ing based models were proposed, which typically use mul-
tilayer perceptron based moving objectness detector, adopt
two-stream architecture [67, 33], or CNN encoder-decoder
structure [66, 11, 45, 46, 64]. These deep UVOS models
generally achieve promising performance, due to the strong
learning ability of deep neural networks.

Although a handful of UVOS models [13, 77, 56, 81, 27,
64] use saliency (or foreground-map, a similar notion), they
are either heuristic methods lacking end-to-end trainability
or based on object-level saliency cues, instead of an explicit,
biologically-inspired visual attention representation. None
of them quantiﬁes the consistency between visual attention
and explicit primary video object determination. Addition-
ally, previous deep UVOS models are limited to the avail-
ability of large-scale well-annotated video data. By con-
trast, via leveraging dynamic visual attention as an inter-
mediate video object representation, our approach offers a
feasible way to alleviate this problem.

Video Salient Object Detection. VSOD is a very close
topic to UVOS. VSOD [16, 49, 79, 77, 80] aims to give a
gray saliency value for each pixel in the videos sequence.
The continuous saliency maps are valuable for a wide range
of applications, such as cropping, object tracking, and video
object segmentation. However, previous VSOD simply use
the UVOS datasets for benchmarking, which lacks a biolog-
ical evidence for such choice. In this work, through demon-

strating the consistency between human ﬁxations and ex-
plicit object judgement, we given an in-depth glimpse into
both UVOS and VSOD, which share a uniﬁed basis, i.e.,
top-down task-driven visual attention mechanism.

Visual Attention Prediction. Human attention mechanism
plays an essential role in visual information perception and
processing. In the past decade, the computer vision com-
munity has made active research efforts on computationally
modeling such selective attention process [32]. According
to the underlying mechanism, attention models can be cat-
egorized as either bottom-up (stimuli-inspired) or top-down
(task-driven). Early attention models [42, 90, 19, 6, 22, 25,
36, 15, 20, 26, 61, 22] are based on biologically-inspired
features (color, edge, optical ﬂow, etc.) and cognitive the-
ories about visual attention (attention shift [39], feature in-
tegration theory [68], guided search [82], etc.). Recently,
deep learning based attention models [71, 28, 55, 73, 75]
were proposed and generally yield better performance.

However, most previous methods use static, bottom-up
models and none of them is specially designed for modeling
UVOS-driven, top-down attention in dynamic scenes. Pre-
vious dynamic eye-tracking datasets [31, 52, 21, 75] were
constructed under free-viewing or other task-driven settings
(see Table 1). In this work, numerous eye gaze data on pop-
ular video segmentation datasets [58, 60, 44] are carefully
collected in the UVOS setting. Consequently, for the ﬁrst
time, a dynamic, top-down attention model is learned for
guiding UVOS. With above efforts, we expect to establish a
closer link between UVOS and visual attention prediction.

Trainable Attention in Neural Networks. Recent years
have witnessed growth of research towards integrating neu-
ral networks with fully-differentiable attention mechanism.
The neural attention stimulates the human selective atten-
tion mechanism and allows the network focus on the most
task-relevant parts of the input.
It has shown wide suc-
cesses in natural language processing and computer vi-
sion tasks, such as machine translation [2], image caption-
ing [85], visual question answering [88], human object in-
teraction [14], and image classiﬁcation [72], to list a few.
Those neural attentions are learned in an implicit, goal-
driven and end-to-end way.

Our DVAP module can also be viewed as a neural at-
tention mechanism, as it is end-to-end trainable and used
for soft-weighting the feature of AGOS models.
It dif-
fers from the others in its UVOS-aware nature, explicitly-
training ability (with the availability of ground-truth data),
and spatiotemporal application domain.

3. UVOS-Aware Eye-Tracking Data Collection

One objective of our work is to contribute extra eye-
ﬁxation annotations to three public video segmentation
datasets [58, 60, 44]. Fig. 2 shows some example frames
with our UVOS-aware eye-tracking annotation, along with

3066

Figure 2. Example frames from three datasets ([58, 60, 44]) with
our eye-tracking annotation (§3). The last column shows the av-
erage attention maps of these datasets. We quantitatively verify
(§4) the high consistency between human attention behavior (2nd
column) and primary-object determination (3rd column).

visual attention distributions over each dataset.
Stimuli: The dynamic stimuli are from DAVIS16 [58],
Youtube-Objects [60], and SegTrackV 2 [44]. DAVIS16 is a
popular UVOS benchmark containing 50 video sequences
with totally 3,455 frames. Youtube-Objects is a large
dataset with 126 videos covering 10 common object cate-
gories, with 20,647 frames in total. SegTrackV 2 consists of
14 short videos with totally 947 frames.
Apparatus: Observer eye movements were recorded us-
ing a 250 Hz SMI RED250 eye tracker (SensoMotoric In-
struments). The dynamic stimuli were displayed on a 19”
computer monitor at a resolution of 1440×900 and in their
original speeds. A headrest was used to maintain a viewing
distance of about 68 cm, as advised by the product manual.
Participants: Twenty participants (12 males and 8 females,
aging between 21 and 30), who passed the eye tracker cal-
ibration with less than 10% ﬁxation dropping rate, were
qualiﬁed for our experiment. All had normal/corrected-to-
normal vision and never seen the stimuli before.
Recording protocol: The experimenters ﬁrst ran the stan-
dard SMI calibration routine with recommended settings
for the best results. During viewing, the stimulus videos
were displayed in random order and the participants were
instructed to identify the primary object occurring in each
stimulus. Since we aim to explore human attention behavior
in UVOS setting, each stimulus was repeatedly displayed
three times to help the participants better capture the video
content. Such data capturing design is inspired by the proto-
col in [21]. To avoid eye fatigue, 5-second black screen was
intercalated between each. Additionally, the stimuli were
split into 5 sessions. After undergoing a session of videos,
the participant can take a rest. Finally, a total of 12,318,862
ﬁxations were recorded from 20 subjects on 190 videos.

4. In-depth Data Analysis

Inter-subject consistency: We ﬁrst conduct experiments
to analyze eye movement consistency within subjects. To
quantify such inter-subject consistency (ISC), following the
protocols in [47], data from half of the subjects are ran-

Aspect Metric

DAVIS16 [58] Youtube-Object[60] SegTrackV 2 [44]

ISC

ITC

AUC-J
(chance=0.5)
AUC-J
(chance=0.5)

0.899±0.029

0.876±0.056

0.883±0.036

0.704±0.078

0.733±0.105

0.747±0.071

Table 2. Quantitative results of inter-subject consistency (ISC)
and inter-task correlation (ITC), measured by AUC-Juddy.

domly selected as the test subset, leaving the rest as the
new ground-truth subset. After that, AUC-Juddy [7], a
classic visual attention evaluation metric, is employed to
the test subset to measure ISC. The experimental results
are shown in Table 2.
It is interesting to ﬁnd that there
exists high consistency of attention behaviors among hu-
man subjects, across all the three datasets. The correlation
scores (0.899 on DAVIS16, 0.876 on Youtube-Object, 0.883
on SegTrackV 2) are signiﬁcantly above chance (0.5). The
chance level is the accuracy of a random map with value
of each pixel drawn uniformly random between 0 and 1.
This novel observation further suggests that, even though
‘unsupervised video object(s)’ is often considered as ill-
deﬁned [70, 1, 78], there do exist some ‘universally-agreed’
visually important clues that attract human attentions stably
and consistently.
Correlation between visual attention and video object
determination: It is essential to study whether human vi-
sual attention and video primary object judgement agree
with each other, which has never been explored before.
Here we apply the experimental protocol suggested by [4]
to calculate the inter-task correlation (ITC). More speciﬁ-
cally, we use the segmentation mask to explain the ﬁxation
map. During the computation of AUC-Juddy metric, human
ﬁxations are considered as the positive set and some points
sampled from other non-ﬁxation positions as the negative
set. The segmentation mask is then used as a binary classi-
ﬁer to separate positive samples from negative samples. The
results are reported in Table 2, showing that visual attention
does not fall on the background signiﬁcantly higher than its
corresponding chance level. Taking Youtube-Objects as an
example, the correlation score 0.733 (std = 0.105) is signif-
icantly above chance using t-test (p < 0.05). This observa-
tion reveals the strong correlation between human dynamic
visual attention and video object determination.

5. Proposed UVOS Method

5.1. Problem Formulation

Denote an input video with T frames as {It ∈
RW×H×3}T
t=1, then the goal of UVOS is to generate the cor-
responding sequences of binary video object segmentation-
masks {St ∈ {0, 1}W×H }T
t=1. Many recently proposed
UVOS methods [64, 46, 33, 67] learn a DNN as a map-
ping function FUVOS : RW×H×3×T 7→ {0, 1}W×H×T that di-
rectly maps the input into the segmentation masks:

{St}T

t=1 = FUVOS({It}T

t=1).

(1)

3067

Figure 3. Illustration of the proposed UVOS model. (a) Simpliﬁed schematization of our model that solves UVOS in a two-step manner,
without the need of training with expensive precise video object masks. (b) Detailed network architecture, where the DVAP (§5.2) and
AGOS (§5.3) modules share the weights of two bottom conv blocks. The UVOS-aware attention acts as an intermediate object representa-
tion that connects the two modules densely. Best viewed in color. Zoom in for details.

To learn such direct input-output mapping FUVOS, numer-
ous pixel-wise video segmentation annotations are needed,
which are however very expensive to obtain.

In this work, we instead propose an input-attention-
output mapping strategy to tackle UVOS. Speciﬁcally, a
DVAP module FDVAP is ﬁrst designed to predict dynamic
UVOS-aware visual attentions {At ∈ [0, 1]W ′

×1}T

t=1:

×H ′

the dynamic visual attention. ConvLSTM [63], proposed as
a convolutional counterpart of conventional fully connected
LSTM, introduces convolution operation into input-to-state
and state-to-state transitions. ConvLSTM is favored here as
it preserves spatial details as well as modeling temporal dy-
namics simultaneously. Our DVAP module FDVAP can be
formulated as follows:

{At}T

t=1 = FDVAP({It}T

t=1).

(2)

Xt = CNN(It), Yt = convLSTM(Xt, Yt−1), At = R(Yt), (4)

An AGOS module FAGOS, which takes a single frame image
It and corresponding attention map At as input, is then used
to generate ﬁnal segmentation result St:

St = FAGOS(It, At),

t ∈ {1, 2, . . . , T }.

(3)

As shown in Fig. 3 (a), {At}T
t=1 encode both static object
infomation and temporal dynamics, enabling AGOS to fo-
cus on ﬁne-grained segmentation in spatial domain, i.e., ap-
plying AGOS for each frame individually. Essentially, the
visual attention, as a biologically-inspired visual cue and in-
termediate object representation, links DVAP and AGOS to-
gether, and offers an explicit interpretation by telling where
our model is looking at.

where Yt indicates the 3D-tensor hidden state (with 32
channels) of convLSTM at time step t. R is a readout func-
tion that produces the attention map from the hidden state,
implemented as a 1×1 convolution layer with the sigmoid
activation function.

In the next section, we employ DVAP as an attention
mechanism to guide AGOS to concentrate more on the vi-
sually important regions. An extra advantage of such design
lies in disentangling spatial and temporal characteristics of
foreground objects, as DVAP captures temporal informa-
tion by learning from dynamic-gaze data, and thus allows
AGOS to focus on pixel-wise segmentation only in spatial
domain (beneﬁting from existing large-scale image datasets
with paired ﬁxation and object segmentation annotation).

5.2. DVAP Module

5.3. AGOS Module

The DVAP module is built on a CNN-convLSTM archi-
tecture (see Fig. 3 (b)), where the CNN layers are borrowed
from the ﬁrst ﬁve convolutional blocks of ResNet101 [24].
To preserve more spatial details, we reduce the stride of the
last block to 1. Given the input video sequence {It}T
with typical 473 × 473 spatial resolution, the spatial fea-
ture sequence {Xt ∈ R30×30×2048}T
t=1 from the top-layer
of the CNN network is fed into a convLSTM for learning

t=1

The attention obtained from DVAP suggests the location
of the primary object(s), offering informative cue to AGOS
for pixel-wise segmentation, as achieved by a neural atten-
tion architecture. Before going deep into our model, we ﬁrst
give a general formulation of neural attention mechanisms.
General neural attention mechanism: A neural attention
mechanism equips a network with the ability to focus on a
subset of input feature. It computes a soft-mask to enhance

3068

the feature by multiplication operation. Let i ∈ Rd be an
input vector, z ∈ Rk a feature vector, a ∈ [0, 1]k an atten-
tion vector, g ∈ Rk an attention-enhanced feature and fA an
attention network. The neural attention is implemented as:

a = fA(i),

z = fZ(i), g = a ⊙ z,

(5)

where ⊙ is element-wise multiplication, and fZ indicates
a feature extraction network. Some neural attention mod-
els equip attention function fA with soft-max to constraint
the values of attention between 0 and 1. Since the above
attention framework is fully differentiable, it is end-to-end
trainable. However, due to the lack of ‘ground-truth’ of the
attention, it is trained in an implicit way.
Explicit, spatiotemporal, and UVOS-aware attention
mechanism: We integrate DVAP into AGOS as an attention
mechanism. Let Zt, Gt denote respectively a segmentation
feature and an attention glimpse with the same dimensions,
our UVOS-aware attention is formulated as:

Spatiotemporal attention: {At}T

t=1 = FDVAP({It}T

t=1),

Spatial feature enhancement:

Zt = FZ(It),

G

c

t = At ⊙ Z

c
t ,

(6)

where FZ extracts segmentation features from the input
frame It (will be detailed latter). Gc and Zc indicate the
feature slices of G and Z in d-th channel, respectively. As
seen, our UVOS-aware attention encodes spatial foreground
information as well as temporal characteristics, enabling
the AGOS module perform object segmentation over each
frame individually. For the position with an attention value
close to 0, the corresponding feature response will be sup-
pressed greatly. This may lose some meaningful informa-
tion. Inspired by [24, 72], the feature enhancement step in
Eq. 6 is enhanced with a residual form (see Fig. 3 (b)):

G

c

t = (1 + At) ⊙ Z

c
t .

(7)

This strategy retains the original information (even with a
very small attention value), while enhances object-relevant
features efﬁciently. Besides, due to the availability of the
ground-truth gaze data, our UVOS-aware attention mecha-
nism is trained in an explicit manner (detailed in §5.4).

The AGOS module is also built upon convolutional
blocks of ResNet101 [24] and modiﬁed with the ASPP
module proposed in DeepLabV 3 [10]. With an input
frame image It ∈ R473×473×3, a segmentation feature Zt ∈
R60×60×1536 can be extracted from the ASPP module FASPP.
The attention map At is also ×2 upsampled by bilinear in-
terpolation. Finally, our AGOS module in Eq. 6 is imple-
mented as:

Spatiotemporal attention: {At}T

t=1 = FDVAP({It}T

t=1),

Spatial feature enhancement:

Zt = FASPP(It),

(8)

G

c

t = (1 + At) ⊙ Z

c
t .

Knowledge sharing between DVAP and AGOS: DVAP
and AGOS modules share similar underlying network ar-
chitectures (conv1-conv5 of ResNet101), while capturing

object information from different perspectives. We develop
a technique to encourage knowledge sharing between the
two networks, rather than learning each of them separately.
In particular, we allow the two modules share the weights
of the ﬁrst three convolutional blocks (conv1, conv2, and
conv3), and then learn other higher-level layers separately.
This is because the bottom-layers typically capture low-
level information (edge, corner, etc.), while the top-layers
tend to learn high-level, task-speciﬁc knowledge. More-
over, such weight-sharing strategy improves our computa-
tional efﬁciency and decreases parameter storage.

5.4. Implementation Details

Training loss: For DAVP, given an input frame I ∈
R473×473×3, it predicts an attention map A ∈ [0, 1]30×30. De-
note by P ∈ [0, 1]30×30 and F ∈ {0, 1}30×30 the ground-truth
continuous attention map and the binary ﬁxation map, re-
spectively. F is a discrete map, recording whether a pixel
receives human-eye ﬁxation position, and P is obtained by
blurring F with a small Gaussian ﬁlter. Inspired by [28], the
loss function LDVAP for DAVP is designed as:

LDVAP(A, P, F) =LCE(A, P) + α1LNSS(A, F)+

α2LSIM(A, F) + α3LCC(A, P),

(9)

where the LCE indicates the classic cross entropy loss, and
LCC, LNSS, LSIM are derived respectively from three widely-
used visual attention evaluation metrics named Normalized
Scanpath Saliency (NSS), Similarity Metric (SIM) and Lin-
ear Correlation Coefﬁcient (CC). Such combination leads
to improved performance due to comprehensive consider-
ation of different quantiﬁcation factors as in [28]. We use
LCE as the primary loss, and set α1 = α2 = α3 = 0.1.

For AGOS, given I, it produces the ﬁnal segmentation
prediction2 S ∈ [0, 1]60×60. Let M ∈ {0, 1}60×60 denote the
ground-truth binary segmentation mask, the loss function
LAGOS of the AGOS module is formulated as:

LAGOS(S, M) = LCE(S, M).

(10)

Training protocol: We leverage both video gaze data and
attention-segmentation paired image data to train our whole
UVOS model. The training process is iteratively performed
on a video training batch and an image train batch. Speciﬁ-
cally, in the video training batch, we use dynamic gaze data
to train the DVAP module only. Given the training video
sequence {It}T
t=1 denote the corre-
sponding attention predictions, ground-truth continuous at-
tention maps and discrete ﬁxation maps, we train our model
by minimizing the following loss (see Fig. 3 (a)):

t=1, let {At, Pt, Ft}T

Ld = X

T

t=1

LDVAP(A

d
t , P

d
t , F

d

t ),

(11)

where the superscript ‘d’ represents dynamic video data.
Note that we do not consider LAGOS loss to save the ex-
pensive pixel-wise segmentation ground-truth.

2We slightly reuse S for representing the segmentation prediction.

3069

The image training batch contains several attention-
segmentation paired image masks, which is used to train
both DVAP and AGOS modules simultaneously.
Let
{I, S, F, M} denote a training sample in the image train-
ing batch, which includes a static image and corresponding
ground-truth (i.e., continuous attention map, binary ﬁxation
map, and segmentation mask). The overall loss function
combines both LDVAP and LAGOS:

Ls = LDVAP(A

s

, P

s

, F

s) + LAGOS(S

s

, M

s),

(12)

where a superscript ‘s’ is used to emphasize the static na-
ture. By using static data, the total time span of convL-
STM in DVAP is set to 1. Each video training batch uses 2
videos, each with 3 consecutive frames. Both the videos and
the start frames are randomly selected. Each image training
batch contains 6 randomly sampled images.

6. Experiments

Training data: During training, we use the video se-
quences and corresponding ﬁxation data from the train-
ing split of DAVIS16 [58] and the whole SegTrackV 2 [44]
dataset, leading to totally 54 video sequences with 6,526
frames. Additionally, two image salient object segmen-
tation datasets, DUT-O [87] and PASCAL-S [47], offer
both static gaze data and segmentation annotations, and
are thus also used in our training phase, resulting in to-
tally 6,018 static training examples. Therefore, our model
is trained without labor-intensive pixel-wise video segmen-
tation masks, by leveraging easily-acquired dynamic gaze
data and static attention-segmentation annotation pairs. In
§6.2, we quantitatively demonstrate that, even without train-
ing on video segmentation annotations, the suggested model
is still able to achieve state-of-the-art performance.
Testing phase: Given a test video, all the frames are uni-
formly resized to 473×473 and fed into our model for ob-
taining the corresponding primary object predictions. Fol-
lowing the common protocol [66, 8, 84, 57] in video seg-
mentation, the fully-connected CRF [41] is employed to ob-
tain the ﬁnal binary segmentation results. For each frame,
the forward propagation of our network takes about 0.1s,
while the CRF-based post-processing takes about 0.5s.

6.1. Performance of DVAP module

Test datasets: We evaluate our DVAP module on the test set
of DAVIS16 [58] and the full Youtube-Objects [60], with the
gaze-tracking ground-truth and there is no overlap between
the training and test data.
Evaluation metrics: Five standard metrics: AUC-Judd
(AUC-J), shufﬂed AUC (s-AUC), NSS, SIM, and CC, are
used for comprehensive study (see [3] for details).
Quantitative and qualitative results: We compare our
DVAP module with 12 state-of-the-art visual attention mod-
els, including 5 deep models [75, 35, 73, 55, 28] and 7 tra-
ditional models [15, 20, 26, 61, 22, 32]. Quantitative results

Dataset

Methods

AUC-J ↑ SIM ↑ s-AUC ↑ CC ↑ NSS ↑

DAVIS16

ACL [75]
OMCNN [35]
DVA [73]
DeepNet [55]
ShallowNet [55]
SALICON [28]
STUW [15]
PQFT [20]
Seo et al. [61]
Hou et al. [26]
GBVS [22]
ITTI [32]
Ours

0.901
0.889
0.885
0.880
0.874
0.818
0.892
0.685
0.724
0.782
0.882
0.820
0.909

0.453
0.408
0.382
0.318
0.293
0.276
0.363
0.202
0.234
0.263
0.294
0.249
0.504

0.617
0.621
0.647
0.644
0.622
0.628
0.636
0.584
0.582
0.581
0.617
0.621
0.667

0.559 2.252
0.518 2.101
0.494 1.906
0.470 1.866
0.471 1.871
0.352 1.432
0.508 2.019
0.191 0.821
0.222 0.923
0.273 1.119
0.442 1.683
0.354 1.332
0.620 2.507

Table 3. Quantitative comparison of visual attention models on
the test set of DAVIS16 [58] (§6.1). The three best scores are indi-
cated in red, blue and green, respectively (same for other tables).

Dataset

Methods

AUC-J ↑ SIM ↑ s-AUC ↑ CC ↑ NSS ↑

ACL [75]
OMCNN [35]
DVA [73]
DeepNet [55]
ShallowNet [55]
Youtube- SALICON [28]
STUW [15]
Objects
PQFT [20]
Hou et al. [26]
Seo et al. [61]
GBVS [22]
ITTI [32]
Ours

0.912
0.889
0.905
0.894
0.890
0.840
0.869
0.730
0.786
0.763
0.881
0.837
0.914

0.405
0.326
0.372
0.268
0.252
0.265
0.264
0.170
0.221
0.210
0.244
0.214
0.419

0.711
0.698
0.741
0.737
0.704
0.692
0.666
0.646
0.639
0.605
0.706
0.709
0.747

0.531 2.627
0.461 2.307
0.526 2.294
0.448 2.182
0.436 2.069
0.380 1.956
0.388 1.876
0.210 1.061
0.243 1.223
0.224 1.118
0.395 1.919
0.339 1.638
0.543 2.700

Table 4. Quantitative comparison of different visual attention
models on Youtube-Objects [60] (§6.1).

over the test set of DAVIS16 [58] and Youtube-Objects [60]
are summarized in Tables 3 and 4, respectively. As seen,
our DVAP generally outperforms other competitors, as none
of them is speciﬁcally designed for UVOS-aware attention
prediction. Our DVAP can guide our UVOS model to accu-
rately attend to visually attractive regions in videos.

6.2. Performance of full UVOS model

Test datasets: The test sets of DAVIS16 [58] and the full
Youtube-Objects [60] are used for assessing the perfor-
mance of our full UVOS model.
Evaluation metrics: For the UVOS task, we use three stan-
dard metrics suggested by [58], i.e., region similarity J ,
boundary accuracy F , and time stability T .
Quantitative and qualitative results: The quantitative
comparison results over above two datasets are reported
in Tables 5 and 6, respectively. We can observe that the
proposed model outperforms other competitors over most
metrics across all the datasets. This is signiﬁcant and
distinguishes our model from previous deep UVOS mod-
els [40, 46, 67, 33, 66, 11] since our model is trained without
precise segmentation mask ground-truths. Some qualita-
tive results are shown in Fig. 4, validating our model yields
high-quality results with interpretable dynamic attentions.

3070

Dataset Metric Ours PDB [64] ARP [40] LVO [67] FSEG [33] LMP [66] SFL [11] FST [56] CUT [38] NLC [13] MSG [53] KEY [43] CVOS [65] TRC [17]

DAVIS16

J

Mean ↑ 79.7
Recall ↑ 91.1
Decay ↓ 0.0
Mean ↑ 77.4
Recall ↑ 85.8
Decay ↓ 0.0
T Mean ↓ 44.5

F

77.2
90.1
0.9
74.5
84.4
-0.2
29.1

76.2
91.1
7.0
70.6
83.5
7.9
39.3

75.9
89.1
0.0
72.1
83.4
1.3
26.5

70.7
83.5
1.5
65.3
73.8
1.8
32.8

70.0
85.0
1.3
65.9
79.2
2.5
57.2

67.4
81.4
6.2
66.7
77.1
5.1
28.2

55.8
64.9
0.0
51.1
51.6
2.9
36.6

55.2
57.5
2.2
55.2
61.0
3.4
27.7

55.1
55.8
12.6
52.3
51.9
11.4
42.5

53.3
61.6
2.4
50.8
60.0
5.1
30.2

49.8
59.1
14.1
42.7
37.5
10.6
26.9

48.2
54.0
10.5
44.7
52.6
11.7
25.0

47.3
49.3
8.3
44.1
43.6
12.9
39.1

Table 5. Quantitative UVOS results on the test sequences of DAVIS16 [58]. The results selected from the public leaderboard (https:
//davischallenge.org/davis2016/soa_compare.html) maintained by the DAVIS challenge. See §6.2 for details.

Dataset Category Ours PDB [64] ARP [40] LVO [67] SFL [11] FSEG [33] FST [56] COSEG [69] LTV [54]

Youtube
-Object

Airplane

Bird
Boat
Car
Cat
Cow
Dog
Horse

Motorbike

Train

J Mean ↑

87.7
76.7
72.2
78.6
69.2
64.6
73.3
64.4
62.1
48.2
69.7

78.0
80.0
58.9
76.5
63.0
64.1
70.1
67.6
58.4
35.3
65.5

73.6
56.1
57.8
33.9
30.5
41.8
36.8
44.3
48.9
39.2
46.2

86.2
81.0
68.5
69.3
58.8
68.5
61.7
53.9
60.8
66.3
67.5

65.6
65.4
59.9
64.0
58.9
51.2
54.1
64.8
52.6
34.0
57.1

81.7
63.8
72.3
74.9
68.4
68.0
69.4
60.4
62.7
62.2
68.4

70.9
70.6
42.5
65.2
52.1
44.5
65.3
53.5
44.2
29.6
53.8

69.3
76.0
53.5
70.4
66.8
49.0
47.5
55.7
39.5
53.4
58.1

13.7
12.2
10.8
23.7
18.6
16.3
18.2
11.5
10.6
19.6
15.5

Table 6. Quantitative UVOS results on Youtube-Objects [60]. Performance over each category and the average score are reported.

Figure 4. Visual results on two example videos. The dynamic attention results from our DVAP module are shown in the second row,
which are biologically-inspired and used to guide our AGOS module for ﬁne-grained UVOS (see the last row).

Dataset Metric Ours PDB [64] FGRNE [45] FCNS [80] SGSP [48] GAFL [79] SAGE [77] STUW [16] SP [49]

DAVIS16

F max ↑ 0.870
MAE ↓ 0.026

0.849
0.030

0.786
0.043

0.729
0.053

0.677
0.128

0.578
0.091

0.479
0.105

0.692
0.098

0.601
0.130

Table 7. Quantitative VSOD results on the test sequences of DAVIS16 [58] with MAE and max F-measure (see §6.3).

6.3. Performance on the VSOD task

Test datasets: The test sets of DAVIS16 [58] is used for
testing our model in the VSOD setting.
Evaluation metrics: Standard F-measure and MAE met-
rics are used for quantitative evaluation [74].
Quantitative results: As shown in Table 7, our model
(without CRF binaryzation) outperforms previous VSOD
models [64, 45, 80, 48, 79, 77, 16, 49] with human read-
able attention maps. This veriﬁes the strong correlation be-
tween VSOD and UVOS from a view of top-down attention
mechanism.

7. Conclusion

This work systematically studied the role of visual at-
tention in UVOS and its related task, VSOD. We extended
three popular video object segmentation datasets with real
human eye-tacking records. Through in-depth analysis, for

the ﬁrst time, we quantitatively validated that human vi-
sual attention mechanism plays an essential role in UVOS
and VSOD tasks. With this novel insight, we proposed
a novel visual attention-driven UVOS model, where the
DVAP module, mimicking human attention behavior in the
dynamic UVOS setting, is used as a supervised neural atten-
tion to guide the subsequent AGOS module for ﬁne-grained
video object segmentation. With the visual attention as an
intermediate representation, our model is able to produce
promising results without training on expensive pixel-wise
video segmentation ground-truths, and it gains better post-
hoc, biologically-consistent interpretability. Experimental
results demonstrated the proposed model outperforms other
state-of-the-art UVOS methods. The suggested model also
gains best performance in the VSOD setting. Therefore,
we closely connect the top-down, segmentation-aware vi-
sual attention mechanism, UVOS and VSOD tasks, and of-
fer a new glimpse into the rationale behind them.

3071

References

[1] S Avinash Ramakanth and R Venkatesh Babu. SeamSeg:
In CVPR,

Video object segmentation using patch seams.
2014. 4

[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. In ICLR, 2015. 3

[3] Ali Borji and Laurent Itti. State-of-the-art in visual attention

modeling. IEEE TPAMI, 35(1):185–207, 2013. 7

[4] Ali Borji, Dicky N Sihite, and Laurent Itti. What stands out
in a scene? A study of human explicit saliency judgment.
Vision Research, 91:62–77, 2013. 4

[5] Thomas Brox and Jitendra Malik. Object segmentation by

long term analysis of point trajectories. In ECCV, 2010. 3

[6] Neil Bruce and John Tsotsos. Saliency based on information

maximization. In NIPS, 2006. 3

[7] Zoya Bylinskii, Tilke Judd, Ali Borji, Laurent Itti, Fr´edo Du-
rand, Aude Oliva, and Antonio Torralba. Mit saliency bench-
mark. http://saliency.mit.edu/. 4

[8] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset,
Laura Leal-Taix´e, Daniel Cremers, and Luc Van Gool. One-
shot video object segmentation. In CVPR, 2017. 7

[9] Lin Chen, Jianbing Shen, Wenguan Wang, and Bingbing
Ni. Video object segmentation via dense trajectories. IEEE
TMM, 17(12):2225–2234, 2015. 3

[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587,
2017. 6

[11] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-
Hsuan Yang. Segﬂow: Joint learning for video object seg-
mentation and optical ﬂow. In ICCV, 2017. 3, 7, 8

[12] Ahmed Elgammal, Ramani Duraiswami, David Harwood,
and Larry S Davis. Background and foreground model-
ing using nonparametric kernel density estimation for visual
surveillance. Proceedings of the IEEE, 90(7):1151–1163,
2002. 1

[13] Alon Faktor and Michal Irani. Video segmentation by non-

local consensus voting. In BMVC, 2014. 3, 8

[14] Hao-Shu Fang, Jinkun Cao, Yu-Wing Tai, and Cewu Lu.
Pairwise body-part attention for recognizing human-object
interactions. In ECCV, 2018. 3

[15] Yuming Fang, Weisi Lin, Zhenzhong Chen, Chia-Ming Tsai,
and Chia-Wen Lin. A video saliency detection model in com-
pressed domain. IEEE TCSVT, 24(1):27–38, 2014. 3, 7

[16] Yuming Fang, Zhou Wang, Weisi Lin, and Zhijun Fang.
Video saliency incorporating spatiotemporal cues and uncer-
tainty weighting. IEEE TIP, 23(9):3910–3921, 2014. 3, 8

[17] Katerina Fragkiadaki, Geng Zhang, and Jianbo Shi. Video
segmentation by tracing discontinuities in a trajectory em-
bedding. In CVPR, 2012. 3, 8

[18] Huazhu Fu, Dong Xu, Bao Zhang, and Stephen Lin. Object-
based multiple foreground video co-segmentation. In CVPR,
2014. 3

[20] Chenlei Guo and Liming Zhang. A novel multiresolution
spatiotemporal saliency detection model and its applications
in image and video compression. IEEE TIP, 19(1):185–198,
2010. 3, 7

[21] Hadi Hadizadeh, Mario J Enriquez, and Ivan V Bajic. Eye-
tracking database for a set of standard video sequences. IEEE
TIP, 21(2):898–903, 2012. 3, 4

[22] Jonathan Harel, Christof Koch, and Pietro Perona. Graph-

based visual saliency. In NIPS, 2007. 3, 7

[23] Eric Hayman and Jan-Olof Eklundh. Statistical background

subtraction for a mobile observer. In ICCV, 2003. 1

[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 5, 6

[25] Xiaodi Hou and Liqing Zhang. Saliency detection: A spec-

tral residual approach. In CVPR, 2007. 3

[26] Xiaodi Hou and Liqing Zhang. Dynamic visual attention:
Searching for coding length increments. In NIPS, 2009. 3, 7
[27] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing.
Unsupervised video object segmentation using motion
saliency-guided spatio-temporal propagation.
In ECCV,
2018. 3

[28] Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. Sal-
icon: Reducing the semantic gap in saliency prediction by
adapting deep neural networks. In ICCV, 2015. 3, 6, 7

[29] Michal Irani and P Anandan. A uniﬁed approach to mov-
IEEE TPAMI,

ing object detection in 2d and 3d scenes.
20(6):577–589, 1998. 1

[30] Michal Irani, Benny Rousso, and Shmuel Peleg. Computing
occluding and transparent motions. IJCV, 12(1):5–16, 1994.
1

[31] Laurent Itti. Automatic foveation for video compression us-
ing a neurobiological model of visual attention. IEEE TIP,
13(10):1304–1318, 2004. 3

[32] Laurent Itti, Christof Koch, and Ernst Niebur. A model
of saliency-based visual attention for rapid scene analysis.
IEEE TPAMI, 20(11):1254–1259, 1998. 3, 7

[33] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusion-
seg: Learning to combine motion and appearance for fully
automatic segmention of generic objects in videos. In CVPR,
2017. 1, 3, 4, 7, 8

[34] Won-Dong Jang, Chulwoo Lee, and Chang-Su Kim. Primary
object segmentation in videos via alternate convex optimiza-
tion of foreground and background distributions. In CVPR,
2016. 3

[35] Lai Jiang, Mai Xu, Tie Liu, Minglang Qiao, and Zulin Wang.
Deepvs: A deep learning based video saliency prediction ap-
proach. In ECCV, 2018. 7

[36] Tilke Judd, Krista Ehinger, Fr´edo Durand, and Antonio Tor-
In ICCV,

ralba. Learning to predict where humans look.
2009. 3

[37] Fumi Katsuki and Christos Constantinidis. Bottom-up and
top-down attention: Different processes and overlapping
neural systems. The Neuroscientist, 20(5):509–521, 2014.
2

[19] Dashan Gao and Nuno Vasconcelos. Discriminant saliency
for visual recognition from cluttered scenes. In NIPS, 2005.
3

[38] Margret Keuper, Bjoern Andres, and Thomas Brox. Mo-
tion trajectory segmentation via minimum cost multicuts. In
ICCV, 2015. 8

3072

[39] Christof Koch and Shimon Ullman. Shifts in selective vi-
In

sual attention: Towards the underlying neural circuitry.
Matters of Intelligence, pages 115–141. 1987. 2, 3

[40] Yeong Jun Koh and Chang-Su Kim. Primary object segmen-
tation in videos based on region augmentation and reduction.
In CVPR, 2017. 3, 7, 8

[41] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
In

in fully connected crfs with gaussian edge potentials.
NIPS, 2011. 7

[42] Olivier Le Meur, Patrick Le Callet, Dominique Barba, and
Dominique Thoreau. A coherent computational approach to
model bottom-up visual attention. IEEE TPAMI, 28(5):802–
817, 2006. 3

[43] Yong Jae Lee, Jaechul Kim, and Kristen Grauman. Key-
segments for video object segmentation. In ICCV, 2011. 3,
8

[44] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and
James M Rehg. Video segmentation by tracking many ﬁgure-
ground segments. In ICCV, 2013. 2, 3, 4, 7

[45] Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi,
Qin Huang, and C.-C. Jay Kuo. Instance embedding transfer
to unsupervised video object segmentation. In CVPR, 2018.
1, 3, 8

[46] Siyang Li, Bryan Seybold, Alexey Vorobyov, Xuejing Lei,
and C.-C. Jay Kuo. Unsupervised video object segmentation
with motion-based bilateral networks. In ECCV, 2018. 3, 4,
7

[47] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and
Alan L Yuille. The secrets of salient object segmentation.
In CVPR, 2014. 2, 4, 7

[48] Zhi Liu, Junhao Li, Linwei Ye, Guangling Sun, and Li-
quan Shen. Saliency detection for unconstrained videos us-
ing superpixel-level graph and spatiotemporal propagation.
IEEE TCSVT, 27(12):2527–2542, 2017. 8

[49] Zhi Liu, Xiang Zhang, Shuhua Luo, and Olivier Le Meur.
IEEE

Superpixel-based spatiotemporal saliency detection.
TCSVT, 24(9):1522–1540, 2014. 3, 8

[50] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian
Reid, and Ming-Hsuan Yang. Deep regression tracking with
shrinkage loss. In ECCV, 2018. 1

[51] Tianyang Ma and Longin Jan Latecki. Maximum weight
cliques with mutex constraints for video object segmenta-
tion. In CVPR, 2012. 3

[52] Stefan Mathe and Cristian Sminchisescu. Actions in the eye:
dynamic gaze datasets and learnt saliency models for visual
recognition. IEEE TPAMI, 37(7):1408–1424, 2015. 3

[53] Peter Ochs and Thomas Brox. Object segmentation in video:
a hierarchical variational approach for turning point trajecto-
ries into dense regions. In ICCV, 2011. 3, 8

[54] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation
of moving objects by long term video analysis. IEEE TPAMI,
36(6):1187–1200, 2014. 3, 8

[55] Junting Pan, Elisa Sayrol, Xavier Giro-i Nieto, Kevin
McGuinness, and Noel E O’Connor. Shallow and deep con-
volutional networks for saliency prediction. In CVPR, 2016.
3, 7

[56] Anestis Papazoglou and Vittorio Ferrari. Fast object segmen-

tation in unconstrained video. In ICCV, 2013. 3, 8

[57] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt
Schiele, and Alexander Sorkine-Hornung. Learning video
object segmentation from static images. In CVPR, 2017. 7

[58] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc
Van Gool, Markus Gross, and Alexander Sorkine-Hornung.
A benchmark dataset and evaluation methodology for video
object segmentation. In CVPR, 2016. 2, 3, 4, 7, 8

[59] Federico Perazzi, Oliver Wang, Markus Gross, and Alexan-
der Sorkine-Hornung. Fully connected object proposals for
video segmentation. In ICCV, 2015. 3

[60] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia
Schmid, and Vittorio Ferrari. Learning object class detectors
from weakly annotated video. In CVPR, 2012. 2, 3, 4, 7, 8

[61] Hae Jong Seo and Peyman Milanfar. Static and space-time
visual saliency detection by self-resemblance. Journal of Vi-
sion, 9(12):15–15, 2009. 3, 7

[62] Dian Shao, Yu Xiong, Yue Zhao, Qingqiu Huang, Yu Qiao,
and Dahua Lin. Find and focus: Retrieve and localize video
events with natural language queries. In ECCV, 2018. 1

[63] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM
network: A machine learning approach for precipitation
nowcasting. In NIPS, 2015. 5

[64] Hongmei Song, Wenguan Wang, Sanyuan Zhao, Jianbing
Shen, and Kin-Man Lam. Pyramid dilated deeper convL-
STM for video salient object detection. In ECCV, 2018. 1,
2, 3, 4, 8

[65] Brian Taylor, Vasiliy Karasev, and Stefano Soatto. Causal
video object segmentation from persistence of occlusions. In
CVPR, 2015. 8

[66] Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid.
Learning motion patterns in videos. In CVPR, 2017. 3, 7,
8

[67] Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid.
Learning video object segmentation with visual memory. In
ICCV, 2017. 1, 3, 4, 7, 8

[68] Anne M Treisman and Garry Gelade. A feature-integration
theory of attention. Cognitive Psychology, 12(1):97–136,
1980. 2, 3

[69] Yi-Hsuan Tsai, Guangyu Zhong, and Ming-Hsuan Yang. Se-

mantic co-segmentation in videos. In ECCV, 2016. 8

[70] Amelio Vazquez-Reina, Shai Avidan, Hanspeter Pﬁster, and
Eric Miller. Multiple hypothesis video segmentation from
superpixel ﬂows. In ECCV, 2010. 4

[71] Eleonora Vig, Michael Dorr, and David Cox. Large-scale
optimization of hierarchical features for saliency prediction
in natural images. In CVPR, 2014. 3

[72] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classiﬁcation.
In
CVPR, 2017. 3, 6

[73] Wenguan Wang and Jianbing Shen. Deep visual attention

prediction. IEEE TIP, 27(5):2368–2378, 2018. 3, 7

[74] Wenguan Wang, Jianbing Shen, Xingping Dong, Ali Borji,
and Ruigang Yang. Inferring salient objects from human ﬁx-
ations. IEEE TPAMI, 2019. 8

3073

[75] Wenguan Wang, Jianbing Shen, Fang Guo, Ming-Ming
Cheng, and Ali Borji. Revisiting video saliency: A large-
scale benchmark and a new model. In CVPR, 2018. 3, 7

[76] Wenguan Wang, Jianbing Shen, Xuelong Li, and Fatih
IEEE TIP,

Porikli. Robust video object cosegmentation.
24(10):3137–3148, 2015. 3

[77] Wenguan Wang, Jianbing Shen, and Fatih Porikli. Saliency-
aware geodesic video object segmentation. In CVPR, 2015.
2, 3, 8

[78] Wenguan Wang, Jianbing Shen, Fatih Porikli, and Ruigang
Semi-supervised video object segmentation with

Yang.
super-trajectories. IEEE TPAMI, 2018. 4

[79] Wenguan Wang, Jianbing Shen, and Ling Shao. Consistent
video saliency using local gradient ﬂow optimization and
global reﬁnement. IEEE TIP, 24(11):4185–4196, 2015. 2, 3,
8

[80] Wenguan Wang, Jianbing Shen, and Ling Shao. Video salient
object detection via fully convolutional networks. IEEE TIP,
27(1):38–49, 2018. 3, 8

[81] Wenguan Wang, Jianbing Shen, Hanqiu Sun, and Ling Shao.
IEEE TCSVT,

Video co-saliency guided co-segmentation.
28(8):1727–1736, 2018. 3

[82] Jeremy M Wolfe, Kyle R Cave, and Susan L Franzel. Guided
search: An alternative to the feature integration model for
visual search. Journal of Experimental Psychology: Human
Perception and Performance, 15(3):419, 1989. 2, 3

[83] Fanyi Xiao and Yong Jae Lee. Track and segment: An iter-
ative unsupervised approach for video object proposals. In
CVPR, 2016. 3

[84] Huaxin Xiao, Jiashi Feng, Guosheng Lin, Yu Liu, and Mao-
jun Zhang. Monet: Deep motion exploitation for video ob-
ject segmentation. In CVPR, 2018. 7

[85] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In ICML, 2015. 3

[86] Wenqiang Xu, Yonglu Li, and Cewu Lu. Srda: Generat-
ing instance segmentation annotation via scanning, reason-
ing and domain adaptation. In ECCV, 2018. 1

[87] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In CVPR, 2013. 2, 7

[88] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and
Alex Smola. Stacked attention networks for image question
answering. In CVPR, 2016. 3

[89] Dong Zhang, Omar Javed, and Mubarak Shah. Video ob-
ject segmentation through spatially accurate and temporally
dense extraction of primary object regions. In CVPR, 2013.
3

[90] Lingyun Zhang, Matthew H Tong, Tim K Marks, Honghao
Shan, and Garrison W Cottrell. Sun: A bayesian frame-
work for saliency using natural statistics. Journal of Vision,
8(7):32–32, 2008. 3

3074

