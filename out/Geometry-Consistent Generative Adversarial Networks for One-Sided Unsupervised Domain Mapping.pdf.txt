Geometry-Consistent Generative Adversarial Networks for One-Sided

Unsupervised Domain Mapping

Huan Fu∗ 1 Mingming Gong∗ 2,3
UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Darlington, NSW 2008, Australia

Kayhan Batmanghelich2

Chaohui Wang4

Dacheng Tao1

Kun Zhang3

1

2

Department of Biomedical Informatics, University of Pittsburgh

3

Department of Philosophy, Carnegie Mellon University

4

Universit´e Paris-Est, LIGM (UMR 8049), CNRS, ENPC, ESIEE Paris, UPEM, Marne-la-Vall´ee, France

{hufu6371@uni., dacheng.tao@}sydney.edu.au

{mig73, kayhan}@pitt.edu

chaohui.wang@u-pem.fr

kunz1@cmu.edu

Abstract

Unsupervised domain mapping aims to learn a function
GXY to translate domain X to Y in the absence of paired
examples. Finding the optimal GXY without paired data
is an ill-posed problem, so appropriate constraints are re-
quired to obtain reasonable solutions. While some promi-
nent constraints such as cycle consistency and distance
preservation successfully constrain the solution space, they
overlook the special properties of images that simple geo-
metric transformations do not change the image’s seman-
tic structure. Based on this special property, we develop
a geometry-consistent generative adversarial network (Gc-
GAN), which enables one-sided unsupervised domain map-
ping. GcGAN takes the original image and its counterpart
image transformed by a predeﬁned geometric transforma-
tion as inputs and generates two images in the new domain
coupled with the corresponding geometry-consistency con-
straint. The geometry-consistency constraint reduces the
space of possible solutions while keep the correct solutions
in the search space. Quantitative and qualitative compar-
isons with the baseline (GAN alone) and the state-of-the-art
methods including CycleGAN [66] and DistanceGAN [5]
demonstrate the effectiveness of our method.

1. Introduction

Domain mapping or image-to-image translation, which
targets at translating an image from one domain to another,
has been intensively investigated over the past few years.
Let X ∈ X denote a random variable representing source
domain images and Y ∈ Y represent target domain images.
According to whether we have access to a paired sample
{(xi, yi)}N
i=1, domain mapping can be studied in a super-

∗equal contribution

vised or unsupervised manner. While several works have
successfully produced high-quality translations by focusing
on supervised domain mapping with constraints provided
by cross-domain image pairs [46, 26, 59, 58], the progress
of unsupervised domain mapping is relatively slow.

In unsupervised domain mapping, the goal is to model
the joint distribution PXY given samples drawn from the
marginal distributions PX and PY in individual domains.
Since the two marginal distributions can be inferred from
an inﬁnite set of possible joint distributions, it is difﬁcult
to guarantee that an individual input x ∈ X and the output
GXY (x) are paired up in a meaningful way without addi-
tional assumptions or constraints.

To address this problem, recent approaches have ex-
ploited the cycle-consistency assumption, i.e., a mapping
GXY and its inverse mapping GY X should be bijections
[66, 28, 61]. Speciﬁcally, when feeding an example x ∈ X
into the networks GXY ◦ GY X : X → Y → X, the output
should be a reconstruction of x and vise versa for y, i.e.,
GY X (GXY (x)) ≈ x and GXY (GY X (y)) ≈ y. Further,
DistanceGAN [5] showed that maintaining the distances
between images within domains allows one-sided unsuper-
vised domain mapping.

Existing constraints overlook the special properties of
images that simple geometric transformations (global geo-
metric transformations without shape deformation) do not
change the image’s semantic structure. Here, semantic
structure refers to the information that distinguishes differ-
ent object/staff classes, which can be easily perceived by
humans regardless of trivial geometric transformations such
as rotation. Based on this property, we develop a geometry-
consistency constraint, which helps in reducing the search
space of possible solutions while still keeping the correct set
of solutions under consideration, and results in a geometry-
consistent generative adversarial network (GcGAN).

Our geometry-consistency constraint is motivated by the

12427

Input

Ground Truth

GAN alone

GAN alone (rot)

GcGAN

GcGAN (rot)

Figure 1: Geometry consistency. The original input image is denoted by x, and the predeﬁned function f (·) is a 90◦ clockwise rotation
(rot). GAN alone: G1
˜X ˜Y (f (x)). It can be seen
that GAN alone produces geometrically-inconsistent output images, indicating that the learned GXY and G ˜X ˜Y are far away from the
correct mapping functions. By enforcing geometry consistency, our method results in more sensible domain mapping.

XY (x). GAN alone (rot): f −1(G1

XY (x). GcGAN (rot): f −1(G2

˜X ˜Y (f (x))). GcGAN: G2

fact that a given geometric transformation f (·) between
the input images should be preserved by related transla-
tors GXY and G ˜X ˜Y , if ˜X and ˜Y are the domains obtained
by applying f (·) on the examples of X and Y , respec-
tively. Mathematically, given a random example x from the
source domain X and a predeﬁned geometric transforma-
tion function f (·), geometry consistency can be expressed
as f (GXY (x)) ≈ G ˜X ˜Y (f (x)) and f −1(G ˜X ˜Y (f (x))) ≈
GXY (x), where f −1(·) is the inverse function of f (·). Be-
cause it is unlikely that GXY and G ˜X ˜Y always fail in the
same location, GXY and G ˜X ˜Y co-regularize each other by
the geometry-consistency constraint and thus correct each
others’ failures in local regions of their respective trans-
lations (see Figure 1 for an illustration). Our geometry-
consistency constraint allows one-sided unsupervised do-
main mapping, i.e., GXY can be trained independently from
GY X . In this paper, we employ two simple but represen-
tative geometric transformations as examples, i.e., vertical
ﬂipping (vf ) and 90 degrees clockwise rotation (rot), to il-
lustrate geometry consistency. Quantitative and qualitative
comparisons with the baseline (GAN alone) and the state-
of-the-art methods including CycleGAN [66] and Distance-
GAN [5] demonstrate the effectiveness of our method.

2. Related Work

Generative Adversarial Networks. Generative adver-
sarial networks (GANs) [21, 45, 14, 47, 51, 3] learn two
networks, i.e., a generator and a discriminator, in a staged
zero-sum game fashion to generate images from inputs.
Many tasks have recently been developed based on deep
convolutional GANs, such as image inpainting, style trans-
fer, and domain adaptation [7, 62, 46, 48, 31, 60, 9, 52, 23,
53, 64, 27, 50, 19, 18, 35, 63]. The key component enabling
GANs is the adversarial constraint, which enforces the
generated images to be indistinguishable from real images.

Domain Mapping. Recent adversarial domain mapping
has been studied in a supervised or unsupervised manner
with respect to paired or unpaired inputs. There are a va-

riety of literatures [46, 31, 26, 59, 56, 58, 25, 37, 4, 10]
on supervised domain mapping. One representative exam-
ple is conditional GAN [26], which learned the discrimina-
tor to distinguish (x, y) and (x, GXY (x)) instead of y and
GXY (x), where (x, y) is a meaningful pair across domains.
Further, Wang et al. [59] showed that conditional GANs can
be used to generate high-resolution images with a novel fea-
ture matching loss, as well as multi-scale generator and dis-
criminator architectures. While there has been signiﬁcant
progress in supervised domain mapping, many real-word
applications can not provide aligned images across domains
because data preparation is expensive. Thus, different con-
straints and frameworks have been proposed for image-to-
image translation in the absence of training pairs.

In unsupervised domain mapping, only unaligned exam-
ples in individual domains are provided, making the task
more practical but more difﬁcult. Unpaired domain map-
ping has a long history, and some successes in adversarial
networks have recently been presented [40, 66, 5, 39, 42,
38, 6, 11]. For example, Liu and Tuzel [40] introduced
coupled GAN (CoGAN) to learn cross-domain represen-
tations by enforcing a weight-sharing constraint. Subse-
quently, CycleGAN [66], DiscoGAN [28], and DualGAN
[61] enforced that translators GXY and GY X should be
bijections. Thus, jointly learning GXY and GY X by en-
forcing cycle consistency can help to produce convinc-
ing mappings. Since then, many constraints and assump-
tions have been proposed to improve cycle consistency
[8, 17, 24, 32, 34, 11, 2, 67, 20, 44, 39, 36, 1]. Recently, Be-
naim and Wolf [5] reported that maintaining the distances
between samples within domains allows one-sided unsuper-
vised domain mapping. GcGAN is also a one-sided frame-
work coupled with our geometry-consistency constraint,
and produces competitive and even better translations than
the two-sided CycleGAN in various applications.

3. Preliminaries

Let X and Y be two domains with unpaired training ex-
j=1, where xi and yj are drawn

i=1 and {yj}M

amples {xi}N

2428

IM

"EF

JN

"FE

&
JM

&
IN

"FE

"EF

&&
IM

&&
JN

IM

"EF

&
JM

IM

P(IM, IN)

P(JM

&, JN
&)

=(IM)

"EF

share

parameters

&
JM

&)
=(JM

IN

"EF

&
JN

IKM

"EOFO

&
JKM

&)
=-.(JKM

	cyclic	reconstruction
	for	cycle	consistency

&
JM

&
IN

/F

/E

*/1

*/1

preserving	P 3 	

for	distance	consistency

preserving	= 3 	

for	geometry	consistency

&
JM

&
JN

/F

/F

*/1

*/1

&
JM

&
JKM

/F

/FO

*/1

*/1

CycleGAN

DistanceGAN

GcGAN

Figure 2: An illustration of the differences between CycleGAN [66], DistanceGAN [5], and our GcGAN. x and y are random examples
from domain X and Y, respectively. d(xi, xj) is the distance between images xi and xj . f (·) is a predeﬁned geometric transformation
function for images, which satisﬁes f −1(f (x)) = f (f −1(x)) = x. GXY and G ˜X ˜Y are the generators (or translators) which target
the domain translation tasks from X to Y and ˜X to ˜Y, where ˜X and ˜Y are two domains obtained by applying f (·) on all the images
in X and Y, respectively. DY is an adversarial discriminator in domain Y. The red dotted lines denote the unsupervised constraints
with respect to cycle consistency (x ≈ GY X (GXY (x))), distance consistency (x ≈ GY X (GXY (x))), and our geometry consistency
(f (GXY (x)) ≈ G ˜X ˜Y (f (x))), respectively.

from the marginal distributions PX and PY , where X and
Y are two random variables associated with X and Y, re-
spectively. In the paper, we exploit style transfer without
undesirable semantic distortions, and have two goals. First,
we need to learn a mapping GXY such that GXY (X) has
the same distribution as Y , i.e., PGXY (X) ≈ PY . Second,
the learned mapping function only changes the image style
without distorting the semantic structures.

i=1 and {˜yj}M

While many works have modeled the invertibility be-
tween GXY and GY X for convincing mappings since the
success of CycleGAN, here we propose to enforce geom-
etry consistency as a constraint that allows one-sided do-
main mapping. Let f (·) be a predeﬁned geometric trans-
formation. We can obtain two extra domains ˜X and ˜Y
with examples {˜xi}N
j=1 by applying f (·) on
X and Y , respectively. We learn an additional transla-
tor G ˜X ˜Y : ˜X → ˜Y while learning GXY : X → Y ,
and introduce our geometry-consistency constraint based
on the predeﬁned transformation such that the two net-
works can co-regularize each other. Our framework en-
forces that GXY (x) and G ˜X ˜Y (˜x) should keep the same ge-
ometric transformation with the one between x and ˜x, i.e.,
f (GXY (x)) ≈ G ˜X ˜Y (˜x), where ˜x = f (x). We denote the
two adversarial discriminators as DY and D ˜Y with respect
to domains Y and ˜Y, respectively.

4. Proposed Method

We present our geometry-consistency constraint and Gc-
GAN beginning with a review of the cycle-consistency con-
straint and the distance constraint. An illustration of the dif-
ferences between these constraints is shown in Figure 2.

4.1. Unsupervised Constraints

Cycle-consistency constraint.
Following the cycle-
consistency assumption [28, 66, 61], through the translators
GXY ◦ GY X : X → Y → X and GY X ◦ GXY : Y →
X → Y , the examples x and y in domain X and Y should
recover the original images, i.e., x ≈ GY X (GXY (x)) and
y ≈ GXY (GY X (y)). Cycle consistency is implemented
by a bidirectional reconstruction process that requires GXY
and GY X to be jointly learned, as shown in Figure 2 (Cycle-
GAN). The cycle consistency loss Lcyc(GXY , GY X , X, Y )
takes the form as:

Lcyc = Ex∼PX [kGY X (GXY (x)) − xk1]

+ Ey∼PY [kGXY (GY X (y)) − yk1].

(1)

Distance constraint. The assumption behind the distance
constraint is that the distance between two examples xi and
xj in domain X should be preserved after mapping to do-
main Y , i.e., d(xi, xj) ≈ a · d(GXY (xi), GXY (xj)) + b,
where d(·) is a predeﬁned function to measure the distance
between two examples and a and b are the linear coefﬁcient
and bias. In DistanceGAN [5], the distance consistency loss
Ldis(GXY , X, Y ) is the exception to the absolute differ-
ences between distances:

Ldis = Exi,xj ∼PX [|φ(xi, xj) − ψ(xi, xj)|],

φ(xi, xj) =

ψ(xi, xj) =

1
σX
1
σY

(kxi − xjk1 − µX ),

(2)

(kGXY (xi) − GXY (xj)k1 − µY ),

where µX , µY (σX , σY ) are the means (standard devia-
tions) of distances of all the possible pairs of (xi, xj) within

2429

domain X and (yi, yj) within domain Y, respectively.

4.2. Geometry consistent Generative Adversarial

Lgeo during the model training. Carefully tuning λ may
give preferable results to speciﬁc translation tasks.

Networks

Adversarial constraint. Taking GXY as an example, an
adversarial loss Lgan(GXY , DY , X, Y ) [21] enforces GXY
and DY to simultaneously optimize each other in an mini-
max game, i.e., minGXY maxDY Lgan(GXY , DY , X, Y ). In
other words, DY aims to distinguish real examples {y}
from translated samples {GXY (x)}. By contrast, GXY
aims to fool DY so that DY can label a fake example
y′ = GXY (x) as a sample satisfying y′ ∼ PY . The ob-
jective can be expressed as:

Lgan = Ey∼PY [log DY (y)]

+ Ex∼PX [log(1 − DY (GXY (x)))].

(3)

In the transformed domains ˜X and ˜Y, we employ the
adversarial loss Lgan(G ˜X ˜Y , D ˜Y , ˜X, ˜Y ) that has the same
form to Lgan(GXY , DY , X, Y ).

Geometry-consistency constraint. As shown in Figure 2
(GcGAN), given a predeﬁned geometric transformation
function f (·), we feed the images x ∈ X and ˜x = f (x) into
the translators GXY and G ˜X ˜Y , respectively. Following our
geometry-consistency constraint, the outputs y′ = GXY (x)
and ˜y′ = G ˜X ˜Y (˜x) should also satisfy ˜y′ ≈ f (y′) like x and
˜x. Considering both f (·) and the inverse geometric transfor-
mation function f −1(·), our complete geometry consistency
loss Lgeo(GXY , G ˜X ˜Y , X, Y ) has the following form:
Lgeo = Ex∼PX [kGXY (x) − f −1(G ˜X ˜Y (f (x)))k1]
+ Ex∼PX [kG ˜X ˜Y (f (x)) − f (GXY (x))k1].

(4)

This geometry-consistency loss can be seen as a recon-
struction loss that relies on the predeﬁned geometric
transformation function f (·).
In this paper, we only take
two common geometric transformations as examples,
namely vertical ﬂipping (vf ) and 90◦ clockwise rotation
(rot), to demonstrate the effectiveness of our geometry-
consistency constraint. Note that, GXY and G ˜X ˜Y have the
same architecture and share all the parameters.

Full objective. By combining our geometry-consistency
constraint with the standard adversarial constraint, a re-
markable one-sided unsupervised domain mapping can
be targeted.
The full objective for our GcGAN
LGcGAN (GXY , G ˜X ˜Y , DY , D ˜Y , X, Y ) will be:

LGcGAN = Lgan(GXY , DY , X, Y )

+ Lgan(G ˜X ˜Y , D ˜Y , X, Y )
+ λLgeo(GXY , G ˜X ˜Y , X, Y ),

(5)

where λ (λ = 20.0 in all the experiments) is a trade-off
hyperparameter to weight the contribution of Lgan and

Network architecture. The full framework of our GcGAN
is illustrated in Figure 2. Our experimental settings,
network architectures, and learning strategies follow Cy-
cleGAN. We employ the same discriminator and generator
as CycleGAN depending on the speciﬁc tasks. Speciﬁcally,
the generator is a standard encoder-decoder, where the
encoder contains two convolutional layers with stride 2 and
several residual blocks [22] (6 / 9 blocks with respect to
128 × 128 / 256 × 256 of input resolution), and the decoder
contains two deconvolutional layers also with stride 2.
The discriminator distinguishes images at the patch level
following PatchGANs [26, 33]. Like CycleGAN, we also
use an identity mapping loss [55] in all of our experiments
(except SVHN → MNIST), including our baseline (GAN
alone). For other details, we use LeakyReLU as nonlinear-
ity for the discriminators and instance normalization [57]
to normalize convolutional feature maps.

Learning and inference. We use the Adam solver [29] with
a learning rate of 0.0002 and coefﬁcients of (0.5, 0.999),
where the latter is used to compute running averages of gra-
dients and their squares. The learning rate is ﬁxed in the
initial 100 epochs, and linearly decays to zero over the next
100 epochs. Following CycleGAN, the negative log likeli-
hood objective is replaced with a more stable and effective
least-squares loss [43] for Lgan. The discriminator is up-
dated with random samples from a history of generated im-
ages stored in an image buffer [54] of size 50. The generator
and discriminator are optimized alternately. In the inference
phase, we feed an image only into the learned generator
GXY to obtain a translated image.

5. Experiments

We apply our GcGAN to a wide range of applications
and make both quantitative and qualitative comparisons
with the baseline (GAN alone) and previous state-of-the-
art methods including DistanceGAN and CycleGAN. We
also study different ablations (based on rot) to analyze
our geometry-consistency constraint. Since adversarial net-
works are not always stable, every independent experiment
could result in slightly different scores. The scores in the
quantitative analysis are computed by the average on three
independent experiments.

5.1. Quantitative Analysis

The results demonstrate that our geometry-consistency
constraint can not only partially ﬁlter out the candidate so-
lutions having mode collapse or semantic distortions and
thus produce more sensible translations, but also compati-

2430

Input

Ground Truth

GAN alone

CycleGAN

GcGAN

Input

Ground Truth

GAN alone

GcGAN

Input

Ground Truth

GAN alone

GcGAN

Figure 3: Qualitative comparison on Cityscapes (Parsing ⇋ Image) and Google Maps (Map ⇋ Aerial photo). GAN alone suffers from
mode collapse. Translated images by GcGAN contain more details. GcGAN = GAN alone + geometry consistency.

ble with other unsupervised constraints such as cycle con-
sistency [66] and distance preservation [5].

est distance. Then, the aforementioned metrics are used to
evaluate our mapping on the 19 category labels.

Cityscapes. Cityscapes [12] contains 3975 image-label
pairs, with 2975 used for training and 500 for validation
(test in this paper). For a fair comparison with CycleGAN,
the translators are trained at a resolution of 128 × 128 in
an unaligned fashion. We evaluate our domain mappers us-
ing FCN scores and scene parsing metrics following pre-
vious works [41, 12, 66]. Speciﬁcally, for parsing → im-
age, we assume that a high-quality translated image should
produce qualitative semantic segmentation like real images
when feeding it into a scene parser. Thus, we employ the
pretrained FCN-8s [41] provided by pix2pix [26] to pre-
dict semantic labels for the 500 translated images. The
label maps are then resized to the original resolution of
1024 × 2048 and compared against the ground truth labels
using some standard scene parsing metrics including pixel
accuracy, class accuracy, and mean IoU [41]. For image
→ parsing, since the fake labels are in the RGB format, we
simply convert them into class-level labels using the nearest
neighbor search strategy. In particular, we have 19 (cate-
gory labels) + 1 (ignored label) categories for Cityscapes,
each with a corresponding color value (RGB). For a pixel i
in a translated parsing, we compute the distances between
the 20 groundtruth color values and the color value of pixel
i. The label of pixel i should be the one with the small-

The parsing scores are presented in Table 1. Our Gc-
GAN outperforms the baseline (GAN alone) by a large mar-
gin. We take the average of pixel accuracy, class accu-
racy, and mean IoU as the ﬁnal score for analysis [65], i.e.,
score = (pixel acc + class acc + mean IoU)/3. For im-
age → parsing, GcGAN (32.6%) yields a slightly higher
score than CycleGAN (32.0%). For parsing → image, Gc-
GAN (29.0% ∼ 29.5%) obtains a convincing improvement
of 1.3% ∼ 1.8% over distanceGAN (27.7%).

We next perform ablation studies to further discuss Gc-
GAN. The scores are reported in Table 1. Speciﬁcally,
GcGAN-rot-Seperate shows that the generator GXY em-
ployed in GcGAN is sufﬁcient to handle both the style trans-
fers (without shape deformation) X → Y and ˜X → ˜Y .
GcGAN-Mix-{comb, rand} demonstrate that persevering a
geometric transformation can ﬁlter out most of the candi-
date solutions having mode collapse or undesired shape de-
formation, but preserving more ones can not leach more.
Besides, GcGAN-Mix-rand performs slightly worse than
GcGAN-Mix-comb. One of the possible reasons is that nei-
ther Xrot→Yrot nor Xvf→Yvf are sufﬁciently trained in the
random case, which would decrease the effect of the afore-
mentioned co-regularization mechanism. For GcGAN-rot
+ Cycle, we set the trade-off parameter for Lcyc to 10.0 as
published in CycleGAN. The consistent improvement is a

2431

method

image → parsing

parsing → image

pixel acc

class acc mean IoU pixel acc

class acc mean IoU

CoGAN [40]

BiGAN/ALI [15, 16]

SimGAN [54]

CycleGAN (Cycle) [66]

DistanceGAN [5]

GAN alone (baseline)

GcGAN-rot
GcGAN-vf

Benchmark Performance

0.45
0.41
0.47
0.58

-

0.514
0.574
0.576

0.11
0.13
0.11
0.22

-

0.160
0.234
0.232

0.08
0.07
0.07
0.16

-

0.104
0.170
0.171

0.40
0.19
0.20
0.52
0.53
0.437
0.551
0.548

LGcGAN w/o Lgeo (λ = 0)
LGcGAN w/o Lgan( ˜X, ˜Y )

GcGAN-rot-Seperate
GcGAN-Mix-comb
GcGAN-Mix-rand
GcGAN-rot + Cycle

Ablation Studies (Robustness & Compatibility)
0.396
0.526
0.545
0.545
0.547
0.557

0.102
0.139
0.170
0.168
0.156
0.182

0.486
0.549
0.575
0.573
0.564
0.587

0.163
0.199
0.233
0.229
0.217
0.246

0.10
0.06
0.10
0.17
0.19
0.161
0.197
0.196

0.148
0.184
0.196
0.197
0.192
0.201

0.06
0.02
0.04
0.11
0.11
0.098
0.129
0.127

0.088
0.111
0.124
0.128
0.123
0.132

Table 1: Parsing scores on Cityscapes. LGcGAN : The objective in Eqn. 5 with rot. GcGAN-rot-Separate: GXY and G ˜X ˜Y do not share
parameters. GcGAN-Mix-comb: Training GcGAN with both vf and rot in each iteration. GcGAN-Mix-rand: Training GcGAN with
randomly chosen vf and rot in each iteration. GcGAN-rot + Cycle: GcGAN-rot with the cycle-consistency constraint.

method

class acc (%)

erate preferable translations.

Benchmark Performance

DistanceGAN (Dist.) [5]
CycleGAN (Cycle) [66]

Self-Distance [5]

GcGAN-rot
GcGAN-vf

26.8
26.1
25.2
32.5
33.3

Ablation Studies (Compatibility)
Cycle + Dist. [5]

GcGAN-rot + Dist.
GcGAN-rot + Cycle

GcGAN-rot + Dist. + Cycle

18.0
34.0
33.8
33.2

Table 2: Quantitative scores for SVHN → MNIST.

credible support that our geometry-consistency constraint
is compatible with the widely-used cycle-consistency con-
straint. Moreover, when setting λ = 0 in LGcGAN , both
GXY and GY X perform badly. One of the possible rea-
sons is that, without the geometry consistency constraint,
jointly modeling X→Y and ˜X→ ˜Y with the shared gen-
erator GXY would decrease the performance due to do-
main diversities caused by the geometric transformations.
When removing Lgan(G ˜X ˜Y , D ˜Y ), the obtained scores are
much higher than baseline (GAN alone) because Y ′ can
partially correct ˜Y ′ so that GXY is able to handle the map-
ping ˜X→ ˜Y , and then ˜Y ′ can constrain the mapping X→Y .
As the analysis, when learning both Lgan(GXY , DY ) and
Lgan(G ˜X ˜Y , D ˜Y ) with Lgeo, the co-regularization help gen-

SVHN → MNIST. We apply our approach to the SVHN →
MNIST translation task. The translation models are trained
on 73257 and 60000 training images of resolution 32 × 32
contained in the SVHN and MNIST training sets, respec-
tively. The experimental settings follow DistanceGAN [5],
including the default trade-off parameters for Lcyc and Ldis.
We compare our GcGAN with both DistanceGAN and Cy-
cleGAN in this translation task. To obtain quantitative re-
sults, we feed the translated images into a pretrained classi-
ﬁer trained on the MNIST training split, as done in [5].

Classiﬁcation accuracies are reported in Table 2. Both
GcGAN-rot and GcGAN-vf outperform DistanceGAN and
CycleGAN by a large margin (about 6% ∼ 7%). From
the ablations, adding our geometry-consistency constraint
to current unsupervised domain mapping frameworks will
achieve different levels of improvements against the origi-
nal ones. Note that, it seems that the distance-preservation
constraint is not compatible with the cycle-consistency con-
straint on this task, but our geometry-consistency constraint
can improve both ones.

Google Maps. We obtain 2194 (map, aerial photo) pairs of
images in and around New York City from Google Maps
[26], and split them into training and test sets with 1096
and 1098 pairs, respectively. We train Map ⇋ Aerial photo
translators with an image size of 256×256 using the training
set in an unsupervised manner (unpaired) by ignoring the
pair information. For Aerial photo → Map, we make com-

2432

DistanceGAN [5]

GcGAN

Figure 4: Qualitative comparison for SVHN → MNIST.

i|, |gi − g′

parisons with CycleGAN using average RMSE and pixel
accuracy (%). Given a pixel i with the ground-truth RGB
value (ri, gi, bi) and the predicted RGB value (r′
i), if
max(|ri − r′
i|) < δ, we consider this is
an accurate prediction. Since maps only contain a limited
number of different RGB values, it is reasonable to compute
pixel accuracy using this strategy (δ1 = 5 and δ2 = 10 in
this paper). For Map → Aerial photo, we only show some
qualitative results in Figure 3.

i|, |bi − b′

i, g′

i, b′

method

RMSE acc (δ1)

acc (δ2)

Benchmark Performance

CycleGAN [66]

GAN alone (baseline)

GcGAN-rot
GcGAN-vf

28.15
33.27
28.31
28.50

41.8
19.3
41.2
37.3

63.7
42.0
63.1
58.9

Ablation Studies (Robustness & Compatibility)
60.8
64.6
63.5

GcGAN-rot-Separate
GcGAN-Mix-comb
GcGAN-rot + Cycle

30.25
27.98
28.21

40.7
42.8
40.6

Table 3: Quantitative scores for Aerial photo → Map.

From the scores presented in Table 3, GcGAN produces
superior translations to the baseline (GAN alone). In partic-
ular, GcGAN yields an 18.0% ∼ 21.9% improvement over
the baseline with respect to pixel accuracy when δ = 5.0,
demonstrating that the fake maps obtained by our GcGAN
contain more details. In addition, GcGANs achieve com-
petitive scores compared with CycleGAN.

5.2. Qualitative Evalutation

The qualitative results are shown in Figure 3, Figure 4,
and Figure 5. Our geometry-consistency constraint improve
the training of GAN alone, and helps to generate empiri-
cally more impressive translations on various applications.
The following applications are trained in the image size of
256 × 256 with the rot geometric transformation.

Horse → Zebra. We apply GcGAN to the widely studied
object transﬁguration application task, i.e., Horse → Zebra.
The images are randomly sampled from ImageNet [13] us-
ing the keywords (i.e., wild horse and zebra). The numbers
of training images are 939 and 1177 for horse and zebra, re-

spectively. We ﬁnd that training GcGAN without parameter
sharing would produce preferable translations for the task.
Synthetic ⇋ Real. We employ the 2975 training images
from Cityscapes as the real-world scenes, and randomly se-
lect 3060 images from SYNTHIA-CVPR16 [49], which is
a virtual urban scene benchmark, as the synthetic images.
Summer ⇋ Winter. The images used for the season trans-
lation tasks are provided by CycleGAN. The training set
sizes for Summer and Winter are 1273 and 854.
Photo ⇋ Artistic Painting. We translate natural images to
artistic paintings with different art styles, including Monet,
Cezanne, Van Gogh, and Ukiyo-e. We also perform Gc-
GAN on the translation task of Monet’s paintings → pho-
tographs. We use the photos and paintings (Monet: 1074,
Cezanne: 584, Van Gogh: 401, Ukiyo-e: 1433, and Pho-
tographs: 6853) collected by CycleGAN for training.
Day ⇋ Night. We randomly extract 4500 training images
for both Day and Night from the 91 webcam sequences cap-
tured by [30].

6. Conclusion

In this paper, we propose to enforce geometry consis-
tency as a constraint for unsupervised domain mapping,
which can be viewed as a predeﬁned geometric transforma-
tion f (·) preserving the geometry of a scene. The geometry-
consistency constraint makes the translation networks on
the original images and transformed images co-regularize
each other, which not only provides an effective remedy to
the mode collapse problem suffered by standard GANs, but
also reduces the semantic distortions in the translation. We
evaluate our model, i.e., the geometry-consistent generative
adversarial network (GcGAN), both qualitatively and quan-
titatively in various applications. The experimental results
demonstrate that GcGAN achieves competitive and some-
times even better translations than the state-of-the-art meth-
ods including DistanceGAN and CycleGAN. Finally, our
geometry-consistency constraint is compatible with other
well-studied unsupervised constraints.

7. Acknowledgement

This research was supported by Australian Research
Council Projects FL-170100117, DP-180103424, and IH-
180100002. This work was partially supported by SAP
SE and CNRS INS2I-JCJC-INVISANA. This work is par-
tially supported by NIH Award Number 1R01HL141813-
01, NSF 1839332 Tripod+X, and SAP SE. We gratefully
acknowledge the support of NVIDIA Corporation with the
donation of the Titan X Pascal GPU used for this research.
We were also grateful for the computational resources pro-
vided by Pittsburgh Super Computing grant number TG-
ASC170024.

2433

Horse → Zebra

Monet → Photo

Input

CycleGAN

GcGAN

Input

CycleGAN

GcGAN

Real → Synthetic

Synthetic → Real

Input

GAN alone

GcGAN

Input

GAN alone

GcGAN

Winter → Summer

Summer → Winter

Input

GAN alone

GcGAN

Input

GAN alone

GcGAN

Photo → Artistic Painting

Photographs

Monet

Cezanne

Van Gogh

Ukiyo-e

Day → Night

Night → Day

Input

GcGAN

Input

GcGAN

Input

GcGAN

Input

GcGAN

Figure 5: Qualitative results on different applications, including Horse → Zebra, Monet → Photo, Synthetic ⇋ Real, Summar ⇋ Winter,
Photo → Artist Painting, and Day ⇋ Night. GcGAN has the potential to produce realistic images. Zoom in for better view.

2434

References

[1] Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip
Bachman, and Aaron Courville. Augmented cyclegan:
Learning many-to-many mappings from unpaired data.
ICML, 2018. 2

[2] Asha Anoosheh, Eirikur Agustsson, Radu Timofte, and Luc
Van Gool. Combogan: Unrestrained scalability for image
domain translation. In CVPRW, 2018. 2

[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
2

[4] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen
Wang, Eli Shechtman, and Trevor Darrell. Multi-content gan
for few-shot font style transfer. In CVPR, 2018. 2

[5] Sagie Benaim and Lior Wolf. One-sided unsupervised do-

main mapping. In NIPS, 2017. 1, 2, 3, 5, 6, 7

[6] Sagie Benaim and Lior Wolf. One-shot unsupervised cross

domain translation. NIPS, 2018. 2

[7] Konstantinos Bousmalis, George Trigeorgis, Nathan Silber-
man, Dilip Krishnan, and Dumitru Erhan. Domain separa-
tion networks. In NIPS, 2016. 2

[18] Mingming Gong, Kun Zhang, Biwei Huang, Clark Gly-
mour, Dacheng Tao, and Kayhan Batmanghelich. Causal
generative domain adaptation networks.
arXiv preprint
arXiv:1804.04333, 2018. 2

[19] Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao,
Clark Glymour, and Bernhard Sch¨olkopf. Domain adapta-
tion with conditional transferable components.
In ICML,
pages 2839–2848, 2016. 2

[20] Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Ben-
gio. Image-to-image translation for cross-domain disentan-
glement. NIPS, 2018. 2

[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2, 4

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 4

[23] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adapta-
tion. ICML, 2018. 2

[8] Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel-
stein. Pairedcyclegan: Asymmetric style transfer for apply-
ing and removing makeup. In CVPR, 2018. 2

[24] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan
Kautz. Multimodal unsupervised image-to-image transla-
tion. ECCV, 2018. 2

[9] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang

Hua. Stereoscopic neural style transfer. In CVPR, 2018. 2

[10] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks. In ICCV, 2017.
2

[11] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR, 2018. 2

[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR),
2016. 5

[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 7

[14] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a? laplacian pyramid of ad-
versarial networks. In NIPS, 2015. 2

[15] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-
versarial feature learning. arXiv preprint arXiv:1605.09782,
2016. 6

[16] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier
Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. arXiv preprint
arXiv:1606.00704, 2016. 6

[17] Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie,
Kwang In Kim, and James Tompkin. Improving shape defor-
mation in unsupervised image-to-image translation. ECCV,
2018. 2

[25] Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Mat-
sushita, and Yasushi Yagi. Probabilistic plant modeling via
multi-view image-to-image translation. CVPR, 2018. 2

[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017. 1, 2, 4, 5, 6

[27] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016. 2

[28] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations
with generative adversarial networks. In ICML, 2017. 1, 2, 3

[29] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 4

[30] Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian,
and James Hays. Transient attributes for high-level un-
derstanding and editing of outdoor scenes. ACM TOG,
33(4):149, 2014. 7

[31] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, 2017. 2

[32] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. In ECCV, 2018.
2

[33] Chuan Li and Michael Wand. Precomputed real-time texture
synthesis with markovian generative adversarial networks. In
ECCV, 2016. 4

2435

[34] Minjun Li, Haozhi Huang, Lin Ma, Wei Liu, Tong Zhang,
and Yugang Jiang. Unsupervised image-to-image translation
with stacked cycle-consistent adversarial networks. ECCV,
2018. 2

[35] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang
Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza-
tion via conditional invariant adversarial networks. In ECCV,
pages 624–639, 2018. 2

[36] Xiaodan Liang, Hao Zhang, and Eric P Xing. Generative
semantic manipulation with contrasting gan. NIPS, 2017. 2

[37] Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-
Yan Liu. Conditional image-to-image translation. In CVPR,
2018. 2

[38] Alexander Liu, Yen-Chen Liu, Yu-Ying Yeh, and Yu-
Chiang Frank Wang. A uniﬁed feature disentangler for multi-
domain image translation and manipulation. NIPS, 2018. 2

[39] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised

image-to-image translation networks. In NIPS, 2017. 2

[40] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversar-

ial networks. In NIPS, 2016. 2, 6

[41] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 5

[42] Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei.
Da-gan: Instance-level image translation by deep attention
generative adversarial networks. In CVPR, 2018. 2

[43] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In ICCV, 2017. 4

[44] Youssef A Mejjati, Christian Richardt, James Tompkin, Dar-
ren Cosker, and Kwang In Kim. Unsupervised attention-
guided image to image translation. NIPS, 2018. 2

[45] A¨aron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse
Espeholt, Alex Graves, and Koray Kavukcuoglu. Condi-
tional image generation with pixelcnn decoders.
In NIPS,
2016. 2

[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR, 2016. 1, 2

[47] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016. 2

[48] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML, 2016. 2

[49] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M. Lopez. The synthia dataset: A
large collection of synthetic images for semantic segmenta-
tion of urban scenes. In CVPR, 2016. 7

[50] Am´elie Royer, Konstantinos Bousmalis, Stephan Gouws,
Fred Bertsch, Inbar Moressi, Forrester Cole, and Kevin Mur-
phy. Xgan: Unsupervised image-to-image translation for
many-to-many mappings. ICLR, 2018. 2

[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NIPS, 2016. 2

[52] Falong Shen, Shuicheng Yan, and Gang Zeng. Neural style

transfer via meta networks. In CVPR, 2018. 2

[53] Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatar-
net: Multi-scale zero-shot style transfer by feature decora-
tion. In CVPR, 2018. 2

[54] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Joshua
Susskind, Wenda Wang, and Russell Webb.
Learning
from simulated and unsupervised images through adversarial
training. In CVPR, 2017. 4, 6

[55] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised

cross-domain image generation, 2016. 4

[56] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor Lempitsky. Texture networks: feed-forward synthesis of
textures and stylized images. In ICML, 2016. 2

[57] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022, 2016. 4

[58] Chao Wang, Haiyong Zheng, Zhibin Yu, Ziqiang Zheng,
Zhaorui Gu, and Bing Zheng. Discriminative region pro-
posal adversarial networks for high-quality image-to-image
translation. In ECCV, 2018. 1, 2

[59] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional gans.
arXiv preprint arXiv:1711.11585, 2017. 1, 2

[60] Yaxing Wang, Joost van de Weijer, and Luis Herranz. Mix
and match networks: encoder-decoder alignment for zero-
pair image translation. In CVPR, 2018. 2

[61] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan:
Unsupervised dual learning for image-to-image translation.
In CVPR, 2017. 1, 2, 3

[62] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In ICCV, 2017. 2

[63] Kun Zhang, Bernhard Sch¨olkopf, Krikamol Muandet, and
Zhikun Wang. Domain adaptation under target and condi-
tional shift. In ICML, pages 819–827, 2013. 2

[64] Yexun Zhang, Ya Zhang, and Wenbin Cai. Separating style
and content for generalized style transfer. In CVPR, 2018. 2
[65] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Scene parsing through

Barriuso, and Antonio Torralba.
ade20k dataset. In CVPR, 2017. 5

[66] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks.
In CVPR, 2017. 1, 2, 3,
5, 6, 7

[67] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In NIPS, 2017.
2

2436

