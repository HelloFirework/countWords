Steady-state Non-Line-of-Sight Imaging

Wenzheng Chen1

,

2* Simon Daneau1

,

3 Fahim Mannan1 Felix Heide1

4

,

1Algolux 2University of Toronto 3Universit´e de Montr´eal

4Princeton University

Abstract

Conventional intensity cameras recover objects in the di-
rect line-of-sight of the camera, whereas occluded scene
parts are considered lost in this process. Non-line-of-sight
imaging (NLOS) aims at recovering these occluded objects
by analyzing their indirect reﬂections on visible scene sur-
faces. Existing NLOS methods temporally probe the indirect
light transport to unmix light paths based on their travel
time, which mandates specialized instrumentation that suf-
fers from low photon efﬁciency, high cost, and mechanical
scanning. We depart from temporal probing and demon-
strate steady-state NLOS imaging using conventional in-
tensity sensors and continuous illumination. Instead of as-
suming perfectly isotropic scattering, the proposed method
exploits directionality in the hidden surface reﬂectance,
resulting in (small) spatial variation of their indirect re-
ﬂections for varying illumination. To tackle the shape-
dependence of these variations, we propose a trainable ar-
chitecture which learns to map diffuse indirect reﬂections to
scene reﬂectance using only synthetic training data. Rely-
ing on consumer color image sensors, with high ﬁll factor,
high quantum efﬁciency and low read-out noise, we demon-
strate high-ﬁdelity color NLOS imaging for scene conﬁgu-
rations tackled before with picosecond time resolution.

1. Introduction

Recovering objects from conventional monocular im-
agery represents a central challenge in computer vision,
with a large body of work on sensing techniques using
controlled illumination with spatial [50, 41] or temporal
coding [32, 24, 19, 39], multi-view reconstruction meth-
ods [18], sensing via coded optics [47], and recently learned
reconstruction methods using single-view monocular im-
ages [49, 11, 16]. While these sensing methods drive
applications across domains, including autonomous vehi-
cles, robotics, augmented reality, and dataset acquisition
for scene understanding [52], they only recover objects in
the direct line-of-sight of the camera. This is because ob-

*The majority of this work was done while interning at Algolux.

Figure 1: We demonstrate that it is possible to image occluded
objects outside the direct line-of-sight using continuous illumina-
tion and conventional cameras, without temporal sampling. We
sparsely scan a diffuse wall with a beam of white light and recon-
struct “hidden” objects only from spatial variations in steady-state
indirect reﬂections.

jects outside the line-of-sight only contribute to a measure-
ment through indirect reﬂections via visible diffuse object
surfaces. These reﬂections are extremely weak due to the
multiple scattering, and they lose (most) angular informa-
tion on the diffuse scene surface (as opposed to a mirror
surface in the scene). NLOS imaging aims at recovering
objects outside a camera’s line-of-sight from these indirect
light transport components.

To tackle the lack of angular resolution, a number of
NLOS approaches have been described that temporally
probe the light-transport in the scene, thereby unmixing
light path contributions by their optical path length [1, 30,
36, 43] and effectively trading angular with temporal res-
olution. To acquire temporally resolved images of light
transport, existing methods either directly sample the tem-
poral impulse response of the scene by recording the tem-
poral echoes of laser pulses [54, 43, 17, 7, 53, 3, 42], or
they use amplitude-coded illumination and time-of-ﬂight
sensors [21, 26, 25]. While amplitude coding approaches

16790

suffer from low temporal resolution due to sensor demod-
ulation bandwidth limitations [32] and the corresponding
ill-posed inverse problem [19], direct probing methods
achieve high temporal resolution already in the acquisition
phase, but in turn require ultra-short pulsed laser illumi-
nation and detectors with < 10 ps temporal resolution for
macroscopic scenes. This mandates instrumentation with
high temporal resolution, that suffers from severe practi-
cal limitations including low photon efﬁciency, large mea-
surement volumes, high-resolution timing electronics, ex-
cessive cost and monochromatic acquisition. Early streak-
camera setups [54] hence require hours of acquisition time,
and, while emerging single photon avalance diode (SPAD)
detectors [7, 42] are sensitive to individual photons, they
are in fact photon-inefﬁcient (diffuse experiments in [42])
due to very low ﬁll factors and pileup distortions at higher
pulse power. To overcome this issue without excessive
integration times, recent approaches [42, 20] restrict the
scene to retro-reﬂective material surfaces, which eliminates
quadratic falloff from these surfaces, but effectively also
constrains practical use to a single object class.

In this work, we demonstrate that it is possible to im-
age objects outside of the direct line-of-sight using conven-
tional intensity sensors and continuous illumination, with-
out temporal coding. In contrast to previous methods, that
assume perfectly isotropic reﬂectance, the proposed method
exploits directionality of the hidden object’s reﬂectance, re-
sulting in spatial variation of the indirect reﬂections for
varying illumination. To handle the shape-dependence of
these variations, we learn a deep model trained using a train-
ing corpus of simulated indirect renderings. By relying on
consumer color image sensors, with high ﬁll factor, high
quantum efﬁciency and low read-out noise, we demonstrate
full-color NLOS imaging at fast imaging rates and in setup
scenarios identical to those tackled by recent pulsed systems
with picosecond resolution.

Speciﬁcally, we make the following contributions:

• We formulate an image formation model for steady-
state NLOS imaging and an efﬁcient implementation
without ray-tracing. Based on this model, we derive
an optimization method for the special case of planar
scenes with known reﬂectance.

• We propose a learnable architecture for steady-state

NLOS imaging for representative object classes.

• We validate the proposed method in simulation, and
experimentally using setup and scene speciﬁcations
identical to the ones used in previous time-resolved
methods. We demonstrate that the method generalizes
across objects with different reﬂectance and shapes.

2. Related Work

Transient Imaging Kirmani et al. [30] ﬁrst proposed the
concept of recovering “hidden” objects outside a camera’s
direct line-of-sight using temporally resolved light trans-
port measurements in which short pulses of light are cap-
tured “in ﬂight” before the global transport reaches a steady
state. These transient measurements are the temporal im-
pulse response of light transport in the scene. Abram-
son [1] ﬁrst demonstrated a holographic capture system
for transient imaging, and Velten et al. [55] showed the
ﬁrst experimental NLOS imaging results using a femto-
second laser and streak camera system. Since these seminal
works, a growing body of work has been exploring transient
imaging with a focus on enabling improved NLOS imag-
ing [43, 36, 56, 17, 21, 19, 7, 38].

Impulse Non-Line-of-Sight-Imaging A major line of re-
search [43, 54, 17, 42, 53, 3, 45, 40, 58] proposes to ac-
quire transient images directly, by sending pulses of light
into the scene and capturing the response with detectors ca-
pable of high temporal sampling. While the streak camera
setup from Velten et al. [55] allows for temporal precision
of < 10 ps, corresponding to a path length of 3 mm, the
high instrumentation cost and sensitivity has sparked work
on single photon avalanche diodes (SPADs) as a detector
alternative [7, 40]. Recently, O’Toole et al. [40] propose
scanned SPAD capture setup that allows for computational
efﬁciency by modeling transport as a shift-invariant con-
volution. Although SPAD detectors can offer comparable
resolution < 10 ps [37], they typically suffer from low ﬁll
factors typically around a few percent [44] and low spatial
resolution in the kilo-pixel range [35]. Compared to ubiq-
uitous intensity image sensors with > 10 megapixel reso-
lution, current SPAD sensors are still ﬁve orders of magni-
tude more costly, and two orders of magnitude less photon-
efﬁcient.

Modulated and Coherent Non-Line-of-Sight-Imaging
As an alternative to impulse-based acquisition, correlation
time-of-ﬂight setups have been proposed [19, 25, 21, 26]
which encode travel-time indirectly in a sequence of phase
measurements. While correlation time-of-ﬂight cameras are
readily available, e.g. Microsoft’s Kinect One, their ap-
plication to transient imaging is limited due to amplitude
modulation bandwidths around 100 MHz, and hence tem-
poral resolution in the nanosecond range. A further line
of work [29, 28] explores using correlations in the carrier
wave itself, instead of amplitude modulation. While this ap-
proach allows for single-shot NLOS captures, it is limited
to scenes at microsopic scales [28].

• We introduce a synthetic training set for steady-state
NLOS imaging. The dataset and models will be pub-
lished for full reproducibility.

Tracking and Classiﬁcation Most similar to the proposed
method are recent approaches that use conventional inten-
sity measurements for NLOS vision tasks [31, 8, 9, 5]. Al-

6791

though not requiring temporal resolution, these existing ap-
proaches are restricted to coarse localization and classiﬁ-
cation to a limited extent, in contrast to full imaging and
geometry reconstruction applications.

3. Image Formation Model

Non-line-of-sight imaging methods recover object prop-
erties outside the direct
line-of-sight from third-order
bounces. Typically, a diffuse wall patch in the direct line-
of-sight is illuminated, where the light then scatters and
partially reaches a hidden object outside the direct line-
of-sight. At the object surface, the scattered light is re-
ﬂected back to the visible wall where it may be measured.
In contrast to existing methods that rely on temporally re-
solved transport, the proposed method uses stationary third-
bounce transport, i.e. without time information, to recover
reﬂectance and geometry of the hidden scene objects.

3.1. Stationary Light Transport

Specializing the Rendering Equation [27] to non-line-of-
sight imaging, we model the radiance L at a position w on
the wall as

L(w) =ZΩ

ρ (x − l, w − x) (nx· (x − l))

1
r2

xw

1
r2

xl

L(l)dx

+ δ (kl − wk) L(l),

(1)
with x, nx the position and corresponding normal on the
object surface Ω, l being a given beam position on the
wall, and ρ denoting the bi-directional reﬂectance distri-
bution function (BRDF). This image formation model as-
sumes three indirect bounces, with the distance function r
modeling intensity falloff between input positions, and one
direct bounce, when l and w are identical in the Dirac delta
function δ(·), and it ignores occlusions in the scene outside
the line-of-sight. We model the BRDF with a diffuse and
specular term as

ρ (ωi, ωo) = αd ρd (ωi, ωo) + αs ρs (ωi, ωo) .

(2)

temporally coded illumination.

The diffuse component ρd models light scattering, re-
sulting in almost orientation-independent low-pass reﬂec-
tions without
In con-
the specular reﬂectance component ρs contributes
trast,
high-frequency specular highlights, i.e. mirror-reﬂections
blurred by a specular lobe. These two components are
mixed with a diffuse albedo αd and specular albedo αs.
While the spatial and color distributions of these two albedo
components can vary,
they are often correlated for ob-
jects composed of different materials, changing only at the
boundaries of materials on the same surface. Although the
proposed method is not restricted to a speciﬁc BRDF model,
we adopt a Phong model [46] in the following.

3.2. Sensor Model

We use a conventional color camera in this work. We
model the raw sensor readings with the Poisson-Gaussian
noise model from Foi et al. [15, 14] as samples

b ∼

1
κ

P(cid:18)ZT ZW ZΩA

L(w) dω dw dt

κ

E(cid:19) + N (0, σ2),

(3)
where we integrate Eq. (1) over the solid angle ΩA of the
camera’s aperture, over spatial position W that the given
pixel maps to, and exposure time T , resulting in the incident
photons when divided by the photon energy E. The sensor
measurement b at the given pixel is then modeled with the
parameters κ > 0 and σ > 0 in a Poisson and Gaussian
distribution, respectively, accurately reﬂecting the effects of
analog gain, quantum efﬁciency and readout noise. For no-
tational brevity, we have not included sub-sampling on the
color ﬁlter array of the sensor.

4. Inverse Indirect Transport for Planar Scenes

In this section, we address the special case of planar ob-
jects. Assuming planar scenes in the hidden volume al-
lows us to recover reﬂectance and 3D geometry from in-
direct reﬂections. Moreover, in this case, we can formu-
late the corresponding inverse problem using efﬁcient opti-
mization methods with analytic gradients. In the remainder
of this paper, we assume that the shape and reﬂectance of
the directly visible scene parts are known, i.e.
the visible
wall area. The proposed hardware setup allows for high-
frequency spatially coded illumination, and hence the wall
geometry can be estimated using established structured-
Illuminating a patch l on the visible
light methods [50].
wall, a hidden planar scene surface produces a diffuse low-
frequency reﬂection component, encoding the projected po-
sition independently of the orientation [31], and higher-
frequency specular reﬂection components of the blurred
specular albedo mapped to orientation-dependent positions
on the wall. Assuming a single point light source at l on the
wall, see Fig. 2, the specular direction at a plane point p is
the mirror direction r = (p − l) − 2((p − l) · n)n with the
plane normal being n. The center of the specular lobe c on
the wall is the mirror point of l, i.e. the intersection of the
reﬂected ray in direction r with the wall. Conversely, if we
detect a specular lobe around c in a measurement, we can
solve for the corresponding plane point as

p(v, n) = c + ((v − c) · n)(cid:18)n −

v − l − ((c − l) · n)n

n · (2v − c − l) (cid:19),

(4)
that is a function of the planar surface represented by its
normal n and a point v on the plane. Eq. (4) follows imme-
diately from the constraint that the orthogonal projections
of the points l and c onto the plane result in equal triangles

6792

6793

Figure 3: Experimental geometry and albedo reconstructions for the special case of planar objects, captured with the protoype from
Sec. 7.2 and setup geometry from [40]. We demonstrate reconstructions for three different surface materials. The ﬁrst row shows an object
with diamond grade retroreﬂective surface coating as they are found on number plates and high-quality street signs, identical to the objects
in [40], which surprisingly contain faint specular components visible in the measurements (please zoom into the electronic version of this
document). The second and third rows show a conventionally painted road sign and an engineering-grade street sign. The proposed method
runs at around two seconds including capture and reconstruction, and achieves high resolution results without temporal sampling.

projecting light beams to different positions on the wall re-
sults in different observations which we dub indirect reﬂec-
tion maps, i.e. indirect component of the image on the wall
without the direct reﬂection. Each map contains informa-
tion about the object shape and normal information in spe-
ciﬁc direction if the BRDF is angle-dependent. Note that
this is not only the case for highly specular BRDFs, but also
for lambertian BRDFs due to foreshortening and varying
albedo. Hence, by changing the beam position we acquire
variational information about shape and reﬂectance.

Assuming locally smooth object surfaces, we sample the
available wall area uniformly in a 5 × 5 grid and acquire
multiple indirect reﬂection maps. We stack all the captured
images, forming a h × w × (5 · 5 · 3) dimension tensor
as network input. The virtual source position is a further
important information that may be provided to the network.
However, since we use uniform deterministic sampling, we
found that the model learns this structured information, in
contrast to random source sampling.

We use the orthogonal view of the scene as our ground
truth latent variable, as if the camera had been placed in the
center of the visible wall in wall normal direction and with
ambient illumination present. Given the stack of indirect
reﬂection maps, the proposed network is trained to estimate

the corresponding orthogonal view into the hidden scene.

Network Architecture We propose a variant of the U-Net
architecture [48] as our network backbone structure, shown
in Fig. 4. It contains a 8 layers encoder and decoder. Each
encoder layer reduces the image size by a factor of two
in each dimension and doubles the feature channel. This
scaling is repeated until we retrieve a 1024 dimension la-
tent vector. In corresponding convolution and deconvolu-
tion layer pairs with the same size, we concatenate them to
learn residual information.

Loss functions We use a multi-scale ℓ2 loss function

Vmulti−scale = Xk

γkkik − okk2,

(8)

where i is the predicted network output and o is the ground-
truth orthogonal image. Here, k represents different scales
and γk is the corresponding weight of that layer. With fea-
ture map at k-the layer, we adopt an extra one deconvolution
layer to convert the feature to an estimate at the target res-
olution. We predict 64 × 64, 128 × 128 and 256 × 256
ground truth images and set the weights γk as 0.6, 0.8 and
1.0. See the Supplemental Material for training details.

6794

6795

6796

6797

References

[1] N. Abramson. Light-in-ﬂight recording by holography. Op-

tics Letters, 3(4):121–123, 1978. 1, 2

[2] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers,
and J. Davis. Scape: shape completion and animation of
people. In ACM transactions on graphics (TOG), volume 24,
pages 408–416. ACM, 2005. 6

[3] V. Arellano, D. Gutierrez, and A. Jarabo.

Fast back-
projection for non-line of sight reconstruction. Optics Ex-
press, 25(10):11574–11583, 2017. 1, 2

[4] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv preprint arXiv:1511.00561, 2015. 4

[5] K. L. Bouman, V. Ye, A. B. Yedidia, F. Durand, G. W. Wor-
nell, A. Torralba, and W. T. Freeman. Turning corners into
cameras: Principles and methods. In International Confer-
ence on Computer Vision, volume 1, page 8, 2017. 2

[6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Dis-
tributed optimization and statistical learning via the alternat-
ing direction method of multipliers. Found.&Trends® in Ma-
chine Learning, 3(1):1–122, 2011. 4

[7] M. Buttafava, J. Zeman, A. Tosi, K. Eliceiri, and A. Velten.
Non-line-of-sight imaging using a time-gated single pho-
ton avalanche diode. Optics express, 23(16):20997–21011,
2015. 1, 2, 6, 8

[8] P. Caramazza, A. Boccolini, D. Buschek, M. Hullin, C. F.
Higham, R. Henderson, R. Murray-Smith, and D. Faccio.
Neural network identiﬁcation of people hidden from view
with a single-pixel, single-photon detector. Scientiﬁc reports,
8(1):11945, 2018. 2, 6

[9] S. Chan, R. E. Warburton, G. Gariepy, J. Leach, and D. Fac-
cio. Non-line-of-sight tracking of people at long range. Op-
tics express, 25(9):10109–10117, 2017. 2, 6

[10] W. Chen, H. Wang, Y. Li, H. Su, Z. Wang, C. Tu, D. Lischin-
ski, D. Cohen-Or, and B. Chen. Synthesizing training images
for boosting human 3d pose estimation. In 3D Vision (3DV),
2016. 6

[11] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in neural information processing systems, pages
2366–2374, 2014. 1

[12] M. A. Figueiredo and J. M. Bioucas-Dias. Restoration of
poissonian images using alternating direction optimization.
IEEE transactions on Image Processing, 19(12):3133–3145,
2010. 4

[13] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 4

[14] A. Foi. Clipped noisy images: Heteroskedastic modeling and
practical denoising. Signal Processing, 89(12):2609–2629,
2009. 3

[15] A. Foi, M. Trimeche, V. Katkovnik, and K. Egiazarian.
Practical Poissonian-Gaussian noise modeling and ﬁtting
for single-image raw-data.
IEEE Trans. Image Process.,
17(10):1737–1754, 2008. 3

[16] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised
In

monocular depth estimation with left-right consistency.
CVPR, volume 2, page 7, 2017. 1

[17] O. Gupta, T. Willwacher, A. Velten, A. Veeraraghavan, and
R. Raskar. Reconstruction of hidden 3d shapes using diffuse
reﬂections. Opt. Express, 20(17):19096–19108, Aug 2012.
1, 2

[18] R. Hartley and A. Zisserman. Multiple view geometry in

computer vision. Cambridge university press, 2003. 1

[19] F. Heide, M. B. Hullin, J. Gregson, and W. Heidrich. Low-
budget transient imaging using photonic mixer devices. ACM
Transactions on Graphics (ToG), 32(4):45, 2013. 1, 2, 6, 8

[20] F. Heide, M. O’Toole, K. Zhang, D. Lindell, S. Diamond,
and G. Wetzstein. Robust non-line-of-sight imaging with
single photon detectors. arXiv preprint arXiv:1711.07134,
2017. 2

[21] F. Heide, L. Xiao, W. Heidrich, and M. B. Hullin. Diffuse
mirrors: 3d reconstruction from diffuse indirect illumina-
tion using inexpensive time-of-ﬂight sensors.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3222–3229, 2014. 1, 2

[22] W. Jakob. Mitsuba renderer, 2010. http://www.mitsuba-

renderer.org. 6

[23] A. Jarabo, J. Marco, A. Munoz, R. Buisan, W. Jarosz,
and D. Gutierrez. A framework for transient rendering.
ACM Transactions on Graphics (Proceedings of SIGGRAPH
Asia), 33(6), nov 2014. 6

[24] A. Kadambi, R. Whyte, A. Bhandari, L. Streeter, C. Barsi,
A. Dorrington, and R. Raskar. Coded time of ﬂight cameras:
sparse deconvolution to address multipath interference and
recover time proﬁles. ACM Transactions on Graphics (ToG),
32(6):167, 2013. 1

[25] A. Kadambi, R. Whyte, A. Bhandari, L. Streeter, C. Barsi,
A. Dorrington, and R. Raskar. Coded time of ﬂight cameras:
sparse deconvolution to address multipath interference and
recover time proﬁles. ACM Transactions on Graphics (ToG),
32(6):167, 2013. 1, 2

[26] A. Kadambi, H. Zhao, B. Shi, and R. Raskar. Occluded
imaging with time-of-ﬂight sensors. ACM Transactions on
Graphics (ToG), 35(2):15, 2016. 1, 2

[27] J. T. Kajiya. The rendering equation. In Proc. SIGGRAPH,

pages 143–150, 1986. 3

[28] O. Katz, P. Heidmann, M. Fink, and S. Gigan. Non-
invasive single-shot imaging through scattering layers and
around corners via speckle correlations. Nature photonics,
8(10):784, 2014. 2

[29] O. Katz, E. Small, and Y. Silberberg. Looking around cor-
ners and through thin turbid layers in real time with scattered
incoherent light. Nature photonics, 6(8):549–553, 2012. 2

[30] A. Kirmani, T. Hutchison, J. Davis, and R. Raskar. Looking
around the corner using transient imaging. In Proc. ICCV,
pages 159–166, 2009. 1, 2

[31] J. Klein, C. Peters, J. Mart´ın, M. Laurenzis, and M. B. Hullin.
Tracking objects outside the line of sight using 2d intensity
images. Scientiﬁc reports, 6:32491, 2016. 2, 3

[32] R. Lange. 3d time-of-ﬂight distance measurement with cus-
tom solid-state image sensors in cmos/ccd-technology. 2000.
1, 2

6798

[48] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015. 4, 5, 6

[49] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from
single monocular images. In Advances in neural information
processing systems, pages 1161–1168, 2006. 1

[50] D. Scharstein and R. Szeliski. High-accuracy stereo depth
maps using structured light. In Computer Vision and Pattern
Recognition, 2003. Proceedings. 2003 IEEE Computer So-
ciety Conference on, volume 1, pages I–I. IEEE, 2003. 1,
3

[51] D. Shin, F. Xu, D. Venkatraman, R. Lussana, F. Villa,
F. Zappa, V. K. Goyal, F. N. Wong, and J. H. Shapiro.
Photon-efﬁcient imaging with a single-photon camera. Na-
ture communications, 7:12046, 2016. 4

[52] S. Song, S. P. Lichtenberg, and J. Xiao. Sun rgb-d: A rgb-
d scene understanding benchmark suite. In The IEEE con-
ference on computer vision and pattern recognition, pages
567–576, 2015. 1

[53] C.-Y. Tsai, K. N. Kutulakos, S. G. Narasimhan, and A. C.
Sankaranarayanan. The geometry of ﬁrst-returning photons
for non-line-of-sight imaging. In IEEE International Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017. 1, 2

[54] A. Velten, T. Willwacher, O. Gupta, A. Veeraraghavan,
M. Bawendi, and R. Raskar. Recovering three-dimensional
shape around a corner using ultrafast time-of-ﬂight imaging.
Nature Communications, 3:745, 2012. 1, 2, 6

[55] A. Velten, D. Wu, A. Jarabo, B. Masia, C. Barsi, C. Joshi,
E. Lawson, M. Bawendi, D. Gutierrez, and R. Raskar.
Femto-photography: Capturing and visualizing the propaga-
tion of light. ACM Trans. Graph., 32, 2013. 2

[56] D. Wu, M. O’Toole, A. Velten, A. Agrawal, and R. Raskar.
Decomposing global light transport using time of ﬂight
imaging. In Proc. CVPR, pages 366–373, 2012. 2

[57] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1912–1920, 2015. 6

[58] F. Xu, G. Shulkind, C. Thrampoulidis, J. H. Shapiro, A. Tor-
ralba, F. N. C. Wong, and G. W. Wornell. Revealing hidden
scenes by photon-efﬁcient occlusion-based opportunistic ac-
tive imaging. OSA Opt. Express, 26(8):9945–9962, 2018. 2

[33] D. C. Liu and J. Nocedal. On the limited memory bfgs
method for large scale optimization. Mathematical program-
ming, 45(1-3):503–528, 1989. 4

[34] D. G. Lowe. Object recognition from local scale-invariant
features. In Computer vision, 1999. The proceedings of the
seventh IEEE international conference on, volume 2, pages
1150–1157. Ieee, 1999. 4

[35] Y. Maruyama and E. Charbon. A time-gated 128x128 cmos
spad array for on-chip ﬂuorescence detection.
In Proc.
Intl. Image Sensor Workshop (IISW), number EPFL-CONF-
178145, 2011. 2

[36] N. Naik, S. Zhao, A. Velten, R. Raskar, and K. Bala. Sin-
gle view reﬂectance capture using multiplexed scattering
and time-of-ﬂight imaging. ACM Trans. Graph., 30(6):171,
2011. 1, 2

[37] F. Nolet, S. Parent, N. Roy, M.-O. Mercier, S. Charlebois,
R. Fontaine, and J.-F. Pratte. Quenching circuit and spad
integrated in cmos 65 nm with 7.8 ps fwhm single photon
timing resolution. Instruments, 2(4):19, 2018. 2

[38] M. O’Toole, F. Heide, D. B. Lindell, K. Zang, S. Diamond,
and G. Wetzstein. Reconstructing transient images from
single-photon sensors. In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 2289–
2297, July 2017. 2, 4

[39] M. O’Toole, F. Heide, L. Xiao, M. B. Hullin, W. Heidrich,
and K. N. Kutulakos. Temporal frequency probing for 5d
transient analysis of global light transport. ACM Transac-
tions on Graphics (ToG), 33(4):87, 2014. 1

[40] M. O’Toole, D. B. Lindell, and G. Wetzstein. Confocal non-
line-of-sight imaging based on the light cone transform. Na-
ture, pages 338–341, 2018. 2, 5, 6, 7, 8

[41] M. O’Toole, R. Raskar, and K. N. Kutulakos.

Primal-
dual coding to probe light transport. ACM Trans. Graph.,
31(4):39–1, 2012. 1

[42] M. OToole, D. B. Lindell, and G. Wetzstein. Confocal non-
line-of-sight imaging based on the light-cone transform. Na-
ture, 555(7696):338, 2018. 1, 2

[43] R. Pandharkar, A. Velten, A. Bardagjy, E. Lawson,
M. Bawendi, and R. Raskar. Estimating motion and size of
moving non-line-of-sight objects in cluttered environments.
In Proc. CVPR, pages 265–272, 2011. 1, 2

[44] L. Parmesan, N. A. Dutton, N. J. Calder, A. J. Holmes, L. A.
Grant, and R. K. Henderson. A 9.8 µm sample and hold time
to amplitude converter cmos spad pixel. In Solid State De-
vice Research Conference (ESSDERC), 2014 44th European,
pages 290–293. IEEE, 2014. 2

[45] A. K. Pediredla, M. Buttafava, A. Tosi, O. Cossairt, and
A. Veeraraghavan. Reconstructing rooms using photon
echoes: A plane based model and reconstruction algorithm
for looking around the corner. In 2017 IEEE International
Conference on Computational Photography (ICCP). IEEE,
2017. 2

[46] B. T. Phong. Illumination for computer generated pictures.

Communications of the ACM, 18(6):311–317, 1975. 3, 6

[47] R. Raskar and J. Tumblin. Computational Photography:
Mastering New Techniques For Lenses, Lighting, and Sen-
sors. A K Peters, Limited, 2007. 1

6799

