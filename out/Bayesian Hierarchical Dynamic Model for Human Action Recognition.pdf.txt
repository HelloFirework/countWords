Bayesian Hierarchical Dynamic Model for Human Action Recognition

Rui Zhao1, Wanru Xu2, Hui Su1

,

3, Qiang Ji1

1RPI, 2Beijing Jiaotong University, 3IBM Research

{zhaorui.zju,bjtuxuwanru}@gmail.com, huisuibmres@us.ibm.com, qji@ecse.rpi.edu

Abstract

Human action recognition remains as a challenging task
partially due to the presence of large variations in the ex-
ecution of an action. To address this issue, we propose
a probabilistic model called Hierarchical Dynamic Model
(HDM). Leveraging on Bayesian framework, the model pa-
rameters are allowed to vary across different sequences of
data, which increase the capacity of the model to adapt to
intra-class variations on both spatial and temporal extent
of actions. Meanwhile, the generative learning process al-
lows the model to preserve the distinctive dynamic pattern
for each action class. Through Bayesian inference, we are
able to quantify the uncertainty of the classiÔ¨Åcation, provid-
ing insight during the decision process. Compared to state-
of-the-art methods, our method not only achieves competi-
tive recognition performance within individual dataset but
also shows better generalization capability across different
datasets. Experiments conducted on data with missing val-
ues also show the robustness of the proposed method.

1. Introduction

Being able to recognize human action is crucial for un-
derstanding the intention of human. Over the past decades,
numerous methods have been proposed to recognize hu-
man actions from visual inputs [53]. More recently, ac-
tion recognition from 3D data becomes popular [1] with
the availability of low-cost 3D sensing equipment and real-
time 3D pose estimation technique [41, 30, 13]. Despite the
signiÔ¨Åcant progress made in this area, action recognition
remains as one of the most challenging problems in com-
puter vision partially due to signiÔ¨Åcant variations caused by
subject behavior, view change, occlusion, camera motion,
cluttered background, etc. In particular, the difference of
people‚Äôs behavior in performing an action results in spatial
and temporal intra-class variations. Even the same person
may perform the same action differently. Such signiÔ¨Åcant
intra-class variation makes the inter-class difference vague.
In this paper, we address the issue of intra-class spatio-
temporal variations for better action recognition. In addi-

tion, we provide a way of quantizing the uncertainty associ-
ated with classiÔ¨Åcation, leveraging on Bayesian inference.
We focus on the variations mainly caused by behavior dif-
ference rather than camera motion or occlusion and adopt
the deÔ¨Ånition for such variations similar to [12]. The spatial
variation is deÔ¨Åned as body pose and appearance change
when presenting a particular gesture. The temporal varia-
tion involves three factors: speed, duration and transition.
Speed refers to the pace of executing an action. Duration
represents the time spent in completing different phases of
an action. Transition controls the change and order among
different sub-actions. As an example, Figure 1 (left) shows
skeleton joints of different subjects performing bowling ac-
tion, which can be roughly divided into four phases includ-
ing standing still, stepping forward, arm extending back-
ward and leaning forward with arm extending forward. For
spatial variation, different subjects stretch their arms and
legs differently in both extent and orientation. For temporal
variation, different subjects perform action using different
orders of phases and spend different amount of time therein.

Our speciÔ¨Åc contributions are as follows. First of all, we
propose the Hierarchical Dynamic Model (HDM), which
is constructed to model different aspects of variations in a
principled way. The temporal variation is handled in two as-
pects. First, we incorporate a probabilistic duration mech-
anism to allow Ô¨Çexible speed at each phase of an action.
Second, the transitions among different phases of an action
are modeled by transition probabilities among different hid-
den states. The spatial variation is modeled by probability
distribution on observations at each individual frame. To
further improve the capability of handling intra-class varia-
tion, we extend the model following Bayesian framework
by allowing the parameters to vary across data, yielding
a hierarchical structure. Secondly, we develop a learning
algorithm to estimate the hyperparameters, which are usu-
ally treated as Ô¨Åxed in existing literatures. Furthermore,
leveraging on Bayesian inference techniques, we propose
a measure to quantize the uncertainty of the classiÔ¨Åcation
results. Finally, we conduct experiments on a variety of
benchmark datasets to show the beneÔ¨Åt of modeling vari-
ations and quantifying uncertainty for action recognition.

17733

Figure 1. Left: Skeleton data examples from UTD-MHAD dataset [6] show spatial and temporal variations of different subjects performing
the same action. All sequences have the same time scale. Different colors indicate different pose cluster assignments. Right: Overview
of the recognition process. During training, we learn a set of models by Ô¨Åtting each model to its corresponding type of action (details
in Section 3.2). During testing, the predictive likelihood computed by different models are used to determine class label and uncertainty
(details in Section 3.3). Images are selected from Gaming 3D dataset [3]. (Best view in color)

We demonstrate our method has competitive classiÔ¨Åcation
performance, data efÔ¨Åciency, better generalization, and ro-
bustness to missing data.

2. Related Work

Modeling spatial and temporal variations: To account
for spatial variation with known dynamic pattern, para-
metric HMM [54] and parametric switching LDS [37] are
proposed by associating the observation probability with a
global parameter. More Ô¨Çexible dynamic model with non-
parametric observation model is also proposed in [12]. De-
spite better Ô¨Çexibility, it is difÔ¨Åcult to generalize to poses
that deviate from the training data due to variation. Our ap-
proach instead uses parametric distribution for pose features
and leverages on the hierarchical extension for better gener-
alization. To handle speed variation, Hidden semi-Markov
Model (HSMM) [56] and its variants [11, 34, 33] are pro-
posed to explicitly model the lasting time of hidden states.
HSMM relaxes the Markov assumption of state transition
in HMM and thus allows more Ô¨Çexible modeling of the dy-
namic process. Besides extending HSMM with structure,
Bayesian extension of HSMM has been proposed in [15, 19]
to further increase the modeling capacity. Another line of
work tries to handle temporal variation through constructing
a time-invariant representation of data. For instance, vari-
ants of temporal warping methods are used to handle recog-
nition under speed variation [31, 46, 49]. Aggregate fea-
tures extracted from different temporal scales are explored
in [44, 50, 24], which can achieve certain temporal invariant
representation. But its temporal granularities are manually
decided. Our approach focuses on modeling dynamics of
human action. We further improve the intra-class variation

modeling capacity of HSMM by leveraging on Bayesian
framework. Compared to existing work, we allow all the
parameters to vary as random variables to account for spa-
tial and temporal variations simultaneously. Furthermore,
compared to previous work with Ô¨Åxed hyperparameters, we
develop learning algorithm for hyperparameters estimation.
The beneÔ¨Åt of such extension is two-fold. First, the hierar-
chical structure allows the parameters to change across dif-
ferent data, while still sharing the property through prior
distribution learned from all the within-class data. Sec-
ond, the prior can regularize model complexity. Subject
to the prior distribution, the model parameters can adapt to
data variations without increasing model complexity, which
helps avoid overÔ¨Åtting.

Action recognition frameworks:
It is popular to adopt a
discriminative framework for action recognition task, such
as conditional random Ô¨Åeld (CRF) [23] and its extensions
[39, 52, 27, 44]. Discriminative approaches mainly focus
on modeling the conditional distribution of class labels in
order to classify different classes. So it lacks the capability
to model data distribution, which limits the use of discrim-
inative model to classiÔ¨Åcation only. Recently, deep learn-
ing framework becomes more popular as it can learn useful
representation automatically. Typical approaches either use
deep models to extract features to supply classiÔ¨Åer learning
[17, 55, 28] or combine variants of CNN and RNN to per-
form end-to-end learning [9, 43, 16, 40, 21, 42, 45]. It has
been shown that modeling spatial and temporal dynamics
is helpful for recognition [25, 10]. However, deep models
rely on increasing model complexity to handle variations.
It is prone to overÔ¨Åtting especially with limited data, thus
proper regularization is essential [29, 59]. Joshi et al. [20]

7734

Query‚ãØ‚ãØHDM3HDM2‚ãØ‚ãØ‚ãØ‚ãØ‚ãØAction 1HDM1ùëÉ1(ùëã‚Ä≤|ùëø)Action 2Action 3LabelTestingTrainingInferenceLearningLearningLearningùëÉ2(ùëã‚Ä≤|ùëø)ùëÉ3(ùëã‚Ä≤|ùëø)Uncertaintyproposed a Bayesian NN to better handle subject-dependent
variation. We choose to use a generative model primarily
due to its capability of capturing the data distributions sub-
ject to spatial and temporal variations. Furthermore, gener-
ative model can handle data with missing values. Compared
to deep learning approach, HDM requires less training data
and is less likely to overÔ¨Åt due to prior on parameters. It
is also easier to train with very few model parameters to be
tuned. Furthermore, the use of Bayesian inference allows us
to quantify the uncertainty of the prediction to avoid overly
conÔ¨Ådent but potentially incorrect predictions [22].

3. Methods

In this section, we introduce our methods, starting with
a description of the model. Then we introduce learning and
inference methods. We train one model for each type of ac-
tion as illustrated by Figure 1 (right) and use the predictive
likelihoods of the models for classiÔ¨Åcation and uncertainty
estimation.

Figure 2. The topology of HDM.

3.1. Model description

Overview: Figure 2 shows the topology of our model.
Random variables X = {Xt ‚àà RO}T
t=1 represent a se-
quence of observations, where O is the dimension of each
observation. Z = {Zt ‚àà {1, ..., Q}}T
t=1 represent hidden
states associated with observations, where Q is the number
of hidden states. D = {Dt ‚àà {1, ..., T }}T
t=1 represent du-
ration of the state e.g. Dt = d means state chain Z remains
at current value for the next d time stamps. Xt is continu-
ous and observed, while Zt and Dt are discrete and hidden.
T can be different for different sequences. The parameters
are Œ∏ = {œÄ, A, œÑ, œà}, which specify the conditional dis-
tributions of random variables. The hyperparameters are
Œ± = {Œ∑0, Œ∑, Œæ, Œª}, which specify the prior distributions of
parameters. The joint distribution of random variables is as
follows.

P (X, Z, D) = P (Z1)P (D1|Z1)

T

Yt=1

P (Xt|Zt)

(1)

T

Yt=2

[P (Zt|Dt‚àí1, Zt‚àí1)P (Dt|Dt‚àí1, Zt)]

We use Gaussian mixture for emission distribution, Pois-
son for duration distribution and multinomial for initial state
and transition distribution. The prior distributions of param-
eters are assumed independent of each other and conjugate
prior is used i.e. P (Œ∏|Œ±) = P (œÄ|Œ∑0)P (A|Œ∑)P (œÑ |Œæ)P (œà|Œª).
The detailed parameterization is provided in the supplemen-
tary materials.

Modeling temporal variation: The temporal varia-
tion is modeled at two levels. First, at the random vari-
the hidden state chain Z models the transi-
able level,
tion dynamics among different statuses, speciÔ¨Åed by ini-
tial state distribution P (Z1) and state transition distribution
P (Zt|Dt‚àí1, Zt‚àí1) in Eq. (1). The duration of each state,
which is mainly determined by the speed of action, is ex-
plicitly speciÔ¨Åed by D with distribution P (Dt|Dt‚àí1, Zt) in
Eq. (1). Second, to model temporal variation at parame-
ter level, instead of Ô¨Åxing one set of parameters for all the
within-class data, we allow parameters {œÄ, A, œÑ } to vary as
random variables across different sequences, whose distri-
butions are speciÔ¨Åed by hyperparameters {Œ∑0, Œ∑, Œæ}. On one
hand, the hierarchy can accommodate large intra-class tem-
poral variations as each sequence has its own temporal pa-
rameters. On the other hand, the parameters share the same
prior, which is learned from all the within-class data. Thus
the overall within-class temporal dynamics is preserved.

Modeling spatial variation: Similar to temporal varia-
tion, spatial variation is also modeled at two levels. First,
at the random variable level, the observation Xt describes
the pose or appearance at a given time t, speciÔ¨Åed by emis-
sion distribution P (Xt|Zt) in Eq. (1). Second, the spatial
parameters œà are also treated as random variables whose
distributions are speciÔ¨Åed by hyperparameters Œª. Different
from temporal parameters, we do not vary spatial parame-
ters across different sequences to ensure the consistency of
hidden state value. Such hierarchy allows for large variation
without needing to increase the mixture number, which reg-
ularizes the model complexity and avoids overÔ¨Åtting. Fur-
thermore, since the prior is learned from data and shared by
all spatial parameters, the overall within-class spatial distri-
bution is preserved.

Generalization of existing models: Our model can be
considered as a generalization of several existing models. If
we set all the hyperparameters as Ô¨Åxed, it can be considered
as Bayesian HSMM 1. If we take out all the hyperparame-
ters, it degenerates into explicit duration HMM [14]. If we
further set Dt = 1 for all t, it reduces to HMM.

3.2. Learning

The goal of learning is to estimate hyperparameters Œ±
using training data, which is considered as an empirical
Bayesian method. We Ô¨Åt one model for one action class

1A special case of Bayesian HSMM which has been proposed in [15]

only considered placing prior on duration parameters.

7735

ùëã2ùê∑ùëáùëç2ùê∑1ùëã1ùëç1ùê∑2ùëçùëáùëãùëáùêÄùúèùúìùúâùúÇùúÇ0ùúÜùúãùõºùúÉso that each model only captures intra-class variation in the
corresponding class. The following learning process applies
to model for each class. The maximum likelihood estima-
tion is an initial attempt to estimate Œ±, which requires the
integration of both hidden variables and model parameters.

Œ±‚àó = arg max

Œ±

log P ({Xn}|Œ±)

= arg max

Œ±

logZŒ∏ Yn XZn,Dn

(2)

P (Xn, Zn, Dn|Œ∏)P (Œ∏|Œ±)dŒ∏

where n is the index of sequence. However, the integra-
tion over transition parameters introduces additional depen-
dencies among hidden variables that are not directly linked
together. Thus the efÔ¨Åcient forward-backward type of infer-
ence can no longer be performed. For sequence with more
than moderate length, the summation becomes intractable.
To bypass the integration challenge, we instead estimate Œ±
as follows.

Œ±‚àó = arg max

Œ±

logYn XZn,Dn

P (Xn, Zn, Dn|Œ∏‚àó)P (Œ∏‚àó|Œ±)

(3)

where Œ∏‚àó is one particular choice of Œ∏. It leads to an alternat-
ing estimation process between Œ∏ and Œ±. First, we compute
MAP estimation of Œ∏ given current estimate of Œ±. The ob-
jective of the estimation is the same as Eq. (3), except that
the target variable becomes Œ∏.

Œ∏‚àó = arg max

Œ∏ Xn

log XZn,Dn

P (Xn, Zn, Dn|Œ∏) + log P (Œ∏|Œ±)

(4)

We solve Eq. (4) using EM [7] based algorithm, which
we call MAP-EM. The details are provided in supplemen-
tary materials. Second, we compute estimate of Œ± using
Eq. (3) given current estimate Œ∏‚àó. Since the hyperparame-
ters are independent of random variables given Œ∏‚àó. Eq. (3)
reduces to computing MLE of Œ± as follows.

Œ±‚àó = arg max

Œ±

log P (Œ∏‚àó|Œ±)

(5)

Solving Eq. (5) can be done for each individual hyperpa-
rameter separately. The details are provided in supplemen-
tary materials.

Algorithm 1 Learning HDM
Input: Xn: observation sequences
Output: Hyperparameters Œ±

1: Initialization of Œ±, Œ∏
2: repeat
3:

Update Œ∏ by solving Eq. (4)
Update Œ± by solving Eq. (5)

4:
5: until convergence
6: return Œ±

The above alternating process will generate a se-
quence of estimations of Œ∏, Œ± that increase the value of
log P ({Xn}, Œ∏|Œ±). In experiment, it often converges in a
few iterations. To initialize Œ±, we use values that produce
uniform initial, transition, duration distribution and mix-
ture weights. We initialize œà based on the mean and co-
variance of data. To initialize Œ∏ for MAP-EM, we use K-
means to cluster data and use cluster assignment as hidden
state value, from which we can estimate the model param-
eters. For evaluation of convergence, we use the change of
log P ({Xn}, Œ∏|Œ±) between two consecutive iterations. Al-
gorithm 1 summarizes the overall learning process.

3.3. Inference

The goal of inference is to compute the posterior predic-

tive likelihood of unseen data X.

pl(X|Œ±‚àó) , P (X|D, Œ±‚àó)

(6)

= ZŒ∏ XZ,D

P (X, Z, D|Œ∏)P (Œ∏|D, Œ±‚àó)dŒ∏

where D = {Xn} is the set of training data. For the
same reason discussed in Section 3.2, exact computation of
Eq. (6) is intractable and approximate inference is needed.
We use Monte Carlo estimation to approximate the integra-
tion by sampling Œ∏ from its posterior distribution.

pl(X|Œ±‚àó) ‚âà

1
L

L

Xl=1 XZ,D

P (X, Z, D|Œ∏(l))

(7)

where Œ∏(l) ‚àº P (Œ∏|D, Œ±‚àó) and L is the total num-
ber of samples. To generate samples of parameters from
their posterior distributions, we consider two methods.
The Ô¨Årst one is structured mean-Ô¨Åeld variational infer-
ence [2], which Ô¨Ånds an optimal variational distribution
q(Œ∏, H|œÜ) , q(Œ∏|œÜ)q(H|œÜ) that maximizes a lower bound
on log P (D|Œ±‚àó). Here œÜ is the parameters of q and H =
{Zn, Dn} is the hidden states of all the training data D.
After we obtain optimal œÜ‚àó, parameter Œ∏(l) is then sampled
from q(Œ∏|œÜ‚àó). The second one is blocked Gibbs sampling
[19], which alternates the sampling between hidden state
chain {Zn, Dn} and parameters Œ∏. This process simulates
a Markov chain whose stationary distribution converges to
the true posterior distribution. Samples are collected after
the burn-in period, which we determine by the change of
log-likelihood of parameters. The inference algorithms are
implemented using Pyhsmm [18] and BNT [32]. Given Œ∏(l),
each term of summation in Eq. (7) can be computed using
forward-recursion [57]. The same inference process is per-
formed for each class model with hyperparameters learned
in Section 3.2. The classiÔ¨Åcation criterion is as follows.

y‚àó = arg max

i

pl(X|Œ±‚àó
i )

(8)

7736

where the subscript i is the class index. The overall com-
plexity is O(KLQ2T 2).
In our experiments Q is usu-
ally between 10-20, whose value is determined by cross-
validation. T is usually less than 200. K varies from 11 to
27. L is set to 100, which we found sufÔ¨Åcient.

3.4. Uncertainty of classiÔ¨Åcation

The use of Bayesian inference allows us to quantize
the uncertainty of classiÔ¨Åcation results. SpeciÔ¨Åcally, we
treat the class label y as a random variable that follows
y ‚àº Cat(p), where p =
categorical distribution i.e.
[p1, ..., pK] is a stochastic vector specifying the probabil-
ity of the y being one of the K classes. For a sequence
X, we obtain p by normalizing the likelihood of differ-
ent classes‚Äô model parameters evaluated on X i.e. p(l)
i =
P (X|Œ∏(l)
j ). To generate uncertainty mea-
sure, we Ô¨Årst compute total covariance of y. Given the sam-
ples of parameters, the total covariance can be computed by
Eq. (9). The proof is provided in supplementary materials.

i )/PK

j=1 P (X|Œ∏(l)

V [y|X] = EŒ∏[V [y|X, Œ∏]] + VŒ∏[E[y|X, Œ∏]]

(9)

‚âà

1
L

Œ£L

l=1Cl +

1

L ‚àí 1

Œ£L

l=1(pk ‚àí ¬Øp)(pk ‚àí ¬Øp)T

where Cl is the covariance matrix of the categorical distri-
bution corresponding to the lth set of parameters. The entry
of covariance can be computed by C(i, j) = Œ¥(i, j)pi ‚àí
pipj . A similar decomposition of total variance is proposed
in [22]. To obtain the uncertainty, we compute the trace of

total covariance matrix i.e. U (y) , Pi V [y|X](i, i). The

trace attains its minimum value 0 if and only if exactly one
of the pk equals to 1 and 0 otherwise.
In such case, the
prediction is absolutely certain. Our uncertainty measure
indicates how conÔ¨Ådent the prediction is.

Figure 3. Left: Example histograms of right hand in high arm
wave. Zero-count bin is pruned for compactness. Right: Actual
waving action sequences from different datasets. (1) MSRA; (2)
UTD; (3) G3D. (Best view in color)

4. Experiments

First, we perform a quantitative analysis of spatio-
temporal variation on selected benchmark datasets. Second,
we evaluate the performance of action recognition on indi-
vidual dataset and compare with both baseline and state-of-
the-art methods, followed by an uncertainty analysis. Third,

we evaluate the generalization capability of our method
across different datasets. Finally, we perform action recog-
nition with missing observations2.

4.1. Action datasets and feature extraction

Our experiments involve four benchmark action recog-
nition datasets, where all datasets involve multiple subjects
and action types ranging from hand movement to whole
body movement. SpeciÔ¨Åcally, MSR Action3D (MSRA)
[26] includes 567 sequences from 20 types of action. UTD-
MHAD (UTD) [6] includes 861 sequences from 27 types of
actions. Gaming 3D (G3D) [3] consists of 600 sequences
of 20 action types. UPenn Action (Penn) [58] contains
2326 RGB videos of 15 types of sports. We select a subset
of 1650 videos from 11 actions, excluding 4 actions with
large portion of missing body annotations due to occlusion.
In all datasets, only skeleton is used for action recognition.
The location and size of skeletons are normalized to ensure
translation and scale invariance. Besides position, the mo-
tion is also extracted by computing the difference between
consecutive frames for every pair of joints. Similar repre-
sentation is adopted in [1, 3, 51]. The raw feature dimension
is 266 per frame for 3D data and 117 per frame for 2D data.
We further perform PCA for position and motion feature
separately and retain 95% energy for each type of features
at each frame. Finally, the two features are concatenated.

4.2. Spatial and temporal variation analysis

We Ô¨Årst introduce a quantitative measure of intra-class
variation based on a histogram representation of action se-
quence. We divide the 3D space into 5 √ó 5 √ó 5 grids with
equal volume. Then for each joint in each sequence, we
construct a histogram whose number of bins is equal to the
number of spatial grids. The bin value equals to the number
of times when the joint position occupies the grid. We keep
the bin value unnormalized so that it depends on both the
spatial pose and the temporal pace. Figure 3 shows an ex-
ample of obtained histogram for different sequences of the
same action and the same joint. All three histograms show
a bi-modal distribution. However, the speciÔ¨Åc bin counts
are very different due to position and speed variation of the
hand joint. After computing histograms, we compute the
standard deviation of each bin value over all the sequences
and sum over all the bins, yielding total variation. Finally,
the total variation is averaged over all the joints as the Ô¨Ånal
variation score. Such metric satisÔ¨Åes the following prop-
erties. First, if all the sequences are identical, the metric
attains its minimum value 0. Second, the metric increases
as the intra-class variation increases.

Figure 4 shows the measured variation scores for differ-
ent actions in different datasets. In addition, we evaluate
the variation score on combined dataset, where the same

2Code available at http://bit.ly/BayesianHDM

7737

(1)(2)(3)Figure 4. Variation score and the corresponding classiÔ¨Åcation accuracy, which are obtained by training and testing on combined dataset.
The details are referred to Section 4.5. Left: UTD and MSRA. Middle: MSRA and G3D. Right: G3D and UTD. (Best view in color)

pre-processing is applied on combined dataset for scale and
translation invariance. From the Ô¨Ågure we observe that ac-
tion involves larger extent of whole body movement tends to
have larger variations e.g. golf swing and bowling. Action
with ambiguous explanation also has large variations e.g.
high wave. For each action, the combined dataset has larger
variation score than each individual dataset. We also draw
class-wise classiÔ¨Åcation accuracy obtained on the combined
dataset. The classiÔ¨Åcation details are discussed in Section
4.5. In general, our method performs better than the base-
line methods especially on actions with larger intra-class
variation score. This shows the beneÔ¨Åt of explicit modeling
of intra-class variations.

Table 1. Compare recognition accuracy (%) on different datasets
with different baseline models.

Model
HMM
HSMM
LSTM
HCRF

HDM-PI
HDM-PL
HDM-BV
HDM-BG

MSRA UTD G3D Penn Avg.
75.3
76.3
81.1
77.6
81.0
87.5
88.0
91.1

67.8
66.3
74.7
70.7
70.3
80.6
82.1
86.1

82.8
82.3
77.0
74.2
84.4
90.2
91.4
92.8

68.1
77.5
82.2
79.0
79.4
87.7
87.7
92.0

82.3
78.9
90.3
86.3
89.8
91.6
90.8
93.4

4.3. Individual dataset experiments

For individual dataset experiments, training-testing split
follows convention suggested by dataset authors. We con-
duct an ablation study by comparing our models with dif-
ferent simpliÔ¨Åed models. For our model, we consider four
variants depending on how the inference is performed. The
Ô¨Årst two are based on point estimate of parameters. The
MAP estimation of the parameters is obtained during learn-
ing and the predictive likelihood is simply computed as the
likelihood of the MAP parameters. For PI, the initial values
of hyperparameters are used. For PL, the learned hyper-
parameters are used. The last two variants use Bayesian
inference, where the predictive likelihood is computed fol-
lowing Section 3.3 using either variational inference (BV)
or Gibbs sampling (BG). Based on the results in Table 1,
we have following observations. First, compared with non-
hierarchical baseline HMM and HSMM, HDM achieves
consistent improvement. Furthermore, HDM is superior to

both HCRF and LSTM, which do not explicitly consider
data variations. These results demonstrate the beneÔ¨Åt of
modeling spatial and temporal variations. Second, compar-
ing the two point estimate approaches, using learned hyper-
parameters improves accuracy by 6.5%. This demonstrates
the beneÔ¨Åt of learning hyperparameters. Third, compared to
point estimate, Bayesian inference improves performance
by 0.5% (BV) and 3.3% (BG). This shows that by aver-
aging out the model uncertainty in inference, we can im-
prove the prediction. While variational inference is easier
to determine the convergence of approximation, the quality
of the approximation may not be optimal. Gibbs sampling
on the other hand can converge to true posterior provided
with enough sampling iterations and proper determination
of mixing condition. In our experiment, we observe the log-
likelihood of correct model obtained by Gibbs sampling is
usually higher than variational inference, which is also con-
sistent with its performance in classiÔ¨Åcation. For the re-
maining experiments, we report the results of HDM-BG.

Table 2. Compare recognition accuracy (%) with state-of-the-art.

MSRA

UTD

Method
AS[38]
AL[48]
MT[8]
HDM

G3D

Method

LRBM[35]
R3DG[47]
CNN[51]

HDM

Acc.
83.5
88.2
92.0
86.1

Acc.
90.5
91.1
96.0
92.0

Method
Fusion[6]
DMM[4]
CNN[51]

HDM

Penn

Method

Actemes[58]

AOG[36]
JDD[5]
HDM

Acc.
79.1
84.2
87.9
92.8

Acc.
86.5
84.8
93.2
93.4

Then we compare the performance of our method with
state-of-the-art methods. The average recognition accuracy
is shown in Table 2. Compared to feature based meth-
ods, we achieve 4.9% improvement on UTD. For G3D, our
model is better than both model based approach [35] and
skeleton feature based approach [47]. Another approach
[51] requires dataset-dependent encoding of features, while
we use the same data processing for all datasets. In Penn
dataset, we outperform methods based on pose features
[58, 36] and we are slightly better than appearance feature

7738

01020304050607080901000102030405060708090highthrowhandcatchtennisswinghandsclaptennisservejoggingdraw xpick up& throwdrawcirclehigh armwaveaccuracy (%) variation score UTD onlyMSRA onlyCombinedOursHSMMHMM0102030405060708090100010203040506070side kickhandsclaptennisswingforwardpunchjoggingtennisservegolfswinghigh armwaveaccuracy (%) variation score MSRA onlyG3D onlyCombinedOursHSMMHMM01020304050607080901000102030405060708090joggingwalkingtennisswinghandwavetennisservebowlinghandsclapaccuracy (%) variation score UTD onlyG3D onlyCombinedOursHSMMHMMFigure 5. Class-wise uncertainty in different datasets, where standard deviation is indicated by the error bar. The curve corresponds to
class-wise accuracy. The Pearson correlation coefÔ¨Åcients between the two are MSRA:-0.5811, UTD:-0.5723, G3D:-0.8999, Penn:-0.6215.

based method [5], which used more information than ours.
On MSRA, the performance gap between ours and [8] is
mainly due to use of a sophisticated encoding of skeleton
features, which we plan to explore as future work. We use
the same kinematic features for all datasets without heavily
engineering the features. Overall, these results demonstrate
that by capturing intra-class variations, our model achieves
competitive recognition performance on various datasets.

4.4. Uncertainty analysis

First, we verify the validity of the proposed uncertainty
measure as deÔ¨Åned in Section 3.4. We compute the error
rate of different portions of data ranging from the most cer-
tain to the least certain. The curve in Figure 6 shows that the
uncertainty correlates well with the error rate. For example,
in MSRA, when we select the 30% of data with lowest un-
certainty, the error rate is 0. When we expand the portion
to 50%, the error rate increases to 8%. We also visualize
data and corresponding class probability with different un-
certainty values in Figure 6. For low uncertainty data we
see the probability value is almost peak at the correct class.
While for data with high uncertainty at the upper right cor-
ner, we see a diffused and low probability value.

Then we analyze the class-wise uncertainty by comput-
ing the mean and standard deviation of uncertainty within
each class. Figure 5 plots the class-wise uncertainty and
accuracy. We observe in general that the higher the uncer-
tainty, the lower the accuracy. Actions only involving small
extent of motion tend to have higher uncertainty. For in-
stance, the top 5 uncertain actions in MSRA and UTD are
all single-hand actions. Some actions have subtle difference
such as ‚Äòhigh throw‚Äô and ‚Äòcatch an object‚Äô in UTD. Some
actions involve similar motion like ‚Äòhammer‚Äô and ‚Äòforward

Figure 6. ClassiÔ¨Åcation error rate versus different portions of un-
certainty values. (See Section 4.4 for details)

punch‚Äô in MSRA. More results are provided in supplemen-
tary materials. These results suggest that we should take un-
certainty into consideration for classiÔ¨Åcation decision. One
future direction of this work is to incorporate the uncertainty
during testing to automatically reÔ¨Åne the model.

4.5. Multi dataset experiments

To further demonstrate the capability of our model in
generalizing across different subjects and trials. We per-
form two experiments involving multiple datasets includ-
ing: A. MSRA; B. UTD; C. G3D. They share multiple ac-
tion types in common.

In the Ô¨Årst experiment, we train our model on combined
dataset and test on each individual dataset with subjects that
are not included in combined dataset. For the combined
dataset, we expect signiÔ¨Åcant intra-class variation. The re-
sults are shown in column 2-8 of Table 3. From the re-
sults, we observe that 1) HDM consistently outperforms

7739

Actual: Jumping Jack. Predicted: Jumping JackActual: Hammer. Predicted: High throw.ProbabilityClassProbabilityClassTable 3. ClassiÔ¨Åcation accuracy (%) on multi-dataset experiments. Results of other methods are obtained using original implementation.
The number of shared actions for (A,B), (A,C), (B,C) and (A,B,C) is 10, 8, 7 and 5, respectively. The action names are shown in Figure 4.

Train
Test

HSMM
DMM[4]
R3DG[47]
DLSTM[59]

HDM

A,B

A,C

B,C

A

73.7
76.6
82.5
83.9
86.9

B

82.5
90.6
91.9
93.1
91.9

A

89.0
91.7
93.6
88.1
93.6

C

87.0
84.3
90.0
87.0
92.6

B

91.0
92.8
97.3
82.9
97.3

C

83.2
76.4
82.9
80.9
91.0

Avg.

84.4
85.4
89.7
86.0
92.2

B,C A,C A,B
A

B

C

65.3
76.2
44.9
70.8
89.2

61.9
86.3
84.4
85.0
75.0

42.5
51.1
72.7
38.9
61.2

Avg.

56.5
71.2
67.3
64.9
75.1

Figure 7. Average accuracy and standard deviation (error bar) under different portions of missing data.

HSMM, which lacks the improved capacity due to hierar-
chical structure. 2) HDM outperforms or achieves similar
results as other three methods [4, 47, 59] in Ô¨Åve out of six
cases. On average, it outperforms all other methods with
2.5% improvement compared to the second best method,
which uses sophisticated scheme to extract features. This
demonstrates our model‚Äôs generalization capability across
datasets through capturing large intra-class variation.

In the second experiment, we train different models on
two datasets and test on the remaining one dataset. This is
a more challenging scenario, since the data collection set-
tings in training and testing are very different. The results
are reported in Table 3 column 9-12. HDM signiÔ¨Åcantly
outperforms non-hierarchical baseline HSMM with average
improvement of 18.6%. HDM also outperforms [4], [47]
and [59] by 3.9%, 7.8% and 10.2%. Although the abso-
lute performance drops for all the methods, the relative im-
provement of our method compared to others becomes more
signiÔ¨Åcant than pairwise case. These results further demon-
strate that HDM has enough capacity to absorb large varia-
tion. Thus, it can generalize better across different datasets.

4.6. ClassiÔ¨Åcation with missing data

One of the beneÔ¨Åts using generative model is to handle
missing data. In skeleton based action recognition, it is pos-
sible to have missing values in the observations, which are
often caused by failure of tracking or occlusion. To demon-
strate the robustness of the proposed approach in handling
missing values, we conduct an experiment where the model
is trained and tested on skeleton data with randomly miss-
ing values. To handle input with missing values, we com-
pute likelihood P (Xt|Zt) using only observed part of Xt.

For fair comparison, the same data with missing values are
used by other methods. We repeat the classiÔ¨Åcation 10
times and the results are shown in Figure 7. Our method
achieves the smallest decrease in performance as missing
portion increases. This shows that the combination of gen-
erative model with Bayesian inference maintains the robust-
ness against missing values in data.

5. Conclusion

In this paper, we proposed a probabilistic hierarchical
dynamic model to handle intra-class spatio-temporal vari-
ations for human action recognition. By treating model
parameters as random variables with designated prior dis-
tributions, the model can better adapt to intra-class varia-
tions. An algorithm of learning hyperparameters is devel-
oped. The use of Bayesian inference not only improves the
generalization of the model but also allows us to provide
an uncertainty measure of the prediction, which provides a
reference on the decision. Experiments conducted within
individual and across multiple datasets show that the pro-
posed HDM not only can capture the underlying dynamics
of different actions but also possess enough capacity to al-
low large intra-class variations. Experiment with missing
values also shows the robustness of the proposed method.

Acknowledgment

This work is partially supported by Cognitive Immersive
Systems Laboratory (CISL), a collaboration between IBM
and RPI, and also a center in IBM‚Äôs AI Horizon Network.
Xu is also supported by NSFC61672089 and partially sup-
ported by a CSC scholarship.

7740

References

[1] Jake K Aggarwal and Lu Xia. Human activity recognition

from 3d data: A review. Pattern Recognition Letters, 2014.

[2] Matthew J Beal. Variational algorithms for approximate

Bayesian inference. University of London, 2003.

[3] Victoria Bloom, Dimitrios Makris, and Vasileios Argyriou.
G3d: A gaming action dataset and real time action recogni-
tion evaluation framework. In CVPR Workshop, 2012.

[4] Mohammad Farhad Bulbul, Yunsheng Jiang, and Jinwen
Ma. Dmms-based multiple features fusion for human ac-
tion recognition. International Journal of Multimedia Data
Engineering and Management, 2015.

[5] Congqi Cao, Yifan Zhang, Chunjie Zhang, and Hanqing Lu.
Action recognition with joints-pooled 3d deep convolutional
descriptors. In IJCAI, 2016.

[6] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. Utd-
mhad: A multimodal dataset for human action recognition
utilizing a depth camera and a wearable inertial sensor. In
ICIP, 2015.

[7] Arthur P Dempster, Nan M Laird, and Donald B Rubin.
Maximum likelihood from incomplete data via the em al-
gorithm. Journal of the royal statistical society. Series B
(methodological), 1977.

[8] Maxime Devanne, Hazem Wannous, Stefano Berretti, Pietro
Pala, Mohamed Daoudi, and Alberto Del Bimbo. 3-d human
action recognition by shape analysis of motion trajectories
on riemannian manifold. Cybernetics, 2015.

[9] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,
and Trevor Darrell. Long-term recurrent convolutional net-
works for visual recognition and description. In CVPR, 2015.
[10] Yong Du, Wei Wang, and Liang Wang. Hierarchical recur-
rent neural network for skeleton based action recognition. In
CVPR, 2015.

[11] Thi Duong, Dinh Phung, Hung Bui, and Svetha Venkatesh.
EfÔ¨Åcient duration and hierarchical modeling for human ac-
tivity recognition. ArtiÔ¨Åcial Intelligence, 2009.

[12] Ahmed Elgammal, Vinay Shet, Yaser Yacoob, and Larry S
Davis. Learning dynamics for exemplar-based gesture recog-
nition. In CVPR, 2003.

[13] Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, and
Song-Chun Zhu. Learning pose grammar to encode human
body conÔ¨Åguration for 3d pose estimation. In AAAI, 2018.

[14] Jack D Ferguson. Variable duration models for speech. In
Symposium on the Application of HMMs to Text and Speech,
1980.

[15] Kei Hashimoto, Yoshihiko Nankaku, and Keiichi Tokuda.
A bayesian approach to hidden semi-markov model based
speech synthesis. In INTERSPEECH, 2009.

[16] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc
Van Gool. Deep learning on lie groups for skeleton-based
action recognition. In CVPR, 2017.

[17] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. TPAMI,
2013.

[18] Matthew J Johnson. Bayesian time series models and scal-

able inference. PhD thesis, MIT, 2014.

[19] Matthew J Johnson and Alan S Willsky. Bayesian nonpara-

metric hidden semi-markov models. JMLR, 2013.

[20] Ajjen Joshi, Soumya Ghosh, Margrit Betke, Stan Sclaroff,
and Hanspeter PÔ¨Åster. Personalizing gesture recognition us-
ing hierarchical bayesian neural networks. In CVPR, 2017.

[21] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous
Sohel, and Farid Boussaid. A new representation of skeleton
sequences for 3d action recognition. In CVPR, 2017.

[22] Alex Kendall and Yarin Gal. What uncertainties do we need
In NIPS,

in bayesian deep learning for computer vision?
2017.

[23] John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random Ô¨Åelds: Probabilistic models for seg-
menting and labeling sequence data. In ICML, 2001.

[24] Zhengzhong Lan, Ming Lin, Xuanchong Li, Alex G Haupt-
mann, and Bhiksha Raj. Beyond gaussian pyramid: Multi-
skip feature stacking for action recognition. In CVPR, 2015.
[25] Quoc V Le, Will Y Zou, Serena Y Yeung, and Andrew Y
Ng. Learning hierarchical invariant spatio-temporal features
for action recognition with independent subspace analysis.
In CVPR. IEEE, 2011.

[26] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Action
recognition based on a bag of 3d points. In CVPR Workshop,
2010.

[27] Ivan Lillo, Alvaro Soto, and Juan Niebles. Discriminative hi-
erarchical modeling of spatio-temporally composable human
activities. In CVPR, 2014.

[28] Mengyuan Liu and Junsong Yuan. Recognizing human ac-
In CVPR,

tions as the evolution of pose estimation maps.
2018.

[29] Behrooz Mahasseni and Sinisa Todorovic. Regularizing long
short term memory with 3d human-skeleton sequences for
action recognition. In CVPR, June 2016.

[30] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,
Helge Rhodin, Mohammad ShaÔ¨Åei, Hans-Peter Seidel,
Weipeng Xu, Dan Casas, and Christian Theobalt. Vnect:
Real-time 3d human pose estimation with a single rgb cam-
era. TOG, 2017.

[31] Meinard M¬®uller and Tido R¬®oder. Motion templates for auto-
matic classiÔ¨Åcation and retrieval of motion capture data. In
SIGGRAPH, 2006.

[32] Kevin Murphy. The bayes net toolbox for matlab. Computing

science and statistics, 2001.

[33] Pradeep Natarajan and Ramakant Nevatia. Coupled hidden
In WMVC,

semi markov models for activity recognition.
2007.

[34] Pradeep Natarajan and Ramakant Nevatia. Online, real-time
tracking and recognition of human actions. In WMVC, 2008.
[35] Siqi Nie, Ziheng Wang, and Qiang Ji. A generative restricted
boltzmann machine based method for high-dimensional mo-
tion data modeling. CVIU, 2015.

[36] Xiaohan Nie, Caiming Xiong, and Song-Chun Zhu. Joint
action recognition and pose estimation from video. In CVPR,
2015.

[37] Sang Min Oh, James M Rehg, Tucker Balch, and Frank Del-
laert. Learning and inferring motion patterns using paramet-
ric segmental switching linear dynamic systems. IJCV, 2008.

7741

[57] Shun-Zheng Yu and Hisashi Kobayashi.

An efÔ¨Åcient
forward-backward algorithm for an explicit-duration hidden
markov model. Signal processing letters, 2003.

[58] Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis.
From actemes to action: A strongly-supervised representa-
tion for detailed action understanding. In ICCV, 2013.

[59] Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng,
Yanghao Li, Li Shen, Xiaohui Xie, et al. Co-occurrence
feature learning for skeleton based action recognition using
regularized deep lstm networks. In AAAI, 2016.

[38] Eshed Ohn-Bar and Mohan Trivedi. Joint angles similarities

and hog2 for action recognition. In CVPRW, 2013.

[39] Ariadna Quattoni, Sybor Wang, Louis-Philippe Morency,
Morency Collins, and Trevor Darrell. Hidden conditional
random Ô¨Åelds. PAMI, 2007.

[40] Hossein Rahmani and Mohammed Bennamoun. Learning
action recognition model from depth and skeleton videos. In
ICCV, 2017.

[41] Jamie Shotton, Toby Sharp, Alex Kipman, Andrew Fitzgib-
bon, Mark Finocchio, Andrew Blake, Mat Cook, and
Richard Moore. Real-time human pose recognition in parts
from single depth images. Communications of the ACM,
2013.

[42] Chenyang Si, Ya Jing, Wei Wang, Liang Wang, and Tieniu
Tan. Skeleton-based action recognition with spatial reason-
ing and temporal stack learning. ECCV, 2018.

[43] Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and
Jiaying Liu. An end-to-end spatio-temporal attention model
for human action recognition from skeleton data. In AAAI,
2017.

[44] Yale Song, Louis-Philippe Morency, and Randall Davis. Ac-
tion recognition by hierarchical sequence summarization. In
CVPR, 2013.

[45] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou.
Deep progressive reinforcement learning for skeleton-based
action recognition. In CVPR, 2018.

[46] Ashok Veeraraghavan, Anuj Srivastava, Amit K Roy-
Chowdhury, and Rama Chellappa. Rate-invariant recogni-
tion of humans and their activities. TIP, 2009.

[47] Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa.
Human action recognition by representing 3d skeletons as
points in a lie group. In CVPR, 2014.

[48] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan.
Mining actionlet ensemble for action recognition with depth
cameras. In CVPR, 2012.

[49] Jiang Wang and Ying Wu. Learning maximum margin tem-

poral warping for action recognition. In ICCV, 2013.

[50] Limin Wang, Yu Qiao, and Xiaoou Tang. Latent hierarchical
model of temporal structure for complex activity classiÔ¨Åca-
tion. TIP, 2014.

[51] Pichao Wang, Wanqing Li, Chuankun Li, and Yonghong
Hou. Action recognition based on joint trajectory maps with
convolutional neural networks. Knowledge-Based Systems,
2018.

[52] Yang Wang and Greg Mori. Learning a discriminative hidden

part model for human action recognition. In NIPS, 2009.

[53] Daniel Weinland, Remi Ronfard, and Edmond Boyer. A sur-
vey of vision-based methods for action representation, seg-
mentation and recognition. CVIU, 2011.

[54] Andrew D Wilson and Aaron F Bobick. Parametric hidden

markov models for gesture recognition. TPAMI, 1999.

[55] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-
ral graph convolutional networks for skeleton-based action
recognition. arXiv, 2018.

[56] Shun-Zheng Yu. Hidden semi-markov models. ArtiÔ¨Åcial In-

telligence, 2010.

7742

