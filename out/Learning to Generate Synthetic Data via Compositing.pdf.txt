Learning to Generate Synthetic Data via Compositing

Shashank Tripathi1

,

2⋆†

Siddhartha Chandra1⋆

Amit Agrawal1

Ambrish Tyagi1

James M. Rehg1

Visesh Chari1

1Amazon Lab126 2Carnegie Mellon University

{shatripa, chansidd, aaagrawa, ambrisht, jamerehg, viseshc}@amazon.com

Abstract

We present a task-aware approach to synthetic data gen-
eration. Our framework employs a trainable synthesizer
network that is optimized to produce meaningful training
samples by assessing the strengths and weaknesses of a
‘target’ network. The synthesizer and target networks are
trained in an adversarial manner wherein each network is
updated with a goal to outdo the other. Additionally, we
ensure the synthesizer generates realistic data by pairing it
with a discriminator trained on real-world images. Further,
to make the target classiﬁer invariant to blending artefacts,
we introduce these artefacts to background regions of the
training images so the target does not over-ﬁt to them.

We demonstrate the efﬁcacy of our approach by apply-
ing it to different target networks including a classiﬁca-
tion network on AffNIST, and two object detection networks
(SSD, Faster-RCNN) on different datasets. On the AffNIST
benchmark, our approach is able to surpass the baseline re-
sults with just half the training examples. On the VOC per-
son detection benchmark, we show improvements of up to
2.7% as a result of our data augmentation. Similarly on the
GMU detection benchmark, we report a performance boost
of 3.5% in mAP over the baseline method, outperforming
the previous state of the art approaches by up to 7.5% on
speciﬁc categories.

1. Introduction

Synthetic data generation is now increasingly utilized to
overcome the burden of creating large supervised datasets
for training deep neural networks. A broad range of
data synthesis approaches have been proposed in literature,
ranging from photo-realistic image rendering [22, 35, 48]
and learning-based image synthesis [36, 40, 46] to meth-
ods for data augmentation that automate the process for
generating new example images from an existing training

⋆Equal Contribution
†Work done during an internship at Amazon Lab126

Figure 1: Comparison of object detection results using SSD.
Baseline: trained on VOC data, Ours: trained on VOC and
synthetic data generated using our approach. Green and red
bounding boxes denote correct and missed detections re-
spectively. SSD ﬁne-tuned with our synthetic data shows
improved performance on small, occluded and truncated
person instances.

set [9, 14, 15, 33]. Traditional approaches to data augmen-
tation have exploited image transformations that preserve
class labels [3, 46], while recent works [15, 33] use a more
general set of image transformations, including even com-
positing images.

For the task of object detection, recent works have ex-
plored a compositing-based approach to data augmentation
in which additional training images are generated by pasting
cropped foreground objects on new backgrounds [6, 7, 10].
The compositing approach, which is the basis for this work,
has two main advantages in comparison to image synthesis:
1) the domain gap between the original and augmented im-
age examples tends to be minimal (resulting primarily from
blending artefacts) and 2) the method is broadly-applicable,
as it can be applied to any image dataset with object anno-
tations.

A limitation of prior approaches is that the process that

461

baselineoursbaselineoursComposite Image

Target 

Network (T)

Synthesizer 
Network (S)

Discriminator

(D)

Real/Fake

Foreground

mask

Real Images

(a) VOC-SSD

(b) our-SSD

(a) VOC-SSD

(b) our-SSD

Figure 2: Our pipeline consists of three components: a synthesizer S, the target network T , and a natural image discriminator
D. S pastes an optimally transformed foreground into a background image to generate a synthetic composite image that can
‘fool’ T and D. T is updated using the synthesized image to improve its accuracy. D provides feedback to S to improve
realism of the synthetic image. T and D are updated with the synthesized data, in lock-step with S.

(a) VOC-SSD

(a) VOC-SSD

(a) VOC-SSD

(a) VOC-SSD

(b) our-SSD

(b) our-SSD

(b) our-SSD

(b) our-SSD

generates synthetic data is decoupled from the process of
training the target classiﬁer. As a consequence, the data
augmentation process may produce many examples which
are of little value in improving performance of the target
network. We posit that a synthetic data generation approach
must generate data have three important characteristics. It
must be a) efﬁcient: generate fewer and meaningful data
samples, b) task-aware: generate hard examples that help
improve target network performance, and c) realistic: gen-
erate realistic examples that help minimize domain gaps and
improve generalization.

We achieve these goals by developing a novel approach
to data synthesis. We set up a 3-way competition among the
synthesizer, target and discriminator networks. The synthe-
sizer is tasked with generating composite images by com-
bining a given background with an optimally transformed
foreground, such that it can fool the target network as shown
in Figure 2. The goal of the target network is to correctly
classify/detect all instances of foreground object in the com-
posite images. The synthesizer and target networks are up-
dated iteratively, in a lock-step. We additionally introduce
a real image discriminator to ensure the composite images
generated by the synthesizer conform to the real image dis-
tribution. Enforcing realism prevents the model from gener-
ating artiﬁcial examples which are unlikely to occur in real
images, thereby improving the generalization of the target
network.

A key challenge with all composition-based methods is
the sensitivity of trained models to blending artefacts. The
target and discriminator networks can easily learn to latch
on to the blending artefacts, thereby rendering the data gen-
eration process ineffective. To address these issues with
blending, Dwibedi et al. [7] employed 5 different blend-
ing methods so that the target network does not over-ﬁt to a
particular blending artefact. We propose an alternate solu-

tion to this problem by synthesizing examples that contain
similar blending artefacts in the background. The artefacts
are generated by pasting foreground shaped cutouts in the
background images. This makes the target network insen-
sitive to any blending artefacts around foreground objects,
since the same artefacts are present in the background im-
ages as well.

We apply our synthesis pipeline to demonstrate improve-
ments on tasks including digit classiﬁcation on the AffNIST
dataset [45], object localization using SSD [29] on Pascal
VOC [8], and instance detection using Faster RCNN [34]
on GMU Kitchen [11] dataset. We demonstrate that our
approach is a) efﬁcient: we achieve similar performance
to baseline classiﬁers using less than 50% data (Sec. 4.1),
b) task-aware: networks trained on our data achieve up
to 2.7% improvement for person detection (Sec. 4.2) and
3.5% increase in mAP over all classes on the GMU kitchen
dataset over baseline (Sec. 4.3). We also show that our ap-
proach produces >2X hard positives compared to state-of-
the-art [6, 7] for person detection. Our paper makes the
following contributions:

• We present a novel image synthesizer network that
learns to create composites speciﬁcally to fool a tar-
get network. We show that the synthesizer is effective
at producing hard examples to improve the target net-
work.

• We propose a strategy to make the target network in-
variant to artefacts in the synthesized images, by gen-
erating additional hallucinated artefacts in the back-
ground images.

• We demonstrate applicability of our framework to im-
age classiﬁcation, object detection, and instance detec-
tion.

462

paradigm to train our synthesizer, target, and discriminator
networks. Previous works such as A-Fast-RCNN [49] and
the adversarial spatial transformer (ST-GAN) [26] have
also employed adversarial learning for data generation. The
A-Fast-RCNN method uses adversarial spatial dropout to
simulate occlusions and an adversarial spatial transformer
network to simulate object deformations, but does not
generate new training samples. The ST-GAN approach
uses a generative model to synthesize realistic composite
images, but does not optimize for a target network.

Rendering: Recent works [1, 16, 35, 40, 47, 50] have
used simulation engines to render synthetic images to aug-
ment training data. Such approaches allow ﬁne-grained
control over the scale, pose, and spatial positions of fore-
ground objects, thereby alleviating the need for manual an-
notations. A key problem of rendering based approaches
is the domain difference between synthetic and real data.
Typically, domain adaptation algorithms (e.g. [40]) are nec-
essary to bridge this gap. However, we avoid this problem
by compositing images only using real data.

Hard example mining: Previous works have shown
the importance of hard examples for training robust mod-
els [19, 27, 38, 51, 52, 29]. However, most of these ap-
proaches mine existing training data to identify hard exam-
ples and are bound by limitations of the training set. Unlike
our approach, these methods do not generate new examples.
Recently, [18, 53] proposed data augmentation for generat-
ing transformations that yields additional pseudo-negative
training examples. In contrast, we generate hard positive
examples.

3. Task-Aware Data Synthesis

Our approach for generating hard training examples
through image composition requires as input a background
image, b, and a segmented foreground object mask, m, from
the object classes of interest. The learning problem is for-
mulated as a 3-way competition among the synthesizer S,
the target T , and the discriminator D. We optimize S to
produce composite images that can fool both T and D. T is
updated with the goal to optimize its target loss function,
while D continues to improve its classiﬁcation accuracy.
The resulting synthetic images are both realistic and consti-
tute hard examples for T . The following sections describe
our data synthesis pipeline and end-to-end training process
in more detail.

3.1. Synthesizer Network

The synthesizer operates on the inputs b and m and out-
puts a transformation function, A. This transformation is
applied to the foreground mask to produce a composite syn-
thetic image, f = b ⊕ A(m), where ⊕ denotes the alpha-
blending [26] operation. In this work, we restrict A to the
set of 2D afﬁne transformations (parameterized by a 6− di-

463

Figure 3: Using a discriminator improves the realism of
generated images. (Top) Composite images generated with-
out discriminator in the loop. (Bottom) Composite images
generate with discriminator.

2. Related Work

To the best of our knowledge, ours is the ﬁrst approach
to generate synthetic data by compositing images in a task-
aware fashion. Prior work on synthetic data generation can
be organized into three groups: 1) Image composition, 2)
Adversarial generation, and 3) Rendering.

Image Composition: Our work is inspired by recent cut
and paste approaches [6, 7, 10] to synthesize positive ex-
amples for object detection tasks. The advantage of these
approaches comes from generating novel and diverse jux-
tapositions of foregrounds and backgrounds that can sub-
stantially increase the available training data. The starting
point for our work is the approach of Dwibedi et al. [7], who
were ﬁrst to demonstrate empirical boosts in performance
through the cut and paste procedure. Their approach uses
random sampling to decide the placement of foreground
patches on background images. However, it can produce
unrealistic compositions which limits generalization perfor-
mance as shown by [6]. To help with generalization, prior
works [6, 10] exploited contextual cues [4, 30, 31] to guide
the placement of foreground patches and improve the real-
ism of the generated examples. Our data generator network
implicitly encodes contextual cues which is used to gener-
ate realistic positive examples, guided by the discriminator.
We therefore avoid the need to construct explicit models of
context [4, 6]. Other works have used image compositing
to improve image synthesis [44], multi-target tracking [20],
and pose tracking [37]. However, unlike our approach, none
of these prior works optimize for the target network while
generating synthetic data.

learning:

Adversarial

learning has
emerged as a powerful framework for tasks such as image
synthesis, generative sampling, synthetic data genera-
tion etc. [2, 5, 26, 43] We employ an adversarial learning

Adversarial

Figure 4: Data generated by our approach over multiple
iterations for the AffNIST experiment (Section 4.1). As
training progresses (top to bottom) the synthesized exam-
ples become more complex, from single modes of failure of
the target network to multiple modes at later stages.

mensional feature vector), but the approach can trivially be
extended to other classes of image transformations. b, f, A
are then fed to a Spatial Transformer module [17] which
produces the composite image f (Figure 2). The composite
image is fed to the discriminator and target networks with
the goal of fooling both of them. The synthesizer is trained
in lockstep with the target and discriminator as described in
the following sections.

Blending Artefacts:
In order to paste foreground re-
gions into backgrounds, we use the standard alpha-blending
method described in [17]. One practical challenge, as dis-
cussed in [7], is that the target model can learn to exploit
any artefacts introduced by the blending function, as these
will always be associated with positive examples, thereby
harming the generalization of the classiﬁer. Multiple blend-
ing strategies are used in [7] to discourage the target model
from exploiting the blending artefacts. However, a target
model with sufﬁcient capacity could still manage to over-
ﬁt on all of the different blending functions that were used.
Moreover, it is challenging to generate a large number of
candidate blending functions due to the need to ensure dif-
ferentiability in end-to-end learning.

We propose a simple and effective strategy to address
this problem. We explicitly introduce blending artefacts
into the background regions of synthesized images (see
Fig. 5). To implement this strategy, we (i) randomly choose
a foreground mask from our training set, (ii) copy back-
ground region shaped like this mask from one image, and
(iii) paste it onto the background region in another image
using the same blending function used by S. As a con-
sequence of this process, the presence of a composited re-
gion in an image no longer has any discriminative value, as
the region could consist of either foreground or background
pixels. This simple strategy makes both the discriminator
and the target model invariant to any blending artefacts.

Figure 5: Examples of blending artefacts pasted into back-
ground regions of training images in order to remove any
discriminative cues associated with compositing. A random
foreground-shaped cut-out from a different background im-
age is pasted on the background regions of the given image.
Images from COCO (top row) and VOC (bottom row) are
shown.

3.2. Target Network

The target model is a neural network trained for speciﬁc
objectives such as image classiﬁcation, object detection, se-
mantic segmentation, regression, etc. Typically, we ﬁrst
train the target T with a labeled dataset to obtain a base-
line level of performance. This pre-trained baseline model
T is then ﬁne-tuned in lockstep with S and D. Our synthetic
data generation framework is applicable to a wide range of
target networks. Here we derive the loss for the two com-
mon cases of image classiﬁcation and object detection.
Image Classiﬁcation: For the task of image classiﬁcation,
the target loss function LT is the standard cross-entropy
loss over the training dataset.
Object Detection: For detection frameworks such as
SSD [29] and faster-RCNN [34], for each bounding-box
proposal, the target network outputs (a) probability distribu-
tion p = (p0, · · · , pL) over the L + 1 classes in the dataset
(including background), (b) bounding-box regression off-
sets r ∈ R4. While SSD uses ﬁxed anchor-boxes, faster-
RCNN uses CNN based proposals for bounding boxes. The
ground truth class labels and bounding box offsets for each
proposal are denoted by c and v, respectively. Anchor boxes
with an Intersection-over-Union (IoU) overlap greater than
0.5 with the ground-truth bounding box are labeled with the
class of the bounding box, and the rest are assigned to the
background class. The object detector target T is trained to
optimize the following loss function:

LT (p, c, r, v) = − log(pc)

|

{z

}

+ λ[c > 0]Lloc(r, v)
}

{z

|

(1)

classiﬁcation objective

localization objective

464

Generated digits123904141219235869Iterations8765m, b

Shared Feature 

Network

Foreground

branch

Background

branch

FC

Regression

Network

A

Figure 6: Synthesizer Architecture

where, Lloc is the smooth L1 loss function deﬁned in [12].
The Iverson bracket indicator function [c > 0] evaluates to 1
for c > 0, i.e. for non-background classes and 0 otherwise.
In other words, only the non-background anchor boxes con-
tribute to the localization objective.

3.3. Natural Image Discriminator

An unconstrained cut-paste approach to data augmenta-
tion can produce non-realistic composite images (see for
example Fig. 3). Synthetic data generated in such a way
can still potentially improve the target network as shown
by Dwibedi et al. [7]. However, as others [4, 30, 31] have
shown, generating contextually salient and realistic syn-
thetic data can help the target network to learn more efﬁ-
ciently and generalize more effectively to real world tasks.
Instead of learning speciﬁc context and affordance mod-
els, as employed in aforementioned works, we adopt an ad-
versarial training approach and feed the output of the syn-
thesizer to a discriminator network as negative examples.
The discriminator also receives positive examples in the
form of real-world images. It acts as a binary classiﬁer that
differentiates between real images r and composite images
f . For an image I, the discriminator outputs D(I), i.e. the
probability of I being a real image. D is trained to maxi-
mize the following objective:

LD = Er log(D(r)) + Ef log(1 − D(f )).

(2)

As illustrated in Figure 3, the discriminator helps the

synthesizer to produce more natural looking images.

3.4. Training Details

The three networks, S, T , and D, are trained according

to the following objective function:

LS,T ,D = max

S

min

T

LT + min

S

max

D

LD

(3)

For a given training batch, parameters of S are updated
while keeping parameters of T and D ﬁxed. Similarly, pa-
rameters of T and D are updated by keeping parameters of
S ﬁxed. S can be seen as an adversary to both T and D.
Synthesizer Architecture. Our synthesizer network (Fig-
ure 6) consists of (i) a shared low-level feature extraction
backbone that performs identical feature extraction on

foreground masks m and background images b, (ii) and
parallel branches for mid-level feature extraction on m, b,
and (iii) a fully-connected regression network that takes
as input the concatenation of mid-level features of m, b
and outputs a 6−dimensional feature vector representing
the afﬁne transformation parameters. For the AffNIST
experiments, we use a 2− layer network as the backbone.
For experiments on Pascal VOC and GMU datasets, we
use the VGG-16 [41] network up to Conv-5. The mid-level
feature branches each consist of 2 bottlenecks, with one
convolutional layer, followed by ReLU and BatchNorm
layers. The regression network consists of 2 convolutional
and 2 fully connected layers.

Synthesizer hyper parameters. We use Adam [21] opti-
mizer with a learning rate of 1e − 3 for experiments on the
AffNIST dataset and 1e − 4 for all other experiments. We
set the weight decay to 0.0005 in all of our results.
Target ﬁne-tuning hyper parameters. For the AffNIST
benchmark, the target classiﬁer is ﬁnetuned using the SGD
optimizer with a learning rate of 1e − 2, a momentum of
0.9 and weight decay of 0.0005. For person detection on
VOC, the SSD is ﬁnetuned using the Adam optimizer with
a learning rate of 1e − 5, and weight decay of 0.0005. For
experiments on the GMU dataset, the faster-RCNN model
is ﬁnetuned using the SGD optimizer with a learning rate of
1e − 3, weight decay of 0.0001 and momentum of 0.9.

4. Experiments & Results

We now present qualitative and quantitative results to

demonstrate the efﬁcacy of our data synthesis approach.

4.1. Experiments on AffNIST Data

We show the efﬁciency of data generated using our ap-
proach on AffNIST [45] hand-written character dataset. It
is generated by transforming MNIST [24] digits by ran-
domly sampled afﬁne transformations. For generating syn-
thetic images with our framework, we apply afﬁne transfor-
mations on MNIST digits and paste them onto black back-
ground images.
Target Architecture: The target classiﬁcation model is a
neural network consisting of two 5 × 5 convolutional layers
with 10 and 20 output channels, respectively. Each layer
uses ReLU activation, followed by a dropout layer. The
output features are then processed by two fully-connected
layers with output sizes of 50 and 10, respectively.

We conducted two experiments with AffNIST dataset:
Efﬁcient Data Generation: The baseline classiﬁer is
trained on MNIST. The AffNIST model is ﬁne-tuned by
incrementally adding samples undergoing random afﬁne
transformation as described in [45]. Similarly, results from
our method incrementally improves the classiﬁer using
composite images generated by S.

465

Figure 7: Performance of MNIST classiﬁer on AffNIST
test data when progressively augmented with (i) AffNIST
training data (red), (ii) our synthetic images (green). Our
approaches achieves baseline accuracy (≈ 90%) with less
than half the data (12K samples vs 25K samples). Note
that even with 5K samples we reach an accuracy of ≈ 80%,
compared to baseline accuracy of ≈ 40%.

Table 1: Our approach achieves better classiﬁcation accu-
racy compared to previous pseudo-negative data synthesis
approaches on AffNIST dataset. Numbers are reported from
the respective papers.

Figure 8: Comparison of our approach with Cut-Paste-
Learn [7] and Context-Data-Augmentation [6], on the frac-
tion of hard positives generated for the person class.

Baseline [29]

Column No.
Ann. Cleanup

1

2
✓

Dropout
Blending
1 : 1 Ratio

Discriminator

AP0.5 → 78.93 AP0.8 → 29.52
3
✓

4
✓

5
✓

6
✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

-

]
3
1
[
P
G
N
A
G
W

]
2
3
[
N
A
G
C
D

]
8
1
[
N
C

I

]
5
2
[
N
N
W

I

]
3
5
[
N
T
I

2.78

2.76

2.97

2.56

1.52

Method
Error (%)

AP0.5
AP0.8

79.02 79.13 79.02 79.34
29.64 30.72 30.80 31.25

79.61 79.53
31.96 32.22

Ours
0.99

Table 2: Ablation Studies. We show the effect of design
choices on the performance of our approach. Signiﬁcant im-
provements are observed by introducing blending artefacts
in background regions (col. 4) and maintaining a 1 : 1 ratio
between real and synthetic images (col 5) during training.
Adding a discriminator provides additional boost at AP0.8.

Figure 7 shows the performance of the target model on
the AffNIST test set by progressively increasing the size
of training set. When trained on MNIST dataset alone,
the target model has a classiﬁcation accuracy of 17% on
the AffNIST test set. We iteratively ﬁne-tune the MNIST
model from this point by augmenting the training set with
500 images either from the AffNIST training set (red curve)
or from the synthetic images generated by S (green curve).
Note that our approach achieves baseline accuracy with less
than half the data. In addition, as shown in Figure 7, us-
ing only 5K examples, our method improves accuracy from
40% to 80%. Qualitative results in Figure 4 shows the
progression of examples generated by S. As training pro-
gresses, our approach generates increasingly hard examples
in a variety of modes.

Improvement in Accuracy: In Table 1, we compare our
approach with recent methods [53, 32, 13, 25, 18] that gen-
erate synthetic data to improve accuracy on AffNIST data.
For the result in Table 1, we use 55000, 5000, 10000 split
for training, validation and testing as in [53] along with the
same classiﬁer architecture. We outperform hard negative

generation approaches [53, 25, 18] by achieving a low error
rate of 0.99%. Please ﬁnd more details in the supplemen-
tary.

4.2. Experiments on Pascal VOC Data

We demonstrate improved results using our approach for
person detection on the Pascal VOC dataset [8], using the
SSD−300 network [29]. We use ground-truth person seg-
mentations and bounding box annotations to recover in-
stance masks from VOC 2007 and 2012 training and val-
idation sets as foreground. Background images were ob-
tained from the COCO dataset [28]. We do an initial clean
up of those annotations since we ﬁnd that for about 10%
of the images, the annotated segmentations and bounding-
boxes do not agree. For evaluation we augment the VOC
2007 and 2012 training dataset with our synthetic images,
and report mAP for detection on VOC 2007 test set for all
experiments.

466

Amount of Training DataTest AccuracyComparison of AffNIST & Our Data for TrainingTarget ProbabilityPercentage of Synthetic DataHardness of Synthetic Dataeasy exampleshard examplespop
se-
cret
92.3

94.0

pringles
bbq

red
bull

mAP

88.9

92.2

58.6

65.4

86.3

88.8

Dataset

coca
cola

coffee
mate

honey
bunches

hunt’s
sauce

mahatma
rice

nature
v1

nature
v2

palmolive
orange

Baseline faster-RCNN
Cut-Paste-Learn [7]
Ours

81.9

88.5

95.3

95.5

92.0

94.1

87.3

88.1

86.5

90.3

96.8

97.2

88.9

91.8

80.5

80.1

86.9

95.9

93.9

90.2

90.0

96.6

92.0

87.6

94.9

90.9

69.2

89.8

Table 3: Comparison of our approach with the baseline Faster-RCNN and [7] on the GMU Kitchen Dataset. Our approach
improves overall mAP and outperforms other approaches in most classes.

4.2.1 Comparison with Previous Cut-Paste Methods

We compare our results with the performance of the base-
line SSD network after ﬁne-tuning it with the data gener-
ated by recent approaches from [6, 7]. We use the pub-
licly available software from authors of [6, 7] to generate
the same amount of synthetic data that we use in our exper-
iments. To ensure a fair comparison, we use the same fore-
ground masks and background images with added blending
artifacts for the generation of synthetic data. We report de-
tailed results over multiple IoU thresholds in Table 4, and
some qualitative results in Figure 1.

As observed in [6], we note that adding data generated
from [7] to training leads to a drop in performance. We also
noticed that adding data generated by [6] also leads to a drop
in SSD performance. In contrast, our method improves SSD
performance by 2.7% at AP0.8.
Quality of Synthetic Data: We develop another metric to
evaluate the quality of synthetic data for the task of person
detection. A hardness metric is deﬁned as 1 − p, where p is
the probability of the synthetic composite image containing
a person, according to the baseline SSD. We argue that if
the baseline network is easily able to detect the person in
a composite image, then it is an easy example and may not
boost the network’s performance when added to the training
set. A similar metric has been proposed by previous works
[19, 39, 49, 52] for evaluating the quality of real data.

In Figure 8, we compare the hardness of data generated
by our approach to to that of [6, 7]. The X-axis denotes the
SSD conﬁdence and the Y-axis captures fraction of samples
generated. We generate the same amount of data with all
methods and take an average over multiple experiment runs
to produce this result. As shown in Figure 8, we generate
signiﬁcantly harder examples than [6, 7]. Please ﬁnd more
qualitative examples and experiments in the supplementary
material.

IoU Baseline
78.93
0.5
69.61
0.6
52.97
0.7
0.8
29.54

[7]
76.65
66.88
52.12
28.82

[6]
76.81
66.91
50.21
28.14

Ours no-D
79.61 (+0.68)
70.39 (+0.78)
53.71 (+0.74)
31.96 (+2.44)

Ours +D
79.53 (+0.60)
70.67 (+1.06)
54.50 (+1.53)
32.22 (+2.68)

Table 4: Results on VOC 2007 test data for person detec-
tion. Our augmentation improves the baseline over different
IoU thresholds by 2.7% at an IoU of 0.8.

in the background, (iv) ﬁne-tuning with real and synthetic
data, and (v) adding the discriminator. Our performance
metric is mAP at an IoU threshold of 0.5. While we note
progressive improvements in our performance with each ad-
dition, we see a slight drop in performance after the addition
of the discriminator. We investigate this further in Table 4,
and note that adding the discriminator improves our perfor-
mance on all IoU thresholds higher > 0.5, allowing us to
predict bounding boxes which are much better aligned with
the ground truth boxes.

4.3. Experiments on GMU Data

Lastly, we apply our data synthesis framework to im-
prove the Faster-RCNN [34] for instance detection. We
compare our approach with baseline Faster-RCNN and the
method of [7] on the GMU Kitchen Dataset [11].

The GMU Kitchen Dataset comprises 11 classes and
has 3-fold train/test splits as reported in [7]. We use fore-
grounds from the Big Berkeley Instance Recognition (Big-
BIRD) [42] dataset and backgrounds from the UW Scenes
dataset [23].

Table 3 reports per class accuracy and mean average pre-
cision on the GMU test set. Our approach out-performs
baseline Faster-RCNN and [7] by 3.5% and 1% in mAP,
respectively.
Interestingly, we improve accuracy of some
categories such as ’palmolive-orange’ by up to 7.5%.

4.2.2 Ablation Studies

5. Conclusion

Table 2 studies the effect of various parameters on the per-
formance of an SSD network ﬁne-tuned on our data.
In
particular, we study the effect of (i) excluding noisy fore-
ground segmentation annotations during generation, (ii) us-
ing dropout in the synthesizer, (iii) adding blending artifacts

The recent success of deep learning has been fueled by
supervised training requiring human annotations. Large
training sets are essential for improving performance under
challenging real world environments, but are difﬁcult, ex-
pensive and time-consuming to obtain. Synthetic data gen-

467

Figure 9: Qualitative results for VOC 2007 test set before and after training SSD with our synthesized data. Green and red
boxes show correct and missed detections, respectively. Note that synthetic data helps improve the SSD performance on
severely occluded and small instances.

eration offers promising new avenues to augment training
sets to improve the accuracy of deep neural networks.

In this paper, we introduced the concept of task-aware
synthetic data generation to improve the performance of a
target network. Our approach trains a synthesizer to gen-
erate efﬁcient and useful synthetic samples, which helps to
improve the performance of the target network. The tar-
get network provides feedback to the synthesizer, to gener-
ate meaningful training samples. We proposed a novel ap-
proach to make the target model invariant to blending arte-
facts by adding similar artefacts on background regions of
training images. We showed that our approach is efﬁcient,
requiring less number of samples as compared to random
data augmentations to reach a certain accuracy. In addition,
we show a 2.7% improvement in the state-of-art person de-
tection using SSD. Thus, we believe that we have improved
the state-of-art in synthetic data generation tailored to im-
prove deep learning techniques.

Our work opens up several avenues for future research.
Our synthesizer network outputs afﬁne transformation pa-
rameters, but can be easily extended to output additional

learnable photometric transformations to the foreground
masks and non-linear deformations. We showed compo-
sition using a foreground and background image, but com-
positing multiple images can offer further augmentations.
While we showed augmentations in 2D using 2D cut-outs,
our work can be extended to paste rendered 3D models into
2D images. Our approach can also be extended to other tar-
get networks such as regression and segmentation networks.
Future work includes explicitly adding a diversity metric to
data synthesis to further improve its efﬁciency.

6. Acknowledgements

We wish to thank Kris Kitani for valuable discussions on

this topic.

468

References

[1] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for op-
tical ﬂow evaluation. In European Conference on Computer
Vision, pages 611–625. Springer, 2012. 3

[2] Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dy-
lan Drover, Rohith MV, Stefan Stojanov, and James M.
Rehg. Unsupervised 3d pose estimation with geometric self-
supervision.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2019. 3

[3] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man´e, Vijay Va-
sudevan, and Quoc V. Le. Autoaugment: Learning augmen-
tation policies from data. CoRR, abs/1805.09501, 2018. 1

[4] Santosh K. Divvala, Derek Hoiem, James H. Hays, Alexei A.
Efros, and Martial Hebert. An empirical study of context
in object detection.
In Proc. IEEE Conf. on Computer Vi-
sion and Pattern Recognition (CVPR 09), pages 1271–1278,
2009. 3, 5

[5] Dylan Drover, Ching-Hang Chen, Amit Agrawal, Ambrish
Tyagi, and Cong Phuoc Huynh. Can 3d pose be learned from
2d projections alone? In European Conference on Computer
Vision Workshop, pages 78–94. Springer, 2018. 3

[6] Nikita Dvornik, Julien Mairal, and Cordelia Schmid. Mod-
eling visual context is key to augmenting object detection
datasets. In IEEE European Conference on Computer Vision
(ECCV), 2018. 1, 2, 3, 6, 7

[7] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut,
paste and learn: Surprisingly easy synthesis for instance de-
tection. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 1, 2, 3, 4, 5, 6, 7

[8] Mark Everingham, Luc Gool, Christopher K. Williams, John
Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. Int. J. Comput. Vision, 88(2):303–
338, June 2010. 2, 6

[9] Alhussein Fawzi, Horst Samulowitz, Deepak Turaga, and
Pascal Frossard. Adaptive data augmentation for image clas-
siﬁcation.
In Proc. IEEE Intl. Conf. on Image Processing
(ICIP 16), pages 3688–3692, 2016. 1

[10] Georgios Georgakis, Arsalan Mousavian, Alexander C.
Berg, and Jana Kosecka. Synthesizing training data for
object detection in indoor scenes. CoRR, abs/1702.07836,
2017. 1, 3

[11] Georgios Georgakis, Md. Alimoor Reza, Arsalan Mousa-
vian, Phi-Hung Le, and Jana Kosecka. Multiview RGB-D
dataset for object instance detection. CoRR, abs/1609.07826,
2016. 2, 7

[12] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015. 5

[13] Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent
Improved training of

Dumoulin, and Aaron C. Courville.
wasserstein gans. CoRR, abs/1704.00028, 2017. 6

[15] Søren Hauberg, Oren Freifeld, John W. Fisher III, and
Lars Kai Hansen. Dreaming More Data: Class-dependent
Distributions over Diffeomorphisms for Learned Data Aug-
mentation. In Proceedings of the 19th International Confer-
ence on Artiﬁcial Intelligence and Statistics (AISTATS 16),
volume JMLR: W&CP, pages 342–350, 2016. 1

[16] Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, and
Kurt Konolige. On pre-trained image features and synthetic
images for deep learning. CoRR, abs/1710.10710, 2017. 3

[17] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and
Koray Kavukcuoglu. Spatial transformer networks. CoRR,
abs/1506.02025, 2015. 4

[18] Long Jin, Justin Lazarow, and Zhuowen Tu.

Introspec-
tive classiﬁcation with convolutional nets.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems 30, pages 823–833. Curran As-
sociates, Inc., 2017. 3, 6

[19] S. Jin, A. RoyChowdhury, H. Jiang, A. Singh, A. Prasad, D.
Chakraborty, and E. Learned-Miller. Unsupervised Hard Ex-
ample Mining from Videos for Improved Object Detection.
European Conference on Computer Vision (ECCV), 2018. 3,
7

[20] Anna Khoreva, Rodrigo Benenson, Eddy Ilg, Thomas Brox,
and Bernt Schiele. Lucid data dreaming for multiple object
tracking. arXiv preprint arXiv:1703.09554, 2017. 3

[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014. 5

[22] Philipp Kr¨ahenb¨uhl. Free supervision from video games. In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR 18), pages 2955–2964, 2018. 1

[23] Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox.
A large-scale hierarchical multi-view rgb-d object dataset.
2011 IEEE International Conference on Robotics and Au-
tomation, pages 1817–1824, 2011. 7

[24] Yann LeCun and Corinna Cortes. MNIST handwritten digit

database. 2010. 5

[25] Kwonjoon Lee, Weijian Xu, Fan Fan, and Zhuowen
Tu. Wasserstein introspective neural networks. CoRR,
abs/1711.08875, 2017. 6

[26] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Schecht-
man, and Simon Lucey. ST-GAN: Spatial Transformer Gen-
erative Adversarial Networks for Image Compositing.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR 18), 2018. 3

[27] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll´ar. Focal loss for dense object detection.
CoRR, abs/1708.02002, 2017. 3

[28] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft
COCO: common objects in context. CoRR, abs/1405.0312,
2014. 6

[14] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.
Synthetic Data for Text Localisation in Natural Images. In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR 16), pages 2315–2324, 2016. 1

[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexan-
der C. Berg. SSD: single shot multibox detector. CoRR,
abs/1512.02325, 2015. 2, 3, 4, 6

469

[44] Fuwen Tan, Crispin Bernier, Benjamin Cohen, Vicente Or-
au-
CoRR,

donez, and Connelly Barnes. Where and who?
tomatic semantic-aware person composition.
abs/1706.01021, 2017. 3

[45] Tijmen Tieleman. affnist dataset. 2013. 2, 5
[46] Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and
Ian Reid. A Bayesian Data Augmentation Approach for
Learning Deep Models. In Proceedings Advances in Neural
Information Processing Systems 31 (NIPS 17), pages 1–10,
2017. 1

[47] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark
Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cam-
eracci, Shaad Boochoon, and Stan Birchﬁeld. Training deep
networks with synthetic data: Bridging the reality gap by do-
main randomization. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 3

[48] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-
mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
Learning from synthetic humans. 1

[49] Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta.
A-Fast-RCNN: Hard positive generation via adversary for
object detection. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR 17), pages
2606–2615, 2017. 3, 7

[50] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian.
Building generalizable agents with a realistic and rich 3d en-
vironment. arXiv preprint arXiv:1801.02209, 2018. 3

[51] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan
Liu, and Dawn Song. Spatially transformed adversarial ex-
amples. CoRR, abs/1801.02612, 2018. 3

[52] Hao Yu, Zhaoning Zhang, Zheng Qin, Hao Wu, Dongsheng
Li, Jun Zhao, and Xicheng Lu. Loss rank mining: A general
hard example mining method for real-time detectors. CoRR,
abs/1804.04606, 2018. 3, 7

[53] Yunhan Zhao, Ye Tian, Wei Shen, and Alan Yuille. To-
wards resisting large data variations via introspective learn-
ing. 2018. 3, 6

[30] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The Role of Context for Object Detection and
Semantic Segmentation in the Wild. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR 14), pages 891–898, 2014. 3, 5

[31] Aude Oliva and Antonio Torralba. The role of context in ob-
ject recognition. Trends in Cognitive Sciences, 11(12):520–
527, 2007. 3, 5

[32] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gen-
erative adversarial networks. CoRR, abs/1511.06434, 2015.
6

[33] Alexander J Ratner, Henry R Ehrenberg, Zeshan Hussain,
Jared Dunnmon, and Christopher R´e. Learning to Compose
Domain-Speciﬁc Transformations for Data Augmentation.
In Proceedings Advances in Neural Information Processing
Systems 31 (NIPS 17), 2017. 1

[34] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. CoRR, abs/1506.01497, 2015. 2, 4,
7

[35] Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun.

Playing for benchmarks. In ICCV, 2017. 1, 3

[36] Tim Salimans, Ian Goodfellow, Vicki Cheung, Alec Rad-
ford, and Xi Chen. Improved Techniques for Training GANs.
In Proceedings Advances in Neural Information Processing
Systems 30 (NIPS 16), pages 1–10, 2016. 1

[37] I. S´ar´andi, T. Linder, K. O. Arras, and B. Leibe. Synthetic
Occlusion Augmentation with Volumetric Heatmaps for the
2018 ECCV PoseTrack Challenge on 3D Human Pose Esti-
mation. ArXiv e-prints, Sept. 2018. 3

[38] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard ex-
ample mining. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 761–769,
2016. 3

[39] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard ex-
ample mining. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2016. 7

[40] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel,

Josh
Susskind, Wenda Wang, and Russ Webb. Learning from sim-
ulated and unsupervised images through adversarial training.
arXiv preprint arXiv:1612.07828, 2016. 1, 3

[41] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 5

[42] Arjun Singh, James Sha, Karthik S. Narayan, Tudor Achim,
and Pieter Abbeel. Bigbird: A large-scale 3d database of
object instances. 2014 IEEE International Conference on
Robotics and Automation (ICRA), pages 509–516, 2014. 7

[43] Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon.
Constructing unrestricted adversarial examples with genera-
tive models. In Advances in Neural Information Processing
Systems, pages 8312–8323, 2018. 3

470

