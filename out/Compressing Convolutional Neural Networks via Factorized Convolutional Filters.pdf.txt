Compressing Convolutional Neural Networks via

Factorized Convolutional Filters

Tuanhui Li1 Baoyuan Wu2∗ Yujiu Yang1∗ Yanbo Fan2 Yong Zhang2 Wei Liu2

1Graduate School at Shenzhen, Tsinghua University 2Tencent AI Lab

lth17@mails.tsinghua.edu.cn, wubaoyuan1987@gmail.com,yang.yujiu@sz.tsinghua.edu.cn,

{fanyanbo0124,zhangyong201303}@gmail.com, wl2223@columbia.edu

Abstract

This work studies the model compression for deep con-
volutional neural networks (CNNs) via ﬁlter pruning. The
workﬂow of a traditional pruning consists of three sequen-
tial stages: pre-training the original model, selecting the
pre-trained ﬁlters via ranking according to a manually de-
signed criterion (e.g., the norm of ﬁlters), and learning the
remained ﬁlters via ﬁne-tuning. Most existing works fol-
low this pipeline and focus on designing different ranking
criteria for ﬁlter selection. However, it is difﬁcult to con-
trol the performance due to the separation of ﬁlter selection
and ﬁlter learning. In this work, we propose to conduct ﬁl-
ter selection and ﬁlter learning simultaneously, in a uniﬁed
model. To this end, we deﬁne a factorized convolutional
ﬁlter (FCF), consisting of a standard real-valued convolu-
tional ﬁlter and a binary scalar, as well as a dot-product op-
erator between them. We train a CNN model with factorized
convolutional ﬁlters (CNN-FCF) by updating the standard
ﬁlter using back-propagation, while updating the binary
scalar using the alternating direction method of multipli-
ers (ADMM) based optimization method. With this trained
CNN-FCF model, we only keep the standard ﬁlters corre-
sponding to the 1-valued scalars, while all other ﬁlters and
all binary scalars are discarded, to obtain a compact CNN
model. Extensive experiments on CIFAR-10 and ImageNet
demonstrate the superiority of the proposed method over
state-of-the-art ﬁlter pruning methods.

1. Introduction

Many popular deep convolutional neural networks have
emerged in recent years, e.g., VGGNet [32] and ResNet
[10], etc. These models show promising results on many
visual tasks, such as image classiﬁcation [18, 36, 37, 39],

∗indicates corresponding authors. This work was done when Tuanhui

Li was an intern at Tencent AI Lab.

semantic segmentation [2, 26], object detection [6], object
tracking [22, 43] or visual reasoning [34, 42]. However,
the model sizes and computation complexities of these deep
models also grow exponentially. For example, ResNet-152
[10] contains about 60 million parameters and 11.3 billion
FLOPS, which precludes the application of these models
to mobile systems. A feasible approach to tackle this dif-
ﬁculty is model compression, whose goal is to reduce the
parameters while keeping the model performance as much
as possible.

Many seminal works have been developed in the liter-
ature of model compression for deep convolutional neural
networks. They can be generally partitioned to four cate-
gories, including pruning [8, 12, 21, 23, 27], low-rank fac-
torization [16, 19, 45], weight quantiﬁcation [24, 25] and
compact network design [13, 28], respectively. In this work,
we focus on the pruning approach, and we refer the readers
to [4] for more details about other categories. Speciﬁcally,
we focus on the ﬁlter-level pruning (ﬁlter pruning), which
prunes the output channel of a ﬁlter tensor. A typical work-
ﬂow of the ﬁlter pruning is demonstrated in Fig. 1 (top). It
consists of three sequential stages, including the training of
the original model, pruning ﬁlters according to a manually
designed ranking criterion, and ﬁne-tuning the model with
remained ﬁlters. Many existing works focused on design-
ing different ranking criteria. However, most of these crite-
ria depend on the weights values themselves or the results
(e.g., the classiﬁcation accuracy) of the pre-trained original
model. A typical criterion is the assumption that the ﬁlters
with small ’weight’ norms have small contributions to the
model, thus they can be pruned [21]. However, to the best of
our knowledge, we have never found a rigorous veriﬁcation
of this assumption. What is more important, the manually
designed ranking criterion only depends on the pre-trained
original model, rather than the followed ﬁne-tuning process
of the pruned model. The efﬁcacy of the ranking criterion

3977

Figure 1. Overview of the workﬂow of ﬁlter pruning on layer l (1 ≤ l ≤ L), where the dotted green cubes indicate the pruned ﬁlters.
(Top): Traditional pruning consists of three sequential stages: pre-training, selecting ﬁlters according to a ranking criterion, and ﬁne-tuning.
(Bottom): Our method conducts the ﬁlter learning and ﬁlter selection jointly, through training factorized convolutional ﬁlters.

can be only measured according to the ﬁne-tuning result. It
may take many iterations of pruning and ﬁne-tuning to ﬁnd
a good ranking criterion.

In this work, we propose to conduct ﬁlter learning and
ﬁlter selection jointly, in a uniﬁed optimization framework.
To this end, we design a novel type of ﬁlter, dubbed fac-
torized convolutional ﬁlter (FCF). As shown in Fig. 1 (bot-
tom), a FCF consists of a standard convolutional ﬁlter Wl
i
(l is the layer index and i is the ﬁlter index), and a bi-
nary scalar vl
i ∈ {0, 1}, as well as a dot-product operator
between them. After training the CNN model with fac-
torized convolutional ﬁlters (CNN-FCF), the standard ﬁl-
ters corresponding to 0-valued binary scalars and all binary
scalars in other FCFs are directly abandoned, to construct
a compact CNN model (see the green ﬁlters in Fig. 1 (bot-
tom)). However, due to the binarity of vl
i, the standard back-
propagation with gradient descent algorithm cannot be di-
rectly adopted to train CNN-FCF. To tackle this difﬁculty,
inspired by the general integer programming framework,
i.e., ℓp-Box ADMM [38], the binary vector vl (including
all binary scalars in layer l) is equivalently reformulated as
a continuous vector, being subjected to the intersected con-
straint space between the box constraint and the ℓ2-sphere
constraint. Consequently, we propose a novel training al-
gorithm for CNN-FCF by inserting the alternating direction
method of multipliers (ADMM) [3] algorithm into the back-
propagation framework. Speciﬁcally, Wl is updated by the
standard gradient descent algorithm, while vl is updated by
the ADMM algorithm. We compress the popular ResNet
models with different layers on two benchmark datasets, in-
cluding CIFAR-10 [17] and ImageNet LSVRC-2012 [31]
(ImageNet for clarity). Experimental results verify the com-
petitive performance of the proposed method, compared to
the state-of-the-art ﬁlter pruning methods.

The main contributions of this work are three-fold. (1)
We propose to conduct ﬁlter learning and ﬁlter selection
jointly in a uniﬁed optimization framework, through train-
ing CNNs with factorized convolutional ﬁlters. (2) We pro-
pose a novel algorithm to train CNN-FCF by inserting the
ADMM algorithm into the standard gradient-based back-
propagation framework. (3) Extensive experiments on pop-
ular ResNet models and benchmark datasets demonstrate
the efﬁcacy of the proposed method.

2. Related Work

In this section, we brieﬂy review the pruning-based com-
pressing approach, which can be generally partitioned to
two levels, including weight-level and ﬁlter-level pruning.

Weight-level pruning (weight pruning for clarity) is an
unstructured pruning method that prunes some entries in
each ﬁlter.
It was ﬁrstly proposed in optimal brain dam-
age [20] and optimal brain surgeon [9], which pruned the
weights according to the second order derivatives of the loss
function. Recently, Han et al. [8] proposed to remove the
weights with small values below the threshold. Guo et al.
[7] proposed an interactive method, which is composed of
pruning and splicing. Zhang et al. [44] formulated the prun-
ing problem into a non-convex optimization problem and
combined the weights with cardinality constraints. How-
ever, weight pruning can only produce a sparse network.
Consequently, the memory and computational cost of the
compressed model are not signiﬁcantly reduced.

Filter-level pruning (ﬁlter pruning for clarity) is a struc-
tured pruning method that prunes the whole channel of ﬁl-
ters. Thus, it can reduce more parameters and computation
costs than the weight pruning method. (1) Some works cal-
culate the importance score for each ﬁlter according to a
manually designed evaluation criterion. A commonly used
metric is the norm of ﬁlters [11, 21], based on the assump-

3978

𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖1𝑙𝑣𝐶𝑙𝑙⨀𝐯𝑙0.2361.4250.395𝑣1𝑙Initial convolutional filtersOptimized convolutional filtersOptimized factorized convolutional filters0.962𝑣2𝑙𝑣3𝑙Train൝𝐯𝑙∈0,1𝐶𝑙𝟏⊤𝐯𝑙=𝑘𝑙scoreRankPruneFine-tuneFine-tuned convolutional filters…Joint trainPruneInitial factorized convolutional filtersPruned convolutional filters𝑣𝐶𝑙𝑙=1𝑣1𝑙=1𝑣2𝑙=0𝑣3𝑙=0𝐖𝑙𝐖𝑙𝐖2𝑙𝐖3𝑙𝐖𝐶𝑙𝑙…𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖𝑙𝐖2𝑙𝐖3𝑙𝐖𝐶𝑙𝑙…𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖𝑙𝐖2𝑙𝐖3𝑙𝐖𝐶𝑙𝑙…𝐖2𝑙𝐖𝐶𝑙𝑙…𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖1𝑙𝐖𝑙𝐖2𝑙𝐖3𝑙𝐖𝐶𝑙𝑙…𝐖1𝑙⨀𝐯𝑙…𝐖1𝑙𝐖2𝑙𝐖3𝑙𝐖1𝑙𝐖𝑙𝐖𝐶𝑙𝑙…𝐖1𝑙𝐖𝑙𝐖𝐶𝑙𝑙𝐖1𝑙…[40] and Ye et al.

tion that ﬁlters with small norms do not contribute much
to the model performance. Similar to ﬁlter norm, Hu et al.
[14] proposed to rank the ﬁlters with the average percentage
of zeros of the corresponding output feature maps. Yu et al.
[41] proposed to obtain the importance score of other layers
via back-propagating the ﬁlter scores of the ﬁnal response
layer. (2) Regularization constraints are also usually used in
many studies for pruning. Some studies introduced group
lasso with ﬁlters to directly derive sparse ﬁlters [1, 35].
[23] imposed ℓ1 constraint
Liu et al.
to the scaling factors of BN layers, and the magnitudes of
these scaling factors are used as the ﬁlter scores. Huang
et al. [15] introduced the scaling factors with a ℓ1 regular-
ization for the selection of different micro-structures such
as residual blocks. (3) Some works proposed to minimize
the reconstruction error between the original model and the
pruned model. He et al. [12] formulated the reconstruc-
tion error using lasso regression to retain the representative
ﬁlters. Luo et al. [27] proposed to minimize the reconstruc-
tion error based on greedy search. Inspired by optimal brain
damage, Dong et al. [5] performed Taylor expansion on the
reconstruction error function, and determined the parame-
ter importance according to the second order derivatives.
Zhuang et al. [46] proposed to seek discriminative chan-
nels by minimizing both reconstruction error loss and addi-
tional loss. The main commonality of above related works
is that the ﬁlter selection is somewhat separated with ﬁlter
learning. In contrast, the proposed method conducts ﬁlter
selection and ﬁlter learning simultaneously.

3. Model Compression

3.1. Convolutional Filter
dataset

training

consists

of N samples
Given
{xi, yi}N
i=1, with xi being input feature and yi being
ground-truth label of xi. Considering a CNN model with L
layers, we use Wl ∈ RC l×N l×W l×H l
to represent the ﬁl-
ters of the l-th convolutional layer, where (C l, N l, W l, H l)
are the number of output channels, the number of input
channels, the width of kernel and the height of kernel,
respectively. The convolutional operation of layer l is
formulated as

Rl

out = Conv(Rl

in, Wl),

(1)

where Conv denotes the convolutional operation. Rl
RN ×N l×W l
the input and output responses of layer l, respectively.

out ∈ RN ×C l×W l

in ∈
out indicate

in and Rl

×H l

×H l

out

in

3.2. Factorized Convolutional Filter

To facilitate the ﬁlter selection, we propose a novel ﬁlter,
dubbed factorized convolutional ﬁlter (FCF), by associating
i ∈ RN l×W l×H l
a binary scalar vl
(l is layer index and i is ﬁlter index), through a dot-product
operator. It is formulated as follows:

i ∈ {0, 1} to each ﬁlter Wl

Rl

out = Conv(Rl

in, Wl ⊙ vl),

where vl = [. . . ; vl
i ⊙ vl
[. . . ; Wl
cating that vl

i; . . .] ∈ {0, 1}C l
i; . . .] ∈ RC l×N l×W l×H l
i is multiplied to every element in Wl
i.

, with Wl

(2)
and Wl ⊙ vl =
i indi-

i ⊙ vl

3.3. Training CNNs with Factorized Convolutional

Filters

Joint training.
In this section, we present how to con-
duct ﬁlter learning and ﬁlter selection jointly, based on
the factorized convolutional ﬁlters. Let W = {Wl}L
l=1,
v = {vl}L
l=1, we denote f (xi; W, v) as the output proba-
bility of a CNN model with factorized convolutional ﬁlters
(CNN-FCF). Then, the objective function of training CNN-
FCF is formulated as follows:

arg min

W,v

1
N

N

Xi=1

ℓ(cid:0)yi, f (xi; W, v)(cid:1)

s.t. 1⊤vl = kl, vl ∈ {0, 1}C l

, ∀ l ∈ {1, 2, ..., L},

(3)

where kl ∈ {1, 2, . . . , C l} denotes the number of remained
ﬁlters in layer l after pruning. The loss function ℓ can be
speciﬁed by any loss function that could be used to train
standard CNNs. In our experiments for image classiﬁcation,
we adopt the cross entropy loss. Due to the binary constraint
on v, Problem (3) cannot be directly optimized by contin-
uous optimization algorithm (e.g., the gradient-based back-
propagation algorithm). Thus, we propose a novel continu-
ous optimization algorithm, dubbed back-propagation with
ADMM, as detailed in Section 4.

Filter pruning. Given a trained CNN-FCF model through
optimizing Problem (3), one can obtain 1) a compact CNN
model by pruning the ﬁlters Wl
i corresponding to zero-
valued vl
i in each layer (see Fig. 1 (bottom)), or 2) a sparse
CNN model by setting zeros to the ﬁlters Wl
i correspond-
ing to the zero-valued vl
i. The choice of this two types of
models depends on the model architecture and the pruning
strategy, which will be detailed in Section 5.2.

4. Optimization

4.1. Continuous Reformulation

To tackle the binary constraint in Problem (3), we ﬁrstly
transform the binary constraint to continuous constraints us-
ing the following technique proposed in [38], as follows:

vl ∈ {0, 1}C l

⇔ vl ∈ Sb ∩ vl ∈ Sp,

(4)

denotes a box constraint, and Sp =

where Sb = [0, 1]C l
2 = C l

nvl : kvl − 1

2 k2

4 o indicates a ℓ2-sphere constraint.

Furthermore, following the ℓp-Box ADMM algorithm [38],
we introduce two additional variables to split the continuous
constraints, so that they can be satisﬁed alternatively. Con-
sequently, Problem (3) can be equivalently reformulated as

3979

the following continuous problem:

arg min
W,v,z1,z2

1
N

N

Xi=1

ℓ(cid:0)yi, f (xi; W, v)(cid:1)

s.t. vl = zl

1, vl = zl
2,
zl
2 ∈ Sp,

zl
1 ∈ Sb,

1⊤zl

1 = kl,

∀ l ∈ {1, 2, ..., L},

(5)

1}L
where z1 = {zl
after we shorten 1

l=1 and z2 = {zl

2}L

l=1. For clarity, here-

N PN

i=1 ℓ(cid:0)yi, f (xi; W, v)(cid:1) as L(W, v).

4.2. Back propagation with ADMM

is continuous,

Although Problem (5)

the back-
propagation algorithm with the standard gradient-based op-
timizer (e.g., stochastic gradient descent (SGD) [30] ) can’t
be directly used to optimize the constrained problem. We
propose a new algorithm by inserting the ADMM algo-
rithm into the standard back-propagation training frame-
work. Speciﬁcally, considering layer l, the parameters are
updated by the following alternative steps:

• Given the ﬁxed vl, the parameter Wl can be updated

using gradient-based optimizer (see Section 4.2.1);

• Given the ﬁxed Wl, the variables vl, zl

1, zl

2 are up-

dated by the ADMM algorithm (see Section 4.2.2).

This algorithm is outlined in Algorithm 1. To facilitate the
following derivations of the proposed algorithm, we decom-
pose the output function f (xi; W, v) with respect to the
parameters of layer l, as follows:

out; {Wj}L

j=l+1, {vj}L

j=l+1(cid:17),

Rl

out = Conv(Rl

f (xi; W, v) = f1(cid:16)Rl
in = f2(cid:16)xi; {Wj}l−1

Rl




where Rl
of layer l, respectively.

in and Rl

in, Wl ⊙ vl),

j=1, {vj}l−1

j=1(cid:17),

(6)
out denote the input and output response

4.2.1 Given vl, Solving Wl Using Gradient Descent

Wl can be updated using the standard gradient descent,

Wl = Wl − ηW

∂L(W, v)

∂Wl

,

(7)

where ηW denotes the learning rate. Using the chain rule
and the decomposition in Eq. (6), we have




∂L(W, v)

∂Wl

=

∂L
∂f1

×

∂f1
∂Rl

×

out

∂Rl
Wl

,

out
∂Wl ⊙ vl

out

out

∂Rl
Wl =

∂Rl
∂Wl ⊙ vl ×

∂Rl
∂Wl ⊙ vl × Vl,
where Vl is expanded from vl, and has the same shape as
Wl. All the terms can be easily computed as did in training
standard CNNs.

∂Wl =

out

Algorithm 1 Back-propagation with ADMM
Input: Training data {xi, yi}N

i=1 and initial Wl, vl, ∀ l ∈

{1, 2, ..., L}.

Output: ˆWl.
1: while not converged do
2:

for l = 1 to L do

3:

4:

Given vl, update Wl using gradient descent (see
Section 4.2.1);
Given Wl, update vl using ADMM (see Section
4.2.2).

end for
5:
6: end while
7: if vl

i = 1 then
ˆWl
i = Wl
8:
9: end if
10: return ˆWl

i (l is layer index and i is ﬁlter index).

4.2.2 Given Wl, Solving vl Using ADMM

With the ﬁxed Wl and parameters of all other layers, Prob-
lem (5) with respect to vl, zl
2 can be solved by the alter-
nating direction method of multipliers (ADMM) [3] algo-
rithm. Following the standard ADMM procedure, we ﬁrstly
present the augmented Lagrangian function, as follows:

1, zl

L(Wl, vl, zl

1, zl

2, ul
1) + h2(zl

1, ul
2) + (ul

2) = L(W, v)

1)⊤(vl − zl
1)

+ h1(zl

(8)

+ (ul

2)⊤(vl − zl

2) +

1k2

2 + kvl − zl

2k2

ρl

2hkvl − zl

2i,

1 : 1⊤zl

2) = I(zl

1) = I(zl

1 ∈ Sb ∩ {zl

where h1(zl
1 = kl}) and
h2(zl
2 ∈ Sp) are indicator functions. I(a) = 0
if a is true, otherwise I(a) = ∞. ul
are dual
variables, and ρl > 0 is a penalty parameter. Then, in the
following, we iteratively update (zl
2, vl) by minimizing
the Lagrangian during each training iteration, and update
(ul
1, zl
Update (zl
sub-problems:

2): They are updated by solving the following

2) by gradient ascent.

2 ∈ RC l

1, ul

1, ul

1, zl

( zl

1 = arg minzl
zl
2 = arg minzl

1∈Sc

1∈Sp

1

2 kzl
2 kzl

2 − (vl + ul
2 − (vl + ul

1k2
2k2

ρl )⊤zl
1,
ρl )⊤zl
2,

1

1

2

(9)

1 : 1⊤zl

where Sc = Sb ∩ {zl
1 = kl}. The ﬁrst sub-problem
is a standard quadratic program (QP) problem, which can
be globally optimized by any off-the-shelf QP solver. In our
experiments, we adopt the OSQP solver [33]. The second
sub-problem can be globally optimized by projecting the
solution of the unconstrained problem onto the ℓ2-sphere
(i.e., Sp), as presented in [38].
Update vl: Due to the loss term L(W, v), the sub-problem

3980

with respect to vl doesn’t have a closed form solution. In-
stead, we adopt the gradient descent algorithm, as follows:

vl = vl − ηv

where,

∂L(Wl, vl, zl
∂vl

1, zl

2, ul

1, ul
2)
,

(10)

∂L
∂vl =
∂L(W, v)

∂L(W, v)

+

×

∂C
∂vl

,

∂f1
∂Rl

∂vl

=

∂L
∂f1

×

∂Rl
vl

out

,

out
∂Wl ⊙ vl

out

∂Rl
∂Wl ⊙ vl ×
1 + ul

∂vl
2 + ρl(2vl − zl

1 − zl

2).




∂vl

∂Rl

out

vl =
∂C
∂vl = ul
Update (ul

1, ul

2):

=

∂Rl
∂Wl ⊙ vl × Wl,

out

(ul

ul

1 = ul
2 = ul

1 + ρl(vl − zl
2 + ρl(vl − zl

1),
2).

(11)

Besides, to accelerate the convergence process, we increase
ρl by µl after each iteration, i.e., ρl ← µlρl, and set an
upper bound ρl
max to avoid early stopping. They will be
speciﬁed in experiments.

4.2.3 Complexity Analysis

analyze
in Algorithm 1.

the

computational

layer

Here we
of
the forward Rl
Eq. 2)

l

complexity
The complexity of
in, Wl ⊙ vl)
(see
in).
The complex-

out = Conv(Rl
inH l

is O(N C lN lW lH lW l
the backward ∂Rl
outW lH l).

outH l

out

Wl

(see Section 4.2.1)

ity of
is
O(N C lN lW l
They are same with the
complexities of forward and backward pass in standard
convolutional layer. The additional complexity is mainly
1, which is O((C l)3). Considering
from the QP solver for zl
the performance enhancement from the joint training, we
believe that such an extra training cost is acceptable.

5. Experiments

5.1. Experimental Settings

Dataset. Our experiments are conducted on two bench-
including CIFAR-10 [17] and ImageNet
mark datasets,
[31]. CIFAR-10 has 50k training images and 10k validation
images, which is annotated by 10 classes. ImageNet con-
tains 1.28 million training images and 50k validation im-
ages of 1000 classes.
Models and compared methods. We evaluate the pro-
posed method on the ResNet [10] architecture with different
layers. On CIFAR-10, we compress ResNet-20, ResNet-
32, ResNet-56 and ResNet-110, comparing with state-of-
the-art methods, inlcuding SNLI [40], SFP [11], Pruning
[21], NISP [41]. On ImageNet, we compress ResNet-34

and ResNet-50 to compare with NISP [41], SFP [11], Prun-
ing [21], ThiNet [27], SSS [15] and Channel Pruning [12].
Evaluation metrics. We adopt three metrics to evaluate
the compressed model, including Params.↓%, FLOPs↓%,
and Acc.↓%. Params.↓% denotes the ratio of pruned pa-
rameters from the original model. FLOPs↓% represents the
ratio of decreased computational cost compared to the orig-
inal model. Acc.↓% indicates the reduced accuracy com-
pared to the accuracy of the orginal model. A better com-
pressing performance corresponds the higher Params.↓%
and FLOPs↓%, while the lower Acc.↓%.
Implementation details. In the ﬁrst stage, the joint train-
ing process of W and v using the proposed algorithm of
back-propagation with ADMM for at most 30 epochs. The
learning rates of ηW (see Eq. 7) and ηv (see Eq. 10) are
initialized as 0.1, and they are divided by 10 after every 10
epochs.
In terms of the hyper-parameters of the ADMM
part (see Section 4.2.2), we adopt same settings for all lay-
ers. Speciﬁcally, for l-th layer, vl is initialized as 1 to in-
clude all ﬁlters at the very beginning, (ul
2) are
initialized as 0. On ImageNet, ρl is initialized as 0.001;
µl, ρl
max and the batch-size are set as 1.001, 6 and 256, re-
spectively. On CIFAR-10, ρl is initialized as 0.01; µl, ρl
max
and the batch-size are set as 1.01, 6 and 128, respectively.
Besides the maximum number of epochs (i.e., 30), we also
set another stopping criterion, i.e., kvl − zl
2 6 10−4 and
kvl − zl
2 6 10−4, ∀ l ∈ {1, 2, . . . , L}. Then, we binarize
vl to exactly 0 or 1. Theoretically speaking, if vl converges
exactly to 0 or 1, the trained CNN-FCF model will be the
output. However, due to the numerical reason, there are still
small changes on vl after binarization, i.e., Wl
i| or
Wl
i − 0|. In Section 5.5, we will study the inﬂuence of
these small differences. To alleviate the potential inﬂuence
of these small differences, we also conduct ﬁne-tuning on
the compact CNN model in the second stage. In the ﬁne-
tuning process, we adopt SGD with 0.9 momentum, and the
intial learning rate is 0.01. The weight decay factor is set as
0.0001. On ImageNet, we ﬁne-tune for 90 epochs with the
batch-size 256, and the learning rate is divided by 10 every
30 epochs. On CIFAR-10, we ﬁne-tune for 150 epochs with
the batch-size 128, and the learning rate is divided by 10
every 60 epochs. All experiments are implemented using
PyTorch [29].

i ⊙ |1 − vl

i ⊙ |vl

1, ul

1, zl

2, zl

1k2

2k2

5.2. Pruning Strategies and Pruning Ratios

Pruning strategies. In the experiments, we ﬁnd that many
existing ﬁlter pruning works (e.g., [21], [41]) not only prune
the output channels of the parameter tensor, but also prune
the input channels. Compared to the pruning only on the
output channels, this strategy can remove useless parame-
ters and reduce unnecessary computations, leading to higher
Param.↓% and FLOPs↓%.
In the following, we present
brief deﬁnitions of these two pruning strategies.

3981

• Single-channel pruning. As shown in Fig. 2 (top),
single-channel pruning only prunes the output chan-
nels in Wl and Wl+1.

• Pair-channel pruning. However, as shown in Fig. 2
(bottom), when a ﬁlter in Wl is pruned, the corre-
sponding response in Rl
out will be removed (see the
green dashed part of Rl
out in Fig. 2). Consequently,
the ﬁlter at the input channel of Wl+1 that corresponds
to this removed response should also be removed (see
the cube with green dashed lines in Fig. 2 (bottom)).

Remark. As most existing works change the model archi-
tecture via pruning the ﬁlters before ﬁne-tuning, they have
to adopt the pair-channel pruning strategy to match the out-
put channel of Wl and the input channel of Wl+1. Be-
sides, if there is a short-cut connection (e.g., ResNet), then
the last layer in each block cannot be pruned in many exist-
ing works. Besides, as most entries in the pruned ﬁlters are
likely to be non-zero, the ﬁne-tuning is always required to
recover the performance after pruning.

i corresponding to zero-valued vl

In contrast, as we don’t change the model architecture
when training CNN-FCF, all above limitations don’t exist
in our method. Our pruning method is much more ﬂexi-
ble than existing works. The training process of CNN-FCF
can be seen as the single-channel pruning strategy. How-
ever, given a trained CNN-FCF model, we can also adopt
the pair-channel pruning strategy to set ﬁlters’ parameters
w.r.t. the pruned input channels to 0 for each layer. Given
a trained CNN-FCF model, we can obtain a compact or
sparse CNN models depending on the original model struc-
tures: 1) If there is no short-cut, we can directly prune the
ﬁlter Wl
i to obtain a com-
pact CNN model. One further point should be pointed out
is that if there is a batch-normalization (BN) layer (e.g.,
ResNet), then the mean and variance values of BN will be
modiﬁed after pruning, leading to the change of responses.
Thus, it also requires ﬁne-tuning to resume the model per-
formance, as did in most existing works. 2) If there is a
short-cut and we also perform ﬁlter pruning (i.e., set a prun-
ing ratio using the cardinality constraint) for the last layer
in each block, then we cannot directly prune the ﬁlters as
did above. The reason is that the remained ﬁlters in two
layers connected by the short-cut cannot be aligned.
In-
stead, we obtain a sparse CNN model, by setting zeros to
the convolutional ﬁlters corresponding to zero-valued vl
i.
When using this sparse CNN model for inference, we de-
sign a squeeze-convolution-expand procedure. Speciﬁcally,
for each sparse convolutional tensor, we ﬁrstly squeeze it
by removing zero-valued ﬁlters to obtain a small dense ten-
sor, then conduct convolution using this small tensor, ﬁnally
expand the obtained feature map to the original shape by
ﬁlling zeros.
In practice, the computational costs of the
squeeze and expand steps are negligible compared to the
convolution step.

Figure 2. (Top): Single-channel pruning only prunes the output
channels of Wl and Wl+1. (Bottom): Pair-channel pruning not
only prunes the output channels of Wl+1, but also prunes its input
channels corresponding to the pruned output channels in Wl.

Model

ResNet-20

ResNet-56

Method
SNLI [40]
SFP [11]
CNN-FCF
SNLI [40]
CNN-FCF
SFP [11]

Params.↓% FLOPs↓% Ref.%1 Acc.↓%
37.22
–
42.75
67.83
68.44
–
42.71
69.46

92.00
92.20
92.20
92.00
92.20
92.63
92.43
92.43
93.04
93.04
93.59
–
93.14
93.14
93.53
93.53
93.68
–
93.58
93.58
1 Ref. denotes the accuracy of the pre-trained original model.
2 – means that the metric value is not reported in the compared method.
3 Negative value means the pruned model accuracy is higher than the Ref.

ResNet-32 CNN-FCF
CNN-FCF
Pruning-A [21] 9.40
Pruning-B [21] 13.70
SFP [11]
NISP [41]
CNN-FCF
CNN-FCF
Pruning-A [21] 2.30
Pruning-B [21] 32.40
SFP [11]
NISP [41]
CNN-FCF
CNN-FCF

–2
42.20
41.60
–
68.91
41.50
42.21
70.21
10.40
27.60
41.10
43.61
42.78
70.90
15.90
38.60
40.80
43.78
43.08
70.81

1.10
1.37
1.07
3.20
2.67
0.55
0.25
1.69
-0.06
-0.02
-0.19
0.03
-0.243
1.22
0.02
0.23
-0.18
0.18
-0.09
0.62

–
42.60
43.09
69.74

–
43.25
43.19
69.51

ResNet-110

Table 1. Comparison results on CIFAR-10.

Pruning ratios. We also notice that some existing works
(e.g., ThiNet [27] and NISP [41]) adopted the same pruning
ratio for all pruned convolutional layers, especially when
there is short-cut in the model (e.g., ResNet). However, we
believe that different layers of one CNN model have dif-
ferent contributions to the model performance. Thus, the
ratios of parameter redundancy of different layers should
be different. We will evaluate the inﬂuence of the same or
different pruning ratios in experiments (see Section 5.4).

5.3. Comparisons with State of the art Methods

In this section, to compare with state-of-the-art meth-
ods (e.g., ThiNet [27] and NISP [41]), our method which
denoted as CNN-FCF also adopts the pair-channel pruning
strategy and same pruning ratios for all layers.

Results on CIFAR-10. As shown in Table 1, we compress
ResNet-20, ResNet-32, ResNet-56 and ResNet-110 with
two pruning ratios, including 43% and 69%. On ResNet-
20, at the pruning ratio of about 43%, our method gives
the smallest accuracy loss (i.e., 1.07% Acc.↓%).
In con-

3982

𝐖𝑙𝐑𝑜𝑢𝑡𝑙𝐖𝑙+1𝐶𝑙𝑁𝑙𝐶𝑙𝐶𝑙+1𝑁𝑙+1(=𝐶𝑙)…………Model

Method

Top5↓%

5.4. Analysis of Pruning Strategies and Ratios

Pruning [21]
NISP [41]
CNN-FCF
SFP [11]
NISP [41]
CNN-FCF
CNN-FCF
CNN-FCF
SSS [15]
NISP [41]
CNN-FCF
ThiNet [27]
SFP [11]
NISP [41]
Channel pruning [12] –
CNN-FCF
CNN-FCF
CNN-FCF

10.80
27.14
27.05
–
43.68
42.19
55.80
67.24
27.06
27.12
26.70
33.72
–
43.82

Params.↓% FLOPs↓% Top1
Ref.%
73.23
–
73.30
73.92
–
73.30
73.30
73.30
76.12
–
76.15
75.30
76.15
–
–
76.15
76.15
76.15

24.20
27.32
26.83
41.10
43.76
41.38
54.87
66.05
31.08
27.31
29.24
36.79
41.80
44.01
50.00
46.05
57.10
66.17

42.41
52.52
61.01

Top1↓% Top5
Ref.%
–
–
91.42
91.62
–
91.42
91.42
91.42
92.86
–
92.87
92.20
92.87
–
92.20
92.87
92.87
92.87

1.06
0.28
-0.25
2.09
0.92
0.51
1.97
3.59
1.94
0.21
-0.35
1.27
1.54
0.89
–
0.47
1.60
2.62

–
-
-0.08
1.29
–
0.47
1.22
2.11
0.95
–
-0.26
0.09
0.81
–
1.40
0.19
0.69
1.37

ResNet-34

ResNet-50

Table 2. Comparison results on ImageNet. “Top1 Ref.%” denotes
the top1 accuracy of the original model.

trast, SNLI [40] gives the slightly higher accuracy loss (i.e.,
1.1% Acc.↓%), while the pruned ratio of parameters is less
than ours, i.e., 37.22 vs. 42.75 of Params.↓%. The pruned
ratio of parameters by SFP [11] is similar with ours, but its
accuracy loss is higher, i.e., 1.37 vs. 1.07 of Acc.↓%. At
the pruning ratio of about 69%, SNLI also performs worse
than our method, with the higher accuracy loss, i.e., 3.2
vs. 2.67 of Acc.↓%. On other ResNet models with dif-
ferent layers, our method also show very competitive per-
formance, compared to other methods. Moreover, when
comparing the compressing performance of our method on
ResNet models with different layers, an interesting obser-
vation is that the accuracy loss decreases along with the in-
creasing of the number of layers, at the same pruning ratio.
Speciﬁcally, at the pruning ratio of about 43%, the values
of Acc.↓% of our method are 1.07, 0.25, −0.24, −0.09, on
ResNet models with 20, 32, 56 and 110 layers, respectively;
at the pruning ratio of about 69%, the corresponding values
are 2.67, 1.69, 1.22, 0.62. It demonstrates that the ResNet
models with deeper layers have the larger redundancy on
the image classiﬁcation task on CIFAR-10.

Results on ImageNet. As shown in Table 2, we compress
ResNet-34 and ResNet-50 with four pruning ratios, includ-
ing about 27%, 43%, 55%, 67%. We present the losses on
both top1 and top5 accuracy. On ResNet-34, at the prun-
ing ratio of about 27%, the Top1↓% value of our method is
−0.25, while that of NISP [41] is 0.28. At the pruning ratio
of about 43%, Param.↓% and FLOPs↓% of our method are
slightly lower than NISP, but our Top1↓% is also lower than
that of NISP, i.e., 0.51 vs. 0.92. At the pruning ratios of
about 55% and 67%, the values of Top1↓% of our method
are only 1.97 and 3.59, respectively; the values of Top5↓%
of our method are only 1.22 and 2.11, respectively. On
ResNet-50, our method also shows very competitive per-
formance, compared to state-of-the-art methods. Moreover,
similar to the results on CIFAR-10, we observe that there is
also a large proportion of redundant parameters in ResNet,
even for the image classiﬁcation task on ImageNet.

i = |Wl

As demonstrated in Section 5.2, here we evaluate the in-
ﬂuence if we allocate distinct pruning ratios for different
layers. Inspired by [21, 23], we try a simple allocation of
pruning ratios as follows: (1) Calculating the score of each
ﬁlter using the ℓ1-norm, i.e., sl
i|1; (2) Storing all
ﬁlter scores sl
i to a vector s, and sorting s in an ascending
order; (3) Assume that the overall pruning ratio is p, then
the number of ﬁlters to be pruned is Np = p|s|. We set
the Np-th value of s as a global threshold, denoted as sp;
(4) Counting the number of sl
i exceeding sp in each layer,
which is recorded as kl
i (see Problem (3)). For comparison,
we also evaluate the identical ratio for all layers. After train-
ing the CNN-FCF with same or distinct pruning ratios, we
can choose single-channel or pair-channel pruning strategy
before ﬁne-tuning. Thus, there are four settings, including
identical-ratio with single-channel, identical-ratio with pair-
channel, distinct-ratio with single-channel and distinct-ratio
with pair-channel, respectively. The results are shown in Ta-
ble 3. Given the same level of Params.↓% on same dataset
and model, the accuracy loss of distinct ratios is always
much lower than that of identical ratio.
It demonstrates
that distinct ratios obtain much better compressing perfor-
mance. However, the FLOPs↓% value of distinct ratios is
always lower than that of identical ratio. The reason is that
in the setting of distinct ratios, the ratio allocation strategy
described above assigns more pruning ratios to the higher
layers (i.e., closer to the output layer), where the #FLOPs is
smaller than that of the lower layers, as the corresponding
feature maps become smaller. In the future, we will explore
other allocation strategies to achieve more reductions of
#FLOPs. In comparison between single-channel and pair-
channel pruning, given the same level of Params.↓%, the
same dataset and model, pair-channel pruning always gives
lower accuracy loss. The reason is that there are some use-
less parameters in the single-channel pruning. In summary,
the setting of training the CNN-FCF model with distinct
pruning ratio, and with the pair-channel pruning strategy
for ﬁne-tuning, obtains the best compressing performance.

5.5. Analysis of Different Compressing Stages

In this section, we analyze the inﬂuence of different
stages of a compression procedure of the proposed method,
including the joint optimization of W and v in the CNN-
FCF model, pruning the ﬁlter corresponding to the 0-valued
vl
i, and ﬁne-tuning. Besides, we also compare with two
baselines, including pruning the pre-trained CNN model
randomly (denoted as Random in Table 4 and Fig. 3), and
pruning using the ranking (i.e., ranking the ﬁlters in each
layer using the ℓ1 norm in descent order, and pruning the
lower-ranked ﬁlters with a pre-deﬁned ratio. This setting
is denoted as Ranking in Table 4 and Fig. 3). We evalu-
ate above methods for compressing the ResNet-56 model

3983

(a) single-channel pruning

Dataset

Model

Params.↓% FLOPs↓% Top1↓% Top5↓%

CIFAR-10

ImageNet

ResNet56-I
ResNet56-D
ResNet56-I
ResNet56-D
ResNet34-I
ResNet34-D
ResNet34-I
ResNet34-D

43.18
42.66
69.14
70.17
27.33
27.23
42.42
43.71

43.15
32.02
71.09
53.08
26.96
16.31
42.13
22.88

0.42
-0.14
2.33
0.75
0.59
-0.03
2.25
0.87

–
–
–
–

0.36
0.16
1.40
0.50

(b) pair-channel pruning

Dataset

Model

Params.↓% FLOPs↓% Top1↓% Top5↓%

CIFAR-10

ImageNet

ResNet56-I
ResNet56-D
ResNet56-I
ResNet56-D
ResNet34-I
ResNet34-D
ResNet34-I
ResNet34-D

43.09
41.97
69.74
69.57
27.05
27.38
42.19
43.72

42.78
33.99
70.90
55.67
26.83
22.21
41.38
28.42

-0.24
-0.16
1.22
0.64
-0.25
-0.28
0.51
0.39

–
–
–
–

-0.08
-0.08
0.47
0.28

Table 3. Results of CNN-FCF with identical and distinct prun-
ing ratios. ResNet56-I denotes pruning ResNet-56 using identical
pruning ratio for each layer, while ResNet56-D indicates distinct
ratios for different layers.

Ref.%1 Optim.%2 Pruning%3 Fine-tuning%4
93.14
93.14
93.14
93.14

Method
Random
91.14
92.09
Ranking
92.35
CNN-FCF-S
CNN-FCF-P
92.89
1 Ref.% indicates the accuracy of the original model.
2 Optim.% denotes the accuracy of the optimized CNN-FCF

10.00
10.00
91.44
30.17

–
–
91.92
92.36

model.

3 Pruning% denotes the accuracy after pruning.
4 Fine-tuning% denotes the accuracy after ﬁne-tuning.

Table 4. Compressing Performance of ResNet-56 on CIFAR-10,
with the pruning ratio of 45% parameters.

on CIFAR-10, with the overall pruning ratio of 45% param-
eters and the identical pruning ratio of all layers. The joint
optimization of CNN-FCF takes at most 150 epochs. As
shown in Table 4, all methods starts from the same check-
point of ResNet-56, of which the accuracy is 93.14%. In
terms of the Random method, after pruning 45% parame-
ters from the pre-trained model, the accuracy drops to 10%;
after ﬁne-tuning, the accuracy resumes to 91.14%. In terms
of the Ranking method, the accuracy also drops to 10% after
pruning, and it achieves 92.09% after ﬁne-tuning. It demon-
strates that the ﬁne-tuning is crucial for these two baselines,
and Ranking keeps better ﬁlters than Random. CNN-FCF-
S denotes our method with single-channel pruning. After
the joint optimization of W and v in CNN-FCF, the accu-
racy achieves 91.92%, which is very close to 93.14% of the
pre-trained CNN model. After the single-channel pruning,
there is a slight drop of accuracy, i.e., 0.48%. As analyzed
in Implementation details of Section 5.1, it is due to the nu-
merical difference between the converged v and the bina-
rized v. After ﬁne-tuning, the accuracy resumes to 92.35%.
CNN-FCF-P denotes our method with pair-channel prun-

Figure 3. The accuracy curves of ﬁne-tuning of four methods.
As the accuracies of all methods excluding CNN-FCF-S are small
in the ﬁrst 5 epochs, we ignore them to highlight the accuracy
differences in ﬁnal epochs. Please refer to Section 5.5 for details.

ing. After the joint optimization, the accuracy achieves
92.36%, which is higher than that of CNN-FCF-S. The rea-
son is that although the overall pruning ratios are same
(i.e., 45%), the ﬁlter cardinality (i.e., kl) of CNN-FCF-P
is larger than that of CNN-FCF-S, as it will prune more pa-
rameters in the pruning stage. However, after pair-channel
pruning, the accuracy signiﬁcantly drops to 30.17%. The
reason is that the pruning on the input channels of ﬁlters
signiﬁcantly changes the trained CNN-FCF model. But the
accuracy achieves 92.89% after ﬁne-tuning, which is high-
est among all compared methods. We also show the ﬁne-
tuning curves of above four methods in Fig. 3. The above
comparisons demonstrate that: (1) The proposed compress-
ing method based on CNN-FCF performs better than base-
lines; (2) The joint training based on CNN-FCF produces
a very good compressed model, even without ﬁne-tuning;
(3) When adopting the pair-channel pruning strategy, ﬁne-
tuning is useful to resume the model performance.

6. Conclusion

This work presented a novel model compression method
for deep CNNs, of which the core idea is conducting ﬁlter
learning and ﬁlter selection jointly. To this end, we deﬁned
a novel type of ﬁlters, dubbed factorized convolutional ﬁlter
(FCF), consisting of a standard convolutional ﬁlter and a bi-
nary scalar, as well as a dot-product operator between them.
To train a CNN model with FCF, we proposed to insert the
ADMM algorithm into the back-propagation framework via
gradient descent. Given a trained CNN-FCF model, a com-
pact or a sparse standard CNN model can be obtained by
only keeping the standard ﬁlters corresponding to the 1-
valued scalars. Experiments on compressing ResNet mod-
els demonstrate the superiority of the proposed method over
state-of-the-art ﬁlter pruning methods.

Acknowledgement The involvements of Yujiu Yang and
Tuanhui Li
in this work were supported in part by
the National Key Research and Development Program
of China (No.2018YFB1601102), and Shenzhen special
fund for the strategic development of emerging industries
(No.JCYJ20170412170118573).

3984

0520406080100120140epoch0.800.820.840.860.880.900.92accuracyRandomRankingCNN-FCF-SCNN-FCF-PReferences

[1] Jose M Alvarez and Mathieu Salzmann. Learning the num-
ber of neurons in deep networks. In Advances in Neural In-
formation Processing Systems, pages 2270–2278, 2016. 3

[2] Linchao Bao, Baoyuan Wu, and Wei Liu. Cnn in mrf: Video
object segmentation via inference in a cnn-based higher-
order spatio-temporal mrf. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5977–5986, 2018. 1

[3] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
Jonathan Eckstein, et al. Distributed optimization and sta-
tistical learning via the alternating direction method of mul-
tipliers. Foundations and Trends R(cid:13) in Machine learning,
3(1):1–122, 2011. 2, 4

[4] Jian Cheng, Pei-song Wang, Gang Li, Qing-hao Hu, and
Han-qing Lu. Recent advances in efﬁcient computation
of deep convolutional neural networks. Frontiers of Infor-
mation Technology & Electronic Engineering, 19(1):64–77,
2018. 1

[5] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to
prune deep neural networks via layer-wise optimal brain sur-
geon.
In Advances in Neural Information Processing Sys-
tems, pages 4857–4867, 2017. 3

[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation.
In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
580–587, 2014. 1

[7] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic net-
work surgery for efﬁcient dnns. In Advances in Neural In-
formation Processing Systems, pages 1379–1387, 2016. 2

[8] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.
In International
Conference on Learning Representations, 2016. 1, 2

[9] Babak Hassibi and David G Stork. Second order derivatives
for network pruning: Optimal brain surgeon.
In Advances
in Neural Information Processing Systems, pages 164–171,
1993. 2

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016. 1, 5

[11] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi
Yang. Soft ﬁlter pruning for accelerating deep convolutional
neural networks. International Joint Conferences on Artiﬁ-
cial Intelligence, 2018. 2, 5, 6, 7

[12] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In Proceedings
of IEEE International Conference on Computer Vision, 2017.
1, 3, 5, 7

[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017. 1

[14] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung
Tang. Network trimming: A data-driven neuron pruning ap-
proach towards efﬁcient deep architectures. arXiv preprint
arXiv:1607.03250, 2016. 3

[15] Zehao Huang and Naiyan Wang. Data-driven sparse struc-
arXiv preprint

ture selection for deep neural networks.
arXiv:1707.01213, 2017. 3, 5, 7

[16] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim
Choi, Lu Yang, and Dongjun Shin. Compression of deep
convolutional neural networks for fast and low power mobile
applications. arXiv preprint arXiv:1511.06530, 2015. 1

[17] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer, 2009. 2, 5

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 1097–1105, 2012. 1

[19] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Os-
eledets, and Victor Lempitsky. Speeding-up convolutional
neural networks using ﬁne-tuned cp-decomposition. arXiv
preprint arXiv:1412.6553, 2014. 1

[20] Yann LeCun, John S Denker, and Sara A Solla. Optimal
brain damage. In Advances in Neural Information Process-
ing Systems, pages 598–605, 1990. 2

[21] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. In In-
ternational Conference on Learning Representations, 2017.
1, 2, 5, 6, 7

[22] Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, and Ming-
Hsuan Yang. Target-aware deep tracking.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019. 1

[23] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. In Pro-
ceedings of IEEE International Conference on Computer Vi-
sion, pages 2755–2763, 2017. 1, 3, 7

[24] Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei
Liu, and Kwang-Ting Cheng. Bi-real net: Binarizing deep
network towards real-network performance. arXiv preprint
arXiv:1811.01335, 2018. 1

[25] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu,
and Kwang-Ting Cheng. Bi-real net: Enhancing the perfor-
mance of 1-bit cnns with improved representational capabil-
ity and advanced training algorithm. In Proceedings of the
European Conference on Computer Vision, pages 722–737,
2018. 1

[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3431–3440, 2015. 1

[27] Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie,
Jianxin Wu, and Weiyao Lin. Thinet: Pruning cnn ﬁlters
for a thinner net. IEEE transactions on pattern analysis and
machine intelligence, 2018. 1, 3, 5, 6, 7

3985

[42] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-
Seng Chua. Visual translation embedding network for visual
relation detection. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5532–
5540, 2017. 1

[43] Tianzhu Zhang, Bernard Ghanem, Si Liu, and Narendra
Ahuja. Robust visual tracking via multi-task sparse learn-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2042–2049, 2012. 1

[44] Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wu-
jie Wen, Makan Fardad, and Yanzhi Wang. A systematic
dnn weight pruning framework using alternating direction
method of multipliers. In Proceedings of the European Con-
ference on Computer Vision, 2018. 2

[45] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
Accelerating very deep convolutional networks for classiﬁ-
cation and detection. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 38(10):1943–1955, 2016. 1

[46] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu,
Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu.
Discrimination-aware channel pruning for deep neural net-
works. In Advances in Neural Information Processing Sys-
tems, 2018. 3

[28] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn archi-
tecture design. Proceedings of the European Conference on
Computer Vision, 2018. 1

[29] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory
Chanan. Pytorch: Tensors and dynamic neural networks in
python with strong gpu acceleration, 2017. 5

[30] Herbert Robbins and Sutton Monro. A stochastic approxi-
mation method. In Herbert Robbins Selected Papers, pages
102–109. Springer, 1985. 4

[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015. 2, 5

[32] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations, 2015.
1

[33] B. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S.
Boyd. OSQP: An operator splitting solver for quadratic pro-
grams. ArXiv e-prints, Nov. 2017. 4

[34] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo,
and Wei Liu. Learning to compose dynamic tree structures
for visual contexts. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2019. 1

[35] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. Learning structured sparsity in deep neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 2074–2082, 2016. 3

[36] Baoyuan Wu, Weidong Chen, Yanbo Fan, Yong Zhang,
Jinlong Hou, Junzhou Huang, Wei Liu, and Tong Zhang.
Tencent ml-images: A large-scale multi-label
image
database for visual representation learning. arXiv preprint
arXiv:1901.01703, 2019. 1

[37] Baoyuan Wu, Weidong Chen, Peng Sun, Wei Liu, Bernard
Ghanem, and Siwei Lyu. Tagging like humans: Diverse and
distinct image annotation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
7967–7975, 2018. 1

[38] Baoyuan Wu and Bernard Ghanem. lp-box admm: A versa-
tile framework for integer programming. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018. 2, 3, 4
[39] Baoyuan Wu, Fan Jia, Wei Liu, and Bernard Ghanem. Di-
verse image annotation.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2559–2567, 2017. 1

[40] Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethink-
ing the smaller-norm-less-informative assumption in channel
pruning of convolution layers. In International Conference
on Learning Representations, 2018. 3, 5, 6, 7

[41] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I
Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and
Larry S Davis. Nisp: Pruning networks using neuron impor-
tance score propagation. In Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition, 2018. 3, 5, 6,
7

3986

