ADCrowdNet: An Attention-Injective Deformable Convolutional Network for

Crowd Understanding

Ning Liu1

,

2 Yongchao Long1

,

2 Changqing Zou3 Qun Niu1

2

,

Li Pan4 Hefeng Wu1

5

,

,

6 ∗

1School of Data and Computer Science, Sun Yat-sen University

2Guangdong Key Laboratory of Information Security Technology

3University of Maryland

4Shanghai Jiao Tong University 5Guangdong University of Foreign Studies 6WINNER Technology

liuning2@mail.sysu.edu.cn, {longych3, niuqun}@mail2.sysu.edu.cn,

cqzou@umiacs.umd.edu, panli@sjtu.edu.cn, wuhefeng@gmail.com

Abstract

We propose an attention-injective deformable convolu-
tional network called ADCrowdNet for crowd understand-
ing that can address the accuracy degradation problem of
highly congested noisy scenes. ADCrowdNet contains two
concatenated networks. An attention-aware network called
Attention Map Generator (AMG) ﬁrst detects crowd regions
in images and computes the congestion degree of these re-
gions. Based on detected crowd regions and congestion
priors, a multi-scale deformable network called Density
Map Estimator (DME) then generates high-quality density
maps. With the attention-aware training scheme and multi-
scale deformable convolutional scheme, the proposed AD-
CrowdNet achieves the capability of being more effective
to capture the crowd features and more resistant to vari-
ous noises. We have evaluated our method on four popu-
lar crowd counting datasets (ShanghaiTech, UCF CC 50,
WorldEXPO’10, and UCSD) and an extra vehicle counting
dataset TRANCOS, and our approach beats existing state-
of-the-art approaches on all of these datasets.

1. Introduction

Crowd understanding has attracted much attention re-
cently because of its wide range of applications like public
safety, congestion avoidance, and ﬂow analysis. The cur-
rent research trend for crowd understanding has developed
from counting the number of people to displaying distribu-
tion of crowd through density map. Generally, generating
accurate crowd density maps and performing precise crowd
counting for highly congested noisy scenes is challenging

∗ The corresponding author is Hefeng Wu. This research is supported
by the National Natural Science Foundation of China (Grant No. 91746204
and 61876045), and Opening Project of Guangdong Province Key Labora-
tory of Information Security Technology (Grant No. 2017B030314131).

due to the complexity of crowd scenes caused by various
factors including background noises, occlusions, and diver-
siﬁed crowd distributions.

Researchers recently have leveraged deep neural net-
works (DNN) for accurate crowd density map generation
and precise crowd counting. Although these DNNs-based
methods [37, 25, 29, 18, 2, 17] have made signiﬁcant suc-
cess in solving the above issues, they still have the problem
of accuracy degradation when applied in highly congested
noisy scenes. As shown in Figure 1, the state-of-the-art ap-
proach [17], which has achieved much lower Mean Abso-
lute Error (MAE) than the previous state-of-the-art meth-
ods, is still severely affected by background noises, occlu-
sions, and non-uniform crowd distributions.

In this paper, we aim at an approach which is capa-
ble of dealing with highly congested noisy scenes for the
crowd understanding problem. To achieve this, we designed
an attention-injective deformable convolutional neural net-
work called ADCrowdNet which is empowered by a visual
attention mechanism and a multi-scale deformable convolu-
tion scheme. The visual attention mechanism is delicately
designed for alleviating the effects from various noises in
the input. The multi-scale deformable convolution scheme
is specially introduced for the congested environments. The
basic principle of visual attention mechanism is to use the
pertinent information rather than all available information
in the input image to compute the neural response. This
principle of focusing on speciﬁc parts of the input has been
successfully applied in various deep learning models for im-
ages classiﬁcation [11], semantic segmentation [24], image
deblurring [22], and visual pose estimation [6], which also
suits our problem where the interest regions containing the
crowd need to be recognized and highlighted out from noisy
scenes. The multi-scale deformable convolution scheme
takes as input the information of the dynamic sampling lo-
cations, other than evenly distributed locations, which has

3225

Figure 1. From left to right: a congested sample (top) and a noisy sample (bottom) from ShanghaiTech dataset [37], ground truth density
map, and the generated density maps from the proposed ADCrowdNet and the state-of-the-art method [17]. ADCrowdNet outperforms the
state-of-the-art method on both congested and noisy scenes.

the capability of modeling complex geometric transforma-
tion and diverse crowd distribution. This scheme ﬁts well
the nature of the distortion caused by the perspective view
of the camera and diverse crowd distributions in real world,
therefore guaranteeing more accurate crowd density maps
for the congested scenes.

To incorporate the visual attention mechanism and de-
formable convolution scheme, we leverage an architecture
consisting of two neural networks as shown in Figure 2. Our
training contains two stages. The ﬁrst stage generates an at-
tention map for a target image via a network called Atten-
tion Map Generator (AMG). The second stage takes the out-
put of AMG as input and generates the crowd density map
via a network called Density Map Estimator (DME). The
attention map generator AMG mainly provides two types
of priors for the DME network: 1) candidate crowd regions
and 2) the congestion degree of crowd regions. The for-
mer prior enables the multi-scale deformable convolution
scheme empowered DME network to pay more attention to
those regions having people crowds, and thus improving the
capacity of being resistant to various noises. The latter prior
indicates each crowd region with congestion degree (i.e.,
how crowded each crowd region is), which provides ﬁne-
grained congestion context prior for the subsequent DME
network and boosts the performance of the DME network
on the scenes containing diverse crowd distribution.

The main contributions of this paper are summarized as
follows. First, a novel attention-injective deformable convo-
lutional network framework ADCrowdNet is proposed for
crowd understanding. Second, our AMG model that attends
the crowd regions in images, is innovatively formulated as a
binary classiﬁcation network by introducing third party neg-
ative data (i.e., background images with no crowds). Third,
our DME model can estimate the crowds effectively by us-
ing the proposed structure of aggregating multi-scale de-
formable convolution representations. Furthermore, exten-
sive experiments conducted on all popular datasets demon-

strate the superior performance of our approach over exist-
ing leading ones.
In particular, the proposed model AD-
CrowdNet outperforms the state-of-the-art crowd counting
solution CSRNet [17] with 3.0%, 18.8%, 3.0%, 13.9%
and 5.1% lower Mean Absolute Error (MAE) on Shang-
haiTech Part A, Part B, UCF CC 50, WorldExpo10, UCSD
datasets, respectively. Apart from crowd counting, AD-
CrowdNet is also general for other counting tasks. We
have evaluated ADCrowdNet on a popular vehicle counting
dataset named TRANCOS [10], and ADCrowdNet achieves
32.8% lower MAE than CSRNet.

2. Related Work

Counting by detection: Early approaches of crowd
understanding mostly focus on the number of people in
crowds [9]. The major characteristics of these approaches
are the sliding window based detection scheme and hand
crafted features extracted from the whole human body or
particular body parts with low-level descriptors like Haar
wavelets [31] and HOG [8]. Generally, approaches in these
groups deliver accurate counts when their underlying as-
sumptions are met but are not applicable in more challeng-
ing congested scenes.

Counting by regression: Counting by regression ap-
proaches differs depending on the target of regression: ob-
ject count [5, 4], or object density [15]. This group of ap-
proaches avoid solving the hard detection problem. Instead,
they deploy regression model to learn the mapping between
image characteristics (mainly histograms of lower level or
middle level features) and object count or density. These
approaches that directly regress the total object count dis-
card the information of the location of the objects and only
use 1-dimensional object count for learning. As a result, a
large number of training images with the supplied counts
are needed in training. Lempitsky et al. [15] propose a
method to solve counting problem by modeling the crowd

3226

Figure 2. Architecture overview of ADCrowdNet. The well trained AMG generates the attention map of the input image. The pixel-wise
product of the input image and its attention map is taken as the input to train the DME network.

density at each pixel and cast the problem as that of estimat-
ing an image density whose integral over any image region
gives the count of objects within that region. Since the ideal
linear mapping is hard to obtain, Pham et al. [21] use ran-
dom forest regression to learn a non-linear mapping instead
of the linear one.

Crowd understanding by CNN: Inspired by the great
success in visual classiﬁcation and recognition, literature
also focuses on the CNN-based approaches to predict crowd
density map and count the number of crowds [32, 20, 16,
13, 23]. Walach et al. [32] use CNN with a layered training
structure. Shang et al. [26] adapt an end-to-end CNN which
uses the entire images as input to learn the local and global
count of the images and ultimately outputs the crowd count.
A dual-column network combining shallow and deep layers
is used in [1] to generate density maps. In [37], a multi-
column CNN is proposed to estimate density map by exact-
ing features at different scales. Similar idea is used in [20].
Marsden et al. [19] try a single-column fully convolutional
network to generate density map while Sindagi et al. [28]
present a CNN that uses high-level prior to boost accuracy.
More recently, Sindagi et al. [29] propose a multi-
column CNN called CP-CNN that uses context at various
levels to improve generate high-quality density maps. Li et
al. [17] propose a model called CSRNet that uses dilated
convolution to enlarge receptive ﬁelds and extract deeper
features for boosting performance. These two approaches
have achieved the state-of-the-art performances.

3. Attention-Injective Deformable Convolu-

tional Network

The architecture of the proposed ADCrowdNet method
is illustrated in Figure 2. It employs two concatenated net-
works: AMG and DME. AMG is a classiﬁcation network
based on fully convolutional architecture for attention map
generation, while DME is a multi-scale network based on
deformable convolutional layers for density map genera-
tion. Before training DME, we train the AMG module
with crowd images (positive training examples) and back-
ground images (negative training examples). We then use
the well-trained AMG to generate the attention map of the
input image. Afterward, we train the DME module using
the pixel-wise product of input images and the correspond-
ing attention maps. In the following sections, we will detail

Figure 3. Attention maps generated by AMG at various crowd den-
sity levels (density level increases from left to right).

the architectures of the AMG and DME netwroks.

3.1. Attention Map Generator

3.1.1 Attention map

Attention map is an image-sized weight map where crowd
regions have higher values. In our work, attention map is
a feature map from a two-category classiﬁcation network
AMG which classiﬁes an input image into crowd image or
background image. The idea of using feature map to ﬁnd
the crowd regions in the input is motivated by an object lo-
calization work [38] which points out that the feature maps
of classiﬁcation network contain the location information of
target objects.

The pipeline of the attention map generation is shown
in Figure 4. Fc and Fb are the feature maps from the last
convolution layer of AMG. Wc and Wb are the spatial aver-
age of the Fc and Fb after global average pooling (i.e., GAP
in Figure 4). Pc and Pb are conﬁdence scores of the pre-
dicted two class. They are generated by softmax from Wc
and Wb. The attention map is obtained by up-sampling the
linear weighted fusion of the two feature maps Fc and Fb
(i.e., Fc · Pc + Fb · Pb) to the same size as the input image.
We also normalize the attention map such that all element
values fall in the range [0, 1].

The attention map highlights the regions of crowds. In
addition, it also indicates the degree of congestion in indi-
vidual regions, i.e., higher congestion degree values indi-
cate more congested crowds and lower values indicate less
congested ones. Figure 3 illustrates the effect of attention
maps at different density levels. The pixel-wise product be-
tween the attention map and the input image produces the
input data used by the DME network.

3227

Input image Attention map Pixel-wise product Density map ⊙ front end AMG back end front end DME back end Figure 4. Architecture of AMG. All convolutional layers use
padding to maintain the previous size. The convolutional lay-
ers’ parameters are denoted as “Conv-(kernel size)-(number of
ﬁlters)-(dilation rate)”, max-pooling layers are conducted over a
2×2 pixel window, with stride 2.

3.1.2 Architecture of attention map generator

The architecture of AMG is shown in Figure 4, we use the
ﬁrst 10 layers of trained VGG-16 model [27] as the front
end to extract low-level features. We build the back end
by adopting multiple dilated convolution layers of different
dilation rates with an architecture similar to the inception
module in [30]. The multiple dilated convolution architec-
ture is motivated from [34]. It has the capability of localiz-
ing people clusters with enlarged receptive ﬁelds. The in-
ception module was originally proposed in [30] to process
and aggregate visual information of various scales. We use
this module to deal with the diversiﬁed crowd distribution
in congested scenes.

3.2. Density Map Estimator

The DME network consists of two components: the front
end and the back end. We remove the fully-connected lay-
ers of VGG-16 [27] and leave 10 convolutional layers to as
the front end of the DME. The back end is a multi-scale
deformable convolution based CNN network [7]. The ar-
chitecture of DME is shown in Figure 5. The front end
uses the ﬁrst 10 layers of trained VGG-16 model [27] to
extract low-level features. The back end uses multi-scale
deformable convolutional layers with a structure similar to
the inception module in [30], which enables DME to cope
with various occlusion, diversiﬁed crowd distribution, and
the distortion caused by perspective view.

The deformable convolution scheme was originally pro-
posed in [7]. Beneﬁcial from the adaptive (deformable)
sampling location selection scheme, deformable convolu-
tion has shown its effectiveness on various tasks, such as
object detection, in the wild environment. The deformable
convolution treats the offsets of sampling locations as learn-
ing parameters. Rather than uniform sampling, the sam-

Figure 5. Architecture of DME. The convolutional layers’ pa-
rameters are denoted as “Conv-(kernel size)-(number of ﬁlters)-
(stride)”, max-pooling layers are conducted over a 2×2 pixel win-
dow, with stride 2. The deformable convolutional layers’ pa-
rameters are denoted as “Dconv-(kernel size)-(number of ﬁlters)-
(stride)”.

pling locations in the deformable convolution can be ad-
justed and optimized via training (see Figure 6 for the de-
formed sampling points by the deformable convolution on
an example form ShanghaiTech Part B dataset [37]). Com-
pared to the uniform sampling scheme, this kind of dynamic
sampling scheme is more suitable for the crowd understand-
ing problem of congested noisy scenes. We will show the
comparative advantages of the deformable convolution in
our experimental section.

Figure 6. Illustration of the deformed sampling locations. Left:
standard convolution; right: deformable convolution; top: activa-
tion units on the feature map; bottom: the sampling locations of
the 3 × 3 ﬁlter.

4. Experiments

4.1. Datasets and Settings

We evaluate ADCrowdNet on four challenging datasets
for crowd counting:
the
UCF CC 50 dataset [12], the WorldExpo’10 dataset [35],
and the UCSD dataset [3].

ShanghaiTech dataset

[37],

ShanghaiTech dataset [37]. The ShanghaiTech dataset
contains 1,198 images with a total of 330,165 people. It is
divided into two parts: Part A and Part B. Part A contains

3228

front end back end GAP Wc    Wb ⊕ ⊙ ⊙ Fb Fc Softmax Pc    Pb Attention map Conv-3-64-1 Conv-3-64-1 Max-Pooling Conv-3-128-1 Conv-3-128-1 Max-Pooling Conv-3-256-1 Conv-3-256-1 Conv-3-256-1 Max-Pooling Conv-3-512-1 Conv-3-512-1 Conv-3-512-1 front end (fine-tuned from VGG-16) back end Conv-1-2-1 Global Average Pooling Filter Concatenation Filter Concatenation Filter Concatenation Conv- 3-256-1 Conv-3-256-9 Conv-3-256-6 Conv-3-256-3 Conv-1-256-1 Conv-3-64-1 Conv-3-64-9 Conv-3-64-6 Conv-3-64-3 Conv-1-128-1 Conv-3-128-1 Conv- 3-128-9 Conv-3-128-6 Conv-3-128-3 front end back end Density Map Conv-3-64-1 Conv-3-64-1 Max-Pooling Conv-3-128-1 Conv-3-128-1 Max-Pooling Conv-3-256-1 Conv-3-256-1 Conv-3-256-1 Max-Pooling Conv-3-512-1 Conv-3-512-1 Conv-3-512-1 front end (fine-tuned from VGG-16) Dconv-3-128-1 Dconv-7-128-1 Dconv-5-128-1 Conv-1-1-1 Filter Concatenation Filter Concatenation Filter Concatenation back end Dconv-3-64-1 Dconv-7-64-1 Dconv-5-64-1 Conv-1-128-1 Conv-1-256-1 Dconv-3-256-1 Dconv-7-256-1 Dconv-5-256-1 482 pictures of congested scenes, in which 300 are used as
training dataset and 182 are used as testing dataset; Part B
contains 716 images of sparse scene, 400 of which are used
as training dataset and 316 are used as testing dataset.

UCF CC 50 dataset [12]. This dataset contains 50 im-
ages downloaded from the Internet. The number of persons
per image ranges from 94 to 4543 with an average of 1280
individuals. It is a very challenging dataset with two prob-
lems: the limited number of the images and the large span
in person count between images. We used 5-fold-cross-
validation setting described in [12].

WorldExpo’10 dataset [35].

It contains 3980 from 5
different scenes. Among 3980 images, 3380 images are
used as training dataset and the remaining 600 images are
used as testing dataset. Region-of-Interest (ROI) regions are
provided in this dataset.

UCSD dataset [3]. The UCSD dataset contains 2000
images in sparse scene. The dataset also provides ROI re-
gion information. We created the ground truth in the same
way as we did for the WorldExpo’10 dataset. Since the size
of each image is too small to support the generation of high-
quality density maps, we therefore enlarge each image to
952×632 size by bilinear interpolation. Among the 2000
images, 800 images were used as training dataset, and the
rest were used as testing dataset. Region-of-Interest (ROI)
regions are also provided in this dataset.

We show a representative example for each crowd count-
ing dataset in Figure 7. These four crowd counting datasets
have their own characteristics.
In general, the scenes in
ShanghaiTech Part A dataset are congested and noisy. Ex-
amples in ShanghaiTech Part B are noisy but not highly
congested. The UCF CC 50 dataset consists of extremely
congested scenes which have hardly any background noises.
Both WorldExpo’10 dataset and UCSD dataset provide ex-
ample with sparse crowd scenes in the form of ROI regions.
Scenes in the ROI regions of the WorldExpo’10 dataset
are generally noisier than the only one scene in the UCSD
dataset.

Following [29, 17], we use the mean absolute error
(MAE) and the mean square error (MSE) for quantitative
evaluation of the estimated density maps. PSNR (Peak
Signal-to-Noise Ratio) and SSIM [33] are used to measure
the quality of the generated density map. For fair compar-
ison, we follow the measurement procedure in [17] and re-
size the density map and ground truth to the size of the orig-
inal input image by linear interpolation.

4.2. Training

4.2.1 AMG Training

Training data for the binary classiﬁcation network AMG
consists of two groups of samples: positive and negative
samples. The positive samples are from the training sets
of the four crowd counting datasets. The negative samples

are 650 background images downloaded from the Internet.
These negative samples are shared by the training of each
individual dataset. These 650 negative samples contain var-
ious outdoor scenes where people appear, such as streets,
squares, etc., ensuring that the biggest difference between
positive sample and negative samples is whether the image
contains people. Adam [14] is selected as the optimization
method with the learning rate at 1e-5 and Standard cross-
entropy loss is used as the loss function.

4.2.2 DME training

We simply crop 9 patches from each image where each
patch is 1/4 of the original image size. The ﬁrst four patches
contain four quarters of the image without overlapping. The
other ﬁve patches are randomly cropped from the image.
After that, we mirror the patches so that we double the train-
ing dataset. We generate the ground truth for DME training
following the procedure in [17]. We select Adam [14] as the
optimization method with the learning rate at 1e-5. As pre-
vious works [37, 25, 17], we use the euclidean distance to
measure the difference between the generating density map
and ground truth and deﬁne the loss function as

L(Θ) =

1
2N

N

X

i=1

||F (Xi; Θ) − Fi||2

2

(2),

where N is the batch size, F (Xi; Θ) is the estimated den-
sity map generated by DME with the parameterΘ, Xi is the
input image, and Fi is the ground truth of Xi.

4.3. Results and Analyses

In this section, we ﬁrst study several alternative network
design of ADCrowdNet. After that, we evaluate the overall
performance of ADCrowdNet and compare it with previous
state-of-the-art methods.

4.3.1 Alternative study

DME

AMG-DME

AMG-bAttn-DME AMG-attn-DME

Dataset

ShanghaiTech Part A [37]
ShanghaiTech Part B [37]

UCF CC 50 [12]

The WorldExpo’10 [35]

The UCSD [3]

107.5
16.9
363.5

MAE MSE MAE MSE MAE
63.2
68.5
8.2
9.3
257.1
266.4
8.5
7.7
0.98
1.39

66.1
7.6
257.9
7.4
1.10

102.1
13.9
357.7

1.42

1.25

-

-

MSE
98.9
15.7
358.0

-

1.68

MAE
70.9
7.7
273.6
7.3
1.09

MSE
115.2
12.9
362.0

-

1.35

Table 1. Results of different variants of ADCrowdNet on four
crowd counting datasets.

DME. Our ﬁrst study is to investigate the inﬂuence of
the AMG network, we compared two network designs on
all the four datasets. The ﬁrst one named AMG-DME has
the architecture shown in Figure 2. The other one named
DME uses the only DME network. Our quantitative exper-
imental results in Table 1 show that AMG-DME is signif-
icantly superior than DME on the those datasets which are

3229

Figure 7. Representative examples from four crowd counting datasets.

Figure 8. DME vs. AMG-DME. From left to right: representative
samples from the ShanghaiTech Part A dataset, ground truth den-
sity map, density map generated from the architectures of single
DME and AMG-DME.

characteristic of noisy scenes: ShanghaiTech Part A, Part B
and WorldExpo’10. In Figure 8, we illustrate two represen-
tative samples from the testing set of ShanghaiTech Part A.
On the top example which contains a congested noisy scene,
estimated people number of AMG-DME is 198 that is much
closer to the groud truth 171 than that estimated by DME.
From the density map in the 3rd column of Figure 8, we
can see the trees in the distance have been recognized as
people by the single DME model. However, AMG-DME
does not suffer this problem due to the help from the AMG
network. On the middle-row example containing a noisy
and more congested scene, the performances of AMG-DME
and DME agree with those on the top example. The com-
parison results indicate that AMG-DME is more effective
than DME on those noisy examples.

On the UCF CC 50 dataset, AMG-DME has approxi-
mate performance (slightly higher AME but lower MSE)
with DME. It may due to the fact most of examples in
the UCF CC 50 dataset have a large regions of congested
crowds while rarely have background noises. On the UCSD
dataset where scenes are neither congested nor noisy, both
MSE and MAE of AMG-DME is slightly higher than DME.
This might because the examples in the UCSD dataset have
already provide the accurate information of ROI regions.
The attention map generated by the AMG network may de-
stroy the ROI regions, which degrades the performance of
the DME network since some ROI regions may be erased
from its input.

AMG-bAttn-DME. Since the AMG network has shown
its strength in coping with noise background of scenes,
our second study is to explore if a hard binary attention
mask is more effective than the soft attention employed by
AMG-DME. We therefore set up an variant of AMG-DME

called AMG-bAttn-DME in Table 1. AMG-bAttn-DME has
the same architecture as AMG-DME while differing with
AMG-DME on the attention map (i.e., the attention maps
of AMG-bAttn-DME contain either 0 or 1, other than a
ﬂoating point within [0, 1] in the attention maps of AMG-
DME). We ﬁrst conducted the experiments on the Shang-
haiTech dataset to ﬁnd out the optimal binarization thresh-
old for AMG-bAttn-DME. We set three different threshold
attention values,{0.2, 0.1, 0.0}, for the binarization of atten-
tion maps. The ROI regions are gradually enlarged with the
decreasing the threshold values as shown in Figure 9. The
results shown in Table 2 indicates AMG-bAttn-DME with
attention threshold of 0.1 achieved the best performance.
We then evaluated AMG-bAttn-DME with this optimal at-
tention threshold on the rest three datasets and reported the
results in Table 1.
It is observed that AMG-bAttn-DME
is superior than AMG-DME only on ShanghaiTech Part A
while AMG-DME outperforms AMG-bAttn-DME on all
other datasets. It may be due to the AMG network can learn
more accurate attention maps on ShanghaiTech Part A and
the binarization process does not destroy too much informa-
tion of the crowd regions.

Figure 9. Illustration of the ROI regions extracted by different at-
tention thresholds. The pixels which have the value of attention
lower than t are changed to black.

Part A

Part B

Threshold MAE MSE MAE MSE
17.8
15.7
15.0

t = 0.2
t = 0.1
t = 0.0

104.1
98.9
100.6

68.0
63.2
63.2

9.2
8.2
8.6

Table 2. Performance of AMG-bAttn-DME under different bina-
rization thresholds on the ShanghaiTech dataset.

3230

ShanghaiTech Part_A ShanghaiTech Part_B UCF_CC_50 WoldExpo'10 UCSD Figure 10. Architecture of AMG-attn-DME in both training and
testing phases.

AMG-attn-DME. Complement to the above experi-
ments, we stretched the design choice exploration to study-
ing an alternative way of injecting the learned attention from
the AMG network to the DME network. In our proposed
architecture, the DME network directly takes the crowd im-
ages as input. An alternative architecture is to weigh in-
termediate the feature map of a certain layer of the DME
network with the attention map from the AMG network.
In our implementation, we inject the attention map into
the output of the front end of the DME network as shown
in Figure 10. Following the same training procedures as
those in Table 1, this alternative architecture, named AMG-
attn-DME, performs slightly worse than AMG-DME on
the datasets with congested noisy scenes like ShanghaiTech
Part A and ShanghaiTech Part B. This may be due to some
non-crowd pixels in the attention map from the AMG net-
work having an attention value of zero, which, during the
injection, would make convolution features at those cor-
responding locations vanish, reducing the feature informa-
tion learned by previous convolutional lays from the input.
On the UCF CC 50 dataset and UCSD dataset, AMG-attn-
DME is worse than the the only DME network as AMG-
bAttn-DME and AMG-DME. This is because the scenes
of these two datasets have less noisy background, AMG-
attn-DME may reduce the information of the ROI regions
through the injected attention map. On the UCSD and
WorldExpo’10 datasets, AMG-attn-DME achieved higher
effectiveness. Maybe it is because the convolution feature
vanishing problem has been alleviated by the black regions
around the ROI regions in the input.

4.3.2 Quantitative results

In this section, we study the overall performance of AD-
CrowdNet and compare it with existing methods on each
individual crowd counting dataset.

Comparison on MAE and MSE. We ﬁrst compare
the variants of the proposed ADCrowdNet network with
the state-of-the-art work CSRNet [17] along with several
previous methods including CP-CNN [29], MCNN [37],
Cascaded-MTL [28], Switching-CNN [25] on the Shang-
haiTech dataset and the UCF CC 50 dataset. These two
datasets are characteristic of congested and/or noisy scenes.

The comparison results were summarized in Table 3.
On the ShanghaiTech dataset, two of our approach vari-
ants ADCrowdNet(AMG-DME) and ADCrowdNet(AMG-
bAttn-DME) achieved better performances than existing ap-
proaches. The only DME network achieved the perfor-
mance generally close to the state-of-the-art approach CSR-
Net [17].

Part A

Part B

UCF CC 50

Method

MCNN [37]

Cascaded-MTL [28]
Switching-CNN [25]

CP-CNN [29]
CSRNet [17]

ADCrowdNet(DME)

ADCrowdNet(AMG-DME)

ADCrowdNet(AMG-bAttn-DME)
ADCrowdNet(AMG-attn-DME)

MAE MSE MAE MSE MAE MSE
509.1
110.2
397.9
101.3
439.2
90.4
320.9
73.6
397.5
68.2
68.5
363.5
357.7
66.1
63.2
358.0
70.9
362.0

173.2
152.4
135.0
106.4
115.0
107.5
102.1
98.9
115.2

377.6
322.8
318.1
295.8
266.1
257.1
257.9
266.4
273.6

26.4
20.0
21.6
20.1
10.6
9.3
7.6
8.2
7.7

41.3
31.1
33.4
30.1
16.0
16.9
13.9
15.7
12.9

Table 3. Estimation errors on ShanghaiTech and UCF CC 50.

The WorldEXpo’10

UCSD

Method

MCNN [37]

Switching-CNN [25]

CSRNet [17]

ADCrowdNet(DME)

ADCrowdNet(AMG-DME)

ADCrowdNet(AMG-bAttn-DME)
ADCrowdNet(AMG-attn-DME)

Sce.1
3.4
4.4
2.9
1.6
1.6
1.7
1.6

Sce.2
20.6
15.7
11.5
15.8
13.8
14.4
13.2

Sce.3
12.9
10.0
8.6
11.0
10.7
11.5
8.7

Sce.4
13.0
11.0
16.6
10.9
8.0
7.9
10.6

Sce.5 Ave. MAE MSE
1.35
8.1
2.10
5.9
3.4
1.47
1.25
3.2
1.42
3.2
1.68
3.0
2.6
1.35

1.07
1.62
1.16
0.98
1.10
1.39
1.09

11.6
9.4
8.6
8.5
7.4
7.7
7.3

Table 4. Estimation error comparison on the WorldExpo’10 and
UCSD. Note that only MAE is provided on WorldExpo’10 as pre-
vious approaches.

On the two relatively less challenging datasets World-
Expo’10 and UCSD, we compared ADCrowdNet with re-
cent state-of-the-art recent approaches including Switching-
CNN [25], MCNN [37], and CSRNet [17]. The comparison
results are shown in Table 4. Our method achieved the best
accuracy in scenes 1, 4, 5 as well as the best average accu-
racy on the WorldExpo’10 dataset . On the UCSD dataset,
our DME model achieved the best accuracy on terms of both
MAE and MSE.

Comparison on PSNR and SSIM. To study the qual-
ity of the density maps generated by ADCrowdNet, an-
other experiment was conducted on all the ﬁve datasets for
both ADCrowdNet and the state-of-the-art method CSR-
Net [17]. The comparison results are shown in Table 5. Our
method outperforms CSRNet on all the ﬁve datasets. On
UCF CC 50 dataset, our method improves 7.03% on PSNR
and 55.76% on SSIM. On USCD dataset, our method im-
proves 31.81% on PSNR and 8.13% on SSIM.

Evaluation on vehicle counting dataset. We conducted
experiments on the TRANCOS [10] dataset for vehicle
counting to evaluate the generalization capability of the pro-
posed approach. The positive samples for training are from
the training set of TRANCOS [10]. The negative samples
use 250 background images downloaded from the Internet,
including various road scenes without vehicle. As previous

3231

front end  AMG Density map ⊙ back end Input image Attention map DME Dataset

ShanghaiTech Part A [37]
ShanghaiTech Part B [37]

UCF CC 50 [12]

The WorldExpo’10 [35]

CSRNet [17]

ADCrowdNet

PSNR SSIM PSNR SSIM
0.88
23.79
0.97
27.02
0.81
18.76
0.95
26.94
0.93
20.02
0.97
27.10

24.48
29.35
20.08
29.12
26.39
29.56

0.76
0.89
0.52
0.92
0.86
0.93

The UCSD [3]
TRANCOS [10]
Table 5. CSRNet vs. ADCrowdNet (AMG-DME).

work CSRNet [17], we use the Grid Average Mean Abso-
lute Error (GAME) to measure the counting accuracy. The
comparison results are shown in Table 6. It clearly shows
that the ADCrowdNet approach achieved the best perfor-
mance at all levels of GAMEs.

Method

Hydra-3s[20]
FCN-HA [36]
CSRNet [17]

ADCrowdNet(DME)

ADCrowdNet(AMG-DME)

ADCrowdNet(AMG-bAttn-DME)
ADCrowdNet(AMG-attn-DME)

GAME0 GAME1 GAME2 GAME3

10.99
4.21
3.56
2.65
2.39
2.69
2.44

13.75

16.69

19.32

-

5.49
4.49
4.23
4.61
4.14

-

8.75
7.09
6.89
7.13
6.78

-

15.04
14.29
14.82
14.14
13.58

Table 6. Evaluation on TRANCOS.

4.3.3 Qualitative results

In this section, we further investigate the general perfor-
mance of the proposed ADCrowdNet by qualitative results.
We mainly compared ADCrowdNet with the state-of-the-art
approach CSRNet [17] which have demonstrated the best
performance on the datasets including the ShanghaiTech,
UCF CC 50, the WorlExpo’10, and UCSD datasets.
In
general, CSRNet has a front-end and back-end architecture
as the DME network of the proposed ADCrowdNet. It is
empowered by a dilated convolution design in the back-end
of its architecture. Apart from the additional AMG netwok,
ADCrowdNet differs from CSRNet by two additional fea-
tures in its DME network: 1) the multiple-scale convolution
scheme different from the single scale scheme of CSRNet,
and 2) the deformable sampling scheme different from the
evenly ﬁxed-offset sampling in the dilated convolution of
CSRNet.

Figure 11 shows some qualitative comparisons be-
tween the proposed ADCrowdNet (the variant AMG-DME
is used) and the state-of-the-art approach CSRNet [17].
Through visualization, it is observed that CSRNet is much
less effective on those examples with various noises than
ADCrowdNet. We can see the evidence from the noise re-
gions marked by red boxes of the 1st column where noises
exist in the background regions, as well as the marked re-
gions of the 3rd column where noises can be found in the
crowd regions. This may be due to CSRNet directly takes
the crowd image as input while the DME network of AD-

CrowdNet takes as the input the crowd information high-
lighted by its AMG network. On the example of the 2nd
column where there is not much noise but a signiﬁcantly
non-uniform crowd distribution, ADCrowdNet also clearly
outperforms CSRNet. This indicates that the multi-scale de-
formable convolution scheme in ADCrowdNet is more ef-
fective than the single-scale ﬁxed-offset dilated convolution
scheme in CSRNet.

On the rightmost example of Figure 11 which have
highly occluded crowd regions (see the regions within the
two green dotted bordered rectangle), ADCrowdNet only
recognized part of the severely occluded crowd regions.
It may because the AMG network of ADCrowdNet can-
not highlight out the whole occluded crowd regions for the
DME network. Nevertheless, ADCrowdNet still achieved
better performance in terms of all the measurement param-
eters: estimated number, PSNR and SSIM.

Figure 11. From top to bottom: representative samples from the
testing set of the ShanghaiTech dataset, ground truth density maps,
estimated density maps generated by the state-of-the-art approach
CSRNet [17] and ADCrowdNet (AMG-DME) respectively.

5. Conclusion

We propose a convolutional neural network based ar-
chitecture named ADCrowdNet for crowd understanding
of congested noisy scenes. Beneﬁting from the multi-
scale deformable convolutional layers and attention-aware
training scheme, ADCrowdNet generally achieved more
accurate crowd counting and density map estimation than
existing methods by suppressing the problems caused by
noises, occlusions, and diversiﬁed crowd distributions com-
monly presented in highly congested noisy environments.
On four popular crowd counting datasets (ShanghaiTech,
UCF CC 50, WorldEXPO’10, UCSD) and an extra vehi-
cle counting dataset TRANCOS, ADCrowdNet achieved
signiﬁcant improvements over recent state-of-the-art ap-
proaches.

3232

References

[1] Lokesh Boominathan, Srinivas S.S. Kruthiventi,

and
R. Venkatesh Babu. Crowdnet: A deep convolutional net-
work for dense crowd counting. In Proc. ACM MM, pages
640–644, 2016. 3

[2] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale
aggregation network for accurate and efﬁcient crowd count-
ing. In Proc. Springer ECCV, pages 757–773, 2018. 1

[3] Antoni B Chan, Zhang-Sheng John Liang, and Nuno Vas-
concelos. Privacy preserving crowd monitoring: Counting
people without people models or tracking.
In Proc. IEEE
CVPR, pages 1–7, 2008. 4, 5, 8

[4] Ke Chen, Shaogang Gong, Tao Xiang, and Chen Change
Loy. Cumulative attribute space for age and crowd density
estimation. In Proc. IEEE CVPR, pages 2467–2474, 2013. 2
[5] Ke Chen, Chen Change Loy, Shaogang Gong, and Tony Xi-
ang. Feature mining for localised crowd counting. In Proc.
BMVC, pages 1–11, 2012. 2

[6] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L
Yuille, and Xiaogang Wang. Multi-context attention for hu-
man pose estimation.
In Proc. IEEE CVPR, pages 1831–
1840, 2018. 1

[7] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In Proc. IEEE ICCV, pages 764–773, 2017. 4

[8] Navneet Dalal and Bill Triggs. Histograms of oriented gradi-
ents for human detection. In Proc. IEEE CVPR, pages 886–
893, 2005. 2

[9] Piotr Doll´ar, Christian Wojek, Bernt Schiele, and Pietro Per-
ona. Pedestrian detection: An evaluation of the state of the
art. IEEE Transactions on Pattern Analysis and Machine In-
telligence, 34(4):743–761, 2012. 2

[10] Ricardo Guerrero-G´omez-Olmedo, Beatriz Torre-Jim´enez,
Roberto L´opez-Sastre, Saturnino Maldonado-Basc´on, and
Daniel Onoro-Rubio. Extremely overlapping vehicle count-
ing. In Proc. Springer IbPRIA, pages 423–431, 2015. 2, 7,
8

[11] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. In Proc. IEEE CVPR, pages 7132–7141, 2018. 1

[12] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak
Shah. Multi-source multi-scale counting in extremely dense
crowd images.
In Proc. IEEE CVPR, pages 2547–2554,
2013. 4, 5, 8

[13] Di Kang and Antoni B. Chan. Crowd counting by adaptively
fusing predictions from an image pyramid. In Proc. BMVC,
2018. 3

[14] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In Proc. ICLR, 2015. 5

[15] Victor Lempitsky and Andrew Zisserman. Learning to count
objects in images. In Proc. NIPS, pages 1324–1332, 2010. 2
[16] Hanhui Li, Xiangjian He, Hefeng Wu, Saeed Amirgholipour
Kasmani, Ruomei Wang, Xiaonan Luo, and Liang Lin.
Structured inhomogeneous density map learning for crowd
counting. arXiv preprint arXiv:1801.06642, 2018. 3

[17] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet:
Dilated convolutional neural networks for understanding the

highly congested scenes. In Proc. IEEE CVPR, pages 1091–
1100, 2018. 1, 2, 3, 5, 7, 8

[18] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and
Liang Lin. Crowd counting using deep recurrent spatial-
aware network. In Proc. IJCAI, pages 849–855, 2018. 1

[19] Mark Marsden, Kevin McGuinness, Suzanne Little, and
Noel E O’Connor. Fully convolutional crowd counting on
highly congested scenes. arXiv preprint arXiv:1612.00220,
2016. 3

[20] Daniel Onoro-Rubio and Roberto J L´opez-Sastre. Towards
perspective-free object counting with deep learning. In Proc.
Springer ECCV, pages 615–629, 2016. 3, 8

[21] Viet-Quoc Pham, Tatsuo Kozakaya, Osamu Yamaguchi, and
Ryuzo Okada. Count forest: Co-voting uncertain number of
targets using random forest for crowd density estimation. In
Proc. IEEE ICCV, pages 3253–3261, 2015. 3

[22] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiay-
ing Liu. Attentive generative adversarial network for rain-
drop removal from a single image.
In Proc. IEEE CVPR,
pages 2482–2491, 2018. 1

[23] Zhilin Qiu, Lingbo Liu, Guanbin Li, Qing Wang, Nong Xiao,
and Liang Lin. Crowd counting via multi-view scale aggre-
gation networks. In Proc. IEEE ICME, 2019. 3

[24] Mengye Ren and Richard S Zemel. End-to-end instance seg-
In Proc. IEEE CVPR,

mentation with recurrent attention.
pages 21–26, 2017. 1

[25] Deepak Babu Sam, Shiv Surya, and R Venkatesh Babu.
Switching convolutional neural network for crowd counting.
In Proc. IEEE CVPR, pages 4031–4039, 2017. 1, 5, 7

[26] Chong Shang, Bo, Haizhou Ai, and Bai. End-to-end crowd
counting via joint learning local and global count. In Proc.
IEEE ICIP, pages 1215–1219, 2016. 3

[27] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Proc.
ICLR, 2015. 4

[28] Vishwanath A Sindagi and Vishal M Patel. Cnn-based cas-
caded multi-task learning of high-level prior and density esti-
mation for crowd counting. In Proc. IEEE AVSS, pages 1–6,
2017. 3, 7

[29] Vishwanath A Sindagi and Vishal M Patel. Generating high-
quality crowd density maps using contextual pyramid cnns.
In Proc. IEEE ICCV, pages 1879–1888, 2017. 1, 3, 5, 7

[30] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proc. IEEE CVPR, pages 1–9, 2015. 4

[31] Paul Viola and Michael J Jones. Robust real-time face detec-
tion. International Journal of Computer Vision, 57(2):137–
154, 2004. 2

[32] Elad Walach and Lior Wolf. Learning to count with cnn
In Proc. Springer ECCV, pages 660–676, 2016.

boosting.
3

[33] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600–612, 2004. 5

3233

[34] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi
Feng, and Thomas S Huang. Revisiting dilated convolution:
A simple approach for weakly-and semi-supervised semantic
segmentation. In Proc. IEEE CVPR, pages 7268–7277, 2018.
4

[35] Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang
Yang. Cross-scene crowd counting via deep convolutional
neural networks.
In Proc. IEEE CVPR, pages 833–841,
2015. 4, 5, 8

[36] Shanghang Zhang, Guanhang Wu, Joao P Costeira, and
Jos´e MF Moura. Fcn-rlstm: Deep spatio-temporal neural
networks for vehicle counting in city cameras. In Proc. IEEE
ICCV, pages 3687–3696, 2017. 8

[37] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao,
and Yi Ma. Single-image crowd counting via multi-column
convolutional neural network. In Proc. IEEE CVPR, pages
589–597, 2016. 1, 2, 3, 4, 5, 7, 8

[38] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proc. IEEE CVPR, pages 2921–2929,
2016. 3

3234

