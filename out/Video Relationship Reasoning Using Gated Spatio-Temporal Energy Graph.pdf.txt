Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph

Yao-Hung Hubert Tsai†, Santosh Divvala‡, Louis-Philippe Morency†, Ruslan Salakhutdinov†, Ali Farhadi‡∗

†Carnegie Mellon University, ‡Allen Institute for AI, ∗University of Washington

https://github.com/yaohungt/GSTEG_CVPR_2019

Abstract

Visual relationship reasoning is a crucial yet challeng-
ing task for understanding rich interactions across visual
concepts. For example, a relationship {man, open, door}
involves a complex relation {open} between concrete en-
tities {man, door}. While much of the existing work has
studied this problem in the context of still images, under-
standing visual relationships in videos has received limited
attention. Due to their temporal nature, videos enable us
to model and reason about a more comprehensive set of vi-
sual relationships, such as those requiring multiple (tem-
poral) observations (e.g., {man, lift up, box} vs. {man,
put down, box}), as well as relationships that are often
correlated through time (e.g., {woman, pay, money} fol-
lowed by {woman, buy, coffee}). In this paper, we construct
a Conditional Random Field on a fully-connected spatio-
temporal graph that exploits the statistical dependency be-
tween relational entities spatially and temporally. We in-
troduce a novel gated energy function parametrization that
learns adaptive relations conditioned on visual observa-
tions. Our model optimization is computationally efﬁcient,
and its space computation complexity is signiﬁcantly amor-
tized through our proposed parameterization. Experimen-
tal results on benchmark video datasets (ImageNet Video
and Charades) demonstrate state-of-the-art performance
across three standard relationship reasoning tasks: Detec-
tion, Tagging, and Recognition.

1. Introduction

Relationship reasoning is a challenging task that not
only involves detecting low-level entities (subjects, objects,
etc.) but also recognizing the high-level interaction be-
tween them (actions, sizes, parts, etc.). Successfully rea-
soning about relationships not only enables us to build
richer question-answering models (e.g., Which objects are
larger than a car?), but also helps in improving image re-
trieval [20](e.g., images with elephants drawing a cart),
scene graph parsing [41] (e.g., woman has helmet), caption-
ing [42], and many other visual reasoning tasks.

Most contemporary research in visual relationship rea-

(Monkey, Creep Up, Car)

ambiguous

(Monkey, Creep Down, Car)

(Image) 

(Video) 

(Monkey, Creep Down, Car)

(Monkey, Jump Left, Car)

requires multiple temporal observations

contingency between temporal events

Figure 1. Visual relationship reasoning in images (top) vs. videos
(bottom): Given a single image, it is ambiguous whether the mon-
key is creeping up or down the car. Using a video not only helps to
unambiguously recognize a richer set of relations, but also model
temporal correlations across them (e.g., creep down and jump left).

soning has been focused in the domain of static images.
While this has resulted in several exciting and attractive rea-
soning modules [26, 20, 42, 18, 40, 45, 3, 17], it lacks the
ability from reasoning about complex relations that are in-
herently temporal and/or correlated in nature. For example,
in Fig. 1 it is ambiguous to infer from a static image whether
the monkey is creeping down or up the car. Also, it is dif-
ﬁcult to model relations that are often correlated through
time, such as man enters room and man open door.

In this paper, we present a novel approach for reason-
ing about visual relationships in videos. Our proposed ap-
proach jointly models the spatial and temporal structure of
relationships in videos by constructing a fully-connected
spatio-temporal graph (see Fig. 2). We refer to our model
as a Gated Spatio-Temporal Energy Graph. In our graph,
each node represents an entity and the edges between them
denote the statistical relations. Unlike much of the previ-
ous work [15, 43, 27, 4, 31] that assumed a predeﬁned or
globally-learned pairwise energy function, we introduce an
observation-gated version that allows us to make the statis-
tical dependency between entities adaptive (conditioned on
the observation).

Our adaptive parameterization of energy function helps
the natural diversiﬁcation of relationships in

us model

110424

Input Instance

(in a segment of video)

Fully-Connected 

Spatio-Temporal Graph

Output

Video Relationships

S

S

S

S

S

S

S

S

S

P

P

P

P

P

P

P

P

P

O

O

O

O

O

O

O

O

O

(Monkey, Creep Down, Car)

(Monkey, Jump Left, Car)

(Monkey, Sit Front, Car)

Spatio-Temporal Fully-Connect Energy Graph

Non-Gated Energy Function

Gated Energy Function

S

P

S

P

O

O

Figure 2. An overview of our Proposed Gated Spatio-Temporal Energy Graph. Given an input instance (a video clip), we predict the output
relationships (e.g., {monkey, creep down, car}, etc.,) by reasoning over a fully-connected spatio-temporal graph with nodes S (Subject),
P (Predicate) and O (Object). Unlike previous works that assumed a non-gated (i.e., predeﬁned or globally-learned) pairwise energy
function, we explore the use of gated energy functions (i.e., conditioned on the speciﬁc visual observation) . Best viewed zoomed in and in
color.

videos. For instance, the dependency between man and
cooking should be different conditioned on the observation
(i.e., whether the location is kitchen or gym). However,
given the large state space of observations (in videos), di-
rectly maintaining observation-dependent statistical depen-
dencies may be computationally intractable [22, 35]. To-
wards this end, we develop an amortized parameterization
of our new gated pairwise energy function, which com-
bines ideas from clique template [33, 34, 21], neural net-
works [8, 35], and tensor factorization [14] for achieving
efﬁcient inference and learning.

We evaluate our model on two benchmark datasets, Ima-
geNet Video [24] and Charades [32]. Our method achieves
state-of-the-art performance across three standard relation-
ship reasoning tasks: detection, tagging, and recognition.
We also study the utility of our model in the zero-shot set-
ting and learning from semantic priors.

2. Related Work

Video Activity Recognition. The notion of activity in a
video represents the interaction between objects [9, 12] or
the interaction between an object and a scene [32]. While
related to our task of relation reasoning, activity recogni-
tion does not require explicit prediction of all entities, such
as subject, object, scene, and their relationships. The term
relation used in activity recognition and relationship rea-
soning has different connotations.
In the visual relation-
ship reasoning literature, it refers to the correlation between
different entities, such as object, verb, and scene, while in
activity recognition, it refers to either correlation between
activity predictions (i.e., single entity) or correlation be-
tween video segments. For example, [44] proposed Tem-
poral Relation Network to reason the temporal ‘relations’
[6] introduced a
across frames at multiple time scales.
spatio-temporal aggregation on local convolutional features
for better learning representations in the video. [38] pro-
posed Non-Local Neural Networks to model pairwise rela-
tions for every pixel in the feature space from low-layers
to higher-layers. The work was extended to [39] for con-
structing a Graph Convolutional Layer that further modeled

relation between object-level features.

Visual Relationship Reasoning. Most recent works in re-
lation reasoning have focused their analysis on static im-
ages [40, 45, 3, 17]. For example, [26] introduced the
idea of visual phrases for compositing visual concepts of
[20] decomposed the di-
subject, predicate, and object.
rect visual phrase detection task into individual detection
on subject, predicate, and object leading to improved per-
formance. [4] further applied conditional random ﬁelds on
top of the individual predictions to leverage their statisti-
cal correlations. [18] proposed a deep variation-structured
reinforcement learning framework and then formed a di-
rected semantic action graph. The global interdependency
in this graph facilitated predictions in local regions of the
image. One of the key challenges of learning relationships
in videos has been the lack of relevant annotated datasets.
In this context, the recent work of [29] is inspiring as it
contributes manually annotated relations for the ImageNet
video dataset. Our work improves upon [29] on multiple
fronts: (1) Instead of assuming no temporal contingency be-
tween relationships, we introduce a gated fully-connected
spatio-temporal energy graph for modeling the inherently
rich structure from videos; (2) We extend the study of rela-
tion triplet from subject/predicate/object to a more general
setting, such as object/verb/scene [32]; (3) We consider a
new task ‘relation recognition’ (apart from relation detec-
tion and tagging) which requires the model to make predic-
tions in a ﬁne-grained manner; (4) For various metrics and
tasks, our model demonstrates improved performance.

Deep Conditional Random Fields. Conditional Random
Fields (CRFs) have been popularly used to model the sta-
tistical dependencies among predictions in images [10, 43,
27, 25, 4] and videos [23, 31]. Several extensions have been
recently introduced for fully-connected CRF graphs. For
example, [43, 27, 31] attempted to express fully-connected
CRFs as recurrent neural networks and made the whole net-
work end-to-end trainable, which has led to interesting ap-
plications in image segmentation [43, 27] and video activity
recognition tasks [31]. In the characterization of CRFs, the
unary energy function represents the inverse likelihood for

10425

assigning a label, while the binary energy function measures
the cost of assigning multiple labels jointly. However, most
of the existing parameterizations of binary energy func-
tions [15, 43, 27, 4, 31] have limited or no connections to
observed variables. Such parameterizations may not be op-
timal for video relationship reasoning due to the adaptive
idiosyncrasy for statistical dependencies between entities.
To address the issue, we instead propose an observation-
gated pairwise energy function with efﬁcient and amortized
parameterization.

3. Proposed Approach

The task of video relationship reasoning not only re-
quires modeling the entity predictions spatially and tempo-
rally, but also maintaining a changeable correlation struc-
ture between entities across videos with various contents.
To this end, we propose a Gated Spatio-Temporal Fully-
Connected Energy Graph for capturing the inherently rich
video structure into account.

t=1.

t }T

t }T

t }T

t , Y 2

t · · · , Y K

We start by deﬁning our notations using Fig. 2 as a run-
ning example. The input instance X lies in a video seg-
ment and consists of K synchronous input streams X =
{X k}K
k=1. In this example, input streams are {object tra-
jectories, predicate trajectories, subject trajectories}, and
thus K = 3, where trajectories refer to the consecutive
frames or bounding boxes in the video segment. Each
input stream contains observations for T time steps (i.e.,
X k = {X k
t=1), where for example object trajectories
represent object bounding boxes through time. For each
input stream, our goal is to predict a sequence of entities
(labels) Y k = {Y k
In Fig. 2, the output sequence
of predicate trajectories represent predicate labels through
time. Hence we formulate the data-entities tuple as (X, Y )
with Y = {Y 1
t=1 representing a set of se-
quence of entities.
The entity Y k
t
t , Y 2
t · · · , Y K
t } \ {Y k

should spatially relate to entities
{{Y 1
t }} and temporally relate to en-
tities {{Y k
t }}. For example, sup-
pose that the visual relationships observed in a grocery
store are {{mother, pay, money}, {infant, get, milk},
{infant, drink, milk}}; spatial correlation must exist be-
tween mother/pay/money and temporal correlation must
exist between pay/get/drink. We also note that implicit
correlation may also exist between Y k
for t 6=
t′, k 6= k′. Based on the structural dependencies between
entities, we propose to construct a Spatio-Temporal Fully-
Connected Energy Graph (see Sec. 3.1), where each node
represents an entity and each edge denotes the statistical
dependencies between the connected nodes. To further take
account that the statistical dependency between “get” and
“drink” may be different depending on different observa-
tions (i.e., location in grocery store v.s. home), we introduce
an observation-gated parameterization for pairwise energy

t and Y k′

T } \ {Y k

2 · · · , Y k

1 , Y k

t′

In the new parameterization, we amortize the
functions.
potentially large computational cost by using clique tem-
plates [33, 34, 21], neural network approximation [22, 35],
and tensor factorization [14] (see Sec. 3.2).

3.1. Spatio Temporal Fully Connected Graph

By treating the predictions of entities as random vari-
ables, the construction of the graph can be realized by form-
ing a Markov Random Field (MRF) conditioned on a global
observation, which is the input instance (i.e., X). Then, the
tuple (X, Y ) can be modeled as a Conditional Random Field
(CRF) parametrized by a Gibbs distribution of the form:
P(cid:16)Y = y|X(cid:17) = 1
Z(X) exp(cid:16) − E(y|X)(cid:17), where Z(X)
is the partition function and E(y|X) is the energy of as-
signing labels Y = y = {y1
t=1 conditioned
on X. Assuming only pairwise cliques in the graph (cid:0)i.e.,
P (y|X) := Pψ,ϕ(y|X), E(y|X) := Eψ,ϕ(y|X)(cid:1), the en-
ergy can be expressed as:
Eψ,ϕ(y|X) = X
ψt,k(yk

t |X)+ X

t , · · · , yK

ϕt,k,t′,k′ (yk

t , yk′

t , y2

t }T

t,k

{t,k}6={t′,k′}

t′ |X),

t = yk

(1)
where ψt,k and ϕt,k,t′,k′ are the unary and pairwise energy,
respectively. In Eq. (1), the unary energy, which is deﬁned
on each node in the graph, captures inverse likelihood for
assigning Y k
t conditioned on the observation X. Typ-
ically, this term can be derived from an arbitrary classiﬁer
or regressor, such as a deep neural network [16]. On the
other hand, the pairwise energy models interactions of label
assignments across nodes Y k
t′ conditioned
on the observation X. Therefore, the pairwise term deter-
mines the statistical dependencies between entities spatially
and temporally. However, the parameterization in most pre-
vious works on fully-connected CRF [43, 27, 31, 4] as-
sumes that the pairwise energy function is non-adaptive to
the current observation, which may not be ideal to model
changeable dependencies between entities across videos.
In the following Sec.3.2, we propose an observation-gated
parametrization for pairwise energy function to address the
issue.

t′ = yk′

t = yk

t , Y k′

3.2. Gated Pairwise Energy Function

t , yk′

t′ |X) is deﬁned as µ(yk

Much of existing work uses a simpliﬁed parameter-
ization of pairwise energy function and typically con-
siders only the smoothness of the joint
label assign-
ment. For instance, in Asynchronous Temporal Field [31],
t′ )K(t, t′), where µ
ϕ·(yk
represents the label compatibility matrix and K(t, t′) is
an afﬁnity kernel measurement which represents the dis-
counting factor between t and t′. Similarly, in the im-
age segmentation domain [43, 27], ϕ·(si, sj|I) is deﬁned
as µ(si, sj)K(Ii, Ij), where s{i,j} is the segment label
and I{i,j} is the input feature for location {i, j} in im-
age I.
In these models, the pairwise energy comprises

t , yk′

10426

an observation-independent label compatibility matrix fol-
lowed by a spatio or temporal discounting factor. We argue
that the parametrization of pairwise energy function should
be more expressive. To this end, we deﬁne the pairwise en-
ergy as:

ϕt,k,t′,k′ (yk

t , yk′

t′ |X) := hf ϕiX,t,t′,k,k′,yk

t ,yk′
t′

,

(2)

t | × |Y k′

where f ϕ can be seen as a discrete lookup table that takes
the input X of size |X| and outputs a large transition matrix
of size (T 2K 2 − 1) × |Y k
t′ |, and where h·iz repre-
sents its zth item. Directly maintaining this lookup table is
computationally intractable due to the large state space of
X. Considering a simple case that X is a pairwise-valued
32 × 32 image, we have |X| = 232×32 possible states.
The state space complexity aggravates when X becomes an
RGB-valued video. Thanks to the recent advances in graph-
ical models [33, 34, 21], deep neural networks [22, 35], and
tensor factorization [14], our workaround is to parametrize
and approximate f ϕ as f ϕ
θ with learnable parameters θ as
follows:

hf ϕiX,t,t′,k,k′,yk

≈ f ϕ

θ (X k

t , yk′
t′ )

t ,yk′
t′
t ) ⊗ hkk′

θ

(X k

(X k

t ) ⊗ skk′

θ

t , t, t′, k, k′, yk
t )Eyk
(X k

t ,yk′
t′
t )Eyk

t ,yk′
t′

θ

Dgkk′

(X k
Kσ(cid:16)t, t′(cid:17)Drkk′

θ

=




(3)

t = t′

t 6= t′

,

θ

θ

θ

(·) ∈

(·), skk′

(·), rkk′

(·) ∈ R|Y k

t |×r and hkk′

where gkk′
θ
R|Y k′
t′ |×r represent the r-rank projection from X k
t , which
is modeled by a deep neural network. A ⊗ B = AB⊤ de-
notes the function on matrix A and B, and results in a tran-
t′ |. Kσ(cid:16)t, t′(cid:17) is the Gaussian
sition matrix of size |Y k
kernel with bandwidth σ representing discounting factor for
different time steps.

t |×|Y k′

The intuition behind our parametrization is as follows:
First, we note that clique templates [33, 34, 21] are adopted
spatially and temporally, which leads to scalable learning
and inference. Second, the idea of using neural networks
for approximating the lookup tables ensures both parame-
ter efﬁciency and generalization [8, 35]. The lookup table
maintains the state transitions of X → Y k × Y k′
where cal-
ligraphy font denotes the corresponding state space. Finally,
we choose r << mink{|Y k
t |} so that a low-rank decompo-
to Y k′
sition is performed on the transition matrix from Y k
t′ .
t
The low-rank decomposition allows us to substantially re-
duce the number of learnable parameters. To summarize,
our design for f ϕ
θ amortize the large space complexity for
f ϕ and is gated by observation.

3.3. Inference, Message Passing, and Learning

Minimizing the CRF energy in Eq. (1) returns the
most probable label assignment problem of Y =
{y1
t=1 given the observation X. However,

t , · · · , yK

t , y2

t }T

the exact inference in a fully connected CRF is often com-
putationally intractable even with variables enumeration or
elimination [13]. In this work, we adopt the commonly used
mean-ﬁeld algorithm [13] as approximate inference, which
ﬁnds the approximate posterior distribution Q(Y ) such that
Q(·) is closest to Pψ,ϕ(Y |X) in terms of KL(Q//Pψ,ϕ)
within the class of distributions representable as a product
t ). Follow-
ing [13], inference can now be realized as the naive mean-
ﬁeld updates with the coordinate descent optimization, and
it can be expressed in terms of ﬁxed-point message passing
equations:

of independent marginals Q(Y ) = Qt,k Q(Y k

Q(yk

t ) ∝ Ψt,k(cid:16)yk

t |X(cid:17) Y

{t′,k′}6={t,k}

mt′,k′,t,k(yk

t |X)

(4)

with Ψt,k = exp(cid:16) − ψt,k(cid:17) representing the unary potential
and m·(·) denoting the message having form1 of

m·(·) = exp(cid:16) − X

yk′
t′

ϕt,k,t′,k′ (yk

t , yk′

t′ |X)Q(yk′

t′ )(cid:17).

(5)

To parametrize the unary energy function, we use a sim-

ilar formulation:

ψt,k(yk

t |X) :=Df ψEX,t,k,yk
t , t, k, yk

θ (X k

≈f ψ

t

t ) = Dwk

θ (X k

t )Eyk

t

,

where wk
logits of size |Y k

θ ∈ R|Y k

t | represents the projection from X k
t

t |, modeled by a deep neural network.

(6)

to

Lastly, we cast the learning problem as minimizing con-
ditional cross-entropy between the proposed distribution
and the true one, where θ denotes the parameters we need
in our model: θ∗ = arg minθ

EX,Y [−log Q(Y )].

4. Experimental Results & Analysis

In this section, we report our quantitative and qualitative
analyses for validating the beneﬁt of our proposed method.
Our experiments are designed to compare different base-
lines and ablations for detecting and tagging relationships
given a video as well as recognizing relationships in a
ﬁne-grained manner.

Datasets. We perform our analysis on two datasets:
ImageNet Video [24] and Charades [32].
(a) ImageNet
Video [24] contains videos (from daily-life as well as
in-the-wild) with manually labeled bounding boxes for
objects. We utilize the annotations from [29], in which a
subset of the videos having rich visual relationships were
selected (1, 000 videos in total with 800 for training & rest

1In Supplementary, we make connection from our gated amortized
parametrization for pairwise energy function in message form with Self-
Attention [36] in machine translation and Non-Local Means [1] in Image
Denoising.

10427

Correponding

Relationship Detection

Relationship Tagging

Relationship Recognition

Method

Image-Relationship or

relationship

relationship

subject

predicate

object

relationship

Video-Activity Method

R@50

R@100

mAP

P@1

P@5

P@10

Acc@1

Acc@1

Acc@1

Acc@1

VidVRD∗ [29]

Visual Phrases [26]

UEG
UEG†
SEG

STEG

VRDV [20]

VRD [20]

DRN [4]

AsyncTF [31]

GSTEG (Ours)

-

VidVRD∗ [29]

Visual Phrases [26]

UEG
UEG†
SEG

STEG

VRDV [20]

VRD [20]

DRN [4]

AsyncTF [31]

GSTEG (Ours)

-

Standard Evaluation

6.94

2.94

4.52

4.16

4.71

9.52

41.00

31.50

36.00

35.00

40.00

51.50

Zero-Shot Evaluation

0.18

1.30×1e-5
5.36×1e-5
6.70×1e-5

0.02

0.15

0.0

0.0

0.0

0.0

1.37

2.74

29.60

19.88

21.60

27.10

24.45

39.50

0.82

0.27

0.82

0.82

1.10

1.92

21.85

14.98

15.41

20.90

17.66

28.23

0.82

0.82

0.82

1.23

0.96

1.92

6.68

3.64

4.05

5.32

4.98

8.67

1.16

0.23

0.23

0.46

0.69

2.08

5.58

2.81

3.41

4.34

4.18

7.05

0.93

0.0

0.23

0.23

0.23

1.16

80.28

80.15

80.15

85.15

89.91

90.60

74.54

74.31

78.24

81.02

80.09

82.18

16.55

23.95

25.92

25.85

25.92

28.78

2.78

5.09

5.79

6.94

7.18

7.87

80.40

80.55

80.55

84.26

89.33

89.79

74.07

74.77

78.47

74.54

79.17

79.40

12.93

18.62

22.47

20.97

22.54

25.01

1.62

3.24

3.47

3.47

4.40

6.02

Table 1. Evaluation for different methods on ImageNet Video dataset. ∗ denotes the re-implementation of [29] after ﬁxing the bugs in their
released evaluation code (by contacting authors). † denotes the implementation with additional triplet loss term for language priors [20].

for evaluation, available at [28]). The visual relationship
is deﬁned on the triplet {subject, predicate, object}. For
example, {person, ride, bicycle} or {dog, larger, monkey},
It has 35 categories of subject and object, and 132
etc.
categories of predicate (see Suppl.
for details) with
trajectory denoting consecutive bounding boxes through
time. A relation triplet is labeled on a pair of trajectories
(one for subject and another for object). The entire video
has multiple pairs of trajectories and these pairs may or
may not overlap with each other spatially or temporally. (b)
Charades [32] contains videos of human indoor activities
(9, 848 in total with 7, 985 for training and the rest for
evaluation, available at [30]) . The visual relationship is
deﬁned on the triplet {verb, object, scene} or {object, verb,
scene}. For example, {hold, blanket, bedroom}, {someone,
cook, kitchen}, etc. It has 33 categories of verb, 38 objects
and 16 scenes (see Suppl.
for details). Different from
ImageNet Videos, as suggested by [32, 31, 38, 39], we treat
the entire video as an input instance. Therefore, a video
comprises multiple relation triplets, and each relation triplet
is deﬁned within a time segment. The relation triplets may
or may not overlap temporally with each other.

Tasks. For the above two datasets, we consider the follow-
ing three experimental tasks.
(i) Relationship Detection. For ImageNet Videos, we aim at
predicting a set of visual relationships with estimated sub-
ject and object trajectories. Speciﬁcally, a predicted visual
relationship is counted as correct if the predicted triplet is
in the ground truth set and the estimated bounding boxes
have high voluminal intersection over union (vIoU) with the
ground truth (vIoU threshold of 0.5). Following [29], this
task is termed relationship detection, which contains both
relationship prediction and object localization. For Cha-
rades dataset, as suggested by [31]2, we aim at detecting the

2The performance reported in [31] refers to the mean Avergage Preci-
sion (mAP) of 157 activities, while ours consider the detection of relation
triplets. Although not being our focus, our method with the 157 activities

visual relationships in a video without object localization,
i.e., relationship detection happens in the scale of the entire
video. For evaluation, we follow [20, 29] and adopt mean
average precision (mAP) and Recall@K (K equals 50 and
100) metrics, where mAP measures the average of the max-
imum precisions at different recall values and Recall@K
measures the fraction of the positives detected in the top K
detection results.
(ii) Relationship Tagging. For ImageNet Videos, the rela-
tionship tagging task [29] focuses on only relationship pre-
diction. This is motivated by the fact that video object lo-
calization is still an open problem. Similarly, in Charades,
relationship tagging focuses on only relationship prediction
(where relationship tagging happens at the scale of entire
video). Following [29], we use Precision@K (K equals 1,
5, and 10) to measure the accuracy of the tagging results.
(iii) Relationship Recognition. Different from performing
relationship reasoning at the scale of entire video, we
would also like to measure how well the model recognizes
the relationship in a ﬁne-grained manner. For example,
given an object trajectory and a subject trajectory, can the
model predict accurate relationships? For the ImageNet
Video experiments: given an input instance (with object
and subject trajectories in a time segment), we measure the
recognition accuracy of subject, predicate, object, and the
relationship, which we term it relationship recognition. As
the Charades dataset does not consider object localization,
we perform recognition on object, verb, scene, and the rela-
tionship within a time segment (where relation recognition
happens at the scale of a time segment in the video). We
use Accuracy@K (K equals 1) for emphasizing whether
the model gives the correct recognition result on the top 1
relationship prediction.

Pre-Reasoning Modules For all our experiments and ab-

output achieves 33.3 mAP on activity detection as compared to 18.3 mAP
in [31] when using only RGB frames as input. See Suppl. for details.

10428

Correponding

Relationship Detection

Relationship Tagging

Relationship Recognition

Method

Image-Relationship or

relationship

relationship

object

verb

scene

relationship

Video-Activity Method

R@50

R@100

mAP

P@1

P@5

P@10

Acc@1

Acc@1

Acc@1

Acc@1

VidVRD [29]

Visual Phrases [26]

UEG
UEG†
SEG

STEG

VRDV [20]

VRD [20]

DRN [4]

AsyncTF [31]

GSTEG (Ours)

-

13.62

22.53

22.35

23.68

23.79

24.95

18.36

29.70

29.65

31.56

31.65

33.37

3.12

7.93

7.90

8.77

8.84

9.86

3.97

16.05

16.10

18.04

18.46

19.16

4.62

11.47

11.38

12.50

12.57

12.93

4.26

8.72

8.67

9.37

9.37

9.55

28.70

41.74

41.70

42.84

42.87

43.53

63.64

64.70

64.73

64.36

64.53

64.82

34.91

34.62

35.17

35.28

35.71

40.11

7.83

11.94

11.85

12.60

12.76

14.73

Table 2. Evaluation for different methods on Charades dataset. Our method outperforms all competing baselines across the three tasks.

lation studies, we use the following three (exactly same)
pre-reasoning modules:

◦ Video Chunking. As suggested by [2], we treat the video
as consecutive overlapping segments with each segment
comprising continuous frames. For ImageNet Video, each
segment contains 30 frames, and adjacent segments have 15
overlapping frames. Since the video is chunked, the object
and subject trajectories are also decomposed into chunks.
For Charades, each segment contains 10 frames, and adja-
cent segments have 6 overlapping frames.
◦ Tracklet Proposal. Tracklet proposal is required in the
ImageNet Video dataset for object localization. For each
chunk in the video, we generate proposals for the possible
subject and object tracklets. We utilize Faster-RCNN [7]
as object detector trained on the 35 objects (categories in
the annotation) from MS-COCO [19] and ImageNet Detec-
tion [24] datasets. Next, the method described in [5] is used
to relate frame-level into a chunk-level object proposals.
Then, non-maximum suppression (NMS) with vIoU > 0.5
is performed to reduce the numbers of generated chunk-
level proposals. During training, proposals that have vIoU
> 0.5 with the ground truth trajectories are selected to be
the training proposals. However, all the generated proposals
are preserved for evaluation.

t }T

t , X o

t , X p

, and Y o

◦ Feature Representation. Following Sec. 3 notation, we ex-
press the input instance X into K synchronous streams of
features. For the ImageNet Video, K equals 3 and the syn-
chronous streams of features are {X s
t=1. s, p, o
and T denote subject, predicate, object, and the number of
chunks in the input instance, respectively. Note that each in-
stance may have different numbers of chunks, i.e., different
T , because of various duration of relationships. The output
t , Y p
Y s
t follow categorical distribution. As in [29],
in the tth chunk of the input instance, we choose the sub-
ject and object features (i.e., X s
t ) to be the averaged
features for the Faster-RCNN label probability distribution
outputs. X p
t , on the other hand, is chosen to be the concate-
nation of the following three features: the improved dense
trajectory (iDT) feature [37] for subject tracklet, the iDT
feature for object tracklet, and the relative spatio-temporal
positions [29] between subject and object tracklets. See
Suppl. for more details.

t and X o

t

For Charades,
t , X v

the input instance X is expressed as
{X o
t=1 with o, v, and s denoting object, verb,
and scene , respectively. Since we are performing relation-

t , X s

t }T

ship reasoning directly in the entire video, we let Y o
t , Y v
t
be a multinomial distribution while Y s
t still remains to be
a categorical distribution. The multinomial distribution
suggests that each chunk may contain ≥ 0 number of
objects or verbs. We set X o
t to have identical
the output feature layer from I3D network [2].
features:
See Suppl. for more details.

t , and , X s

t , X v

t , X p

t , and X o

Baselines The closest baseline to our proposed model is
VidVRD [29]. Beyond comparisons to [29], we also per-
form a detailed ablation study of our method as well as re-
late to the image-based visual relationship reasoning meth-
ods (when applicable).
VidVRD. VidVRD [29] adopted a structured loss on the
multiplication of three features (i.e., X s
t for
ImageNet Video). The loss took softmax over all training
triplets, which resembles the training objective in Visual
Phrases [26] (designed for image-based visual relationship
reasoning). Note that VidVRD fails to consider the tempo-
ral structure of relationship predictions.
GSTEG (Ours). We denote our proposed method as GSTEG
(Gated Spatio-Temporal Energy Graph). For the ablation
study, we choose the Energy Graph (EG) when considering
different energy function designs as described below.
STEG. Spatio-Temporal Energy Graph (STEG) takes into
account the spatial and temporal structure of video enti-
ties. However, it assumes ﬁxed statistical dependencies be-
tween entities. Speciﬁcally, it is the non-gated version of
our full model. STEG can be seen as a modiﬁed version of
Asynchronous Temporal Fields (AsyncTF) [31] such that
we have (1) AsyncTF’s output to be a relationship predic-
tion, and (2) a fully-connected spatial graph.
SEG. Compared to STEG, the Spatio Energy Graph (SEG)
method does not consider the temporal structure of video
entities.
Speciﬁcally, SEG assumes a spatially-fully-
connected graph and thus the relationship predictions are
made temporally independently. The counterpart in image-
based visual relationship reasoning methods is Deep Re-
lational Networks (DRN) [4]. We can view SEG as cast-
ing DRN to (1) take the video-based input features and (2)
consider continuous object bounding boxes through time in-
stead of a bounding box in a single frame.
UEG and UEG†. The Unary Energy Graph (UEG) consid-
ers the prediction of entities both spatially and temporally
independently. The counterpart in image-based visual rela-
tionship reasoning methods is the Visual Relationship De-

10429

Relationship Detection

Relationship Tagging

Dd : correctly tagged relation

Dd : incorrectly tagged relation

Ground Truth
car-move_front-person
car-move_left-person
person-walk_right-car
person-walk_behind-car
person-stand_right-car
person-stand_behind-car
car-move_past-person

VidVRD

(52) car-move_left-person
(85) person-walk_right-car

STEG

(10) person-walk_right-car
(22) person-stand_right-car

GSTEG

SEG

(24) car-move_left-person

(88) person-walk_right-car

UEG!

UEG

(24) person-walk_right-car

(1) person-walk_right-car

Ground Truth
horse-stand_right-person
horse-stand_behind-person
horse-larger-person
person-walk_front-horse
person-walk_left-horse
person-pull-horse

STEG

(1) horse-larger-person
(3) person-walk_left-horse

GSTEG
(1) horse-larger-person
(3) person-walk_left-horse
(32) person-walk_front-horse
(40) horse-stand_behind-person
(64) person-pull-horse

VidVRD
(5) horse-larger-person
(29) horse-stand_behind-person
(35) person-walk_front-horse

Ground Truth
dog-jump_right-person
dog-play-person
person-left-dog
person-taller-dog
person-play-dog
dog-jump_above-sofa
sofa-beneath-dog
sofa-taller-dog
sofa-larger-dog
person-stand_left-sofa
person-taller-sofa
sofa-right-person
sofa-larger-person

GSTEG

VidVRD

STEG

(1) sofa-larger-domestic_cat
(2) person-sit_above-sofa
(3) person-left-sofa
(4) person-front-sofa
(5) sofa-beneath-domestic_cat

(1) person-ride-sofa
(2) sofa-move_beneath-person
(3) person-stand_left-sofa
(4) domestic_cat-move_beneath-person
(5) sofa-fly_with-person

(1) sofa-larger-domestic_cat
(2) sofa-larger-rabbit
(3) sofa-larger-hamster
(4) person-taller-domestic_cat
(5) sofa-larger-bird

SEG

(1) sofa-larger-domestic_cat
(2) sofa-left-sofa
(3) sofa-beneath-domestic_cat
(4) rabbit-sit_above-sofa
(5) sofa-taller-domestic_cat

!
UEG

(1) sofa-larger-domestic_cat
(2) sofa-larger-hamster
(3) sofa-larger-bird
(4) sofa-larger-rabbit
(5) person-taller-bird

UEG

(1) sofa-larger-domestic_cat
(2) sofa-larger-rabbit
(3) sofa-larger-hamster
(4) sofa-taller-domestic_cat
(5) sofa-larger-bird

GSTEG

VidVRD

STEG

(1) bicycle-move_beneath-person
(2) person-ride-bicycle
(3) person-sit_above-bicycle
(4) person-stand_above-bicycle
(5) bicycle-jump_beneath-person

(1) person-ride-bicycle
(2) bicycle-move_beneath-person
(3) person-sit_above-bicycle
(4) bicycle-fly_with-bicycle
(5) person-ride-person

(1) bicycle-move_beneath-person
(2) person-ride-bicycle
(3) person-sit_above-bicycle
(4) person-taller-ball
(5) bicycle-move_front-bicycle

Ground Truth

bicycle-move_beneath-person
person-stand_above-bicycle
person-ride-bicycle
bicycle-jump_beneath-person

SEG

!
UEG

UEG

(1) bicycle-move_beneath-person
(2) person-ride-bicycle
(3) person-sit_above-bicycle
(4) sofa-left-sofa
(5) person-behind-sofa

(1) bicycle-move_beneath-person
(2) person-ride-bicycle
(3) person-sit_above-bicycle
(4) skateboard-move_beneath-person
(5) skateboard-stop_behind-person

(1) bicycle-move_beneath-person
(2) person-ride-bicycle
(3) person-sit_above-bicycle
(4) person-front-sofa
(5) person-taller-ball

UEG

UEG!

SEG

(1) horse-larger-person

(1) horse-larger-person

(33) horse-larger-person

Figure 3. Examples from ImageNet Video dataset of Relationship Detection (Left) & Tagging (Right) using baselines, ablations, and our
full model. The bar plots illustrate the R@100 (left) and P@5 (right) difference comparing our model to VidVRD [29]. To show the
results on all the methods, green boxes refer to a video where our model performs better and orange boxes refer to a video where VidVRD
performs better. For tagging (right), we use green to highlight the correctly tagged relation and yellow for incorrectly tagged relation. The
numbers in bracket represent the order of detection or tagging. Best viewed in color.

tection (VRD) method of [20] without using language pri-
ors (denoted as V RDV ). Similar to the modiﬁcation from
DRN to SEG, the accommodation from V RDV to UEG is
having V RDV take the video-based features and consider
object trajectories. We also perform experiments that ex-
tend UEG with additional triplet loss deﬁned with language
priors [20], which we denote it as UEG†. The counterpart
in image-based methods is the full V RD model of [20].
(Please see Suppl. for more details about parameterizations
and training for all the methods and datasets).

4.1. Quantitative Analysis

ImageNet Video. Table. 1 shows our results and compar-
isons to the baselines. We ﬁrst observe that, for every met-
ric across the three tasks (detection, tagging, and recogni-
tion), our proposed method (GSTEG) outperforms all the
competing methods. Comparing the numbers between UEG
and UEG†, we ﬁnd that language priors can help promote
visual relation reasoning. We also observe performance im-
provement from UEG to SEG, which could be explained
by the fact that SEG explicitly models the spatial statistical
dependency in {subject, predicate, object} and leads to a
better relation learning between different entities. However,
comparing SEG to STEG, the performance drops in some
metrics, indicating that modeling temporal statistical depen-
dency using a ﬁxed pairwise energy parameterization may
not be ideal. For example, although STEG gives a much
better relationship recognition results as compared to SEG,
it becomes worse in R@50 for detection and P@5 for tag-
ging. This indicates that observation-gated parametrization
for pairwise energy is able to capture different structure for
different videos. When comparing energy graph models,
VidVRD is able to outperform all our ablation baselines
(except for the full version) in relation detection and tag-
ging. However, it suffers from relation recognition, which
requires a ﬁne-grained understanding of visual relation in
the given object and subject tracklets.

Apart from the ‘standard evaluation’, we also consid-

ered the ‘zero-shot’ setting, where zero-shot refers to the
evaluation on the relative complement of training triplets
in evaluation triplets. More speciﬁcally, in the ImageNet
Video dataset, the number of all possible relation triplets
is 35 × 132 × 35 = 161, 700. While the training set
contains 2, 961 relation triplets (i.e., 1.83% of 161, 700),
the evaluation set has 1, 011 relation triplets (i.e., 0.63% of
161, 700). The number of zero-shot relation triplets is 258,
which is 25.5% in the evaluation set. Zero-Shot evaluation
is very challenging due to the fact that we need to infer
the never-seen relationship in the training set. We observe
that, for most cases, our proposed method reaches the best
performance compared to various baselines. The exception
is mAP, where VidVRD attains the best performance
using a structural objective. However, the overall trend of
zero-shot evaluation mirrors standard evaluation.

Charades. Our results and comparisons are shown in Ta-
ble. 2. We ﬁnd that our method outperforms all relevant
baselines. We also note some interesting differences be-
tween the trend of results in Charades vs. ImageNet Video:
First, comparing UEG to UEG†, we observe that language
priors do not really help the visual relationship reasoning
in Charades. We argue that it may because of the larger
inter-class distinction in Charades’ categories set. For ex-
ample, dog/cat or horse/zebra or sit front/front/jump front
share some similarity in the category set in ImageNet Video,
while the categories are less semantically similar in Cha-
rades. Second, STEG constantly outperforms SEG which
indicates modeling a ﬁxed temporal statistical dependency
between entities may aid the visual relationship reasoning
in Charades. We hypothesize that, as compared to the Im-
ageNet Video dataset that has a diversiﬁed set of videos in
the wild between animals or inorganic substances, Charades
contains videos of human indoor activities where relations
between entities are much easier to model by a ﬁxed de-
pendency. Finally, we observe that VidVRD performs sub-
stantially worse compared to all the other models, suggest-

10430

(Table, Sit, Kitchen)

STEG 

GSTEG

(non-gated pairwise energy)

(gated pairwise energy)

b
r
e
v

(Chair, Sit, Kitchen)

(None, Stand, Kitchen)

Object

Object

STEG
GSTEG

Figure 4. Analysis of non-gated and gated pairwise energies: Given an input video (top left) from Charades (that has {object, verb, scene}
relationships), the matrices (top right) visualize the non-gated and gated pairwise energies between the verbs and objects (rows: 33 verbs,
cols: 38 objects). Notice that for the verb sit (highlighted in red), the gated energy with objects chair, and table is lower compared to the
corresponding non-gated pairwise energies, thereby helping towards improved relationship reasoning. A similar behavior is observed in
case of verb to scene pairwise function (bottom left) as well as verb to verb pairwise function (bottom middle), which models the temporal
correlations e.g., sit/sit or sit/stand. Best viewed in color and color in the matrix or vector is normalized in its own scale.

ing that the structural loss introduced by VidVRD may not
generalize well to other datasets. In case of Charades, we
do not perform zero-shot evaluation as the number of zero-
shot relation triplets is low. (cid:16)The number of all the possible
relation triplets is 33 × 38 × 16 = 20, 064. The training
set contains 2, 285 relation triplets (i.e., 11.39% of 20, 064)
and the evaluation set contains 1, 968 relation triplets (i.e.,
9.81% of 20, 064). The number of zero-shot relation triplets
is 46, which is 2.34% in the evaluation set.(cid:17)

In Supplementary, we also provide the results when
leveraging language priors into our model and also provide
the comparisons with Structural-RNN [11] and Graph Con-
volutional Network [39].

4.2. Qualitative Analysis

We next illustrate our qualitative results in Fig. 3 in the
ImageNet Video dataset. For the relationship detection, in
a scene with a person interacting with a horse, our model
successfully detects 5 out of 6 relationships, while failing
to detect horse-stand right-person in the top 100 detected
relationships. In another scene with a car interacting with
a person, our model only detects 1 relationship out of 7
ground-truth relationships. We argue that the reason may be
because of the sand occlusion and the small size of a person.
For relationship tagging, in a scene with a person riding a
bike over another person, our model successfully tags all
four relationships in the top 5 tagged results. Nevertheless,
the third tagged result person-sit above-bicycle also looks
visually plausible in this video. In another scene with a per-
son playing with a dog on a sofa, our model fails to tag any
correct relationships in the top 5 tagged results. Our model
incorrectly identiﬁed dog as cat, representing the main rea-
son why it failed.

Since pairwise energy in a graphical model represents

the negative statistical dependency between entities,
in
Fig. 4, for a video in Charades dataset, we provide the il-
lustration of pairwise energy when considering our gated
and non-gated parameterization. Observe that the pairwise
energies between the related entities are lower for the gated
parameterization as compared to the non-gated one, sug-
gesting that the gating mechanism is able to aid video rela-
tionship reasoning by improving statistical dependency be-
tween spatially or temporally correlated entities.

5. Conclusion

In this paper, we have presented a Gated Spatio-
Temporal Energy Graph (GSTEG) model for the task of
visual relationship reasoning in videos.
In the graph, we
consider a spatially and temporally fully-connected struc-
ture with an amortized observation-gated parameterization
for the pairwise energy functions. The gated design en-
ables the model to detect adaptive relations between en-
tities conditioned on the current observation (i.e., current
video). On two benchmark video datasets (ImageNet Video
and Charades), our method achieves state-of-the-art perfor-
mance across three relationship reasoning tasks (Detection,
Tagging, and Recognition).

Acknoledgement

Work done when YHHT was in Allen Institute for AI.
The work was supported in part by the DARPA grants
D17AP00001 and FA875018C0150, NSF IIS1763562,
1722822, IIS-165205, IIS-1637479 and IIS-1703166, Sloan
Fellowship, Ofﬁce of Naval Research N000141812861,
NVIDIA Artiﬁcial Intelligence Lab, and Allen Institute for
artiﬁcial intelligence. We would also like to acknowledge
NVIDIAs GPU support.

10431

References

[1] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local
algorithm for image denoising. In Computer Vision and Pat-
tern Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference on, volume 2, pages 60–65. IEEE, 2005. 4

[2] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 4724–4733. IEEE, 2017. 6

[3] Zhen Cui, Chunyan Xu, Wenming Zheng, and Jian Yang.
Context-dependent diffusion network for visual relationship
detection. arXiv preprint arXiv:1809.06213, 2018. 1, 2

[4] Bo Dai, Yuqi Zhang, and Dahua Lin. Detecting visual rela-
tionships with deep relational networks. In Computer Vision
and Pattern Recognition (CVPR), 2017 IEEE Conference on,
pages 3298–3308. IEEE, 2017. 1, 2, 3, 5, 6

[5] Martin Danelljan, Gustav H¨ager, Fahad Khan, and Michael
Felsberg. Accurate scale estimation for robust visual track-
ing.
In British Machine Vision Conference, Nottingham,
September 1-5, 2014. BMVA Press, 2014. 6

[6] Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic,
and Bryan Russell. Actionvlad: Learning spatio-temporal
aggregation for action classiﬁcation.
In CVPR, volume 2,
page 3, 2017. 2

[7] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015. 6

[8] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and
Yoshua Bengio. Deep learning, volume 1. MIT press Cam-
bridge, 2016. 2, 4

[9] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
Valentin Haenel,
Ingo Fruend, Peter Yianilos, Moritz
Mueller-Freitag, et al. The something something video
database for learning and evaluating visual common sense.
In The IEEE International Conference on Computer Vision
(ICCV), volume 1, page 3, 2017. 2

[10] Xuming He, Richard S Zemel, and Miguel

´A Carreira-
Perpi˜n´an. Multiscale conditional random ﬁelds for image
labeling. In Computer vision and pattern recognition, 2004.
CVPR 2004. Proceedings of the 2004 IEEE computer society
conference on, volume 2, pages II–II. IEEE, 2004. 2

[11] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh
Saxena. Structural-rnn: Deep learning on spatio-temporal
graphs. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5308–5317, 2016. 8

[12] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950,
2017. 2

[13] Daphne Koller, Nir Friedman, and Francis Bach. Probabilis-
tic graphical models: principles and techniques. MIT press,
2009. 4

[14] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix fac-
torization techniques for recommender systems. Computer,
(8):30–37, 2009. 2, 3, 4

[15] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
in fully connected crfs with gaussian edge potentials. In Ad-
vances in neural information processing systems, pages 109–
117, 2011. 1, 3

[16] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. nature, 521(7553):436, 2015. 3

[17] Kongming Liang, Yuhong Guo, Hong Chang, and Xilin
Chen. Visual relationship detection with deep structural
ranking. 2018. 1, 2

[18] Xiaodan Liang, Lisa Lee, and Eric P Xing. Deep variation-
structured reinforcement learning for visual relationship and
attribute detection. In Computer Vision and Pattern Recogni-
tion (CVPR), 2017 IEEE Conference on, pages 4408–4417.
IEEE, 2017. 1, 2

[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014. 6

[20] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-
Fei. Visual relationship detection with language priors. In
European Conference on Computer Vision, pages 852–869.
Springer, 2016. 1, 2, 5, 6, 7

[21] Andrew McCallum, Karl Schultz, and Sameer Singh. Facto-
rie: Probabilistic programming via imperatively deﬁned fac-
tor graphs. In Advances in Neural Information Processing
Systems, pages 1249–1257, 2009. 2, 3, 4

[22] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. Playing atari with deep reinforcement learning.
arXiv preprint arXiv:1312.5602, 2013. 2, 3, 4

[23] Ariadna Quattoni, Sybor Wang, Louis-Philippe Morency,
Morency Collins, and Trevor Darrell. Hidden conditional
random ﬁelds.
IEEE transactions on pattern analysis and
machine intelligence, 29(10), 2007. 2

[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge.
International Journal of Computer Vision (IJCV),
115(3):211–252, 2015. 2, 4, 6

[25] Fereshteh Sadeghi, Santosh K Kumar Divvala, and Ali
Farhadi. Viske: Visual knowledge extraction and question
answering by visual veriﬁcation of relation phrases. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 1456–1464, 2015. 2

[26] Mohammad Amin Sadeghi and Ali Farhadi. Recognition us-
ing visual phrases. In Computer Vision and Pattern Recogni-
tion (CVPR), 2011 IEEE Conference on, pages 1745–1752.
IEEE, 2011. 1, 2, 5, 6

[27] Alexander G Schwing and Raquel Urtasun. Fully connected
deep structured networks. arXiv preprint arXiv:1503.02351,
2015. 1, 2, 3

[28] Xindi Shang, Tongwei Ren,

Jingfan Guo, Hanwang
Zhang, and Tat-Seng Chua. URL for ImageNet Video.
https://lms.comp.nus.edu.sg/research/
VidVRD.html. 5

10432

[29] Xindi Shang, Tongwei Ren, Jingfan Guo, Hanwang Zhang,
and Tat-Seng Chua. Video visual relation detection. In Pro-
ceedings of the 2017 ACM on Multimedia Conference, pages
1300–1308. ACM, 2017. 2, 4, 5, 6, 7

Huang, and Philip HS Torr. Conditional random ﬁelds as
recurrent neural networks. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1529–1537,
2015. 1, 2, 3

[30] Gunnar A Sigurdsson, Santosh Kumar Divvala, Ali
URL for Charades.

and Abhinav Gupta.

Farhadi,
http://ai2-website.s3.amazonaws.com/
data/Charades.zip. 5

[31] Gunnar A Sigurdsson, Santosh Kumar Divvala, Ali Farhadi,
and Abhinav Gupta. Asynchronous temporal ﬁelds for action
recognition. In CVPR, volume 5, page 7, 2017. 1, 2, 3, 5, 6
[32] Gunnar A Sigurdsson, G¨ul Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in
homes: Crowdsourcing data collection for activity under-
standing.
In European Conference on Computer Vision,
pages 510–526. Springer, 2016. 2, 4, 5

[33] Ben Taskar, Pieter Abbeel, and Daphne Koller. Discrimi-
native probabilistic models for relational data. In Proceed-
ings of the Eighteenth conference on Uncertainty in artiﬁcial
intelligence, pages 485–492. Morgan Kaufmann Publishers
Inc., 2002. 2, 3, 4

[34] Graham W Taylor and Geoffrey E Hinton. Factored con-
ditional restricted boltzmann machines for modeling motion
style. In Proceedings of the 26th annual international con-
ference on machine learning, pages 1025–1032. ACM, 2009.
2, 3, 4

[35] Yao-Hung Hubert Tsai, Han Zhao, Ruslan Salakhutdi-
nov, and Nebojsa Jojic. Discovering order in unordered
datasets: Generative markov networks.
arXiv preprint
arXiv:1711.03167, 2017. 2, 3, 4

[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems, pages 5998–6008, 2017. 4
[37] Heng Wang and Cordelia Schmid. Action recognition with
improved trajectories.
In Proceedings of the IEEE inter-
national conference on computer vision, pages 3551–3558,
2013. 6

[38] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
2, 5

[39] Xiaolong Wang and Abhinav Gupta. Videos as space-time
region graphs. arXiv preprint arXiv:1806.01810, 2018. 2, 5,
8

[40] Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang
Wang, Jing Shao, and Chen Change Loy. Zoom-net: Mining
deep feature interactions for visual relationship recognition.
arXiv preprint arXiv:1807.04979, 2018. 1, 2

[41] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin
Choi. Neural motifs: Scene graph parsing with global con-
text. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5831–5840, 2018. 1

[42] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-
Seng Chua. Visual translation embedding network for visual
relation detection. In CVPR, volume 1, page 5, 2017. 1

[43] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang

[44] Bolei Zhou, Alex Andonian, and Antonio Torralba. Tem-
arXiv preprint

reasoning in videos.

relational

poral
arXiv:1711.08496, 2017. 2

[45] Yaohui Zhu and Shuqiang Jiang. Deep structured learning

for visual relationship detection. 2018. 1, 2

10433

