Interaction-and-Aggregation Network for Person Re-identiﬁcation

Ruibing Hou1,2, Bingpeng Ma2, Hong Chang1,2, Xinqian Gu1,2, Shiguang Shan1,2,3, Xilin Chen1,2
1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),

Institute of Computing Technology, CAS, Beijing, 100190, China

2University of Chinese Academy of Sciences, Beijing, 100049, China

3CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, 200031, China

{ruibing.hou, xinqian.gu}@vipl.ict.ac.cn, bpma@ucas.ac.cn, {changhong, sgshan,xlchen}@ict.ac.cn

Abstract

Person re-identiﬁcation (reID) beneﬁts greatly from deep
convolutional neural networks (CNNs) which learn robust
feature embeddings. However, CNNs are inherently lim-
ited in modeling the large variations in person pose and
scale due to their ﬁxed geometric structures.
In this pa-
per, we propose a novel network structure, Interaction-and-
Aggregation (IA), to enhance the feature representation ca-
pability of CNNs. Firstly, Spatial IA (SIA) module is in-
troduced. It models the interdependencies between spatial
features and then aggregates the correlated features cor-
responding to the same body parts. Unlike CNNs which
extract features from ﬁxed rectangle regions, SIA can adap-
tively determine the receptive ﬁelds according to the input
person pose and scale. Secondly, we introduce Channel IA
(CIA) module which selectively aggregates channel features
to enhance the feature representation, especially for small-
scale visual cues. Further, IA network can be constructed by
inserting IA blocks into CNNs at any depth. We validate the
effectiveness of our model for person reID by demonstrating
its superiority over state-of-the-art methods on three bench-
mark datasets.

1. Introduction

Person re-identiﬁcation (reID) aims at identifying a per-
son of interest across different cameras with a given probe.
It plays a signiﬁcant role in intelligent surveillance sys-
tems. In recent years, Deep Convolutional Neural Networks
(CNNs), which typically stack convolution and pooling lay-
ers to learn discriminative features, have obtained state-of-
the-art results for person reID. Despite of years of efforts,
there still exist many challenges such as large variations in
person pose, scale, and background clutter.

Body part misalignment is a critical inﬂuencing factor on
reID results, which can be attributed to two causes. First,

(a)

(b)

Figure 1. The critical inﬂuencing factors for person reID. (a) A
person appears in various poses; (b) various scales due to imper-
fect detection results.

pedestrians naturally take on various poses as shown in
Fig. 1 (a). Second, the body parts have various scales across
different images of the same person caused by imperfect
pedestrian detection, as illustrated in Fig. 1 (b). To resolve
these problems, some approaches have been proposed re-
cently. One way is to localize body parts explicitly and
combine the representations over them [59, 61, 38, 37, 20].
This scheme requires highly-accurate part detection. Un-
fortunately, even state-of-the-art part detection solutions are
not perfect. Another type of methods resorts to multi-scale
features fusion where the feature maps are computed at mul-
tiple layers of a network [5, 48, 4, 12]. Nevertheless, these
methods only employ manually speciﬁed scales, which are
ineffective to model large scale variations. In short, exist-
ing methods, which attempt to utilize body part detection or
multi-scale features, are still limited in modeling the large
variations in body pose and scale.

An essential reason why these approaches are not ro-
bust to body pose and scale variations is that they all use
CNNs to extract pedestrian features. Actually, CNNs are
inherently limited in modeling large geometric transforma-
tions. The limitation originates from the ﬁxed geometric
structures of CNNs modules: a convolution unit which sam-
ples the input feature map at ﬁxed locations and a pooling
layer which reduces the spatial resolution at a ﬁxed ratio.
There lacks internal mechanisms to handle the body pose
and scale variations. For one thing, the receptive ﬁelds of
the feature maps are pre-deﬁned rectangles, which can not
adaptively localize the non-rigid body parts with different

9317

poses. For another, the receptive ﬁelds of all activation
units in the same CNN layer have the same size, which is
undesirable for high level CNN layers to encode semantics
for body parts of different scales.

In this paper, we propose a new network structure,
Interaction-and-Aggregation (IA), to enhance the feature
representation capability of CNNs, especially at the pres-
ence of body pose and scale variations. IA consists of two
modules: Spatial Interaction-and-Aggregation (SIA) and
Channel Interaction-and-Aggregation (CIA). Unlike CNNs
which extract features with ﬁxed geometric structure, SIA
adaptively determines the receptive ﬁelds according to the
pose and scale of input person image. More speciﬁcally,
given the intermediate feature maps from CNNs, SIA gen-
erates spatial semantic relation maps to discover two types
of interdependencies between different image positions: ap-
pearance relations where positions with similar feature rep-
resentations have a higher correlation, and location relations
where positions close to each other tend to have a higher
correlation. In this way, the body parts with various poses
and scales can be adaptively localized. Based on the spatial
relation maps, an aggregation operation is adopted to up-
date the feature maps via aggregating the semantically cor-
related features across different positions. Similar with SIA
in principle, we propose CIA to further enhance the rep-
resentation power of CNNs. Unlike CNNs where the fea-
tures from different channels are assumed independently,
CIA explicitly models the semantic interdependencies be-
tween channels. Specially, for small-scale visual cues (e.g.
bags) that easily fade away in the high-level features from
CNNs, CIA can selectively aggregate the semantically simi-
lar features of the visual cues across all channels to manifest
their feature representations.

Both modules are computationally lightweight and im-
pose only a slight increase in model complexity. They
can be readily inserted into deep CNNs at any depth.
In
our work, we add IA blocks to ResNet-50 [13] to gener-
ate Interaction-and-Aggregation network (IANet) for per-
son reID. We demonstrate the effectiveness of IANet on
three reID datasets, and our method outperforms state-of-
the-art methods under multiple evaluation metrics.

2. Related Work

Person re-identiﬁcation. Person reID methods focus on
two key points: learning a powerful feature representation
for images [8, 43, 14, 44, 24, 26, 28, 42, 1] and designing
an effective distance metric [32, 29, 56, 2, 57]. Recently,
deep learning approaches have obtained state-of-art results
for reID. We focus our discussion on those which attempt
to address the problem of body pose and scale variations.

Body part detection results have been exploited for reID
to extract features robust to pose and scale variations. Most
approaches attempt to localize body parts explicitly and

combine the representations over global features. Specif-
ically, Zhao et al. [59] used a region proposal network,
which is trained on an auxiliary pose dataset, to detect body
parts. Su et al. [38] proposed a sub-network to estimate the
human pose that is used to crop the body parts. Besides,
human parsing method [20, 37] and body part speciﬁc at-
tention modeling [60] had also been adopted to explicitly
alleviate the pose variations problem. However, part detec-
tion in low resolution pedestrian images has its own chal-
lenges, and the inevitable detection errors could propagate
to the subsequent reID task.

Another line of approaches attempts to utilize multi-
scale features. Liu et al. [27] and Chen et al. [7] proposed
an architecture consisting of multiple branches for learning
multi-scale features and one branch for feature fusion. Chen
et al. [5] and Shen et al. [36] used the hourglass-like net-
work [31] to generate multi-scale features. Wang et al. [48]
and Chang et al. [4] directly fused the feature maps across
multiple layers to generate a single feature. Nevertheless,
these methods employ pre-deﬁned scales that are limited in
modeling large scale variations.

In contrast to the above works that rely on part detec-
tion or pre-deﬁned scales, our proposed SIA can adaptively
localize the body parts under various poses and scales and
aggregate semantic features therein. Therefore, SIA can be
easily inserted into existing networks, enhancing their fea-
ture representation power.

Modeling geometric variations. There are some works
which enhance the feature representation power with re-
spect to geometric variations. Traditional methods include
scale invariant feature transform (SIFT) [30] and ORB [34].
A lot of recent works are aimed at CNNs. Some works
learn invariant CNN representations with respect to speciﬁc
transformations such as symmetry [11], scale [21] and rota-
tion [52]. However, these works assume the transformations
are ﬁxed and known, which restricts their generalization
to new tasks with unknown transformations. Other works
adaptively learn the spatial transformations from data. Spa-
tial Transform Network [18] warped the feature map via a
global parametric transformation. The works [19, 9] aug-
mented the sampling locations in the convolution with off-
sets and learn the offsets via back-propagation end-to-end.

Our work is fundamentally different from those works in
two folds. First, the basic idea and formulation are different.
The above works usually learn a parametric transformation
with large amount of training data, which is infeasible for
reID task with a small dataset. Differently, our proposed
SIA computes spatial semantic similarities to adaptively ag-
gregate features from same body parts without any param-
eters. Second, all above works do not take the channel re-
lations into consideration. In contrast, our proposed CIA
explicitly models the correlations between channels, which
signiﬁcantly enhances the feature representation power.

9318

H*W

MC

H*W

reshape

(cid:2162)

C

H

W

Interaction

H*W

Appearance
relation 

H*W

map(cid:2175)(cid:2157)
map (cid:2175)(cid:2168)

Location
relation

(cid:2161)

C

H

W

Aggregation

H*W

H*W

(cid:2175)

Spatial Semantic
relation map

MC

: multi-context interaction

Figure 2. The architecture of Spatial Interaction-and-Aggregation
(SIA) module. We omit the softmax layer for clarity.

W

(cid:2188)(cid:2191)

W

(cid:2188)(cid:2192)

H*W

H*W

H*W

H*W

ADP

K=1

K=2

ADP

H*W

K=3

H*W

ADP

(cid:1845)(cid:2869)(cid:3002) (cid:3036)(cid:3037)
(cid:2175)(cid:2778)(cid:2157)
(cid:1845)(cid:2870)(cid:3002) (cid:3036)(cid:3037)
(cid:2175)(cid:2779)(cid:2157)
(cid:1845)(cid:2871)(cid:3002) (cid:3036)(cid:3037)
(cid:2175)(cid:2780)(cid:2157)

H

H

(cid:2162)

(cid:2162)

H*W

H*W

(cid:2175)(cid:2157)

ADP

: accumulation over
dot-products operation

Figure 3. The multi-context interaction operation of SIA. For clar-
ity, we omit the channel dimensions of the input feature map and
the softmax layer. The number of context levels is 3 in this ﬁgure.

3. Interaction-and-Aggregation Network

In this section, we ﬁrst introduce SIA and CIA modules,
respectively. Then, IA block, which integrates SIA and CIA
modules, is illustrated, followed by IANet for person reID.
Finally, we provide some discussions on the relationships
between the proposed modules and other related models.

3.1. SIA Module

With ﬁxed local receptive ﬁelds, CNNs are limited in
representing person images with large variations in body
pose and scale. To address this problem, we design the SIA
module to model spatial features interdependencies. SIA
could adaptively determine the receptive ﬁeld for each spa-
tial feature, thus improving the feature robustness to body
pose and scale variations.

As shown in Fig. 2, suppose a convolutional feature map
F ∈ RC×H×W is given, where C, H and W denote the
number of channels, the height and the width of the feature
map respectively. We ﬁrst reshape F to RC×M where M
(M = H ×W ) is the number of spatial features, then feed it
into two sequential operations, interaction and aggregation.
Interaction operation explicitly models the interdependen-
cies between spatial features to generate a semantic relation
map S. Two types of relations are considered: appearance
relations and location relations. The generated relation map
is then used to aggregate correlated spatial features in the
following aggregation operation.

Appearance Relations. We measure the appearance

(cid:28431)a(cid:28432)

(cid:1837)(cid:3404) 1

2

(cid:28431)b(cid:28432)

3

5

(cid:28431)c(cid:28432)

Figure 4. Visualization of the receptive ﬁelds in SIA with single-
context and multi-context interaction operations.
(a) The input
image, (b) The respective ﬁelds of the point marked in the input
image with different single-context interaction operations, (c) The
respective ﬁelds with multi-context interaction. Warmer color in-
dicates higher value.

similarity between any two positions of an input feature
map to generate the appearance relation map. Du et al. [10]
have pointed out that local features at neighboring spatial
positions have high correlation since their receptive ﬁelds
are often overlapped. So the patches involving neighboring
positions could capture more precise appearance. Inspired
by their views, we propose to incorporate contextual infor-
mation for any position in order to obtain more precise ap-
pearance similarities.

As illustrated in Fig. 3, fi, fj ∈ RC denote the features
in the ith and jth spatial positions of the feature map F . In
order to calculate the appearance similarity between fi and
fj , we ﬁrst extract the K × K patches Pi and Pj around i
and j, respectively. Then, the appearance similarity is ob-
tained by accumulating the dot-products between features
of corresponding positions, and then normalizing across all
spatial positions in F with softmax:

(cid:2)SA

K(cid:3)ij =

exp(cid:4)(cid:5)K×K

k=1 (pT
exp(cid:4)(cid:5)K×K

(cid:5)H×W

t=1

i,kpj,k)(cid:6)

k=1 (pT

i,kpt,k)(cid:6)

,

(1)

where pi,k and pj,k denote the features in the kth spatial po-
sition of patches Pi and Pj , respectively. Notably, the soft-
max dramatically suppresses small similarity values corre-
sponding to different body parts. Through incorporating
context and suppressing dissimilarities, the relation map can
roughly localize the body parts under various poses and
scales. We call the process single-context interaction as
only one patch size is considered, and SA
K is the single-
context appearance relation map.

As shown in Fig. 4, the relation maps with small con-
text patches (e.g., K = 1) capture more positive regions,
but introduce some outliers, e.g., the located regions of the
foot contain some positions corresponding to the trunk. The
relation maps with large context patches (e.g., K = 5)
ﬁlter out the outliers, but ignore some positive regions.
Therefore, we introduce multi-context interaction by fusing
multiple single-context relation maps with different context
patch sizes. The multi-context appearance relation map SA
is computed as:

SA = sof tmax(cid:2)F (cid:2)SA

1 , . . . , SA

N(cid:3)(cid:3) ,

(2)

9319

where N denotes the number of context levels and F is a
fusion function with element-wise product. From Fig. 4 (c),
multi-context interaction can alleviate both problems and
localize the body parts more precisely.

Location Relations. As for pedestrian images, local
features corresponding to the same body part are spatially
close. To take advantage of the spatial structure informa-
tion, we introduce location relations, in which features from
nearby locations have a higher correlation.

Formally, the location relation between spatial features
fi and fj is computed via a two-dimensional Gaussian func-
tion as follows:

lij =

1

2πσ1σ2

exp(cid:2)−

1

2 (cid:3) (xj − xi)2

σ2

1

+

(yj − yi)2

σ2

2

(cid:4)(cid:5) ,

(3)

where (xi, yi) and (xj, yj) denote the location coordinates
of features fi and fj respectively, and (σ1, σ2) are the stan-
dard deviations used to tune the Gaussian function. We then
normalize lij ’s so that the sum of the location relation values
connected to fi equals to 1. The resulting spatial location
relation map SL is:

(cid:2)SL(cid:3)ij =

lij

(cid:5)H×W

t=1

.

lit

(4)

We can see that the location relation between fi and fj ex-
ponentially decreases with the increase of their spatial dis-
tance. Notably, SL is computed based on the spatial struc-
ture of the input image, which can constrain and comple-
ment the appearance relations.

The spatial semantic relations (S) integrates the appear-

ance with location relations, which is formulated as:

S = sof tmax(cid:2)F (cid:2)SA, SL(cid:3)(cid:3)

(5)

Aggregation Operation. To make use of the semantic
relation map in the interaction operation, we follow it with
the aggregation operation which aims to aggregate the in-
put spatial features based on the semantic relation map. As
shown in Fig. 2, we compute the aggregated feature map
ES ∈ RC×M through matrix multiplication between F and
the transpose of S:

ES = F ST .

(6)

(cid:2162)

reshape

C

C

H

W

reshape & transpose

H*W

Interaction

H*W

C

C

softmax

C

(cid:2159)

Channel Semantic

relation map

(cid:2161)

C

W

H

Aggregation

Figure 5.
Aggregation (CIA) module.

The architecture of Channel

Interaction-and-

al. [58] have discovered that most channel maps of high-
level features show strong responses for speciﬁc parts. Mo-
tivated by their views, we build the CIA module to aggre-
gate semantically similar features across all channels, which
could enhance the feature representation of speciﬁc parts.

The structure of CIA is illustrated Fig. 5. In the inter-
action stage, given an input convolutional feature map F ,
CIA explicitly models the semantic interdependencies be-
tween different channels of F to generate a channel seman-
tic relation map. To this end, we ﬁrst reshape F to RC×M
(M = H × W ). Then we perform matrix multiplication
between F and the transpose of F and normalize the result
to obtain the channel semantic relation map C ∈ RC×C .
Speciﬁcally, the semantic similarity between any two chan-
nels is calculated as:

Cmn =

exp(cid:2)f T
mfn(cid:3)
l=1 exp (f T

mfl)

(cid:5)C

,

(7)

where fm, fn ∈ RM denote the features in the mth and nth
channels of F respectively.

The channel features are then aggregated based on the
channel relation map in the following aggregation opera-
tion, where we perform matrix multiplication between C
and F to obtain the aggregated feature map EC ∈ RC×M :

EC = CF.

(8)

EC is ﬁnally reshaped to RC×H×W to maintain the input
size. Note that the resulting feature map aggregates seman-
tically similar features according to input-speciﬁc channel
relation map C. This is complementary to SIA which ag-
gregates features according to spatial relation map. Similar
with SIA, CIA can adaptively adjust the input feature map,
helping to boost the feature discriminability.

ES is then reshaped to RC×H×W to maintain the input size.

3.3. IA Block

3.2. CIA Module

Current reID models typically stack multiple convolu-
tion layers to extract pedestrian features. With increas-
ing the number of layers, these models could easily lose
small scale visual cues, such as bags and shoes. However,
these ﬁne-grained cues are very useful to distinguish the
pedestrian pairs with small inter-class variations. Zhang et

We turn the SIA (CIA) module into SIA (CIA) block
that can be easily incorporated into existing architectures.
As shown in Fig. 6 (a), SIA (CIA) block is deﬁned as:

Y = BN(E) + F,

(9)

where F is the input feature map, E is the output of SIA
or CIA modules that is given in Eq. 6 or Eq. 8, and BN

9320

(a) IA

SIA

BN
SIA Block

Input feature

CIA

BN
CIA Block

Output feature

(b) IANet

Stage 1

Stage 2

Stage 3

Stage 4

….

l

k
c
o
B
v
n
o
C

 

l

k
c
o
B
v
n
o
C

 

l

k
c
o
B
v
n
o
C

 

….

l

k
c
o
B
v
n
o
C

 

IA

….

l

k
c
o
B
v
n
o
C

 

l

k
c
o
B
v
n
o
C

 

IA

….

l

k
c
o
B
v
n
o
C

 

l

k
c
o
B
v
n
o
C

 

g
n

i
l

o
o
P

ID

Input

Figure 6. (a) The structure of IA block, which is sequentially con-
sisted of SIA and CIA blocks, (b) The architecture of IANet.

is a batch normalization layer [17] which adjusts the scale
of E with respect to the input. The residual connection
(+F ) [13] allows us to insert a new block into any pre-
trained model, without breaking its initial performance (e.g.
the parameters of BN are initialized to zeros).

Given an input feature map, SIA and CIA blocks com-
pute complementary interdependencies. We sequential ar-
range SIA and CIA blocks to form the IA block (see Fig. 6
(a)). IA block can be inserted at any depth of a network.
Considering the computational complexity, we only place
it at the bottlenecks of models where the downsampling of
feature maps occurs. Multiple IA blocks located at bottle-
necks of different levels can progressively enhance the fea-
ture representations with negligible number of parameters.

3.4. IANet for Person ReID

The architecture of IANet is illustrated in Fig. 6 (b).
Here we use ResNet-50 [13] pre-trained on ImageNet [22]
as the backbone network for person reID. The output di-
mension of the classiﬁcation layer is set to the number of
training identities. Following [40], we remove the last spa-
tial down-sampling operation in the backbone network to
increase retrieval accuracy with very light computation cost
added. IA blocks are then inserted into the backbone net-
work after stage-2 and stage-3 layers. The training proce-
dure of IANet follows the standard identity classiﬁcation
paradigm [39, 53, 59], where the identify of each person is
treated as a distinct class. IANet is end-to-end trained with
cross-entropy loss. During testing, the features of probe and
gallery images are extracted by IANet, and the cosine dis-
tance is used for matching.

3.5. Discussions

In this subsection, we give a brief discussion on the re-
lations between our proposed IA block and some existing
models.

Relations to Non-local Our IA and Non-local (NL) are
both the concrete forms of self-attention. Compared to NL,
IA is more suitable to reID because of the following advan-
tages: (1) the proposed CIA is the ﬁrst attempt to apply self-

attention on the channel dimension, which is conductive to
highlighting important but small details or body parts. (2)
NL can be seen a special case of SIA in the single-context
version. Multi-context SIA fuses appearance similarities
across multiple patch size, which could localize the body
parts more precisely. (3) SIA considers the spatial structure
of pedestrians and models the location relations to constrain
and complement the appearance similarity.

Relations to SCA-CNN and CBAM SCA-CNN [6] and
CBAM [51] propose spatial and channel attention to en-
hance important features and suppress unnecessary ones.
However there is no direct guidance for this process, mak-
ing these methods easily produce unreliable attentions. On
the contrary, our IA models generates the attention maps
guided by semantic similarity between features, which
could adaptively locate body parts and are more reliable.

Relations to Squeeze-and-Excitation CIA has some
similarities with Squeeze-and-Excitation network (SE) [15]
because both are designed to model the interdependen-
cies between channels to improve the feature representa-
tion power. However, SE computes channel-wise attention
that selectively emphasizes informative features, while ig-
noring the spatial-wise responses due to global spatial pool-
ing. Therefore, the spatial structure information is lost.

Relations to Graph Convolutional Network SIA and
CIA could be treated as the extended Graph Convolutional
Network (GCN), where the nodes of graph are deﬁned by
the spatial features and channel features, respectively, and
the adjacent matrix is the semantic relation map. Compared
to conventional GCN where the adjacent matrix is ﬁxed,
SIA and CIA change the graph structure adaptively during
training, which is more desirable for information propaga-
tion between feature nodes.

4. Experiments

4.1. Experiment Setup

Datasets and Evaluation Metric. We conduct ex-
tensive experiments on four person reID benchmarks,
CUHK03 [25], Market-1501 [62], DukeMTMC-reID [64]
and MSMT17 [49]. For CUHK03, we follow the standard
protocol detailed in [25] and report the results on manually
annotated and DPM-detected images. We adopt mean Aver-
age Precision (mAP) [63] and Cumulative Matching Char-
acteristics (CMC) [3] as evaluation metrics.

Implementation details. For our implementation, the
input images are resized to 256 × 128 after random left-
right ﬂipping. The initial learning rate is set to 0.0003.
Adam [23] optimizer is used with a mini-batch size of 32
for training. The number of context levels (N in Eq. 2) is set
to 3. Because the feature maps at different stages of ResNet
have different spatial sizes, we use different standard devi-
ations (σ1 and σ2 in Eq. 3) for IA blocks at different stages.

9321

Table 1. Comparison with state-of-the-arts on Market-1501 and
DukeMTMC. The methods are separated into three groups: global
features (G), part features (P) where * denotes those requiring aux-
iliary part detection, and multi-scale features (MS).

Methods

SVDNet [39]
MGCAM [37]
BraidNet [47]

G

GAN [64]

Adversarial [16]

Dual [10]
Mancs [44]
Spindle* [59]

PAR [60]

AACN* [54]

PSE* [35]

HA-CNN [26]
SPReID* [20]

RPP [40]

DaRe(R) [48]

DPFL [55]
MLFN [4]
KPM [36]
Group [5]

IANet

P

MS

–

Market-1501
top-1 mAP
82.3
62.1
74.3
83.7
69.5
83.7
66.1
84.0
86.4
70.4
76.6
91.4
93.1
82.3
76.9
81.0
85.9
87.7
91.2
92.5
93.8
86.4
88.6
90.0
90.1
93.5
94.4

63.4
66.9
69.0
75.7
81.3
81.6
69.3
72.6
74.3
75.3
81.6
83.1

DukeMTMC
top-1 mAP
76.7
56.8

–

76.4
67.7
79.1
81.8
84.9

–
–

76.8
79.8
80.5
84.4
83.3
75.2
79.2
81.0
80.3
84.9
87.1

–

59.5
47.1
62.1
64.6
71.8

–
–

59.2
62.0
63.8
70.9
69.2
57.4
60.6
62.8
63.2
69.5
73.4

Specially, σ1 and σ2 are set to 5 and 10 when IA blocks are
added to stage-3 layers, and σ1 and σ2 are set to 10 and 20
when IA blocks are added to stage-2 layers.

4.2. Comparison with State-of-the-art Approaches

Market-1501 and DukeMTMC. In Tab. 1, we com-
pare IANet with state-of-the-arts on Market-1501 and
DukeMTMC. IANet achieves the best performance on all
evaluation criteria. It is noted that: (1) The gaps between
our results and the spatial alignment methods (Spindle [59],
AACN [54] and PSE [35]) that incorporate an external part
detection sub-network are signiﬁcant: about 10% improve-
ment on top-1 accuracy and mAP. We argue that these meth-
ods are prone to performance degeneration due to inaccu-
rate part detection. On the contrary, our method can adap-
tively locate the body parts guided by the semantic simi-
larity without external network. (2) IANet outperforms the
attention-centric methods (AACN* [54], HA-CNN [26] and
RPP [40]) that uses spatial attention to learn discrimina-
tive parts. We argue that these methods often fail to pro-
duce reliable attentions as there is no guidance for this pro-
cess. On the contrary, our method generates spatial atten-
tion maps guided by semantic similarity between spatial
features, which are more reliable. (3) IANet outperforms
the multi-scale methods (DaRe [48], DPFL [55], MLFN [4]
and KPM [36]), with an improvement up to 8% on mAP.
The superiority of IANet over the multi-scale methods in-

Table 2. Comparison with state-of-the-art methods on CUHK03.

G

P

MS

Methods

SVDNet [39]
BraidNet [47]
PN-GAN [33]
MSCAN [24]

PAR [60]

AACN [54]
PABR [60]

SPReID* [20]

DPFL [55]

CSN [4]
Group [5]
MLFN [4]
KPM [36]

IANet

Labeled

Detected

top-1

top-5

–

88.1

–

74.2
85.4
91.3
91.5
93.8
86.7
87.5
90.2

–

91.1
92.4

–
–
–

94.3
97.6
98.8
99.0
98.7

–

97.8
98.5

–

98.3
99.0

top-1
81.8
85.8
79.7
67.9
81.6
89.5
88.0

–

82.0
86.4
88.8
82.8

–

top-5
95.2

–

96.2
91.0
97.3
97.6
97.6

–
–

97.5
97.2

–
–

90.1

98.2

Table 3. Comparison with state-of-the-arts on MSMT17.

Methods
GoogleNet [41]
Pose-driven [38]
GLAD [50]
IANet

top-1
47.6
58.0
61.4
75.5

top-5
65.0
73.6
76.8
85.5

top-10 mAP
23.0
71.8
29.7
79.4
34.0
81.6
88.7
46.8

dicates that without explicitly fusing features from multiple
scales, IANet could also cope with large scale changes.

CUHK03. In Tab. 2, we report the top-1 and top-5 accu-
racies on CUHK03. IAnet outperforms the state-of-the-arts.
It is noteworthy that there is a small gap between the labeled
evaluation and the detected evaluation of our method, which
indicates that our method is robust at the presence of the im-
perfect detection.

MSMT17. We further evaluate our method on a recent
large scale dataset, namely MSMT17. As shown in Tab. 3,
our method signiﬁcantly outperform existing works with
14.1% top-1 and 12.8% mAP. Since MSMT17 is the largest
dataset with more than 120, 000 images, this result strongly
demonstrates the superiority of our proposed method.

4.3. Ablation Study

In this section, we investigate the effectiveness of each
component in IA block by conducting a series of abla-
tion studies on Market-1501 and DukeMTMC datasets. We
adopt ResNet-50 [13] trained with cross-entropy loss as the
baseline (denoted as base.). If there is no special explana-
tion, we add the proposed blocks to the last residual block
(bottleneck) of stage3 layer of ResNet-50.

Multi-context combination. We ﬁrst compare the
single-context interaction operations (SA
K in Eq. 1) with dif-
ferent patch sizes K. In this part, SIA uses single-context
appearance relation map in the aggregation operation. As
shown in Tab. 4a, there is an improvement in performance

9322

Table 4. Ablations on Market-1501 and DukeMTMC datasets.

(a) Different Context: comparison on single-
context interaction operations of SIA with differ-
ent patch sizes K.

(b) Different fusion functions F : comparison on
different fusion functions in the multi-context inter-
action operation.

(c) Location Relation: comparison on different SIA
blocks that respectively use location, appearance and se-
mantic relation maps in the aggregation operation.

K

base.
K=1
K=2
K=3
K=5

Market-1501
top-1 mAP
90.4
76.2
79.0
91.8
79.0
93.1
78.9
92.2
91.1
77.9

DukeMTMC
top-1 mAP
82.1
66.0
68.8
83.6
68.8
84.1
68.7
83.8
83.5
67.3

F

base.
MAX
SUM
PROD

Market-1501
top-1 mAP
90.4
76.2
81.8
93.6
81.9
93.5
93.9
82.0

DukeMTMC
top-1 mAP
82.1
66.0
71.0
85.2
70.9
85.4
85.6
71.5

Model

base.
Location
Appearance
Semantic

Market-1501
top-1 mAP
90.4
76.2
79.2
92.1
82.0
93.9
94.1
82.5

DukeMTMC
top-1 mAP
82.1
66.0
68.5
83.7
71.5
85.6
85.9
72.2

(d) Combining methods of CIA and SIA blocks.

(e) Positions to place IA blocks: an IA block is
added into the bottlenecks of different stages.

(f) Bottleneck vs. Inside each Convolution Block:
stage23-c denotes IA blocks are inserted to each con-
volution block in stage2 and stage3 layers.

Combine

base.
CIA
SIA & CIA
CIA + SIA
SIA + CIA

Market-1501 DukeMTMC
top-1 mAP top-1 mAP
66.0
90.4
68.7
91.9
72.2
94.0
72.3
94.1
72.3
94.3

76.2
79.3
82.5
82.5
82.8

82.1
84.3
85.7
85.7
85.9

Model

base.
stage1
stage2
stage3
stage4

Market-1501 DukeMTMC
top-1 mAP top-1 mAP
66.0
90.4
71.5
93.7
94.4
71.8
72.3
94.3
69.3
92.3

76.2
81.8
82.8
82.8
79.6

82.1
85.3
86.5
85.9
84.8

Model

base.
stage2
stage3
stage23-c
stage23

Market-1501 DukeMTMC
top-1 mAP top-1 mAP
66.0
90.4
71.8
94.4
72.3
94.3
93.6
72.2
73.4
94.4

82.1
86.5
85.9
86.1
87.1

76.2
82.8
82.8
81.9
83.1

when K is increased, showing the effectiveness of incor-
porating contextual information. However, as K is further
increased, the accuracy drops gradually. So we only use
context patches with K of 1, 2 and 3 in the multi-context in-
teraction operation. We then explore three different fusion
functions F in the multi-context interaction operation (SA
in Eq. 2): element-wise maximum, summation, and prod-
uct. In this part, SIA uses multi-context appearance relation
map in the aggregation operation. Tab. 4b lists the com-
parison results of different fusion strategies. Element-wise
product performs better than other functions and is therefore
selected as the default fusion function.

We ﬁnally report the computation cost of SIA. For
single-context SIA (K=1), the relation map SA
1 could be
worked out by one matrix multiplication. The multi-context
SIA (K=1,2,3) does not incur extra multiplying operation
compared to single-context, as SA
1 has got the dot-product
between every two spatial position. Specially, baseline re-
quires ∼4.06 Multiply GFLOPs in a single forward pass for
a 256×128 pixel input image. Baseline added SIA requires
∼4.09 Multiply GFLOPS, corresponding only ∼ 0.73% rel-
ative increase over ResNet50.

Effectiveness of introducing location prior. In the lo-
cation relation map, the standard deviations σ1 and σ2 de-
termine the shape of Gaussian function. In order to reduce
the search space of hyper-parameters, we set σ2 to 2σ1 ac-
cording to the input aspect ratio 2:1. Fig. 7 shows the top-1
accuracy and mAP changes with σ1, where SIA uses spatial
location relation map in the aggregation operation. We can
see that the performance of SIA is not sensitive to σ1 with a

Figure 7. Parameter analysis for location relation map. (a) top-
1 accuracy changes with the standard deviation σ1.
(b) mAP
changes with the standard deviation σ1.

certain range of values.

We then investigate whether location relations can im-
prove the performance of SIA. Tab. 4c compares different
SIA blocks: Location, Appearance and Semantic that re-
spectively use location (SL), appearance (SA) and semantic
(S) relation maps in aggregation operation. As seen, Lo-
cation achieves better results than baseline, which demon-
strates the effectiveness of introducing location relations.
Besides, Semantic consistently outperforms Appearance,
showing that location relations complement appearance re-
lations, which can achieve more precise semantic relations.
Arrangement of CIA and SIA blocks. We ﬁrst ver-
ify the effectiveness of CIA block in Tab. 4d by adding it
to baseline. CIA outperforms baseline by ∼ 3% in mAP,
which implies that it is effective to enhance feature rep-
resentation power by aggregating similar channel features.
We then compare three different ways of arranging CIA
and SIA blocks: parallel, sequential channel-spatial, and se-
quential spatial-channel. As shown in Tab. 4d, the sequen-

9323

Table 5. Performance w.r.t. different backbones on Market-
1501 dataset.

model
top-1
mAP

R32
88.7
72.5

R32-IA R101 R101-IA

92.6
79.2

91.1
76.5

94.0
83.0

GN
83.4
64.0

GN-IA

87.1
69.7

tial spatial-channel produces the best performance. Note
that the results outperform adding CIA or SIA blocks inde-
pendently, showing that utilizing both blocks is crucial and
the best-arranging strategy further pushes performance.

Efﬁcient positions to place IA blocks. Table 4e com-
pares an IA block added to the bottlenecks of different
stages of ResNet. The improvements of an IA block in
stage2 and stage3 are similar, but smaller in stage1 and
stage4. Therefore, we only insert IA blocks into stage2
and stage3 layers. Finally, we empirically verify that the
bottlenecks are the effective positions to place IA blocks.
Recent studies [46, 15, 45] mainly focus on modiﬁcations
within the ’convolution blocks’ rather than the ’bottlenecks’.
Tab. 4f compares two different locations, where stage23
adds 2 IA blocks to the bottlenecks, while stage23-c adds
10 blocks to every residual block of stage2 and stage3 lay-
ers. We can clearly observe that placing the blocks at the
bottlenecks is more effective. We argue that too many IA
blocks are difﬁcult to optimize on small reID datasets.

Effectiveness of IA block across different backbones.
In order to further verify the validity of IA block, we
try another three backbones besides the ResNet-50, i.e.,
ResNet32 (R32), ResNet101 (R101), and GoogleNet (GN).
As shown in Tab. 5, our method (-IA) improves the perfor-
mance w.r.t. different backbones consistently.

4.4. Visualization for Pose and Scale Robustness

To verify whether SIA can adaptively localize body parts
under various poses and scales, we visualize the pixel-wise
receptive ﬁelds learned with SIA. Speciﬁcally, each speciﬁc
position has a corresponding sub-relative map. We deﬁne
the sub-relation maps with high relation values as valid re-
ceptive ﬁelds and highlight them. As shown in Fig. 8, for
each input image, we select ﬁve positions from head, torso,
belt/bag, legs and shoe and show their corresponding valid
receptive ﬁelds. We can clearly observe that SIA can adap-
tively localize the body parts and visual attributes under var-
ious poses and scales. For example, in Fig. 8 (a) and Fig. 8
(b), the person take on different poses. SIA could aggre-
gate the features of regions corresponding to the body parts
independently of the pose. In Fig. 8 (c), the person is in
different scales due to the detection errors. SIA could adap-
tively adjust the scales of receptive ﬁelds based on the scales
of body parts. In addition, the receptive ﬁelds of different
body parts in SIA have different shapes and scales, which is
superior to the ﬁxed geometric receptive ﬁeld in CNNs.

For CIA, it is hard to give comprehensive visualization

(cid:28431)a(cid:28432)

(cid:28431)b(cid:28432)

(cid:28431)c(cid:28432)

Image                Receptive Field                Channel Map 

Figure 8. Visualization results of SIA and CIA on Market-1501.
For each row, we show an input image, ﬁve receptive ﬁelds corre-
sponding to the points masked in the input image, and three chan-
nel maps from the output of CIA. The channel maps are from 5th,
11th (a) / 25th (b, c) and 121th channels, respectively.

about the relation maps directly. Instead, we show some ag-
gregated channels to see whether they highlight small body
parts or attribute areas. In Fig. 8, we display 5th, 11th (a) /
25th (b, c) and 121th channels. We ﬁnd that the responses
of speciﬁc body parts and attributes are noticeable after CIA
enhances. For example, 11th, 25th and 121th channel maps
respond to attribute belt, bag and shoes. In short, the visu-
alizations further demonstrate the necessity of modeling the
interdependencies between channels for improving feature
representation, especially for ﬁne-grained attributes.

5. Conclusion

In this paper, we propose SIA and CIA blocks to im-
prove the representational capacity of deep convolutional
networks. SIA models the interdependencies between spa-
tial features of convolutional feature maps. It can adaptively
localize the body parts under various poses and scales. CIA
models the interdependencies between channel features. It
can further enhance the feature representations especially
for small visual cues. Extensive experiments show that
IANet outperforms state-of-the-arts on three public person
reID datasets.

Acknowledgement This work is partially supported by Na-
tional Key R&D Program of China (No.2017YFA0700800),
Natural Science Foundation of China (NSFC): 61876171
and 61572465.

9324

References

[1] S. Bai, X. Bai, and Q. Tian. Scalable person re-identiﬁcation
on supervised smoothed manifold. In CVPR, pages 2530–
2539, 2017.

[2] S. Bak and P. Carr. One-shot metric learning for person re-

identiﬁcation. In CVPR, pages 2990–2999, 2017.

[3] R. M. Bolle, J. H. Connell, S. Pankanti, N. K. Ratha, and
A. W. Senior. The relation between the roc curve and the
cmc. In AUTOID, pages 15–20, 2005.

[4] X. Chang, T. M. Hospedales, and T. Xiang. Multi-level fac-
In CVPR, pages

torisation net for person re-identiﬁcation.
2109–2118, 2018.

[5] D. Chen, D. Xu, H. Li, N. Sebe, and X. Wang. Group
consistent similarity learning via deep crf for person re-
identiﬁcation. In CVPR, pages 8649–8658, 2018.

[6] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian
Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and
channel-wise attention in convolutional networks for image
captioning. In CVPR, pages 5659–5667, 2017.

[7] Y. Chen, X. Zhu, and S. Gong. Person re-identiﬁcation by
In ICCV, pages

deep learning multi-scale representations.
2590–2600, 2017.

[8] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng. Per-
son re-identiﬁcation by multi-channel parts-based cnn with
improved triplet loss function. In CVPR, pages 1335 – 1344,
2016.

[9] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.
Deformable convolutional networks. In ICCV, pages 764–
773, 2017.

[10] Y. Du, C. Yuan, B. Li, L. Zhao, Y. Li, and W. Hu. Interaction-
aware spatio-temporal pyramid attention networks for action
classiﬁcation. In ECCV, pages 373–389, 2018.

[11] R. Gens and P. M. Domingos. Deep symmetry networks. In

NIPS, pages 2537–2545, 2014.

[12] Y. Guo and N. M. Cheung. Efﬁcient and deep person re-
In CVPR, pages

identiﬁcation using multi-level similarity.
2335–2344, 2018.

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770 – 778, 2016.

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[15] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017.

[16] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang. Ad-
versarially occluded samples for person re-identiﬁcation. In
CVPR, pages 5098–5107, 2018.

[20] M. M. Kalayeh, E. Basaran, M. G¨okmen, M. E. Kamasak,
and M. Shah. Human semantic parsing for person re-
identiﬁcation. In CVPR, pages 1062–1071, 2018.

[21] A. Kanazawa, A. Sharma, and D. Jacobs. Locally scale-
arXiv preprint

invariant convolutional neural networks.
arXiv:1412.5104, 2014.

[22] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, pages 1725–1732, 2014.

[23] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[24] D. Li, X. Chen, Z. Zhang, and K. Huang. Learning deep
context-aware features over body and latent parts for person
re-identiﬁcation. In CVPR, pages 384–393, 2017.

[25] W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep ﬁlter
pairing neural network for person re-identiﬁcation. In 2014,
pages 152–159, CVPR.

[26] W. Li, X. Zhu, and S. Gong. Harmonious attention network
In 2018, pages 2285 – 2294,

for person re-identiﬁcation.
CVPR.

[27] J. Liu, Z. J. Zha, Q. Tian, D. Liu, T. Yao, Q. Ling, and T.
Mei. Multi-scale triplet cnn for person re-identiﬁcation. In
ACM, pages 192–196, 2016.

[28] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, J. Yan,
and X. Wang. Hydraplus-net: Attentive deep features for
pedestrian analysis. In ICCV, pages 350–359, 2017.

[29] Z. Liu, D. Wang, and H. Lu. Stepwise metric promotion for
unsupervised video person re-identiﬁcation. In 2017, pages
2429–2438, ICCV.

[30] D. G. Lowe. Object recognition from local scale-invariant

features. In ICCV, pages 1150–1157, 1999.

[31] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
In ECCV, pages 483 –

works for human pose estimation.
499, 2016.

[32] S. Paisitkriangkrai, C. Shen, and A. van den Hengel. Learn-
ing to rank in person re-identiﬁcation with metric ensembles.
In CVPR, pages 1846–1855, 2015.

[33] X. Qian, Y. Fu, W. Wang, T. Xiang, Y. Wu, Y. G. Jiang, and
X. Xue. Pose-normalized image generation for person re-
identiﬁcation. In ECCV, pages 650–667, 2018.

[34] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. Orb:
an efﬁcient alternative to sift or surf. In ICCV, pages 2564–
2571, 2011.

[35] M. S. Sarfraz, A. Schumann, A. Eberle, and R. Stiefelhagen.
A pose-sensitive embedding for person re-identiﬁcation with
expanded cross neighborhood re-ranking.
In CVPR, pages
420–429, 2018.

[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.

[36] Y. Shen, T. Xiao, H. Li, S. Yi, and X. Wang. End-to-end deep
kronecker-product matching for person re-identiﬁcation. In
CVPR, pages 6886–6895, 2018.

[18] M. Jaderberg, K. Simonyan, A. Zisserman,

and K.
Kavukcuoglu. Spatial transformer networks. In NIPS, pages
2017–2025, 2015.

[37] C. Song, Y. Huang, W. Ouyang, and L. Wang. Mask-guided
In

contrastive attention model for person reidentiﬁcation.
CVPR, pages 1179–1188, 2018.

[19] Y. Jeon and J. Kim. Active convolution: Learning the shape
In CVPR, pages

of convolution for image classiﬁcation.
4201–4209, 2017.

[38] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, and Q. Tian. Pose-
driven deep convolutional model for person re-identiﬁcation.
arXiv preprint arXiv:1709.08325, 2017.

9325

[39] Y. Sun, L. Zheng, W. Deng, and S. Wang. Svdnet for pedes-

trian retrieval. In ICCV, pages 3800–3808, 2017.

[40] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang. Beyond
part models: Person retrieval with reﬁned part pooling (and
a strong convolutional baseline). In ECCV, pages 480–496,
2018.

[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D.
Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Go-
ing deeper with convolutions. In CVPR, pages 1–9, 2015.

[42] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, and
X. Wang. Eliminating background-bias for robust person re-
identiﬁcation. In CVPR, pages 5794–5803, 2018.

[43] R. R. Varior, M. Haloi, and G. Wang. Gated siamese convo-
lutional neural network architecture for human reidentiﬁca-
tion. In ECCV, pages 791–808, 2016.

[44] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang. Mancs:
A multi-task attentional network with curriculum sampling
for person re-identiﬁcation.
In ECCV, pages 365 – 381,
2018.

[45] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classiﬁcation. arXiv
preprint arXiv:1704.06904, 2017, 2017.

[46] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural

networks. In CVPR, pages 7794–7803, 2018.

[47] Y. Wang, Z. Chen, F. Wu, and G. Wang.

identiﬁcation with cascaded pairwise convolutions.
CVPR, pages 1470–1478, 2018.

Person re-
In

[57] R. Yu, Z. Dou, S. Bai, Z. Zhang, Y. Xu, and X. Bai. Hard-
aware point-to-set deep metric for person re-identiﬁcation.
In ECCV, pages 188–204, 2018.

[58] S. Zhang, J. Yang, and B. Schiele. Occluded pedestrian de-
In CVPR, pages

tection through guided attention in cnns.
6995 – 7003, 2018.

[59] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang,
and X. Tang. Spindle net: Person re-identiﬁcation with hu-
man body region guided feature decomposition and fusion.
In CVPR, pages 1077–1085, 2017.

[60] L. Zhao, X. Li, J. Wang, and Y. Zhuang. Deeply-learned
part-aligned representations for person re-identiﬁcation. In
ICCV, pages 3239 – 3248, 2017.

[61] L. Zheng, Y. Huang, H. Lu, and Y. Yang. Pose invariant
embedding for deep person re-identiﬁcation. arXiv preprint
arXiv:1701.07732, 2017.

[62] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.
In ICCV,

Scalable person re-identiﬁcation: A benchmark.
pages 1116–1124, 2015.

[63] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.
In ICCV,

Scalable person re-identiﬁcation: A benchmark.
pages 1116–1124, 2015.

[64] Z. Zheng, L. Zheng, and Y. Yang. Unlabeled samples gener-
ated by gan improve the person re-identiﬁcation baseline in
vitro. In ICCV, pages 3754–3762, 2017.

[48] Y. Wang, L. Wang, Y. You, X. Zou, V. Chen, S. Li, G. Huang,
B. Hariharan, and K. Q. Weinberger. Resource aware person
re-identiﬁcation across multiple resolutions. In CVPR, pages
8042–8051, 2018.

[49] L. Wei, S. Zhang, W. Gao, and Q. Tian. Person trasfer gan
to bridge domain gap for person re-identiﬁcation. In CVPR,
pages 79–88, 2018.

[50] L. Wei, S. Zhang, H. Yao, W. Gao, and Q. Tian. Glad: global-
local-alignment descriptor for pedestrian retrieval. In ACM,
pages 420–428, 2017.

[51] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In
So Kweon. Cbam: Convolutional block attention module.
In ECCV, pages 3–19, 2018.

[52] D. E. Worrall, S. J. Garbin, D. Turmukhambetov, and G. J.
Brostow. Harmonic networks: Deep translation and rotation
equivariance. arXiv preprint arXiv:1612.04642, 2016.

[53] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep fea-
ture representations with domain guided dropout for person
re-identiﬁcation. In CVPR, pages 1249–1258, 2016.

[54] J. Xu, R. Zhao, F. Zhu, H. Wang, and W. Quyang. Attention-
aware compositional network for person re-identiﬁcation. In
CVPR, pages 2119–2128, 2018.

[55] C. Yanbei, Z. Xiatian, and G. Shaogang. Person reidentiﬁca-
tion by deep learning multi-scale representations. In ICCV,
pages 2590–2600, 2017.

[56] H. X. Yu, A. Wu, and W. S. Zhen. Cross-view asymmetric
metric learning for unsupervised person re-identiﬁcation. In
2017, pages 994–1002, ICCV.

9326

