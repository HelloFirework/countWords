MAGSAC: Marginalizing Sample Consensus

Daniel Barath12, Jiri Matas1, and Jana Noskova1

1 Centre for Machine Perception, Department of Cybernetics

Czech Technical University, Prague, Czech Republic

2 Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary

barath.daniel@sztaki.mta.hu

Abstract

A method called, σ-consensus, is proposed to elimi-
nate the need for a user-deﬁned inlier-outlier threshold in
RANSAC. Instead of estimating the noise σ, it is marginal-
ized over a range of noise scales. The optimized model is
obtained by weighted least-squares ﬁtting where the weights
come from the marginalization over σ of the point like-
lihoods of being inliers. A new quality function is pro-
posed not requiring σ and, thus, a set of inliers to deter-
mine the model quality. Also, a new termination criterion
for RANSAC is built on the proposed marginalization ap-
proach. Applying σ-consensus, MAGSAC is proposed with
no need for a user-deﬁned σ and improving the accuracy of
robust estimation signiﬁcantly. It is superior to the state-of-
the-art in terms of geometric accuracy on publicly available
real-world datasets for epipolar geometry (F and E) and
homography estimation. In addition, applying σ-consensus
only once as a post-processing step to the RANSAC output
always improved the model quality on a wide range of vi-
sion problems without noticeable deterioration in process-
ing time, adding a few milliseconds.1

1. Introduction

The RANSAC (RANdom SAmple Consensus) algo-
rithm proposed by Fischler and Bolles [5] in 1981 has be-
come the most widely used robust estimator in computer
vision. RANSAC and its variants have been successfully
applied to a wide range of vision tasks, e.g. motion seg-
mentation [25], short baseline stereo [25, 27], wide baseline
stereo matching [18, 13, 14], detection of geometric primi-
tives [21], image mosaicing [7], and to perform [28] or ini-
tialize multi-model ﬁtting [10, 17]. In brief, the RANSAC
approach repeatedly selects random subsets of the input
point set and ﬁts a model, e.g. a plane to three 3D points
or a homography to four 2D point correspondences. Next,

the quality of the estimated model is measured, for instance
by the size of its support, i.e. the number of inliers. Finally,
the model with the highest quality, polished e.g. by least
squares ﬁting on its inliers, is returned.

Since the publication of RANSAC, a number of modi-
ﬁcations has been proposed. NAPSAC [16], PROSAC [1]
and EVSAC [6] modify the sampling strategy to increase
the probability of selecting an all-inlier sample early.
NAPSAC assumes that the inliers are spatially coherent,
PROSAC exploits an a priori predicted inlier probability
of the points and EVSAC estimates a conﬁdence in each
of them. MLESAC [26] estimates the model quality by a
maximum likelihood process with all its beneﬁcial proper-
ties, albeit under certain assumptions about inlier and out-
lier distributions.
In practice, MLESAC results are often
superior to the inlier counting of plain RANSAC and they
are less sensitive to the user-deﬁned inlier-outlier threshold.
In MSAC [24], the robust estimation is formulated as a pro-
cess that estimates both the parameters of the data distribu-
tion and the quality of the model in terms of maximum a
posteriori. timates the model quality by a maximum likeli-
hood process with all its beneﬁcial properties, albeit under
certain assumptions about inlier and outlier distributions.

One of the highly attractive properties of RANSAC is
its small number of control parameters. The termination
is controlled by a manually set conﬁdence value η and the
sampling stops as soon as the probability of ﬁnding a model
with higher support falls below 1−η.2 The setting of η is not
problematic, the typical values are 0.95 or 0.99, depending
on the required conﬁdence in the solution.

The second, and most critical, parameter is the inlier
noise scale σ that determines the inlier-outlier threshold
τ (σ) which strongly inﬂuences the outcome of the proce-
In standard RANSAC and its variants, σ must be
dure.
provided by the user which limits its fully automatic out-
of-the-box use and requires the user to acquire knowledge
about the problem at hand. In Fig. 1, the inlier residuals

2Note that the probabilistic interpretation of η holds only for the stan-

1The source code is at https://github.com/danini/magsac

dard {0, 1} cost function.

110197

are shown for four real datasets demonstrating that σ varies
scene-by-scene and, thus, there is no single setting which
can be used for all cases.

To reduce the dependency on this threshold, MIN-
PRAN [22] assumes that the outliers are uniformly dis-
tributed and ﬁnds the model where the inliers are least likely
to have occurred randomly. Moisan et al. [15] proposed a
contrario RANSAC, to optimize each model by selecting
the most likely noise scale.

As the major contribution of this paper, we propose an
approach, σ-consensus, that eliminates the need for σ, the
noise scale parameter. Instead of σ, only an upper limit is
required. The ﬁnal outcome is obtained by weighted least-
squares ﬁtting, where the weights are given for marginal-
izing over σ, using likelihood of the model given data and
σ. Besides ﬁnessing the need for a precise scale param-
eter, the novel method, called MAGSAC, is more precise
than previously published RANSACs. Also, we propose
a post-processing step applying σ-consensus to the so-far-
the-best-model without noticeable deterioration in process-
ing time, i.e. at most a few milliseconds.
In our experi-
ments, the method always improved the input model (com-
ing from RANSAC, MSAC or LO-RANSAC) on a wide
range of problems. Thus we see no reason for not applying
it after the robust estimation ﬁnished. As a second contribu-
tion, we deﬁne a new quality function for RANSAC. It mea-
sures the quality of a model without requiring σ and, there-
fore, a set of inliers to measure the model quality. Moreover,
as a third contribution, due to not having a single inlier set
and, thus, an inlier ratio, the standard termination criterion
of RANSAC is marginalized over σ to be applicable to the
proposed method.

2. Notation

In this paper,

the input points are denoted as P =
{p | p ∈ Rk, k ∈ N>0}, where k is the dimension, e.g.
k = 2 for 2D points and k = 4 for point correspondences.
The inlier set is I ⊆ P. The model to ﬁt is represented
by its parameter vector θ ∈ Θ, where Θ = {θ | θ ∈
Rd, d ∈ N>0} is the manifold, for instance, of all possi-
ble 2D lines and d is dimension of the model, e.g. d = 2 for
2D lines (angle and offset). Fitting function F : P ∗ → Θ
calculates the model parameters from n ≥ m points, where
P ∗ = exp P is the power set of P and m ∈ N>0 is the min-
imum point number for ﬁtting a model, e.g. m = 2 for 2D
lines. Note that F is a combined function applying different
estimators on the basis of the input set, for example, a min-
imal method if n = m and least-squares ﬁtting otherwise.
Function D : Θ × P → R is the point-to-model residual
function. Function I : P ∗ × Θ × R → P ∗ selects the inliers
given model θ and threshold σ. For instance, if the origi-
nal RANSAC approach is considered, IRANSAC(θ, σ, P) =
{p ∈ P | D(θ, p) < σ}, for truncated quadratic dis-

(a) homogr dataset

(b) EVD dataset

(c) AdelaideRMF dataset

(d) kusvod2 dataset

Figure 1: The average residuals (RMSE in pixels; vertical
axis) of manually annotated inliers given the ground truth
model for each scene (horizontal) of four datasets.

- Set of data points
- Model parameters
- Inlier selector function
- Fitting function
- Inlier-outlier threshold

Notation
σ
D
Q
m - Minimal sample size
σmax

- Noise standard deviation
- Residual function
- Model quality function

- Upper bound of σ

P
θ
I
F

τ (σ)

tance of MSAC, IMSAC(θ, σ, P) = {p ∈ P | D2(θ, p) <
9/4σ2}. The quality function is Q : P ∗ × Θ × R → R.
Higher quality is interpreted as better model. For RANSAC,
QRANSAC(θ, σ, P) = |I(θ, σ, P)| and for MSAC, it is

QMSAC(θ, σ, P) =

|I(θ,σ,P)|

Xi=1

(cid:18)1 −

D2(θ, Ii(θ, σ, P))

9/4σ2

(cid:19) ,

where Ii(θ, σ, P) is the ith inlier.

3. Marginalizing sample consensus

A method called MAGSAC is proposed in this section
eliminating the threshold parameter from RANSAC-like ro-
bust model estimation.

3.1. Marginalization over σ

Let us assume the noise σ to be a random variable with
density function f (σ) and let us deﬁne a new quality func-
tion for model θ marginalizing over σ as follows:

Q∗(θ, P) = Z Q(θ, σ, P)f (σ)dσ.

(1)

210198

BostonBostonLibBruggeSquareBruggeTowerBrusselsCapitalRegionEiffelLePoint1LePoint2LePoint3WhiteBoardadamboatcitygraf00.10.20.30.40.50.6RMSE of inliers (in px)adamcafecatdumfacefoxgirlgrafgrandindexmagpkkshoptherevin00.20.40.60.8RMSE of inliers (in px)barrsmithbonhallbonythonelderhallaelderhallbhartleyjohnssonajohnssonbladysymonlibrarynapieranapierbneemneseoldclassicswingphysicsseneunihouseunionhouse00.10.20.30.4RMSE of inliersKyotobookshboxcastlecorrgraffhartleyheadkampaleafslibraryphysicsplantrotundashoutvalbonnewallwashzoom00.050.10.150.2RMSE of inliers (in px)Having no prior information, we assume σ being uniformly
distributed, σ ∼ U(0, σmax). Thus

Q∗(θ, P) =

1

σmax Z σmax

0

Q(θ, σ, P)dσ.

(2)

For instance, using Q(θ, σ, P) of plain RANSAC, i.e. the
number of inliers, where σ is the inlier-outlier threshold
and {D(θ, pi)}|P|
i=1 are the distances to model θ such that
0 ≤ D(θ, p1) < D(θ, p2) < .... < D(θ, pK) < σmax <
D(θ, pK+1) < ... < D(θ, p|P|) we get a quality function

Q∗(θ, P) = K−

1

σmax

K

Xk=1

D(θ, pk) =

K

Xk=1

(cid:18)1 −

D(θ, pk)

σmax (cid:19) .

Assuming the distribution of inliers and outliers to be
uniform (inlier ∼ U(0, σ); outlier ∼ U(0, l)) and using log-
likelihood of model θ as its quality function Q, we get

Q∗(θ, P) = K(ln

−

1

σmax

K

Xk=1

D(θ, pk)(1 + ln

l

D(θ, pk)

l

σmax

+ 1)

) − |P| ln l.

(3)

Typically, the residuals of the inliers are calculated as the
Eucledian-distance from the model in some ρ-dimensional
space.
In case of assuming errors of the distances along
each axis of this ρ-dimensional space to be indepen-
dent and normally distributed with the same variance σ2,
(residuals)2/σ2 have chi-squared distribution with ρ de-
grees of freedom. Therefore,

g(r | σ) = 2C(ρ)σ−ρ exp (−r2/2σ2)rρ−1

is a density of residuals of inliers with

C(ρ) =

1

2ρ/2Γ(ρ/2)

,

where

Γ(a) = Z +∞

0

ta−1 exp (−t)dt

for a > 0 is the gamma function.

In MAGSAC, the residuals of the inliers are described
by a distribution with density g(r | σ), and the outliers by
a uniform one on the interval [0, l]. Note that, for images, l
can be set to the image diagonal. The inlier-outlier thresh-
old τ (σ) is set to the 0.95 or 0.99 quantile of the distribu-
tion with density g(r | σ). Consequently, the likelihood of
model θ given σ is

L(θ, P | σ) =

1

l|P|−|I(σ)|

(cid:20)2C(ρ)σ−ρDρ−1(θ, p) exp(cid:18) −D2(θ, p)

2σ2

(cid:19)(cid:21) .

Yp∈I(σ)

(4)

0

1

σmax Z σmax
Xi=1

K

1

σmax

MAGSAC, for a given σ, uses log-likelihood of model θ as
its quality function as follows: Q(θ, σ, P) = ln L(θ, P|σ).
Thus, the quality marginalized over σ is the following.

Q∗

MAGSAC(θ, P) =

ln L(θ, P|σ)dσ

≈ −|P| ln l +

[i(ln 2C(ρ)l − ρ ln σi)

(5)

−

Ri
σ2
i

+ (ρ − 1)Lri](σi − σi−1),

where {D(θ, pi)}|P|
i=1 are the distances to model θ, σ0 = 0
and 0 ≤ D(θ, p1) = τ (σ1) < D(θ, p2) = τ (σ2) <
... < D(θ, pK) = τ (σK) < τ (σmax) < D(θ, pK+1) <
... < D(θ, p|P|), Ri = 1
j=1 D(θ, pj)2 and Lri =
j=1 ln D(θ, pj). As a consequence, the proposed new
MAGSAC does not depend on a manually

2 Pi

Pi

quality function Q∗
set noise level σ.

3.2. σ consensus model ﬁtting

Due to not having a set of inliers which could be used to
polish the model obtained from a minimal sample, we pro-
pose to use weighted least-squares ﬁtting where the weights
are the point probabilities of being inliers.

Suppose that we are given model θ estimated from a min-
imal sample. Let θσ = F (I(θ, σ, P)) be the model implied
by the inlier set I(θ, σ, P) selected using τ (σ) around the
input model θ. It can be seen from Eq. 4 that the likelihood
of point p ∈ P being inlier given model θσ is

L(p | θσ, σ) = 2C(ρ)σ−ρDρ−1(θσ, p) exp(cid:18) −D2(θσ, p)

2σ2

(cid:19) .

For ﬁnding the likelihood of a point being an inlier
marginalized over σ, the same approach is used as before:

L(p | θ) ≈

2C(ρ)
σmax

K

Xi=1

(σi − σi−1)

σ−ρ

i Dρ−1(θσi , p) exp(cid:18) −D2(θσi , p)

2σ2
i

(cid:19) .

(6)

and the polished model θ∗
MAGSAC is estimated using
weighted least-squares, where the weight of point p ∈ P
is L(p | θ).

3.3. Termination criterion

Not having an inlier set and, thus, at least a rough es-
timate of the inlier ratio, makes the standard termination
criterion of RANSAC [8] inapplicable, which is as follows:

k(θ, σ, P) =

ln(1 − η)

ln(cid:16)1 −(cid:16) |I(θ,σ,P)|

|P|

,

(cid:17)m(cid:17)

(7)

310199

where k is the iteration number, η a manually set conﬁdence
in the results, m the size of the minimal sample needed for
the estimation, and |I(θ, σ, P)| is the inlier number of the
so-far-the-best model.

In order to determine k without using a particular σ, it
is a straightforward choice to marginalize similarly to the
model quality. It is as follows:

k(θ, σ, P)dσ

(a) Homography; homogr dataset. Errors: ǫLO-MSC = 4.3
(2nd) and ǫMAGSAC = 2.9 pixels (1st).

k∗(P, θ) =

≈

1

σmax

0

1

σmax Z σmax
Xi=1

K

(σi − σi−1) ln(1 − η)

ln(cid:16)1 −(cid:16) |I(θ,σi,P)|

|P|

(cid:17)m(cid:17)

(8)

.

Thus the number of iterations required for MAGSAC is cal-
culated during the process and updated whenever a new so-
far-the-best model is found, similarly as in RANSAC.

4. Algorithms using σ-consensus

In this section, we propose two algorithms applying
σ-consensus. First, MAGSAC will be discussed incor-
porating the proposed marginalizing approach, weighted
least-squares and termination criterion. Second, a post-
processing step is proposed which is applicable to the output
of every robust estimator. In the experiments, it always im-
proved the input model without noticeable deterioration in
the processing time, adding maximum a few milliseconds.

4.1. Speeding up the procedure

Since plain MAGSAC would apply least-squares ﬁtting
a number of times, the implied computational complexity
would be fairly high. Therefore, we propose techniques for
speeding up the procedure. In order to avoid unnecessary
operations, we introduce a σmax value and use only the σs
smaller than σmax in the optimization procedure. Thus, from
σ1 < σ2 < ... < σK < σmax < σK+1 < ... < σn
only σ1, σ2, ..., and σi are used. This σmax can be set to a
fairly big value, for example, 10 pixels. In the case when
the results suggest that σmax is too low, e.g. if the density
mode of the residuals is close to σmax, the computation can
be repeated with a higher value.

Instead of calculating θσi for every σi, we divide the
range of σs uniformly into d partitions. Thus the pro-
cessed set of σs are the following: σ1 + (σmax − σ1)/d,
σ1 + 2(σmax − σ1)/d, ..., σ1 + (d − 1)(σmax − σ1)/d, σmax.
By this simpliﬁcation, the number of least-squares ﬁttings
drops to d from K, where d ≪ K. In the experiments, d
was set to 10.

Also, as it was proposed for USAC [19], there are several
ways of skipping early the evaluation of models which do
not have the chance of being better than the previous so-far-
the-best. For this purpose, we apply SPRT [2] with a τref
threshold. Threshold τref is not used in the model evaluation

(b) Homography; EVD dataset. Errors: ǫLO-RSC = 9.1 (2nd)
and ǫMAGSAC = 4.4 pixels (1st).

(c) Fundamental matrix; kusvod2 dataset. Errors: ǫMSC =
14.3 (2nd) and ǫMAGSAC = 0.5 pixels (1st).

(d) Essential matrix; Strecha dataset. Errors: ǫMSC = 4.2
(2nd) and ǫMAGSAC = 2.5 pixels (1st).

(e) Essential matrix; Strecha dataset. Errors: ǫMSC = 5.6
(2nd) and ǫMAGSAC = 3.9 pixels (1st).

Figure 2: Example results of MAGSAC where it was sig-
niﬁcantly more accurate than the second most accurate
method. Average errors (in pixels) are written in the cap-
tions. Inlier correspondences are drawn by color and out-
liers by black crosses.

or inlier selection steps, but is used merely to skip applying
σ-consensus when it is unnecessary. In the experiments, τref

410200

was set to 1 pixel.

Finally, the parallel implementation of σ-consensus can
be straightforwardly done on GPU or multiple CPUs evalu-
ating each σ on a different thread. In our C++ implementa-
tion, it runs on multiple CPU cores.

4.2. The σ consensus algorithm

The proposed σ-consensus is described in Alg. 1. The
input parameters are: the data points (P), initial model pa-
rameters (θ), a user-deﬁned partition number (d), and a limit
for σ (σmax).

As a ﬁrst step, the algorithm takes the points which are
closer to the initial model than τ (σmax) (line 1). Function
τ returns the threshold implied by the input σ parameter.
In case of χ2(4) distribution, it is τ (σ) = 3.64σ. Then
the residuals of the inliers are sorted, therefore, in {σi}|I|
i=1,
σi < σj ⇔ i < j. In Iord, the indices of the points are
ordered reﬂecting to {σi}|I|
i=1, thus σi = D(θ, Iord,i)/3.64
In lines 3 and 4, the weights are initialized to
(line 2).
zero, and σmax is set to max({σi}|I|
i=1). Then the current
σ range is calculated. For instance, the ﬁrst range to pro-
cess is [σ1, σ1 + δσ]. Note that σ1 = 0 due to having at least
m points at zero distance from the model. The cycle runs
from the ﬁrst to the last point and, since Iord is ordered, each
subsequent point is farther from the model than the previ-
ous ones. Until the end of the current range, i.e. partition,
is not reached (line 7), it collects the points (line 8) one-by-
one. After exceeding the boundary of the current range, θσ
is calculated using all the previously collected points (line
10). Then, for each point, the weight is updated by the im-
plied probability (line 12). Finally, the algorithm jumps to
the next range (line 13). After the weights have been calcu-
lated for each point, weighted least-squares ﬁtting is applied
to obtain the marginalized model parameters (line 14).

4.3. MAGSAC

The MAGSAC procedure polishing every estimated
model by σ-consensus is shown in Alg. 2. First, it initializes
the model quality to zero and the required iteration number
to ∞ (line 1). In each iteration, it selects a minimal sample
(line 3), ﬁts a model to the selected points (line 4) validates
it (line 5) and applies σ-consensus to obtain the parameters
marginalized over σ (line 6). The validation step includes
degeneracy testing and tests which stop the evaluation of the
model if there is no chance of being better than the previous
so-far-the-best, e.g. by SPRT test [2]. Note that, for SPRT,
the validation step is also included into σ-consensus when
the distances from the current model are calculated (line 1
in Alg. 1). Finally, the model quality is calculated (line 8),
the so-far-the-best model and required iteration number are
updated (line 10) if required (line 9). As a post-processing
step in time sensitive applications, σ-consensus is a pos-
sible option for polishing the RANSAC output instead of

applying a least-squares ﬁtting to the inliers. In this case, σ-
consensus is applied only once, thus improving the results
without noticeable deterioration in the processing time.

Algorithm 1 σ-consensus.

Input: P – points; θ – model parameters; d – partition

number; σmax – σ limit; η – conﬁdence
Output: θ∗ – optimal model parameters
1: I ← I(P, θ, τ (σmax))
2: Iord, {σi}|I|
3: {wi}|I|
4: δσ ← σmax/d, σnext ← δσ, Itmp ← ∅
5: for i = 1 → |Iord| do
6:

i=1 ← sort({D(θ, p)}p∈I)

i=1 ← {0}|I|

i=1, σmax ← max({σi}|I|

p ← Iord,i, dp ← D(θ, p)
if dp ≤ τ (σnext) then

i=1)

7:

8:

9:

10:

11:

12:

4:

5:

6:

7:

8:

9:

10:

Itmp ← Itmp ∪ {p}
continue

θσ ← F (Itmp)
for i = 1 → |I| do

wi ← wi + W (θσ, Ii, δσ)/σmax

⊲ Eq. 6

Itmp ← Itmp ∪ {p}, σnext ← σnext + δσ

13:
14: θ∗ ← F (I, {wi}|I|

i=1)

⊲ Weighted LSQ

Algorithm 2 MAGSAC

Input: P – data points; σmax – σ limit; σref – reference σ;
m – sample size; d – partition number; η – conﬁdence

Output: θ∗ – optimal model; q∗ – model quality
1: q∗ ← 0, k ← ∞
2: for i = 1 → k do
3:

j=1 ← Sample(P)

{pj}m
θ ← F ({pj}m
if ¬Validate(θ, σref) then

j=1)

continue

θ′ ← σ-consensus(P, θ, d, σmax)
q′ ← Q(θ′, P)
if q > q∗ then

⊲ Alg. 1

q∗, θ∗, k ← q′, θ′, Iters(q′, |P|, η)

⊲ Eq. 8

5. Experimental Results

To evaluate the proposed post-processing step, we
tested several approaches with and without
this step.
The compared algorithms are: RANSAC, MSAC, LO-
RANSAC, LO-MSAC, LO-RANSAAC [20], and a con-
trario RANSAC [15] (AC-RANSAC). LO-RANSAAC is a
method including model averaging into robust estimation.
AC-RANSAC estimates the noise σ. The same random
seed was used for all methods and they performed a ﬁnal
least-squares on the obtained inlier set. The difference be-
tween RANSAC – MSAC and LO-RANSAC – LO-MSAC

510201

is merely the quality function. Moreover, the methods with
LO preﬁx run the local optimization step proposed by Chum
et al. [3] with an inner RANSAC applied to the inliers. The
parameters used are as follows: σ = 0.3 was the inlier-
outlier threshold used for the RANSAC loop (this value was
proposed in [11] and also suited for us). The number of in-
ner RANSAC iterations was r = 20. The required conﬁ-
dence η was 0.95. There was a minimum number of iter-
ations required (set to 20) before the ﬁrst LO step applied
and also before termination. The reported error values are
the root mean square (RMS) errors. For σ-consensus, σmax
was set to 10 pixels for all problems. The partition of σ
range was set to d = 10. Therefore, the processed set of σs
were σmax/d, 2σmax/d, ..., (d − 1)σmax/d, and σmax.

5.1. Synthesized Tests

To test the proposed method in a fully controlled envi-
ronment, two cameras were generated by their 3 × 4 projec-
tion matrices P1 = K1[I3×3 | 0] and P2 = K2[R2 | − R2t2].
Camera P1 was located in the origin and its image plane was
parallel to plane XY. The position of the second camera was
at a random point inside a unit-sphere around the ﬁrst one,
thus |t2| ≤ 1. Its orientation was determined by three ran-
dom rotations affecting around the principal directions as
follows: R2 = RX,αRY,βRZ,γ , where RX,α, RY,β and RZ,γ
are 3D rotation matrices rotating around axes X, Y and Z, by
α, β and γ degrees, respectively (α, β, γ ∈ [0, π/2]). Both
cameras had a common intrinsic camera matrix with focal
length fx = fy = 600 and principal points [300, 300]T. A
3D plane was generated with random tangent directions and
origin [0, 0, 5]T. It was sampled at ni locations, thus gener-
ating ni 3D points at most one unit far from the plane origin.
These points were projected into the cameras. All of the
random parameters were selected using uniform distribu-
tion. Zero-mean Gaussian-noise with σ standard deviation
was added to the projected point coordinates. Finally, no
outliers, i.e. uniformly distributed random point correspon-
dences, were added. In total, 200 points were generated,
therefore ni + no = 200.

The mean results of 500 runs are reported in Fig. 3.
The competitor algorithms are: RANSAC (RSC), MSAC
(MSC), LO-RANSAC (LO-RSC), LO-MSAC (LO-MSC)
and MAGSAC. Sufﬁx ”+σ” means that σ-consensus was
applied as a post-processing step. Plots (a–c) reports the
geometric accuracy (in pixels) as a function of the noise
level σ using different outlier ratios (a – 0.2, b – 0.5, c –
0.8). The RANSAC conﬁdence was set to 0.95. For in-
stance, outlier ratio 0.8 means that no = 160 and ni = 40.
By looking at the differences between methods with and
without the proposed post-processing step (”+σ”), it can be
seen that it almost always improved the results. E.g. the ge-
ometric error of LO-MSC is higher than that of LO-MSC
+ σ for every noise σ. MAGSAC results are superior to

that of the competitor algorithms on every outlier ratio. It
can be seen that it is less sensitive to noise and more ro-
bust to outliers. In (d), the processing time (in seconds) is
reported as the function of the noise σ. MAGSAC is the
slowest on the easy scenes, i.e. when the noise σ < 0.3
pixels. Thereafter, it becomes the fastest method due to re-
quiring signiﬁcantly fewer iterations than the others. Plots
(e–f) of Fig. 3 demonstrate that the accuracy provided by
MAGSAC cannot be achieved by simply letting RANSAC
run longer. The charts report the results for a ﬁxed itera-
tion number, i.e. calculated from the ground truth inlier ra-
tio and conﬁdence set to 0.999. For outlier ratio 0.8, it was
log(0.001)/ log(1 − 0.24) = 4 314. For outlier ratio 0.9,
it was log(0.001)/ log(1 − 0.14) = 69 074. It can be seen
that MAGSAC obtains signiﬁcantly more accurate results
than the competitor algorithms. It ﬁnds the desired model
in most of the cases even when the outlier ratio is high.

5.2. Real World Experiments

In this section, MAGSAC and the proposed post-
processing step is compared with state-of-the-art robust es-
timators on real-world data for fundamental matrix, homog-
raphy and essential matrix ﬁtting. See Fig. 2 for exam-
ple image pairs where the error (ǫMAGSAC; in pixels) of the
MAGSAC estimate was signiﬁcantly lower than that of the
second best method.
Fundamental Matrices. To evaluate the performance on
fundamental matrix estimation we downloaded kusvod23
(24 pairs), Multi-H4 (5 pairs), and AdelaideRMF5 (19
pairs) datasets.
Kusvod2 consists of 24 image pairs
of different sizes with point correspondences and funda-
mental matrices estimated from manually selected inliers.
AdelaideRMF and Multi-H consist a total of 24 image
pairs with point correspondences, each assigned manually
to a homography or the outlier class. All points which are
assigned to a homography were considered as inliers and the
others as outliers. In total, 48 image pairs were used from
three publicly available datasets. All methods applied the
seven-point method [8] as a minimal solver for estimating
F. Thus they drew minimal sets of size seven in each itera-
tion. For the ﬁnal least squares ﬁtting, the normalized eight-
point algorithm [9] was ran on the obtained inlier set. Note
that all fundamental matrices were discarded for which the
oriented epipolar constraint [4] did not hold.

The ﬁrst three blocks of Table 1, each consisting of three
rows, report the quality of the estimation on each dataset
as the average of 100 runs on every image pair. The ﬁrst
two columns show the name of the tests and the investi-
gated properties: (1) eavg is the RMS geometric error in pix-
els of the obtained model w.r.t. the manually annotated in-

3http://cmp.felk.cvut.cz/data/geometry2view/
4http://web.eee.sztaki.hu/˜dbarath/
5cs.adelaide.edu.au/˜hwong/doku.php?id=data

610202

(a) 20% outl., 95% conf.

(b) 50% outl., 95% conf.

(c) 80% outl., 95% conf.

(d) 95% conf.

(e) 80% outl., 4 314 iters.

(f) 90% outl., 69 074 iters.

Figure 3: Synthetic homography ﬁtting. The competitor methods are: RANSAC, MSAC, LO-RANSAC, LO-MSAC and
MAGSAC. Sufﬁx ”+σ” means that σ-consensus was applied to the output. Plots (a–c) report the errors (in pixels) as function
of the noise σ with conﬁdence set to 0.95. Plot (d) shows the avg. processing time (in seconds). Plots (e–f) report the results
made by using a ﬁxed iteration number calculated from the ground truth inlier ratio and conﬁdence set to 0.999.

liers. For fundamental matrices and homographies, it is the
average Sampson distance and re-projection error, respec-
tively. For essential matrices, it is the mean Sampson dis-
tance of the implied F and the correspondences. (2) Value t
is the mean processing time in milliseconds. (3) Value s is
the mean number of samples, i.e. RANSAC iterations, had
to be drawn till termination. Note that the iteration num-
bers of methods applied with or without the proposed post-
processing are equal.

It can be seen that for F estimation the proposed post-
processing step improved the results in nearly all of the tests
with negligible deterioration in the processing time. The er-
rors were reduced by approximately 8% compared with the
methods without σ-consensus. MAGSAC led to the most
accurate results for kusvod2 and Multi-H datasets and it
was the third best for AdelaideRMF dataset by a small mar-
gin of 0.03 pixels.
Homographies. To test homography estimation we down-
loaded homogr (16 pairs) and EVD6 (15 pairs) datasets. Each
consists of image pairs of different sizes from 329 × 278
up to 1712 × 1712 with point correspondences and inliers
selected manually. The Homogr dataset consists of mostly
short baseline stereo images, whilst the pairs of EVD un-
dergo an extreme view change, i.e. wide baseline or ex-

6http://cmp.felk.cvut.cz/wbs/

treme zoom. All algorithms applied the normalized four-
point algorithm [8] for homography estimation both in the
model generation and local optimization steps. The 4th and
5th blocks of Fig. 1 show the mean results computed us-
ing all the image pairs of each dataset. Similarly as for F
estimation, the proposed post-processing step always im-
proved (by 1.42 pixels on average). For both datasets, the
results obtained by MAGSAC were signiﬁcantly more ac-
curate than what the competitor algorithms obtained.

Essential Matrices. To estimate essential matrices, we
used the strecha dataset [23] consisting of image se-
quences of buildings. All images are of size 3072 × 2048.
The ground truth projection matrices are provided. The
methods were applied to all possible image pairs in each
sequence. The SIFT detector [12] was used to obtain corre-
spondences. For each image pair, a reference point set with
ground truth inliers was obtained by calculating F from the
projection matrices [8]. Correspondences were considered
as inliers if the symmetric epipolar distance was smaller
than 1.0 pixel. All image pairs with less than 50 inliers
found were discarded. In total, 467 image pairs were used
in the evaluation. The results are reported in the 6th block of
Table 1. The trend is similar to the previous cases. The most
accurate essential matrices were obtained by MAGSAC.
Also it was the fastest algorithm on average.

710203

00.10.20.30.40.50.60.70.80.9Noise (px)00.511.522.533.5Error (px)RSCRSC + MSCMSC + LO-RSCLO-RSC + LO-MSCLO-MSC + MAGSAC00.10.20.30.40.50.60.70.80.9Noise (px)00.511.522.533.544.5Error (px)RSCRSC + MSCMSC + LO-RSCLO-RSC + LO-MSCLO-MSC + MAGSAC00.10.20.30.40.50.60.70.80.9Noise (px)01020304050607080Error (px)RSCRSC + MSCMSC + LO-RSCLO-RSC + LO-MSCLO-MSC + MAGSAC00.10.20.30.40.50.60.70.80.9Noise (px)00.511.522.533.5Processing time (secs)RSCRSC + MSCMSC + LO-RSCLO-RSC + LO-MSCLO-MSC + MAGSAC00.10.20.30.40.50.60.70.80.9Noise (px)024681012141618Error (px)RSCRSC + MSCMSC + LO-RSCLO-RSC + LO-MSCLO-MSC + MAGSAC00.10.20.30.40.50.60.70.80.9Noise (px)020406080100120140160180200Error (px)RSCRSC + MSCMSC + LO-RSCLO-RSC + LO-MSCLO-MSC + MAGSAC4
2

,

F

9
1

,

F

4

,

F

6
1

,

H

5
1

,

H

7
6
4

,

E

2
d
o
v
s
u
k

e
d
i
a
l
e
d
A

H
-
i
t
l
u
M

r
g
o
m
o
h

D
V
E

a
h
c
e
r
t
s

l
l
a

eavg
t
s

fails
eavg
t
s

fails
eavg
t
s

fails
eavg
t
s

fails
eavg
t
s

fails
eavg
t
s

fails
eavg
emed

t

fails

RSC
0.73
38
661
0.06
0.58
491
3 327
0.00
0.70
321
1 987
0.00
3.61
83
1 815
0.12
5.73
381
6 212
0.57
7.05
3 046
3 530
0.24

3.07
2.17
727
0.19

+ σ
0.60
39
661
0.06
0.54
493
3 327
0.00
0.59
329
1 987
0.00
2.12
85
1 815
0.12
4.08
383
6 212
0.50
6.91
3 052
3 530
0.22

2.47
1.36
730
0.16

MSC
0.75
19
313
0.06
0.66
420
2 752
0.00
0.84
149
908
0.00
3.64
64
1 395
0.12
5.15
379
6 106
0.57
7.32
2 894
3 315
0.26

3.04
2.24
654
0.17

+ σ
0.64
19
313
0.06
0.63
420
2 752
0.00
0.75
149
908
0.00
2.18
65
1 395
0.12
3.57
380
6 106
0.43
7.13
2 894
3 315
0.22

2.48
1.47
655
0.14

LO-RSC
0.56
25
316
0.06
0.28
393
2 221
0.00
0.53
132
580
0.00
3.39
71
1 478
0.12
5.42
367
5 847
0.57
9.61
2 548
2 789
0.27

3.30
1.98
589
0.18

+ σ
0.52
25
316
0.06
0.27
394
2 221
0.00
0.52
140
580
0.00
2.13
72
1 478
0.12
4.07
369
5 847
0.50
9.48
2 549
2 789
0.22

2.83
1.33
592
0.15

LO-MSC
0.58
17
160
0.06
0.31
380
2 091
0.00
0.50
119
327
0.00
3.53
64
1 222
0.12
4.78
353
5 540
0.57
10.62
2 535
2 770
0.26

3.39
2.06
578
0.16

+ σ
0.50
17
160
0.06
0.31
380
2 091
0.00
0.50
128
327
0.00
2.19
65
1 222
0.12
3.55
356
5 540
0.43
10.23
2 537
2 770
0.22

2.88
1.35
581
0.14

LO-RSAAC AC-RSC MAGSAC
0.38
31
382
0.00
0.30
939
2 638
0.00
0.47
467
1 324
0.00
1.37
131
877
0.06
1.76
162
2 239
0.29
6.51
2 398
2 183
0.00

1.01
17
160
0.06
0.33
380
2 091
0.00
0.58
126
327
0.00
2.95
65
1 222
0.12
4.55
355
5 540
0.53
10.17
2 536
2 770
0.24

0.63
55
71
0.06
0.46
447
2 047
0.00
0.72
46
23
0.00
1.83
37
148
0.00
5.05
291
3 463
0.33
15.56
4 637
3 680
0.23

3.27
1.98
580
0.16

4.04
1.28
921
0.10

1.80
0.92
688
0.03

Table 1: Accuracy of robust estimators on two-view geometric estimation. Fundamental matrix estimation (F) on kusvod2
(24 pairs), AdelaideRMF (19 pairs) and Multi-H (4 pairs) datasets, homography estimation (H) on homogr (16 pairs) and
EVD (15 pairs) datasets, and essential matrix estimation (E) on the strecha dataset (467 pairs). In total, the testing included
545 image pairs. The datasets, the problem, the number of the image pairs (#) and the reported properties are shown in the
ﬁrst three columns. The other columns show the average results (100 runs on each image pair) of the competitor methods
at 95% conﬁdence. Columns with ”+σ” show the results when the proposed σ-consensus was applied to the output of the
method on its left. The mean geometric error (eavg; in pixels) of the estimated model w.r.t. the manually selected inliers are
written in each 1st row; the mean processing time (t, in milliseconds) and the required number of samples (s) are written in
every 2nd and 3rd rows. In the 4th one, the proportion of failures, i.e. when the sough model is not found, is shown. The
geometric error is the RMS Sampson distance for F and E, and the RMS re-projection error for H using the ground truth
inlier set. The thresholds proposed in [11] were used. For MAGSAC, σmax = 10 pixels.

6. Conclusion

A robust approach, called σ-consensus, was proposed
for eliminating the need of a user-deﬁned threshold by
marginalizing over a range of noise scales. Also, due to not
having a set of inliers, a new model quality function and ter-
mination criterion were proposed. Applying σ-consensus,
we proposed two methods: ﬁrst, MAGSAC applying σ-
consensus to each of the models estimated from a mini-
mal sample. The method is superior to the state-of-the-art
in terms of geometric accuracy on publicly available real-
world datasets for epipolar geometry (both F and E) and ho-
mography estimation. The method is often faster than other
RANSAC variants in case of high outlier ratio. The pro-

posed post-processing step applies σ-consensus only once:
to polish the RANSAC output. The method nearly always
improved the model quality on a wide range of vision prob-
lems without noticeable deterioration in processing time,
i.e. at most a few milliseconds. We see no reason for not
applying it after the robust estimation ﬁnished.

7. Acknowledgement

This work was supported by the OP VVV project
CZ.02.1.01/0.0/0.0/16019/000076 Research Center for In-
formatics, by the Czech Science Foundation grant GA18-
05360S, and by the Hungarian Scientic Research Fund (No.
NKFIH OTKA KH-126513).

810204

References

[1] O. Chum and J. Matas. Matching with PROSAC-progressive
sample consensus. In Computer Vision and Pattern Recogni-
tion. IEEE, 2005. 1

[2] Ondˇrej Chum and Jiˇr´ı Matas.

randomized
RANSAC. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 30(8):1472–1482, 2008. 4, 5

Optimal

[3] O. Chum, J. Matas, and J. Kittler.

Locally optimized
In Joint Pattern Recognition Symposium.

RANSAC.
Springer, 2003. 6

[4] O. Chum, T. Werner, and J. Matas. Epipolar geometry es-
timation via RANSAC beneﬁts from the oriented epipolar
constraint. In International Conference on Pattern Recogni-
tion, 2004. 6

[5] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 1981. 1

[6] V. Fragoso, P. Sen, S. Rodriguez, and M. Turk. EVSAC:
accelerating hypotheses generation by modeling matching
scores with extreme value theory. In International Confer-
ence on Computer Vision, 2013. 1

[7] D. Ghosh and N. Kaabouch. A survey on image mosaick-
ing techniques. Journal of Visual Communication and Image
Representation, 2016. 1

[8] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge university press, 2003. 3, 6,
7

[9] R. I. Hartley. In defense of the eight-point algorithm. Trans-
actions on Pattern Analysis and Machine Intelligence, 1997.
6

[10] H. Isack and Y. Boykov. Energy-based geometric multi-
International Journal of Computer Vision,

model ﬁtting.
2012. 1

[11] K. Lebeda, J. Matas, and O. Chum. Fixing the locally op-
In British Machine Vision Conference.

timized RANSAC.
Citeseer, 2012. 6, 8

[12] D. G. Lowe. Object recognition from local scale-invariant
features. In International Conference on Computer vision.
IEEE, 1999. 7

[13] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-
baseline stereo from maximally stable extremal regions. Im-
age and Vision Computing, 2004. 1

[14] D. Mishkin, J. Matas, and M. Perdoch. MODS: Fast and
robust method for two-view matching. Computer Vision and
Image Understanding, 2015. 1

[15] Lionel Moisan, Pierre Moulon, and Pascal Monasse. Auto-
matic homographic registration of a pair of images, with a
contrario elimination of outliers. Image Processing On Line,
2:56–73, 2012. 2, 5

[16] D. Nasuto and J. M. B. R. Craddock. NAPSAC: High noise,
high dimensional robust estimation - its in the bag. 2002. 1

[17] T. T. Pham, T-J. Chin, K. Schindler, and D. Suter. Interacting
geometric priors for robust multimodel ﬁtting. Transactions
on Image Processing, 2014. 1

[18] P. Pritchett and A. Zisserman. Wide baseline stereo match-
ing. In International Conference on Computer Vision. IEEE,
1998. 1

[19] R. Raguram, O. Chum, M. Pollefeys, J. Matas, and J-M.
Frahm. USAC: a universal framework for random sample
consensus. Transactions on Pattern Analysis and Machine
Intelligence, 2013. 4

[20] Martin Rais, Gabriele Facciolo, Enric Meinhardt-Llopis,
Jean-Michel Morel, Antoni Buades, and Bartomeu Coll. Ac-
curate motion estimation through random sample aggregated
consensus. CoRR, abs/1701.05268, 2017. 5

[21] C. Sminchisescu, D. Metaxas, and S. Dickinson. Incremental
model-based estimation using geometric constraints. Pattern
Analysis and Machine Intelligence, 2005. 1

[22] Charles V. Stewart. Minpran: A new robust estimator for
computer vision. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 17(10):925–938, 1995. 2

[23] C. Strecha, R. Fransens, and L. Van Gool. Wide-baseline
stereo from multiple views: a probabilistic account. In Con-
ference on Computer Vision and Pattern Recognition. IEEE,
2004. 7

[24] P. H. S. Torr. Bayesian model estimation and selection for
Interna-

epipolar geometry and generic manifold ﬁtting.
tional Journal of Computer Vision, 50(1):35–61, 2002. 1

[25] P. H. S. Torr and D. W. Murray. Outlier detection and mo-
tion segmentation. In Optical Tools for Manufacturing and
Advanced Automation. International Society for Optics and
Photonics, 1993. 1

[26] P. H. S. Torr and A. Zisserman. MLESAC: A new robust esti-
mator with application to estimating image geometry. Com-
puter Vision and Image Understanding, 2000. 1

[27] P. H. S. Torr, A. Zisserman, and S. J. Maybank. Robust detec-
tion of degenerate conﬁgurations while estimating the funda-
mental matrix. Computer Vision and Image Understanding,
1998. 1

[28] M. Zuliani, C. S. Kenney, and B. S. Manjunath. The multi-
ransac algorithm and its application to detect planar homo-
graphies. In International Conference on Image Processing.
IEEE, 2005. 1

910205

