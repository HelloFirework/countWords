Animating Arbitrary Objects via Deep Motion Transfer

Aliaksandr Siarohin1, St´ephane Lathuili`ere1, Sergey Tulyakov2, Elisa Ricci1

,

3 and Nicu Sebe1

4

,

1DISI, University of Trento, Italy, 2 Snap Inc., Santa Monica, CA,

3 Fondazione Bruno Kessler (FBK), Trento, Italy,
4Huawei Technologies Ireland, Dublin, Ireland

{aliaksandr.siarohin,stephane.lathuilire,e.ricci,niculae.sebe}@unitn.it, stulyakov@snap.com

Abstract

This paper introduces a novel deep learning framework
for image animation. Given an input image with a target
object and a driving video sequence depicting a moving
object, our framework generates a video in which the tar-
get object is animated according to the driving sequence.
This is achieved through a deep architecture that decou-
ples appearance and motion information. Our framework
consists of three main modules: (i) a Keypoint Detector un-
supervisely trained to extract object keypoints, (ii) a Dense
Motion prediction network for generating dense heatmaps
from sparse keypoints, in order to better encode motion
information and (iii) a Motion Transfer Network, which
uses the motion heatmaps and appearance information ex-
tracted from the input image to synthesize the output frames.
We demonstrate the effectiveness of our method on several
benchmark datasets, spanning a wide variety of object ap-
pearances, and show that our approach outperforms state-
of-the-art image animation and video generation methods.
Our source code is publicly available 1.

1. Introduction

This paper introduces a framework for motion-driven
image animation to automatically generate videos by com-
bining the appearance information derived from a source
image (e.g. depicting the face or the body silhouette of a
certain person) with motion patterns extracted from a driv-
ing video (e.g. encoding the facial expressions or the body
movements of another person). Several examples are given
in Fig. 1. Generating high-quality videos from static images
is challenging, as it requires learning an appropriate repre-
sentation of an object, such as a 3D model of a face or a hu-
man body. This task also requires accurately extracting the
motion patterns from the driving video and mapping them
on the object representation. Most approaches are object-
speciﬁc, using techniques from computer graphics [7, 38].
These methods also use an explicit object representation,

1 https://github.com/AliaksandrSiarohin/monkey-net

Figure 1: Our deep motion transfer approach can animate
arbitrary objects following the motion of the driving video.

such as a 3D morphable model [5], to facilitate animation,
and therefore only consider faces.

Over the past few years, researchers have developed ap-
proaches for automatic synthesis and enhancement of visual
data. Several methods derived from Generative Adversarial
Networks (GAN) [16] and Variational Autoencoders (VAE)
[24] have been proposed to generate images and videos
[19, 32, 30, 39, 29, 37, 36, 33]. These approaches use addi-
tional information such as conditioning labels (e.g. indicat-
ing a facial expression, a body pose) [45, 31, 15, 35]. More
speciﬁcally, they are purely data-driven, leveraging a large
collection of training data to learn a latent representation
of the visual inputs for synthesis. Noting the signiﬁcant
progress of these techniques, recent research studies have
started exploring the use of deep generative models for im-
age animation and video retargeting [46, 9, 4, 43, 3]. These
works demonstrate that deep models can effectively trans-
fer motion patterns between human subjects in videos [4],
or transfer a facial expression from one person to another
[46]. However, these approaches have limitations: for ex-
ample, they rely on pre-trained models for extracting object
representations that require costly ground-truth data anno-
tations [9, 43, 3]. Furthermore, these works do not address
the problem of animating arbitrary objects: instead, consid-

43212377

ering a single object category [46] or learning to translate
videos from one speciﬁc domain to another [4, 22].

This paper addresses some of these limitations by intro-
ducing a novel deep learning framework for animating a
Inspired by [46], we
static image using a driving video.
propose learning a latent representation of an object cate-
gory in a self-supervised way, leveraging a large collection
of video sequences. There are two key distinctions between
our work and [46]. Firstly, our approach is not designed
for speciﬁc object category, but rather is effective in ani-
mating arbitrary objects. Secondly, we introduce a novel
strategy to model and transfer motion information, using a
set of sparse motion-speciﬁc keypoints that were learned in
an unsupervised way to describe relative pixel movements.
Our intuition is that only relevant motion patterns (derived
from the driving video) must be transferred for object an-
imation, while other information should not be used. We
call the proposed deep framework Monkey-Net, as it en-
ables motion transfer by considering MOviNg KEYpoints.

We demonstrate the effectiveness of our framework by
conducting an extensive experimental evaluation on three
publicly available datasets, previously used for video gener-
ation: the Tai-Chi [39], the BAIR robot pushing [11] and the
UvA-NEMO Smile [10] datasets. As shown in our exper-
iments, our image animation method produces high qual-
ity videos for a wide range of objects. Furthermore, our
quantitative results clearly show that our approach outper-
forms state-of-the-art methods for image-to-video transla-
tion tasks.

2. Related work

Deep Video Generation. Early deep learning-based ap-
proaches for video generation proposed synthesizing videos
by using spatio-temporal networks. Vondrick et al. [42] in-
troduced VGAN, a 3D convolutional GAN which simulta-
neously generates all the frames of the target video. Sim-
[30] proposed TGAN, a GAN-based
ilarly, Saito et al.
model which is able to generate multiple frames at the same
time. However, the visual quality of these methods outputs
is typically poor.

More recent video generation approaches used recur-
rent neural networks within an adversarial training frame-
work. For instance, Wang et al. [45] introduced a Condi-
tional MultiMode Network (CMM-Net), a deep architec-
ture which adopts a conditional Long-Short Term Mem-
ory (LSTM) network and a VAE to generate face videos.
Tulyakov et al. [39] proposed MoCoGAN, a deep architec-
ture based on a recurrent neural network trained with an ad-
versarial learning scheme. These approaches can take con-
ditional information as input that comprises categorical la-
bels or static images and, as a result, produces high quality
video frames of desired actions.

Video generation is closely related to the future frame
prediction problem addressed in [34, 26, 13, 40, 48]. Given
a video sequence, these methods aim to synthesize a se-
quence of images which represents a coherent continuation
of the given video. Earlier methods [34, 26, 23] attempted to
directly predict the raw pixel values in future frames. Other
approaches [13, 40, 2] proposed learning the transforma-
tions which map the pixels in the given frames to the future
frames. Recently, Villegas et al. [41] introduced a hierarchi-
cal video prediction model consisting of two stages: it ﬁrst
predicts the motion of a set of landmarks using an LSTM,
then generates images from the landmarks.

Our approach is closely related to these previous works
since we also aim to generate video sequences by using
a deep learning architecture. However, we tackle a more
challenging task: image animation requires decoupling and
modeling motion and content information, as well as a re-
combining them.

Object Animation. Over the years, the problems of im-
age animation and video re-targeting have attracted atten-
tion from many researchers in the ﬁelds of computer vision,
computer graphics and multimedia. Traditional approaches
[7, 38] are designed for speciﬁc domains, as they operate
only on faces, human silhouettes, etc. In this case, an ex-
plicit representation of the object of interest is required to
generate an animated face corresponding to a certain per-
son’s appearance, but with the facial expressions of another.
For instance, 3D morphable models [5] have been tradi-
tionally used for face animation [49]. While especially ac-
curate, these methods are highly domain-speciﬁc and their
performance drastically degrades in challenging situations,
such as in the presence of occlusions.

Image animation from a driving video can be interpreted
as the problem of transferring motion information from one
domain to another. Bansal et al. [4] proposed Recycle-
GAN, an approach which extends conditional GAN by
incorporating spatio-temporal cues in order to generate a
video in one domain given a video in another domain. How-
ever, their approach only learns the association between two
speciﬁc domains, while we want to animate an image de-
picting one object without knowing at training time which
object will be used in the driving video. Similarly, Chan
et al. [9] addressed the problem of motion transfer, cast-
ing it within a per-frame image-to-image translation frame-
work. They also proposed incorporating spatio-temporal
constraints. The importance of considering temporal dy-
namics for video synthesis was also demonstrated in [43].
[46] introduced X2Face, a deep architecture
Wiles et al.
which, given an input image of a face, modiﬁes it according
to the motion patterns derived from another face or another
modality, such as audio. They demonstrated that a purely
data-driven deep learning-based approach is effective in an-

43222378

imating still images of faces without demanding explicit 3D
representation.
In this work, we design a self-supervised
deep network for animating static images, which is effec-
tive for generating arbitrary objects.

3. Monkey-Net

The architecture of the Monkey-Net is given in Fig. 2.

We now describe it in detail.

3.1. Overview and Motivation

The objective of this work is to animate an object based
on the motion of a similar object in a driving video. Our
framework is articulated into three main modules (Fig. 2).
The ﬁrst network, named Keypoint Detector, takes as input
the source image and a frame from the driving video and
automatically extracts sparse keypoints. The output of this
module is then fed to a Dense Motion prediction network,
which translates the sparse keypoints into motion heatmaps.
The third module, the Motion Transfer network, receives as
input the source image and the dense motion heatmap and
recombines them producing a target frame.

The output video is generated frame-by-frame as illus-
trated in Fig. 2.a. At time t, the Monkey-Net uses the source
image and the tth frame from the driving video. In order to
train a Monkey-Net one just needs a dataset consisting of
videos of objects of interest. No speciﬁc labels, such as
keypoint annotations, are required. The learning process
is fully self-supervised. Therefore, at test time, in order
to generate a video sequence, the generator requires only a
static input image and a motion descriptor from the driving
sequence. Inspired by recent studies on unsupervised land-
mark discovery for learning image representations [20, 47],
we formulate the problem of learning a motion representa-
tion as an unsupervised motion-speciﬁc keypoint detection
task. Indeed, the keypoints locations differences between
two frames can be seen as a compact motion representation.
In this way, our model generates a video by modifying the
input image according to the landmarks extracted from the
driving frames. Using a Monkey-Net at inference time is
detailed in Sec. 3.6.

The Monkey-Net architecture is illustrated in Fig. 2.b.
Let x and x′ ∈ X be two frames of size H × W extracted
from the same video. The H × W lattice is denoted by
U . Inspired by [20], we jointly learn a keypoint detector ∆
together with a generator network G according to the fol-
lowing objective: G should be able to reconstruct x′ from
the keypoint locations ∆(x) ∈ U , ∆(x′) ∈ U , and x. In
this formulation, the motion between x and x′ is implicitly
modeled. To deal with large motions, we aim to learn key-
points that describe motion as well as the object geometry.
To this end, we add a third network M that estimates the
optical ﬂow F ∈ RH×W ×2 between x′ and x from ∆(x),

∆(x′) and x. The motivation for this is twofold. First,
this forces the keypoint detector ∆ to predict keypoint lo-
cations that capture not only the object structure but also its
motion. To do so, the learned keypoints must be located es-
pecially on the object parts with high probability of motion.
For instance, considering the human body, it is important
to obtain keypoints on the extremities (as in feet or hands)
in order to describe the body movements correctly, since
these body-parts tend to move the most. Second, following
common practises in conditional image generation, the gen-
erator G is implemented as an encoder-decoder composed
of convolutional blocks [19]. However, standard convolu-
tional encoder-decoders are not designed to handle large
pixel-to-pixel misalignment between the input and output
images [31, 3, 14]. To this aim, we introduce a deformation
module within the generator G that employs the estimated
optical ﬂow F in order to align the encoder features with x′.

3.2. Unsupervised Keypoint Detection

In this section, we detail the structure employed for un-
supervised keypoint detection. First, we employ a standard
U-Net architecture that, from the input image, estimates K
heatmaps Hk ∈ [0, 1]H×W , one for each keypoint. We em-
ploy softmax activations for the last layer of the decoder in
order to obtain heatmaps that can be interpreted as detection
conﬁdence map for each keypoint. An encoder-decoder ar-
chitecture is used here since it has shown good performance
for keypoints localization [6, 27].

To model the keypoint location conﬁdence, we ﬁt a
Gaussian on each detection conﬁdence map. Modeling the
landmark location by a Gaussian instead of using directly
the complete heatmap Hk acts as a bottle-neck layer, and
therefore allows the model to learn landmarks in an indirect
way. The expected keypoint coordinates hk ∈ R and its
covariance Σk are estimated according to:

hk = X

Hk[p]p; Σk = X

Hk[p](p−hk)(p−hk)⊤ (1)

p∈U

p∈U

The intuition behind the use of keypoint covariances is that
they can capture not only the location of a keypoint but also
its orientation. Again considering the example of the hu-
man body: in the case of the legs, the covariance may cap-
ture their orientation. Finally, we encode the keypoint dis-
tributions as heatmaps H i
k ∈ [0, 1]H×W , such that they can
be used as inputs to the generator and to the motion net-
works. Indeed, the advantage of using a heatmap represen-
tation, rather than considering directly the 2D coordinates
hk, is that heatmaps are compatible with the use of convolu-
tional neural networks. Formally, we employ the following
Gaussian-like function:

∀p ∈ U , Hk(p) =

1
α

exp (cid:0)−(p − hk)Σ−1

k (p − hk)(cid:1) (2)

43232379

Figure 2: A schematic representation of the proposed motion transfer framework for image animation. At testing time (Fig.
(a)), the model generates a video with the object appearance of the source image but with the motion from the driving video.
Monkey-Net (Fig. (b)) is composed of three networks: a motion-speciﬁc keypoint detector ∆, a motion prediction network
M and an image generator G. G reconstructs the image x′ from the keypoint positions ∆(x) and ∆(x′). The optical ﬂow
computed by M is used by G to handle misalignments between x and x′. The model is learned with a self-supervised
learning scheme.

where α is normalization constant. This process is applied
independently on x and x′ leading to two sets of K key-
points heatmaps H = {Hk}k=1..K and H ′ = {H ′
k}k=1..K .

3.3. Generator Network with Deformation Module

In this section, we detail how we reconstruct the tar-
get frame x′ from x, ∆(x) = H and ∆(x′) = H ′.
First we employ a standard convolutional encoder com-
posed of a sequence of convolutions and average pooling
layers in order to encode the object appearance in x. Let
ξr ∈ RHr ×Wr ×Cr denote the output of the rth block of the
encoder network (1 ≤ r ≤ R). The architecture of this
generator network is also based on the U-Net architecture
[28] in order to obtain better details in the generated im-
age. Motivated by [31], where it was shown that a standard
U-net cannot handle large pixel-to-pixel misalignment be-
tween the input and the output images, we propose using
a deformation module to align the features of the encoder
with the output images. Contrary to [31] that deﬁnes an
afﬁne transformation for each human body part in order to
compute the feature deformation, we propose a deformation
module that can be used on any object. In particular, we
propose employing the optical ﬂow F to align the features
ξr with x′. The deformation employs a warping function
fw(·, ·) that warps the feature maps according to F :

ξ′
r = fw(ξr, F)

(3)

This warping operation is implemented using a bilinear
sampler, resulting in a fully differentiable model. Note that
F is down-sampled to Hr ×Wr via nearest neighbour inter-
polation when computing Eq. (3). Nevertheless, because of
the small receptive ﬁeld of the bilinear sampling layer, en-
coding the motion only via the deformation module leads to

Figure 3: A schematic representation of the adopted part-
based model for optical ﬂow estimation from sparse repre-
sentation. From the appearance of the ﬁrst frame and the
keypoints motion, the network M predicts a mask for each
keypoint and the residual motion (see text for details).

optimization problems. In order to facilitate network train-
ing, we propose inputing the decoder the difference of the
keypoint locations encoded as heatmaps ˙H = H ′ − H. In-
deed, by providing ˙H to the decoder, the reconstruction loss
applied on the G outputs (see Sec. 3.5) is directly prop-
agated to the keypoint detector ∆ without going through
M .
In addition, the advantage of the heatmap difference
representation is that it encodes both the locations and the
motions of the keypoints. Similarly to F , we compute R
tensors ˙Hr by down-sampling ˙H to Hr × Wr. The two ten-
sors ˙Hr and ξ′
r are concatenated along the channel axis and
are then treated as skip-connection tensors by the decoder.

3.4. From Sparse Keypoints to Dense Optical Flow

In this section, we detail how we estimate the optical
ﬂow F . The task of predicting a dense optical ﬂow only
from the displacement of a few keypoints and the appear-
ance of the ﬁrst frame is challenging. In order to facilitate

43242380

-the task of the network, we adopt a part base formulation.
We make the assumption that each keypoint is located on an
object part that is locally rigid. Thus, the task of computing
the optical ﬂow becomes simpler since, now, the problem
consists in estimating masks Mk ∈ RH×W that segment
the object in rigid parts corresponding to each keypoint. A
ﬁrst coarse estimation of the optical ﬂow can be given by:

Fcoarse =

K+1

X

k=1

Mk ⊗ ρ(hk)

(4)

where ⊗ denotes the element-wise product and ρ(·) ∈
RH×W ×2 is the operator that returns a tensor by repeat-
ing the input vector H × W times. Additionally, we em-
ploy one speciﬁc mask MK+1 without deformation (which
corresponds to ρ([0, 0])) to capture the static background.
In addition to the masks Mk, the motion network M also
predicts the residual motion Fresidual. The purpose of this
residual motion ﬁeld is to reﬁne the coarse estimation by
predicting non-rigid motion that cannot be modeled by the
part-based approach. The ﬁnal estimated optical ﬂow is:
F = Fcoarse + Fresidual.

Concerning the inputs of the motion network, M takes
˙H and x corresponding respectively to the
two tensors,
sparse motion and the appearance. However, we can ob-
serve that, similarly to the generator network, M may suffer
from the misalignment between the input x and the output
F . Indeed, F is aligned with x′. To handle this problem, we
use the warping operator fw according to the motion ﬁeld
of each keypoint ρ(hk), e.g. xk = fw(x, ρ(hk)). This so-
lution provides images xk that are locally aligned with F in
the neighborhood of h′
k. Finally, we concatenate H ′ − H,
{xk}k=1..K and x along the channel axis and feed them
into a standard U-Net network. Similarly to the keypoint
and the generator network, the use of U-Net architecture is
motivated by the need of ﬁne-grained details.

3.5. Network Training

We propose training the whole network in an end-to-end
fashion. As formulated in Sec. 3.1, our loss ensures that
x′ is correctly reconstructed from ∆(x) ∈ U , ∆(x′) ∈ U
and x. Following the recent advances in image generation,
we combine an adversarial and the feature matching loss
proposed in [44] in order to learn to reconstruct x′. More
precisely, we use a discriminator network D that takes as
input H ′ concatenated with either the real image x′ or the
generated image ˆx′. We employ the least-square GAN for-
mulation [25] leading to the two following losses used to
train the discriminator and the generator:

LD

gan(D) =Ex

′∈X [(D(x′ ⊕ H ′) − 1)2]

+ E(x,x

′)∈X 2 [D(ˆx′ ⊕ H ′))2]

LG

gan(G) =E(x,x

′)∈X 2 [(D(ˆx′ ⊕ H ′) − 1)2]

(5)

where ⊕ denotes the concatenation along the channel axis.
Note that in Eq (5), the dependence on the trained param-
eters of G, M , and ∆ appears implicitly via ˆx′. Note that
we provide the keypoint locations H ′ to the discriminator to
help it to focus on moving parts and not on the background.
However, when updating the generator, we do not propa-
gate the discriminator loss gradient through H ′ to avoid that
the generator tends to fool the discriminator by generating
meaningless keypoints.

The GAN loss is combined with a feature matching loss
that encourages the output image ˆx′ and x′ to have simi-
lar feature representations. The feature representations em-
ployed to compute this loss are the intermediate layers of
the discriminator D. The feature matching loss is given by:

Lrec = E(x,x

′) (cid:2)kDi(ˆx′ ⊕ H ′) − Di(x′ ⊕ H ′))k1(cid:3)

(6)

where Di denotes the ith-layer feature extractor of the dis-
criminator D. D0 denotes the discriminator input. The
main advantage of the feature matching loss is that, dif-
ferently from other perceptual losses, [31, 21], it does not
require the use of an external pre-trained network. Finally
the overall loss is obtained by combining Eqs. (6) and (5),
Ltot = λrecLrec + LG
gan. In all our experiments, we chose
λrec = 10 following [44]. Additional details of our imple-
mentation are shown in the Supplementary Material A.

3.6. Generation Procedure

At test time, our network receives a driving video and a
source image. In order to generate the tth frame, ∆ esti-
mates the keypoint locations hs
k in the source image. Sim-
ilarly, we estimate the keypoint locations h1
k from
ﬁrst and the tth frames of the driving video. Rather than
generating a video from the absolute positions of the key-
points, the source image keypoints are transferred according
to the relative difference between keypoints in the video.
The keypoints in the generated frame are given by:

k and ht

hs
k

′ = hs

k + (ht

k − h1
k)

(7)

′ and hs

The keypoints hs
k are then encoded as heatmaps
k
using the covariance matrices estimated from the driving
video, as described in Sec. 3.2. Finally, the heatmaps are
given to the dense motion and the generator networks to-
gether with the source image (see Secs. 3.3 and 3.4). Im-
portantly, one limitation of transferring relative motion is
that it cannot be applied to arbitrary source images. Indeed,
if the driving video object is not roughly aligned with the
source image object, Eq. (7) may lead to absolute keypoint
positions that are physically impossible for the considered
object as illustrated in Supplementary Material C.1.

43252381

Tai-Chi

L1

(AKD, MKR) AED

L1

Nemo
AKD AED

Bair
L1

order to provide an in-depth comparison with other meth-
ods. We employ the following metrics.

X2Face

Ours

0.068
0.050

(4.50, 35.7%)
(2.53, 17.4%)

0.27
0.21

0.022
0.017

0.47
0.37

0.140
0.072

0.069
0.025

Table 1: Video reconstruction comparisons

4. Experiments

In this section, we present a in-depth evaluation on three
problems, tested on three very different datasets and em-
ploying a large variety of metrics.

Datasets. The UvA-Nemo dataset [10] is a facial dynam-
ics analysis dataset composed of 1240 videos We follow
the same pre-processing as in [45]. Speciﬁcally, faces are
aligned using the OpenFace library [1] before re-sizing each
frame to 64×64 pixels. Each video starts from a neutral ex-
pression and lasts 32 frames. As in [45], we use 1110 videos
for training and 124 for evaluation.

The Tai-Chi dataset [39] is composed of 4500 tai-chi
video clips downloaded from YouTube. We use the data
as pre-processed in [39]. In particular, the frames are re-
sized to 64 × 64 pixels. The videos are split into 3288 and
822 videos for training and testing respectively. The video
length varies from 32 to 100 frames.

The BAIR robot pushing dataset [11] contains videos col-
lected by a Sawyer robotic arm pushing a variety of objects
over a table. It contains 40960 training and 256 test videos.
Each video is 64 × 64 pixels and has 30 frames.

Evaluation Protocol. Evaluating the results of image an-
imation methods is a difﬁcult task, since ground truth an-
imations are not available. In addition, to the best of our
knowledge, X2Face [46] is the only previous approach for
data-driven model-free image animation. For these two rea-
sons, we evaluate our method also on two closely related
tasks. As proposed in [46], we ﬁrst evaluate Monkey-Net
on the task of video reconstruction. This consists in recon-
structing the input video from a representation in which mo-
tion and content are decoupled. This task is a “proxy” task
to image animation and it is only introduced for the purpose
of quantitative comparison. In our case, we combine the ex-
tracted keypoints ∆(x) of each frame and the ﬁrst frame of
the video to re-generate the input video. Second, we evalu-
ate our approach on the problem of Image-to-Video transla-
tion. Introduced in [42], this problem consists of generating
a video from its ﬁrst frame. Since our model is not directly
designed for this task, we train a small recurrent neural net-
work that predicts, from the keypoint coordinates in the ﬁrst
frame, the sequence of keypoint coordinates for the other 32
frames. Additional details can be found in the Supplemen-
tary Material A. Finally, we evaluate our model on image
animation. In all experiments we use K=10.

Metrics. In our experiments, we adopt several metrics in

• L1. In the case of the video reconstruction task where the
ground truth video is available, we compare the average L1
distance between pixel values of the ground truth and the
generated video frames.
• AKD. For the Tai-Chi and Nemo datasets, we employ ex-
ternal keypoint detectors in order to evaluate whether the
motion of the generated video matches the ground truth
video motion. For the Tai-Chi dataset, we employ the
human-pose estimator in [8]. For the Nemo dataset we use
the facial landmark detector of [6]. We compute these key-
points for each frame of the ground truth and the generated
videos. From these externally computed keypoints, we de-
duce the Average Keypoint Distance (AKD), i.e. the average
distance between the detected keypoints of the ground truth
and the generated video.
• MKR. In the case of the Tai-Chi dataset, the human-pose
estimator returns also a binary label for each keypoint in-
dicating whether the keypoints were successfully detected.
Therefore, we also report the Missing Keypoint Rate (MKR)
that is the percentage of keypoints that are detected in the
ground truth frame but not in the generated one. This metric
evaluates the appearance quality of each video frame.
• AED. We compute the feature-based metric employed in
[12] that consists in computing the Average Euclidean Dis-
tance (AED) between a feature representation of the ground
truth and the generated video frames. The feature embed-
ding is chosen such that the metric evaluates how well the
identity is preserved. More precisely, we use a network
trained for facial identiﬁcation [1] for Nemo and a network
trained for person re-id [17] for Tai-Chi.
• FID. When dealing with Image-to-video translation, we
complete our evaluation with the Frechet Inception Dis-
tance [18] (FID) in order to evaluate the quality of indi-
vidual frames.

Furthermore, we conduct a user study for both the Image-
to-Video translation and the image animation tasks (see
Sec. 4.3).

4.1. Ablation Study

In this section, we present an ablation study to empiri-
cally measure the impact of each part of our proposal on the
performance. First, we describe the methods obtained by
“amputating” key parts of the model described in Sec. 3.1:
(i) No F - the dense optical ﬂow network M is not used; (ii)
No Fcoarse - in the optical ﬂow network M , we do not use
the part based-approach; (iii) No Fresidual - in the Optical
Flow network M , we do not use Fresidual; (iv) No Σk - we
do not estimate the covariance matrices ΣK in the keypoint
detector ∆ and the variance is set to Σk = 0.01 as in [20];
(v) the source image is not given to the motion network M ,

43262382

Tai-Chi

L1

(AKD, MKR) AED

No F

No Fresidual
No Fcoarse

No Σk
No x
Full

0.057
0.051
0.052
0.054
0.051
0.050

(3.11, 23.8%)
(2.81, 18.0%)
(2.75, 19.7%)
(2.86, 20.6%)
(2.71, 19.3%)
(2.53, 17.4%)

0.24
0.22
0.22
0.23
0.21
0.21

Table 2: Video reconstruction ablation study TaiChi.

Real x′

No F

No
Fcoarse

Full

Figure 4: Qualitative ablation evaluation of video recon-
struction on Tai-Chi.

M estimates the dense optical ﬂow only from the keypoint
location differences; (vi) Full denotes the full model as de-
scribed in Sec. 3.

In Tab. 2, we report the quantitative evaluation. We
ﬁrst observe that our full model outperforms the baseline
method without deformation. This trend is observed ac-
cording to all the metrics. This illustrates the beneﬁt of
deforming the features maps according to the estimated mo-
tion. Moreover, we note that No Fcoarse and No Fresidual
both perform worse than when using the full optical ﬂow
network. This illustrates that Fcoarse and Fresidual alone
are not able to estimate dense motion accurately. A possible
explanation is that Fcoarse cannot estimate non rigid mo-
tions and that Fresidual, on the other hand, fails in predicting
the optical ﬂow in the presence of large motion. The quali-
tative results shown in Fig. 4 conﬁrm this analysis. Further-
more, we observe a drop in performance when covariance
matrices are replaced with static diagonal matrices. This
shows the beneﬁt of encoding more information when deal-
ing with videos with complex and large motion, as in the
case of the TaiChi dataset. Finally, we observe that if the
appearance is not provided to the deformation network M ,
the video reconstruction performance is slightly lower.

4.2. Comparison with Previous Works

Tai-Chi

FID

AED MKR

54.83
19.75

0.27
0.17

46.2%
30.3%

Bair

MoCoGAN [39]

Ours

Nemo

MoCoGAN [39]
CMM-Net [45]

Ours

FID

AED

51.50
27.27
11.97

0.33
0.13
0.12

MoCoGAN [39]

SV2P [2]

Ours

FID

244.00
57.90
23.20

Table 3: Image-to-video translation comparisons.

proposal. Note that this comparison can be done since we
employ image and motion representation of similar dimen-
sion. In our case, each video frame is reconstructed from
the source image and 10 landmarks, each one represented
by 5 numbers (two for the location and three for the sym-
metric covariance matrix), leading to a motion representa-
tion of dimension 50. For X2face, motion is encoded into
a driving vector of dimension 128. The quantitative com-
parison is reported in Tab. 1. Our approach outperforms
X2face, according the all the metrics and on all the eval-
uated datasets. This conﬁrms that encoding motion via
motion-speciﬁc keypoints leads to a compact but rich rep-
resentation.

Image-to-Video Translation:

In Tab. 3 we compare
with the state of the art Image-to-Video translation methods:
two unsupervised methods MoCoGAN [39] and SV2P [2],
and CMM-Net which is based on keypoints [45]. CMM-
Net is evaluated only on Nemo since it requires facial land-
marks. We report results SV2P on the Bair dataset as in
[2]. We can observe that our method clearly outperforms
the three methods for all the metrics. This quantitative eval-
uation is conﬁrmed by the qualitative evaluation presented
in the Supplementary material C.3. In the case of MoCo-
GAN, we observe that the AED score is much higher than
the two other methods. Since AED measures how well the
identity is preserved, these results conﬁrm that, despite the
realism of the video generated by MoCoGAN, the identity
and the person-speciﬁc details are not well preserved. A
possible explanation is that MoCoGAN is based on a fea-
ture embedding in a vector, which does not capture spatial
information as well as the keypoints. The method in [45]
initially produces a realistic video and preserves the iden-
tity, but the lower performance can be explained by the ap-
parition of visual artifacts in the presence of large motion
(see the Supplementary material C.3 for visual examples).
Conversely, our method both preserves the person identity
and performs well even under large spatial deformations.

Video Reconstruction. First, we compare our results
with the X2Face model [46] that is closely related to our

Image Animation. In Fig. 5, we compare our method with
X2Face [46] on the Nemo dataset. We note that our method

43272383

Figure 5: Qualitative results for image animation on the Nemo dataset: X2face (2-nd row) against our method (3-rd row).

Figure 6: Qualitative results for image animation on the Tai-Chi dataset: X2face (2-nd row) against our method (3-rd row)

Tai-Chi

Nemo

Bair

85.0% 79.2% 90.8%

Table 4: User study results on image animation. Proportion
of times our approach is preferred over X2face [46].

generates more realistic smiles on the three randomly se-
lected samples despite the fact that the XFace model is
speciﬁcally designed for faces. Moreover, the beneﬁt of
transferring the relative motion over absolute locations can
be clearly observed in Fig. 5 (column 2). When abso-
lute locations are transferred, the source image inherits the
face proportion from the driving video, resulting in a face
In Fig. 6, we compare our method
with larger cheeks.
with X2Face on the Tai-Chi dataset. X2Face [46] fails to
consider each body-part independently and, consequently,
warps the body in such a way that its center of mass matches
the center of mass in the driving video. Conversely, our
method successfully generates plausible motion sequences
that match the driving videos. Concerning the Bair dataset,
exemplar videos are shown in the Supplementary material
C.3. The results are well in line with those obtained on the
two other datasets.

4.3. User Evaluation

In order to further consolidate the quantitative and qual-
itative evaluations, we performed user studies for both the
Image-to-Video translation (see the Supplementary Mate-
rial C.3) and the image animation problems using Amazon
Mechanical Turk.

tocol: we randomly select 50 pairs of videos where ob-
jects in the ﬁrst frame have a similar pose. Three videos
are shown to the user: one is the driving video (reference)
and 2 videos from our method and X2Face. The users are
given the following instructions: Select the video that bet-
ter corresponds to the animation in the reference video. We
collected annotations for each video from 10 different users
The results are presented in Tab. 4. Our generated videos
are preferred over X2Face videos in almost more than 80%
of the times for all the datasets. Again, we observe that
the preference toward our approach is higher on the two
datasets which correspond to large motion patterns.

5. Conclusion

We introduced a novel deep learning approach for image
animation. Via the use of motion-speciﬁc keypoints, pre-
viously learned following a self-supervised approach, our
model can animate images of arbitrary objects according
to the motion given by a driving video. Our experiments,
considering both automatically computed metrics and hu-
man judgments, demonstrate that the proposed method out-
performs previous work on unsupervised image animation.
Moreover, we show that with little adaptation our method
can perform Image-to-Video translation. In future work, we
plan to extend our framework to handle multiple objects and
investigate other strategies for motion embedding.

Acknowledgments

For the image animation problem, our model is again
compared with X2face [46] according to the following pro-

This work was carried out under the “Vision and Learning
joint Laboratory” between FBK and UNITN.

43282384

imageSourceDrivingvideoimageSourceDrivingvideoimageSourceDrivingvideoimageSourceDrivingvideoimageSourceDrivingvideoimageSourceDrivingvideoReferences

[1] Brandon Amos, Bartosz Ludwiczuk, and Mahadev Satya-
narayanan. Openface: A general-purpose face recognition.
2016.

[2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,
Roy H Campbell, and Sergey Levine. Stochastic variational
video prediction. In ICLR, 2017.

[3] Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Du-
rand, and John Guttag. Synthesizing images of humans in
unseen poses. In CVPR, 2018.

[4] Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser
In

Sheikh. Recycle-gan: Unsupervised video retargeting.
ECCV, 2018.

[5] Volker Blanz and Thomas Vetter. A morphable model for the

synthesis of 3d faces. In SIGGRAPH, 1999.

[6] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2d & 3d face alignment problem? (and a
dataset of 230,000 3d facial landmarks). In ICCV, 2017.

[7] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic
expression regression for real-time facial tracking and ani-
mation. TOG, 2014.

[8] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017.

[9] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A

Efros. Everybody dance now. In ECCV, 2018.

[10] Hamdi Dibeklio˘glu, Albert Ali Salah, and Theo Gevers. Are
you really smiling at me? spontaneous versus posed enjoy-
ment smiles. In ECCV, 2012.

[11] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey
Levine. Self-supervised visual planning with temporal skip
connections. In CoRL, 2017.

[12] Patrick Esser, Ekaterina Sutter, and Bj¨orn Ommer. A varia-
tional u-net for conditional appearance and shape generation.
In CVPR, 2018.

[13] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsuper-
vised learning for physical interaction through video predic-
tion. In NIPS, 2016.

[14] Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, and
Victor Lempitsky. Deepwarp: Photorealistic image resyn-
thesis for gaze manipulation. In ECCV, 2016.

[15] Zhenglin Geng, Chen Cao, and Sergey Tulyakov. 3d guided

ﬁne-grained face manipulation. In CVPR, 2019.

[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
In
loss for person re-identiﬁcation.

[17] Alexander Hermans, Lucas Beyer, and Bastian Leibe.

defense of the triplet
arXiv:1703.07737, 2017.

[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NIPS.

[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017.

[20] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea
Vedaldi. Unsupervised learning of object landmarks through
conditional image generation. In NIPS, 2018.

[21] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016.

[22] Donggyu Joo, Doyeon Kim, and Junmo Kim. Generating a
fusion image: One’s identity and another’s shape. In CVPR,
2018.

[23] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan,
Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray
Kavukcuoglu. Video pixel networks. In ICML, 2016.

[24] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. In ICLR, 2014.

[25] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In ICCV.

[26] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis,
and Satinder Singh. Action-conditional video prediction us-
ing deep networks in atari games. In NIPS, 2015.

[27] Joseph P Robinson, Yuncheng Li, Ning Zhang, Yun Fu, et al.

Laplace landmark localization. arXiv:1903.11633, 2019.

[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI. Springer, 2015.

[29] Subhankar Roy, Enver Sangineto, Nicu Sebe, and Beg¨um
Demir. Semantic-fusion gans for semi-supervised satellite
image classiﬁcation. In ICIP, 2018.

[30] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Tempo-
ral generative adversarial nets with singular value clipping.
In ICCV, 2017.

[31] Aliaksandr Siarohin, Enver Sangineto, St´ephane Lathuili`ere,
and Nicu Sebe. Deformable gans for pose-based human im-
age generation. In CVPR, 2018.

[32] Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe.
Whitening and coloring transform for GANs. In ICLR, 2019.
[33] Aliaksandr Siarohin, Gloria Zen, Nicu Sebe, and Elisa Ricci.
Enhancing perceptual attributes with bayesian style genera-
tion. In ACCV, 2018.

[34] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-
nov. Unsupervised learning of video representations using
lstms. In ICML, 2015.

[35] Hao Tang, Wei Wang, Dan Xu, Yan Yan, and Nicu Sebe.
Gesturegan for hand gesture-to-gesture translation in the
wild. In ACM MM, 2018.

[36] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso,
and Yan Yan. Multi-channel attention selection gan with cas-
caded semantic guidance for cross-view image translation. In
CVPR, 2019.

[37] Hao Tang, Dan Xu, Wei Wang, Yan Yan, and Nicu Sebe.
Dual generator generative adversarial networks for multi-
domain image-to-image translation. In ACCV, 2019.

[38] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Face2face: Real-time
face capture and reenactment of rgb videos. In CVPR, 2016.
[39] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for
video generation. In CVPR, 2018.

43292385

[40] Joost Van Amersfoort, Anitha Kannan, Marc’Aurelio Ran-
zato, Arthur Szlam, Du Tran, and Soumith Chintala.
Transformation-based models of video sequences.
arXiv
preprint arXiv:1701.08435, 2017.

[41] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn,
Xunyu Lin, and Honglak Lee. Learning to generate long-
term future via hierarchical prediction. In ICML, 2017.

[42] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.

Generating videos with scene dynamics. In NIPS, 2016.

[43] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
video synthesis. In NIPS, 2018.

[44] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
CVPR, 2017.

[45] Wei Wang, Xavier Alameda-Pineda, Dan Xu, Pascal Fua,
Elisa Ricci, and Nicu Sebe.
Every smile is unique:
Landmark-guided diverse smile generation. In CVPR, 2018.
[46] Olivia Wiles, A Sophia Koepke, and Andrew Zisserman.
X2face: A network for controlling face generation using im-
ages, audio, and pose codes. In ECCV, 2018.

[47] Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He,
and Honglak Lee. Unsupervised discovery of object land-
marks as structural representations. In CVPR, 2018.

[48] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim-
itris Metaxas. Learning to forecast and reﬁne residual motion
for image-to-video generation. In ECCV.

[49] Michael Zollh¨ofer, Justus Thies, Pablo Garrido, Derek
Bradley, Thabo Beeler, Patrick P´erez, Marc Stamminger,
Matthias Nießner, and Christian Theobalt. State of the art
on monocular 3d face reconstruction, tracking, and applica-
tions. In Computer Graphics Forum.

43302386

