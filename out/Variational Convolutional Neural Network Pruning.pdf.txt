Variational Convolutional Neural Network Pruning

Chenglong Zhao1∗ Bingbing Ni1∗†

Jian Zhang1∗ Qiwei Zhao1 Wenjun Zhang1 Qi Tian2

1Shanghai Jiao Tong University

2Huawei Noah’s Ark Lab

{cl-zhao,nibingbing,stevenash0822,wwqqzzhi,zhangwenjun}@sjtu.edu.cn

tian.qi1@huawei.com

Abstract

We propose a variational Bayesian scheme for pruning
convolutional neural networks in channel level. This idea is
motivated by the fact that deterministic value based pruning
methods are inherently improper and unstable.
In a nut-
shell, variational technique is introduced to estimate dis-
tribution of a newly proposed parameter, called channel
saliency, based on this, redundant channels can be removed
from model via a simple criterion. The advantages are
two-fold: 1) Our method conducts channel pruning with-
out desire of re-training stage, thus improving the compu-
tation efﬁciency. 2) Our method is implemented as a stand-
alone module, called variational pruning layer, which can
be straightforwardly inserted into off-the-shelf deep learn-
ing packages, without any special network design. Exten-
sive experimental results well demonstrate the effectiveness
of our method: For CIFAR-10, we perform channel removal
on different CNN models up to 74% reduction, which results
in signiﬁcant size reduction and computation saving. For
ImageNet, about 40% channels of ResNet-50 are removed
without compromising accuracy.

1. Introduction

Deep convolutional neural networks have achieved a
signiﬁcant success in computer vision community, such
as object recognition [13, 25, 37, 44], semantic segmen-
tation [2, 28], object detection [9, 26] and video analy-
sis [45, 46]. Due to the huge storage and computation
costs, deep models are difﬁcult to implement on resource-
constrained platforms, such as, mobile and wearable device.
To solve this problem, various methods have been pro-
posed to improve the efﬁciency of CNN models or to
compress the models into more compact representations.
These approaches include tensor factorization [35, 48], net-

∗Equal contribution
†Corresponding author

…

…

…

Before Pruning

Channel Saliency

After Pruning

Figure 1. Variational CNN Pruning: We prune redundant chan-
nel based on distribution of channel saliency γ. Dotted line de-
notes pruned channel and solid line denotes preserved one. Best
view in color.

work quantization [4, 40], and weight pruning [11, 32].
Though theoretically plausible, these methods usually re-
quire speciﬁc software or hardware implementation, which
inevitably introduce extra overhead, i.e., not practically ap-
plicable. Some other methods [15,30] obtain compact mod-
els with hand-crafted manner, which need manual interven-
tion and suffer from compromise performance.

An alternative method is channel pruning [14,27,29,43],
which removes redundant channels in models. Channel
pruning operations are fully supported by off-the-shelf deep
learning library, thus they are very ﬂexible in practical im-
plementation. Moreover, channel pruning reduces memory
footprint dramatically (i.e., feature maps). Recent meth-
ods [14,29] remove one layer by minimizing the reconstruc-
tion error between the consecutive layers. However, these
methods adopt the greedy algorithm for channel selection,
which is time-consuming, as the computation complexity is
linearly proportional to the layers of model. Sparsity based
methods [23,43] impose Lp norm on a group of weights, for
channel shrinkage. However, despite their favorable perfor-
mance, approaches for channel pruning have inherent draw-
backs. 1) Complexity: most of these methods need extra re-
training stage for performance restoration. Even some need

2780

to conduct several iterations of alternative pruning and re-
training, which is very time-consuming and not friendly for
operation in practice. 2) Stability: layer-alternative param-
eter optimization approach inevitably suffers from instabil-
ity. Namely, parameters of hidden layers may change dras-
tically during consecutive iterations. This results in arbi-
trary removal of some channels during optimization, which
is NOT interpretable. This eventually causes unexpected
performance degradation after pruning.

To address aforementioned issues, we re-formulate the
channel pruning problem within a Bayesian probabilistic
learning framework (i.e., in contrast to previous determin-
istic methods), called variational CNN pruning, to yield
compact, stable,
interpretable and ﬂexible compression.
To avoid introducing additional parameters, the proposed
probabilistic pruning framework directly operates on re-
deﬁned scaling factor in Batch Normalization (BN). Dif-
ferent from [27], we extend the scaling factor to include
bias term, called channel saliency, and still keep the lin-
ear form of Batch Normalization. Instead of deterministic
value computation, we regard each channel saliency as a
random variable and seek a proper probabilistic distribution
to model it.

Thus, unimportant layers could be pruned based on the
distribution of the channel saliency. However, directly esti-
mating these distributions involves intractable multidimen-
sional integral in Bayesian manner. We therefore introduce
a stochastic variational inference [20] to estimate the distri-
bution of channel saliency induced by a sparse prior. Our
Bayesian framework possesses the following advantages.
First, by directly applying probabilistic learning on the scal-
ing factor (i.e., our method can also be represented as a
variational-pruning layer ), no additional parameter is in-
troduced in deep CNN model learning, therefore ours can be
directly plugged into any off-the-shelf deep learning pack-
age, without any requirements for special network compo-
nent design. Second, the proposed method conducts chan-
nel pruning with no desire of re-training step. Therefore,
computational efﬁciency is improved. Third, being formu-
lated in a probabilistic way, optimization process becomes
stable and interpretable. Extensive experiments on CIFAR-
10 show that we can prune up to 74% channels with little
accuracy loss. On ImageNet, the ResNet-50 achieves better
performance than baseline while about 40% channels are
removed.

2. Related Work

Tensor Decomposition.

Jaderberg et al. [19] obtain
CNNs substantial speedups via low-rank decomposition.
Denil et al. [5] remove redundant parameters in deep learn-
ing model by using a few weight values to represent each
feature map, which saves a large number of the memory
and computation consumption. Techniques of matrix de-

composition, SVD, is introduced to approximate the weight
matrix in neural networks in [6]. In [42], Tensor Ring (TR)
factorization technology is applied to compress deep neu-
ral networks. However, these methods are impracticable,
because of involving the computation-expensive decompo-
sition operations.

Network Quantization. Courbariaux et al. [4] pro-
pose a method to quantize the network weight with binary
value. To obtain signiﬁcant improvement for computation
efﬁciency, Gupta et al. [10] leverage stochastic rounding to
represent weight by a 16-bit wide ﬁxed-point. In [3], net-
work weights are hashed into different groups. For each
group, shared weights are saved with same hash index. To
further compress networks, Tung et al. [40] combine weight
pruning and quantization in a single learning framework.
Rastegari et al. [36] propose XNOR-Networks which im-
pose XNOR and bitcount operations on convolution layer
to quantize weights. Park et al. [34] propose a method for
quantizing weights and activations based on weighted en-
tropy. These low-bit approximation methods usually suffer
form accuracy loss.

Non-structured Pruning.

Inspired by neurobiology,
optimal brain damage [22] and optimal brain surgeon [12]
are proposed to remove parameters in networks for sav-
ing storage through an analysis of the Hessian of the loss
function. However,
these heuristic based methods are
computationally-intensive and cannot boost running time.
Han et al. [11] judge the importance of weights in net-
work based on magnitude and remove redundant ones with
small value. A data-free algorithm is proposed in [39] to
eliminate the superﬂuous neurons in fully-connected layers.
Lebedev et al. [21] boost ConvNets speed by group-wise
sparsity operations. Wang et al. [41] propose a density-
diversity penalty on weight to compress fully-connected
networks. Pavlo et al. [32] remove unimportant kernels
based on brute-force search and restore the performance of
compressed networks via ﬁne-tuning. These non-structured
pruning methods [22,24,38] need special software and hard-
ware to speedup inference, due to the irregular sparsity in
weight tensor.

Structured Pruning. Wen et al. [43] exploit CNNs
compression at different grains via imposing L1 norm on a
group of weights. Li et al. [23] propose an one-shot pruning
and retraining strategy to compress ﬁlters across multiple
layers. Liu et al. [27] leverage the scale factor of Batch Nor-
malization layer [18] to remove non-efﬁcient channels and
ﬁne-tune the pruned model to restore the comparable accu-
racy. He et al. [14] propose a method that reduces redun-
dant channels based on LASSO regression and least square
reconstruction. [29, 47] transform network pruning into an
optimization problem and perform channel selection in a
layer-wise manner via greedy algorithms. All these struc-
tured pruning methods are hardware friendly, i.e., boosting

2781

networks inference directly. However, most of these prun-
ing approaches [1, 7, 16] need re-training stage to restore
comparable accuracy, which increases computational com-
plexity and is not friendly for operation. In contrast to these,
our method conducts channel pruning with no desire of re-
training stage, while keeping favorable performance in both
compression and speedup.

3. Variational CNN Pruning

In this section, we propose a variational Bayesian
scheme for channel pruning in convolutional networks:
Variational CNN Pruning. First, we reformulate the Batch
Normalization layer, by extending scale factor to shift term,
called channel saliency. Instead of deterministic value, we
estimate the distribution of channel saliency via variational
inference. To further facilitate pruning, a sparsity-inducing
prior is imposed on the channel saliency. Then we prune re-
dundant channels based on distributions of channel saliency
via a simple criterion. Our method is implemented as a
variational-pruning layer, which can be inserted into any
existing framework without special design.

3.1. Batch Normalization Revisit

Batch Normalization (BN) [18] has been introduced in
current convolutional neural networks as a standard layer,
which improves training speed and accelerates convergence
process. This great performance beneﬁts from normaliz-
ing activation distribution by using the mini-batch statistics
(i.e., σB and µB). xin and xout are represented as input
and output activation map, respectively, and B denotes the
mini-batch. BN layer performs normalization as follows:

BN(x) =

xin − µB
pσ2
B + ǫ

; xout = γ · BN(x) + β

(1)

where µB and σB denote the mean and standard deviation
of input activations over B. An afﬁne transformation is ap-
plied on normalized activations by a linear function learn-
able parameter, i.e., scale factor γ and shift factor β.

Batch Normalization (BN) has been inserted between
convolution and non-linearity layer, constituting a basic cell
in modern networks. γ and β are channel-wise parameters
on models, with function of scaling and shifting. Param-
eter γ can represent the effectiveness coefﬁcient of corre-
sponding channels and no parameter need to be introduced.
Because of this characteristic, it makes sense to choose the
parameter γ as a factor to indicate the importance of con-
volution channel [27]. However, two signiﬁcant issues are
ignored here:

1) we notice the fact that parameters γ and β are inde-
pendent in BN. Thus, eliminating unimportant channel with
small γ ′s value is an improper manner, due to ignoring the
inﬂuence of shift term. For example, a channel has zero

value of γ but with a big value of β. Pruning this channel
is arbitrary, because activations of this channel are not zero
and still contribute effect to the next layer.

2) The value of parameter in hidden layer changes dras-
tically after several iterations in training stage. This means
that the value (e.g., scale factor) is dynamical. Determining
the effect of layer by only observing a value of the param-
eter is not sufﬁcient. We argue that this is one signiﬁcant
cause for performance degradation after pruning [27].

To remedy this, we reformulate Batch Normalization

layer in Equation 1 as follows:

xout = γ · BN(x) + ˜β,

where, ˜β = γ · β.

(2)

(3)

We extend the scale factor γ on shift term β, while still
maintaining afﬁne function of BN. In the case, the parame-
ter γ can be directly utilized as factors on channels, called
channel saliency, because activation of each channel is to-
tally depended on γ. Rather than through the value of γ, we
prune unimportant channels based on the distribution of γ,
which is estimated by variational inference (more details in
Sec 3.2). We do not introduce any parameters in BN layer
and the reformulated BN layer can be inserted into any ex-
isting frameworks.

3.2. Variational Inference on Channel Saliency

Instead of deterministic value, we prune channel with the
distribution of channel saliency. Obviously, this method
is more stable and interpretable, because the distribution
contains rich information and has good mathematical prop-
erty. Thus, we estimate distribution of channel saliency via
Bayes rule. More details are discussed as follows:

Consider a dataset D = {(xi, yi)}N

i=1, x is input data
and y is corresponding label. Our goal is to learn a model
with parameter γ of the conditional probability p(y|x, γ).
Parameter γ is channel saliency deﬁned in previous section
and we leverage γ to determine the effect of each channel.
After obtaining the prior knowledge of γ (prior distribu-
tion), we can inference the posterior distribution of γ with
Bayes rule. However, computing such a posterior distribu-
tion p(γ|D) = p(γ)p(D|γ)/p(D) is difﬁcult, because the
p(D) = R p(D, γ)dγ is a computation-intractable integral.
It is hard to obtain distribution of channel saliency directly.
Hence, an effectively approximated method, variational in-
ference, is introduced to tackle this problem. Compared to
MCMC, variational inference is a good choice, beneﬁting
from its solidly theoretical property and small computation
cost.

In variational inference, instead of computing the true
posterior distribution directly, we assume a parameterized
distribution qφ(γ) to approximate p(γ|D). By this way, the

2782

unsolvable inference problem is cast to a tractable optimiza-
tion problem. We can estimate the posterior distribution by
minimizing the distance between qφ(γ) and the true pos-
terior distribution p(γ|D) by Kullback-Leibler divergence,
i.e., minφ DKL(qφ(γ)||p(γ|D). Minimizing the KL diver-
gence is equivalent to maximizing the evidence lower bound
(ELBO) as follows:

L(φ) = LD(φ) − DKL(qφ(γ)||p(γ)),

(4)

where, LD(φ) = X

E

qφ(γ)[log p(y|x, γ)].

(5)

(x,y)∈D

The object function consists of two terms, expected log-
likelihood LD(φ) and KL divergence. The LD(φ) is a re-
construction term which aims to maximize the probability
of the model prediction, e.g., minimizing the sum of pre-
diction error. The KL divergence term is a regularization
term, where a sparse prior will be introduced as sparsity-
induced penalty on the channel saliency γ. Optimizing the
two terms, we can take into account the performance and
compression simultaneously. The trade-off between the two
terms leads to a compact and effective model.

Due to the expectation in Equation 5, the gradient can
not be computed directly. Following [8, 20, 31], we intro-
duce Reparametrization Trick to obtain an unbiased dif-
ferentiable minibatch-based Monte Carlo estimator of ex-
pected log-likelihood. M is the mini-batch size and N is
the number of data. In this way qφ(γ) can be represented as
a differentiable function γ = f (φ, ǫ), where ǫ ∼ N (0, 1).
Then Equation 5 can be reformulated as follows:

LD(φ) ≃ LA

D(φ) =

N
M

M

X

m=1

log p(yim|xim, γim = f (φ, ǫ)),

L(φ) ≃ LA

D(φ) − DKL(qφ(γ)||p(γ)).

(6)

(7)

In this way, we can solve optimization problem (Eqn. 5) by
an approximation manner.

Let w be the weights of the neural networks. The model
can be represented as p(y|x, w, γ), which is conditional on
w. Therefore, we can still keep the the evidence lower
bound (ELBO) as object function:

L(φ, w) ≃ LA

D(φ, w) − DKL(qφ(γ)||p(γ)).

(8)

Optimizing the object function is to obtain the approx-
imate distribution of channel saliency qφ(γ) and networks
weights w. The model can be trained in an end-to-end man-
ner [33].

3.3. KL divergence with Sparse Prior

As shown in Equation 8, KL-divergence between the
posterior and prior distributions need to be computed. A
common choice of the approximate posterior is a fully fac-
torized Gaussian distribution, formulated as follows:

qφ(γ) =

C

Y

i=1

q(γi),

γi ∼ N (µi, σi).

(9)

Our goal is to ﬁne-tune the learnable parameters φ = (µ, σ)
with the object function. In order to remove channels, the
approximate distribution qφ(γ) should be sparse. Then the
inefﬁcient channels can be determined easily. Namely, we
eliminate channels based on the mean and variance of dis-
tribution of channel saliency γ. Thus, we introduce a prior
distribution as follows:

p(γ) =

C

Y

i=1

p(γi),

γi ∼ N (0, σ∗

i ),

(10)

where we ﬁx mean to zero value. Thus, induced by this
sparse prior, the channel saliency is encouraged to towards
zero. Then the KL-divergence in ELBO can be calculated
tractable as below:

DKL(qφ(γ)||p(γ)) = X

DKL(qφ(γi)||p(γi))

i

= X

log

i

σ∗
i
σi

+

σ2
i + µ2
2(σ∗

i )2 −

i

(11)

1
2

.

The reason of making this choice is summarized as below:
1) The selected prior distribution has sparse property
which can encourage parameters γ towards zero. Accord-
ing to this, we can straightforwardly prune ineffective layers
based on γ .

2) There is no distribution gap between qφ(γ) and p(γ).
The KL divergence will be zero, when the expectation µ
and variance σ of two distributions have same value. This
character guarantees that the γ can be calculated accurately.
3) The KL divergence DKL(qφ(γ)||p(γ)) can be com-
puted tractable, because both distributions belong to Gaus-
sian. Other sparsity distributions, such as Laplace distribu-
tion or log-uniform distribution, also can be used as prior
distribution. However, we cannot obtain closed-form solu-
tion of the KL-divergence, due to the involved intractable
integrals. Although some methods of numerical estimation
can be used to estimate the KL term, this will introduce in-
evitable errors.

Let the variance of the both distributions to be identical,

Equation 11 can be simpliﬁed as follows:

DKL(qφ(γ)||p(γ)) = X

kµ2
i

(12)

i

2783

Where k is a coefﬁcient and inversely proportional to vari-
ance. In this case, the mean value (i.e., µ ) of the approx-
imated distribution is encouraged to be small, due to im-
posing a L2 norm. Hence, we can discard corresponding
channels safely.

3.4. Variational Pruning on Channels

We optimize ELBO with the KL-divergence mentioned
above to obtain the distribution of channels salience γ,
where γ ∼ q(γ|φ = (µ, σ)). Then we remove redundant
channel based on the following criterion.

Based above section, obtained channel saliency γ obeys
a gaussian distribution. Consider to the centrality prop-
erty of gaussian, samples distribute around the expectation.
When the expectation µ is close to zero and variance is
small, the probability of variable γ is close to zero. Based
on this idea, we eliminate redundant channels when the op-
timized parameters are less than thresholds, i.e., (µ, σ) <
(τ, θ). The pseudocode of variational constitutional neural
networks pruning is illustrated in Algorithm 1.

Algorithm 1 Variational CNN Pruning
Input: N pairs data {(xi, yi)}N
Output: φ, w

i=1, C channels {γi}C

i=1

1: for epoch= 1 to K do
2:

i=1 q(γi)

qφ(γ) = QC
γi ∼ N (µi, σi)
L(φ, w) ≃ LA
Optimize : L(φ, w)
Update Parameters
for i = 1 to C do

3:

4:

5:

6:

7:

8:

9:

if ui < τ , σi < θ then

Pruning the i-channel

D(φ, w) − DKL(qφ(γ)||p(γ)).

10:

end if

end for

11:
12: end for

The algorithm is implemented in BN as a special layer,
called variational pruning layer. We do not introduce extra
parameter here and all operations can be integrated in this
layer. The variational pruning layer is easy to implement
as a separate module, which can be inserted to any existing
framework.

4. Experiments

In this section, we carry out extensive experiments to
evaluate the performance of the proposed method on im-
age classiﬁcation task. Three representative networks, in-
cluding VGG Net [37], DenseNet [17], and ResNet [13],
are chosen for compression. We report the performances on
CIFAR and ImageNet datasets, and compare to state-of-the-
arts. All these experiments demonstrate the effectiveness of
our method.

4.1. Implementation Details

Training Strategy. For CIFAR, the learning rate is set
as 0.1, and divided by 10 at the 150 and 240 epochs, respec-
tively. We train networks for 300 epochs and with batch size
of 256. For ImageNet, the learning rate is set as 0.1, and di-
vided by 10 at 60 and 90 epochs. We train them with batch
size of 256 and 120 epochs for total. All these networks are
optimized by stochastic gradient decent (SGD), with nes-
terov momentum 0.9 and weight decay 10−4. Random ﬂip
and crop are applied for data augmentation on CIFAR and
ImageNet datasets.

Compression Metric. Channels, Parameter and FLOPs
(ﬂoating point operations) are used to measure network
compression. Channels indicate the memory footprint. Pa-
rameter and FLOPs denote the storage space and compu-
tation cost, respectively. In this paper, we only count pa-
rameter and FLOPs over convolutional layer, because the
proposed method focuses on channel-level compression of
convolutional neural networks. In addition, we only count
multiply operation for FLOPs.

Pruning Details. All models are trained from scratch
as baseline. We prune unimportant channels based on the
distribution of the proposed channel saliency. When the pa-
rameters of the distribution are less than thresholds, the cor-
responding channel will be removed from the model. The
thresholds τ and θ are empirically set as 0.02 and 0.01, re-
spectively. We set the variance of prior and approximated
posterior to be identical for simplifying the KL loss, which
beneﬁts for training [8]. Different from prior arts, the pro-
posed method does not need ﬁne-tuning to reﬁne the pruned
model.

4.2. Results on CIFAR 10

The purpose of our approach is pruning redundant chan-
nels for saving storage space and computation consumption.
We conduct our experiments on CIFAR-10 dataset with
three classic deep networks: VGG, DenseNet and ResNets.
Channels, parameters and FLOPs are used to measure the
performance of the pruning models. The experiment results
are reported in Table 1.

VGG Net. For VGG net, the 16-layer (13-Conv + 3FC)
model is adopted to perform on CIFAR-10 dataset. We re-
move 62% channels while keeping the accuracy at 93.18%,
which is slightly lower than baseline. Notably, more than
70% of the parameters are reduced and nearly 40% compu-
tation is saved. This greatly facilitates VGG model, popular
backbone for object detection and semantic segmentation,
to deploy on mobile devices.

DenseNet. For DenseNet, we remove 60% inefﬁcient
channels with only 1% drop of accuracy. This is extremely
meaningful for DenseNet which consumes a vast amount
of memory, because the reduction of channels will decrease
memory footprint directly. We also save more than half stor-

2784

Model

Accuracy

Channels

Pruned

Parameters Pruned

FLOPs

Pruned

VGG-16 Base
VGG-16 Pruned

DenseNet-40 Base
DenseNet-40 Pruned

ResNet-20 Base
ResNet-20 Pruned
ResNet-56 Base
ResNet-56 Pruned
ResNet-110 Base
ResNet-110 Pruned
ResNet-164 Base
ResNet-164 Pruned

93.25%
93.18%

94.11%
93.16%

92.01%
91.66%
93.04%
92.26%
93.21%
92.96%
93.58%
93.16%

4224
1599

9360
3705

1808
1114
4496
2469
8528
3121
12560
3238

-
62%

-
60%

-
38%
-
45%
-
63%
-
74%

14.71M
3.92M

1.04M
0.42M

0.21M
0.17M
0.57M
0.46M
1.12M
0.66M
1.68M
0.73M

-
73.34%

-
59.67%

-
20.41%
-
20.49%
-
41.27%
-
56.70%

313M
190M

282M
156M

8.9M
7.5M
22.3M
17.8M
42.4M
26.9M
62.4M
31.8M

-
39.10%

-
44.78%

-
16.47%
-
20.30%
-
36.44%
-
49.08%

Table 1. Accuracy and pruning ratio on CIFAR-10. We count pruned channels, parameters and FLOPs over different deep models, and the
accuracy of pruned models are reported without retraining stage. We train these models from scratch without pruning as baseline in our
experiments.

Model

Accuracy

Channels

Pruned

Parameters Pruned

FLOPs

Pruned

VGG-16 Base
VGG-16 Pruned

DenseNet-40 Base
DenseNet-40 Pruned

ResNet-164 Base
ResNet-164 Pruned

73.26%
73.33%

74.64%
72.19%

75.56%
73.76%

4224
2883

9360
5851

12560
6681

-
32%

-
37%

-
47%

14.71M
9.14M

1.04M
0.65M

1.68M
1.38M

-
37.87%

-
37.73%

-
17.59%

313M
256M

282M
218M

62.4M
45.4M

-
18.05%

-
22.67%

-
27.16%

Table 2. Accuracy and pruning ratio on CIFAR-100. We count pruned channels, parameters and FLOPs on VGG-16, DenseNet-40 and
ResNet-164.

age space by reducing 60% of parameters and boost com-
putation by decreasing about 45% FLOPs. Although struc-
ture of DenseNet is pretty compact, our method still exerts
pruning on channels. We consider this beneﬁts from the
proposed variational pruning method.

ResNets. For ResNets, we adopt four different structures
on CIFAR-10, including ResNet-20, ResNet-56, ResNet-
110, and ResNet-164. ResNets for CIFAR-10 have three
stages of residual block, and each stages followed by down-
sampling layer to resize the scale of feature maps. As shown
in the table 1 and Figure 2, we note that the pruned chan-
nels increase obviously form ResNet-20 to ResNet-164, and
the reduction ratio raises from 38% to 74%. The proposed
method has achieved notable result on ResNet-164, by re-
moving 57% parameters and 49% FLOPs. As a common
choice, we increase the layers to improve the performance
of models. However, this operation will involve more re-
dundant layers at the same time. The purpose of our method
is eliminating redundant channels. Therefore, with more
layers, the effect of our method is more obvious.

Channels

Parameters

FLOPs

80

60

40

20

)

%

(
 

 

o
i
t
a
R
g
n
i
n
u
r
P

0

ResNet20

ResNet56

ResNet110

ResNet164

Figure 2. ResNets performed on CIFAR-10. We compare chan-
nels, parameters and FLOPs on four different layers of ResNet
network. Best view in color.

4.3. Results on CIFAR 100

As illustrated in Table 2, the proposed method is evalu-
ated on CIFAR-100 for three deep networks, e.g., VGG-16,
DenseNet-40 and ResNet-164. We note that our method ob-
tain the better performance on deeper model. The removed

2785

(a) VGG-16

(b) DenseNet-40

(c) ResNet-20

(c) ResNet-56

(d) ResNet-110

(e) ResNet-164

Figure 3. Compression and Accuracy Curves. We show the details of compression process about VGG-16, DenseNet-40 and ResNets. Best
view in color.

Model

Top-1

Top-5

Channels Pruned

ResNet-50 Base

75.1% 92.8% 26560

-

ResNet-50 [29]
ResNet-50 Ours

72.8% 91.1% 18592
75.2% 92.1% 15920

30%
40%

Table 3. Performance and Comparison on ImageNet.

Dataset

Model

Accuracy Channels Params

CIFAR-10

CIFAR-100

Dense40∗ [27]
Denset40 Ours
Res164∗ [27]
Res164 Ours

Dense40∗ [27]
Dense40 Ours
Res164∗ [27]
Res164 Ours

89.5%
93.1%
47.7%
93.1%

67.7%
72.1%
48.0%
73.7%

60%
60%
60%
74%

40%
37%
40%
47%

54%
59%
34%
56%

35%
38%
13%
17%

Table 4. Comparison with other method on CIFAR dataset. ∗ de-
notes pruning without ﬁne-tuning stage.

channels raise from 32% in VGG-16 to 47% in ResNet-164.
We consider deep models contain more redundant channels,
and the proposed method is sensitive to these. Removing
these non-essential channels is useful for saving memory,
storage and computation.

4.4. Results on ImageNet

ImageNet dataset is a large scale image recognition
benchmark. The dataset contains 1.2 million images for

training and 50000 images for validation. All these images
come from 1000 different categories.

To further evaluate the effect of variational pruning on
large scale dataset, we perform our method on ImageNet
dataset for ResNet-50. From Table 3, we note that the
pruned model performs better than baseline in Top-1 ac-
curacy, while 40% channels have been removed. This
demonstrates the pruned model is compact and efﬁcient,
and is meaningful for reducing memory footprint. Com-
pared to [29], our methods obtains higher accuracy and
eliminates more channels.

4.5. Comparison with Other Method

As shown in Table 4, we compare the proposed method
with [27], which utilizes the deterministic value of scale
factor to remove channels. Unlike this, we eliminate unim-
portant channels based distributions and do not need ﬁne-
tuning stage to restore performance.

To keep comparison fair, we use the results of [27] with
no ﬁne-tuning stage. For ResNet-164, we exceed [27] al-
most two times of accuracy and obtain higher compression
performance in Parameters and FLOPs on CIFAR-10. Af-
ter pruning, the accuracy of [27] drops drastically. The poor
performance suffers from eliminating unimportant channels
based on deterministic value. Because weights in hidden
layers change straightly during training process, removing
channels based on deterministic value is arbitrary. In con-
trast to this, our method is more robust and maintains accu-
racy after compression, which beneﬁts from pruning based
on distribution.

2786

Figure 4. ResNet-50 on ImageNet. Statistics of preserved channels
in 16 residual blocks and the ﬁrst convolutional layer. Best view
in color.

4.6. Analysis

4.6.1 Compression and Accuracy

As illustrated in Figure 3, accuracy and compression curves
are given about 6 networks, which perform on CIFAR-10
dataset. The compression ratio is deﬁned as follows:

#compression rate =

#preserved channels

#all channels

(13)

We note that the accuracy is increasing, while the compres-
sion rate is decreasing. The proposed method removes re-
dundant channels and improves accuracy at the same time.
We consider this beneﬁts from the loss (Eqn 8). Namely,
the KL term focuses on channel shrinking and the expect-
log term updates the weights for prediction. The trade-off,
between the two constraint item, leads the model to be a
compact and effective one.

4.6.2 Channel Pruning Analysis

As shown in Figure 4, we reveal the pruning details about
ResNet-50 on ImageNet dataset. We account the preserved
channels about 16 residual-blocks and the ﬁrst convolu-
tional layer. The pruned channels mainly concentrate on
the middle stage of the model, and channels have few re-
duction at both ends. This phenomenon is caused by that
channels at the last block contain abundant semantic in-
formation, which is crucial for image recognition. So it is
hard to remove these channels. Interlayer-channels include
more detailed information. Some of these are non-efﬁcient
and redundant. Removing these channels has little impact
on the performance of model. This proves that our varia-
tional pruning method could determine the importance of
channels, namely, maintaining efﬁcient channels while re-
moving superﬂuous channels. The superior character of our
method leads to the result that removing a large number of
channels with considerable margin of baseline, and further
demonstrates the fact that some of parameters and activa-
tions in deep model are invalid and cumbersome.

95

93

91

89

87

)

%

(
y
c
a
r
u
c
c
A

85

50

=0.010

=0.015

=0.020

=0.025

=0.030

55

60

65

70

75

Compression Rate(%)

Figure 5. Sensitivity Analysis. We report compression rate and
accuracy at different value of threshold τ . Best view in color.

4.6.3 Sensitivity Analysis

We choose 5 different value of parameter τ (threshold for
mean), and perform VGG-16 on CIFAR-10 with these pa-
rameters. As illustrated in Figure 5, with the increase of τ ,
the compression rate is increased from 51% to 70% and ac-
curacy is dropped down from 93.5% to 89%. This means
more compact model will sacriﬁce more accuracy. When
we tune the τ below 0.02, a little accuracy is obtained but
the compression rate drops drastically. When we turn up the
threshold for smaller size model, the performance becomes
worse. Therefore, to achieve a trade-off between compres-
sion and accuracy, we empirically set τ as 0.02.

5. Conclusion

We propose a variational pruning method for removing
unimportant channels in convolutional neural networks.
We reformulate the Batch Normalization layer to obtain
a new parameter, called channel saliency, which can be
leveraged to measure the effect of channel. To avoid the
deterministic value, we estimate the distribution of channel
saliency via Bayes rule, then a simple yet effective criterion
is used to prune redundant channels. Our method conducts
channel pruning with no desire of re-training stage and can
be implemented as separated module which is ﬂexible and
transplantable. The extensive experiments on CIFAR and
ImageNet demonstrate the outstanding performance of the
proposed method.

Acknowledgements

This work was supported by National Science Foundation
of China (U1611461,61521062). This work was partly
supported by STCSM(18DZ1112300,18DZ2270700). This
work was also partially supported by joint research grant
of SJTU-BIGO LIVE,
joint research grant of SJTU-
Minivision, and China’s Thousand Talent Program.

2787

References

[1] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Struc-
tured pruning of deep convolutional neural networks. ACM
Journal on Emerging Technologies in Computing Systems,
13(3):32, 2017.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence, 40(4):834–848, 2018.

[3] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Wein-
berger, and Yixin Chen. Compressing neural networks with
the hashing trick. In International Conference on Machine
Learning, pages 2285–2294, 2015.

[4] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio. Binarized neural networks:
Training deep neural networks with weights and activations
constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830,
2016.

[5] Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Fre-
itas, et al.
In
Advances in neural information processing systems, pages
2148–2156, 2013.

Predicting parameters in deep learning.

[6] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann Le-
Cun, and Rob Fergus. Exploiting linear structure within con-
volutional networks for efﬁcient evaluation.
In Advances
in neural information processing systems, pages 1269–1277,
2014.

[7] Abhimanyu Dubey, Moitreya Chatterjee, and Narendra
Ahuja. Coreset-based neural network compression. In Pro-
ceedings of the European Conference on Computer Vision,
pages 454–470, 2018.

[8] Patrick Esser, Ekaterina Sutter, and Bj¨orn Ommer. A varia-
tional u-net for conditional appearance and shape generation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8857–8866, 2018.

[9] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.

[10] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and
Pritish Narayanan. Deep learning with limited numerical
precision. In International Conference on Machine Learn-
ing, pages 1737–1746, 2015.

[11] Song Han, Jeff Pool, John Tran, and William Dally. Learning
both weights and connections for efﬁcient neural network. In
Advances in neural information processing systems, pages
1135–1143, 2015.

[12] Babak Hassibi and David G Stork. Second order derivatives
for network pruning: Optimal brain surgeon.
In Advances
in neural information processing systems, pages 164–171,
1993.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[14] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In Proceedings

of the IEEE International Conference on Computer Vision,
pages 1389–1397, 2017.

[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[16] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
Network trimming: A data-driven neuron pruning approach
towards efﬁcient deep architectures.
International Confer-
ence on Learning Representations, 2016.

[17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017.

[18] Sergey Ioffe and Christian Szegedy. Batch normalization:
accelerating deep network training by reducing internal co-
variate shift. In Proceedings of the 32nd International Con-
ference on International Conference on Machine Learning-
Volume 37, pages 448–456. JMLR. org, 2015.

[19] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
Speeding up convolutional neural networks with low rank
expansions.
In Proceedings of the British Machine Vision
Conference. BMVA Press, 2014.

[20] Diederik P Kingma, Tim Salimans, and Max Welling. Vari-
ational dropout and the local reparameterization trick.
In
Advances in Neural Information Processing Systems, pages
2575–2583, 2015.

[21] Vadim Lebedev and Victor Lempitsky. Fast convnets us-
ing group-wise brain damage.
In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, pages 2554–
2564. IEEE, 2016.

[22] Yann LeCun, John S Denker, and Sara A Solla. Optimal
brain damage. In Advances in neural information processing
systems, pages 598–605, 1990.

[23] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. Inter-
national Conference on Learning Representations, 2017.

[24] Chen Lin, Zhao Zhong, Wu Wei, and Junjie Yan. Synap-
tic strength for convolutional neural network. In Advances
in Neural Information Processing Systems, pages 10170–
10179, 2018.

[25] Jinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo
Pose transferrable person re-

Cheng, and Jianguo Hu.
identiﬁcation. In CVPR, pages 4099–4108, 2018.

[26] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European con-
ference on computer vision, pages 21–37. Springer, 2016.

[27] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 2736–2744, 2017.

[28] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 3431–3440, 2015.

2788

[44] Yichao Yan, Bingbing Ni, Zhichao Song, Chao Ma, Yan Yan,
and Xiaokang Yang. Person re-identiﬁcation via recurrent
feature aggregation. In ECCV, pages 701–716, 2016.

[45] Yichao Yan, Bingbing Ni, and Xiaokang Yang. Predicting
In IJCAI,

human interaction via relative attention model.
pages 3245–3251, 2017.

[46] Yichao Yan, Jingwei Xu, Bingbing Ni, Wendong Zhang, and
Xiaokang Yang. Skeleton-aided articulated motion genera-
tion. In ACM MM, pages 199–207, 2017.

[47] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I
Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and
Larry S Davis. Nisp: Pruning networks using neuron impor-
tance score propagation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
9194–9203, 2018.

[48] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao.
On compressing deep models by low rank and sparse decom-
position. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 7370–7379,
2017.

[29] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter
level pruning method for deep neural network compression.
In Proceedings of the IEEE international conference on com-
puter vision, pages 5058–5066, 2017.

[30] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn architec-
ture design. In Proceedings of the European Conference on
Computer Vision, pages 116–131, 2018.

[31] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
Variational dropout sparsiﬁes deep neural networks. In Pro-
ceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 2498–2507. JMLR. org, 2017.

[32] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for
resource efﬁcient inference.
International Conference on
Learning Representations, 2017.

[33] Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and
Dmitry P Vetrov. Structured bayesian pruning via log-normal
multiplicative noise. In Advances in Neural Information Pro-
cessing Systems, pages 6775–6784, 2017.

[34] Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo. Weighted-
entropy-based quantization for deep neural networks.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition, 2017.

[35] Bo Peng, Wenming Tan, Zheyang Li, Shun Zhang, Di Xie,
and Shiliang Pu. Extreme network compression via ﬁlter
group approximation. In Proceedings of the European Con-
ference on Computer Vision, pages 300–316, 2018.

[36] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In European Conference
on Computer Vision, pages 525–542. Springer, 2016.

[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. Interna-
tional Conference on Learning Representations, 2015.

[38] Sanghyun Son, Seungjun Nah, and Kyoung Mu Lee. Cluster-
ing convolutional kernels to compress deep neural networks.
In Proceedings of the European Conference on Computer Vi-
sion, pages 216–232, 2018.

[39] Suraj Srinivas, R Venkatesh Babu, and Supercomputer Ed-
ucation. Data-free parameter pruning for deep neural net-
works.

[40] Frederick Tung and Greg Mori. Clip-q: Deep network com-
pression learning by in-parallel pruning-quantization.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7873–7882, 2018.

[41] Shengjie Wang, Haoran Cai, Jeff Bilmes, and William No-
ble. Training compressed fully-connected networks with a
density-diversity penalty. 2016.

[42] Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, and
Vaneet Aggarwal. Wide compression: Tensor ring nets. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 9329–9338, 2018.

[43] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. Learning structured sparsity in deep neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 2074–2082, 2016.

2789

