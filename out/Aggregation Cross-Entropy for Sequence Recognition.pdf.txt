Aggregation Cross-Entropy for Sequence Recognition

Zecheng Xie∗, Yaoxiong Huang∗, Yuanzhi Zhu, Lianwen Jin†, Yuliang Liu, Lele Xie

South China University of Technology

{zcheng.xie,hwang.yaoxiong,lianwen.jin,zzz.yuanzhi,shaxiaoai18,arlog.lele}@gmail.com

Abstract

In this paper, we propose a novel method, aggregation
cross-entropy (ACE), for sequence recognition from a brand
new perspective. The ACE loss function exhibits compet-
itive performance to CTC and the attention mechanism,
with much quicker implementation (as it involves only four
fundamental formulas), faster inference\back-propagation
(approximately O(1) in parallel), less storage requirement
(no parameter and negligible runtime memory), and
convenient employment (by replacing CTC with ACE).
Furthermore, the proposed ACE loss function exhibits two
noteworthy properties: (1) it can be directly applied for
2D prediction by ﬂattening the 2D prediction into 1D pre-
diction as the input and (2) it requires only characters and
their numbers in the sequence annotation for supervision,
which allows it to advance beyond sequence recognition,
e.g., counting problem. The code is publicly available
at
https://github.com/summerlvsong/Aggregation-Cross-
Entropy.

1. Introduction

Sequence recognition, or sequence labelling [13] is to as-
sign sequences of labels, drawn from a ﬁxed alphabet, to se-
quences of input data, e.g., speech recognition[14, 2], scene
text recognition [38, 39], and handwritten text recognition
[34, 48], as shown in Fig. 1. The recent advances in deep
learning [30, 41, 20] and the new architectures [42, 5, 4, 46]
enabled the construction of systems that can handle one-
dimensional (1D) [38, 34] and two-dimensional (2D) pre-
diction problems [56, 4]. For 1D prediction problems, the
topmost feature maps of the network are collapsed across
the vertical dimension to generate 1D prediction [5] because
characters in the original images are generally distributed
sequentially. Typical examples are regular scene text recog-
nition [38, 54], online/ofﬂine handwritten text recognition
[12, 34, 48], and speech recognition [14, 2]. For 2D predic-
tion problems, characters in the input image are distribut-

∗Zecheng Xie and Yaoxiong Huang make equal contribution.
†Corresponding auther.

Regular Scene Text Recognition

Handwritten Text Recognition

Irregular Scene Text Recognition

Counting Object In Everyday Scenes

Figure 1. Examples of sequence recognition and counting prob-
lems.

ed in a speciﬁc spatial structure. For example, there are
highly complicated spatial relations between adjacent char-
acters in mathematical expression recognition [56, 57]. In
paragraph-level text recognition, characters are generally
distributed line by line [4, 46], whereas in irregular scene
text recognition, they are generally distributed in a side-
view or curved angle pattern [51, 8].

For the sequence recognition problem, traditional meth-
ods generally require to separate training targets for each
segment or time-step in the input sequence, resulting in
inconvenient pre-segmentation and post-processing stages
[12]. The recent emergence of CTC [13] and attention
mechanism [1] signiﬁcantly alleviate this sequential train-
ing problem by circumventing the prior alignment between
input image and their corresponding label sequence. How-
ever, although CTC-based networks have exhibited remark-
able performance in 1D prediction problem, the underlying
methodology is sophisticated; moreover, its implementa-
tion, the forward-backward algorithm [12], is complicated,
resulting in large computation consumption. Besides, CTC
can hardly be applied to 2D prediction problems. Mean-
while, the attention mechanism relies on its attention mod-
ule for label alignment, resulting in additional storage re-
quirement and computation consumption. As pointed out
by Bahdanau et al.
[2], recognition model is difﬁcult to
learn from scratch with attention mechanism, due to the

6538

misalignment between ground truth strings and attention
predictions, especially on longer input sequences [25, 9].
Bai et al. [3] also argues that the misalignment problem can
confuse and mislead the training process, and consequently
make the training costly and degrade recognition accuracy.
Although the attention mechanism can be adapted for 2D
prediction problem, it turns out to be prohibitive in terms of
memory and time consumption, as indicated in [4] and [46].
Compelled by the above observations, we propose a nov-
el aggregation cross-entropy (ACE) loss function for the se-
quence recognition problem, as detailed in Fig. 2. Given
the prediction of the network, the ACE loss consists of three
simple stages: (1) aggregation of the probabilities for each
category along the time dimension; (2) normalization of the
accumulative result and label annotation as probability dis-
tributions over all the classes; and (3) comparison between
these two probability distributions using cross-entropy. The
advantages of the proposed ACE loss function can be sum-
marized as follows:
• Owing to its simplicity, the ACE loss function is much
quicker to implement (four fundamental formulas), faster
to infer and back-propagate (approximately O(1) in par-
allel), less memory demanding (no parameter and ba-
sic runtime memory), and convenient to use (simply re-
place CTC with ACE), as compared to CTC and attention
mechanism. This is illustrated in Table 5, Section 3.4,
and Section 4.4.

• Despite its simplicity, the ACE loss function achieves
competitive performance to CTC and the attention mech-
anism, as established in experiments of regular\irregular
scene text recognition and handwritten text recognition
problems.

• The ACE loss function can be adapted to the 2D predic-
tion problem by ﬂattening the 2D prediction into 1D pre-
diction, as veriﬁed in the experiments of irregular scene
text recognition and counting problems.

• The ACE loss function does not require instance order
information for supervision, which enable it to advance
beyond sequence recognition, e.g., counting problem.

2. Related Work

2.1. Connectionist temporal classiﬁcation

The advantages of the popular CTC loss were ﬁrst
demonstrated in speech recognition [16, 14] and online
handwritten text recognition [15, 12]. Recently, an integrat-
ed CNN-LSTM-CTC model was proposed to address the
scene text recognition problem [38]. There are also meth-
ods that aim to extend CTC in applications; e.g., Zhang
[55] proposed an extended CTC (ECTC) objective
et al.
function adapted from CTC to allow RNN-based phoneme
recognizers to be trained even when only word-level an-
notation is available. Hwang et al.
[21] developed an

expectation-maximization-based online CTC algorithm that
allows RNNs to be trained with an inﬁnitely long input se-
quence, without pre-segmentation or external reset. How-
ever, the calculation process of CTC is highly complicated
and time-consuming, and it require substantial effort to re-
arrange the feature map and annotation when applied to 2D
problems [46, 4].

2.2. Attention mechanism

The attention mechanism was ﬁrst proposed in machine
translation [1, 42] to enable a model to automatically search
for parts of a source sentence for prediction. Then, the
method rapidly became popular in applications such as (vi-
sual) question answering [32, 52], image caption generation
[50, 52, 31], speech recognition [2, 25, 32] and scene tex-
t recognition [39, 3, 19]. Most importantly, the attention
mechanism can also be applied to 2D predictions, such as
mathematical expression recognition [56, 57] and paragraph
recognition [4, 5, 46]. However, the attention mechanism
relies on a complex attention module to fulﬁll its function-
ality, resulting in additional network parameters and run-
time. Besides, missing or superﬂuous characters can easily
cause misalignment problem, confusing and misleading the
training process, and consequently degrading the recogni-
tion accuracy [3, 2, 9].

3. Aggregation Cross-Entropy

Formally, given the input image I and its sequence an-
notation S from a training set Q, the general loss function
for the sequence recognition problem evaluates the proba-
bility of annotation S of length L conditioned on image I
under model parameter ω as follows:

L(ω) = − X

(I,S)∈Q

log P (S|I; ω)

= − X

(I,S)∈Q

L

X

l=1

log P (Sl|l, I; ω)

(1)

where P (Sl|l, I; ω) represents the probability of predict-
ing character Sl at the l-th position of the predicted se-
quence. Therefore, the problem is to estimate the gen-
eral loss function Eq. (1) based on the model prediction
{yt
k, t = 1, 2, · · · , T, k = 1, 2, · · · , |Cǫ|}, where Cǫ = C ∪ǫ,
with C being the character set and ǫ the blank label. Nev-
ertheless, directly estimating the probability P (S|I; ω) was
excessively challenging until the emergence of the popu-
lar CTC loss function. The CTC loss function elegantly
calculates P (S|I; ω) using a forward-backward algorithm,
which removes the need for pre-segmented data and exter-
nal post-processing. The attention mechanism provides an
alternative solution to estimate the general loss function by
directly predicting P (Sl|l, I; ω) based on its attention mod-
ule. However, the forward-backward algorithm of CTC is

6539

Handwritten Text 

Recognition

Regular Scene

Text Recognition

Counting Object

In Everyday Scenes

Irregular Scene
Text Recognition

CNN

(+BLSTM)

(cid:1877)(cid:2869)

(cid:1877)(cid:3047)

1D-Prediction

Flatten

(cid:1877)(cid:2869)

class

(cid:1877)(cid:3021)

(cid:1877)(cid:3047)

(cid:1877)(cid:3021)

width

FCN

height

class 

2D-Prediction

(cid:1877)(cid:3038)(cid:3047)

|(cid:3004)(cid:3332)|

(cid:2278) (cid:1835),(cid:2285) = (cid:3398) (cid:3533) (cid:1840)(cid:3038) (cid:1864)(cid:1866)(cid:1877)(cid:3038)
(cid:2280)(cid:3365)(cid:2869)
(cid:1877)(cid:3364)(cid:2869)

(cid:3038)(cid:2880)(cid:2869)

(cid:1877)(cid:3364)(cid:3038)

(cid:1877)(cid:3364)|(cid:2269)(cid:3332)|

(cid:2280)(cid:3365)(cid:3038)

(cid:2280)(cid:3365) |(cid:2269)(cid:3332)|

(cid:2280)(cid:3365)(cid:2965) =0.2
(cid:2280)(cid:3365)a =0.2
(cid:2280)(cid:3365)c =0.3

(cid:2280)(cid:3365) l =0.1
(cid:2280)(cid:3365)o =0.2

(cid:2280)(cid:578) = 10 (cid:3398) 8
(cid:2280)a = 2
(cid:2280)c = 3

(cid:2280)l = 1
(cid:2280)o =2

Aggregation

Cross Entropy

Example ((cid:147)cocacola(cid:148)(cid:875)T=10)

Figure 2. (Left) Illustration of proposed ACE loss function. Generally, the 1D and 2D predictions are generated by integrated CNN-LSTM
and FCN model, respectively. For the ACE loss function, the 2D prediction is further ﬂattened to 1D prediction, {yt
k, t = 1, 2, . . . , T }.
During aggregation, the 1D predictions at all time-steps are accumulated for each class independently, according to yk = PT
k. After
normalization, the prediction y, together with the ground-truth N , is utilized for loss estimation based on cross-entropy. (Right) A simple
example indicates the generation of annotation for the ACE loss function. Na = 2 implies that there are two “a” in cocacola.

t=1 yt

highly complicated and time-consuming whereas the atten-
tion mechanism requires extra complex network to ensure
the alignment between attention prediction and annotation.
In this paper, we present the ACE loss function to es-
timate the general loss function based on model prediction
yt
k. In Eq. (1), the general loss function can be minimized by
maximizing the predictions at each position of the sequence
annotation, i.e., P (Sl|l, I; ω). However, directly calculat-
ing P (Sl|l, I; ω) based on yt
k is challenging because the
alignment between the l-th character in the annotation and
model prediction yt
k is unclear. Therefore, rather than pre-
cisely estimating the probability P (Sl|l, I; ω), the problem
is mitigated by supervising only the accumulative probabil-
ity of each class; without considering its sequential order in
the annotation. For example, if a class appears twice in the
annotation, we require its accumulative prediction probabil-
ity over T time-steps to be exactly two, anticipating that its
two corresponding predictions approximate to one. There-
fore, we can minimize the general loss function by requir-
ing the network to precisely predict the character number of
each class in the annotation as follows:

L(ω) = − X

(I,S)∈Q

≈ − X

(I,S)∈Q

L

X

l=1

|Cǫ|

X

k=1

log P (Sl|l, I; ω)

log P (Nk|I; ω)

(2)

where Nk represents the number of times that character Cǫ
k
occurs in the sequence annotation S. Note that this new
loss function does not require character order information
but only the classes and their number for supervision.

3.1. Regression Based ACE Loss Function

Now, the problem is to bridge model prediction yt

k to the
number prediction of each class. We propose to calculate
the number of each class yk by summing up the probabilities

of the k-th characters for T time-steps, i.e., yk = PT

as illustrated by aggregation in Fig. 2. Note that,

t=1 yt
k,

max

|Cǫ|

X

k=1

log P (Nk|I; ω) ⇔ min

|Cǫ|

X

k=1

(Nk − yk)2

(3)

Therefore, we adapt the loss function (Eq. (2)) from the per-
spective of regression problem as follows:

L(ω) =

1
2 X

(I,S)∈Q

|Cǫ|

X

k=1

(Nk − yk)2.

(4)

Also note that a total of (T − |S|) predictions are expected
to yield null emission. Therefore, we have Nǫ = T − |S|.

To ﬁnd the gradient for each example (I, S), we ﬁrst
differentiate L(I, S) with respect to the network output yt
k:

∂L(I, S)

∂yt
k

= ∆k,

(5)

where ∆k = (yk − Nk). Recall that for Softmax functions,
we have:

yi =

eai
Pj eaj

,

∂yi
∂aj

= yi(δij − yj),

(6)

where δij = 1 if i = j and zero otherwise. Now, we can
differentiate the loss function with respect to at
k to back-
propagate the gradient through the output layer:

∂L(I, S)

∂at
k

=

|Cǫ|

X

k′=1

∂L(I, S)

∂yt
k′

|Cǫ|

X

k′=1

=

∂yt
k′
∂at
k
k) − X

k′6=k

= ∆k′ · yt

k′ (1 − yt

∆k′ · yt

k′ yt

k

(7)

∆k′ · yt

k′ (δkk′ − yt
k)

3.1.1 Gradient vanishing problem

From Eq. (7), we observe that the regression-based ACE
loss (Eq. (4)) is not convenient in term of back-propagation.

6540

k′ ≈ 1/|Cǫ|, ∀k′, t}.
In the early training stage, we have {yt
Therefore, yt
k′ will be negligible for large vocabulary se-
quence recognition problems, where |Cǫ| is large (e.g.,
7,357 for the HCTR problem). Although the other terms
in Eq. (7) (e.g., ∆k′ ) have acceptable magnitudes for back-
propagation, the gradient would be scaled to a remarkably
small size by the term yt
k, resulting in gradient van-
ishing problem.

k′ and yt

3.2. Cross Entropy Based ACE Loss Function

To prevent the gradient-vanishing problem, It is neces-
sary to offset the negative effect of the term yt
k′ introduced
by the Softmax function in Eq. (7). We borrow the con-
cept of cross-entropy from information theory, which is de-
signed to measure the “distance” between two probability
distributions. Therefore, we ﬁrst normalize the accumula-
tive probability of the k-th character yk to yk = yk/T , and
the character numbers Nk to N k = Nk/T . Then, the cross-
entropy between y and N is expressed as:

L(I, S) = −

|Cǫ|

X

k=1

N k ln yk

(8)

The loss function derivatives with respect to at

k before

the Softmax activation function has the following form:

∂L(I, S)

∂at
k

=

|Cǫ|

X

k′=1

∂L(I, S)

∂yk′

∂yk′
∂yt
k′

∂yt
k′
∂at
k

−

= −

= X
Cǫ
k′ ∈S
1
T X
Cǫ
k′ ∈S
1
T X
Cǫ
k′ ∈S

= −

N k′
yk′

·

1
T

· yt

k′ (δkk′ − yt
k)

N k′

N k′

yt
k′
yk′

yt
k′
yk′

(δkk′ − yt
k)

(δkk′ − yt
k)

(9)

3.2.1 Discussion

In the following, we explain how the updated loss function
solves the gradient vanishing problem:
(1) In the early training stage, yt

k′ has an approxi-
mately identical order of magnitude at all the time-steps.
Thus, the normalized accumulated probability yk′
is al-
so of an identical order of magnitude as yt
k′ . That is,
yt
yk′ ≈ 1; therefore, the gradient through the k′-th class is
k′
now − 1
k). Thus, the gradient can straight-
forwardly back-propagate to at
k through the characters that
appear in sequence annotation S. Besides, when k = k′,
i.e., Cǫ
k ∈ S; the corresponding gradient is approximately
− 1
k), which will encourage the model to make

T N k′ (δkk′ − yt

T N k′ (1 − yt

a larger prediction yt
in S become smaller. This was our original intention.

k, whereas characters that do not appear

(2) In the later training stage, only a few of the prediction
yt∗
k∗ will be very large, leaving the other predictions small e-
nough to be omitted. In this situation, prediction yt∗
k∗ will
∗
k∗
yk∗ .
k∗ ∈ S, the gradient can be straightfor-

occupy the majority of yk∗ , and we have
Therefore, when Cǫ
wardly back-propagated to the recognition network.

yk∗ = T · yt

∗
k∗

yt

3.3. Two dimensional Prediction

In some 2D prediction problem like irregular scene text
recognition with image level annotations, it is challenging
to deﬁne the spatial relation between characters. Characters
may be arranged in multiple lines, in a curved or sloped di-
rection, or even distributed in a random manner. Fortunate-
ly, the proposed ACE loss function can naturally be gener-
alized for the 2D prediction problem, because it does not re-
quire character-order information for the sequence-learning
process.

Suppose that the output 2D prediction y has height H
and width W, and the prediction at the h-th line and w-th
row is denoted as yhw
k . This requires a marginal adaptation
of the calculation of yk and N k as follows, yk = yk
HW =
PH
HW . Then, the loss function for

, N k = Nk

h=1 PW
HW

w=1 yhw

k

the 2D prediction can be transformed as follows:

L(I, S) = −

|Cǫ|

X

k=1

N k ln yk = −

|Cǫ|

X

k=1

Nk
HW

ln

yk
HW

(10)

In our implementation, we directly ﬂatten the 2D prediction
{yhw, h = 1, 2, · · · , H, w = 1, 2, · · · , W} into 1D predic-
tion {yt, t = 1, 2, · · · , T }, where T = HW, and then apply
Eq. (8) to calculate the ﬁnal loss.

3.4. Implementation and Complexity Analysis

Implementation As illustrated in Eq.

(2), N =
{Nk|k = 1, 2, · · · , |Cǫ|} represents the annotation for the
ACE loss function; here, Nk represents the number of times
that the character Cǫ
k occurs in the sequence annotation S.
A simple example describing the translation of sequence an-
notation cocacola into ACE’s annotation is shown in Fig. 2.
In conclusion, given the model prediction yt
k and its anno-
tation N , the key implementation for a cross-entropy-based
ACE loss function consists of four fundamental formulas:

• yk = PT

t=1 yt

k to calculate the character number of each
class by summing up the probabilities of the k-th class
for all T time-steps.

• yk = yk/T to normalize the accumulative probabilities.
• N k = Nk/T to normalize the annotation.

• L(I, S) = −P|Cǫ|
entropy between N k and yk.

k=1 N k ln yk to estimate the cross-

6541

In practical employment, the model prediction yt

k is gen-
erally provided by the integrated CNN-LSTM model (1D
prediction) or FCN model (ﬂattened 2D prediction). That
is, the input assumption of ACE is identical to that of CTC;
therefore, the proposed ACE can be conveniently applied
by replacing the CTC layer in the framework.

Complexity Analysis The overall computation of the
ACE loss function is implemented based on the above-
mentioned four formulas that have computation complex-
ities of O(1), O(|Cǫ|), O(|Cǫ|), and O(|Cǫ|), respective-
ly. Therefore, the computation complexity of the ACE loss
function is O(|Cǫ|). Note however that the element-wise
multiplication, division, and log operation in these four for-
mulas can be implemented in parallel with GPU at O(1).
In contrast, the implementation of CTC [12] based on a
forward-backward algorithm has a computation complexi-
ty of O(T ∗ |S|). Because the forward variable α(t, u) and
backward variable β(t, u) [12] of CTC depend on the pre-
vious result (e.g., α(t − 1, u) and β(t + 1, u)) to calculate
the present output, CTC can hardly be accelerated in paral-
lel in the time dimension. Moreover, the elementary oper-
ation α(t, u) of CTC is already very complicated, resulting
in larger overall time consumption than that of ACE. With
regard to the attention mechanism, its computation com-
plexity is proportional to the times of ‘attention’. However,
the computation complexity of the attention module at each
time already has similar magnitude as that of CTC.

From the perspective of memory consumption,

the
proposed ACE loss function requires nearly no memo-
ry consumption because the ACE loss result can be di-
rectly calculated based on the four fundamental formulas.
However, CTC requires additional space to preserve the
forward\backward variable that is proportional to the time-
step T and the length of the sequence annotation. Mean-
while, the attention mechanism requires additional module
to implement “attention”. Thus, its memory consumption is
signiﬁcantly larger than that of CTC and ACE.

In conclusion, the proposed ACE loss function exhibit-
s signiﬁcant advantages with regard to both computation
complexity and memory demand, as compared to CTC and
attention.

4. Performance Evaluation

In our experiment, three tasks were employed to evalu-
ate the effectiveness of the proposed ACE loss function, in-
cluding scene text recognition, ofﬂine handwritten Chinese
text recognition, and counting objects in everyday scenes.
For these tasks, we estimated the ACE loss for 1D and 2D
predictions, where 1D implies that the ﬁnal prediction is a
sequence of T predictions and 2D indicates that the ﬁnal
feature map has 2D predictions of shape H × W.

4.1. Scene Text Recognition

Scene text recognition often encounter problems owing
to the large variations in the background, appearance, reso-
lution, text font, and color, making it a challenging research
topic. In this section, we study both 1D and 2D predictions
on scene text recognition by utilizing the richness and vari-
ety of the testing benchmark in this task.

4.1.1 Dataset

Two types of datasets are used for scene text recognition:
regular text datasets, such as IIIT5K-Words [35], Street
View Text [43], ICDAR 2003 [33], and ICDAR 2013 [24],
and irregular text datasets, such as SVT-Perspective [36],
CUTE80 [37], and ICDAR 2015 [23]. The regular dataset-
s were used to study the 1D prediction for the ACE loss
function while the irregular text datasets were applied to e-
valuate the 2D prediction.

IIIT5K-Words (IIIT5K) contains 3000 cropped word im-

ages for testing.

Street View Text (SVT) was collected from Google Street
View, including 647 word images. Many of them are severe-
ly corrupted by noise and blur, or have very low resolutions.
ICDAR 2003 (IC03) contains 251 scene images that are
labeled with text bounding boxes. The dataset contains 867
cropped images.

ICDAR 2013 (IC13) inherits most of its samples from

IC03. It contains 1015 cropped text images.

SVT-Perspective (SVT-P) contains 639 cropped images
for testing, which are selected from side-view angle snap-
shots from Google Street View. Therefore, most of images
are perspective distorted. Each image is associated with a
50-word lexicon and a full lexicon.

CUTE80 (CUTE) contains 80 high-resolution images
taken of natural scenes.
It was speciﬁcally collected for
curve text recognition. The dataset contains 288 cropped
natural images for testing. No lexicon is associated.

ICDAR 2015 (IC15) contains 2077 cropped images in-
cluding more than 200 irregular text. No lexicon is associ-
ated.

4.1.2

Implementation Details

For 1D sequence recognition on regular datasets, our ex-
periments were based on the CRNN [38] network, trained
only on 8-million synthetic data released by Jaderberg et al.
[22]. For 2D sequence recognition on irregular datasets, our
experiments were based on the ResNet-101 [18], with con-
v1 changed to 3×3, stride 1, and conv4 x as output. The
training dataset consists of 8-million synthetic data released
by Jaderberg et al. [22] and 4-million synthetic instances
(excluding the images that contain non-alphanumeric char-
acters) cropped from 80-thousand images [17]. The input

6542

Table 1. Comparison between regression and cross-entropy.

Method

Shi et al. [38]

ACE (1D, Regression)

ACE (1D, Cross Entropy)

IIIT5K SVT
82.7
6.6
82.6

81.2
19.4
82.3

IC03
91.9
12.0
92.1

IC13
89.6
9.3
89.7

images are normalized to the shape of (96,100) and the ﬁnal
2D prediction has the shape of (12,13), as shown in Fig. 5.
To decode the 2D prediction, we ﬂattened the 2D prediction
by concatenating each column in order from left to right and
top to bottom and then decoded the ﬂattened 1D prediction
following the general procedure.

In our experiment, we observed that directly normaliz-
ing the input image to the size of (96,100) overloads the
network training process. Therefore, we trained another
network to predict the character number in the text image
and normalized the text image with respect to the character
number to keep the character size within acceptable limits.

4.1.3 Experimental Result

To study the role of regression and cross-entropy for the
ACE loss function, we conducted experiments with 1D pre-
diction using regular scene text datasets, as detailed in Table
1 and Fig. 3. Because there are only 37 classes in scene text
recognition, the negative effect of the term yt
k′ in Eq. (7) is
not as signiﬁcant as that of the HCTR problem (7357 class-
es). As shown in Fig. 3, with regression-based ACE loss,
the network can converge but at a relatively slow rate, prob-
ably due to the gradient vanishing problem. With cross-
entropy-based ACE loss, the WER and CER evolve at a rel-
atively higher rate and in a smoother manner at the early
training stage and attain a signiﬁcantly better convergence
result in the subsequent training stage. Table 1 clearly re-
veals the superiority of the cross-entropy-based ACE loss
function over the regression-based one. Therefore, we use
cross-entropy-based ACE loss functions for all the remain-
ing experiments. Moreover, with the same network setting
(CRNN) and training set (8-million synthetic data), the pro-
posed ACE loss function exhibits performance comparable

Table 2. Comparison with previous methods for scene text recog-
nition problem (without rectiﬁcation)

Method

2D

50
92.6
Shi et al. [38]
94.3
Liu et al. [28]
Yang et al. [51] X 93.0
Cheng et al. [7] X 92.6
Cheng et al. [8] X 94.0
Liu et al. [29]
Shi et al. [39]
ACE (2D)

X 94.9

–
–

SVT-P
Full None
66.8
72.6
83.6
73.5
75.8
80.2
71.5
81.6
73.0
83.7
73.9
74.1
70.1

87.8

–
–

CUTE
None
54.9

-

69.3
63.9
76.8
62.5
73.3
82.6

IC15
None

-
-
-

66.2
68.2

–
–

68.9

with that of previous work [38] with CTC.

To validate the independence of the proposed ACE loss
to character order, we conduct experiments with ACE, CTC,
and attention on four datasets; the character order of anno-
tation is randomly shufﬂed at different ratios, as shown in
Fig. 4. It is observed that the performance of attention and
CTC on all the datasets degrades as the shufﬂe ratio increas-
es. Speciﬁcally, attention is more sensitive than CTC be-
cause misalignment problem can easily misleads the train-
ing process of attention [3]. In contrast, the proposed ACE
loss function exhibits similar recognition results for all the
settings of the shufﬂe ratio, this is because it only requires
classes and their number for supervision, completely omit-
ting character order information.

For irregular scene text recognition, we conducted tex-
t recognition experiments with 2D prediction. In Table 2,
we provide a comparison with previous methods that con-
sidered only recognition model and no rectiﬁcation for fair
comparison. As illustrated in Table 2, the proposed ACE
loss function exhibits superior performance on the dataset-
s CUTE and IC15, particularly on CUTE with an abso-
lute error reduction of 5.8%. This is because the dataset
CUTE was speciﬁcally collected for curved text recogni-
tion and therefore, fully demonstrates the advantages of the
ACE loss function. For the dataset SVT-P, our naive de-
coding result is less effective than that of Yang et al. [51].
This is because numerous images in the dataset SVT-P have

Figure 3. Word error rate (left) and character error rate (right)
of ACE loss on validation set under regression and cross entropy
perspective.

Figure 4. Performance of ACE, CTC, and attention under different
shufﬂe ratios and datasets.

6543

very low resolutions, which creates a very high requiremen-
t for semantic context modeling. However, our network is
based only on CNN, with neither LSTM/MDLSTM nor at-
tention mechanism to leverage the high-level semantic con-
text. Nevertheless, it is noteworthy that our recognition
model achieved the highest result when using lexicons, with
which semantic context is accessible. This again validates
the robustness and effectiveness of the proposed ACE loss
function.

In Fig. 5, we provide a few real images processed by
a recognition model using the ACE loss function. The o-
riginal text images were ﬁrst normalized and placed in the
center of a blank image of shape (96, 100). We observe that
after recognition, the 2D prediction exhibits a spatial distri-
bution highly similar to that of the characters in the original
text image, which implies the effectiveness of the proposed
ACE loss function.

Figure 5. Real images processed by recognition model using ACE
loss function. The left two columns represent original text images
and their normalized versions within the shape of (96, 100). The
third column shows the 2D prediction of the recognition model for
the text images. In the right column, we overlap the input and the
prediction images, and observe similar character distribution in the
2D space.

(126, 576)Input − 8C3 − M P 2 − 32C3 − M P 2 − 128C3 −
M P 2−5∗256C3−M P 2−512C3−512C3−M P 2−512C2−
3 ∗ 512ResLST M − 7357F C − Output,
where xCy represents a convolutional layer with kernel
number of x and kernel size of y ∗ y, M P y denotes a max
pooling layer with kernel size of y, and xF C is a fully
connected layer with kernel number of x, and ResLSTM
is residual LSTM proposed in [49]. The evaluation criteria
for the HCTR problem are correct rate (CR) and accuracy
rate (AR) speciﬁed by ICDAR2013 competition [53].

4.2.2 Experimental Result

In Table 3, we provide the comparison between ACE loss
and previous methods.
It is evident that the proposed
ACE loss function exhibits higher performance than previ-
ous methods, including MDLSTM-based models [34, 47],
HMM-based model [10], and over-segmentation method-
s [27, 44, 45, 48] with and without language model (LM).
Compared to scene text recognition, handwritten Chinese
text recognition problem possesses its unique challenges,
such as large character set (7357 classes) and character-
touching problem. Therefore, the superior performance of
ACE loss function over previous methods can properly ver-
ify its robustness and generality for sequence recognition
problems.

Table 3. Comparison with previous methods for HCTR.

Method

HIT-2 [27]
Wang et al. [44]
Messina et al. [34]
Wu et al. [47]
Du et al. [10]
Wang et al. [45]
Wu et al. [48]
ACE (1D)

w.o LM

with LM

CR
–
–
–

87.43

–

90.67

–

AR
–
–

83.50
86.64
83.89
88.79

–

91.68

91.25

CR

88.76
91.39

–
–
–

95.53
96.32
96.70

AR
86.73
90.75
89.40
92.61
93.50
94.02
96.20
96.22

4.2. Ofﬂine Handwritten Chinese Text Recognition

4.3. Counting Objects in Everyday Scenes

Owing to its large character set (7,357 classes), diverse
writing style, and character-touching problem, the ofﬂine
HCTR problem is highly complicated and challenging to
solve. Therefore, it is a favorable testbed to evaluate the
robustness and effectiveness of the ACE loss in 1D predic-
tions.

Counting the number of instances of object classes in
natural everyday images generally encounters complex re-
al life situations, e.g., large variance in counts, appearance,
and scales of object. Therefore, we veriﬁed the ACE loss
function on the problem of counting objects in everyday
scenes to demonstrate its generality.

4.2.1

Implementation Details

4.3.1

Implementation Details

For the ofﬂine HCTR problem, our model was trained using
the CASIA-HWDB [26] datasets and tested with the stan-
dard benchmark ICDAR 2013 competition dataset [53].

For the HCTR problem, our network architecture with a

prediction sequence of length 70 is speciﬁed as follows:

As a benchmark for multi-label object classiﬁcation and ob-
ject detection tasks, the PASCAL VOC [11] datasets contain
category labels per image, as well as bounding box anno-
tations that can be converted to the object number label-
s. In our implementation, we accumulated the prediction

6544

for category k to obtain ˆcik by thresholding counts at ze-
ro and rounding predictions to the closest integers. Giv-
en these predictions and the ground truth counts cik for a
category k and image i, RMSE and relRMSE is calculat-
ed by RMSEk = q 1
i=1(ˆcik − cik)2 and relRMSEk =
q 1
.

N PN

(ˆcik−cik)2

cik+1

i=1

N PN

4.3.2 Experimental Result

Table 4 presents a comparison between the proposed ACE
loss function and previous methods for the PASCAL VOC
2007 test dataset for counting objects in everyday scenes.
The proposed ACE loss function outperforms the previ-
ous glancing and subitizing method [6], correlation loss
method [40], and Always-0 method (predicting most-
frequent ground truth count). The results have shown the
generality of ACE loss function, in that it can be readily
applied to problem other than sequence recognition, e.g.,
counting problems, requiring minimal domain knowledge.
In Fig. 6, we provide real images processed by the count-
ing model under ACE loss. As shown in the images, our
counting model trained with ACE loss manage to pay “at-
tention” to the position where crucial objects occur. Unlike
the text recognition problem, where the recognition model
trained with the ACE loss function tends to make a pre-
diction for a character, the counting model trained with the
ACE loss function provides a more uniform prediction dis-
tribution over the body of the object. Moreover, it assigns
different levels of “attention” to different parts of an object.
For example, when observing the red color in the pictures,
we notice that the counting model pays more attention to
the face of a person. This phenomenon corresponds to our
intuition because the face is the most distinctive part of an
individual.

Table 4. Comparison with previous methods on PASCAL VOC
2007 test dataset for object counting problem.

Method
Always-0
Glance [6]
Sub-ens [6]
Two-stream [40]
ACE (2D)

m-RMSE m-relRMSE

0.665
0.500
0.420
0.389
0.381

0.284
0.270
0.200
0.189
0.185

4.4. Complexity Analysis

In Table 5, we compare the parameter, runtime memory,
and run time of ACE with those of CTC and attention. The
result is executed with minibatch 64 and model prediction
length T=144 on a single NVIDIA TITAN X graphics card
of 12GB memory. Similar to CTC, the proposed ACE does
not require any parameter to fulﬁll its function. Owing to
its simplicity, ACE requires marginal runtime memory, ﬁve

Figure 6. Real images processed by counting model using ACE
loss function. The ﬁrst four columns display examples that are cor-
rectly recognized by our model. The top-right image is correctly
recognized, but with an incorrectly annotated label. (Incorrect pre-
dictions are provided with labels in brackets)

times less than those for CTC and attention. Furthermore,
its speed is as least 30 times higher than those of CTC and
attention. It is note worthy that with all these advantages,
the proposed ACE achieve performance that is comparable
or higher than those with CTC and attention.

Table 5. Investigation over parameter (Para), runtime memory
(Mem), and speed (Speed) (in units of MB, MB, and ms, respec-
tively) of CTC, attention, and ACE.

Method

CTC

Attention

ACE

37 classes

7357 classes

Para Mem Time
3.1
none
0.1
6.6
2.8
78.9
0.02 <0.1
none

Para Mem Time
16.2
none
85.5
17.2
<0.1
none

47.8
143.6

4.2

5. Conclusion

In this paper, a novel and straightforward ACE loss func-
tion is proposed for sequence recognition problem with
competitive performance to CTC and attention. Owing to
its simplicity, the ACE loss function is easy to employ by
simply replacing CTC with ACE, quick to implement with
only four basic formulas, fast to infer and back-propagate
at approximately O(1) in parallel, and exhibits marginal
memory requirement. Two following effective properties
of ACE loss function are also investigated: (1) it can eas-
ily handle 2D prediction problem with marginal adaption
and (2) it does not require character-order information for
supervision, which allows it to advance beyond sequence
recognition problem, e.g., counting problem.

Acknowledgments

This

research is

supported in part by GD-NSF (no.
2017A030312006), the National Key Research and Developmen-
t Program of China (No. 2016YFB1001405), NSFC (Grant No.:
61673182, 61771199), and GDSTP (Grant No.:2017A010101027)
, GZSTP(no. 201704020134).

6545

References

[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine trans-
lation by jointly learning to align and translate. CoRR, ab-
s/1409.0473, 2014.

[2] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and
Y. Bengio. End-to-end attention-based large vocabulary
speech recognition. In ICASSP, 2016.

[3] F. Bai, Z. Cheng, Y. Niu, S. Pu, and S. Zhou. Edit probability

for scene text recognition. CVPR, 2018.

[4] T. Bluche. Joint line segmentation and transcription for end-

to-end handwritten paragraph recognition. NIPS, 2016.

[5] T. Bluche and R. Messina. Scan, attend and read: End-to-
end handwritten paragraph recognition with mdlstm atten-
tion. ICDAR, 2016.

[6] P. Chattopadhyay, R. Vedantam, R. R. Selvaraju, D. Ba-
tra, and D. Parikh. Counting everyday objects in everyday
scenes. In CVPR, 2017.

[7] Z. Cheng, F. Bai, Y. Xu, G. Zheng, S. Pu, and S. Zhou. Fo-
cusing attention: Towards accurate text recognition in natural
images. In ICCV, 2017.

[8] Z. Cheng, X. Liu, F. Bai, Y. Niu, S. Pu, and S. Zhou.

Arbitrarily-oriented text recognition. ICDAR, 2017.

[9] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and
Y. Bengio. Attention-based models for speech recognition.
In NIPS, 2015.

[10] J. Du, Z.-R. Wang, J.-F. Zhai, and J.-S. Hu. Deep neural
network based hidden markov model for ofﬂine handwritten
chinese text recognition. In ICPR.

[11] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective. IJCV, 2015.

[12] A. Graves. Supervised sequence labelling. Springer, 2012.
[13] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber.
Connectionist temporal classiﬁcation: labelling unsegment-
ed sequence data with recurrent neural networks.
ICML,
2006.

[14] A. Graves and N. Jaitly. Towards end-to-end speech recog-

nition with recurrent neural networks. ICML, 2014.

[15] A. Graves, M. Liwicki, S. Fern´andez, R. Bertolami,
H. Bunke, and J. Schmidhuber. A novel connectionist sys-
tem for unconstrained handwriting recognition. IEEE TPA-
MI, 2009.

[16] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recogni-
tion with deep recurrent neural networks. In ICASSP, 2013.
[17] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for

text localisation in natural images. In CVPR, 2016.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016.

[19] P. He, W. Huang, Y. Qiao, C. C. Loy, and X. Tang. Reading

scene text in deep convolutional sequences. AAAI, 2016.

[20] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[21] K. Hwang and W. Sung. Sequence to sequence training of

ctc-rnns with partial windowing. In ICML, 2016.

[23] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh,
A. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R.
Chandrasekhar, S. Lu, et al. Icdar 2015 competition on ro-
bust reading. In ICDAR, 2015.

[24] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big-
orda, S. R. Mestre, J. Mas, D. F. Mota, J. A. Almazan, and
L. P. De Las Heras. Icdar 2013 robust reading competition.
In ICDAR, 2013.

[25] S. Kim, T. Hori, and S. Watanabe. Joint ctc-attention based
end-to-end speech recognition using multi-task learning. In
ICASSP, 2017.

[26] C.-L. Liu, F. Yin, D.-H. Wang, and Q.-F. Wang. Casia online

and ofﬂine chinese handwriting databases. ICDAR, 2011.

[27] C.-L. Liu, F. Yin, Q.-F. Wang, and D.-H. Wang.

ICDAR

2011 chinese handwriting recognition competition (2011).

[28] W. Liu, C. Chen, K.-Y. K. Wong, Z. Su, and J. Han. Star-net:
A spatial attention residue network for scene text recogni-
tion. In BMVC, 2016.

[29] Y. Liu, Z. Wang, H. Jin, and I. Wassell. Synthetically super-
vised feature learning for scene text recognition. In ECCV,
2018.

[30] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. CVPR, 2015.

[31] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when
to look: Adaptive attention via a visual sentinel for image
captioning. In CVPR, 2017.

[32] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
question-image co-attention for visual question answering.
In NIPS, 2016.

[33] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, R. Y-
oung, K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto,
et al.
Icdar 2003 robust reading competitions: entries, re-
sults, and future directions. IJDAR, 2005.

[34] R. Messina and J. Louradour. Segmentation-free handwritten

chinese text recognition with lstm-rnn. ICDAR, 2015.

[35] A. Mishra, K. Alahari, and C. Jawahar. Scene text recogni-

tion using higher order language priors. In BMVC, 2012.

[36] T. Quy Phan, P. Shivakumara, S. Tian, and C. Lim Tan. Rec-
ognizing text with perspective distortion in natural scenes. In
ICCV, 2013.

[37] A. Risnumawan, P. Shivakumara, C. S. Chan, and C. L. Tan.
A robust arbitrary text detection system for natural scene im-
ages. Expert Systems with Applications, 2014.

[38] B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural
network for image-based sequence recognition and its appli-
cation to scene text recognition. IEEE TPAMI, 2016.

[39] B. Shi, M. Yang, X. Wang, P. Lyu, C. Yao, and X. Bai. Aster:
An attentional scene text recognizer with ﬂexible rectiﬁca-
tion. IEEE TPAMI, 2018.

[40] Z. Song and Q. Qiu. Learn to classify and count: A uniﬁed
framework for object classiﬁcation and counting. In ICIGP,
2018.

[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CVPR, 2015.

[22] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.
Synthetic data and artiﬁcial neural networks for natural scene
text recognition. CoRR, abs/1406.2227, 2014.

[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need. In NIPS, 2017.

6546

[43] K. Wang, B. Babenko, and S. Belongie. End-to-end scene

text recognition. In ICCV, 2011.

[44] Q.-F. Wang, F. Yin, and C.-L. Liu. Handwritten chinese text
recognition by integrating multiple contexts. IEEE TPAMI,
2012.

[45] S. Wang, L. Chen, L. Xu, W. Fan, J. Sun, and S. Naoi. Deep
knowledge training and heterogeneous cnn for handwritten
chinese text recognition. In ICFHR, 2016.

[46] C. Wigington, C. Tensmeyer, B. Davis, W. Barrett, B. Price,
and S. Cohen. Start, follow, read: End-to-end full-page hand-
writing recognition. In ECCV, 2018.

[47] Y.-C. Wu, F. Yin, Z. Chen, and C.-L. Liu. Handwritten chi-
nese text recognition using separable multi-dimensional re-
current neural network. In ICDAR, 2017.

[48] Y.-C. Wu, F. Yin, and C.-L. Liu. Improving handwritten chi-
nese text recognition using neural network language model-
s and convolutional neural network shape models. Pattern
Recognition, 2017.

[49] Z. Xie, Z. Sun, L. Jin, H. Ni, and T. Lyons. Learning spatial-
semantic context with fully convolutional recurrent network
for online handwritten chinese text recognition. TPAMI,
2018.

[50] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention. CoRR, ab-
s/1502.03044, 2015.

[51] X. Yang, D. He, Z. Zhou, D. Kifer, and C. L. Giles. Learning
to read irregular text with attention mechanisms. In IJCAI,
2017.

[52] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
attention networks for image question answering. In CVPR,
2016.

[53] F. Yin, Q.-F. Wang, X.-Y. Zhang, and C.-L. Liu.

ICDAR
2013 chinese handwriting recognition competition. ICDAR,
2013.

[54] F. Yin, Y.-C. Wu, X.-Y. Zhang, and C.-L. Liu. Scene tex-
t recognition with sliding convolutional character models.
CoRR, abs/1709.01727, 2017.

[55] B. Zhang, Y. Gan, Y. Song, and B. Tang. Application of
pronunciation knowledge on phoneme recognition by lstm
neural network. In ICPR, 2016.

[56] J. Zhang, J. Du, and L. Dai. Track, attend and parse (tap): An
end-to-end framework for online handwritten mathematical
expression recognition. TMM, 2018.

[57] J. Zhang, J. Du, S. Zhang, D. Liu, Y. Hu, J. Hu, S. Wei, and
L. Dai. Watch, attend and parse: An end-to-end neural net-
work based approach to handwritten mathematical expres-
sion recognition. Pattern Recognition, 2017.

6547

