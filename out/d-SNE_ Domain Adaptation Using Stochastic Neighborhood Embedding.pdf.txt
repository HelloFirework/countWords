d-SNE: Domain Adaptation using Stochastic Neighborhood Embedding

Xiang Xu‚Ä† ‚àó, Xiong Zhou ‚Ä°‚àó, Ragav Venkatesan‚Ä°, Gurumurthy Swaminathan‚Ä°, Orchid Majumder‚Ä°

‚Ä†Computational Biomedicine Lab, University of Houston, Houston, USA

‚Ä†xxu18@central.uh.edu, ‚Ä°{xiongzho,ragavven,gurumurs,orchid}@amazon.com

‚Ä° AWS AI, Seattle, USA

Abstract

Deep neural networks often require copious amount of
labeled-data to train their scads of parameters. Training
larger and deeper networks is hard without appropriate reg-
ularization, particularly while using a small dataset. Lat-
erally, collecting well-annotated data is expensive, time-
consuming and often infeasible. A popular way to regu-
larize these networks is to simply train the network with
more data from an alternate representative dataset. This
can lead to adverse effects if the statistics of the represen-
tative dataset are dissimilar to our target. This predica-
ment is due to the problem of domain shift. Data from a
shifted domain might not produce bespoke features when
a feature extractor from the representative domain is used.
In this paper, we propose a new technique (d-SNE) of do-
main adaptation that cleverly uses stochastic neighborhood
embedding techniques and a novel modiÔ¨Åed-Hausdorff dis-
tance. The proposed technique is learnable end-to-end and
is therefore, ideally suited to train neural networks. Exten-
sive experiments demonstrate that d-SNE outperforms the
current states-of-the-art and is robust to the variances in
different datasets, even in the one-shot and semi-supervised
learning settings. d-SNE also demonstrates the ability to
generalize to multiple domains concurrently.

1. Introduction

The use of pre-trained models and transfer-learning
have become commonplace in today‚Äôs deep learning-centric
computer vision. Consider a pre-trained model MDs
trained using a large dataset Ds = {(xs
i=1, where
xs
i is the ith sample of the sth domain and N s is the number
of samples in the sth domain. Suppose that a typical user has
j=1, with N t << N s, on
a smaller dataset Dt = {(xt
which they want to train their model. Also consider that the
label spaces are the same, i.e., {ys, yt} ‚àà [0, 1, . . . c ‚àí 1].

i )}N s

j)}N t

i , ys

j, yt

‚àóEqual contribution. This work was done during Xiang‚Äôs internship at

AWS AI.

ùê∑#

ùê∑"

ùê∑"

Expectation:

ùê∑" ‚à™ ùê∑# = ùê∑#	

Direct Deployment

Reality:

ùê∑" ‚à© ùê∑# = 	‚àÖ	

Domain Adaptation

ùê∑#$

ùê∑#%

Distribution of labeled training set, ùê∑"

Distribution of labeled evaluation set, ùê∑#$

Distribution of unlabeled evaluation set ùê∑#%

Figure 1: Domain adaptation in the true data space: Expec-
tation vs. Reality.

The user should be able to repurpose the model MDs to
work with dataset Dt. Unless the user is extremely lucky as
shown in the top case of Ô¨Ågure 1, such a deployment will not
work. This is due to domain-shift. Features become mean-
ingless and their spaces get transformed, therefore classiÔ¨Åer
boundaries have to be redrawn. The class of such problems
where the knowledge from another domain is recycled to
work to a new target domain is called domain adaptation.
If the solution can perform equally-well in both domains, it
is called as domain generalization.

Typical choices of dataset for source are large-scale
datasets such as ImageNet [3]. Donahue et al. popular-
ized the idea of repurposing networks trained on this dataset
to be used as generic feature extractors [5]. They hypoth-
esized and successfully demonstrated that in many cases,
when there is limited labeled data available in the target do-
main, as long as it contains only natural images, the feature
extractors learnt from ImageNet are general enough to pro-
duce discriminative features. Follow-up studies have anal-
ysed the transferability of neural networks and the gener-
ality of datasets in-detail [33, 29]. In all these cases, the

2497

label-space is considered independently for both domains
and the classiÔ¨Åer layer of the networks are sanitized. Do-
main adaptation improves the performance of an existing
model MDs for Dt by adapting the knowledge of the model
learned from Ds to Dt with the assumption that the label
spaces are same, and therefore not needing to sanitize the
classiÔ¨Åer layers [19, 31]. There are two different philoso-
phies in which domain adaptation is typically attacked: (i)
Domain Transformation: To build a transformation from
target data to source domain and reuse the source feature
extractor and classiÔ¨Åer (xt ‚Üí xs). Consider the GAN-
based methods [1, 21]. These work on the input-level and
transform samples from the target domain to mimic distri-
butions of source domains. (ii) Latent-Space Transforma-
tion: To build a transformation of features extracted from
source and features extracted from target into each other or
into a common latent space. Since these are working on
the conditional feature spaces, these methods are typically
supervised.

Figure 2 illustrates the major branches and types of do-
main adaptation. d-SNE falls under the latent-space trans-
formation philosophy, where we create a joint-latent em-
bedding space that is agnostic and invariant to domain-shift.
We also focus on the tougher problem where N t << N s,
or few-shot supervised domain adaptation. This imposes a
constraint that only a few labeled-target samples are avail-
able.

To create this embedding space, we use a strategy that
is very similar to the popular stochastic neighborhood em-
bedding technique (SNE) [12]. To modiÔ¨Åy SNE for do-
main adaptation, we use a novel modiÔ¨Åed-Hausdorff dis-
tance metric in a min ‚àí max formulation. d-SNE mini-
mizes the distance between the samples from Ds and Dt
so as to maximize the margin of inter-class distance for dis-
crimination and minimize the intra-class distance from both
domains to achieve domain-invariance. This discrimination
is learnt as a max-margin nearest-neighbor form to make the
network optimization easy. Our proposed idea is still learn-
able in an end-to-end fashion, therefore making it ideal for
training neural networks.

Extensive experimental results in different scenarios in-
dicate that our algorithm is robust and outperforms the state-
of-the-art algorithms with only a few labeled data sam-
In several cases, d-SNE outperforms even unsuper-
ples.
vised methods that have access to all samples in the tar-
get domain. We generalize d-SNE such that it can work on
a semi-supervised setting that further pushes the states-of-
the-art. Furthermore, d-SNE also demonstrates good capa-
bilities in domain generalization without additional training
required, which is typically not the case in any state-of-the-
art.

The key contributions in this paper include the following:

1. Use of stochastic neighborhood embedding and large-

margin nearest neighborhood to learn a domain-
agnostic latent-space.

2. Use of a modiÔ¨Åed-Hausdorff distance and a novel
min ‚àí max formulation in this space to help few-shot
supervised learning.

3. Demonstration of domain generalization and achiev-
ing states-of-the-art results on common benchmark
datasets.

4. Extension to semi-supervised settings pushing the

states-of-the-art further.

The rest of this article is organized as follows: section 2
surveys related literature in all categories described in Ô¨Ågure
2, section 3 dives deep into the proposed idea and provides
a theoretical exposition, section 4 presents validation of this
idea through experimental evidence and section 5 provides
concluding remarks.

2. Related Works

In this section we will survey recent related works in

each category of domain adaptation.
Domain transformation: In this category, methods learn a
generative model that can transform either the source to the
target domain (which is more common) or vice-versa. To
learn this generative model itself, no supervision is required.
Since unlabeled data is available in plenty, this model can
be learnt easily leveraging a plethora of unsupervised data.
They use this generative model to prepare a joint dataset in
either one of the domains and learn a feature extractor and
classiÔ¨Åer in that common domain [14, 1, 13, 21].

With the advent of Generative Adversarial Networks
(GANs) [9] these transformations have become easier Liu
and Tuzel proposed a pair of GANs, each of which was re-
sponsible for synthesizing the images in the source and tar-
get domain, respectively [14]. With an innovative weight-
sharing constraint as a regularizer, the generative models
were used for generating a pair of images in two domains.
Rather than generating the images from random variables in
two domains, the generator in PixelDA, proposed by Bous-
malis et al. transformed the images from the source domain
and forced them to map into the distribution of the target
domain [1]. CyCADA, proposed by Hoffman et al. used
cycle-consistent loss and semantic loss along with the GAN
losses and bettered the state-of-the-art [13].

While all the previous methods transformed the source
domain data to target domain, Russo et al. proposed
SBADA-GAN, which also considered the transformation
from target to source domain [21]. They deÔ¨Åned class con-
sistency loss, which learned to obtain the same label used
when mapping from source to target and back tp the source
domain. Since they generated images in both domains, they

2498

Feature Space

Shared Feature Space

ùëì(#)

ùëì(#)

ùëî(#)

‚Ñé(#)

Domain 
Adaptation

Domain 

Transformation

Generative Adversarial 

Learning

Adversarial 

Learning

Source Domain

Target Domain

Source Domain

Target Domain

Domain Transformation

Latent-space Transformation

Latent-Space 
Transformation

Multi-task 
Learning

Determinative 

Learning

Action Scope level

Loss function level

Figure 2: Various types of domain adaptation.

learnt two independent classiÔ¨Åers in each. This implied that
they were able to make a prediction using the linear combi-
nation or predictions from both domains.

Latent-space transformation: Latent-space transforma-
tions can be further divided into two major categories: do-
main adversarial learning (DAL) and domain multi-task
learning (DMTL).

Domain adversarial learning: Perhaps the most popular
of DAL techniques is the Domain Adversarial Neural Net-
works (DANN) introduced by Ganin et al. [7]. This work
introduced a gradient reversal layer to Ô¨Çip the gradients
when the network was back-propagating. Using this gra-
dient Ô¨Çipping, they were able to learn both a discrimina-
tive and a domain-invariant feature space. The network was
optimized to simultaneously minimize the label error and
maximize the loss of the domain classiÔ¨Åer. Tzeng et al.
generalized the architecture of adversarial domain adapta-
tion for unsupervised learning in their work, Adversarial
Discriminative Domain Adaptation (ADDA) [28]. ADDA
used two independent discriminators from source and tar-
get domain to map features in the shared feature space. A
label-relaxed version of domain adversarial learning was
proposed in [2].

Domain multi-task learning: In order to improve the dis-
criminative capabilities of feature representations, Tzeng et
al. introduced a shared feature extractor for both source
and target domain with three different losses in a multi-
task learning manner [27]. Ding et al. uses a knowledge
graph model to jointly optimize target labels with domain-
free features in a uniÔ¨Åed framework [4]. These losses also
acted as a strong regularizer. Rozantsev et al. argued that
the weights of the network learnt from different domain
should be related, yet different for each other [20]. To this
end, they added linear transformations between the weights
to regularize the networks to behave thusly. Associative Do-
main Adaptation is another technique in the DMTL regime
proposed by Haeusser et al. which enforced association be-
tween the source and target domains [10]. CCSA and FADA
furthered the contrastive loss techniques by creating a uni-

Ô¨Åed framework for supervised domain adaptation and gen-
eralization [16, 15]. A decision-boundary iterative reÔ¨Åne-
ment training strategy (DIRT-T) was proposed by Shu et
al. which required an initialization using virtual adversarial
training [25]. They reÔ¨Åned the model‚Äôs weights with a KL
divergence loss. Self-ensembling extended the mean teacher
model in the domain adaptation setting and introduced some
tricks such as conÔ¨Ådence thresholding, data augmentation,
and class imbalance loss [6, 26]. Others learn a shared fea-
ture space from the images in the source and target domain
[30, 23] .

3. d-SNE

Consider the distance between a sample from the source

domain and one from a target domain in the latent-space,

d(xs

i , xt

j) = kŒ¶Ds (xs

i ) ‚àí Œ¶Dt (xt

j)k2
2,

(1)

where Œ¶Ds (¬∑) ‚Üí Rd and Œ¶Dt (¬∑) ‚Üí Rd are neural networks
that transform the samples to a common latent-space of d-
dimensions from the source and target domains, parameter-
ized by ws and wt respectively. In this latent-space,

pij =

exp(‚àíd(xs

i , xt

j))

Px‚ààDs exp(‚àíd(x, xt

j))

.

(2)

i = yt

i and xt

j , which are ys

j ‚àà Dt has the
is the probability that the target sample xt
same label as the source sample xs
i ‚àà Ds. Since we are
working under the supervised regime, we actually have the
label for both xs
j , respectively.
If ys
j , we want pij to be maximized. If otherwise, we
want pij to be minimized. Notice that in this framework,
the training samples in the source domain are chosen from
a probability distribution that favors nearby points over far-
away ones. In other words, the larger the distance between x
and xt
j , the smaller probability for selecting x as the neigh-
bor of xt

j for any sample x ‚àà Ds.

i and yt

Consider that yt

j = k and that Ds

k = {‚àÄxs

l |ys

l = k}. The

2499

probability pj of making the correct prediction of xt

j is:

ùíü"

Œ¶ùíü$

pj = Px‚ààDs

k

exp(‚àíd(x, xt

Px‚ààDs exp(‚àíd(x, xt

j))
j))

=

N s
k

Xi=0

pij,

(3)

k|. Notice that given a target sample and
j = k), the source domain Ds is split into two

where, N s
k = |Ds
label (xt
j, yt
parts as a same-class set Ds
k and a different-class set, Ds
6k.
now
in
j)) +
for one sample,

can
exp(‚àíd(x, xt

exp(‚àíd(x, xt

denominator

decomposed

as Px‚ààDs

Given pj

equation

j)).

The

(3)

the objective function for the domain adaptation problem
can be derived as,

Px‚ààDs

be

6k

k

1
pj

Xxj ‚ààDt

= Xxj ‚ààDt Px‚ààDs
Px‚ààDs

6k

k

exp(‚àíd(x, xj))

exp(‚àíd(x, xj))

, for k = yj!.

(4)
Since we want to maximize the probability pj of making
the correct prediction of xj , we minimize the log-likelihood
of 1
, which is equivalent to minimizing the ratio of intra-
pj
class distances to inter-class distances in the latent space.

L = log Px‚ààDs
Px‚ààDs

6k

k

exp(‚àíd(x, xj))

exp(‚àíd(x, xj))

, for k = yj!.

(5)

Relaxation: Since we have sum of exponentials in the like-
lihood formulation, the ratio in equation (5) may have a
scaling issue. This leads to adverse effects in stochastic op-
timization techniques such as stochastic gradient descent.
Since our feature extractors Œ¶Ds and Œ¶Dt are neural net-
works, this is essential. Therefore, we relax this likelihood
with the use of a modiÔ¨Åed-HausdorfÔ¨Åan distance. Instead of
optimizing the global distance as in equation (5), we only
minimize the largest distance between the samples of the
same class and maximize the smallest distance between the
samples of different classes. The Ô¨Ånal loss is,

ÀúL = sup
x‚ààDs
k

{a|a ‚àà d(x, xj)} ‚àí inf
x‚ààDs
6k

{b|b ‚àà d(x, xj)},

for k = yj.

(6)

End-to-End Learning: Our feature extractors are two in-
dependent neural networks Œ¶Ds and Œ¶Dt . Pragmatically,
a single network can be shared between the two domains
(Œ¶Ds = Œ¶Dt ) if the input data from the source and tar-
get domains have the same dimensionality. d-SNE allows
the target points to select neighbors from the source do-
main, therefore, the supervision can be transferred from the
source domain to the target domain. Since we have labeled
data from both domains, standard cross-entropy losses can
be used as regularization on top of the domain adaptation
losses to train the networks. Since each domain gets its own

Œ¶ùíü$ (‚ãÖ)

Œ¶ùíü& (‚ãÖ)

Œ¶.

ùíü& (‚ãÖ)

"
‚Ñí()

‚Ñí*

%
‚Ñí()

‚Ñí(

ùíü%

ùíü%/

Œ¶ùíü&

Œ¶.

ùíü&

Figure 3: The learning setup. The segment in the bottom in
lighter shade and dotted lines is the semi-supervised exten-
sion.

cross-entropy, we create a multi-task setup to learn these
networks in parallel. Our learning formulation is therefore
deÔ¨Åned by,

argmin
ws,wd

ÀúL + Œ±Ls

ce + Œ≤Lt

ce

(7)

Although we have one minimization form, we divide them
for each network, since the weight updates can be con-
ducted independently. Figure 3 illustrates our setup.

Semi-supervised Extension: As was already discussed in
the introduction section, having access to unlabeled data
helps boost performance. d-SNE can be extended easily
to accommodate unlabeled data. This extends our proposal
into a semi-supervised setting. This is illustrated in the bot-
tom row of Ô¨Ågure 3. Suppose that the unlabeled data from
the target domain is represented as Dt
u. We train an unsuper-
vised network ÀÜŒ¶Dt
, parameterized by ÀÜwt to produce an em-
bedding for the unsupervised image in the latent space. Us-
ing a technique similar to the Mean-Teacher network tech-
nique proposed by Tarvainen et al., [26]. We use a consis-
tency loss Lc across ÀÜŒ¶Dt
and Œ¶Dt , by taking an L2 error
between the embeddings.

u

u

In particular, the source and target networks are Ô¨Årst
trained as equation (7). The unlabeled data Dt
u from the tar-
get domain are then used to train the Mean-Teacher model,
where new network networks are initialized with the trained
target network ÀÜŒ¶Dt ‚Üí Œ¶Dt . To generate inputs for both
networks, stochastic augmentations, such as Ô¨Çipping, crop-
ping, color jittering, are used to create two sibling samples.
Since these are two variants of the same sample and belong
to the same class, the consistency loss is an error of the
embedding. The weights of ÀÜŒ¶Dt
network are updated by
back-propagating the consistency loss. Instead of sharing
weights, the weights of Œ¶Dt are updated with an exponen-
tial moving average of the network weights of ÀÜŒ¶Dt

.

u

u

2500

MNIST

MNIST-M

SVHN

USPS

Amazon

DSLR

Webcam

VisDA-C
sythetic

VisDA-C
real

Figure 4: Samples from the datasets used.

4. Experiments and Results

To demonstrate the efÔ¨Åciency of d-SNE, three sets of ex-
periments were conducted using three kinds of datasets: (i)
digits datasets [18]: four datasets are included as different
domains in the digits datasets. MNIST contains 28 √ó 28
grayscale images with 70,000 images overall. MNIST-M
is a synthetic dataset generated from MNIST by superim-
posing random backgrounds. USPS consists of 16 √ó 16
grayscale images, with 9,298 images overall. SVHN con-
tains RGB photographs of house numbers, with 99,280 im-
ages. (ii) ofÔ¨Åce datasets [22]: three sets are included in the
ofÔ¨Åce domains. These images are of the same objects but
are collected from different sources. SpeciÔ¨Åcally, OfÔ¨Åce-31
A has 2,817 images, which is collected from the Amazon
website; 498 images in ofÔ¨Åce-31 D are captured by DSLR
camera and 795 images in ofÔ¨Åce-31 W are captured by web
camera. (iii) VisDA-C dataset [7]: two synthetic and real
image domains are included in VisDA-C dataset. 152,397
synthetic images are rendered using 3D CAD models as the
source domain while the target domain consists of real im-
ages. Figure 4 shows samples from the datasets used.

4.1. Digit Datasets

The Ô¨Årst set of experiments adapts the domains of digit
datasets.
In the Ô¨Årst experiment, the domains considered
are MNIST and USPS. A total of 2, 000 samples in MNIST
are randomly selected for the source domain. A small num-
ber of samples per-class ranging from 1 to 7 were randomly
selected from the target domain for training. The inputs

|Dt

k|, ‚àÄk

CCSA [16]
FADA [15]

d-SNE

0

1

3

5

7

65.40
65.40

73.01

85.00
89.10

90.10
91.90

92.40
93.40

92.90
94.40

92.90

93.55

95.13

96.13

Table 1: MNIST ‚Üí USPS datasets. |Dt
k|, ‚àÄk is essentially
number of samples per-class from the target domain. As can
be seen, d-SNE is clearly able to outperform the states-of-
the-art in all scenarios. As the cardinality of the samples
per-class increases, the performance across the algorithms
converge.

D = Œ¶t

from the source and target domains have the same dimen-
sionalities, Œ¶s
D. The states-of-the-art that we use as
benchmarks for this experiments are CCSA [16] and FADA
[15]. We use the same network architecture as them. Ta-
ble 1 shows the overall classiÔ¨Åcation accuracies for adapta-
tion from MNIST to USPS datasets. As can be seen that the
proposed method outperforms both CCSA and FADA in all
the cases even in the one-shot learning case. For the non-
adaptation baseline (|Dt
k| = 0), it can be noticed that our
implementation achieved a higher accuracy than CCSA and
FADA. We attribute this to a better hyperparameter tuning.
For the other four cases, we were unable to out-tune their
parameters both with our and their own implementations.
Therefore, we consider their reported numbers as the best
for CCSA [16] and FADA [15].

In the second experiment, we used four datasets in-
cluding MNIST, USPS, MNIST-M, and SVHN to create
Ô¨Åve domain adaptation experiments: MNIST ‚Üí MNIST-M,
MNIST ‚Üî USPS, and MNIST ‚Üî SVHN. Several states-of-
the-art algorithms including, the ones from before use this
setup, therefore enabling us to do a lot of comparisons. It is
to be noted that some of the benchmarks are unsupervised,
wherein the algorithm uses all of the images in an unlabeled
fashion, while the supervised algorithms only use 10 images
per-class in the target domain. We use the same network ar-
chitecture as Wen et al., [32]. The overall classiÔ¨Åcation ac-
curacies are shown in Table 2. Compared to the other super-
vised benchmarks, d-SNE outperforms the states-of-the-art
in all experiments. We observed that all supervised meth-
ods in general achieved lower accuracies than unsupervised
methods in domain pairs MNIST ‚Üí MNIST-M and SVHN
‚Üí MNIST. In experiments of MNIST‚Üî USPS, d-SNE can
achieve higher accuracies than even unsupervised methods.
MNIST and USPS datasets has relatively lower intra-class
variance compared to MNIST-M and SVHN, which we at-
tribute to these results. Even though the comparison to un-
supervised setting is unfair, we can clearly note that the
semi-supervised setting of d-SNE pushed our supervised
performance closer. The methods that outperform us are

2501

MNIST ‚Üí	MNIST-M

MNIST ‚Üí SVHN 

SVHN ‚Üí MNIST

MNIST ‚Üí USPS

USPS ‚Üí MNIST

Figure 5: t-SNE visualizations without (top) and with (bottom) domain adaptations.

Method

|Dt

k|, ‚àÄk

Setting MNIST ‚Üí MNIST-M MNIST ‚Üí USPS USPS ‚Üí MNIST MNIST ‚Üí SVHN SVHN ‚Üí MNIST

PixelDA [1]
ADA [10]
I2I [17]
DIRT-T [25]
SE [6]
SBADA-GAN [21]
G2A [23]

FADA [15]
CCSA [16]

d-SNE

d-SNE

7
10

0
7
10

10

U

S

S

98.20
87.47

-

98.90

-

99.40

-

-

95.90

-

95.1

-

-
-

92.2

-

-
-
-

54.50

-

97.60
92.1
99.40

98.26 ¬± 0.11

98.07 ¬± 2.82

13.96 ¬± 4.41

99.18 ¬± 0.12

95.04

97.60

95.30 ¬± 0.70

90.80 ¬± 1.30

94.40

91.50

61.08

-

47.00

76.14

92.40 ¬± 0.90

87.20

78.29 ¬± 2.00

97.27 ¬± 0.19

95.71 ¬± 0.42

37.63 ¬± 3.62

94.57 ¬± 0.40

50.98 ¬± 1.64
84.62 ¬± 0.04
87 .80 ¬± 0 .16

93.16 ¬± 0.71
97.53 ¬± 0.10
99.00 ¬± 0.08

83.37 ¬± 0.93
97.52 ¬± 0.08
98.49 ¬± 0.35

SS

94.12

-

-

26.22 ¬± 2.02
53.19 ¬± 0.28
61.73 ¬± 0.47

77.63 ¬± 0.26

66.02 ¬± 0.72
95.68 ¬± 0.03
96 .45 ¬± 0 .20

97.60

Table 2: ClassiÔ¨Åcation accuracy for domain adaptation methods on digits datasets. The unsupervised setting (U ) uses all
the images in the target domain. The supervised setting (S) uses 10 labeled samples per-class from the target domain. We
reimplemented CCSA and FADA using the same network and settings as our method. The best results are marked in bold.
If the best result is not in the supervised-only setting, we mark the best among the supervised-only methods in italics. The
results are averaged over three runs and we report mean and standard deviations over the three runs.

MNIST ‚Üí MNIST-M MNIST ‚Üí SVHN SVHN ‚Üí MNIST

before
after

99.45%
99.51%

99.51%
99.59%

88.96%
94.94%

Table 3: Results of domain generalization.

typically good when using simple datasets. In Table 5 we
can see that with more realistic and complicated datasets,
our semi-supervised formulation is also on par and often
better than the states-of-the-art. We suppose this difference
in performance to the intuition that digit datasets are eas-

ily separable even in unsupervised setting. Figure 5 shows
some t-SNE visualizations of adaptations.

Domain Generalization: As an added beneÔ¨Åt, d-SNE
shows good domain generalization. Here, we use the model
artifacts that we get after the model is adapted to the target
domain, and without re-training or Ô¨Åne-tuning, we measure
the accuracy on the source dataset. Table 3 shows classi-
Ô¨Åcation accuracies on the source domain before and after
domain adaptation. We can notice from Table 3 that the
network actually improves the original performance on the
source dataset. This is further evidence of d-SNE and the
strength of the latent-space it produces.

2502

Method

DANN [7]
DRCN [8]
kNN-Ad [24]
I2I [17]
G2A [23]

SDA [27]
FADA [15]
CCSA [16]
CCSA [16]

d-SNE (VGG-16)

d-SNE (ResNet-101)

U

S

S

S

3
3
0
3

0
3

0
3

|Dt

k|, ‚àÄk

Setting

A ‚Üí D

-

A ‚Üí W

73.00

D ‚Üí A

-

D ‚Üí W

96.40

W ‚Üí A

-

W ‚Üí D

99.20

67.10 ¬± 0.30

68.70 ¬± 0.30

56.00 ¬± 0.50

96.40 ¬± 0.30

54.09 ¬± 0.50

99.00 ¬± 0.2

84.10
71.10

81.10
75.30

58.30
50.10

96.40
96.50

63.80
52.10

99.20
99.60

87.70 ¬± 0.50

89.50 ¬± 0.50

72.80 ¬± 0.30

97.90 ¬± 0.30

71.40 ¬± 0.40

99.8 ¬± 0.4

86.10 ¬± 1.20
88.20 ¬± 1.00
61.20 ¬± 0.90
89.00 ¬± 1.20

62.40 ¬± 0.40
91.44 ¬± 0.23

82.70 ¬± 0.80
88.10 ¬± 1.20
62.3 ¬± 0.80
88.20 ¬± 1.00

61.49 ¬± 0.75
90.13 ¬± 0.07

66.20 ¬± 0.30
68.10 ¬± 0.60
58.5 ¬± 0.80
71.80 ¬± 0.50

48.92 ¬± 1.03
71.06 ¬± 0.18

95.70 ¬± 0.50
96.40 ¬± 0.80
80.1 ¬± 0.60
96.40 ¬± 0.80

82.24 ¬± 1.42
97.10 ¬± 0.07

65.00 ¬± 0.5
71.10 ¬± 0.90
51.6 ¬± 0.90
72.10 ¬± 1.00

47.52 ¬± 0.94
71.74 ¬± 0.42

97.60 ¬± 0.20
97.50 ¬± 0.90
95.6 ¬± 0.70
97.60 ¬± 0.40

90.42 ¬± 1.00
97.46 ¬± 0.24

80.41 ¬± 0.79
94.65 ¬± 0.38

75.26 ¬± 1.32
96.58 ¬± 0.14

67.39 ¬± 0.18
75.51 ¬± 0.44

96.39 ¬± 0.41
99.10 ¬± 0.24

65.55 ¬± 1.91
74.20 ¬± 0.24

98.31 ¬± 1.87

100.00 ¬± 0.00

80.55
90.01

Avg.

-

73.60
80.48
74.12
86.50

82.22
84.90
68.20
85.80

65.49
86.49

Table 4: Results of ofÔ¨Åce31 experiments. d-SNE with ResNet-101 base network achieves the best results with only 3 samples
in the target domain while d-SNE with VGG-16 base network outperforms the baselines in the majority of cases.

4.2. OfÔ¨Åce31 Dataset

Method

Setting

For our experiment involving the various domains of the
ofÔ¨Åce31 dataset, we followed the same protocol used by
Tzeng et al., and Motiian et al., [27, 15, 16]. For the source
dataset, we randomly selected 20 samples per-class from A,
8 samples per-class from D and W domains. For the target
dataset, 3 samples per-class were selected from all three do-
mains. The rest of samples from the target domain were
used for evaluation.

We used ResNet-101 as the base network and added two
extra dense layers to obtain feature representations [11].
The dimensionality of the latent-space was 512. As is cus-
tomary in literature, we used pre-trained weights from Im-
ageNet as initialization [15, 16, 23, 6]. Table 4 reports the
results of the experiments. Even with drastically less data
samples, d-SNE signiÔ¨Åcantly outperformed all the states-
of-the-art benchmarks. When the domain shift is extremely
large, which was the case with in A‚ÜíW, W‚ÜíA, A‚ÜíD and
D‚ÜíA, d-SNE shows larger margins compared to baselines.
d-SNE can handle domain shift better than other states-of-
the-art, especially under limited data conditions. CCSA and
FADA used VGG-16 as their base network. While ResNets
are the preferred base networks contemporaneously, for
fair comparisons we also report accuracies of d-SNE with
VGG-16 as the base network. As can be seen from Table
4 that the proposed d-SNE outperformed both CCSA and
FADA in the majority of cases. It is worth mentioning that
our non-adaptation baseline ((|Dt
k| = 0)) with VGG-16 ac-
tually achieved worse results than CCSA and FADA‚Äôs base-
lines. This highlights the strength of our domain adaptation
loss.

4.3. VisDA C dataset

VisDA-C is a new dataset, therefore we only have a few
reported benchmarks: G2A, SE and CCSA [23, 6, 16]. G2A
is unsupervised, SE is semi-supervised and CCSA is super-

G2A [23]
SE [6]
CCSA [16]

d-SNE

U
SS
S

S
SS

|Dt

k| = 0, ‚àÄk Adaptation
44.50
52.80
52.80

77.10
85.40
76.89

52.80
52.80

80.66
86.15

Table 5: Results on the VisDA-C dataset. Source domain
was synthetic and target domain was real images with 10
images per-class used for training. The metrics for both
G2A and SE were reported from the original source. The
results for CCSA were obtained from our own implementa-
tion.

vised like d-SNE. With the settings being unique for each
algorithm, the results may not be fair, but we include them
for comparisons. We replicated the experimental protocol
for the VisDA-C dataset as described in G2A [23].

Similar to the baselines, pre-trained models trained on
ImageNet were used as initialization. Table 5 shows the re-
sults on VisDA-C dataset. With only 10 samples per-class
from the target domain for training, d-SNE outperformed
CCSA and G2A (in unsupervised setting which used all the
unlabeled images in the target domain also). The super-
vised d-SNE cannot match the performance of the semi-
supervised SE, albeit this is not a fair comparison to make.
With the semi-supervised extension, d-SNE established it-
self as the clear state-of-the-art. Fig. 6 demonstrates that
the proposed domain adaptation algorithm can align fea-
tures from both domains while making them discriminative.

5. Conclusions

Domain adaptation has recently seen a massive boom,
largely due to the availability of large quantities of data

2503

Figure 6: t-SNE visualization of d-SNE‚Äôs latent-embedding space for the VisDA-C dataset. (a) Embeddings produced by the
model trained with source images only. (b) Embeddings produced by the model trained with target images only and (c) The
joint latent-embedding space of d-SNE. Different colors represent different classes. Embeddings from the source and target
domains are indicated by circles and stars, respectively.

but from varying domains. Deep-learning-based domain
adaptation is a fairly new phenomenon.
In this arti-
cle, we propose a novel use of the stochastic neigh-
borhood embedding technique-based supervised domain
is capable of training neural networks
adaptation,
end-to-end.
Experiments across standard benchmark-
ing datasets show that our method establishes a clear
state-of-the-art in most datasets. We propose a semi-
supervised extension that pushes the performance fur-
ther.

that

References

[1] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages 95‚Äì104,
Honolulu, Hawaii, jul 2017.

[2] Z. Cao, L. Ma, M. Long, and J. Wang. Partial adversarial do-
main adaptation. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 135‚Äì150, 2018.

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
Proc. Computer Vision and Pattern Recognition, 2009.

[4] Z. Ding, S. Li, M. Shao, and Y. Fu. Graph adaptive knowl-
edge transfer for unsupervised domain adaptation. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 37‚Äì52, 2018.

[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In International
conference on machine learning, pages 647‚Äì655, 2014.

[6] G. French, M. Mackiewicz, and M. Fisher. Self-Ensembling
for Visual Domain Adaptation. In Proc. International Con-
ference on Learning Representations, pages 1‚Äì15, Vancou-
ver, Canada, apr 2018.

[7] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
Adversarial Training of Neural Networks. Journal of Ma-
chine Learning Research, 17:1‚Äì35, 2016.

[8] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiÔ¨Åcation networks for un-
supervised domain adaptation.
In Proc. European Confer-
ence on Computer Vision, pages 597‚Äì613, Amsterdam, The
Netherlands, oct 2016.

[9] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672‚Äì2680, 2014.

[10] P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. As-
sociative Domain Adaptation.
In Proceedings of the IEEE
International Conference on Computer Vision, pages 2784‚Äì
2792, Venice, Italy, oct 2017.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
In European conference on com-

deep residual networks.
puter vision, pages 630‚Äì645. Springer, 2016.

[12] G. E. Hinton and S. T. Roweis. Stochastic neighbor embed-
ding. In Advances in neural information processing systems,
pages 857‚Äì864, 2003.

[13] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
A. A. Efros, and T. Darrell. CyCADA: Cycle-Consistent
Adversarial Domain Adaptation.
In ArXiv preprint arXiv:
1711.03213, pages 1‚Äì12, 2017.

[14] M.-Y. Liu and O. Tuzel. Coupled Generative Adversarial
Networks. In Proc. Advances in Neural Information Process-
ing Systems, pages 469‚Äì477, Barcelona, Spain, dec 2016.

[15] S. Motiian, Q. Jones, S. M. Iranmanesh, and G. Doretto.
Few-Shot Adversarial Domain Adaptation.
In Proc. Ad-
vances in Neural Information Processing Systems, pages 1‚Äì
11, Long Beach, CA, dec 2017.

[16] S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Uni-
Ô¨Åed Deep Supervised Domain Adaptation and Generaliza-

2504

tion. In Proc. IEEE International Conference on Computer
Vision, pages 5715‚Äì5725, Venice, Italy, oct 2017.

on Computer Vision and Pattern Recognition, pages 5018‚Äì
5027, 2017.

[31] M. Wang and W. Deng. Deep Visual Domain Adaptation:
A Survey. ArXiv preprint arXiv: 1802.03601, pages 1‚Äì17,
2018.

[32] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina-
tive feature learning approach for deep face recognition. In
European Conference on Computer Vision, pages 499‚Äì515.
Springer, 2016.

[33] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320‚Äì3328,
2014.

[17] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and
K. Kim. Image to image translation for domain adaptation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4500‚Äì4509, 2018.

[18] Y. Netzer and T. Wang. Reading digits in natural images with
unsupervised feature learning. In Proc. Advances in Neural
Information Processing Systems, pages 1‚Äì9, Granada, Spain,
dec 2011.

[19] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual
Domain Adaptation: A Survey of Recent Advances. IEEE
Signal Processing, 32(3):53‚Äì69, 2015.

[20] A. Rozantsev, M. Salzmann, and P. Fua. Beyond Sharing
Weights for Deep Domain Adaptation. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018.

[21] P. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo. From
source to target and back: symmetric bi-directional adaptive
GAN. In Proc. IEEE Computer Vision and Pattern Recogni-
tion, Salt Lake City, Utah, jun 2018.

[22] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting
visual category models to new domains.
In Proc. Euro-
pean conference on computer vision, pages 213 ‚Äì 226, Grete,
Greece, 2010.

[23] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chel-
lappa. Generate To Adapt: Aligning Domains using Gener-
ative Adversarial Networks. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, Salt Lake City,
Utah, jun 2018.

[24] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learn-
ing Transferrable Representations for Unsupervised Domain
Adaptation. In Proc. Advances in Neural Information Pro-
cessing Systems, pages 2110‚Äì2118, Barcelona, Spain, dec
2016.

[25] R. Shu, H. H. Bui, H. Narui, and S. Ermon. A DIRT-T Ap-
proach to unsupervised Domain Adaption. In Proc. Interna-
tional Conference on Learning Representations, pages 1‚Äì19,
Vancouver, Canada, apr 2018.

[26] A. Tarvainen and H. Valpola. Mean teachers are better role
models: Weight-averaged consistency targets improve semi-
supervised deep learning results. In Proc. Advances in Neu-
ral Information Processing Systems, Long Beach, CA, dec
2017.

[27] E. Tzeng, J. Hoffman, T. Darrell, K. Saenko, and U. Low-
ell. Simultaneous Deep Transfer Across Domains and Tasks.
In Proc. International Conference on Computer Vision, Las
Condes, Chile, dec 2015.

[28] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages 2962‚Äì
2971, Honolulu, Hawaii, jul 2017.

[29] R. Venkatesan, V. Gatupalli, and B. Li. On the general-
ity of neural image features.
In Image Processing (ICIP),
2016 IEEE International Conference on, pages 41‚Äì45. IEEE,
2016.

[30] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Pan-
chanathan. Deep hashing network for unsupervised do-
main adaptation.
In Proceedings of the IEEE Conference

2505

