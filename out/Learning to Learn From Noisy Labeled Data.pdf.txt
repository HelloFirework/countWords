Learning to Learn from Noisy Labeled Data

Junnan Li1, Yongkang Wong1, Qi Zhao2, Mohan S. Kankanhalli1

1National University of Singapore

2University of Minnesota

lijunnan@u.nus.edu, yongkang.wong@nus.edu.sg, qzhao@cs.umn.edu, mohan@comp.nus.edu.sg

Abstract

Despite the success of deep neural networks (DNNs) in
image classiﬁcation tasks, the human-level performance re-
lies on massive training data with high-quality manual an-
notations, which are expensive and time-consuming to col-
lect. There exist many inexpensive data sources on the web,
but they tend to contain inaccurate labels. Training on noisy
labeled datasets causes performance degradation because
DNNs can easily overﬁt to the label noise. To overcome this
problem, we propose a noise-tolerant training algorithm,
where a meta-learning update is performed prior to conven-
tional gradient update. The proposed meta-learning method
simulates actual training by generating synthetic noisy la-
bels, and train the model such that after one gradient update
using each set of synthetic noisy labels, the model does not
overﬁt to the speciﬁc noise. We conduct extensive experi-
ments on the noisy CIFAR-10 dataset and the Clothing1M
dataset. The results demonstrate the advantageous perfor-
mance of the proposed method compared to state-of-the-art
baselines.

1. Introduction

One of the key reasons why deep neural networks
(DNNs) have been so successful in image classiﬁcation is
the collections of massive labeled datasets such as COCO
[14] and ImageNet [20]. However, it is time-consuming
and expensive to collect such high-quality manual annota-
tions. A single image often requires agreement from mul-
tiple annotators to reduce label error. On the other hand,
there exist other less expensive sources to collect labeled
data, such as search engines, social media websites, or re-
ducing the number of annotators per image. However, those
low-cost approaches introduce low-quality annotations with
label noise. Many studies have shown that label noise
can signiﬁcantly affect the accuracy of the learned classi-
ﬁers [2, 23, 32].
In this work, we address the following
problem: how to effectively train on noisy labeled datasets?
Some methods learn with label noise by relying on hu-
man supervision to verify seed images [11, 29] or estimate

label confusion [16, 31]. However, those methods exhibit a
disadvantage in scalability for large datasets. On the other
hand, methods without human supervision (e.g. label cor-
rection [18, 24] and noise correction layers [5, 23]) are
scalable but less effective and more heuristic. In this work
we propose a meta-learning based noise-tolerant (MLNT)
training to learn from noisy labeled data without human su-
pervision or access to any clean labels. Rather than design-
ing a speciﬁc model, we propose a model-agnostic training
algorithm, which is applicable to any model that is trained
with gradient-based learning rule.

The prominent issue in training DNNs on noisy labeled
data is that DNNs often overﬁt to the noise, which leads to
performance degradation. Our method addresses this issue
by optimizing for a model’s parameters that are less prone
to overﬁtting and more robust against label noise. Speciﬁ-
cally, for each mini-batch, we propose a meta-objective to
train the model, such that after the model goes through con-
ventional gradient update, it does not overﬁt to the label
noise. The proposed meta-objective encourages the model
to produce consistent predictions after it is trained on a vari-
ety of synthetic noisy labels. The key idea of our method is:
a noise-tolerant model should be able to consistently learn
the underlying knowledge from data despite different label
noise. The main contribution of this work are as follows.

• We propose a noise-tolerant training algorithm, where
a meta-objective is optimized before conventional
training. Our method can be theoretically applied to
any model trained with gradient-based rule. We aim
to optimize for a model that does not overﬁt to a wide
spectrum of artiﬁcially generated label noise.

• We formulate our meta-objective as: train the model
such that after it learns from various synthetic noisy
labels using gradient update, the updated models give
consistent predictions with a teacher model. We adapt
a self-ensembling method to construct the teacher
model, which gives more reliable predictions unaf-
fected by the synthetic noise.

• We perform experiments on two datasets with syn-
thetic and real-world label noise, and demonstrate the

15051

advantageous performance of the proposed method in
image classiﬁcation tasks compared to state-of-the-art
methods. In addition, we conduct extensive ablation
study to examine different components of the proposed
method. Our code is publicly available1.

2. Related Work

Learning with label noise. A number of approaches have
been proposed to train DNNs with noisy labeled data. One
line of approaches formulate explicit or implicit noise mod-
els to characterize the distribution of noisy and true labels,
using neural networks [5, 8, 11, 19, 16, 23, 29], directed
graphical models [31], knowledge graphs [13], or condi-
tional random ﬁelds [27]. The noise models are then used
to infer the true labels or assign smaller weights to noisy
samples. However, these methods often require a small set
of data with clean labels to be available, or use expensive
estimation methods. They also rely on speciﬁc assumptions
about the noise model, which may limit their effectiveness
with complicated label noise. Another line of approaches
use correction methods to reduce the inﬂuence of noisy la-
bels. Bootstrap method [18] introduces a consistency ob-
jective that effectively re-labels the data during training.
Tanaka et al. [24] propose to jointly optimize network pa-
rameters and data labels. An iterative training method is
proposed to identify and downweight noisy samples [30]. A
few other methods have also been proposed that use noise-
tolerant loss functions to achieve robust learning under label
noise [3, 4, 28].
Meta-Learning. Recently, meta-learning methods for
DNNs have resurged in its popularity. Meta-learning gen-
erally seeks to perform the learning at a level higher than
where conventional learning occurs, e.g. learning the up-
date rule of a learner [17], or ﬁnding weight initializations
that can be easily ﬁne-tuned [1] or transferred [12]. Our ap-
proach is most related to MAML [1], which aims to train
model parameters that can learn well based on a few ex-
amples and a few gradient descent steps. Both MAML and
our method are model-agnostic and perform training by do-
ing gradient updates on simulated meta-tasks. However, our
objective and algorithm are different from that of MAML.
MAML addresses few-shot transfer to new tasks, whereas
we aim to learn a noise-tolerant model. Moreover, MAML
trains using classiﬁcation loss on a meta-test set, whereas
we use a consistency loss with a teacher model.
Self-Ensembling. Several recent methods based on self-
ensembling have improved the state-of-the-art results for
semi-supervised learning [10, 21, 25], where labeled sam-
ples are scarce and unlabeled samples are abundant. These
methods apply a consistency loss to the unlabeled samples,
which regularizes a neural network to make consistent pre-

1https://github.com/LiJunnan1992/MLNT

Figure 1: Left: conventional gradient update with cross entropy
loss may overﬁt to label noise. Right: a meta-learning update is
performed beforehand using synthetic label noise, which encour-
ages the network parameters to be noise-tolerant and reduces over-
ﬁtting during the conventional update.

dictions for the same samples under different data augmen-
tation [21], dropout and noise conditions [10]. We focus
in particular on the self-ensembling approach proposed by
Tarvainen & Valpola [25] as it forms one of the basis of our
approach. Their approach proposes two networks: a stu-
dent network and a teacher network, where the weights of
the teacher are the exponential moving average of those of
the student. They enforce the student network to make con-
sistent predictions with the teacher network. In our method,
we use the teacher network in meta-test to train the student
network such that it is more tolerant to label noise.

3. Method

3.1. Problem Statement

We consider a classiﬁcation problem with a training set
D = {(x1, y1), ..., (xn, yn)}, where xi denotes the ith
sample and yi ∈ {0, 1}c is a one-hot vector representing
the corresponding noisy label over c classes. Let f (xi, θ)
denotes the discriminative function of a neural network pa-
rameterized by θ, which maps an input to an output of the
c-class softmax layer. The conventional objective for super-
vised classiﬁcation is to minimize an empirical risk, such as
the cross entropy loss:

Lc = −

1
n

n

X

i=1

yi · log(f (xi, θ)),

(1)

where · denotes dot product.

However, since yi contains noise, the neural network can
overﬁt and perform poorly on the test set. We propose a
meta-objective that encourages the network to learn noise-
tolerant parameters. The details are delineated next.

3.2. Meta Learning based Noise Tolerant Training

Our method can learn the parameters of a DNN model
in such a way as to “prepare” the model for label noise.
The intuition behind our method is that when training with a

5052

Original Labels

Automobile

Automobile

Truck

Dog

Cat

Horse

Automobile

Truck

Truck

Synthetic Labels

Dog

Cat

Horse

Automobile

Truck

Automobile

Synthetic Labels

Cat

Dog

Horse

𝒀𝒀
�𝒀𝒀1
�𝒀𝒀𝑀𝑀

.
 
.
 
.
 
 
.
 
.
 
.

Deer

Horse

Deer

Classification Loss

ℒ𝑐𝑐(𝑿𝑿,𝒀𝒀,𝜽𝜽)

𝜽𝜽−𝛼𝛼∇ℒ𝑐𝑐(𝑿𝑿,�𝒀𝒀1,𝜽𝜽)
𝜽𝜽−𝛼𝛼∇ℒ𝑐𝑐(𝑿𝑿,�𝒀𝒀𝑀𝑀,𝜽𝜽)

Meta-Train

Predictions

Meta-Test

.
 
.
 
.
 
 
.
 
.
 
.

.
 
.
 
.
 
 
.
 
.
 
.

Consistency Loss

𝒥𝒥(𝑿𝑿,𝜽𝜽1′ ,�𝜽𝜽)

.
 
.
 
.
 
 
.
 
.
 
.

.

𝒥𝒥(𝑿𝑿,𝜽𝜽𝑴𝑴′ ,�𝜽𝜽)

Meta Loss

ℒmeta(𝜽𝜽)

Student
Model

Teacher
Model

exponential 
moving
average

Figure 2: Illustration of the proposed meta-learning based noise-tolerant (MLNT) training. For each mini-batch of training data, a meta
loss is minimized before training on the conventional classiﬁcation loss. We ﬁrst generate multiple mini-batches of synthetic noisy labels
with random neighbor label transfer (marked by orange arrow). The random neighbor label transfer can preserve the underlying noise
transition (e.g. DEER → HORSE, CAT ↔ DOG), therefore generating synthetic label noise in a similar distribution as the original data. For
each synthetic mini-batch, we update the parameters with gradient descent, and enforce the updated model to give consistent predictions
with a teacher model. The meta-objective is to minimize the consistency loss across all updated models w.r.t θ.

gradient-based rule, some network parameters are more tol-
erant to label noise than others. How can we encourage the
emergence of such noise-tolerant parameters? We achieve
this by introducing a meta-learning update before the con-
ventional update for each mini-batch. The meta-learning
update simulates the process of training with label noise
and makes the network less prone to over-ﬁtting. Specif-
ically, for each mini-batch of training data, we generate a
variety of synthetic noisy labels on the same images. With
each set of synthetic noisy labels, we update the network
parameters using one gradient update, and enforce the up-
dated network to give consistent predictions with a teacher
model unaffected by the synthetic noise. As shown in Fig-
ure 1, the meta-learning update optimizes the model so that
it can learn better with conventional gradient update on the
original mini-batch.
In effect, we aim to ﬁnd model pa-
rameters that are less sensitive to label noise and can con-
sistently learn the underlying knowledge from data despite
label noise. The proposed meta-learning update consists of
two procedures: meta-train and meta-test.
Meta-Train. Formally, at each training step, we con-
sider a mini-batch of data (X, Y ) sampled from the train-
ing set, where X = {x1, ..., xk} are k samples, and
Y = {y1, ..., yk} are the corresponding noisy labels.
We want to generate multiple mini-batches of noisy labels
{ ˆY 1, ..., ˆY M } with similar label noise distribution as Y .
We will describe the procedure for generating one set of

1 , ..., ˆym

noisy labels ˆY m = {ˆym
k }. First, we randomly se-
lect ρ samples out of the mini-batch of k samples. For
each selected sample xi, we rank its neighbors within the
mini-batch. Then we randomly select a neighbor xj from
its top 10 nearest neighbors (10 is experimentally deter-
mined), and use the neighbor’s label yj to replace the la-
bel for xi, ˆym
i = yj . Because we transfer labels among
neighbors, the synthetic noisy labels are from a similar dis-
tribution as the original noisy labels. We repeat the above
procedure M times to generate M mini-batches of synthetic
noisy labels. Note that we compute nearest neighbors based
on the Euclidean distance between feature representations
(pre-softmax layer activations) generated by a DNN pre-
trained on the entire noisy training set D.

Let θ denote the current model’s parameters, for each
m using

synthetic mini-batch (X, ˆY m), we update θ to θ′
one gradient descent step on the mini-batch.

m = θ − α∇θLc(X, ˆY m, θ),
θ′

(2)

where Lc(X, ˆY m, θ) is the cross entropy loss described in
equation 1, and α is the step size.

Meta-Test. Our meta-objective is to train θ such that
each updated parameters θ′
m do not overﬁt to the speciﬁc
noisy labels ˆY m. We achieve this by enforcing each up-
dated model to give consistent predictions with a teacher
model. We consider the model parameterized by θ as the

5053

student model, and construct the teacher model parameter-
ized by ˜θ following the self-ensembling [25] method. The
teacher model usually gives better predictions than the stu-
dent model. Its parameters are computed as the exponential
moving average (EMA) of the student’s parameters. Specif-
ically, at each training step, we update ˜θ with:

˜θ = γ ˜θ + (1 − γ)θ,

(3)

where γ is a smoothing coefﬁcient hyper-parameter.

Since the teacher model is unaffected by the synthetic
label noise, we enforce a consistency loss J (θ′
m) that en-
courages each updated model (with parameters θ′
m) to give
consistent predictions with the teacher model on the same
input X. We deﬁne J (θ′
m) as the Kullback-Leibler (KL)-
divergence between the softmax predictions from the up-
dated model f (X, θ′
m) and the softmax predictions from
the teacher model f (X, ˜θ). We ﬁnd that KL-divergence
produces better results compared to the mean squared error
used in [25].

J (θ′

m) =

=

1
k

1
k

k

X

i=1

k

X

i=1

DKL(f (xi, ˜θ)||f (xi, θ′

m))

(4)

E(log(f (xi, ˜θ)) − log(f (xi, θ′

m))).

(5)

We want to minimize the consistency loss for all of the
M updated models with parameters {θ′
M }. There-
fore, the meta loss is deﬁned as the average of all consis-
tency losses:

1, ..., θ′

Lmeta(θ) =

=

1
M

1
M

M

X

m=1

M

X

m=1

J (θ′

m)

(6)

J (θ − α∇θLc(X, ˆY m, θ)).

(7)

Although Lmeta(θ) is computed using the updated
model parameters θ′
m, the optimization is performed over
the student model parameters θ. We perform stochastic gra-
dient descent (SGD) to minimize the meta loss. The model
parameters θ are updated as follows:

θ ← θ − η∇Lmeta(θ),

(8)

where η is the meta-learning rate.

After the meta-learning update, we perform SGD to
optimize the classiﬁcation loss on the original mini-batch
(X, Y ).

θ ← θ − β∇Lc(X, Y , θ),

(9)

where β is the learning rate. The full algorithm is outlined
in Algorithm 1.

Algorithm 1 Meta-Learning based Noise-Tolerant Training

1: Randomly initialize θ
2: Initialize teacher model ˜θ = θ
3: while not done do
4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

Sample a mini-batch (X, Y ) of size k from D.
for m = 1 : M do

Generate synthetic noisy labels ˆY m by random
neighbor label transfer
Compute updated parameters with gradient de-
m = θ − α∇θLc(X, ˆY m, θ)
scent: θ′
Evaluate consistency loss with teacher:
J (θ′
end for
Evaluate Lmeta(θ) = 1
Meta-learning update θ ← θ − η∇Lmeta(θ)
Evaluate classiﬁcation loss Lc(X, Y , θ)
Update θ ← θ − β∇Lc(X, Y , θ)
Update teacher model: ˜θ = γ ˜θ + (1 − γ)θ

i=1 DKL(f (xi, ˜θ)||f (xi, θ′

m=1 J (θ′

M PM

m) = 1

k Pk

m))

m)

14:
15: end while

Note that the meta-gradient ∇Lmeta(θ) involves a gra-
dient through a gradient, which requires calculating the
second-order derivatives with respect to θ. In our experi-
ments we use a ﬁrst-order approximation by omitting the
second-order derivatives, which can signiﬁcantly increase
the computation speed. The comparison in Section 4.4
shows that this approximation performs almost as well as
using second-order derivatives. This provides another in-
tuition to explain our method: The ﬁrst-order approxima-
tion considers the term α∇θLc(X, ˆY m, θ) in equation 7
as a constant. Therefore, we can consider the update
θ − α∇θLc(X, ˆY m, θ) as injecting data-dependent noise
to the parameters, and adding noise to the network during
training has been shown by many studies to have a regular-
ization effect [10, 22].

3.3. Iterative Training

We propose an iterative training scheme for two pur-
poses: (1) Remove samples with potentially wrong class la-
bels from the classiﬁcation loss Lc(X, Y , θ). (2) Improve
the predictions from the teacher model f (xi, ˜θ) so that the
consistency loss is more effective.

First, we perform an initial training iteration following
the method described in Algorithm 1, and acquire a model
with the best validation accuracy (usually the teacher). We
name that model as mentor and use θ∗ to denote its param-
eters. In the second training iteration, we repeat the steps in
Algorithm 1 with two changes described as follows.

First, if the classiﬁcation loss Lc(X, Y , θ) is applied to
the entire training set D, samples with wrong ground-truth
labels can corrupt training. Therefore, we remove a sam-

5054

ple from the classiﬁcation loss if the mentor model assigns
a low probability to the ground-truth class. In effect, the
classiﬁcation loss would now sample batches from a ﬁltered
training set D′ which contains fewer corrupted samples.

D′ = {(xi, yi) ∈ D | yi · f (xi, θ∗) > τ },

(10)

where f (xi, θ∗) is the softmax prediction of the mentor
model, and τ is a threshold to control the balance between
the quality and quantity of D′.

Second, we improve the effectiveness of the consistency
loss by merging the predictions from the mentor model and
the teacher model to produce more reliable predictions. The
new consistency loss is:

J ′(θ′

m) =

1
k

k

X

i=1

DKL(s||f (xi, θ′

m)),

s = λf (xi, ˜θ) + (1 − λ)f (xi, θ∗)),

(11)

(12)

where λ is a weight to control the importance of the teacher
model and the mentor model. It is ramped up from 0 to 0.5
as training proceeds.

We train for three iterations in our experiments. The
mentor model is the best model from the previous iteration.
We observe that further training iterations beyond three do
not give noticeable performance improvement.

4. Experiments

4.1. Datasets

We conduct experiments on two datasets, namely
CIFAR-10 [9] and Clothing1M [31]. We follow the same
experimental setting as previous studies [16, 24, 27] for fair
comparison.

For CIFAR-10, we split 10% of the training data for val-
idation, and artiﬁcially corrupt the rest of the training data
with two types of label noise: symmetric and asymmetric.
The symmetric label noise is injected by using a random
one-hot vector to replace the ground-truth label of a sam-
ple with a probability of r. The asymmetric label noise is
designed to mimic some of the structure of real mistakes
for similar classes [16]: TRUCK → AUTOMOBILE, BIRD
→ AIRPLANE, DEER → HORSE, CAT ↔ DOG. Label transi-
tions are parameterized by r ∈ [0, 1] such that true class and
wrong class have probability of 1 − r and r, respectively.

Clothing1M [31] consists of 1M images collected from
online shopping websites, which are classiﬁed into 14
classes, e.g. t-shirt, sweater, jacket. The labels are generated
using surrounding texts provided by sellers, which contain
real-world errors. We use the 14k and 10k clean data for
validation and test, respectively, but we do not use the 50k
clean training data.

4.2. Implementation

For experiments on CIFAR-10, we follow the same ex-
perimental setting as [24] and use the network based on Pre-
Act ResNet-32 [7]. By common practice [24], we normalize
the images, and perform data augmentation by random hor-
izontal ﬂip and 32 × 32 random cropping after padding 4
pixels per side. We use a batch size k = 128, a step size
α = 0.2, a learning rate β = 0.2, and update θ using SGD
with a momentum of 0.9 and a weight decay of 10−4. For
each training iteration, we divide the learning rate by 10 af-
ter 80 epochs, and train until 120 epochs. For the initial
iteration, we ramp up η (meta-learning rate) from 0 to 0.4
during the ﬁrst 20 epochs, and keep η = 0.4 for the rest of
the training. In terms of the EMA decay γ, we use γ = 0.99
for the ﬁrst 20 epochs and γ = 0.999 later on, because the
student improves quickly early in the training, and thus the
teacher should have a shorter memory [25].
In the abla-
tion study (Section 4.4), we will show the effect of the three
important hyper-parameters, namely M , the number of syn-
thetic mini-batches, ρ, the number of samples with label re-
placement, and the threshold τ for data ﬁltering. The value
for all hyper-parameters are determined via validation.

For experiments on Clothing1M, we follow previous
works [16, 24] and use the ResNet-50 [6] pre-trained on
ImageNet.
For preprocessing, we resize the image to
256 × 256, crop the middle 224 × 224 as input, and per-
form normalization. We use a batch size k = 32, a step
size α = 0.02, a learning rate β = 0.0008, and update θ
using SGD with a momentum of 0.9 and a weight decay of
10−3. We train for 3 epochs for each iteration. During the
ﬁrst 2000 mini-batches in the initial iteration, we ramp up
η from 0 to 0.0008, and set γ = 0.99. For the rest of the
training, we use η = 0.0008 and γ = 0.999. Other hyper-
parameters are set as M = 10, ρ = 0.5k, and τ = 0.3.

4.3. Experiments on CIFAR 10

We compare the proposed MLNT with multiple base-
line methods using CIFAR-10 dataset with symmetric label
noise (noise ratio r = 0, 0.1, 0.3, 0.5, 0.7, 0.9) and asym-
metric label noise (noise ratio r = 0.1, 0.2, 0.3, 0.4, 0.5).
The baselines include:
(1) Cross Entropy: conventional training without the meta-
learning update. We report both the results from [24] and
from our own implementation.
(2) Forward [16]: forward loss correction based on the
noise transition matrix.
(3) CNN-CRF [27]: a CRF model is proposed to represent
the relationship between noisy and clean labels. It requires
a small set of clean labels during training.
(4) Joint Optimization [24]: alternatively updates network
parameters and corrects labels during training.

Both Forward [16] and CNN-CRF [27] require the
ground-truth noise transition matrix, and Joint Optimiza-

5055

Table 1: Classiﬁcation accuracy (%) on CIFAR-10 test set for different methods trained with symmetric label noise. We report the mean
and standard error across 5 runs.

Method

Cross Entropy [24]
Cross Entropy (reproduced)
Joint Optimization [24]

MLNT-student (1st iter.)
MLNT-teacher (1st iter.)

MLNT-student (2nd iter.)
MLNT-teacher (2nd iter.)

MLNT-student (3nd iter.)
MLNT-teacher (3nd iter.)

r = 0

93.5

r = 0.1

r = 0.3

r = 0.5

r = 0.7

r = 0.9

91.0

88.4

85.0

78.4

41.1

91.84±0.05

90.33±0.06

87.85±0.08

84.62±0.08

78.06±0.16

45.85±0.91

93.4

92.7

91.4

89.6

85.9

58.0

93.18±0.07
93.21±0.07

93.24±0.09
93.35±0.07

93.29±0.08
93.52±0.08

92.16±0.05
92.43±0.05

92.63±0.07
92.91±0.09

92.91±0.10
93.24±0.08

90.57±0.08
91.06±0.07

91.99±0.13
91.89±0.06

92.02±0.09
92.50±0.07

87.68±0.06
88.43±0.05

89.71±0.07
90.03±0.08

90.27±0.10
90.65±0.09

81.96±0.19
83.27±0.22

86.28±0.19
86.24±0.18

86.95±0.17
87.11±0.19

55.45±1.11
57.39±1.13

58.21±1.09
58.33±1.10

58.57±1.12
59.09±1.12

Table 2: Classiﬁcation accuracy (%) on CIFAR-10 test set for different methods trained with asymmetric label noise. We report the mean
and standard error across 5 runs.

Method

r = 0.1

r = 0.2

r = 0.3

r = 0.4

r = 0.5

Cross Entropy [24]
Cross Entropy (reproduced)
Forward [16]
CNN-CRF [27]
Joint Optimization [24]

MLNT-student (1st iter.)
MLNT-teacher (1st iter.)

MLNT-student (2nd iter.)
MLNT-teacher (2nd iter.)

MLNT-student (3nd iter.)
MLNT-teacher (3nd iter.)

91.8

90.8

90.0

87.1

77.3

91.04±0.07

90.19±0.09

88.88±0.06

86.34±0.22

77.48±0.79

92.4
92.0
93.2

92.89±0.11
93.05±0.10

93.01±0.12
93.33±0.13

93.36±0.14
93.61±0.10

91.4
91.5
92.7

91.84±0.10
92.19±0.09

92.65±0.11
92.97±0.11

92.98±0.13
93.25±0.12

91.0
90.7
92.4

90.55±0.09
91.47±0.04

91.87±0.12
92.43±0.19

92.59±0.10
92.82±0.18

90.3
89.5
91.5

88.70±0.13
88.69±0.08

90.60±0.12
90.93±0.15

91.87±0.12
92.30±0.10

83.8
84.0
84.6

79.95±0.71
78.44±0.45

81.53±0.66
81.47±0.54

82.25±0.68
82.09±0.47

tion [24] requires the ground-truth class distribution among
training data. Our method does not require any prior knowl-
edge on the data, thus is more general. Note that all base-
lines use the same network architecture as our method. We
report the numbers published in [24].

Table 1 and Table 2 show the results for symmetric
and asymmetric label noise, respectively. Our implemen-
tation of Cross Entropy has lower overall accuracy com-
pared to [24]. The reason could be the different program-
ming frameworks used (we use PyTorch [15], whereas [24]
used Chainer [26]). For both types of noise, the proposed
MLNT method with one training iteration signiﬁcantly im-
proves accuracy compared to Cross Entropy (reproduced),
and achieves comparable performance to state-of-the-art
methods.
Iterative training further improves the perfor-
mance. MLNT-teacher after three iterations signiﬁcantly
outperforms previous methods. An exception where MLNT
does not outperform baselines is with 50% asymmetric la-
bel noise. This is because that asymmetric label noise is
generated by exchanging CAT and DOG classes, and it is
theoretically impossible to distinguish them without prior
knowledge when the noise ratio is 50%.

In Table 1, we also show the results with clean train-
ing data (r = 0). The proposed MLNT can achieve an
improvement of +1.68 in accuracy compared to Cross En-
tropy, which shows the regularization effect of the proposed
meta-objective.

4.4. Ablation Study

Progressive Comparison. Figure 3 plots the model’s ac-
curacy on noisy training data and its test accuracy on clean
test data as training proceeds. We show a representative
training process using asymmetric label noise with r = 0.4.
Accuracy is calculated every epoch, and training accuracy
is computed across all mini-batches within the epoch. Com-
paring the proposed MLNT methods (1st iter.) with Cross
Entropy, MLNT learns more quickly during the beginning
of training, as shown by the higher test accuracy. Cross
Entropy has the most unstable training process, as shown
by its ﬂuctuating test accuracy curve. MLNT-student is
more stable because of the regularization from the meta-
objective, whereas MLNT-teacher is extremely stable be-
cause its parameters change smoothly during training. At
the 80th epoch, the learning rate is divided by 10, which

5056

)

%

(
 
y
c
a
r
u
c
c
A
 
g
n
n
a
r
T

i

i

90

85

80

75

70

65

60

55

50

MLNT-teacher
MLNT-student
Cross Entropy

10 20 30 40 50 60 70 80 90 100 110 120

Epoch

(a) Training accuracy on noisy training data

(b) Test accuracy on clean test data

Figure 3: Progressive performance comparison of the proposed MLNT and Cross Entropy as training proceeds.

Figure 4: Performance of MLNT-student (1st iter.) on CIFAR-10
trained with different number of synthetic mini-batches M .

Figure 5: Performance of MLNT-student (1st iter.) on CIFAR-10
trained with asymmetric label noise using different ρ.

causes a drastic increase in both training and test accu-
racy for MLNT-student and Cross Entropy. After the 80th
epoch, the model begins to overﬁt because of the small
learning rate. However, the proposed MLNT-student suf-
fers less overﬁtting compared to Cross Entropy, as shown
by its lower training accuracy and higher test accuracy.
Hyper-parameters. We conduct ablation study to exam-
ine the effect of three hyper-parameters: M, ρ, τ . M is
the number of mini-batches {(X, ˆY m)}M
m=1 with synthetic
noisy labels that we generate for each mini-batch (X, Y )
from the original training data. Intuitively, with larger M ,
the model is exposed to a wider variety of label noise, and
In Figure 4 we
thus can learn to be more noise-tolerant.
show the test accuracy on CIFAR-10 for MLNT-student
(1st iter.) with M = 0, 5, 10, 15 (M = 0 is the same as
Cross Entropy) trained using labels with symmetric noise
(SN) and asymmetric noise (AN) of different ratio. The
result shows that the accuracy indeed increases as M in-
creases. The increase is most signiﬁcant when M changes
from 0 to 5, and is marginal when M changes from 10 to
15. Therefore, the experiments in this paper are conducted
using M = 10, as a trade-off between the training speed
and the model’s performance.

ρ is the number of samples whose labels are changed in
each synthetic mini-batch ˆY m of size k. We experiment

Figure 6: Performance of MLNT-student (2nd iter.) on CIFAR-
10 trained with asymmetric label noise using different τ .

with ρ = 0.1k, 0.2k, 0.3k, 0.4k, 0.5k, which correspond to
13, 26, 39, 51, 64 samples with a batch size of 128. Fig-
ure 5 shows the performance of MLNT-student (1st iter.)
using different ρ trained on CIFAR-10 with different ratio
of asymmetric label noise. The performance is insensitive
to the value of ρ. For different noise ratio, the optimal ρ
generally falls into the range of [0.3k, 0.5k].

τ is the threshold to determine which samples are ﬁltered
out by the mentor model during the 2nd and 3nd training
iteration.
It controls the balance between the quality and
quantity of the data that is used by the classiﬁcation loss. In

5057

102030405060708090100110120Epoch405060708090Test Accuracy (%)MLNT-teacherMLNT-studentCross Entropyr=0.7(SN)r=0.3(SN)r=0.4(AN)r=0.2(AN)788082848688909294Test Accuracy (%)M=0M=5M=10M=15r=0.1r=0.2r=0.3r=0.4888990919293Test Accuracy (%)ρ=0.1kρ=0.2kρ=0.3kρ=0.4kρ=0.5kr=0.1r=0.2r=0.3r=0.4899091929394Test Accuracy (%)τ=0.3τ=0.4τ=0.5τ=0.6Vest (0.02)

Suit (0.22)

Underwear (0.18)

Dress (0.93)

Jacket (0.48)

Hoodie (0.77)

Knitwear (0.18)

Knitwear (0.03)

Jacket (0.08)

Sweater (0.65)

Hoodie (0.87)

Downcoat (0.87)

Figure 7: Example images from Clothing1M that are ﬁltered out
by the mentor model. We show the ground-truth label (red) and
the label predicted by the mentor (blue) with their corresponding
probability scores.

Figure 6 we show the performance of MLNT-student (2nd
iter.) trained using different value of τ . As the noise ratio
r increases, the optimal value of τ also increases to ﬁlter
out more samples. Figure 7 shows some example images
from Clothing1M dataset that are ﬁltered out and their cor-
responding probability scores given by the mentor model.
Full optimization. We have been using a ﬁrst-order
approximation to optimize Lmeta for faster computation
speed. Here we conduct experiments using full optimiza-
tion by including the second-order derivative with respect
to θ. Table 3 shows the comparison on four representa-
tive sets of experiments with different label noise. We show
the test accuracy (averaged across 5 runs) of MLNT-student
(1st iter.) trained with full optimization and ﬁrst-order ap-
proximation. The result shows that the performance from
ﬁrst-order approximation is nearly the same as that obtained
with full second derivatives. This suggests that the improve-
ment of MLNT mostly comes from the gradients of the meta
loss at the updated parameter values, rather than the second-
order gradients.

4.5. Experiments on Clothing1M

We demonstrate the efﬁcacy of the proposed method on
real-world noisy labels using the Clothing1M dataset. The
results are shown in Table 4. We show the accuracy for
baselines #1 and #3 reported in [24], and the accuracy for
#2 reported in [16]. The proposed MLNT method with
one training iteration achieves better performance compared
to state-of-the-art methods. After three training iterations,
MLNT achieves a signiﬁcant improvement in accuracy of
+4.19 over Cross Entropy, and an improvement of +1.31
over the best baseline method #3.

Table 3: Test accuracy (%) on CIFAR-10 for MNLT-student (1st
iter.) with full optimization of the meta-loss and its ﬁrst-order ap-
proximation.

Optimization

SN

AN

r = 0.3

r = 0.7

r = 0.2

r = 0.4

First-order approx.
Full

90.57
90.74

81.96
82.05

91.84
91.89

88.70
88.91

Table 4: Classiﬁcation accuracy (%) of different methods on the
Clothing1M test set.

Method

Accuracy

#1 Cross Entropy [24]
Cross Entropy (reproduced)
#2 Forward [16]
#3 Joint Optimization [24]

MLNT-student (1st iter.)
MLNT-teacher (1st iter.)

MLNT-student (2nd iter.)
MLNT-teacher (2nd iter.)

MLNT-student (3nd iter.)
MLNT-teacher (3nd iter.)

69.15
69.28
69.84
72.16

72.34
72.08

73.13
73.10

73.44
73.47

5. Conclusion

In this paper, we propose a meta-learning method to
learn from noisy labeled data, where a meta-learning up-
date is performed prior to conventional gradient update. The
proposed meta-objective aims to ﬁnd noise-tolerant model
parameters that are less prone to overﬁtting. In the meta-
train step, we generate multiple mini-batches with synthetic
noisy labels, and use them to update the parameters. In the
meta-test step, we apply a consistency loss between each
updated model and a teacher model, and train the original
parameters to minimize the total consistency loss. In ad-
dition, we propose an iterative training scheme, where the
model from previous iteration is used to clean data and re-
ﬁne predictions. We evaluate the proposed method on two
datasets. The results validate the advantageous performance
of our method compared to state-of-the-art methods. For
future work, we plan to explore using the proposed model-
agnostic method to other domains with different model ar-
chitectures, such as learning Recurrent Neural Networks for
machine translation with corrupted ground-truth sentences.

Acknowledgment

This research is supported by the National Research
Foundation, Prime Minister’s Ofﬁce, Singapore under its
Strategic Capability Research Centres Funding Initiative.

5058

References

[1] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
In ICML, pages 1126–1135, 2017. 2

[2] Benoˆıt Fr´enay and Michel Verleysen. Classiﬁcation in the
presence of label noise: A survey.
IEEE Transactions
on Neural Networks and Learning Systems, 25(5):845–869,
2014. 1

[3] Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust
loss functions under label noise for deep neural networks. In
AAAI, pages 1919–1925, 2017. 2

[4] Aritra Ghosh, Naresh Manwani, and P. S. Sastry. Making
risk minimization tolerant to label noise. Neurocomputing,
160:93–107, 2015. 2

[5] Jacob Goldberger and Ehud Ben-Reuven. Training deep
In ICLR,

neural-networks using a noise adaptation layer.
2017. 1, 2

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 5

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In ECCV, vol-
ume 9908 of Lecture Notes in Computer Science, pages 630–
645, 2016. 5

[8] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and
Li Fei-Fei. Mentornet: Learning data-driven curriculum for
very deep neural networks on corrupted labels.
In ICML,
pages 2309–2318, 2018. 2

[9] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Mater’s thesis, Univer-
sity of Toronto, 2009. 5

[10] Samuli Laine and Timo Aila. Temporal ensembling for semi-

supervised learning. In ICLR, 2017. 2, 4

[11] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun
Yang. Cleannet: Transfer learning for scalable image clas-
siﬁer training with label noise. In CVPR, pages 5447–5456,
2018. 1, 2

[12] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M.
Hospedales. Learning to generalize: Meta-learning for do-
main generalization. In AAAI, 2018. 2

[13] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao,
Jiebo Luo, and Li-Jia Li. Learning from noisy labels with
distillation. In ICCV, pages 1928–1936, 2017. 2

[14] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV, volume 8693 of Lecture Notes in Com-
puter Science, pages 740–755, 2014. 1

[15] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS Workshop, 2017. 6

[16] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon,
Richard Nock, and Lizhen Qu. Making deep neural networks
robust to label noise: A loss correction approach. In CVPR,
pages 2233–2241, 2017. 1, 2, 5, 6, 8

[17] Sachin Ravi and Hugo Larochelle. Optimization as a model

for few-shot learning. In ICLR, 2017. 2

[18] Scott E. Reed, Honglak Lee, Dragomir Anguelov, Christian
Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training
deep neural networks on noisy labels with bootstrapping. In
ICLR, 2015. 1, 2

[19] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta-
sun. Learning to reweight examples for robust deep learning.
In ICML, pages 4331–4340, 2018. 2

[20] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-
thy, Aditya Khosla, Michael S. Bernstein, Alexander C.
Berg, and Fei-Fei Li.
ImageNet large scale visual recog-
nition challenge. International Journal of Computer Vision,
115(3):211–252, 2015. 1

[21] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Regularization with stochastic transformations and pertur-
bations for deep semi-supervised learning. In NIPS, pages
1163–1171, 2016. 2

[22] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. Journal of Ma-
chine Learning Research, 15(1):1929–1958, 2014. 4

[23] Sainbayar Sukhbaatar,

Joan Bruna, Manohar Paluri,
Lubomir Bourdev, and Rob Fergus. Training convolutional
networks with noisy labels. In ICLR Workshop, 2015. 1, 2

[24] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy-
oharu Aizawa.
Joint optimization framework for learning
with noisy labels. In CVPR, pages 5552–5560, 2018. 1, 2, 5,
6, 8

[25] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In NIPS, pages 1195–
1204, 2017. 2, 4, 5

[26] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton.
Chainer: a next-generation open source framework for deep
learning. In NIPS Workshop, 2015. 6

[27] Arash Vahdat. Toward robustness against label noise in train-
In NIPS, pages

ing deep discriminative neural networks.
5601–5610, 2017. 2, 5, 6

[28] Brendan van Rooyen, Aditya Krishna Menon, and Robert C.
Williamson. Learning with symmetric label noise: The im-
portance of being unhinged.
In NIPS, pages 10–18, 2015.
2

[29] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhi-
nav Gupta, and Serge J. Belongie. Learning from noisy large-
scale datasets with minimal supervision.
In CVPR, pages
6575–6583, 2017. 1, 2

[30] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey,
Hongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learn-
ing with open-set noisy labels. In CVPR, pages 8688–8696,
2018. 2

[31] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang
Wang. Learning from massive noisy labeled data for image
classiﬁcation. In CVPR, pages 2691–2699, 2015. 1, 2, 5

[32] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. Understanding deep learning re-
quires rethinking generalization. In ICLR, 2017. 1

5059

