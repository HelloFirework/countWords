Max-Sliced Wasserstein Distance and its use for GANs

Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros†, Nasir Siddiqui†,

Sanmi Koyejo, Zhizhen Zhao, David Forsyth, Alexander Schwing

University of Illinois at Urbana-Champaign

†Dupage Medical Group

ishan.sd@gmail.com, {ythu2, ruoyus}@illinois.edu, ayis@ayis.org, nsiddiqui@gmail.com,

{sanmi, zhizhenz, daf, aschwing}@illinois.edu

Abstract

Generative adversarial nets (GANs) and variational
auto-encoders have signiﬁcantly improved our distribution
modeling capabilities, showing promise for dataset aug-
mentation, image-to-image translation and feature learn-
ing. However, to model high-dimensional distributions, se-
quential training and stacked architectures are common, in-
creasing the number of tunable hyper-parameters as well as
the training time. Nonetheless, the sample complexity of the
distance metrics remains one of the factors affecting GAN
training. We ﬁrst show that the recently proposed sliced
Wasserstein distance has compelling sample complexity
properties when compared to the Wasserstein distance. To
further improve the sliced Wasserstein distance we then an-
alyze its ‘projection complexity’ and develop the max-sliced
Wasserstein distance which enjoys compelling sample com-
plexity while reducing projection complexity, albeit neces-
sitating a max estimation. We ﬁnally illustrate that the pro-
posed distance trains GANs on high-dimensional images up
to a resolution of 256x256 easily.

1. Introduction

Generative modeling capabilities have improved tremen-
dously in the last few years, especially since the advent of
deep learning-based models like generative adversarial nets
(GANs) [11] and variational auto-encoders (VAEs) [17].
Instead of sampling from a high-dimensional distribution,
GANs and VAEs transform a sample obtained from a sim-
ple distribution using deep nets. These models have found
use in dataset augmentation [31], image-to-image transla-
tion [15, 37, 21, 14, 24, 29, 35, 38], and even feature learn-
ing for inference related tasks [9].

GANs and many of their variants formulate generative
modeling as a two player game. A ‘generator’ creates sam-
ples that resemble the ground truth data. A ‘discriminator’
tries to distinguish between ‘artiﬁcial’ and ‘real’ samples.

Both, the generator and discriminator, are parametrized us-
ing deep nets and trained via stochastic gradient descent.
In its original formulation [11], a GAN minimizes the
Jenson-Shannon divergence between the data distribution
and the probability distribution induced in the data space
by the generator. Many other variants have been proposed,
which use either some divergence or the integral probabil-
ity metric to measure the distance between the distribu-
tions [2, 22, 12, 20, 8, 7, 27, 4, 26, 23, 13, 30]. When
carefully trained, GANs are able to produce high quality
samples [28, 16, 25, 16, 25]. Training GANs is, however,
difﬁcult – especially on high dimensional datasets.

the sample complexity.

The scaling difﬁculty of GANs may be related to one
fundamental theoretical issue:
It
is shown in [3] that KL-divergence, Jenson-Shannon and
Wasserstein distance do not generalize, in the sense that the
population distance cannot be approximated by an empir-
ical distance when there are only a polynomial number of
samples. To improve generalization, one popular method
is to limit the discriminator class [3, 10] and interpret the
training process as minimizing a neural-net distance [3].

In this work, we promote a different path that resolves
the sample complexity issue. A fundamental reason for the
exponential sample complexity of the Wasserstein distance
is the sparsity of points in a high dimensional space. Even if
two collections of points are randomly drawn from the same
ball, these two collections are far away from each other.
Our intuition is that projection onto a low-dimensional sub-
space, such as a line, mitigates the artiﬁcial distance effect
in high dimensions and the distance of the projected sam-
ples reﬂects the true distance.

We ﬁrst apply this intuition to analyze the recently pro-
posed sliced Wasserstein distance GAN, which is based on
the average Wasserstein distance of the projected versions
of two distributions along a few randomly picked direc-
tions [8, 20, 34]. We prove that the sliced Wasserstein dis-
tance is generalizable for Gaussian distributions (i.e., it has
polynomial sample complexity), while Wasserstein distance
is not, thus partially explaining why [8, 20, 34] may exhibit

110648

better behavior than the Wasserstein distance [2].

One drawback of the sliced Wasserstein distance is that it
requires a large number of projection directions, since ran-
dom directions lose a lot of information. To address this
concern, we propose to project onto the “best direction,”
along which the projected distance is maximized. We call
the corresponding metric the “max-sliced Wasserstein dis-
tance,” and prove that it is also generalizable for Gaussian
distributions.

Using this new metric, we are able to train GANs to gen-
erate high resolution images from the CelebA-HQ [16] and
LSUN Bedrooms [36] datasets. We also achieve improved
performance in other distribution matching tasks like un-
paired word translation [6].

The main contributions of this paper are the following:

• We analyze in Sec. 3.1 the sample complexity of the
Wasserstein and sliced Wasserstein distances. We
show that for a certain class of distributions the
Wasserstein distance has an exponential sample com-
plexity, while the sliced Wasserstein distance [8, 34]
has a polynomial sample complexity.

• We then study in Sec. 3.2 the projection complexity of
the sliced Wasserstein distance, i.e., how the number
of random projection directions affects estimation.

• We introduce the max-sliced Wasserstein distance in

Sec. 3.3 to address the projection complexity issue.

• We then employ the max-sliced Wasserstein distance
to train GANs in Sec. 4, demonstrating signiﬁcant re-
duction in the number of projection directions required
for the sliced-Wasserstein GAN.

2. Background

Generative modeling is the task of learning a probabil-
ity distribution from a given dataset D = {(x)} of sam-
ples x ∼ Pd drawn from an unknown data distribution
Pd. While this has traditionally been seen through the lens
of likelihood-maximization, GANs pose generative model-
ing as a distance minimization problem. More speciﬁcally,
these approaches recommend learning the data distribution
Pd by ﬁnding a distribution Pg that solves:

argmin

D(Pg, Pd),

Pg

(1)

where D(·, ·) is some distance or divergence between dis-
tributions. Arjovsky et al. [1] proposed using the Wasser-
stein distance in the context of GAN formulations. The
Wasserstein-p distance between distributions Pg and Pd is
deﬁned as:

Wp(Pg, Pd) =

inf

γ∈Π(Pg ,Pd)

(E(x,y)∼γ[||x − y||p])

1

p ,

(2)

where Π(Pg, Pd) is the set of all possible joint distributions
on (x, y) with marginals Pg and Pd.

Estimating the Wasserstein distance is, however, not
straightforward. Arjovsky et al. [2] used the Kantorovich-
Rubinstein duality to the Wasserstein-1 distance, which
states that:

W (Pg, Pd) = sup

Ex∼Pg [f (x)] − Ex∼Pd [f (x)],

(3)

kf kL≤1

where the supremum is over all 1-Lipschitz functions f :
X → R. The function f is commonly represented via a
deep net and various ways have been suggested to enforce
the Lipschitz constraint, e.g., [12].

While the Wasserstein distance based approaches have
been successful in several complex generative tasks, they
suffer from instability arising from incorrect estimation.
The cause behind this was noted in [33], where it was shown
that estimates of the Wasserstein distance suffer from the
‘curse of dimensionality.’ To tackle the instability and com-
plexity, a sliced version of the Wasserstein-2 distance was
employed by [8, 20, 18, 34], which only requires estimating
distances of 1-d distributions and is, therefore, more efﬁ-
cient. The “sliced Wasserstein-p distance” [5] between dis-
tributions Pd and Pg is deﬁned as

˜Wp(Pd, Pg) = (cid:20)Zω∈Ω

W p

p (Pω

d , Pω

g )dω(cid:21)

1
p

,

(4)

g , Pω

where Pω
d denote the projection (i.e., marginal) of Pg,
Pd onto the direction ω, and Ω is the set of all possible di-
rections on the unit sphere. Kolouri et al. [19] have shown
that the sliced Wasserstein distance satisﬁes the properties
of non-negativity, identity of indiscernibles, symmetry, and
subadditivity. Hence, it is a true metric.

In practice, Deshpande et al. [8] approximate the sliced
Wasserstein-2 distance between the distributions by using
samples D ∼ Pd, F ∼ Pg, and a ﬁnite number of ran-
dom Gaussian directions, replacing the integration over Ω
with a summation over a randomly chosen set of unit vec-
tors ˆΩ ∝ N (0, I), where ‘∝’ is used to indicate normaliza-
tion to unit length. With Pg (and hence, F ) being implicitly
parametrized by θg, [8] uses the following program for gen-
erative modeling:

min
θg

1

| ˆΩ| Xω∈ ˆΩ

W 2

2 (Dω, F ω).

(5)

The Wasserstein-2 distance between the projected sam-
ples Dω and F ω can be computed by ﬁnding the opti-
mal transport map. For 1-d distributions, this can be done
through sorting [32], i.e.,

W 2

2 (Dω, F ω) =

1

|D| Xi

10649

||Dω

πD(i) − F ω

πF (i)||2
2,

(6)

πD(|D|).

where πD and πF are permutations that sort the pro-
jected sample sets Dω and F ω respectively, i.e., Dω
πD(1) ≤
Dω

πD(2) ≤ . . . ≤ Dω
The program in Eq. (5), when coupled with a discrimina-
tor, was shown to work well on high-dimensional datasets.
Instead of working directly with sets D and F , it was pro-
posed that we transform them to an adversarially learnt fea-
ture space, say hD and hF respectively, where h is implic-
itly parameterized by θd, e.g., by using a deep net. The
generator, parametrized by θg, minimizes

min
θg

1

| ˆΩ| Xω∈ ˆΩ

W 2

2 (hω

D, hω

F ).

(7)

The adversarial feature space h is learnt via a discrimina-
tor which classiﬁes real and fake data. This discriminator
can be written as ωT
d h, where ωd is a logistic layer and the
parameters are learnt using

ˆθd, ˆωd = argmax

θd,ωd Xx∈D

ln(σ(ωT

d hx))+Xˆx∈F

ln(1 − σ(ωT

d hˆx)).

(8)

3. Analysis and Max-Sliced Distance

In this section we provide the ﬁrst analysis of the sample-
complexity beneﬁts of the sliced Wasserstein distance com-
pared to the Wasserstein distance. We discuss how ‘projec-
tion complexity’ is a shortcoming of the sliced Wasserstein
distance and present as a ﬁx the max-sliced Wasserstein dis-
tance, which – as we will show – enjoys the same beneﬁcial
sample-complexity as the slice Wasserstein distance, albeit
necessitating estimation of a maximum. We will then show
how those results are used for training GANs.

3.1. Sample complexity of the Wasserstein and

sliced Wasserstein distances

We ﬁrst show the beneﬁts of using the sliced Wasserstein
distance over the Wasserstein distance. Speciﬁcally, we
show that, in certain cases, estimation of the sliced Wasser-
stein distance has polynomial complexity, while the Wasser-
stein distance does not. To make this notion concrete, we
introduce ‘generalizability’ of a distance:

Deﬁnition 1 Consider a family of distributions P over Rd.
A distance dist(·, ·) is said to be P-generalizable if there
exists a polynomial g such that for any two distributions
µ, ν ∈ P, and their empirical ensembles ˆµ, ˆν with size n =
g(d, 1/ǫ), ǫ > 0, the following holds:

|dist(µ, ν) − dist(ˆµ, ˆν)| ≤ ǫ w.p. ≥ 1 − polynomial(−n).

With this deﬁnition, we can prove the following result:

Claim 1 Consider the family of Gaussian distributions

P = {N (a, I) | a ∈ Rd}.

The sliced Wasserstein-2 distance ˜W2 deﬁned in Eq. (4)
is P-generalizable whereas the Wasserstein-2 distance W2
deﬁned in Eq. (2) is not.

Proof. See the supplementary material.

(cid:4)

Claim 1 implies that for GAN training, under certain
conditions, it is better to use the sliced Wasserstein distance
as we can get a more accurate training signal with a ﬁxed
computational budget. This will result in a more stable dis-
criminator.

Even though the sliced Wasserstein distance enjoys bet-
ter sample complexity, it has limitations when a ﬁnite num-
ber of random projection directions is used. We refer to this
property as ‘projection complexity’ and illustrate it in the
following section. We then present our proposed method to
help alleviate this problem.

3.2. Projection complexity of the Sliced Wasserstein

Distance

We begin with a simple example to demonstrate the lim-
itations of using ˜W2 deﬁned in Eq. (4) for learning distribu-
tions through gradient descent. To analyze the ‘projection
complexity’ of ˜W2 we use inﬁnitely many samples, but we
use only ﬁnitely many directions ω ∈ ˆΩ.

Concretely, consider two d-dimensional Gaussians µ, ν
with identity covariance. Let µ = N (0, I) = Pd be the
data distribution and let ν = N (βˆe, I) = Pg be the in-
duced generator distribution, parametrized only by its mean
β, while ˆe is a ﬁxed unit vector. Using gradient descent on
the estimated sliced Wasserstein distance between µ and ν,
we aim to learn β so that µ = ν. Thus, the updates for β are

β ← β − α∇β ˜W2(µ, ν),

(9)

where α is the learning rate.

The sliced Wasserstein distance ˜W2 is calculated by pro-
jecting the distributions (since we use inﬁnitely many sam-
ples) onto random directions and comparing the projections,
i.e., marginals. Therefore, the estimated distance is

˜W2(µ, ν) =

1

| ˆΩ| Xω∈ ˆΩ

W2(µω, ν ω),

(10)

where W2(µω, ν ω) is the Wasserstein distance between
marginal distributions µω, ν ω. Note that each ω is normal-
ized to unit norm.

Intuitively, projection of the Gaussians µ, ν onto any di-
rection other than ˆe makes them appear closer than they ac-
tually are – making the learning process slower. For any

10650

1

0.8

0.6

0.4

0.2

0

2
|
|

µ

|
|

˜W2, 1 projections
˜W2, 10 projections
˜W2, 100 projections
˜W2, 1000 projections
discriminator max- ˜W2
max- ˜W2

2
|
|

µ

|
|

1

0.8

0.6

0.4

0.2

0

2
|
|

µ

|
|

1

0.8

0.6

0.4

0.2

0

0

500

1,000

1,500

2,000

0

500

1,000

1,500

2,000

0

500

1,000

1,500

2,000

Iterations
(a) d = 10

Iterations
(b) d = 100

Iterations
(c) d = 1000

Figure 1: Convergence of the mean for different sampling strategies for learning the mean of a d-dimensional Gaussian
using the sliced Wasserstein distance and the max-sliced Wasserstein distance. Numbers in the legend denote the number of
projection directions used.

)
ω
(
2 2
W

2

1.5

1

0.5

0

0

Discriminator direction

π
2

π

(c) Wasserstein-2 distance along different
projection angles (in radians) in the fea-
ture space.

(a) Original distributions.

(b) In feature space.

Figure 2: The discriminator is able to identify important projection directions. The discriminator transforms the distributions
in Fig. 2a to Fig. 2b. In this new space, the discriminator’s direction is aligned with the one along which the distributions are
the most dissimilar as shown in Fig. 2c.

given ω, it is easy to see that W2(µω, νω) = β|ˆeT ω|. There-
fore, the update equation for β is

β → β − α

1

| ˆΩ| Xω∈Ω

|ˆeT ω|.

(11)

The updates to β are particularly small for high dimen-
sional distributions, since any random unit-norm direction
ω is orthogonal to ˆe with high probability. Therefore, β →
0 very slowly. We verify this effect empirically in Fig. 1, ex-
perimenting with different numbers of random projections
and ﬁnd that using the sliced Wasserstein distance results in
very slow convergence. This problem is further aggravated
when the dimensions of the distributions increase.

It is intuitively obvious that the aforementioned problem
can easily be solved by choosing ˆe as the projection direc-
tion. This results in larger updates and, consequently, faster
convergence. This intuition is also veriﬁed empirically. We
repeat the same experiment of learning β, but this time we
use only one projection direction ω = ˆe. This is labelled as
max- ˜W2 in Fig. 1. By simply using the important projection
direction, we achieve fast convergence of the mean.

Considering this example, it is evident that some projec-

tion directions are more meaningful than others. Therefore,
GAN training should beneﬁt from including such directions
when comparing distributions. This observation motivates
the max-sliced Wasserstein distance which we discuss next.

3.3. Max sliced Wasserstein distance

In this section we introduce the max-sliced Wasserstein
distance and illustrate that it ﬁxes the ‘projection complex-
ity’ concern. We also prove that the max-sliced Wasserstein
distance enjoys the same sample-complexity as the sliced
Wasserstein distance, i.e., we are not trading one beneﬁt for
another.

As noted in Sec. 3.2, it is useful to include the most
meaningful projection direction. Formally, for the afore-
mentioned example of µ = N (0, I), ν = N (βˆe, I), we
want to use the direction ω∗ that satisﬁes

ω∗ = argmax

|ˆeT ω|.

ω∈Ω

(12)

Comparing distributions along such a direction ω∗ can, in
fact, be shown to be a proper distance. We call it the ‘max-
sliced Wasserstein distance’ and deﬁne it as follows:

10651

Algorithm 1: Training the improved Sliced Wasserstein Generator

Given : Generator parameters θg, Discriminator parameters θd, ωd, sample size n, learning rate α

1 while θg not converged do
2

for i ← 0 to k do

3

4

5

6

7

8

9

10

11

12

Sample data {Di}n
compute surogate loss s(ωT hD, ωT hF (θg))

i=1 ∼ Pd, generated samples {F i
θg

return L ← s(ωT hD), ωT hF (θg));

}n
i=1 ∼ Pg;

(ˆω, ˆθd) ← (ˆω, ˆθd) − α∇ω,θd L;

end
compute max-sliced Wasserstein Distance max- ˜W2(ˆωT hD, ˆωT hF (θg))

Sample data {Di}n
sort ˆωT hD and ˆωT hF (θg) to obtain permutations πD, πF ;

i=1 ∼ Pd, generated samples {F i
θg

}n
i=1 ∼ Pg;

return L = Pi kˆωT hDπD (i) − ˆωT hFπF (i)(θg)k2

θg ← θg − α∇θg L;

2;

13 end

Deﬁnition 2 Let Ω be the set of all directions on the unit
sphere. Then, the max-sliced Wasserstein-2 distance be-
tween distributions µ and ν is deﬁned as:

max- ˜W2(µ, ν) = (cid:20)max

ω∈Ω

W 2

2 (µω, ν ω)(cid:21)

1
2

.

(13)

As illustrated in the following claim, it can be shown

easily that max- ˜W2(·, ·) is a valid distance.

Claim 2 The max-sliced Wasserstein-2 distance deﬁned in
Eq. (13) is a well deﬁned distance between distributions.

Proof. See supplementary material.

(cid:4)

We can also show that the max-sliced Wasserstein dis-

tance has polynomial sample complexity:

Claim 3 Consider the family of Gaussian distributions

P = {N (a, I) | a ∈ Rd}.

The max-sliced Wasserstein-2 (max- ˜W2) distance is P-
generalizable.

Proof. See the supplementary material.

(cid:4)

Since it is a valid metric, we can directly use the max-

sliced Wasserstein distance for learning distributions.

By deﬁnition, the max-sliced Wasserstein distance over-
comes the limitation discussed in Sec. 3.2. However, we
note that the use of a max-estimator is necessary, which is
harder than estimation of a conventional random variable.
In the following section, we discuss how the max-sliced
Wasserstein distance can be estimated and used in a GAN-
like setting.

3.4. max sliced GAN

In this section, we discuss our approach that uses the
max-sliced Wasserstein distance to train a GAN. We also

discuss how we approximate the max-sliced Wasserstein
distance in practice. Since we use max- ˜W2, we are able to
achieve signiﬁcant savings in terms of the number of pro-
jection directions needed as compared to [8].

Intuitively, we want to project data into a space where
real samples can easily be differentiated from artiﬁcially
generated points. To this end, we work with an adversar-
ially learnt feature space, i.e., we use the penultimate layer
of a discriminator network. In this feature space, we mini-
mize the max-sliced Wasserstein distance max- ˜W2. As will
be discussed later in this section, ﬁnding the actual max is
hard and therefore we resort to approximating it.

Let Pd again denote the data distribution and let Pg refer
to the induced generator distribution. Further, let the dis-
criminator be represented as ωT
d h(.), where ω denotes the
weights of a fully connected layer and h represents the fea-
ture space we are interested in. Further, let hD and hF rep-
resent the two empirical distributions in this feature space.
Then, we would like to solve

max- ˜W2(hD, hF ) = max
ω∈Ω

W2(hω

D, hω

F ),

(14)

where Ω is the set of all normalized directions. There is no
easy way in general to solve

ω∗ = argmax

ω∈Ω

W2(hω

D, hω

F ),

(15)

D, hω

even if the parameters θd of the feature transform h are
ﬁxed. This is because computation of the Wasserstein dis-
tance W2(hω
F ) in the 1-dimensional case requires sort-
ing, i.e., solving of a minimization problem. Hence the pro-
gram given in Eq. (15) is a saddlepoint objective, for which
both maximization and minimization can be solved exactly
when assuming the parameters of the other program to be
ﬁxed.

10652

en-es es-en

en-fr fr-en

en-de de-en

en-ru ru-en

en-zh zh-en

[6] - NN
[6] - CSLS
Max-sliced WGAN - NN
Max-sliced WGAN - CSLS

79.1 78.1
81.7 83.3
79.6 79.1
82.0 84.1

78.1 78.2
82.3 82.1
78.2 78.5
82.5 82.3

71.3
74.0
71.9
74.8

69.6
72.2
69.6
73.1

37.3 54.3
44.0 59.1
38.4 58.7
44.6 61.7

30.9
32.5
34.9
35.3

21.9
31.4
25.1
31.9

Table 1: Unsupervised word translation. We show the retrieval precision P@1 on 5 pairs of languages on MUSE bilingual
dictionaries [6]: English (‘en’), French (‘fr’), German (‘de’), Russian (‘ru’) and Chinese (‘zh’).

If we want to jointly ﬁnd the parameters θd of the feature
transform h and the projection direction ω, i.e., if we want
to solve

ω∗, θ∗

d = argmax
ω∈Ω,θd

W2(hω

D, hω

F ),

(16)

using gradient descent based methods, we also need to pay
attention to bounded-ness of the objective. Using regular-
ization often proves tricky and may require separate tuning
for each use case.

To circumvent those difﬁculties when jointly searching
d, we use a surrogate function s and write the

for ω∗ and θ∗
objective for the discriminator as follows:

ˆω, ˆθd = argmax
ω∈Ω,θd

s(ωT hD, ωT hF ).

(17)

Intuitively, and in spirit similar to max- ˜W2, we want the
surrogate function s to transform the data via h into a space
where hD and hF are easy to differentiate. Moreover, we
want ω to be the direction which best separates the trans-
formed real and generated data. A variety of surrogate func-
tions such as the log-loss as speciﬁed in Eq. (8), the hinge-
loss, or a moment separator with

s(ωT hD, ωT hF ) = Xx∈D

ωT hx − Xˆx∈F

ωT hˆx

(18)

come to mind immediately.

For instance, in case of a log-loss, ωT h learns to classify
real and fake samples, essentially performing linear logis-
tic regression using ω on a learned feature representation h.
If trained to optimality, the two distributions are well sep-
arated in the discriminator’s feature space h. An example
is given in Fig. 2. The discriminator takes two distribu-
tions, shown in Fig. 2a and is trained to classify them. In
doing so the discriminator transforms them to the feature
space shown in Fig. 2b.
In this simple example, we can
plot the Wasserstein distance along the different projection
directions. This is visualized in Fig. 2c. The discrimina-
tor’s ﬁnal layer can be considered as a projection direction.
This direction is very close to the maximizer of the pro-
jected Wasserstein distance in the feature space.

Additionally, in this case, ω∗ can be approximated with
ˆω – because the discriminator, trained for classiﬁcation, es-
sentially separates the distributions along ˆω. If we compute
the Wasserstein-2 distance for projections onto different an-
gles (as in Fig. 2c), we see that the maximum distance is

achieved close to the projection direction from the discrim-
inator, i.e., ˆω. We next assess: ‘how close?’

While log-loss and all other functions seem intuitive, we
provide for the special case of the moment separator given
in Eq. (18) and an identity transform h the maximal sub-
optimality in terms of the max-sliced Wasserstein distance:

Claim 4 For the surrogate function s given in Eq. (18), h
the identity, and ˆω computed as speciﬁed in Eq. (17), we
obtain

α(D, F) ≤ W 2

2 (D ˆω, F ˆω) ≤ V ∗ = max- ˜W2(D, F)2,

for a lower bound α(D, F) = kmk2

Pi Fi is the difference of dataset means.

Proof. See the supplementary material.

2, where m = Pi Di −

(cid:4)

To summarize, training the discriminator for classiﬁca-
tion provides a rich feature space which can be utilized for
faster training. We note that the discriminator might be
trained to obtain such features in a more explicit manner,
but we leave this to future research.

3.5. max sliced GAN Algorithm

We summarize the resulting training process in Alg. 1.
It proceeds as follows: In every iteration, we draw a set of
samples D and F from the true and fake distributions. We
optimize the parameters θd and ω of the feature transform
h for k iterations (k is a hyper-parameter) to maximize a
surrogate loss function s(ωT hD, ωT hF ). Then we compute
the Wasserstein-2 distance between the output distributions
of the discriminator, i.e., W2(ˆωT hD, ˆωT hF ). The generator
is trained to minimize this distance. In our experiments, we
choose h to be the binary classiﬁcation loss.

4. Experiments

In this section, we present results to demonstrate the ef-
fectiveness of the max-sliced Wasserstein distance and the
computational beneﬁts it offers over the sliced Wasserstein
distance. We show quantitative results on unpaired word
translation [6], and qualitative and quantitative results on
image generation tasks using the CelebA-HQ [16] and the
LSUN Bedrooms [36] datasets.

10653

(a) Max-sliced Wasserstein GAN

(b) Sliced Wasserstein GAN with 100 projections

(c) Sliced Wasserstein GAN with 1000 projections

(d) Sliced Wasserstein GAN with 10,000 projections

Figure 3: Generated samples (256 × 256) from CelebA-HQ.

4.1. Word Translation without Parallel Data

We evaluate the effectiveness of the max-sliced GAN
on unsupervised word translation tasks,
i.e., without
paired/parallel data [6]. This allows us to quantitatively
compare different methods.

The setting of this experiment is as follows. We are given
embeddings of words from two languages, say X, Y ∈ Rd.
We want to learn an orthogonal transformation W ∗ that
maps the source embeddings X to Y , i.e.:

W ∗ =

argmin

||W X − Y ||F .

(19)

W ∈Rd×d,orthogonal

The current state-of-the-art [6] employs a GAN-like [11]
adversary to learn the transformation. Therefore, the trans-
formation is learned by minimizing the Jenson-Shannon di-
vergence between W X and Y . We instead minimize the
max-sliced Wasserstein distance to learn W .

We follow the training method and evaluation in [6] and

report the word translation precision by computing the re-
trieval precision@k for k = 1 on the MUSE bilingual dic-
tionaries [6]. During testing, 1,500 queries are tested and
200k words of the target language are taken into account.
We compare our method with [6] and present results for 5
pairs of languages in Tab. 1. In Tab. 1 ‘NN’ represents use
of nearest neighbors to build the dictionary after training
the transformation W , and ‘CSLS’ stands for use of cross-
domain similarity local scaling [6]. Our method with CSLS
outperforms the baseline in all tested language pairs. This
demonstrates the competitiveness of our method with cur-
rent established GAN frameworks.

4.2. Image Generation

In this section, we present results on the task of image
generation. Using the max-sliced Wasserstein distance, we
train a GAN on the CelebA [16] and LSUN Bedrooms [36]
datasets for images of resolution 256x256. We compare
with the sliced Wasserstein GAN [8].

10654

(a) Max-sliced Wasserstein GAN

(b) Sliced Wasserstein GAN with 100 projections

(c) Sliced Wasserstein GAN with 1000 projections

(d) Sliced Wasserstein GAN with 10,000 projections

Figure 4: Generated samples (256 × 256) from LSUN Bedrooms.

Samples generated by each trained model are presented
in Fig. 3 and Fig. 4. The results of the max-sliced Wasser-
stein GAN are shown Fig. 3a and Fig. 4a. We train the sliced
Wasserstein GAN with 100, 1000, and 10000 random pro-
jections. Results of each of these are respectively shown in
Fig. 3b, Fig. 3c, and Fig. 3d for CelebA-HQ, and in Fig. 4b,
Fig. 4c, and Fig. 4d for LSUN. The max-sliced Wasserstein
GAN using just one projection direction is able to produce
results which are either comparable or better than the sliced
Wasserstein GAN even when using 10000 projections. This
signiﬁcantly reduces the computational complexity and also
the memory footprint of the model.

We used a simple extension of the popular DCGAN ar-
chitecture for the generator and discriminator. Two extra
strided (transpose) convolutional layers are added to the
generator and the discriminator to scale to 256x256. We
do not use any special normalization/ initialization to train
the models. Speciﬁc details are given in the supplementary.

5. Conclusion

In this paper, we analyzed the Wasserstein and sliced
Wasserstein distance and developed a simple yet effective
training strategy for generative adversarial nets based on the
max-sliced Wasserstein distance. We showed that this dis-
tance enjoys a better sample complexity than the Wasser-
stein distance, and a better projection complexity than the
sliced Wasserstein distance. We developed a method to
approximate it using a surrogate loss, and also analyzed
the approximation error for one such surrogate. Empiri-
cally, we showed that the discussed approach is able to learn
high dimensional distributions. The method requires orders
of magnitude fewer projection directions than the sliced
Wasserstein GAN even though both work in a similar dis-
tance space.

Acknowledgments: This work is supported in part by NSF under
Grant No. 1718221, Samsung, and 3M. We thank NVIDIA for
providing GPUs used for this work.

10655

References

[1] M. Arjovsky and L. Bottou. Towards principled methods for

training generative adversarial networks. In ICLR, 2017. 2

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan.

In ICML, 2017. 1, 2

[3] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generaliza-
tion and equilibrium in generative adversarial nets (gans). In
ICML, 2017. 1

[4] D. Berthelot, T. Schumm, and L. Metz. Began: Boundary
equilibrium generative adversarial networks. arXiv preprint
arXiv:1703.10717, 2017. 1

[5] N. Bonneel, J. Rabin, G. Peyr´e, and H. Pﬁster. Sliced and
radon wasserstein barycenters of measures. Journal of Math-
ematical Imaging and Vision, 2015. 2

[6] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, and H. Je-
gou. Word translation without parallel data. In ICLR, 2018.
2, 6, 7

[7] R. W. A. Cully, H. J. Chang, and Y. Demiris. Magan: Mar-
gin adaptation for generative adversarial networks. arXiv
preprint arXiv:1704.03817, 2017. 1

[8] I. Deshpande, Z. Zhang, and A. Schwing. Generative mod-
eling using the sliced wasserstein distance. In CVPR, 2018.
1, 2, 5, 7

[9] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial fea-

ture learning. In ICLR, 2017. 1

[10] S. Feizi, C. Suh, F. Xia, and D. Tse. Understanding gans: the

lqg setting. arXiv preprint arXiv:1710.10793, 2017. 1

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 7

[12] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
In

Improved training of wasserstein gans.

A. Courville.
NIPS, 2017. 1, 2

[13] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. Gans trained by a two time-scale update rule
converge to a local nash equilibrium. In NIPS, 2017. 1

[14] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
Unsupervised Image-to-Image Translation. In Proc. ECCV,
2018. 1

[15] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 1

[16] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In ICLR, 2017. 1, 2, 6, 7

[17] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013. 1

[18] S. Kolouri, C. E. Martin, and G. K. Rohde.

Sliced-
wasserstein autoencoder: An embarrassingly simple gener-
ative model. arXiv preprint arXiv:1804.01947, 2018. 2

[19] S. Kolouri, S. R. Park, and G. K. Rohde. The radon cu-
mulative distribution transform and its application to image
classiﬁcation. IEEE transactions on image processing, 2016.
2

[20] S. Kolouri, G. K. Rohde, and H. Hoffman. Sliced wasserstein
In CVPR,

distance for learning gaussian mixture models.
2018. 1, 2

[21] H. Y. Lee, H. Y. Tseng, J. B. Huang, M. K. Singh, and M. H.
Yang. Diverse image-to-image translation via disentangled
representation. In Proc. ECCV, 2018. 1

[22] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. P´oczos.
Mmd gan: Towards deeper understanding of moment match-
ing network. In NIPS, 2017. 1

[23] Z. Lin, A. Khetan, G. Fanti, and S. Oh. Pacgan: The power
of two samples in generative adversarial networks. In NIPS,
2018. 1

[24] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised Image-to-

Image Translation Networks. In Proc. NIPS, 2017. 1

[25] L. Mescheder, A. Geiger, and S. Nowozin. Which training

methods for gans do actually converge? In ICML, 2018. 1

[26] Y. Mroueh and T. Sercu. Fisher gan. In NIPS, 2017. 1
[27] Y. Mroueh, T. Sercu, and V. Goel. Mcgan: Mean
arXiv preprint

and covariance feature matching gan.
arXiv:1702.08398, 2017. 1

[28] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016. 1

[29] A. Royer, K. Bousmalis, S. Gouws, F. Bertsch, I. Moressi,
F. Cole, and K. Murphy. Xgan: Unsupervised image-
to-image translation for many-to-many mappings.
In
arXiv:1711.05139, 2017. 1

[30] T. Salimans, H. Zhang, A. Radford, and D. Metaxas.
proving gans using optimal transport. In ICLR, 2018. 1

Im-

[31] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training. In CVPR, 2017. 1

[32] C. Villani. Optimal transport: old and new. Springer Science

& Business Media, 2008. 2

[33] J. Weed and F. Bach. Sharp asymptotic and ﬁnite-sample
rates of convergence of empirical measures in wasserstein
distance. arXiv preprint arXiv:1707.00087, 2017. 2

[34] J. Wu, Z. Huang, W. Li, J. Thoma, and L. Van Gool.
arXiv preprint

Sliced wasserstein generative models.
arXiv:1706.02631, 2017. 1, 2

[35] Z. Yi, H. Zhang, P. Tan, and M. Gong. Dualgan: Unsuper-
vised dual learning for image-to-image translation. In Proc.
ICCV, 2017. 1

[36] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and
J. Xiao. Lsun: Construction of a large-scale image dataset
using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015. 2, 6, 7

[37] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In ICCV, 2017. 1

[38] J. Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-
to-image translation. In Proc. NIPS, 2017. 1

10656

