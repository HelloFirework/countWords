Collaborative Learning of Semi-Supervised Segmentation and Classiﬁcation for

Medical Images

Yi Zhou, Xiaodong He, Lei Huang, Li Liu, Fan Zhu, Shanshan Cui and Ling Shao

Inception Institute of Artiﬁcial Intelligence (IIAI), Abu Dhabi, UAE

{yi.zhou, xiaodong.he, lei.huang, li.liu, fan.zhu, shanshan.cui, ling.shao}@inceptioniai.org

Abstract

Medical image analysis has two important research ar-
eas: disease grading and ﬁne-grained lesion segmentation.
Although the former problem often relies on the latter, the
two are usually studied separately. Disease severity grad-
ing can be treated as a classiﬁcation problem, which only
requires image-level annotations, while the lesion segmen-
tation requires stronger pixel-level annotations. However,
pixel-wise data annotation for medical images is highly
time-consuming and requires domain experts. In this pa-
per, we propose a collaborative learning method to jointly
improve the performance of disease grading and lesion seg-
mentation by semi-supervised learning with an attention
mechanism. Given a small set of pixel-level annotated data,
a multi-lesion mask generation model ﬁrst performs the tra-
ditional semantic segmentation task. Then, based on ini-
tially predicted lesion maps for large quantities of image-
level annotated data, a lesion attentive disease grading
model is designed to improve the severity classiﬁcation ac-
curacy. Meanwhile, the lesion attention model can reﬁne
the lesion maps using class-speciﬁc information to ﬁne-tune
the segmentation model in a semi-supervised manner. An
adversarial architecture is also integrated for training. With
extensive experiments on a representative medical problem
called diabetic retinopathy (DR), we validate the effective-
ness of our method and achieve consistent improvements
over state-of-the-art methods on three public datasets.

1. Introduction

In the medical imaging community, automatic disease
diagnosis has been widely explored and applied to var-
ious practical computer-aided medical systems. Disease
grading [28, 6, 50, 40] and pixel-wise lesion segmentation
[12, 10, 29] are two main fundamental problems in this area.
The goal of disease grading is to predict the classiﬁcation
label for the severity of a disease, while segmentation aims
to address more ﬁne-grained, pixel-wise lesion detection.

Pixel-level	annotated	data

Pixel-wise	
Supervision

Discriminator

Adversarial	

Learning

~	×10& images

Lesion	Segmentation

Pixel-wise	
Supervision

Pseudo	Lesion	

Masks

Lesion	Attentive	

Classification

Disease	
Grading

~	×10’ images

Image-level	annotated	data

Attention	maps	for	semi-
supervised	learning

Figure 1. Illustration of the proposed collaborative learning
method of semi-supervised multi-lesion segmentation and disease
severity classiﬁcation. Here we conduct studies on the fundus im-
ages for diabetic retinopathy.

These two tasks are usually studied independently. How-
ever, accurate lesion detection can make huge contributions
to classifying the disease grades, while class-speciﬁc infor-
mation can also beneﬁt segmentation performance.

Labeling medical images is expensive since it requires
the very time-consuming dedication of domain experts, es-
pecially for pixel-level annotations. Compared with general
object segmentation tasks [23, 16, 45, 51, 8], which have
large amounts of annotated training data available, employ-
ing a fully-supervised architectures [26, 9, 20] to train med-
ical models is impractical. However, purely-unsupervised
learning approaches [39, 12, 55] are also not acceptable due
to their limited accuracy. As such, we aim to develop a
semi-supervised method [34, 35], which can use the lim-
ited number of pixel-level annotated images available along
with the large quantities of broader, image-level annotations
to simultaneously enhance the performance of both the seg-
mentation and classiﬁcation models.

In this paper, we propose a collaborative learning method
for disease grading and lesion segmentation and select a
common disease called diabetic retinopathy (DR) for eval-
uation. DR is an eye disease that results from diabetes
mellitus, and can lead to blindness. The severity of DR
can be graded into ﬁve stages: normal, mild, moderate, se-
vere non-proliferative and proliferative, according to inter-
national protocol [19, 3]. The severity grading has strong
correlations with different lesion symptoms, such as mi-

2079

croaneurysms, haemorrhages, hard exudates and soft exu-
dates appearing on the fundus images. Therefore, multi-
lesion segmentation is highly beneﬁcial for analyzing DR
gradings. However, since acquiring large quantaties of
pixel-level lesion annotation is difﬁcult, a semi-supervised
segmentation method is proposed together with image-level
severity classiﬁcation for joint optimization. Fig. 1 illus-
trates the idea of our proposed method. For images with
pixel-level lesion annotations, we pre-train a segmentation
model in a fully-supervised manner. Then, a large num-
ber of images with only disease grade labels can be passed
through the pre-trained segmentation model to generate
weak lesion maps. We take the predicted masks with the
original images as inputs for learning the lesion attentive
classiﬁcation model. This model both improves the disease
grading performance and outputs lesion attention maps as
reﬁned pseudo masks that can be used to ﬁne-tune the seg-
mentation model. The main contributions of our method are
highlighted as follows:

(1) A multi-lesion mask generator is proposed for the
pixel-wise segmentation. Due to extremely limited train-
ing data, we carefully design an Xception-module based U-
shape network and a joint objective function that incorpo-
rates a supervised segmentation loss and an unsupervised
adversarial loss for training.

(2) For image-level annotated data, we devise a lesion
attention model that can automatically predict lesion maps
adopting only weak supervision of class-speciﬁc informa-
tion. The predicted maps can be used to ﬁne-tune the previ-
ous segmentation model together with fully-annotated data
in a semi-supervised learning manner.

(3) The lesion segmentation and disease grading tasks
are optimized in an end-to-end model. The massive amount
of class-annotated data can beneﬁt the segmentation perfor-
mance. Meanwhile, enhanced pixel-wise lesion segmenta-
tion can improve grading accuracy. Extensive ablation stud-
ies and comparison experiments conducted on the DR data
have shown the effectiveness and superiority of our method.

2. Related Work

Disease Grading and Lesion Detection. Recent state-
of-the-art disease grading and lesion detection methods in
medical imaging tend to adopt general deep learning mod-
els. CNN architectures [33, 15, 19] have been proposed to
diagnose DR by classifying its severity. Feature selection
from the determined features [17] has been introduced for
classifying breast cancer malignancy. Moreover, to recog-
nize detailed lesions, bounding-box level detection [41, 49]
and pixel-level segmentation [13, 36] have also been stud-
ied. However, only a few works [50, 46, 5] have associated
the lesion detection and disease severity classiﬁcation.

Semi-Supervised Semantic Segmentation. For the se-
mantic segmentation task, due to the shortage of pixel-

level annotated data, semi-supervised segmentation meth-
ods [23, 47, 31] have been explored. An adversarial learn-
ing mechanism was used in [22], where the network’s dis-
criminator outputs predicted probability maps as the conﬁ-
dence maps for semi-supervised learning. Hong et al. [21]
proposed a decoupled deep neural network to separate the
classiﬁcation and segmentation networks, and used bridg-
ing layers to deliver class-speciﬁc information.

Attention Mechanisms. Visual attention addresses the
problem of extracting task-speciﬁc salient regions from im-
ages, while ignoring irrelevant parts. Attention mechanisms
have been studied for many vision tasks such as image clas-
siﬁcation [38, 27, 53, 52], ﬁne-grained recognition [54, 48]
and image captioning [4, 7]. These mechanisms can be cat-
egorized into soft and hard attention models, where the for-
mer is fully-differentiable to learning attention maps and the
latter is not differentiable and involves a stochastic process
sampling hidden states with probabilities.

3. Proposed Methods

3.1. Problem Formulation

Given pixel-level annotated images XP and image-level
annotated images XI , the ﬁnal aim of our method is to col-
laboratively optimize a lesion segmentation model G(·) and
a disease grading model C(·), which would work together
to improve the precision of one another. To train the seg-
mentation model, we aim to minimize the difference be-
tween the predicted lesion maps and the ground-truth masks
by the following function:

min

G

LX

l=1

LSeg(G(XP ), G(XI ), sP

l ),

l ,esI

(1)

where sP

images andesI

l denotes the ground-truth of pixel-level annotated
l is the pseudo masks of image-level annotated
images learned by the lesion attentive grading model. L is
the total number of lesion varieties related to a particular
disease. The optimization function for the disease grading
model is deﬁned as:

min
C,att

LCls(C(XI ) · att(G(XI )), yI ),

(2)

notated data. Note that esI

where att(·) indicates the lesion attention model and yI is
the disease severity classiﬁcation label for image-level an-
l in Eq. 1 is equal to att(G(XI )).
The detailed deﬁnitions of LSeg and LCls are explained in
Sec. 3.2 and 3.3, respectively. Therefore, to collaboratively
learn the two tasks, the most important factor to consider is
how to design and optimize G(·), C(·) and att(·).

An overview of the proposed network architecture,
which consists of two parts, is illustrated in Fig. 2. For the
ﬁrst part, we take the few XP as inputs to train a multi-
lesion mask generator in a fully-supervised manner. Once it
is pre-trained, the remaining large-scale XI are also passed

2080

Lesion	1:	

Generated	Multi-lesion	Masks
Lesion	3:

Lesion	2:

Lesion	4:

Microaneurysms

Haemorrhages

Hard	Exudates

Soft	Exudates

Pixel-level	annotated	data	𝑋"

Multi-lesion	Masks	

Data	augmentation	on	𝑋"
(e.g.	flip	and	rotation)

Generator

Original	Image

Input	
image

Xception-module	based	

U-Shape	Network

Output	
masks

i

s
t
h
g
e
w
g
n
i
r
a
h
S

	

20*20*512

40*40*256

80*80*128

40*40*256

80*80*128

160*160*64

160*160*64

320*320*32

640*640*3

320*320*32

640*640*L

Conv,	1x1,	stride=2

Tuple	Architecture

Pre-processed	Image
(mitigate	variation	

due	to	lighting	
conditions	and	

resolution)

Image-level	annotated	data	𝑋#

Multi-lesion	Masks	

Generator

"
𝑚𝑎𝑠𝑘(
)

ℒ+,

"
𝑋" +	𝑚𝑎𝑠𝑘(
)

Real	branch:	predictions	
supervised	by	pixel-level	

"
𝑚𝑎𝑠𝑘)

annotated	data	

Input	image	+	
predicted	lesion	masks

Global	Average	

Pooling

1	/	0

20*20*512

40*40*256

80*80*128

160*160*64

320*320*32

640*640*(3+L)

#
𝑋# +	𝑚𝑎𝑠𝑘(
)

Fake	branch:	predictions	
supervised		by	image-level	

annotated	data

Fully-connected

Sigmoid	activation

𝑋#

#
𝑚𝑎𝑠𝑘(
)

𝑰 for	semi-supervised	learning.
Lesion	attention	maps	as	pseudo	𝒎𝒂𝒔𝒌>
𝒍

Multi-lesion	Masks	

Discriminator

Feature	Extraction

ℒ-./

Multi-lesion	Attentive	

Model

ℒ0123423_
674.189

Figure 2. Pipeline of the proposed method. The input data consists of a very small set of pixel-level annotated lesion images XP and a large
set of images XI with only image-level labels showing the disease severity. A multi-lesion masks generator is proposed for learning the
lesion segmentation task in a semi-supervised manner, where XP has real ground-truth masks and XI uses the pseudo masks learned from
the lesion attentive disease grading model. An adversarial architecture is also proposed to beneﬁt the training. Moreover, the segmented
lesion masks are adopted to generate attentive features for improving the ﬁnal disease grading performance. The two tasks are jointly
optimized in an end-to-end network.

through the generator. A discriminator, optimized by an ad-
versarial training loss, is designed to distinguish these two
types of data. For the second part, the XI and its initially
predicted lesion maps are adopted to learn a lesion attention
model, which only employs disease grading labels. The le-
sion attentive grading model improves the classiﬁcation ac-
curacy. Moreover, the generated lesion attention maps can
be used as pseudo masks to reﬁne the lesion mask generator
for large unannotated data in a semi-supervised manner.

3.2. Adversarial Multi Lesion Masks Generator

Training a semantic segmentation model usually requires
large quantities of pixel-level annotated data. However, for
medical imaging where annotation cost is extremely high,
we have to ﬁnd a way of effectively training a model using
the limited annotated data available. In our method, we pro-
pose a multi-lesion mask generator, derived from a U-shape
network and embedded with an Xception module [11] for
this task. The U-shape network [36] was ﬁrst introduced
for the segmentation of neuron structures in electron micro-
scopic stacks. It deploys an encoder-decoder structure built
with a fully convolutional network. The skip connections
concatenate the feature maps of contracting and expansive
parts of the same spatial size. This design can best preserve
the edge and texture details in the decoding process of the
input images and speed up the convergence.

We ﬁrst extend the U-shape network with a built-in
Xception module and modify it to be a multi-lesion mask

generator. The Xception module essentially inherits its idea
from the Inception module [42], with the difference being
that a separable convolution performs the spatial convolu-
tion over each channel and the 1 × 1 convolution projects
new channels independently. We incorporate the Xcep-
tion module for lesion segmentation since the spatial cor-
relations over each channel of feature maps and the cross-
channel correlations have less inner relationship and are not
expected to jointly learn the mappings. A schematic dia-
gram of the segmentation model is shown in the yellow part
of Fig. 2. Together, the encoder and decoder consist of a to-
tal of nine feature mapping tuples. Apart from the ﬁrst tuple
of the encoder, which employs normal convolution opera-
tions, the remaining tuples are designed with the Xception
module. Each tuple is composed of two separable convo-
lutions followed by batch normalization, ReLU activation,
max-pooling and a shortcut of 1×1 convolution. The spatial
convolution kernel size is 3 × 3 and the padding is set to be
the same. In the decoder part, up-sampling and a skip con-
nection are employed before each tuple. At the end, we add
L convolution layers with Sigmoid activation to generate L
different lesion masks. Other hyper-parameter settings are
based on [36].

To optimize the lesion mask generator, we use both the
pixel-level annotated data and the image-level annotated
data. With pixel-level annotated lesion masks, a binary
cross-entropy loss LCE is used to minimize distances be-
tween the predictions and the ground-truths. Based on the

2081

lesion attention model introduced in Sec. 3.3, we also ob-
tain pseudo mask ground-truths for the image-level anno-
tated data to optimize LCE. Moreover, to generate better
lesion masks by exploiting data without pixel-level annota-
tions, we add a multi-lesion discriminator D, which con-
tributes to the training through a generative adversarial net-
work (GAN [18]) architecture. Traditional GANs consist of
a generative net and discriminative net playing a competi-
tive min-max game. A latent random vector z from a uni-
form or Gaussian distribution is usually used as the input for
the generator to synthesize samples. The discriminator then
aims to distinguish the real data x from the generated sam-
ples. The essential goal is to converge pz(z) to a target real
data distribution pdata(x). In this paper, rather than gener-
ating samples from random noise, we take the lesion maps
predicted by the generator from the pixel-level annotated
data as the real data branch and those from the image-level
annotated data as the fake sample branch. The total loss for
optimizing the lesion segmentation task can be deﬁned as:

LSeg = LAdv + λLCE
(3)
= E[log(D(XP , G(XP ))] + E[log(1 − D(XI , G(XI ))]
+ λE[−s · log G(X(P,I) − (1 − s) · log(1 − G(X(P,I)))],
where s is a brief expression of sP
l for the ground-
truths of pixel-level and image-level annotated data, respec-
tively. λ is the balance weight of two objective functions.

l and esI

The predicted multi-lesion masks are concatenated with
the input images and then taken as inputs for the discrimi-
nator D which has ﬁve convolution mapping tuples. Each
tuple consists of two convolutional layers with kernel size
of 3 and one max-pooling layer with a stride of 2 to pro-
gressively encode contextual information for an increasing
receptive ﬁeld. For each tuple, we also adopt ReLU activa-
tion and batch normalization. A global average pooling is
employed at the end of D, followed by a dense connection
and Sigmoid activation that outputs if the predicted lesion
maps are supervised by real or pseudo mask ground-truths.

3.3. Lesion Attentive Disease Grading

To grade the severity of a disease, human experts usually
make a diagnosis by observing detailed lesion signs char-
acteristic of the disease. Adopting a classic deep classiﬁca-
tion model can achieve basic performance for this, but with
limited accuracy. Visual attention models address recogni-
tion tasks in a human-like manner, automatically extract-
ing task-speciﬁc regions and neglecting irrelevant informa-
tion to improve their performance. However, most conven-
tional attention models are proposed for general object im-
ages, and only need to predict coarse attention maps. The
attention mechanism is usually designed using high-level
features. For medical images, where the lesion regions are
very small and are expected to be attended in a pixel-wise
manner, in our model we also adopt low-level feature maps

Multi-lesion	

Attentive	Features

640*640*3

𝑓JKL

𝑚J

e
c
n
a
d
u
g
	
l

i

e
v
e
l
-

w
o
L

𝑓JKL_BNN

𝛼J

Element-

wise	

product
1×1	𝐶𝑜𝑛𝑣

Sigmoid

)
s
k
s
a
m
n
o
i
s
e

	

l
	

o
d
u
e
s
P
(

Concat

3×3	𝐶𝑜𝑛𝑣

𝑓P?HP

e
c
n
a
d
u
g
	
l

i

e
v
e
l
-
h
g
i
H

𝒍

	

𝛂
	
s
p
a
M
n
o
i
t
n
e
t
t
A

Global	Average	Pooling

1*1*1024

1*1*4096

Concat								1×1	𝐶𝑜𝑛𝑣

1*1*1024

ℒ>?@AB@A_DEBF?GH

𝑣
𝑛
𝑜
𝐶
1
×
1

	

640*640*1

Global	Context	Vector

1*1*32

Figure 3. The details of the lesion attentive disease grading. The
blue part is the classiﬁcation model for disease grading and the
orange part is the attention model for learning reﬁned lesion maps.

with high resolutions to guide the learning of the attention
model. Moreover, for those images with only image-level
disease grade annotations, our lesion attentive model can
generate pixel-level attention maps, which are then used as
the pseudo masks for semi-supervised learning in the lesion
segmentation model.

The lesion attentive disease grading model, as shown
in Fig. 3, is composed of a main branch for feature ex-
traction and classiﬁcation of the input disease images, and
L branches for learning the attention models of the L le-
sions. We do not use the lesion masks initially predicted by
the segmentation model to directly attend the classiﬁcation
model because the number of pixel-level annotated medical
images is usually very small and thus the initially predicted
masks are too weak to use. Moreover, the image-level grad-
ing labels can be exploited to deliver discriminative local-
ization information to reﬁne the lesion attention maps.

The disease grading model C(·) and lesion attention
model att(·) in our method are tightly integrated. We ﬁrst
take a disease classiﬁcation model with a basic convolu-
tional neural network to learn grading using only input im-
ages. Once it is pre-trained, f low and f high, which de-
note the low-level and high-level feature representations,
respectively, can be extracted as pixel-wise and category-
wise guidance for learning the attention model. Moreover,
we also encode the initially predicted lesion maps, denoted
by mL
l=1, as inputs to the attention model. The overall ex-
pression is deﬁned by the following equation:
l=1),

αL
l=1 = att(f low, f high, mL

(4)

where the outputs αL
l=1 are the attention maps that give high
responses to different lesion regions that characterize the
disease. The proposed attention mechanism consists of two
steps. The ﬁrst step is to exploit pixel-wise lesion features

2082

by fusing the encoded low-level embeddings from both the
input images and the initially predicted lesion masks. For
the l-th lesion, we can obtain an intermediate state for an
attentive feature by the equation:

f low att
l

= ReLU(Wlow

l

concat(ml, f low) + blow

l

),

(5)

where concat(·) indicates the channel-wise concatenation.
For the second step, we use a global context vector to cor-
relate with the low-level attentive features and further gen-
erate the lesion maps as:
αl = Sigmoid(Whigh

⊙ f high] + bhigh

[f low att

(6)

),

l

l

l

l

where ⊙ denotes an element-wise multiplication. The
global context vector f high has the same channel dimension
as f low att
, which is computed through a 1 × 1 convolution
over the top layer feature from the basic pre-trained clas-
siﬁcation model. This high-level guidance contains abun-
dant category information to weight low-level features and
, Whigh
reﬁne precise lesion details. Note that Wlow
and
bias terms are learnable parameters for the l-th lesion.

l

l

Based on the L lesion attention maps, we conduct an
element-wise multiplication with the low-level image fea-
tures f low separately and use these attentive features to ﬁne-
tune the pre-trained disease classiﬁcation model. All the le-
sion attentive features share the same weights as the grading
model and the output feature vectors are concatenated for
learning a ﬁnal representation. The objective function LCls
for disease grading adopts the focal loss [24] due to the im-
balanced data problem. Meanwhile, the reﬁned multi-lesion
attention maps are used as pseudo masks to co-train the seg-
mentation model in a semi-supervised manner.

3.4. Implementation Details

The training scheme for our model consists of two
stages. In the ﬁrst step, we pre-train the multi-lesion seg-
mentation model using the pixel-level annotated data by
LCE, and the basic disease severity classiﬁcation model
using the image-level annotated data by LCls. Both are
trained in a fully-supervised manner. The ADAM optimizer
is adopted with the learning rate of 0.0002 and momentum
of 0.5. The mini-batch size is set to 32 for pre-training
the segmentation model over 60 epochs, while the grading
model is pre-trained over 30 epochs with batch size of 128.
Once the pre-training is complete, the initially predicted
lesion masks, along with the low-level and high-level fea-
ture representations of the input images, can be obtained
to simultaneously train the lesion attention model for semi-
supervised segmentation and further improve the grading
In this stage, we add the LAdv for semi-
performance.
supervised learning and the lesion attention module for dis-
ease grading. The whole model is ﬁne-tuned in an end-to-
end manner. λ in Eq. 3 is set to 10, which yields the best
performance. The batch size is set to 16 for ﬁne-tuning over
50 epochs. All experiments are run on an Nvidia DGX-1.

4. Experimental Results

4.1. Datasets and Evaluation Metrics

IDRID Dataset [32] is the only DR dataset provid-
ing pixel-level multi-lesion annotations, to the best of our
knowledge. It contains 81 color fundus images with symp-
toms of DR and is split into 54 images for training and 27
images for testing. The lesions, including microaneurysms,
haemorrhages, hard exudates and soft exudates are anno-
tated by medical experts with binary masks.
IDRID also
has an image-level annotated set containing 413 training
images and 103 testing images, which only have severity
grading labels. We use the lesion segmentation set to train
the multi-lesion mask generator in a fully-supervised man-
ner. Then, the grading set is used to learn the lesion atten-
tive model for classiﬁcation and semi-supervised segmenta-
tion. EyePACS Dataset [2] consists of 35,126 training im-
ages and 53,576 testing images. The grading protocol is the
same as the IDRID dataset, with ﬁve DR categories. How-
ever, the images collected from this dataset are captured by
different types of cameras, under various light conditions
and weak annotation quality. Since the dataset only has
image-level grading labels, we mainly adopt it to train the
lesion attentive disease grading model. Messidor Dataset
[14] contains 1200 eye fundus images but its grading scale
is different from that of the previous two datasets, having
only 4 levels. Grades 0 and 1 are marked as non-referable,
while Grades 2 and 3 are considered referable. All grades
other than Grade 0 indicate an abnormal case of DR. Fol-
lowing the evaluation protocol used in [46], we only adopt
this dataset for testing the models trained on EyePACS.

Data Pre-Processing and Augmentation. Since the
fundus images from different datasets have various illumi-
nations and resolutions, we proposed a data pre-processing
method (clariﬁed in the supplementary ﬁle) based on [43]
to unify the image quality and sharpen the texture details.
Moreover, to augment the data, horizontal ﬂips, vertical
ﬂips and rotations are conducted, which can also mitigate
the imbalance of samples across different classes.

Evaluation Metrics. To quantitively evaluate the per-
formance of the lesion segmentation task, we compute the
area under curve (AUC) value for both the receiving operat-
ing characteristic (ROC) curve and precision and recall (PR)
curve. Moreover, to evaluate the precision of the DR grad-
ing model, in addition to the normal classiﬁcation accuracy,
a quadratic weighted kappa metric [2] is introduced.

4.2. Ablation Studies

4.2.1 Qualitative Multi-lesion Segmentation Results

Before evaluating the quantitative lesion segmentation pre-
cision and DR grading accuracy, we ﬁrst qualitatively
demonstrate the effectiveness of the lesion attention model
for semi-supervised segmentation on the IDRID dataset,

2083

Microaneurysms

Pre-trained				Semi-supervised			Ground-truth

Haemorrhages

Pre-trained				Semi-supervised			Ground-truth

Hard	Exudates

Pre-trained				Semi-supervised			Ground-truth

Soft	Exudates

Pre-trained				Semi-supervised			Ground-truth

Figure 4. Qualitative multi-lesion segmentation results. We coarsely mark some regions to compare the initial model pre-trained on the
limited data with pixel-level lesion annotations and the semi-supervised model trained using large-scale image-level annotated data. The
green boxes denote the ground-truth. The blue boxes show the performance of our semi-supervised method, while the yellow and red boxes
highlight the miss detections and false alarms, respectively. (Best viewed zoomed in.)

which has the segmentation ground-truth. Fig. 4 compares
the segmentation results of four different lesions for the pre-
trained model adopting only the limited pixel-level anno-
tated data and the ﬁnal model semi-supervised trained with
large-scale image-level annotated data. For the pre-trained
model, the failure case is usually the miss detection of the
lesion patterns (false negative). False alarms (false positive)
also occur in some small regions. With the help of image-
level annotated data for semi-supervised segmentation, the
results are obviously improved over all lesions. The effec-
tiveness of the lesion segmentation for improving DR grad-
ing is evaluated by the ablation study in Sec. 4.2.2.

4.2.2 Effect of Lesion Attentive Disease Grading

To evaluate the effectiveness of lesion segmentation for DR
grading and the improvement for semi-supervised learning

by the attention model, we compare three baselines with our
ﬁnal proposed model. Ori: We ﬁrst study if the lesion seg-
mentation model can enhance the DR grading accuracy. In
this baseline, we do not use the lesion attentive features but
directly train the grading model on the pre-processed fun-
dus images. Lesion (Pretrained): A baseline model pre-
trained only on the limited pixel-level lesion annotated data
is tested as well. The initially generated multi-lesion masks
are weighted on image feature maps to train the grading
model without the lesion attention model. Lesion (Semi):
We also explore the improvement of semi-supervised learn-
ing by the lesion-attention model, using large-scale image-
level grading annotated data. In this baseline, we only adopt
the cross-entropy loss for learning the lesion segmentation
model. Lesion (Semi + Adv): The adversarial training ar-
chitecture is integrated into the lesion segmentation objec-

2084

tive function as our ﬁnal method.

4.2.3 Effect of Semi-Supervised Lesion Segmentation

Table 1. Evaluation of the effectiveness of the lesion attentive dis-
ease grading on the IDRID and EyePACS dataset.

Datasets
Methods

Ori

Lesion(Pretrained)

Lesion(Semi)

Lesion(Semi+Adv)

IDRID

EyePACS

Acc.
0.8458
0.8725
0.9016
0.9134

Kappa
0.7926
0.8306
0.8892
0.9047

Acc.
0.8541
0.8598
0.8792
0.8912

Kappa
0.8351
0.8445
0.8617
0.8720

Table 1 shows the classiﬁcation accuracy and kappa
score of different methods. On the IDRID dataset, com-
pared with the basic classiﬁcation model that doesn’t use
the lesion mask information, the initial segmentation model
pre-trained on the pixel-level annotated data can increase
the accuracy of grading by 2.67% and the kappa score by
3.8%. With the semi-supervised learning using the image-
level annotated data, an even more signiﬁcant improvement
can be achieved. In particular, the huge gain in the kappa
score of 5.86% proves the proposed lesion attention model
can effectively reﬁne the lesion maps and thus improve
the grading results. Moreover, the adversarial training ar-
chitecture can also beneﬁt the ﬁnal result with a further
gain of 1.18% for classiﬁcation accuracy and 1.55% for
kappa score. Since the EyePACS dataset only has image-
level annotations, we adopt the fully-supervised model pre-
trained on the IDRID dataset. A similar comparison can be
made for the performance results obtained on the EyePACS
dataset and those as those produced on the IDRID dataset,
where each component of our model has a positive contri-
bution to the grading task, compared to the other methods.

Table 2. Performance comparisons of two binary classiﬁcation
tasks on the Messidor dataset.

Settings
Methods

Ori

Lesion(Pretrained)

Lesion(Semi)

Lesion(Semi+Adv)

Referral

Normal

AUC
0.934
0.953
0.971
0.976

Acc.
0.902
0.909
0.930
0.939

AUC
0.889
0.919
0.937
0.943

Acc.
0.878
0.901
0.918
0.922

To further evaluate our model, we also conduct exper-
iments on the Messidor dataset. Following the evaluation
method and protocol in [46], the AUC of ROC and the ac-
curacy for normal and referral classiﬁcation are compared
in Table 2. For both experimental settings, the proposed
method with the lesion attentive model, semi-supervised
segmentation and adversarial training architecture achieves
the highest performance. Since the image quality of the
Messidor dataset is close to that of IDRID, even the pre-
trained lesion based model can obtain a substantial gain
compared with the basic holistic classiﬁcation model.

In addition to the improvement of disease grading per-
formance, we also investigate the effectiveness of semi-
supervised segmentation based on the lesion pseudo masks
by the lesion attention model. We evaluate the segmen-
tation performance on the IDRID dataset with the pixel-
level ground-truths. Four different lesions, including mi-
croaneurysms, haemorrhages, hard exudates and soft exu-
dates, which are the main signs of DR, are assessed by the
ROC, PR curves and the corresponding AUC values. We
explore each proposed component of the ﬁnal model with
three baselines: the pre-trained segmentation model using
the normal convolution tuple, the Xception-module based
model and the semi-supervised learning component with-
out an adversarial training architecture.

The ROC and PR curves are illustrated in Fig. 5 and
detailed AUC values are listed in Table 3. As shown in
the upper part of the table, the Xception-module based le-
sion segmentation model consistently outperforms the nor-
mal convolution-based version, over four different lesions.
The AUC of the ROC and PR curves increases on average
by 1.02% and 1.92%, respectively, proving that separable
spatial and channel-wise convolution can indeed beneﬁt the
segmentation results. With the lesion attention model de-
sign, which exploits more image-level annotated data to
generate pseudo masks for semi-supervised segmentation,
a clear improvement is observed, with an average gain of
2.16% for the AUC of the PR curve. Besides, the adver-
sarial training architecture for semi-supervised learning can
slightly further increase the segmentation precision.

The bottom part of Table 3 shows the overall top three
places with AUC scores for the PR curves of different le-
sions in the challenge [1], as well as the performance of the
two semi-supervised segmentation methods AdvSeg [22]
and ASDNet [30], transferred from other vision tasks. Al-
though our method shows a slightly lower (0.57%) perfor-
mance than the current top model for microaneurysms de-
tection, moderate improvements are obtained for the other
three lesions. A particularly large improvement of 4.12% is
achieved for the soft exudate lesion. Moreover, our model
outperforms the AdvSeg and ASDNet by an average in-
crease of 6.89% and 5.22% on AUC of PR, respectively.

4.3. Comparisons with State of the art Models

To make our method more convincing, we compare it
with state-of-the-art DR grading models. The combined
kernels with multiple losses network (CKML) [44] and
VGGNet with extra kernels (VNXK) [44] aims to adopt
multiple ﬁlter sizes to learn ﬁne-grained discriminant fea-
tures. Zoom-in-Net [46] was proposed with a gated atten-
tion model and combines three sub-networks to classify the
holistic image, high-resolution crops and gated regions. The
attention fusion network (AFN) [25] has a similar idea of

2085

Microaneurysms

Haemorrhages

Hard Exudates

Soft Exudates

Figure 5. ROC and PR curves for segmentation over four lesions of DR. Four methods are compared to explore the effectiveness of the
Xception-module based architecture, the lesion attentive model for semi-supervised segmentation and the adversarial training loss.

Table 3. Performance comparisons of multi-lesion segmentation on the IDRID dataset. CE1 and CE2 denotes the segmentation model
adopting the normal convolution and the Xception module, respectively.
Haemorrhages

Microaneurysms

Hard Exudates

Soft Exudates

Lesion
Methods

CE1(Conv)

CE2(Xception)

CE2+Semi

CE2+Semi+Adv

VRT

PATech

iFLYTEK-MIG

AdvSeg [22]
ASDNet [30]

AUC ROC

0.9503
0.9653
0.9776
0.9828

-
-
-

0.9612
0.9692

AUC PR
0.4625
0.4733
0.4886
0.4960

0.4951 (2)
0.474 (3)
0.5017 (1)

0.4706
0.4782

AUC ROC

0.9438
0.9540
0.9699
0.9779

-
-
-

0.9256
0.9324

AUC PR
0.6456
0.6579
0.6812
0.6936

0.6804 (1)
0.649 (2)
0.5588 (3)

0.5923
0.6285

AUC ROC

0.9615
0.9675
0.9886
0.9935

-
-
-

0.9456
0.9502

AUC PR
0.8263
0.8455
0.8757
0.8872

0.7127 (11)

0.885 (1)
0.8741 (2)

0.8032
0.8095

AUC ROC

0.9443
0.9537
0.9713
0.9936

-
-
-

0.9318
0.9489

AUC PR
0.6817
0.7161
0.7337
0.7407

0.6995 (1)

-

0.6588 (3)

0.6756
0.6924

Table 4. Performance comparisons of DR grading on the EyePACS
and Messidor datasets.

EyePACS

Test set

Methods

Min-Pooling

o O
RG

Zoom-in [46]

AFN [25]

Ours

Kappa
0.849
0.845
0.839
0.854
0.859
0.872

Settings
Methods

VNXK [44]
CKML [44]
Expert [37]
Zoom-in [46]

AFN [25]

Ours

Messidor
Referral

AUC
0.887
0.891
0.94
0.957
0.968
0.976

Acc.
0.893
0.897

-

0.911

-

0.939

Normal

AUC
0.870
0.862
0.922
0.921
0.935
0.943

Acc.
0.871
0.858

-

0.905

-

0.922

proposes to collaborate the semantic lesion mask guidance
and the class-driven attention guidance to enhance the ﬁnal
model which obtains 1.3% gain over AFN. Moreover, for
both the referable/non-referable and normal/abnormal set-
tings of Messidor, our method can obtain the highest AUC
scores of ROC and also grading accuracy, compared to other
approaches. It is worth mentioning that our method outper-
forms the human experts by 3.6% and 2.1% on the AUC of
referral and normal settings, respectively.

unifying lesion detection and DR grading. However, the
attention model used is only class-driven and cannot learn
precise semantic lesion maps. Moreover, human experts
[37] are also invited to grade on the Messidor dataset.

Table 4 compares the results of different methods. On the
EyePACS dataset, Kappa values of the top three places from
the Kaggle competition [2] are shown where the top-1 place
can achieve 84.9%. The Zoom-in-Net and AFN slightly
improve the performance by introducing attention mecha-
nisms for learning class-driven lesion maps. Our method

5. Conclusion

In this paper, we proposed a collaborative learning
method of semi-supervised lesion segmentation and disease
grading for medical imaging. Lesion masks were used to at-
tend the classiﬁcation model and improve the grading accu-
racy, while a lesion attentive model exploiting class-speciﬁc
labels also beneﬁted the segmentation results. Extensive ex-
periments showed that our method achieves improvements
on the DR problem.

2086

References

[1] Idrid diabetic retinopathy segmentation challenge. https:

//idrid.grand-challenge.org/. 7

[2] Kaggle

diabetic

tition.
diabetic-retinopathy-detection. 5, 8

retinopathy
compe-
https://www.kaggle.com/c/

detection

[3] International clinical diabetic retinopathy disease severity

scale. American Academy of Ophthalmology, 2012. 1

[4] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down atten-
tion for image captioning and visual question answering. In
CVPR, June 2018. 2

[5] B. Antal, A. Hajdu, et al. An ensemble-based system for
microaneurysm detection and diabetic retinopathy grading.
IEEE transactions on biomedical engineering, 59(6):1720,
2012. 2

[6] A. M. Boers, R. S. Barros, I. G. Jansen, C. H. Slump, D. W.
Dippel, A. van der Lugt, W. H. van Zwam, Y. B. Roos, R. J.
van Oostenbrugge, C. B. Majoie, et al. Quantitative collateral
grading on ct angiography in patients with acute ischemic
stroke. In MICCAI, pages 176–184. Springer, 2017. 1

[7] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and
T.-S. Chua. Sca-cnn: Spatial and channel-wise attention in
convolutional networks for image captioning. In CVPR, July
2017. 2

[8] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff,
P. Wang, and H. Adam. Masklab: Instance segmentation
by reﬁning object detection with semantic and direction fea-
tures. In CVPR, June 2018. 1

[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. TPAMI, 40(4):834–848, 2018. 1

[10] X. Chen, J. Hao Liew, W. Xiong, C.-K. Chui, and S.-H. Ong.
Focus, segment and erase: An efﬁcient network for multi-
label brain tumor segmentation. In ECCV, September 2018.
1

[11] F. Chollet. Xception: Deep learning with depthwise separa-
ble convolutions. arXiv preprint, pages 1610–02357, 2017.
3

[12] A. V. Dalca, J. Guttag, and M. R. Sabuncu. Anatomical pri-
ors in convolutional networks for unsupervised biomedical
segmentation. In CVPR, June 2018. 1

[13] T. de Moor, A. Rodriguez-Ruiz, R. Mann, and J. Teuwen.
Automated soft tissue lesion detection and segmentation in
digital mammography using a u-net deep learning network.
arXiv preprint arXiv:1802.06865, 2018. 2

[14] E. Decenci`ere, X. Zhang, G. Cazuguel, B. Lay, B. Coch-
ener, C. Trone, P. Gain, R. Ordonez, P. Massin, A. Erginay,
et al. Feedback on a publicly distributed image database: the
messidor database. Image Analysis & Stereology, 33(3):231–
234, 2014. 5

[15] D. Doshi, A. Shenoy, D. Sidhpura, and P. Gharpure. Di-
abetic retinopathy detection using deep convolutional neu-
ral networks. In Computing, Analytics and Security Trends
(CAST), International Conference on, pages 261–266. IEEE,
2016. 2

[16] R. Fan, Q. Hou, M.-M. Cheng, G. Yu, R. R. Martin, and S.-
M. Hu. Associating inter-image salient instances for weakly
supervised semantic segmentation.
In ECCV, September
2018. 1

[17] P. Filipczuk, M. Kowal, and A. Marciniak. Feature selection
for breast cancer malignancy classiﬁcation problem. Journal
of Medical Informatics & Technologies, 15:193–199, 2010.
2

[18] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in NIPS, pages 2672–
2680, 2014. 4

[19] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu,
A. Narayanaswamy, S. Venugopalan, K. Widner, T. Madams,
J. Cuadros, et al. Development and validation of a deep
learning algorithm for detection of diabetic retinopathy in
retinal fundus photographs.
Jama, 316(22):2402–2410,
2016. 1, 2

[20] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

In ICCV, pages 2980–2988. IEEE, 2017. 1

[21] S. Hong, H. Noh, and B. Han. Decoupled deep neural net-
work for semi-supervised semantic segmentation. In NIPS,
pages 1495–1503, 2015. 2

[22] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H.
Yang. Adversarial learning for semi-supervised semantic
segmentation. arXiv preprint arXiv:1802.07934, 2018. 2,
7, 8

[23] Q. Li, A. Arnab, and P. H. Torr. Weakly- and semi-supervised

panoptic segmentation. In ECCV, September 2018. 1, 2

[24] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal

loss for dense object detection. TPAMI, 2018. 5

[25] Z. Lin, R. Guo, Y. Wang, B. Wu, T. Chen, W. Wang,
D. Z. Chen, and J. Wu. A framework for identifying dia-
betic retinopathy based on anti-noise detection and attention-
based fusion. In MICCAI, pages 74–82. Springer, 2018. 7,
8

[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015. 1

[27] X. Long, C. Gan, G. de Melo, J. Wu, X. Liu, and S. Wen.
Attention clusters: Purely attention based local feature inte-
gration for video classiﬁcation. In CVPR, June 2018. 2

[28] E. Miranda, M. Aryuni, and E. Irwansyah. A survey of medi-
cal image classiﬁcation techniques. In Information Manage-
ment and Technology (ICIMTech), International Conference
on, pages 56–61. IEEE, 2016. 1

[29] T. Nair, D. Precup, D. L. Arnold, and T. Arbel. Exploring
uncertainty measures in deep networks for multiple sclerosis
lesion detection and segmentation. In MICCAI, pages 655–
663. Springer, 2018. 1

[30] D. Nie, Y. Gao, L. Wang, and D. Shen. Asdnet: Attention
based semi-supervised deep networks for medical image seg-
mentation. In MICCAI, pages 370–378. Springer, 2018. 7,
8

[31] G. Papandreou, L.-C. Chen, K. P. Murphy, and A. L. Yuille.
Weakly- and semi-supervised learning of a deep convolu-
tional network for semantic image segmentation. In ICCV,
December 2015. 2

2087

and semi-supervised semantic segmentation. In CVPR, June
2018. 2

[48] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
X. He. Attngan: Fine-grained text to image generation with
attentional generative adversarial networks. In CVPR, June
2018. 2

[49] K. Yan, X. Wang, L. Lu, and R. M. Summers. Deeplesion:
automated mining of large-scale lesion annotations and uni-
versal lesion detection with deep learning. Journal of Medi-
cal Imaging, 5(3):036501, 2018. 2

[50] Y. Yang, T. Li, W. Li, H. Wu, W. Fan, and W. Zhang. Lesion
detection and grading of diabetic retinopathy via two-stages
deep convolutional neural networks. In MICCAI, pages 533–
540. Springer, 2017. 1, 2

[51] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. Learn-
ing a discriminative feature network for semantic segmenta-
tion. In CVPR, June 2018. 1

[52] Y. Zhou, L. Liu, and L. Shao. Vehicle re-identiﬁcation by
deep hidden multi-view inference. IEEE Transactions on Im-
age Processing, 27(7):3275–3287, 2018. 2

[53] Y. Zhou and L. Shao. Viewpoint-aware attentive multi-view
inference for vehicle re-identiﬁcation. In CVPR, June 2018.
2

[54] C. Zhu, X. Tan, F. Zhou, X. Liu, K. Yue, E. Ding, and
Y. Ma. Fine-grained video categorization with redundancy
reduction attention. In ECCV, September 2018. 2

[55] Y. Zou, Z. Yu, B. Vijaya Kumar, and J. Wang. Unsu-
pervised domain adaptation for semantic segmentation via
class-balanced self-training. In ECCV, September 2018. 1

[32] P. Porwal, S. Pachade, R. Kamble, M. Kokare, G. Desh-
mukh, V. Sahasrabuddhe, and F. Meriaudeau.
Indian dia-
betic retinopathy image dataset (idrid): A database for dia-
betic retinopathy screening research. Data, 3(3):25, 2018.
5

[33] H. Pratt, F. Coenen, D. M. Broadbent, S. P. Harding,
and Y. Zheng. Convolutional neural networks for diabetic
retinopathy. Procedia Computer Science, 90:200–205, 2016.
2

[34] S. Qiao, W. Shen, Z. Zhang, B. Wang, and A. Yuille. Deep
co-training for semi-supervised image recognition. In ECCV,
September 2018. 1

[35] T. Robert, N. Thome, and M. Cord. Hybridnet: Classiﬁcation
and reconstruction cooperation for semi-supervised learning.
In ECCV, September 2018. 1

[36] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, pages 234–241. Springer, 2015. 2, 3

[37] C. I. S´anchez, M. Niemeijer, A. V. Dumitrescu, M. S.
Suttorp-Schulten, M. D. Abramoff, and B. van Ginneken.
Evaluation of a computer-aided diagnosis system for diabetic
retinopathy screening on public data. Investigative ophthal-
mology & visual science, 52(7):4866–4871, 2011. 8

[38] N. Saraﬁanos, X. Xu, and I. A. Kakadiaris. Deep imbalanced
attribute classiﬁcation using visual attention aggregation. In
ECCV, September 2018. 2

[39] F. Sener and A. Yao. Unsupervised learning and segmenta-
tion of complex activities from video. In CVPR, June 2018.
1

[40] L. Seoud, J. Chelbi, and F. Cheriet. Automatic grading
In MICCAI.

of diabetic retinopathy on a public database.
Springer, 2015. 1

[41] L. Seoud, T. Hurtut, J. Chelbi, F. Cheriet, and J. P. Langlois.
Red lesion detection using dynamic shape features for dia-
betic retinopathy screening. IEEE transactions on medical
imaging, 35(4):1116–1126, 2016. 2

[42] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, pages 2818–2826, 2016. 3

[43] M. J. van Grinsven, B. van Ginneken, C. B. Hoyng, T. Thee-
len, and C. I. S´anchez. Fast convolutional neural network
training using selective data sampling: application to hemor-
rhage detection in color fundus images. IEEE transactions
on medical imaging, 35(5):1273–1284, 2016. 5

[44] H. H. Vo and A. Verma. New deep neural nets for ﬁne-
grained diabetic retinopathy recognition on hybrid color
space. In Multimedia (ISM), 2016 IEEE International Sym-
posium on, pages 209–215. IEEE, 2016. 7, 8

[45] X. Wang, S. You, X. Li, and H. Ma. Weakly-supervised se-
mantic segmentation by iteratively mining common object
features. In CVPR, June 2018. 1

[46] Z. Wang, Y. Yin, J. Shi, W. Fang, H. Li, and X. Wang. Zoom-
in-net: Deep mining lesions for diabetic retinopathy detec-
tion. In MICCAI, pages 267–275. Springer, 2017. 2, 5, 7,
8

[47] Y. Wei, H. Xiao, H. Shi, Z. Jie, J. Feng, and T. S. Huang. Re-
visiting dilated convolution: A simple approach for weakly-

2088

