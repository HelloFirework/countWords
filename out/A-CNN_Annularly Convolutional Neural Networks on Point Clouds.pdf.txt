A-CNN: Annularly Convolutional Neural Networks on Point Clouds

Artem Komarichev

Zichun Zhong

Jing Hua

Department of Computer Science, Wayne State University

{artem.komarichev,zichunzhong,jinghua}@wayne.edu

Abstract

Input

Analyzing the geometric and semantic properties of 3D
point clouds through the deep networks is still challenging
due to the irregularity and sparsity of samplings of their ge-
ometric structures. This paper presents a new method to
deﬁne and compute convolution directly on 3D point clouds
by the proposed annular convolution. This new convolu-
tion operator can better capture the local neighborhood
geometry of each point by specifying the (regular and di-
lated) ring-shaped structures and directions in the compu-
tation. It can adapt to the geometric variability and scal-
ability at the signal processing level. We apply it to the
developed hierarchical neural networks for object classi-
ﬁcation, part segmentation, and semantic segmentation in
large-scale scenes. The extensive experiments and com-
parisons demonstrate that our approach outperforms the
state-of-the-art methods on a variety of standard bench-
mark datasets (e.g., ModelNet10, ModelNet40, ShapeNet-
part, S3DIS, and ScanNet).

1. Introduction

Nowadays, the ability to understand and analyze 3D
data is becoming increasingly important in computer vision
and computer graphics communities. During the past few
years, the researchers have applied deep learning methods
to analyze 3D objects inspired by the successes of these
techniques in 2D images and 1D texts. Traditional low-
level handcrafted shape descriptors suffer from not being
able to learn the discriminative and sufﬁcient features from
3D shapes [1]. Recently, deep learning techniques have
been applied to extract hierarchical and effective informa-
tion from 3D shape features captured by low-level descrip-
tors [20, 6]. 3D deep learning methods are widely used in
shape classiﬁcation, segmentation, and recognition, etc. But
all these methods are still constrained by the representation
power of the shape descriptors.

One of the main challenges to directly apply deep learn-
ing methods to 3D data is that 3D objects can be represented
in different formats, i.e., regular / structured representation

Output

Airplane?

A-CNN

 
n
o

i
t

i

a
c
ﬁ
s
s
a
C

l

n
o

i
t

 
t
r
a
P

a

t

n
e
m
g
e
S

 
c
i
t

n
a
m
e
S

n
o

i
t

t

a
n
e
m
g
e
S

Figure 1: The proposed annularly convolutional neural networks
(A-CNN) model on point clouds to perform classiﬁcation, part
segmentation, and semantic segmentation tasks.

(e.g., multi-view images and volumes), and irregular / un-
structured representation (e.g., point clouds and meshes).
There are extensive approaches based on regular / structured
representation, such as multi-view convolutional neural net-
works (CNNs) [32, 26, 10] and 3D volumetric / grid CNN
methods and its variants [38, 26, 28, 35, 36, 16, 9]. These
methods can be conveniently developed and implemented
in 3D data structure, but they easily suffer from the heavy
computation and large memory expense. So it is better to
deﬁne the deep learning computations based on 3D shapes
directly, i.e., irregular / unstructured representation, such as
point cloud based methods [25, 27, 13, 30, 3, 18, 19, 33, 17,
42, 34, 8, 40]. However, deﬁning the convolution on the ir-
regular / unstructured representation of 3D objects is not an
easy task. Very few methods on point clouds have deﬁned
an effective and efﬁcient convolution on each point. Mean-
while, several approaches have been proposed to develop
convolutional networks on 2D manifolds [21, 4, 23, 39].
Their representations (e.g., 3D surface meshes) have point
positions as well as connectivities, which makes it relatively
easier to deﬁne the convolution operator on them.

In this work, we present a new method to deﬁne and
compute convolutions directly on 3D point clouds effec-
tively and efﬁciently by the proposed annular convolu-
tions. This new convolution operator can better capture lo-
cal neighborhood geometry of each point by specifying the
(regular and dilated) ring-shaped structures and directions
in the computation. It can adapt to the geometric variabil-

17421

ity and scalability at the signal processing level. Then, we
apply it along with the developed hierarchical neural net-
works to object classiﬁcation, part segmentation, and se-
mantic segmentation in large-scale scene as shown in Fig. 1.
The key contributions of our work are as follows:

• We propose a new approach to deﬁne convolutions on
point cloud. The proposed annular convolutions can
deﬁne arbitrary kernel sizes on each local ring-shaped
region, and help to capture better geometric represen-
tations of 3D shapes;

• We propose a new multi-level hierarchical method
based on dilated rings, which leads to better captur-
ing and abstracting shape geometric details. The new
dilated strategy on point clouds beneﬁts our proposed
closed-loop convolutions and poolings;

• Our proposed network models present new state-of-
the-art performance on object classiﬁcation, part seg-
mentation, and semantic segmentation of large-scale
scenes using a variety of standard benchmark datasets.

2. Related Work

Due to the scope of our work, we focus only on recently
related deep learning methods, which are proposed on dif-
ferent 3D shape representations.

Volumetric Methods. One traditional way to analyze a
3D shape is to convert it into the regular volumetric occu-
pancy grid and then apply 3D CNNs [38, 26]. The major
limitation of these approaches is that 3D convolutions are
more expensive in computations than 2D cases.
In order
to make the computation affordable, the volume grid size
is usually in a low resolution. However, lower resolution
means loosing some shape geometric information, espe-
cially in analyzing large-scale 3D shapes / scenes. To over-
come these problems, octree-based methods [28, 35, 36]
have been proposed to allow applying 3D CNNs on higher
/ adaptive resolution grids. PointGrid [16] is a 3D CNN
that incorporates a constant number of points within each
grid cell and allows it to learn better local geometric de-
tails. Similarly, Hua et al. [9] presented a 3D convolution
operator based on a uniform grid kernel for semantic seg-
mentation and object recognition on point clouds.

Point Cloud based Methods. PointNet [25] is the ﬁrst
attempt of applying deep learning directly on point clouds.
PointNet model is invariant to the order of points, but it
considers each point independently without including lo-
cal region information. PointNet++ [27] is a hierarchical
extension of PointNet model and learns local structures of
point clouds at different scales. But [27] still considers ev-
ery point in its local region independently. In our work, we
address the aforementioned issues by deﬁning the convolu-
tion operator that learns the relationship between neighbor-
ing points in a local region, which helps to better capture
the local geometric properties of the 3D object.

Klokov et al. [13] proposed a new deep learning archi-
tecture called Kd-networks, which uses kd-tree structure
to construct a computational graph on point clouds. KC-
Net [30] improves PointNet model by considering the lo-
cal neighborhood information.
It deﬁnes a set of learn-
able point-set kernels for local neighboring points and
presents a pooling method based on a nearest-neighbor
graph. PCNN [3] is another method to apply convolu-
tional neural networks to point clouds by deﬁning extension
and restriction operators, and mapping point cloud func-
tions to volumetric functions. SO-Net [17] is a permutation
invariant network that utilizes spatial distribution of point
clouds by building a self-organizing map. There are also
some spectral convolution methods on point clouds, such
as SyncSpecCNN [42] and spectral graph convolution [34].
Point2Sequence [19] learns the correlation of different areas
in a local region by using attention mechanism, but it does
not propose a convolution on point clouds. PointCNN [18]
is a different method that proposes to transform neighboring
points to the canonical order and then apply convolution.

Recently, there are several approaches proposed to pro-
cess and analyze large-scale point clouds from indoor and
outdoor environments. Engelmann et al. [8] extended Point-
Net model to exploit the large-scale spatial context. Ye et
al. [40] proposed a pointwise pyramid pooling to aggregate
features at local neighborhoods as well as two-directional
hierarchical recurrent neural networks (RNNs) to learn spa-
tial contexts. However, these methods do not deﬁne convo-
lutions on large-scale point clouds to learn geometric fea-
tures in the local neighborhoods. TangentConv [33] is an-
other method that deﬁnes the convolution on point clouds by
projecting the neighboring points on tangent planes and ap-
plying 2D convolutions on them. The orientation of the tan-
gent image is estimated according to the local point / shape
curvature, but as we know the curvature computation on the
local region of the point clouds is not stable and not robust
(see the discussion in Sec. 3.4), which makes it orientation-
dependent. Instead, our method proposes an annular convo-
lution, which is invariant to the orientations of local patches.
Also, ours does not require additional input features while
theirs needs such features (e.g., depth, height, etc.).

Mesh based Methods. Besides point cloud based meth-
ods, several approaches have been proposed to develop
convolutional networks on 3D meshes for shape analy-
sis. Geodesic CNN [21] is an extension of the Euclidean
CNNs to non-Euclidean domains and is based on a lo-
cal geodesic system of polar coordinates to extract local
patches. Anisotropic CNN [4] is another generalization of
Euclidean CNNs to non-Euclidean domains, where classi-
cal convolutions are replaced by projections over a set of
oriented anisotropic diffusion kernels. Mixture Model Net-
works (MoNet) [23] generalizes deep learning methods to
non-Euclidean domains (graphs and manifolds) by com-

27422

bining previous methods, e.g., classical Euclidean CNN,
Geodesic CNN, and Anisotropic CNN. MoNet proposes a
new type of kernel in parametric construction. Direction-
ally Convolutional Networks (DCN) [39] applies convolu-
tion operation on the triangular mesh of 3D shapes to ad-
dress part segmentation problem by combining local and
global features. Lastly, Surface Networks [14] propose up-
grades to Graph Neural Networks to leverage extrinsic dif-
ferential geometry properties of 3D surfaces for increasing
their modeling power.

3. Method

In this work, we propose a new end-to-end frame-
work named as annularly convolutional neural networks (A-
CNN) that leverages the neighborhood information to bet-
ter capture local geometric features of 3D point clouds. In
this section, we introduce main technique components of
the A-CNN model on point clouds that include: regular and
dilated rings, constraint-based k-nearest neighbors (k-NN)
search, ordering neighbors, annular convolution, and pool-
ing on rings.

3.1. Regular and Dilated Rings on Point Clouds

To extract local spatial context of the 3D shape, Point-
Net++ [27] proposes multi-scale architecture. The major
limitation of this approach is that multiple scaled regions
may have overlaps (i.e., same neighboring points could be
duplicately included in different scaled regions), which re-
duces the performance of the computational architecture.
Overlapped points at different scales lead to redundant in-
formation at the local region, which limits a network to
learn more discriminative features.

In order to address the above issue, our proposed frame-
work is aimed to leverage a neighborhood at different scales
more wisely. We propose two ring-based schemes, i.e., reg-
ular rings and dilated rings. Comparing to multi-scale strat-
egy, the ring-based structure does not have overlaps (no du-
plicated neighboring points) at the query point’s neighbor-
hood, so that each ring contains its own unique points, as
illustrated in Sec. 1 of Supplementary Material.

The difference between regular rings and dilated rings is
that dilated rings have empty space between rings. The idea
of proposed dilated rings is inspired by dilated convolutions
on image processing [43], which beneﬁts from aggregat-
ing multi-scale contextual information. Although each ring
may deﬁne the same number of computation / operation pa-
rameters (e.g., number of neighboring points), the coverage
area of each ring is different (i.e., dilated rings will have
larger coverage than the regular rings) as depicted in Fig. 2.
Regular rings can be considered as dilated rings with the
dilation factor equal to 0.

The proposed regular rings and dilated rings will con-
tribute to neighboring point search, convolution, and pool-

Rinner

Router

Rinner

Router

(a) Regular Rings

(b) Dilated Rings

Figure 2: The comparison of the regular and dilated ring-shaped
structures (such as with two rings). We can see that comparing two
sectors (e.g., black solid points) in the regular and dilated rings,
the dilated rings cover larger space by using the same number of
neighbors as in regular rings. Moreover, each ring contains unique
neighboring points comparing to the other ring.

ing in the follow-up processes. First, for k-NN algorithm,
we constrain search areas in the local ring-shaped neigh-
borhood to ensure no overlap. Second, the convolutions de-
ﬁned on rings cover larger areas with the same kernel sizes
without increasing the number of convolution parameters.
Third, the regular / dilated ring architectures will help to
aggregate more discriminative features after applying max-
pooling at each ring of the local region. We will discuss
them in more detail in the following subsections.

To justify the aforementioned statements, we will com-
pare multi-scale approach with our proposed multi-ring
scheme on object classiﬁcation task in the ablation study
(Sec. 5.4). The results show that ring-based structure cap-
tures better local geometric features than previous multi-
scale method, since it achieves higher accuracy.

3.2. Constraint based K NN Search

In the original PointNet++ model, the ball query algo-
rithm returns the ﬁrst K neighbors found inside a search ball
speciﬁed by a radius R and query point qi, so that it can-
not guarantee that the closest points will always be found.
However, our proposed k-NN search algorithm guarantees
returning closest points inside the searching area by using
the Euclidean metric. Each ring is deﬁned by two parame-
ters: the inner radius Rinner and the outer radius Router (in
Fig. 2); therefore, the constraint-based k-NN search ensures
that the closest and unique points will be found in each ring.

3.3. Ordering Neighbors

In order to learn relationships between neighboring
points in a local regions, we need ﬁrst to order points in a
clockwise / counterclockwise manner and then apply annu-
lar convolutions. Our proposed ordering operator consists
of two main steps: projection and ordering. The importance
of the projection before ordering is that the dot product has
its restriction in ordering points. By projecting points on

37423

a tangent plane at a query point qi, we effectively order
neighbors in clockwise / counterclockwise direction by tak-
ing use of cross product and dot product together. The de-
tailed explanations of normal estimation, orthogonal pro-
jection, and ordering are given in the following subsections.

3.3.1 Normal Estimation on Point Clouds

Normal is an important geometric property of a 3D shape.
We use it as a tool for projecting and ordering neighboring
points at a local domain. The simplest normal estimation
method approximates the normal ni at the given point qi by
calculating the normal of the local tangent plane Ti at that
point, which becomes a least-square plane ﬁtting estimation
problem [29]. To calculate normal ni, one needs to compute
eigenvalues and eigenvectors of the covariance matrix C as:

C =

1
K

K

Xj=1

(xj − qi) · (xj − qi)T ,

(1)

C · vγ = λγ · vγ , γ ∈ {0, 1, 2},

where K is the number of neighboring points xj s around
query point qi (e.g., K = 10 in our experiments), λγ and
vγ are the γth eigenvalue and eigenvector of the covari-
ance matrix C, respectively. The covariance matrix C is
symmetric and positive semi-deﬁnite. The eigenvectors vγ
form an orthogonal frame, in respect to the local tangent
plane Ti. The eigenvector v0 that corresponds to the small-
est eigenvalue λ0 is the estimated normal ni.

3.3.2 Orthogonal Projection

After extracting neighbors xj, j ∈ {1, ..., K} for a query
point qi, we calculate projections pj s of these points on a
tangent plane Ti described by a unit normal ni (estimated
in Sec. 3.3.1) as:

pj = xj − ((xj − qi) · ni) · ni,

j ∈ {1, ..., K}.

(2)

Fig. 3 (a) illustrates the orthogonal projection of neighbor-
ing points on a ring.

3.3.3 Counterclockwise Ordering

Firstly, we use the geometric deﬁnition of the dot product to
compute the angle between two vectors c (i.e., starts from
the query point qi and connects with a randomly starting
point, such as p1) and pj − qi (i.e., starts from the query
point qi and connects with other neighboring points pj ):

cos(θpj ) =

c · (pj − qi)
||c||||pj − qi||

.

(3)

We know that cos(θpj ) lies in [−1, 1], which corre-
sponds to angles between [0◦, 180◦].
In order to sort
the neighboring points around the query point between

x2

ni 

p2 

qi 

xk

pk 

x1

p1 

xj

pj

n
o

i
t
c
e
o
r
P

j

 
)
a
(

x3

p3 

1x3 conv 
with stride 1 

2
x

1
x

k
x

points

.
.
.

.
.
.

i

 

g
n
i
r
e
d
r
O
e
s
w
k
c
o
c
r
e
n
u
o
C

t

l

 
)
b
(

p1 

c 

order

t
a
c
n
o
c

p2 

p3 

pk 

qi 

pj

1
+
x

j

j

x

1
-
j

x

.
.
.

3
x

2
x

1
x

.
.
.

(c) Annular Convolution

Figure 3: The illustration of the proposed annular convolution on
a ring.
(a) Projection: qi is a query point. After applying the
constraint-based k-NN search, neighboring points X = {xj|j =
1, ..., K} are extracted on a ring. Given the normal ni at query
point qi, we project the searched points on the tangent plane Ti.
(b) Counterclockwise Ordering: After projection, we randomly
pick a starting point as our reference direction c and order points
in counterclockwise. It is worth mentioning that we order origi-
nal points [x1, x2, ..., xj , ..., xK ] based on their projections. (c)
Annular Convolution: Depending on the kernel size, we copy sev-
eral original points from the beginning position and concatenate
them to the end of the ordered points. Finally, we apply annular
convolution with the given kernel.

[0◦, 360◦), we must to decide which semicircle the consid-
ered point pj belongs to as follows:

signpj = (c × (pj − qi)) · ni,

(4)

where signpj ≥ 0 is θpj ∈ [0◦, 180◦], and signpj < 0 is
θpj ∈ (180◦, 360◦).

Then, we can recompute the cosine value of the angle as:

∠

pj = (cid:26) −cos(θpj ) − 2

cos(θpj )

signpj < 0
signpj ≥ 0.

(5)

Now the values of the angles lie in (−3, 1], which maps
angles between [0◦, 360◦).

Finally, we sort neighboring points xj by descending the
value of ∠
pj to obtain the counterclockwise order. Fig. 3
(b) illustrates the process of ordering in a local neighbor-
hood. The neighboring points can be ordered in the clock-
wise manner, if we sort neighboring points xj by ascending
the value of ∠

pj .

Our experiments show in Sec. 5.4 that ordering points
in the local regions is an important step in our framework
and our model achieves better classiﬁcation accuracy with
ordered points than without ordering them.

3.4. Annular Convolution on Rings

Through the previous computation, we have the ordered
In or-

neighbors represented as an array [x1, x2, ..., xK].

47424

der to develop the annular convolution, we need to loop
the array of neighbors with respect to the size of the kernel
(e.g., 1 × 3, 1 × 5, ...) on each ring. For example, if the
convolutional kernel size is 1 × 3, we need to take the ﬁrst
two neighbors and concatenate them with the ending ele-
ments in the original array to construct a new circular array
[x1, x2, ..., xK , x1, x2]. Then, we can perform the standard
convolutions on this array as shown in Fig. 3 (c).

There are some nice properties of the proposed annu-
lar convolutions as follows: (1) The annular convolution is
invariant to the orientation of the local patch. That is be-
cause the neighbors are organized and ordered in a closed
loop in each ring by concatenating the beginning with the
end of the neighboring points’ sequence. Therefore, we
can order neighbors based on any random starting position,
which does not negatively affect the convolution results.
Compared with some previous convolutions deﬁned on 3D
shapes [4, 39, 33], they all need to compute the real princi-
pal curvature direction as the reference direction to deﬁne
the local patch operator, which is not robust and cumber-
some. In particular, 3D shapes have large areas of ﬂat and
spherical regions, where the curvature directions are arbi-
trary. (2) As we know, in reality, the normal direction ﬂip-
ping issues are widely existing in point clouds, especially
the large-scale scene datasets. Under the annular convolu-
tion strategy, no matter the neighboring points are ordered
in clockwise or counterclockwise manner, the results are the
same. (3) Another advantage of annular convolution is that
we can deﬁne an arbitrary kernel size, instead of just 1 × 1
kernels [25, 27]. Therefore, the annular convolution can
provide the ability to learn the relationship between ordered
points inside each ring as shown in Fig. 3 (c).

Annular convolutions can be applied on both regular and
dilated rings. By applying annular convolutions with the
same kernel size on different rings, we can cover and con-
volve larger areas by using the dilated structure, which helps
us to learn larger spatial contextual information in the local
regions. The importance of annular convolutions is shown
in the ablation study in Sec. 5.4.

3.5. Pooling on Rings

After applying a set of annular convolutions sequentially,
the resulting convolved features encode information about
its closest neighbors in each ring as well as spatial remote-
ness from a query point. Then we aggregate the convolved
features across all neighbors on each ring separately. We
apply the max-pooling strategy in our framework. Our pro-
posed ring-based scheme allows us to aggregate more dis-
criminative features. The extracted max-pooled features
contain the encoded information about neighbors and the re-
lationship between them in the local region, unlike the pool-
ing scheme in PointNet++ [27], where each neighbor is con-
sidered independently from its neighbors. In our pooling
process, the non-overlapped regions (rings) will aggregate

different types of features in each ring, which can uniquely
describe each local region (ring) around the query point.
The multi-scale approach in PointNet++ does not guaran-
tee this and might aggregate the same features at different
scales, which is redundant information for a network. The
(regular and dilated) ring-based scheme helps to avoid ex-
tracting duplicate information but rather promotes extract-
ing multi-level information from different regions (rings).
This provides a network with more diverse features to learn
from. After aggregating features at different rings, we con-
catenate and feed them to another abstract layer to further
learn hierarchical features.

4. A-CNN Architecture

Our proposed A-CNN model follows a design where the
hierarchical structure is composed of a set of abstract layers.
Each abstract layer consists of several operations performed
sequentially and produces a subset of input points with
newly learned features. Firstly, we subsample points by
using Farthest Point Sampling (FPS) algorithm [22] to ex-
tract centroids randomly distributed on the surface of each
object. Secondly, our constraint-based k-NN extracts neigh-
bors of a centroid for each local region (i.e., regular / dilated
rings) and then we order neighbors in a counterclockwise
manner using projection. Finally, we apply sequentially a
set of annular convolutions on the ordered points and max-
pool features across neighbors to produce new feature vec-
tors, which uniquely describe each local region.

Given the point clouds of 3D shapes, our proposed end-
to-end network is able to classify and segment the objects.
In the following, we discuss the classiﬁcation and segmen-
tation network architectures on 3D point clouds.

4.1. Classiﬁcation Network

The classiﬁcation network is illustrated at the top of
Fig. 4.
It consists of two major parts: encoder and clas-
siﬁcation. The encoder extracts features from each ring in-
dependently inside every layer and concatenates them at the
end to process further to extract high-level features. The
proposed architecture includes both regular rings and di-
lated rings. We end up using two rings per layer, because
it gives us pretty good experimental results as shown in the
Sec. 5. It can be easily extended to more than two rings per
layer, if necessary.

We use regular rings in the ﬁrst layer and dilated rings in
the second layer in the encoder. Annular convolutions with
the kernel sizes 1 × 3 and stride 1 are applied in the ﬁrst two
layers, followed by a batch normalization [12] (BN) and a
rectiﬁed linear unit [24] (ReLU). Different rings of the same
query point are processed in parallel. Then, the aggregated
features from each ring concatenate together to propagate to
the next layer. The last layer in the encoder performs con-
volutions with kernel sizes 1 × 1 followed by BN and ReLU
layers, where only spatial positions of the sampled points

57425

l

s
a
m
r
o
N

3

 
x
 
N

3

 
x
 
1
N

3

 
x
 
2
N

Encoder

s
t

i

n
o
P

3

 
x
 
N

3FPS 

 
x
 
1
N

Annular

Convolutions 

8
2
1

 
x
 
1
N

8FPS 

2
1

 
x
 
2
N

Annular

Convolutions 

 

6
5
2

 
x
 
2
N

mlp(256,512,1024)

pooling

mlp(512,256,c)

 
c

 

4
2
0
1

conv1x3(64,64,128)

conv1x3(128,128,256)

n
o

i
t

i

a
c
ﬁ
s
s
a
C

l

t

u
p

t

u
O

Constraint-
based k-NN

F
 
x
 
K
 
x
 

i

N

Projection & Ordering

ni 

x1

qi 

xk

x2

x3

conv1x3(F1,F2,...,Fn)

n
F
 
x
 
'

max-pooling

K
 
x
 

i

N

Interpolation

 

6
5
2
 
x
 
N

mlp(256,128,m)

m
 
x
 
N

n
o

i
t

a

t

n
e
m
g
e
S

t

u
p

t

u
O

n
F
 
x
 

i

N

F
 
x
 
'

K
 
x
 

i

N

F
 
x
 

i

N

Annular Convolutions

conv1x3(F1,F2,...,Fn)

Ni x 3
Normals

Figure 4: The architecture of A-CNN. Both classiﬁcation and segmentation networks share encoder part for the feature extraction. Normals
are used only to determine the order of neighboring points in the local regions (dashed arrows mean no backpropagation during training)
and not used as additional features, unless it is mentioned explicitly in the experiments. N , N1, N2 (where N > N1 > N2) are the
numbers of points as input, after the ﬁrst and second layer, respectively. K and K ′ are the unordered and ordered points inside the local
rings, respectively. c is the number of classiﬁcation classes. m is the number of segmentation classes. “FPS” stands for Farthest Point
Sampling algorithm. “mlp” stands for multi-layer perceptron. conv1×3(F1, F2, ..., Fn) stands for annular convolutions with the kernel
size 1 × 3 applied sequentially with corresponding feature map sizes Fi, i ∈ 1, ..., n.

are considered. After that aggregated high-level features
are fed to the set of fully-connected layers with integrated
dropout [31] and ReLU layers to calculate probability of
each class. The output size of the classiﬁcation network is
equal to the number of classes in the dataset.

4.2. Segmentation Network

The segmentation network shares encoder part with the
classiﬁcation network as shown in Figure 4. In order to pre-
dict the segmentation label per point, we need to upsample
the sampled points in the encoder back to the original point
cloud size. As pointed out by [44], the consecutive feature
propagation proposed by [27] is not the most efﬁcient ap-
proach. Inspired from [44], we propagate features from dif-
ferent levels from the encoder directly to the original point
cloud size, and concatenate them by allowing the network
to learn the most important features from different levels as
well as to learn the relationship between them.

The output of each level has different sizes due to the
hierarchical feature extractions, so we have to restore hier-
archical features from each level back to the original point
size by using an interpolation method [27]. The interpola-
tion method is based on the inverse squared Euclidean dis-
tance weighted average of the three nearest neighbors as:

f (l+1)(x) =

3

Xj=1

f (l)(xj)

wj(x)
j=1 wj(x)

P3

,

(6)

where wj(x) =

d(x,xj )2 is an inverse squared Euclidean

1

distance weight.

Then, we concatenate upsampled features from different
levels and pass them through 1 × 1 convolution to reduce
feature space and learn the relationship between features
from different levels. Finally, the segmentation class dis-
tribution for each point is calculated.

5. Experiments

We evaluate our A-CNN model on various tasks such
as point cloud classiﬁcation, part segmentation, and large-
scale scene segmentation. In the following subsections, we
demonstrate more details on each task. It is noted that for
the comparison experiments, best results in the tables are
shown in bold font.

All models in this paper are trained on a single NVIDIA
Titan Xp GPU with 12 GB GDDR5X. The training time of
our model is faster than that of PointNet++ model. More
details about the network conﬁgurations, training settings
and timings in our experiments can be found in Sec. 2 and
Tab. 2 of Supplementary Material. The source code of the
framework will be made available later.

5.1. Point Cloud Classiﬁcation

We evaluate our classiﬁcation model on two datasets:
ModelNet10 and ModelNet40 [38]. ModelNet is a large-
scale 3D CAD model dataset. ModelNet10 is a subset of
ModelNet dataset that consists of 10 different classes with
3991 training and 908 testing objects. ModelNet40 includes

67426

Table 1: Classiﬁcation results on ModelNet10 and ModelNet40
datasets. AAC is accuracy average class, OA is overall accuracy.

ModelNet10 ModelNet40
AAC OA AAC OA
different methods with additional input or more points
AO-CNN [36]
90.5
O-CNN [35]
90.6
91.9
PointNet++ [27]
93.4
SO-Net [17]
MVCNN-MultiRes [26]
93.8
95.5
VRN Ensemble [5]

90.8
91.4

95.5

95.7

97.1

-
-
-

-
-
-

-
-
-

-
-

-

-

point cloud based methods with 1024 points

PointNet [25]
Kd-Net (depth 15) [13]
Pointwise CNN [9]
KCNet [30]
PointGrid [16]
PCNN [3]
PointCNN [18]
Point2Sequence [19]
A-CNN (our)

-

-

93.5

94.0

-
-
-
-
-

95.1
95.3

-

94.4

-

94.9

-

95.3
95.5

86.2
88.5
81.4

-

88.9

-

88.1
90.4
90.3

89.2
91.8
86.1
91.0
92.0
92.3
92.2
92.6
92.6

40 different classes with 9843 objects for training and 2468
objects for testing. Point clouds with 10,000 points and
normals are sampled from meshes, normalized into a unit
sphere, and provided by [27].

For experiments on ModelNet10 and ModelNet40, we
sample 1024 points with normals, where normals are only
used to order points in the local region. For data augmenta-
tion, we randomly scale object sizes, shift object positions,
and perturb point locations. For better generalization, we
apply point shufﬂing in order to generate different centroids
for the same object at different epochs.

In Tab. 1, we compare our method with several state-of-
the-art methods in the shape classiﬁcation results on both
ModelNet10 and ModelNet40 datasets. Our model achieves
better accuracy among the point cloud based methods
(with 1024 points), such as PointNet [25], PointNet++ [27]
(5K points + normals), Kd-Net (depth 15) [13], Point-
wise CNN [9], KCNet [30], PointGrid [16], PCNN [3],
and PointCNN [18]. Our model is slightly better than
Point2Sequence [19] on ModelNet10 and shows compara-
ble performance on ModelNet40.

Meanwhile, our model performs better than other volu-
metric approaches, such as O-CNN [35] and AO-CNN [36];
while we are a little worse than SO-Net [17], which uses
denser input points, i.e., 5000 points with normals as the in-
put (1024 points in our A-CNN); MVCNN-MultiRes [26],
which uses multi-view 3D volumes to represent an object
(i.e., 20 views of 30 × 30 × 30 volume); the VRN Ensem-
ble [5], which involves an ensemble of six models.

We also provide some feature visualization results in
Sec. 3 of Supplementary Material, including global feature
(e.g., t-SNE clustering) visualization and local feature (e.g.,
the magnitude of the gradient per point) visualization.

t

 
+
+
e
N
n
o
P

t

i

 

r
u
O

 
h

t

 

u
r
T
d
n
u
o
r
G

(a) Rocket

(b) Table

(c) Skateboard

(d) Bag

Figure 5: Qualitative results on ShapeNet-part dataset. We com-
pare our results with PointNet++ [27] and ground truth.

5.2. Point Cloud Segmentation

We evaluate our segmentation model on ShapeNet-
part [41] dataset. The dataset contains 16,881 shapes from
16 different categories with 50 label parts in total. The main
challenge of this dataset is that all categories are highly im-
balanced. There are 2048 points sampled for each shape
from the dataset, where most shapes contain less than six
parts. We follow the same training and testing splits pro-
vided in [25, 41]. For data augmentation, we perturb point
locations with the point shufﬂing for better generalization.
We evaluate our segmentation model with two differ-
ent inputs. One of the models is trained without feed-
ing normals as additional features and the other model is
trained with normals as additional features. The quan-
titative results are provided in Tab. 2, where mean IoU
(Intersection-over-Union) is reported. The qualitative re-
sults are visualized in Fig. 5. Our approach with point loca-
tions only as input outperforms PointNet [25], Kd-Net [13],
KCNet [30], and PCNN [3]; and shows slightly worse per-
formance comparing to PointGrid [16] (volumetric method)
and PointCNN [18]. Meanwhile, our model achieves the
best performance with the input of point locations and nor-
mals, compared with PointNet++ [27], SyncSpecCNN [42],
SO-Net [17], SGPN [37], O-CNN [35], RSNet [11], and
Point2Sequence [19]. The more detailed quantitative results
(e.g., per-category IoUs) and more visualization results are
provided in Sec. 5 of Supplementary Material.

5.3. Semantic Segmentation in Scenes

We also evaluate our segmentation model on two large-
scale indoor datasets Stanford 3D Large-Scale Indoor
Spaces (S3DIS) [2] and ScanNet [7]. S3DIS contains 6
large-scale indoor areas with 271 rooms sampled from 3
different buildings, where each point has the semantic label
that belongs to one of the 13 categories. ScanNet includes
1513 scanned indoor point clouds, where each voxel has
been labeled with one of the 21 categories.

We employ the same training and testing strategies as

77427

Ceiling

Floor Wall Beam Column Window Door

Table

Chair

Sofa

Bookcase

Board

Clutter

(a) Input

(b) PointNet [25]

(c) Our

(d) Ground Truth

Figure 6: Qualitative results on S3DIS dataset. We compare our results with PointNet [25] and ground truth. The auditorium is a challenging
room type and appears only in Area 2. Our model produces much better segmentation result, compared with the result of PointNet.

PointNet [25] on S3DIS, where we use 6-fold cross valida-
tion over all six areas. The evaluation results are reported
in Tab. 2, and qualitative results are visualized in Fig. 6.
Our model demonstrates better segmentation results com-
pared with PointNet [25], MS+CU (2) [8], G+RCU [8], 3P-
RNN [40], SPGraph [15], and TangentConv [33]. However,
our model performs slightly worse than PointCNN [18]
due to their non-overlapping block sampling strategy with
paddings which we do not use. Meanwhile, our approach
shows the best segmentation results on ScanNet [7] and
achieves the state-of-the-art performance, compared with
PointNet [25], PointNet++ [27], TangentConv [33], and
PointCNN [18] according to Tab. 2.

More qualitative visualization results and data prepara-
tion details on both datasets are provided in Sec. 4 and Sec.
5, respectively, of Supplementary Material and Video.

annular convolution (Sec. 3.4) on the classiﬁcation task of
ModelNet40 dataset as shown in Tab. 3. In the ﬁrst exper-
iment, we replace our proposed constraint-based k-NN on
ring regions with ball query in [27], but keep ordering and
annular convolutions on.
In the second and third experi-
ments, we turn off either annular convolutions or ordering,
respectively; and keep the rest two components on. Our ex-
periments show that the proposed ring-shaped scheme con-
tributes the most to our model.
It is because multi-level
rings positively affect annular convolutions. Finally, A-
CNN model with all three components (i.e., rings without
overlaps, ordering, and annular convolutions) achieves the
best results. We also discover that reducing overlap / redun-
dancy in multi-scale scheme can improve existing methods.
We evaluate the original PointNet++ [27] with and without
overlap as shown in Sec. 1 of Supplementary Material.

Table 2: Segmentation results on ShapeNet-part, S3DIS, and Scan-
Net. “mean” is mean IoU (%), OA is overall accuracy.

Table 3: Ablation experiments on ModelNet40 dataset. AAC is
accuracy average class, OA is overall accuracy.

ShapeNet-part

S3DIS

ScanNet

without normals

with normals

PointNet [25]
PointNet++ [27]
SyncSpecCNN [42]
O-CNN [35]
Kd-Net [13]
KCNet [30]
SO-Net [17]
SGPN [37]
MS+CU (2) [8]
G+RCU [8]
RSNet [11]
3P-RNN [40]
SPGraph [15]
TangentConv [33]
PCNN [3]
Point2Sequence [19]
PointGrid [16]
PointCNN [18]
A-CNN (our)

mean
83.7

-
-
-

82.3
84.7

-
-
-
-
-
-
-
-

85.1

-

86.4
86.1
85.9

mean

-

85.1
84.7
85.9

-
-

84.9
85.8

-
-

84.9

-
-
-
-

85.2

-
-

86.1

OA

78.5

-
-
-
-
-
-
-

79.2
81.1

-

86.9
85.5

*
-
-
-

88.1
87.3

OA

73.9
84.5

-
-
-
-
-
-
-
-
-
-
-

80.9

-
-
-

85.1
85.4

Note: * TangentConv [33] OA on S3DIS Area 5 is 82.5% (as reported

in their paper), which is worse compared with our OA of 85.5%.

5.4. Ablation Study

The goal of our ablation study is to show the importance
of the proposed technique components (in Sec. 3) in our A-
CNN model. We evaluate three proposed components, such
as rings without overlaps (Sec. 3.1), ordering (Sec. 3.3), and

A-CNN (without rings / with overlap)
A-CNN (without annular conv.)
A-CNN (without ordering)
A-CNN (with all components)

AAC OA
91.7
89.2
91.8
89.2
92.0
89.6
90.3
92.6

6. Conclusion

In this work, we propose a new A-CNN framework on
point clouds, which can better capture local geometric in-
formation of 3D shapes. Through extensive experiments on
several benchmark datasets, our method has achieved the
state-of-the-art performance on point cloud classiﬁcation,
part segmentation, and large-scale semantic segmentation
tasks. Since our work does not solely focus on large-scale
scene datasets, we will explore some new deep learning ar-
chitectures to improve the current results. We will also in-
vestigate to apply the proposed framework on large-scale
outdoor datasets in our future work.

Acknowledgment. We would like to thank the review-
ers for their valuable comments. This work was partially
supported by the NSF IIS-1816511, CNS-1647200, OAC-
1657364, OAC-1845962, Wayne State University Sub-
award 4207299A of CNS-1821962, NIH 1R56AG060822-
01A1, and ZJNSF LZ16F020002.

87428

References

[1] E. Ahmed, A. Saint, A. Shabayek, K. Cherenkova, R. Das,
G. Gusev, D. Aouada, and B. Ottersten. Deep learning ad-
vances on different 3D data representations: A survey. arXiv
preprint arXiv:1808.01462, 2018.

[2] I. Armeni, O. Sener, A. Zamir, H. Jiang, I. Brilakis, M. Fis-
cher, and S. Savarese. 3D semantic parsing of large-scale
indoor spaces.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1534–
1543, 2016.

[3] M. Atzmon, H. Maron, and Y. Lipman. Point convolutional
neural networks by extension operators. ACM Transactions
on Graphics, 37(4):71:1–71:12, 2018.

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learn-
ing shape correspondence with anisotropic convolutional
neural networks.
In Advances in Neural Information Pro-
cessing Systems, pages 3189–3197, 2016.

[5] A. Brock, T. Lim, J. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv preprint arXiv:1608.04236, 2016.

[6] S. Bu, P. Han, Z. Liu, J. Han, and H. Lin. Local deep feature
learning framework for 3D shape. Computers & Graphics,
46:117–129, 2015.

[7] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. ScanNet: Richly-annotated 3D reconstruc-
tions of indoor scenes.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5828–5839, 2017.

[8] F. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe.
Exploring spatial context for 3D semantic segmentation of
point clouds.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 716–724,
2017.

[9] B.-S. Hua, M.-K. Tran, and S.-K. Yeung. Pointwise convo-
lutional neural networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
984–993, 2018.

[10] H. Huang, E. Kalogerakis, S. Chaudhuri, D. Ceylan, V. G.
Kim, and E. Yumer.
Learning local shape descriptors
from part correspondences with multiview convolutional net-
works. ACM Transactions on Graphics, 37(1):6, 2018.

[11] Q. Huang, W. Wang, and U. Neumann. Recurrent slice net-
works for 3D segmentation of point clouds.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2626–2635, 2018.

[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.

[13] R. Klokov and V. Lempitsky. Escape from cells: Deep Kd-
networks for the recognition of 3D point cloud models. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 863–872, 2017.

[14] I. Kostrikov, Z. Jiang, D. Panozzo, D. Zorin, and J. Bruna.
Surface networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2540–
2548, 2018.

[15] L. Landrieu and M. Simonovsky. Large-scale point cloud
semantic segmentation with superpoint graphs. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4558–4567, 2018.

[16] T. Le and Y. Duan. PointGrid: A deep network for 3D
shape understanding. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 9204–
9214, 2018.

[17] J. Li, B. M. Chen, and G. H. Lee. SO-Net: Self-organizing
network for point cloud analysis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 9397–9406, 2018.

[18] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. PointCNN:
Convolution on X-transformed points. In Advances in Neural
Information Processing Systems, pages 828–838, 2018.

[19] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker. Point2Sequence:
Learning the shape representation of 3D point clouds with an
attention-based sequence to sequence network. In Associa-
tion for the Advancement of Artiﬁcial Intelligence, 2019.

[20] Z. Liu, S. Chen, S. Bu, and K. Li. High-level semantic fea-
ture for 3D shape based on deep belief networks.
In Pro-
ceedings of the IEEE International Conference on Multime-
dia and Expo, pages 1–6, 2014.

[21] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.
Geodesic convolutional neural networks on Riemannian
manifolds. In Proceedings of the IEEE International Con-
ference on Computer Vision Workshops, pages 37–45, 2015.
[22] C. Moenning and N. A. Dodgson. Fast marching farthest
point sampling. Technical report, University of Cambridge,
Computer Laboratory, 2003.

[23] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and
M. Bronstein. Geometric deep learning on graphs and man-
ifolds using mixture model CNNs.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5115–1543, 2017.

[24] V. Nair and G. Hinton. Rectiﬁed linear units improve re-
In Proceedings of the Inter-
stricted boltzmann machines.
national Conference on Machine Learning, pages 807–814,
2010.

[25] C. Qi, H. Su, K. Mo, and L. Guibas. PointNet: Deep learning
on point sets for 3D classiﬁcation and segmentation. Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 652–660, 2017.

[26] C. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.
Volumetric and multi-view CNNs for object classiﬁcation on
3D data. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 5648–5656,
2016.

[27] C. Qi, L. Yi, H. Su, and L. Guibas. PointNet++: Deep hier-
archical feature learning on point sets in a metric space. In
Advances in Neural Information Processing Systems, pages
5105–5114, 2017.

[28] G. Riegler, A. Ulusoy, and A. Geiger. OctNet: Learning deep
3D representations at high resolutions. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3577–3586, 2017.

[29] R. Rusu. Semantic 3D Object Maps for Everyday Manipula-
tion in Human Living Environments. PhD thesis, Computer

97429

[44] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-
Net: Point cloud upsampling network. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2790–2799, 2018.

Science department, Technische Universitaet Muenchen,
Germany, October 2009.

[30] Y. Shen, C. Feng, Y. Yang, and D. Tian. Mining point cloud
local structures by kernel correlation and graph pooling. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4548–4557, 2018.

[31] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014.

[32] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-
view convolutional neural networks for 3D shape recogni-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 945–953, 2015.

[33] M. Tatarchenko, J. Park, V. Koltun, and Q.-Y. Zhou. Tan-
gent convolutions for dense prediction in 3D.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3887–3896, 2018.

[34] C. Wang, B. Samari, and K. Siddiqi. Local spectral graph
convolution for point set feature learning. In Proceedings of
The European Conference on Computer Vision, September
2018.

[35] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong. O-
CNN: Octree-based convolutional neural networks for 3D
shape analysis. ACM Transactions on Graphics, 36(4):72,
2017.

[36] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong. Adaptive O-
CNN: A patch-based deep representation of 3D shapes. ACM
Transactions on Graphics, 37(6), 2018.

[37] W. Wang, R. Yu, Q. Huang, and U. Neumann. SGPN:
Similarity group proposal network for 3D point cloud in-
stance segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2569–
2578, 2018.

[38] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1912–1920, 2015.

[39] H. Xu, M. Dong, and Z. Zhong. Directionally convolutional
networks for 3D shape segmentation. In Proceedings of the
IEEE International Conference on Computer Vision, pages
2698–2707, 2017.

[40] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang. 3D recurrent
neural networks with context fusion for point cloud semantic
segmentation. In Proceedings of The European Conference
on Computer Vision, September 2018.

[41] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu,
Q. Huang, A. Sheffer, L. Guibas, et al. A scalable active
framework for region annotation in 3D shape collections.
ACM Transactions on Graphics, 35(6):210, 2016.

[42] L. Yi, H. Su, X. Guo, and L. Guibas. SyncSpecCNN: Syn-
chronized spectral CNN for 3D shape segmentation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6584–6592, 2017.

[43] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In International Conference on Learning
Representations, 2016.

107430

