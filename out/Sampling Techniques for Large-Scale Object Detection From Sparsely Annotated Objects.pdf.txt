Sampling Techniques for Large-Scale Object Detection

from Sparsely Annotated Objects

Yusuke Niitani Takuya Akiba

Tommi Kerola Toru Ogawa
Preferred Networks, Inc.

Shotaro Sano

Shuji Suzuki

{niitani,akiba,tommi,ogawa,sano,ssuzuki}@preferred.jp

Abstract

missing annotations.

Efﬁcient and reliable methods for training of object de-
tectors are in higher demand than ever, and more and more
data relevant to the ﬁeld is becoming available. How-
ever, large datasets like Open Images Dataset v4 (OID) are
sparsely annotated, and some measure must be taken in or-
der to ensure the training of a reliable detector. In order to
take the incompleteness of these datasets into account, one
possibility is to use pretrained models to detect the presence
of the unveriﬁed objects. However, the performance of such
a strategy depends largely on the power of the pretrained
model. In this study, we propose part-aware sampling, a
method that uses human intuition for the hierarchical rela-
tion between objects. In terse terms, our method works by
making assumptions like “a bounding box for a car should
contain a bounding box for a tire”. We demonstrate the
power of our method on OID and compare the performance
against a method based on a pretrained model. Our method
also won the ﬁrst and second place on the public and pri-
vate test sets of the Google AI Open Images Competition
2018.

1. Introduction

With recent advances in automation technologies that are
dependent on the method of extracting information from the
images, the task of object detection has been becoming in-
creasingly important in the ﬁeld of artiﬁcial intelligence.

Also growing with the interest for the methods of ob-
ject detection is the size of the dataset that is available for
training purpose. Recently published Open Images Dataset
v4 (OID) features up to 500 categories and 1.6M images
with 14M objects to be detected [11]. It is a dataset on an
unprecedented scale in terms of the number of annotated
images, and each image in the dataset on average contains
∼ 7 categories that were veriﬁed by humans (veriﬁed cate-
gories) . As a human-annotated dataset, however, the com-
pleteness of OID is inevitably somewhat questionable. As
claimed in their work, the annotation recall is 43%, which
means that more than half the objects in the images are

The major problem with this kind of a dataset is that a
network would suffer from incorrect training signals due to
objects missing annotations. One naive but sure way to deal
with such case is to simply exclude regions surrounding ob-
jects missing annotations during the evaluation of the objec-
tive function. This task, however, is easier said than done,
because the validity of this approach depends on our ability
to detect the unveriﬁed object that is actually present. One
option is to train a pretrained model and use it as an oracle
to tell the presence of unveriﬁed object [26]. Nevertheless,
the performance of such an approach is limited by the power
of the pretrained model.

A more intuitive approach is to utilize the intuition that
we are born with. If ”car” is present in the image, ”tire”
should be present in the bounding box of ”car” with high
probability. If ”door” is present in the image, ”door han-
dle” should be present in the bounding box of ”door” as
well. Therefore, leaving some pathological cases side, if
”car” is veriﬁed but ”tire” is not, then it probably repre-
sents the dangerous case that we are concerned with—that
is, ”tire” is an unveriﬁed but present object in the image,
and we should not include the cost regarding the ”tire” in
the objective function. Better yet, if ”car” is present, then
we should simply not question the presence of an object of
a part category like ”tire” in the bounding box of ”car”.

This is in fact exactly what our part-aware sampling
method does. Given a veriﬁed object in the image and a set
of all sub-bounding boxes contained in the bounding box of
the object, we refrain from asking the detector to detect in
the sub-boxes the objects that are, based on our intuition,
part of the object captured by the parent bounding box.

In order to ﬁnd the better way to detect unannotated ob-
jects, we compared our approach against the method based
on the pretrained model. The method using a pretrained
model is optimized to work well for sparsely annotated
dataset by us. We call the method pseudo label-guided sam-
pling, and veriﬁed its effectiveness on OID and artiﬁcially
created sparsely annotated data based on MS COCO [14].

Our part-aware sampling leads to an average 0.7 AP im-

16510

a similar effect as OHEM. Contrary to these methods, our
proposed part-aware sampling fully ignores losses of prob-
able unannotated false negatives during training. Loss at-
tenuation methods can be applied jointly with our method,
making our approach orthogonal to these previous works.

Prediction results of a network are used to train an
object detector with limited annotation in many previous
works [20, 22, 2, 3, 23]. Yan et al. [26] uses pseudo la-
bels to tackle the problem where a subset of a dataset is
annotated with bounding boxes and the rest of the dataset
without annotations. Inoue et al. [10] works on learning an
object detector on a domain with no bounding box annota-
tion by using pseudo labels generated by a model trained
on another domain with shared categories. Wu et al. [25]
proposes score-based soft sampling, which uses pseudo la-
bels to complement sparse annotations. The method is stud-
ied only in PASCAL VOC, which is quite small in today’s
standard. Although they conclude the technique to be un-
reliable, their conclusion assumes that performance of an
object detector is weak, which may not be true in the case
when a large network is trained on a large dataset.

The work of Zhang et al. [27] targets the task of ﬁne-
grained object classiﬁcation.
In their method, they train
a weakly supervised network for detecting discriminative
local parts that can be used for ﬁne-grained classiﬁcation.
Their approach is similar to ours in that it is part-aware, al-
though it differs in the fact that while they learn a network
to detect parts as an auxiliary task during training, we lever-
age the relationship between parts in order to prevent false
negative due to missing ground truths.

Fang et al. [5] propose a framework for training an ob-
ject detector that utilizes an external knowledge graph, ex-
ploiting knowledge about what types of objects commonly
occur together. While their approach aims to learn knowl-
edge possibly not in the training set, such as that a cat often
sits on a table, our approach directly aims to tackle the issue
of missing ground truth annotations in an image, which de-
teriorates object detection performance by increasing false
negatives.

Figure 1: Example annotations in Open Images Dataset v4.
The positively and negatively veriﬁed labels are displayed
on the right of the images in green and red, respectively.
Many human part categories are absent in the veriﬁed la-
bels of the top image. Such missing annotations create false
training signals for a normal object detector.

provement across all categories and 9.2 AP improvement on
part categories. In particular, for human part categories like
human ear and human hand, we conﬁrmed an improvement
of 22.7 AP on average. Based on our part-aware sampling,
we achieved the 1st and 2nd places on the public and private
test sets of Google AI Open Images Competition 2018.

2. Related Works

3. Problems

An object detector is commonly trained with standard
cross-entropy loss for classifying the category of each
bounding box and robust loss for regressing the size of
the detection box [18, 15, 7, 17, 19]. Some recent work
has, however, begun to address modiﬁcations of the loss
function when training an object detector in order to im-
prove performance. Shrivastava et al. [21] proposed online
hard example mining (OHEM) that only backpropagates
losses of hard examples, thus making the network focus
on discriminating difﬁcult cases. Focal loss [13] is another
method that proposes to attenuate the loss the more conﬁ-
dent the network is about a prediction, which also leads to

Open Images Dataset v4 (OID) is a recently introduced
object detection dataset on an unprecedented scale in terms
of the numbers of annotated images and bounding boxes as
well as the number of categories supported. The dataset is
different from its predecessors not only in terms of its size
but also in terms of its requirement on an annotation by al-
lowing a subset of categories to be not annotated even if the
categories are present in an image. For each image, anno-
tation only covers a set of categories called veriﬁed cate-
gories, which is a subset of categories that annotators check
for existence in an image. In some images, veriﬁed cate-
gories do not span all categories present in an image, re-

26511

(a) Human annotations on ”human” and ”car”.

(b) RoI proposals that are used for training.

(c) Blue: Proposals that ignore parts of ”person”,
such as ”footwear” and ”human face”. Green:
Proposals that ignore parts of ”cars”, such as ”li-
cense plate” and ”tire”.

Figure 2: Description of part-aware sampling. In the left and the middle images, the ground truths and RoI proposals are
displayed. These are the inputs to the algorithm. On the right, we display a subset of the RoI proposals that are ignored for
classiﬁcation loss of certain part categories based on part and subject relationships.

sulting in a subset of present instances not annotated. Ver-
iﬁed categories consist of positively veriﬁed and negatively
veriﬁed categories. Positively veriﬁed categories exist in
an image, and negatively veriﬁed categories are checked by
human annotators not to exist in the image. Figure 1 demon-
strates two example images containing people with different
veriﬁed categories. Since a much larger set of human part
categories are veriﬁed in the bottom image, the annotations
of these categories are denser.

In OID, there are on average 7.4 categories in veriﬁed
categories for each image, which is much fewer than the 500
categories supported by the dataset. Moreover, although
the number of supported categories are more than six times
larger than COCO, OID contains on average almost the
same number of positively veriﬁed categories (i.e., 3.4 and
2.9 categories per image for OID and COCO, respectively)
implying that the annotations of OID are much more sparse
than COCO. In fact, the authors of OID reported that the
recall of positively veriﬁed categories is only 43% [11].

4. Methods

We explore two methods of determining objects that are
unannotated in a sparsely annotated dataset. First, we pro-
pose part-aware sampling, which ignores classiﬁcation loss
for part categories when an instance of them is inside an
instance of their subject categories. Second, we use pseudo
labels generated from a pretrained model to exclude regions
that are likely not to be annotated. Despite the idea of
pseudo labels being widely recognized [26, 25], there is not
much consensus on how to utilize it, especially for object
detection. We propose a pipeline to ﬁlter unreliable pseudo
labels using cues that are available from the structure of

the problem. We call the method pseudo label-guided sam-
pling.

4.1. Basic Architecture

We use a proposal-based object detector like Faster R-
CNN [18] in this work. The detection pipeline consists
of a region proposal network that produces a set of class-
agnostic region proposals around instances and the main
network that classiﬁes each proposal into categories and re-
ﬁnes it to better localize an instance. A proposal is clas-
siﬁed to the background category or one of the foreground
categories. During training, a category is assigned to each
proposal, and this assignment is used to calculate classiﬁ-
cation loss and localization loss like Faster R-CNN [18]. In
this work, the classiﬁcation loss is calculated as the sum of
sigmoid cross entropy loss for each proposal and each cate-
gory as:

Lcls = − X

X

lic log pic

i

c

lic ∈ {−1, 0, 1} ,

(1)

where lic = 1 and lic = −1 when the i-th proposal is as-
signed or not assigned to category c, respectively. Also, lic
can be set to 0, which means that the classiﬁcation loss for
category c is ignored for the i-th proposal. We explore later
in this section how to determine ignored categories for each
RoI proposal, which plays a critical role in diminishing in-
correct training signals created by missing annotations.

4.2. Part Aware Sampling

For certain pairs of categories, one is a part or a posses-
sion of the other in most of the images where they co-occur.

36512

We call the categories in such kind of a pair as part category
and subject category. For instance, human parts like faces
are usually parts of people, and tires are often parts of cars.
Also, accessories and clothes are in many cases possessed
by people in the images of the OID dataset. Furthermore,
for these pairs of categories, we ﬁnd that annotation is often
lacking for part categories.

In Table 1, we show statistics that supports our observa-
tion of part and subject relationships. First, we measure the
ratio of a bounding box of a part category to be included in a
subject category as shown in row included. We determine if
a box b1 is included in another box b2 when asymmetric in-
tersection over union (aiou(b1, b2) = area(b1∩b2)
) is higher
area(b1)
than a certain threshold τ . The ratio included is formally
computed as

#{bp ∈ Bp | ∃bs ∈ Bs, aiou(bp, bs) > τ }

#Bp

,

(2)

where Bp and Bs are the sets of bounding boxes of a part
category p and a category s, which is the subject category
of p. Note that we only consider bounding boxes in a set of
images Ip ∩ Is, where Ic is the set of images that contain
category c. Furthermore, in row co-occur of Table 1, we
show the ratio of images that contain annotations of both
part and subject categories, which is formulated as #I
.

∩I

p

s

#I

s

From row included, We can observe that a part cate-
gory is included in its subject category in more than 90%
of bounding boxes for 39 out of 47 pairs. Thus, the part and
subject relationships are reﬂected in spatial relationships of
objects in OID. From row co-occur, the percentages are too
small for many pairs of categories based on our common
sense, implying that annotation is severely missing for part
categories. For instance, the percentage of human eyes is
only 2.8%.

To reduce false training signals, we introduce part-aware
sampling that selects categories to ignore for classiﬁcation
loss from RoI proposals. The main idea behind this tech-
nique is that given the high likelihood that instances of part
categories are included in subject categories, it is safer to ig-
nore classiﬁcation loss for part categories for RoI proposals
included in a subject category. The technique is used only
when part categories are not included in veriﬁed categories.
Figure 2 illustrates the technique by visualizing a subset of
RoI proposals that are ignored for classiﬁcation loss of parts
of people and cars.

In our work, we use statistics collected as in Table 1 and
prior knowledge of category relationships to design a map-
ping P, which maps a label to its part categories. Algo-
rithm 1 summarizes this method.

Algorithm 1 Framework of Part-Aware Sampling

Input: RoI proposals {ri}N
j=1, ground truth labels {lj}M

i=1, ground truth boxes
{bj}M
j=1, veriﬁed labels V, and
a set P that maps a subject category to the list of its part
categories.
Initialize: Set I ← {{}i}N
1: for i = 1 to N do
2:

for j = 1 to M do

i=1

3:

4:

5:

6:

if aiou(ri, bj) > τ and lj in P then

for p in P[lj] do

if p not in V then
Append p to Ii

Output: Set of categories I, which are ignored when cal-
culating classiﬁcation loss for each RoI proposal.

4.3. Pseudo Label Guided Sampling

Alternatively to part-aware sampling, we also present
pseudo label-guided sampling to tackle the sparse annota-
tion problem. For this method, we train a network twice
and use the pseudo labels generated from the ﬁrst model to
guide the training of the second model.

We ﬁlter prediction results of a trained model to generate
pseudo labels to complement sparse annotation for unanno-
tated regions. We ﬁrst ignore all prediction results with the
categories included in veriﬁed labels. Second, we ignore
predicted boxes with high IoU with any of the actual ground
truths because these predicted boxes are likely the result of
misclassifying an annotated instance. Third, prediction re-
sults with scores below a score threshold are rejected. The
score threshold T is determined for each category based
on the precision on the withheld dataset at different score
thresholds. The minimum precision is speciﬁed as a hy-
perparameter that is used to determine the score threshold
T by setting it as the minimum threshold that achieves the
precision.

The algorithm is summarized in Algorithm 2. Figure 3
shows examples of pseudo labels generated by the algo-
rithm.

5. Experiments

We conduct experiments on sparse COCO and Open Im-
ages Dataset v4 (OID). Sparse COCO is a dataset created
from MS COCO [14] that contains sparse annotation. Since
the size of sparse COCO is much smaller than OID and has
access to complete annotations for all objects for analysis,
we use this dataset to study pseudo label-guided sampling
and the negative effect of missing annotations in general
before experimenting on OID. Since part and subject rela-
tionships are not common in sparse COCO, we were only
able to experiment part-aware sampling with OID.

46513

Table 1: Statistics of part and subject categories. See the text for deﬁnitions of included and co-occur.

Subject
Part

Included (%)
Co-occur (%)

Arm

91.3

4.87

Ear

Nose Mouth Hair Eye Beard Face Head

Foot

Leg

Hand

Glove

Hat

Dress Fedora

94.0

0.98

93.1

3.40

94.7

2.90

89.2 94.7

99.2

91.8

88.1

6.75 2.81

0.30

39.7

5.76

87.9

0.05

90.5

1.98

90.3

2.40

95.0

0.05

93.7

0.99

97.5

3.77

94.5

0.33

Person

Subject
Part

Footwe. Sandal

Boot

Sports. Coat Sock Glasse. Belt Helmet

Jeans High h. Scarf Swimwe. Earrin. Bicycl. Shorts

Person

Included (%)
Co-occur (%)

84.6

15.2

89.8

0.07

90.8

0.08

95.0

0.07

92.9 85.5

96.6

96.7

93.7

0.37 0.02

5.04

0.02

0.82

91.2

3.67

93.0

0.09

97.4

0.20

Subject
Part

Person

Baseba. Minisk. Cowboy. Goggles Jacket Shirt Sun ha. Suit Trouse. Brassi.

Tie

Licens. Wheel

92.1

0.36

Car

96.7

0.02

94.8

0.55

92.2

0.79

Door
Tire Handle

Included (%)
Co-occur (%)

96.1

0.17

98.6

0.01

93.3

0.21

95.6

0.79

94.8 97.9

91.8

96.7

90.8

1.51 0.59

0.48

5.38

0.54

98.2

0.11

98.5

0.91

91.3

2.52

80.6

39.97

79.2

14.5

95.3

1.65

Algorithm 2 Framework of Pseudo Label-Guided Sam-
pling

k=1, labels { ˆlk}L

Input: RoI proposals {ri}N
j=1, ground truth labels {lj}M

i=1, ground truth boxes
{bj}M
j=1, and veriﬁed labels V.
Also, output of a pre-trained model that includes bounding
boxes { ˆbk}L
k=1. Also,
T is a set of score thresholds for each category.
Initialize: Set I ← {{}i}N
1: K = {1, · · · , L}
2: for k = 1 to L do
3:

if ˆsk < T [ˆlk] or ˆlk in V then

k=1, and scores { ˆsk}L

i=1

4:

5:

6:

Remove k from K; Continue

for j = 1 to M do

if iou(bj, ˆbk) > 0.8 then

Remove k from K; Break

7:
8: for i = 1 to N do
for k in K do
9:

10:

11:

if iou(bi, ˆbk) > 0.5 then

Append ˆlk to Ii

Output: Set of categories I, which are ignored when cal-
culating classiﬁcation loss for each RoI proposal.

5.1. Implementation Details

We use Feature Pyramid Networks [12] for our exper-
iments. The feature extractor is ResNet50 [8] for experi-
ments using sparse COCO and SE-ResNeXt50 [9] for ex-
periments using OID. The larger network is selected for
Open Images Dataset because the capacity of the base ex-
tractor needs to be large enough to learn such a large dataset.
The initial bias for the ﬁnal classiﬁcation layer is set to
a large negative number to prevent the training from get-
ting unstable in the beginning. We set the initial weight
of the base extractor with the weights of an image clas-
siﬁcation network trained on the ImageNet classiﬁcation
task [4]. We use stochastic gradient descent with momen-

(a)

(b)

Figure 3: Examples of pseudo labels. Red bounding boxes
are the ground truths annotations and green bounding boxes
are pseudo labels. (a): ”Bottle” is annotated. ”Windows”
and ”cars” are included in the pseudo labels. (b): ”French
fry” and ”wine glass” are annotated. ”Wine”, ”cocktail” and
”plate” are included in the pseudo labels.

tum set to 0.9 for optimization. The base learning rate is
set to 0.00125 × batchsize. We use a warm-up learning rate
schedule to stabilize training in the beginning. For sparse
MS COCO, we trained for 90000 iterations with 16 images
in each batch. The learning rate is multiplied by 0.1 at the
60000-th and the 80000-th iterations. For OID, we trained
for 12 epochs. The learning rate is scheduled by a cosine
function η = η0
, where η and η0 are the
learning rate and the initial learning rate. We scale images
during training so that the length of the smaller edge is be-
tween [650, 1056]. Also, we randomly ﬂip images horizon-
tally to augment training data. We use Chainer [24, 1, 16]
as our deep learning framework.

cos (% of progress×π)+1

2

5.2. Sparse COCO

Sparse COCO is a dataset artiﬁcially created by ran-
domly deleting labels in images of MS COCO. For each
category in MS COCO, among the set of images contain-

56514

Table 2: Statistics of sparse COCO for different probabili-
ties α of deleting annotations.

number of boxes per image
number of distinct categories per image

2.90
7.19

5.03
2.03

3.60
1.45

2.17
0.87

0

0.3

0.5

0.7

ing the category, we delete all annotations of the category
for the images selected from the set by probability α. This
means that for each image in the artiﬁcially created dataset,
the instances of the labeled categories are annotated exhaus-
tively as in the original MS COCO dataset, but there could
be categories with no annotation even if instances of the cat-
egories exist. Table 2 shows the statistics of the dataset cre-
ated artiﬁcially with different probabilities (0.3, 0.5, 0.7).

We use the training set of COCO 2017 object detection
challenge to create sparse COCO for training networks and
tuning hyper parameters. The validation split is used with-
out deleting annotations for validation. We evaluate models
using mmAP used in the COCO competition.

We ﬁrst evaluate different methods with different level
of missing annotations. Among the methods we tried, we
ﬁxed every setting except the way different RoI proposals
are evaluated as positive, negative or ignored samples for
training the classiﬁcation network. Note that we do not
compare with part-aware sampling since COCO categories
do not include part categories. Here are the methods:

• Baseline: This method follows the standard training
procedure that assumes a dataset with an exhaustive
annotation. RoIs around instances that are not anno-
tated are evaluated falsely as negative samples for this
method.

• Oracle ignore: This method uses the ground-truth that
are deleted in order to evaluate how much performance
loss can be recovered by labeling RoIs using oracle
information. For any RoI proposals that have IoU with
the deleted ground-truth higher than 0.5, this method
ignores classiﬁcation loss calculated from them.

• Oracle positive: Similarly to ”oracle ignore” de-
scribed above, this method also uses the ground-truth
that is deleted.
Instead of ignoring RoIs overlap-
ping with the deleted ground-truth during training, this
method uses those RoIs as positive samples. The dif-
ference between this method and training on a fully
annotated dataset is that during the sampling of RoIs
that are actually used for training, this method does
not use the deleted ground-truth to oversample regions
around the ground-truth, thus making the comparison
with other methods fair.

• Pseudo label-guided sampling: The method is de-
scribed in Section 4.3. Score thresholds are selected

by training another model with the default training
scheme using 80% of the training data and using the
remaining 20% of the training data to calculate pre-
cisions at different score thresholds. We also experi-
mented assigning positive labels to RoIs that overlap
with highly conﬁdent pseudo labels.

• Overlap-based soft sampling [25]: This method mul-
tiplies weights on the loss computed using negative
samples. The weights are determined based on over-
laps with annotated bounding boxes. The method is
designed based on the assumption that regions close
to annotated ground truth can be conﬁdently assigned
to the background. The method is demonstrated to
work well with PASCAL VOC according to the au-
thors. We use the same values for all hyper parame-
ters of the nonlinear function that takes overlap as in-
put and weight multiplied on the loss as output. Since
the scale of the total loss changes from the rest of the
methods, for a fair comparison, we choose the optimal
learning rate by searching over a set of learning rates
that are × 1
2 , ×1, ×2, ×3, ×4 of the learning rate used
by the rest of the methods.

Table 3 summarizes the main result from sparse COCO.
The model
trained using full annotation obtains 36.75
mmAP on the validation set, and this can be considered as
the maximum score that any methods can obtain. We have
the following observations. First, the performance recov-
ers from the baseline by using oracle information to ignore
proposals. The amount of negative effect caused by miss-
ing annotation increases as the ratio of missing annotation
increases. The difference between Baseline and Oracle ig-
nore is 0.70 mmAP and 1.67 mmAP when α = 0.3 and
α = 0.7, respectively.

Second, by using pseudo-ground truths to decide in-
stances that are falsely annotated as negatives, the result
matches the method using oracle information to ignore sam-
ples. The pseudo labels cover regions included in oracle in-
formation and also regions that a network falsely detected.
The result suggests that training may work better when ig-
noring regions that are unannotated, but susceptible to mis-
takingly recognizing as the foreground.

Third, despite making extra efforts to tune parameters,
overlap-based soft sampling [25] performs worse than the
baseline on sparse COCO. Unlike the other methods, this
method discourages contribution of negative samples unan-
imously based on their distance from the closest ground
truth bounding box. Perhaps, this method discourages too
many negative samples from contribution to the loss for this
dataset.

Table 4 shows an ablative study of methods using pseudo
labels. We make the following observations. First, perfor-
mance improves by selecting score thresholds for each cat-

66515

Table 3: Comparison of different methods on sparse MS
COCO. A model trained on COCO with complete annota-
tion achieves 36.75 mmAP.

Table 5: Results on the validation set of Open Images
Dataset v4.

0.3

0.5

0.7

Baseline
Oracle Ignore
Pseudo Label-guided (Ours)
Overlap-based soft [25]
Oracle Positive

34.22 31.69 27.31
34.92 32.73 28.98
35.00 32.79 29.03
33.98 31.39 27.30
35.66 34.17 32.19

Table 4: Comparison of different score thresholds for
pseudo label-guided sampling. If pseudo labels are not used
to select positive RoI proposal samples, the second column
is left empty. Uniform (x) indicates that the constant thresh-
old x is used for all categories. Prec (> y) indicates that
thresholds are selected for each class differently based on
the minimum tolerable precision y.

ignore threshold positive threshold mmAP

uniform (0.3)
uniform (0.5)
prec(> 0.3)
prec(> 0.5)
prec(> 0.7)
prec(> 0.5)

34.76
34.71
34.94
35.00
34.93
34.59

prec(> 0.8)

egory based on category-wise precision compared to uni-
formly setting the values. Second, the performance is rel-
atively robust to different precision thresholds, but the pre-
cision threshold at 0.5 works the best. Third, performance
drops by using pseudo labels to assign positive labels to RoI
proposals. This is contrary to our expectation that a network
learns better by assigning proposals to positives instead of
ignoring them when pseudo-labels are created highly con-
ﬁdent prediction results. We think that the object detector
is not robust to false positives because the false positives
play a big role due to the number of positive samples being
small.

5.3. Open Images Dataset v4

Open Images Dataset v4 (OID) is a newly introduced
dataset that can be used for object detection. We use the
split of the data and the subset of the categories that were
used for the competition held in 2018 hosted by dataset
authors. 1 The training and the validation split contain
1, 643, 042 and 100, 000 images, respectively. There are
500 distinct categories annotated with bounding boxes in
OID. These categories have clearly deﬁned spatial extents
and considered as important concepts by the dataset authors.
Table 5 summarizes the main results using OID. The
baseline follows the standard training procedure and does

1https://storage.googleapis.com/openimages/web/

challenge.html

validation mAP

Baseline
Pseudo label-guided sampling
Part-aware sampling

64.49

64.84

65.18

not use any special technique to tackle missing annotation.
We use the precision threshold at 0.5 for pseudo label-
guided sampling. Both pseudo label-guided sampling and
part-aware sampling improve upon the baseline. Although
pseudo label-guided sampling performs competitively even
on results using oracle information for sparse COCO, part-
ware sampling achieves better results on OID. Since the ra-
tio of missing annotation is sometimes lower than 10% as
suggested from Table 1, we suggest that it is difﬁcult to train
a pretrained model for some categories in OID.

In Table 6, we take a closer look at evaluation results by
examining category-wise AP for part categories. The part-
aware sampling leads to on average 0.7 AP improvement
across all categories and 9.2 AP improvement on part cat-
egories.
In particular, for human part categories, such as
”human face” and ”human ear”, we see a signiﬁcant im-
provement of 22.7 AP on average.

Figure 4 shows the averages of APs at different score
thresholds for all categories and the subset of categories
that are used as part categories. The difference between the
baseline and part-aware sampling is already large for part
categories with low score threshold, but the gap widens as
the score threshold increases.

In Figure 5 shows a qualitative comparison of models
trained with and without part-aware sampling. For the mod-
els with part-aware sampling, part categories are detected
with a relatively high score threshold. For instance, in the
right image, tires and license plates are only detected by the
model trained with part-aware sampling.

Open Images Competition 2018: Based on the model
trained with part-aware sampling, we integrate context
head [28], longer training time, a stronger feature extrac-
tor [9], an additional number of anchors, and test-time aug-
mentation for our submission to the object detection track of
Google AI Open Images Competition 2018. For evaluation,
the test set is split into the public and private sets. During
the period of the competition, scores on the public set were
always available to the competitors, but scores on the pri-
vate set were not disclosed until the end of the competition.
Our best single model achieves 55.81 mAP and 53.43 mAP
on public and private sets. Our ensemble of models achieves
1st and 2nd best scores on the public and private sets with
62.88 mAP and 58.63 mAP. Table 7 summarizes the result
of ours and other top competitors.

76516

Table 6: Ablative study of part-aware sampling on categories that can be ignored by the technique. The scores are AP
calculated on the validation set of OID.

Arm

Ear

Nose Mouth Hair Eye Beard Face Head

Foot

Leg

Hand

Glove

Hat

Dress

Fedora

Baseline
40.9
Part-aware 55.2

17.5

62.6

34.7

69.6

21.4

63.8 27.3

55.5

82.7

55.1

50.7

41.6

32.3

55.2

74.7 64.0 76.8 91.4 78.9

59.5

54.4

53.6

63.4

60.8

64.9

70.6

69.0

73.9

67.0

70.3

Footwe. Sandal

Boot

Sports. Coat Sock Glasse. Belt Helmet

Jeans High h. Scarf Swimwe. Earrin. Bicycl. Shorts

Baseline
61.9
Part-aware 68.5

53.6

58.9

61.6

57.9

52.9

58.0 70.6 74.9 66.8 80.2

62.7

76.6

71.6

61.2

73.3 67.1 85.4 61.9 82.4

77.6

78.8

75.8

63.4

63.4

82.0

75.1

86.1

75.8

69.7

75.4

Baseba. Minisk. Cowboy. Goggles Jacket Shirt Sun ha. Suit Trouse. Brassi.

Tie

Licens. Wheel

Tire Handle Average

Baseline
Part-aware

67.2

62.2

62.5

58.7

65.0

73.3

79.3

69.5 70.9

61.3

83.7

62.5

82.6

84.7

72.1

86.7

74.3 81.6 66.4 87.0 69.8

74.5

91.5

74.6

48.3

66.4

49.4

41.1

69.6

46.2

61.1

70.3

(a)

(b)

Figure 4: mAP on OID at different score thresholds for the
baseline and part-aware sampling.

e
n

i
l

e
s
a
B

e
r
a
w
a
-
t
r
a
P

Figure 5: The visualization of outputs of models trained
without part-aware sampling (top-row) and with it (bottom-
row) on OID. The score thresholds are kept the same for all
images.

Table 7: Results on the test set of OID. Unlike the other
results, test-time augmentation is used.

public test

private test

Single best (Ours)
Ensemble (Ours)
Private LB 1st place
Private LB 3rd place [6]

55.81

62.88

61.71

62.16

53.43

58.63

58.66

58.62

6. Discussions and Future works

In this paper, we proposed part-aware sampling and
pseudo label-guided sampling to train object detectors on
datasets with sparse annotation. On Open Images Dataset
v4, our part-aware sampling signiﬁcantly improved results
over the baseline for part categories. The success of our
method suggests the importance of choosing a right mea-
sure to determine the presence of unveriﬁed objects.

Indeed, our study provides no guarantee that our method
is the best method for this purpose. Trivially, if one can
prepare a perfect pretrained model that can detect the pres-
ence of unveriﬁed objects with 100% accuracy, the method
based on such model will perform optimally. However, the
presence of such a model completely defeats the purpose
for training the model, and we need to seek methods that
work in a more realistic situation. To understand the prob-
lem better, we made an extensive empirical study of detect-
ing unannotated objects using a pretrained model that can
actually be obtained on large-scale datasets [14, 11]. For
sparse COCO, we found the method to work very well out-
performing preexisting methods [25] and matching methods
with access to actual ground truths. The method, however,
underperformed against part-ware sampling for OID. This
shows that a method based on a simple prior performs bet-
ter when it is difﬁcult to obtain a reliable detector. It is our
hope that our study will instigate further exploration for the
method of detecting the presence of unveriﬁed objects.

86517

[20] Z. Shi, P. Siva, and T. Xiang. Transfer learning by ranking

for weakly supervised object annotation. In BMVC, 2012.

[21] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 761–769, 2016.

[22] P. Siva and T. Xiang. Weakly supervised object detector

learning with model drift detection. In ICCV, 2011.

[23] K. Tang, A. Joulin, L.-J. Li, and L. Fei-Fei. Co-localization

in real-world images. In CVPR, 2014.

[24] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
In LearningSys workshop in NIPS, 2015.

[25] Z. Wu, N. Bodla, B. Singh, M. Najibi, R. Chellappa, and
Soft sampling for robust object detection.

L. S. Davis.
arXiv:1806.06986, 2018.

[26] Z. Yan, J. Liang, W. Pan, J. Li, and C. Zhang. Weakly-
and semi-supervised object detection with expectation-
maximization algorithm. arXiv:1702.08740, 2017.

[27] Y. Zhang, K. Jia, and Z. Wang. Part-aware ﬁne-grained
object categorization using weakly supervised part detection
network. arXiv preprint arXiv:1806.06198, 2018.

[28] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, H. Lu, et al. Cou-
plenet: Coupling global structure with local parts for object
detection. In ICCV, 2017.

References

[1] T. Akiba, K. Fukuda, and S. Suzuki. ChainerMN: Scal-
able Distributed Deep Learning Framework. In LearningSys
workshop in NIPS, 2017.

[2] H. Bilen and A. Vedaldi. Weakly supervised deep detection

networks. In CVPR, 2016.

[3] R. G. Cinbis, J. Verbeek, and C. Schmid. Weakly supervised
object localization with multi-fold multiple instance learn-
ing. IEEE Trans. on PAMI, 2016.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009.

[5] Y. Fang, K. Kuan, J. Lin, C. Tan, and V. Chandrasekhar.

Object detection meets knowledge graphs. In IJCAI, 2017.

[6] Y. Gao, X. Bu, Y. Hu, H. Shen, T. Bai, X. Li, and
S. Wen. Solution for large-scale hierarchical object detec-
tion datasets with incomplete annotation and data imbalance.
arXiv:1810.06208, 2018.

[7] R. Girshick. Fast r-cnn. In ICCV, 2015.
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[9] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. CVPR, 2018.

[10] N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa. Cross-
domain weakly-supervised object detection through progres-
sive domain adaptation. In CVPR, 2018.

[11] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin,
J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig,
and V. Ferrari. The open images dataset v4: Uniﬁed image
classiﬁcation, object detection, and visual relationship detec-
tion at scale. arXiv:1811.00982, 2018.

[12] T.-Y. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, 2017.

[13] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. IEEE transactions on pattern
analysis and machine intelligence, 2018.

[14] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick,
J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollr.
Microsoft coco: Common objects in context. ECCV, 2014.

[15] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. Ssd: Single shot multibox detector. In
ECCV, pages 21–37. Springer, 2016.

[16] Y. Niitani, T. Ogawa, S. Saito, and M. Saito. Chainercv: a
library for deep learning in computer vision. In ACM MM,
2017.

[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
In

only look once: Uniﬁed, real-time object detection.
CVPR, 2016.

[18] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015.

[19] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.

96518

