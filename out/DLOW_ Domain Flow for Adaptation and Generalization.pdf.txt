DLOW: Domain Flow for Adaptation and Generalization

Rui Gong1 Wen Li1 ∗ Yuhua Chen1 Luc Van Gool1,2

1Computer Vision Laboratory, ETH Zurich 2VISICS, ESAT/PSI, KU Leuven

gongr@student.ethz.ch {liwen, yuhua.chen, vangool}@vision.ee.ethz.ch

Abstract

In this work, we present a domain ﬂow genera-
tion(DLOW) model to bridge two different domains by gen-
erating a continuous sequence of intermediate domains
ﬂowing from one domain to the other. The beneﬁts of our
DLOW model are two-fold. First, it is able to transfer
source images into different styles in the intermediate do-
mains. The transferred images smoothly bridge the gap be-
tween source and target domains, thus easing the domain
adaptation task. Second, when multiple target domains are
provided for training, our DLOW model is also able to gen-
erate new styles of images that are unseen in the training
data. We implement our DLOW model based on CycleGAN.
A domainness variable is introduced to guide the model to
generate the desired intermediate domain images.
In the
inference phase, a ﬂow of various styles of images can be
obtained by varying the domainness variable. We demon-
strate the effectiveness of our model for both cross-domain
semantic segmentation and the style generalization tasks on
benchmark datasets. Our implementation is available at
https://github.com/ETHRuiGong/DLOW .

1. Introduction

The domain shift problem is drawing increasing atten-
tion in recent years [21, 64, 54, 52, 15, 8]. In particular,
there are two tasks that are of interest in computer vision
community. One is the domain adaptation problem, where
the goal is to learn a model for a given task from a label-
rich data domain (i.e., source domain) to perform well in
a label-scarce data domain (i.e., target domain). The other
one is the image translation problem, where the goal is to
transfer images in the source domain to mimic the image
style in the target domain.

Generally, most existing works focus on the target do-
main only. They aim to learn models that well ﬁt the target
data distribution, e.g., achieving good classiﬁcation accu-
racy in the target domain, or transferring source images into

∗The corresponding author.

Figure 1: Illustration of data ﬂow generation. Traditional
image translation methods directly map the image from
the source domain to the target domain, while our DLOW
model is able to produce a sequence of intermediate do-
mains shifting from the source domain to the target domain.

the target style. In this work, we instead are interested in the
intermediate domains between source and target domains.
We present a new domain ﬂow generation (DLOW) model,
which is able to translate images from the source domain
into an arbitrary intermediate domain between source and
target domains. As shown in Fig 1, by translating a source
image along the domain ﬂow from the source domain to the
target domain, we obtain a sequence of images that natu-
rally characterize the distribution shift from the source do-
main to the target domain.

The beneﬁts of our DLOW model are two-fold. First,
those intermediate domains are helpful to bridge the distri-
bution gap between two domains. By translating images
into intermediate domains, those translated images can be
used to ease the domain adaptation task. We show that the
traditional domain adaptation methods can be boosted to
achieve better performance in target domain with interme-
diate domain images. Moreover, the obtained models also
exhibit good generalization ability on new datasets that are
not seen in the training phase, beneﬁting from the diverse
intermediate domain images.

2477

Second, our DLOW model can be used for style general-
ization. Traditional image-to-image translation works [64,
28, 30, 38] mainly focus on learning a deterministic one-
to-one mapping that transfers a source image into the target
style. In contrast, our DLOW model allows to translate a
source image into an intermediate domain that is related to
multiple target domains. For example, when performing the
photo to painting transfer, instead of obtaining a Monet or
Van Gogh style, our DLOW model could produce a mixed
style of Van Gogh, Monet, etc. Such mixture can be cus-
tomized in the inference phase by simply adjusting an input
vector that encodes the relatedness to different domains.

We implement our DLOW model based on Cycle-
GAN [64], which is one of the state-of-the-art unpaired
image-to-image translation methods. We augment the Cy-
cleGAN to include an additional input of domainness vari-
able. On one hand, the domainness variable is injected into
the translation network using the conditional instance nor-
malization layer to affect the style of output images. On
the other hand, it is also used as weights on discriminators
to balance the relatedness of the output images to different
domains. For multiple target domains, the domainness vari-
able is extended as a vector containing the relatedness to
all target domains. Extensive results on benchmark datasets
demonstrate the effectiveness of our proposed model for do-
main adaptation and style generalization.

2. Related Work

Image to Image Translation: Our work is related to
the image-to-image translation works. The image-to-image
translation task aims at translating the image from one do-
main into another domain. Inspired by the success of Gener-
ative Adversarial Networks(GANs) [17], many works have
been proposed to address the image-to-image translation
based on GANs [28, 56, 64, 38, 39, 20, 65, 27, 1, 8, 33, 58,
37]. The early works [28, 56] assume that paired images
between two domains are available, while the recent works
such as CycleGAN [64], DiscoGAN [30] and UNIT [38] are
able to train networks without using paired images. How-
ever, those works focus on learning deterministic image-to-
image mappings. Once the model is learnt, a source image
can only be transferred to a ﬁxed target style.

A few recent works [39, 20, 65, 27, 1, 8, 33, 58, 37, 32]
concentrate on learning a uniﬁed model to translate im-
ages into multiple styles. These works can be divided into
two categories according to the controllability of the target
styles. The ﬁrst category, such as [27, 1], realizes the mul-
timodal translation by sampling different style codes which
are encoded from the target style images. However, those
works focus on modelling intra-domain diversity, while our
DLOW model aims at characterizing the inter-domain di-
versity. Moreover, they cannot explicitly control the trans-
lated target style using the input codes.

The second category, such as [8, 32], assigns the domain
labels to different target domains and the domain labels are
proven to be effective in controlling the translation direc-
tion. Among those, [32] shows that they could make in-
terpolation between target domains by continuously shift-
ing the different domain labels to change the extent of the
contribution of different target domains. However, these
methods only use the discrete binary domain labels in the
training. Unlike the above work, the domainness variable
proposed in this work is derived from the data distribution
distance, and is used explicitly to regularize the style of out-
put images during training.

Domain Adaptation and Generalization: Our work
is also related to the domain adaptation and generalization
works. Domain adaptation aims to utilize a labeled source
domain to learn a model that performs well on an unlabeled
target domain [13, 18, 12, 55, 29, 3, 31, 16, 6, 61, 57].
Domain generalization is a similar problem, which aims
to learn a model
that could be generalized to an un-
seen target domain by using multiple labeled source do-
mains [42, 15, 45, 41, 44, 34, 36, 35].

Our work is partially inspired by [18, 16, 10], which have
shown that the intermediate domains between source and
target domains are useful for addressing the domain adap-
tation problem. They represent each domain as a subspace
or covariance matrix, and then connect them on the corre-
sponding manifold to model intermediate domains. Differ-
ent from those works, we model the intermediate domains
by directly translating images on pixel level. This allows us
to easily improve the existing deep domain adaptation mod-
els by using the translated images as training data. More-
over, our model can also be applied to image-level domain
generalization by generating mixed-style images.

Recently, there is an increasing interest to apply domain
adaptation techniques for semantic segmentation from syn-
thetic data to the real scenario [22, 21, 7, 67, 40, 25, 11,
46, 51, 53, 23, 47, 62, 54, 43, 50, 52, 66, 5]. Most of those
works conduct the domain adaptation by adversarial train-
ing on the feature level with different priors. The recent
Cycada [21] also shows that it is beneﬁcial to perform pixel-
level domain adaptation ﬁrstly by transferring source image
into the target style based on the image-to-image translation
methods like CycleGAN [64]. However, those methods ad-
dress domain shift by adapting to only the target domain. In
contrast, we aim to perform pixel-level adaptation by trans-
ferring source images to a ﬂow of intermediate domains.
Moreover, our model can also be used to further improve
the existing feature-level adaptation methods.

2478

3. Domain Flow Generation

3.1. Problem Statement

In the domain shift problem, we are given a source do-
main S and a target domain T containing samples from two
different distributions PS and PT , respectively. Denoting a
source sample as xs ∈ S and a target sample as xt ∈ T , we
have xs ∼ PS, xt ∼ PT , and PS 6= PT .

Such distribution mismatch usually leads to a signiﬁ-
cant performance drop when applying the model trained
on S to T . Many works have been proposed to ad-
dress the domain shift for different vision applications. A
group of recent works aim to reduce the distribution dif-
ference on the feature level by learning domain-invariant
features [13, 18, 31, 16], while others work on the image
level to transfer source images to mimic the target domain
style [64, 38, 65, 27, 1, 8].

In this work, we also propose to address the domain shift
problem on image level. However, different from existing
works that focus on transferring source images into only the
target domain, we instead transfer them into all intermedi-
ate domains that connect source and target domains. This
is partially motivated by the previous works [18, 16, 10],
which have shown that the intermediate domains between
source and target domains are useful for addressing the do-
main adaptation problem.

In the follows, we ﬁrst brieﬂy review the conven-
tional image-to-image translation model CycleGAN. Then,
we formulate the intermediate domain adaptation problem
based on the data distribution distance. Next, we present
our DLOW model based on the CycleGAN model. We then
show the beneﬁts of our DLOW model with two applica-
tions: 1) improve existing domain adaptation models with
the images generated from DLOW model, and 2) transfer
images into arbitrarily mixed styles when there are multiple
target domains.

3.2. The CycleGAN Model

We build our model based on the state-of-the-art Cycle-
GAN model [64] which is proposed for unpaired image-to-
image translation. Formally, the CycleGAN model learns
two mappings between S and T , i.e., GST : S → T
which transfers the images in S into the style of T , and
GT S : T → S which acts in the inverse direction. We take
the S → T direction as an example to explain CycleGAN.
To transfer source images into the target style and also
preserve the semantics, the CycleGAN employs an adver-
sarial training module and a reconstruction module, respec-
tively. In particular, the adversarial training module is used
to align the image distributions for two domains, such that
the style of mapped images matches the target domain. Let
us denote the discriminator as DT , which attempts to distin-
guish the translated images and the target images. Then the

Figure 2: Illustration of domain ﬂow. Many possible paths
(the green dash lines) connect source and target domains,
while the domain ﬂow is the shortest one (the red line).
There are multiple domains (the blue dash line) keeping the
expected relative distances to source and target domains.
An intermediate domain (the blue dot) is the point at the
domain ﬂow that keeps the right distances to two domains.

objective function of the adversarial training module can be
written as,

min
GST

max
DT

E

x

+ Ex

t∼PT (cid:2)log(DT (x
s∼PS [log(1 − DT (GST (x

t))(cid:3)

s)))] .

(1)

Moreover,
the reconstruction module is to ensure the
mapped image GST (xs) to preserve the semantic content
of the original image xs. This is achieved by enforcing a
cycle consistency loss such that GST (xs) is able to recover
xs when being mapped back to the source style, i.e.,

min
GST

Ex

s∼PS [kGT S(GST (x

s)) − x

sk1] .

(2)

Similar modules are applied to the T → S direction. By
jointly optimizing all modules, CycleGAN model is able to
transfer source images into the target style and v.v.

3.3. Modeling Intermediate Domains

Intermediate domains have been shown to be helpful for
domain adaptation [18, 16, 10], where they model interme-
diate domains as a geodesic path on Grassmannian or Rie-
mannian manifold. Inspired by those works, we also char-
acterize the domain shift using intermediate domains that
connect the source and target domains. Diffrent from those
works, we directly operate at the image level, i.e., trans-
lating source images into different styles corresponding to
intermediate domains. In this way, our method can be easily
integrated with deep learning techniques for enhancing the
cross-domain generalization ability of models.

In particular, let us denote an intermediate domain as

M(z), where z ∈ [0, 1] is a continous variable which mod-
els the relatedness to source and target domains. We refer to
z as the domainness of intermediate domain. When z = 0,
the intermediate domain M(z) is identical to the source do-
main S; and when z = 1, it is identical to the target domain
T . By varying z in the range of [0, 1], we thus obtain a
sequence of intermediate domains that ﬂow from S to T .

2479

Figure 3: The overview of our DLOW model: the generator takes domainness z as additional input to control the image
translation and to reconstruct the source image; The domainness z is also used to weight the two discriminators.

There are many possible paths to connect the source and
target domains. As shown in Fig 2, assuming there is a
manifold of domains, where a domain with given data dis-
tribution can be seen as a point residing at the manifold. We

expect the domain ﬂow M(z) to be the shortest geodesic
path connecting S and T . Moreover, given any z, the dis-
tance from S to M(z) should also be proportional to the
distance between S to T by the value of z. Denoting the
data distribution of M(z) as P (z)
M , we expect that,
dist(cid:16)PS, P (z)
M (cid:17)
dist(cid:16)PT , P (z)
M (cid:17)

1 − z

(3)

=

z

,

where dist(·,·) is a valid distance measurement over two
distributions. Thus, generating an intermediate domain
M(z) for a given z becomes ﬁnding the point satisfying Eq.
(3) that is closet to S and T , which leads to minimize the
following loss,

L = (1 − z) · dist(cid:16)PS, P (z)

M (cid:17) + z · dist(cid:16)PT , P (z)

M (cid:17) .

(4)

As shown in [2], many types of distance have been exploited
for image generation and image translation. The adversarial
loss in Eq. (1) can be seen as a lower bound of the Jessen-
Shannon divergence. We also use it to measure distribution
distance in this work.

3.4. The DLOW Model

We now present our DLOW model to generate interme-

diate domains. Given a source image xs ∼ Ps, and a do-
mainness variable z ∈ [0, 1], the task is to transfer xs into
the intermediate domain M(z) with the distribution P (z)
that minimizes the objective in Eq. (4). We take the S → T
direction as an example, and the other direction can be sim-
ilarly applied.

M

In our DLOW model, the generator GST no longer aims
to directly transfer xs to the target domain T , but to move
xs towards it. The interval of such moving is controlled by
the domainness variable z. Let us denote Z = [0, 1] as the
domain of z, then the generator in our DLOW model can
be represented as GST (xs, z) : S × Z → M(z) where the
input is a joint space of S and Z.
Adversarial Loss: As discussed in Section 3.3, We de-
ploy the adversarial loss as the distribution distance mea-
surement to control the relatedness of an intermediate do-
main to the source and target domains. Speciﬁcally, we

introduce two discriminators, DS(x) to distinguish M(z)
and S, and DT (x) to distinguish M(z) and T , respectively.
Then, the adversarial losses between M(z) and S and T can

be written respectively as,
Ladv( GST , DS) = Ex

+ Ex

Ladv( GST , DT ) = E

+ Ex

s∼PS [log(DS(x
s∼PS [log(1 − DS(GST (x
t∼PT (cid:2)log(DT (x
s∼PS [log(1 − DT (GST (x

x

s))]
s, z)))]
t))(cid:3)
s, z)))] .

(5)

(6)

By using the above losses to model dist(PS, P (z)
dist(PT , P (z)
M ) in Eq. (4), we derive the following loss,
Ladv = (1 − z)Ladv(GST , DS) + zLadv(GST , DT ).

M ) and

(7)

Image Cycle Consistency Loss: Similarly as in Cyl-
ceGAN, we also apply a cycle consistency loss to ensure
the semantic content is well-preserved in the translated im-
age. Let us denote the generator on the other direction as

GT S(xt, z) : T × Z → M(1−z), which transfers a sam-
ple xt from the target domain towards the source domain
by a interval of z. Since GT S acts in an inverse direction to
GST , we can use it to recover xs from the translated version
GST (xs, z), which gives the following loss,
s, z), z) − x

s∼Ps [kGT S(GST (x

Lcyc =Ex

sk1] .

(8)

2480

Full Objective: We integrate the losses deﬁned above,

then the full objective can be deﬁned as,

L = Ladv + λ1Lcyc,

(9)

where λ1 is a hyper-parameter used to balance the two
losses in the training process.

Similar loss can be deﬁned for the other direction T →
S. Due to the usage of adversarial loss Ladv, the training
is performed in an alternating manner. We ﬁrst minimize
the full objective with regard to the generators, and then
maximize it with regard to the discriminators.

Implementation: We illustrate the network structure of
of our DLOW model in Fig 3. First, the domainness vari-
able z is taken as the input of the generator GST . This is
implemented with the Conditional Instance Normalization
(CN) layer [1, 26]. We ﬁrst use one deconvolution layer
to map the domainness variable z to the vector with dimen-
sion (1, 16, 1, 1), and then use this vector as the input for the
CN layer. Moreover, the domainness variable also plays the
role of weighting discriminators to balance the relatedness
of the generated images to different domains. It is also used
as input in the image cycle consistency module. During the
training phase, we randomly generate the domainess param-
eter z for each input image. As inspired by [24], we force
the domainness variable z to obey the beta distribution, i.e.
f (z, α, β) = 1
B(α,β) zα−1(1 − z)β−1, where β is ﬁxed as 1,
and α is a function of the training step α = e
0.25T with t
being the current iteration and T being the total number of
iterations. In this way, z tends to be sampled more likely as
small values at the beginning, and gradually shift to larger
values at the end, which gives slightly more stable training
than uniform sampling.

t−0.5T

3.5. Boosting Domain Adaptation Models

i , yi)|n

i=1} where yi is the label of xs

With the DLOW model, we are able to translate
each source image xs into an arbitrary intermediate do-
main M(z). Let us denote the source dataset as S =
{(xs
i . By feeding each
of the image xs
i combined with zi randomly sampled from
the uniform distribution U (0, 1), we then obtain a translated
dataset ˜S = {(˜xs
i , yi)|n
i=1} where ˜xs
i , zi) is the
i . The images in ˜S spread along
translated version of xs
the domain ﬂow from source to target domain, and there-
fore become much more diverse. Using ˜S as the train-

ing data is helpful to learn domain-invariant models for
In Section 4.1, we demonstrate
computer vision tasks.

i = GST (xs

cross-domain semantic segmentation problem.

that model trained on ˜S achieves good performance for the
Moreover, the translated dataset ˜S can also be used to
tion approaches. Images in ˜S ﬁll the gap between the source

boost the existing adversarial training based domain adapta-

and target domains, and thus ease the domain adaptation

Figure 4: Illustration of boosting domain adaptation model
for corss-domain semantic segmentation with DLOW
model.
Intermediate domain images are used as source
dataset, and the adversarial loss is weighted by domainness.

task. Taking semantic segmentation as an example, a typ-
ical way is to append a discriminator to the segmentation
model, which is used to distinguish the source and target
samples. Using the adversarial training strategy to optimize
the discriminator and the segmentation model, the segmen-
tation model is trained to be more domain-invariant.

As shown in Fig 4, we replace the source dataset S with
the translated version ˜S, and apply a weight √1 − zi to the
adversarial loss. The motivation is as follows, for each sam-
ple ˜xs
i , if the domainness zi is higher, it is closer to the target
domain, then the weight of adversarial loss can be reduced.
Otherwise, we should enhance the loss weight.

3.6. Style Generalization

Most existing image-to-image translation works learn a
deterministic mapping between two domains. After the
model is learnt, source images can only be translated to a
ﬁxed style. In contrast, our DLOW model takes an random
z to translate images into various styles. When multiple
target domains are provided, it is also able to transfer the
source image into a mixture of different target styles.
In
other words, we are able to generalize to an unseen inter-
mediate domain that is related to existing domains.

In particular, suppose we have K target domains, de-
noted as T1, . . . ,TK . Accordingly, the domainness variable
z is expanded as a K-dim vector z = [z1, . . . , zK]′ with
PK
k=1 zk = 1. Each elelment zk represents the relatedness
to the k-th target domain. To map an image from the source
domain to the intermediate domain deﬁned by z, we need
to optimize the following objective,

L =

K

X

k=1

zk · dist(PM , PTk ),

s.t.

K

X

1

zk = 1

(10)

where PM is the distribution of the intermediate domain,
PTK is the distribution of Tk. The network structure can
be easily adjusted from our DLOW model to optimize the

2481

above objective. We leave the details in the Supplementary
due to the space limitation.

4. Experiments

In this section, we demonstrate the beneﬁts of our
DLOW model with two tasks.
In the ﬁrst task, we ad-
dress the domain adaptation problem, and train our DLOW
model to generate the intermediate domain samples to boost
the domain adaptation performance.
In the second task,
we consider the style generalization problem, and train our
DLOW model to transfer images into new styles that are
unseen in the training data.

4.1. Domain Adaptation and Generalization

4.1.1 Experiments Setup

For the domain adaptation problem, we follow [22, 21, 7,
67] to conduct experiments on the urban scene semantic
segmentation by learning from synthetic data to real sce-
nario. The GTA5 dataset [48] is used as the source domain
while the Cityscapes dataset [9] as the target domain. More-
over, we also evaluate the generalization ability of learnt
segmentation models to unseen domains, for which we take
the KITTI [14], WildDash [60] and BDD100K [59] datasets
as additional unseen datasets for evaluation. We also con-
duct experiments using the SYNTHIA dataset [49] as the
source domain, and provide the results in Supplementary.

Cityscapes is a dataset consisting of urban scene images
taken from some European cities. We use the 2, 993 training
images without annotation as unlabeled target samples in
training phase, and 500 validation images with annotation
for evaluation, which are densely labelled with 19 classes.
GTA5 is a dataset consisting of 24, 966 densely labelled
synthetic frames generated from the computer game whose
scenes are based on the city of Los Angeles. The annota-
tions of the images are compatible with the Cityscaps.

KITTI is a dataset consisting of images taken from
mid-size city of Karlsruhe. We use 200 validation images
densely labeled and compatible with Cityscapes.

WildDash is a dataset covers images from different
sources, different environments(place, weather, time and so
on) and different camera characteristics. We use 70 labeled
and Cityscapes annotation compatible validation images.

BDD100K is a driving dataset covering diverse images
taken from US whose label maps are with training indices
speciﬁed in Cityscapes. We use 1, 000 densely labeled im-
ages for validation in our experiment.

In this task, we ﬁrst train our proposed DLOW model us-
ing the GTA5 dataset as the source domain, and Cityscapes
as the target domain. Then, we generate a translated GTA5
dataset with the learnt DLOW model. Each source image
is fed into DLOW with a random domainness variable z.
The new translated GTA5 dataset contains exactly the same

number of images as the original one, but the styles of im-
ages randomly drift from the synthetic style to the real style.
We then use the translated GTA dataset as the new source
domain to train segmentation models.

We implement our model based on Augmented Cycle-
GAN [1] and CyCADA [21]. Following their setup, all
images are resized to have width 1024 while keeping the
aspect ratio and the crop size is set as 400 × 400. When
training the DLOW model, the image cycle consistency loss
weight is set as 10. The learning rate is ﬁxed as 0.0002. For
the segmentation network, we use the AdaptSegNet [54]
model, which is based on DeepLab-v2 [4] with ResNnet-
101 [19] as the backbone network. The training images are
resized to 1280×720. We follow the exact the same training
policy as in the AdaptSegNet.

4.1.2 Experimental Results

Intermediate Domain Images: To verify the ability of our
DLOW model to generate intermediate domain images, in
the inference phase, we ﬁx the input source image, and
vary the domainness variable from 0 to 1. A few exam-
ples are shown in Fig 5. It can be observed that the styles
of translated images gradually shift from the synthetic style
of GTA5 to the real style of Cityscapes, which demonstrates
the DLOW model is capable of modeling the domain ﬂow to
bridge the source and target domains as expected. Enlarged
images and more discussion are provided in Supplementary.
Cross-Domain Semantic Segmentation: We further eval-
uate the usefulness of intermediate domain images in two
settings.
In the ﬁrst setting, we compare with the Cy-
cleGAN model [64], which is used in the CycADA ap-
proach [21] for performing pixel-level domain adaptation.
The difference between CycleGAN and our DLOW model
is that CycleGAN transfers source images to mimic only
the target style, while our DLOW model transfers source
images into random styles ﬂowing from the source domain
to the target domain. We ﬁrst obtain a translated version of
the GTA5 dataset with each model. Then, we respectively
use the two transalated GTA5 datasets to train DeepLab-v2
models, which are evaluated on the Cityscapes dataset for
semantic segmentation. We also include the “NonAdapt”
baseline which uses the original GTA5 images as training
data, as well as a special case of our approach, “DLOW(z =
1)”, where we set z = 1 for all source images when making
image translation using the learnt DLOW model.

The results are shown in Table 1. We observe that all
pixel-level adaptation methods outperform the “NonAdapt”
baseline, which veriﬁes that image translation is helpful
for training models for cross-domain semantic segmenta-
tion. Moreover, “DLOW(z = 1)” is a special case of our
model that directly translates source images into the target
domain, which non-surprisingly gives comparable result as

2482

(a) z = 0

(b) z = 0.3

(c) z = 0.6

(d) z = 0.8

(e) z = 1

Figure 5: Examples of intermediate domain images from GTA5 to Cityscapes. As the domainness variable increases from 0
to 1, the styles of the translated images shift from the synthetic GTA5 style to the realistic Cityscapes style gradually.

k
l
a
w
e
d
i
s

g
n
i
d
l
i
u
b

d
a
o
r

l
l
a
w

e
c
n
e
f

e
l
o
p

GTA5 → Cityscapes

t
h
g
i
l

c
ﬁ
f
a
r
t

n
g
i
s

c
ﬁ
f
a
r
t

n
o
i
t
a
t
e
g
e
v

n
a
i
r
r
e
t

y
k
s

n
o
s
r
e
p

r
e
d
i
r

r
a
c

k
c
u
r
t

s
u
b

n
i
a
r
t

e
k
i
b
r
o
t
o
m

e
l
c
y
c
i
b

Method

mIoU
NonAdapt[54] 75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6
CycleGAN[21] 81.7 27.0 81.7 30.3 12.2 28.2 25.5 27.4 82.2 27.0 77.0 55.9 20.5 82.8 30.8 38.4 0.0 18.8 32.3 41.0
DLOW(z = 1) 88.5 33.7 80.7 26.9 15.7 27.3 27.7 28.3 80.9 26.6 74.1 52.6 25.1 76.8 30.5 27.2 0.0 15.7 36.0 40.7
87.1 33.5 80.5 24.5 13.2 29.8 29.5 26.6 82.6 26.7 81.8 55.9 25.3 78.0 33.5 38.7 0.0 22.9 34.5 42.3

DLOW

Table 1: Results of semantic segmentation on the CityScapes dataset based on DeepLab-v2 model with ResNet-101 backbone
using the images translated with different models. The results are reported on mIoU over 19 categories. The best result is
denoted in bold.

Cityscapes KITTI WildDash BDD100K

Original [54]

DLOW

42.4
44.8

30.7
36.6

18.9
24.9

37.0
39.1

Table 2: Comparison of the performance of AdaptSeg-
Net [54] when using original source images and interme-
diate domain images translated with our DLOW model for
semantic segmention under domain adaptation (1st column)
and domain generalization (2nd to 4th columns) scenarios.
The results are reported on mIoU over 19 categories. The
best result is denoted in bold.

the CycADA-pixel method (40.7% v.s. 41.0%). By fur-
ther using intermediate domain images, our DLOW model
is able to improve the result from 40.7% to 42.3%, which
demonstrates that intermediate domain images are helpful
for learning a more robust domain-invariant model.

In the second setting, we further use intermediate do-
main images to improve the feature-level domain adpata-
tion model. We conduct experiments based on the Adapt-
SegNet method [54], which is open source and has re-
ported the state-of-the-art result for GTA5→CityScapes. It
consists of multiple levels of adversarial training, and we
augment each level with the loss weight discussed in Sec-
tion 3.5. The results are reported in Table 2. The “Origi-
nal” method denotes the AdaptSegNet model that is trained
using GTA5 as the source domain, for which the results
are obtained using their released pretrained model. The
“DLOW” method is AdaptSegNet trained using translated
dataset with our DLOW model. From the ﬁrst column, we

observe that the intermediate domain images are able to
improve the AdaptSegNet model by 2.5% from 42.3% to
44.8%. More interestingly, we show that the AdaptSegNet
model with DLOW translated images also exhibits excellent
domain generalization ability when being applied to unseen
domains, which achieves signiﬁcantly better results than the
original AdaptSegNet model on the KITTI, WildDash and
BDD100K datasets as reported in the second to the fourth
columns, respectively. This shows that intermediate domain
images are useful to improve the model’s cross-domain gen-
eralization ability.

4.2. Style Generalization

We conduct the style generalization experiment on the
Photo to Artworks dataset[64], which consists of real pho-
tographs (6, 853 images) and artworks from Monet(1, 074
images), Cezanne(584 images), Van Gogh(401 images) and
Ukiyo-e(1, 433 images). We use the real photographs as the
source domain, and the remaining as four target domains.
As discussed in Section 3.6, The domainness variable in this
experiment is expanded as a 4-dim vector [z1, z2, z3, z4]′
meeting the condition P4
i=1 zi = 1. Also, z1, z2, z3 and
z4 corresponds to Monet, Van Gogh, Ukiyo-e and Cezanne,
respectively. Each element zi can be seen as how much
each style contributes to the ﬁnal mixture style. In every 5
steps of the training, we set the domainness variable z as
[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1] and uniformly
distributed random variable. The qualitative results of the
style generalization are shown in Fig 6. From the qualita-
tive results, it is shown that our DLOW model can translate

2483

Figure 6: Examples of style generalization. Results with red rectangles at four corners are images translated into the four
target domains, and those with green rectangles in between are images translated into intermediate domains. The results
show that our DLOW model generalizes well across styles, and produces new images styles smoothly.

the photo image to corresponding artworks with different
styles. When varying the values of domainness vector, we
can also successfully produce new styles related to differ-
ent painting styles, which demonstrates the good general-
ization ability of our model to unseen domains. Note, dif-
ferent from [63, 26], we do not need any reference image
in the test phase, and the domainness vector can be changed
instantly to generate different new styles of images. We
provide more examples in Supplementary.

Quantitative Results: To verify the effectiveness of our
model for style generalization, we conduct an user study on
Amazon Mechanical Turk (AMT) to compare with the ex-
isting methods FadNet [32] and MUNIT [27]. Two cases
are considered, style transfer to Van Gogh, and style gen-
eralization to mixed Van Gogh and Ukiyo-e. For FadNet,
domain labels are treated as attributes. For MUNIT, we mix
Van Gogh and Ukiyo-e as the target domain. The data for
each trial is gathered from 10 participants and there are 100
trials in total for each case. For the ﬁrst case, participants
are shown the example Van Gogh style painting and are re-
quired to choose the image whose style is more similar to
the example. For the second case, participants are shown
the example Van Gogh and Ukiyo-e style painting and are
required to choose the image with a style that is more like
the mixed style of the two example paintings. The user pref-
erence is summarized in Table 3, which shows that DLOW
outperforms FadNet and MUNIT on both tasks. Qualita-
tive comparison between different methods is provided in
Supplementary due to the space limitation.

FadNet[32] / DLOW MUNIT[27] / DLOW

Van Gogh

Van Gogh + Ukiyo-e

1.4% / 98.6%
1.6% / 98.4%

21.4% / 78.6%
15.3% / 84.7%

Table 3: User preference for style transfer and generaliza-
tion. It is shown that more users prefer our translated results
on both of the style transfer and generalization tasks com-
pared with the existing methods FadNet and MUNIT.

5. Conclusion

In this paper, we have presented the DLOW model to
generate intermediate domains for bridging different do-
mains. The model takes a domainness variable z (or do-
mainness vector z) as the conditional input, and transfers
images into the intermediate domain controlled by z or z.
We demonstrate the beneﬁts of our DLOW model in two
scenarios. Firstly, for the cross-domain semantic segmenta-
tion task, our DLOW model can improve the performance
of the pixel-level domain adaptation by taking the translated
images in intermediate domains as training data. Secondly,
our DLOW model also exhibits excellent style generaliza-
tion ability for image translation and we are able to transfer
images into a new style that is unseen in the training data.
Extensive experiments on benchmark datasets have veriﬁed
the effectiveness of our proposed model.

Acknowledgments The authors gratefully acknowledge the
support by armasuisse.

2484

References

[1] Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip
Bachman, and Aaron Courville. Augmented CycleGAN:
Learning many-to-many mappings from unpaired data.
In
ICML, 2018. 2, 3, 5, 6

[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.

Wasserstein gan. arXiv:1701.07875, 2017. 4

[3] Mahsa Baktashmotlagh, Mehrtash T Harandi, Brian C
Lovell, and Mathieu Salzmann. Unsupervised domain adap-
tation by domain invariant projection. In ICCV, 2013. 2

[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence, 40(4):834–848, 2018. 6

[5] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.
Learning semantic segmentation from synthetic data: A
geometrically guided input-output adaptation approach.
arXiv:1812.05040, 2018. 2

[6] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and
Luc Van Gool. Domain adaptive faster r-cnn for object de-
tection in the wild. In CVPR, 2018. 2

[7] Yuhua Chen, Wen Li, and Luc Van Gool. Road: Reality ori-
ented adaptation for semantic segmentation of urban scenes.
In CVPR, 2018. 2, 6

[8] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. StarGAN: Uniﬁed gener-
ative adversarial networks for multi-domain image-to-image
translation. In CVPR, 2018. 1, 2, 3

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016. 6

[10] Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen,
and Xuelong Li. Flowing on riemannian manifold: Domain
adaptation by shifting covariance. IEEE transactions on cy-
bernetics, 44(12):2264–2273, 2014. 2, 3

[11] Aysegul Dundar, Ming-Yu Liu, Ting-Chun Wang, John
Zedlewski, and Jan Kautz. Domain stylization: A strong,
simple baseline for synthetic to real image domain adapta-
tion. arXiv:1807.09384, 2018. 2

[12] Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne
Tuytelaars. Unsupervised visual domain adaptation using
subspace alignment. In ICCV, 2013. 2

[13] Yaroslav Ganin and Victor S. Lempitsky. Unsupervised do-

main adaptation by backpropagation. In ICML, 2015. 2, 3

[14] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In CVPR, 2012. 6

[15] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,
and David Balduzzi. Domain generalization for object recog-
nition with multi-task autoencoders. In ICCV, 2015. 1, 2

[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2

[18] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Do-
main adaptation for object recognition: An unsupervised ap-
proach. In ICCV, 2011. 2, 3

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 6

[20] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan,
and Xilin Chen. Arbitrary facial attribute editing: Only
change what you want. arXiv:1711.10678, 2017. 2

[21] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell.
CyCADA: Cycle-consistent adversarial domain adaptation.
In ICML, 2018. 1, 2, 6, 7

[22] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell.
Fcns in the wild: Pixel-level adversarial and constraint-based
adaptation. arXiv:1612.02649, 2016. 2, 6

[23] Weixiang Hong, Zhenzhen Wang, Ming Yang, and Junsong
Yuan. Conditional generative adversarial network for struc-
tured domain adaptation. In CVPR, 2018. 2

[24] Yann N. Dauphin David Lopez-Paz Hongyi Zhang,
Moustapha Cisse. mixup: Beyond empirical risk minimiza-
tion. In ICLR, 2018. 5

[25] Haoshuo Huang, Qixing Huang, and Philipp Kr¨ahenb¨uhl.
In

Domain transfer through deep activation matching.
ECCV, 2018. 2

[26] Xun Huang and Serge Belongie. Arbitrary style transfer in
In ICCV,

real-time with adaptive instance normalization.
2017. 5, 8

[27] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
In

Multimodal unsupervised image-to-image translation.
ECCV, 2018. 2, 3, 8

[28] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017. 2

[29] I-Hong Jhuo, Dong Liu, DT Lee, and Shih-Fu Chang. Robust
In

visual domain adaptation with low-rank reconstruction.
CVPR, 2012. 2

[30] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations
with generative adversarial networks. In ICML, 2017. 2

[31] Elyor Kodirov, Tao Xiang, Zhenyong Fu, and Shaogang
Gong. Unsupervised domain adaptation for zero-shot learn-
ing. In ICCV, 2015. 2, 3

[32] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, An-
toine Bordes, Ludovic DENOYER, et al. Fader networks:
Manipulating images by sliding attributes. In NIPS, 2017. 2,
8

[33] Hsin-Ying Lee, Hung-Yu Tseng,

Jia-Bin Huang, Ma-
neesh Kumar Singh, and Ming-Hsuan Yang. Diverse image-
to-image translation via disentangled representations.
In
ECCV, 2018. 2

[16] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman.
Geodesic ﬂow kernel for unsupervised domain adaptation.
In CVPR, 2012. 2, 3

[34] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot.
Domain generalization with adversarial feature learning. In
CVPR, 2018. 2

2485

[35] Wen Li, Zheng Xu, Dong Xu, Dengxin Dai, and Luc
Van Gool. Domain generalization and adaptation using low
rank exemplar svms. IEEE transactions on pattern analysis
and machine intelligence, 40(5):1114–1127, 2018. 2

[36] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang
Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza-
tion via conditional invariant adversarial networks. In ECCV,
2018. 2

[37] Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-
Yan Liu. Conditional image-to-image translation. In CVPR,
2018. 2

[38] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised

image-to-image translation networks. In NIPS, 2017. 2, 3

[39] Yongyi Lu, Yu-Wing Tai, and Chi-Keung Tang. Condi-
tional cyclegan for attribute guided face image generation.
arXiv:1705.09966, 2017. 2

[40] Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi
Yang. Taking a closer look at domain shift: Category-
level adversaries for semantics consistent domain adaptation.
arXiv:1809.09478, 2018. 2

[41] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gi-
anfranco Doretto. Uniﬁed deep supervised domain adapta-
tion and generalization. In ICCV, 2017. 2

[42] Krikamol Muandet, David Balduzzi, and Bernhard Schlkopf.
Domain generalization via invariant feature representation.
In ICML, 2013. 2

[43] Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ra-
mamoorthi, and Kyungnam Kim. Image to image translation
for domain adaptation. arXiv:1712.00479, 2017. 2

[44] Li Niu, Wen Li, and Dong Xu. Multi-view domain general-

ization for visual recognition. In ICCV, 2015. 2

[45] Li Niu, Wen Li, and Dong Xu. Visual recognition by learning
from web data: A weakly supervised domain generalization
approach. In CVPR, 2015. 2

[52] Swami Sankaranarayanan, Yogesh Balaji, Arpit

Jain,
Ser Nam Lim, and Rama Chellappa. Unsupervised do-
main adaptation for semantic segmentation with gans. arXiv
preprint arXiv:1711.06969, 2017. 1, 2

[53] Swami Sankaranarayanan, Yogesh Balaji, Arpit

Jain,
Ser Nam Lim, and Rama Chellappa. Learning from synthetic
data: Addressing domain shift for semantic segmentation. In
CVPR, 2018. 2

[54] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang,
and M. Chandraker. Learning to adapt structured output
space for semantic segmentation. In CVPR, 2018. 1, 2, 6,
7

[55] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
In CVPR,

Adversarial discriminative domain adaptation.
2017. 2

[56] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
CVPR, 2018. 2

[57] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre-
mental adversarial domain adaptation for continually chang-
ing environments. In ICRA, 2018. 2

[58] Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong.
Dualgan: Unsupervised dual learning for image-to-image
translation. In ICCV, 2017. 2

[59] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike
Liao, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A
diverse driving video database with scalable annotation tool-
ing. arXiv:1805.04687, 2018. 6

[60] Oliver Zendel, Katrin Honauer, Markus Murschitz, Daniel
Steininger, and Gustavo Fernandez Dominguez. Wilddash -
creating hazard-aware benchmarks. In ECCV, 2018. 6

[61] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Col-
laborative and adversarial network for unsupervised domain
adaptation. In CVPR, 2018. 2

[46] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two
at once: Enhancing learning and generalization capacities
via ibn-net. In ECCV, 2018. 2

[62] Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao
Mei. Fully convolutional adaptation networks for semantic
segmentation. In CVPR, 2018. 2

[47] Xingchao Peng, Ben Usman, Neela Kaushik, Dequan
Wang, Judy Hoffman, Kate Saenko, Xavier Roynard, Jean-
Emmanuel Deschaud, Francois Goulette, Tyler L Hayes,
et al. Visda: A synthetic-to-real benchmark for visual do-
main adaptation. In CVPR Workshops, 2018. 2

[48] Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for data: Ground truth from computer
games. In ECCV, 2016. 6

[49] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M. Lopez. The synthia dataset: A
large collection of synthetic images for semantic segmenta-
tion of urban scenes. In CVPR, 2016. 6

[50] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-
suya Harada. Maximum classiﬁer discrepancy for unsuper-
vised domain adaptation. arXiv:1712.02560, 2017. 2

[51] Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian,
Mathieu Salzmann, Lars Petersson, and Jose M Alvarez. Ef-
fective use of synthetic data for urban scene semantic seg-
mentation. In ECCV, 2018. 2

[63] Yexun Zhang, Ya Zhang, and Wenbin Cai. A uniﬁed frame-
work for generalizable style transfer: Style and content sep-
aration. arXiv:1806.05173, 2018. 8

[64] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks.
In ICCV, 2017. 1, 2, 3,
6, 7

[65] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In NIPS, 2017.
2, 3

[66] Xinge Zhu, Hui Zhou, Ceyuan Yang, Jianping Shi, and
Dahua Lin. Penalizing top performers: Conservative loss for
semantic segmentation adaptation. arXiv:1809.00903, 2018.
2

[67] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong
Wang. Unsupervised domain adaptation for semantic seg-
mentation via class-balanced self-training. In ECCV, 2018.
2, 6

2486

