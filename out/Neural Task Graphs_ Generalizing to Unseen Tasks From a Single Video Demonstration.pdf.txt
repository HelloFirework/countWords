Neural Task Graphs: Generalizing to Unseen Tasks

from a Single Video Demonstration

De-An Huang*, Suraj Nair*, Danfei Xu*, Yuke Zhu, Animesh Garg,

Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles

Computer Science Department, Stanford University

Abstract

Single Video Demonstration

Task Completion

Our goal is to generate a policy to complete an unseen
task given just a single video demonstration of the task in
a given domain. We hypothesize that to successfully gener-
alize to unseen complex tasks from a single video demon-
stration, it is necessary to explicitly incorporate the compo-
sitional structure of the tasks into the model. To this end,
we propose Neural Task Graph (NTG) Networks, which use
conjugate task graph as the intermediate representation to
modularize both the video demonstration and the derived
policy. We empirically show NTG achieves inter-task gen-
eralization on two complex tasks: Block Stacking in Bul-
letPhysics and Object Collection in AI2-THOR. NTG im-
proves data efÔ¨Åciency with visual input as well as achieve
strong generalization without the need for dense hierarchi-
cal supervision. We further show that similar performance
trends hold when applied to real-world data. We show that
NTG can effectively predict task structure on the JIGSAWS
surgical dataset and generalize to unseen tasks.

1. Introduction

test

Learning sequential decisions and adapting to new task
time is a long-standing challenge in
objectives at
AI [5, 9]. In rich real domains, an autonomous agent has
to acquire new skills with minimal supervision. Recent
works have tackled the problem of one-shot imitation learn-
ing [8, 11, 40, 41] that learns from a single demonstration.
In this work, we push a step further to address one-shot vi-
sual imitation learning that operates directly on videos. We
Ô¨Årst train a model on a set of seen in-domain tasks. The
model can then be applied on a single video demonstration
to obtain an execution policy of the new unseen task.

Learning directly from video is crucial for advancing the
existing imitation learning approaches to real-world scenar-
ios as it is infeasible to annotate states, such as object tra-
jectories, in each video. We focus on long-horizon tasks,

* indicates equal contribution

NTG

Generator

Action

Env.

NTG

Execution

Engine

Conjugate Task Graph

Visual Observation

Figure 1. Our goal is to execute an unseen task from a single video
demonstration. We propose Neural Task Graph Networks that
leverage compositionality by using the task graph as the interme-
diate representation. This leads to strong inter-task generalization.

as real-world tasks such as cooking or assembly are inher-
ently long-horizon and hierarchical. Recent works have
attempted learning from pixel space [11, 27, 35, 42], but
learning long-horizon tasks from video in a one-shot setting
remains a challenge, since both the visual learning and task
complexity exacerbate the demand for better data efÔ¨Åciency.
Our solution explicitly models the compositionality in
the task structure and policy, enabling us to scale one-
shot visual imitation to complex tasks. This is in con-
trast to previous works using unstructured task represen-
tations and policies [8, 11]. The use of compositionality
has led to better generalization in Visual Question Answer-
ing [17, 20, 24] and Policy Learning [3, 7, 38]. We propose
Neural Task Graph (NTG) Networks, a novel framework
that uses task graph as the intermediate representation to
explicitly modularize both the visual demonstration and the
derived policy. NTG consists of a generator and an execu-
tion engine, where the generator builds a task graph from
the task demo video to capture the structure of the task, and
the execution engine interacts with the environment to per-
form the task conditioned on the inferred task graph. Figure
1 shows an overview of NTG Networks.

The main technical challenge in using graphical task rep-
resentations is that the unseen demos can easily introduce
states that are never observed during training. For example,

8565

the goal state of an unseen block stacking task [8, 41] is a
block conÔ¨Åguration that never appears during training. This
challenge is ampliÔ¨Åed by our goal of learning from visual
observation without strong supervision, which obscures the
state structure and prevents direct state space decomposi-
tion, as done in prior work [8]. Our key observation is
that, while there can be countless possible states, the num-
ber of possible actions in a certain domain is often limited.
We leverage this conjugate relationship between states and
actions, and propose to learn NTG on the Conjugate Task
Graph (CTG) [16], where the nodes are actions, and the
states are captured by the edges. This allows us to modu-
larize the policy and address the challenge of an unknown
number of novel states. This is critical when operating in
visual space, where states are high dimensional images and
modeling a graph over a combinatorial state space is in-
feasible. Additionally, the CTG intermediate representation
can yield alternate action sequences to complete the task, a
property that is vital for generalization to unseen scenarios
in a world with stochastic dynamics. This sets NTG apart
from previous works that directly output the policy over op-
tions [41] or actions [8] from a single demonstration.

We evaluate NTG Networks on one-shot visual imitation
learning in two domains: Block Stacking in a robot simu-
lator [6] and Object Collection in AI2-THOR [23]. Both
domains involve multi-step planning for interaction and are
inherently compositional. We show that NTG signiÔ¨Åcantly
improves the data efÔ¨Åciency on these complex tasks for di-
rect imitation from video by explicitly incorporating com-
positionality. We also show that with the data-driven task
structure, NTG outperforms methods that learn unstruc-
tured task representation [8] and methods that use strong
hierarchically structured supervision [41], albeit without re-
quiring detailed supervision. Further, we evaluate NTG
on real-world videos. We show that NTG can effectively
predict task graph structure on the JIGSAWS [12] surgical
dataset and generalize to unseen human demonstrations.

In summary, the main contributions of our work are:
(1) Introducing compositionality to both the task and pol-
icy representation to enable one-shot visual imitation learn-
ing on long-horizon tasks; (2) Proposing Neural Task Graph
(NTG) Networks, a novel framework that uses task graph to
capture the structure and the goal of a task; (3) Addressing
the challenge of novel visual state decomposition using a
Conjugate Task Graph (CTG) formulation.

2. Related Work

Imitation Learning. Traditional imitation learning work
uses physical guidance [1, 31] or teleoperation [39, 43] as
demonstration. While, third-person imitation learning uses
date from other agents or viewpoints [27, 35]. Recent meth-
ods for one-shot imitation learning [8, 11, 13, 40, 41, 42]
can translate a single demonstration to an executable pol-

1

k
s
a
T

2

k
s
a
T

s
n
o

i
t

a
r
t
s
n
o
m
e
D
k
s
a
T

Supervised Loss

Model

ùúë

ùúã"(ùëé|ùëú)

ùúã((ùëé|ùëú)

Seen Tasks for Training (ùëá*++,)

Single Video
Demonstration

3

k
s
a
T

Action

Model

ùúë

ùúã/(ùëé|ùëú)

Env.

Unseen Tasks for Testing (ùëá-,*++,)

Observation

Figure 2. Overview of the setting of one-shot visual imitation
learning. The seen tasks (Task 1 and 2) are used to train the model
œÜ to instantiate the policy œÄi from the demonstration. During test-
ing, œÜ is applied to a single video demonstration from the unseen
Task 3 to generate the policy œÄ3 to interact with the environment.

icy. The most similar to ours is NTP [41] that also learns
long-horizon tasks. However, NTP (1) uses strong hierar-
chical frame label supervision and (2) suffers from a no-
ticeable drop in performance with visual state. Our method
reduces the need for this strong supervision, requiring only
the demonstration action sequence during training, while
achieving a performance boost of over 25% in success rates.

Task Planning and Representations. Conventionally task
planning focuses on high-level plans and low-level state
spaces [10, 36]. Recent works integrate perception via deep
learning [15, 32, 44]. HTN compounds low-level sub-tasks
into higher-level abstraction to reduce the planning com-
plexity [29, 33]. Other representations include:
integrat-
ing task and motion planning [21] and behavior-based sys-
tems [30]. In vision, And-Or Graphs capture the hierarchi-
cal structures and have been used to parse video demonstra-
tions [26]. Unlike previous methods, our task graph repre-
sentation is data-driven and domain-agnostic: we generate
nodes and edges directly from task demonstrations.

Structural Video Understanding. Generating task graphs
from demonstrations is related to video understanding. An-
notation in videos is hard to obtain. One solution is to
use the language as supervision. This includes instructional
video [2, 18, 34], movie script [37, 45], and caption anno-
tation [14, 25]. We focus on how the structure is helpful for
task learning, and assume the annotation for the seen tasks.

Compositional Models in Vision and Robotics. Recent
works have utilized compositionality to improve models‚Äô
generalization, including visual question answering [4, 17,
20] and policy learning [3]. We show the same principle can
signiÔ¨Åcantly improve data efÔ¨Åciency in imitation learning to
enable visual learning of complex tasks.

8566

NTG Generator

NTG Execution Engine

Demo

Interpreter

Graph

Completion

Network

Conjugate
Task Graph

Visual Observation (ùëú)

Node

Localizer

Edge

Classifier

Env.

Demonstration

Action (ùëé)

Figure 3. Overview of our Neural Task Graph (NTG) networks. The NTG networks consist of a generator that produces the conjugate
task graph as the intermediate representation, and an execution engine that executes the graph by localizing node and deciding the edge
transition in the task graph based on the current visual observation.

3. Problem Formulation

Our goal is to learn to execute a previously unseen task
from a single video demonstration. We refer to this as one-
shot visual imitation, where the model directly learns from
visual inputs. Let T be the set of all tasks in the domain
of interest, A be the set of high-level actions, and O be the
space of visual observation. A video demonstration d for a
task œÑ is deÔ¨Åned as a video, dœÑ = [o1, . . . , oT ], that com-
plete the task. As shown in Figure 2, T is split into two
sets: Tseen with a large amount of demonstrations and su-
pervision for training, and Tunseen with only task demon-
strations for evaluation. The goal is to learn a model œÜ(¬∑)
from Tseen that can instantiate a policy œÄd(a|o) from d to
perform the tasks in Tunseen using visual observations.

The learning problem is formulated as learning a model
œÜ(¬∑) that maps demonstration d to the policy œÜ(d) =
œÄd(a|o). Tseen is used to train this model with demonstra-
tions and potentially extra supervision. At test time, given a
demonstration d from an unseen task, the hope is that œÜ(¬∑)
trained on Tseen can generalize to novel task instances in
Tunseen and produce a policy that can complete the novel
task illustrated by the visual demonstration.

4. Neural Task Graph Networks

We have formulated one-shot visual imitation as learn-
ing the model œÜ(¬∑) that maps a video demonstration to the
policy. As shown in Figure 1, our key contribution is ex-
plicitly incorporating compositionality to improve the data
efÔ¨Åciency of generalization. We decompose œÜ(¬∑) into two
components: a graph generator œÜgen(¬∑) that generates the
task graph G from the demonstration (G = œÜgen(d)), and a
graph execution engine œÜexe(¬∑) that executes the task graph
and acts as the policy (œÄd = œÜexe(G)). The structure of the
task graph G modularizes both the demonstration and the
policy. This leads to stronger data efÔ¨Åciency of generaliz-
ing to unseen tasks. An overview is shown in Figure 3.

4.1. Neural Task Graph Generator

The NTG Generator generates a task graph capturing the
structure of an unseen task from a single video demonstra-

tion. This is challenging since the video demonstration of
an unseen task introduces novel visual states that are not ob-
served in the seen tasks. This challenge is ampliÔ¨Åed by our
goal of learning from visual observation, which prevents
direct state space decomposition. In this case, generating
the traditional task graph is ill-posed due to the exploding
number of nodes. We address this by leveraging the con-
jugate relationship between state and action and work with
the conjugate task graph [16], where the nodes are the ac-
tions, and the edges implicitly depend on the current state.
In the experiments, we show that this scheme signiÔ¨Åcantly
simpliÔ¨Åes the (conjugate) task graph generation problem.

Conjugate Task Graph (CTG). A task graph ¬ØG = { ¬ØV , ¬ØE}
contains nodes ¬ØV as the states and ¬ØE the directed edges for
the transitions or actions between them. A successful exe-
cution of the task is equivalent to a path in the graph that
reaches the goal node. The task graph captures the structure
of the task, and the effect of each action. However, generat-
ing this graph for an unseen task is extremely challenging,
as each unseen state would be mapped to a new node. This
is especially the case in visual tasks, where the state space
is high dimensional. We thus work with the conjugate task
graph(CTG) [16], G = {V, E}, where the actions are now
the nodes V , and the states become edges E, which implic-
itly encode the preconditions of the actions. This allows us
to bypass explicit state modeling, while still being able to
perform tasks by traversing the conjugate task graph.

We assume that all actions are observed during training
from the seen tasks, which is reasonable for tasks in the
same domain. This gives all the nodes in CTG, and the
goal is to infer the correct edges. This can be viewed as un-
derstanding the preconditions for each action. We propose
two steps for generating the edges: (i) Demo Interpretation:
First we obtain a valid path traversing the conjugate task
graph by observing the action order in the demonstration;
(ii) Graph Completion: The second step is to add the edges
that are not observed in the demonstration. There might be
actions whose order can be permutated without affecting the
Ô¨Ånal outcome. As we only have a single demonstration, this
interchangeability is not captured in the previous step. We

8567

learn a Graph Completion Network, which adds more edges
that are proper given the edges initialized by step (i).
Demo Interpreter. Given d = [o1, . . . , oT ], our goal is to
output A = [a1, . . . , aK], the sequence of the actions exe-
cuted in the demonstration as the initial edges in the CTG
as shown in Figure 4. The visual observations ot are Ô¨Årst
encoded by a CNN as Enc(ot). We then adapt a seq2seq
model [28] as our demo interpreter to take Enc(ot) as in-
puts and generate A. We do not use a frame-based classi-
Ô¨Åer, as we do not need accurate per-frame action classiÔ¨Åca-
tion. What is critical here is that the sequence of actions A
provides reasonable initial action order constraints (edges)
to our conjugate task graph. We do assume the training
demonstrations in Tseen come with the action sequence A
as supervision for our demo interpreter. We only require
this ‚ÄúÔ¨Çat‚Äù supervision for Tseen, as opposed to the strong
hierarchical supervision used in the previous work [41].

Graph Completion Network (GCN). Given a valid path
(action sequence) from the demo interpreter, the goal is to
complete the edges that are not observed in the demo. We
formulate this as learning graph state transitions [19, 22].
Our GCN iterates between two steps: (i) edge update and
(ii) propagation. Given the node embedding N Egcn(ni) for
each node ni, the edge strengths are updated as:

Ct+1
ij = (1 ‚àí Ct

ij) ¬∑ fset(N t

i , N t

j ) + Ct

ij ¬∑ freset(N t

i , N t

j ),

(1)

where Ct
ij is the adjacency matrix of the previous iteration,
fset and freset are MLPs for setting and resetting the edge,
and Ni = N Egcn(ni) is the node embedding for node i.
Given Ct and the current node embeddings N t, the propa-
gation step updates the node embeddings with:

N t+1

i = rnn(ai, N t

i ), ai = X

j

Ct
ij ff (N t

j ) + Ct

jifb(N t

j ),

(2)

where rnn(ai, N t
as input and updates the hidden state N t

i ) takes the message ai from other nodes

i to N t+1

.

i

4.2. Neural Task Graph Execution

We have discussed how the NTG generates a CTG as the
compositional representation of a task demonstration. Next
we show how to instantiate a policy from this task graph.
We propose the NTG execution engine that interacts with
the environment by executing the task graph. The execu-
tion engine executes a task graph in two steps: (i) Node
Localization: The execution engine Ô¨Årst localizes the cur-
rent node in the graph based on the visual observation. (ii)
Edge ClassiÔ¨Åcation: For a given node, there can be multiple
outgoing edges for transitions to different actions. The edge
classiÔ¨Åer checks the (latent) preconditions of each possible
next action and picks the most Ô¨Åtting one. These two steps
enable the execution engine to use the generated Conjugate
Task Graph as a reactive policy which completes the task
given observations. Formally, we decompose this policy as:

1
o
m
e
D

2
o
m
e
D

ùê¥%

start

A	->	B

C	->	D

ùëë%

Learn Graph

Completion Network

A	->	B

ùê¥&

start

C	->	D

A	->	B

start

ùëë&

Learn Demo Interpreter

(a) Learning Graph Generation

start

C	->	D

A	->	B

Conjugate
Task Graph

(ùê∫)

C	->	D

Learn Node

Localizer

‚Ä¶

‚Ä¶

‚Ä¶

start

A	->	B

start

C	->	D

v.s.

Learn Edge

Classifier

(b) Learning Graph Execution

Figure 4. Illustration of our learning setting with a block stacking
task. The video demonstrations di in the seen tasks only require
corresponding action sequence Ai. We aggregate data from all the
demonstrations in the same task and use it as the supervision of
each component of our model. This approach allows us to bypass
the need for strong supervision as in previous works.

œÄ(a|o) ‚àù «´(a|n, o)‚Ñì(n|o), where the localizer ‚Ñì(n|o) local-
izes the current node n based on visual observation o, and
the edge classiÔ¨Åer «´(a|n, o) classiÔ¨Åes which edge transition
from n and o. Deciding the edge transition given the node
is equivalent to selecting the next action a.

Node Localizer. We deÔ¨Åne the localizer as: ‚Ñì(n|o) ‚àù
Enc(o)T N Eloc(n), where the probability of a node is pro-
portional to the inner product between Enc(o), the encoded
visual observation, and N Eloc(n), the node embedding of
the node. Since our nodes are actions that are already ob-
served in the seen tasks, we can learn the node embeddings
effectively. This shows the beneÔ¨Åt of modularizing our pol-
icy, where sub-modules are more generalizable.

Edge ClassiÔ¨Åer. The edge classiÔ¨Åer is the key for NTG
to generalize to unseen tasks. Unlike the localizer, which is
approx. invariant across seen and unseen tasks, deciding the
correct edge requires the edge classiÔ¨Åer to correctly infer the
underlying states from the visual observations. Take block
stacking as an example. For a task that aims to stack blocks
A, B, and C in order, the robot should not pick-and-place C
unless B is already on A. The edge classiÔ¨Åer thus needs to
recognize such prerequisites for actions involving block C.

«´(a|n, o) ‚àù (W«´[Enc(o), N Egcn(n)])T N Eloc(na),

(3)

where na is the node for action a, and N Egcn(¬∑) is the Ô¨Ånal
node embedding from our GCN in Section 4.1. As the GCN
node embedding is used to generate edges in the conjugate

8568

Conjugate
Task Graph

Move B

Pick B

Move F

Place D

‚Ä¶

‚Ä¶

Action

Move B

Pick B

Move F

Visual

Observation

‚Ä¶

Action

Move B

Place D

Complete

(a) Block Stacking Results with Full State

Visual

Observation

‚Ä¶

Figure 5. Execution of NTG based on the conjugate task graph.
Although the execution engine visited the (Move B) node twice,
it is able to correctly decide the next action using the edge classiÔ¨Åer
by understanding the second visit needs to (Place D).

task graph, it captures the task structure. We use N Eloc
from localization for the destination node.

4.3. Learning NTG Networks

We have described how we decompose œÜ(¬∑) into the gen-
erator and the execution engine. As discussed in Section 3
we train both on Tseen. In contrast to previous works that
require strong supervision on Tseen (state-action pairs [8] or
hierarchical supervision [41]), NTG only requires the raw
visual observation along with the Ô¨Çat action sequence (low-
est level program in [41] without a manually deÔ¨Åned action
hierarchy). An overview of learning different components
of NTG is shown in Figure 4.
Learning Graph Generation. For each demonstration dœÑ
i
of task œÑ , we have the corresponding AœÑ
i = [a1, . . . , aK],
the executed actions. First, we translate Ai to a path {P œÑ
i =
( ÀúV , ÀúEœÑ
i )} by using all actions as nodes ÀúV and adding edges
of the transitions in Ai to ÀúEi. For a single task œÑ , we use
the union of all demonstrated paths of œÑ as the edges Et =
Si
i of the groud truth conjugate task graph gœÑ = (V, Et).
In this case, the goal of GCN is to transform each P œÑ
to gœÑ
i
by completing the missing edges in P œÑ
i . We use the binary
cross entropy loss following [19] to train the GCN, where
the input is P œÑ

i and the goal is to generate gœÑ .

ÀúEœÑ

Learning Graph Execution. Given a task graph from the
generator, we learn an execution engine that derives the pol-
icy. As discussed in Section 4.2, we decompose the policy
into node localizer and edge classiÔ¨Åer. For the localizer, we
use the video frames as input and the corresponding action
labels from the demonstrations as targets. For the edge clas-
siÔ¨Åer, we collect all pairs of source-target nodes connected
by transitions, and use the action label from the demonstra-

/

A
N

/

A
N

(b) Block Stacking Results with Visual State

Figure 6. Results for generalizing block stacking to unseen target
conÔ¨Åguration. (a) Results with the block locations as input, and
(b) Results with raw video as input. Our NTG model signiÔ¨Åcantly
outperforms the baselines despite using only Ô¨Çat supervision.

tion as the target. Additionally, the edge classiÔ¨Åer uses the
node embedding from our Graph Completion Network. The
idea is that the embedding from the GCN can inform the
edge classiÔ¨Åer about what kind of visual state it should clas-
sify and learn to generalize to the unseen task.

5. Experiments

Our experiments aim to answer the following questions:
(1) With a single video demonstration, how does NTG gen-
eralize to unseen tasks and compare to baselines without us-
ing compositionality? (2) How do each of the components
of NTG contributes to its performance? (3) Is NTG appli-
cable to real-world data? For the Ô¨Årst two questions, we
evaluate and perform ablation study of NTG in two chal-
lenging task domains:
the Block Stacking [41] using the
BulletPhysics [6] and the Object Collection task in the AI2-
THOR [46]. For the last question, we evaluate NTG on real-
world surgical data and examine its graph prediction and
evaluation of unseen tasks on the JIGSAWS [12] dataset.

5.1. Evaluating Block Stacking in BulletPhysics

We evaluate NTG‚Äôs generalization to unseen target con-
Ô¨Ågurations. The hierarchical structure of block stacking
provides a large number of unique tasks and is ideal for ana-

8569

(a) Block Stacking Ablation Study

(b) Object Sorting Ablation Study

Figure 7. Ablation study of NTG. (a) Demo Int. and Node Loc.
are almost indispensable. (b) Both GCN and Edge Cls are required
to generalize to execution order different from the demonstration.

lyzing the effect of explicitly introducing compositionality.

Experimental Setup. The goal of Block Stacking is to
stack the blocks into a target conÔ¨Åguration. We follow the
setup in Xu et al. [41]. We use eight 5 cm cubes with dif-
ferent colors and lettered IDs. A task is considered success-
ful if the end conÔ¨Åguration matches the task demonstration.
We use the 2000 distinct Block Stacking tasks and follow
the training/testing split of Xu et al. [41].

Baselines. We compare to the following models:
- Neural Task Programming (NTP) [41] learns to synthe-
size policy from demonstration by decomposing a demon-
stration recursively. In contrast to ours, NTP assumes strong
structural supervision: both the program hierarchy and the
demonstration decomposition are required at training. We
use NTP as an example of methods that encourage compo-
sitionality via strong structural supervision.
- NTP Flat is an ablation of NTP, which only uses the same
supervision as our NTG model (lowest level program).
- NTP (Detector) Ô¨Årst detects the block and feeds that into
the model as the approximated full state. The detector is
trained separately with additional supervision.

Results. Results are shown in Figure 6. The x-axis is the
number of training seen tasks. We compare models with
full state (State) and visual state (Vid) as input. Full state
uses the 3D block location, and the visual state uses 64 √ó 64
RGB frames. For both input modalities, NTG can capture
the structure of the tasks and generalize better to unseen tar-
get conÔ¨Åguration compared to the baseline. NTG with raw
visual input (Ours (Vid)) performs on-par with NTP using
full state (NTP (State)). When there is not enough training
data (50 tasks), the NTP (State) and NTP (Detector) in are
able to outperform NTG because of the extra supervision
(hierarchical for NTP (State), and detection for NTP (De-

Demonstrated Path

P(0.05)

P(0.03)

P(0.07)

Pick_Place

Pick_Place

Pick_Place

Pick_Place

(Cyan, Blue)

(Cyan, Blue)

(Cyan, Blue)

(Cyan, Blue)

P(0.40)

P(0.02)

P(0.06)

Pick_Place

Pick_Place

Pick_Place

Pick_Place

(Blue, Cyan)

(Blue, Cyan)

(Blue, Cyan)

(Blue, Cyan)

Pick_Place
(Grn, Grn)

Pick_Place
(Red, Red)

T = 1

P(0.23)
Pick_Place
(Grn, Grn)

P(0.32)

Pick_Place
(Red, Red)

T = 2

P(0.62)

Pick_Place
(Grn, Grn)

P(0.33)
Pick_Place
(Red, Red)

P(0.06)

Pick_Place
(Grn, Grn)

P(0.81)
Pick_Place
(Red, Red)

T = 3

T = 4

Executed Path

Figure 8. Using GCN, our policy is able to solve an unseen sorting
task in a different order than the provided demonstration.

tector)). However, once NTG is trained with more than 100
tasks, it is able to quickly interpret novel tasks and signiÔ¨Å-
cantly outperforms the baselines. Figure 5 shows an NTG
execution trace. Although the execution engine visited the
(Move B) node twice, it is able to correctly decide the
next action based on the visual observation by interpreting
the underlying state from the visual observation.

5.2. Ablation Analysis of NTG Model Components

Before evaluating other environments, we analyze the
importance of each component of our model. Some sub-
systems are almost indispensable. For example, without the
Demo Interpreter, there is no information from the video
demonstration, and the policy is no longer task-conditional.
We perform the ablation study using 1000 training tasks
as follows: For Demo Interpreter, we initialize CTG as a
fully connected graph without order constraints from the
demonstration. For Node Localizer and Edge ClassiÔ¨Åer,
we replace the corresponding term in the policy œÄ(a|o) ‚àù
«´(a|n, o)‚Ñì(n|o) by a constant. For GCN, we skip the graph
completion step. As shown in Figure 7(a), the policy cannot
complete any of the tasks without Demo Interpreter or Node
Localizer. While our full model still performs the best, re-
moving Edge ClassiÔ¨Åer or GCN does not give as big a per-
formance gap. This is because the Block Stacking tasks
from [41] do not all require task structure understanding.
Alternate Solutions for Task. GCN is particularly im-
portant for situations requiring alternative execution orders.
For example, the task of ‚Äúputting the red ball into the red
bin and the blue ball into the blue bin‚Äù. It is obvious to us
that we can either put the red ball Ô¨Årst or the blue ball Ô¨Årst.

8570

o
m
e
D

s
r
u
O

t

l

a
F

Search Potato

Pickup Potato

Place Table

Search Bowl

Pickup Bowl

Place Table

Action

Visual

Observation

Search Potato

Pickup Potato

Pickup Potato

Task Description:

Find Potato and Put on the Table and

Find Bowl and Put on the Table

(a) Example Task Execution Comparing Ours to Flat Policy

(b) Object Collection Results

Figure 9. (a) Object Collection results. The bnding boxes are only for visualization and is not used anywhere in our model. The objects can
appear in locations that are different from the demonstration, which leads to challenging and diverse visual state. NTG is able to understand
the underlying state (e.g. if the object is found) from the visual input and successfully complete the task. (b) Object Collection results on
varying numbers of steps. The NTG model is only trained with 6 and 12 steps, and is able to generalize well to other numbers of steps.

This ability to generalize to alternative execution orders is
exactly what we aim to capture with GCN. Without GCN,
the policy can be easily stuck at unseen execution order (i.e.,
not understanding object sorting order can be swapped). We
thus analyze GCN on the ‚ÄúObject Sorting‚Äù task (details in
Section VI of [41]), but initialize the scene to require execu-
tion order different from the demonstration. These settings
will occur often when the policy needs to recover from fail-
ure or complete a partially completed tasks. This is chal-
lenging because: (i) GCN has to generalize and introduce
alternative execution order beyond the demonstration. (ii)
Edge ClassiÔ¨Åer needs to correctly select the action from the
newly introduced edges by GCN. As shown in Figure 7(b),
the policy cannot complete any of the tasks without Edge
ClassiÔ¨Åer because of the ambiguities in the completed task
graph. Figure 8 shows an qualitative example of how our
method learns to complete ‚ÄúObject Sorting‚Äù with order dif-
ferent from the demonstration using GCN. This shows the
importance of both the Edge ClassiÔ¨Åer and GCN, which are
required to complete this challenging task.

5.3. Evaluating Object Collection in AI2 THOR

In this experiment, we evaluate the Object Collection
task, in which an agent collects and drop off objects from
a wide range of locations with varying visual appearances.
We use AI2-THOR [46] as the environment, which allows
the agent to navigate and interact with objects via seman-
tic actions (e.g. Open). This task is more complicated than
block stacking because: First, the agent is navigating in the
scene and thus can only have partial observations. Second,
the photo-realistic simulation enables a variety of visual ap-
pearance composition. In order to complete the task, the
model needs to understand various appearances of the ob-
ject and location combinations.

Experimental Setup. An Object Collection task involves
visiting M randomly selected searching locations for a
set of N target objects out of C categories. Upon pick-
ing up a target object, the agent visits and drops off the
object at one of K designated drop-off receptacles. A
task is considered successful if all of the target objects
are placed at their designated receptacles at the end of the
task episode. The available semantic actions are search,
pickup(object), dropoff(receptacle).
The
search action visits each searching locations in a random-
ized order. pickup(object) picks up a selected object
and the action would fail if the selected object is not visible
to the agent. dropoff(receptacle) would teleport
the agent to a selected drop-off receptacle (tabletop,
cabinet, etc) and drop off. We use N = [1, 5] objects
(3-15 steps) out of C = 8 categories, M = N + 3 search
locations, and K = 5 drop-off receptacles.

Baseline. We compare to the ‚ÄúFlat Policy‚Äù baseline in [8]
to show the importance of incorporating compositionality
to the policy. At each step, the Flat Policy uses attention
to extract relevant information from the demonstration and
combine it with the observation to decide action. For a fair
comparison, we implement the Flat Policy using the same
architecture as our demo interpreter. Note that the Object
Collection domain doesn‚Äôt have hand-designed hierarchy.
Hence NTP [41] is reduced to a similar Ô¨Çat policy model.

Results. The results for Object Collection are shown in Fig-
ure 9(b). The models are only trained on 2 and 4 objects
and generalize to 1, 3, 5 objects. NTG signiÔ¨Åcantly outper-
forms the Flat Policy on all numbers of objects. This shows
the importance of explicitly incorporating compositionality.
Qualitative comparison is shown in Figure 9(a). The bound-
ing boxes are for visualization only and are not used in the
model. During evaluation, the objects of interest can appear

8571

Video

Predicted

Path

Predicted

Graph

Orienting	 Needle

Positioning	 Needle

Pushing	 Needle	
through	 Tissue

Pulling	 Suture	 with	

Left	Hand

Orienting	 Needle

Positioning	 Needle

Pushing	 Needle	
through	 Tissue

Pulling	 Suture	 with	

Left	Hand

Figure 10. Part of a predicted graph from a single demonstration of an unseen task on the JIGSAWS dataset. Our method is able to learn
that for the Needle Passing task, after failing any of the step in this subtask, the agent should restart by reorienting the needle.

Figure 11. Negative loglikehood (NLL) of expert demonstrations
on the JIGSAWS dataset. The policy generated by our full model
can best capture the actions performed in human demonstration.

in locations that are different from the demonstration and
thus lead to diverse and challenging visual appearances. It
is thus important to understand the structure of the demon-
stration instead of naive appearance matching. Our explicit
model of the task structure sets NTG apart from the Ô¨Çat pol-
icy and leads to stronger generalization to unseen tasks.

5.4. Evaluating Real world Surgical Data

We have shown that NTG signiÔ¨Åcantly improves one-
shot visual imitation learning by explicitly incorporating
compositionality. We now evaluate if this structural ap-
proach can be extended to the challenging real-world sur-
gical data from the JIGSAWS dataset [12], which con-
tains videos and states for surgical tasks, and the associated
atomic action labeling. In this setting, our goal is to assess
NTG‚Äôs ability to generalize to the task of ‚ÄúNeedle Passing‚Äù,
after training on the tasks of ‚ÄúKnot Tying‚Äù and ‚ÄúSuturing‚Äù.
This is especially challenging because it requires general-
ization to a new task with signiÔ¨Åcant structural and visual
differences, given only 2 task types for training.

Without a surgical environment, we cannot directly eval-
uate the policy learned by NTG on the JIGSAWS dataset.
Therefore, we evaluate how well the NTG policy is able to
predict what a human will do in other demonstrations. This
entails generating a policy conditioned on a single demon-
stration of ‚ÄúNeedle passing‚Äù, and using it to evaluate the
negative log-likelihood (NLL) of all the other demonstra-
tions in the ‚ÄúNeedle Passing‚Äù task. A lower negative log
likelihood corresponds to the generated policy better ex-
plaining the other demonstrations, and in turn better cap-

turing the task structure.

The results are shown in Figure 11. We compare to the
no graph variant of our model and also the lower bound
of a uniform policy. Unsurprisingly, the uniform policy
performs the worst without capturing anything from the
demonstration. The no-graph variant is able to capture
some parts of the expert policy and better capture the ex-
pert demonstration. However, the policy generated by full
NTG model substantially improves the NLL and is the most
consistent with the expert demonstration.

In addition, we show qualitative results of part of our
task graph prediction on the JIGSAWS dataset in Figure 10.
Again, we train on ‚ÄúKnot Tying‚Äù and ‚ÄúSuturing‚Äù and evalu-
ate on ‚ÄúNeedle Passing‚Äù. By comparing the predicted path
and the Ô¨Ånal predicted graph, we can see that our model is
able to introduce several new edges going back to the action
‚ÄúOrienting Needle‚Äù. This captures the behavior that when
the execution fails in any of step in this subtask of ‚ÄúNeedle
Passing‚Äù, the agent should return to ‚ÄúOrienting Needle‚Äù and
reorient the needle to restart the subtask. This is consistent
with our intuition and the ground truth graph.

6. Conclusion

We presented Neural Task Graph (NTG) Network, a
one-shot visual imitation learning method that explicitly in-
corporates task compositionality into both the intermediate
task representation and the policy. Our novel Conjugate
Task Graph (CTG) formulation effectively handles unseen
visual states and serves as a reactive and executable pol-
icy. We demonstrate that NTG is able to outperform both
methods with unstructured representation [8], and meth-
ods with a hand-designed hierarchical structure [41] on a
diverse set of tasks, including simulated environment with
photo-realistic rendering and a real-world dataset.

Acknowledgements. Toyota Research Institute (‚ÄúTRI‚Äù)
provided funds to assist the authors with their research but
this article solely reÔ¨Çects the opinions and conclusions of
its authors and not TRI or any other Toyota entity. This
research was also sponsored in part by the NSF graduate
research fellowship.

8572

References

[1] Baris Akgun, Maya Cakmak, Karl Jiang, and Andrea L
Thomaz. Keyframe-based learning from demonstration. In-
ternational Journal of Social Robotics, 4(4):343‚Äì355, 2012.

[2] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,
Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Unsu-
pervised learning from narrated instruction videos. In CVPR,
2016.

[3] Jacob Andreas, Dan Klein, and Sergey Levine. Modular mul-
titask reinforcement learning with policy sketches. In ICML,
2017.

[4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan
Klein. Deep compositional question answering with neural
module networks. In CVPR, 2016.

[5] Rodney Brooks. A robust layered control system for a mo-
bile robot. IEEE journal on robotics and automation, 1986.

[6] Erwin Coumans and Yunfei Bai. pybullet, a python module
for physics simulation, games, robotics and machine learn-
ing. http://pybullet.org/, 2016‚Äì2017.

[7] Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter
Abbeel, and Sergey Levine.
Learning Modular Neural
Network Policies for Multi-Task and Multi-Robot Transfer.
arXiv preprint arXiv:1609.07088, 2017.

[8] Yan Duan, Marcin Andrychowicz, Bradly C. Stadie,
Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel,
and Wojciech Zaremba. One-Shot Imitation Learning.
In
NIPS, 2017.

[9] Richard E Fikes, Peter E Hart, and Nils J Nilsson. Learning
and executing generalized robot plans. ArtiÔ¨Åcial Intelligence,
1972.

[10] Richard E Fikes and Nils J Nilsson. Strips: A new approach
to the application of theorem proving to problem solving.
ArtiÔ¨Åcial intelligence, 2(3-4):189‚Äì208, 1971.

[11] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and
Sergey Levine. One-shot visual imitation learning via meta-
learning. In CoRL, 2017.

[12] Yixin Gao, S. Swaroop Vedula, Carol E. Reiley, Narges
Ahmidi, Balakrishnan Varadarajan, Henry C. Lin, Lingling
Tao, Luca Zappella, Benjamn B¬¥ejar, David D. Yuh, Chi Chi-
ung Grace Chen, Ren¬¥e Vidal, Sanjeev Khudanpur, and Gre-
gory D. Hager. Jhu-isi gesture and skill assessment working
set ( jigsaws ) : A surgical activity dataset for human motion
modeling. 2014.

[13] Wonjoon Goo and Scott Niekum. One-shot learning of multi-
step tasks from observation via activity localization in auxil-
iary video. arXiv preprint arXiv:1806.11244, 2018.

[14] Abhinav Gupta, Praveen Srinivasan, Jianbo Shi, and Larry S
Davis. Understanding videos, constructing plots learning a
visually grounded storyline model from annotated videos. In
CVPR, 2009.

[17] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor
Darrell, and Kate Saenko. Learning to reason: End-to-end
module networks for visual question answering.
In ICCV,
2017.

[18] De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg,
Li Fei-Fei, and Juan Carlos Niebles. Finding it: Weakly-
supervised reference-aware visual grounding in instructional
videos. CVPR, 2018.

[19] Daniel D Johnson. Learning graphical state transitions. In

ICLR, 2017.

[20] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Inferring and executing programs for visual rea-
soning. In ICCV, 2017.

[21] Leslie Pack Kaelbling and Tom¬¥as Lozano-P¬¥erez. Hierarchi-

cal task and motion planning in the now. In ICRA, 2011.

[22] Thomas N Kipf and Max Welling. Semi-supervised classiÔ¨Å-

cation with graph convolutional networks. In ICLR, 2017.

[23] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu,
Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive
3D Environment for Visual AI. arXiv, 2017.

[24] Satwik Kottur, Jos¬¥e MF Moura, Devi Parikh, Dhruv Batra,
and Marcus Rohrbach. Visual coreference resolution in vi-
sual dialog using neural module networks. In ECCV, 2018.
[25] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
ICCV, 2017.

[26] Changsong Liu, Shaohua Yang, Sari Saba-Sadiya, Nishant
Shukla, Yunzhong He, Song-Chun Zhu, and Joyce Chai.
Jointly learning grounded task structures from language in-
struction and visual demonstration. In EMNLP, 2016.

[27] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey
Levine. Imitation from observation: Learning to imitate be-
haviors from raw video via context translation.
In ICRA,
2018.

[28] Minh-Thang Luong, Hieu Pham, and Christopher D Man-
ning. Effective approaches to attention-based neural machine
translation. In EMNLP, 2015.

[29] Dana Nau, Yue Cao, Amnon Lotem, and Hector Munoz-
Avila. Shop: Simple hierarchical ordered planner. In IJCAI,
1999.

[30] Monica N Nicolescu and Maja J Matari¬¥c. A hierarchical ar-
chitecture for behavior-based robots. In Proceedings of the
First International Joint Conference on Autonomous Agents
and Multiagent Systems, pages 227‚Äì233, 2002.

[31] Scott Niekum, Sarah Osentoski, George Konidaris, and An-
drew G Barto. Learning and generalization of complex tasks
from unstructured demonstrations. In IROS, 2012.

[32] Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park,
and Abhinav Gupta. The curious robot: Learning visual rep-
resentations via physical interactions. In ECCV, 2016.

[15] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Suk-
thankar, and Jitendra Malik. Cognitive mapping and plan-
ning for visual navigation. In CVPR, 2017.

[33] Earl D Sacerdoti. A structure for plans and behavior. Techni-
cal report, SRI International‚Äôs ArtiÔ¨Åcial Intelligence Center,
1975.

[16] Bradley Hayes and Brian Scassellati. Autonomously con-
structing hierarchical task networks for planning and human-
robot collaboration. In ICRA, 2016.

[34] Ozan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh
Saxena. Unsupervised semantic parsing of video collections.
In ICCV, 2015.

8573

[35] Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey
Self-supervised
arXiv preprint

Levine.
Time-contrastive networks:
learning from multi-view observation.
arXiv:1704.06888, 2017.

[36] Siddharth Srivastava, Eugene Fang, Lorenzo Riano, Ro-
han Chitnis, Stuart Russell, and Pieter Abbeel. Combined
task and motion planning through an extensible planner-
independent interface layer. In ICRA, 2014.

[37] Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen.
Book2movie: Aligning video scenes with book chapters. In
CVPR, 2015.

[38] Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler.
Nervenet: Learning structured policy with graph neural net-
works. In ICLR, 2018.

[39] David Whitney, Eric Rosen, Elizabeth Phillips, George
Konidaris, and Stefanie Tellex. Comparing Robot Grasping
Teleoperation across Desktop and Virtual Reality with ROS
Reality. In International Symposium on Robotics Research,
2017.

[40] Yan Wu and Yiannis Demiris. Towards one shot learning by

imitation for humanoid robots. In ICRA, 2010.

[41] Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg,
Li Fei-Fei, and Silvio Savarese. Neural task programming:
Learning to generalize across hierarchical tasks.
In ICRA,
2018.

[42] Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tian-
hao Zhang, Pieter Abbeel, and Sergey Levine. One-shot im-
itation from observing humans via domain-adaptive meta-
learning. RSS, 2018.

[43] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Ken
Goldberg, and Pieter Abbeel. Deep imitation learning for
complex manipulation tasks from virtual reality teleopera-
tion. arXiv preprint arXiv:1710.04615, 2017.

[44] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-
Fei, Abhinav Gupta, Roozbeh Mottaghi, and Ali Farhadi. Vi-
sual semantic planning using deep successor representations.
In ICCV, 2017.

[45] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,
Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Align-
ing books and movies: Towards story-like visual explana-
tions by watching movies and reading books. In ICCV, 2015.
[46] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Ab-
hinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven vi-
sual navigation in indoor scenes using deep reinforcement
learning. In ICRA, 2017.

8574

