Generating Multiple Hypotheses for 3D Human Pose Estimation

with Mixture Density Network

Chen Li

Gim Hee Lee

Department of Computer Science, National University of Singapore

{lic, gimhee.lee}@comp.nus.edu.sg

Abstract

3D human pose estimation from a monocular image or
2D joints is an ill-posed problem because of depth ambi-
guity and occluded joints. We argue that 3D human pose
estimation from a monocular input is an inverse problem
where multiple feasible solutions can exist. In this paper,
we propose a novel approach to generate multiple feasible
hypotheses of the 3D pose from 2D joints. In contrast to
existing deep learning approaches which minimize a mean
square error based on an unimodal Gaussian distribution,
our method is able to generate multiple feasible hypothe-
ses of 3D pose based on a multimodal mixture density net-
works. Our experiments show that the 3D poses estimated
by our approach from an input of 2D joints are consistent
in 2D reprojections, which supports our argument that mul-
tiple solutions exist for the 2D-to-3D inverse problem. Fur-
thermore, we show state-of-the-art performance on the Hu-
man3.6M dataset in both best hypothesis and multi-view
settings, and we demonstrate the generalization capacity
of our model by testing on the MPII and MPI-INF-3DHP
datasets. Our code is available at the project website1.

1. Introduction

3D human pose estimation from a single RGB image is
an extensively studied problem in computer vision because
of many potential useful real world applications such as
forensic science, sports analysis and surveillance etc. Sig-
niﬁcant progress in 3D human pose estimation has been
made with deep learning in the recent years. One of the
commonly used and effective deep learning based methods
for 3D human pose estimation is the two-stage approach,
where the 2D joints are ﬁrst detected from the image input
[18, 24] followed by the 3D joint estimations from the de-
tected 2D joints [1, 29, 4, 15, 10, 6, 25, 17]. The advantage

1https://github.com/chaneyddtt/Generating-

Multiple-Hypotheses-for-3D-Human-Pose-
Estimation-with-Mixture-Density-Network

Figure 1: An example of multiple feasible 3D pose hypothe-
ses generated from our network reprojecting into similar 2D
joint locations. (Best view in color)

of the two-stage approach is that it decouples the harder
problem of 3D depth estimation from the easier 2D pose
estimation. In particular, variations in background scene,
lighting, clothing shape, skin color etc. are removed before
the 3D joint estimation stage. Furthermore, the model can
be trained on different domains, e.g. indoor and outdoor,
with 2D annotations that are readily available.

Despite the signiﬁcant progress with deep learning, 3D
human pose estimation remains as a very challenging task
due to the ambiguity in recovering 3D information from a
single RGB image. More speciﬁcally, recovering 3D infor-
mation from a single RGB image or 2D joint locations is
an inverse problem [3] where multiple solutions may ex-
ist for the depth of a 3D joint along the light ray that re-
projects onto the same 2D joint location, as illustrated in
Figure 1. The problem is further aggravated by the non-
rigidity of the human pose and joint occlusions on the 2D
image. Consequently, there could be many solutions of the
3D pose that satisfy the same 2D pose on an image, even
after eliminating the infeasible 3D pose solutions by en-

9887

yxzyxzHypothesis 1Hypothesis 4yxzHypothesis 2yxzHypothesis 32D ReprojectionsxyyxzHypothesis 5forcing various geometric constraints, e.g. joint limits [1]
and bone ratio [27] etc. In view of the inherent ambiguity
of the 3D human pose estimation problem, we argue that it
is more reasonable to design a model that generates mul-
tiple hypotheses of geometrically feasible 3D human pose
that are consistent with the detected 2D joints from a single
RGB image. In contrast, the widely adopted single estimate
for the inverse problem with inherent ambiguity could lead
to overﬁtting the model to the training data, and might not
generalize well. This idea of generating multiple 3D pose
hypotheses was ﬁrst suggested very recently by Jahangiri
and Yuille in [12].

To this end, we introduce the mixture density networks
(MDN) [3, 26] to the 3D joint estimation module of the
two-stage approach. Contrary to most existing works that
generate a single 3D pose by minimizing the negative log-
likelihood of an unimodal Gaussian, i.e. a mean squared er-
ror, we propose to estimate multiple hypotheses of the 3D
pose by minimizing the negative log-likelihood of a mul-
timodal mixture-of-Gaussians. The outputs of our mixture
model is a set of mixing coefﬁcients and parameters of the
Gaussian kernels, i.e. means and variances. The set of 3D
pose hypotheses are given by the means of the Gaussian
kernels, and the mixing coefﬁcient and variances represent
the uncertainties of each 3D pose hypothesis. Speciﬁcally
our network consists of a feature extractor to lift the 2D
joints into a feature space, and a hypotheses generator to
generate multiple hypotheses. The whole network is a sim-
ple network made up of several linear layers with different
non-linear activation units.

We show that our network achieves state-of-the-art re-
sults on the Human3.6M dataset [11] in both best hypoth-
esis and multi-view settings. We also report results of our
network on the outdoor MPII [2] dataset and the MPI-INF-
3DHP [16] dataset, where 3D pose labels are not used for
training the network. Furthermore, we show the robustness
of our network by applying it to scenarios where one or two
limb joints are occluded/missing. Our main contributions
are as follows:

• We explore the idea of generating multiple 3D pose
hypotheses to alleviate the ambiguity problem that has
not received much attention in the literature.

• To the best of our knowledge, we are the ﬁrst to intro-
duce the mixture density model into 3D human pose
estimation, which is more powerful than the single
Gaussian distribution.

• Our network achieves state-of-the art results on Hu-
man3.6M dataset in both best hypothesis and multi-
view settings, and in cases where one or two limb
joints are occluded/missing.

2. Related Work

Existing human 3D pose estimation approaches fall into
two categories according to their training techniques. The
ﬁrst category is to train deep convolutional neural networks
(CNNs) end-to-end to estimate 3D human poses directly
from the input images [20, 16, 28, 19, 27, 14, 23]. Zhou et
al. [28] use a sparse representation for 3D poses and predict
the 3D pose with an expectation-maximization (EM) algo-
rithm. The 2D poses are regarded as a hidden variable in
the EM algorithm to remove the need of synchronized 2D-
3D data. Park et al. [19] improve conventional CNNs by
concatenating 2D pose estimation as well as information on
relative positions with respect to multiple joints. Pavlakos et
al. [20] use volumetric representation to represent 3D poses
and adopt the stacked hourglass network [18], which is orig-
inally designed for 2D pose estimation, to predict 3D vol-
umetric heatmaps. Mehta et al. [16] use transfer learning
to transfer the knowledge learned for 2D pose estimation to
the task of 3D pose estimation. Similarly, Zhou et al. [27]
propose a weakly-supervised transfer learning method that
uses mixed 2D and 3D labels. The 2D pose estimation
sub-network and 3D depth regression sub-network share the
same features such that the 3D pose labels for indoor envi-
ronments can be transferred to in-the-wild images. The di-
rect approach beneﬁts from the rich information contained
in images, e.g. the front-back orientation of limbs. How-
ever, it will also be affected by a number of factors such as
background, lighting, clothing etc. A network trained on
one dataset can not be generalized well to the other datasets
with different environment, for example from indoor and
outdoor environment.

The second category [1, 29, 4, 15, 10, 6, 25, 17] de-
couples 3D pose estimation into the well-studied 2D joint
detection [18, 24] and 3D pose estimation from the de-
tected 2D joints. Akhter et al. [1] propose a multi-stage
approach to estimate the 3D pose from 2D joints using an
over-complete dictionary of poses. Bogo et al. [4] estimate
3D pose by ﬁrst ﬁtting a statistical body shape model to the
2D joints, and then minimizing the error between the repro-
jected 3D model and detected 2D joints. Chen [6] and Yasin
[25] regard 3D pose estimation as a matching between the
estimated 2D pose and the 3D pose from a large pose li-
brary. Martinez et al. [15] design a simple fully connected
residual network to regress 3D pose from 2D joint detec-
tions. The decoupled approach can make use of both in-
door and in-the-wild images to train the 2D pose estimators.
More importantly, this approach is domain invariant since
the input of the second stage is the 2D joints. However,
estimating 3D pose from 2D joints is more challenging be-
cause 2D pose data contains less information than images,
thus there are more ambiguities.

To solve the ill-posed problem of estimating 3D pose
from 2D joints, Jahangiri and Yuille [12] ﬁrst proposed

9888

…

#

$

%

2D	pose	estimator

Feature	extractor

Hypotheses	generator

Stage	one

Stage	two

=

⊕

⊕

Residual	block

Softmax

mElu

Linear

Hourglass

Components

Figure 2: Our network consists of a feature extractor and a 3D pose hypotheses generator, it generates multiple pose hypothe-
ses from the 2D joints detected by 2D pose estimator.

to generate multiple diverse pose hypotheses. They ﬁrst
learned a 3D Gaussian mixture model (GMM) model [22]
from a uniformly sampled set of Human3.6M poses, and
then use conditional sampling to get samples of the 3D
poses with reprojected joints errors that are within a thresh-
old. Inspired by their work, we solve the ambiguity prob-
lem by generating multiple hypotheses. Instead of using the
traditional GMM approach, we introduce the MDN which
was ﬁrst proposed in [3]. The MDN can represent arbitrary
conditional distributions by combining a conventional neu-
ral network with a mixture density model. Ye et al. [26]
used a hierarchical MDN to solve the occlusion problem in
hand pose estimation. Inspired by the work of Ye et al., we
use the MDN to solve the depth ambiguity and occlusion
problem in 3D human pose estimation.

3. Our Mixture Density Network

Figure 2 shows the illustration of our deep network to
generate multiple hypotheses for 3D human pose estima-
tion. Our network follows the commonly used two-stage
approach that ﬁrst estimates the 2D joints from the input
images followed by the 3D pose estimation from the esti-
mated 2D joints. We adopt the state-of-the-art stacked hour-
glass [18] network as the 2D joint estimation module, and
use our MDN which consists of a feature extractor and a
hypotheses generator to generate the multiple 3D pose hy-
potheses. Given the 2D joint detections x ∈ R2N , where
N is the number of joints in one pose, our goal is to learn
a function f : x 7→ Θ which maps x into a set of output
parameters Θ = {µ, σ, α} for our mixture model. µ =
{µ1, ..., µM | µi ∈ R3N }, σ = {σ1, ..., σM | σi ∈ R}

and α = {α1, ..., αM | 0 ≤ αi ≤ 1, Pi αi = 1} are

the means, variances and mixing coefﬁcients of the mixture
model. M is the number of Gaussian kernels. The mean of
each Guassian kernel µi ∈ µ represents one 3D pose hy-
pothesis, and the number of Gaussian kennels M decides
the number of hypotheses generated by our model.

3.1. Model Representation

The probability density of the 3D pose y ∈ R3N given
the 2D joints x ∈ R2N is represented as a linear combina-
tion of Gaussian kernel functions

p(y | x) =

αi(x)φi(y | x),

(1)

M

Xi=1

where M is the number of Gaussian kernels, i.e. the number
of hypotheses. αi(x) is the mixing coefﬁcients, which can
be regarded as a prior probability of a 3D pose data y being
generated from the ith Gaussian kernel given the input 2D
joints x. Here αi(x) must satisfy the constraint

0 ≤ αi(x) ≤ 1,

αi(x) = 1.

(2)

M

Xi=1

φi(y | x) is the conditional density of the 3D pose y for the
ith kernel, which can be expressed as a Gaussian distribu-
tion

φi(y | x) =

1

(2π)d/2σi(x)d exp −

ky − µi(x)k2

2σi(x)2

.

(3)

µi(x) and σi(x) denote the mean and variance of the ith ker-
nel, respectively. d is the dimension of the output 3D pose
y. All the parameters of the mixture model, including the
mixing coefﬁcients αi(x), the mean µi(x) and the variance
σi(x) are functions of the input 2D pose x.

Note that the mixture model degenerates to a single
Gaussion distribution when the means and variances of all
Gaussian kernels are similar, i.e. µi(x) ≈ µ(x), σi(x) ≈
σ(x) for i = 1, ..., M . Hence,

M

Xi=1
Xi=1

M

p(y | x) =

≈

αi(x)φi(y | x)

αi(x)N (µ(x), σ(x))

(4)

= N (µ(x), σ(x)).

9889

Speciﬁcally in our case, the 3D pose hypotheses generated
by the MDN will collapse into approximately a single Gaus-
sian when the given 2D pose is simple and less ambiguous,
e.g. no occlusions and/or missing joints.

3.2. Network Architecture

From Eqn. (1), (2) and (3), we can see that all param-
eters Θ(x) = {µ(x), σ(x), α(x)} of the Gaussian mixture
distribution of y are functional form of x. Hence, we learn
this function f : x 7→ Θ using a deep network which can be
expressed as

Θ = f (x; w),

(5)

where w is the set of learnable weights in the deep network.
The probability density in Eqn. (1) can be rewritten to in-
clude the learnable weights w of the deep network, i.e.,

p(y | x, w) =

M

Xi=1

where

αi(x, w)φi(y | x, w),

(6)

φi(y | x, w) =

1

(2π)d/2σi(x, w)d exp −

ky − µi(x, w)k2

2σi(x, w)2

.

(7)
The parameters Θ(x, w) = {µ(x, w), σ(x, w), α(x, w)} are
now dependent on the learnable weights w of the deep net-
work f (x; w).

We modify the 3D pose estimation module in [15] to
form our deep network f (x; w). More speciﬁcally, our ap-
proach is a simple multilayer neural network. Given an in-
put of 2D joints x ∈ R2N , we use one linear layer to map the
input into an 1024 dimensional feature space, followed by
two residual blocks which respectively consists of a linear
layer, batch normalization , dropout , and Rectiﬁed Linear
Units. And there are residual connections between the in-
put and output of each residual block. Different from [15]
which adds another linear layer to directly regress the 3D
pose y ∈ R3N from the feature space, our network estimates
the parameters Θ of the mixture model. In particular, we use
different activation functions to satisfy the constraints of the
three parameters Θ(x, w) = {µ(x, w), σ(x, w), α(x, w)}.
Speciﬁcally, we use a normal linear layer for parameter
µ(x, w), a softmax function for the mixture coefﬁcient
α(x, w) so that it lies in the range of [0, 1] and sums up
to 1, and a modiﬁed ELU function [7] deﬁned as:

h(t) =(t + 1,

γ(exp(t) − 1) + 1,

if t ≥ 0
otherwise

(8)

for the variance σ(x, w) to keep it positive. Here, γ is a
scale for negative factor.

3.3. Optimization

Given a training dataset with K pairs of ground truth la-
bels for the corresponding 2D joints X and 3D poses Y, i.e.
{X, Y} = {{xj, yj} | j = 1, ..., K}, the objective is to ﬁnd
the maximum a posterior of the set of learnable weights w.
More formally, assuming that each training data is indepen-
dent and identically distributed (i.i.d), the posterior distri-
bution of w is given by

p(w | X, Y, Ψ) ∝ p(Y | X, w)p(w | X, Ψ)

(9)

= p(w | X, Ψ)

= p(w | X, Ψ)

K

Yj=1
Yj=1

K

p(yj | xj, w)

αi(xj, w)φi(yj | xj, w),

M

Xi=1

where Ψ is the hyperparameter of the prior over the learn-
able weights w. Hence, the optimal weight w∗ can be ob-
tained from the minimization of the negative log-posterior

w∗ = argmin

− ln p(w | X, Y, Ψ)

(10)

w

= argmin

−

w

K

Xj=1

|

ln p(yj | xj, w) − ln p(w | X, Ψ)

,

L

{z

}

where L is taken to be the loss function for training our deep
network f (x; w). More speciﬁcally,

ln p(yj | xj, w) − ln p(w | X, Ψ)

(11)

ln

αi(xj, w)φi(yj | xj, w) − ln p(w | X, Ψ)

L = −

= −

K

Xj=1
Xj=1

K

M

Xi=1

= L3D + Lprior.

The prior loss Lprior can be further evaluated into:

Lprior = − ln p(w | X, Ψ)

(12)

= − ln p(w, X | Ψ) + ln p(X | Ψ)
∝ − ln p(Θ(w, X) | Ψ)
= − ln p(α(w, X) | Ψ) − ln p(µ(w, X), σ(w, X) | Ψ),

where the term ln p(X | Ψ) can be dropped in the loss func-
tion since it is independent of w, and we write the random
variables {w, X} in its functional form Θ(w, X) given by
the deep network. We further assume a uniform prior over
µ(w, X) and σ(w, X), and a Dirichlet conjugate prior over
the mixing coefﬁcients α(w, X) that follows a Categorical

9890

distribution, we get

4.1. Datasets and Protocols

Lprior = − ln p(α(w, X) | Λ)

(13)

= −

K

Xj=1

where

ln p(α1(w, xj), ..., αM (w, xj) | Λ),

p(α1, ..., αM | Λ) = Dir[α1,...,αM ][λ1, ..., λM ]

(14)

=

i=1 λi]
i=1 Γ[λi]

Γ[PM
QM

αi(w, xj)λi−1.

M

Yi=1

Γ[.] is the Gamma function, and Λ = {λ1, ..., λM } are the
hyperparameters of the Dirichlet distribution, where λi ∈

[0, 1] and Pi λi = 1. The total loss function to train our

deep network is given by

L = L3D + Lprior,

αi(xj, w)φi(yj | xj, w)

(15)

where

L3D = −

ln

K

Xj=1
Xj=1

K

M

Xi=1
Xi=1

M

Lprior = −

(λi − 1) ln αi(w, xj).

Note that we drop

dent of w.

Γ[PM
QM

i=1 λi]
i=1 Γ[λi]

in Lprior because it is indepen-

Remarks: The term Lprior regularizes the mixing coefﬁ-
cients of our mixture model. Setting λi = 1 for i = 1, .., M
implies that we have no prior knowledge over the mixing
coefﬁcients. In our experiments, we set λ1 = ... = λM =
C, where C > 1 is a constant scalar value to prevent overﬁt-
ting of a single Gaussian kernel in the MDN to the training
data, i.e. a single mixing coefﬁcient αi ≈ 1 and αj6=i ≈ 0.

4. Experiments

Our model is implemented in Tensorﬂow, and we use
the ADAM [13] optimizer with an initial learning rate of
0.001 and exponential decay. The batch size is set to 64 and
we initialize the weights of linear layers with the Kaiming
initialization [9]. The number of Gaussian kernels is set to 5
and the hyperparameters {λ1, ..., λM } in Eqn. (14) are set to
2. We train our network for 200 epoches with a dropout rate
of 0.5. We also apply max-norm constraint on the weight
of each layer so that it is in range [0, 1]. Moreover, we clip
the value of αi(x) to [1e−8, 1] and σi(x) to [1e−15, 1e15]
to prevent the training loss from becoming NaN. We also
use the log-sum-exp trick as previous work [5] to avoid the
underﬂow problem.

results

We show numerical

the Human3.6M
dataset [11] and compare with other state-of-the-art ap-
proaches. We also apply our approach to other datasets in-
cluding MPII [2] and MPI-INF-3DHP [16] datasets to test
the generalization capacity of our network.

for

Human3.6M dataset:
This is currently the largest avail-
able video pose dataset, which provides accurate 3D body
joint locations recorded by a Vicon motion capture system.
There are 15 activity scenarios in total such as “Walking”,
“Eating”, “Sitting” and “Discussion”, each action is per-
formed by 7 professional actors. Accurate 2D joint loca-
tions , 3D pose annotations and camera parameters are pro-
vided. Following [15], we apply standard normalization to
the 2D inputs and 3D outputs by subtracting the mean and
dividing by the standard deviation of the training data. We
also zero-center the 3D poses around the hip joint since we
do not predict the global position of the 3D pose.

MPII dataset: This is a standard dataset for 2D human
pose estimation, which contains 25K unconstrained images
collected from YouTube videos. This is the most challeng-
ing in-the-wild dataset and we use it to test the generaliza-
tion of our approach. We report qualitative result for this
datset because 3D pose information is not provided.

MPI-INF-3DHP dataset:
It is a newly released 3D hu-
man pose dataset which is captured by a Mocap system in
both indoor and outdoor scenes. We only use the test split
of this dataset that includes 2935 frames from six subjects
performing seven actions.

2D detections: We use the state-of-the-art stacked hour-
glass network [18] to get the 2D joint detections. The
stacked hourglass network is pretrained on the MPII dataset
and then ﬁne-tuned on the Human3.6M dataset.

Evaluation protocols: For the Human3.6M dataset, we
follow the standard protocol of using S1, S5, S6, S7 and
S8 for training, and S9 and S11 for testing. The evalua-
tion metric is the Mean Per Joint Position Error (MPJPE)
in millimeter between the ground truth and the estimated
3D pose. Since our network generating multiple hypothe-
ses for each 2D detection, we follow [12] to compute the
MPJPE between the ground truth and the best 3D hypothe-
sis generated by our network. The 3D Percentate of Correct
Keypoints (3DPCK) [16] is adopted as the metric for the
MPI-INF-3DHP dataset .

4.2. Results on Human3.6M dataset

We ﬁrst report our results on the Human3.6 dataset and
compare with other state-of-the-art approaches. From the
results shown in Table 1, we can see that our method outper-
forms the others in most cases. Our approach achieves an

9891

Table 1: Quantitative results of MPJPE in millimeter on Human3.6M under protocol # 1 and # 2. (Best result in bold)

Protocol #1
LinKDE et al. [11] 132.7 183.6 132.3 164.4 162.1 205.9 150.6 171.3 151.6
112.7 104.9 122.1 139.1 135.9 105.9 166.2 117.5
85.1
Du et al. [8]
87.1 103.2 116.2 143.3 106.9 99.8 124.5
109.3
Zhou et al. [28]
87.4
Pavlakos et al. [20] 67.4
71.9
66.7 69.1 72.0 77.0 65.0 68.3
83.7
58.1 64.5 68.7 61.3 55.6 86.1 117.6
55.9
Jahangiri et al. [12] 63.1
Zhou et al. [27]
75.2
58.2 71.4 62.0 65.5 53.8 55.6
60.7
54.8
Martinez et al. [15] 51.8
74.0
58.1 59.0 69.5 78.4 55.2 58.1
56.2
56.9
48.8 53.1 52.2 74.9 52.7 44.6
43.8
Lee et al. [14]
51.7
43.8
48.6
49.1 49.8 57.6 61.5 45.9 48.3
Ours
62.0
Protocol #2
88.4
Yasin et al. [25]
62.0
Bogo et al. [4]
Moreno et al. [17]
66.1
Martinez et al. [15] 39.5
Lee et al. [14]
37.4
35.5
Ours

Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.
162.1 170.7 177.1 96.6 127.9 162.1
120.0 117.7 137.4 99.3 106.5 126.5
97.7 113.0
107.4 118.1 114.2 79.4
71.7
59.1
63.2
71.9
68.0
61.0
62.5
71.2
64.9
55.3
63.2
64.1
62.9
52.4
49.5
62.3
55.8
45.6
68.4
56.7
54.8
43.4
45.5
52.7
Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.
92.1 165.7 102.0 110.1
82.3
86.8
71.5
74.0
47.7
49.5
33.2
45.7
42.6
44.1

108.5 110.2 97.1 142.5 81.6 107.2 119.0
67.8 76.5 92.1 77.0 73.0 75.3 100.3
84.5 73.7 65.2 67.2 60.9 67.3 103.5
56.5
46.4 47.0 51.0 56.0 41.4 40.6
53.0
45.6 43.8 48.5 54.6 39.9 39.2
41.3 42.3 46.0 48.9 36.9 37.3
51.0

108.2 86.9
77.3
83.4
92.6
69.6
45.0
49.2
38.4
51.5
44.9
40.2

243.0
226.9
199.2
96.5
71.0
111.6
94.6
74.3
73.4

170.8
137.3
74.6
69.4
68.5
60.6

74.9
57.1
51.4
65.1
47.5
56.0

72.5
60.2
61.7
43.2
38.9
39.8

65.8
66.3
66.0
59.1
66.4
50.6

87.7
73.2
43.1
37.8
36.9

79.7
78.0
38.0
55.8
33.1

Figure 3: Qualitative results on the MPII test set. The ﬁrst and second columns are the input images and output 2D joint
detections of the stacked hourglass network, the last column is the 3D pose generated by our network.

Table 2: Results by using multi-view information

Methods Lee[14] Hossain[10] Pavlakos [21] Ours
49.6

Avg.

56.9

51.9

52.8

improvement of 5.5% compared to the previous best result
55.8 mm [14] and 16.2 % compared to the baseline archi-
tecture [15]. This indicates the efﬁciency of our approach
by generating multiple hypotheses. Moreover, our network
outperforms [12] which also generates multiple hypotheses
by 22.5%. Following previous work, We show our result
under Protocol #2 [4, 17] where the estimated pose has been
further aligned with the ground truth via a rigid transferma-
tion. The MPJPE error in Table 1 shows that our approach
consistently outperforms other approaches.

It is difﬁcult to disambiguate the multiple 3D pose hy-

potheses generated by our model in a monocular view be-
cause most of them are feasible solutions to the inverse
2D-to-3D problem. Hence, we utilize the multi-view im-
ages from the set of calibrated cameras provided by the Hu-
man3.6M dataset to disambiguate and verify the correctness
of the multiple 3D pose hypotheses generated by our net-
work. Speciﬁcally, we transform the same pose under dif-
ferent cameras into the global world coordinates, and then
we choose the pose which is most consistent with the poses
from other camera coordinates. Finally, we get our esti-
mated pose by averaging all poses from different camera
coordinates. We list our result in Table 2 and compare with
other state-of-the-art approaches based on multi-view [21]
(spatial constraint) or video (temporal constraint) informa-
tion [14, 10]. Note that it is however not a fair comparison

9892

Table 3: Results with one (the ﬁrst three rows) or two (the last three rows) missing joints

Jahangiri et al. [12] 108.6 105.9 105.6 109.0 105.5 109.9 102.0 111.3 119.6
Martinez et al. [15] 57.4
84.0
48.9
69.7
Ours
Jahangiri et al. [12] 125.0 121.8 115.1 124.1 116.9 123.8 116.4 119.6 130.8
Martinez et al. [15] 62.9
90.6
54.0
77.8
Ours

Direct. Discuss Eating Greet Phone Smoke Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.
107.1 111.3 108.4 107.0 110.3 108.6
69.1
68.2
60.7
58.8
118.4 127.1 125.9 121.6 127.6 122.3
75.1
74.2
66.2
64.6

107.8
101.1
83.9
120.6
109.7
92.4

64.3 65.6 73.3
54.5 55.5 62.6

69.9 71.4 80.2
60.6 61.4 68.6

61.0 62.1
51.3 52.0

66.3 65.9
56.6 57.0

85.5
70.4

93.8
77.9

66.7
57.2

70.8
62.4

55.6
48.3

72.1
62.6

75.5
67.5

61.7
52.5

59.6
50.8

65.7
55.0

61.6
53.9

66.9
58.5

Figure 4: 3D Pose hypotheses generated by our network. The ﬁrst column is the input of our network, i.e. the 2D joints
estimated by the stacked hourglass network. The second column is the ground truth 3D pose, and the third to seventh
columns are the hypotheses generated by our network. The last column is the 2D reprojections of all ﬁve hypotheses. The
corresponding 2D projection and 3D pose are drawn in the same color. (Best view in color)

Table 4: Quantitative results on MPI-INF-3DHP dataset

Studio GS Studio no GS Outdoor All PCK

Mehta et al. [16]
Ours

84.1
70.1

68.9
68.2

59.6
66.6

72.5
67.9

Table 5: Comparison between different number of kernels

Number of kernels
Avg. MPJPE

1

3

5

8

62.9

55.2

52.7

52.6

with other results listed in Table 1 because they did not use
any multi-view or video information. The results show that
our approach has the best performance among both spatial
and temporal constraints based methods, indicating the ad-
vantage of our approach by generating multiple hypotheses.
In realistic scenarios, it is common that some joints are
occluded and cannot be detected. In order to show that our
model can handle with missing joints, we ran experiments
with different number of missing joints selected randomly
from the limb joints including l/r elbow, l/r wrist, l/r knee,
l/r ankle. We show our results in Table 3 and compared
with the baseline 2D-to-3D estimator [15] and the GMM

based methods [12] which also focus on generating mul-
tiple hypotheses. The baseline outperforms GMM based
methods by a large margin, which indicates the advantage
of using deep networks. Moreover, our method improves
the baseline for all actions with average error decreased by
10.4mm for both cases, further showing the robustness of
our method.

4.3. Transfer to MPII and MPI INF 3DHP datasets

We test our method on the MPII and MPI-INF-3DHP
datasets to validate the generalization capacity. Note that
we train the feature extractor and hypotheses generator
on the Human3.6 dataset which contains data from only
the indoor environment. The validation set of MPI-INF-
3DHP dataset includes images recorded under three differ-
ent scenes: 1143 images in studio with green screen back-
ground (Studio GS), 1064 images in studio without green
screen background (Studio no GS) and 728 images in out-
door environment (Outdoor). We use the 2D joints provided
by the dataset as input and compute the 3DPCK. The re-
sults in Table 4 show that the 3DPCK of our approach is
slightly lower than [16] even though we did not train on

9893

Table 6: Comparison of our network with and without Dirichlet prior

Wo prior 44.4
43.8
W prior

Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.
53.7
52.7

63.0 46.0 49.2
61.5 45.9 48.3

51.4
50.6

64.1
62.0

55.4
54.8

56.8
56.0

78.7
73.4

51.0
49.8

49.6
48.6

50.0
49.1

57.3
57.6

43.1
43.4

44.9
45.5

Table 7: The similarity of the 2D reprojections of all ﬁve pose hypotheses

PCKh@0.5 99.6

Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.
98.1

99.7 99.9 98.8

99.5

94.9

99.5

99.0

87.6

99.6

94.6

99.1

99.2

99.5

99.6

their dataset, indicating the generalization of our network.
Moreover, our results for different scenes do not change too
much compared to the results of [16]. This further suggests
the domain-invariant capability of the two-stage approach
that we adopted. We only give qualitative results for the
MPII dataset in Figure 3 since the ground truth 3D pose
data is not provided. We can see that our network can be
generalized well to outdoor unseen scenes.

4.4. Ablation Study

Different number of kernels Our hypotheses generator
is based on MDN where each of the M Gaussian kernels
in Eqn.(1) yields different result. We note that our network
cannot ﬁt the data completely if M is too small, while larger
M requires more computation resource. We thus train three
different models with M setting to 3, 5, 8, respectively. We
show the average MPJPE on the Human3.6M dataset in Ta-
ble 5 and compare them with the baseline method, which is
based on single Gaussian distribution. The results suggest
that our MDN has better performance than single Gaussion
based method. Moreover, the performance does not im-
prove much when M is larger than ﬁve. Consequently, we
set M to ﬁve in our experiments in view of the trade-off
between accuracy and computational complexity.

Dirichlet prior We add a Dirichlet conjugate prior to the
distribution of the mixture coefﬁcients α(x) to prevent over-
ﬁtting of a single Gaussion kernel to the training data. In
order to explore the role of the Dirichlet prior, we compare
the performance of our model with and without Lprior. The
results are shown in Table 6, it can be seen that the per-
formance improves by adding the Dirichlet conjugate prior,
especially for the difﬁcult poses in actions “Sitting” and
“SittingDown”. The reason is that most of the poses in
the Human3.6 dataset are in a standing position, resulting
in a worse performance on the “Sitting” and “SittingDown”
actions. This further indicates that the Dirichlet conjugate
prior can prevent overfﬁting effectively.

What is generated by each kernel?
In order to explore
the relation between different hypotheses, we reproject all
ﬁve pose hypotheses into the image plane and compute the
difference between projections and the 2D input joints. We

adopt the PCKh@0.5 score [18] which is the standard met-
ric for 2D pose estimation to measure the difference. The
high PCKh@0.5 score in Table 7 suggests that all the ﬁve
hypotheses have almost the same 2D reprojections which
are consistent with the 2D input. Note that we do not add
any constraint as [12] did to force all hypotheses to be con-
sistent in the 2D reprojections.

We give several visualization results in Figure 4 to fur-
ther illustrate the relations between all pose hypotheses. As
described by Eqn. (4), each Gaussian kernel can be seen
to generate the same hypotheses for simple pose with less
ambiguity, e.g. standing (ﬁrst row). This means that sin-
gle Gaussian distribution is sufﬁcient for simple poses. In
comparison, our network can be seen to generate differ-
ent hypotheses for challenging poses like “GettingDown” or
“SittingDown” (second and third rows) due to two reasons.
Firstly, our network receives lesser information on this type
of poses since most of the poses in the Human3.6 dataset
are the “standing” poses. Secondly, there are more ambi-
guities and occlusions for the “GettingDown” or “Sitting-
Down” poses. As a result, our network generates multiple
pose hypotheses to mitigate the increase of the uncertainty.
We also visualize the 2D reprojections of all hypotheses in
the last column. We indicate the corresponding 2D repro-
jection and 3D pose with the same color. The overlaps be-
tween the 2D reprojections further validate that our network
generates hypotheses that are consistent in the 2D image co-
ordinates.

5. Conclusion

In this work, we introduce the use of a mixture density
network to generate multiple feasible hypotheses for the in-
verse problem of 3D human pose estimation from 2D in-
puts. Experimental results show that our network achieves
state-of-the-art results in both best hypothesis and multi-
view settings. Furthermore, the 3D pose hypotheses gen-
erated by our network are consistent in the 2D reprojections
suggests that the hypotheses model the ambiguity along the
depth of the joints. Results on the MPII and MPI-INF-
3DHP datasets further show the generalization capacity of
our network.

9894

mation in the wild using improved cnn supervision. In 3D
Vision,International Conference on, pages 506–516, 2017.

[17] F. Moreno-Noguer. 3d human pose estimation from a single
image via distance matrix regression. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 1561–
1570, 2017.

[18] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499, 2016.

[19] S. Park, J. Hwang, and N. Kwak. 3d human pose estimation
using convolutional neural networks with 2d pose informa-
tion.
In European Conference on Computer Vision, pages
156–169, 2016.

[20] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis.
Coarse-to-ﬁne volumetric prediction for single-image 3d hu-
man pose. In IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 1263–1272, 2017.

[21] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis.
Harvesting multiple views for marker-less 3d human pose
annotations. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 6988–6997, 2017.

[22] L. Sigal, S. Bhatia, S. Roth, M. J. Black, and M. Isard. Track-
ing loose-limbed people. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 421–428, 2004.

[23] X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei. Integral hu-
man pose regression. In European Conference on Computer
Vision, pages 529–545, 2018.

[24] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
volutional pose machines. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 4724–4732, 2016.

[25] H. Yasin, U. Iqbal, B. Kruger, A. Weber, and J. Gall. A
dual-source approach for 3d pose estimation from a single
image. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 4948–4956, 2016.

[26] Q. Ye and T.-K. Kim. Occlusion-aware hand pose estimation
using hierarchical mixture density network. arXiv preprint
arXiv:1711.10872, 2017.

[27] X. Zhou, Q. Huang, X. Sun, X. Xue, and Y. Wei. Towards
3d human pose estimation in the wild: a weakly-supervised
approach. In IEEE International Conference on Computer
Vision, pages 398–407, 2017.

[28] X. Zhou, X. Sun, W. Zhang, S. Liang, and Y. Wei. Deep kine-
matic pose regression. In European Conference on Computer
Vision, pages 186–201, 2016.

[29] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and
K. Daniilidis. Sparseness meets deepness: 3d human pose
estimation from monocular video.
In IEEE Conference
on Computer Vision and Pattern Recognition, pages 4966–
4975, 2016.

References

[1] I. Akhter and M. J. Black. Pose-conditioned joint angle lim-
its for 3d human pose reconstruction. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 1446–
1455, 2015.

[2] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d
human pose estimation: New benchmark and state of the art
analysis. In IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 3686–3693, 2014.

[3] C. M. Bishop. Mixture density networks. Technical report,

Citeseer, 1994.

[4] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero,
and M. J. Black. Keep it smpl: Automatic estimation of 3d
human pose and shape from a single image.
In European
Conference on Computer Vision, pages 561–578, 2016.

[5] A. Brando Guillaumes. Mixture density networks for distri-
bution and uncertainty estimation. Master’s thesis, Universi-
tat Polit`ecnica de Catalunya, 2017.

[6] W. Chen, H. Wang, Y. Li, H. Su, Z. Wang, C. Tu, D. Lischin-
ski, D. Cohen-Or, and B. Chen. Synthesizing training images
for boosting human 3d pose estimation. In 3D Vision, Inter-
national Conference on, pages 479–488, 2016.

[7] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(elus). arXiv preprint arXiv:1511.07289, 2015.

[8] Y. Du, Y. Wong, Y. Liu, F. Han, Y. Gui, Z. Wang, M. Kankan-
halli, and W. Geng. Marker-less 3d human motion capture
with monocular image sequence and height-maps. In Euro-
pean Conference on Computer Vision, pages 20–36, 2016.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation.
In IEEE International Conference on Com-
puter Vision, pages 1026–1034, 2015.

[10] M. R. I. Hossain and J. J. Little. Exploiting temporal infor-
mation for 3d human pose estimation. In European Confer-
ence on Computer Vision, pages 69–86, 2018.

[11] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu.
Human3. 6m: Large scale datasets and predictive meth-
ods for 3d human sensing in natural environments.
IEEE
transactions on pattern analysis and machine intelligence,
36(7):1325–1339, 2014.

[12] E. Jahangiri and A. L. Yuille. Generating multiple diverse
hypotheses for human 3d pose consistent with 2d joint detec-
tions. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 805–814, 2017.

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[14] K. Lee, I. Lee, and S. Lee. Propagating lstm: 3d pose estima-
tion based on joint interdependency. In European Conference
on Computer Vision, pages 119–135, 2018.

[15] J. Martinez, R. Hossain, J. Romero, and J. J. Little. A simple
yet effective baseline for 3d human pose estimation. In IEEE
International Conference on Computer Vision, pages 2640–
2649, 2017.

[16] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko,
W. Xu, and C. Theobalt. Monocular 3d human pose esti-

9895

