Online high rank matrix completion

Jicong Fan, Madeleine Udell

Cornell University

Ithaca, NY 14853, USA

{jf577,udell}@cornell.edu

Abstract

Recent advances in matrix completion enable data impu-
tation in full-rank matrices by exploiting low dimensional
(nonlinear) latent structure.
In this paper, we develop a
new model for high rank matrix completion (HRMC), to-
gether with batch and online methods to ﬁt the model and
out-of-sample extension to complete new data. The method
works by (implicitly) mapping the data into a high dimen-
sional polynomial feature space using the kernel trick; im-
portantly, the data occupies a low dimensional subspace in
this feature space, even when the original data matrix is of
full-rank. The online method can handle streaming or se-
quential data and adapt to non-stationary latent structure,
and enjoys much lower space and time complexity than pre-
vious methods for HRMC. For example, the time complexity
is reduced from O(n3) to O(r3), where n is the number of
data points, r is the matrix rank in the feature space, and
r ≪ n. We also provide guidance on sampling rate re-
quired for these methods to succeed. Experimental results
on synthetic data and motion data validate the performance
of the proposed methods.

1. Introduction

ten years,

In the past

low rank matrix completion
(LRMC) has been widely studied [4, 16, 22, 23, 20, 13, 3,
18, 9]. For instance, Cand`es and Recht [4] showed that any
n×n incoherent matrices of rank r can be exactly recovered
from Cn1.2r log n uniformly randomly sampled entries
with high probability through solving a convex problem of
nuclear norm minimization (NNM). However, LRMC can-
not recover high rank or full-rank matrices, even when the
the data lies on a low dimensional (nonlinear) manifold. To
address this problem, recently a few researchers have devel-
oped new high rank matrix completion (HRMC) methods
[8, 17, 28] for data drawn from multiple subspaces [7, 6, 11]
or nonlinear models [1, 24, 10]. These HRMC methods can
outperform LRMC methods for many real problems such as
subspace clustering with missing data, motion data recovery

[6, 24], image inpainting, and classiﬁcation [1, 10].

All the aforementioned LRMC and HRMC methods are
ofﬂine methods. However, for many problems, we ob-
tain one sample at a time and would like to update the
model as each new sample arrives using online optimiza-
tion. In addition, compared to ofﬂine methods, online meth-
ods [25, 21, 29] often have lower space and time complexi-
ties and can adapt to changes in the latent data structure. For
these reasons, online matrix completion has recently gained
increasing attention [2, 5, 15, 19].

2. Related work and our contribution

minimizeP(i,j)∈Ω(cid:0)Xij − Ui:V ⊤j:(cid:1)2

Online matrix completion. Sun and Luo [26] and Jin
et al.
[14] proposed to use stochastic gradient descent
(SGD) to solve the low rank factorization (LRF) problem
with variables U ∈
Rm×r, V ∈ Rn×r, where X ∈ Rm×nand Ω denotes the
locations of observed entries of X. Speciﬁcally, given an
entry Xij , the i-th row of U and j-th row of V are updated
by gradient descent. Yun et al. [29] studied the streaming or
online matrix completion problem when the columns of the
matrix are presented sequentially. The GROUSE method
proposed in [2] used incremental gradient descent on the
Grassmannian manifold of subspaces to learn a low rank
factorization from incomplete data online. These online
methods have a common limitation:
they cannot recover
high rank matrices. Mairal et al. [21] also studied the online
factorization problem with the goal of learning a dictionary
2 kx − Dαk2 + λkαk1. A
for sparse coding: minimize
D∈C,α
sparse factorization based matrix completion algorithm was
proposed in [11]. It is possible to recover a high rank matrix
online by combining ideas from [21] with [11].

1

High rank matrix completion. Elhamifar [6] proposed
to use group-sparse constraint to complete high rank ma-
trix consisting of data drawn from union of low-dimensional
subspaces. Alameda-Pineda et al. [1] proposed a nonlinear
matrix completion method for classiﬁcation. The method
performs matrix completion on a matrix consisting of (non-

8690

linear) feature-label pairs, where the unknown labels are re-
garded as missing entries. The method is inapplicable to
general matrix completion problems in which the locations
of all missing entries are not necessarily in a single block.
Ongie et al. [24] assumed X is given by an algebraic variety
and proposed a method called VMC to recover the missing
entries of X through minimizing the rank of φ(X), where
φ(X) is a feature matrix given by polynomial kernel. Fan
and Chow [10] assumed the data are drawn from a non-
linear latent variable model and proposed a nonlinear ma-
trix completion method (NLMC) that minimizes the rank of
φ(X), where φ(X) is composed of high-dimensional non-
linear features induced by polynomial kernel or RBF kernel.

Challenges in HRMC. First, existing HRMC methods
lack strong theoretical guarantee on the sample complexity
required for recovery. For example, in VMC, the authors
provide a lower bound of sampling rate (ρ0, equation (6)
of [24]) only for low-order polynomial kernel and ρ0 in-
volved an unknown parameter R owing to the algebraic va-
riety assumption. In NLMC [10], the authors only provided
a coarse lower bound of sampling rate, i.e. ρ > O(d/m),
where d is the dimension of latent variables. Second, exist-
ing HRMC methods are not scalable to large matrices. For
example, VMC and NLMC require singular value decom-
position on an n × n kernel matrix in every iteration. The
method of [6] is also not efﬁcient because of the sparse op-
timization on an n × n coefﬁcients matrix. Third, existing
HRMC methods have no out-of-sample extensions, which
means they cannot efﬁciently complete new data. Last but
not least, existing HRMC methods are ofﬂine methods and
cannot handle online data.

Contributions.
In this paper, we aim to address these
challenges. We propose a novel high rank matrix com-
pletion method based on kernelized factorization (KFMC).
KFMC is more efﬁcient and accurate than state-of-the-art
methods. Second, we propose an online version for KFMC,
which can outperform online LRMC signiﬁcantly. Third,
we propose an out-of-sample extension for KFMC, which
enables us to use the pre-learned high rank model to com-
plete new data directly. Finally, we analyze the sampling
rate required for KFMC to succeed.

polynomial with random coefﬁcients. For example, when
d = 2 and p = 2, for i = 1, . . . , m, xi = c⊤i ¯s, where
ci ∈ R6 and ¯s = [1, s1, s2, s2
2, s1s2]⊤. Lemma 1 shows
that X is of high rank when p is large.

1, s2

Lemma 1. Suppose the columns of X satisfy (1). Then with

p (cid:1)}.
probability 1, rank(X) = min{m, n,(cid:0)d+p

1 · · · sµd

Proof. Expand the polynomial fi for each i = 1, . . . , m to
write xi = fi(s) = c⊤i ¯s, where ¯s = {sµ1
d }|µ|≤p and
ci ∈ R(d+p
p ). Each column of X satisﬁes x = C ¯s, where
C = [c1 · · · cm] ∈ Rm×(d+p
p ). The matrix X can be writ-
ten as X = C ¯S, where ¯S = (¯s⊤1 , . . . , ¯s⊤n ) ∈ R(d+p
p )×n.
The variables s are uncorrelated and the coefﬁcients c are
random, so generically both C and ¯S are full rank. Hence

p (cid:1)}.
rank(X) = min{m, n,(cid:0)d+p

In this paper, our goal is to recover X from a few ran-
domly sampled entries we denote by {Mij}(i,j)∈Ω. When p
is large, X is generically high rank and cannot be recovered
by conventional LRMC methods.

Remark. Throughout this paper, we use the terms “low
rank” or “high rank” matrix to mean a matrix whose rank is
low or high relative to its side length.

1 · · · xµm

Let φ : Rm → R ¯m be a q-order polynomial feature map

φ(x) = {xµ1
φ(X) = [φ(x1), φ(x2), · · · , φ(xn)] and consider its rank:

q (cid:1). Write
m }|µ|≤q. Here ¯m = (cid:0)m+q
pq (cid:1)}.
with probability 1, rank(φ(X)) = min{ ¯m, n,(cid:0)d+pq

Theorem 1. Suppose the columns of X satisfy (1). Then

Proof. Deﬁne the pq-order polynomial map ψ(s)
:=
φ(x) = φ(f(s)). Expanding as above, write the vector
φ(x) = Ψ˜s with Ψ ∈ R ¯m×(d+pq
pq ), and
write the matrix φ(X) = Ψ ˜S with ˜S = (˜s⊤1 , . . . , ˜s⊤n ) ∈
R(d+pq
pq )×n. As above, Ψ and ˜S are generically full rank, so

pq ) and ˜s ∈ R(d+pq

rank(φ(X)) = min{ ¯m, n,(cid:0)d+pq

pq (cid:1)} with probability 1.

While rank(φ(X)) ≥ rank(X), Theorem 1 shows that
φ(X) is generically low rank when d is small and n is large.
For example, when d = 2, m = 20, n = 200, p = 4, and
q = 2, generically rank(X)
=
min{m,n}
0.225: X is high rank but φ(X) is low rank.

= 0.75 while rank(φ(X))
min{ ¯m,n}

3. Methodology

3.1. High rank matrices

3.2. Kernelized factorization

To recover the missing entries of X, we propose to solve

We assume the columns of X ∈ Rm×n are given by

x = f(s) = [f1(s), f2(s), · · · , fm(s)]⊤,

(1)

minimizeX,A,Z
subject to

1

2 kφ(X) − AZk2
Xij = Mij, (i, j) ∈ Ω,

F

(2)

where s ∈ Rd (d ≪ m < n) consists of uncorrelated vari-
ables and each fi : Rd → R, i = 1, . . . , m, is a p-order

where A ∈ R ¯m×r, Z ∈ Rr×n, and r = (cid:0)d+pq

solution to (2) completes the entries of X using the natural

pq (cid:1). The

8691

low rank structure of φ(X). Problem (2) implicitly deﬁnes
an estimate for f , ˆf(S) := X ≈ φ−1(AZ).

For numerical stability, we regularize A and Z and solve

minimize

X,A,Z

1

2 kφ(X) − AZk2

F + α

2 kAk2

F + β

2 kZk2
F ,

(3)

subject to Xij = Mij, (i, j) ∈ Ω,

where α and β are regularization parameters, instead of (2).
It is possible to solve (3) directly but the computational
cost is quite high if m and q are large. The following lemma
shows that there is no need to model A explicitly.

Lemma 2. For any X generated by (1), there exist D ∈
Rm×r and Z ∈ Rr×n such that φ(X) = φ(D)Z.
Proof. Suppose D ∈ Rm×r are also generated by (1)
any r columns of X), so φ(D) and φ(X) share
(e.g.
their column space and (with probability 1) φ(D) is full
rank. More precisely, φ(D) = BCD and φ(X) = BCX ,
where B ∈ R ¯m×r is a basis for the column space, and
both CX ∈ Rr×n and CD ∈ Rr×r are full rank. Deﬁne
Z = C−1

D CX and the result follows.

Hence any solution of the following also solves (3):

minimize

X,D,Z

1

2 kφ(X) − φ(D)Zk2

F + α

2 kφ(D)k2

F + β

2 kZk2

F

subject to Xij = Mij, (i, j) ∈ Ω,

(4)

where D ∈ Rm×r is much smaller than A ∈ R ¯m×r of (3).
Use the trace function Tr to rewrite the objective in (4) as

1

+ α

2 kZk2
F .

2 Tr(cid:0)φ(X)⊤φ(X) − 2φ(X)⊤φ(D)Z + Z⊤φ(D)⊤φ(D)Z(cid:1)
2 Tr(cid:0)φ(D)⊤φ(D)(cid:1) + β

Now we use the kernel trick to avoid explicitly comput-
ing the feature map φ. Deﬁne k(x, y) := φ(x)⊤φ(y) =
hφ(x), φ(y)i, so φ(X)⊤φ(X) = KXX , φ(X)⊤φ(D) =
KXD, and φ(D)⊤φ(D) = KDD, where KXX , KXD,
and KDD are the corresponding kernel matrices. The most
widely-used kernels are the polynomial kernel (Poly) and
the radial basis function kernel (RBF)

3.3. Optimization for KFMC

The optimization problem (KFMC) is nonconvex and
has three blocks of variables. We propose using coordinate
descent over these three blocks to ﬁnd a stationary point.

Update Z. To begin, complete entries of X arbitrarily
and randomly initialize D. Deﬁne the r × r identity Ir. Fix
X and D and update Z as

Z ← arg min

ℓ(Z, D, X)

Z

= arg min

Z

−Tr(KXDZ) + 1

2 Tr(Z⊤KDDZ) + β

2 kZk2

F

= (KDD + βIr)−1K⊤XD,

(6)

Update D. There is no closed form solution for the min-
imization of ℓ(Z, D, X) with respect to D due to the
kernel matrices.
Instead, we propose the additive update
D ← D − ∆D. We compute ∆D using a relaxed Newton
method, described below for the Poly and RBF kernels.

For the polynomial kernel, rewrite the terms in the ob-

jective in which D appears as

ℓ(Z, D, X) := − Tr((W1 ⊙ (X T D + c))Z)

+ 1
+ α

2 Tr(Z T (W2 ⊙ (DT D + c))Z)
2 Tr(W2 ⊙ (DT D + c)).

(7)

deﬁning W1 = hX⊤D+ciq−1 and W2 = hD⊤D+ciq−1,
where ⊙ is elementwise multiplication and h·iu denotes the
element-wise u-power. Inspired by iteratively reweighted
optimization, ﬁx W1 and W2 to approximate the gradient
and Hessian of ℓ(Z, D, X) with respect to D as

gD := −X(W1 ⊙ Z⊤) + D((ZZ⊤ + αIr ⊙ W2))
HD := ZZ⊤ ⊙ W2 + αW2 ⊙ Ir.

HD is positive deﬁnite by the Schur product theorem. Now
choose τ > 1 for numerical stability and deﬁne the update

Poly : k(x, y) = (x⊤y + c)q

RBF : k(x, y) = exp(cid:0)− 1

σ2 kx − yk2(cid:1) ,

with hyperparameters c, q, and σ. The (implicit) feature
maps φ(x) of Poly and RBF are the q-order and inﬁnite-
order polynomial maps respectively. Rewrite (4) to deﬁne
kernelized factorizatiom matrix completion (KFMC)

minimize

X,D,Z

ℓ(Z, D, X)

subject to Xij = Mij, (i, j) ∈ Ω

(KFMC)

where ℓ(Z, D, X) = 1
2 Tr(KDD) + β
α
2 kZk2
is a constant and can be dropped from the objective.

2 Tr(cid:0)KXX − 2KXDZ + Z⊤KDDZ(cid:1)+

F . For the RBF kernel, Tr(KDD) ≡ r

(5)

∆D := 1

τ gDH−1
D .

(8)

The effectiveness of our update for D is guaranteed by the
following lemma. (The proof of the lemma and discussion
about the role of τ are in the supplementary material.)

Lemma 3. The update (8) is a relaxed Newton’s method
and ensures sufﬁcient decrease in the objective:

ℓ(Z, D − ∆D, X) − ℓ(Z, D, X) ≤ − 1

2τ Tr(gDH−1

D g⊤D ).

For the RBF kernel, the gradient is

∇Dℓ = 1

σ2 (XQ1 − DΓ1) + 2

σ2 (DQ2 − DΓ2).

(9)

8692

(Throughout, we abuse notation to write ℓ for ℓ(Z, D, X).)
Here Q1 = −Z⊤ ⊙ KXD, Q2 = (0.5ZZ⊤ + 0.5αIr) ⊙
KDD, Γ1 = diag(1⊤n Q1), and Γ2 = diag(1⊤r Q2), where
1n ∈ Rn and 1r ∈ Rr are composed of 1s. The following
lemma (proved in the supplementary material) indicates that
XQ1 in (9) is nearly a constant compared to DΓ1, DQ2,
and DΓ2, provided that σ and n are large enough:

Lemma 4. kX(Z⊤ ⊙ KXD1) − X(Z⊤ ⊙ KXD2)kF ≤
c
σ√n kXk2kD1 − D2kF , where c is a small constant.

Therefore, we can compute an approximate Hessian ne-
glecting XQ1. As in (8), we deﬁne

∆D := 1

τ ∇Dℓ( 1

σ2 (2Q2 − Γ1 − 2Γ2))−1.

(10)

Update X. Finally, ﬁxing Z and D, we wish to mini-
mize (KFMC) over X, which again has no closed-form so-
lution. Again, we suggest updating X using a relaxed New-
ton method X ← X − ∆X . For the polynomial kernel,

gX = X(W3 ⊙ In) − D(W ⊤4 ⊙ Z)

= qX ⊙ (1mw⊤) − qD(W ⊤4 ⊙ Z),

(11)

where W3 = hX⊤X + ciq−1, W4 = hX⊤D + ciq−1,
1m ∈ Rm consists of 1s, and w ∈ Rm consists of the
diagonal entries of W3. As above, we deﬁne

∆X := 1

τ gX ⊙ (1mw−T ).

When RBF kernel is used, we get

∇Xℓ = 1

σ2 (DQ3 − X Γ3) + 2

σ2 (XQ4 − X Γ4).

(12)

(13)

Here Q3 = −Z ⊙ K⊤XD, Q4 = 0.5In ⊙ KXX , Γ3 =
diag(1⊤r Q3), and Γ4 = diag(1⊤n Q4). As in (10), deﬁne

Here the computational cost is not high in practice because
the matrix to be inverted is diagonal.

We can also use a momentum update to accelerate the

convergence of D and X:

(b∆D ← ηb∆D + ∆D, D ← D − b∆D
b∆X ← ηb∆X + ∆X, X ← X − b∆X

where 0 < η < 1 is a constant. The optimization method
is summarized as Algorithm 1. The following lemma (with
proof in the supplement) shows the method converges.

(15)

Lemma 5. For sufﬁciently small η, Algorithm 1 converges
to a stationary point.

Algorithm 1 Ofﬂine KFMC
Input: M , Ω, r, k(·, ·), α, β, tmax, η

1: Initialize: t = 0, X, D ∼ N (0, 1), b∆D = 0, b∆X = 0

2: repeat
3:

t ← t + 1
Z = (KDD + βIr)−1K⊤XD
Compute ∆D using (8) or (10)

Compute ∆X using (12) or (14)

b∆D = ηb∆D + ∆D
D ← D − b∆D
b∆X = ηb∆X + ∆X
X ← X − b∆X and Xij = Mij ∀(i, j) ∈ Ω

10:
11: until converged or t = tmax
Output: X, D

4:

5:

6:

7:

8:

9:

3.4. Online KFMC

Suppose we get an incomplete sample x at time t and
need to update the model of matrix completion timely or
solve the optimization online. In (4), we can put the con-
straint into the objective function directly and get the fol-
lowing equivalent problem

minimize
[X] ¯Ω,D,Z

nXj=1

1

2 kφ(xj)−φ(D)zjk2+ α

2n kφ(D)k2

F + β

2 kzjk2,

(16)

where [X] ¯Ω denotes the unknown entries of X. Denote

ℓ([xj]ωj , D) := min

zj ,[xj ] ¯ωj

1

2 kφ(xj) − φ(D)zjk2

+ α

2n kφ(D)k2

F + β

2 kzjk2,

(17)
where [xj]ωj ([xj]¯ωj ) denotes the observed (unknown) en-
tries of xj and ωj (¯ωj ) denotes the corresponding locations.
Then (16) minimizes the empirical cost function

1
n

nXj=1

ℓ([xj]ωj , D).

(18)

The expected cost is

g(D) := E[x]ω [ℓ([x]ω, D)] = lim
n→∞

gn(D).

(19)

To approximately minimize (19) online, we propose the fol-
lowing optimization for a given incomplete sample x

minimize
[x] ¯ω,D,z

ˆℓ(z, [x]¯ω, D) := 1

2 kφ(x) − φ(D)zk2

(20)

+ α

2 kφ(D)k2

F + β

2 kzk2.

With randomly initialized D, we ﬁrst compute z and [x]¯ω
via alternately minimizing ˆℓ(z, [x]¯ω, D), which is equiva-
lent to

1

2 kxx − kxDz + 1

2 z⊤KDDz + β

2 kzk2.

(21)

minimize

[x] ¯ω,z

8693

∆X := 1

τ ∇Xℓ( 1

σ2 (2Q4 − Γ3 − 2Γ4))−1.

(14)

gn(D) :=

Speciﬁcally, in each iteration, z is updated as

z = (KDD + βIr)−1k⊤xD.

(22)

We propose to update [x]¯ω by Newton’s method,
i.e.,
[x]¯ω ← [x]¯ω − [∆x]¯ω. When polynomial kernel is used,
we obtain

∇x

ˆℓ = w1x − D(w⊤2 ⊙ z)

(23)

where w1 = hx⊤x + ciq−1, w2 = hx⊤D + ciq−1. Then

∆x = 1
τ w1

∇x

ˆℓ.

When RBF kernel is used, we have

∇x

ˆℓ = 1

σ2 (Dq − γx),

where q = −z ⊙ k⊤xD and γ = 1⊤r q. Then

∆x = σ2

τ γ ∇x

ˆℓ.

(24)

(25)

(26)

The derivations of (24) and (26) are similar to those of (8)
and (10). Then we repeat (22)−(26) until converged.

After z and [x]¯ω are computed, we compute D via min-

imizing ˆℓ(z, [x]¯ω, D), which is equivalent to

minimize

D

−kxDz + 1

2 z⊤KDDz + α

2 Tr(KDD).

(27)

We propose to use SGD to update D, i.e., D ← D − ∆D.
When polynomial kernel is used, we have

∇D

ˆℓ = −x(w1 ⊙ z⊤) + D(zz⊤ ⊙ W2) + αD(W2 ⊙ Ir)),
(28)
where w1 = hx⊤D + ciq−1 and W2 = hD⊤D + ciq−1.
Then we have

∆D = 1

τ ∇D

ˆℓ/kzz⊤ ⊙ W2 + αW2 ⊙ Irk2.

(29)

Here we cannot use the method of (8) because zz⊤ is not as
stable as ZZ⊤. In addition, the following lemma (proved
in the supplementary material) ensures the effectiveness of
updating D:

Similar to ofﬂine KFMC, we also use momentum to ac-
celerate the optimization of online KFMC. The optimiza-
tion steps are summarized in Algorithm 2. The error of on-
line matrix completion can be reduced with multi-pass opti-
mization or increasing the number of samples. In Algorithm
2, the sequence ℓ([xt]ωt , D) deﬁned in (17) may not de-
crease continuously because the incomplete sample xt can
introduce high uncertainty. However, the sequence gt(D),
the empirical cost function deﬁned in (18), is convergent be-
cause for j = 1, · · · , t, ℓ([xj]ωj , D) is minimized through
optimizing [xj]¯ωj , zj , and D.

Algorithm 2 Online KFMC
Input: Incomplete samples {[x1]ω1 , [x2]ω2 , · · · , [xt]ωt},

r, k(·, ·), α, β, niter, η, npass

1: Initialize: D ∼ N (0, 1), b∆D = 0

2: for u = 1 to npass do
for j = 1 to t do
3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

repeat

l ← l + 1 and zj = Ck⊤XD
Compute ∆x using (24) or (26)

l = 0, b∆X = 0, C = (KDD + βIr)−1
b∆x ← ηb∆x + ∆x
[xj]¯ωj ← [xj]¯ωj − [b∆x]¯ωj
b∆D ← ηb∆D + ∆D and D ← D − b∆D

until converged or l = niter
Compute ∆D using (29) or (31)

end for

13:
14: end for
Output: Xt = [x1, x2, · · · , xt], D

3.5. Out-of-sample extension of KFMC

The matrix D obtained from ofﬂine matrix completion
(1) or online matrix completion (2) can be used to recover
the missing entries of new data without updating the model.
We can also compute D from complete training data: the
corresponding algorithm is similar to Algorithms 1 and 2,
but does not require the X update. We can complete a new
(incomplete) sample xnew by solving

Lemma 6. Updating D as D − ∆D does not diverge and
ˆℓ(z, [x]¯ω, D −∆D)− ˆℓ(z, [x]¯ω, D) ≤ − 1
F pro-
2τ τ0
vided that τ > 1, where τ0 = kzz⊤ ⊙ W2 + αW2 ⊙ Irk2.

k∇D

ˆℓk2

minimize
[xnew] ¯ωnew ,znew

1

2 kφ(xnew)−φ(D)znewk2 + β

2 kznewk2,

(32)

When RBF kernel is used, the derivative is

where [xnew]¯ωnew denotes unknown entries of xnew. This out-
of-sample extension of KFMC is displayed as Algorithm 3.

∇D

ˆℓ = 1

σ2 (xQ1 − DΓ1) + 2

σ2 (DQ2 − DΓ2),

(30)

3.6. Complexity analysis

where Q1 = −z⊤ ⊙ kXD, Q2 = (0.5zz⊤ + 0.5αIr) ⊙
KDD, Γ1 = diag(Q1), and Γ2 = diag(1⊤r Q2). Similar to
(29) and Lemma 6, we obtain

∆D = 1

τ ∇D

ˆℓ/k 1

σ2 (2Q2 − Γ1 − 2Γ2)k2.

(31)

Consider a high (even, full) rank matrix X ∈ Rm×n
(m ≪ n) given by (1). In the methods VMC and NLMC,
and our KFMC, the largest object stored is the kernel matrix
K ∈ Rn×n. Hence their space complexities are all O(n2).
In VMC and NLMC, the major computational step is to

8694

Algorithm 3 Out-of-sample extension for KFMC
Input: D (computed from training data), k(·, ·), β, niter, η,
new incomplete samples {[x1]ω1 , [x2]ω2 , · · · , [xt]ωt}

1: C = (KDD + βIr)−1
2: for j = 1 to t do
3:

4:

5:

6:

7:

8:

repeat

l ← l + 1 and zj = Ck⊤xD
Compute ∆x using (24) or (26)

l = 0, b∆x = 0
b∆x ← ηb∆x + ∆x
[xj]¯ωj ← [xj]¯ωj − [b∆x]¯ωj

until converged or l = niter

9:
10: end for
Output: Xnew = [x1, x2, · · · , xt]

Space complexity Time complexity
O(n2)
VMC
O(n2)
NLMC
O(n2)
KFMC
O(mr + r2)
OL-KFMC
OSE-KFMC O(mr + r2)

O(n3 + mn2)
O(n3 + mn2)
O(mn2 + rmn)
O(r3)
O(mr)

Table 1: Time and space complexities (X ∈ Rm×n, m ≪ n)

compute K and its singular value decomposition at every
iteration. Hence their time complexities are O(mn2 + n3).
In our KFMC, the major computational steps are to form K,
to invert the r × r matrix in (6), and to multiply an m × n
and n × r matrix to compute the derivatives. Hence the time
complexity is O(mn2+r3+rmn) = O(mn2+rmn), since
n ≫ r.

Online KFMC does not store the kernel matrix K. In-
stead, the largest objects stored are D and KDD. Hence the
space complexity is O(mr + r2). The major computational
step is to invert an r × r matrix (see Algorithm 2). Thus the
time complexity is O(r3). In the out-of-sample extension,
the largest objects stored are D and C (see Algorithm 3), so
the space complexity is O(mr + r2). For each online sam-
ple, we only need to multiply m × r matrices with vectors.
Hence the time complexity is just O(mr).

This analysis are summarized in Table 1. We see that
the space and time complexities of the proposed three ap-
proaches are much lower than those of VMC and NLMC.

3.7. Generalization for union of subspaces

KFMC can also handle data drawn from a union of sub-

spaces. Suppose the columns of X ∈ Rm×n are given by

{x{k} = f{k}(s{k})}u

k=1,

(33)

where s{k} ∈ Rd (d ≪ m < n) are random variables and
f{k} : Rd → Rm are p-order polynomial functions for each

k = 1, . . . , u. For convenience, we write

X = [X{1}, X{2}, · · · , X{u}],

(34)

where the columns of each X{k}are in the range of f{k},
though we do not know which subspace each column of X
is drawn from. An argument similar to Lemma 1 shows

p (cid:1)}
rank(X) = min{m, n, u(cid:0)d+p

(35)

with probability 1, so X is very likely to be of high rank or
full rank when u or p is large.

We can generalize Theorem 1 to show rank(φ(X)) =

min{ ¯m, n, r} with probability 1, where ¯m = (cid:0)m+q
r = u(cid:0)d+pq

q (cid:1) and
pq (cid:1). Hence when d is small and n is large, φ(X)

is low rank, so missing entries of X can still be recovered
by the ofﬂine and online methods proposed in this paper. In
particular, for data drawn from a union of linear subspaces
(p = 1 and u > 1), generically rank(X) = min(m, n, ud)

q (cid:1).
while rank(φ(X)) = u(cid:0)d+q

3.8. On the sampling rate

Suppose X is generated by (33), and a proportion ρKFMC
of its entries are observed. We provide some heuristics to
help decide how many entries should be observed for com-
pletion with the polynomial and RBF kernels. Detailed cal-
culations for (36) and (37) are deferred to the supplement.
To complete φ(X) uniquely using a q-order polynomial
kernel, one rule of thumb is that the number of entries ob-
served should be at least as large as the number of degrees of
freedom in the matrix φ(X) [24]. Here, φ(X) is a ¯m × n

matrix with rank r = u(cid:0)d+pq

pq (cid:1), where ¯m = (cid:0)m+q

count the degrees of freedom of matrices with this rank to
argue sampling rate should satisfy

q (cid:1). We

ρKFMC ≥(cid:0)r/n + r/ ¯m − r2/n/ ¯m(cid:1)1/q

.

(36)

Equation (36) bounds the number of degrees of freedom
of φ(X) by considering its rank and size. But of course
φ(X) is a deterministic function of X, which has many
fewer degrees of freedom. Hence while (36) provides a
good rule of thumb, we can still hope that lower sampling
rates might produce sensible results.

For the RBF kernel, q = ∞, so the condition (36) is vac-
uous. However, the RBF kernel can be well approximated
by a polynomial kernel and we have

φi(x) = ˆφi(x) + O(q cq+1

(q+1)! ),

(37)

where ˆφi(x) denotes the i-th feature of q-order polynomial
kernel and φi(x) denotes the i-th feature of the RBF ker-
nel. Hence exact recovery of ˆφi(x) implies approximate
(q+1)! ). This argument

recovery of φi(x) with error O(q cq+1

8695

mapping. The model can be reformulated as x = P z,
where P ∈ R30×((3+p
p )−1), P ∼ N (0, 1), and z consists of
the polynomial features of s. Consider the following cases:

• Single nonlinear subspace Let p = 3, generate one P

and 100 s. Then the rank of X ∈ R30×100 is 19.

• Union of nonlinear subspaces Let p = 3, generate
three P and for each P generate 100 s. Then the rank
of X ∈ R30×300 is 30.

• Union of linear subspaces Let p = 1, generate ten P
and for each P generate 100 s. Then the rank of X ∈
R30×1000 is 30.

We randomly remove some portions of the entries of the
matrices and use matrix completion to recover the missing
entries. The performances are evaluated by the relative error

deﬁned as RE = kcX −XkF /kXkF [6], wherecX denotes

the recovered matrix. As shown in Figure 1, the recovery
errors of LRMC methods, i.e. LRF [26] and NNM [4], are
considerably high. In contrast, HRMC methods especially
our KFMC have signiﬁcantly lower recovery errors. In ad-
dition, our KFMC(Poly) and KFMC(RBF) are much more
efﬁcient than VMC [24] and NLMC [10], in which random-
ized SVD [12] has been performed.

Figure 2 shows the results of online matrix completion,
in which OL-DLSR (dictionary learning and sparse repre-
sentation) is an online matrix completion method we mod-
iﬁed from [21] and [11] and detailed in our supplemen-
tary material. We see that our method OL-KFMC outper-
formed other methods signiﬁcantly. Figures 3 shows the re-
sults of out-of-sample extension (OSE) of HRMC, in which
our OSE-KFMC outperformed other methods. More details
about the experiment/parameter settings and analysis are in
the supplementary material.

the low-order (≤ q) features of φ(x) with error O(q cq+1

provides the intuition that the RBF kernel should recover
(q+1)! )
provided that (36) holds. Of course, we can identify missing
entries of X by considering the ﬁrst block of the completed
matrix φ(X).

In experiments, we observe that the RBF kernel often
works better than polynomial kernel. We hypothesize two
reasons for the effectiveness of the RBF kernel: 1) It cap-
tures the higher-order features in φ(x), which could be use-
ful when n is very large 2) It is easier to analyze and to
optimize, speeding convergence.

Low rank matrix completion methods can only uniquely

complete a matrix given a sampling rate that satisﬁes

(38)

ρLRMC >(cid:0)(m + n)rX − r2

X(cid:1)/(mn),

where rX = min{m, n, u(cid:0)d+p

p (cid:1)}. This bound can be vacu-

ous (larger than 1) if u or p are large. In contrast, ρKFMC given
by (36) can still be smaller than 1 in the same regime, pro-
vided that n is large enough. For example, when m = 20,
d = 2, p = 2, and u = 3, we have ρLRMC > 0.91. Let
q = 2 and n = 300, we have ρKFMC > 0.56. If p = 1 and
u = 10, we have ρLRMC > 1 and ρKFMC > 0.64. This cal-
culation provides further intuition for how our methods can
recover high rank matrices while classical low rank matrix
completion methods fail.

3.9. Analytic functions and smooth functions

Hitherto, we have assumed that f is a ﬁnite order poly-
nomial function. However, our methos also work when f
is an analytic or smooth function. Analytic functions are
well approximated by polynomials. Furthermore, smooth
functions can be well approximated by polynomial func-
tions at least on intervals. Hence for a smooth function
h : Rd → Rm, we consider the generative model

x = h(s) = f(s) + e

(39)

where f is a p-order Taylor expansion of h and e ∈ Rm
denotes the residual, which scales as e ∼ O(
(p+1)! ) where
c is the p + 1th derivative of h.

c

We see that the error e from our polynomial model de-
creases as p increases. To ﬁt a model with larger p, the
bound (36) suggests we need more samples n. We con-
jecture that for any smooth h, it is possible to recover the
missing entries with arbitrarily low error provided n is suf-
ﬁciently large.

4. Experiments

4.1. Synthetic data

We generate the columns of X by x = f(s) where
s ∼ U (0, 1) and f : R3 → R30 is a p-order polynomial

Figure 1: Ofﬂine matrix completion on synthetic data

8696

Figure 2: Online matrix completion on synthetic data

Figure 3: Out-of-sample extension of matrix completion on
synthetic data

4.2. Hopkins155 data

Similar to [24], we consider the problem of subspace
clustering on incomplete data, in which the missing data of
Hopkins155 [27] were recovered by matrix completion and
then SSC (sparse subspace clustering [7]) was performed.
We consider two downsampled video sequences, 1R2RC
and 1R2TCR, each of which consists of 6 frames. The av-
erage clustering errors [7] of 10 repeated trials are reported
in Figure 4. Our method KFMC with RBF kernel is more
accurate and efﬁcient than VMC and NLMC.

Figure 4: Subspace clustering on incomplete data

4.3. CMU motion capture data

We use matrix completion to recover the missing data of
time-series trajectories of human motions (e.g. running and
jumping). Similar to [6, 24], we use the trial #6 of subject
#56 of the CMU motion capture dataset, which forms a
high rank matrix [6]. We consider two cases of incomplete
data, randomly missing and continuously missing. More
details about the experimental settings are in the supple-
mentary material. The average results of 10 repeated trials
are reported in Figure 5. We see that HRMC methods out-

performed LRMC methods while online methods outper-
formed ofﬂine methods. One reason is that the structure of
the data changes with time (corresponding to different mo-
tions) and online methods can adapt to the changes. Com-
paring Figure 5 with the Figure 4 of [6], we ﬁnd that VMC,
NLMC, and our KFMC outperformed the method proposed
in [6].
In addition, our OL-KFMC especially with RBF
kernel is the most accurate one. Regarding the computa-
tional cost, there is no doubt that the linear methods includ-
ing LRF, NNM, GROUSE, and DLSR are faster than other
methods. Hence we only show the computational cost of
the nonlinear methods in Table 2 for comparison (random-
ized SVD [12] has been performed in VMC and NLMC).
Our KFMC is faster than VMC and NLMC while our OL-
KFMC is at least 10 times faster than all methods.

Figure 5: CMU motion capture data recovery

370
VMC
KFMC(RBF) 190

NLMC
OL-KFMC(Poly) 16

610

KFMC(Poly)
OL-KFMC(RBF) 19

170

Table 2: Time cost (second) on CMU motion capture data

5. Conclusion

In this paper, we proposed kernelized factorization ma-
trix completion (KFMC), a new method for high rank ma-
trix completion, together with an online version and an
out-of-sample extension, which outperform state-of-the-art
methods. Our numerics demonstrate the success of the
method for motion data recovery. We believe our meth-
ods will also be useful for transductive learning (classiﬁca-
tion), vehicle/robot/chemistry sensor signal denoising, rec-
ommender systems, and biomedical data recovery.

Acknowledgement

This work was supported in part by DARPA Award

FA8750-17-2-0101.

8697

References

[1] Xavier Alameda-Pineda, Elisa Ricci, Yan Yan, and Nicu
Sebe. Recognizing emotions from abstract paintings using
non-linear matrix completion. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 5240–5248, 2016. 1

[2] Laura Balzano, Robert Nowak, and Benjamin Recht. Online
identiﬁcation and tracking of subspaces from highly incom-
plete information.
In Communication, Control, and Com-
puting (Allerton), 2010 48th Annual Allerton Conference on,
pages 704–711. IEEE, 2010. 1

[3] Srinadh Bhojanapalli and Prateek Jain. Universal matrix
completion. In Proceedings of The 31st International Con-
ference on Machine Learning, pages 1881–1889. JMLR,
2014. 1

[4] Emmanuel J. Cand`es and Benjamin Recht. Exact matrix
completion via convex optimization. Foundations of Com-
putational Mathematics, 9(6):717–772, 2009. 1, 7

[5] Charanpal Dhanjal, Romaric Gaudel,

and St´ephan
Cl´emenc¸on. Online matrix completion through nuclear
norm regularisation.
In Proceedings of the 2014 SIAM
International Conference on Data Mining, pages 623–631.
SIAM, 2014. 1

[6] Ehsan Elhamifar. High-rank matrix completion and cluster-
ing under self-expressive models. In Advances in Neural In-
formation Processing Systems 29, pages 73–81, 2016. 1, 2,
7, 8

[7] E. Elhamifar and R. Vidal. Sparse subspace clustering: Al-
gorithm, theory, and applications. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 35(11):2765–2781,
2013. 1, 8

[8] Brian Eriksson, Laura Balzano, and Robert Nowak. High-
rank matrix completion. In Artiﬁcial Intelligence and Statis-
tics, pages 373–381, 2012. 1

[9] Jicong Fan and Tommy W.S. Chow. Matrix completion by
least-square, low-rank, and sparse self-representations. Pat-
tern Recognition, 71:290 – 305, 2017. 1

[10] Jicong Fan and Tommy W.S. Chow. Non-linear matrix com-

pletion. Pattern Recognition, 77:378 – 394, 2018. 1, 2, 7

[11] J. Fan, M. Zhao, and T. W. S. Chow. Matrix completion
via sparse factorization solved by accelerated proximal alter-
nating linearized minimization. IEEE Transactions on Big
Data, pages 1–1, 2018. 1, 7

[12] N. Halko, P. G. Martinsson, and J. A. Tropp.

Finding
structure with randomness: Probabilistic algorithms for con-
structing approximate matrix decompositions. SIAM Review,
53(2):217–288, 2011. 7, 8

[13] Moritz Hardt. Understanding alternating minimization for
matrix completion.
In Foundations of Computer Science
(FOCS), 2014 IEEE 55th Annual Symposium on, pages 651–
660. IEEE, 2014. 1

[14] Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable
efﬁcient online matrix completion via non-convex stochastic
gradient descent.
In Advances in Neural Information Pro-
cessing Systems, pages 4520–4528, 2016. 1

[15] Jaya Kawale, Hung H Bui, Branislav Kveton, Long Tran-
Thanh, and Sanjay Chawla. Efﬁcient thompson sampling for

In Advances
online matrix-factorization recommendation.
in neural information processing systems, pages 1297–1305,
2015. 1

[16] R. H. Keshavan, A. Montanari, and S. Oh. Matrix comple-
tion from a few entries. IEEE Transactions on Information
Theory, 56(6):2980–2998, June 2010. 1

[17] Chun-Guang Li and Rene Vidal. A structured sparse
plus structured low-rank framework for subspace clustering
and completion.
IEEE Transactions on Signal Processing,
64(24):6557–6570, 2016. 1

[18] Guangcan Liu and Ping Li. Low-rank matrix completion in
the presence of high coherence. IEEE Transactions on Signal
Processing, 64(21):5623–5633, 2016. 1

[19] Brian Lois and Namrata Vaswani. Online matrix completion
and online robust pca. In Information Theory (ISIT), 2015
IEEE International Symposium on, pages 1826–1830. IEEE,
2015. 1

[20] C. Lu, J. Tang, S. Yan, and Z. Lin. Generalized noncon-
vex nonsmooth low-rank minimization. In 2014 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4130–4137, June 2014. 1

[21] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo
Sapiro. Online dictionary learning for sparse coding.
In
Proceedings of the 26th annual international conference on
machine learning, pages 689–696. ACM, 2009. 1, 7

[22] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani.
Spectral regularization algorithms for learning large incom-
plete matrices.
Journal of machine learning research,
11(Aug):2287–2322, 2010. 1

[23] Feiping Nie, Heng Huang, and Chris Ding. Low-rank matrix
recovery via efﬁcient schatten p-norm minimization. In Pro-
ceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial
Intelligence, pages 655–661. AAAI Press, 2012. 1

[24] Greg Ongie, Rebecca Willett, Robert D. Nowak, and Laura
Balzano. Algebraic variety models for high-rank matrix
completion. In Proceedings of the 34th International Confer-
ence on Machine Learning, pages 2691–2700. PMLR, 2017.
1, 2, 6, 7, 8

[25] Steffen Rendle and Lars Schmidt-Thieme. Online-updating
regularized kernel matrix factorization models for large-
scale recommender systems.
In Proceedings of the 2008
ACM conference on Recommender systems, pages 251–258.
ACM, 2008. 1

[26] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix comple-
tion via non-convex factorization. IEEE Transactions on In-
formation Theory, 62(11):6535–6579, 2016. 1, 7

[27] R. Tron and R. Vidal. A benchmark for the comparison of
3-d motion segmentation algorithms.
In 2007 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1–8, June 2007. 8

[28] Congyuan Yang, Daniel Robinson, and Rene Vidal. Sparse
subspace clustering with missing entries.
In International
Conference on Machine Learning, pages 2463–2472, 2015.
1

[29] Se-Young Yun, Marc Lelarge, and Alexandre Proutiere.
Streaming, memory limited matrix completion with noise.
arXiv preprint arXiv:1504.03156, 2015. 1

8698

