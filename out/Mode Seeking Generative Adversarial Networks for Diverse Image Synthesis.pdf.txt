Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis

Qi Mao∗1, Hsin-Ying Lee∗2, Hung-Yu Tseng∗2, Siwei Ma1

,

3, Ming-Hsuan Yang2

4

,

1Institute of Digital Media, Peking University
3Peng Cheng Laboratory

2University of California, Merced
4Google Cloud

MSGANs

cGANs

Input

Image

Latent code z

This bird has 

feathers that are 
black and has a red 

belly

Text 

Figure 1: Mode seeking generative adversarial networks (MSGANs). (Left) Existing conditional generative adversarial
networks tend to ignore the input latent code z and generate images of similar modes. (Right) We propose a simple yet
effective mode seeking regularization term that can be applied to arbitrary conditional generative adversarial networks in
different tasks to alleviate the mode collapse issue and improve the diversity.

Abstract

1. Introduction

Most conditional generation tasks expect diverse out-
puts given a single conditional context. However, condi-
tional generative adversarial networks (cGANs) often focus
on the prior conditional information and ignore the input
noise vectors, which contribute to the output variations. Re-
cent attempts to resolve the mode collapse issue for cGANs
are usually task-speciﬁc and computationally expensive. In
this work, we propose a simple yet effective regularization
term to address the mode collapse issue for cGANs. The
proposed method explicitly maximizes the ratio of the dis-
tance between generated images with respect to the cor-
responding latent codes, thus encouraging the generators
to explore more minor modes during training. This mode
seeking regularization term is readily applicable to vari-
ous conditional generation tasks without imposing training
overhead or modifying the original network structures. We
validate the proposed algorithm on three conditional image
synthesis tasks including categorical generation, image-to-
image translation, and text-to-image synthesis with different
baseline models. Both qualitative and quantitative results
demonstrate the effectiveness of the proposed regularization
method for improving diversity without loss of quality.

∗ Equal contribution

Generative adversarial networks (GANs) [8] have been
shown to capture complex and high-dimensional image data
with numerous applications effectively. Built upon GANs,
conditional GANs (cGANs) [20] take external information
as additional inputs. For image synthesis, cGANs can be
applied to various tasks with different conditional contexts.
With class labels, cGANs can be applied to categorical im-
age generation. With text sentences, cGANs can be applied
to text-to-image synthesis [22, 29]. With images, cGANs
have been used in tasks including image-to-image transla-
tion [10, 11, 14, 16, 31, 32], semantic manipulation [28] and
style transfer [15].

For most conditional generation tasks, the mappings are
in nature multimodal, i.e., a single input context corre-
sponds to multiple plausible outputs. A straightforward ap-
proach to handle multimodality is to take random noise vec-
tors along with the conditional contexts as inputs, where the
contexts determine the main content and noise vectors are
responsible for variations. For instance, in the dog-to-cat
image-to-image translation task [14], the input dog images
decide contents like orientations of heads and positions of
facial landmarks, while the noise vectors help the genera-
tion of different species. However, cGANs usually suffer
from the mode collapse [8, 24] problem, where generators

1429

only produce samples from a single or few modes of the
distribution and ignore other modes. The noise vectors are
ignored or of minor impacts, since cGANs pay more at-
tention to learn from the high-dimensional and structured
conditional contexts.

There are two main approaches to address the mode
collapse problem in GANs. A number of methods focus
on discriminators by introducing different divergence met-
rics [1, 18] and optimization process [6, 19, 24]. The other
methods use auxiliary networks such as multiple genera-
tors [7, 17] and additional encoders [2, 4, 5, 25]. However,
mode collapse is relatively less studied in cGANs. Some
recent efforts have been made in the image-to-image trans-
lation task to improve diversity [10, 14, 32]. Similar to the
second category with the unconditional setting, these ap-
proaches introduce additional encoders and loss functions
to encourage the one-to-one relationship between the out-
put and the latent code. These methods either entail heavy
computational overheads on training or require auxiliary
networks that are often task-speciﬁc that cannot be easily
extended to other frameworks.

In this work, we propose a mode seeking regularization
method that can be applied to cGANs for various tasks to
alleviate the mode collapse problem. Given two latent vec-
tors and the corresponding output images, we propose to
maximize the ratio of the distance between images with re-
spect to the distance between latent vectors. In other words,
this regularization term encourages generators to generate
dissimilar images during training. As a result, generators
can explore the target distribution, and enhance the chances
of generating samples from different modes. On the other
hand, we can train the discriminators with dissimilar gen-
erated samples to provide gradients from minor modes that
are likely to be ignored otherwise. This mode seeking regu-
larization method incurs marginal computational overheads
and can be easily embedded in different cGAN frameworks
to improve the diversity of synthesized images.

We validate the proposed regularization algorithm
through an extensive evaluation of three conditional image
synthesis tasks with different baseline models. First, for cat-
egorical image generation, we apply the proposed method
on DCGAN [21] using the CIFAR-10 [12] dataset. Second,
for image-to-image translation, we embed the proposed reg-
ularization scheme in Pix2Pix [11] and DRIT [14] using the
facades [3], maps [11], Yosemite [31], and cat⇋dog [14]
datasets. Third, for text-to-image synthesis, we incorporate
StackGAN++ [29] with the proposed regularization term
using the CUB-200-2011 [27] dataset. We evaluate the
diversity of synthesized images using perceptual distance
metrics [30].

However, the diversity metric alone cannot guarantee
the similarity between the distribution of generated images
and the distribution of real data. Therefore, we adopt two

recently proposed bin-based metrics [23], the Number of
Statistically-Different Bins (NDB) metric which determines
the relative proportions of samples fallen into clusters pre-
determined by real data, and the Jensen-Shannon Diver-
gence (JSD) distance which measures the similarity be-
tween bin distributions. Furthermore, to verify that we do
not achieve diversity at the expense of realism, we evaluate
our method with the Fr´echet Inception Distance (FID) [9]
as the metric for quality. Experimental results demonstrate
that the proposed regularization method can facilitate exist-
ing models from various applications achieving better di-
versity without loss of image quality. Figure 1 shows the
effectiveness of the proposed regularization method for ex-
isting models.

The main contributions of this work are:
• We propose a simple yet effective mode seeking
regularization method to address the mode collapse
problem in cGANs. This regularization scheme can
be readily extended into existing frameworks with
marginal training overheads and modiﬁcations.

• We demonstrate the generalizability of the proposed
regularization method on three different conditional
generation tasks: categorical generation,
image-to-
image translation, and text-to-image synthesis.

• Extensive experiments show that the proposed method
can facilitate existing models from different
tasks
achieving better diversity without sacriﬁcing visual
quality of the generated images.

Our code and pre-trained models are available at

https://github.com/HelenMao/MSGAN/.

2. Related Work

Conditional generative adversarial networks. Genera-
tive adversarial networks [1, 8, 18, 21] have been widely
used for image synthesis. With adversarial training, gener-
ators are encouraged to capture the distribution of real im-
ages. On the basis of GANs, conditional GANs synthesize
images based on various contexts. For instances, cGANs
can generate high-resolution images conditioned on low-
resolution images [13], translate images between different
visual domains [10, 11, 14, 16, 31, 32], generate images
with desired style [15], and synthesize images according to
sentences [22, 29]. Although cGANs have achieved success
in various applications, existing approaches suffer from the
mode collapse problem. Since the conditional contexts pro-
vide strong structural prior information for the output im-
ages and have higher dimensions than the input noise vec-
tors, generators tend to ignore the input noise vectors, which
are responsible for the variation of generated images. As a
result, the generators are prone to produce images with sim-
ilar appearances. In this work, we aim to address the mode
collapse problem for cGANs.

1430

Real data

M2

M4

Mode collapse
M2

M4

Mode seeking

M2

M4

d

I

(I , I )

a

b

M1

M3

M5

M1

Image space I  

I1

I2

I3

I1

I2

I3

d z

(

z

,

a

z
b

)
0.68
0.58
0.62
0.17

Image data 
distribution

Latent code 
distribution

Mode M Image I

z1 z2

z3

Latent space Z 

Figure 2: Illustration of motivation. Real data distribution contains numerous modes. However, when mode collapse occurs,
generators only produce samples from a few modes. From the data distribution when mode collapse occurs, we observe that
for latent vectors z1 and z2, the distance between their mapped images I1 and I2 will become shorter in a disproportionate
rate when the distance between two latent vectors is decreasing. We present on the right the ratio of the distance between
images with respect to the distance of the corresponding latent vectors, where we can spot an anomalous case (colored in red)
where mode collapse occurs. The observation motivates us to leverage the ratio as the training objective explicitly.

Reducing mode collapse. Some methods focus on the
discriminator with different optimization process [19] and
divergence metrics [1, 18] to stabilize the training process.
The minibatch discrimination scheme [24] allows the dis-
criminator to discriminate between whole mini-batches of
samples instead of between individual samples. In [6], Du-
rugkar et al. use multiple discriminators to address this is-
sue. The other methods use auxiliary networks to alle-
viate the mode collapse issue. ModeGAN [2] and VEE-
GAN [25] enforce the bijection mapping between the in-
put noise vectors and generated images with additional en-
coder networks. Multiple generators [7] and weight-sharing
generators [17] are developed to capture more modes of
the distribution. However, these approaches either entail
heavy computational overheads or require modiﬁcations of
the network structure, and may not be easily applicable to
cGANs.

In the ﬁeld of cGANs, some efforts [10, 14, 32] have
been recently made to address the mode collapse issue on
the image-to-image translation task. Similar to ModeGAN
and VEEGAN, additional encoders are introduced to pro-
vide a bijection constraint between the generated images
and input noise vectors. However, these approaches require
other task-speciﬁc networks and objective functions. The
additional components make the methods less generalizable
and incur extra computational loads on training.
In con-
trast, we propose a simple regularization term that imposes
no training overheads and requires no modiﬁcations of the

network structure. Therefore, the proposed method can be
readily applied to various conditional generation tasks.

3. Diverse Conditional Image Synthesis

3.1. Preliminaries

The training process of GANs can be formulated as a
mini-max problem: a discriminator D learns to be a clas-
siﬁer by assigning higher discriminative values to the real
data samples and lower ones to the generated ones. Mean-
while, a generator G aims to fool D by synthesizing real-
istic examples. Through adversarial training, the gradients
from D will guide G toward generating samples with the
distribution similar to the real data one.

The mode collapse problem with GANs is well known
in the literature. Several methods [2, 24, 25] attribute the
missing mode to the lack of penalty when this issue occurs.
Since all modes usually have similar discriminative values,
larger modes are likely to be favored through the training
process based on gradient descent. On the other hand, it is
difﬁcult to generate samples from minor modes.

The mode missing problem becomes worse in cGANs.
Generally, conditional contexts are high-dimensional and
structured (e.g., images and sentences) as opposed to the
noise vectors. As such, the generators are likely to focus on
the contexts and ignore the noise vectors, which account for
diversity.

1431

3.2. Mode Seeking GANs

In this work, we propose to alleviate the missing mode
problem from the generator perspective. Figure 2 illus-
trates the main ideas of our approach. Let a latent vec-
tor z from the latent code space Z be mapped to the im-
age space I. When mode collapse occurs, the mapped im-
ages are collapsed into a few modes. Furthermore, when
two latent codes z1 and z2 are closer, the mapped images
I1 = G(c, z1) and I2 = G(c, z2) are more likely to be col-
lapsed into the same mode. To address this issue, we pro-
pose a mode seeking regularization term to directly maxi-
mize the ratio of the distance between G(c, z1) and G(c, z2)
with respect to the distance between z1 and z2,

Lms = max

G

(

dI(G(c, z1), G(c, z2))

dz(z1, z2)

),

(1)

where d∗(·) denotes the distance metric.

The regularization term offers a virtuous circle for train-
ing cGANs. It encourages the generator to explore the im-
age space and enhances the chances for generating samples
of minor modes. On the other hand, the discriminator is
forced to pay attention to generated samples from minor
modes. Figure 2 shows a mode collapse situation where
two close samples, z1 and z2, are mapped onto the same
mode M2. However, with the proposed regularization term,
z1 is mapped to I1, which belongs to an unexplored mode
M1. With the adversarial mechanism, the generator will
thus have better chances to generate samples of M1 in the
following training steps.

As shown in Figure 3, the proposed regularization term
can be easily integrated with existing cGANs by appending
it to the original objective function.

Lnew = Lori + λmsLms,

(2)

where Lori denotes the original objective function and λms
the weights to control the importance of the regularization.
Here, Lori can be as a simple loss function. For example, in
categorical generation task,

Lori = Ec,y[log D(c, y)] + Ec,z[log (1 − D(c, G(c, z)))],
(3)
where c, y, z denote class labels, real images, and noise vec-
tors, respectively. In image-to-image translation task [11],

Lori = LGAN + Ex,y,z[ky − G(x, z)k1],

(4)

where x denotes input images and LGAN is the typical GAN
loss. Lori can be arbitrary complex objective function from
any task, as shown in Figure 3 (b). We name the proposed
method as Mode Seeking GANs (MSGANs).

4. Experiments

We evaluate the proposed regularization method through
extensive quantitative and qualitative evaluation. We ap-

this bird has 

feathers 

that are black and 

has a red belly
$"

Latent code

$#

(a) Proposed regularization

1

StackGAN++

!"

!#

max

()(G(c, z"), G(c, z#))

(0(z", z#)

(b) Applying proposed regularization on StackGAN++

Figure 3: Proposed regularization. (a) We propose a reg-
ularization term that maximizes the ratio of the distance
between generated images with respect to the distance be-
tween their corresponding input latent codes. (b) The pro-
posed regularization method can be applied to arbitrary
cGANs. Take StackGAN++ [29], a model for text-to-image
synthesis, as an example, we easily apply the regulariza-
tion term regardless of the complex tree-like structure of
the original model.

ply MSGANs to the baseline models from three representa-
tive conditional image synthesis tasks: categorical genera-
tion, image-to-image translation, and text-to-image synthe-
sis. Note that we augment the original objective functions
with the proposed regularization term while maintaining
original network architectures and hyper-parameters. We
employ L1 norm distance as our distance metrics for both
dI and dz and set the hyper-parameter λms = 1 in all exper-
iments. More implementation and evaluation details, please
refer to the supplementary material.

4.1. Evaluation Metrics

We conduct evaluations using the following metrics.
FID. To evaluate the quality of the generated images, we
use FID [9] to measure the distance between the generated
distribution and the real one through features extracted by
Inception Network [26]. Lower FID values indicate better
quality of the generated images.
LPIPS. To evaluate diversity, we employ LPIPS [30] fol-
lowing [10, 14, 32]. LIPIS measures the average feature
distances between generated samples. Higher LPIPS score
indicates better diversity among the generated images.
NDB and JSD. To measure the similarity between the dis-

1432

Table 1: NDB and JSD results on the CIFAR-10 dataset.

Metrics Models
DCGAN
MSGAN

NDB ↓

airplane

49.60 ± 3.43
46.60 ± 7.40

automobile

53.00 ± 7.28
51.80 ± 2.28

bird

cat

deer

34.40 ± 6.11
39.40 ± 1.95

46.00 ± 1.41
41.80 ± 3.70

44.80 ± 3.90
46.80 ± 4.92

JS ↓

0.034 ± 0.001
DCGAN
MSGAN 0.031 ± 0.001

0.035 ± 0.002
0.033 ± 0.001

0.025 ± 0.002
0.027 ± 0.001

0.030 ± 0.002
0.027 ± 0.001

0.033 ± 0.001
0.035 ± 0.003

NDB ↓

DCGAN
MSGAN

50.40 ± 4.62
33.80 ± 3.27

52.00 ± 3.81
42.00 ± 2.92

54.40 ± 4.04
47.60 ± 5.03

42.80 ± 5.45
41.00 ± 2.92

47.80 ± 4.55
43.80 ± 6.61

dog

frog

horse

ship

truck

JS ↓

0.033 ± 0.001
DCGAN
MSGAN 0.024 ± 0.001

0.034 ± 0.002
0.030 ± 0.002

0.035 ± 0.001
0.033 ± 0.003

0.029 ± 0.003
0.027 ± 0.001

0.032 ± 0.001
0.029 ± 0.003

Table 2: FID results on the CIFAR-10 dataset.

Model

DCGAN

MSGAN

FID↓

29.65 ± 0.06

28.73 ± 0.06

tribution between real images and generated one, we adopt
two bin-based metrics, NDB and JSD, proposed in [23].
These metrics evaluate the extent of mode missing of gen-
erative models. Following [23], the training samples are
ﬁrst clustered using K-means into different bins which can
be viewed as modes of the real data distribution. Then
each generated sample is assigned to the bin of its near-
est neighbor. We calculate the bin-proportions of the train-
ing samples and the synthesized samples to evaluate the
difference between the generated distribution and the real
data distribution. NDB score and JSD of the bin-proportion
are then computed to measure the mode collapse. Lower
NDB score and JSD mean the generated data distribution
approaches the real data distribution better by ﬁtting more
modes. Please refer to [23] for more details.

4.2. Conditioned on Class Label

We ﬁrst validate the proposed method on categorical
generation. In categorical generation, networks take class
labels as conditional contexts to synthesize images of dif-
ferent categories. We apply the regularization term to the
baseline framework DCGAN [21].

We conduct experiments on the CIFAR-10 [12] dataset
which includes images of ten categories. Since images in
the CIFAR-10 dataset are of size 32 × 32 and upsampling
degrades the image quality, we do not compute LPIPS in
this task. Table 1 and Table 2 present the results of NDB,
JSD, and FID. MSGAN mitigates the mode collapse issue
in most classes while maintaining image quality.

4.3. Conditioned on Image

Image-to-image translation aims to learn the mapping
between two visual domains. Conditioned on images from
the source domain, models attempt to synthesize corre-

Figure 4: Diversity comparison. The proposed regulariza-
tion term helps Pix2Pix learn more diverse results.

Table 3: Quantitative results on the facades and maps
dataset.

Pix2Pix [11]

Facades
MSGAN

BicycleGAN [32]

139

.

.

14

19 ± 2
40 ± 1
074 ± 0
0003 ± 0

.

.

.

.

0

94

.

82

012

.

.

12

92

84 ± 1
40 ± 0
038 ± 0
1894 ± 0

.

.

.

0

00

55

.

.

004

0000 0

0011 0

.

.

98

13

85 ± 1
80 ± 0
058 ± 0
1413 ± 0

.

.

.

0

.

21

45

.

.

004

0005

Datasets

FID ↓
NDB↓
JSD↓

LPIPS↑ 0

.

Datasets

FID ↓
NDB↓
JSD↓

Pix2Pix [11]

168

.

.

49

99 ± 2
00 ± 1
088 ± 0
0016 ± 0

.

.

.

.

0

58

.

00

018

.

.

Maps

MSGAN

BicycleGAN [32]

.

.

41

152

43 ± 2
60 ± 0
031 ± 0
2189 ± 0

0

.

.

.

52

.

55

.

003

145

90

.

34

002

.

.

46

78 ± 3
60 ± 1
023 ± 0
1150 ± 0

.

.

.

.

0007

0

LPIPS↑ 0

.

0003 0

0004 0

.

sponding images in the target domain. Despite the multi-
modal nature of the image-to-image translation task, early

1433

Figure 5: Diversity comparison. We compare MSGAN with DRIT on the dog-to-cat, cat-to-dog, and winter-to-summer
translation tasks. Our model produces more diverse samples over DRIT.

Train ± standard error

DRIT

MSGAN

space. To demonstrate the generalizability, we apply the
proposed method to a unimodal model Pix2Pix [11] using
paired training data and a multimodal model DRIT [14] us-
ing unpaired images.

 

n
o
i
t
r
o
p
o
r
P
n
B

i

 

5
1
.
0

0
0
0

.

0

10

20
Bin (Mode)

30

40

Figure 6: Visualization of the bins on dog→cat trans-
lation. The translated results of DRIT collapse into few
modes, while the generated image of MSGAN ﬁt the real
data distribution better.

work [11, 31] abandons noise vectors and performs one-to-
one mapping since the latent codes are easily ignored dur-
ing training as shown in [11, 32]. To achieve multimodality,
several recent attempts [10, 14, 32] introduce additional en-
coder networks and objective functions to impose a bijec-
tion constraint between the latent code space and the image

4.3.1 Conditioned on Paired Images

We take Pix2Pix as the baseline model. We also compare
MSGAN to BicycleGAN [32] which generates diverse im-
ages with paired training images. For fair comparisons,
architectures of the generator and the discriminator in all
methods follow the ones in BicycleGAN [32].

We conduct experiments on the facades and maps
datasets. MSGAN obtains consistent improvements on all
metrics over Pix2Pix. Moreover, MSGAN demonstrates
comparable diversity to BicycleGAN, which applies an ad-
ditional encoder network. Figure. 4 and Table. 3 demon-
strate the qualitative and quantitative results, respectively.

4.3.2 Conditioned on Unpaired Images

We choose DRIT [14], one of the state-of-the-art frame-
works to generate diverse images with unpaired training

1434

Figure 7: Diversity comparison. We show examples of StackGAN++ [29] and MSGAN on the CUB-200-2011 dataset of
text-to-image synthesis. When the text code is ﬁxed, the latent codes in MSGAN help to generate more diverse appearances
and poses of birds as well as different backgrounds.

Table 4: Quantitative results of the Yosemite (Summer⇋Winter) and the Cat⇋Dog dataset.

Datasets

Summer2Winter

Winter2Summer

FID ↓
NDB↓
JSD↓
LPIPS↑

Datasets

FID↓
NDB↓
JSD↓
LPIPS↑

DRIT [14]

MSGAN

DRIT [14]

MSGAN

57.24 ± 2.03
25.60 ± 1.14
0.066 ± 0.005
0.1150 ± 0.0003

51.85 ± 1.16
22.80 ± 2.96
0.046 ± 0.006

0.1468 ± 0.0005

47.37 ± 3.25
30.60 ± 2.97
0.049 ± 0.009
0.0965 ± 0.0004

46.23 ± 2.45
27.80 ± 3.03
0.038 ± 0.004

0.1183 ± 0.0007

Cat2Dog

Dog2Cat

DRIT [14]

22.74 ± 0.28
42.00 ± 2.12
0.127 ± 0.003
0.245 ± 0.002

MSGAN

16.02 ± 0.30
27.20 ± 0.84
0.084 ± 0.002
0.280 ± 0.002

DRIT [14]

62.85 ± 0.21
41.00 ± 0.71
0.272 ± 0.002
0.102 ± 0.001

MSGAN

29.57 ± 0.23
31.00 ± 0.71
0.068 ± 0.001
0.214 ± 0.001

data, as the baseline framework. Though DRIT synthe-
sizes diverse images in most cases, mode collapse occurs
in some challenging shape-variation cases (e.g., transla-
tion between cats and dogs). To demonstrate the robust-
ness of the proposed method, we evaluate on the shape-
preserving Yosemite (summer⇋winter) [31] dataset and the
cat⇋dog [14] dataset that requires shape variations.

As the quantitative results exhibited in Table. 4, MSGAN
performs favorably against DRIT in all metrics on both
datasets. Especially in the challenging cat⇋dog dataset,
MSGAN obtains substantial diversity gains. From the sta-
tistical point of view, we visualize the bin proportions of
the dog-to-cat translation in Figure. 6. The graph shows the
severe mode collapse issue of DRIT and the substantial im-

provement with the proposed regularization term. Qualita-
tively, Figure. 5 shows that MSGAN discovers more modes
without the loss of visual quality.

4.4. Conditioned on Text

Text-to-image synthesis targets at generating images
conditioned on text descriptions. We integrate the proposed
regularization term on StackGAN++ [29] using the CUB-
200-2011 [27] dataset. To improve diversity, StackGAN++
introduces a Conditioning Augmentation (CA) module that
re-parameterizes text descriptions into text codes of the
Gaussian distribution. Instead of applying the regulariza-
tion term on the semantically meaningful text codes, we fo-
cus on exploiting the latent codes randomly sampled from

1435

Table 5: Quantitative results on the CUB-200-2011 dataset. We conduct experiments in two settings: 1) Conditioned on
text descriptions, where every description can be mapped to different text codes. 2) Conditioned on text codes, where the text
codes are ﬁxed so that their effects are excluded.

Conditioned on text descriptions

Conditioned on text codes

StackGAN++ [29]

MSGAN

StackGAN++ [29]

MSGAN

FID ↓
NDB↓
JSD↓
LPIPS↑

25.99 ± 4.26
38.20 ± 2.39
0.092 ± 0.005
0.362 ± 0.004

25.53 ± 1.83
30.60 ± 2.51
0.073 ± 0.003
0.373 ± 0.007

27.12 ± 1.15
39.00 ± 0.71
0.102 ± 0.016
0.156 ± 0.004

27.94 ± 3.10
30.60 ± 2.41
0.095 ± 0.016
0.207 ± 0.005

Figure 8: Linear interpolation between two latent codes in MSGAN. Image synthesis results with linear-interpolation
between two latent codes in the dog-to-cat translation and text-to-image synthesis.

the prior distribution. However, for a fair comparison, we
evaluation MSGAN against StackGAN++ in two settings:
1) Perform generation without ﬁxing text codes for text de-
scriptions. In this case, text codes also provide variations
for output images. 2) Perform generation with ﬁxed text
codes. In this setting, the effects of text codes are excluded.

Table. 5 presents quantitative comparisons between MS-
GAN and StackGAN++. MSGAN improves the diversity
of StackGAN++ and maintains visual quality. To better il-
lustrate the role that latent codes play for the diversity, we
show qualitative comparisons with the text codes ﬁxed. In
this setting, we do not consider the diversity resulting from
CA. Figure. 7 illustrates that latent codes of StackGAN++
have minor effects on the variations of the image. On the
contrary, latent codes of MSGAN contribute to various ap-
pearances and poses of birds.

4.5. Interpolation of Latent Space in MSGANs

We perform linear interpolation between two given la-
tent codes and generate corresponding images to have a bet-
ter understanding of how well MSGANs exploit the latent
space. Figure. 8 shows the interpolation results on the dog-
to-cat translation and the text-to-image synthesis task. In
the dog-to-cat translation, we can see the coat colors and
patterns varies smoothly along with the latent vectors. In
the text-to-image synthesis, both orientations of birds and
the appearances of footholds change gradually with the vari-
ations of the latent codes.

5. Conclusions

In this work, we present a simple but effective mode
seeking regularization term on the generator to address the
model collapse in cGANs. By maximizing the distance be-
tween generated images with respect to that between the
corresponding latent codes, the regularization term forces
the generators to explore more minor modes. The proposed
regularization method can be readily integrated with ex-
isting cGANs framework without imposing training over-
heads and modiﬁcations of network structures. We demon-
strate the generalizability of the proposed method on three
different conditional generation tasks including categorical
generation, image-to-image translation, and text-to-image
synthesis. Both qualitative and quantitative results show
that the proposed regularization term facilitates the base-
line frameworks improving the diversity without sacriﬁcing
visual quality of the generated images.

Acknowledgements. This work is supported in part by the
NSF CAREER Grant # 1149783, gifts from Verisk, Adobe
and NEC, the National Basic Research Program of China
(973 Program, 2015CB351800), National Natural Science
Foundation of China (61632001), and High-performance
Computing Platform of Peking University, which are grate-
fully acknowledged. We additionally thank NVIDIA for
generously providing DGX-1 super-computer and support
through the NVAIL program.

1436

[19] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
In

Dickstein. Unrolled generative adversarial networks.
ICLR, 2017.

[20] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv preprint arXiv:1411.1784, 2014.

[21] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016.

[22] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML, 2016.

[23] Eitan Richardson and Yair Weiss. On GANs and GMMs. In

NIPS, 2018.

[24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training GANs. In NIPS, 2016.

[25] Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U.
Gutmann, and Charles Sutton. VEEGAN: Reducing mode
collapse in GANs using implicit variational learning.
In
NIPS, 2017.

[26] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, 2015.

[27] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. Technical Report CNS-TR-2011-001, California In-
stitute of Technology, 2011.

[28] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional GANs. In
CVPR, 2018.

[29] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stack-
GAN++: Realistic image synthesis with stacked generative
adversarial networks. TPAMI, 2018.

[30] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In CVPR, 2018.

[31] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017.

[32] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A. Efros, Oliver Wang, and Eli Shechtman.
Toward multimodal image-to-image translation.
In NIPS,
2017.

References

[1] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
In ICML,

Wasserstein generative adversarial networks.
2017.

[2] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio,
and Wenjie Li. Mode regularized generative adversarial net-
works. In ICLR, 2017.

[3] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016.

[4] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-

versarial feature learning. In ICLR, 2017.

[5] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier
Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. In ICLR, 2017.

[6] Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Gener-

ative multi-adversarial networks. In ICLR, 2017.

[7] Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri,
Philip H.S. Torr, and Puneet K. Dokania. Multi-agent di-
verse generative adversarial networks. In CVPR, 2018.

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. GANs trained by
a two time-scale update rule converge to a local nash equi-
librium. In NIPS, 2017.

[10] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
In

Multimodal unsupervised image-to-image translation.
ECCV, 2018.

[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017.

[12] Alex Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, Citeseer, 2009.

[13] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR, 2017.

[14] Hsin-Ying Lee, Hung-Yu Tseng,

Jia-Bin Huang, Ma-
neesh Kumar Singh, and Ming-Hsuan Yang. Diverse image-
to-image translation via disentangled representations.
In
ECCV, 2018.

[15] Chuan Li and Michael Wand. Precomputed real-time texture
synthesis with markovian generative adversarial networks. In
ECCV, 2016.

[16] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised

image-to-image translation networks. In NIPS, 2017.

[17] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversar-

ial networks. In NIPS, 2016.

[18] Xudong Mao, Qing Li, Haoran Xie, Raymond YK, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In ICCV, 2017.

1437

