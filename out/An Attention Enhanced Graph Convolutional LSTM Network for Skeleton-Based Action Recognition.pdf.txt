An Attention Enhanced Graph Convolutional LSTM Network for

Skeleton-Based Action Recognition

Chenyang Si1

2

,

Wentao Chen1

3

,

Wei Wang1

2 ∗

,

Liang Wang1

2

,

Tieniu Tan1

2

3

,

,

1Center for Research on Intelligent Perception and Computing (CRIPAC),

National Laboratory of Pattern Recognition (NLPR),

Institute of Automation, Chinese Academy of Sciences (CASIA)

2University of Chinese Academy of Sciences (UCAS)

3University of Science and Technology of China (USTC)

{chenyang.si, wentao.chen}@cripac.ia.ac.cn, {wangwei, wangliang, tnt}@nlpr.ia.ac.cn

Abstract

Skeleton-based action recognition is an important task
that requires the adequate understanding of movemen-
t characteristics of a human action from the given skele-
ton sequence. Recent studies have shown that exploring
spatial and temporal features of the skeleton sequence is
vital for this task. Nevertheless, how to effectively extract
discriminative spatial and temporal features is still a chal-
lenging problem. In this paper, we propose a novel Atten-
tion Enhanced Graph Convolutional LSTM Network (AGC-
LSTM) for human action recognition from skeleton data.
The proposed AGC-LSTM can not only capture discrimina-
tive features in spatial conﬁguration and temporal dynam-
ics but also explore the co-occurrence relationship between
spatial and temporal domains. We also present a tempo-
ral hierarchical architecture to increase temporal receptive
ﬁelds of the top AGC-LSTM layer, which boosts the ability
to learn the high-level semantic representation and signiﬁ-
cantly reduces the computation cost. Furthermore, to select
discriminative spatial information, the attention mechanis-
m is employed to enhance information of key joints in each
AGC-LSTM layer. Experimental results on two datasets are
provided: NTU RGB+D dataset and Northwestern-UCLA
dataset. The comparison results demonstrate the effective-
ness of our approach and show that our approach outper-
forms the state-of-the-art methods on both datasets.

1. Introduction

In the computer vision ﬁeld, human action recognition
plays a fundamental and important role, with the purpose
of predicting the action classes from videos. It has been s-

∗Corresponding Author: Wei Wang

Figure 1. The structure of one AGC-LSTM layer. Different from
traditional LSTM, the graph convolutional operator within AGC-
LSTM causes the input, hidden state, and cell memory of AGC-
LSTM to be graph-structured data.

tudied for decades and is still very popular due to its exten-
sive potential applications, e.g., video surveillance, human-
computer interaction, sports analysis and so on [18, 36, 1].
Action recognition is a challenging task in the comput-
er vision community. There are various attempts on hu-
man action recognition based on RGB video and 3D skele-
ton data. The RGB video based action recognition methods
[24, 34, 27, 35] mainly focus on modeling spatial and tem-
poral representations from RGB frames and temporal opti-
cal ﬂow. Despite RGB video based methods have achieved
promising results, there still exist some limitations, e.g.,
background clutter, illumination changes, appearance varia-
tion, and so on. 3D skeleton data represents the body struc-
ture with a set of 3D coordinate positions of key joints. S-
ince skeleton sequence does not contain color information,
it is not affected by the limitations of RGB video. Such
robust representation allows to model more discriminative
temporal characteristics about human actions. Moreover,
Johansson et al. [9] have given an empirical and theoretical
basis that key joints can provide highly effective informa-
tion about human motion. Besides, the Microsoft Kinect

1227

Figure 2. The architecture of the proposed attention enhanced graph convolutional LSTM network (AGC-LSTM). Feature augmentation
(FA) computes feature differences with position features and concatenates both position features and feature differences. LSTM is used
to dispel scale variance between feature differences and position features. Three AGC-LSTM layers can model discriminative spatial-
temporal features. Temporal average pooling is the implementation of average pooling in the temporal domain. We use the global feature
of all joints and the local feature of focused joints from the last AGC-LSTM layer to predict the class of human action.

[42] and advanced human pose estimation algorithms [3]
make it easier to gain skeleton data.

For skeleton based action recognition, the existing meth-
ods explore different models to learn spatial and temporal
features. Song et al. [25] employ a spatial-temporal atten-
tion model based on LSTM to select discriminative spatial
and temporal features. The Convolutional Neural Network-
s (CNNs) are used to learn spatial-temporal features from
skeletons in [4, 14, 10]. [39, 26] employ graph convolution-
al networks (GCN) for action recognition. Compared with
[39, 26], Si et al. [22] propose to utilize the graph neural net-
work and LSTM to represent spatial and temporal informa-
tion, respectively. In short, all these methods are trying to
design an effective model that can identify spatial and tem-
poral features of skeleton sequence. Nevertheless, how to
effectively extract discriminative spatial and temporal fea-
tures is still a challenging problem.

Generally, there are three notable characteristics for hu-
man skeleton sequences: 1) There are strong correlations
between each node and its adjacent nodes so that the skele-
ton frames contain abundant body structural information. 2)
Temporal continuity exists not only in the same joints (e.g.,
hand, wrist and elbow), but also in the body structure. 3)
There is a co-occurrence relationship between spatial and
temporal domains. In this paper, we propose a novel and
general framework called attention enhanced graph convo-
lutional LSTM network (AGC-LSTM) for skeleton-based
action recognition, which improves the skeleton representa-
tion by synchronously learning spatiotemporal characteris-
tics mentioned above.

The architecture of the proposed AGC-LSTM network
is shown in Fig.2. Firstly, the coordinate of each join-
t is transformed into a spatial feature with a linear layer.
Then we concatenate spatial feature and feature difference

between two consecutive frames to compose an augment-
ed feature. In order to dispel scale variance between both
features, a shared LSTM is adopted to process each join-
t sequence. Next, we apply three AGC-LSTM layers to
model spatial-temporal features. As shown in Fig.1, due
to the graph convolutional operator within AGC-LSTM, it
can not only effectively capture discriminative features in s-
patial conﬁguration and temporal dynamics but also explore
the co-occurrence relationship between spatial and tempo-
ral domains. More specially, the attention mechanism is
employed to enhance the features of key nodes at each time
step, which can promote AGC-LSTM to learn more dis-
criminative features. For example, the features of “elbow”,
“wrist” and “hand” are very important for action “hand-
shaking” and should be enhanced in the process of iden-
tifying the behavior. Inspired by spatial pooling in CNNs,
we present a temporal hierarchical architecture with tempo-
ral average pooling to increase temporal receptive ﬁelds of
the top AGC-LSTM layers, which boosts the ability to learn
high-level spatiotemporal semantic features and signiﬁcant-
ly reduces the computational cost. Finally, we use the glob-
al feature of all joints and the local feature of focused joints
from the last AGC-LSTM layer to predict the class of hu-
man actions. Although the joint-based model achieves the
state-of-the-art results, we also explore the performance of
the proposed model on the part level. For the part-based
model, the concatenation of joints of each part serves as a
node to construct the graph. Furthermore, the two-stream
model based on joint and part can lead to further perfor-
mance improvement.

The main contributions of this work are summarized as

follows:

• We propose a novel and general AGC-LSTM network
for skeleton-based action recognition, which is the ﬁrst

1228

attempt of graph convolutional LSTM for this task.

• The proposed AGC-LSTM is able to effectively cap-
ture discriminative spatiotemporal features. More spe-
cially, the attention mechanism is employed to enhance
the features of key nodes, which assists in improving
spatiotemporal expressions.

• A temporal hierarchical architecture is proposed to
boost the ability to learn high-level spatiotemporal se-
mantic features and signiﬁcantly reduce the computa-
tional cost.

• The proposed model achieves the state-of-the-art re-
sults on both NTU RGB+D dataset and Northwestern-
UCLA dataset. We perform extensive experiments to
demonstrate the effectiveness of our model.

2. Related Work

Neural networks with graph

Recently, graph-based
models have attracted a lot of attention due to the effective
representation for the graph structure data [38]. Existing
graph models mainly fall into two architectures. One frame-
work called graph neural network (GNN) is the combina-
tion of graph and recurrent neural network. Through mul-
tiple iterations of message passing and states updating of
nodes, each node captures the semantic relation and struc-
tural information within its neighbor nodes. Qi et al. [19]
apply GNN to address the task of detecting and recogniz-
ing human-object interactions in images and videos. Li et
al. [15] exploit the GNNs to model dependencies between
roles and predict a consistent structured output for situation
recognition. The other framework is graph convolutional
network (GCN) that generalizes convolutional neural net-
works to graph. There are two types of GCNs: spectral
GCNs and spatial GCNs. Spectral GCNs transform graph
signals on graph spectral domains and then apply spectral
ﬁlters on spectral domains. For example, the CNNs are uti-
lized in the spectral domain relying on the graph Laplacian
[6, 7]. Kipf et al. [12] introduce Spectral GCNs for semi-
supervised classiﬁcation on graph-structured data. For s-
patial GCNs, the convolution operation is applied to com-
pute a new feature vector for each node using its neigh-
borhood information. Simonovsky et al. [23] formulate a
convolution-like operation on graph signals performed in
the spatial domain and are the ﬁrst to apply graph convo-
lutions to point cloud classiﬁcation. In order to capture the
spatial-temporal features of graph sequences, a graph con-
volutional LSTM is ﬁrstly proposed in [20], which is an
extension of GCNs to have the recurrent architecture. In-
spired by [20], we exploit a novel AGC-LSTM network to
learn inherent spatiotemporal representations from skeleton
sequences.

Skeleton-based action recognition

Human action
recognition based on skeleton data has received a lot of at-
tention, due to its effective representation of motion dynam-
ics. Traditional skeleton-based action recognition methods
mainly focus on designing hand-crafted features [28, 31, 8].
Vemulapalli et al. [29] represent each skeleton using the rel-
ative 3D rotations between various body parts. The relative
3D geometry between all pairs of body parts is applied to
represent the 3D human skeleton in [28].

Recent works mainly learn human action representation-
s with deep learning networks[40, 37, 2]. Du et al. [5] di-
vide human skeleton into ﬁve parts according to the human
physical structure, and then separately feed them into a hi-
erarchical recurrent neural network to recognize actions. A
spatial-temporal attention network learns to selectively fo-
cus on discriminative spatial and temporal features in [25].
Zhang et al. [41] present a view adaptive model for skele-
ton sequence, which is capable of regulating the observa-
tion viewpoints to the suitable ones by itself. The works in
[39, 26, 14, 22] further show that learning discriminative s-
patial and temporal features is the key element for human
action recognition. A hierarchical CNN model is presented
in [14] to learn representations for joint co-occurrences and
temporal evolutions. A spatial-temporal graph convolution-
al network (ST-GCN) is proposed for action recognition in
[39]. Each spatial-temporal graph convolutional layer con-
structs spatial characteristics with a graph convolutional op-
erator, and models temporal dynamic with a convolutional
operator. In addition, a part-based graph convolutional net-
work (PB-GCN) is proposed to learn the relations between
parts in [26]. Compared with ST-GCN [39] and PB-GCN
[26], Si et al. [22] apply graph neural networks to capture
spatial structural information and then use LSTM to mod-
el temporal dynamics. Despite the signiﬁcant performance
improvement in [22], it ignores the co-occurrence relation-
ship between spatial and temporal features. In this paper,
we propose a novel attention enhanced graph convolution-
al LSTM network that can not only effectively extract dis-
criminative spatial and temporal features but also explore
the co-occurrence relationship between spatial and tempo-
ral domains.

3. Model Architecture

3.1. Graph Convolutional Neural Network

Graph convolutional neural network (GCN) is a gener-
al and effective framework for learning representation of
graph structured data. Various GCN variants have achieved
the state-of-the-art results on many tasks. For skeleton-
based action recognition, let Gt = {Vt, Et} denotes a graph
of human skeleton on a single frame at time t, where Vt
is the set of N joint nodes and Et is the set of skele-
ton edges. The neighbor set of a node vti is deﬁned as

1229

N(vti) = {vtj|d(vti, vtj) ≤ D}, where d(vti, vtj) is the
minimum path length from vtj to vti. A graph labeling
function ℓ : Vt → {1, 2, ..., K} is designed to assign the
labels {1, 2, ..., K} to each graph node vti ∈ Vt, which can
partition the neighbor set N(vti) of node vti into a ﬁxed
number of K subsets. The graph convolution is generally
computed as:

Yout(vti) = Xvtj ∈N(vti)

1

Zti(vtj)

X(vtj)W(ℓ(vtj))

(1)

where X(vtj) is the feature of node vtj . W(·) is a weight
function that allocates a weight indexed by the label ℓ(vtj)
from K weights. Zti(vtj) is the number of the corre-
sponding subset, which normalizes feature representations.
Yout(vti) denotes the output of graph convolution at node
vti. More speciﬁcally, with the adjacency matrix, the Eqn.
1 can be represented as:

Yout =

Λ

KXk=1

−

1
2

−

1
2

k AkΛ

k XWk

(2)

where Ak is the adjacency matrix in spatial conﬁguration
of the label k ∈ {1, 2, ..., K}. Λii
k is a degree
matrix.

k = Pj Aij

3.2. Attention Enhanced Graph Convolutional L 

STM

For sequence modeling, a lot of studies have demonstrat-
ed that LSTM, as a variant of RNN, has an amazing ability
to model long-term temporal dependencies. Various LSTM-
based models are employed to learn temporal dynamics of
skeleton sequences. However, due to the fully connected
operator within LSTM, there is a limitation of ignoring spa-
tial correlation for skeleton-based action recognition. Com-
pared with LSTM, AGC-LSTM can not only capture dis-
criminative features in spatial conﬁguration and temporal
dynamics, but also explore the co-occurrence relationship
between spatial and temporal domains.

Like LSTM, AGC-LSTM also contains three gates: an
input gate it, a forget gate ft, an output gate ot. However,
these gates are obtained with the graph convolution oper-
ator. The input Xt, hidden state Ht, and cell memory Ct
of AGC-LSTM are graph-structured data. Fig.3 shows the
strcture of AGC-LSTM unit. Due to the graph convolutional
operator within AGC-LSTM, the cell memory Ct and hid-
den state Ht are able to exhibit temporal dynamics, as well
as contain spatial structural information. The functions of
AGC-LSTM unit are deﬁned as follows:

Figure 3. The structures of AGC-LSTM unit. Compared with LST-
M, the inner operator of AGC-LSTM is graph convolutional calcu-
lation. To highlight more discriminative information, the attention
mechanism is employed to enhance the features of key nodes.

it = σ(Wxi ∗G Xt + Whi ∗G Ht−1 + bi)
ft = σ(Wxf ∗G Xt + Whf ∗G Ht−1 + bf )
ot = σ(Wxo ∗G Xt + Who ∗G Ht−1 + bo)
ut = tanh(Wxc ∗G Xt + Whc ∗G Ht−1 + bc)
Ct = ft ⊙ Ct−1 + it ⊙ ut

(3)

bHt = ot ⊙ tanh(Ct)
Ht = fatt(cid:16)bHt(cid:17) +bHt

where ∗G denotes the graph convolution operator and ⊙ de-
notes the Hadamard product. σ (·) is the sigmoid activation

hidden state. Wxi ∗G Xt denotes a graph convolution of Xt
with Wxi, which can be written as Eqn.1. fatt(·) is an atten-
tion network that can select discriminative information of

function. ut is the modulated input. bHt is an intermediate
key nodes. The sum of fatt(cid:16)bHt(cid:17) andbHt as the output aims

to strengthen information of key nodes without weakening
information of non-focused nodes, which can maintain the
integrity of spatial information.

The attention network is employed to adaptively focus
on key joints with a soft attention mechanism that can auto-
matically measure the importance of joints. The illustration
of the spatial attention network is shown in Fig.4. The inter-

mediate hidden state bHt of AGC-LSTM contains rich spa-

tial structural information and temporal dynamics that are
beneﬁcial in guiding the selection of key joints. So we ﬁrst
aggregate the information of all nodes as a query feature:

qt = ReLU  NXi=1

WbHti!

(4)

where W is the learnable parameter matrix. Then the atten-
tion scores of all nodes can be calculated as:

αt = Sigmoid(cid:16)Ustanh(cid:16)WhbHt + Wqqt + bs(cid:17) + bu(cid:17)(5)

1230

Figure 4. Illustration of the spatial attention network.

where αt = (αt1, αt2, ..., αtN ), and Us, Wh, Wq are the
learnable parameter matrixes. bs, bu are the bias. We use
the non-linear function of Sigmoid due to the possibility of
existing multiple key joints. The hidden state Hti of node

vti can also be represented as (1 + αti) ·bHti. The attention

enhanced hidden state Ht will be fed into the next AGC-
LSTM layer. Note that, at the last AGC-LSTM layer, the
aggregation of all node features will serve as a global fea-
ture Fg
t , and the weighted sum of focused nodes will serve
as a local feature Fl
t:

Fg

t =

Fl

t =

NXi=1
NXi=1

Hti

αti ·bHti

(6)

(7)

The global feature Fg
dict the class of human action.

t and local feature Fl

t are used to pre-

3.3. AGC LSTM Network

We propose an end-to-end attention enhanced graph
convolutional LSTM network (AGC-LSTM) for skeleton-
based human action recognition. The overall pipeline of
our model is shown in Fig.2. In the following, we discuss
the rationale behind the proposed framework in detail.

Joints Feature Representation. For the skeleton se-
quence, we ﬁrst map the 3D coordinate of each joint into
a high-dimensional feature space using a linear layer and
an LSTM layer. The ﬁrst linear layer encodes the coordi-
nates of joints into a 256-dim vector as position features
Pt ∈ RN ×256 , and Pti ∈ R1×256 denotes the position rep-
resentation of joint i. Due to only containing position infor-
mation, the position feature Pti is beneﬁcial for learning s-
patially structured characteristic in the graph model. Frame
difference features Vti between two consecutive frames can
facilitate the acquisition of dynamic information for AGC-
LSTM. In order to take into account both advantages, the
concatenation of both features serve as an augmented fea-
ture to enrich feature information. However, the concatena-
tion of position feature Pti and frame difference feature Vti

exists the scale variance of the features vectors. Therefore,
we adopt an LSTM layer to dispel scale variance between
both features:

Eti = flstm (concat (Pti, Vti))

= flstm(cid:0)concat(cid:0)Pti,(cid:0)Pti − P(t−1)i(cid:1)(cid:1)(cid:1)

where Eti is the augmented feature of joint i at time t. Note
that the linear layer and LSTM are shared among different
joints.

(8)

Temporal Hierarchical Architecture. After the LSTM
layer, the sequence {E1, E2, ..., ET } of augmented features
will be fed into the following GC-LSTM layers as the n-
ode features, where Et ∈ RN ×de . The proposed model
stacks three AGC-LSTM layers to learn the spatial conﬁg-
uration and temporal dynamics. Inspired by spatial pooling
in CNNs, we present a temporal hierarchical architecture of
AGC-LSTM with average pooling in temporal domain to
increase the temporal receptive ﬁeld of the top AGC-LSTM
layers. Through the temporal hierarchical architecture, the
temporal receptive ﬁeld of each time input at the top AGC-
LSTM layer becomes a short-term clip from a frame, which
can be more sensitive to the perception of the temporal dy-
namics.
In addition, it can signiﬁcantly reduce computa-
tional cost on the premise of improving performance.

t and ol

Learning AGC-LSTM. Finally, the global feature Fg
t
and local feature Fl
t of each time step are transformed
into the scores og
t for C classes, where ot =
(ot1, ot2, ..., otC). And the predicted probability being the
ith class is then obtained as:
eoti
j=1 eotj

, i = 1, ..., C

ˆyti =

(9)

During training, considering that the hidden state of each
time step on the top AGC-LSTM contains a short-term dy-
namics, we supervise our model with the following loss:

PC

L = −

+ λ

T3

C

3

ti −

yilog ˆyg

Xi=1
Xt=1
Xn=1 1 − PTj
Xj=1

N

T3

Xt=1

C

Xi=1
!2

t=1 αtnj

Tj

yilog ˆyl
ti

(10)

+ β

1

Tj

3

Xj=1

Tj

Xt=1  N
Xn=1

2

αtnj!

where y = (y1, ..., yC) is the groundtruth label. Tj denotes
the number of time step on jth AGC-LSTM layer. The third
term aims to pay equal attention to different joints. The last
term is to limit the number of interested nodes. λ and β
are weight decaying coefﬁcients. Note that only the sum
probability of ˆyg
T3 at the last time step is used to
T3
predict the class of the human action.

and ˆyl

Although the joint-based AGC-LSTM network has
achieved the state-of-the-art results, we also explore the per-
formance of the proposed model on the part level. Accord-
ing to human physical structure, the body can be divided

1231

Figure 5. Illustration of the hybrid model based on joins and parts.

into several parts. Similar to joint-based AGC-LSTM net-
work, we ﬁrst capture part features with a linear layer and
a shared LSTM layer. Then the part features as node rep-
resentations are fed into three AGC-LSTM layers to model
spatial-temporal characteristics. The results illustrate that
our model can also achieve superior performance on the
part level. Furthermore, the hybrid model (shown in Fig.5)
based on joints and parts can lead to further performance
improvement.

4. Experiments

4.1. Datasets

NTU RGB+D dataset [21]. This dataset contains 60 d-
ifferent human action classes that are divided into three ma-
jor groups: daily actions, mutual actions, and health-related
actions. There are 56,880 action samples in total which are
performed by 40 distinct subjects. Each action sample con-
tains RGB video, depth map sequence, 3D skeleton data,
and infrared video captured by three Microsoft Kinect v2
cameras concurrently. The 3D skeleton data that we focus
on consists of 3D positions of 25 body joints per frame.
There are two evaluation protocols for this dataset: Cross-
Subject (CS) and Cross-View (CV) [21]. Under the Cross-
Subject protocol, actions performed by 20 subjects consti-
tute the training set and the rest of actions performed by the
other 20 subjects are used for testing. For Cross-View eval-
uation, samples captured by the ﬁrst two cameras are used
for training and the rest are for testing.

Northwestern-UCLA dataset [33]. This dataset con-
tains 1494 video clips covering 10 categories. It is captured
by three Kinect cameras simultaneously from a variety of
viewpoints. Each action sample contains RGBD and hu-
man skeleton data performed by 10 different subjects. The
evaluation protocol is the same as in [33]. Samples from
the ﬁrst two cameras constitute the training set and samples
from the other camera constitute the testing dataset.

4.2. Implementation Details

In our experiments, we sample a ﬁxed length T from
each skeleton sequence as the input. We set the length
T = 100 and 50 for NTU dataset and Northwestern-UCLA
dataset, respectively.
In the proposed AGC-LSTM, the

neighbor set of each node contains only nodes directly con-
nected with itself, so D = 1. In order to compare fairly with
ST-GCN [39], the graph labeling function in AGC-LSTM
will partition the neighbor set into K = 3 subsets according
to [39]: the root node itself, centripetal group, and centrifu-
gal group. The channels of three AGC-LSTM layers are set
to 512. During training, we use the Adam optimizer [11] to
optimize the network. Dropout with a probability of 0.5 is
adopted to avoid over-ﬁtting on these two datasets. We set
λ and β to 0.01 and 0.001, respectively. The initial learning
rate is set to 0.0005 and reduced by multiplying it by 0.1
every 20 epochs. The batch sizes for the NTU dataset and
Northwestern-UCLA dataset are 64 and 30, respectively.

4.3. Results and Comparisons

In this section, we compare our proposed attention en-
hanced graph convolutional LSTM network (AGC-LSTM)
with several state-of-the-art methods on the used two
datasets.

4.3.1 NTU RGB+D Dataset

From Table 1, we can see that our proposed method
achieves the best performance of 95.0% and 89.2% in terms
of two protocols on the NTU dataset. To demonstrate the ef-
fectiveness of our method, we choose the following related
methods to compare and analyze the results:

AGC-LSTM vs HCN. HCN [14] employs the CNN
model for learning global co-occurrences from skeleton da-
ta. It treats each joint of a skeleton as a channel, then uses
the convolution layer to learn the glob co-occurrence fea-

Methods

Year

CV

CS

HBRNN-L [5]
Part-aware LSTM [21]
Trust Gate ST-LSTM [16]
Two-stream RNN [30]
STA-LSTM [25]
Ensemble TS-LSTM [13]
Visualization CNN [17]
VA-LSTM [41]
ST-GCN [39]
SR-TSL [22]
HCN [14]
PB-GCN [26]
AGC-LSTM (Joint)
AGC-LSTM (Part)
AGC-LSTM (Joint&Part)

2015
2016
2016
2017
2017
2017
2017
2017
2018
2018
2018
2018

-
-
-

64.0
70.3
77.7
79.5
81.2
81.3
82.6
87.6
88.3
92.4
91.1
93.2
93.5
93.8
95.0

59.1
62.9
69.2
71.3
73.4
74.6
76.0
79.4
81.5
84.8
86.5
87.5
87.5
87.5
89.2

Table 1. Comparison with the state-of-the-art methods on the N-
TU RGB+D dataset for Cross-View (CS) and Cross-Subject (CV)
evaluation in accuracy.

1232

tures from all joints. We can see that our performances
signiﬁcantly outperform the HCN [14] by about 3.9% and
2.7% for cross-view evaluation and cross-subject evalua-
tion, respectively.

AGC-LSTM vs GCN models. In order to compare fairly
with [39], we use the same GCN operator in the proposed
AGC-LSTM layer as in ST-GCN. On the joint-level evalu-
ation, the results of AGC-LSTM are 93.5% and 87.5% that
outperform 5.2% and 6.0% than ST-GCN. Moreover, Our
model outperforms the PB-GCN [26] by 1.8% and 1.7%
for tow evaluations. The comparison results prove that the
AGC-LSTM is optimal for skeleton-based action recogni-
tion than ST-GCN.

Co-occurrence relationship between spatial and tem-
poral domains. Although Si et al.[22] propose a spa-
tial reasoning and temporal stack learning network with
graph neural network (GNN) and LSTM, they ignore the co-
occurrence relationship between spatial and temporal do-
mains. Due to the ability to explore the co-occurrence rela-
tionship between spatial and temporal domains, Our AGC-
LSTM outperforms [22] by 2.6% and 4.4%.

The performances on joint level and part level. Recen-
t methods can be grouped into two categories: joint-based
[39, 41, 13, 30, 14] and part-based methods [22, 30, 5]. Our
method achieves the state-of-the-art results on joint-level
and part-level, which illustrates the better generalization of
our model for joint-level and part-level inputs.

4.3.2 Northwestern-UCLA Dataset

As shown in Table 2,
the proposed AGC-LSTM again
achieves the best accuracy of 93.3% on the Northwestern-
UCLA dataset. The previous state-of-the-art model [13]
employs multiple Temporal Sliding LSTM (TS-LSTM) to
extract short-term, medium-term and long-term temporal
dynamics respectively, which has similar functionality to
our temporal hierarchical architecture. However, our mod-
el outperforms TS-LSTM [13] by 4.1%. Compared with
the CNN-based method [17], our method also obtains much

Methods

Year Accuracy (%)

Lie group [28]
Actionlet ensemble [32]
HBRNN-L [5]
Visualization CNN [17]
Ensemble TS-LSTM [13]
AGC-LSTM (Joint)
AGC-LSTM (Part)
AGC-LSTM (Joint&Part)

2014
2014
2015
2017
2017

-
-
-

74.2
76.0
78.5
86.1
89.2
92.2
90.1
93.3

Table 2. Comparison with the state-of-the-art methods on the
Northwestern-UCLA dataset in accuracy.

Methods

CV

CS

Joint

LSTM
GC-LSTM
LSTM+TH
GC-LSTM+TH
AGC-LSTM+TH (AGC-LSTM)
Part AGC-LSTM+TH (AGC-LSTM)

AGC-LSTM (Joint&Part)

89.4
92.4
90.4
92.9
93.5
93.8
95.0

80.3
85.6
81.4
86.3
87.5
87.5
89.2

Table 3. The comparison results between several baselines and our
AGC-LSTM on the NTU RGB+D dataset.

Methods

Accuracy (%)

Joint

LSTM
GC-LSTM
LSTM+TH
GC-LSTM+TH
AGC-LSTM+TH (AGC-LSTM)
Part AGC-LSTM+TH (AGC-LSTM)

AGC-LSTM (Joint&Part)

70.0
87.5
78.5
89.4
92.2
90.1
93.3

Table 4. The comparison results between several baselines and our
AGC-LSTM on the Northwestern-UCLA dataset.

better performance.

4.4. Model Analysis

4.4.1 Architecture Analysis

Tables 3 and 4 show experimental results of several base-
lines on the NTU RGB+D dataset and Northwestern-UCLA
dataset, respectively. TH denotes temporal hierarchical ar-
chitecture. Compared with LSTM and GC-LSTM, LST-
M+TH and GC-LSTM+TH can increase the temporal re-
ceptive ﬁelds of each time step on the top layer. The
improved performances prove that the temporal hierarchi-
cal architecture can boost the ability of representing tem-
poral dynamics. Replacing LSTM with GC-LSTM, GC-
LSTM+TH increases the accuracies to 2.5%, 4.9% on
the NTU dataset and 10.9% on the Northwestern-UCLA
dataset, respectively. Substantial performance improve-
ments verify the effectiveness of GC-LSTM, which can
capture more discriminative spatial-temporal features from
skeleton data. Compared with GC-LSTM, AGC-LSTM can
employ the spatial attention mechanism to select spatial in-
formation of key joints, which can promote the ability of
feature representation. In addition, the fusion of part-based
and joint-based AGC-LSTM can further improve the per-
formance.

We also visualize the attention weights of three AGC-
LSTM layers in Fig.6. For the “handshaking” action, the re-
sults show our method can gradually enhance the attention

1233

(a)

(b)

(c)

Figure 6. Visualizations of the attention weights of three AGC-
LSTM layers on one actor of the action “handshaking“. Vertical
axis denotes the joints. Horizontal axis denotes the frames. (a),
(b), (c) are the attention results of the ﬁrst, second and third AGC-
LSTM layer, respectively.

of “right elbow”, “right wrist”, and “right hand”. Mean-
while, “tip of the right hand” and “right thumb” have some
degree of attention. Furthermore, we analyze the experi-
mental results with a confusion matrix on the Northwestern-
UCLA dataset. As show in Fig.7(a), it is very confusing
for LSTM to recognize similar actions. For example, the
actions “pick up with one hand” and “pick up with two
hands” have very similar skeleton sequences. Nevertheless,
we can see that the proposed AGC-LSTM can signiﬁcantly
improve the ability to classify these similar actions (shown
in Fig.7(b)). The above results illustrate that the proposed
AGC-LSTM is an effective method for skeleton-based ac-
tion recognition.

4.4.2 Failure Case

Finally, we analyze misclassiﬁcation results with a confu-
sion matrix on the NTU dataset. Fig.8 shows the part con-
fusion matrix comparison of the actions (“eat meal/snack”,
“reading”, “writing”, “playing with phone/tablet”, “typing
on a keyboard”, “pointing to something with ﬁnger”, “s-
neeze/cough”, “pat on back of other person”) with accura-
cies less than 80% for the cross-subject setting on the N-
TU dataset. We can see that misclassiﬁed actions are main-
ly very similar movements. For example, 20% samples of

(a) LSTM

(b) AGC-LSTM

Figure 7. Confusion matrix comparison on the Northwestern-
UCLA dataset. (a) LSTM. (b) AGC-LSTM.

Figure 8. Confusion matrix comparison on the NTU dataset. It
shows the part of confusion matrix comparison of the actions (“eat
meal/snack”, “reading”, “writing”, “playing with phone/tablet”,
“typing on a keyboard”, “pointing to something with ﬁnger”, “s-
neeze/cough”, “pat on back of other person”) with accuracies less
than 80% on NTU dataset.

“reading” are misclassiﬁed as “writing”, and there are 19%
sequences of “writing” misclassiﬁed as “typing on as key-
board”. For the NTU dataset, only two joints are marked
on ﬁngers (“tip of the hand” and “thumb”), so that it is very
challenging to capture such subtle movements of the hands.

5. Conclusion and Future Work

In this paper, we propose an attention enhanced graph
convolutional LSTM network (AGC-LSTM) for skeleton-
based action recognition, which is the ﬁrst attempt of graph
convolutional LSTM for this task. The proposed AGC-
LSTM can not only capture discriminative features in spa-
tial conﬁguration and temporal dynamics, but also explore
the co-occurrence relationship between spatial and temporal
domains. Furthermore, the attention network is employed
to enhance information of key joints in each AGC-LSTM
layer.
In addition, we also propose a temporal hierarchi-
cal architecture to capture high-level spatiotemporal seman-
tic features. On two challenging benchmarks, the proposed
AGC-LSTM achieves the state-of-the-art results. Learning
the pose-object relation is helpful to overcome the limita-
tions mentioned in the failure case. In the future, we will
try the combination of skeleton sequence and object appear-
ance to promote the performance of human action recogni-
tion.

6. Acknowledgements

This work is jointly supported by National Key Research
and Development Program of China (2016YFB1001000),
National Natural Science Foundation of China (61525306,
61633021, 61721004, 61420106015, 61572504), Capital
Science and Technology Leading Talent Training Project
(Z181100006318030), and Beijing Science and Technolo-
gy Project (Z181100008918010).

1234

References

[1] Jake K Aggarwal and Michael S Ryoo. Human activity anal-

ysis: A review. ACM Computing Surveys, 2011.

[2] Fabien Baradel, Christian Wolf, and Julien Mille. Human ac-
tion recognition: Pose-based attention draws focus to hands.
In ICCV Workshop, 2017.

[3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017.

[4] Yong Du, Yun Fu, , and Liang Wang. Skeleton based action
In ACPR,

recognition with convolutional neural network.
2015.

[19] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,
and Song-Chun Zhu. Learning human-object interactions by
graph parsing neural networks. In ECCV, 2018.

[20] Youngjoo Seo, Michal Defferrard, Pierre Vandergheynst,
and Xavier Bresson. Structured sequence modeling with
graph convolutional recurrent networks. arXiv preprint arX-
iv:1612.07659, 2016.

[21] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.
Ntu rgb+d: A large scale dataset for 3d human activity anal-
ysis. In CVPR, 2016.

[22] Chenyang Si, Ya Jing, Wei Wang, Liang Wang, and Tieniu
Tan. Skeleton-based action recognition with spatial reason-
ing and temporal stack learning. In ECCV, 2018.

[5] Yong Du, Wei Wang, and Liang Wang. Hierarchical recur-
rent neural network for skeleton based action recognition. In
CVPR, 2015.

[23] Martin Simonovsky and Nikos Komodakis. Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In CVPR, 2017.

[6] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and
Ryan P Adams. Convolutional networks on graphs for learn-
ing molecular ﬁngerprints. In NIPS. 2015.

[7] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convo-
lutional networks on graph-structured data. arXiv preprint
arXiv:1506.05163, 2015.

[8] Mohamed E. Hussein, Marwan Torki, Mohammad A.
Gowayyed, and Motaz El-Saban. Human action recognition
using a temporal hierarchy of covariance descriptors on 3d
joint locations. In IJCAI, 2013.

[9] Gunnar Johansson. Visual perception of biological motion
and a model for its analysis. Perception & Psychophysics,
1973.

[10] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous
Sohel, and Farid Boussaid. A new representation of skeleton
sequences for 3d action recognition. In CVPR, 2017.

[11] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015.

[12] Thomas N. Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In ICLR, 2017.

[13] Inwoong Lee, Doyoung Kim, Seoungyoon Kang, and
Sanghoon Lee. Ensemble deep learning for skeleton-based
action recognition using temporal sliding lstm networks. In
ICCV, 2017.

[14] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Co-
occurrence feature learning from skeleton data for action
recognition and detection with hierarchical aggregation. In
IJCAI, 2018.

[15] Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel
Urtasun, and Sanja Fidler. Situation recognition with graph
neural networks. In ICCV, 2017.

[16] Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang.
Spatio-temporal lstm with trust gates for 3d human action
recognition. In ECCV, 2016.

[17] Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skele-
ton visualization for view invariant human action recogni-
tion. Pattern Recognition, 2017.

[18] Ronald Poppe. A survey on vision-based human action

recognition. Image and Vision Computing, 2010.

[24] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. In NIP-
S, 2014.

[25] Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and
Jiaying Liu. An end-to-end spatio-temporal attention model
for human action recognition from skeleton data. In AAAI,
2017.

[26] Kalpit Thakkar and P J Narayanan. Part-based graph convo-

lutional network for action recognition. In BMVC, 2018.

[27] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In ICCV, 2015.

[28] Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa.
Human action recognition by representing 3d skeletons as
points in a lie group. In CVPR, 2014.

[29] Raviteja Vemulapalli and Rama Chellappa. Rolling rotation-
In

s for recognizing human actions from 3d skeletal data.
CVPR, 2016.

[30] Hongsong Wang and Liang Wang. Modeling temporal
dynamics and spatial conﬁgurations of actions using two-
stream recurrent neural networks. In CVPR, 2017.

[31] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet en-
semble for action recognition with depth cameras. In CVPR,
2012.

[32] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan.
Learning actionlet ensemble for 3d human action recogni-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2014.

[33] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-
Chun Zhu. Cross-view action modeling, learning, and recog-
nition. In CVPR, 2014.

[34] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
networks: Towards good practices for deep action recogni-
tion. In ECCV, 2016.

[35] Pichao Wang, Wanqing Li, Philip Ogunbona, Jun Wan, and
Sergio Escalera. Rgb-d-based human motion recognition
with deep learning: A survey. Computer Vision and Image
Understanding, 2018.

[36] Daniel Weinland, Remi Ronfard, and Edmond Boyer. A sur-
vey of vision-based methods for action representation, seg-

1235

mentation and recognition. Computer Vision and Image Un-
derstanding, 2011.

[37] Chunyu Xie, Ce Li, Baochang Zhang, Chen Chen, Jungong
Han, Changqing Zou, and Jianzhuang Liu. Memory attention
networks for skeleton-based action recognition.
In IJCAI,
2018.

[38] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural networks? In arXiv preprint
arXiv:1810.00826, 2018.

[39] Sijie Yan, Yuanjun Xiong, Dahua Lin, and xiaoou Tang. S-
patial temporal graph convolutional networks for skeleton-
based action recognition. In AAAI, 2018.

[40] Zhengyuan Yang, Yuncheng Li, Jianchao Yang, , and Jiebo
Luo. Action recognition with spatio-temporal visual atten-
tion on skeleton image sequences.
IEEE Transactions on
Circuits and Systems for Video Technology, 2018.

[41] Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng,
Jianru Xue, and Nanning Zheng. View adaptive recurrent
neural networks for high performance human action recog-
nition from skeleton data. In ICCV, 2017.

[42] Zhengyou Zhang. Microsoft kinect sensor and its effect.

IEEE Multimedia, 2012.

1236

