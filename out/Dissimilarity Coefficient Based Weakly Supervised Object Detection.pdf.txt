Dissimilarity Coefﬁcient based Weakly Supervised Object Detection

Aditya Arun
CVIT, KCIS

C.V. Jawahar
CVIT, KCIS

M. Pawan Kumar

University of Oxford,

IIIT Hyderabad

IIIT Hyderabad

The Alan Turing Institute

Abstract

We consider the problem of weakly supervised object de-
tection, where the training samples are annotated using
only image-level labels that indicate the presence or ab-
sence of an object category. In order to model the uncer-
tainty in the location of the objects, we employ a dissimilar-
ity coefﬁcient based probabilistic learning objective. The
learning objective minimizes the difference between an an-
notation agnostic prediction distribution and an annotation
aware conditional distribution. The main computational
challenge is the complex nature of the conditional distri-
bution, which consists of terms over hundreds or thousands
of variables. The complexity of the conditional distribution
rules out the possibility of explicitly modeling it. Instead,
we exploit the fact that deep learning frameworks rely on
stochastic optimization. This allows us to use a state of the
art discrete generative model that can provide annotation
consistent samples from the conditional distribution. Ex-
tensive experiments on PASCAL VOC 2007 and 2012 data
sets demonstrate the efﬁcacy of our proposed approach.

1. Introduction

Object detection requires us to localize all the instances
of an object category of interest in a given image. In re-
cent years, signiﬁcant advances in speed and accuracy have
been achieved by detection frameworks based on Convolu-
tional Neural Networks (CNNs) [7, 13, 14, 16, 24, 26, 27].
Most of the existing methods require a strongly supervised
data set, where each image is labeled with the ground-
truth bounding boxes of all the object instances. Given the
high cost of obtaining such detailed annotations, researchers
have recently started exploring the weakly supervised ob-
ject detection (WSOD) problem [3, 9, 18, 21, 22, 23, 33,
34, 39, 40, 41, 42]. The goal of WSOD is to learn an accu-
rate detector using training samples that are annotated with
image-level labels (which indicate the presence of an object
category).

Given the wide availability of

labels,
WSOD offers a cost-effective and highly scalable learning
paradigm. However, this comes at the cost of introducing

image-level

uncertainty in the location of the object instances during
training. For example, consider the task of detecting a car.
Given a training image annotated to indicate the presence of
a car, we are still faced with the challenge of identifying the
bounding box for the car.

In order to effectively model uncertainty in weakly su-
pervised learning, Kumar et al. [20] proposed a probabilis-
tic framework that models two distributions: (i) a condi-
tional distribution, which represents the probability of an
output conditioned on the given annotation during training;
and (ii) a prediction distribution which represents the prob-
ability of an output at test time. The parameters of the two
distributions are estimated jointly by minimizing the dis-
similarity coefﬁcient [25], which measures the distance be-
tween any two distributions using a task speciﬁc loss func-
tion.

The aforementioned dissimilarity coefﬁcient based
framework has provided promising results in domains
where the conditional distribution is simple to model (that
is, consists of terms that depend on a few variables at a
time) [1, 20]. However, WSOD presents a more challenging
scenario due to the complexity of the underlying conditional
distribution. Speciﬁcally, given the hundreds or even thou-
sands of bounding box proposals for an image, the anno-
tation constraint imposes a term over all of these bounding
box proposals such that at least one of them corresponds
to the given image-level label. This leads to a challenging
scenario where the distribution is not factorizable over the
bounding box proposals. While previous works have ap-
proximated this uncertainty as a fully factorized distribution
for computational efﬁciency, we argue that such a choice
leads to poor accuracy.

To overcome the difﬁculty of a complex conditional dis-
tribution, we make the key observation that deep learning
relies on stochastic optimization. Therefore, we do not
need to explicitly model this complex distribution but sim-
ply estimate the distribution using samples. This observa-
tion opens the door to the use of state-of-the-art deep gen-
erative models such as the Discrete DISCO Net [4, 5].

We test the efﬁcacy of our approach on the challeng-
ing PASCAL VOC 2007 and 2012 data sets. To generate

9432

the weakly supervised data sets, we use the image-level la-
bels, discarding the bounding box annotations. We achieve
53.6% detection AP on PASCAL VOC 2007 and 49.5% de-
tection AP on PASCAL VOC 2012 data set, signiﬁcantly
improving the state-of-the-art by 1.5% on both data sets.
To summarize, we make the following contributions.

• Efﬁciently model the complex non-factorizable, an-
notation aware conditional distribution using the deep
generative model, the Discrete DISCO Net.

• Empirically show the importance of modeling the un-
certainty in the annotations in a single uniﬁed proba-
bilistic learning objective, the dissimilarity coefﬁcient.

• State-of-the art performance for the task of WSOD on

challenging PASCAL VOC 2007 and 2012 data sets.

2. Related Work

Conventional methods often treat WSOD as a Multi-
ple Instance Learning (MIL) problem [10] by representing
each image as a bag of instances (that is, putative bounding
boxes) [2, 6, 31, 36, 38]. The learning procedure alternates
between training an object classiﬁer and selecting the most
conﬁdent positive instances. However, these methods are
susceptible to poor initialization. To address this, differ-
ent strategies have been developed, which aim to improve
the initialization [19, 29, 30, 31], regularize the model with
extra cues [2, 6], or relax the MIL constraint [38] to make
the objective differentiable. These hard-MIL based methods
have demonstrated their effectiveness, specially when CNN
features are used to represent object proposals [6]. How-
ever, these models are not end to end trainable and also do
not explicitly model the uncertainty.

A more interesting line of work is to integrate MIL strat-
egy as deep networks such that they are end to end train-
able [3, 9, 12, 33, 34, 37, 40, 41, 42]. In their work, Bilen
et al. [3] proposed a smoothed version of MIL that softly
labels object proposals instead of choosing the highest scor-
ing ones. Building on this soft-MIL based approach, Diba et
al. [9] integrate the MIL strategy with better bounding box
proposals into an end-to-end cascaded deep network. Tang
et al. [33] reﬁne the prediction iteratively through multi-
stage instance classiﬁer. Zhang et al. [40] add curriculum
learning using the MIL framework. As we shall see, our for-
mulation brings out the curriculum learning naturally dur-
ing training. Other end-to-end trainable frameworks for
WSOD employ domain adaptation [22, 31], expectation-
maximization algorithm [18, 39] and saliency based meth-
ods [21]. Although these methods are end to end trainable,
they not only model a single distribution for two related
tasks, but also model the complex distribution with a fully
factorized one. This makes these approach sub-optimal as
what we truly want is to model a distribution which enforces

at least one bounding box proposals corresponding to the
image-level label.

There have been attempts to further improve the perfor-
mance of the weakly supervised detectors by combining
them with the strongly supervised detectors. Typically, the
predicted instances from a trained weakly supervised detec-
tor are treated as a pseudo-strong label to train a strongly
supervised network [12, 22, 33, 34, 40, 41, 42]. However,
there is only a unidirectional connection between the two
detectors. In their work, Wang et al. [37] train a weakly and
strongly supervised model jointly, in a collaborative man-
ner. This is similar in spirit to ours in using two distribu-
tions. However, they model their weakly supervised detec-
tor with a fully factorized distribution. The improvement in
results reported by these papers advocates the importance
of modeling two separate distributions. In this work, we ex-
plicitly deﬁne the two distributions employed during train-
ing and test time and jointly train them by minimizing the
dissimilarity coefﬁcient [25] based objective function.

3. Model

3.1. Notation

We denote an input image as x ∈ R(H×W ×3), where H
and W are the height and the width of the image respec-
tively. For the sake of simplifying the subsequent descrip-
tion of our approach, we assume that we have extracted B
bounding box proposals from each image. In our experi-
ments, we use Selective Search [35]. Each bounding box
proposal, b(i), can belong to one of C + 1 categories from
the set {0, 1, . . . , C}, where category 0 is background, and
categories {1, . . . , C} are object classes.

We denote an image-level label by a ∈ {0, 1}C , where
(j) = 1 if image x contains the j-th object. Further-
a
more, we denote the unknown bounding box labels by
(i) = j if the i-th bounding box
y ∈ {0, . . . , C}B, where y
b(i) is of the j-th category. A weakly supervised data set
W = {(xi, ai)|i = 1, . . . , N } contains N pairs of images
xi and their corresponding image-level labels ai.

3.2. Probabilistic Modeling

Given a weakly supervised data set W, we wish to learn
an object detector that can predict the bounding box labels
y of a previously unseen image. Due to the uncertainty in-
herent in this task, we advocate the use of a probabilistic
formulation. Following [1, 20], we deﬁne two distributions.
The ﬁrst one is the prediction distribution Prp(y|x; θp),
which models the probability of the bounding box labels
y given an input image x. Here θp are the parameters of the
distribution. As the name suggest, this distribution is used
to make the prediction at test time.

In addition to the prediction distribution, we also con-
struct a conditional distribution Prc(y|x, a; θc), which

9433

models the probability of the bounding box labels y given
the input image x and its image-level annotations a. Here
θc are the parameters of the distribution. The conditional
distribution contains additional information, namely the
presence of foreground objects in each image. Thus, we
can expect it to provide better predictions for the bounding
box labels y. We will use this property during training in
order to learn an accurate prediction distribution using the
conditional distribution. The details on the modeling of the
two distributions are discussed below.

3.2.1 Prediction Distribution

The task of the prediction distribution is to accurately model
the probability of the bounding box labels given the input
image. Taking inspiration from the supervised models [13,
14, 27], we assume independence between the probability
of the output for each bounding box proposal. Therefore,
the overall distribution for an image equals the product of
the probabilities of each proposal,

Prp(y|x; θp) =

B

Y

i=1

Prp(y

(i)|x; θp).

(1)

We model this distribution using the Fast-RCNN architec-
ture [13] (see Figure 1(a)). As the prediction distribution
is speciﬁed by a neural network, we henceforth refer to it
as the prediction net. In this setting, the parameters of the
distribution θp are the weights of the prediction net.

3.2.2 Conditional Distribution

label a,

Given B bounding box proposals for an image x and
the image-level
the conditional distribution
Prc(y|x, a; θc) models the probability of bounding box la-
bels y under the constraint that they are compatible with the
annotation a. Speciﬁcally, there exists at least one bound-
(i) = j, for every positive image-level
ing box i such that y
label a

(j) = 1.

Note that due to the requirement that the bounding box
labels y are compatible with the annotation a, the con-
ditional distribution cannot be trivially decomposed over
bounding box proposals. This is in stark contrast to the sim-
ple prediction net, which uses a fully factorized distribution.
If one were to explicitly model the conditional distribution,
then one would be required to compute its partition function
during training, which would be prohibitively expensive. To
alleviate this computational challenge, we make a key ob-
servation that in practice we only need access to a represen-
tative set of samples from the conditional distribution. This
opens the door to the use of the recently proposed Discrete
DISCO Net [4]. In what follows, we brieﬂy describe Dis-
crete DISCO Nets while highlighting their applicability to
our framework.

Discrete DISCO Net: Discrete DISCO Net [4] is a deep
probabilistic framework that implicitly represents a proba-
bility distribution over a discrete structured output space.
The strength of the framework lies in the fact that it allows
us to adapt a pointwise deep network (a network that pro-
vides a single pointwise prediction) to a probabilistic one
by the introduction of noise.

In the context of our setting, consider the modiﬁed Fast-
RCNN network in Figure 1(b) for the conditional distribu-
tion. Once again, as we are using a neural network, we will
henceforth refer to it as the conditional net. The parameters
of the conditional distribution θc are the weights of the con-
ditional net. The colored ﬁlters in the middle of the network
represent the noise that is sampled from a uniform distribu-
tion. Each value of the noise ﬁlter zk results in a different
score function1 Gk(y; x, zk, θc) ∈ RB×C . We generate K
different score functions using K different noise samples.
These score functions are then used to sample correspond-
k
ing bounding box labels ˆy
c such that all ground truth labels
are present in it. This enables us to generate samples from
the underlying distribution encoded by the network param-
eters. Note that obtaining a single sample is as efﬁcient as
a simple forward pass through the network. By placing the
ﬁlters sufﬁciently far away from the output layer of the net-
work, we can learn a highly non-linear mapping from the
uniform distribution (used to generate the noise ﬁlter) to the
output distribution (used to generate bounding box labels).

Inference: For the input pair (x, zk), the classiﬁcation
branch of the conditional net outputs a score function
Gk(y; x, zk, θc), which is a B × C matrix. The (i, j)-th
element of the matrix, denoted by G(i,j)
, denotes the score
of the bounding box i belonging to the category j. We will
now redeﬁne this score function such that it respects the
constraints imposed by the annotation a. In other words,
(j) = 1 there must exist at
for each category j such that a
(i) = j. The joint
least one bounding box i in y such that y
score for all the bounding box labels y is given by,

k

Sk(y; x, zk, θc) =

B

X

i=1

Gk(y

(i); x, zk, θc) − Hk(y),

(2)

where,

Hk(y) =

0




if ∀j ∈ {1, . . . , C} s.t. a
∃i ∈ {1, . . . , B} s.t. y

(i) = j,

(j) = 1,

(3)

∞ otherwise.

Given the scoring function in equation (2), we compute the
k-th sample as

ˆy

k
c = arg max

y∈Y

Sk(y; x, zk, θc).

(4)

1The use of score function in this paper should not be confused with
the scoring rule theory, which is used to design the learning objective of
DISCO Nets.

9434

Figure 1. The overall architecture. (a) Prediction Network: a standard Fast-RCNN architecture is used to model the prediction net. For
an input image, bounding box proposals are generated from selective search [35]. Features from each of these proposals are computed
by the region of interest (ROI) pooling layers, which are then passed through the classiﬁer and regressor to predict the ﬁnal bounding
box. (b) Conditional Network: a modiﬁed Fast-RCNN architecture is used to model the conditional net. For a single input image x and
(3)}
three different noise samples {z1 , z2 , z3} (represented as red, green and blue matrix), three different bounding boxes {y
are sampled for the given image-level label (bird in this example). Here the noise ﬁlter is concatenated as an extra channel to the ﬁnal
convolutional layer. For both the networks, the initial conv-layers are ﬁxed during training. Best viewed in color.

(2)

(1)

, y

, y

Note that in equation (4) the arg max needs to be computed
over the entire output space Y. A na¨ıve brute force algo-
rithm for this would be computationally infeasible. How-
ever, by using the structure of the higher order term Hk, we
can design an efﬁcient yet exact algorithm for equation (4).
Speciﬁcally, we assign each bounding box proposal i to its
maximum scoring object class. If all the ground truth anno-
tations a are not present in the generated bounding box la-
bels, then we sample the bounding box which has the high-
est score corresponding to the foreground label, otherwise
we sample all bounding boxes which satisﬁes the constraint.

4. Learning Objective

In order to estimate the parameters of the prediction and
conditional distribution, θp and θc, we deﬁne a uniﬁed
probabilistic learning objective based on the dissimilarity
coefﬁcient [25]. To this end, we require a task speciﬁc loss
function, which we deﬁne next.

4.1. Task Speciﬁc Loss Function

We deﬁne a loss function for object detection that de-

composes over the bounding box proposals as follows:

∆(y1, y2) =

1
B

B

X

i=1

∆(y

(i)
1 , y

(i)
2 ).

(5)

(i)
1 , y

Following the standard practice in most modern object
(i)
2 ) is further decomposed as a
detectors [17], ∆(y
weighted combination of the classiﬁcation loss and the lo-
calization loss. We use λ to denote the loss ratio ( ratio of
the weight of localization loss to the weight of classiﬁcation
loss). We use a simple 0 − 1 loss as our classiﬁcation loss
∆cls, and smoothL1 [13] for our localization loss ∆loc.
Formally, the task speciﬁc loss is given by,

∆(y

(i)
1 , y

(i)
2 ) = ∆cls(y

(i)
1 , y

4.2. Objective Function

(i)

2 ) + λ∆loc(b(i)

1 , b(i)
2 ).

(6)

The task of both the prediction distribution and the con-
ditional distribution is to predict the bounding box labels.
Moreover, as the conditional distribution utilizes the extra
information in the form of the image-level label, it is ex-
pected to provide more accurate predictions for the bound-
ing box labels y. Leveraging on the task similarity between
the two distributions, we would like to bring the two distri-
butions close to each other, so that the extra knowledge of
the conditional distribution can be transferred to the predic-
tion distribution. Taking inspiration from [1, 20], we design
a joint learning objective that can minimize the dissimilarity
coefﬁcient [25] between the prediction distribution and con-
ditional distribution. In what follows, we brieﬂy describe

9435

the concept of dissimilarity coefﬁcient before applying it to
our setting.

Dissimilarity Coefﬁcient: The dissimilarity coefﬁcient
between any two distributions Pr1(·) and Pr2(·) is deter-
mined by measuring their diversities. The diversity of a dis-
tribution Pr1(·) and a distribution Pr2(·) is deﬁned as the
expected difference between their samples, where the dif-
ference is measured by a task-speciﬁc loss function ∆′(·, ·).
Formally, we deﬁne the diversity as,

DIV∆′ (Pr1, Pr2) =E

y1∼Pr1(·)[E

y2∼Pr2(·)

[∆′(y1, y2)]].

(7)

If the model correctly brings the two distribution close to
each other, we could expect the diversity DIV∆′ (Pr1, Pr2)
to be small. Using this deﬁnition of diversity, the dissimi-
larity coefﬁcient of Pr1 and Pr2 is given by,

DISC∆′ (Pr1, Pr2) =DIV∆′ (Pr1, Pr2)

− γDIV∆′ (Pr2, Pr2)
− (1 − γ)DIV∆′ (Pr1, Pr1),

(8)

where γ ∈ [0, 1]. In other words, the dissimilarity coefﬁ-
cient between Pr1 and Pr2 is the difference between the
diversity of Pr1 and Pr2, and a convex combination of
their self-diversities. The self-diversity terms encourages
the samples from each of the two distribution to be diverse,
thus better representing the uncertainty of the task. In our
experiments, we use γ = 0.5, which results in a symmetric
dissimilarity coefﬁcient between two distributions.

Learning Objective for Detection: Given the above def-
inition of dissimilarity coefﬁcient, we can now specify our
learning objective for the task speciﬁc loss ∆ tuned for ob-
ject detection (6) as

θ∗
p, θ∗

c = arg min

θp,θc

DISC∆(Prp(θp), Prc(θc)),

(9)

where each of the diversity terms can be derived from equa-
tion (7). As discussed in Section 3.2, the conditional dis-
tribution is difﬁcult to model directly. Therefore, the cor-
responding diversity terms are computed by stochastic es-
k
timators from K samples ˆy
c of the conditional net. Thus,
each of the diversity terms can be written as2

DIV∆(Prp, Prc)
K

B

=

1

BK

X

i=1

X

k=1

X

(i)
p

y

Prp(y

(i)
p ; θp)∆(y

(i)
p , ˆy

k,(i)
c

),

(10)

2Details in the supplementary material

DIV∆(Prc, Prc)

=

1

K(K − 1)B

B

X

i=1

K

X

k,k′=1
k′6=k

∆(ˆy

k,(i)
c

, ˆy
′

k′,(i)
c

),

(11)

DIV∆(Prp, Prp)

(12)

=

1
B

B

X

i=1

X

(i)
p

y

X

′ (i)
p

y

Prp(y

(i)
p ; θp) Prp(y

′(i)
p ; θp)∆(y

(i)
p , y

′(i)
p ).

Here, DIV∆(Prp, Prc) measures the diversity between the
prediction net and the conditional net, which is the ex-
pected difference between the samples from the two dis-
tributions as measured by the task speciﬁc loss function
∆. Here Prp is explicitly modeled, hence the expectation
of its sample can be computed easily. However, as Prc is
not explicitly modeled, we compute the required expecta-
tion by drawing K samples from the distribution. Likewise,
DIV∆(Prc, Prc) measures the self diversity of the condi-
tional net. We draw K samples from the distribution to
compute the required expectation. Also, the self diversity
of the prediction net DIV∆(Prp, Prp) can be exactly com-
puted as Prp is explicitly modeled.

5. Optimization

As we employ deep neural networks to model the two
distributions, our objective function (9) is ideally suited to
be minimized by stochastic gradient descent. While it may
be possible to compute the gradients of both the networks
simultaneously, in this work we use a simple coordinate de-
scent optimization strategy. In more detail, the optimiza-
tion proceeds by iteratively ﬁxing the prediction network
and learning the conditional network, followed by learning
the prediction network for ﬁxed conditional network.

The main advantage of using the iterative training strat-
egy is that it results in an approach similar to the fully su-
pervised learning of each network. This in turn allows us to
readily use the algorithm developed in Fast-RCNN [13] and
Discrete DISCO Net [4]. The outputs from the ﬁxed network
are treated as the pseudo ground truth bounding box labels
for the other network. Furthermore, the iterative learning
strategy also reduces the memory complexity of learning as
only one network is trained at a time.

Figure 2 provides the visualization of the performance
of the two networks over the different iterations of the iter-
ative learning procedure for two difﬁcult cases. In Columns
1 and 2, different category objects are present in the same
image whereas, in columns 3 and 4, multiple instances of
the same category is present. The estimated bounding box
labels from the prediction net and those sampled from the
conditional net for two images are depicted. For conditional

9436

Figure 2. Example of predictions of prediction net and conditional net. For prediction net, the visualization is after taking standard non
maximal suppression using standard score threshold = 0.7. Column 1 and 3 are output of the prediction network while column 2 and 4
are output from the conditional network. Row 1 represents prediction of the two networks after ﬁrst iteration and row 2 and 3 represents
prediction of the two networks after third and sixth (ﬁnal) iteration respectively. Each object class is represented by different colored
bounding box, where green box represents the person category and red and blue represents the bottle and dog category respectively.

net, we superimpose ﬁve different samples of bounding box
labels. If all the samples agree with each other on bounding
box labels, then the bounding boxes will have a high over-
lap, otherwise they will be scattered across the image. For
visualization purposes only, a standard non maximal sup-
pression (NMS) is applied with a score threshold of 0.7 on
the output of the prediction net. However, note that the non
maximal suppression is not used during training of the pre-
diction net. The two steps of the iterative algorithm are de-
scribed below in brief. For completeness, the details are
provided in the supplementary material.

5.1. Optimization over Prediction Distribution

For a ﬁxed set of parameters θc of the conditional net-
work, the learning objective of the prediction net corre-
sponds to the following:

θ∗

p = arg min

θp

DIV∆(Prp, Prp)−(1−γ)DIV∆(Prp, Prp).

(13)
Note that, due to the use of dissimilarity coefﬁcient, the
above objective differs slightly from the one used for Fast-
RCNN [13]. However, importantly, it is still differentiable
with respect to θp. Hence, the prediction net can be directly

optimized via stochastic gradient descent.

In order to visualize the optimization of the prediction
net, let us consider Figure 2. The ﬁrst two columns show the
bounding box labels from the prediction and the conditional
nets for an image with single foreground object. As the im-
age has a large foreground object with a clean background,
both the prediction and the conditional nets have low uncer-
tainty. This represents an easy case where the prediction net
already has a high conﬁdence for the bounding box labels
in initial iterations, and therefore has little to gain from the
conditional net. As expected, we see only a minor improve-
ment in the predicted bounding box labels of the prediction
net over the iterations.

The last two columns show bounding box labels from
the prediction and conditional nets for a challenging im-
age. The object dog presents moderate difﬁculty to our al-
gorithm, where initially the prediction net is highly uncer-
tain while the conditional net has low uncertainty. After few
iterations, the information present in the conditional net is
successfully transferred over to the prediction net. This is
shown in last row of the third column where the prediction
net does a reasonable job at estimating the bounding boxes.

The second object bottle in the image is a difﬁcult exam-

9437

ple because of its small scale. We observe high uncertainty
in both the networks. In such cases the prediction and the
conditional nets will reject the bounding box labels having
high diversity. Moreover, the uncertainty in the prediction
net also decreases by learning from other easier instances of
the object present in the data set.

5.2. Optimization over Conditional Distribution

For a ﬁxed set of parameters θp of the prediction net-
work, the learning objective for the conditional network cor-
responds to the following,

θ∗

c = arg min

θc

DIV∆(Prp, Prc) − γDIV∆(Prc, Prc).

(14)
The above objective function is similar to the one used in
[4] for supervised learning of Discrete DISCO Nets. As our
conditional net employs a sampling procedure over the scor-
ing function Sk(y; θc), objective (14) is non-differentiable.
However, as observed in [4], it is possible to compute an
unbiased estimate of the gradients using the direct loss min-
imization technique [15, 32]. Therefore, the conditional
net can be optimized using stochastic gradient descent. We
present the technical details of optimization, which are sim-
ilar to those in [4], in the supplementary material.

In order to visualize the optimization of the conditional
net, let us ﬁrst consider the easy case in Figure 2 (columns
1-2). Similar to the prediction net in the previous sub-
section, the uncertainty in the conditional net decreases
marginally over the iterations, as it already has high con-
ﬁdence for the bounding box labels. For the challenging
objects present in the image of the last two columns, we
see that the prediction net has high uncertainty. The im-
provement in the predictions of the conditional net for these
two cases are mainly attributed to the information gained by
training on other easier examples of the dog and the bottle
category present in the data set.

6. Experiments

6.1. Data set and Evaluation Metrics

Data set: We evaluate our method on the challenging
PASCAL VOC 2007 and 2012 data sets [11] which have
9, 962 and 22, 531 images respectively for 20 object cat-
egories. These two data sets are divided into the train, val
and test sets. Here we choose trainval set of 5011 images for
VOC 2007 and 11, 540 images for VOC 2012 to train our
network. The trainval set is further split into 80% − 20%
to create new training and validation sets. We use a non-
standard training-validation split in order to maximize the
number of training images for our networks, while not over-
ﬁtting our hyper-parameters on the test set. As we focus
on weakly supervised detection, only image-level labels are
utilized during training.

Evaluation Metric We use two metrics to evaluate our
detection performance. First we evaluate detection using
mean Average Precision (mAP) on the PASCAL VOC 2007
and 2012 test sets, following the standard PASCAL VOC
protocol [11]. Second, we compute CorLoc [8] on the PAS-
CAL VOC 2007 and 2012 trainval splits. CorLoc is the
fraction of positive training images in which we localize an
object of the target category correctly. Following [11], a de-
tected bounding box is considered correct if it has at least
0.5 IoU with a ground truth bounding box.

6.2. Implementation Details

We use standard Fast-RCNN [13] to model prediction
distribution and a modiﬁed Fast-RCNN to model the condi-
tional distribution, as shown in Figure 1(a). We use the Im-
ageNet pre-trained VGG16 Network [28] as the base CNN
architecture for both our prediction and conditional nets.

The Fast-RCNN architecture is modiﬁed by adding a
noise ﬁlter in its 5th conv-layer as an extra channel as shown
in Figure 1(b). A 1 × 1 ﬁlter is used to bring the number of
channels back to the original dimensions (512 channels).
No architectural changes are made for the prediction net.
The bounding box proposals required for the Fast-RCNN
is obtained from the Selective Search algorithm [35]. Re-
sults based on the Region Proposal Networks are given in
the supplementary material.

Following the standard practice followed in Fast-RCNN,
we train and test our method on a single scale. We also
construct an ensemble by taking the ImageNet pre-trained
VGG11 and VGG13 along with VGG16 and report its re-
sults. For all our experiments we choose K = 5 for the
conditional net. That is, we sample 5 bounding boxes cor-
responding to 5 noise ﬁlters, which are themselves sampled
from a uniform distribution. For all other hyper-parameters,
we use the same conﬁgurations as described in [13].

6.3. Results

In this subsection, we will ﬁrst compare our method with
existing state-of-the-art methods for detection and correct
localization tasks on VOC 2007 and 2012 data sets. Then
through ablation experiments, see how various terms of our
dissimilarity coefﬁcient based objective function contribute
towards the accuracy gained. We present further ablation
studies in the supplementary material.

6.3.1 Comparison with other methods

We compare our proposed method with other state-of-the-
art weakly supervised methods. The detection average pre-
cision (AP) and correct localization (CorLoc) on the PAS-
CAL VOC 2007 and 2012 data sets are shown in Table 1,
Table 2 and Table 3 respectively. Compared with the other
methods, our proposed framework achieves state-of-the-art
performance using a single model.

9438

Method
WSDDN [3]
WSCCN [9]
k-EM [39]
OICR [33]
ZLDN [40]
CL [37]
ML-LocNet [41]
WS-RPN [34]
W2F [42]
Pred Net (VGG)
Pred Net (Ens)

Method
WSCCN [9]
WSDDN [3]
ZLDN [40]
OICR [33]
CL [37]
k-EM [39]
WS-RPN [34]
ML-LocNet [41]
W2F [42]
Pred Net (VGG)
Pred Net (Ens)

aero
46.4
49.5
59.8
65.5
55.4
61.2
60.8
63.0
63.5
66.7
67.7

aero
83.9
68.9
74.0
85.8
85.8
79.8
83.8
81.7
85.4
88.6
89.2

bike
58.3
60.6
64.6
67.2
68.5
66.6
70.6
69.7
70.1
69.5
70.4

bike
72.8
68.7
77.8
82.7
80.4
77.8
82.7
82.9
87.5
86.3
86.7

boat
25.9
29.2
28.8
21.6
16.8
26.0
30.2
11.6
31.9
31.4
31.3

bird
35.5
38.6
47.8
47.2
50.1
48.3
47.8
40.8
50.5
52.8
52.9
Table 1. Detection average precision (%) for different methods on VOC 2007 test set.

horse mbike
44.7
59.0
64.1
47.9
65.9
48.8
66.1
64.7
68.2
69.7
66.1
48.5
68.7
69.5
70.3
75.7
67.2
57.2
70.8
69.9
70.8
70.2

cow table
41.8
26.6
29.9
44.1
34.0
51.5
49.5
63.1
47.5
57.8
46.2
61.2
51.3
55.5
60.6
66.7
49.4
53.4
46.1
53.0
54.0
47.3

bottle
14.0
16.2
21.4
22.1
20.8
15.8
24.8
27.7
14.4
24.7
26.1

plant
17.3
23.5
21.1
25.6
27.2
22.0
25.2
26.5
23.8
28.4
29.2

chair
8.9
10.9
17.2
5.7
2.1
24.7
11.0
10.0
23.3
14.6
14.9

pson
10.8
13.8
9.3
13.0
21.6
12.1
28.3
25.7
27.6
18.5
19.7

dog
38.6
42.2
42.3
30.3
40.1
53.5
48.1
34.7
65.9
52.9
53.7

bus
66.7
70.8
67.7
68.0
62.7
66.5
64.9
70.5
72.0
74.5
75.5

car
53.0
56.9
70.3
68.5
66.8
65.4
68.4
74.1
67.8
74.1
73.7

cat
39.2
42.5
61.2
35.9
56.5
53.9
57.9
58.5
73.7
67.3
68.6

sheep
40.7
45.9
53.6
50.0
53.4
49.2
51.3
55.4
51.8
54.6
54.9

bird
64.5
65.2
65.2
62.8
73.0
66.7
60.7
68.7
62.5
71.8
72.2

bottle
40.1
40.6
46.7
43.5
36.6
57.0
53.8
53.9
35.5
51.2
51.8

boat
44.1
42.5
37.0
45.2
42.6
50.3
35.1
44.4
54.3
53.4
50.9
Table 2. CorLoc (in %) for different methods on VOC 2007 trainval set.

horse mbike
77.4
67.4
86.1
65.9
87.4
76.7
91.2
83.7
91.6
72.4
73.2
90.2
93.6
90.8
91.6
81.4
90.0
74.8
93.3
87.7
88.5
94.6

cow table
25.6
72.5
33.5
68.1
49.0
73.1
51.0
82.2
36.9
78.1
75.9
30.5
68.8
86.3
62.7
74.0
49.4
82.9
58.8
86.6
87.4
59.7

chair
33.7
29.7
17.5
15.7
34.1
29.9
22.0
32.6
39.7
33.2
33.6

pson
26.8
27.5
30.6
22.2
22.2
25.4
44.0
46.0
46.8
30.9
30.4

cat
58.9
53.7
58.8
46.8
66.0
71.5
67.4
70.5
82.3
65.3
65.6

dog
53.7
45.6
51.3
45.6
68.6
58.9
50.9
61.7
76.5
65.9
66.4

bus
65.7
72.6
75.8
84.8
79.7
80.1
82.7
80.3
85.3
87.6
88.3

car
82.5
75.2
83.7
87.0
82.8
89.9
88.6
88.9
86.6
89.0
89.5

plant
49.1
44.9
47.8
59.7
51.3
51.8
61.2
60.6
53.9
58.9
60.2

sheep
68.1
76.0
75.0
75.3
79.4
80.2
82.5
75.2
84.5
83.4
83.8

sofa
49.6
54.1
51.4
57.1
56.1
53.2
56.5
56.4
58.7
60.7
61.3

sofa
27.9
62.4
62.5
65.1
63.7
60.3
65.9
69.2
68.3
67.8
68.9

train
56.9
60.8
54.7
60.2
52.5
66.2
60.0
55.5
64.0
67.1
67.6

train
64.5
66.3
64.8
76.8
74.5
72.4
71.1
78.7
79.1
78.7
78.9

tv

50.8
54.5
50.7
59.0
58.2
59.4
43.1
54.9
62.3
60.4
61.2

tv

55.7
66.8
68.8
78.1
74.6
78.9
76.7
65.8
79.9
80.2
81.3

mAP
39.3
42.8
46.1
47.0
47.6
48.3
49.7
50.4
52.4
52.9
53.6

mean
56.7
58.0
61.2
64.3
64.7
65.0
68.4
68.6
70.3
70.9
71.4

37.9

WSCCN [9] DSL [18] OICR [33] W2F [42]

Method
mAP %
CorLoc %
Table 3. Results for different methods on VOC 2012. See supple-
mentary meterial for details.

PredNet(VGG)

PredNet(Ens)

38.3
58.8

42.5
65.6

47.8
69.4

48.4
69.5

49.5
70.2

-

Tables 1 and 2 shows that we signiﬁcantly outperform
methods which only employ a fully factorized distribution
in MIL [3, 9]. This empirically demonstrates the useful-
ness of modeling a complex distribution. Compared to the
state-of-the-art method, which trains two separate networks
like ours, if we were to only train and test Zhang et al. [42]
(W2F) using a single scale, where they achieve 49.0% mAP,
we get an improvement of 3.9%. We approximate the use
of multiple scales by ensembling, which gives us a ﬁnal im-
provement over the state-of-the-art method by over 1.2%
when compared on multiple scales.

The weakly supervised detector employed in W2F mod-
els the annotation constraint using a fully factorized distri-
bution. We argue that our choice of modeling the annotation
aware conditional distribution exactly but efﬁciently, using
Discrete DISCO Net, gives us the improved performance.
Moreover, unlike W2F, our method combines the weakly
supervised and the strongly supervised detectors with a sin-
gle learning objective instead of training them in a non-end-
to-end, cascaded fashion.

6.3.2 Effect of the diversity coefﬁcient terms

In order to understand the effect of various diversity coefﬁ-
cient terms in our objective (8), we remove the self-diversity
term in one or both of our probabilistic networks (Prc and
Prp). In order to obtain a single sample from our condi-
tional network, we feed a zero noise vector (denoted by
P Wc). The prediction network still outputs the probabil-
ity of each bounding box belonging to each class. However,
by removing the self-diversity term, we encourage it to out-

Method

Prp, Prc
(proposed)

Prp, P Wc P Wp, Prc P Wp, P Wc

Mean AP

52.9

50.1

52.6

49.5

Table 4. Detection Average Precision (%) for various ablative set-
tings on VOC 2007 test set

put a peakier distribution (denoted by P Wp). Table 4 shows
that both the self-diversity terms are important to obtain the
maximum accuracy. Relatively speaking, it is more impor-
tant to include the self-diversity in the conditional network
in order to deal with the difﬁcult examples (example, bottle
in ﬁgure 2). Moreover, this enforces a diverse set of outputs
from the conditional network, which helps the prediction
network to avoid overﬁtting the samples during training.

7. Discussion

We presented a novel framework to train an object detec-
tor using a weakly supervised data set. Our framework em-
ploys a probabilistic objective based on dissimilarity coefﬁ-
cient to model the uncertainty in the location of objects. We
show that explicitly modeling the complex non-factorizable
conditional distribution is a necessary modeling choice and
present an efﬁcient mechanism based on a discrete gener-
ative model, the Discrete DISCO Nets, to do so. Extensive
experiments on the benchmark data sets have shown that our
framework successfully transfers the information present in
the image-level annotations for the task of object detection.
In future, we would like to investigate the use of active
learning, to further beneﬁt our network in terms of the ac-
curacy of the fully supervised annotations. This will help
bridge the performance gap between the strongly supervised
detectors and detectors trained using low-cost annotations.

8. Acknowledgements

This work is partially funded by a CEFIPRA grant.

Aditya is supported by Visvesvaraya Ph.D. fellowship.

9439

References

[1] Aditya Arun, C V Jawahar, and M Pawan Kumar. Learning

human poses from actions. In BMVC, 2018.

[2] Hakan Bilen, Marco Pedersoli, and Tinne Tuytelaars.
Weakly supervised object detection with convex clustering.
In CVPR, 2015.

[3] Hakan Bilen and Andrea Vedaldi. Weakly supervised deep

detection networks. In CVPR, 2016.

[4] Diane Bouchacourt. Task-Oriented Learning of Structured
Probability Distributions. PhD thesis, University of Oxford,
2017.

[20] M Pawan Kumar, Ben Packer, and Daphne Koller. Modeling
latent variable uncertainty for loss-based learning. In ICML,
2012.

[21] Baisheng Lai and Xiaojin Gong. Saliency guided end-to-end
learning for weakly supervised object detection. In IJCAI,
2017.

[22] Dong Li, Jia-Bin Huang, Yali Li, Shengjin Wang, and Ming-
Hsuan Yang. Weakly supervised object localization with pro-
gressive domain adaptation. In CVPR, 2016.

[23] Siyang Li, Xiangxin Zhu, Qin Huang, Hao Xu, and C-C Jay
Kuo. Multiple instance curriculum learning for weakly su-
pervised object detection. In BMVC, 2017.

[5] Diane Bouchacourt, M Pawan Kumar, and Sebastian
Nowozin. Disco nets: Dissimilarity coefﬁcients networks.
In NIPS, 2016.

[24] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.
Berg. SSD: Single shot multibox detector. In ECCV, 2016.

[6] Ramazan Gokberk Cinbis, Jakob Verbeek, and Cordelia
Schmid. Weakly supervised object localization with multi-
fold multiple instance learning. TPAMI, 2017.

[7] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: Object
detection via region-based fully convolutional networks. In
NIPS, 2016.

[8] Thomas Deselaers, Bogdan Alexe, and Vittorio Ferrari.
Weakly supervised localization and learning with generic
knowledge. IJCV, 2012.

[9] Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash,
and Luc Van Gool. Weakly supervised cascaded convolu-
tional networks. In CVPR, 2017.

[10] Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. Solving the multiple instance problem with
axis-parallel rectangles. Artiﬁcial intelligence, 1997.

[11] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV, 2010.

[12] Weifeng Ge, Sibei Yang, and Yizhou Yu. Multi-evidence ﬁl-
tering and fusion for multi-label classiﬁcation, object detec-
tion and semantic segmentation based on weakly supervised
learning. In CVPR, 2018.

[13] Ross Girshick. Fast R-CNN. In ICCV, 2015.

[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, 2014.

[15] Tamir Hazan, Joseph Keshet, and David A McAllester. Di-
In NIPS,

rect loss minimization for structured prediction.
2010.

[16] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask R-CNN. In ICCV, 2017.

[17] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,
Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-
jna, Yang Song, Sergio Guadarrama, and Kevin Murphy.
Speed/accuracy trade-offs for modern convolutional object
detectors. In CVPR, 2017.

[18] Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, and Wei
Liu. Deep self-taught learning for weakly supervised object
localization. In CVPR, 2017.

[19] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-

paced learning for latent variable models. In NIPS, 2010.

[25] C Radhakrishna Rao. Diversity and dissimilarity coefﬁ-
cients: a uniﬁed approach. Theoretical population biology,
1982.

[26] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In CVPR, 2016.

[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In NIPS, 2015.

[28] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[29] Parthipan Siva, Chris Russell, and Tao Xiang.

In defence
of negative mining for annotating weakly labelled data. In
ECCV, 2012.

[30] Parthipan Siva and Tao Xiang. Weakly supervised object
detector learning with model drift detection. In ICCV, 2011.
[31] Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, and Trevor
Darrell. Weakly-supervised discovery of visual pattern con-
ﬁgurations. In NIPS, 2014.

[32] Yang Song, Alexander Schwing, Raquel Urtasun, et al.
Training deep neural networks via direct loss minimization.
In ICML, 2016.

[33] Peng Tang, Xinggang Wang, Xiang Bai, and Wenyu Liu.
Multiple instance detection network with online instance
classiﬁer reﬁnement. In CVPR, 2017.

[34] Peng Tang, Xinggang Wang, Angtian Wang, Yongluan Yan,
Wenyu Liu, Junzhou Huang, and Alan Yuille. Weakly su-
pervised region proposal network and object detection.
In
ECCV, 2018.

[35] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-
ers, and Arnold WM Smeulders. Selective search for object
recognition. IJCV, 2013.

[36] Chong Wang, Weiqiang Ren, Kaiqi Huang, and Tieniu Tan.
Weakly supervised object localization with latent category
learning. In ECCV, 2014.

[37] Jiajie Wang, Jiangchao Yao, Ya Zhang, and Rui Zhang. Col-
laborative learning for weakly supervised object detection.
In IJCAI, 2018.

[38] Xinggang Wang, Zhuotun Zhu, Cong Yao, and Xiang Bai.
Relaxed multiple-instance svm with application to object
discovery. In ICCV, 2015.

9440

[39] Ziang Yan, Jian Liang, Weishen Pan, Jin Li, and Chang-
shui Zhang. Weakly-and semi-supervised object detection
with expectation-maximization algorithm. arXiv preprint
arXiv:1702.08740, 2017.

[40] Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong, and Qi Tian.
Zigzag learning for weakly supervised object detection. In
CVPR, 2018.

[41] Xiaopeng Zhang, Yang Yang, and Jiashi Feng. ML-Locnet:
Improving object localization with multi-view learning net-
work. In ECCV, 2018.

[42] Yongqiang Zhang, Yancheng Bai, Mingli Ding, Yongqiang
Li, and Bernard Ghanem. W2F: A weakly-supervised to
fully-supervised framework for object detection. In CVPR,
2018.

9441

