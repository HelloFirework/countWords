Learning Spatio-Temporal Representation with Local and Global Diffusion∗

Zhaofan Qiu†, Ting Yao‡, Chong-Wah Ngo§, Xinmei Tian†, and Tao Mei‡

† University of Science and Technology of China, Hefei, China

‡ JD AI Research, Beijing, China

§ City University of Hong Kong, Kowloon, Hong Kong

{zhaofanqiu, tingyao.ustc}@gmail.com, cscwngo@cityu.edu.hk, xinmei@ustc.edu.cn, tmei@live.com

Abstract

Convolutional Neural Networks (CNN) have been re-
garded as a powerful class of models for visual recog-
nition problems. Nevertheless,
the convolutional ﬁlters
in these networks are local operations while ignoring the
large-range dependency.
Such drawback becomes even
worse particularly for video recognition, since video is an
information-intensive media with complex temporal varia-
tions. In this paper, we present a novel framework to boost
the spatio-temporal representation learning by Local and
Global Diffusion (LGD). Speciﬁcally, we construct a novel
neural network architecture that learns the local and global
representations in parallel. The architecture is composed
of LGD blocks, where each block updates local and global
features by modeling the diffusions between these two rep-
resentations. Diffusions effectively interact two aspects of
information, i.e., localized and holistic, for more power-
ful way of representation learning. Furthermore, a ker-
nelized classiﬁer is introduced to combine the representa-
tions from two aspects for video recognition. Our LGD
networks achieve clear improvements on the large-scale
Kinetics-400 and Kinetics-600 video classiﬁcation datasets
against the best competitors by 3.5% and 0.7%. We fur-
ther examine the generalization of both the global and local
representations produced by our pre-trained LGD networks
on four different benchmarks for video action recognition
and spatio-temporal action detection tasks. Superior per-
formances over several state-of-the-art techniques on these
benchmarks are reported.

1. Introduction

Today’s digital contents are inherently multimedia. Par-
ticularly, with the proliferation of sensor-rich mobile de-
vices, images and videos become media of everyday com-
munication. Therefore, understanding of multimedia con-
tent becomes highly demanded, which accelerates the

∗This work was performed at JD AI Research.

Figure 1. The schematic illustration of the Local and Global Dif-
fusion block. The diffusion between local and global paths enrich
the representation learnt on each path.

development of various techniques in visual annotation.
Among them, a fundamental breakthrough underlining the
success of these techniques is representation learning. This
can be evidenced by the success of Convolutional Neural
Networks (CNN), which demonstrates high capability of
learning and generalization in visual representation. For ex-
ample, an ensemble of residual nets [11] achieves 3.57%
top-5 error on ImageNet test set, which is even lower than
5.1% of the reported human-level performance. Despite
these impressive progresses, learning powerful and generic
spatio-temporal representation remains challenging, due to
larger variations and complexities of video content.

A natural extension of CNN from image to video do-
main is by direct exploitation of 2D CNN on video frames
[18, 34, 41] or 3D CNN on video clips [15, 28, 29, 38]. An
inherent limitation of this extension, however, is that each
convolution operation, either 2D or 3D, processes only a
local window of neighboring pixels. As window size is nor-
mally set to a small value, the holistic view of ﬁeld can-
not be adequately captured. This problem is engineered by
performing repeated convolution and pooling operations to
capture long-range visual dependencies. In this way, recep-
tive ﬁelds can be increased through progressive propagation
of signal responses over local operations. When a network
is deep, the repeated operations, however, post difﬁculty
to parameter optimization. Concretely, the connection be-

12056

C x T x H x WC x T x H x WC x 1 x 1 x 1C x 1 x 1 x 1Local PathGlobal Pathtween two distant pixels are only established after a large
number of local operations, resulting in vanishing gradient.
In this paper, we present Local and Global Diffusion
(LGD) networks – a novel architecture to learn spatio-
temporal representations capturing large-range dependen-
cies, as shown in Figure 1. In LGD networks, the feature
maps are divided into local and global paths, respectively
describing local variation and holistic appearance at each
spatio-temporal location. The networks are composed of
several staked LGD blocks of each couples with mutually
inferring local and global paths. Speciﬁcally, the infer-
ence takes place by attaching the residual value of global
path to the output of local feature map, while the feature of
global path is produced by linear embedding of itself with
the global average pooling of local feature map. The diffu-
sion is constructed at every level from bottom to top such
that the learnt representations encapsulate a holistic view
of content evolution. Furthermore, the ﬁnal representations
from both paths are combined by a novel kernel-based clas-
siﬁer proposed in this paper.

The main contribution of this work is the proposal of
the Local and Global Diffusion networks, which is a two-
path network aiming to model local and global video in-
formation. The diffusion between two paths enables the
capturing of large-range dependency by the learnt video
representations economically and effectively. Through an
extensive set of experiments, we demonstrate that our
LGD network outperforms several state-of-the-art models
on six benchmarks, including Kinetics-400, Kinetics-600,
UCF101, HMDB51 for video action recognition and J-
HMDB, UCF101D for spatio-temporal action detection.

2. Related Work

We broadly categorize the existing research in video rep-
resentation learning into hand-crafted and deep learning
based methods.

Hand-crafted representation starts by detecting spatio-
temporal interest points and then describing them with lo-
cal representations. Examples of representations include
Space-Time Interest Points (STIP) [21], Histogram of Gra-
dient and Histogram of Optical Flow [22], 3D Histogram
of Gradient [19], SIFT-3D [33] and Extended SURF [45].
These representations are extended from image domain to
model temporal variation of 3D volumes. One particularly
popular representation is the dense trajectory feature pro-
posed by Wang et al., which densely samples local patches
from each frame at different scales and then tracks them in
a dense optical ﬂow ﬁeld [40]. These hand-crafted descrip-
tors, however, are not optimized and hardly to be general-
ized across different tasks of video analysis.

The second category is deep learning based video rep-
resentation. The early works are mostly extended from im-
age representation by applying 2D CNN on video frames.

Karparthy et al. stack CNN-based frame-level representa-
tions in a ﬁxed size of windows and then leverage spatio-
temporal convolutions for learning video representation
[18]. In [34], the famous two-stream architecture is devised
by applying two 2D CNN architectures separately on visual
frames and staked optical ﬂows. This two-stream architec-
ture is further extended by exploiting convolutional fusion
[5], spatio-temporal attention [24], temporal segment net-
works [41, 42] and convolutional encoding [4, 27] for video
representation learning. Ng et al. [49] highlight the draw-
back of performing 2D CNN on video frames, in which
long-term dependencies cannot be captured by two-stream
network. To overcome this limitation, LSTM-RNN is pro-
posed by [49] to model long-range temporal dynamics in
videos. Srivastava et al. [37] further formulate the video
representation learning task as an autoencoder model based
on the encoder and decoder LSTMs.

The aforementioned approaches are limited by treating
video as a sequence of frames and optical ﬂows for repre-
sentation learning. More concretely, pixel-level temporal
evolution across consecutive frames are not explored. The
problem is addressed by 3D CNN proposed by Ji et al. [15],
which directly learns spatio-temporal representation from a
short video clip. Later in [38], Tran et al. devise a widely
adopted 3D CNN, namely C3D, for learning video repre-
sentation over 16-frame video clips in the context of large-
scale supervised video dataset. Furthermore, performance
of the 3D CNN is further boosted by inﬂating 2D convo-
lutional kernels [3], decomposing 3D convolutional kernels
[28, 39] and aggregated residual transformation [9].

Despite these progresses, long-range temporal depen-
dency beyond local operation remains not fully exploited,
which is the main theme of this paper. The most closely
related work to this paper is [43], which investigates the
non-local mean operation proposed in [2]. The work cap-
tures long-range dependency by iterative utilization of lo-
cal and non-local operations. Our method is different from
[43] in that local and global representations are learnt simul-
taneously and the interaction between them encapsulates a
holistic view for the local representation. In addition, we
combine the ﬁnal representations from both paths for more
accurate prediction.

3. Local and Global Diffusion

We start by introducing the Local and Global Diffusion
(LGD) blocks for representation learning. LGD is a cell
with local and global paths interacting each other. A clas-
siﬁer is proposed to combine local and global representa-
tions. With these, two LGD networks, namely LGD-2D and
LGD-3D deriving from temporal segment networks [41]
and pseudo-3D convolutional networks [28], respectively,
are further detailed.

12057

3.1. Local and Global Diffusion Blocks

Unlike the existing methods which stack the local opera-
tions to learn spatio-temporal representations, our proposed
Local and Global Diffusion (LGD) model additionally in-
tegrates the global aspect into video representation learn-
ing. Speciﬁcally, we propose the novel neural networks that
learn the discriminative local representation and global rep-
resentation in parallel while combining them to synthesize
new information. To achieve this, the feature maps in neural
networks are splitted into local path and global path. Then,
we deﬁne a LGD block to model the interaction between
two paths as:

{xl, gl} = B({xl−1, gl−1}) ,

(1)

where {xl−1, gl−1} and {xl, gl} denote the input pair and
output pair of the l-th block. The local-global pair consists
of local feature map xl ∈ RC×T ×H×W and global feature
vector gl ∈ RC , where C, T , H and W are the number
of channels, temporal length, height and width of the 4D
volume data, respectively.

The detailed operations inside each block B are shown
in Figure 2 and can be decomposed into two diffusion di-
rections as following.

(1) Global-to-local diffusion. The ﬁrst direction is to
learn the transformation from xl−1 to the updated local fea-
ture xl with the priority of global vector gl−1. Taking the
inspiration from the recent successes of Residual Learning
[11], we aim to formulate the global priority as the global
residual value, which can be broadcasted to each location as

xl = ReLU(F(xl−1) + US(Wx,ggl−1)) ,

(2)

where Wx,g ∈ RC×C is the projection matrix, US is
the upsampling operation duplicating the residual vector to
each location and F is a local transformation function (i.e.,
3D convolutions). The choice of function F is dependent on
the network architecture and will be discussed in Section 4.
(2) Local-to-global diffusion. The second direction is to
update the global vector with current local feature xl. Here,
we simply linearly embed the input global feature gl−1 and
Global Average Pooling (GAP) of local feature P(xl) by

gl = ReLU(Wg,xP(xl) + Wg,ggl−1) ,

(3)

where Wg,x ∈ RC×C and Wg,g ∈ RC×C are the projec-
tion matrices combining local and global features.

Compared with the traditional convolutional block
which directly apply the transformation F to local feature,
the LGD block introduced in Eq.(2) and Eq.(3) only re-
quires three more projection matrices to produce the out-
put pair. In order to reduce the additional parameters for
LGD block, we exploit the low-rank approximation of each
projection matrix as W = W1W2, in which W1 ∈ RC× ˆC

Figure 2. A diagram of a LGD block.

ˆC×C . When ˆC ≪ C, the parameters as well as
and W2 ∈ R
computational cost can be sharply reduced. Through cross-
validation, we empirically set ˆC = C
16 which is found not to
impact the performance negatively. By this approximation,
the number of additional parameters is reduced from 3C 2
to 3

8 C 2 for each block.

3.2. Local and Global Combination Classiﬁer

With the proposed LGD block, the network can learn lo-
cal and global representations in parallel. The next question
is how to make the ﬁnal prediction by combining the two
representations. Here, we consider the kernelized view of
similarity measurement between two videos. Formally, de-
note {xL, gL} and {x′
L} as the last output pair of two
videos, we choose the bilinear kernel [25] on both the lo-
cal and global features, which can be trained end-to-end in
neural network. Thus, the kernel function can be given by

L, g′

L, g′

L})
Li2 + hgL, g′

Li2

=

k({xL, gL}, {x′
= hxL, x′
1
N 2 X
1
N 2 X

X

X

≈

j

i

i

j

hxi

L, x′j

Li2 + hgL, g′

Li2

hϕ(xi

L), ϕ(x′j

L)i + hϕ(gL), ϕ(g′

,

L)i

(4)
in which N = L × H × W is the number of spatio-temporal
locations, h·, ·i2 is the bilinear kernel and xi
L ∈ RC denotes
the feature vector of i-th position in xL. In the last line of
Eq (4), we approximate the bilinear kernel by Tensor Sketch
Projection ϕ in [6], which can effectively reduce the dimen-
sion of feature space. By decomposing the kernel function
in Eq (4), the feature mapping is formulated as

φ({xL, gL}) = [

1
N X

i

ϕ(xi

L), ϕ(gL)] ,

(5)

12058

F(xl-1) + US(Wx,g gl-1)Wg,x P(xl) + Wg,x gl-1++LGD blockLegendUnweighted connectionsWeighted connectionsLocal TransformationGlobal average poolingUpsamplingReLU activation+Function of sumgl-1glxl-1xlFigure 3. The overview of two different Local and Global Diffusion networks. The upper one, called LGD-2D, applies the LGD block on
the temporal segment network [41], which sparsely samples several frames and exploits 2D convolution as the local transformation. The
lower one, called LGD-3D, continuously samples a short video clip and exploits pseudo-3D convolution [28] as the local transformation.
For both LGD networks, the learnt local and global features are combined to achieve the ﬁnal representation.

where [·, ·] denotes concatenation of two vectors. The
φ({xL, gL}) combines the pair into a high dimensional rep-
resentation. The whole process can be trained end-to-end in
the neural networks. Finally, the resulting representation is
fed into a fully connected layer for class labels prediction.

4. Local and Global Diffusion Networks

The proposed LGD block and the classiﬁer can be eas-
ily integrated with most of the existing video representation
learning frameworks. Figure 3 shows two different con-
structions of LGD blocks, called LGD-2D and LGD-3D,
with different transformation F and training strategies.

4.1. LGD 2D

The straightforward way to learn video representation di-
rectly employs 2D convolution as the transformation func-
tion F . Thus, in the local path of LGD-2D, a shared 2D
CNN is performed as backbone network on each frame in-
dependently, as shown in the upper part in Figure 3. To
enable efﬁcient end-to-end learning, we uniformly split a
video into T snippets and select only one frame per snippet
for processing. The idea is inspired by Temporal Segment
Network (TSN) [41, 42], which overcomes computational
issue by selecting a subset of frames for long-term temporal
modeling. Thus, the input of LGD-2D consists of T non-
continuous frames, and the global path learns a holistic rep-
resentation of all these frames. Please note that the initial
local representation x1 is achieved by a single local oper-
ation F applied on the input frames, and the initial global
representation g1 = P(x1) is the global average of x1. At
the end of the networks, the local and global combination
classiﬁer is employed to achieve a hybrid prediction.

4.2. LGD 3D

Another major branch of video representation learning
is 3D CNN [15, 28, 38]. Following the common settings of

3D CNN, we feed T consecutive frames into the LGD-3D
network and exploit 3D convolution as local transformation
F , as shown in the lower part in Figure 3. Nevertheless,
the training of 3D CNN is computationally expensive and
the model size also has a quadratic growth compared with
2D CNN. Therefore, we choose the pseudo-3D convolution
proposed in [28] that decomposes 3D learning into 2D con-
volutions in spatial space and 1D operations in temporal di-
mension. To simplify the decomposition, in this paper, we
only choose P3D-A block with the highest performance in
[28], which cascades the the spatial convolution and tempo-
ral convolution in turn.

Here, we show the exampler architecture of LGD-3D
based on the ResNet-50 [11] backbone in Table 1. The
LGD-3D ﬁrstly replaces each 3 × 3 convolutional kernel
in original ResNet-50 with one 1 × 3 × 3 spatial convolu-
tion and 3 × 1 × 1 temporal convolution, and then builds a
LGD block based on each residual unit. All the weights of
spatial convolutions can be initialized from the pre-trained
ResNet-50 model as done in [28]. The dimension of input
video clip is set as 16 × 112 × 112 consisting of 16 consec-
utive frames with resolution 112 × 112. The clip length will
be reduced twice by two max pooling layers with temporal
stride of 2. The computational cost and training time thus
can be effectively reduced by the small input resolution and
temporal pooling. The ﬁnal local representation with di-
mension 4 × 7 × 7 is combined with global representation
by the kernelized classiﬁer. This architecture can be easily
extended to ResNet-101 or deeper networks by repeating
more LGD blocks.

4.3. Optimization

Next, we present the optimization of LGD networks.
Considering the difﬁculty in training the whole network
from scratch by kernelized classiﬁer [6, 25], we propose a
two-stage strategy to train the LGD networks. At the begin-
ning of the training, we optimize the basic network without

12059

3D Conv3D Conv3D ConvGAPGAP+......GAP+GAP+Continuously SamplingGlobal PathLocal PathLGD-3DCombination2D Conv2D Conv2D Conv2D Conv2D Conv2D Conv2D Conv2D Conv2D ConvGAPGAP+......GAP+GAP+Sparse SamplingGlobal PathLocal PathLGD-2DCombinationTable 1. The detailed architecture of LGD-3D with the ResNet-50
backbone network. The LGD blocks are shown in brackets and
the kernel size for each convolution is presented followed by the
number of output channels.

Layer

conv1

pool1

res2

Operation

Local path size

1 × 7 × 7, 64
3 × 1 × 1, 64

, stride 1, 2, 2

16 × 56 × 56

2 × 1 × 1, max, stride 2, 1, 1

8 × 56 × 56




1 × 1 × 1, 64
1 × 3 × 3, 64
3 × 1 × 1, 64
1 × 1 × 1, 256




LGD

× 3

8 × 56 × 56

pool2

2 × 1 × 1, max, stride 2, 1, 1

4 × 56 × 56

res3

res4

res5

LGD








1 × 1 × 1, 128
1 × 3 × 3, 128
3 × 1 × 1, 128
1 × 1 × 1, 512

1 × 1 × 1, 256
1 × 3 × 3, 256
3 × 1 × 1, 256
1 × 1 × 1, 1024

1 × 1 × 1, 512
1 × 3 × 3, 512
3 × 1 × 1, 512
1 × 1 × 1, 2048








× 4

4 × 28 × 28

× 6

4 × 14 × 14

× 3

4 × 7 × 7

LGD

LGD

the combination classiﬁer, and adjust local and global rep-
resentations separately. Denote {xL, gL} and y as the last
output pair and corresponding category of the input video,
the optimization function is given as

LW

g (gL, y) + LW

x (P(xL), y) ,

(6)

where LW denotes the softmax cross-entropy loss with pro-
jection matrix W. The overall loss consists of the classiﬁ-
cation errors from both global representation and local rep-
resentation after global average pooling. After the training
of basic network, we then tune the whole network with the
following loss:

LW

c (φ({xL, gL}), y) ,

(7)

where φ(·) is the feature mapping proposed in Section 3.2.

5. Experiments

5.1. Datasets

We empirically evaluate LGD networks on the Kinetcis-
400 [3] and Kinetcis-600 [7] datasets. The Kinetics-400
dataset is one of the large-scale action recognition bench-
marks. It consists of around 300K videos from 400 action
categories. The 300K videos are divided into 240K, 20K,
40K for training, validation and test sets, respectively. Each
video in this dataset is 10-second short clip cropped from
the raw YouTube video. Note that the labels for test set are
not publicly available and the performances on Kinetics-
400 dataset are all reported on the validation set. The
Kinetics-600 is an extended version of Kinetics-400 dataset,
ﬁrstly made public in ActivityNet Challenge 2018 [7]. It

consists of around 480K videos from 600 action categories.
The 480K videos are divided into 390K, 30K, 60K for train-
ing, validation and test sets, respectively. Since the labels
for Kinetics-600 test set are available, we report the ﬁnal
performance on both the validation and test sets.

5.2. Training and Inference Strategy

Our proposal is implemented on Caffe [16] framework
and the mini-batch Stochastic Gradient Descent (SGD) al-
gorithm is employed to optimize the model. In the training
stage, for LGD-2D, we set the input as 224 × 224 image
which is randomly cropped from the resized 240×320 video
frame. For LGD-3D, the dimension of input video clips is
set as 16 × 112 × 112, which is randomly cropped from
the resized non-overlapping 16-frame clip with the size of
16 × 120 × 160. Each frame/clip is randomly ﬂipped along
horizontal direction for data augmentation. We set each
mini-batch as 128 triple frames for LGD-2D, and 64 clips
for LGD-3D, which are implemented with multiple GPUs
in parallel. The network parameters are optimized by stan-
dard SGD. For each stage in Section 4.3, the initial learning
rate is set as 0.01, which is divided by 10 after every 20
epochs. The training is stopped after 50 epoches.

There are two weights initialization strategies for LGD
networks. The ﬁrst one is to train the whole networks
from scratch. In this way, all the convolutional kernels and
the projection matrices W in LGD block are initialized by
Xavier initialization [8], and all the biases are set as zero.
The second one initializes the spatial convolutions with the
existing 2D CNN pre-trained on ImageNet dataset [31]. In
order to keep the semantic information for these pre-trained
convolutions, we set the projection matrix Wx,g as zero,
making the global residual value vanishes when the training
begins. Especially, the temporal convolutions in LGD-3D
are initialized as an identity mapping in this case.

In the inference stage, we resize the video frames with
the shorter side 240/120 for LGD-2D/LGD-3D, and per-
form spatially fully convolutional inference on the whole
frame. Thus, the LGD-2D will predict one score for each
triple frames and the video-level prediction score is calcu-
lated by averaging all scores from 10 uniformly sampled
triple frames. Similarly, the video-level prediction score
from LGD-3D is achieved by averaging all scores from 15
uniformly sampled 16-frame clips.

5.3. Evaluation of LGD block

We ﬁrstly verify the effectiveness of our proposed LGD
block for spatio-temporal representation learning and com-
pare with two diffusion block variants, i.e., blockv1 and
blockv2 by different diffusion functions. Speciﬁcally, com-
pared with LGD block, the blockv1 ignores the global rep-
resentation from lower layers, making the output function

12060

Table 2. Performance comparisons between baseline and LGD
block variants on Kinetics-600 validation set. All the backbone
networks are ResNet-50 trained from scratch. The local and global
combination classiﬁer is not used for fair comparison.

(a) LGD-2D

(b) LGD-3D

Method

Top-1 Top-5

Method

Top-1 Top-5

TSN baseline

blockv1

blockv2

71.0

71.6

90.0

90.2

P3D baseline

blockv1

72.2

90.5

blockv2

71.2

72.7

90.5

91.1

73.6

91.6

LGD block

72.5

90.7

LGD block

74.2

92.0

Table 3. Performance contribution of each design in LGD net-
works. Top-1 accuracies are shown on Kinetics-600 validation set.

R50 R101
√
√
√

Method

LGD-2D

LGD-3D

√
√
√
√

√
√
√

√
√
√
√

Img Com Long Top-1

√
√
√
√
√
√
√
√
√
√

√

√

√
√

√
√

72.5
74.4
74.8
74.5
76.4
76.7
74.2
75.8
76.3
79.4
76.0
77.7
78.3
81.5

√

√

(a) LGD-2D

(b) LGD-3D

Figure 4. The training loss on Kinetics-600 datasets. All the back-
bone networks in this ﬁgure are ResNet-50 trained from scratch.

of global path as

gl = P(xl) .

(8)

Motivated by the channel-wise scaling proposed in [13], the
blockv2 utilizes the global priority as channel-wise multi-
plication. Thus, the output of local path in blockv2 can be
formulated as

xl = ReLU(F(xl−1) ⊙ U S(sigmoid(Wx,ggl−1))) ,

(9)

where ⊙ denotes the element-wise multiplication.

Table 2 summarizes the performance comparisons on
Kinetics-600 dataset. The backbone architectures are all
ResNet-50 trained from scratch. Overall, all the three diffu-
sion blocks (i.e., LGD block, blockv1 and blockv2) exhibit
better performance than baseline networks for both 2D and
3D CNNs. The results basically indicate the advantage of
exploring large-scale dependency by the diffusion between
local path and global path. In particular, as indicated by our
results, utilizing the proposed LGD block which embeds
both input local and global representations and explores the
global priority as residual value, can constantly lead to bet-
ter performance than blockv1 and blockv2.

The loss curves of baseline networks and LGD networks
are shown in Figure 4. The training losses of local and
global paths in Eq.
(9) are given separately. Generally,
the LGD networks produce lower losses than baseline net-
works, and converge faster and stably. Another observation
is that the loss on local path is consistently lower than the
loss on global path. We speculate that this may be due to
information lost by low-rank approximation of projection
matrices in Eq. (3).

5.4. An Ablation Study of LGD networks

Next, we study how each design in LGD networks inﬂu-
ences the overall performance. Here, we choose ResNet-50
(R50) or ResNet-101 (R101) as backbone network. This
backbone network is either trained from scratch or pre-
trained by ImageNet (Img). The local and global combina-
tion classiﬁer (Com) uses the kernelized classiﬁer for pre-
diction. In order to capture long-term temporal information,
we further extend the LGD-3D network with 128-frame in-
put (Long). Following the settings in [43], we ﬁrstly train
the networks with 16-frame clips in the ﬁrst stage in Sec-
tion 4.3 and then with 128-frame clips in the second stage.
When training with 128-frame clips, we increase the stride
of pool1 layer to 4, and set each mini-batch as 16 clips to
meet the requirements of GPU memory. The training is
stopped after 12.5 epoches.

Table 3 details the accuracy improvement on Kinetics-
600 dataset by different designs of LGD networks. When
exploiting ResNet-50 as backbone network, the pre-training
on ImageNet dataset successfully boosts up the top-1 accu-
racy from 72.5% to 74.4% for LGD-2D and from 74.2% to
75.8% for LGD-3D. This demonstrates the effectiveness of
pre-training on large-scale image recognition dataset. The
local and global combination classiﬁer which combines the
representations from two paths leads to the performance
boost of 0.4% and 0.5% for LGD-2D and LGD-3D, respec-
tively. Especially for LGD-3D, the training on 128-frame
clips contributes a large performance increase of 3.1% by
involving long-term temporal information in the network.
Moreover, compared with ResNet-50, both the LGD-2D
and LGD-3D based on ResNet-101 exhibit signiﬁcantly
better performance, with the top-1 accuracy of 76.7% and
81.5% for LGD-2D and LGD-3D, respectively. The results
verify that deeper networks have larger learning capacity for
spatio-temporal representation learning.

12061

00.20.40.60.811.21.41.61.821050.511.522.533.544.5TSNLGD-2D localLGD-2D globalIterationsSoftmax Cross-Entropy Loss00.511.522.533.541050.511.522.533.544.5P3DLGD-3D localLGD-3D globalIterationsSoftmax Cross-Entropy LossTable 4. Performance comparisons with the state-of-the-art meth-
ods on Kinetics-400 validation set.

Backbone

Top-1 Top-5

Inception
Inception
Inception
custom
custom
custom
custom

ResNet-101

Inception
Inception
Inception

Method
I3D RGB [3]
I3D Flow [3]
I3D Two-stream [3]
ResNeXt-101 RGB [9]
R(2+1)D RGB [39]
R(2+1)D Flow [39]
R(2+1)D Two-stream [39]
NL I3D RGB [43]
S3D-G RGB [46]
S3D-G Flow [46]
S3D-G Two-stream [46]
From Anet17 winner report [1]
2D CNN RGB
Three-stream late fusion
Three-stream SATT
LGD-3D RGB
LGD-3D Flow
LGD-3D Two-stream

Inception-ResNet-v2
Inception-ResNet-v2
Inception-ResNet-v2

ResNet-101
ResNet-101
ResNet-101

72.1
65.3
75.7
65.1
74.3
68.5
75.4
77.7
74.7
68.0
77.2

73.0
74.9
77.7
79.4
72.3
81.2

90.3
86.2
92.0
85.7
91.4
88.1
91.9
93.3
93.4
87.6
93.0

90.9
91.6
93.2
94.4
90.9
95.2

Table 5. Performance comparisons with the state-of-the-art meth-
ods on Kinetics-600. Most of the performances are reported on
validation set except the performance of LGD-3D Two-stream*
are on the test set.

Inception-ResNet-v2

Backbone

ResNet-101

SENet-152
SENet-152

Method
From Anet18 winner report [10]
TSN RGB
TSN Flow
StNet RGB
NL I3D RGB
Three-stream Attention
Three-stream iTXN
From Anet18 runner-up report [48]
P3D RGB
P3D Flow
P3D Two-stream
LGD-3D RGB
LGD-3D Flow
LGD-3D Two-stream
LGD-3D Two-stream*

ResNet-152
ResNet-152
ResNet-152
ResNet-101
ResNet-101
ResNet-101
ResNet-101

mixed
mixed

Top-1 Top-5

76.2
71.3
78.9
78.6
82.3
82.4

78.4
71.0
80.9
81.5
75.0
83.1
82.7

–
–
–
–

96.0
95.8

93.9
90.0
94.9
95.6
92.4
96.2
96.0

5.5. Comparisons with State of the Art

We compare with several state-of-the-art techniques on
Kinetics-400 and Kinetics-600 datasets. The performance
comparisons are summarized in tables 4 and 5, respectively.
Please note that most recent works employ fusion of two
or three modalities on these two datasets. Broadly, we
can categorize the most common modalities into four cate-
gories, i.e., RGB, Flow, Two-stream and Three-stream. The
RGB/Flow feeds the video frames/optical ﬂow images into
the networks. The optical ﬂow image in this paper con-
sists of two-direction optical ﬂow extracted by TV-L1 algo-
rithm [50]. The predictions from RGB and Flow modalities
are fused by Two-stream methods. The Three-stream ap-
proaches further merge the prediction from audio input.

As shown in Table 4, with only RGB input, the LGD-3D
achieves 79.4% top-1 accuracy, which makes the relative
improvement over the recent approaches I3D [3], R(2+1)D
[39], NL I3D [43] and S3D-G [46] by 10.1%, 6.8%, 2.1%
and 6.2%, respectively. This accuracy is also higher than
2D CNN with a deeper backbone reported by the Activi-
tyNet 2017 challenge winner [1]. Note that the LGD-3D
with RGB input can obtain higher performance even com-
pared with the Two-stream or Three-stream methods. When
fusing the prediction from both RGB and Flow modali-
ties, the accuracy of LGD-3D will be further improved to
81.2%, which is to-date the best published performance on
Kinetics-400.

Similar results are also observed on Kinetics-600, as
summarized in Table 5.
Since this dataset is recently
made available for ActivityNet 2018 challenge, we show the
performance of different approaches reported by the chal-
lenge winner [10] and challenge runner-up [48]. With the

RGB inputs, LGD-3D achieves 81.5% top-1 accuracy on
Kinetics-600 validation set, which obtains 3.4% relative im-
provement than P3D with the deeper backbone of ResNet-
152. The performance is higher than that of NL I3D which
also explores large-range dependency. This result basically
indicates that LGD network is an effective way to learn
video representation with a global aspect. By combining
the RGB and Flow modalities, the top-1 accuracy of LGD-
3D achieves 83.1%, which is even higher than three-stream
method proposed by ActivityNet 2018 challenge winner.

5.6. Evaluation on Video Representation

Here we evaluate video representation learnt by our
LGD-3D for two different
tasks and on four popular
datasets, i.e., UCF101, HMDB51, J-HMDB and UCF101D.
UCF101 [36] and HMDB51[20] are two of the most popu-
lar video action recognition benchmarks. UCF101 consists
of 13K videos from 101 action categories, and HMDB51
consists of 7K videos from 51 action categories. We follow
the three training/test splits provided by the dataset organ-
isers. Each split in UCF101 includes about 9.5K training
and 3.7K test videos, while a HMDB51 split contains 3.5K
training and 1.5K test videos.

J-HMDB and UCF101D are two datasets for spatio-
temporal action detection. J-HMDB [14] contains 928 well
trimmed video clips of 21 actions. The videos are truncated
to actions and the bounding box annotations are available
for all frames. It provides three training/test splits for eval-
uation. UCF101D [36] is a subset of UCF101 for action de-
tection task. It consists of 3K videos from 24 classes with
spatio-temporal ground truths.

We ﬁrst validate the global representations learnt by
the pre-trained LGD-3D network. Therefore, we ﬁne-tune
the pre-trained LGD-3D on the UCF101 and HMDB51

12062

Table 6. Performance comparisons with the state-of-the-art meth-
ods on UCF101 (3 splits) and HMDB51 (3 splits).

Method
IDT [40]
Two-stream [34]
TSN [41]
I3D RGB [3]
I3D Flow [3]
I3D Two-stream [3]
ResNeXt-101 RGB [9]
R(2+1)D RGB [39]
R(2+1)D Flow [39]
R(2+1)D Two-stream [39]
S3D-G RGB [46]
LGD-3D RGB
LGD-3D Flow
LGD-3D Two-stream

Pretraining

U101 H51

–

ImageNet
ImageNet

86.4 61.7
88.0 59.4
94.2 69.4
ImageNet+Kinetics-400 95.4 74.5
ImageNet+Kinetics-400 95.4 74.6
ImageNet+Kinetics-400 97.9 80.2
94.5 70.2
96.8 74.5
95.5 76.4
97.3 78.7
ImageNet+Kinetics-400 96.8 75.9
ImageNet+Kinetics-600 97.0 75.7
ImageNet+Kinetics-600 96.8 78.9
ImageNet+Kinetics-600 98.2 80.5

Kinetics-400
Kinetics-400
Kinetics-400
Kinetics-400

Table 7. The performance in terms of video-mAP on J-HMDB (3
splits) and UCF101D datasets.

Method

J-HMDB
0.2
0.5

UCF101D
0.2

0.05 0.1

0.3

Weinzaepfel et al. [44]

63.1 60.7

54.3 51.7 46.8 37.8

Saha et al. [32]
Peng et al. [26]

72.6 71.5

79.1 76.6 66.8 55.5

74.3 73.1

78.8 77.3 72.9 65.7

Singh et al. [35]

73.8 72.0

Kalogeiton et al. [17]

74.2 73.7

–

–

–

–

73.5

77.2

–

–

Hou et al. [12]
Yang et al. [47]

78.4 76.9

78.2 77.9 73.1 69.4

–

–

79.0 77.3 73.5 60.8

Li et al. [23]

82.7 81.3
77.3 74.2
LGD-3D RGB
84.5 82.9
LGD-3D Flow
LGD-3D Two-stream 85.7 84.9

82.1 81.3 77.9 71.4
78.8 77.6 69.3 64.1
86.5 84.2 79.8 74.7
88.3 87.1 82.2 75.6

datasets. The performance comparisons are summarized in
Table 6. Overall, the two-stream LGD-3D achieves 98.2%
on UCF101 and 80.5% on HMDB51, which consistently
indicate that video representation produced by our LGD-
3D attains a performance boost against baselines on ac-
tion recognition task. Speciﬁcally, the two-stream LGD-3D
outperforms three traditional approaches, i.e., IDT, Two-
stream and TSN by 11.8%, 10.2% and 4.0% on UCF101,
respectively. The results demonstrate the advantage of pre-
training on large-scale video recognition dataset. Moreover,
compared with recent methods pre-trained on Kinetics-400
dataset, LGD-3D still surpasses the best competitor Two-
stream I3D by 0.3% on UCF101.

Next, we turn to evaluate the local representations
from pre-trained LGD-3D networks on the task of spatio-
temporal action detection. To build the action detection
framework based on LGD-3D, we ﬁrstly obtain the action
proposals in each frame by a region proposal network [30]
with ResNet-101. The action tubelet is generated by pro-
posal linking and temporally trimming in [32]. Then the
prediction score of each proposal is estimated by the ROI-

Figure 5. Four detection examples of our method from J-HMDB
(upper two rows) and UCF101D (lower two rows). The proposal
score is given for each bounding box. Top predicted action classes
for each tubelet are on the right.

pooled local feature from LGD-3D network.
In Table 7,
we summarize the performance comparisons on J-HMDB
(3 splits) and UCF101D with different IoU thresholds. Our
LGD-3D achieves the best performance at all the cases.
Speciﬁcally, at the standard threshold (0.5 for J-HMDB,
and 0.2 for UCF101D), LGD-3D makes relative improve-
ment of 4.4% and 5.5% than the best competitor [23] on
J-HMDB and UCF101D, respectively. Figure 5 showcases
four detection examples from J-HMDB and UCF101D.

6. Conclusion

We have presented Local and Global Diffusion (LGD)
network architecture which aims to learn local and global
representations in an uniﬁed fashion. Particularly, we in-
vestigate the interaction between localized and holistic rep-
resentations, by designing LGD block with diffusion op-
erations to model local and global features. A kernelized
classiﬁer is also formulated to combine the ﬁnal prediction
from two representations. With the development of the two
components, we have proposed two LGD network architec-
tures, i.e., LGD-2D and LGD-3D, based on 2D CNN and
3D CNN, respectively. The results on large-scale Kinetics-
400 and Kinetics-600 datasets validate our proposal and
analysis. Similar conclusion is also drawn from the other
four datasets in the context of video action recognition
and spatio-temporal action detection. The spatio-temporal
video representation produced by our LGD networks is not
only effective but also highly generalized across datasets
and tasks. Performance improvements are clearly observed
when comparing to other feature learning techniques. More
remarkably, we achieve new state-of-the-art performances
on all the six datasets.

Our future works are as follows. First, more advanced
techniques, such as attention mechanism, will be investi-
gated in the LGD block. Second, more in-depth study of
how to combine the local and global representations could
be explored. Third, we will extend the LGD network to
other types of inputs, e.g., audio information.

12063

0.800.830.680.670.83Diving0.642Cliff diving0.280Surfing0.0720.730.810.710.850.70Skijet0.551Surfing0.362Fencing0.0600.820.710.790.820.69Shoot bow0.753Shoot gun0.229Brush hair0.0050.940.890.840.790.85Kick ball0.937Jump0.013Stand0.007References

[1] Yunlong Bian, Chuang Gan, Xiao Liu, Fu Li, Xiang Long,
Yandong Li, Heng Qi, Jie Zhou, Shilei Wen, and Yuan-
qing Lin. Revisiting the effectiveness of off-the-shelf tempo-
ral modeling approaches for large-scale video classiﬁcation.
arXiv preprint arXiv:1708.03805, 2017.

[2] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local

algorithm for image denoising. In CVPR, 2005.

[3] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In CVPR,
2017.

[4] Ali Diba, Vivek Sharma, and Luc Van Gool. Deep temporal

linear encoding networks. In CVPR, 2017.

[5] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.
Convolutional two-stream network fusion for video action
recognition. In CVPR, 2016.

[6] Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell.

Compact bilinear pooling. In CVPR, 2016.

[7] Bernard Ghanem,

Juan Carlos Niebles, Cees Snoek,
Fabian Caba Heilbron, Humam Alwassel, Victor Escorcia,
Ranjay Khrisna, Shyamal Buch, and Cuong Duc Dao. The
activitynet large-scale activity recognition challenge 2018
summary. arXiv preprint arXiv:1808.03766, 2018.

[8] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
culty of training deep feedforward neural networks. In AIS-
TATS, 2010.

[9] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
spatiotemporal 3d cnns retrace the history of 2d cnns and
imagenet. In CVPR, 2018.

[10] Dongliang He, Fu Li, Qijie Zhao, Xiang Long, Yi Fu,
and Shilei Wen. Exploiting spatial-temporal modelling and
multi-modal fusion for human action recognition.
arXiv
preprint arXiv:1806.10319, 2018.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[12] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolu-
tional neural network (t-cnn) for action detection in videos.
In ICCV, 2017.

[13] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. In CVPR, 2018.

[14] Hueihan Jhuang,

Juergen Gall, Silvia Zufﬁ, Cordelia
Schmid, and Michael J Black. Towards understanding ac-
tion recognition. In ICCV, 2013.

[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. IEEE
Trans. on PAMI, 35(1):221–231, 2013.

[16] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In ACM MM, 2014.

[17] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari,
and Cordelia Schmid. Action tubelet detector for spatio-
temporal action localization. In ICCV, 2017.

[18] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video

classiﬁcation with convolutional neural networks. In CVPR,
2014.

[19] Alexander Klaser, Marcin Marszałek, and Cordelia Schmid.
In

A spatio-temporal descriptor based on 3d-gradients.
BMVC, 2008.

[20] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
HMDB: a large video database for human motion recogni-
tion. In ICCV, 2011.

[21] Ivan Laptev. On space-time interest points.

International

Journal of Computer Vision, 64(2-3):107–123, 2005.

[22] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Ben-
jamin Rozenfeld. Learning realistic human actions from
movies. In CVPR, 2008.

[23] Dong Li, Zhaofan Qiu, Qi Dai, Ting Yao, and Tao Mei. Re-
current tubelet proposal and recognition networks for action
detection. In ECCV, 2018.

[24] Dong Li, Ting Yao, Lingyu Duan, Tao Mei, and Yong Rui.
Uniﬁed spatio-temporal attention networks for action recog-
nition in videos. IEEE Trans. on MM, 21(2):416–428, 2019.
[25] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.
Bilinear cnn models for ﬁne-grained visual recognition. In
ICCV, 2015.

[26] Xiaojiang Peng and Cordelia Schmid. Multi-region two-

stream r-cnn for action detection. In ECCV, 2016.

[27] Zhaofan Qiu, Ting Yao, and Tao Mei. Deep quantiza-
tion: Encoding convolutional activations with deep gener-
ative model. In CVPR, 2017.

[28] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-
temporal representation with pseudo-3d residual networks.
In ICCV, 2017.

[29] Zhaofan Qiu, Ting Yao, and Tao Mei.

Learning deep
spatio-temporal dependence for semantic video segmenta-
tion. IEEE Trans. on MM, 20(4):939–949, 2018.

[30] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015.

[32] Suman Saha, Gurkirt Singh, Michael Sapienza, Philip HS
Torr, and Fabio Cuzzolin. Deep learning for detecting multi-
ple space-time action tubes in videos. In BMVC, 2016.

[33] Paul Scovanner, Saad Ali, and Mubarak Shah. A 3-
dimensional sift descriptor and its application to action
recognition. In ACM MM, 2007.

[34] Karen Simonyan and Andrew Zisserman. Two-stream con-
In

volutional networks for action recognition in videos.
NIPS, 2014.

[35] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip HS
Torr, and Fabio Cuzzolin. Online real-time multiple spa-
tiotemporal action localisation and prediction.
In ICCV,
2017.

[36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
UCF101: A dataset of 101 human action classes from videos
in the wild. CRCV-TR-12-01, 2012.

12064

[37] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdi-
nov. Unsupervised learning of video representations using
lstms. In ICML, 2015.

[38] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In ICCV, 2015.

[39] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In CVPR, 2018.

[40] Heng Wang and Cordelia Schmid. Action recognition with

improved trajectories. In ICCV, 2013.

[41] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
networks: Towards good practices for deep action recogni-
tion. In ECCV, 2016.

[42] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
networks for action recognition in videos. IEEE Trans. on
PAMI, 2018.

[43] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-

ing He. Non-local neural networks. In CVPR, 2018.

[44] Philippe Weinzaepfel, Zaid Harchaoui,

and Cordelia
Schmid. Learning to track for spatio-temporal action local-
ization. In ICCV, 2015.

[45] Geert Willems, Tinne Tuytelaars, and Luc Van Gool. An efﬁ-
cient dense and scale-invariant spatio-temporal interest point
detector. In ECCV, 2008.

[46] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
Kevin Murphy. Rethinking spatiotemporal feature learning:
Speed-accuracy trade-offs in video classiﬁcation. In ECCV,
2018.

[47] Zhenheng Yang, Jiyang Gao, and Ram Nevatia. Spatio-
temporal action detection with cascade proposal and location
anticipation. In BMVC, 2017.

[48] Ting Yao and Xue Li. Yh technologies at activitynet chal-

lenge 2018. arXiv preprint arXiv:1807.00686, 2018.

[49] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vi-
jayanarasimhan, Oriol Vinyals, Rajat Monga, and George
Toderici. Beyond short snippets: Deep networks for video
classiﬁcation. In CVPR, 2015.

[50] C Zach, T Pock, and H Bischof. A duality based approach

for realtime tv-l1 optical ﬂow. Pattern Recognition, 2007.

12065

