Content-Aware Multi-Level Guidance for Interactive Instance Segmentation

Soumajit Majumder

Angela Yao

Institute of Computer Science II

School of Computing

University of Bonn, Germany

National University of Singapore

majumder@cs.uni-bonn.de

yaoa@comp.nus.edu.sg

Abstract

In interactive instance segmentation, users give feedback
to iteratively reﬁne segmentation masks. The user-provided
clicks are transformed into guidance maps which provide
the network with necessary cues on the whereabouts of the
object of interest. Guidance maps used in current systems
are purely distance-based and are either too localized or
non-informative. We propose a novel transformation of
user clicks to generate content-aware guidance maps that
leverage the hierarchical structural information present in
an image. Using our guidance maps, even the most basic
FCNs are able to outperform existing approaches that re-
quire state-of-the-art segmentation networks pre-trained on
large scale segmentation datasets. We demonstrate the ef-
fectiveness of our proposed transformation strategy through
comprehensive experimentation in which we signiﬁcantly
raise state-of-the-art on four standard interactive segmen-
tation benchmarks.

1. Introduction

Interactive object selection and segmentation allows
users to interactively select objects of interest down to
the pixel level by providing inputs such as clicks, scrib-
bles, or bounding boxes. The segmented results are use-
ful for downstream applications such as image/video edit-
ing [6, 30], image-based medical diagnosis [50, 51], human-
machine collaborative annotation [2], etc. GrabCut [45] is a
pioneering example of interactive segmentation which seg-
ments objects from a user-provided bounding box by iter-
atively updating a colour-based Gaussian mixture model.
Other methods include Graph Cuts [7], Random Walk [18]
and GeoS [13] though more recent methods [32, 35, 36, 52,
53] approach the problem with deep learning architectures
such as convolutional neural networks (CNNs).

In standard, non-interactive instance segmentation [4,
15, 21, 22, 23], the RGB image is given as input and seg-
mentation masks for each object instance are predicted. In
an interactive setting, however, the input consists of the

Figure 1. Existing interactive instance segmentation [26, 31, 32,
53] techniques do not utilize any image information when gener-
ating guidance maps (second column). In contrast, our proposed
technique exploits image structures such as superpixels and ob-
ject proposals, allowing us to generate more informative guidance
maps (ﬁrst column, bottom row).

RGB image as well as ‘guidance’ maps based on user-
provided supervision. The guidance map helps to select the
speciﬁc instance to segment; when working in an iterative
setting, it can also help correct errors from previous seg-
mentations [6, 32, 35, 53].

User feedback is typically given as clicks [26, 31, 32, 35,
36, 53] or bounding boxes [52] and are then transformed
into guidance signals fed as inputs into the CNN. By work-
ing with high-level representations encoded in pre-trained
CNNs, the number of user interactions required to gen-
erate quality segments have been greatly reduced. How-
ever, there is still a large incongruence between the im-
age encoding versus the guidance signal, as user interac-
tions are transformed into simplistic primitives such as Eu-
clidean [53, 31, 26] or Gaussian distance maps [6, 35, 36],
the latter being the preferred choice in more recent works
due to their ability to localize user clicks [35]. Examples of
such guidance maps can be found in Fig. 1 second column,
ﬁrst row and second row respectively.

11602

Our observation is that current guidance signals disre-
gard even the most basic image consistencies present in the
scene, such as colour, local contours, and textures. This
of course also precludes even more sophisticated structures
such as object hypotheses, all of which can be determined
in an unsupervised way. As such, we are motivated to max-
imize the information which can be harnessed from user-
provided clicks and generate more meaningful guidance
maps for interactive instance segmentation.

To that end, we propose a simple yet effective transfor-
mation of user clicks which enables us to leverage a hierar-
chy of image information, starting from low-level cues such
as appearance and texture, based on superpixels, to more
high-level information such as class-independent object hy-
potheses (see Fig. 3). Ours is the ﬁrst work to investigate
the impact of guidance map generation for interactive seg-
mentation. Our ﬁndings suggest that current Gaussian- and
Euclidean distance based maps are too simple and do not
fully leverage structures present in the image. A second
and common drawback of current distance-based guidance
maps is that they fail to account for the scale of the object
during interaction. Object scale has a direct impact on the
network performance when it comes to classiﬁcation [41] or
segmentation [40]. Gaussian- and Euclidean distance maps
are primarily used for localizing the user clicks and do not
account for the object scale. Our algorithm roughly esti-
mates the object scale based on the user-provided clicks and
reﬁnes the guidance maps accordingly.

Our approach is extremely ﬂexible in that the gener-
ated guidance map can be paired with any method which
accepts guidance as a new input channel [53, 32, 35, 6].
We demonstrate via experimentation that providing content-
aware guidance by leveraging the structured information in
an image leads to a signiﬁcant improvement in performance
when compared to the existing state-of-the-art, all the while
using a simple, off-the-shelf, CNN architecture. The key
contributions of our work are as follows :

• We propose a novel transformation of user-provided
clicks which generates guidance maps by leveraging
hierarchical information present in a scene.

• We propose a framework which can account for the
scale of an object and generate the guidance map ac-
cordingly in a click-based user feedback scheme.

• We perform a systematic study of the impact of guid-
ance maps on the interactive segmentation perfor-
mance when generated based on features at different
levels of the image hierarchy.

• We achieve state-of-the-art performance on four seg-
mentation benchmarks; our proposed method signiﬁ-
cantly reduces the amount of user interaction required
for accurate segmentation and uses the fewest number
of average clicks per instance.

2. Related Works

Segmenting objects interactively using clicks, scribbles,
or bounding boxes has always been a problem of interest
in computer vision research, as it can solve some quality
problems faced by fully-automated segmentation methods.
Early variants of interactive image segmentation methods,
such as the parametric active contour model [27] and intel-
ligent scissors [39] mainly considered boundary properties
when performing segmentation; as a result they tend to fare
poorly on weak edges. More recent methods are based on
graph cuts [7, 45, 49, 30], geodesics [5, 13], and or a combi-
nation of the two [19, 44]. However, all these algorithms try
to estimate the foreground/background distributions from
low-level features such as color and texture, which are un-
fortunately insufﬁcient in several instances, e.g. in images
with similar foreground and background appearances, intri-
cate textures, and poor illumination.

As with many other areas of computer vision, deep
learning-based methods have become popular also in inter-
active segmentation in the past few years. In the initial work
of [53], user-provided clicks are converted to Euclidean
distance transform maps which are concatenated with the
color channels and fed as input to a FCN [34]. Clicks are
then added iteratively based on the errors of the previous
prediction. On arrival of each new click, the Euclidean
distance transform maps are updated and inference is per-
formed. The process is repeated until a satisfactory result
is obtained. Subsequent works have focused primarily on
making extensions with newer CNN architectures [35, 6]
and iterative training procedures [35, 32]. In the majority of
these works, user guidance has been provided in the form of
point clicks [53, 35, 32, 36, 31] which are then transformed
into a Euclidean-based distance map [53, 31]. One obser-
vation made in [6, 35, 36] was that encoding the clicks as
Gaussians led to some performance improvement because it
localizes the clicks better [35] and can encode both positive
and negative click in a single channel [6]. In [9], the authors
explore the use of superpixels to generate the guidance map.
However, in contrast to [9] which uses superpixels to
maintain computational efﬁciency w.r.t. to their graph op-
timization, our guidance maps uses superpixels to leverage
the local similarities contained within it. This is a general
principle that we carry across image structures of varying
levels for encoding user inputs. For the most part, there has
been little attention paid to how user inputs should be incor-
porated as guidance; the main focus in interactive segmenta-
tion has been dedicated towards the training procedure and
network architectures.

3. Proposed Approach

We follow previous interactive frameworks [53, 32, 35,
6] in which a user can provide both ‘positive’ and ‘negative’

11603

Superpixel­based Guidance Map

Input

FCN

Prediction

Object­based  
Guidance Map

Distance Transform  
of the initial prediction  

 (optional)

Figure 2. Outline. Given an input image and user interactions, we transform the positive and negative clicks (denoted by the green and
red dots respectively) into three separate channels (2 channel superpixel-based and 1 object proposal-based guidance map), which are
concatenated (denoted as ⊕) with the 3-channel image input and is fed to our network. Additionally, we concatenate the euclidean distance
transform of the predicted mask from the previous iteration as our ﬁnal non-color channel. The solid green line indicates our estimate of
the object scale based on the initial pair of positive and negative click. The output is the ground truth map of the selected object.

clicks to indicate foreground and background/other objects
respectively (as shown in Fig. 2). We denote the set of click
positions as {p0, p1} with subscripts 0 and 1 for positive
and negative clicks respectively. To date, guidance maps
have been generated by as a function of the distance be-
tween each pixel of the image grid to the point of interac-
tion. More formally, for each pixel position p on the image
grid, the pair of distance-based guidance maps for positive
and negative clicks can be computed as

Gd
0 (p) = min
c∈{p0}

d(p, c) and Gd

1 (p) = min
c∈{p1}

d(p, c).

(1)

In the case of Euclidean guidance maps [53], the function
d(·,·) is simply the Euclidean distance.
However, such guidance is image-agnostic and assumes
that each pixel in the scene is independent. Our proposed
approach eschews this assumption and proposes the gen-
eration of multiple guidance maps which align with both
low-level and high-level image structures present in the
scene. We represent low-level structures with superpixels
and high-level ones with region-based object proposals and
describe how we generate guidance maps from these struc-
tures in Sections 3.1 and 3.2.

3.1. Superpixel based guidance map

We ﬁrst consider a form of guidance based on non-
overlapping regions;
in our implementation, we use su-
perpixels.
Superpixels group together locally similarly
coloured pixels while respecting object boundaries [1] and
were the standard working unit of pre-CNN-based segmen-
tation algorithms [42, 17]. Previous works have shown that
most, if not all, pixels in a superpixel belong to the same
category [17, 42, 25]. Based on this observation, we propa-
gate user-provided clicks which are marked on single pixels

to the entire superpixel. We then assign guidance values
to each of the other superpixels in the scene based on the
minimum Euclidean distance from the centroid of each su-
perpixel to the centroid of a user-selected superpixel. One
can think of the guidance as a discretized version of Eq. 1
based on low-level image structures.

More formally, let {S} represent the set of superpixels
from an image and fSP (p) be a function which maps each
pixel location p in the image to the corresponding super-
pixel in {S}. We further deﬁne a positive and negative su-
perpixel set based on the positive and negative clicks, i.e.
{s0 = fSP (p0)} and {s1 = fSP (p1)} respectively. Simi-
lar to the distance-based guidance maps in Eq. 1, we gener-
ate a pair of guidance maps. However, rather than treating
each pixel individually, we propagate the distances between
superpixel centers to all pixels within each superpixel, i.e.

(2)

j of superpixels si and sj respectively, where sc

dc (s, fSP (p)) , where t = {0, 1},

Gsp
t (p) = min
s∈{st}
and dc(si, sj) is the Euclidean distance between the centers
sc
i and sc
i =
(Pi xi/|si|, Pi yi/|si|) where |si| denotes the number of
pixels within si. For consistency across training images, the
guidance maps values are scaled between [0, 255]. When
the user provides no clicks, all pixel values are set to 255.
Examples guidance maps are shown in the second and third
column of Fig. 3 respectively.

3.2. Object based guidance map

Superpixels can be grouped together perceptually into
category-independent object proposals. We also generate
guidance maps from higher-level image structures, speciﬁ-
cally region-based object proposals [3, 29, 37, 43, 47]. Such
proposals have been used in the past as weak supervision

11604

Figure 3. Example of guidance maps. We transform the user-provided positive (shown as green dots) and negative (shown as red dots)
clicks into guidance maps for the instance segmentation network (columns 2 to 5). The second and third column correspond to the positive
and negative superpixel based guidance map respectively. Examples of the object based guidance map and the scale-aware guidance map
are shown in columns 4 and 5 respectively. For the clarity of visualization, we inverted the values of the object-based guidance map and
the scale-aware guidance map (Best viewed in color).

for semantic segmentation [28, 14] and allow us to incorpo-
rate a weak object-related prior to the guidance map, even if
the instance is not explicitly speciﬁed by the user-provided
clicks. To do so, we begin with a set of object propos-
als [43], which have positive clicks its pixel support. For
each pixel in the guidance map, we count the number of
proposals from this set to which the pixel belongs. Pix-
els belonging to same object proposals are more likely to
belong in the same object category and the number of pro-
posals to which pixels belong incorporates a co-occurrence
prior with respect to the current positive clicks.

More formally, let {Lp} be the set of object proposals for
an image with support of pixel location p. The object-based
guidance map can be generated as follows:

Go(p) = X

′∈{p0}

p

X

L∈{Lp′ }

1[p ⊂ L]

(3)

where 1[p ⊂ L] is an indicator function which returns 1
if object proposal L has in its support or contains pixel p.
Similar to the superpixel-base guidance map, the object-
based guidance is also re-scaled to [0, 255]. In the absence
of user-provided clicks, all pixels are set to the value of 0.
Examples are shown in the fourth column of Fig. 3.

3.3. Scale aware guidance

Within an image, object instances can exhibit a large
variation in their spatial extent [46]. While deep CNNs
are known for their ability to handle objects at different

scales [10], specifying the scale explicitly leads to an im-
provement in performance [41].
Interactive instance seg-
mentation methods [36] which isolate the object tend to
have a superior performance. For segmenting object in-
stances, it is thus desirable to construct guidance maps
which exhibit spatial extents consistent with the object.

A common limitation of most click-based interactive ap-
proaches is that the provided guidance is non-informative
about scaling of the intended object instance. The com-
monly used forms of guidance are either too localized [35]
(guidance map values are clipped to 0 at a distance of 20
pixels from the clicks) or non-informative [53].

Suppose now that we have some rough estimate of an ob-
ject’s scale in pixels, either in width or length. A convenient
way to make our guidance maps scale-aware is to incorpo-
rate contributions of superpixels and object proposals which
are in agreement with this scale. More speciﬁcally, we can
apply this to the superpixel guidance map by truncating dis-
tances exceeding some factor f of our scale measure s, i.e.

Gsp-sc

t

(p) = min (cid:2)Gsp

t (p), f s(cid:3) .

(4)

We can apply similar constraints to the object-proposal
based guidance by considering only the proposals within an
accepted size range bounded by tolerance factors f1 and f2:

Go-sc(p) = X

′∈{p0}

p

X

L∈{Lp′ }

1[p ⊂ L]·1[f1 ≤ |L|/s2 ≤ f2].

(5)

11605

3.4. Simulating user interactions

Even when selecting the same object instance, it is un-
likely that different users will provide the same interactions
inputs. For the model to fully capture expected behaviour
across different users, one would need signiﬁcant amounts
of interaction training data. Rather than obtaining these
clicks from actual users for training, we simply simulate
user clicks and generate guidance maps accordingly.

We follow the sampling strategies proposed in [53]. For
each object instance, we sample Npos positive clicks within
the object maintaining a distance din
1 pixels from the ob-
ject boundary and din
2 pixels from each other. For nega-
tive clicks, we test the ﬁrst two of the three sampling strate-
gies outlined in [53], one in which N 1
neg clicks are sampled
randomly from the background, ensuring a distance of dout
pixels away from the object boundary and dout
pixels from
each other and one in which N 2
neg clicks on each of the neg-
ative objects (objects not of interest).

1

2

The above click-sampling strategy helps the network to
understand notions such as negative objects and background
but cannot train the network to identify and correct errors
made during the prediction [35]. To this end, we also ran-
domly sample Niter clicks based on the segmentation er-
rors. After an initial prediction is obtained, positive or neg-
ative clicks are randomly sampled from the error. Existing
set of clicks are then replaced with the newly sampled clicks
with a probability of 0.3. To mimic a typical user’s behav-
ior [35], the error-correction clicks are placed closest to the
center of the largest misclassiﬁed region.

To estimate the scale measure s, we reserve the ﬁrst two
clicks, one positive and one negative, and assume that the
Euclidean distance between the two is a roughly propor-
tional measure; f , f1 and f2 are then set accordingly.

4. Experimental Validation

4.1. Datasets & Evaluation

We apply our proposed guidance maps and evaluate the
resulting instance segmentations on four publicly available
datasets: PASCAL VOC 2012 [16], GrabCut [45], Berke-
ley [38], and MS COCO [33].
PASCAL VOC 2012 consists of 1464 training images and
1449 validation images spread across 20 object classes.
GrabCut consists of 50 images with the corresponding
ground truth segmentation masks and is a used as a com-
mon benchmark for most interactive segmentation methods.
Typically, the images have a very distinct foreground and
background distribution.
Berkeley consists of 100 images with a single foreground
object. The images in this dataset represent the various chal-
lenges encountered in an interactive segmentation setting
such as low contrast between the foreground and the back-
ground, highly textured background etc.

MS COCO is a large-scale image segmentation dataset
with 80 different object categories, 20 of which are from
the Pascal VOC 2012 dataset. For fair comparison with [53,
32], we randomly sample 10 images per category for eval-
uation and splitting the evaluation for the 20 Pascal cate-
gories versus the 60 additional categories.

Evaluation Fully automated instance segmentation is usu-
ally evaluated with mean intersection over union (mIoU)
between the ground truth and predicted segmentation mask.
Interactive instance segmentation is differently evaluated
because a user can always add more positive and negative
clicks to improve the segmentation and thereby increase the
mIoU. As such, the established way of evaluating an in-
teractive system is according to the number of clicks re-
quired for each object instance to achieve a ﬁxed mIoU.
Like [53, 32, 35, 6], we limit the maximum number of clicks
per instance to 20. Note that unlike [53, 32], we do not ap-
ply any post-processing with a conditional random ﬁeld and
directly use the segmentation output from the FCN.

4.2. Implementation Details

Training As our base segmentation network, we adopt the
FCN [34] pre-trained on PASCAL VOC 2012 dataset [16]
as provided by MatConvNet [48]. The output layer is re-
placed with a two-class softmax layer to produce binary
segmentations of the speciﬁed object instance. We ﬁne-
tune the network on the 1464 training images with instance-
level segmentation masks of PASCAL VOC 2012 seg-
mentation dataset [16] together with the 10582 masks of
SBD [20]. We further augment the training samples with
random scaling and ﬂipping operations. We use zero ini-
tialization for the extra channels of the ﬁrst convolutional
layer (conv1 1). Following [53], we ﬁne-tune ﬁrst the
stride-32 FCN variant and then the stride-16 and stride-8
variants. The network is trained to minimize the average bi-
nary cross-entropy loss.For optimization, we use a learning
rate of 0.01 and stochastic gradient descent with Nesterov
momentum with the default value of 0.9 is used.

Click Sampling We generate training images with a va-
riety of click numbers and locations; sometimes, clicks
end up being sampled from the same superpixel, which
reduces training data variation. To prevent this and also
make the network more robust to the click number and lo-
cation for training, we sample randomly from the follow-
ing hyperparameters rather than ﬁxing them to single val-
ues: Npos = {2, 3, 4, 5}, N 1
neg = {3, 5},
1 = {15, 20, 40}, din
din
1 = {15, 40, 60},
dout
2 = {10, 15, 25}. The randomness in the number of
clicks and their relative distances prevents the network from
over-ﬁtting during training.

2 = {7, 10, 20}, dout

neg = {5, 10}, N 2

11606

Guidance Dropout Since the FCNs are pre-trained on
PASCAL VOC 2012, we expect the network to return a
good initial prediction for images with object instances
from one of its 20 classes. Thus, during training, when
the network receives images without any instance ambigu-
ity (i.e. an image with single object), we zero the guidance
maps (value of 0 for object guidance map and 255 for the
superpixel based guidance map) with a probability of 0.2 to
encourage good segmentations without any guidance. We
further increase robustness by resetting the positive or neg-
ative superpixel-based guidance with a probability of 0.4.

Interaction Loop During evaluation, a user provides pos-
itive and negative clicks sequentially to segment the object
of interest. After each click is added, the guidance maps
are recomputed; in addition the a distance transform of pre-
dicted mask from the previous iteration is provided as an
extra channel [35]. The newly generated guidance map is
concatenated with the image and given as input to the FCN-
8s network which produces an updated segmentation map.

Superpixels & Object Proposals We use the implemen-
tation provided in [43] for generating superpixels; on aver-
age, each frame has 500 − 1000 superpixels. For compari-
son, we also try other superpixelling variants e.g. SLIC [1]
and CTF [54]. Although several object proposal algorithms
exist [47, 8, 43], we use only MCG [43] as it has been shown
to have higher quality proposals [14]. The ﬁnal stage of
MCG returns a ranking which we disregard. We use the
pre-computed object proposals for PASCAL VOC 2012 and
MS COCO provided by the authors of [43]. For GrabCut
and Berkeley, we run MCG [43] on the ‘accurate’ setting to
obtain our set of object proposals.

4.3. Impact of Structure Based Guidance

We begin by looking at the impact of superpixel based
guidance. As a baseline, we compare with [53], which
uses a standard Euclidean distance-based guidance as given
in Eq. 1 (see examples in second row of Fig. 1). Sim-
ilar to [53], we concatenate our positive and negative
superpixel-based guidance maps with the three color chan-
nels and feed it as an input to the FCN-8s [34]. We use the
superpixels computed using MCG [43]. For a fair compari-
son, we train our network non-iteratively, i.e., during train-
ing, we do not generate click samples based on the error in
the prediction and do not append the distance transform of
the current predicted mask as an extra channel. Looking at
Table. 1, we see that our superpixel based guidance maps
signiﬁcantly reduce the number of clicks required to reach
the standard mIoU threshold.

The object-based guidance provides the network with a
weak localization prior of the object of interest. adding the
object-based guidance with the superpixel based guidance

leads to further improvements in performance (see third row
of Table. 1). The impact is more prominent for datasets with
a single distinct foreground object (e.g. 9.3% and 14% rel-
ative improvement for the Berkeley and GrabCut dataset).
Finally, by making the feedback iterative, i.e. based on pre-
vious segmentation errors, we can further reduce the num-
ber of clicks. Overall, our structure-based guidance maps
can reduce the number of clicks by 35% to 47% and un-
equivocally proves that having structural information in the
guidance map is highly beneﬁcial.

GrabCut Berkeley VOC 2012
@90%

@85%

@90%

Euclidean ([53])

SP

SP + Obj.

SP + Obj. + Iter

6.04
4.44
3.82

3.58

8.65
6.67
6.05

5.60

6.88
4.23
4.02

3.62

Table 1. Clicks required for different types of guidance. Guid-
ance maps leveraging structural information require signiﬁcantly
less clicks than Euclidean distance-based guidance. SP refers to
the superpixel guidance maps and Obj refers to the obect based
guidance map and Iter refers to iterative training.

4.4. Impact of Scale Aware Guidance

Due to ﬁxed-size receptive ﬁeld, FCNs experience difﬁ-
culty when segmenting small objects [40]. The beneﬁts of
our scale-aware guidance map is most pronounced for seg-
menting small objects; for large objects (¿ 32×32 pixels), it
does not seem to much effect. To highlight the impact of our
guidance on small object instances, we pick the subset of
621 objects (from PASCAL VOC 2012) which are smaller
than 32 × 32; objects smaller than this size are harder to
identify [46].
In the scale agnostic setting, we consider all object pro-
posals which has the click in its pixel support for generat-
ing the object-based guidance map, i.e. (as shown in Equa-
tion. 3; note that this is equivalent to having f1 = 0, f2 =
∞). Since the lower bound on scale has little effect, we
set f1 = 0. Looking at the average number of clicks re-
quired per instance to reach 85% mIoU for the subset of
small objects (see Fig. 4 (a)), we ﬁnd that having a soft
scale estimate improves the network performance when it
comes to segmenting smaller objects. This is primarily be-
cause the guidance map disregards object proposals which
are not consistent in scale and can degrade the network per-
formance by inducing a misleading co-occurrence prior.

When the scale s is based on ground truth (as the square
root of the number of pixels in the mask foreground, see
black curve in Fig. 4 (a)), the average clicks required per
instance is consistently lower than the scale-agnostic case,
even when as we relax f2 up to 6, i.e. allowing for object

11607

s
k
c
i
l
c

f
o
r
e
b
m
u
n

.

g
v
A

4.5

4.4

4.3

4.2

4.1

Estimated scale

Ground truth scale

Scale agnostic (f2 = +∞,f1 = 0)

2

3

6

f2

1
.
1

2
.
1

5
.
1

7

6.5

6

5.5

5

4.5

s
k
c
i
l
c

f
o
r
e
b
m
u
n

.

g
v
A

5
.
0

1

2

5

#superpixels (in thousands)

iFCN

0
1

(a) Scale-Aware Guidance

(b) Number of superpixels

Figure 4. (a) Scale-Aware Guidance. The ﬁgure shows the average number of clicks required for segmenting small object instances
(smaller than 32 × 32 pixels [46]) for varying degrees of tolerance till which we accept object proposals for generating our guidance map
based on our estimated object scale and the ground truth object scale (computed as the square root of the number of pixels in the object
mask). (b) Number of superpixels. The ﬁgure shows the average number of clicks required for segmenting object instances in PASCAL
VOC 12 val set for different number of superpixels.

proposals which are 6 times larger than the actual object
scale. Estimating the scale from the clicks is of course
much less accurate than when it is take from the ground
truth masks (compare black curve vs blue curve in Fig. 4
(a)). Nevertheless, even with such a coarse estimate, we
ﬁnd improvements in the number of clicks required as com-
pared to the scale-agnostic scenario (compare red dashed
line in Fig. 4 (a)). Given the ﬁrst pair of positive and neg-
ative clicks, our estimated object scale is √πd where d is
the euclidean distance between the positive and the nega-
tive click.
In our experiments, we observed that our es-
timated scale varies between 50 − 300% from the ground
truth scale). In comparison to a scale-agnostic setting, over
the PASCAL VOC 2012 val set, we observe an improve-
ment of 0.1 clicks on the small objects subset and an im-
provement of 0.032 clicks per instance for objects larger
than 32 × 32 pixels.
Segmenting small objects with CNNs can be problem-
atic [40]; we observed similar difﬁculties in preliminary ex-
periments. For objects smaller than 32 × 32 pixels from
PASCAL VOC 2012 val set, we require an average of 4.33
clicks which is signiﬁcantly higher than our dataset average
of 3.62 clicks.

4.5. Superpixels

Type of Superpixels To study the impact of the super-
pixeling algorithm, we consider the two variants SLIC [1]
and CTF [54] and use only the superpixel based guidance
map. On an average, MCG [43] generates 500 − 1000 su-
perpixels for each image in its default setting. For a fair
comparison, we generate 500 and 1000 superpixels using
SLIC and CTF. We observe that using 1000 SLIC superpix-

els results in performance similar to the MCG. However, ir-
respective of the superpixeling method, we found an overall
improvement when the guidance maps are generated based
on superpixels instead of pixel-based distances.

#superpixels

SLIC [1] CTF [54] MCG [43]

500
1000

4.45
4.29

4.82
4.58

4.23

Table 2. Choice of superpixel algorithm

Number of Superpixels For this study, we consider only
the superpixel-based map as guidance and use SLIC [1] as
the superpixel algorithm. In the extreme case, all superpix-
els will have one pixel in its support and the guidance map
degenerates to the Euclidean distance transform commonly
used in existing interactive methods [53, 31]. We use the
reported results in iFCN [53] on PASCAL VOC 2012 val
set as our degenerate case (as shown by the red curve in
Fig. 4 (b)). In addition to the reported results for 500 and
1000 superpixels on PASCAL VOC 2012 val set (as shown
in Table 4 of the paper), we generate 2000, 5000 and 10000
superpixels using SLIC [1]. We notice an initial gain in per-
formance, but with increase in the number of superpixels,
the performance drops as our network requires more and
more clicks to segment the object of interest. As the num-
ber of superpixels increase, the beneﬁts of local structure
based grouping is lost as each superpixel is segmented into
similar and redundant superpixels.

11608

Method

Base
Network

GrabCut Berkeley
@90%
@90%

PascalVOC12 MS-COCO MS-COCO
seen@85% unseen@85%

@85%

FCN-8s [34]
iFCN [53]
DeepLab-LargeFOV [10]
RIS-Net [32]
DeepLabV3+ [12]
ITIS [35]
DEXTR [36]
DeepLabV2 [11]
VOS-Wild [6] ResNet-101 [24]
FCTSFN [26] Custom
IIS-LD [31]
Ours

CAN [55]
FCN-8s [34]

6.04
5.00
5.60
4.00
3.80
3.76
4.79
3.58

8.65
6.03

-
-
-

6.49

-

5.60

6.88
5.12
3.80
4.00
5.60
4.58

-

3.62

8.31
5.98

-
-
-

9.62
12.45
5.40

7.82
6.44

-
-
-

9.62
12.45
6.10

Table 3. The average number of clicks required to achieve a particular mIoU. The best results are indicated in bold.

4.6. Comparison to State of the Art

We compare the average number of clicks required to
reach some required mIoU (see Table 3) against other meth-
ods reported in the literature. The methods vary in the base
segmentation network from the basic FCNs to the highly
sophisticated DeepLabV3 and also make use of additional
CRF post-processing. We achieve the lowest number of
clicks required for all datasets across the board, again prov-
ing the beneﬁts of applying guidance maps based on exist-
ing image structures. We report results for our best trained
SP+Obj+Iter network. To reach the mIoU threshold of 90%
on GrabCut and Berkeley, our full model needs the fewest
number of clicks as shown in Table 3 with a relative im-
provement of 5.79% and 7.13% over the current bench-
mark. For PASCAL VOC 2012 val set, we observe a rel-
ative improvement of 4.7%. For MS COCO, we observe a
larger improvement for the 20 seen categories from PAS-
CAL VOC 2012, as our networks were trained heavily on
these object categories. Overall, we achieve an improve-
ment of 9.7% and 5.28% over the 20 seen and 60 unseen
object categories. We note that such an improvement is
achieved despite the fact that our base network is the most
primitive of the methods compared, i.e. an FCN-8s, in com-
parison to the others who use much deeper (ResNet-101)
and more complex (DeepLabV3) network architectures. It
should be noted that FCTSFN [26] and IIS-LD [31] report
their result over all the 80 classes of MS COCO and not
separately for 20 seen and 60 unseen classes.

We also compare our approach to that of [9]. [9] targets
images with only a single foreground objects. To be com-
parable, we consider only our results with a single positive
(foreground) click. We ﬁnd that for the GrabCut and Berke-
ley dataset, our mIoU is higher by 4% and 8% respectively.

5. Discussion & Conclusion

In this work, we investigated the impact of the guid-
ance maps for interactive object segmentation. Conven-
tional methods use distance transform based approaches for
generating guidance maps which disregard the inherent im-

age structure. We proposed a scale aware guidance map
generated using hierarchical image information which leads
to signiﬁcant reduction in the average number of clicks re-
quired to obtain a desirable object mask.

During experimentation, we observed that the object in-
stances within the datasets varied greatly in difﬁculty. For
instance, on PASCAL VOC 2012, the base network, with-
out any user guidance, is able to meet the ≥ 85% mIoU
criteria for 433 of the 697 instances. Similarly observations
were made for GrabCut (≥ 90% mIoU, 13 out of 50) and
Berkeley (≥ 90% mIoU, 15 out of 100). On the other hand,
we encountered instances where our algorithm repeatedly
exhausted the 20 click budget regardless of sampled click
locations and iterative feedback based on prediction errors.
This is especially true for objects with very ﬁne detailing,
such as such as spokes in bicycle wheels, partially occluded
chairs, etc. Based on these two extreme cases, we conclude
that interactive segmentation is perhaps not so relevant for
single object instances featuring prominently at the center
of the scene and should feature more challenging scenar-
ios. On the other hand, we need to design better algorithms
which can handle objects that are not contiguous in region,
i.e. has holes and are able to handle scenarios of occlusion.
Depending on the target application, dedicated base archi-
tectures may be necessary to efﬁciently handle these cases.

Acknowledgement Research in this paper was partly
supported by the Singapore Ministry of Education Aca-
demic Research Fund Tier 1.

References

[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien
Lucchi, Pascal Fua, Sabine S¨usstrunk, et al. SLIC superpix-
els compared to state-of-the-art superpixel methods. TPAMI,
34(11):2274–2282, 2012.

[2] Mykhaylo Andriluka, Jasper RR Uijlings, and Vittorio Fer-
rari. Fluid annotation: A human-machine collaboration in-
terface for full image annotation. In 2018 ACM Multimedia
Conference on Multimedia Conference, pages 1957–1966.
ACM, 2018.

11609

[3] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-
tendra Malik. Contour detection and hierarchical image seg-
mentation. TPAMI, 33(5):898–916, 2011.

[22] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Ji-
tendra Malik. Hypercolumns for object segmentation and
ﬁne-grained localization. In CVPR, 2015.

[4] Min Bai and Raquel Urtasun. Deep watershed transform for

[23] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

instance segmentation. In CVPR, 2017.

shick. Mask r-cnn. In ICCV, 2017.

[5] Xue Bai and Guillermo Sapiro. Geodesic matting: A frame-
work for fast interactive image and video segmentation and
matting. IJCV, 82(2):113–132, 2009.

[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[6] Arnaud Benard and Michael Gygli. Interactive video object
segmentation in the wild. arXiv preprint:1801.00269, 2017.
[7] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for op-
timal boundary & region segmentation of objects in nd im-
ages. In ICCV, 2001.

[8] Joao Carreira and Cristian Sminchisescu. Cpmc: Automatic
object segmentation using constrained parametric min-cuts.
TPAMI, (7):1312–1328, 2011.

[9] Ding-Jie Chen, Jui-Ting Chien, Hwann-Tzong Chen, and
In AAAI,

Long-Wen Chang. Tap and shoot segmentation.
2018.

[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. TPAMI, 40(4):834–848, 2018.
[11] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. TPAMI, 40(4):834–848, 2018.
[12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV, 2018.

[13] Antonio Criminisi, Toby Sharp, and Andrew Blake. Geos:

Geodesic image segmentation. In ECCV, 2008.

[14] Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploit-
ing bounding boxes to supervise convolutional networks for
semantic segmentation. In ICCV, 2015.

[15] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In

mantic segmentation via multi-task network cascades.
CVPR, 2016.

[16] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV, 88(2):303–338, 2010.

[17] Alon Faktor and Michal Irani. Video segmentation by non-

local consensus voting. In BMVC, 2014.

[18] Leo Grady, Thomas Schiwietz, Shmuel Aharon, and R¨udiger
Westermann. Random walks for interactive organ segmenta-
tion in two and three dimensions: Implementation and vali-
dation. In MICCAI, 2005.

[19] Varun Gulshan, Carsten Rother, Antonio Criminisi, Andrew
Blake, and Andrew Zisserman. Geodesic star convexity for
interactive image segmentation. In CVPR, 2010.

[20] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In ICCV, 2011.

[21] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Ji-
tendra Malik. Simultaneous detection and segmentation. In
ECCV, 2014.

[25] Xuming He, Richard S Zemel, and Debajyoti Ray. Learning
and incorporating top-down cues in image segmentation. In
ECCV, 2006.

[26] Yang Hu, Andrea Soltoggio, Russell Lock, and Steve Carter.
A fully convolutional two-stream fusion network for interac-
tive image segmentation. Neural Networks, 2018.

[27] Michael Kass, Andrew Witkin, and Demetri Terzopoulos.
Snakes: Active contour models. IJCV, 1(4):321–331, 1988.
[28] Anna Khoreva, Rodrigo Benenson, Jan Hendrik Hosang,
Matthias Hein, and Bernt Schiele. Simple does it: Weakly
supervised instance and semantic segmentation.
In CVPR,
2017.

[29] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Geodesic object

proposals. In ECCV, 2014.

[30] Yin Li, Jian Sun, Chi-Keung Tang, and Heung-Yeung Shum.
In ACM Transactions on Graphics (ToG),

Lazy snapping.
volume 23, pages 303–308. ACM, 2004.

[31] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive
In CVPR, pages

image segmentation with latent diversity.
577–585, 2018.

[32] JunHao Liew, Yunchao Wei, Wei Xiong, Sim-Heng Ong, and
Jiashi Feng. Regional interactive image segmentation net-
works. In ICCV, 2017.

[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV, 2014.

[34] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015.

[35] Sabarinath Mahadevan, Paul Voigtlaender, and Bastian
In

Iteratively trained interactive segmentation.

Leibe.
BMVC, 2018.

[36] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and
Luc Van Gool. Deep extreme cut: From extreme points to
object segmentation. In CVPR, 2018.

[37] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbel´aez,
and Luc Van Gool. Convolutional oriented boundaries. In
ECCV, 2016.

[38] Kevin McGuinness and Noel E Oconnor. A comparative
evaluation of interactive segmentation algorithms. Pattern
Recognition, 43(2):434–444, 2010.

[39] Eric N Mortensen and William A Barrett. Intelligent scissors

for image composition. In SIGGRAPH. ACM, 1995.

[40] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In ICCV, 2015.

[41] George Papandreou, Iasonas Kokkinos, and Pierre-Andr´e
Savalle. Modeling local and global deformations in deep

11610

learning: Epitomic convolution, multiple instance learning,
and sliding window detection. In CVPR, 2015.

[42] Anestis Papazoglou and Vittorio Ferrari. Fast object segmen-

tation in unconstrained video. In ICCV, 2013.

[43] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Fer-
ran Marques, and Jitendra Malik. Multiscale combinatorial
grouping for image segmentation and object proposal gener-
ation. TPAMI, 39(1):128–140, 2017.

[44] Brian L Price, Bryan Morse, and Scott Cohen. Geodesic
In CVPR.

graph cut for interactive image segmentation.
IEEE, 2010.

[45] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.
Interactive foreground extraction using iterated
In ACM transactions on graphics (TOG), vol-

Grabcut:
graph cuts.
ume 23, pages 309–314. ACM, 2004.

[46] Bharat Singh and Larry S Davis. An analysis of scale invari-

ance in object detection–snip. In CVPR, 2018.

[47] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-
ers, and Arnold WM Smeulders. Selective search for object
recognition. IJCV, 104(2):154–171, 2013.

[48] Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional

neural networks for matlab. In MM. ACM, 2015.

[49] Vladimir Vezhnevets and Vadim Konouchine. Growcut: In-
teractive multi-label nd image segmentation by cellular au-
tomata. In Graphicon, 2005.

[50] Guotai Wang, Wenqi Li, Maria A Zuluaga, Rosalind Pratt,
Premal A Patel, Michael Aertsen, Tom Doel, Anna L David,
Jan Deprest, S´ebastien Ourselin, et al.
Interactive medical
image segmentation using deep learning with image-speciﬁc
ﬁne-tuning. IEEE Transactions on Medical Imaging, 2018.

[51] Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt,
Premal A Patel, Michael Aertsen, Tom Doel, Anna L Divid,
Jan Deprest, S´ebastien Ourselin, et al. Deepigeos: a deep
interactive geodesic framework for medical image segmen-
tation. TPAMI, 2018.

[52] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas
Huang. Deep grabcut for object selection. In BMVC, 2017.
[53] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and
In

Thomas S Huang. Deep interactive object selection.
CVPR, 2016.

[54] Jian Yao, Marko Boben, Sanja Fidler, and Raquel Urtasun.
Real-time coarse-to-ﬁne topologically preserving segmenta-
tion. In CVPR, 2015.

[55] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-

tion by dilated convolutions. In ICLR, 2016.

11611

