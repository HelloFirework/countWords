Data-Driven Neuron Allocation for Scale Aggregation Networks

Yi Li

Zhanghui Kuang

Yimin Chen

Wayne Zhang

{liyi,kuangzhanghui,chenyimin,wayne.zhang}@sensetime.com

SenseTime

Abstract

Successful visual recognition networks beneﬁt from ag-
gregating information spanning from a wide range of
scales. Previous research has investigated information fu-
sion of connected layers or multiple branches in a block,
seeking to strengthen the power of multi-scale representa-
tions. Despite their great successes, existing practices often
allocate the neurons for each scale manually, and keep the
same ratio in all aggregation blocks of an entire network,
rendering suboptimal performance. In this paper, we pro-
pose to learn the neuron allocation for aggregating multi-
scale information in different building blocks of a deep net-
work. The most informative output neurons in each block
are preserved while others are discarded, and thus neurons
for multiple scales are competitively and adaptively allo-
cated. Our scale aggregation network (ScaleNet) is con-
structed by repeating a scale aggregation (SA) block that
concatenates feature maps at a wide range of scales. Fea-
ture maps for each scale are generated by a stack of down-
sampling, convolution and upsampling operations. The
data-driven neuron allocation and SA block achieve strong
representational power at the cost of considerably low com-
putational complexity. The proposed ScaleNet, by replacing
all 3×3 convolutions in ResNet with our SA blocks, achieves
better performance than ResNet and its outstanding vari-
ants like ResNeXt and SE-ResNet, in the same computa-
tional complexity. On ImageNet classiﬁcation, ScaleNets
absolutely reduce the top-1 error rate of ResNets by 1.12
(101 layers) and 1.82 (50 layers). On COCO object detec-
tion, ScaleNets absolutely improve the mAP with backbone
of ResNets by 3.6 and 4.6 on Faster-RCNN, respectively.
Code and models are released on https://github.com/Eli-
YiLi/ScaleNet.

1. Introduction

Deep convolutional neural networks (CNNs) have been
successfully applied to a wide range of computer vision
tasks, such as image classiﬁcation [18], object detec-
tion [25], and semantic segmentation [22], due to their

Figure 1: Illustration of the data-driven neuron allocation for the
scale aggregation (SA) block. The proportion of output neurons
(or channels) of different scales in an SA block is learned, and
thus adaptively changes across layers in a network.

powerful end-to-end learnable representations. From bot-
tom to top, the layers of CNNs have larger receptive ﬁelds
with coarser scales, and their corresponding representations
become more semantic. Aggregating context information
from multiple scales has been proved to be effective for im-
proving accuracy [32, 9, 13, 20, 6, 3, 16, 28, 4]. Small
scale representations encode local structures such as tex-
tures, corners and edges, and are useful for localization,
while coarse scale representations encode global contexts
such as object categories, object interaction and scene, and
thus clarify local confusion.

There exist many previous attempts to fuse multi-scale
representations by designing network architecture. They
aggregate multi-scale representations of connected layers
with different depths [32, 9, 13, 20, 6, 3, 16, 12, 26] or
multiple branches in a block with different convolutional
kernel sizes [28, 4]. The proportion of multi-scale rep-
resentations for in each aggregation block is manually set
in a trial-and-error process and kept the same in the entire
network. Ideally, the most efﬁcient architecture design of
multi-scale information aggregation is adaptive. The pro-

11526

c 1c 2c 30%20%40%60%80%100%12345678910111213141516scale 1scale 2scale 3scale 4feature mapsc 3c 2c 11: downsample2: convolution3: upsample4: concatenate8thSA BlockblockNeuron proportion in each SA blockFigure 2: Comparison of the ScaleNets and modern architectures’ top-1 error rates (single-crop testing) on the ImageNet validation
dataset (left) and mAP on MS COCO mini-validation set (right) as a function of FLOPs during testing. ScaleNet-50-light indicates a
light ScaleNet which is also constructed from ResNet-50. Architectures are given in the supplementary material.

portion of neurons for each scale is determinate according
to the importance of the scale in gathering context. Such
proportion should also be adaptive to the stage in the net-
work. Bottom layers may prefer ﬁne scales and top layers
may prefer coarse scales.

In this paper, we propose a novel data-driven neuron
allocation method for multi-scale aggregation, which au-
tomatically learns the neuron proportion for each scale in
all aggregation blocks of one network. We model the neu-
ron allocation as one network optimization problem under
FLOPs constraints which is solved by SGD and back pro-
jection. Concretely, we train one seed network with abun-
dant output neurons for all scales using SGD, and then
project the trained network into one feasible network that
meets the constraints by selecting the top most informative
output neurons amongst all scales. In this way, the neuron
allocation for multi-scale representations is learnable and
tailored for the network architecture.

To effectively extract and utilize multi-scale information,
we present a simple yet effective Scale Aggregation (SA)
block to strengthen the multi-scale representational power
of CNNs. Instead of generating multi-scale representations
with connected layers of different depths or multi-branch
different kernel sizes as done in [28, 9, 32, 9, 13, 20, 6,
3, 16], an SA block explicitly downsamples the input fea-
ture maps with a group of factors to small sizes, and then
independently conducts convolution, resulting in represen-
tations in different scales. Finally the SA block upsam-
ples the multi-scale representations back to the same res-
olution as that of the input feature maps and concatenate
them in channel dimension together. We use SA blocks to
replace all 3×3 convolutions in ResNets to form ScaleNets.
Thanks to downsampling in each SA block, ScaleNets are

very efﬁcient by decreasing the sampling density in the
spatial domain, which is independent yet complementary
to network acceleration approaches in the channel domain.
Thanks to the downsampling operation, the proposed SA
block is more computationally efﬁcient and can capture a
larger scale (or receptive ﬁeld) range as shown in Figure 6,
compared with previous multi-scale architecture.

We apply the proposed technique of data-driven neuron
allocation to the SA block to form a learnable SA block. To
demonstrate the effectiveness of the learnable SA block, we
use learnable SA blocks to replace all 3 × 3 convolutions in
ResNet to form a novel architecture called ScaleNet. The
proposed ScaleNet outperforms ResNet and its outstand-
ing variants such as ResNeXt [31] and SE-ResNet [11], as
well as recent popular architectures such as DenseNet [13],
with impressive margins on image classiﬁcation and ob-
ject detection while keeping the same computational com-
plexity as shown in Figure 2. Speciﬁcally, ScaleNet-50
and ScaleNet-101 absolutely reduces the top-1 error rate of
ResNet-101 and ResNet-50 by 1.12% and 1.82% on Ima-
geNet respectively. Beneﬁting from the strong multi-scale
representation power of learnable SA blocks, ScaleNets
are considerably effective on object detection. The Faster
RCNN [25] with backbone ScaleNet-101 and ScaleNet-50
absolutely improve the mAP of those with ResNet-101 and
ResNet-50 by 3.6 and 4.6 on MS COCO.

2. Related Work

Multi-scale representation aggregation has been studied
for a long time. It can be categorized into shortcut connec-
tion approaches and multi-branch approaches.

Shortcut connection approaches. Connected layers

11527

24681012# FLOPs (billion)20.521.021.522.022.523.023.524.024.525.0ImageNet top1 err (%)50101152169201161501011525010150-light50101152ResNetsDenseNetsSE-ResNetsResNeXtsScaleNets468101214# backbone FLOPs (billion)303234363840mAP@IoU=0.50:0.95scaleNet-50ResNet-50scaleNet-101ResNet-101ResNeXt-101ResNet-152SE-ResNet-152DPN-98scaleNet-50ResNet-50scaleNet-101ResNet-101Faster-RCNN 600*1000Faster-RCNN 800*1200with different depths usually have different receptive ﬁelds,
and thus multi-scale representations. Shortcut connections
between layers not only maximize information ﬂow to avoid
vanishing gradient, but also strengthen multi-scale repre-
sentation power of CNNs. ResNet [9], DenseNet [13],
and Highway Network [27] fuse multi-scale information
by identity shortcut connections or gating function based
ones. Deep layer aggregation [32] further extends short-
cut connection with trees that cross stages.
In object de-
tection, FPN [20] fuses coarse scale representations to ﬁne
scale ones from top to down in one detector’s header [20].
ASIF [4] merges multi-scale representations from 4 layers
both from top to down and from down to top. HyperNet [16]
and ION [3] concatenate multi-scale features from different
layers to make prediction. All the shortcut connection ap-
proaches focus on reusing ﬁne scale representations from
preceding layers or coarse scale ones from subsequent lay-
ers. Due to limited connection patterns between layers, the
scale (or receptive ﬁled) range is limited. Instead, the pro-
posed approach generates a wide range scale of represen-
tations with a group of downsampling factors itself in each
SA block. Therefore, it is a general and standard module
which can replace any convolutional layer of existing net-
works, and be effectively used in various tasks such as im-
age classiﬁcation and object detection as validated in our
experiments.

Multi-branch approaches. The most inﬂuential multi-
branch network is GoogleNet [28], where each branch is de-
signed with different depths and convolutional kernel sizes.
Its branches have varied receptive ﬁelds and multi-scale
representations. Similar multi-branch network is designed
for crowd counting in [4]. Different from previous multi-
branch approaches, the proposed SA block generates multi-
scale representations by downsampling the input feature
maps by different factors to expand the scale of represen-
tations. Again, it can generate representations with wider
scale range than [28, 4]. Downsampling is also used in the
context module of PSPNet [34] and ParseNet [24]. How-
ever, the context module is only used in the network header
while the proposed SA block is used in the whole backbone
and thus more general. Moreover, the neuron proportion
for each scale is manually set and ﬁxed in the context mod-
ule while automatically learned and different from one SA
block to another in one network.

Our data-driven neuron allocation method is also related
to network pruning methods [8, 2, 30, 19] or network archi-
tecture search methods [35, 29]. However, our data-driven
neuron allocation method targets at multi-scale representa-
tion aggregation but not the whole architecture design. It
learns the neuron proportion for scales in each SA block
separately.
In this way, the neuron allocation problem is
greatly simpliﬁed, and easily optimized.

1 x 1 conv

3 x 3 conv

1 x 1 conv

+

1 x 1 conv

max pool  

max pool  

3 x 3 conv

3 x 3 conv

...

3 x 3 conv

upsample 

upsample 

concatenate

1 x 1 conv

+

Figure 3: Illustration of the SA block. The left shows the original
residual block, and the right shows the module after replacing the
3 × 3 convolution by the SA block.

3. ScaleNets

3.1. Scale Aggregation Block

The proposed scale aggregation block is a standard com-
putational module which is constructed for any given trans-
formation Y = T(X), where X ∈ RH×W ×C , Y ∈
RH×W ×Co with C and Co being the input and output chan-
nel number respectively. T is any operator such as a con-
volution layer or a series of convolution layers. Assume
we have L scales. Each scale l is generated by sequentially
conducting a downsampling Dl, a transformation Tl and an
unsampling operator Ul:

X

′

l = Dl(X),

Y

′

l = Tl(X

′

l),

Yl = Ul(Y

′

l),

(1)

(2)

(3)

′

l ∈ RHl×Wl×C , Y

l ∈ RHl×Wl×Cl , and Yl ∈
where X
RH×W ×Cl . Substitute Equation (1) and (2) into Equa-
tion (3), and concatenate all L scales together, getting

′

′

Y

= kL
1

Ul(Tl(Dl(X))),

(4)

where k indicates concatenating feature maps along the
1 Cl is the ﬁnal
channel dimension, and Y
output feature maps of the scale aggregation block.

∈ RH×W ×PL

′

In our implementation, the downsampling Dl with factor
s is implemented by a max pool layer with s × s kernel size
and s stride. The upsampling Ul is implemented by resizing
with the nearest neighbor interpolation.

3.2. Data Driven Neuron Allocation

There exist L scales in each SA block. Different scales
should play different roles in blocks with different depths.
Therefore, simply allocating the output neuron proportion
of scales equally would lead to suboptimal performance.

11528

Our core idea is to identify the importance of each output
neuron, and then prune the unimportant neurons while pre-
serving the important ones. We employ the scale weights
(γ in the paper) of BatchNorm[15] layers of each chan-
nel to evaluate its importance.
Its underlying reason is
that γ restores the original response after normalization, so
the weights are positively correlated with the feature con-
ﬁdence. These neurons with lower weights mean they can
not extract credible features.

Let K, Ok (1 ≤ k ≤ K), and Okl (1 ≤ k ≤ K and
1 ≤ l ≤ L) denote the total SA block index of the target
network, the computational complexity budget of the kth
SA block, and the computational complexity of one output
neuron at scale l in the kth block respectively. We target at
optimally allocating neurons for each scale in the SA block
k with the budget Ok. Formally, we optimize

min

θ

F (θ), s.t. ∀k, X

1≤n≤Nk

Okl(θkn) ≤ Ok,

(5)

where F (θ) is the loss function of the whole network with
θ being the learnable weights of the network, and θkn be-
ing the weight of nth output neuron in the kth SA block.
l(·) indicates the scale index, and Nk is the total number of
output neurons in the kth SA block.

We optimize the objective function 5 by SGD with pro-
jection. We ﬁrst optimize F (θ), getting θt, we then project
θt back to the feasible domain deﬁned by constraints for
each SA block k by optimizing

θ X
min

n

(cid:12)(cid:12)

V(θkn) − V(θt

kn)(cid:12)(cid:12)

s.t. X

1≤n≤Nk

Okl(θkn) ≤ Ok,

(6)

where V(θkn) indicates the importance of the nth neuron
in the SA block k. It is deﬁned to be the scale weight in
BatchNorm layer for the target channel (k, n)’s output as
done in [8]. The more important and of less computational
complexity the neuron is, the more likely it should be pre-
served. Equation (6) is greedily solved by selecting neu-
rons with top biggest V(θkn)/Ob
kl(θkn) in each SA block
k. Note that b is an exponential balance factor of compu-
tational complexity. b is set to 0 to avoid too much hyper
parameter adjustment.

Algorithm 1 lists the procedure of neuron allocation.
First, we set a seed network by setting C neurons for each
scale (i.e., Nk = CL). Second, we train the seed network
till convergence. Third, we select the top most important
neurons in SA blocks by solving Equation( 6), and get a new
network. Finally, we retrain the new network from scratch.

3.3. Instantiations

Algorithm 1 Data-driven neuron allocation
Initialize a seed network by setting Nk = LC
Train the seed network till convergence
for k = 1 : K do

for n = 1 : Nk do

Compute pkn = V(θkn)/Ob

kl(θkn)

end
Select neurons with top biggest pkn under the constraint
of Equation( 6)

end
Retrain the new network till convergence.

layers or modules. To illustrate this point, we develop
ScaleNets by incorporating SA blocks into the recent popu-
lar ResNets [9].

In ResNets [9], 3 × 3 convolutions account for most of
the whole network computational complexity. Therefore,
we replace all 3 × 3 layers with SA blocks as shown in
Figure 3. We replace the stride in 3 × 3 convolution by
extra max pool layer as done in DenseNets [13]. In this way,
all 3 × 3 layers can be replaced by SA blocks consistently.
As shown in Table 1, using ResNet-50, ResNet-101, and
ResNet-152 as the start points, we obtain the corresponding
ScaleNets1 by setting the computational complexity budget
of each SA block to that of its corresponding 3 × 3 conv in
the residual block during the neuron allocation procedure.

3.4. Computational Complexity

The proposed SA block is of practical use.

SA block to C (i.e., PL
9C(PL

It makes
ScaleNets efﬁcient, because the feature maps are smaller.
Theoretically, if we set the output channel number of one
1 Cl = C), the saved FLOPs is
1 HlWlCl)−9C(HW C), Take ScaleNet-50-light as
an example, it reduces FLOPs of its start point ResNet-50
by 29% while absolutely improving the single-crop top-1
accuracy by 0.98 on ImageNet and performs better than the
state-of-art pruning methods shown in Table 2.

We evaluate the running time with Tensorﬂow on a
GTX1060 GPU and i7 CPU. The image is resize to 224 ×
224 and batch size is 16. The inference time of ScaleNet-50,
ResNet-50, SE-ResNet-50, ResNeXt-50 are 93ms, 95ms,
98ms, 147ms respectively, which demonstrate the superior-
ity of our proposed architecture.

3.5. Implementation

Our implementation for ImageNet follows the practice
in [9, 11, 28]. We perform standard data augmentation with
random cropping, random horizontal ﬂipping and photo-
metric distortions [28] during training. All input images are

The proposed SA block can be integrated into stan-
dard architectures by replacing its existing convolutional

1Note that the number indicates the layer number of their start points

but not ScaleNets.

11529

output size
112×112

56×56

56×56

28×28

28×28

14×14

14×14

7 × 7

7 × 7

1 × 1

ScaleNet-50

ScaleNet-101

7 × 7 conv, stride 2

ScaleNet-152

















1 × 1 conv, 64

D[1,2,4,7]

3 × 3 conv[Cl,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 256

1 × 1 conv, 128

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 512

1 × 1 conv, 256

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 1024

1 × 1 conv, 512

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 2048





×3













×4

×6

×3

















3 × 3 max pool, stride 2


1 × 1 conv, 64

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 256

2 × 2 max pool, stride 2


1 × 1 conv, 128

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 512

2 × 2 max pool, stride 2


1 × 1 conv, 256

D[1,2,4,7]

×3

×4







3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

×23

1 × 1 conv, 1024
2 × 2 max pool, stride 2


1 × 1 conv, 512

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 2048

×3



avg pool, 1000-d fc, softmax

















1 × 1 conv, 64

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 256

1 × 1 conv, 128

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 512

1 × 1 conv, 256

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 1024

1 × 1 conv, 512

D[1,2,4,7]

3 × 3 conv[C1,C2,C3,C4]

U[1,2,4,7]

1 × 1 conv, 2048









×3

×8





×36





×3

Table 1: Architectures of ScaleNets. D[1,2,4,7] indicates 1 × 1, 2 × 2, 4 × 4, and 7 × 7 downsampling layers. U[1,2,4,7] indicates 1 × 1,
2 × 2, 4 × 4, and 7 × 7 upsampling layers. We select 7 × 7 (but not 8×8) downsampling and upsampling layers since the spatial resolution
of last stage of networks is 7 × 7. 3 × 3 conv[C1,C2,C3,C4] indicates 3 × 3 convolution layers with output channels of C1, C2, C3, and
C4. Note that C1, C2, C3, and C4 are different from one SA block to another, and are detailed in the supplementary material.

top-1 acc.↑

FLOPs(109)↓

CP-ResNet-50 [1, 10]
SSS-ResNet-50 [14]
NISP-ResNet-50 [33]
LCP-ResNet-50 [7]
ScaleNet-50-light

-3.68
-1.94
-0.21
+0.09
+0.98

1.5
1.3
1.1
1.0
1.2

Table 2: Comparison with state-of-the-art pruning methods on
ImageNet. ResNet-50 and ScaleNet-50-light are trained in same
settings, and others are reported in their papers or websites.

resized to 224×224 before feeding them into networks. Op-
timization is performed using synchronous SGD with mo-
mentum 0.9, weight decay 0.0001 and batch size 256 on
servers with 8 GPUs. The initial learning rate is set to 0.1
and decreased by a factor of 10 every 30 epoches. All mod-
els are trained for 100 epoches from scratch.

On CIFAR-100, we train models with a batch size of 64
for 300 epoches. The initial learning rate is set to 0.1, and
is reduced by 10 times in 150 and 225. The data augmen-

tation only includes random horizontal ﬂipping and random
cropping with 4 pixels padding.

On MS COCO, we train all detection models using the
publicly available implementation2 of Faster RCNN. Mod-
els are trained on servers with 8 GPUs. The batch size and
epoch number are set to 16 and 10 respectively. The initial
learning rate is set to 0.01 and reduced by a factor of 10 at
epoch 4 and epoch 8.

4. Experiments

4.1. ImageNet Classiﬁcation

We evaluate our method on the ImageNet 2012 classiﬁ-
cation dataset [18] that consists of 1000 classes. The models
are trained on the 1.28 million training images, and evalu-
ated on the 50k validation images with both top-1 and top-5
error rate. When evaluating the models we apply centre-
cropping so that 224×224 pixels are cropped from each im-
age after its shorter edge is ﬁrst resized to 256.

2https://github.com/jwyang/faster-rcnn.pytorch

11530

method

original

re-implementation

ResNet-50
ResNet-101
ResNet-152

top-1 err.
24.7
23.6
23.0

top-5 err.
7.8
7.1
6.7

top-1 err.
24.02
22.09
21.58

top-5 err. GFLOPs
7.13
6.03
5.75

4.1
7.8
11.5

top-1 err.
22.20(−1.82)
20.97(−1.12)
20.62(−0.96)

ScaleNet
top-5 err.
6.04(−1.09)
5.58(−0.45)
5.34(−0.41)

GFLOPs
3.8
7.5
11.2

Table 3: Comparisons between ScaleNets and their baseline ResNets with single-crop error rates (%) on ImageNet validation set. The
original column refers to the reported results in the original paper. For fair comparison, we retrain the baselines using the same strategy of
training ScaleNet and report the results in the reimplementation column.

method
ResNeXt-50
ResNeXt-101
SE-ResNet-50
SE-ResNet-101
SE-ResNet-152
DenseNet-121
DenseNet-169
DenseNet-201
ScaleNet-50
ScaleNet-101
ScaleNet-152

top-1 err.
22.2
21.2
23.29
22.38
21.57
25.02
23.8
22.58
22.2
20.97
20.62

top-5 err. GFLOPs
-
5.6
6.62
6.07
5.73
7.71
6.85
6.34
6.04
5.58
5.34

4.2
8.0
4.1
7.8
11.5
2.9
3.4
4.3
3.8
7.5
11.2

Table 4: Comparison with state-of-the-art architectures with
single-crop top-1 and top-5 error rates (%) on ImageNet valida-
tion set.

Comparisons with baselines. We begin evaluations by
comparing the proposed ScaleNets with their correspond-
ing baseline networks in Table 3. It has been shown that
ScaleNets with different depths consistently improve their
baselines with impressive margins while using comparable
(or even a little less) computational complexity. Speciﬁ-
cally, Compared with baselines, ScaleNet-50, 101, and 152
absolutely reduce the top-1 error rate by 1.82, 1.12 and 0.96,
the top-5 error rate by 1.09, 0.45, and 0.41 on ImageNet
respectively. ScaleNet-101 even outperforms ResNet-152,
although it has only 66% FLOPs (7.5 vs. 11.5). It suggests
that explicitly and effectively aggregating multi-scale repre-
sentations of ScaleNets can achieve considerably much per-
formance gain on image classiﬁcation although deep CNNs
are robust against scale variance to some extent.

Comparisons with state-of-the-art architectures. We
next compare ScaleNets with ResNets, ResNeXts, SE-
ResNets, and DenseNets in Table 4.
It has been shown
that ScaleNets consistently outperform them. Remarkably,
ScaleNet-50, 101 and 152 absolutely reduce the top-1 er-
ror rate by 1.09 , 1.41 and 0.95 compared with their coun-
terparts SE-ResNet-50, 101 and 152 respectively. Surpris-
edly, our ScaleNets-101 performs better than ResNeXt-101
by 0.23 and runs much faster without group convolution.
These observations verify the effectiveness and efﬁciency
of the proposed ScaleNets.

# layer
38 layers
56 layers
101 layers

ResNets
26.88
26.19
24.54

ScaleNets
24.60(−2.28)
23.83(−2.36)
22.77(−1.77)

Table 5: Comparisons of the top-1 error rate on CIFAR-100 be-
tween ScaleNets and their baseline ResNets. All the results are
the best of 5 runs.

4.2. CIFAR Classiﬁcation

We also conduct experiments on CIFAR-100 dataset[17].
To make full use of the same SA block architecture, Our
baseline ResNets on CIFAR-100 also employ residual bot-
tleneck blocks (i.e., a subsequent layers of 1 × 1 conv, 3 × 3
conv and 1 × 1 conv) instead of basic residual blocks (two
3 × 3 conv layers) in [9]. The network inputs are 32 × 32
images. The ﬁrst layer is 3 convolutions with 16 channels.
Then we use a stack of n residual bottleneck blocks on each
of these three stages with the feature maps of sizes 32×32,
16×16 and 8×8 respectively. The numbers of channels for
1 × 1 conv , 3 × 3 conv, and 1 × 1 conv in each residual
block are set to 16, 16 and 64 on the ﬁrst stage, 32, 32 and
128 on the second stage, 64, 64 and 256 on the third stage.
The subsampling is performed by convolutions with a stride
of 2 at beginning of each stage. The network ends with a
global average pool layer, a 100-way fully-connected layer,
and softmax layer. There are totally 9n+2 stacked weighted
layers. When n = 4, 6, and 10, we get baselines ResNet-
38, ResNet-56 and ResNet-101 respectively for tiny images.
Their corresponding ScaleNets with comparable computa-
tional complexity are denoted by ScaleNet-38, ScaleNet-56,
and ScaleNet-101.

We compare the performances between ScaleNets and
their baselines on CIFAR-100 in Table 5. Again, the pro-
posed ScaleNets outperforms ResNets with big margins. It
has been validated that ScaleNets can effectively enhance
and improve its strong baseline ResNets across multiple
datasets from ImageNet to CIFAR-100, and multi-scale ag-
gregation is also important for tiny image classiﬁcation.

4.3. Data Driven Neuron Allocation

The proposed ScaleNets can automatically learn the neu-
ron proportion for each scale in each SA block. The neuron

11531

Figure 4: Comparisons between even neuron allocation and data-
driven neuron allocation.(Left)CIFAR-100, (Right)) ImageNet.

allocation depends on the training data distribution and net-
work architectures.

Even allocation vs. data-driven allocation. Figure 4
compares even neuron allocation for scales in each SA
block and data-driven neuron allocation. We conduct exper-
iments on both CIFAR-100 and ImageNet with scale num-
ber L from 2 to 5. Data-driven neuron allocation outper-
forms even allocation with impressive margins in all set-
ting except that on CIFAR-100 with L = 2. We also ob-
serve that data-driven allocation performs best on CIFAR-
100 with L = 3 and ImageNet with L = 4. This is reason-
able since ImageNet has bigger resolution and needs repre-
sentation with wider scale range than CIFAR-100. We set
L to 3 on CIFAR-100 and L to 4 on ImageNet in all our
experiments except otherwise noted. Based on even allo-
cation (gains from SA block), ScaleNet-50 achieves top-1
error rate of 22.76%. With data-driven allocation, the top-1
error rate can be further reduced to 22.20%.

Visualization of neuron allocation. Figure 5 shows
learned neuron proportion in each SA block of ScaleNets.
We observe that neuron proportions for scales are different
from one SA block to another in one network. Speciﬁcally,
scale 2 accounts for more and more proportion from bottom
to top on both CIFAR-100 and ImageNet. Scale 4 mainly
exists in the ﬁrst two stages of ScaleNet-50 on ImageNet.

Figure 5: Neuron proportion for scales in each SA block of
ScaleNets on CIFAR-100 and ImageNet.

4.4. Object Detection on MS COCO

To further evaluate the generalization on other recog-
nition tasks, we conduct object detection experiments on
MS COCO [21] consisting of 80k training images and 40k
validation images, which are further split into 35k minius-
mini and 5k mini-validation set. Following the common
setting [9], we combine the training images and miniusmini
images and thus obtain 115k images for training, and the 5k
mini-validation set for evaluation. We employ the Faster
RCNN framework [25]. We test models by resizing the
shorter edge of image to 800 (or 600) pixels, and restrict
the max size of the longer edge to 1200 (or 1000).

Comparisons with baselines. Table 6 compares the de-
tection results of ScaleNets and their baseline ResNets on
MS COCO. With multi-scale aggregation, Faster RCNN
achieves impressive gains with range from 3.2 to 4.9. Espe-
cially, ScaleNet-101 reaches an mAP of 39.5.

ScaleNets are effective for object detection. Table 7
compares the effectiveness of backbones for object de-
tection.
It has been shown that ScaleNet-101 achieves
the best detection performance with the minimal compu-
tational complexity amongst ResNets, ResNeXts [31], SE-
ResNets [11], and Xception [5].

11532

123456scale number of ScaleNet-5623.524.024.525.025.526.026.5CIFAR-100 top-1 err. (%)Even neuron allocationData-driven neuron allocation123456scale number of ScaleNet-5022.022.222.422.622.823.0ImageNet top-1 err. (%)Even neuron allocationData-driven neuron allocation010020030040050060070080012345678910111213141516scale 1scale 2scale 3scale 40102030405060708090100123456789101112131415161718scale 1scale 2scale 3ImageNet, ScaleNet-50CIFAR-100, ScaleNet-56600/1000

800/1200

ResNet-50
ScaleNet-50
ResNet-101
ScaleNet-101

mmAP APs APm APl mAP APs APm APl
31.7
36.2
34.1
37.3

12.6
17.1
13.8
16.6

35.9
40.7
38.6
42.4

48.3
53.8
51
55.1

32.6
37.2
35.9
39.5

15.9
19.4
17.7
21.3

36.7
41.3
39.9
44

46
52.6
51.6
55.2

Table 6: Comparisons of mAP on MS COCO. mAP indicates the results of mAP@IoU=[0.50:0.95]. Results of ResNets and ScaleNets are
obtained by keeping all settings the same except backbone for fair comparison.

ImageNet

COCO

top-1 err.

FLOPs mAP (600/1000)

with data-driven neuron allocation, ScaleNets perform ef-
fectively and efﬁciently on various visual recognition tasks.

ResNet-152

Xception

SE ResNet-152
ResNeXt-101
ScaleNet-101

21.58
21.11
21.07
21.01
20.97

11.5
9.0
11.5
8.0
7.5

34.3
27.7
37.1
36.7
37.3

Table 7: Comparisons of effectiveness of backbone for object de-
tection on MS COCO. All models are trained with the same strat-
egy for fair comparison.

method
stride 2 of 3 × 3 conv
stride 2 of 3 × 3 conv, dilated 2
2 × 2 average pool
2 × 2 max pool

top1 err.
26.19
25.42
24.58
24.48

Table 8: Top-1 error rate on CIFAR-100 with different downsam-
pling methods. All the methods are of same FLOPs and record
the best result in 5 runs.

4.5. Analysis

The role of max pool. Downsampling can be imple-
mented in several ways: (i) a 3 × 3 conv with stride s; (ii) a
dilated 3 × 3 conv with stride s [23]; (iii) a s × s avg pool
with stride s; (iv) a s × s max pool with stride s. We eval-
uate all the above settings with ScaleNet-56 on CIFAR-100
by setting scale number L to 2 and s to 2 for simplicity. As
shown in Table 8, (iv) performs best. It suggests that max
pool is the key factor of performance boosting. It is reason-
able since max pool preserves and enhances the maximum
activation from previous layers so that the high response of
small foreground regions would not be drowned by back-
ground features as information ﬂows from bottom to top.

Wide range of receptive ﬁeld. Figure 6 compares the
receptive ﬁeld range of each block. It has been shown that
the proposed ScaleNets have much wider range of receptive
ﬁeld than others. Particularly, ScaleNet-50 reaches the res-
olution for classiﬁcation and detection only in second and
third block. On the one hand, ScaleNets potentially aggre-
gate rich representations with large range of scales. On the
other hand, they can extract global context information at
very early stage (e.g., block 3) in one network. Together

5. Conclusion

In this paper, we proposed a scale aggregation block with
data-driven neuron allocation. The SA block can replace
3×3 conv in ResNets to get ScaleNets. The data-driven neu-
ron allocation can effectively allocate the neurons to suit-
able scale in each SA block. The proposed ScaleNets have
wide range of receptive ﬁelds, and perform effectively and
efﬁciently on image classiﬁcation and object detection.

Figure 6: Comparisons of receptive ﬁeld of multi-branch net-
works as a function of block index. The shortcut branch and resid-
ual branch in each residual block of ResNets have the minimal and
maximal receptive ﬁeld respectively. The 1 × 1 conv branch, and
5×5 conv branch in each Inception block of GoogleNet have the
minimal and maximal receptive ﬁeld respectively.

6. Acknowledgement

This work was supported by Beijing Municipal Sci-
ence and Technology Commission (Z181100008918004) in
part. And We thank Zhanglin Peng, Youjiang Xu, Xinjiang
Wang, Huabin Zheng from SenseTime for the helpful sug-
gestions and supports.

11533

0246810121416block index0200400600800100012001400receptive field2241200Min RF of ResNet-50Max RF of ResNet-50Min RF of GoogleNetMax RF of GoogleNetMin RF of ScaleNet-50Max RF of ScaleNet-50References

[1] https://github.com/yihui-he/channel-pruning.

[2] J. M. Alvarez and M. Salzmann. Learning the number of
neurons in neep networks. In NIPS, pages 2270–2278. 2016.

[3] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick.
Inside-outside net: Detecting objects in context with skip
pooling and recurrent neural networks.
In CVPR, pages
2874–2883, 2016.

[4] X. Cao, Z. Wang, Y. Zhao, and F. Su. Scale aggregation
network for accurate and efﬁcient crowd counting. In ECCV,
pages 734–750, 2018.

[5] J. Carreira, H. Madeira, and J. G. Silva. Xception: A tech-
nique for the experimental evaluation of dependability in
modern computers.
IEEE Transactions on Software Engi-
neering, 24(2):125–136, 1998.

[6] X. Chen. Adaptive multi-scale information ﬂow for object

detection. In BMVC.

[7] T.-W. Chin, C. Zhang, and D. Marculescu.

Layer-
compensated pruning for resource-constrained convolutional
neural networks. arXiv preprint, 2018.

[8] A. Gordon, E. Eban, O. Nachum, B. Chen, H. Wu, T.-J.
Yang, and E. Choi. Morphnet: Fast & simple resource-
constrained structure learning of deep networks. In CVPR,
2018.

[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, pages 740–755, 2014.

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015.

[23] D. I. C. Onvolutions. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016.

[24] A. Rabinovich and A. C. Berg. Parsenet looking wider to see

better. pages 1–11, 2016.

[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: to-
wards real-time object detection with region proposal net-
works. PAMI, 39(6):1137–1149, 2017.

[26] S. Saxena and J. Verbeek. Convolutional neural fabrics. In
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, NIPS, pages 4053–4061. Curran Asso-
ciates, Inc., 2016.

[27] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway

networks. arXiv preprint, 2015.

[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9, 2015.
[29] T. Veniat and L. Denoyer. Learning time/emory-efﬁcient
arXiv

deep architectures with budgeted super networks.
preprint, 2017.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

[30] P. M. Williams. Bayesian regularization and pruning using a

for image recognition. CVPR, pages 770–778, 2016.

laplace prior. Neural computation, 7(1):117–143, 1995.

[31] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
pages 5987–5995, 2017.

[32] F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer

aggregation. In CVPR, 2018.

[33] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han,
M. Gao, C.-Y. Lin, and L. S. Davis. Nisp: Pruning networks
using neuron importance score propagation. In CVPR, 2018.
[34] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, pages 2881–2890, 2017.

[35] B. Zoph and Q. V. Le. Neural architecture search with rein-

forcement learning. arXiv preprint, 2016.

[10] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerat-

ing very deep neural networks. In ICCV, 2017.

[11] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. In CVPR, 2018.

[12] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and
K. Weinberger. Multi-scale dense networks for resource ef-
ﬁcient image classiﬁcation. In ICLR, 2018.

[13] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, vol-
ume 1, page 3, 2017.

[14] Z. Huang and N. Wang. Data-driven sparse structure selec-
In ECCV, pages 304–320,

tion for deep neural networks.
2018.

[15] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015.

[16] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards ac-
curate region proposal generation and joint object detection.
In CVPR, pages 845–853, 2016.

[17] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. Technical report, 2009.

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, pages 1097–1105, 2012.

Imagenet
In

[19] V. Lebedev and V. Lempitsky. Fast convnets using group-

wise brain damage. In CVPR, pages 2554–2564, 2016.

[20] T.-y. Lin, P. Doll, R. Girshick, K. He, B. Hariharan, S. Be-
longie, F. Ai, and C. Tech. Fpn feature pyramid networks for
object detection. CVPR, 2017.

11534

