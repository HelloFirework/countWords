Distant Supervised Centroid Shift: A Simple and Efﬁcient

Approach to Visual Domain Adaptation ∗

♮ CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences (CAS), ♯ University of Chinese

Academy of Sciences, ♭ CAS Center for Excellence in Brain Science and Intelligence Technology

Jian Liang, Ran He, Zhenan Sun and Tieniu Tan

liangjian92@gmail.com, {jian.liang,rhe,znsun,tnt}@nlpr.ia.ac.cn

Abstract

Conventional domain adaptation methods usually resort
to deep neural networks or subspace learning to ﬁnd invari-
ant representations across domains. However, most deep
learning methods highly rely on large-size source domains
and are computationally expensive to train, while subspace
learning methods always have a quadratic time complexity
that suffers from the large domain size. This paper provides
a simple and efﬁcient solution, which could be regarded as
a well-performing baseline for domain adaptation tasks.

Our method is built upon the nearest centroid classiﬁer,
seeking a subspace where the centroids in the target do-
main are moderately shifted from those in the source do-
main. Speciﬁcally, we design a uniﬁed objective without
accessing the source domain data and adopt an alternat-
ing minimization scheme to iteratively discover the pseudo
target labels, invariant subspace, and target centroids. Be-
sides its privacy-preserving property (distant supervision),
the algorithm is provably convergent and has a promising
linear time complexity. In addition, the proposed method
can be readily extended to multi-source setting and domain
generalization, and it remarkably enhances popular deep
adaptation methods by borrowing the learned transferable
features. Extensive experiments on several benchmarks in-
cluding object, digit, and face recognition datasets validate
that our methods yield state-of-the-art results in various do-
main adaptation tasks.

1. Introduction

Traditional machine learning paradigms always assume
that the training data and the testing data come from the
same distribution, however, this assumption does not always
hold in real-world applications [52, 66]. To avoid the expen-
sive and time-consuming data labeling step, massive efforts

∗This work is funded by the National Natural Science Foundation of
China (Grants No. 61622310, 61721004), and Beijing Natural Science
Foundation (Grants No. JQ18017).

Figure 1. Illustrative example of centroid shift (arrows) in our ap-
proach. Larger markers indicate the source and target centroids.

over the last decade have been devoted to transfer learn-
ing [64, 8, 34, 36] and multi-task learning [14, 30, 63, 70]
that leverage the latent relationship with previous datasets
to learn a new model for an emerging dataset.

Taking a night object image recognition problem for ex-
ample, it is not desirable to neither train a model on existing
day-time object images to recognize these night object im-
ages nor acquire massive labeled object images at night to
re-train a model on them from scratch. By contrast, we ex-
pect to transfer the knowledge from existing day-time object
images to recognizing these unlabeled night object images,
which is also known as domain adaptation, and such day-
time and night images are termed as source and target do-
mains, respectively. Based on the availability of partial la-
beled target data, domain adaptation can be roughly divided
into two categories, unsupervised and semi-supervised do-
main adaptation. In this paper, we mainly focus on the chal-
lenging unsupervised domain adaptation problem where the
labels of target data are totally unknown.

Since the degradation in performance mainly arises from
the covariate shift (i.e., the change in the data distribution of
the source and target domains), early approaches [72, 58]
favor an intuitive strategy named instance re-weighting,
which tries to align two different domains via estimating
naturally the ratio between the likelihoods of being a source
or target example. Later studies [42, 7] exploit one fa-
vorite distribution measure named Maximum Mean Dis-
crepancy (MMD) [24] to weigh data instances. However,

2975

these instance-based adaptation methods require the strict
assumptions [52] that are hard to satisfy.

Alternatively, a growing number of recent studies focus
on learning transferable representations via deep neural net-
works or subspace discovery since they do not require the
strong assumption. Deep domain adaptation methods are
roughly divided into three main categories, discrepancy-
based methods [60, 39, 44, 73, 68], adversarial-based meth-
ods [17, 38, 2, 63, 64], and reconstruction-based meth-
ods [19, 3, 76]. However, these batch-wise deep learning
methods cannot fully exploit the global information and ad-
dress small-size source domains well. Subspace alignment
methods [23, 15] try to align the subspaces (e.g., PCA) of
different domains together. Later studies [59, 32, 73] fur-
ther consider the second-order and higher-order scatter ma-
trices (moments) across different domains. These subspace-
centric methods are rather easy and efﬁcient to deploy, yet,
they fail to minimize the data distributions between domains
after aligning the subspaces. [41] is a typical data-centric
subspace discovery based approach that learns to project
both domains onto one subspace where the cross-domain
joint distribution discrepancy is minimized. Following stud-
ies [74, 36, 67] are built upon [41] by considering coupled
projections, discriminative target structure and joint classi-
ﬁer learning, respectively. Even these data-centric methods
achieve promising results, yet, they always involve a heuris-
tic pseudo target label estimation step in the EM-like algo-
rithm, making the overall optimization problem hard to con-
verge theoretically. Additionally, all the methods mentioned
above involve several large MMD matrices [41], thus, they
all have a quadratic time complexity w.r.t. the domain size
and can not cope with large-scale datasets well.

In this paper, we propose a simple, efﬁcient, yet effective
approach via subspace discovery for unsupervised domain
adaptation. Inspired by [12], we develop a uniﬁed objective
that assumes the centroids in the target domain are mod-
erately shifted from those in the source domain, and each
instance is closest to its corresponding centroid for both
domains in the projected subspace. Note that, this objec-
tive does not need to access the source domain data like
[6], making it a privacy-preserving method. Then we adopt
an alternating minimization scheme to iteratively discover
the pseudo target labels, adaptive target centroids, and in-
variant subspace learning. Speciﬁcally, each subproblem
has a closed-form solution. Theoretical analysis shows that
our algorithm is convergent and efﬁcient with a linear time
complexity. In addition, the proposed method can be read-
ily extended to multi-source setting and domain generaliza-
tion, and it even remarkably enhances popular deep domain
methods by borrowing the learned transferable features. Ex-
tensive experiments on several benchmarks including ob-
ject (i.e., Ofﬁce31 [55], Ofﬁce-Caltech [21] and Ofﬁce-
Home [66], VLCS [62]), Digits, and face (i.e., PIE [1])

recognition datasets validate that our methods achieve state-
of-the-art results in the vanilla unsupervised domain adapta-
tion, domain generalization, and multi-source domain adap-
tation tasks. Generally, our algorithm is impressively sim-
ple and efﬁcient, making it a strong baseline for domain
adaptation and generalization tasks.

2. Related Work

The last decade has witnessed a boom in studies towards
domain adaptation and related applications. We refer the in-
terested reader to [66, 9] for a survey focusing on computer
vision applications. As stated above, both feature transfor-
mation [51, 41, 74, 36] and feature representation learning
[19, 53, 63, 68, 53] are much more favored by recent do-
main adaptation approaches. Here we analyze several most
closely related work from both cases to our method.

Regarding shallow feature transformation based ap-
proaches, [36] proposes a general objective in order to pur-
sue that instances from the same class of both domains are
dragged closer to each other, which includes many former
methods [51, 41] as special cases. Our method can be con-
sidered to be built upon [36] by developing a built-in clas-
siﬁer to infer the pseudo target labels, and it only acquires
distant supervisions, i.e., class-wise Gaussian estimators of
means and covariance matrices, instead of accessing the
entire source domain data. Besides, previous methods al-
ways involve several large MMD matrices [41, 74, 36, 67]
in the optimization procedure with a O(n2) computation
complexity, making it unsuitable for large-scale adaptation
scenarios. In fact, there are several previous studies [11, 10]
that attempt to investigate Nearest Class Means (NCM) for
domain adaptation. However, they merely integrate NCM
with other domain adaptation models [61, 5] as a novel clas-
siﬁer, ignoring the distribution discrepancy after projection.
Beneﬁting from the rapid development of deep neural
networks, deep domain adaptation methods achieve much
better performance. Generally, a majority of them (e.g.,
[65, 17, 60, 64, 68]) equip the source domain a source clas-
siﬁcation loss function and design another cross-domain
loss function (e.g., MMD or adversarial loss in Genera-
tive Adversarial Network (GAN) [22]) to align two do-
mains. SimNet [53] replaces the conventional source classi-
ﬁer (e.g., soft-max) with a prototype similarity-based clas-
siﬁer and adopts a domain discriminator for domain con-
fusion, which achieves state-of-the-art performances. Fur-
thermore, [19] utilizes a reconstruction loss for target do-
main and [53] designed a similarity-based classiﬁer for la-
beled source domain. However, these methods are always
optimized in a batch-wise manner, making it not suitable to
minimize a global loss function like MMD. Our approach,
applied to learned transferable features extracted from ﬁne-
tuned models and deep domain adaptation methods like
DAN [39], RevGrad [17] and GTA [56], achieves better per-

2976

formance to these more complex methods and is expected
to be incorporated directly into the network structure.

3. Methodology

3.1. Problem Deﬁnition

i , yt

i , xt

i , ys

i )}nt

In conventional domain adaptation with two domains,
i )}ns
we have access to labeled images Xs = {(xs
i=1
drawn from a source domain distribution ps(x, y) and tar-
i, yt
get images Xt = {(xt
i=1 drawn from a target do-
main distribution pt(x, y).
It is commonly assumed that
the label space of both domains are identical, i.e., ys
i ∈
[1, 2, · · · , C] and the length of domain feature space is the
same, i.e., xs
i ∈ Rd. Note that, in the unsupervised set-
ting, we have no information about the labels on the tar-
get domain. Without loss of generality, we assume that
each feature vector is normalized to satisfy kxq
i k2 = 1, q ∈
{s, t}, then zero-centered, i.e., Pi xs
i = 0.
In fact, instead of using the original data in the source
domain, we only require the number of samples in each
class (i.e., mr), the maximum likelihood estimators of the
means ˆµr, and covariance matrices ˆΣr, r = [1, 2, · · · , C]
if we assume the data of the r-th class in the source do-
main follow a d-variate Gaussian distribution. Obviously,
these two estimators are deﬁned as ˆµr = 1
i and
ˆΣr = 1

i = 0 and Pi xt

i − ˆµr)T , respectively.

i − ˆµr)(xs

mr Pys

i =r xs

i =r(xs

mr Pys

3.2. Formulation

To tackle the covariate shift, we propose a shallow fea-
ture transformation based domain adaptation method. Our
method consists of two main components, i.e., source do-
main classiﬁcation and target domain classiﬁcation, and re-
late these two parametric classiﬁers by assuming small per-
turbations on parameters [71]. Since we have no access to
the labels on the target domain, it is hard to directly con-
sider a target domain classiﬁcation task as well as some con-
ditional distribution discrepancies. To address this issue,
JDA [41] exploits the pseudo target labels as supervision
signals for feature transformation learning. This strategy
has proven to work well for unsupervised domain adapta-
tion and has been re-utilized by later works [74, 36]. Thus,
we also consider such a discriminative component to deal
with pseudo-labeled target instances.

Inspired by the popular supervised adaptive feature re-
duction method [12], we ﬁrst introduce a feature transfor-
mation matrix W ∈ Rk×d for the labeled source data points
and expect the following objective with respect to W to be
minimized as much as possible.

min
W

Pi dW (xs
Pi dW (xs

i , ˆµys
i )
i , ˆµ)

=

Pr Pys

i =r dW (xs
i , 0)

Pi dW (xs

i , ˆµr)

,

(1)

where dW (x, x′) = kW x − W x′k2
2, and ˆµs = 0 denotes
the overall mean in the source domain. Furthermore, we can

rewrite the objective above as

min
W

trace(W Ss
trace(W Ss

wW T )
t W T )

,

(2)

w = Pi(xs
i )(xs

r=1 mr ˆΣr
where Ss
w + Pr mr(ˆµr − 0)(ˆµr − 0)T
and Ss
are also well known as the within-class scatter and the total
scatter matrices in the source domain, respectively.

i − ˆµys
i )T = Ss

i )T = PC

t = Pi(xs

i − ˆµys

i )(xs

The only information we know about the target domain
is that it shares the same classes with the source domain,
therefore, we also expect that data points from the target
domain are well separated as those from the source domain,

min

W,ˆµt

r ,ˆyt

i

Pr Pˆyt

i =r dW (xt
i, 0)

Pi dW (xt

i, ˆµt
r)

.

(3)

Since no labels are available for the target domain, we also
need to estimate the optimal pseudo label ˆyt
i for each target
instance and the target means ˆµt
r, 1 ≤ r ≤ C.

For simplicity, we follow the popular framework to learn
a unique projection W for two different domains. Differ-
ent from previous works [41, 74, 36] that generate pseudo
target labels via merely exploiting the source data points,
we aim to obtain a self-training target classiﬁer and assume
that the parameters are shifted from the parameters in the
source classiﬁer [71]. Concretely, we adopt the nearest cen-
troid classiﬁer and force the target centroids are close to
their corresponding source centroids in Fig. 1, that is to say,
we want to minimize the following objective function:

min
W,∆t

r ,ˆyt

i

i , ˆµr)

Pr Pys

i =r dW (xs
i , 0)

Pi dW (xs
Pr Pˆyt

+

i =r dW (xt
Pi dW (xt

+ Xr
i, ˆµr + ∆r)
i, 0)

βrkW ∆rk2
2

(4)

,

where ∆r ∈ Rd×1, 1 ≤ r ≤ C are the the so-called pertur-
bation variables and βr are the trade-off parameters which
rely on the cluster size of each class.

For the sake of simple optimization, we reformulate
these three terms in one trace-ratio objective and obtain a
relaxed ratio-trace objective in the following,

min
W,∆t

r ,ˆyt

i

trace(W (S s

w+S t
trace(W (S s

w+Pr βr∆r∆T
t )W T )

t +S t

r +λI)W T )

,

֒→ minW,∆t

r ,ˆyt

i

trace{ W (S s

w+S t

w+Pr βr∆r∆T
W (S s
t )W T

t +S t

(5)
r +λI)W T

},

s.t. W (Ss

t + St

t )W T = I,

(6)

w = Pi(xt

)T and
where St
i)T are the corresponding scatter variances
t = Pi(xt
St
in the target domain. In fact, to avoid a solution where all

i − ˆµˆyt

i − ˆµˆyt

− ∆ˆyt

− ∆ˆyt

i)(xt

)(xt

i

i

i

i

2977

t + St

the rows in W are identical [46], we impose an orthonor-
t )W T = I on the projection W .
mal constraint W (Ss
Under this constraint, Fukunaga [16] showed that Eq. (5)
is essentially identical to Eq. (6). Besides, we follow pre-
vious works [41, 74, 36, 51] in this literature and impose
a regularization parameter λ to guarantee the optimization
problem to be well-deﬁned.

3.3. Optimization

To optimize the above objective in Eq. (6), we exploit a
popular alternating optimization scheme. In the following,
we provide the solutions to each sub-problem.

W -step: Once we ﬁx these two other groups of variables
r}C
i=1, the objective function w.r.t. W be-

{∆t
comes a classical ratio-trace optimization problem:

r=1 and {ˆyt

i }nt

t + St

w + St

t and Sw = Ss

minW ∈Rk×d trace{(W StW T )−1(W (Sw + λI)W T )},
(7)
where St = Ss
w + Pr βr∆r∆T
r
are two d × d scatter variance matrices. Interestingly, this
problem can be efﬁciently solved by generalized eigenvalue
decomposition (GEVD) Stwa = γa(Sw +λI)wa, where γa
is the a-th smallest generalized eigenvalue. The matrix W
is then constituted of the corresponding eigenvectors wa ∈
Rd×1, 1 ≤ a ≤ k as rows.

ˆY t-step: Once we obtain the domain-invariant projec-
r=1, the objective

tion W and perturbation variables {∆t
function w.r.t. {ˆyt

i=1 has the form

i }nt

r}C

min
i }

nt
i=1

{ˆyt

trace{(W StW T )−1(W SwW T )}

=

=

nt

X

i=1
nt

X

i=1

min
i ∈[1,C]

ˆyt

{(xt

i − ˆµˆyt

i

− ∆ˆyt

i

)T Sp(xt

i − ˆµˆyt

i

− ∆ˆyt

i

)},

(8)

{(xt

i)T Sp(xt

i) + max

ˆyt

i ∈[1,C]

(hˆyt

i

xt
i − bˆyt

i

)},

7:

8:

9:

i) = hrxt

where Sp = W T (W StW T )−1W is a positive deﬁnite
matrix, and the parameters in the classiﬁcation function
fr(xt
i − br of the r-th class are hr = 2(ˆµr +
∆r)Sp ∈ R1×d and br = −(ˆµr + ∆r)T Sp(ˆµr + ∆r), re-
spectively. That is to say, we can estimate each the pseudo
target label ˆyt

i independently via the following rule

ˆyt
i = arg max
r∈[1,C]

hrxt

i − br.

(9)

∆-step: Once we get the domain-invariant projection
i=1, the objective function

W and pseudo target labels {ˆyt
w.r.t. each perturbation variable ∆r can be written as

i }nt

min
∆r

trace{Sp( X
i =r

ˆyt

(xt

i − ˆµr − ∆r)(xt

i − ˆµr − ∆r)T + βr∆r∆T

r )},

⇒ min
∆r

trace{Sp((βr + nr)∆r∆T

r − 2 X
i =r

ˆyt

(xt

i − ˆµr)∆T

r )},

⇒ min
∆r

(∆r −

Pˆyt

i − nr ˆµr

i =r xt
βr + nr

)T Sp(∆r −

Pˆyt

i − nr ˆµr

i =r xt
βr + nr

),

(10)

where nr is the size of the r-th class in the target domain.
Since Sp is easily proven to be a positive deﬁnite matrix,
i.e., ∀x ∈ Rd×1, xT Spx ≥ 0. Thus, the optimal variable
∆r can be obtained via the following equation

∆r = (Xˆyt
and

i =r

xt
i − nr ˆµr)/(βr + nr), r ∈ [1, C],

(11)

ˆµr + ∆r = (βr ˆµr + Xˆyt

i =r

xt
i)/(βr + nr).

(12)

Carefully checking the term above, we can easily discover
that the learned perturbations indeed place the optimal tar-
get centroids/ prototypes in the routines between source
class means and pseudo target class means. Besides, when
the value βr/nr becomes larger, the learned target centroids
are much closer to their corresponding source class means.
Towards this end, we have provided three closed-form
solutions in Eq. (7), Eq. (9), and Eq. (11) for each subprob-
lem, and the complete algorithm is summarized in Algo-
rithm. 1.

Algorithm 1 Unsupervised Domain Adaptation with Mini-
mum Centroid Shift (MCS)
Input: Source domain information {mr, ˆµr, ˆΣr}C

r=1 and target
i=1; subspace dimensionality k, parameters λ

domain {xt
and α, inner/ outer maximum iterations Ti = 5 and To = 10.

i}nt

Output: Feature transformation matrix W ∈ Rk×d, perturbation

variables {∆r}C

i }nt
r=1 and target labels { ˆyt
t , S t
t ;

w, S s

i=1.

w and ∆r as 0, and calculate W via Eq. (7)
i }nt
r=1 and update S t

1: Compute the scatter matrices S s
2: Initialize S t
3: Estimate { ˆyt
4: Compute the size of the r-th target class lr and let βr = α∗lr;
5: while not converge iter ≤ To do
6:

while not converge and iter ≤ Ti do

i=1 using W and { ˆµr}C

w;

Update perturbation variables {∆r}C
Update pseudo target labels { ˆyt

i }nt

i=1 via Eq. (9);

r=1 via Eq. (11);

w and solve the GEVD problem in Eq. (7) for W .

end while
Update S t

10:
11: end while

3.4. Convergence and Time Complexity

It is obvious that each solution above (i.e., Eqs. (7), (9),
(11)) monotonically decreases the overall objective function
trace{(W StW T )−1(W (Sw + λI)W T )} in each iteration
and the objective value is always larger than zero, that is to
say, the proposed iterative algorithm in Algorithm. 1 con-
verges to the local minimizer after certain iterations.

Regarding the computation complexity, the GEVD step
occupies O(kd2), other matrix multiplies occupy O(ntd2 +
ntdk), and the ˆY t step occupies O(Cntk2 + Cntd +
k3). In summary, the overall complexity of our method is
O(Tikd2 + TiToCntd). When compared with massive pre-
vious methods [41, 74, 36, 67] with O(n2) complexity, our
method is more favorable by large-scale datasets. Speciﬁ-
cally, for large-scale digit datasets like SVHN and MNIST,
previous methods (eg., [41, 74, 36, 67]) are not ﬂexible due
to the limited memory.

2978

4. Experiment

In this section, we conduct extensive experiments to
evaluate the effectiveness of the proposed approaches for
unsupervised cross-domain image recognition problems, in-
cluding vanilla domain adaptation, multi-source domain
adaptation and domain generalization where the target do-
main is totally unknown in the training phase.

4.1. Unsupervised Domain Adaptation (UDA)

4.1.1 Datasets and Settings

The Ofﬁce31 [55] dataset includes images of 31 objects
taken from 3 domains, i.e., Amazon (A, images downloaded
from the online web merchants), DSLR (D, high-resolution
images captured by a digital SLR camera), and Webcam
(W, low-resolution images recorded by a web camera).
They contain 2,817, 498 and 795 images, respectively.

The Ofﬁce-Caltech dataset consists of images from
10 overlapping object classes between Ofﬁce31 and Cal-
tech256 [25], and these domains include 958, 157, 295 and
1,123 samples for A, D, W and Caltech (C), respectively.

The Ofﬁce-Home [66] dataset is a new benchmark that
contains 4 domains, with each domain containing 65 kinds
of everyday objects, i.e., Art (Ar, artistic depictions of ob-
jects), Clipart (Cl, clipart images), Product (Pr, objects
without a background) and Real-World (Re, objects cap-
tured with a regular camera). Besides, they contain 2,421,
4,365, 4,428 and 4,357 samples, respectively.

Baseline methods. We compare the proposed method
with several state-of-the-art unsupervised domain adapta-
tion approaches that can be roughly divided into two main
categories, shallow feature transformation approaches, and
deep domain adaptation approaches. 1NN is a basic method
that is trained on the raw source data without any fea-
ture transformation. Shallow UDA methods mainly include
SA [15], JDA [41], CORAL [59], Invariant Latent Space
(ILS) [27], JGSA [74], LDA-inspired Domain Adaptation
(LDADA) [45], and DICE [36]. There are also some pop-
ular baseline methods, including ATI [4], PUnDA [20],
MEDA [67], and GAKT [13].

Regarding deep end-to-end UDA approaches, we col-
lect the reported accuracies from recent studies that share
the same protocol with our method. Some representa-
tive deep methods are listed in the following, DAN [39],
RevGrad [17], DRCN [19], RTN-res [43], ADDA [64],
JAN-A [44], GTA [56], and CDAN+E [40].

Note that A→B indicates that A is the source domain and
B is the target domain. We evaluate different UDA methods
in terms of classiﬁcation accuracy (%) on the target.

4.1.2 Results on the Ofﬁce31 dataset

Table 1. Accuracy (%) on Ofﬁce31 with the evaluation setup of
[60, 36]. The best results of methods with deep features are in
bold red and deep models with bold underlined results are better
than MCS (ours). [∗ using ResNet-34]

Method

1NN
SA
JDA

CORAL

JGSA
ILS

ATI [4]
LDADA

DICE

MEDA [67]
MCS(ours)

A→D A→W D→A D→W W→A W→D Avg.
59.4
99.0 67.3
98.2 67.9
61.0
99.6 73.7
66.5
99.0 67.8
60.4
99.8 72.5
67.5
62.9
99.4 70.4
98.7 74.2
70.3
98.4 72.6
65.9
99.8 75.0
66.7
96.8 74.0
69.5
71.9
99.4 76.5

96.1
95.1
97.7
96.2
98.1
97.2
95.0
94.7
96.9
94.0
96.7

44.8
46.6
53.5
46.3
52.0
48.8
56.9
53.4
58.6
56.0
57.2

57.5
59.5
68.8
57.0
62.3
63.9
68.7
68.1
71.4
69.9
75.1

47.2
46.9
56.3
47.6
55.6
50.0
55.3
55.5
56.5
58.0
58.8

RevGrad
DAN [39]
DRCN [19]
RTN-res [43]

72.3
67.0
66.8
71.0
71.6
72.8
I2I-Adapt [49]∗ 71.1

JAN-A [44]

ADDA

73.0
68.5
68.7
73.3
73.5
75.2
75.3

53.4
54.0
56.0
50.5
54.6
57.5
50.1

96.4
96.0
96.4
96.8
96.2
96.6
96.5

51.2
53.1
54.9
51.0
53.5
56.3
52.1

99.2 74.3
99.0 72.9
99.0 73.6
99.6 73.7
98.8 74.0
99.6 76.3
99.6 74.1

data in the source domain and unlabeled data in the target
domain. We further exploit the AlexNet-FC7 features [59]
ﬁne-tuned on the source domain, making it fair to be com-
pared with deep UDA methods via AlexNet.

As shown in Table 1, our method outperforms all deep
methods and other shallow counterparts in terms of the
average accuracy. Firstly, for the small and easy tasks
D↔W, all the UDA methods achieve promising results,
and our method performs worse than several shallow meth-
ods (e.g., JGSA [74]). Secondly, our method signiﬁcantly
outperforms all the shallow methods and is competitive to
state-of-the-art deep method JAN-A [44] for some relatively
challenging adaptation tasks, including A→D and A→W.
Thirdly, when a small source domain (i.e., D and W) is
adapted to a large target domain A, our method obviously
ranks the ﬁrst and second among all the methods.

Generally speaking, our method outperforms several
state-of-the-art shallow methods (i.e., DICE [36] and
MEDA [67]) by a large margin and achieves quite compet-
itive results to the state-of-the-art deep UDA method. Note
that, even I2I-Adapt [49] explores a more powerful ResNet-
34 model, our method still performs much better than it.

4.1.3 Results on the Ofﬁce-Caltech dataset

We explore two kinds of deep features [27] produced by
different full convolution layers in VGG-net and follow the
sampling protocol [21, 27, 20] by using few labeled data
in the source domain. Concretely, we randomly select 8
instances per class for domain D and 20 instances per class
for other domains (i.e., A, C and W) as ﬁnal sources.

We follow the full protocol that has been widely adopted
in previous studies [44, 36, 56] by using the entire labeled

As can be seen from Table 2, JGSA [74], PUnDA [20]
and DICE [36] are three best performing methods among

2979

(a) Dimensionality k

(b) Parmeter λ

(c) Parmeter α

(d) # Iterations

Figure 2. Domain adaptation performance accuracy (%) on the Ofﬁce31 (A→D and A→W) and Ofﬁce-Home (Ar→Cl and Ar→Pr)
datasets w.r.t. dimensionality k, parameters λ, α and the number of iterations.

Source
Target
1NN
SA
JDA

Table 2. Accuracy (%) on Ofﬁce-Caltech using the VGG-FC6
(above) and VGG-FC7 (below) features with the evaluation setup
of [21, 20]. The best (bold red), the second best (red).

A
D W A

C
D W A

D
C W A

W
C

C

Avg.

D

70.1 52.3 60.9 81.9 55.6 65.9 57.0 48.0 86.7 66.4 60.2 91.3 66.4
77.1 64.9 76.0 83.9 66.2 76.0 69.0 62.3 90.5 80.2 71.9 94.2 76.0
80.2 71.9 80.6 88.7 72.0 82.2 81.8 73.4 94.0 89.0 79.9 96.2 82.5
CORAL 79.0 67.1 74.8 89.4 67.6 77.6 75.8 64.7 94.6 82.3 75.9 96.0 78.7
79.9 71.7 82.6 90.2 76.4 84.5 82.7 73.5 95.4 91.6 79.0 96.3 83.6
78.9 72.5 82.4 87.6 73.0 84.4 79.2 66.5 94.2 87.2 79.9 89.3 81.3
PUnDA 82.3 76.2 82.7 90.3 76.2 88.3 83.1 69.2 93.4 86.9 82.6 89.8 83.4
LDADA 82.3 64.0 80.3 89.7 67.7 82.2 70.9 60.6 86.9 90.2 82.5 87.8 78.8
83.0 66.4 75.9 91.9 67.4 83.7 84.4 78.6 94.8 90.3 80.7 93.8 82.6
MCS(ours) 87.1 74.8 84.8 92.3 77.3 87.1 84.7 76.0 95.9 88.9 87.4 92.9 85.8

JGSA
ILS

DICE

Table 3. Accuracy (%) on Ofﬁce-Home with ResNet-50 features
and the evaluation setup of [66, 36]. [∗ using VGG-FC features]

Avg.

Source
Target
1NN
SA
JDA

Ar
Pr Re Ar

Cl
Re
Pr Re Ar Cl Re Ar Cl

Pr

Pr

CORAL

Cl
44.1 61.8 69.2 49.6 60.6 63.3 51.6 43.1 70.6 63.1 48.9 76.2 58.5
44.8 65.5 70.6 48.8 61.5 64.1 50.1 42.8 71.1 62.2 48.1 76.2 58.8
46.3 66.0 69.1 47.1 63.4 63.3 48.2 44.0 70.8 60.1 49.6 76.8 58.7
47.1 67.3 74.8 52.3 63.6 66.9 51.0 41.9 72.6 62.8 46.8 77.7 60.4
50.3 70.0 73.8 52.7 68.9 68.2 55.6 47.9 75.1 64.0 52.0 78.7 63.1
46.7 64.3 69.7 44.3 60.9 62.6 47.9 42.7 70.4 61.4 48.5 75.8 57.9
53.2 72.4 74.5 56.5 70.1 69.1 58.9 51.5 77.0 66.5 54.8 79.0 65.3
GAKT [13]∗ 34.5 43.6 55.3 36.1 52.7 53.2 31.6 40.6 61.4 45.6 44.6 64.9 47.0
CDAN [40]
49.0 69.3 74.5 54.4 66 68.4 55.6 48.3 75.9 68.4 55.4 80.5 63.8
CDAN+E [40] 50.7 70.6 76.0 57.6 70.0 70.0 57.4 50.9 77.3 70.9 56.7 81.6 65.8
55.9 73.8 79.0 57.5 69.9 71.3 58.4 50.3 78.2 65.9 53.2 82.2 66.3

JGSA
ILS
DICE

MCS(ours)

1NN
SA
JDA

72.6 50.8 64.0 82.6 54.9 65.3 61.2 52.8 88.2 67.8 64.2 88.8 67.8
76.2 60.7 75.0 82.6 63.2 73.6 66.0 59.4 89.5 76.4 69.0 94.0 73.8
79.9 69.2 80.1 87.3 71.5 80.1 78.5 70.9 92.4 86.9 78.3 94.1 80.8
CORAL 78.6 61.3 71.8 88.6 63.8 76.0 71.2 63.0 93.5 82.0 73.7 94.6 76.5
81.1 72.3 81.4 88.3 72.3 82.5 78.9 72.3 93.6 89.8 79.8 95.8 82.3
78.4 71.3 80.9 87.1 67.1 80.1 76.5 66.2 91.8 86.7 76.3 88.2 79.2
PUnDA 81.0 75.8 81.4 91.1 70.8 83.8 80.4 69.1 92.0 85.7 80.1 90.1 81.7
LDADA 83.3 72.5 83.7 91.5 71.5 84.5 71.8 58.4 88.3 88.0 80.1 86.8 80.0
83.7 62.9 79.3 91.7 63.8 84.3 82.3 76.4 94.2 89.4 82.1 91.0 81.7
MCS(ours) 86.3 72.8 86.6 92.8 73.0 89.3 84.6 76.5 95.5 90.4 85.6 88.9 85.2

JGSA
ILS

DICE

the shallow UDA approaches for both kinds of features.
Comparing them with our method, we can easily ﬁnd that
MCS always beats them by a large margin in terms of the
average accuracy. Speciﬁcally, MCS ranks the ﬁrst or sec-
ond in 10 and 11 out of 12 tasks for VGG-FC6 and VGG-
FC7 features, respectively.

Note that, most previous UDA methods except
LDADA [45] favor VGG-FC6 features because they may
be not much more discriminative than VGG-FC7 features,
making them suitable for general feature transformation
based approaches. By contrast, MCS is somewhat robust
to the feature type, since the dropping rate of accuracy from
VGG-FC6 to VGG-FC7 is relatively small.

4.1.4 Results on the Ofﬁce-Home dataset

For the Ofﬁce-Home dataset, we explore PyTorch to ﬁne-
tune the ResNet-50 model [26] pre-trained on the ImageNet
dataset with the labeled source images and extract the fea-
tures after the 5-th pooling layer for each task.

LDADA [45] performs quite worse with a 2.6 average
accuracy, thus, we do not report its results in Table 3. Obvi-
ously, MCS again performs the best among all the methods
while CDAN+E [40] and DICE [36] achieves the second

and third best performance. Carefully comparing our MCS
with CDAN+E and DICE, we ﬁnd that MCS consistently
performs better than CDAN+E and DICE in 7 out of 12
tasks. Checking the results of MCS again, we ﬁnd that the
results are somewhat low when Cl is the source domain.
This may be because our method adopts the nearest cen-
troid classiﬁer which may ignore the diversity of each class
in the source domain like the ‘Clipart’ subset.

Figure 3. Accuracy improvement (%) of RevGrad [17] and
DAN [39] when integrated with MCS(ours) on Ofﬁce31 and
Ofﬁce-Home via ResNet-50. (A-D: A is Source, D is Target)

4.1.5 Parameter Sensitivity Analysis

To investigate the sensitivity of dimensionality k, parame-
ters λ and α in our method, we exploit four UDA tasks on
the Ofﬁce31 and Ofﬁce-Home datasets, i.e., A→D, A→W,
Ar→Cl, and Ar→Pr. We respectively display all these pa-
rameter sensitivity analysis results in Figures 2(a)∼2(c),
with a wide range of d ∈ [100, 110, · · · , 200] and λ, α ∈
{0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5}.

2980

Table 4. Domain generalization performance accuracy (%) on the VLCS dataset with the evaluation setup of CIDG [35].

Source Target

1NN

KPCA

DICA

Undo-bias

SCA

CIDG

MCSA(ours) MCSB(ours)

L,C,S
V,C,S
V,L,S
V,L,C

C,S
L,S
L,C
V,S
V,C
V,L

V
L
C
S

53.27 ± 1.52 58.62 ± 1.44 58.29 ± 1.51 57.73 ± 1.02 57.48 ± 1.78 65.65 ± 0.52 65.74 ± 1.26 65.17 ± 0.44
50.35 ± 0.94 53.80 ± 1.78 50.35 ± 1.45 58.16 ± 2.13 52.07 ± 0.86 60.43 ± 1.57 57.42 ± 0.48 59.95 ± 0.40
76.82 ± 1.56 85.84 ± 1.64 73.32 ± 4.13 82.18 ± 1.77 70.39 ± 1.42 91.12 ± 1.62 92.40 ± 0.62 89.30 ± 0.37
51.78 ± 2.07 53.23 ± 0.62 54.97 ± 0.61 55.02 ± 2.53 54.46 ± 2.71 60.85 ± 1.05 62.07 ± 0.82 65.16 ± 0.68
V,L 52.44 ± 1.87 55.74 ± 1.01 53.76 ± 0.96 56.83 ± 0.67 56.05 ± 0.98 59.25 ± 1.21 58.24 ± 0.44 60.42 ± 0.40
V,C 57.09 ± 1.43 58.50 ± 3.84 44.09 ± 0.58 51.16 ± 3.52 49.98 ± 1.84 55.65 ± 3.57 68.46 ± 2.66 67.19 ± 0.74
45.04 ± 2.49 45.13 ± 3.01 44.81 ± 1.62 52.16 ± 0.80 48.97 ± 1.04 54.04 ± 0.91 57.77 ± 1.29 59.73 ± 1.23
V,S
L,C 58.39 ± 0.78 64.56 ± 0.99 60.68 ± 1.36 68.58 ± 1.62 63.29 ± 1.34 70.44 ± 1.43 70.02 ± 0.32 70.43 ± 0.38
L,S 47.09 ± 2.49 55.79 ± 1.57 49.81 ± 1.40 59.00 ± 2.49 53.47 ± 0.71 61.61 ± 0.67 64.51 ± 0.54 61.46 ± 0.51
C,S 59.21 ± 1.84 63.88 ± 0.36 61.22 ± 0.95 64.26 ± 2.77 66.68 ± 1.09 70.89 ± 1.31 70.88 ± 0.51 72.12 ± 0.53

Table 5. Average accuracy (%) on the Ofﬁce31 dataset via ResNet-
50 with the evaluation setup of [44, 53, 40].

Methods

before 2018

Methods
after 2018

RevGrad DAN ADDA JAN-A RevGrad [17] DAN [39]

[64]
82.9

[39]
81.7

[44] +MCS(ours) +MCS(ours)
[17]
81.8
85.3
GTA SimNet iCAN TEM
[29]
[56]
86.5
87.2

[53]
86.2

[75]
87.2

[40]
86.6

[40]
87.7

CDAN+E

CDAN

87.4

87.8

It can be observed that MCS is robust with regard to
different values of k in Figure 2(a). For high-dimensional
features via AlexNet and ResNet-50, k = 150 is an opti-
mal choice. When λ → 0, the optimization problem is ill-
deﬁned. When λ → ∞, the minimum centroid shift is not
performed, and MCS cannot construct robust representation
for cross-domain classiﬁcation. Concerning the sensitivity
of regularization parameter λ, we plot the accuracies in Fig-
ure 2(b), which indicates that λ ∈[1,2] is an optimal choice.
Theoretically, smaller values of α can make self-learning
in the target domain (i.e., using the class mean itself) more
important in MCS. Observing the accuracy in Figure 2(c),
we can discover that α ∈[0.1,1.0] is an optimal choice un-
der which both the source centroids and the pseudo target
class means are effectively considered. As expected, larger
values of α can degenerate the adaptation performance due
to the essential heterogeneity.

Finally, we plot the accuracy w.r.t. the number of itera-
tions in Figure 2(d) where we consider the inner loop as well
as the outer loop. Obviously, the accuracy always grows
gradually until convergence within 4 outer iterations.

4.1.6 Combination with Deep UDA Methods

The comparisons with recent state-of-the-art shallow meth-
ods demonstrate the effectiveness of our method. How-
ever, we still doubt that whether deep adaptation methods
can be enhanced with our simple method. Here we inves-
tigate this problem by exploiting two popular deep domain
adaptation methods , i.e., RevGrad [17] and DAN [39] for
cross-domain object recognition and GTA [56] for cross-
domain digit recognition. Regarding these deep UDA meth-
ods [17, 39, 56], we extract the intermediate features for
both domains and take them as input for our method MCS.
As can be seen from Figure 3, once MCS is integrated
with deep UDA methods, i.e., RevGrad and DAN on the
Ofﬁce31 and Ofﬁce-Home datasets, the results of most

Table 6. Accuracy (%) cross-domain recognition tasks on three
digit based datasets (each domain using the entire training set [64,
56]). M: MNIST (60,000), U: USPS (7,291), S: SVHN (73,257).

Method

M→U U→M S→M Avg.

Source-only [56]
Source-only [56]+MCS(ours)
GTA [56]
GTA [56]+MCS(ours)

DRCN [19]
ADDA [64]
PixelDA [2]
SBADA-GAN [54]

84.6
91.5
96.5
97.8

-
-

95.9
97.6

68.9
94.5
98.1
98.2

73.7
90.1

-

60.9
82.3
89.7
91.7

82.0
76.0

-

71.5
88.4
94.8
95.9

-
-
-

95.0

76.1

89.6

S→M: LeNet (56.8), DICEMV-SVM [36] (80.9), MCS (ours, 82.6)

cases are improved in 17 and 15 out of 18 tasks, respec-
tively. Checking the average accuracy (%) in Table 5,
with the help of MCS, both the results of RevGrad and
DAN are signiﬁcantly improved, from 81.8 to 87.8 and
81.7 to 87.4. For generic cross-domain object recogni-
tion, we explore the pre-trained ResNet-50 model as our
basic network and compare it with recent state-of-the-art
methods [56, 53, 75, 29, 40] in Table 5. Speciﬁcally,
RevGrad+MCS obtains the best average accuracy, which
even beats the best result reported in CDAN+E [40].
It
can be expected that MCS can achieve much higher results
when combined with better adaptation methods like TEM.
Regarding the digit recognition task, we explore 3 pop-
i.e., MNIST [33], USPS [28] and
ular digit datasets,
SVHN [50]. Speciﬁcally, we follow the standard proto-
col that uses the corresponding entire training sets as do-
mains, and the testing sets for validation. All the images
are rescaled to 32×32. In fact, we exploit the Source-only
and GTA [56] models as a baseline network, and com-
pare them with DRCN [19], PixelDA [2], ADDA [64],
and SBADA-GAN[54] in Table 6. We easily observe that
Source-only+MCS and GTA+MCS are much better than
Source-only and GTA, respectively. Compared with recent
state-of-the-art results [19, 2, 64, 54], GTA+MCS obtains
the best results. Besides, MCS beats DICEMV-SVM with the
features provide in [36]. Regarding the per-class classiﬁca-
tion accuracy (%) in M→U and U→M, MCS scores 97.5
and 98.2 that are higher than 96.4 and 95.6 in SimNet [53].

4.2. Domain Generalization

2981

Table 7. Multi-source domain adaptation performance accuracy
(%) on the PIE dataset with the evaluation setup of StP [37].
Source 1NN StP MCSC MCSD 1NN StP MCSC MCSD 1NN StP MCSC MCSD

C05,C07

C05,C09

C05,C27

C05,C29

C07,C09

C07,C27

C07,C29

C09,C27

C09,C29

Target=C09

Target=C27

Target=C29

45.4 53.1 63.8

72.8 60.7 43.8 90.7

92.9 31.2 60.0 49.6

61.8

Target=C07

Target=C27

Target=C29

45.2 44.9 66.4

70.3 59.1 57.4 90.5

90.7 36.9 67.5 55.7

63.1

Target=C07

Target=C09

Target=C29

69.1 58.3 82.9

81.1 71.3 45.4 73.0

74.6 45.6 58.2 58.8

69.3

Target=C07

Target=C09

Target=C27

38.5 66.8 67.3

63.9 35.8 71.6 71.0

75.9 50.3 71.0 89.1

89.3

Target=C05

Target=27

Target=C29

40.8 51.8 71.0

72.3 62.0 55.7 92.2

92.2 35.3 50.1 70.2

69.2

Target=C05

Target=C09

Target=C29

55.3 46.7 87.4

86.6 74.6 51.0 71.3

80.0 43.0 51.0 68.5

73.5

Target=C05

Target=C09

Target=C27

42.4 40.1 68.4

76.4 51.6 51.0 73.4

77.0 60.0 50.3 89.4

89.5

Target=C05

Target=C07

Target=C29

52.9 51.5 83.8

82.0 70.3 55.3 84.0

86.0 41.6 71.7 71.4

72.3

Target=C05

Target=C07

Target=C27

36.4 43.1 64.0

70.0 47.5 38.5 70.1

65.9 54.2 58.4 89.1

92.8

Target=C05

Target=C07

Target=C09

C27,C29

83.6

81.0 73.4 58.0 74.5

84.9 70.3 57.1 84.6
StP (54.2) MCSC (ours, 75.2) MCSD (ours, 78.0)

53.2 47.2 83.3
Average : 1NN (51.8)
Here we conduct experiments on a real world image clas-
siﬁcation dataset VLCS like [35], including four domains:
VOC2007 (V), LabelMe (L), Caltech-101 (C) and SUN09
(S). These datasets share 5 object categories: bird, car,
chair, dog, and person. The datasets from source domains
are split into two parts: 70% for training and 30% for vali-
dation, following [35]. The whole target domain is used for
testing. We repeat the random selection 10 times and report
the mean classiﬁcation accuracy and standard deviation.

For the domain generalization task, we compute all the
total scatter matrices for each source domain and obtain St
by adding them up. Similarly, we compute all the within-
class scatter matrices for each source domain and obtain
Sw via adding them up, where each ∆r is a ﬁxed variable
by measuring the difference between different source class
means. To this end, we obtain the optimal projection ma-
trix W via Eq. (7). Note that there is no need to learn ˆYt
and ∆r, making this problem to be a one-pass algorithm.
For MCSA, each class has multiple centroids instead of one,
then we estimate ˆYt via Eq. (9). MCSB needs to know all
the source instances instead of the estimated distributional
parameters ˆµr, ˆΣr, it follows DICESVM [36] by training a
linear SVM classiﬁer on the projected source instances.

We compare MCSA and MCSB with KPCA [57],
DICA [48], Undo-bias [31], SCA [18] and CIDG [35]. As
can be seen from Table 4, both MCSA and MCSB win 4
out of 10 tasks, while the best performing baseline CIDG
merely win 2 out of 10 tasks. Besides, the standard devia-
tions are much smaller than CIDG, which indicates that our
methods are somewhat robust.

4.3. Multi source Domain Adaptation

To address the multi-source domain adaptation task, we
develop two different methods MCSC and MCSD. Specif-
ically, MCSC naively combines multiple source domains
into one source domain and becomes a vanilla domain adap-
tation task. Similar to MCSA, MCSD sums up all the

Table 8. Multi-source domain adaptation performance accuracy
(%) on the Ofﬁce31 and Ofﬁce-Caltech datasets with the evalu-
ation setups of DLD [47] and StP [37], respectively.
Dataset A,D→W A,W→D D,W→A
Method
DCTN [69]
DLD [47]
MsDA [47]
MCSC (ours)
MCSD(ours)
Method
StP [37]
MCSC (ours)
MCSD(ours)

A,C,D→W A,C,W→D A,D,W→C C,D,W→A Avg.
93.6
94.3
94.8

Avg.
83.8
83.6
84.5
85.6
86.0

99.6
93.7
94.8
98.2
99.4

54.9
62.6
62.9
62.0
61.3

96.9
94.6
95.8
96.5
97.2

96.2
96.8
100.0

Ofﬁce-
Caltech

94.5
93.5
92.4

88.7
89.2
88.3

94.9
97.6
98.6

Ofﬁce31

within-class scatter matrices and total matrices and adopt
them in Eq. (7), and assumes the optimal centroid on the tar-
get to be close to the mean of different source class means.
We utilize the PIE dataset that includes facial images
of 68 people with various pose, illumination, and expres-
sion changes. Following [41, 37], we select 5 out of 13
poses, i.e., C05 (left), C07 (upward), C09 (downward), C27
(frontal) and C29 (right). These images are cropped to the
size 32×32, constituting 1,024-dimensional features. We
compare MCSC and MCSD with StP [37] in Table 7. Gen-
erally, MCSC and MCSD achieve the best results in 7 and
19 out of 27 tasks, respectively. They signiﬁcantly outper-
form StP for almost all tasks except (C05, C09)→C29, and
MCSD performs slightly better than MCSC . This may be
because that MCSC ignores the size of different source do-
mains, which mainly relies on the larger source.

In addition, we consider the Ofﬁce31 (AlexNet fea-
tures [59]) and Ofﬁce-Caltech (DeCAF features [74, 36])
datasets with the protocols in DLD [47] and StP [37], re-
spectively. As can be seen from Table 8, we discover that
MCSD is always superior to MCSC and both of them out-
perform other methods in terms of the average accuracy.

5. Conclusion

In this paper, we have proposed a simple, efﬁcient, yet
effective approach for visual domain adaptation. The key
idea is to seek a subspace where the target centroids are
moderately shifted from those in the source domain. Then a
uniﬁed objective is designed to derive several sub-problems
with closed-form solutions to subspace discovery and tar-
get pseudo-labeling, and the alternating minimization al-
gorithm is guaranteed to converge. Note that, our method
only acquires some class-wise distribution estimators from
source data as distant supervisions, hence it also provides a
privacy-preserving way for source domain data. Besides, it
can be easily extended for domain generalization and multi-
source domain adaptation problems. Extensive experiments
on several visual benchmarks demonstrate the superiority
of the proposed method over many existing state-of-the-art
methods. Generally, our method is impressively simple and
efﬁcient, hence, it can be considered as a promising baseline
for domain adaptation and generalization tasks.

2982

References

[1] S. Baker and M. Bsat. The cmu pose, illumination, and ex-
pression database. IEEE Trans. Pattern Anal. Mach. Intell.,
25(12):1615, 2003.

[2] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks.
In Proc. CVPR, pages
3722–3731, 2017.

[3] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In Proc. NIPS, pages
343–351, 2016.

[4] P. P. Busto and J. Gall. Open set domain adaptation. In Proc.

ICCV, pages 754–763, 2017.

[5] M. Chen, Z. Xu, K. Q. Weinberger, and F. Sha. Marginal-
ized denoising autoencoders for domain adaptation. In Proc.
ICML, pages 1627–1634, 2012.

[6] B. Chidlovskii, S. Clinchant, and G. Csurka. Domain adap-
tation in the absence of source domain data. In Proc. ACM
SIGKDD, pages 451–460, 2016.

[7] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer
IEEE

machine for personalized facial expression analysis.
Trans. Pattern Anal. Mach. Intell., 39(3):529–545, 2017.

[8] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Op-
timal transport for domain adaptation. IEEE Trans. Pattern
Anal. Mach. Intell., 39(9):1853–1865, 2017.

[9] G. Csurka. Domain adaptation for visual applications: A
comprehensive survey. arXiv preprint arXiv:1702.05374,
2017.

[10] G. Csurka, B. Chidlovskii, and S. Clinchant. Adapted do-
main speciﬁc class means. In Proc. ICCV Workshop, pages
7–11, 2015.

[11] G. Csurka, B. Chidlovskii, and F. Perronnin. Domain adap-
tation with a domain speciﬁc class means classiﬁer. In Proc.
ECCV Workshop, pages 32–46, 2014.

[12] C. Ding and T. Li. Adaptive dimension reduction using dis-
criminant analysis and k-means clustering. In Proc. ICML,
pages 521–528, 2007.

[13] Z. Ding, S. Li, M. Shao, and Y. Fu. Graph adaptive knowl-
edge transfer for unsupervised domain adaptation. In Proc.
ECCV, pages 36–52, 2018.

[14] T. Evgeniou and M. Pontil. Regularized multi-task learning.

In Proc. ACM SIGKDD, pages 109–117, 2004.

[15] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-
supervised visual domain adaptation using subspace align-
ment. In Proc. ICCV, pages 2960–2967, 2013.

[16] K. Fukunaga. Introduction to statistical pattern recognition.

Academic Press Professional, Inc., 1990.

[17] Y. Ganin and V. Lempitsky. Unsupervised domain adapta-
tion by backpropagation. In Proc. ICML, pages 1180–1189,
2015.

[18] M. Ghifary, D. Balduzzi, W. B. Kleijn, and M. Zhang. Scatter
component analysis: A uniﬁed framework for domain adap-
tation and domain generalization. IEEE Trans. Pattern Anal.
Mach. Intell., 39(7):1414–1430, 2017.

[19] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classiﬁcation networks for unsu-

pervised domain adaptation. In Proc. ECCV, pages 597–613,
2016.

[20] B. Gholami, O. Rudovic, and V. Pavlovic. Punda: Probabilis-
tic unsupervised domain adaptation for knowledge transfer
across visual categories. In Proc. ICCV, pages 3601–3610,
2017.

[21] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow
kernel for unsupervised domain adaptation. In Proc. CVPR,
pages 2066–2073, 2012.

[22] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Proc. NIPS, pages 2672–2680,
2014.

[23] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation
for object recognition: An unsupervised approach. In Proc.
ICCV, pages 999–1006, 2011.

[24] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and
A. J. Smola. A kernel method for the two-sample-problem.
In Proc. NIPS, pages 513–520, 2007.

[25] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-

egory dataset. Technical report 7694, 2007.

[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proc. CVPR, pages 770–778, 2016.
[27] S. Herath, M. T. Harandi, and F. Porikli. Learning an in-
variant hilbert space for domain adaptation. In Proc. CVPR,
pages 3956–3965, 2017.

[28] J. J. Hull. A database for handwritten text recognition re-
search. IEEE Trans. Pattern Anal. Mach. Intell., 16(5):550–
554, 1994.

[29] G. Kang, L. Zheng, Y. Yan, and Y. Yang. Deep adversarial
attention alignment for unsupervised domain adaptation: the
beneﬁt of target expectation maximization. In Proc. ECCV,
pages 420–436, 2018.

[30] Z. Kang, K. Grauman, and F. Sha. Learning with whom to
share in multi-task feature learning. In Proc. ICML, pages
521–528, 2011.

[31] A. Khosla, T. Zhou, T. Malisiewicz, A. A. Efros, and A. Tor-
ralba. Undoing the damage of dataset bias. In Proc. ECCV,
pages 158–171, 2012.

[32] P. Koniusz, Y. Tas, and F. Porikli. Domain adaptation by mix-
ture of alignments of second-or higher-order scatter tensors.
In Proc. CVPR, pages 7139–7148, 2017.

[33] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proc. IEEE,
86(11):2278–2324, 1998.

[34] W. Li, L. Chen, D. Xu, and L. Van Gool. Visual recognition
in rgb images and videos by learning from rgb-d data. IEEE
Trans. Pattern Anal. Mach. Intell., 40(8):2030–2036, 2018.

[35] Y. Li, M. Gong, X. Tian, T. Liu, and D. Tao. Domain gener-
alization via conditional invariant representations. In Proc.
AAAI, pages 3579–3587, 2018.

[36] J. Liang, R. He, Z. Sun, and T. Tan. Aggregating randomized
clustering-promoting invariant projections for domain adap-
tation. IEEE Trans. Pattern Anal. Mach. Intell., 41(5):1027–
1042, 2019.

[37] H. Liu, M. Shao, Z. Ding, and Y. Fu. Structure-preserved
unsupervised domain adaptation. IEEE Trans. Knowl. Data
Eng., 31(4):799–812, 2019.

2983

[38] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-

works. In Proc. NIPS, pages 469–477, 2016.

[39] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans-
In Proc.

ferable features with deep adaptation networks.
ICML, pages 97–105, 2015.

[40] M. Long, Z. Cao, J. Wang, and M. I. Jordan. Conditional ad-
versarial domain adaptation. In Proc. NeurIPS, pages 1640–
1650, 2018.

[41] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
feature learning with joint distribution adaptation. In Proc.
ICCV, pages 2200–2207, 2013.

[42] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
joint matching for unsupervised domain adaptation. In Proc.
CVPR, pages 1410–1417, 2014.

[43] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised
domain adaptation with residual transfer networks. In Proc.
NIPS, pages 136–144, 2016.

[44] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep trans-
fer learning with joint adaptation networks. In Proc. ICML,
pages 2208–2217, 2017.

[45] H. Lu, C. Shen, Z. Cao, Y. Xiao, and A. van den Hengel.
An embarrassingly simple approach to visual domain adap-
tation. IEEE Trans. Image Process., 27(7):3403–3417, 2018.
[46] D. Luo, C. Ding, and H. Huang. Linear discriminant anal-
ysis: new formulations and overﬁt analysis. In Proc. AAAI,
pages 417–422, 2011.

[47] M. Mancini, L. Porzi, S. Rota Bul`o, B. Caputo, and E. Ricci.
Boosting domain adaptation by discovering latent domains.
In Proc. CVPR, pages 3771–3780, 2018.

[48] K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain gener-
alization via invariant feature representation. In Proc. ICML,
pages 10–18, 2013.

[49] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and
K. Kim. Image to image translation for domain adaptation.
In Proc. CVPR, pages 4500–4509, 2018.

[50] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In Proc. NIPS Workshop, 2011.

[51] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
IEEE Trans.

adaptation via transfer component analysis.
Neural Netw., 22(2):199–210, 2011.

[52] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE

Trans. Knowl. Data Eng., 22(10):1345–1359, 2010.

[53] P. O. Pinheiro. Unsupervised domain adaptation with simi-

larity learning. In Proc. CVPR, pages 8004–8013, 2018.

[54] P. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo. From
source to target and back: symmetric bi-directional adaptive
gan. In Proc. CVPR, pages 8099–8108, 2018.

[55] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-
sual category models to new domains. In Proc. ECCV, pages
213–226, 2010.

[56] S. Sankaranarayanan and Y. Balaji. Generate to adapt:
Aligning domains using generative adversarial networks. In
Proc. CVPR, pages 8503–8512, 2018.

[57] B. Sch¨olkopf, A. Smola, and K.-R. M¨uller. Nonlinear com-
ponent analysis as a kernel eigenvalue problem. Neural Com-
putation, 10(5):1299–1319, 1998.

[58] M. Sugiyama, S. Nakajima, H. Kashima, P. V. Buenau, and
M. Kawanabe. Direct importance estimation with model se-
lection and its application to covariate shift adaptation.
In
Proc. NIPS, pages 1433–1440, 2008.

[59] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy

domain adaptation. In Proc. AAAI, pages 1–8, 2016.

[60] B. Sun and K. Saenko. Deep coral: Correlation alignment for
In Proc. ECCV Workshop, pages

deep domain adaptation.
443–450, 2016.

[61] T. Tommasi and B. Caputo. Frustratingly easy nbnn domain

adaptation. In Proc. ICCV, pages 897–904, 2013.

[62] A. Torralba and A. A. Efros. Unbiased look at dataset bias.

In Proc. CVPR, pages 1521–1528, 2011.

[63] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane-
ous deep transfer across domains and tasks. In Proc. ICCV,
pages 4068–4076, 2015.

[64] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversar-
ial discriminative domain adaptation. In Proc. CVPR, pages
7167–7266, 2017.

[65] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
arXiv preprint arXiv:1412.3474, 2014.

[66] H. Venkateswara, S. Chakraborty, and S. Panchanathan.
Deep-learning systems for domain adaptation in computer
vision: Learning transferable feature representations. IEEE
Signal Process. Mag., 34(6):117–129, 2017.

[67] J. Wang, W. Feng, Y. Chen, H. Yu, M. Huang, and P. S. Yu.
Visual domain adaptation with manifold embedded distribu-
tion alignment. In Proc. ACM MM, pages 402–410, 2018.

[68] S. Xie, Z. Zheng, L. Chen, and C. Chen. Learning semantic
representations for unsupervised domain adaptation. In Proc.
ICML, pages 5423–5432, 2018.

[69] R. Xu, Z. Chen, W. Zuo, J. Yan, and L. Lin. Deep cocktail
network: Multi-source unsupervised domain adaptation with
category shift. In Proc. CVPR, pages 3964–3973, 2018.

[70] Y. Xu, S. J. Pan, H. Xiong, Q. Wu, R. Luo, H. Min, and
H. Song. A uniﬁed framework for metric transfer learning.
IEEE Trans. Knowl. Data Eng., 29(6):1158–1171, 2017.

[71] J. Yang, R. Yan, and A. G. Hauptmann. Cross-domain video
concept detection using adaptive svms. In Proc. ACM MM,
pages 188–197, 2007.

[72] B. Zadrozny. Learning and evaluating classiﬁers under sam-

ple selection bias. In Proc. ICML, 2004.

[73] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschl¨ager, and
S. Saminger-Platz. Central moment discrepancy (cmd) for
domain-invariant representation learning.
In Proc. ICLR,
2017.

[74] J. Zhang, W. Li, and P. Ogunbona. Joint geometrical and
statistical alignment for visual domain adaptation. In Proc.
CVPR, pages 5150–5158, 2017.

[75] W. Zhang, W. Ouyang, W. Li, and D. Xu. Collaborative and
adversarial network for unsupervised domain adaptation. In
Proc. CVPR, pages 3801–3809, 2018.

[76] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In Proc. ICCV, pages 2242–2251, 2017.

2984

