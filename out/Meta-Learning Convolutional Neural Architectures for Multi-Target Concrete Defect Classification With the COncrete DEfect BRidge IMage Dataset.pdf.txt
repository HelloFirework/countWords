Meta-learning Convolutional Neural Architectures for Multi-target Concrete

Defect Classiﬁcation with the COncrete DEfect BRidge IMage Dataset

Martin Mundt1∗, Sagnik Majumder1, Sreenivas Murali1∗, Panagiotis Panetsos2, Visvanathan Ramesh1∗

1. Goethe University

2. Egnatia Odos A. E.

{mmundt, vramesh}@em.uni-frankfurt.de

{majumder, murali}@ccc.cs.uni-frankfurt.de

ppane@egnatia.gr

Abstract

Recognition of defects in concrete infrastructure, espe-
cially in bridges, is a costly and time consuming crucial ﬁrst
step in the assessment of the structural integrity. Large vari-
ation in appearance of the concrete material, changing illu-
mination and weather conditions, a variety of possible sur-
face markings as well as the possibility for different types of
defects to overlap, make it a challenging real-world task. In
this work we introduce the novel COncrete DEfect BRidge
IMage dataset (CODEBRIM) for multi-target classiﬁcation
of ﬁve commonly appearing concrete defects. We investi-
gate and compare two reinforcement learning based meta-
learning approaches, MetaQNN and efﬁcient neural archi-
tecture search, to ﬁnd suitable convolutional neural network
architectures for this challenging multi-class multi-target
task. We show that learned architectures have fewer overall
parameters in addition to yielding better multi-target accu-
racy in comparison to popular neural architectures from the
literature evaluated in the context of our application.

1. Introduction

To assess a concrete bridge’s structural safety, it is de-
sirable to determine the level of degradation by accurately
recognizing all defect types. Defects tend to be small with
respect to bridge elements and often occur simultaneously
with overlap of defect categories. Although one could
imagine treating each defect category independently, over-
lapping defects are more severe with respect to the struc-
tural safety. The requirement to recognize these multi-class
multi-target defects forms the basis for a challenging real-
world task that is further complicated by a variety of envi-
ronmental factors. Concrete, as a composite material, has
a wide range of variation in surface reﬂectance, roughness,
color and, in some cases, applied surface coatings. Chang-
ing lighting conditions, weather dependent wetness of the

* work conducted while at Frankfurt Institute for Advanced Studies

surface and a diverse set of safety irrelevant surface alter-
ations like small holes, markings, stains or grafﬁti, add to
the factors of variation. This necessitates computer vision
techniques that are capable of addressing such rich appear-
ance spaces.

Deep learning techniques in conjunction with labelled
datasets have turned out to be ideal candidates for recog-
nition tasks of similar complexity. Especially convolu-
tional neural networks (CNNs) [21, 32, 1, 37, 16] have been
shown to excel at object and material recognition bench-
marks [29, 10, 35, 3]. Unfortunately, defect recognition in
concrete bridges is largely yet to beneﬁt from deep learning
approaches. Due to the necessity of expert knowledge in the
annotation process along with tedious image acquisition,
the task is traditionally focused on cracks with algorithms
based on domain speciﬁc modelling or manual inspection
by a human. Recently, datasets [31, 36, 26] and correspond-
ing deep learning applications [36, 23, 18, 8] have presented
signiﬁcant efforts towards data-driven approaches in this
domain. Their work focuses on cracks as only a subset of
structurally relevant defects and concentrates on CNNs pro-
posed in the object recognition literature, that might not be
the best choice for material defect recognition.

In this work we address two crucial open aspects of con-
crete defect recognition:
the establishment of a labelled
multi-target dataset with overlapping defect categories for
use in machine learning and the design of deep neural net-
works that are tailored to the task. For this purpose we in-
troduce our novel COncrete DEfect BRidge IMage (CODE-
BRIM) dataset and employ meta-learning of CNN archi-
tectures speciﬁc to multi-class multi-target defect classiﬁ-
cation. CODEBRIM features six mutually non-exclusive
classes: crack, spallation, efﬂorescence, exposed bars, cor-
rosion (stains) and non-defective background. Our images
were acquired at high-resolution, partially using an un-
manned aerial vehicle (UAV) to gain close-range access,
and feature varying scale and context. We evaluate a va-
riety of best-practice CNN architectures [21, 32, 1, 37, 16]
in the literature on the CODEBRIM’s multi-target defect

111196

recognition task. We show that meta-learned neural archi-
tectures achieve equivalent or better accuracies, while be-
ing more parameter efﬁcient, by investigating and compar-
ing two reinforcement learning neural architecture search
approaches: MetaQNN [2] and ”efﬁcient neural architec-
ture search” (ENAS) [27]. The CODEBRIM dataset is pub-
licly available at: https://doi.org/10.5281/zenodo.2620293
. We also make the code for training the CNN baselines
and both meta-learning techniques available open-source at:
https://github.com/MrtnMndt/meta-learning-CODEBRIM .
To summarize our contributions:

• We introduce a novel high-resolution multi-class
multi-target dataset featuring images of defects in con-
text of concrete bridges.

• We evaluate and compare best-practice CNN architec-

tures for the task of multi-target defect classiﬁcation.

• We adapt and contrast two reinforcement learning
based architecture search methods, MetaQNN and
ENAS, on our multi-target scenario. We show how re-
sulting meta-learned architectures from both methods
improve the presented task in terms of higher accuracy
and lower model parameter count.

2. Prior and related work

Image

Datasets.
classiﬁcation and object detection
benchmarks predominantly focus on the single-target sce-
nario. Popular examples are the ImageNet [29], Pascal
VOC [10] or the scene understanding SUN dataset [35],
where the task is to assign a speciﬁc class to an image, area
or pixel. Much of the recent computer vision deep learning
research is built upon improvements based on these pub-
licly available datasets. The ”materials in context” database
(MINC) [3] followed in spirit and has created a dataset for
material and texture related recognition tasks. To a large
degree MINC has extended previous datasets and applica-
tions built upon prior work of the (CUReT) database [9],
the FMD dataset [30] and KTH-TIPS [11, 5]. With respect
to defects in concrete structures, or bridges in particular,
openly available datasets remain scarce. Depending on the
defect type that needs to be recognized, our task combines
texture anomalies such as efﬂorescence or cracks with ob-
jects such as exposed reinforcement bars. Domain speciﬁc
dataset contributions were very recently proposed with the
”CrackForest” dataset [31], the CSSC database [36] and
SDNET2018 [26]. However, as all of the former works fea-
ture a single-target and in fact single-class task, we have
decided to extend existing work with the multi-class multi-
target CODEBRIM dataset.

Defect (crack) recognition. Koch et al. [20] provide a
comprehensive review on the state of computer vision in

In summary,
concrete defect detection and open aspects.
the majority of approaches follow task speciﬁc modelling.
Data-driven applications are still the exception and are yet
to be leveraged fully. Recent works [23, 8, 18] show appli-
cation to crack versus non-crack classiﬁcation using images
with little clutter and lack of structural context. An addi-
tional defect class of spalling is considered by the authors of
[36]. Similar to other works, they focus on the single-target
scenario and evaluation of well-known CNN baselines from
prior object recognition literature. We extend their work
by meta-learning more task speciﬁc neural architectures for
more defect categories and overlapping defects.

Convolutional neural networks. A broader review of
deep learning, its history and neural architecture innova-
tions is given by LeCun et al. [22]. We recall some CNN
architectures that serve as baselines and give a frame of ref-
erence for architectures produced by meta-learning on our
task. Alexnet [21] had a large success on the ImageNet
[29] challenge that was later followed by a set of deeper
architectures commonly referred to as VGG [32]. Texture-
CNN [1] is an adapted version of the Alexnet design that
includes an energy-based adaptive feature pooling and FV-
CNN [7] augments VGG with Fisher Vector pooling for tex-
ture classiﬁcation. Recent works address information ﬂow
in deeper networks by adding skip connections with resid-
ual networks [14], wide residual networks (WRN) [37] and
densely connected networks (DenseNet) [16].

Meta-learning neural architectures. Although deep
neural networks empirically work well in many practical
applications, networks have initially been designed for dif-
ferent tasks. A recent trend to bypass the human design
intuition is to treat neural architectures themselves from
a meta-learning perspective and conduct a black-box op-
timization on top of the training of weights to ﬁnd suit-
able task-speciﬁc architecture designs. Several works in
the literature have posed architecture meta-learning from
a variety of perspectives based on reinforcement learning
(RL) controllers [2, 38, 27, 4], differentiable methods [24]
or evolutionary strategies [28].
In our work, we evaluate
and adapt two RL based approaches to multi-target defect
classiﬁcation: MetaQNN [2] and ”efﬁcient neural architec-
ture search” (ENAS) [27]. We pick these two approaches as
they share underlying principles of training RL controllers.
This allows us to pick a common reward metric determined
by proposed CNN candidate accuracies. The main differ-
ences lie in the RL agents’ nature: MetaQNN employs Q-
Learning to learn to suggest increasingly accurate CNNs,
whereas ENAS uses policy gradients [34] to train an auto-
regressive recurrent neural network that samples individual
layers based on previous input.

11197

(a) Top row from left to right: 1.) exposed bars, spallation, cracks (hard to see) 2.) hairline crack with efﬂorescence 3.) efﬂorescence
4.) defect-free concrete. Bottom row from left to right: 1.) large spalled area with exposed bars and corrosion 2.) crack with grafﬁti 3.)
corrosion stain, minor onset efﬂorescence 4.) defect-free concrete with dirt and markings.

(b) From left to right: 1.) spalled area with exposed bar, advanced corrosion and efﬂorescence 2.) exposed corroded bar 3.) larger crack
4.) partially exposed corroded bars, cracks 5.) hairline crack 6.) heavy spallation, exposed bars, corrosion 7.) wet/damp crack with
efﬂorescence on the top 8.) efﬂorescence 9.) spalled area 10.) hairline crack with efﬂorescence.

Figure 1: Dataset examples. Top ﬁgure: full high-resolution images. Images heavily down-sampled for view in pdf. Bottom
ﬁgure: Image patches cropped from annotated bounding boxes (not corresponding to top images). Images resized for view
in pdf but with original aspect ratio.

3. The CODEBRIM dataset

The acquisition of the COncrete DEfect BRidge IMage:
CODEBRIM dataset was driven by the need for a more
diverse set of the often overlapping defect classes in con-
trast to previous crack focused work [31, 36, 26]. In par-
ticular, deep learning application to a real-world inspec-
tion scenario requires sampling of real-world context due
to the many factors of variation in visual defect appearance.
Our dataset is composed of ﬁve common defect categories:
crack, spallation, exposed reinforcement bar, efﬂorescence
(calcium leaching), corrosion (stains), found in 30 unique
bridges (disregarding bridges that did not have defects).
The bridges were chosen according to varying overall de-
terioration, defect extent, severity and surface appearance

(e.g. roughness and color). Images were taken under chang-
ing weather conditions to include wet/stained surfaces with
multiple cameras at varying scales. As most defects tend to
be very small one crucial requirement was the acquisition
at high-resolution. Considering that large parts of bridges
are not accessible for a human, a subset of our dataset was
acquired by UAV. We continue with the requirements and
rationale behind the camera choices, the annotation process
that led to the dataset and ﬁnally give a summary of impor-
tant dataset properties.

3.1. Image acquisition and camera choice

Image acquisition and camera choices were motivated by
typical concrete cracks in bridges having widths as small
as 0.3 mm [20]. Resolving such defects on a pixel level

11198

30 unique bridges, acquired at different scales and res-
olutions.

• 5354 annotated defect bounding boxes

(largely
with overlapping defects) and 2506 generated non-
overlapping background bounding boxes.

• Defect numbers for the following classes: crack -
2507, spallation - 1898, efﬂorescence - 833, exposed
bars - 1507 and corrosion stain - 1559.

Examples of images and extracted patches from bound-
ing boxes featuring a variety of overlapping and non-
overlapping defects can be seen in ﬁgure 1a and 1b respec-
tively. We point out that in contrast to most object and tex-
ture based benchmarks, the majority of our dataset has more
than one class occurring at once. We show a corresponding
histogram for the number of defect classes per individual
bounding box annotation in the supplementary material.

Apart from the multi-target nature making our dataset
more challenging than single-class recognition, the task is
difﬁcult because of large variations in aspect ratio, scale
and resolution of the different defects and their bounding
boxes. This is true especially at a scene level, considering
that cracks can be very ﬁne and elongated, whereas spalled
areas can vary almost arbitrarily. To illustrate these varia-
tions we visualize the distributions of defect bounding box
sizes and the sampled background bounding box sizes in
ﬁgure 2. Further details about distributions of image sizes,
bounding box size distributions per category (with overlaps
due to the multi-target nature) and distribution of aspect ra-
tios per defect can be found in the supplementary material.

4. Meta-learning convolutional neural net-
works for multi-target defect classiﬁcation

We use meta-learning to discover models tailored
to multi-target defect classiﬁcation on the CODEBRIM
dataset. In order to ﬁnd a suitable set of hyper-parameters
for the meta-learning search space and training of neu-
ral architectures we start with the T-CNN [1] and VGG-A
[32] baselines and investigate the inﬂuence of learning rate,
batch size and patch size. For this we separate the dataset
into train and validation splits and set aside a ﬁnal test set
for evaluation. We then adapt the MetaQNN [2] and ENAS
[27] architecture meta-learning approaches and contrast the
obtained results with the following set of CNN architectures
proposed in the literature: Alexnet [21], T-CNN [1], VGG-
A and VGG-D [32], wide residual network (WRN) [37]
and densely connected convolutional networks (DenseNet)
[16]. We want to point out that even though bounding box
annotations are present in our dataset, we do not evaluate
any bounding box detection algorithms because our goal
at this stage is the establishment of the already challeng-
ing multi-target classiﬁcation task. We have also evaluated

11199

Figure 2: Top panel: distribution of annotated bounding box
sizes for defects. Bottom panel: distribution of sizes for
sampled non-overlapping background bounding boxes.

imposes a strong constraint on the distance and resolution
at which the images are acquired.
In a naive calculation
for a conventional consumer-grade camera with an example
chip of size 23.50 × 15.60 mm and maximum resolution
6000 × 4000, this translates to around 0.1 mm per pixel at
a focal length of 50 mm and a distance of roughly 1.5 m
(assuming a pinhole camera model and viewing axis per-
pendicular to the surface). Based on this requirement our
dataset was gathered with four different cameras at high res-
olution and large focal lengths under varying distance and
angles. In addition, to homogeneously illuminate the darker
bridge areas, we made use of diffused ﬂash. Exact camera
models and corresponding detailed parameters can be found
in the supplementary material.

3.2. Dataset properties

We employed a multi-stage annotation process by ﬁrst
curating acquired images, annotating bounding boxes per
defect and sequentially labelling each class separately. The
rationale and exact annotation process is outlined in the sup-
plementary material. The acquisition and annotation pro-
cess resulted in a dataset with the following properties:

• 1590 high-resolution images with defects in context of

0100020003000400050006000Larger side01000200030004000Smaller sideDefect bounding box sizes025050075010001250150017502000Larger side0200400600800100012001400Smaller sideBackground bounding box sizesMulti-target accuracy [%] depending on learning rate schedule: max to min

Architecture Batch size

(cid:2)10−1, 10−5(cid:3)

(cid:2)5 · 10−2, 5 · 10−4(cid:3)

(cid:2)10−2, 10−5(cid:3)

best val

bv-test

bv-train

best val

bv-test

bv-train

best val

bv-test

bv-train

T-CNN

VGG-A

16
32
64
128

16
32
64
128

64.62
64.78
63.36
63.67

60.22
63.05
63.36
63.20

69.51
66.19
70.14
67.45

62.08
67.77
69.66
61.29

80.27
87.66
95.21
98.31

75.74
93.88
98.00
92.99

63.67
63.36
63.52
63.36

63.67
63.05
63.37
63.52

65.71
68.72
67.93
66.82

68.24
66.35
70.45
68.07

83.38
94.49
98.10
98.63

94.78
94.27
90.64
98.55

64.30
62.84
62.26
60.53

64.93
65.40
59.90
58.80

67.93
66.35
66.82
65.08

70.45
69.51
63.82
61.29

93.91
96.22
95.85
94.47

98.29
97.01
97.01
92.99

Table 1: Grid-search conducted on different batch sizes and different learning rate schedules for the T-CNN and VGG-A
models. The multi-target best validation accuracy (best val) is shown together with each model’s accuracy on the test set at
the point in time of achieving the best validation accuracy (bv-test). The analogous training accuracy (bv-train) is shown to
demonstrate that models do not under-ﬁt. These validation accuracies have been used to determine training hyper-parameters.

transfer-learning from the ImageNet and MINC datasets, al-
beit without improvements and therefore report these exper-
iments in the supplementary material.

4.1. Dataset training, validation and test splits

We have randomly chosen 150 unique defect examples
per class for validation and test sets respectively. To avoid
over-ﬁtting due to very similar context, we make sure that
we always include all annotated bounding boxes from one
image in one part of the dataset split only. An alternative
way to split the dataset is to separate train, validation and
test sets according to unique bridges. However, it is infea-
sible to balance such a split with respect to equal amount of
occurrences per defect due to individual bridges not featur-
ing defect classes uniformly (particularly with class over-
laps) and thus makes an unbiased training and reporting of
average losses or accuracies difﬁcult. Nevertheless, to in-
vestigate the importance of over-ﬁtting global properties,
we investigate and further discuss the challenges of such
splits in the supplementary material.

4.2. Training procedure

The challenging multi-class multi-target nature of our

dataset makes the following measures necessary:

1. Multi-class multi-target. For a precise estimate of a
model’s performance in a multi-target scenario, a clas-
siﬁcation is considered as correct if, and only if, all
the targets are predicted correctly. To adapt all neural
networks for this scenario we use a Sigmoid function
for every class in conjunction with the binary cross
entropy loss function. When we calculate classiﬁca-
tion accuracies we binarize the Sigmoid output with a
threshold of 0.5. Note that this could be treated as a
hyper-parameter to potentially obtain better results.

2. Variations in scale and resolution. We address the
variation in scale and resolution of bounding boxes
by following the common literature approach based on
previous datasets such as ImageNet [29] and the mod-
els presented in [21, 32, 37, 16]. Here, the smaller side
of the extracted patch is rescaled to a pre-determined
patch size and random quadratic crops of patch size are
taken to extract ﬁxed size images during training.

3. Train set imbalance. We balance the training dataset
by virtually replicating the under-represented class ex-
amples such that the overall defect number per class is
on the same scale to make sure defect classes are sam-
pled equally during training. Note that test and valida-
tion sets are balanced by design.

The reason for adopting step two is to allow for a direct
comparison with CNNs proposed in prior literature without
making modiﬁcations to their architectures. We do not use
individual class accuracies as a performance metric as it is
difﬁcult to compare models that don’t capture overlaps ad-
equately. Nevertheless we provide an example table with
multi-target versus per-class accuracy of later shown CNN
literature baselines in the supplementary material.

4.2.1 Common hyper-parameters

We conduct an initial grid-search to ﬁnd a suitable com-
mon set of hyper-parameters for CNNs (meta-learned or
not) trained with stochastic gradient descent based on the
T-CNN [1] and VGG-A [32] architectures. For this we
use learning rate schedules with warm restarts (SGDWR)
according to the work of [25]. The grid search features
three cycles with ranges inspired by previous work [25,
27]: (cid:2)10−1, 10−5(cid:3) , (cid:2)5 · 10−2, 5 · 10−4(cid:3) and (cid:2)10−2, 10−5(cid:3),

11200

a warm restart cycle length of 10 epochs that is doubled af-
ter every restart, and four different batch sizes: 128, 64, 32
and 16. All networks are trained for four warm restart cy-
cles and thus 150 overall epochs after which we have no-
ticed convergence. Other hyper-parameters are a momen-
tum value of 0.9, a batch-normalization [17] value of 10−4
to accelerate training and a dropout rate [33] of 0.5 in the
penultimate classiﬁcation layer. Weights are initialized ac-
cording to the Kaiming-normal distribution [13].

We determine a suitable set of hyper-parameters using
cross-validation, that is according to the best validation ac-
curacy during the entire training. We then report the test
accuracy based on this model. We show the multi-target ac-
curacy’s dependency on learning rate and batch size for the
two CNN architectures in table 1. We notice that the gen-
eral trend is in favor of lower batch sizes and a learning rate
schedule in the range of (cid:2)10−2, 10−5(cid:3). While the evalu-
ated best validation model’s test accuracy generally follows
a similar trend, the best test accuracies aren’t always cor-
related with a higher validation accuracy, showing a light
distribution mismatch between the splits. We further note
that the absolute best test accuracy doesn’t necessarily co-
incide with the point of training at which the model achieves
the best validation accuracy. In general, the models seem to
have a marginally higher accuracy for the test split. The
table also shows that validation and test sets are reasonably
different from the train set, on which all investigated models
achieve an over-ﬁt.

After determining a suitable set of hyper-parameters,
a batch size of 16 and a learning rate cycle between
(cid:2)10−2, 10−5(cid:3), we have proceeded with the selection of
patch sizes determined through an additional experiment
based on best multi-target validation accuracy. We again
emphasize that we do not pick hyper-parameters based on
test accuracy, even if a model with lower validation accu-
racy has a better test score.

4.2.2 Selection of patch size

Whereas most CNN architectures proposed in the literature
are designed for patch sizes of 224 × 224, we also evaluate
a range of different patch sizes by modifying the number
of parameters in the T-CNN model’s ﬁrst fully-connected
layer according to the last convolution’s spatial output reso-
lution (we do not modify the outgoing feature amounts). In
ﬁgure 3 we show the multi-target best validation and corre-
sponding test accuracies for different patch and batch sizes.
The perceivable trend is that models trained on patch sizes
smaller than 224 yield less accuracy, whereas the validation
accuracy seems to plateau or feature an upwards trend for
larger patch sizes. The corresponding test accuracies mirror
this trend. We leave the evaluation of even larger patch sizes
for future work. For the remainder of this work, we continue

Figure 3: T-CNN multi-target validation accuracy (top
panel) and best validation model’s multi-target test accuracy
(bottom panel) in dependence on patch size.

to use a patch size of 224. Although larger patch sizes seem
promising they prevent a direct comparison and contrasting
of meta-learning approaches with neural network models
proposed in the literature without making modiﬁcations to
their architectures.

4.2.3 Meta-learning speciﬁc parameters

We design the reward for both MetaQNN and ENAS to ﬁt
our multi-target scenario by setting it to the multi-target val-
idation accuracy. We re-iterate that using a per-class accu-
racy as a metric and particularly to design an RL reward,
could lead to controllers being biased towards naively rais-
ing the reward by generating models that predict (the eas-
iest) subsets of classes correctly without considering the
multi-target overlap properly. We try to set the method spe-
ciﬁc hyper-parameters of the two meta-learning methods as
similar as possible to allow for a direct comparison. We
therefore train all child CNN models using the SGDWR
schedules and SGD hyper-parameters speciﬁed earlier.

MetaQNN: We employ an ǫ-greedy schedule for the Q-
learning approach. We train an overall amount of 200 ar-
chitectures and start with a full exploration phase of 100 ar-
chitectures for ǫ = 1.0. We continue with 10 architectures
for ǫ values of 0.9 to 0.3 in steps of 0.1 and ﬁnish with 15
architectures for ǫ values of 0.2 and 0.1. Our search space

11201

96128224384512Patch size565860626466Best validation accuracy [%]T-CNN accuracy based on patch sizebatch size 16batch size 32batch size 64batch size 12896128224384512Patch size5860626466687072Best-val test accuracy [%]batch size 16batch size 32batch size 64batch size 128is designed to allow neural architectures with at least 3 and
a maximum of 10 convolutional layers. We include choices
for quadratic ﬁlters in the sizes of 3, 5, 7, 9, 11 with possible
number of features per layer of 32, 64, 128, 256. We use a
Q-learning rate of 0.1, a discount-factor of 1.0 and an initial
Q-value of 0.15. The latter is motivated by a 15% valida-
tion accuracy early-stopping criterion at the end of the ﬁrst
SGDWR cycle. In analogy to [2], if an architecture doesn’t
pass this threshold, it is discarded and a new one is sampled
and trained.

Apart from the different reward design, we also make
several extensions to the MetaQNN [2]: We cover down-
sampling with an option for convolution stride s = 2 for ﬁl-
ter sizes larger than 5. Convolutional layers are further fol-
lowed by an adaptive pooling stage using spatial-pyramidal
pooling (SPP) [12] of allowed scales 3, 4, 5 and the possi-
bility to pick a hidden fully-connected layer with size 32, 64
or 128 before adding the ﬁnal classiﬁcation stage. All lay-
ers are followed by batch-normalization and a ReLU non-
linearity to accelerate training. We also include the pos-
sibility to add ResNet-like skip connections between two
padded 3 × 3 convolutions that do not change spatial dimen-
sionality. If the number of convolutional output features is
the same the skip connection is a simple addition, whereas
an extra parallel convolution (that isn’t counted as an addi-
tional layer) is added if the amount of output features needs
to change. We make these extensions to provide a fairer
comparison to the architecture search of ENAS, that by de-
sign contains batch-normalization, adaptive pooling and the
possibility of adding skip-connections.

ENAS:
In contrast to MetaQNN where the number of lay-
ers of each architecture is ﬂexible, network depth in ENAS
is pre-determined by the speciﬁcation of number of nodes
in the directed acyclic graph (DAG). Each node deﬁnes a
possible set of feature operations that the RNN controller
samples at each step together with connection patterns. In
the process of the search, the same DAG is used to generate
architectures with candidates sharing weights through shar-
ing of feature operations. We choose to let the search evolve
through alternate training of the CNNs’ shared weights on
the CODEBRIM train set and the RNN controller’s weights
on the validation set, where the controller samples one ar-
chitecture per mini-batch. We design the DAG such that
each architecture has 7 convolutional layers and 1 classi-
ﬁcation layer that is followed by a Sigmoid function. We
choose this depth to have a direct comparison to the average
depth of MetaQNN architectures. The allowed feature op-
erations are convolutions with square ﬁlters of size 3 and 5,
corresponding depth-wise separable convolutions [6], max-
pooling and average-pooling with kernel size 3 × 3. Each
layer is followed by batch-normalization and a ReLU non-
linearity. Because ENAS uses shared weights in the search,

Figure 4: Evolution of the moving average reward deﬁned
as the multi-target validation accuracy of architectures pro-
posed through meta-learning. The top panel additionally
shows individual architecture accuracies for the MetaQNN
in color. ENAS in the bottom panel has shared model
weights during training and thus requires a ﬁnal end-to-end
re-training step for ﬁnal validation accuracies of individual
architectures.

a ﬁnal re-training step of proposed architectures is neces-
sary. We use a feature amount of 64 during the search for
all layers and use a DenseNet growth-pattern [16] of k = 2
in the ﬁnal training consistent with the work of Pham et al.
[27]. The total number of search epochs is 310 (5 SGDWR
cycles) after which we have experienced convergence of the
controller. The RNN controller consists of an LSTM [15]
with two hidden-layers of 64 features that is trained with a
learning rate of 10−3 using ADAM [19].

4.3. Results and discussion

We demonstrate the effectiveness of neural architecture
search with MetaQNN and ENAS for multi-target concrete
defect classiﬁcation on the CODEBRIM dataset. We show
respective moving average rewards based on a window size
of 20 architectures in ﬁgure 4. Individual architecture ac-
curacies for MetaQNN are shown in color for each step
in the top panel. We observe that after the initial explo-
ration phase, the Q-learner starts to exploit and architec-

11202

0255075100125150175200Architecture index565758596061Moving average reward [%] = 0.9 = 0.7 = 0.5 = 0.3 = 0.1Exploration phaseMetaQNN5052545658606264Individual architecture reward [%]050100150200250300Search epoch0102030405060Moving average reward [%]warm restartwarm restartwarm restartENASArchitecture Multi-target accuracy [%]

Params [M] Layers

best val

bv-test

Alexnet
T-CNN
VGG-A
VGG-D
WRN-28-4
Densenet-121

ENAS-1
ENAS-2
ENAS-3
MetaQNN-1
MetaQNN-2
MetaQNN-3

63.05
64.30
64.93
64.00
52.51
65.56

65.47
64.53
64.38
66.02
65.20
64.93

66.98
67.93
70.45
70.61
57.19
70.77

70.78
68.91
68.75
68.56
67.45
72.19

57.02
58.60
128.79
134.28
5.84
11.50

3.41
2.71
1.70
4.53
1.22
2.88

8
8
11
16
28
121

8
8
8
6
8
7

Table 2: Comparison of popular CNNs from the literature
with the top three architectures of MetaQNN and ENAS
in terms of best multi-target validation accuracy (best val),
best validation model’s test accuracies (bv-test), overall
amount of parameters (Params) in million and amount of
trainable layers. For WRN we use a width factor of 4 and a
growth rate of k = 32 for DenseNet.

tures improve in multi-target validation accuracy.
In the
bottom panel of the ﬁgure we show corresponding rewards
for the shared-weight ENAS DAG. We observe that both
methods learn to suggest architectures with improved accu-
racy over time. We remind the reader that in contrast to the
MetaQNN, a ﬁnal re-training step of the top architectures is
needed for ENAS to obtain the task’s ﬁnal accuracy values.
The multi-target validation and test accuracies, again re-
ported at the point in time of best validation, the number of
overall architecture parameters and layers for the top three
MetaQNN and ENAS architectures can be found in table 2.
We also evaluate and provide these values for popular CNN
baselines: Alexnet [21], VGG [32], Texture-CNN [1], wide
residual networks (WRN) [37] and densely connected net-
works (DenseNet) [16]. We see that the Texture-CNN vari-
ant of Alexnet slightly outperforms the latter. The connec-
tivity pattern of the DenseNet architecture also boosts the
performance in contrast to the VGG models. Lastly, we note
that we were only able to achieve heavy over-ﬁtting with
WRN conﬁgurations (even with other hyper-parameters and
other conﬁgurations such as WRN-28-10 or WRN-40).

The accuracies obtained by all of our meta-learned archi-
tectures, independently of the underlying algorithm, outper-
form most baseline CNNs and feature at least similar perfor-
mance in comparison to DenseNet. Moreover, they feature
much fewer parameters with fewer overall layers and are
thus more efﬁcient than their computationally heavy coun-
terparts. Our best meta-learned models have validation ac-
curacies as high as 66%, while the test accuracies go up to
72% with total amount of parameters less than 5 million.
In contrast to literature CNN baselines these architectures

are thus more tailored to our speciﬁc task and its multi-
target nature.
Interestingly, previously obtained improve-
ments from one literature CNN baseline to another on Ima-
geNet, such as Alexnet 81.8% to VGG-D 92.8% top-5 ac-
curacies, do not show similar improvements when evaluated
on our task. This underlines the need for diverse datasets in
evaluation of architectural advances and demonstrates how
architectures that were hand-designed, even with incredi-
ble care and effort, for one dataset such as ImageNet may
nonetheless be inferior to meta-learned neural networks.

Between the two search strategies we do not ob-
serve a signiﬁcant difference in performances. We be-
lieve this is due to previously mentioned modiﬁcations to
MetaQNN, mainly the addition of skip-connections and
batch-normalization that make proposed architectures more
similar to those of ENAS. We point the reader to the sup-
plementary material for exact deﬁnitions of meta-learned
architectures. There, we also include a set of image patches
that are commonly classiﬁed as correct for all targets, im-
ages where only part of the overlapping defect classes is
predicted and completely misclassiﬁed examples.

5. Conclusion

We introduce a novel multi-class multi-target dataset
called CODEBRIM for the task of concrete defect recog-
nition.
In contrast to previous work that focuses largely
on cracks, we classify ﬁve commonly occurring and struc-
turally relevant defects through deep learning. Instead of
limiting our evaluation to common CNN models from the
literature, we adapt and compare two recent meta-learning
approaches to identify suitable task-speciﬁc neural archi-
tectures. Through extension of the MetaQNN, we observe
that the two meta-learning techniques yield comparable
architectures. We show that these architectures feature
fewer parameters, fewer layers and are more accurate
than their human designed counterparts on our presented
multi-target classiﬁcation task. Our best meta-learned
models achieve multi-target
test accuracies as high as
72%. Our work creates prospects for future work such as
multi-class multi-target concrete defect detection, semantic
segmentation and system applications like UAV based
real-time inspection of concrete structures.

Acknowledgements: This project has received
funding from the European Union’s Horizon 2020
research and innovation programme under grant agreement No.
687384 ”AEROBI”. We would like to thank everyone involved in
the AEROBI project. Particular appreciation is given for the civil
engineering team of Egnatia Odos A.E. and Netivei NTIC, without
whom the annotation of the dataset wouldn’t have been possible.
We further thank FADA-CATEC, Tobias Weis and Sumit Pai for
their support in parts of the data acquisition and Hieu Pham for
valuable discussion of ENAS hyper-parameters.

11203

References

[1] Vincent Andrearczyk and Paul F Whelan. Using ﬁlter banks
in Convolutional Neural Networks for texture classiﬁcation.
Pattern Recognition Letters, 84:63–69, 2016. 1, 2, 4, 5, 8

[2] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh
Raskar. Designing Neural Network Architectures using Re-
inforcement Learning. International Conference on Learn-
ing Representations (ICLR), 2016. 2, 4, 7

[3] Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala.
Material recognition in the wild with the Materials in Con-
text Database. In Computer Vision and Pattern Recognition
(CVPR), 2015. 1, 2

[4] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun
Wang. Efﬁcient Architecture Search by Network Transfor-
mation. AAAI Conference on Artiﬁcial Intelligence (AAAI),
2018. 2

[5] Barbara Caputo, Eric Hayman, and P Mallikarjuna. Class-
speciﬁc material categorisation. In International Conference
on Computer Vision (ICCV), 2005. 2

[6] Francois Chollet. Xception: Deep Learning with Depthwise
In Computer Vision and Pattern

Seperable Convolutions.
Recognition (CVPR), pages 1800–1807, 2017. 7

[7] Mircea Cimpoi, Subhransu Maji, and Andrea Vedaldi. Deep
convolutional ﬁlter banks for texture recognition and seg-
mentation.
In Computer Vision and Pattern Recognition
(CVPR), 2015. 2

[8] Wilson R. L. da Silva and Diogo S. de Lucena. Concrete
Cracks Detection Based on Deep Learning Image Classiﬁca-
tion. In International Conference on Experimental Mechan-
ics (ICEM18), 2018. 1, 2

[9] Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and
Jan J. Koenderink. Reﬂectance and texture of real-world sur-
faces. ACM Transactions on Graphics (TOG), 18(1):1–34,
1999. 2

[10] Mark Everingham, S. M.Ali Ali Eslami, Luc Van Gool,
Christopher K.I. I Williams, John Winn, and Andrew Zisser-
man. The Pascal Visual Object Classes Challenge: A Retro-
spective. International Journal of Computer Vision (IJCV),
111(1):98–136, 2014. 1, 2

[11] Eric Hayman, Barbara Caputo, Mario Fritz, and Jan-Olof
Eklundh. On the Signiﬁcance of Real-World Conditions for
Material Classiﬁcation.
In European Conference on Com-
puter Vision (ECCV), 2004. 2

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Spatial Pyramid Pooling in Deep Convolutional Networks
for Visual Recognition.
In European Conference on Com-
puter Vision (ECCV), pages 346–361, 2014. 7

[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In International Conference on Computer Vi-
sion (ICCV), pages 1026–1034, 2015. 6

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learn-
ing for Image Recognition. In Computer Vision and Pattern
Recognition (CVPR), 2016. 2

[15] Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term

Memory. Neural Computation, 9(8):1735–1780, 1997. 7

[16] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. In Computer Vision and Pattern Recognition (CVPR),
pages 2261–2269, 2017. 1, 2, 4, 5, 7, 8

[17] S. Ioffe and C. Szegedy. Batch Normalization: Accelerat-
ing Deep Network Training by Reducing Internal Covari-
ate Shift. In International Conference on Machine Learning
(ICML), volume 37, pages 448–456, 2015. 6

[18] Hyunjun Kim, Eunjong Ahn, Myoungsu Shin, and Sung-
Han Sim. Crack and Noncrack Classiﬁcation from Concrete
Surface Images Using Machine Learning. Structural Health
Monitoring, 2018. 1, 2

[19] Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method
for Stochastic Optimization. In International Conference on
Learning Representations (ICLR), 2015. 7

[20] Christian Koch, Kristina Georgieva, Varun Kasireddy, Burcu
Akinci, and Paul Fieguth. A review on computer vision based
defect detection and condition assessment of concrete and
asphalt civil infrastructure. Advanced Engineering Informat-
ics, 29(2):196–210, 2015. 2, 3

[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. ImageNet Classiﬁcation with Deep Convolutional Neu-
ral Networks.
In Neural Information Processing Systems
(NeurIPS), volume 25, pages 1097–1105, 2012. 1, 2, 4, 5, 8
[22] Yann Lecun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. Nature, 521(7553):436–444, 2015. 2

[23] Yundong Li, Hongguang Li, and Hongren Wang. Pixel-wise
crack detection using deep local pattern predictor for robot
application. Sensors, 18(9), 2018. 1, 2

[24] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS:
International Confer-

Differentiable Architecture Search.
ence on Learning Representations (ICLR), 2019. 2

[25] I. Loshchilov and F. Hutter. SGDR: Stochastic Gradient De-
scent With Warm Restarts. In International Conference on
Learning Representations (ICLR), 2017. 5

[26] Marc Macquire,

Sattar Dorafshan,

and Robert

dataset

SDNET2018:

Thomas.
age
https://digitalcommons.usu.edu/all datasets/48
cess: 06.11.18), Paper 48, 2018. 1, 2, 3

for machine

A concrete

learning

J.
crack im-
applications.
ac-

(last

[27] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and
Jeff Dean. Efﬁcient Neural Architecture Search via Parame-
ters Sharing. In International Conference on Machine Learn-
ing (ICML), 2018. 2, 4, 5, 7

[28] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Sax-
ena, Yutaka Leon Suematsu, Quoc Le, and Alex Kurakin.
Large-Scale Evolution of Image Classiﬁers. In International
Conference on Machine Learning (ICML), 2017. 2

[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. In International Journal of Computer Vision (IJCV),
volume 115, pages 211–252, 2015. 1, 2, 5

[30] Lavanya Sharan, Ruth Rosenholtz, and Edward H. Adelson.
Material perception: What can you see in a brief glance?
Journal of Vision (JOV), 9(8), 2009. 2

11204

[31] Yong Shi, Limeng Cui, Zhiquan Qi, Fan Meng, and Zhen-
song Chen. Automatic road crack detection using random
structured forests. IEEE Transactions on Intelligent Trans-
portation Systems (T-ITS), 17(12):3434–3445, 2016. 1, 2,
3

[32] K. Simonyan and A. Zisserman. Very Deep Convolutional
Networks for Large-Scale Image Recognition.
In Inter-
national Conference on Learning Representations (ICLR),
2015. 1, 2, 4, 5, 8

[33] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout : A Simple
Way to Prevent Neural Networks from Overﬁtting. Jour-
nal of Machine Learning Research (JMRL), 15:1929–1958,
2014. 6

[34] S. Richard Sutton, David McAllester, Satinder Singh, and
Yishay Mansour. Policy Gradient Methods for Reinforce-
ment Learning with Function Approximation. In Neural In-
formation Processing Systems (NeurIPS), pages 1057–1063,
1999. 2

[35] Jianxiong Xiao, James Hays, Krista A Ehinger, and Antonio
Torralba. SUN Database : Large-scale Scene Recognition
from Abbey to Zoo. In Computer Vision and Pattern Recog-
nition (CVPR), 2010. 1, 2

[36] Liang Yang, Bing Li, Wei Li, Zhaoming Liu, Guoyong Yang,
and Jizhong Xiao. Deep Concrete Inspection Using Un-
manned Aerial Vehicle Towards CSSC Database. In Interna-
tional Conference on Intelligent Robots and Systems (IROS),
2017. 1, 2, 3

[37] Sergey Zagoruyko and Nikos Komodakis. Wide Residual
Networks. In British Machine Vision Conference (BMVC),
pages 87.1–87.12, 2016. 1, 2, 4, 5, 8

[38] Barret Zoph and Quoc V. Le. Neural Architecture Search
with Reinforcement Learning. In International Conference
on Learning Representations (ICLR), 2017. 2

11205

