A Cross-Season Correspondence Dataset for Robust Semantic Segmentation

M˚ans Larsson1 Erik Stenborg1,3 Lars Hammarstrand1 Mark Pollefeys2,4 Torsten Sattler1

1Chalmers University of Technology

2Department of Computer Science, ETH Zurich

3 Zenuity

4 Microsoft

Fredrik Kahl1

Abstract

In this paper, we present a method to utilize 2D-2D point
matches between images taken during different image con-
ditions to train a convolutional neural network for seman-
tic segmentation. Enforcing label consistency across the
matches makes the ﬁnal segmentation algorithm robust to
seasonal changes. We describe how these 2D-2D matches
can be generated with little human interaction by geomet-
rically matching points from 3D models built from images.
Two cross-season correspondence datasets are created pro-
viding 2D-2D matches across seasonal changes as well as
from day to night. The datasets are made publicly avail-
able to facilitate further research. We show that adding the
correspondences as extra supervision during training im-
proves the segmentation performance of the convolutional
neural network, making it more robust to seasonal changes
and weather conditions.

1. Introduction

Semantic segmentation is the task of assigning a class
label to each pixel in an image and is one of the funda-
mental problems in computer vision. Semantic segmen-
tation has also been used to integrate higher-level scene
understanding into other computer vision problems, e.g.,
dense 3D reconstruction [6, 13, 14, 23, 28, 31, 55, 56, 61],
SLAM [7, 35], Structure-from-Motion [3], 3D model align-
ment [15, 16, 73], and location recognition [1, 42, 60].

Visual localization is the problem of estimating the cam-
era pose of an image [8, 34], typically from a set of 2D-3D
matches between image pixels and 3D scene points. In the
context of long-term visual localization [58, 63–65, 74], se-
mantics are proven useful to be able to handle variations
in scene geometry and appearance, e.g., due to seasonal
changes. These methods are based on the idea that the se-
mantic meaning of a scene part is invariant to such changes.
Semantics are thus used to establish the 2D-3D matches re-
quired for pose estimation when matching solely based on
image appearance fails.

Figure 1. 2D-2D matches between images taken under different
conditions, established using matches against a 3D point cloud.

As of yet, however, the assumption that the same seman-
tic segmentation can be reliably reproduced under different
conditions does not hold. This is mainly due to that the
labeled datasets used to train the semantic segmentation al-
gorithm itself are limited to only a few conditions. As the
pixel-level annotations are performed by hand, adding more
training data is both time consuming and expensive [17,44].
However, even rather noisy segmentations improve local-
ization performance compared to not using semantic infor-
mation [58,65]. This naturally leads to the question whether
it is possible to build a feedback loop: Can semantic seg-
mentation algorithms be improved via visual localization?
Can this in turn lead to more robust localization results?

As part of estimating the camera pose of an image taken
under one condition against a 3D model built from images, a
set of 2D-3D matches is established. Usually, the reference
3D model is built from images [54]. Thus, the 2D-3D corre-
spondences lead to a set of 2D-2D matches between images
taken under different conditions, as depicted in Fig. 1. In
this paper, we show that these pixel-level matches can be
used to improve semantic segmentation algorithms.

In detail, this paper makes the following contributions:
1) A 2D-2D match between two images taken under differ-

9532

ent conditions provides a constraint on the training process,
namely that these two pixels should receive the same label.
We use this insight to formulate a loss function that can be
added to the training process, without the need to modify
the architecture of the segmentation algorithm. 2) We show
that the required set of correspondences can be generated
with little human supervision, albeit without ground truth
labels. This is in stark contrast to creating labeled train-
ing datasets, where signiﬁcant human effort is required [49].
We make our cross-season correspondence dataset publicly
available1. 3) We show that using our correspondence-
based loss, together with a few coarsely annotated images
required to prevent trivial solutions, can lead to signiﬁ-
cant improvements in segmentation quality in the context of
changing imaging conditions. The improvements are espe-
cially signiﬁcant if the base training set covers only a single
condition.

2. Related Work

Semantic Segmentation. The performance of semantic
segmentation algorithms has seen a large increase during
the last few years based on the advances of deep neural
networks. The seminal work of Long et al. [37] showed
that convolutional neural networks (CNNs), initially trained
for classiﬁcation, can be transformed to fully convolutional
networks (FCNs) for semantic segmentation. Follow up
work has improved upon FCNs by for example enlarging
the receptive ﬁeld [10, 72], incorporating higher level con-
text [76] or fusing multi-scale features [11, 50]. In addition,
the combination of FCNs and structure models, such as con-
ditional random ﬁeld (CRF), have been thoroughly studied,
either as a post-processing step [10] or as part of the net-
work [33, 36, 77], enabling end-to-end training.

Training these networks requires a large amount of an-
notated images, which for semantic segmentation can be
costly and time-consuming to acquire. In response many
weakly supervised approaches have been suggested utiliz-
ing labels in the form of bounding boxes [19, 29, 45], image
level tags [45, 47, 48, 62] or points [4]. Similarly, we im-
prove the segmentation performance of a FCN by utilizing
supervision that requires less manual effort to acquire than
pixel-level annotations. However, instead of using weaker,
but still manually annotated, labels we propose the use of
data that can be acquired in a semi-automatic fashion.

Domain Adaptation. Domain adaptation methods aim at
learning a model that performs well in the target domain,
given that there are only available annotations in the source
domain. Early work includes [30, 52] that transforms the
features to either a domain invariant feature space [52] or
the source feature domain [30]. Several works have fo-
cused on domain adaptation for CNN models [21,38,39,67].

1https://visuallocalization.net

These methods learn models that produce domain invariant
features, either by aligning the target and feature distribu-
tions [38, 39] or by using an adversarial training setup, en-
couraging domain confusion [21, 67].

Recently, several domain adaptation methods for dense
prediction tasks have been presented [12, 26, 53, 66, 68, 69,
78]. Most of these [12, 25, 26, 43, 53, 66, 78] use synthetic
datasets, e.g., [49, 51], enabling automatic generation of
large amount of annotated synthetic images. The methods
presented in [25, 43, 53] all use some form of image trans-
lation method such as to transform the source images into
the target domain before performing segmentation. Another
common approach is to use an adversarial training setting as
in [26, 68, 69] where the network is encouraged to produce
features that fool a domain discriminator.

We are interested in increasing the performance of our
segmentor on images different from the source domain.
Speciﬁcally, the different conditions and seasons included
in our correspondences can be seen as target domains.
Rather than using unsupervised domain adaption, we use
3D geometric consistency as a supervisory signal. Our
cross-season correspondence datasets facilitate the adaption
of the segmentation method across the different target do-
mains, removing the need to rely on purely unsupervised
domain adaptation methods.

Semantic 3D Mapping. Semantic 3D reconstruction ap-
proaches [6,13,14,23,28,31,41,55,56,61] uses semantic im-
age segmentations to aid the reconstruction process. They
use a voxel volume to represent the scene and jointly reason
about geometric and semantic occupancy. Using semantics
typically leads to more consistent and complete 3D mod-
els. These 3D models can be projected into the images to
obtain reﬁned semantic segmentations [28]. However, the
semantic reconstruction process is signiﬁcantly more com-
plex compared to the multi-view stereo process we use to
obtain correspondences. Methods exists that jointly predict
depth and semantics [20, 32] or use depth information to
aid semantic segmentation [24]. Yet, they still rely on la-
belled data. We are not aware of any work using pixel cor-
respondences obtained via 3D models to create additional
constraints for semantic segmentation.

Datasets for 3D and semantics. Large databases for in-
door [9, 18, 70] and outdoor scenes [22, 27] provide both
semantics and 3D geometry and can thus also be used to
semantically annotated images via geometry [71]. Yet, we
are not aware of any such dataset that captures different sea-
sonal and illumination conditions. In this sense, our work
closes a gap in the literature.

3. Semantic Correspondence Loss

The rationale behind using 2D-2D image correspon-
dences is that the CNN initially, being pre-trained on a

9533

large-scale dataset such as Cityscapes [17], performs well
on images taken during favourable conditions (i.e. similar
conditions as in the training set). We can then use the cor-
respondence data to enforce labeling consistency between
images captured during favourable conditions and images
captured during challenging conditions. To this end we de-
ﬁne and test two different loss functions based on the hinge
loss and the cross-entropy loss that will encourage labeling
consistency. The losses are designed for a CNN where the
value of intermediate feature layers can be extracted and
where the ﬁnal output is an estimate of the probability dis-
tribution over the class labels for each input pixel.

We denote the content of one sample from the cross-
season correspondence dataset as (I r, I t, xr, xt). Here I r
is an image from the reference traversal, I t an image from
the target traversal, and xr as well as xt are the pixel posi-
tions of the matched points in the reference and target im-
ages, respectively. The reference traversal is chosen as the
one with images captured during the most favourable image
condition. Note that the reference images are taken from
the same traversal while the target images vary between all
other available traversals. The correspondence loss function
Lcorr will be a sum over all such samples

Lcorr = X(r,t)

l(I r, I t, xr, xt) ,

(1)

where l is a hinge loss lhinge or a cross-entropy loss lCE as
presented below.

Let dx ∈ RF denote a feature vector of the segmentation
CNN of length F at pixel position x. This can be either the
last layer of the network, where F equals the number of
classes or an earlier, intermediate feature layer. We deﬁne
the correspondence hinge loss lhinge for one sample as

lhinge =

1
N

N

Xi=1

max 0, m −

d

dT
xt
xr
i kkd
kdxr

i

i

xt

i

k! ,

(2)

where m is a margin parameter and N is the number of
corresponding points. The loss will encourage the feature
vectors dxr
to align up to a certain angle depending
on m. In the experiments, we have found empirically that
setting m = 0.8 (approximately 37◦) works well.

i and d

xt

i

For the correspondence cross-entropy loss lCE, we begin
by taking the argument of the maximum of the ﬁnal feature
map, i.e. the most likely class, of the reference image. By
describing the most likely class for a pixel at position xi us-
ing an one-hot encoding vector cxi , the loss can be written
as

lCE = −

1
N

N

Xi=1

cT
xr

i

log(cid:16)d

xt

i(cid:17) ,

(3)

where log(·) is taken element-wise. The loss will encourage
the pixels in the target image to have the same labels as the
corresponding pixels in the reference image.

During training, we minimize a loss consisting of one
term for the fully supervised data based on standard cross-
entropy Lsup as well as one correspondence term Lcorr.
The resulting overall loss is L = Lsup + λLcorr, where λ
is a weighting term for the impact of the correspondences.

4. A Cross-Season Correspondence Dataset

This section describes the creation and the content of
the cross-season correspondence dataset. Each sample of
the dataset contains two nearby images taken during differ-
ent seasons or weather conditions as well as a set of 2D-
2D point correspondences between the images. The corre-
spondences are automatically established using geometric
3D consistency between the two points. Geometry is typi-
cally more stable than for instance photometric information
across the different conditions.

A visualization of a few samples can be seen in Fig. 2.
Using the datasets presented in [54] as a starting point, we
create two correspondence datasets. The datasets from [54]
used were originally based on the CMU Visual Localization
dataset [2] and the the RobotCar dataset [40] respectively.

The creation of the correspondence dataset can be di-
vided into four main steps. Firstly, the camera poses for all
images in all conditions need to be calculated in a common
coordinate system. In our case these were kindly provided
by the authors of [54]. Secondly, a dense 3D point cloud of
the surrounding geometry is created, individually for each
condition and traversal. Thirdly, the 3D point clouds are
matched across the conditions. Since the point clouds share
the same coordinate system, this matching can be done us-
ing the position of the 3D points. This removes the need for
feature descriptors which might change substantially across
the different conditions. Lastly, given the 3D point cloud
matches, the pixel positions for the 2D-2D correspondences
in each image can be calculated using the known camera
positions. Each step will be detailed separately for the two
datasets below.

4.1. CMU Seasons Correspondence Dataset

The CMU Visual Localization dataset [2] was collected
over a period of 12 months in Pittsburgh, USA. A vehi-
cle with two cameras facing forward/left and forward/right
drove a 8.5 km long route through central and suburban
areas. We use the camera poses from twelve traversals
during different seasons and weather condition available in
the CMU Seasons dataset [54]. The camera poses have
been calculated using bundle adjustment of SIFT points and
some manually annotated image correspondences across
different traversals, for further details we refer to [54]. This
method gives accurate camera positions, where the position
error is estimated to be under 0.10 m.

To create the dense point cloud for each traversal, we use
the Multi-View Stereo (MVS) pipeline presented in [59].

9534

Figure 2. Visualization of a sample from our cross-season correspondence datasets. Left: CMU, right: RobotCar. Each purple point marks
the pixel position of a correspondence in the respective images. Green lines are drawn to the matching point in the other image. Note that
lines are only drawn for every 50 point pair to avoid cluttering the images.

The MVS pipeline pipeline works in two steps. Firstly,
depth and normal information is estimated using geomet-
ric and photometric information. Secondly, the depth and
normal maps are fused forming a dense point cloud. This
was done using the software Colmap [57, 59] with default
settings. Examples of 3D point clouds from the CMU Sea-
sons dataset can be seen in Fig. 1.

To ﬁnd correspondences between images from two dif-
ferent traversals, we perform geometric matching between
the two corresponding 3D point clouds. Given a reference
and a target traversal, we take the two corresponding dense
3D point clouds and match points using the Euclidean dis-
tance: For each point in the ﬁrst point cloud, we search
for the nearest neighboring point in the other point cloud
and vice versa. We establish a correspondence between two
points if they are mutual nearest neighbors.

For the resulting matches, we check from which cameras
each matched point was triangulated during MVS, giving us
the cameras where the point is visible. We then go through
each camera pair, where one is from the reference and one
from the target traversal and investigate the number of com-
mon 3D points that can be seen in the cameras. For pairs
with at least 500 common points and where the distance
between the two cameras is less than 0.5 m, we do an ad-
ditional pruning step to get rid of poor matches. The prun-
ing is necessary since the ﬁrst 3D matching step does not
enforce any constraint that the points need to be close too
each other. In addition, enforcing a closeness constraint for
each camera pair enables us to use a distance threshold that
depends on the distance between the point and the cameras.
This is beneﬁcial since 3D points close to the camera usu-
ally are reconstructed with greater precision. For a match to
be kept, the distance between the two points must be below
a threshold that depends on the distance between the point
and the camera as follows

||X1 − X2|| < κD .

(4)

Here X1 and X2 are the positions of the matched 3D points
in the reference and target traversals, respectively, D is the
distance from the camera center to X1 and κ is a unitless
parameter set to 0.01. This means that points that are 10 m

Condition

Image pairs Average N

Sunny + Foliage
Sunny + Foliage
Cloudy + Foliage
Sunny + Foliage

Overcast + Mixed Foliage
Low Sun + Mixed Foliage
Low Sun + Mixed Foliage
Cloudy + Mixed Foliage

Low Sun + No Foliage + Snow

Overcast + Foliage

3185
2200
3312
3620
3300
3286
3384
2729
2022
1728

14361
17696
17711
18373
13770
15441
16081
14111
19060
20090

Table 1. Statistics of the CMU Cross-Season Correspondence
dataset. Each row shows the condition, the number of image pairs
as well as the average number of correspondences per image pair
for each traversal. Note that there are several traversals with the
same condition.

from the camera need to be closer than 0.1 m to be kept as a
correspondence. Table 1 provides a summary of the content
of the ﬁnal CMU Cross-Season Correspondence dataset.

4.2. Oxford RobotCar Correspondence Dataset

The original RobotCar dataset [40] was gathered using
an autonomous vehicle that traversed a 10 km route in Ox-
ford, UK during 12 months. Similarly as for the CMU
dataset, we use the camera poses from ten traversals dur-
ing different seasons and weather conditions available in the
RobotCar Seasons dataset provided by the authors of [54].
For the reference traversal these camera poses were initial-
ized using a GPS/INS system and reﬁned by iteratively tri-
angulating 3D points and performing bundle adjustment.
For the other traversals, the camera poses were calculated
using 3D points clouds built from the measurements of the
LIDAR scanners mounted on the vehicle. The LIDAR point
clouds for each traversal were aligned to the LIDAR point
cloud of the reference traversal using the Iterative Closest
Point algorithm [5] and manual corrections when necessary.
The images included in the RobotCar Seasons dataset
are recorded using three synchronized global shutter Point
Grey Grasshopper2 cameras mounted to the left, rear, and
right of the car. Unfortunately the image quality is poor in
general. A lot of the images are overexposed and there is
a lot of motion blur present. In addition there is also a lot
of image noise for the night time images. This makes the
MVS pipeline that we used for the CMU dataset produce

9535

Condition

Image pairs Average N

Dawn
Dusk
Night

Night + Rain

Summer + Overcast
Winter + Overcast

Rain
Snow
Sun

772
646
646
780
809
671
683
823
681

59158
50159
52238
43066
51722
54466
53276
57578
48150

Table 2. Statistics of the RobotCar Cross-Season Correspondence
dataset. Each row shows the condition, the number of image pairs
s as well as the average number of correspondences per image pair
for each traversal. Note that there are several traversals with the
same condition.

point clouds with too few points to be of any use for us. We
instead use the LIDAR point clouds available in the original
RobotCar dataset. Since we know the pose of the multi-
camera system at each timestamp, as well as the poses of
the individual cameras and LIDAR sensors on the car, we
can transform the LIDAR point clouds into the coordinate
system of the cameras. We then determine which points are
visible in which cameras in a separate step. To determine
which points are visible in each camera the depth of points
projected close to each other in the image is compared.

The matching of the 3D points and pruning of the cor-
respondences are done in the same way as for the CMU
Seasons dataset. However, since we do not have as many
images for the RobotCar Seasons dataset, we use a larger
threshold for the distance between camera pairs, speciﬁcally
2.0 m. Table 2 provides a summary of the content of the ﬁ-
nal RobotCar Cross-Season Correspondence dataset.

5. Implementation Details

During the training of the CNN, we minimize the loss
L described in Section 3. As a starting point, we use the
PSPNet [76] network pretrained on the Cityscapes dataset
[17]. In addition to the Cityscapes training images, we also
add a few coarsely annotated images from the CMU and
RobotCar Seasons datasets, respectively. Some examples
of these annotations are shown in Fig 3. This is necessary
to keep the CNN from learning the trivial solution where
the same class is predicted for all pixels on the CMU and
RobotCar images while still producing good segmentations
for the Cityscapes images. Note that only training images
with ﬁne annotations from the Cityscapes dataset are used.
We also add an on-the-ﬂy correspondence reﬁnement
step, where all correspondences that have their pixel posi-
tions in the reference image classiﬁed as one of the non-
stationary classes are removed. The classes concerned are
person, rider, car, truck, bus, train, motorcycle, and bicy-
cle. For a correspondence, if the pixel in the reference im-
age is classiﬁed as a non-stationary class it means one of
two things: The pixel actually depicts a non-stationary ob-
ject and has been incorrectly added to the correspondence

Figure 3. Examples of the manually labeled images added to the
training set. The top row shows images for the CMU Seasons
dataset (66 images in total) while the bottom row shows images
for the RobotCar Seasons dataset (40 images in total).

dataset, i.e., there is no guarantee that the corresponding
pixel in the target image has the same semantic class. The
other explanation is that the CNN incorrectly classiﬁed the
pixel. Adding the loss for said correspondence would not
be useful for any of these scenarios. Additionally, we use a
warm-up period of 500 iterations before the correspondence
loss is added. This ensures that the CNN has produces rea-
sonable segmentations for the reference images.

The optimization method used during training is
Stochastic Gradient Descent with momentum and weight
decay. During all experiments the learning rate was set to
2.5 · 10−5, while the momentum and weight decay were
set to 0.9 and 10−4, respectively. In addition, the loss was
scaled with 1
1+λ to keep the total loss weight to unity. Due
to GPU memory limitations we train using a batch size of
one. We train the networks for at least 30000 iterations
and use the weights that got the best mean Intersection over
Union (mIoU) on the validation set. For the RobotCar and
CMU Seasons validation and test sets, we take the mean
over only classes present in the respective dataset when cal-
culating the mIoU. The number of validation images for the
CMU Seasons and Robotcar were 25 and 27 respectively.
The corresponding numbers for the test sets were 33 and 27.
The training and evaluation is implemented in PyTorch [46]
and the code is publicly available 2.

All evaluation and testing is done in patches on the origi-
nal image scale only. The patch size is 713 × 713 pixels and
the patches are extracted from the image with a step size
of 476 pixels in both directions. The output of the network
is paired with an interpolation weight map that is 1 for the
236×236 center pixels of the patch and drops off linearly to
0 at the edges. For each pixel the weighted mean, using the
interpolation maps as weights, of all patches that contain it

2https://github.com/maunzzz/

cross-season-segmentation

9536

λ

0.01
0.05
0.1
0.5
1.0
2.0
5.0

CE
63.3
64.4
65.6
65.0
67.2
66.0
66.7

HingeC HingeF

62.0
62.6
63.2
62.3
64.0
63.3
62.6

62.2
62.1
62.3
61.9
60.6
59.5
59.4

Table 3. Parameter study of λ for the loss L = Lsup + λLcorr
(cf . Section 3) on the CMU dataset. The correspondence losses
included are cross-entropy loss lCE and hinge loss lhinge applied
to the ﬁnal and second to ﬁnal features, HingeC and HingeF , re-
spectively. All values are mIoU (in %) for the CMU validation
set.

is used to produce the pixel’s class scores. The motivation
behind the interpolation is that the network generally per-
forms better at the center of the patches, since there is more
information about the surroundings available there.

6. Experimental Evaluation

In this section, we present the results of using the Cross-
Season Correspondence Dataset to train a CNN for segmen-
tation. The two main points of interest are a), how does the
Cross-Season Correspondence Dataset inﬂuence the seg-
mentation performance on the images within the dataset,
i.e., the CMU and RobotCar images and b), how does using
the dataset inﬂuence the generalization performance of the
segmentor to other datasets. To investigate a), we manually
annotate a set of test images from the RobotCar and CMU
datasets taken from areas that are not included in the cor-
respondence datasets. To answer b), we use the WildDash
dataset [75]. The WildDash dataset is designed to evaluate
the robustness of segmentation methods and contains many
different and challenging images.

We also investigate the effect of training with correspon-
dences together with different amount of annotated train-
ing images. To this end we do experiments both using just
the Cityscapes dataset [17] training set as well as, on top
of that, adding the training set from the Mapillary Vistas
dataset [44]. The Cityscapes training set contains 2975 an-
notated images taken under favorable weather conditions
and in similar environments while the Vistas training set
contains 18000 images from a diverse set of environments,
seasons and weather conditions [44]. As the Vistas dataset
contains many more annotated classes than the Cityscapes
dataset, we only consider the subset of classes which are
in both datasets and treat the rest as unlabeled during train-
ing. For all segmentation experiments, we investigate three
correspondence loss functions: for the ﬁrst we use the cor-
respondence cross-entropy loss lCE applied to the ﬁnal fea-
tures. For the other two, we apply the correspondence hinge
loss lhinge to the ﬁnal and second to ﬁnal layer, respectively.

Parameter Study.

The parameter λ speciﬁes the trade-

Extra

Corr

S
C

s
a
t
s
i
V
+
S
C

X

X

X

X

X

X

X

X

CE

HingeC
HingeF

CE

HingeC
HingeF

CMU

CMU WD
16.4
31.2
37.0
73.6
39.6
79.3
37.9
72.4
38.7
75.3
49.3
77.4
51.5
82.8
52.9
85.9
82.2
54.9
84.0
54.5

RobotCar
RC WD
16.4
22.2
25.4
45.8
53.8
27.8
25.2
50.6
55.4
27.9
46.8
51.2
59.4
59.0
58.7

49.2
46.4
48.5
47.9
45.6

Table 4. Segmentation results for the models trained on the CMU
correspondence data (left) and the RobotCar correspondence data
(right). Results for the CMU test set, the RobotCar (RC) test set
and the WildDash (WD) validation set are shown in terms of mIoU
(in %). For the bottom ﬁve rows the Vistas training set was used in
addition to Cityscapes (CS). Column one marks if the extra train-
ing annotations from the CMU/RobotCar dataset were used. Col-
umn two speciﬁes the correspondence training loss used, i.e., cor-
respondence cross-entropy loss (CE) applied to the ﬁnal layer and
hinge loss applied to the ﬁnal and second to ﬁnal features, HingeC
and HingeF , respectively.

off between the fully supervised cross-entropy loss (Lsup)
for the annotated training set and the correspondence loss
(Lcorr). A higher λ means that more emphasis is put on
minimizing the correspondence loss compared to the fully
supervised loss. To investigate the impact of λ, we perform
a parameter study which is summarized in Table 3. The
results presented are the mIoU on the CMU validation set.
As can be seen from the table, the best choice of λ for the
correspondence cross-entropy loss (CE) and the hinge loss
on the ﬁnal features (HingeC ) is λ = 1.0 while for the hinge
loss applied to the second to ﬁnal feature layers (HingeF ) it
is λ = 0.1. We hence choose these values for the remaining
experiments.

Segmentation. Table 4 summarizes the results of the seg-
mentation experiments on the CMU as well as the RobotCar
datasets. For the networks trained on the CMU dataset, the
mIoU on the CMU test set and the WildDash validation set
is presented for several baselines as well as networks trained
with the correspondence dataset. For the networks trained
on the RobotCar dataset the RobotCar test set is used in-
stead of the CMU test set. Comparing the two baselines,
there is a large difference in performance between the net-
work trained with both Cityscapes and Vistas compared to
the one trained with Cityscapes only. This is to be expected
since the Vistas dataset has much more diversity when it
comes to seasons and weather conditions, enabling a net-
work trained on it to generalize well to the CMU or Robot-
Car test set.

When investigating the effect of the correspondence
dataset, it is reasonable to compare the network trained with

9537

Image

Annotation

E

E + C

V + E

V + E + C

Figure 4. Qualitative results on the CMU test set. Four different networks are compared, the notation used is: E: trained with extra CMU
annotations, C: trained with correspondence data, V: trained with Vistas training set. The most notable performance difference when adding
correspondences are for areas that are visually different between seasons. This can be seen for the terrain areas covered in leaves for row
two as well as the area of snow in row four. The image in row four is especially challenging since it contains a lot of snow as well as an
apparent lens glare. Yet, the networks trained with correspondences still manages to get part of the snow patch correctly labeled.

Image

Annotation

E

E + C

V + E

V + E + C

Figure 5. Qualitative results on the RobotCar test set. Four different networks are compared, the notation used is: E: trained with extra
RobotCar annotations, C: trained with correspondence data, V: trained with Vistas training set. The most notable performance difference
when adding correspondences are for the night images, row one and two. Comparing the results of E and E + C, we can see that adding
correspondences enables the network to label the road correctly. It however fails at labeling sky and cars correctly since these are not
included in the correspondence dataset.

9538

the extra annotations with the networks trained with extra
annotations and the correspondence loss. As can be seen
in Table 4, we get an improvement in mIoU when adding
the correspondence loss for both the network trained with
Cityscapes and extra annotations as well as the one trained
with Cityscapes, Vistas and extra annotations. This holds
both for the CMU and RobotCar datset. As can be seen
in Fig. 4 where qualitative results on the CMU test set are
presented, adding the correspondences improves segmenta-
tions for areas that are very visually different between sea-
sons. Some examples are yellow leaves on the ground dur-
ing autumn or snow during the winter. These confuse the
network trained only on Cityscapes since there are no such
examples in the training set. The baseline that is also trained
on Vistas handles these situations better but there is still an
improvement when adding the correspondence training, es-
pecially when it comes to, e.g., patches covered in snow,
as these are not included in the subset of classes present in
both datasets and, thus, excluded from the training. For the
RobotCar dataset, the most prevalent improvements are on
the night images, which can be seen in some example seg-
mentations in Fig. 5. Additional qualitative results can be
seen in the supplementary material. Worth to note is also
that we manage to get a larger performance increase us-
ing just a few coarsely annotated images and the correspon-
dence dataset compared to adding the entire Vistas training
set, both for the CMU and the RobotCar data. The Cross-
Season Correspondence datasets required about 30 hours of
manual labour each in the form of annotating correspon-
dences and verifying poses. Adding the two hours it took
to coarsely annotate a few images gives a total of 32 hours
of manual labour required. This, compared to an estimated
28200 hours to annotate the Vistas training set.

The best performing correspondence loss differs be-
tween the datasets. On the CMU dataset, the CE loss per-
forms best both with and without the Vistas training set.
However, for the RobotCar dataset without the Vistas la-
bels, the loss that performs best is HingeF . A difference
between HingeF and CE is that CE imposes a harder con-
straint on the output of the network for the correspondences.
It basically treats the output of the network on the reference
image as the ground truth for the corresponding pixels in
the target image. Since the RobotCar Cross-Season Corre-
spondence dataset is created using LIDAR data, the point
measurements are not perfectly synchronized with the im-
ages which creates a slight misalignment for some corre-
spondences. If there are some erroneous correspondences,
having a hard constraint can be harmful for the performance
of the network. In these cases, imposing a softer constraint
via the HingeF loss, which basically stipulates that the fea-
tures should be similar, can give a better performance, es-
pecially when using a strong baseline training set.

segmentation performance on the WildDash images slightly
for the CMU dataset but not for the RobotCar dataset. A
reason that there is no signiﬁcant performance gain could
be that the camera poses relative to the vehicle (front facing)
and the image resolution are very similar for the Cityscapes,
Vistas and WildDash datasets.
For the CMU Seasons
dataset, there is no front facing camera, only one facing
forward/left and one forward/right. Hence, learning to seg-
ment these well does not necessarily improve the segmenta-
tion performance on WildDash. In addition, the image qual-
ity for both the CMU and the RobotCar images are poorer
than those of Cityscapes, Vistas and WildDash. Despite
this, adding the correspondence loss improves the results for
the network trained on the CMU data and on the RobotCar
data without Vistas. The advantage of learning to segment
images during other weather conditions seems to be large
enough to make a difference on the WildDash validation
set.

7. Conclusion

In this paper, we have introduced two Cross-Season Cor-
respondence datasets, each consisting of a set of 2D-2D
matches between images taken under different conditions.
We described how these datasets can be generated with little
human supervision and demonstrated the usefulness of the
datasets by training an image segmentation network. To this
end we presented and investigated three different training
losses, based on cross-entropy and hinge loss, that can be
used for the correspondence data. Our experiments showed
that adding the correspondences as extra supervision dur-
ing training improves the segmentation performance of the
network, making it more robust to seasonal changes and
weather conditions. Improving the semantic segmentation
performance could in turn lead to more robust localiza-
tion results which provides the ﬁrst step towards an itera-
tive feedback loop improving localization and semantic seg-
mentation. An investigation on how the improved image
segmentation network affect semantic localization methods
is left for future work.

Important future research directions include investigat-
ing options to remove the need for a few, manually anno-
tated images. Possible approaches include an additional un-
supervised domain adaptation step, adapting the segmen-
tation algorithm to the reference images of the correspon-
dence dataset. Additionally, the Cross-Season Correspon-
dence Datasets provide opportunities for other application
such as training robust feature detectors and descriptors.

Acknowledgements This work has been funded by the Swedish Research

Council (grant no. 2016-04445), the Swedish Foundation for Strategic

Research (Semantic Mapping and Visual Navigation for Smart Robots)

Adding the correspondence training does improve the

and Vinnova / FFI (Perceptron, grant no. 2017-01942).

9539

References

[1] Relja Arandjelovi´c and Andrew Zisserman. Visual Vocabu-

lary with a Semantic Twist. In Proc. ACCV, 2014. 1

[2] Hern´an Badino, D Huber, and Takeo Kanade. Visual topo-

metric localization. In Proc. IV, 2011. 3

[3] Sid Yingze Bao and Silvio Savarese. Semantic structure from

motion. In Proc. CVPR, 2011. 1

[4] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li
Fei-Fei. Whats the point: Semantic segmentation with point
supervision. In Proc. ECCV. Springer, 2016. 2

[5] P. J. Besl and N. D. McKay. A method for registration of 3-d

shapes. PAMI, Feb 1992. 4

[6] Maro Bl´aha, Christoph Vogel, Audrey Richard, Jan D. Weg-
ner, Thomas Pock, and Konrad Schindler. Large-Scale Se-
mantic 3D Reconstruction: an Adaptive Multi-Resolution
Model for Multi-Class Volumetric Labeling. In Proc. CVPR,
2016. 1, 2

[7] Sean L Bowman, Nikolay Atanasov, Kostas Daniilidis, and
George J Pappas. Probabilistic data association for semantic
SLAM. In Proc. ICRA, 2017. 1

[8] Eric Brachmann and Carsten Rother. Learning Less is More
In

- 6D Camera Localization via 3D Surface Regression.
Proc. CVPR), 2018. 1

[9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-
D Data in Indoor Environments. In Proc. 3DV, 2017. 2

[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. PAMI, 2018. 2

[11] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and
Alan L Yuille. Attention to scale: Scale-aware semantic im-
age segmentation. In Proc. CVPR, 2016. 2

[12] Yuhua Chen, Wen Li, and Luc Van Gool. Road: Reality ori-
ented adaptation for semantic segmentation of urban scenes.
In Proc. CVPR, 2018. 2

[13] Ian Cherabier, Christian H¨ane, Martin R. Oswald, and Marc
Pollefeys. Multi-Label Semantic 3D Reconstruction using
Voxel Blocks. In Proc. 3DV, 2016. 1, 2

[18] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Niessner. ScanNet:
Richly-annotated 3D Reconstructions of Indoor Scenes. In
Proc. CVPR, 2017. 2

[19] Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploit-
ing bounding boxes to supervise convolutional networks for
semantic segmentation. In Proc. ICCV, 2015. 2

[20] David Eigen and Rob Fergus. Predicting Depth, Surface Nor-
mals and Semantic Labels With a Common Multi-Scale Con-
volutional Architecture. In Proc. ICCV, 2015. 2

[21] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. JMLR, 2016. 2

[22] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. Proc.
IJRR, 2013. 2

[23] Christian H¨ane, Nikolay Savinov, and Marc Pollefeys. Class
Speciﬁc 3D Object Shape Priors Using Surface Normals. In
Proc. CVPR, 2014. 1, 2

[24] Caner Hazirbas, Lingni Ma, Csaba Domokos, and Daniel
Cremers. Fusenet: Incorporating depth into semantic seg-
mentation via fusion-based cnn architecture. In Proc. ACCV.
Springer, 2016. 2

[25] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adapta-
tion. arXiv preprint arXiv:1711.03213, 2017. 2

[26] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell.
Fcns in the wild: Pixel-level adversarial and constraint-based
adaptation. arXiv preprint arXiv:1612.02649, 2016. 2

[27] Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao,
Dingfu Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang.
The apolloscape dataset for autonomous driving.
arXiv
preprint arXiv: 1803.06184, 2018. 2

[28] C. Hne, C. Zach, A. Cohen, and M. Pollefeys. Dense Seman-

tic 3D Reconstruction. PAMI, 2017. 1, 2

[29] Anna Khoreva, Rodrigo Benenson, Jan Hendrik Hosang,
Matthias Hein, and Bernt Schiele. Simple does it: Weakly
supervised instance and semantic segmentation.
In Proc.
CVPR, 2017. 2

[14] Ian Cherabier, Johannes L. Sch¨onberger, Martin R. Oswald,
Marc Pollefeys, and Andreas Geiger. Learning Priors for
Semantic 3D Reconstruction. In Proc. ECCV, 2018. 1, 2

[30] Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw
is not what you get: Domain adaptation using asymmetric
kernel transforms. In Proc. CVPR, 2011. 2

[15] Andrea Cohen, Torsten Sattler, and Marc Pollefeys. Merg-
ing the Unmatchable: Stitching Visually Disconnected SfM
Models. In Proc. ICCV, 2015. 1

[16] Andrea Cohen, Johannes L. Sch¨onberger, Pablo Speciale,
Torsten Sattler, Jan-Michael Frahm, and Marc Pollefeys.
Indoor-Outdoor 3D Reconstruction Alignment.
In Proc.
ECCV, 2016. 1

[17] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding.
In Proc.
CVPR, 2016. 1, 3, 5, 6

[31] Abhijit Kundu, Yin Li, Frank Dellaert, Fuxin Li, and
James M. Rehg. Joint semantic segmentation and 3d recon-
struction from monocular video. In Proc. ECCV, 2014. 1,
2

[32] Lubor Ladick´y, Paul Sturgess, Chris Russell, Sunando Sen-
gupta, Yalin Bastanlar, William Clocksin, and Philip H. S.
Torr. Joint Optimization for Object Class Segmentation and
Dense Stereo Reconstruction. IJCV, 100(2), 2012. 2

[33] M˚ans Larsson, Anurag Arnab, Shuai Zheng, Philip Torr, and
Fredrik Kahl. Revisiting deep structured models for pixel-
level labeling with gradient-based inference. SIIMS, 11(4),
2018. 2

9540

[34] Yunpeng Li, Noah Snavely, Dan Huttenlocher, and Pascal
Fua. Worldwide pose estimation using 3d point clouds. In
Proc. ECCV, 2012. 1

[35] Nektarios Lianos, Johannes L. Sch¨onberger, Marc Pollefeys,
In

and Torsten Sattler. VSO: Visual Semantic Odometry.
Proc. ECCV, 2018. 1

[36] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and
Xiaoou Tang. Semantic image segmentation via deep parsing
network. In Proc. ICCV, 2015. 2

[37] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Proc.
CVPR, 2015. 2

[38] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I
Jordan. Learning transferable features with deep adaptation
networks. arXiv preprint arXiv:1502.02791, 2015. 2

[39] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised domain adaptation with residual trans-
fer networks. In Proc. NIPS, 2016. 2

[40] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul
Newman. 1 year, 1000 km: The oxford robotcar dataset.
IJRR, 2017. 3, 4

[41] John McCormac, Ankur Handa, Andrew Davison, and Ste-
fan Leutenegger. Semanticfusion: Dense 3d semantic map-
ping with convolutional neural networks.
In Proc. ICRA.
IEEE, 2017. 2

[42] Arsalan Mousavian, Jana Kosecka, and JM Lien. Semanti-
cally Guided Location Recognition for Outdoors Scenes. In
Proc. ICRA, 2015. 1

[43] Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ra-
mamoorthi, and Kyungnam Kim. Image to image translation
for domain adaptation. arXiv preprint arXiv:1712.00479, 13,
2017. 2

[44] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o, and
Peter Kontschieder. The mapillary vistas dataset for semantic
understanding of street scenes. In Proc. ICCV, 2017. 1, 6

[45] George Papandreou, Liang-Chieh Chen, Kevin Murphy, and
Alan L Yuille. Weakly-and semi-supervised learning of
a dcnn for semantic image segmentation. arXiv preprint
arXiv:1502.02734, 2015. 2

[46] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS Workshop, 2017. 5

[47] Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell.
Constrained convolutional neural networks for weakly super-
vised segmentation. In Proc. ICCV, 2015. 2

[48] Pedro O. Pinheiro and Ronan Collobert. From image-level
to pixel-level labeling with convolutional networks. In Proc.
CVPR, June 2015. 2

[49] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for Data: Ground Truth from Computer
Games. In Proc. ECCV, 2016. 2

[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In Proc. MICCAI. Springer, 2015. 2

[51] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M Lopez. The synthia dataset: A large

collection of synthetic images for semantic segmentation of
urban scenes. In Proc. CVPR, 2016. 2

[52] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.
Adapting visual category models to new domains. In Proc.
ECCV. Springer, 2010. 2

[53] Swami Sankaranarayanan, Yogesh Balaji, Arpit

Jain,
Ser Nam Lim, and Rama Chellappa. Learning from synthetic
data: Addressing domain shift for semantic segmentation. In
Proc. CVPR, 2018. 2

[54] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii,
Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi
Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and
Tomas Pajdla. Benchmarking 6dof outdoor visual localiza-
tion in changing conditions. In Proc. CVPR, 2018. 1, 3, 4

[55] Nikolay Savinov, Christian H¨ane, Lubor Ladicky, and Marc
Pollefeys. Semantic 3D Reconstruction with Continuous
Regularization and Ray Potentials Using a Visibility Con-
sistency Constraint. In Proc. CVPR, 2016. 1, 2

[56] L. Schneider, M. Cordts, T. Rehfeld, D. Pfeiffer, M. En-
zweiler, U. Franke, M. Pollefeys, and S. Roth. Semantic
Stixels: Depth is not enough. In Proc. IV, 2016. 1, 2

[57] Johannes Lutz Sch¨onberger and Jan-Michael Frahm.

Structure-from-motion revisited. In Proc. CVPR, 2016. 4

[58] Johannes Lutz Sch¨onberger, Marc Pollefeys, Andreas
Geiger, and Torsten Sattler. Semantic Visual Localization.
In Proc. CVPR, 2018. 1

[59] Johannes Lutz Sch¨onberger, Enliang Zheng, Marc Pollefeys,
and Jan-Michael Frahm. Pixelwise view selection for un-
structured multi-view stereo. In Proc. ECCV, 2016. 3, 4

[60] Gautam Singh and Jana Koˇseck´a. Semantically Guided Geo-
In Large-

location and Modeling in Urban Environments.
Scale Visual Geo-Localization, 2016. 1

[61] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang,
Manolis Savva, and Thomas A. Funkhouser. Semantic Scene
Completion from a Single Depth Image.
In Proc. CVPR,
2017. 1, 2

[62] Nasim Souly, Concetto Spampinato, and Mubarak Shah.
Semi supervised semantic segmentation using generative ad-
versarial network. In Proc. ICCV, 2017. 2

[63] Erik Stenborg, Carl Toft, and Lars Hammarstrand. Long-
term visual localization using semantically segmented im-
ages. Proc. ICRA, 2018. 1

[64] Carl Toft, Carl Olsson, and Fredrik Kahl. Long-Term 3D
Localization and Pose from Semantic Labellings. In Proc.
ICCV Workshops, 2017. 1

[65] C. Toft, E. Stenborg, L. Hammarstrand, L. Brynte, M. Polle-
feys, T. Sattler, and F. Kahl. Semantic Match Consistency
for Long-Term Visual Localization. In Proc. ECCV, 2017. 1
[66] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki-
hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.
Learning to adapt structured output space for semantic seg-
mentation. arXiv preprint arXiv:1802.10349, 2018. 2

[67] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Dar-
rell. Adversarial discriminative domain adaptation. In Proc.
CVPR, 2017. 2

[68] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Ad-
dressing appearance change in outdoor robotics with adver-
sarial domain adaptation. In Proc. IROS, 2017. 2

9541

[69] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre-
mental adversarial domain adaptation for continually chang-
ing environments. In Proc. ICRA, 2018. 2

[70] J. Xiao, A. Owens, and A. Torralba. Sun3d: A database
of big spaces reconstructed using sfm and object labels. In
Proc. ICCV. 2

[71] Jun Xie, Martin Kiefel, Ming-Ting Sun, and Andreas Geiger.
Semantic Instance Annotation of Street Scenes by 3D to 2D
Label Transfer. In Proc. CVPR, 2016. 2

[72] Fisher Yu and Vladlen Koltun.

aggregation by dilated convolutions.
arXiv:1511.07122, 2015. 2

Multi-scale context
arXiv preprint

[73] Fisher Yu, Jianxiong Xiao, and Thomas A. Funkhouser. Se-
In Proc.

mantic alignment of LiDAR data at city scale.
CVPR, 2015. 1

[74] Xin Yu, Sagar Chaturvedi, Chen Feng, Yuichi Taguchi,
Teng-Yok Lee, Clinton Fernandes, and Srikumar Rama-
lingam. VLASE: Vehicle Localization by Aggregating Se-
mantic Edges. In Proc. IROS, 2018. 1

[75] Oliver Zendel, Katrin Honauer, Markus Murschitz, Daniel
Steininger, and Gustavo Fern´andez Domınguez. Wilddash-
creating hazard-aware benchmarks.
In Proc. ECCV, 2018.
6

[76] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
In

Wang, and Jiaya Jia. Pyramid scene parsing network.
Proc. CVPR, 2017. 2, 5

[77] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang
Huang, and Philip HS Torr. Conditional random ﬁelds as
recurrent neural networks. In Proc. ICCV, 2015. 2

[78] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong
Wang. Unsupervised domain adaptation for semantic seg-
mentation via class-balanced self-training. In Proc. ECCV,
2018. 2

9542

