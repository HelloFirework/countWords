Embodied Question Answering in

Photorealistic Environments with Point Cloud Perception

Erik Wijmans1†, Samyak Datta1†, Oleksandr Maksymets2†, Abhishek Das1,
Georgia Gkioxari2, Stefan Lee1, Irfan Essa1, Devi Parikh1

2, Dhruv Batra1

2

,

,

1Georgia Institute of Technology 2Facebook AI Research
{etw, samyak, abhshkdz, steflee, irfan, parikh, dbatra}@gatech.edu

1

2

{maksymets, gkioxari}@fb.com

Abstract

To help bridge the gap between internet vision-style
problems and the goal of vision for embodied perception we
instantiate a large-scale navigation task – Embodied Ques-
tion Answering [1] in photo-realistic environments (Mat-
terport 3D). We thoroughly study navigation policies that
utilize 3D point clouds, RGB images, or their combina-
tion. Our analysis of these models reveals several key ﬁnd-
ings. We ﬁnd that two seemingly naive navigation base-
lines, forward-only and random, are strong navigators and
challenging to outperform, due to the speciﬁc choice of the
evaluation setting presented by [1]. We ﬁnd a novel loss-
weighting scheme we call Inﬂection Weighting to be impor-
tant when training recurrent models for navigation with be-
havior cloning and are able to out perform the baselines
with this technique. We ﬁnd that point clouds provide a
richer signal than RGB images for learning obstacle avoid-
ance, motivating the use (and continued study) of 3D deep
learning models for embodied navigation.

1. Introduction

Imagine asking a home robot ‘Hey - can you go check
if my laptop is on my desk? And if so, bring it to me.’ In
order to be successful, such an agent would need a range
of artiﬁcial intelligence (AI) skills – visual perception (to
recognize objects, scenes, obstacles), language understand-
ing (to translate questions and instructions into actions), and
navigation of potentially novel environments (to move and
ﬁnd things in a changing world). Much of the recent suc-
cess in these areas is due to large neural networks trained on
massive human-annotated datasets collected from the web.
However, this static paradigm of ‘internet vision’ is poorly
suited for training embodied agents. By their nature, these

† denotes equal contribution

Figure 1: We extend EmbodiedQA [1] to photorealstic en-
vironments, our agent is spawned in a perceptually and se-
mantically novel environment and tasked with answering a
question about that environment. We examine the agent’s
ability to navigate the environment and answer the question
by perceiving its environment through point clouds, RGB
images, or a combination of the two.

agents engage in active perception – observing the world
and then performing actions that in turn dynamically change
what the agent perceives. What are needed then are richly
annotated, photo-realistic environments where agents may
learn about the consequence of their actions on future per-
ceptions while performing high-level goals.

To this end, a number of recent works have proposed
goal-driven, perception-based tasks situated in simulated
environments to develop such agents [1–10]. While these
tasks are set in semantically realistic environments (i.e.
having realistic layouts and object occurrences), most are
based in synthetic environments (on SUNCG [11] or Unity
3D models [12]) that are perceptually quite different from
what agents embodied in the real world might experience.
Firstly, these environments lack visual realism both in terms
of the ﬁdelity of textures, lighting, and object geometries
but also with respect to the rich in-class variation of ob-

6659

jects1. Secondly, these problems are typically approached
with 2D perception (RGB frames) despite the widespread
use of depth-sensing cameras (RGB-D) on actual robotic
platforms [13–15].

Contributions. We address these points of disconnect by
instantiating a large-scale, language-based navigation task
in photorealistic environments and by developing end-to-
end trainable models with point cloud perception – from
raw 3D point clouds to goal-driven navigation policies.

Speciﬁcally, we generalize the recently proposed Em-
bodied Question Answering (EmbodiedQA) [1] task (orig-
inally proposed in synthetic SUNCG scenes [11]) to
the photorealistic 3D reconstructions from Matterport 3D
(MP3D) [16]. In this task, an agent is spawned at a random
location in a novel environment (e.g. a house) and asked to
answer a question (‘What color is the car in the garage?’).
In order to succeed, the agent needs to navigate from ego-
centric vision alone (without an environment map), locate
the entity in question (‘car in the garage’), and respond with
the correct answer (e.g. ‘orange’).

We introduce the MP3D-EQA dataset, consisting of
1136 questions and answers grounded in 83 environments.
Similar to [1], our questions are generated from functional
programs operating on the annotations (objects, rooms, and
their relationships) provided in MP3D; however, MP3D
lacks color annotations for objects, which we collect from
Amazon Mechanical Turk in order to generate ‘What color
. . . ?’ questions. The MP3D environments provide signiﬁ-
cantly more challenging environments for our agent to learn
to navigate in due to the increased visual variation.

We present a large-scale exhaustive evaluation of design
decisions, training a total of 16 navigation models (2 archi-
tectures, 2 language variations, and 4 perception variations),
3 visual question answering models, and 2 perception mod-
els – ablating the effects of perception, memory, and goal-
speciﬁcation. Through this comprehensive analysis we
demonstrate the complementary strengths of these percep-
tion modalities and highlight surprisingly strong baselines
in the EmbodiedQA experimental setting.

Our analysis reveals that the seemingly naive baselines,
forward-only and random, are strong navigators in the de-
fault evaluation setting presented in [1] and challenging to
beat, providing insight to others working in this space that
models can perform surprisingly well without learning any
meaningful behavior. We also ﬁnd that point clouds pro-
vide a richer signal than RGB images for learning obstacle
avoidance, motivating continued study of utilizing depth in-
formation in embodied navigation tasks.

We ﬁnd a novel weighting scheme we call Inﬂection
Weighting – balancing the contributions to the cross-entropy
loss between inﬂections, where the ground truth action dif-
fers from the previous one, and non-inﬂections – to be an

1To pervert Tolstoy, each ugly lamp is ugly in its own way.

effective technique when performing behavior cloning with
a shortest path expert. We believe this technique will be
broadly useful any time a recurrent model is trained on long
sequences with an imbalance in symbol continuation ver-
sus symbol transition probabilities, i.e. when P (Xt = x |
Xt−1 = x) >> P (Xt 6= x | Xt−1 = x).

To the best of our knowledge, this is the ﬁrst work to
explore end-to-end-trainable 3D perception for goal-driven
navigation in photo-realistic environments. With the use
of point clouds and realistic indoor scenes, our work lays
the groundwork for tighter connection between embodied
vision and goal-driven navigation, provides a testbed for
benchmarking 3D perception models, and hopefully brings
embodied agents trained on simulation one step closer to
real robots equipped with 2.5D RGB-D cameras.

2. Related Work

Embodied Agents and Environments. End-to-end learn-
ing methods – to predict actions directly from raw pix-
els [17] – have recently demonstrated strong performance.
Gupta et al. [2] learn to navigate via mapping and planning.
Sadeghi et al. [18] teach an agent to ﬂy using simulated data
and deploy it in the real world. Gandhi et al. [19] collect a
dataset of drone crashes and train self-supervised agents to
avoid obstacles. A number of new challenging tasks have
been proposed including instruction-based navigation [6,7],
target-driven navigation [2, 4], embodied/interactive ques-
tion answering [1, 9], and task planning [5].

A prevailing problem in embodied perception is the lack
of a standardized, large-scale, diverse, real-world bench-
mark – essentially, there does not yet exist a COCO [20]
for embodied vision. A number of synthetic 3D environ-
ments have been introduced, such as DeepMind Lab [21]
and VizDoom [22]. Recently, more visually stimulating
and complex datasets have emerged which contain action-
able replicas of 3D indoor scenes [3, 23–25]. These efforts
make use of synthetic scenes [25, 26], or scans of real in-
door houses [16,27] and are equipped with a variety of input
modalities, i.e. RGB, semantic annotations, depth, etc.

The closest to our work is the EmbodiedQA work of
Das et al. [1], who train agents to predict actions from ego-
centric RGB frames. While RGB datasets are understand-
ably popular for ‘internet vision’, it is worth stepping back
and asking – why must an embodied agent navigating in 3D
environments be handicapped to perceive with a single RGB
camera? We empirically show that point cloud representa-
tions are more effective for navigation in this task. More-
over, contrary to [1, 9] that use synthetic environments, we
extend the task to real environments sourced from [16].

3D Representations and Architectures. Deep learning has
been slower to impact 3D computer vision than its 2D coun-
terpart, in part due to the increased complexity of repre-
senting 3D data. Initial success was seen with volumetric

6660

(a) RGB Panorama

(b) Mesh Reconstruction

(c) Point Cloud

(d) RGB-D Render

Figure 2: Illustration of mesh construction errors and what
point clouds are able to correct. Notice the warping of ﬂat
surfaces, the extreme differences in color, and texture arti-
facts from reﬂections.

CNN’s [28–30]. These networks ﬁrst discretize 3D space
with a volumetric representation and then apply 3D vari-
ants of operations commonly found in 2D CNN’s – convo-
lutions, pooling, etc. Volumetric representations are greatly
limited due to the sparsity of 3D data and the computa-
tional cost of 3D convolutions. Recent works on 3D deep
learning have proposed architectures that operate directly
on point clouds. Point clouds are a challenging input for
deep learning as they are naturally a set of points with no
canonical ordering. To overcome the ordering issue, some
utilize symmetric functions, PointNet(++) [31, 32], and A-
SCN [33]. Others have used clever internal representations,
such as SplatNet [34], Kd-Net [35], and O-CNN [36].

3. Questions in Environments

In this work, we instantiate the Embodied Question An-
swering (EQA) [1] task in realistic environments from the
Matterport3D dataset [16].

3.1. Environments

The Matterport3D dataset consists of 90 home environ-
ments captured through a series of panoramic RGB-D im-
ages taken by a Matterport Pro Camera (see sample panora-
mas in Fig. 2a). The resulting point clouds are aligned and
used to reconstruct a 3D mesh (like those shown in Fig. 2b)
that is then annotated with semantic labels. The Matter-
port3D dataset is densely annotated with semantic segmen-
tations of 40 object categories for ⇠50,000 instances. Room
type is annotated for over 2050 individual rooms.

These reconstructions offer high degrees of perceptual
realism but are not perfect however and sometimes suffer
from discoloration and unusual geometries such as holes in
surfaces. In this work, we examine both RGB and RGB-D
perception in these environments. For RGB, we take ren-
ders from the mesh reconstructions and for point clouds we
operate directly on the aligned point clouds. Fig. 2c and

Fig. 2d show the point cloud rendered for an agent looking
at the scene shown in Fig. 2a.

Simulator. To enable agents to navigate in MatterPort3D
environments, we develop a simulator based on MINOS
[23]. Among other things, MINOS provides occupancy
checking, RGB frame rendering from the mesh, and short-
est path calculation (though we reimplement this for higher
accuracy and speed). It does not however provide access to
the underlying point clouds. In order to render 2.5D RGB-
D frames, we ﬁrst construct a global point cloud from all of
the panoramas provided in an environment from the dataset.
Then, the agent’s current position, camera parameters (ﬁeld
of view, and aspect ratio), and the mesh reconstruction are
used to determine which points are within its view. See the
supplementary for full details on this.

3.2. Questions

Following [1], we programmatically generate templated
questions based on the Matterport3D annotations, generat-
ing questions of the following three types:

location: What room is the <OBJ> located in?
color: What color is the <OBJ> ?
color room: What color is the <OBJ> in the <ROOM> ?

While EQA [1] included a forth question type prepositions,
we found those questions in MP3D to be relatively few, with
strong biases in their answer, thus we do not include them.

While room and object annotations and positions sup-
porting the three question types above are available in
MP3D, human names for object colors are not. To rec-
tify this, we collect the dominant color of each object from
workers on Amazon Mechanical Turk (AMT). Workers are
asked to select one of 24 colors for each object. The color
palette was created by starting with Kenneth Kelly’s 22 col-
ors of maximum contrast [37] and adding 2 additional col-
ors (off-white and slate-grey) due to their prevalence in in-
door scenes. Overall, the most reported color was gray. For
each object, we collect 5 annotations and take the majority
vote, breaking ties based on object color priors. We include
details of the AMT interface in the supplementary.

Following the protocol in [1], we ﬁlter out questions
that have a low entropy in distribution over answers across
environments i.e. peaky answer priors – e.g. the answer to
‘What room is the shower in?’ is nearly always ‘bathroom’
– to ensure that questions in our dataset require the agent
to navigate and perceive to answer accurately. We remove
rooms or objects that are ambiguous (e.g. “misc” rooms) or
structural (e.g. “wall” objects). Below are the objects and
rooms that appear in our generated questions:

Objects: shelving, picture, sink, clothes, appliances, door,
plant, furniture, ﬁreplace, chest of drawers, seating, sofa, ta-
ble, curtain, shower, towel, cushion, blinds, counter, stool, bed,
chair, bathtub, toilet, cabinet

6661

Homes

Floors Total Qns. Unique Qns.

train
val
test

57
10
16

102
16
28

767
130
239

174
88
112

Table 1: Statistics of splits for EQA in Matterport3D

Rooms: family room, closet, spa, dinning room, lounge, gym,
living room, ofﬁce, laundry room, bedroom, foyer, bathroom,
kitchen, garage, rec room, meeting room, hallway, tv room

In total, we generate ⇠1100 questions across 83 home en-
vironments (7 environments resulted in no questions after
ﬁltering). Note that this amounts to ⇠13 question-answer
pairs per environment compared to ⇠12 per scene in [1].
Color room questions make up the majority of questions.
These questions require searching the environment to ﬁnd
the speciﬁed object in the speciﬁed room. Whereas [1] re-
quires both the object and the room to be unique within the
environment, we only require the (object, room) pair to be
unique, thereby giving the navigator signiﬁcantly less infor-
mation about the location of the object.

We use the same train/val/test split of environments as
in MINOS [23]. Note that in [1], the test environments dif-
fer from train only in the layout of the objects; the objects
themselves have been seen during training. In MP3D-EQA,
the agents are tested on entirely new homes, thus may come
across entirely new objects – testing semantic and percep-
tual generalization. Tab. 1 shows the distribution of homes,
ﬂoors, and questions across these splits. We restrict agent
start locations to lie on the same ﬂoor as question targets
and limit episodes to single ﬂoors.

4. Perception for Embodied Agents

Agents for EmbodiedQA must understand the given
question, perceive and navigate their surroundings collect-
ing information, and answer correctly in order to suc-
ceed. Consider an EmbodiedQA agent
that navigates
by predicting an action at at each time step t based on
its trajectory of past observations and actions  t−1 =
(s1, a1, s2, a2, . . . , st−1, at−1), the current state st, and the
question Q. There are many important design decisions for
such a model – action selection policy, question representa-
tion, trajectory encoding, and observation representation. In
this work, we focus on the observation representation – i.e.
perception – in isolation and follow the architectural pat-
tern in [1] for the remaining components. In this section,
we describe our approach and recap existing model details.

4.1. Learning Point Cloud Representations

Consider a point cloud P 2 P which is an unordered
set of points in 3D space with associated colors, i.e. P =
{(xm, ym, zm, Rm, Gm, Bm)}M
m=1. To enable a neural
agent to perceive the world using point clouds, we must
learn a function f : P ! Rd that maps a point cloud to

an observation representation. To do this, we leverage a
widely used 3D architecture, PointNet++ [32].

PointNet++. At a high-level, PointNet++ alternates be-
tween spatial clustering and feature summarization – result-
ing in a hierarchy of increasingly coarse point clusters with
associated feature representations summarizing their mem-
bers. This approach draws an analogy to convolution and
pooling layers in standard convolutional neural networks.

1

let {pi

1, ..., pi

1, ..., hi

, ..., pi+1
Ni+1

More concretely,

Ni } be the set of Ni
points at the ith level of a PointNet++ architecture and
{hi
Ni } be their associated feature representations
(e.g. RGB values for the input level). To construct the i+1th
level, Ni+1 centroids {pi+1
} are sampled from
level i via iterative farthest point sampling (FPS) – ensuring
even representation of the previous layer. These centroids
will make up the points in level i+1 and represent their local
areas. For each centroid pi+1
, the K closest points within
a max radius are found and a symmetric learnable neural
architecture [31], composed of a series of per-point opera-
tions (essentially 1-by-1 convolutions) and a terminal max-
pool operation, is applied to this set of associated points to
produce the summary representation hi+1
. These cluster-
ing and summarization steps (referred to as Set Abstractions
in [32]) can be repeated arbitrarily many times. In this work
we use a 3 level architecture with N 1 = 1045, N 2 = 256,
and N 3 = 64. We compute a ﬁnal feature with a set of 1-
by-1 convolutions and a max-pool over the 3rd level point
features and denote this network as f (·).

k

k

Given an input point cloud Pt from an agent’s view at
time t, we produce a representation st = f (Pt) where
st 2 R1024. However, point clouds have an interesting
property – as an agent navigates an environment the number
of points it perceives can vary. This may be due to sensor
limitations (e.g. being too close or too far from objects) or
properties of the observed surfaces (e.g. specularity). While
the encoder f is invariant to the number of input points,
representations drawn from few supporting points are not
likely to be good representations of a scene. For a naviga-
tion or question-answering agent, this means there is no way
to discern between conﬁdent and unconﬁdent or erroneous
observation. To address this, we divide the range spanning
the possible number of points in any given point cloud –
[0, 214] – into 5 equal sized bins and represent these bins
as 32-d feature vectors that encode the sparsity of a point
cloud. Now, given a point cloud Pt with |Pt| points, we re-
trieve its corresponding sparsity embedding ct and produce
a ﬁnal encoding [st, ct] 2 R1056 that is used by the agent
for navigation and question-answering.

Visual Pretraining Tasks. To train the encoder architecture
to extract semantically and spatially meaningful representa-
tions of agent views, we introduce three pretraining tasks
based on the annotations provided in Matterport3D. Specif-
ically, these tasks are:

6662

Figure 3: The visual encoders a trained using three pertaining tasks to imbue their scene representations with information
about semantics (segmentation), color (autoencoding), and structure (depth). All decoder heads share the same encoder. Up-
sampling for RGB (Up #) is done with bi-linear interpolation. Upsampling for pointclouds (FP #), is achieved with Feature
Propagation layers [32]. After pretraining, the decoders are discarded, and the encoder is treated as a static feature extractor.

– Semantic Segmentation in which the model predicts the
object annotation for each point, ys
i , from the summa-
rized representation si = f (P ). We train a PointNet++
feature propagation network gs(·) to minimize the cross-
entropy between ys
i = gs(f (P )) [32]. This en-
courages the encoder, f (·), to include information about
which objects are in the frame.

i and ˆys

– Color Autoencoding mirrors the semantic segmentation
task. However, the network gc(·) is now trained to mini-
mize the smooth-L1 loss between yc
i = gc(f (P )).
This task encourages the encoder f (·) to capture holistic
information about the colors in the scene.

i and ˆyc

– Structure Autoencoding where point coordinates must
be recovered from the summarized representation, i.e.
{(xi, yi, zi, Ri, Gi, Bi)}N
i=1. We
implement this decoder as a multi-layer perceptron that
regresses to the N ⇥ 3 spatial coordinates. As in [38],
we use the earth-movers distance as the loss function.

i=1 ! {(xi, yi, zi)}N

We demonstrate these tasks in Fig. 3. These tasks encourage
the model features to represent colors, objects, and spatial
information including free-space and depth that are essen-
tial to navigation. We collect ⇠100,000 frames from Mat-
terport3D using our simulator and train the point cloud en-
coder for these tasks. We discard the decoder networks after
training, and use the encoder f as a ﬁxed feature extractor.
RGB Image representations. We utilize ResNet50 [39]
trained using an analogous set of tasks (semantic segmen-
tation, autoencoding, and depth prediction) to learn a repre-
sentation for egocentric 224 ⇥ 224 RGB images as in [1].
We ﬁnd that ResNet50 is better able to handle the increased
visual complexity of the Matterport3D environments than
the shallow CNN model used in Das et al. We provide fur-
ther details about perception model and decoder architec-
tures in the supplement.

4.2. Navigation and Question Answering

We now provide an overview of the navigation and ques-

tion answering models we use in this work.

Question Encoding. In order to succeed at navigation and

question answering, it is important for an embodied agent
to understand the queries it is being tasked with answering.
We use two layer LSTMs with 128-d hidden states to en-
code questions. The question encoding for navigation and
question answering are learned separately.

Question Answering Models. We experimented with three
classes of question answering models:
– Question-only We examine the question-only baselines
proposed in [1] – a small classiﬁcation network that pre-
dicts the answer using just the question encoding. We
also examine the recently proposed question-only base-
lines in [40] – a simple nearest neighbors approach and
a bag-of-words with a softmax classiﬁer.

– Attention This is the best performing VQA model from
[1]. It computes question-guided attention over the fea-
tures of the last ﬁve frames observed by the agent before
stopping, followed by element-wise product between the
attended feature and question encoding to answer; and

– Spatial Attention utilizes the bag-of-words encoder
proposed in [40] to compute spatial attention over the
last-frame. We use scaled dot-product attention [41] over
the feature map, perform an element-wise product be-
tween attended features and the question feature, and
predict an answer. This model only uses RGB inputs.

Navigation Models. We consider two baseline navigators:
– Forward-only (Fwd) which always predicts forward.
– Random which uniformly chooses one of forward,

turn-left, and turn-right at every time step.

We consider two navigation architectures:
– Reactive (R) which is a simple feed-forward network
that takes a concatenation of the embedding of the ﬁve
most recent visual observations as input to predict an ac-
tion. As such, this is a memory-less navigator.

– Memory (M) which is a two-layer GRU-RNN that takes
the encoding(s) of the current observation and previous
action as inputs to predict the current action.

For each navigation architecture, we examine the combi-
nation of our 4 different perception variations, None (i.e. a

6663

RGBOutputTargetOutputTargetRGB EncoderSkip ConnectionConv 1Residual Block 1Residual Block 2Res Blcok 3Res 4SemanticAutoencodingDepthSemanticAutoencodingDepthPointcloudUp 1Up 2Up 3Up 4Up 1Up 2Up 3Up 4Up 1Up 2Up 3Up 4Skip ConnectionSet Abstraction 1FP 1FP 2FP 3FP 4Set Abstraction 2Set Abs. 3SA 4SemanticSemanticColorColorStructureStructureFP 1FP 2FP 3FP 4FC 1FC 2FC 3PC Encoderblind model as suggested by Thomason et al. [42]), PC, RGB,
and PC+RGB, with the 2 different language variations, None
and Question. For reactive models that utilize the ques-
tion, we incorporate the question embedding by concatena-
tion with the visual embedding. For memory models, the
question embedding is an additional input to the GRU. Due
to the highly correlated observations our agents see during
training, we utilize Group Normalization layers [43] in our
navigation models. The action space for all our navigation
models is forward, turn-left, turn-right, and stop.

4.3. Imitation Learning from Expert Trajectories

To train our models, we ﬁrst create a static dataset of
agent trajectories by generating training episodes based on
shortest-paths from agent spawn locations to the best view
of the object of interest. For example, if a question asks
‘What color is the sofa in the living room?’, we spawn an
agent randomly in the environment in the same ﬂoor as the
target object – the sofa – and compute the shortest naviga-
ble path to the best view of the sofa. The best view of the
sofa is determined by exhausting all possible view positions
within a reasonable radius of the target object. The quality
of a view is determined by the intersection over union of a
pre-determined bounding box and the segmentation mask of
the target. In normalized image coordinates, the bounding
box’s top left corner is at (0.25, 0.25) and it has a height of
0.6 and a width of 0.5. We use this metric instead of simply
maximizing the number of visible pixels in the segmenta-
tion mask to maintain context of the object’s surroundings.
To provide enough data to overcome the complex-
ity of Matterport3D environments, we generate ⇠11,796
such paths in total (corresponding to approximately ⇠15
episodes per question-environment pair, each for a different
spawn location of the agent). For computational efﬁciency
in the large Matterport3D environments, we compute short-
est paths in continuous space using LazyTheta* [44] and
greedily generate agent actions to follow that path, rather
than directly searching in the agent’s action space.

Perception. We use the frozen pre-trained perception mod-
els as described in Section 4.1. For PC+RGB models we sim-
ply concatenate both visual features.

Question Answering. The question answering models are
trained to predict the ground truth answer from a list of 53
answers using Cross Entropy loss. The models with vision
use the ground-truth navigator during training.

4.4. Imitating Long Trajectories Effectively

All navigation models are trained with behavior cloning
where they are made to mimic the ground truth, shortest
path agent trajectories. That is to say the agents are walked
through the ground truth trajectory observing the corre-
sponding frames (though reactive models retain only the
last ﬁve) up until a given time step and then make an ac-
tion prediction. Regardless of the decision, the agent will

be stepped along the ground truth trajectory and repeat this
process. One challenge with this approach is that relatively
unintelligent policies can achieve promising validation loss
without really learning anything useful – one such strategy
simply repeats the previous ground truth action. Insidiously,
these models achieve very high validation accuracy for ac-
tion prediction but miss every transition between actions!

Inﬂection Weighting. To combat this problem and encour-
age agents to focus on important decisions along the tra-
jectory, we introduce a novel weighting scheme we call In-
ﬂection Weighting. Conceptually, we weight predictions at
time steps more heavily if the ground truth action differs
from the previous one – that is if the time step is an in-
ﬂection point in the trajectory. More formally, we deﬁne a
per-time step weight

wt =( N

nI
1

at−1 6= at

otherwise

(1)

where N/nI is the inverse frequency of inﬂection points
(approximately 5.7 in our dataset). We can then write an
inﬂection weighted loss between a sequence of predictions
ˆY and a ground truth trajectory A over as:

`IW ( ˆY , A) =

1
t=1 wt

PT

wt` (ˆyt, at)

(2)

T

Xt=1

where `(·, ·) is the task loss – cross-entropy in our setting.
We deﬁne the ﬁrst action, t = 1, to be an inﬂection.
In
practice, we ﬁnd inﬂection weighting leads to signiﬁcant
gains in performance for recurrent models.

Inﬂection weighting may be viewed as a generalization
of the class-balanced loss methods that are commonly used
in supervised learning under heavily imbalanced class dis-
tributions (e.g. in semantic segmentation [45]) for a partic-
ular deﬁnition of a ‘class’ (inﬂection or not).

5. Experiments and Analysis

We closely follow the experimental protocol of Das et
al. [1]. All results here are reported on novel test environ-
ments. Agents are evaluated on their performance 10, 30, or
50 primitive actions away from the question target, corre-
sponding to distances of 0.35, 1.89, and 3.54 meters respec-
tively. One subtle but important point is that to achieve these
distances the agent is ﬁrst randomly spawned within the en-
vironment, and then the agent is walked along the shortest
path to the target until it is the desired distance from the
target (10, 30, or 50 steps).

We perform an exhaustive evaluation of design deci-
sions, training a total of 16 navigation models (2 architec-
tures, 2 language variations, and 4 perception variations), 3
visual question answering models, and 2 perception models.

6664

1.5

1.0

0.125

0.100

0.075

0.050

0.40

0.35

0.30

dT (meters, ↓ better)

IoUT (↑ better)

QA Accuracy (↑ better)

Random

R/Fwd

M

R+PC

M+PC

Figure 4: Models with memory signiﬁcantly outperform
their memory-less counterparts. Surprisingly, the baselines,
random and forward-only, and a vision-less navigator with
memory perform very well.

2.0

1.5

1.0

0.15

0.10

0.05

0.40

0.35

0.30

dT (meters, ↓ better)

IoUT (↑ better)

QA Accuracy (↑ better)

R/Fwd

NoIW-R+PC+Q

NoIW-M+PC+Q

R+PC+Q

M+PC+Q

Figure 5: Models trained with inﬂection-weighted cross-
entropy loss signiﬁcantly outperform their unweighted
cross-entropy counterparts and the baselines.

5.1. Metrics

Question Answering. For measuring question answer-
ing performance, we report the top-1 accuracy, i.e. did the
agent’s predicted answer match the ground truth or not.

Navigation. For navigation, we report the distance to the
target object from where the agent is spawned (d0) for ref-
erence, measure distance to the target object upon naviga-
tion completion dT (lower is better), and the percentage of
actions that result in a collision with an obstacle %collision
(lower is better). All the distances are geodesic, i.e. mea-
sured along the shortest path.

We propose a new metric, IoUT (higher is better), to
evaluate the quality of the view of the target the agent ob-
tains at the end of navigation. We compute the intersection-
over-union (IoU) score between the ground-truth target seg-
mentation and the same centered bounding box used to se-
lect views during dataset generation (see Section 4.3). To
compensate for object size, we divide by the best attainable
IoU for the target object. We deﬁne IoUT as the maximum
of the last N IoU scores. We set N to 5 as the VQA model
receives the last 5 frames.

5.2. Results and Analysis

Question Answering. The top-1 accuracy for different
answering modules on the validation set using the ground-
truth navigator is shown below.

Top-1 (%)

spatial+RGB+Q
attention+RGB+Q
attention+PC+RGB+Q
attention+PC+Q
lstm-question-only
nn-question-only [40]
bow-question-only [40]

46.2
40.0
38.4
36.1
32.8
35.4
38.3

In-order to compare QA performance between navigators,
we report all QA results with the best-performing module
– spatial+RGB+Q – regardless of the navigator.

Navigation. We use the following notation to specify our
models: For the base architecture, R denotes reactive mod-
els and M denotes memory models. The base architectures
are then augmented with their input types, +PC, +RGB, and
+Q. So a memory model that utilizes point clouds (but no
question) is denoted as M+PC. Unless otherwise speciﬁed (by
the preﬁx NoIW), models are trained with inﬂection weight-
ing. We denote the two baseline navigators, forward-only
and random, as Fwd and Random, respectively.

Due to the large volume of results, we present key ﬁnd-
ings and analysis here (with T−30) and, for the intrepid
reader, provide the full table (with 300+ numbers!) in the
supplement. We make the following observations:

Forward-only is a strong baseline. One of the side-effects
of the evaluation procedure proposed in [1] is that the agent
is commonly facing the correct direction when it is handed
control. This means the right thing to do to make progress
is to go forward. As a result, a forward-only navigator does
quite well, see Fig. 4. Forward-only also tends to not over-
shoot too much due to its ‘functional stop’: continually run-
ning into an obstacle until the max step limit is reached. Our
vision-less reactive models (R/Fwd and R+Q/Fwd) learn to
only predict forward, the most frequent action.

Fig. 4 also shows that the random baseline is a decep-
tively strong baseline. The lack of a backward action, and
left and right cancelling each other out in expectation,
results in random essentially becoming forward-only.

Inﬂection weighting improves navigation. We ﬁnd inﬂec-
tion weighting to be crucial for training navigation models
with behavior cloning of a shortest-path expert; see Fig. 5.
While we see some improvements with inﬂection weighting
for most models, memory models reap the greatest beneﬁts
– improving signiﬁcantly on both dT and IoUT. Interest-
ingly, these gains do not translate into improved QA ac-
curacy. While we have only utilized this loss for behavior
cloning, we suspect the improvements seen from inﬂection
weighting will transfer to models that are ﬁne-tuned with re-
inforcement learning as they begin with better performance.

Memory helps. Fig. 4 shows that models with memory are
better navigators than their reactive counter parts. Surpris-

6665

1.5

1.0

50

25

0.2

0.1

0.40

0.35

0.30

dT (meters, ↓ better)

%collision (↓ better)

IoUT (↑ better)

QA Accuracy (↑ better)

R/Fwd

R+RGB

R+PC

R+PC+RGB

M

M+RGB

M+PC

M+PC+RGB

Figure 6: Vision generally hurts distance-based navigation metrics. However metrics that are dependent on the navigators
ability to look in a particular direction (IoUT and QA) generally improve, and the models collide with the environment less.

1.25

1.00

0.75

0.2

0.1

0.40

0.35

0.30

dT (meters, ↓ better)

IoUT (↑ better)

QA Accuracy (↑ better)

M+RGB

M+RGB+Q

M+PC

M+PC+Q

M+PC+RGB

M+PC+RGB+Q

Figure 7: Comparison of memory navigation models with
and without the question. Interestingly, adding the question
doesn’t appear to aid models trained with behavior cloning.

ingly, a vision-less navigator with memory performs very
well at distance based navigation metrics. Like a vision-
less reactive navigator (forward-only), a vision-less mem-
ory navigator is only able to learn priors on how shortest
paths in the dataset tend to look, however memory allows
the model to count and therefore it is able to stop and turn.

Vision helps gaze direction metrics. Fig. 6 shows the ef-
fect of adding vision to both reactive and memory models.
The addition of vision leads to improvements on IoUT and
QA, however, the improvements in IoUT do not translate
directly improvement on QA. This is likely due to naive
VQA models. Models with vision also tend to collide with
the environment less often, as can be seen by %collision usu-
ally being lower.

Vision hurts distance metrics. Surprisingly, adding vision
hurts distance based navigation metrics (dT). For reactive
models, adding vision causes the models to collide signiﬁ-
cantly less frequently, resulting in a loss of the ‘functional
stop’ that forward-only uses, i.e. continually colliding until
the step limit is reached. For memory models, the story isn’t
as clear; however, memory models with vision stop less
often and thus have a higher average episode length than
their vision-less counterpart, which causes them to over-
shoot more often. We suspect this is because they learn a
more complex function for stopping than the simple count-
ing method used by vision-less memory models and this
function is less able to handle errors during navigation.

Question somewhat helps. Fig. 7 provides a comparison of

M+PC and M+RGB and M+PC+RGB with and without the ques-
tion (Q). Interestingly, we do not see large improvements
when providing models with the question. Given how much
color room dominates our dataset, it seems reasonable to
expect that telling the navigation models which room to go
to would be a large beneﬁt. We suspect that our models
are not able to properly utilize this information due to lim-
itations of behavior cloning. Models trained with behav-
ior cloning never see mistakes or exploration and therefore
never learn to correct mistakes or explore.

PC+RGB provides the best of both worlds. Fig. 6 also pro-
vides a comparison of the three different vision modalities.
The general tend is that point clouds provided a richer signal
for obstacle avoidance (corresponding to lower %collision
values), while RGB provides richer semantic information
(corresponding to a higher IoUT and QA). Combining
both point clouds and RGB provides improvements to both
obstacle avoidance and leveraging semantic information.

6. Conclusion

We present an extension of the task of EmbodiedQA
to photorealistic environments utilizing the Matterport 3D
dataset and propose the MP3D-EQA v1 dataset. We then
present a thorough study of 2 navigation baselines and 2 dif-
ferent navigation architectures with 8 different input varia-
tions. We develop an end-to-end trainable navigation model
capable of learning goal-driving navigation policies directly
from 3D point clouds. We provide analysis and insight
into the factors that affect navigation performance and pro-
pose a novel weighting scheme – Inﬂection Weighting –
that increases the effectiveness of behavior cloning. We
demonstrate that two the navigation baselines, random and
forward-only, are quite strong under the evaluation settings
presented by [1]. Our work serves as a step towards bridg-
ing the gap between internet vision-style problems and the
goal of vision for embodied perception.
Acknowledgements. This work was supported in part by NSF (Grant #
1427300), AFRL, DARPA, Siemens, Samsung, Google, Amazon, ONR
YIPs and ONR Grants N00014-16-1-{2713,2793}. The views and con-
clusions contained herein are those of the authors and should not be in-
terpreted as necessarily representing the ofﬁcial policies or endorsements,
either expressed or implied, of the U.S. Government, or any sponsor.

6666

References

[1] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,
Devi Parikh, and Dhruv Batra. Embodied Question Answer-
ing. In CVPR, 2018. 1, 2, 3, 4, 5, 6, 7, 8

[2] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Suk-
thankar, and Jitendra Malik. Cognitive mapping and plan-
ning for visual navigation. In CVPR, 2017. 1, 2

[3] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian.
Building generalizable agents with a realistic and rich 3d en-
vironments. In ICLR Workshop, 2018. 1, 2

[4] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Ab-
hinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven vi-
sual navigation in indoor scenes using deep reinforcement
learning. In ICRA, 2017. 1, 2

[5] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-
Fei, Abhinav Gupta, Roozbeh Mottaghi, and Ali Farhadi.
Visual Semantic Planning using Deep Successor Represen-
tations. In ICCV, 2017. 1, 2

[6] Devendra Singh Chaplot, Kanthashree Mysore Sathyen-
dra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and
Gated-attention architectures
Ruslan Salakhutdinov.
for task-oriented language grounding.
arXiv preprint
arXiv:1706.07230, 2017. 1, 2

[7] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
Johnson, Niko S¨underhauf, Ian Reid, Stephen Gould, and
Anton van den Hengel. Vision-and-language navigation: In-
terpreting visually-grounded navigation instructions in real
environments. In CVPR, 2018. 1, 2

[8] Karl Moritz Hermann, Felix Hill, Simon Green, Fumin
Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wo-
jtek Czarnecki, Max Jaderberg, Denis Teplyashin, et al.
Grounded language learning in a simulated 3d world. arXiv
preprint arXiv:1706.06551, 2017. 1

[9] Daniel Gordon, Aniruddha Kembhavi, Mohammad Raste-
IQA:
In

gari, Joseph Redmon, Dieter Fox, and Ali Farhadi.
Visual question answering in interactive environments.
CVPR, 2018. 1, 2

[10] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh,
and Dhruv Batra. Neural Modular Control for Embodied
Question Answering. In Proceedings of the Conference on
Robot Learning (CoRL), 2018. 1

[11] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. In CVPR, 2017. 1, 2

Thomas Funkhouser, and Alberto Rodriguez. Robotic pick-
and-place of novel objects in clutter with multi-affordance
grasping and cross-domain image matching. In ICRA, 2018.
2

[15] Shiqi Zhang, Yuqian Jiang, Guni Sharon, and Peter Stone.
Multirobot symbolic planning under temporal uncertainty.
In Proceedings of the 16th International Conference on Au-
tonomous Agents and Multiagent Sytems (AAMAS), May
2017. 2

[16] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-
D data in indoor environments. International Conference on
3D Vision (3DV), 2017. 2, 3

[17] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter
Abbeel. End-to-end training of deep visuomotor policies.
JMLR, 17(1):1334–1373, Jan. 2016. 2

[18] Fereshteh Sadeghi and Sergey Levine. CAD2RL: Real
single-image ﬂight without a single real image. RSS, 2017.
2

[19] Abhinav Gupta Dhiraj Gandhi, Lerrel Pinto. Learning to ﬂy

by crashing. IROS, 2017. 2

[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollr, and C. Lawrence
Zitnick. Microsoft COCO: Common Objects in Context. In
ECCV, 2014. 2

[21] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward,
Marcus Wainwright, Heinrich K¨uttler, Andrew Lefrancq,
Simon Green, V´ıctor Vald´es, Amir Sadik, Julian Schrit-
twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain,
Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass-
abis, Shane Legg, and Stig Petersen. Deepmind lab. arXiv.
2

[22] Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub
Toczek, and Wojciech Jaskowski. Vizdoom: A doom-based
AI research platform for visual reinforcement learning. arXiv
1605.02097, 2016. 2

[23] Manolis Savva, Angel X. Chang, Alexey Dosovitskiy,
Thomas Funkhouser, and Vladlen Koltun. MINOS: Multi-
modal indoor simulator for navigation in complex environ-
ments. arXiv:1712.03931, 2017. 2, 3, 4

[24] Simon Brodeur, Ethan Perez, Ankesh Anand, Florian
Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo
Larochelle, and Aaron C. Courville. Home: a household
multimodal environment. arXiv 1711.11017, 2017. 2

[12] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao,
Hunter Henry, Marwan Mattar, and Danny Lange. Unity: A
general platform for intelligent agents, 2018. 1

[25] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu,
Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive
3D Environment for Visual AI. arXiv, 2017. 2

[13] Albert S Huang, Abraham Bachrach, Peter Henry, Michael
Krainin, Daniel Maturana, Dieter Fox, and Nicholas Roy.
Visual odometry and mapping for autonomous ﬂight using
an rgb-d camera. In Robotics Research. 2017. 2

[14] Andy Zeng, Shuran Song, Kuan-Ting Yu, Elliott Donlon,
Francois Robert Hogan, Maria Bauza, Daolin Ma, Orion
Taylor, Melody Liu, Eudald Romo, Nima Fazeli, Ferran
Alet, Nikhil Chavan Daﬂe, Rachel Holladay, Isabella Mo-
rona, Prem Qu Nair, Druck Green, Ian Taylor, Weber Liu,

[26] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. CVPR, 2017. 2

[27] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
CVPR, 2017. 2

[28] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
3d

guang Zhang, Xiaoou Tang, and Jianxiong Xiao.

6667

[45] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale convo-
lutional architecture. In ICCV, 2015. 6

shapenets: A deep representation for volumetric shapes. In
CVPR, 2015. 3

[29] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition.
In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
International Conference on, pages 922–928. IEEE, 2015. 3

[30] Charles R Qi, Hao Su, Matthias Nießner, Angela Dai,
Mengyuan Yan, and Leonidas J Guibas. Volumetric and
multi-view cnns for object classiﬁcation on 3d data. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 5648–5656, 2016. 3

[31] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In CVPR, 2017. 3, 4

[32] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In NIPS, 2017. 3, 4, 5

[33] Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. At-
In

tentional shapecontextnet for point cloud recognition.
CVPR, 2018. 3

[34] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,
Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.
Splatnet: Sparse lattice networks for point cloud processing.
In CVPR, 2018. 3

[35] Roman Klokov and Victor Lempitsky. Escape from cells:
Deep kd-networks for the recognition of 3d point cloud mod-
els. In ICCV. IEEE, 2017. 3

[36] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-cnn: Octree-based convolutional neu-
ral networks for 3d shape analysis. ACM Transactions on
Graphics (TOG), 36(4):72, 2017. 3

[37] Kenneth L Kelly. Twenty-two colors of maximum contrast.

Color Engineering, 3(26):26–27, 1965. 3

[38] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Learning representations and gen-
arXiv preprint

Leonidas Guibas.
erative models for 3d point clouds.
arXiv:1707.02392, 2017. 5

[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 5

[40] Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo
Larochelle, and Aaron Courville. Blindfold Baselines for
Embodied QA. arXiv preprint arXiv:1811.05013, 2018. 5, 7

[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems, pages 5998–6008, 2017. 5

[42] Jesse Thomason, Daniel Gordan, and Yonatan Bisk. Shifting
the baseline: Single modality performance on visual naviga-
tion & qa. arXiv preprint arXiv:1811.00613, 2018. 6

[43] Yuxin Wu and Kaiming He. Group normalization. arXiv

preprint arXiv:1803.08494, 2018. 6

[44] Alex Nash, Sven Koenig, and Craig Tovey. Lazy theta*:
Any-angle path planning and path length analysis in 3d. In
Third Annual Symposium on Combinatorial Search, 2010. 6

6668

