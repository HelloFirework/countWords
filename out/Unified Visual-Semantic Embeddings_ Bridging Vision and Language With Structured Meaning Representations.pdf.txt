“

”

Uniﬁed Visual-Semantic Embeddings:

Bridging Vision and Language with Structured Meaning Representations

“

”

Hao Wu1,3,4,6,∗,†

, Jiayuan Mao5,6,∗,†

, Yufeng Zhang2,6,†

, Yuning Jiang6, Lei Li6, Weiwei Sun1,3,4, Wei-Ying Ma6

1School of Computer Science, 2School of Economics, Fudan University

3Systems and Shanghai Key Laboratory of Data Science, Fudan University

4Shanghai Insitute of Intelligent Electroics & Systems

5ITCS, Institute for Interdisciplinary Information Sciences, Tsinghua University, 6Bytedance AI Lab

(cid:20)(cid:3)(cid:16)(cid:3)(cid:68)

{wuhao5688, zhangyf, wwsun}@fudan.edu.cn, mjy14@mails.tsinghua.edu.cn,
(cid:68)

{jiangyuning, lileilab, maweiying}@bytedance.com

(cid:65)

(cid:65)

Abstract

We propose the Uniﬁed Visual-Semantic Embeddings
(Uniﬁed VSE) for learning a joint space of visual representa-
tion and textual semantics. The model uniﬁes the embeddings
of concepts at different levels: objects, attributes, relations,
and full scenes. We view the sentential semantics as a combi-
nation of different semantic components such as objects and
relations; their embeddings are aligned with different image
regions. A contrastive learning approach is proposed for the
effective learning of this ﬁne-grained alignment from only
image-caption pairs. We also present a simple yet effective
approach that enforces the coverage of caption embeddings
on the semantic components that appear in the sentence. We
demonstrate that the Uniﬁed VSE outperforms baselines on
cross-modal retrieval tasks; the enforcement of the seman-
tic coverage improves the model’s robustness in defending
text-domain adversarial attacks. Moreover, our model em-
powers the use of visual cues to accurately resolve word
dependencies in novel sentences.

1. Introduction

We study the problem of establishing accurate and gener-
alizable alignments between visual concepts and textual se-
mantics efﬁciently, based upon rich but few, paired but noisy,
or even biased visual-textual inputs (e.g., image-caption
pairs). Consider the image-caption pair A shown in Fig. 1:
“A white clock on the wall is above a wooden table”. The align-
ments are formed at multiple levels: This short sentence can
be decomposed into a rich set of semantic components [3]:

∗indicates equal contribution.
†Work was done when HW, JM and YZ were intern researchers at the

Bytedance AI Lab.

Sentence

A   white   clock   on   the   wall   is   above   a   wooden   table.

Relational phrase

white 

clock 

above

wooden 

table

white 

clock 

on

wall

Object

white 

clock 

wooden 

table

wall

A white basin    is on a wooden   

basin

table
table   in front of the wall     .

wall

Pair A

Pair B

Figure 1. Two examplar image-caption pairs. Humans are able to
establish accurate and generalizable alignments between vision and
language, at different levels: objects, relations and full sentences.
Pair A and B form a pair of contrastive example for the concepts
clock and basin.

objects (clock, table and wall) and relations (clock
above table, and clock on wall). These components are
linked with different parts of the scene.

This motives our work to introduce Uniﬁed Visual-
Semantic Embeddings (Uniﬁed VSE for short) Shown in
Fig. 2, Uniﬁed VSE bridges visual and textual representation
in a joint embedding space that uniﬁes the embeddings for
objects (noun phrases vs. visual objects), attributes (prenom-
inal phrases vs. visual attributes), relations (verbs or prepo-
sitional phrases vs. visual relations) and scenes (sentence vs.
image).

There are two major challenges in establishing such a
factorized alignment. First, the link between the textual
description of an object and the corresponding image region
is ambiguous: A visual scene consists of multiple objects,
and thus it is unclear to the learner which object should be
aligned with the description. Second, it could be problematic
to directly learn a neural network that combines various
semantic components in a caption and form an encoding for
the full sentence, with the training objective to maximize the

16609

Embeddings of Image Local Regions

Global Pooling

Embedding of the Whole Image

“white clock”

“clock on wall”

“wall”

“A white clock on the wall 

is above a wooden table”

Furthermore, we show how our learned embeddings can
provide visual cues to assist the parsing of novel sentences,
including determining content word dependencies and la-
belling semantic roles for certain verbs. It ends up that our
model can build reliable connections between vision and
language using given semantic cues and in return, bootstrap
the acquisition of language.

(cid:31)

(cid:33)

Unified Visual-Semantic Embedding Space (Unit Ball)

Figure 2. We build a visual-semantic embedding space, which
uniﬁes the embeddings for objects, attributes, relations and full
scenes.

 

 

“

”

“

(cid:134)

 
cross-modal retrieval performance in the training set (e.g., in
[49, 30, 40]). As reported by [40], because of the inevitable
bias in the dataset (e.g., two objects may co-occur with each
other in most cases, see the table and the wall in Fig. 1
as an example), the learned sentence encoders usually pay
attention to only part of the sentence. As a result, they are
vulnerable to text-domain adversarial attacks: Adversarial
captions constructed from original captions by adding small
perturbations (e.g., by changing wall to be shelf) can
easily fool the model [40, 39].

“ ”

”

“

”

 

 

 

 

 

 

 

”

“

We resolve the aforementioned challenges by a natu-
ral combination of two ideas: cross-situational learning
and the enforcement of semantic coverage that regularizes
the encoder. Cross-situational learning, or learning from
contrastive examples [12], uses contrastive examples in the
dataset to resolve the referential ambiguity of objects: Look-
ing at both Pair A and B in Fig. 1, we know that Clock
should refer to an object that occurs only in scene A but
not B. Meanwhile, to alleviate the biases of datasets such as
object co-occurrence, we present an effective approach that
enforces the semantic converage: The meaning of a caption
is a composition of all semantic components in the sentence
[3]. Reﬂectively, the embedding of a caption should have a
coverage of all semantic components, while changing any of
them should affect the global caption embedding.

“

”

 

 

 

 

Conceptually and empirically, Uniﬁed VSE makes the

following three contributions.

First, the explicit factorization of the visual-semantic em-
bedding space enables us to build a ﬁne-grained correspon-
dence between visual and textual data, which further beneﬁts
a set of downstream visual-textual tasks. We achieve this
through a contrastive example mining technique that uni-
formly applies to different semantic components, in contrast
to the sentence or image-level contrastive samples used by
existing visual-semantic learning [49, 30, 11]. Uniﬁed VSE
consistently outperforms pre-existing approaches on a di-
verse set of retrieval-based tasks.

Second, we propose a caption encoder that ensures a cov-
erage of all semantic components appeared in the sentence.
We show that this regularization helps our model to learn
a robust semantic representation for captions. It effectively
defends adversarial attacks on the text domain.

 

 

 

 

“

”

“

“ ”

2. Related work

Visual semantic embedding. Visual semantic embedding
[13] is a common technique for learning a joint represen-
tation of vision and language. The embedding space em-
powers a set of cross-modal tasks such as image captioning
[43, 48, 8] and visual question answering [4, 47].

A fundamental technique proposed in [13] for aligning
two modalities is to use the pairwise ranking to learn a dis-
tance metric from similar and dissimilar cross-modal pairs
[44, 35, 23, 9, 28, 24]. As a representative, VSE++ [11] uses
the online hard negative mining (OHEM) strategy [41] for
data sampling and shows the performance gain. VSE-C [40],
based on VSE++, enhances the robustness of the learned
visual-semantic embeddings by incorporating rule-generated
textual adversarial samples as hard negatives during training.
In this paper, we present a contrastive learning approach
based on semantic components.

There are multiple VSE approaches that also use
linguistically-aware techniques for the sentence encoding
and learning. Hierarchical multimodal LSTM (HM-LSTM)
[33] and [46], as two examples, both leverage the con-
stituency parsing tree. Multimodal-CNN (m-CNN) [30] and
CSE [49] apply convolutional neural networks to the caption
and extract the a hierarchical representation of sentences.
Our model differs with them in two aspects. First, Uniﬁed
VSE is built upon a factorized semantic space instead of
the syntactic knowledge. Second, we employ a contrastive
example mining approach that uniformly applies to different
semantic components. It substantially improves the learned
embeddings, while the related works use only sentence-level
contrastive examples.

The learning of object-level alignment in uniﬁed VSE is
also related to [19, 21, 36], where the authors incorporate
pre-trained object detectors for the semantic alignment. [10]
propose a selective pooling technique for the aggregation of
object features. Compared with them, Uniﬁed VSE presents
a more general approach that embeds concepts of different
levels, while still requiring no extra supervisions.

(cid:134)

Structured representation for vision and language. We
connect visual and textual representations in a structured
embedding space. The design of its structure is partially
motivated by the papers on relational visual representations
(scene graphs) [29, 18, 17], where a scene is represented by
a set of objects and their relations. Compared with them, our

6610

“

Caption Alignment Loss

uobj

uattr

urel

usent

A white clock on the wall is above a table.

white  clock 

above

table

white  clock 

clock 

(

basic

nw

Share

(

basic

nw

Object
Encoder

Share

Object
Encoder

Share

Object
Encoder

Share

Object
Encoder

)

)

)

)

(

modif

aw

(

modif

nw

1x1 conv

V

C
N
N

7×7×d

Embeddings of 

Image Local Regions

Semantic Component Alignment Loss

obj(cid:65)

attr(cid:65)

rel(cid:65)

Neural 
Combiner

Share

Neural 
Combiner

urel

uattr

relation 
alignment

Sentence 

Combination

usent

ucomp

sent(cid:65)

(cid:65)

comp

Semantic Component 

Combination

uobj

attribute 
alignment

object 

alignment

Global Image 
Embedding

v

Global 
Pooling

ucomp

(cid:20)(cid:3)(cid:16)(cid:3)(cid:68)

(cid:68)

Caption Embedding 

for Retrieval

ucap

0.08

0.03

0.63

0.11

Text-to-Image Retrieval

Figure 3. Left: the architecture of Uniﬁed VSE. The semantic component alignment is learned from contrastive examples sampled from
factorized semantic space. The model also learns a caption encoder that combines the semantic components and aligns the caption with
the corresponding image. Right: An exemplar computation graph for retrieving images from texts. The presence of ucomp in the caption
encoding enforces the coverage of all semantic components. See Sec. 3.2 for details.

model does not rely on labelled graphs during training.

Researchers have designed various types of representa-
tions [5, 32] as well as different models [26, 50] for trans-
lating natural language sentences into structured represen-
tations. In this paper, we present how the usage of such
semantic parsing into visual-semantic embedding facilitates
the learning of the embedding space. Moreover, we present
how the learned VSE can, in return, helps the parser to re-
solve parsing ambiguities using visual cues.

3. Uniﬁed Visual-Semantic Embeddings

We now describe the overall architecture and training
paradigm for the proposed Uniﬁed Visual-Semantic Embed-
dings. Shown in Fig. 3, given an image-caption pair, we
ﬁrst parse the caption into a structured meaning represen-
tation, composed by a set of semantic components: object
nouns, prenominal modiﬁers, and relational dependencies.
We encode different types of semantic components with
type-speciﬁc encoders. A caption encoder combines the em-
bedding of the semantic components into a caption semantic
embedding. Jointly, we encode images with a convolutional
neural network (CNN) into the same, uniﬁed VSE space. The
distance between the image embedding and the sentential
embedding measures the semantic similarity between the
image and the caption.

We employ a multi-task learning approach for the joint
learning of embeddings for semantic components (as the
“basis” of the VSE space) as well as the caption encoder (as
the combiner of semantic components).

3.1. Visual-Semantic Embedding: A Revisit

We begin the section with an introduction to the two-
stream VSE approach. It jointly learns the embedding spaces
of two modalities: vision and language, and aligns them
using parallel image-text pairs (e.g., image and captions
from the MS-COCO dataset [27]).

Let v 2 Rd be the representation of the image and
u 2 Rd be the representation of a caption matching this

image, both encoded by neural modules. To archive the align-
ment, a bidirectional margin-based ranking loss has been
widely applied [11, 49, 15]. Formally, for an image (cap-
tion) embedding v (u), denote the embedding of its matched
caption (image) as u+ (v+). A negative (unmatched) cap-
tion (image) is sampled whose embedding is denoted as u−
(v−). We deﬁne the bidirectional ranking loss `sent between
captions and images as:

`sent =X

u

+X

v

Fv−  |  + s(u, v−)   s(u, v+)|+ 
Fu−  |  + s(u−, v)   s(u+, v)|+ 

(1)

, where   is a predeﬁned margin, |x|+ = max(x, 0) is the tra-
ditional ranking loss and Fx(·) = maxx(·) denotes the hard
negative mining strategy [11, 41]. s(·, ·) is a similarity func-
tion between two embeddings and is usually implemented
as cosine similarity [11, 40, 49].

3.2. Semantic Encodings

The encoding of a caption is made up of three steps.
As an example, consider the caption shown in Fig. 3, “A
white clock on the wall is above a wooden table”. 1)
We extract a structured meaning representation as a col-
lection of three types of semantic components: object
(clock, wall, table), attribute-object dependencies
(white clock, wooden table) and relational dependencies
(clock above table, clock on wall). 2) We encode each
component as well as the full sentence with type-speciﬁc
encoders into the uniﬁed VSE space. 3) We compose the em-
bedding of the caption by combining semantic components.

Semantic parsing. We implement a semantic parser 1 of
image captions based on [38]. Given the input sentence, the
parser ﬁrst performs a syntactic dependency parsing. A set
of rules is applied to the dependency tree and extracts object
entities appeared in the sentence, adjectives that modify the

1https://github.com/vacancy/SceneGraphParser

6611

object nouns, subjects/objects of the verbs and prepositional
phrases. For simplicity, we consider only single-word nouns
for objects and single-word adjectives for object attributes.

Encoding objects and attributes. We use an uniﬁed object
encoder   for nouns and adjective-noun pairs. For each word
w in the vocabulary, we initialize a basic semantic embed-
ding w(basic) 2 Rdbasic and a modiﬁer semantic embedding
w(modif ) 2 Rdmodif .

  w

(basic)
n

(modif )
n

For a single noun word wn (e.g., clock), we deﬁne its
, where   means
embedding wn as w
the concatenation of vectors. For an (adjective, noun) pair
(wa, wn) (e.g., (white, clock)), its embedding wa,n is
deﬁned as w
encodes the
attribute information. In implementation, the basic semantic
embedding is initialized from GloVe [34]. The modiﬁer
semantic embeddings (both w
randomly initialized and jointly learned. w
regarded as an intrinsic modiﬁer for each nouns.

(modif )
a

(modif )
n

(modif )
a

(modif )
n

(modif )
a

(basic)
n

where w

and w

can be

) are

  w

To fuse the embeddings of basic and modiﬁer semantics,

we employ a gated fusion function:

 (wn) = Norm( (W1wn + b1)) tanh(W2wn + b2)),

 (wa,n) = Norm( (W1wa,n + b1) tanh(W2wa,n + b2)).

Throughout the text,   denotes the sigmoid function:  (x) =
1/(1 + exp( x)), and Norm denotes the L2 normalization,
i.e., Norm(w) = w/kwk2. One may interpret   as a GRU
cell [7] taking no historical state.

Encoding relations and full sentence. Since relations and
sentences are the composed based on objects, we encode
them with a neural combiner  , which takes the embeddings
of word-level semantics encoded by   as input. In practice,
we implement   as an uni-directional GRU [7], and pick the
L2-normalized last state as the output.

To obtain a visual-semantic embedding for a relational
triple (ws, wr, wo) (e.g., (clock, above, table)), we
ﬁrst extract the word embeddings for the subject, relational
word and the object using  . We then feed the encoded
word embeddings in the same order into   and takes the
L2-normalized last state of the GRU cell. Mathematically,
urel =  (ws, wr, wo) =  ({ (ws),  (wr),  (wo)}).

The embedding of a sentence usent is computed over the

word sequence w1, w2, · · · wk of the caption:

usent =  ({ (w1),  (w2), · · · ,  (wk)}),

where for any word x,  (wx) =  (w

(basic)
x

  w

(modif )
x

)

Note that we share the weights of the encoders   and  
among the encoding processes of all semantic levels. This
allows our encoders of various types of components to boot-
strap the learning of each other.

Combining all of the components. A straight-forward im-
plementation of the caption encoder is to directly use the

sentence embedding usent, as it has already combined the
semantics of components in a contextually-weighted manner
[25]. However, it has been revealed in [40] that such com-
bination is vulnerable to adversarial attacks: Because of the
biases in the dataset, the combiner   usually focuses on only
a small set of semantic components appeared in the caption.
We alleviate such biases by enforcing the coverage
of the semantic components appeared in the sentence.
Speciﬁcally,
the
sentence embedding usent is combined with an explicit
bag-of-components embedding ucomp, as illustrated in
Fig. 3 (right). Mathematically, we deﬁne ucomp is computed
by the aggregation of all components in the sentence:

to form the caption embedding ucap,

ucomp = Norm (Φ ({uobj} [ {uattr} [ {urel})),

where Φ(·) is the aggregation function of semantic compo-
nents. Then the caption is encoded as: ucap = ↵usent +
(1   ↵)ucomp, where 0  ↵  1 is a scalar weight. The
presence of ucomp disallows the ignorance of any of the
components in the ﬁnal caption embedding ucap.

3.3. Image Encodings

We use CNN to encode the input RGB image into the
uniﬁed VSE space. Speciﬁcally, we choose a ResNet-152
model [14] pretrained on ImageNet [37] as the image en-
coder. We apply a layer of 1 ⇥ 1 convolution on top of the
last convolutaion layer (i.e., conv5_3) and obtain a convo-
lutional feature map of shape 7 ⇥ 7 ⇥ d for each image. d
denotes the dimension of the uniﬁed VSE space.

The feature map, denoted as V 2 R7×7×d, can be view
as the embeddings of 7 ⇥ 7 local regions in the image. The
embedding v for the whole image is deﬁned as the aggrega-
tion Ψ(·) of the embeddings at all regions through a global
spatial pooling operator.

3.4. Learning Paradigm

In this section, we present how to align vision and lan-
guage into the uniﬁed space using contrastive learning on
different semantic levels. The training pipeline is illustrated
in Fig. 3. We start from the generation of contrastive exampls
for different semantic components.

Negative example sampling. It has been discussed in [40]
that to explore a large compositional space of semantics, di-
rectly sampling negative captions from a human-built dataset
(e.g., MS-COCO captions) is not sufﬁcient. In this paper, in-
stead of manually deﬁne rules that augment the training data
as in [40], we address this problem by sampling contrastive
negative examples in the explicitly factorized semantic space.
The generation does not require manually labelled data, and
can be easily applied to any datasets. For a speciﬁc caption,
we generate the following four types of contrastive negative
samples.

6612

(cid:134)

• Nouns. We sample negative noun words from all nouns

that do not appear in the caption. 2

• Attribute-noun pairs. We sample negative pairs by
randomly substituting the adjective by another adjective
or substituting the noun.

(cid:134)

(cid:33)

(cid:31)

• Relational triples. We sample negative triples by ran-
domly substituting the subject, or the relation, or the
object. Moreover, we also sample the whole relational
triples of captions in the dataset which describe other
images, as the negative triples.

• Sentences. We sample negative sentences from the
whole dataset. Meanwhile, following [13, 11], we also
sample negative images from the whole dataset as con-
trastive images.

The key motivation behind our visual-semantic alignment
is that: an object appears in a local region of the image, while
the aggregation of all local regions should be aligned with
”
the full semantics of a caption.

“

 

 

 

 

“

”

“ ”

Local region-level alignment.
In detail, we propose a
relevance-weighted alignment mechanism for linking textual
object descriptors and local image regions. As shown in
Fig. 4, consider the embedding of a positive textual object
descriptor u+
o and
the set image local region embeddings Vi where i 2 7 ⇥ 7
extracted from the image. We generate a relevance map
M 2 R7×7 with Mi, i 2 7 ⇥ 7 representing the relevance
between u+
o and Vi, computed as as Eq. (2). We compute
the loss for noun and (adjective, noun) pairs by:

o , a negative textual object descriptor u−

Mi =

o , Vi))

exp(s(u+
Pj exp(s(u+

o , Vj))

`obj = X

i∈7×7

⇣Mi ·     + s(u−

o , Vi)   s(u+

o , Vi)  +⌘ (3)

The intuition behind the deﬁnition is that, we explicitly try
to align the embedding at each image region with u+
o . The
losses are weighted by the matching score, thus reinforce the
correspondence between u+
o and the matched region. This
technique is related to multi-instance learning [45].

Global image-level alignment. For relational triples urel,
semantic components aggregations ucomp and sentences
usent, their semantics usually cover multiple objects. Thus,
we align them with the full image embedding v via bidi-
rectional ranking losses as Eq. (1)3. The alignment loss is
denoted as `rel, `comp and `sent, respectively.

(cid:134)

“

”

We want to highlight that, during training, we separately
align the two type of semantic representations of the caption,
i.e., usent and ucomp, with the image. This differs from the
inference-time computation of the caption. Recall that ↵ can
be viewed as a factor that balances the training objective and

u<cat>

u<clock>

Relevance Map 

(as weights on 7×7 ranking losses)

d

d

s

(
u

(cid:31)

clock

(cid:33)

,

V
i

)

softmax

push

pull

(cid:134)

7×7

V

7×7

ranking loss

Weighted Sum

obj(cid:65)

7×7×d

Margin-based Ranking Losses 

at Each Local Region

Figure 4. An illustration of our relevance-weighted alignment mech-
anism. The relevance map shows the similarity of each region with
the object embedding u<clock>. We weight the alignment loss with
the map to reinforce the correspondence between the u<clock> and
its matched region.

the enforcement of semantic coverage. This allows us to
ﬂexibly adjust ↵ during inference.

3.5. Implementation details

We use d = 1024 as the dimension of the uniﬁed VSE
space like [11, 40, 49]. We train the model by minimizing
the alignment losses in a multi-task learning way.

` = `sent + ⌘c`comp + ⌘o`obj + ⌘a`attr + ⌘r`rel

(4)

In the ﬁrst 2 epochs, we set ⌘c, ⌘o and ⌘a to 0.5 and ⌘r to 0
for learning single-object level representations. Then we turn
up ⌘r to 1.0 to make the model learn relational semantics. To
make the comparison with related works fair, we always ﬁx
the weights of the ResNet. We use the Adam [22] optimizer
with learning rate at 0.001. For model details, please refer to
our supplementary material.

We evaluate our model on the MS-COCO [27] dataset. It
contains 82,783 training images with each image annotated
by 5 captions. We use the common 1K validation and test
split from [19]. We also report the performance on a 5K test
split for comparison with [49, 11, 42].

We begin this section with the evaluation of traditional
cross-modal retrieval. Next, we validate the effectiveness of
enforcing the semantic coverage of caption embeddings by
comparing models on cross-modal retrieval tasks with ad-
versarial examples. We then propose a uniﬁed text-to-image
retrieval task to support the contrastive learning on various
semantic components. We end this section with an applica-
tion of using visual cues to facilitate the semantic parsing
of novel sentences. Due to the limitation of the text length,
for mode details on data processing, metrics and model im-
plementation, we refer the readers to our supplementary
material.

4.1. Overall Evaluation on Cross-Modal Retrieval.

(2)

4. Experiments

2For the MS-COCO dataset, in all 5 captions associated with the same

image. This also applies to other components.

3Only textual negative samples are used for `rel.

We ﬁrst show the performance of image-to-sentence and
sentence-to-image retrieval tasks to evaluate learned visual-
semantic embeddings. We report the R@1 (recall@1), R@5,

6613

“

”

Task
Metric

m-RNN [31]
DVSA [20]
MNLM [24]
m-CNN [30]

HM-LSTM[33]

Order-embedding [42]

VSE-C [40, 1]

DeepSP[44]
2WayNet [9]

sm-LSTM [15]
RRF-Net[28]
VSE++ [11, 2]

CSE[49]

UniVSE (Ours)

Order-embedding [42]

VSE-C[11, 1]

CSE[49]

VSE++[11, 2]
UniVSE (Ours)

Image-to-sentence Retrieval

Sentence-to-image Retrieval

R@1 R@5 R@10 Med. r

R@1 R@5 R@10 Med. r

rsum

1K testing split (5,000 captions)

41.0
38.4
43.4
42.8
43.9
46.7
48.0
50.1
55.8
53.2
56.4
57.7
56.3
64.3

23.3
22.3
27.9
31.7
36.1

73.0
69.9
75.7
73.1

-
-

81.0
79.7
75.2
83.1
85.3
86.0
84.4
89.2

83.5
80.5
85.8
84.1
87.8
88.9
89.2
89.2

-

91.5
91.5
94.0
92.2
94.8

2
1
-
3
2
2
2
-
-
1
-
1
1
1

29.0
27.4
31.0
32.6
36.1
37.9
39.7
39.6
39.7
40.7
43.9
42.8
45.7
48.3

5K testing split (25,000 captions)
-

51.1
57.1
60.9
66.4

65.0
65.1
70.4
72.7
77.7

5
5
4
3
3

18.0
18.7
22.2
22.1
25.4

42.2
60.2
66.7
68.6

-
-

72.9
75.2
63.3
75.8
78.1
77.2
81.2
81.7

-

43.8
50.2
49.0
53.0

77.0
74.8
79.9
82.8
86.7
85.9
83.2
86.9

-

87.4
88.6
87.4
90.6
91.2

57.6
56.7
64.4
62.7
66.2

3
3
-
3
3
2
2
-
-
2
-
2
2
2

7
7
5
6
5

345.7
351.2
382.5
384.0

-
-

414
420.7

-

431.8
443.8
445.1
450.4
469.5

-

257.7
292.2
299.1
324.8

Table 1. Results of cross-modal retrieval task on MS-COCO dataset (1K and 5K testing split). All listed baselines and our models ﬁx weights
of the image encoders. For fair comparison, we do not include [10] and [16] that ﬁnetunes the image encoder or adds extra training data.

Metric
VSE++
VSE-C

UniVSE (usent+ucomp)

UniVSE (usent)

UniVSE (usent+uobj )
UniVSE (usent+uattr)
UniVSE (usent+urel)

Object attack
R@10
R@5
81.4
69.6
85.6
76.0
78.3
87.3
85.5
76.4
85.6
77.2
83.3
73.9
77.1
85.5

R@1
32.3
41.1
45.3
40.7
42.9
40.1
45.4

rsum
183.3
202.7
210.9
202.6
205.7
197.3
208.0

Attribute attack
R@5
59.4
61.0
71.5
70.5
69.0
72.0
68.1

R@10
76.0
74.3
83.1
80.6
79.8
81.9
78.5

rsum
155.2
162.0
189.9
181.1
178.9
191.3
175.8

R@1
19.8
26.7
35.3
30.0
30.1
37.4
29.2

Relation attack
R@10
R@5
78.7
66.8
81.5
71.1
76.5
86.7
83.5
72.6
83.6
71.2
81.9
70.0
77.5
85.6

rsum
171.6
188.1
202.2
188.7
188.8
182.4
205.9

R@1
26.1
35.5
39.0
32.6
34.0
30.5
42.8

total sum

510.1
552.8
603.0
572.4
573.4
571.0
589.7

Table 2. Results on image-to-sentence retrieval task with text-domain adversarial attacks. For each caption, we generate 5 adversarial fake
captions which do not match the images. Thus, the models need to retrieve 5 positive captions from 30,000 candidate captions.

R@10, and the median retrieval rank as in [11, 40, 49, 15].
To summarize the performance, we compute rsum as the
summation of R@1, R@5, and R@10.

Shown in Table 1, Uniﬁed VSE outperforms other base-
lines with various model architecture and training techniques
[11, 49, 28, 40, 15]. This validates the effectiveness learn-
ing visual-semantic embeddings in the explicitly factorized
visual-semantic embedding space. We also include the re-
sults under more challenging 5K test split. The gap between
Uniﬁed VSE and other models gets further enlarged across
all metrics.

4.2. Retrieval under text-domain adversarial attack

Recent works [40, 39] have raised their concerns on the
robustness of the learned visual-semantic embeddings. They
show that existing models are vulnerable to text-domain
adversarial attacks (i.e., using adversarial captions) and can
be easily fooled. This is closely related to the bias in small
datasets over a large, compositional semantic space [40]. To
prove the robustness of the learned unifed VSE, we further

conduct experiments on the image-to-sentence retrieval task
with text-domain adversarial attacks. Following [40], we
ﬁrst design several types of adversarial captions by adding
perturbations to existing captions.

1. Object attack: Randomly replace / append by an irrel-

evant one in the original caption.

2. Attribute attack: Randomly replace / add an irrelevant
attribute modiﬁer for one object in the original caption.
3. Relational attack: 1) Randomly replace the sub-
ject/relation/object word by an irrelevant one. 2) Ran-
domly select an entity as a subject/object and add an
irrelevant relational word and object/subject.

We include VSE++ and VSE-C as the baselines and show
the results in Table 2 where different columns represent
different types of attacks. VSE++ performs worst as it is
only optimized for the retrieval performance on the dataset.
Its sentence encoder is insensitive to a small perturbation in
the text. VSE-C explicitly generates the adversarial captions
based on human-designed rules as hard negative examples
during training, which makes it relatively robust to those

6614

1
@
R

60

50

40

30

1
@
R

45

40

35

30

 obj attack (img-to-sent)
 attr attack (img-to-sent)
 rel attack (img-to-sent)

 image-to-sentence retrieval (no attack)
 sentence-to-image retrieval (no attack)

0.0

0.2

0.4

(cid:68)

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

(cid:68)

Task

VSE++
VSE-C

UniVSEall
UniVSEobj
UniVSEattr
UniVSErel

obj

29.95
27.48
39.49
39.71
31.31
37.55

attr

26.64
28.76
33.43
33.37
37.51
32.70

rel

obj (det)

27.54
26.55
39.13
34.38
34.73
39.57

50.57
46.20
58.37
56.84
52.26
59.12

sum

134.70
128.99
170.42
164.30
155.81
168.94

(a) Normal cross-modal
(5,000 captions)

retrieval

(b) Adversarial attacked image-to-
sentence retrieval (30,000 captions)

Table 3. The mAP performance on the uniﬁed text-to-image
retrieval task. Please refer to the text for details.

Figure 5. The performance of UniVSE on cross-modal retrieval
tasks with different combination weight α. Our model can effective
defending adversarial attacks, with no sacriﬁce for the performance
on other tasks by choosing a reasonable α (thus we set α = 0.75
in all other experiments).

with adversarial attacks. Recall that ↵ can be viewed as a fac-
tor that balances the training objective and the enforcement
of semantic coverage. By choosing ↵ from a reasonable
range (0.6 to 0.8), our model can effective defend adversarial
attacks, with no sacriﬁce for the overall performance.

adversarial attacks. Uniﬁed VSE shows strong robustness
across all types of adversarial attacks.

It is worth noting that VSE-C shows inferior perfor-
mances in the normal retrieval tasks without adversarial
captions (see Table 1), even compared with VSE++. Con-
sidering that VSE-C shares the exactly the same model ar-
chitecture as VSE++, we can conclude that directly adding
adversarial captions during training, although improves mod-
els’ robustness, may sacriﬁce the performance on other tasks.
In contrast, the ability of Uniﬁed VSE to defend adversar-
ial texts comes almost for free: we present zero adversarial
captions during training. Uniﬁed VSE builds ﬁne-grained
semantic alignments via the contrastive learning of semantic
components. It use the explicit aggregation of the compo-
nents ucomp to alleviate the dataset biases.

Ablation study: semantic components. We now delve into
the effectiveness of different semantic components by choos-
ing different combinations of components for the caption
embedding. Shown in Table 2, we use different subsets of
the semantic components to form the bag-of-component em-
beddings ucomp. For example, in UniVSEobj , only object
nouns are selected and aggregated as ucomp.

The results demonstrate the effectiveness of the enforce-
ment of semantic coverage: even if the semantic compo-
nents have got ﬁne-grained alignment with visual concepts,
directly using usent as the caption encoding still degener-
ates the robustness against adversarial examples. Consistent
with the intuition, enforcing of coverage of a certain type
of components (e.g., objects) helps the model to defend the
adversarial attacks of the same type (e.g., defending adver-
sarial attacks of nouns). Combining all components leads to
the best performance.

Choice of the combination factor: ↵. We study the choice
of ↵ by conducting experiments on both normal retrieval
tasks and the adversarial one. Fig 4.2 shows the R@1 perfor-
mance under the normal/adversarial retrieval scenario w.r.t.
different choices of ↵. We observe that the ucomp term con-
tributes little on the normal retrieval tasks but largely on tasks

4.3. Uniﬁed Text-to-Image Retrieval

We extend the word-to-scene retrieval used by [40] into
a general uniﬁed text-to-image retrieval task. In this task,
models receive queries of different semantic levels, including
single words (e.g., “Clock.”), noun phrases (e.g., “White
clock.”), relational phrases (e.g., “Clocks on wall”) and full
sentences. For all baselines, the texts of different types as
treated as full sentences. The result is presented in Table 3.
We generate positive image-text pairs by randomly choos-
ing an image and a semantic component from 5 matched
captions with the chosen image. It is worth mention that
the semantic components extracted from captions may not
cover all visual concepts in the corresponding image, which
makes the annotation noisy. To address this, we also leverage
the MS-COCO detection annotations to facilitate the evalua-
tion (see obj(det) column). We treat the labels for detection
bounding boxes as the annotation of objects in the scene.

Ablation study: contrastive learning of components. We
evaluate the effectiveness of using contrastive samples
for different semantic components. Shown in Table 3,
UniVSEobj denotes the model trained with only contrastive
samples of noun components. The same notation applies
to other models. The UniVSE trained with a certain type
of contrastive examples (e.g., UniVSEobj with contrastive
nouns) consistently improves the retrieval performance of
the same type of queries (e.g., retrieving images from a sin-
gle noun). UniVSE trained with all kinds of contrastive
samples performs best in overall and shows a signiﬁcant gap
w.r.t. other baselines.

Visualization of the semantic alignment. We visualize the
semantic-relevance map on an image w.r.t. a given query uq
for a qualitative evaluation of the alignment performance of
various semantic components. The map Mi is computed as
the similarity between each image region vi and uq, in a
similar way as Eq. (2). Shown as Fig. 6, this visualization
helps to verify that our model successfully aligns different
semantic components with the corresponding image regions.

6615

√

 

 

Query: black dog

Query: white dog

Query: player swing bat

Retrieved

Image

Relevance

Map

Grounded

Area

Matching Score

0.257

0.255

0.247

0.211

0.205

0.302

 

 
0.286

0.255

0.251

0.247

0.490

0.406

0.404

0.393

0.359

Figure 6. The relevance maps and grounded areas obtained from the retrieved images w.r.t. three queries. The temperature of the softmax
for visualizing the relevance map is τ = 0.1. Pixels in white indicates a higher matching score. Note that the third image of the query “black
dog” contains two dogs, while our model successfully locates the black one (on the left). It also succeeded in ﬁnding the white dog in the
ﬁrst image of “white dog”. Moreover, for the query “player swing bat”, although there are many players in the image, our model only attend
to the man swinging the bat.

eat

0.42

in

sweater

blue

A

girl

in

blue

sweater

eating

burger

girl

0.48

eat

burger

ambiguity 
in parsing

eat

0.42

in

sweater

blue

A

girl

in

blue

sweater

eating

burger

girl

eat

0.28

burger

Relation: eat

√

object

subject
girl

girl

sweater

burger

-

0.3118

0.4850

sweater

0.3559

-

0.2794

burger

0.2705

0.1877

-

×

Matching score of all possible combinations 
w.r.t. the relation eat,  to the image

Figure 7. Example showing that Uniﬁed VSE can leverage image to parse sentences with ambiguity. The matching score of “girl eat burger”
is much higher than “sweater eat burger”, which resolves the ambiguity. Other components are also correctly inferred.

Task

attributed object

relational phrase

Random
VSE++
VSE-C
UniVSE

37.41
41.12
43.44
64.82

31.90
43.31
41.08
62.69

Table 4. The accuracy of different models on recovering word
dependencies with visual cues. In the “Random” baseline, we
randomly assign the word dependencies.

4.4. Semantic Parsing with Visual Cues

 

 

 

 

 

As a side application, we show how the learned uniﬁed
VSE space can provide the visual cues to help the semantic
parsing of sentences. Fig. 7 shows the general idea. When
parsing a sentence, ambiguity may occur, e.g., the subject of
the relational word eat may be sweater or burger. It
is not easy for a textual parser to decide which one is correct
because of the innate syntactic ambiguity. However, we can
use the image which is depicted by this sentence to assist the
parsing by. This is related to previous works on using image
segmentation models to facilitate the sentence parsing [6].

This motivates us to design two tasks, 1) recovering the
dependency between attributes and entities, and 2) recover-
ing the relational triples. In detail, we ﬁrst extract the entities,
attributes and relational words from the raw sentence without
knowing their dependencies. For each possible combination
of certain semantic component, our model computes its em-
bedding in the uniﬁed joint space. E.g., in Fig. 7, there are
in total 3 ⇥ (3   1) = 6 possible dependencies for eat.
We choose the combination with the highest matching score
with the image to decide the subject/object dependencies of

the relation eat. We use parsed semantic components as the
ground-truth and report the accuracy, deﬁned as the fraction
of the number of correct dependency resolution and the total
number of attributes/relations. Table 4 reports the results
on assisting semantic parsing with visual cues, compared
with other baselines. Fig. 7 shows a real case in which we
successfully resolve the textual ambiguity.

5. Conclusion

We present a uniﬁed visual-semantic embedding approach
that learns a joint representation space of vision and language
in a factorized manner: Different levels of textual semantic
components such as objects and relations get aligned with
regions of images. A contrastive learning approach for se-
mantic components is proposed for the efﬁcient learning
of the ﬁne-grained alignment. We also introduce the en-
forcement of semantic coverage: each caption embedding
should have a coverage of all semantic components in the
sentence. Uniﬁed VSE shows superiority on multiple cross-
modal retrieval tasks and can effectively defend text-domain
adversarial attacks. We hope the proposed approach can
empower machines that learn vision and language jointly,
efﬁciently and robustly.

6. Acknowledgements

We thank Haoyue Shi for helpful discussions and sug-
gestions. This research is supported in part by the National
Key Research and Development Program of China under
grant 2018YFB0505000 and the National Natural Science
Foundation of China under grant 61772138.

6616

References

[1] VSE-C open-sourced code. https://github.com/

ExplorerFreda/VSE-C. 6

[2] VSE++ open-sourced code. https://github.com/

fartashf/vsepp. 6

[3] O. Abend, T. Kwiatkowski, N. J. Smith, S. Goldwater, and
M. Steedman. Bootstrapping Language Acquisition. Cogni-
tion, 164:116–143, 2017. 1, 2

[4] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual Question Answering. In
Proceedings of IEEE International Conference on Computer
Vision (ICCV), 2015. 2

[5] L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Grif-
ﬁtt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. Abstract Meaning Representation for Sembank-
ing. In Linguistic Annotation Workshop and Interoperability
with Discourse, 2013. 3

[6] G. Christie, A. Laddha, A. Agrawal, S. Antol, Y. Goyal,
K. Kochersberger, and D. Batra. Resolving Language
and Vision Ambiguities Together: Joint Segmentation &
Prepositional Attachment Resolution in Captioned Scenes.
arXiv:1604.02125, 2016. 8

[7] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical
Evaluation of Gated Recurrent Neural Networks on Sequence
Modeling. In NIPS 2014 Workshop on Deep Learning, 2014.
4

[8] J. Donahue, L. Anne Hendricks, S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell.
Long-Term Recurrent Convolutional Networks for Visual
Recognition and Description.
In Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2015. 2

[9] A. Eisenschtat and L. Wolf. Linking Image and Text with
2-way Nets. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017. 2, 6

[10] M. Engilberge, L. Chevallier, P. P´erez, and M. Cord. Finding
Beans in Burgers: Deep Semantic-Visual Embedding with Lo-
calization. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2, 6

[11] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler. VSE++:
Improving Visual-Semantic Embeddings with Hard Nega-
tives. In Proceedings of British Machine Vision Conference
(BMVC), 2018. 2, 3, 5, 6

[12] A. Fazly, A. Alishahi, and S. Stevenson. A Probabilistic
Computational Model of Cross-Situational Word Learning.
Cognitive Science, 34(6):1017–1063, 2010. 2

[13] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A Deep Visual-Semantic Embed-
ding Model. In Advances in Neural Information Processing
Systems (NIPS), 2013. 2, 5

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning
for Image Recognition. In Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
4

In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 3, 6

[16] Y. Huang, Q. Wu, and L. Wang. Learning Semantic Concepts
and Order for Image and Sentence Matching. In Proceed-
ings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 6

[17] J. Johnson, A. Gupta, and L. Fei-Fei.

Image Generation
from Scene Graphs. In Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 2

[18] J. Johnson, R. Krishna, M. Stark, L. Li, D. A. Shamma,
M. S. Bernstein, and L. Fei-Fei. Image Retrieval using Scene
Graphs. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2015. 2

[19] A. Karpathy and L. Fei-Fei. Deep Visual-Semantic Align-
ments for Generating Image Descriptions. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 2, 5

[20] A. Karpathy and L. Fei-Fei. Deep Visual-Semantic Align-
ments for Generating Image Descriptions. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 6

[21] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep Fragment Em-
beddings for Bidirectional Image Sentence Mapping. In Ad-
vances in Neural Information Processing Systems (NIPS),
2014. 2

[22] D. P. Kingma and J. Ba. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2017. 5

[23] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal Neural
Language Models. In Proceedings of International Confer-
ence on Machine Learning (ICML), 2014. 2

[24] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying Visual-
Semantic Embeddings with Multimodal Neural Language
Models. arXiv:1411.2539, 2014. 2, 6

[25] O. Levy, K. Lee, N. FitzGerald, and L. Zettlemoyer. Long
Short-Term Memory as a Dynamically Computed Element-
wise Weighted Sum. arXiv:1805.03716, 2018. 4

[26] P. Liang, M. I. Jordan, and D. Klein. Learning Dependency-
Based Compositional Semantics. Computational Linguistics,
39(2):389–446, 2013. 3

[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon Objects in Context. In Proceedings of European Confer-
ence on Computer Vision (ECCV), 2014. 3, 5

[28] Y. Liu, Y. Guo, E. M. Bakker, and M. S. Lew. Learning a
Recurrent Residual Fusion Network for Multimodal Match-
ing. In Proceedings of IEEE International Conference on
Computer Vision (ICCV), 2017. 2, 6

[29] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual
Relationship Detection with Language Priors. In Proceedings
of European Conference on Computer Vision (ECCV), 2016.
2

[30] L. Ma, Z. Lu, L. Shang, and H. Li. Multimodal Convolu-
tional Neural Networks for Matching Image and Sentence. In
Proceedings of IEEE International Conference on Computer
Vision (ICCV), 2015. 2, 6

[15] Y. Huang, W. Wang, and L. Wang. Instance-Aware Image
and Sentence Matching with Selective Multimodal LSTM.

[31] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep Captioning with Multimodal Recurrent Neural Net-

6617

[46] F. Xiao, L. Sigal, and Y. J. Lee. Weakly-Supervised Visual
Grounding of Phrases with Linguistic Structures. In Proceed-
ings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 2

[47] H. Xu and K. Saenko. Ask, Attend and Answer: Explor-
ing Question-Guided Spatial Attention for Visual Question
Answering. In Proceedings of European Conference on Com-
puter Vision (ECCV), 2016. 2

[48] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural
Image Caption Generation with Visual Attention.
In Pro-
ceedings of International Conference on Machine Learning
(ICML), 2015. 2

[49] Q. You, Z. Zhang, and J. Luo. End-to-End Convolutional
Semantic Embeddings. In Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
2, 3, 5, 6

[50] L. S. Zettlemoyer and M. Collins. Learning to Map Sentences
to Logical Form: Structured Classiﬁcation with Probabilis-
tic Categorial Grammars. In Proceedings of Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), 2005. 3

works (m-RNN). In Proceedings of International Conference
on Learning Representations (ICLR), 2015. 6

[32] R. Montague. Universal Grammar. Theoria, 36(3):373–398,

1970. 3

[33] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Hierarchical
Multimodal LSTM for Dense Visual-Semantic Embedding. In
Proceedings of IEEE International Conference on Computer
Vision (ICCV), 2017. 2, 6

[34] J. Pennington, R. Socher, and C. Manning. GloVe: Global
Vectors for Word Representation. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP), 2014. 4

[35] Z. Ren, H. Jin, Z. Lin, C. Fang, and A. Yuille. Joint Image-
Text Representation by Gaussian Visual-Semantic Embed-
ding. In Proceedings of ACM Multimedia (ACM-MM), 2016.
2

[36] Z. Ren, H. Jin, Z. Lin, C. Fang, and A. Yuille. Multiple
In Proceedings of

Instance Visual-Semantic Embedding.
British Machine Vision Conference (BMVC), 2017. 2

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. International Journal of Computer Vision (IJCV),
115(3):211–252, 2015. 4

[38] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D.
Manning. Generating Semantically Precise Scene Graphs
from Textual Descriptions for Improved Image Retrieval. In
Workshop on Vision and Language (VL15), Lisbon, Portugal,
2015. 3

[39] R. Shekhar, S. Pezzelle, Y. Klimovich, A. Herbelot, M. Nabi,
E. Sangineto, and R. Bernardi. FOIL it! Find One Mismatch
between Image and Language Caption. arXiv:1705.01359,
2017. 2, 6

[40] H. Shi, J. Mao, T. Xiao, Y. Jiang, and J. Sun. Learning
Visually-Grounded Semantics from Contrastive Adversarial
Samples.
In Proceedings of International Conference on
Computational Linguistics (COLING), 2018. 2, 3, 4, 5, 6, 7

[41] A. Shrivastava, A. Gupta, and R. Girshick. Training Region-
Based Object Detectors with Online Hard Example Mining.
In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 2, 3

[42] I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun. Order-
Embeddings of Images and Language. In Proceedings of In-
ternational Conference on Learning Representations (ICLR),
2016. 5, 6

[43] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
Tell: A Neural Image Caption Generator. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 2

[44] L. Wang, Y. Li, and S. Lazebnik. Learning Deep Structure-
Preserving Image-Text Embeddings. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016. 2, 6

[45] J. Wu, Y. Yu, C. Huang, and K. Yu. Deep Multiple Instance
Learning for Image Classiﬁcation and Auto-Annotation. In
Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2015. 5

6618

