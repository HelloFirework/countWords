Single Image Reﬂection Removal Beyond Linearity

Qiang Wen1, Yinjie Tan1, Jing Qin2, Wenxi Liu3, Guoqiang Han1, Shengfeng He1 ∗

1 School of Computer Science and Engineering, South China University of Technology

2 Department of Nursing, Hong Kong Polytechnic University

3 College of Mathematics and Computer Science, Fuzhou University

s
i
s
e
h
t
n
y
S
e
g
a
m

I

l
a
v
o
m
e
R
n
o
i
t
c
e
ﬂ
e
R

Transmission

Reﬂection

Linear Synthesis [25]

Clipped Linear Synthesis [2]

Our Synthesis

Real-world Scene

Transmission from [2]

Transmission from [30]

Transmission from [31]

Our Transmission

Figure 1: Existing reﬂection removal methods rely heavily on the linearly synthesized data, which, however, cannot simulate
the real-world reﬂections. We propose to synthesize and remove reﬂection beyond linearity, leading to the controllable
synthesis and clean reﬂection removal.

Abstract

Due to the lack of paired data, the training of image
reﬂection removal relies heavily on synthesizing reﬂection
images. However, existing methods model reﬂection as a
linear combination model, which cannot fully simulate the
real-world scenarios. In this paper, we inject non-linearity
into reﬂection removal from two aspects. First, instead of
synthesizing reﬂection with a ﬁxed combination factor or
kernel, we propose to synthesize reﬂection images by pre-
dicting a non-linear alpha blending mask. This enables a
free combination of different blurry kernels, leading to a
controllable and diverse reﬂection synthesis. Second, we
design a cascaded network for reﬂection removal with three
tasks: predicting the transmission layer, reﬂection layer,
and the non-linear alpha blending mask. The former two
tasks are the fundamental outputs, while the latter one be-
ing the side output of the network. This side output, on
the other hand, making the training a closed loop, so that
the separated transmission and reﬂection layers can be re-
combined together for training with a reconstruction loss.
Extensive quantitative and qualitative experiments demon-
strate the proposed synthesis and removal approaches out-

performs state-of-the-art methods on two standard bench-
marks, as well as in real-world scenarios.

1. Introduction

Undesired reﬂection from glasses not only damages
the image quality, but also inﬂuences the performance of
computer vision tasks like image classiﬁcation. To re-
move reﬂection, early researches design hand-crafted priors
[11, 13, 18, 26], while recent works [2, 31, 25, 29] train
deep models to remove reﬂection patterns.

Notwithstanding the demonstrated success of deep re-
ﬂection removal models, the arguably most critical chal-
lenge is to obtain sufﬁcient paired training data, which in-
cludes the reﬂection images and their corresponding clean
transmission images. To synthesize reﬂection data, existing
methods [2, 31, 25, 29] simply blend two images with a lin-
ear model. Particularly, they express the reﬂection image
S ∈ [0, 1]m×n×3 as the linear combination of the trans-
mission layer T ∈ [0, 1]m×n×3 and the reﬂection layer
R ∈ [0, 1]m×n×3:

S = αT + (1 − α) (K ⊗ R) ,

(1)

∗Corresponding author (hesfe@scut.edu.cn).

where α ∈ (0.5, 1) is the combination factor, ⊗ denotes a

13771

convolution operator, and K represents a Gaussian blurring
kernel. However, blending two images with a constant α
does not simulate the complex real-world reﬂection. The
formation of reﬂection image depends on the relative posi-
tion of the camera to the image plane and on the lighting
conditions [9].

In this paper, we revisit this challenging synthesis prob-
lem. We observe that the synthesis of reﬂection images
should be non-linearly combined with an alpha blending
mask. To this end, we propose a deep synthesis network
SynNet to predict the alpha blending mask of two input im-
ages. This mask can be freely combined with a user deﬁned
kernel to simulate different types of reﬂections. This is in-
dispensable for generating the controllable and diverse re-
ﬂection data. To properly train the network, we selectively
collect a large number of real data with different types of
reﬂections.

On the other end, we involve the non-linear alpha blend-
ing mask into our reﬂection removal process. We design
a cascaded reﬂection removal network RmNet which has
three tasks: predicting the reﬂection layer, transmission
layer, and the alpha blending mask. The ﬁrst two are the
essential outputs of the reﬂection separation, while the last
one is treated as the side output that aids the network train-
ing. We use the predicted mask to re-combine the sepa-
rated transmission and reﬂection layers.
In this way, the
re-combined reﬂection should be consistent with the origi-
nal input, and the entire network is a closed loop thus can
be guided with a reconstruction loss.
In summary, our contributions are:

• We revisit the single image reﬂection removal prob-
lem, and synthesize reﬂection data beyond linearity.
In particular, we propose to predict a non-linear alpha
blending mask, which enables a controllable and di-
verse reﬂection data synthesis.

• We present a reﬂection removal network with the aid of
the predicted alpha blending mask. This mask serves
as the side output of our network, so that the predic-
tions in the ﬁrst stage can be re-combined together, and
the network can be supervised with a reconstruction
loss.

• The proposed networks outperform existing reﬂec-
tion synthesis and removal methods, on two standard
benchmarks and in real-world scenarios.

2. Related Work

Single-image reﬂection removal.

Reﬂection re-
moval/separation has been a long-standing problem. Early
researches many address this problem by proposing differ-
ent image priors [11, 10, 19, 1, 13, 26, 12, 22]. For exam-
ple, sparse gradient priors [11] are used to distinguish the

reﬂection and transmission layers. Li and Brown [13] ex-
tracts the reﬂection and transmission layers using a smooth
gradient prior by assuming that reﬂections are often less in
focus. These priors may work well on the speciﬁc cases, but
cannot generalize to different types of reﬂections.

As a consequence, deep models are adopted for remov-
ing reﬂection. Fan et al.[2] propose the ﬁrst attempt to solve
this ill-posed problem with a deep network. They use an
edge map as the additional cue to guide the layer separation.
Wan et al.[25] develop a two-stage network, while the ﬁrst
one inferences the gradient of the transmission layer, and
the second one associates with the gradient output to predict
the ﬁnal transmission layer. Without augmented cues, Yang
et al.[29] propose a bidirectional network that separately
predicts reﬂection and transmission layers, and then uses
the predicted reﬂection to estimate the transmission layer
in the second step. To exploit low-level and high-level im-
age information, Zhang et al.[31] introduce two perceptual
losses and an exclusion loss into a fully convolutional net-
work. However, all the above methods suffer from the linear
image synthesis model, preventing these methods from gen-
eralizing to different real-world scenarios. Although two
reﬂection datasets [24, 31] have been proposed, they are
too small and captured with a conditioned environment, i.e.,
only one type of reﬂection. These problems motivate us to
synthesize realistic reﬂections beyond linearity.

Physically-based reﬂection models. Some previous
works [9, 17, 27, 21] explore the physical model of reﬂec-
tion. They regard the reﬂection model as a non-linear com-
bination of the light reﬂected off the glass surface and the
light transmitted through the surface. The imaging process
is also determined by the angle between the incoming light
and the surface. In the same time, according to the Malus’s
law [6], when an reﬂection image is taken by the polarizer,
the amount of light is also changed by the angle between the
polarization direction of the incoming light and the trans-
mission axis of the polarizer. All these factors result in dif-
ferent type of reﬂections.

Following the above rules, some physically-based reﬂec-
tion removal methods use simpliﬁed models for speciﬁc
scenarios. Kong et al.[9] require a series of three polar-
ized images in the same scene, each captured with a differ-
ent polarizer angle. However, they assume that thickness
of the medium is thin enough, thus ghosting effect is not
considered in this model.
In contrast, Shih et al.[18] fo-
cus on the ghosting reﬂection. They regard the reﬂection
as double layers. One layer is the primal reﬂection layer,
and the other one is a spatially shifted and attenuated im-
age of the former. Due to this characteristic, it models the
ghosting effect using a double-impulse convolution kernel
and removes the reﬂection with a Gaussian Mixture Model.
Although the physically-based methods model reﬂections
accurately, they are limited to speciﬁc cases. On the con-

3772

(a) Real Focused

(b) Real Defocused

(c) Real Ghosting

(d) Our Focused

(e) Our Defocused

(f) Our Ghosting

Figure 2: Three types of reﬂection examples. The ﬁrst row
shows the real examples and the second row shows our syn-
thesized images. The proposed synthesis method is able to
simulate all the three types of reﬂections.

trary, we leverage the physical reﬂection model from a data
synthesis aspect, creating realistic and diverse training data.

3. SynNet: the Synthesis Network

To inject non-linearity into the synthesis process, we

rewrite Eq. (1) as follows:

S = W ◦ T + (1 − W ) ◦ (K ⊗ R),

(2)

where ◦ is a element-wise multiply operator. W ∈
[0, 1]m×n×3
denotes the alpha blending mask, which is
a non-constant matrix that weighs the contribution of the
transmission layer at each pixel. Each layer combined with
the alpha blending mask is to simulate the intensity of the
physical light from the corresponding objects. In the real
world, the kernel K have different forms according to the
thickness of the glass or the angle between the incoming
light and the surface [24]. According to different scenarios,
reﬂection can be roughly categorized into three types, fo-
cused reﬂection, defocused reﬂection, and ghosting reﬂec-
tion. These criteria are the principles to our data collection
and synthesis processes.

3.1. Reﬂection Types and Data Collection

Focused Reﬂection. When the object behind the glass
and the reﬂected object are in the same focal plane, the re-
ﬂection layer will be as sharp as the transmission layer in
the reﬂection image. In this case, the kernel K is consid-
ered as a one-pulse kernel. To prevent intensity overﬂow,
some works [25, 28] scale down the light of the two lay-
ers linearly by a constant. Both two layers of this type of
reﬂection look sharp, and thus they are difﬁcult to separate

by human eyes. Fig. 2 (a) shows an example of focused
reﬂection.

Defocused Reﬂection.

In most reﬂection images, they
are captured from a certain distance to the camera, and
therefore the reﬂected objects are usually out of focus when
the object behind the glass is in the focal plane. In this case,
the reﬂection layer is blurry and smoother than the trans-
mission layer. Most linear blending methods [29, 31, 2]
model this type of reﬂection images by setting the kernel K
as a Gaussian kernel to simulate the blurry reﬂection. A real
defocused reﬂection image is shown in Fig. 2 (b).

Ghosting Reﬂection. For the two above types, we as-
sume the thickness of the medium, such as a glass, is thin
enough to regard it as single-surface. However, when the
thickness is non-negligible, we should take the refraction
into consideration, as it will cause the quadric reﬂection
with shifting. To model this ghosting reﬂection, Shih et al.
[18] set the kernel K as a two-pulse kernel which is called
the ghosting kernel. Fig. 2 (c) shows an real ghosting re-
ﬂection example.

To simulate the above typical types of reﬂections, we
collect 1109 real-world reﬂection images (each type has
306, 672, 131, respectively) for training the synthesis net-
work SynNet. We denote this real reﬂection dataset as ℜ.

3.2. Network Structure

The network structure of the proposed SynNet is shown
in Fig. 3. SynNet has an encoder-decoder structure [14, 15].
It takes a six-channel image as input, by concatenating two
real-world clean images. The former three channels are
treated as the transmission layer, and the latter are the re-
ﬂection layer that preprocessed by the kernel K. Both en-
coder and decoder contain three convolutional layers. In the
middle of them, we add nine residual blocks [5] to enrich
the reﬂection features representations. All convolution lay-
ers are followed by an InstanceNorm layer [23] and ReLU
activation, except the last layer followed by the Sigmoid ac-
tivation function to scale the output into [0, 1]. The network
outputs a three-channel alpha blending mask.

3.3. Objective Function

The objective function of SynNet contains two terms: an

adversarial loss and a smoothness loss.

Adversarial Loss. As there is no paired data for the
synthesis training process, involving an adversarial loss is
arguably the best solution. We use the collected real data of
three reﬂection types as the real samples for training the dis-
criminator. Note that we do not directly synthesize the re-
ﬂection data, as it is impossible to control the output reﬂec-
tion type. Instead, we synthesize the alpha blending mask,
so that all the three types can be generated and they can be
used to train the network properly. The loss for the discrim-

3773

Figure 3: The proposed synthesis network SynNet. The symbol ⊛ denotes the blending operator of Eq. (2). Instead of directly
synthesizing the reﬂection image, we synthesize the alpha blending mask. With a different choice of the blurring kernel, the
proposed network can be controlled to generate different types of reﬂections.

inator D is deﬁned as:

LD = X

I,S∈ℜ

logD(I) + log(1 − D(S)),

(3)

where D(x) is the probability that x is a real reﬂection im-
age, I denotes the real-world image and S is our synthe-
sized images. According to [3], we optimize the network
with only the ﬁrst term in Eq. (3). The adversarial loss is
then deﬁned as:

of the collected data.
In this way, the proposed network
generates a W , which can be combined with a kernel K
to simulate Eq. (2) with different types of reﬂections. Fig.
2 shows a comparison for the synthesized reﬂections and
the real ones. We can see that the generated images show
similar reﬂection features to the real reﬂection images.

4. RmNet: the Removal Network

Given a large amount of synthetic data S, we propose a

cascaded network for reﬂection removal.

Ladv = X

−log(D(S)).

(4)

S∈ℜ

4.1. Network Structure

Smoothness Loss. We add a smoothness loss Lsmooth
as an augmented loss to avoid the value mutation in the al-
pha blending mask, which will cause the unexpected color
change in the synthetic image. This loss is to encourage
the spatial smoothness, which is also used some other im-
age processing applications like super-resolution [7]. The
smoothness loss is deﬁned as:

Lsmooth = X

X

kWi+1,j − Wi,jk1 + kWi,j+1 − Wi,jk1,

S∈ℜ

i,j

(5)
where Wi,j denotes the pixel value of the alpha blending
mask.

Overall, our objective function of SynNet is:

Lsyn = w1Ladv + w2Lsmooth.

(6)

We heuristically set w1 = 1 and w2 = 10 to balance the
contribution of each term.

During training, the kernel K is selected among three
reﬂection types according to the statistic (2.34 : 5.13 : 1)

The architecture of the proposed network is shown in
Fig. 4. It is a three-stream structure with one encoder and
three decoders, and each layer of the encoder have skip-
connections to the corresponding layers of all the three de-
coders. For the decoder, there are six convolutional layers
with the kernel size of 4 × 4 and stride-2. Each layer is fol-
lowed by the InstanceNorm layer and a Leaky ReLU activa-
tion function (slope is 0.2). For the decoders, there are six
(de)convolutional layers with the kernel size of 4 × 4 and
stride- 1
2 . Similarly, they are followed by the InstanceNorm
layer and ReLU activation function.

Each decoder corresponds to predict a different output
image. Two of them estimate the transmission and reﬂec-
tion images. These are the basic elements of the reﬂection
removal task. On the other hand, our synthesized images
are constructed by the alpha blending masks, and therefore
they can be treated as the ground truth for supervising the
RmNet to produce the alpha blending mask as an additional
output. In this way, all the three outputs can be united to
reconstruct the input reﬂection image. This makes the net-
work an closed loop, and allowing a new reconstruction loss

3774

C9TransmissionLayerReflectionLayerAlpha Blending MaskSmoothness LossSynthetic ImageAdversarial LossConcatenationOperatorReal-world Reflection ImageConv+Norm+ReLUDeconv+Norm+ReLU/SigmoidResNet BlockFigure 4: The proposed reﬂection removal network RmNet. The symbol ⊛ denotes the blending operator of Eq. (2). We
involve the alpha blending mask as the side output, making the training a closed loop. Therefore, our network can be trained
with a reconstruction loss.

for training.

4.2. Objective Function

The objective function of RmNet contains three terms: a

pixel loss, a gradient loss, and a reconstruction loss.

Pixel Loss.

To ensure the outputs as similar to the
ground truth as possible, we utilize L1 loss to measure the
pixel-wise distance between them. Our pixel loss is deﬁned
as:

Lpixel = X

kT − T ∗k1+kR − R∗k1+kW − W ∗k1 ,

T ,R,W ∈S

(7)
where the T ∗, R∗, W ∗ are the predicted transmission, re-
ﬂection layer, and the alpha blending mask, respectively.

Gradient Loss. For an reﬂection image, the gradients
are consistent in the transmission images, while the gradi-
ents vary in the reﬂection images [13, 4]. We use the Sobel
operator to extract the gradient images for the transmission
layer. Then we compare the predicted transmission with its
ground truth in gradient domain to keep the same gradient
distribution. We obtain both the horizontal and vertical gra-
dients, and our gradient loss is deﬁned as:

Lgrad = X

kGx(T ) − Gx(T ∗)k1+kGy(T ) − Gy(T ∗)k1 ,

T ∈S

(8)
where Gx() and Gy() are vertical and horizontal Sobel op-
erators respectively.

Reconstruction Loss. Different from existing reﬂec-
tion removal networks, we introduce a new loss to the pro-
posed network, named reconstruction loss. Due to the ad-
ditional predicted alpha blending mask, we can re-compose
the three outputs of RmNet according to Eq. (2). It is intu-
itive that the re-composed reﬂection image should be sim-
ilar to the original input, if the network is trained properly.
For the reconstruction loss, we measure the perceptual dis-
tance between the recombined image and the input image.
Both the images are fed to a VGG19 network F (·), and the
reconstruction loss is deﬁned as:

Lreconstr = X

kF (S) − F (S ∗)k1 ,

(9)

S∈S

where S ∗ is the recombined image.

Overall, our object function of RmNet is:

Lrm = w1Lbasic + w2Lgrad + w3Lreconstr.

(10)

We heuristically set w1 = 100, w2 = 50 and w3 = 100.

5. Experiments

We implement the proposed two networks in Pytorch on
PC with a Nvidia Geforce GTX 1080 Ti GPU. Every net-
works are trained for 130 epoches with a batch size of 10,
using the Adam optimizer [8] with a learning rate of 0.0002.
To generate our synthetic training data, we collect 4000 im-
ages from Flickr randomly to form the transmission and re-
ﬂection layers, and they are randomly blended to generate

3775

Synthetic ImageUpsampling UnitUpsampling UnitUpsampling UnitUpsampling UnitUpsampling UnitGT Transmission LayerGT Reflection LayerRecombined ImageReflection LayerTransmission LayerAlpha Blending MaskGT Alpha Blending MaskReconstruction LossPixel LossPixel + gradient Lossespixel LossConv+Norm+ReLUDeconv+Norm+ReLU/Sigmoidd
e
s
u
c
o
F

d
e
s
u
c
o
f
e
D

g
n
i
t
s
o
h
G

Input

GT Transmission

CEILNet [2]

Zhang et al.[31]

BDN [29]

Ours

Figure 5: Qualitative comparisons on our synthetic testing set. We show three types of reﬂection images generated by our
SynNet. State-of-the-art models cannot address all the scenarios.

Table 1: Comparison of the generated reﬂection images
with respect to the inception score.

[31]

[2]

[29]

Real Ours

Inception

1.138

1.194

1.134

Score ± 0.034 ± 0.042 ± 0.044

1.272

-
- ± 0.037

Accuracy 78.00% 86.67% 74.00% 97.33% 93.33%

the reﬂection images. We also involve the real-world paired
training data from Zhang et al.[31] and SIR2 datasets [24].
Note that when we train RmNet on these two real-world
datasets, we discard the losses of the reﬂection layer and
alpha blending mask. This is because they cannot capture
the ground truth reﬂection layers.

We evaluate the proposed network on four testing sets.
Two real-world testing sets from Zhang et al.[31] and
SIR2. The former one contains 20 testing reﬂection im-
ages, and the latter one we select 55 testing reﬂection im-
ages from the wild scene subset (this is the only subset with-
out overlapped images). Furthermore, we construct another
synthetic testing set, which includes 300 reﬂection images
in three different types (1:1:1). The above three testing sets
contain ground truth transmission layers. We also construct

a real-world reﬂection dataset with 25 images from the In-
ternet, without any transmission ground truth, for qualita-
tive evaluations and user study.

5.1. Comparison to State of the arts

We compare the proposed RmNet to three state-of-the-
art deep models: CEILNet [2], Zhang et al.[31], and BDN
[29].

5.1.1 Quantitative Evaluations

Reﬂection generation. Evaluating images generated by
a GAN is challenging. Here we use the inception score [16]
to assess the quality of our synthesis method and existing
linear combination methods. In particular, we re-train the
inception v3 network [20] on the real-world reﬂection im-
ages and non-reﬂection images. Then we randomly select
300 images from Flickr, and these images are served as the
foreground and background image pairs for data synthesis
using different methods (i.e.CEILNet [2], Zhang et al.[31],
BDN [29], and ours). All the methods use the same 150
pairs of images for synthesis, and they are evaluated by the
re-trained inception v3 network. Table 1 shows the incep-
tion score and the accuracy of each synthetic set. Note that

3776

Input

GT Transmission

CEILNet [2]

Zhang et al.[31]

BDN [29]

Ours

Figure 6: Qualitative comparisons on the dataset collected by Zhang et al.[31].

Table 2: Quantitative evaluations on three testing sets. We further show the performances on three different types of synthetic
images. The proposed method is able to achieve the best (marked in red) or the second best (marked in blue) performances
on either the real-world or synthetic data.

Model

Zhang et al.[31]
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM

Syn. Focused Syn. Defocused Syn. Ghosting

Syn. All

SIR2 [24]

CEILNet [2]
[2] ﬁne-tuned

18.555
16.753

Zhang et al.[31] 20.744
18.032
[31] ﬁne-tuned

0.725
0.711

0.784
0.737

19.727 0.781 14.365 0.636 14.171 0.664 14.721 0.662 14.339 0.656
17.663 0.740 19.524 0.742 20.122 0.735 19.685 0.753 19.777 0.743

24.271 0.868 12.345 0.602 11.317 0.570 12.909 0.635 12.231 0.605
20.265 0.835 17.090 0.712 18.108 0.758 17.882 0.738 17.693 0.736

BDN [29]

18.136

0.726

20.866 0.806 14.258 0.632 14.053 0.639 14.786 0.660 14.301 0.652

Ours

21.283

0.818

23.707 0.855 21.064 0.770 22.896 0.840 21.008 0.780 21.656 0.796

the accuracy shows the percentage of the synthetic images
classiﬁed as the real-world ones in each set. We can see
that the proposed synthetic method outperforms all the lin-
ear methods, achieving a closer performance to the real im-
ages. Some reﬂection synthesis examples can be found in
Fig. 5.

Reﬂection removal. We also evaluate the proposed re-
moval method on three testing sets with respect to PSNR
and SSIM. The quantitative results are shown in Table 2.
First, we compare the proposed method on two standard
benchmarks, SIR2 [2] and Zhang et al.[31]. These two
datasets are collected in a similar way, and thus the re-
ﬂection images show similar reﬂection type. The proposed
method achieves the best on Zhang et al.[31], and the sec-
ond best on SIR2 [2]. For our synthetic dataset, we fur-
ther separate it to three different reﬂection types. Interest-
ingly, we can see all the methods perform better on the de-
focused type. This is similar to human, as the transmission
and reﬂection layers show different imaging features (clear
vs. blurry), so that they can be easier separated. The pro-
posed method performs the best on all the three scenarios.
These results demonstrate that the proposed method is able
to handle different types of reﬂections, either they are real

Table 3: User study on the removal results. The preference
rate shows the percentage of users that prefer our results
over the competitor.

Preference rate

Ours > CEILNet [2]

Ours > Zhang et al.[31]

Ours > BDN [29]

84.6%
73.8%
78.7%

or synthetic.

In Table 2, we also show the ﬁne-tuned results of CEIL-
Net [2] and Zhang et al.[31] (BDN [29] does not pro-
vide training codes) on our synthetic training set. Surpris-
ingly, ﬁne-tuned with our synthetic data decreases the per-
formance on the datasets of SIR2 [2] and Zhang et al.[31].
This is mainly because these two datasets constructed with a
similar type of reﬂections, and training with the other types
may leading to learning ambiguity. On the other hand, state-
of-the-art methods cannot achieve as good performance as
ours in the synthetic test set. This demonstrates the impor-
tance of predicting alpha blending mask and the supervision
of the reconstruction loss.

3777

Input

Ours
Figure 7: Qualitative comparisons on real-world reﬂection images collected from the Internet.

Zhang et al.[31]

CEILNet [2]

BDN [29]

5.1.2 Qualitative Evaluations

Fig. 5 shows results on our synthetic dataset. We show
three types of reﬂections and their corresponding reﬂection-
free images. It can be seen that the proposed method is able
to handle different types of reﬂections, while state-of-the-
arts fail to remove reﬂections on all the three types. In Fig.
6, we also show the results on the dataset collected from
Zhang et al.[31]. This dataset mainly simulates the focused
reﬂection, and we achieve comparable performance to the
others on this conditioned scenario.

We also examine the proposed method on the real-world
reﬂection images collected from the Internet. These results
are mainly used for conducting a user study. For each eval-
uation, we compare our method to one competitor (three in
total) on these real-world reﬂection images following the
set of Zhang et al.[31]. Each user is presented with an orig-
inal reﬂection image, our predicted transmission layer and
the transmission layer by the competitor. The user needs to
choose the image which is more like the reﬂection-free im-
age. There are 25 real-world reﬂection images presented in
the comparisons. The user study results are shown in Table.
3. The results are statistically signiﬁcant with p < 10−3
and 30 users participate in the user study. Some examples
are also shown in Fig. 7.

5.2. Ablation Study

For better analysing the objective function of RmNet, we
remove three losses one by one. We re-train new models
with the modiﬁed losses. The ablation study is shown in
4. We observe that Lreconstr and Lgrad enhance the gen-
erality of RmNet in both the real-world and synthetic cases,
and both the losses show different contributions to the re-
moval performance. Our complete objective function show
the best results.

Table 4: Ablation studies on three testing sets. Each loss
contributes to the reﬂection performance, while combining
all of them achieves the best result.

Model

Zhang et al.[31] SIR2 [24]
PSNR SSIM PSNR SSIM PSNR SSIM

Syn. All

Lpixel only

18.684 0.727 20.833 0.780 19.413 0.762
w/o Lreconstr 19.029 0.752 21.011 0.805 20.274 0.774
19.303 0.748 21.276 0.813 20.229 0.766

w/o Lgrad

Complete

21.283 0.818 23.707 0.855 21.656 0.796

6. Conclusion

In this paper, we revisit the linear combination problem
of single image reﬂection removal. Particularly, we develop
a reﬂection synthesis network to predict a non-linear alpha
blending mask. In this way, it is able to generate images
with different reﬂection types. Based on the synthesized
diverse data, we propose a multi-branch reﬂection removal
network. This network predicts the alpha blending mask as
the side output, which makes the training a closed loop, so
that it can be supervised by the reconstruction loss. Quanti-
tative and qualitative evaluations on four datasets show that
the proposed method is able to handle different types of
reﬂections and outperform the state-of-the-arts in both the
real-world and synthetic scenarios.

Acknowledgements. This project is supported by the Na-
61472145,
tional Natural Science Foundation of China (No.
No. 61702104, and No. 61702194), the Innovation and Tech-
nology Fund of Hong Kong (Project No. ITS/319/17), the Spe-
cial Fund of Science and Technology Research and Development
on Application From Guangdong Province (SF-STRDA-GD) (No.
2016B010127003), the Guangzhou Key Industrial Technology Re-
search fund (No. 201802010036), and the Guangdong Natural
Science Foundation (No. 2017A030312008).

3778

[19] Ofer Springer and Yair Weiss. Reﬂection separation using

guided annotation. In ICIP, pages 1192–1196, 2017.

[20] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, pages 2818–2826,
2016.

[21] T. Tsuji. Specular reﬂection removal on high-speed camera

for robot vision. In ICRA, pages 1542–1547, 2010.

[22] M. A. C. Tuncer and A. C. Gurbuz. Ground reﬂection
removal in compressive sensing ground penetrating radars.
ICIP, 9(1):23–27, 2012.

[23] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor S Lempitsky. Texture networks: Feed-forward synthesis
of textures and stylized images. In ICML, pages 1349–1357,
2016.

[24] Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, and
Alex C Kot. Benchmarking single-image reﬂection removal
algorithms. In ICCV, 2017.

[25] Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, and
Alex C Kot. Crrn: Multi-scale guided concurrent reﬂection
removal network. In CVPR, pages 4777–4785, 2018.

[26] Renjie Wan, Boxin Shi, Tan Ah Hwee, and Alex C Kot.
Depth of ﬁeld guided reﬂection removal. In ICIP, pages 21–
25, 2016.

[27] R. Wan, B. Shi, T. A. Hwee, and A. C. Kot. Depth of ﬁeld

guided reﬂection removal. In ICIP, pages 21–25, 2016.

[28] Tianfan Xue, Michael Rubinstein, Ce Liu, and William T.
Freeman. A computational approach for obstruction-free
photography. ACM TOG, 34(4):1–11, 2015.

[29] Jie Yang, Dong Gong, Lingqiao Liu, and Qinfeng Shi. See-
ing deeply and bidirectionally: A deep learning approach for
single image reﬂection removal. In ECCV, pages 654–669,
2018.

[30] Jiaolong Yang, Hongdong Li, Yuchao Dai, and Robby T Tan.
Robust optical ﬂow estimation of double-layer images un-
der transparency or reﬂection. In CVPR, pages 1410–1419,
2016.

[31] Xuaner Zhang, Ren Ng, and Qifeng Chen. Single image

reﬂection separation with perceptual losses. CVPR, 2018.

References

[1] Nikolaos Arvanitopoulos, Radhakrishna Achanta,

and
Sabine S¨usstrunk. Single image reﬂection suppression. In
CVPR, pages 1752–1760, 2017.

[2] Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, and
David P Wipf. A generic deep architecture for single image
reﬂection removal and image smoothing.
In ICCV, pages
3258–3267, 2017.

[3] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In NeurIPS,
pages 2672–2680, 2014.

[4] Byeong-Ju Han and Jae-Young Sim. Reﬂection removal us-
ing low-rank matrix completion. In CVPR, volume 2, 2017.
[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[6] Eugene Hecht. Optics (4th Edition). Addison Wesley, 4 edi-

tion, Aug. 2001.

[7] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016.

[8] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2014.

[9] Naejin Kong, Yu-Wing Tai, and Joseph S Shin. A physically-
based approach to reﬂection separation: from physical mod-
eling to constrained optimization. IEEE TPAMI, 36(2):209–
221, 2014.

[10] Anat Levin, Assaf Zomet, and Yair Weiss. Learning to per-
ceive transparency from the statistics of natural scenes. In
NeurIPS, pages 1271–1278, 2003.

[11] Anat Levin, Assaf Zomet, and Yair Weiss. Separating re-
ﬂections from a single image using local features. In CVPR,
volume 1, pages 306–313, 2004.

[12] Yu Li and Michael S. Brown. Exploiting reﬂection change
In ICCV, pages 2432–

for automatic reﬂection removal.
2439, 2013.

[13] Yu Li and Michael S Brown. Single image layer separa-
tion using relative smoothness. In CVPR, pages 2752–2759,
2014.

[14] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, pages 3431–3440, 2015.

[15] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, volume 9351 of LNCS, pages 234–241. Springer, 2015.
[16] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NeurIPS, pages 2234–2242, 2016.

[17] Yoav Y. Schechner, Joseph Shamir, and Nahum Kiryati.
Polarization-based decorrelation of transparent layers: The
inclination angle of an invisible surface. In ICCV, volume 2,
pages 814–819, 1999.

[18] YiChang Shih, Dilip Krishnan, Fredo Durand,

and
William T. Freeman. Reﬂection removal using ghosting
cues. In CVPR, June 2015.

3779

