Dance with Flow: Two-in-One Stream Action Detection

Jiaojiao Zhao and Cees G. M. Snoek

University of Amsterdam

Abstract

The goal of this paper is to detect the spatio-temporal ex-
tent of an action. The two-stream detection network based
on RGB and ﬂow provides state-of-the-art accuracy at the
expense of a large model-size and heavy computation. We
propose to embed RGB and optical-ﬂow into a single two-
in-one stream network with new layers. A motion condi-
tion layer extracts motion information from ﬂow images,
which is leveraged by the motion modulation layer to gener-
ate transformation parameters for modulating the low-level
RGB features. The method is easily embedded in existing
appearance- or two-stream action detection networks, and
trained end-to-end. Experiments demonstrate that lever-
aging the motion condition to modulate RGB features im-
proves detection accuracy. With only half the computation
and parameters of the state-of-the-art two-stream methods,
our two-in-one stream still achieves impressive results on
UCF101-24, UCFSports and J-HMDB.

1. Introduction

This paper strives for the spatio-temporal detection of
human actions in video, which is a crucial ability for self-
driving cars, autonomous care robots, and advanced video
search engines. The leading approach for this challenging
problem relies on fast detectors at the frame level [29, 37],
which are then linked [1, 13, 37] or tracked [48] over time.
Kalogeiton et al. [21] and Singh et al. [36] further showed
it is advantageous to stack the features from subsequent
frames before predicting action class scores and determin-
ing the enclosing tube. Most of the state-of-the-art ac-
tion detectors exploit a two-stream architecture [35], one
for RGB and one for optical-ﬂow, which are individually
trained before fusion. However, the double computation
and parameter demand of two-stream methods does not lead
to double accuracy compared to a single stream. We pro-
pose to embed RGB and optical-ﬂow into a single stream
for action detection.

We are inspired by progress on feature normalization,
especially conditional normalization [8, 10, 18], which has
been successfully employed to visual question answer-

Figure 1: Two-in-one stream. We propose to embed RGB
and optical-ﬂow into a single stream for spatio-temporal ac-
tion detection. Besides efﬁciency gains, it helps recogniz-
ing whether the dancer in the current frame is standing up
or sitting down without considering the future. By utilizing
information from ﬂow images, the dancer is given a moving
direction, up or down, better indicating the action.

ing [5], visual reasoning [30], image style transfer [17]
and super-resolution [47]. Peretz et al. [30] propose a
feature-wise linear modulation layer which enables a recur-
rent neural network over an input question to inﬂuence con-
volutional neural network computation over an image.
It
demonstrates that features are capable to be modulated via
a simple feature-wise afﬁne transformation based on condi-
tioning information. However, as their modulation layer is
agnostic to spatial location, it is unsuited for action detec-
tion. In [47], Wang et al. developed a spatial feature trans-
form layer, which is conditioned on categorical semantic
probability maps, to modulate a super-resolution network.
Encouraged by these works, we propose a motion condi-
tion layer and a motion modulation layer to adjust an RGB-
stream for spatio-temporal action detection.

We make the following contributions in this paper. We
propose to embed RGB and optical-ﬂow into a single stream
for spatio-temporal action detection.
It reduces the com-
putational costs of conventional two-stream detection net-
works by half while maintaining its high accuracy. We in-

9935

stand up?sit down?Flow imageFlow imagetroduce the two-in-one stream with motion condition layer
and motion modulation layer, which learns video repre-
sentations of appearance-stream features conditioned on
optical-ﬂow. As shown in Figure 1, the motion condition
will guide the model to pay more attention on what moves,
rather than the static background. The method is easily em-
bedded in existing appearance- or two-stream action detec-
tion networks, and trained end-to-end, leading to new state-
of-the-art on UCF101-24, UCFSports and J-HMDB.

2. Related Work

The spatio-temporal detection of human actions in video
has a long tradition in computer vision, e.g. [3, 4, 22]. Early
success came from detection based on exhaustive cuboid
search, efﬁcient feature representations, and SVM-based
learning, [42, 43, 52]. This was later extended with more
ﬂexible sequences of bounding boxes [24, 44, 51], or spatio-
temporal proposals [19, 45], together with engineered ap-
pearance and motion features, most notably the dense tra-
jectories [28]. The past few years, architectures integrating
detection and deep representation learning have been lead-
ing [7,11,16,25,41,49,50], typically combining appearance
and ﬂow streams [14, 15, 21, 37]. We follow this tradition.

The two-stream network was ﬁrst introduced by Si-
monyan and Zisserman in [35]. Their convolutional archi-
tecture included a separate RGB-stream and a ﬂow-stream,
which were combined by late fusion, for SVM-based ac-
tion classiﬁcation. In [9], Feichtenhofer et al. investigated
a number of ways to fuse the RGB and ﬂow streams in or-
der to best take advantage of their fused representation for
action classiﬁcation. While we concentrate on action detec-
tion in the paper, we are interested in RGB and ﬂow as well,
but rather than combining the two streams in a late fusion,
we prefer a single stream.

Gkioxari and Malik [13] introduced a two-stream archi-
tecture with R-CNN detectors in action detection. They
fused features from the last layer of an RGB- and a ﬂow-
stream, and then trained action speciﬁc SVM classiﬁers.
A Viterbi algorithm [40] was adopted to link the detection
boxes per frame into a tube. Weinzaepfel et al. [48] also
used a two-stream R-CNN detector but replaced the linking
by a tracking-by-detection method. Both methods are not
end-to-end trainable and restricted to trimmed videos.

End-to-end two-stream detectors based on faster-RCNN
were proposed in [29, 34]. In [29], Peng and Schmid per-
formed region of interest pooling and score fusion to incor-
porate an RGB-stream and a ﬂow-stream. In [16], Hou et
al. extended 2D region of interest pooling to 3D tube-of-
interest pooling with 3D convolutions, which directly gen-
erate tubelets for action detection. Singh et al. adopted a
two-stream single-shot-multibox detector (SSD) [27] for re-
alizing real time detection in [37]. Singh et al. [36] also
introduced a transition matrix to generate a set of action

proposals on pairs of frames. Kalogeiton et al. [21] pro-
posed to exploit temporal continuity by taking as much as
six frames as input for their single-shot multibox detector,
leading to state-of-the-art results. In this paper, we take the
single-shot multibox detector network as our backbone, us-
ing single [37] or multiple [21] frames as input, but rather
than separating the streams for RGB and ﬂow we introduce
a single two-in-one stream.

Li et al. [26] proposed an action detector using an LSTM
architecture with motion-based attention. Our two-in-one
stream not only takes motion as attention, which helps to
locate actions, but also uses motion to modulate RGB fea-
tures which helps to better classify actions. Moreover, our
method is easily embedded in existing appearance- or two-
stream action detection and classiﬁcation networks.

3. Two-in-One Network

We deﬁne the RGB-stream network Drgb

θ

trained on sin-

gle frame for spatio-temporal action detection as:

(Lrgb, Srgb) = Drgb

θ

(I rgb)

(1)

where I rgb ∈ RH×W ×3 is a single RGB frame of height
H and width W which is the input for the network Drgb
.
Lrgb ∈ RQ×4 and Srgb ∈ RQ×(P +1) are Q box locations
and corresponding box classiﬁcation scores for P action
classes and a background class. θ represents the parameters
of the learned network. Similarly, we deﬁne a ﬂow-stream
network on single frame for spatio-temporal action detec-
tion as:

θ

(Lof , Sof ) = Dof

θ (I of )

(2)

I of ∈ RH×W ×2 is a single optical ﬂow image with x
and y components of the velocity respectively in two chan-
nels. The two-stream method includes training the two net-
works Drgb
independently, and fuses the results
(Lrgb, Srgb) and (Lof , Sof ).

and Dof
θ

θ

Motion condition layer. In our method, I of is regarded
as a motion map with the same resolution as the correspond-
ing RGB image I rgb. We take I of as prior information Ψ
when applying an RGB-stream network Drgb
to estimate
where and what actions may occur. Then we formulate our
two-in-one network as a condition network:

θ

(L։, S։) = D։

θ (I rgb|Ψ)
θ (I rgb|MC(I of ))
Ψ = MC(I of ) = MC((I ofx , I ofy ))

= D։

(3)

(4)

։ means two-in-one stream, MC(•) is a mapping func-
tion to generate simple features from the ﬂow images. So
the two-in-one stream D։
learns a model conditioned on
θ
motion information by a motion condition layer.

Motion modulation layer. We introduce a motion mod-
ulation (M2) layer to modify the features learned from RGB

9936

Figure 2: Two-in-one network architecture. The motion condition layer (pink cube) maps ﬂow images to prior condition
information. The condition inputs to the motion modulation layer (purple cube) to generate transformation parameters which
are used to modulate RGB features (F rgb). The network has half the computation and parameters of a two-stream equivalent,
while obtaining better action detection accuracy.

images. An M2 layer is able to inﬂuence the appearance
network by incorporating motion and weighting the action
area. We ﬁrst learn a pair of afﬁne transformation param-
eters (β, γ) from the prior ﬂow condition Ψ by a function
F : Ψ 7−→ (β, γ). Concretely, the two-in-one network is
further expressed as:

(β, γ) = F(Ψ),

(L։, S։) = D։

θ (I rgb|β, γ)

(5)

In order to modulate the appearance network, we apply a
transform function M2(•) with the learned transformation
parameters (β, γ) to the RGB features F rgb.

M2(F rgb) = β ⊙ F rgb + γ

(6)

⊙ is an element-wise multiplication operation. The RGB
feature maps F rgb has the same dimensions with param-
eters β and γ. The ﬂow information represented by (β, γ)
inﬂuences the appearance network by both feature-wise and
spatial-wise manipulations. The complete network with the
motion condition layer and the motion modulation layer is
shown in Figure 2.

Network architecture. Due to sparsity of ﬂow images,
we adopt simple convolutional layers to extract low-level
motion condition information. 1 × 1 convolutional layers
attempt to keep the spatial pixel-wise motion vectors. The
motion condition then inputs to a motion modulation (M2)
layer in which it is separately mapped to a pair of trans-
formation parameters β and γ. Two groups of 1 × 1 con-
volutional layers are independently adopted for generating

each of the parameters β and γ. The low-level RGB fea-
tures from the appearance network are adjusted by β and γ.
The motion modulation layer is capable to be added to any
bottom layer of the appearance network, including conv1,
conv2, conv3 and conv4. All of them share the motion con-
dition layer. The whole network is end-to-end trainable.

Feature visualization. In order to intuitively understand
the method, we show the generated feature maps from the
appearance network before and after modulation by motion
condition in Figure 3. We randomly select some feature
maps from the motion condition layer in the ﬁrst row. The
features are low-level and sparse, which are taken as prior
conditions. From the second row to the last row, we show
the corresponding scale (β) and shift (γ) maps generated
from conditions, RGB features without modulation and fea-
tures modulated by β and γ. It is interesting to see the dif-
ference between the features without and with modulation
in Figure 3. For example the modiﬁed features of the ac-
tor areas in feature maps 0 and 43, after modulation, espe-
cially for the female ice skater, which is blended into the
background on the regular RGB stream. On the 28-th fea-
ture map, a feature response is even hard to see on both
actors before modulation. Feature maps 10 and 127 show
the change in x-direction features and y-direction features.
The ﬂow condition pushes the model to focus on moving
actors.

Training loss. In order to demonstrate the generalization
and ﬂexibility of the proposed method, we embed the mo-
tion condition layer and the motion modulation layer in a

9937

...conv1conv2conv3conv4extra layers     layer1       layer2         layer3          layer4convMC layerMotion Condition (MC) LayerMotion Modulation (      ) Layer.+RGB imageoptical flowmotion conditionconvconvTwo-in-one Streamregression lossconfidence loss          MC layerIce Dancingand for its width (w) and height (h).

Lloc(x, l, g) =

N

X

i∈P os
ˆgcx
j = (gcx

j − dcx

i )/dw

i

X

xk
ijsmoothL1(lm

i − ˆgm
j )

m∈{cx,cy,w,h}

ˆgcy
j = (gcy

j − dcy

i )/dh

i

ˆgw
j = log(

)

ˆgh
j = log(

gw
j
dw
i

gh
j
dh
i

)

(9)
For the multi-frame appearance stream, we follow Kalo-
geiton et al. [21] to train the network.

Figure 3: Feature maps. Visualization of the motion con-
dition maps, scale maps, shift maps, RGB features without
modulation and features with modulation. The modulated
features focus more on moving actors.

single-frame appearance stream and a multi-frame appear-
ance stream. The basic loss function is derived from the
one for object detection [27, 31]. Deﬁning xp
ij = {1, 0}
as an indicator for matching the i-th default box to the j-th
ground truth box of action category p. The overall loss func-
tion contains the localization (loc) loss and the conﬁdence
(conf ) loss:

L(x, c, l, g) =

1
N

(Lloc(x, l, g) + Lconf (x, c))

(7)

with N representing the number of matched default boxes.
c represents multiple classes conﬁdences.
l and g are the
predicted box and the ground truth box.

The conﬁdence loss applies the softmax loss as below:

Lconf (x, c) = −

N

X

xp
ijlog(ˆcp

i ) − X

log(ˆc0
i )

ˆcp
i =

i∈P os
exp(cp
i )
Pp exp(cp
i )

i∈N eg

(8)

The localization loss applies a smooth L1 loss [12] between
the predicted box and the ground truth box. The network re-
gresses to offsets for the center (cx, cy) of the default box(d)

Two-in-one two-stream. Our method emphasizes to uti-
lize RGB and optical ﬂow information in one stream. Fur-
thermore, it is possible to follow the standard practice of
two-stream action detection. We train a two-in-one detec-
tor conditioned on ﬂow images, and a separate ﬂow de-
tector which only takes as input the ﬂow images. For a
single-frame two-in-one two-stream, we use average fu-
sion method to merge the results from each stream follow-
ing [37]. And for multi-frame two-stream, the late fusion
[9] is a better choice [21].

Linking. Once the frame-level detections or tubelet de-
tections are achieved, we link them to build action tubes.
We adopt the linking method described in [37] for frame-
level detections and the method in [21] for tubelet detec-
tions.

Code is available at https://github.com/jiaozizhao/Two-

in-One-ActionDetection.

4. Experiments

4.1. Datasets, Metrics & Implementation

Datasets. We perform experiments on three action de-
tection datasets. UCF101-24 [39] is a subset of UCF101. It
contains 24 sport classes in 3207 untrimmed videos. Each
video contains a single action category. Multiple action in-
stances with the same class, but different spatial and tempo-
ral boundaries may occur. We use the revised annotations
for UCF101-24 from [37]. UCF-Sports [32] contains 10
sport classes in 150 trimmed videos. We follow [24] to
divide the training and test splits. J-HMDB [20] contains
21 action categories in 928 trimmed videos. We report the
average results on three splits.

Metrics. Following [34, 38, 48], we utilize video mean
Average Precision (mAP ) to evaluate action detection ac-
curacy. We calculate an average of per-frame Intersection-
over-Union (IoU) across time between tubes. A detection is
correct if it’s IoU with the ground truth tube is greater than
a threshold and its action label is correctly assigned. We
compute the average precision for each class and report the
average over all classes.

Implementation. We adopt a real-time single shot
multibox detector (SSD) network [27] as the backbone. We

9938

RGB imageFlow imageMotion condition mapsRGB features before modulationFeatures after modulationconv2_1-02_1-02_1-10conv2_1-10scale-0shift-0scale-10shift-102_1-127conv2_1-127scale-127shift-127conv2_1-28conv2_1-432_1-282_1-43scale-28scale-43shift-28shift-43Action Detection

Action Classiﬁcation

Method

mAP

Efﬁciency

Top1 Accuracy

Efﬁciency

%

sec/frame

# param. (M)

ﬂow-stream
RGB-stream
two-stream

11.60
18.49
19.79
20.15
two-in-one two stream 22.02

two-in-one stream

0.04
0.04
0.09
0.04
0.09

26.82
26.82
53.64
26.93
53.75

%

81.65
84.99
91.14
86.94
92.00

sec/frame

# param. (M)

1.10
1.10
2.10
1.15
2.13

58.35
58.35
116.70
58.48
116.83

Table 1: Two-in-one vs. baselines for action detection on UCF101-24 and action classiﬁcation on UCF101. Two-in-one
with motion modulation works well for both action detection and action classiﬁcation.

insert the developed motion layers into two state-of-the-art
appearance SSD networks, one based on single frame [37]
and the other based on multiple frames [21]. We use VGG-
16 pre-trained weights on ImageNet as model initialization.
The input size is 300x300 for both of them. We follow [21]
to use 6 continuous frames as input to the multi-frame SSD.
The initial learning rate is set to 0.001 for the single-frame
network and 0.0001 for the multi-frame network on all the
three datasets and changed by applying step decay strategy.
We trained a ﬂow-stream, an RGB-stream and our two-in-
one stream for 13.2, 13.2 and 15.5 hours, respectively.

Alternatively, we considered to use appearance informa-
tion to modulate ﬂow stream. However, it does not work
well. It appears difﬁcult to modulate features from ﬂow im-
ages which are sparse, using RGB images which are more
dense.

4.2. Ablation Study

All the ablation studies are performed on UCF101-24.
We only report mAP at the most challenging high IoU
thresholds 0.5:0.95 (with step 0.05). Initially, in order to
maintain the spatial pixel-wise motion vectors, we apply
1x1 convolution kernels to all layers in the motion condi-
tion layer and the motion modulation layer. We use layer
parameter stride to control the size of β and γ. Then the
motion modulation layer is applied to conv1 of SSD. Flow
images are generated using the method in [2], which we re-
fer to as BroxFlow.

Two-in-one vs. baselines. We compare the two-in-one
stream to its corresponding RGB-stream, ﬂow-stream and
two-stream in Table 1. Runtime and # param. are also
reported for comparing the efﬁciency. Our single two-in-
one stream exceeds a single RGB-stream by 1.5%. Notably,
two-in-one even outperforms the corresponding two-stream
with only half the computation cost and # param..

We also consider action classiﬁcation, on UCF101. We
follow [46], with ResNet152 as backbone. The Top 1 accu-
racy and efﬁciency shown in Table 1 illustrate our strategy
also works for action classiﬁcation and generalizes beyond
SSD with VGG16. For training, our two-in-one stream
converges at the 100-th epoch, but the RGB- and ﬂow-

Figure 4: Where to add the modulation layer? Accuracy
on UCF101-24 and # param. with varying: (a) single mod-
ulated layer, and (b) multiple modulated layers. A single
modulation layer at conv1 gets the best result.

Figure 5: How to design the condition layer? Comparing
accuracy on UCF101-24 when applying 1x1 conv or 3x3
conv to the last layer of the motion condition layer. The 3x3
conv performs better.

stream converge at 200-th and 300-th epoch, respectively.
Our motion modulation strategy works better for the de-
tection task, which needs localization representations that
are translation-variant, compared to the classiﬁcation task
which favors translation invariance.

Where to add the modulation layer? The motion con-
dition layer is leveraged to generate low-level motion fea-
tures as ﬂow images are more sparse. We add the motion

9939

1234(a) Conv Layer17.017.518.018.519.019.520.020.521.0mAP@IoU=0.5:0.95 (%)20.1520.0319.5719.4711-21-2-31-2-3-4(b) Conv Layer20.1519.9320.0919.9Accuracy26.626.827.027.227.427.627.828.026.8326.927.027.1826.626.827.027.227.427.627.828.0# Param. (M)26.8526.9326.926.99# Param.1234Conv Layer19.019.520.020.521.021.522.0mAP@IoU=0.5:0.95 (%)20.1520.0319.5719.4720.4221.5120.6720.041x1 conv3x3 convFigure 6: What ﬂow? Examples of ﬂow images generated
by different ﬂow methods.

BroxFlow FlowNet RealTimeFlow

ﬂow-stream
RGB-stream
two-stream
two-in-one stream

11.60
18.49
19.79
21.51

7.13
18.49
19.75
19.97

3.58
18.49
18.53
19.16

Table 2: What ﬂow? No matter what ﬂow images are ap-
plied on UCF101-24, our two-in-one stream outperforms
the corresponding ﬂow-, RGB- and two-stream. We obtain
the best result with BroxFlow.

modulation layer to the bottom convolutional layers with
low-level RGB features. We conduct two experiments on
which layer to add the modulation. We compare the ac-
curacy and # param. after applying a modulation layer to
conv1, conv2, conv3 and conv4 in Figure 4 (a). Accuracy
decreases and # param.
increases slightly for deeper lay-
ers. Next we add the modulation layers to multiple convolu-
tional layers simultaneously in Figure 4 (b). Applying mul-
tiple modulation layers does not change the results much.
Thus, we prefer to use a single modulation layer. Note that
accuracy drops for deeper layers as we use 1x1 convolution
kernels to process ﬂow images, leading to smaller receptive
ﬁeld for deeper layers.

How to design the condition layer? To further improve
the method, we consider whether the 1x1 convolution ker-
nel for the motion condition layer is the best choice. Besides
keeping the spatial pixel-wise motion, it may need to con-
sider some context of motion to better ﬁt the RGB features.
We adopt the 3x3 convolution kernels to the last layer of
the condition network. Figure 5 demonstrates that consid-
ering motion context boosts the accuracy for all layers. As a
bigger receptive ﬁeld is used, the conv2 model achieves the
best results, about 1.5% improvement compared to 1x1 con-
volution kernels. The run time hardly increases for deeper
layers, and is still 0.04 sec per frame. The # param. are
26.85, 26.92, 27.01 and 27.19 M respectively of conv1,
conv2, conv3 and conv4. Considering the trade-off between
the results and parameters, we believe conv2 provides the
best accuracy/efﬁciency trade-off.

What ﬂow? As we leverage ﬂow information as prior
conditions, we wonder how the model is inﬂuenced by
ﬂow images. Here we adopt ﬂow images generated by
three different methods (seen in Figure 6) and evaluate how

Figure 7: Generalization ability. Accuracy comparison
on: (a) UCF101-24, (b) UCFSports, (c) J-HMDB, with dif-
ferent methods. Two-in-one stream even outperforms two-
stream on UCF101-24 and UCFSports. Two-in-one stream
fused with a ﬂow-stream obtains the best accuracy on all
three datasets.

our strategies work. We use BroxFlow [2] (accurate ﬂow
method), Flownet [6] (deep network method) and a real-
time but less accurate optical ﬂow method [23] (RealTime-
Flow). From Table 2, it is concluded that no matter which
kind of ﬂow images are applied, our two-in-one stream out-
performs RGB-streams and corresponding two-streams. We
also note that the more accurate the ﬂow images, the more
improvement the two-in-one stream obtains. Even when us-
ing the somewhat noisy RealTimeFlow images, the two-
in-one stream still improves the RGB-stream. However,
a two-stream based on RealTimeFlow obtains almost the
same accuracy as the RGB-stream, which illustrates that
two-stream depends on the the quality of ﬂow images. Our
two-in-one stream is more robust to the quality of ﬂow im-
ages. Moreover, we report the ﬂow computation in sec-
onds/frame for the three kinds of ﬂow methods: BroxFlow
(0.098), FlowNet (0.183) and RealTimeFlow (0.014). Re-
alTimeFlow only needs 0.014 seconds to generate one ﬂow
image, at the expense of a slightly lower mAP.

Generalization ability. To stress the generalization abil-
ity of our proposal, we compare the results on three different
datasets. Following the conclusions of our ablation so far,
we use the BroxFlow image for generating condition and
apply a 3x3 kernel to the last layer of the motion condition
layer. The motion modulation layer is only leveraged for
the conv2 layer of the appearance stream. We report results
in Figure 7.

Obviously,

the proposed two-in-one stream performs
better than other one-stream networks. It is noteworthy that
our two-in-one stream even outperforms traditional two-
stream networks on UCF101-24 and UCFSport by 2% with
only half the parameters of a two-stream network. On J-
HMDB, two-in-one is 3% higher than RGB-stream but 3%
lower than two-stream. We look into J-HMDB and ﬁnd
that most videos in the dataset have neighbouring repeated
frames. For fair comparison, we just download the Brox-
Flow images used in [21,37]. However, the provided Brox-
Flow image between the two repeated RGB frames is not 0,
as it should be, but similar to the last ﬂow frame. The issue
affects our two-in-one stream due to the fact that we need

9940

RGB imageBroxFlowFlowNetRealTimeFlow05101520two-in-one two streamtwo-in-one streamtwo-streamRGB-streamflow-stream22.0221.5119.7918.4911.42(a) UCF101-240204052.3251.6949.6148.2233.27(b) UCFSports010203040     43.238.7241.7835.8228.44(c) J-HMDBmAP@IoU=0.5:0.95 (%)Figure 8: Visualization of detection and heatmaps on
conv4 layers from RGB-stream network in (a) (b) and two-
in-one stream network in (c) (d). We add the green dashed
boxes to indicate the action. The two-in-one stream has
higher activation on actions, resulting in correct detection.

correct ﬂow image as the condition of the corresponding
RGB frame. We expect that two-in-one will present better
results on J-HMDB after correcting the ﬂow images. As
expected, adding a separate ﬂow-stream to our two-in-one
stream gives the best accuracy on all datasets.

4.3. Qualitative Analysis

The motion condition layer and the motion modulation
layer are beneﬁcial to generate better video representations
for spatio-temporal action detection. But how do the lay-
ers make a difference to the appearance network? To un-
derstand this behavior, we visualize in Figure 8, the detec-
tion results of an RGB-stream network and a two-in-one
network. Also, we visualize the gradient-weighted class
activation heatmaps [53] for better understanding how the
motion conditions inﬂuence the behavior of the appearance
network. We choose a challenging case of cliff diving here.
The image resolution is low and the actor is quite tiny. The
cluttered background obviously increases the difﬁculty to

Figure 9: Efﬁciency comparison to the state-of-the-art.
Accuracy vs. (a) inference time (second per frame) and (b)
# param. (M) on UCF101-24. Our two-in-one stream best
balances accuracy and efﬁciency.

detect actions. We manually overlay green dashed boxes
to indicate the locations of the actor and zoom in to high-
light where the action is happening. The second row shows
that the RGB-stream fails to detect any actions. From the
corresponding heatmaps, it is apparent that the appearance
network pays more attention to the background than to the
actions. There are only weak responses on the action po-
sitions. We manually overlay red dashed boxes to high-
light the position of the actor on the heatmaps. From the
heatmaps for the two-in-one network in the last row, we
clearly see it is capable to balance the activation on ac-
tions and background. The responses on action positions are
strengthened. As expected, the two-in-one stream performs
better than the RGB-stream. It outputs correct detections
for cliff diving on all the frames (forth row ).

4.4. Comparison to the State of the art

Accuracy. For fair comparisons, we just use the original
images as in all the state-of-the-arts, without camera motion
removal. We compare the mAP at variable IoU thresholds
in Table 3. Considering the most challenging high IoU
thresholds 0.5:0.95, we observe that for the single-frame
setting, our two-in-one stream achieves even better results
than existing two-stream methods on UCF101-24 and UCF-

9941

 CliffDiving 0.785CliffDiving 0.716CliffDiving 0.711(a) RGB-stream  Results: no detections (confidence scores < 0.5) (b) RGB-stream  Heatmaps: low activation on actor(c) Two-in-one   Results: correct detections (cliff diving scores > 0.5)         (d) Two-in-one   Heatmaps: high activation on actor0.00.10.20.30.40.5(a) inference time (sec/frame)81012141618202224mAP@IoU=0.5:0.95 (%)50100150200250300350(b) #param. (M)81012141618202224mAP@IoU=0.5:0.95 (%)single-framePeng & Schmid [29]Saha et al. [34]Behl et al. [1]Singh et al. [37]This paper: two-in-oneThis paper: two-in-one two-streammulti-frameSaha et al. [33]Kalogeiton et al. [21]This paper: two-in-oneThis paper: two-in-one two-streamUCF101-24

UCFSports

J-HMDB

0.20

0.50

0.75

0.50:0.95

0.50

0.75

0.50:0.95

0.50

0.75

0.50:0.95

Single-frame
71.80
Peng & Schmid [29]
Saha et al. [34]
66.70
71.53
Behl et al. [1]
Singh et al. [37]
73.50
This paper: Two-in-one
75.13
This paper: Two-in-one two stream 77.49

Multi-frame
63.06
Saha et al. [33]
Kalogeiton et al. [21]
76.50
79.00
Singh et al. [36]
75.48
This paper: Two-in-one
This paper: Two-in-one two stream 78.48

35.90
35.90
40.07
46.30
47.47
49.54

33.06
49.20
50.90
48.31
50.30

1.60
7.90
13.91
15.00
17.21
17.62

0.52
19.70
20.10
22.12
22.18

8.80
14.40
17.90
20.40
21.51
22.02

10.72
23.40
23.90
23.90
24.47

94.80

47.30

51.00

–
–
–

–
–
–

–
–
–

87.46
87.81

57.81
62.67

51.69
52.32

–

–

–

92.70

78.40

58.80

–

92.74
96.52

–

83.64
90.41

–

59.60
63.59

70.60
71.50

–

72.00
60.99
70.00

57.31
73.70

–

57.96
74.74

48.20
43.30

–

44.50
47.23
52.00

42.20
40.00

–

41.60
38.72
43.20

–

–

52.10

44.80

–

42.78
53.28

–

34.56
45.01

Table 3: Accuracy comparison to the state-of-the-art. Bold means top accuracy and italic means second top accuracy.
For the high overlap setting of mAP @IoU =0.5:0.95, our two-in-one stream works well in both a single-frame and multiple-
frame network for all three datasets. When we add an additional ﬂow-stream to obtain a two-in-one two stream we further
improve accuracy.

Sports. For instance, two-in-one stream outperforms Singh
et al. [37] with the same SSD detector by more than 1%
and Peng and Schmid [29] with a Faster-RCNN detector by
an absolute 12% on UCF101-24. As analyzed previously,
two-in-one stream performs modest on J-HMDB because
of the data issue of the provided BroxFlow images. When
we combine two-in-one into a regular two-stream network
by fusing with a ﬂow-stream, it produces good results on all
three datasets. Compared to two-in-one stream, it gets about
5% improvement on J-HMDB. Moreover, when feeding our
two-in-one network variants with multiple frames, as sug-
gested by Kalogeiton et al. [21], our two-in-one stream out-
performs the two-stream [21] a little on UCF101-24 and
UCFSports with only half computation and the number of
parameters. Our two-in-one stream fused with a ﬂow stream
further boosts the results, outperforming the very recent
work of Singh et al. [36].

Efﬁciency. Besides good detection accuracy, our method
has the advantage of a reduced inference time and less #
param.. Here we compare our methods from the efﬁciency
aspect to the state-of-the-art on UCF101-24. We test our
models on one NVIDIA GTX 1080 GPU. The trade-off be-
tween accuracy and inference time, as well as parameters
are visualized in Figure 9. Among the single-frame meth-
ods, our two-in-one stream has the fastest run time with
0.04s per frame, two times faster than [1] and [37] and much
faster than [34] and [29] (about 0.5s per frame). Moreover,
the # param. of our two-in-one stream is smallest, about
26.93 M. While our two-in-one accuracy is even better than
the two-stream methods by [1, 29, 34, 37]. Combining our
two-in-one stream with a standard ﬂow-stream gains an ac-
curacy improvement at the expense of more computation

and parameters. Our two-in-one alternative even outper-
forms [21] a little in accuracy with only half the parame-
ters. The two-in-one two stream further improves the re-
sult with almost similar inference time, but slightly more
parameters. We conclude that two-in-one stream networks
provide a good accuracy/efﬁciency trade-off.

5. Conclusion

We propose an effective and efﬁcient two-in-one stream
network for spatio-temporal action detection. It takes ﬂow
images as prior motion condition when training an RGB-
stream network. The network’s motion condition layer and
motion modulation layer address two issues in action de-
tection: frame-level RGB images lack motion information
and (static) background-context may dominant the learned
representation. Our two-in-one stream achieves state-of-
the-art accuracy at high IoU thresholds, using only half of
the parameters and computation of two-stream alternatives.
Besides motion, we believe that other information such as
depth-maps or infrared images may help locate the actors,
and can be exploited as additional prior conditions for train-
ing two-in-one streams.

Acknowledgments Supported by the Intelligence Advanced
In-
Research Projects Activity (IARPA) via Department of
terior/Interior Business Center
(DOI/IBC) contract number
D17PC00343. The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing endorsements, either
expressed or implied, of IARPA, DOI/IBC, or the U.S. Govern-
ment.

9942

References

[1] Harkirat S Behl, Michael Sapienza, Gurkirt Singh, Suman
Saha, Fabio Cuzzolin, and Philip HS Torr. Incremental tube
construction for human action detection. In BMVC, 2018. 1,
8

[2] Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim
Weickert. High accuracy optical ﬂow estimation based on a
theory for warping. In ECCV, 2004. 5, 6

[3] Liangliang Cao, Zicheng Liu, and Thomas S Huang. Cross-

dataset action detection. In CVPR, 2010. 2

[4] Navneet Dalal, Bill Triggs, and Cordelia Schmid. Human
detection using oriented histograms of ﬂow and appearance.
In ECCV, 2006. 2

[5] Harm De Vries, Florian Strub,

J´er´emie Mary, Hugo
Larochelle, Olivier Pietquin, and Aaron C Courville. Mod-
ulating early visual processing by language. In NIPS, 2017.
1

[6] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical ﬂow with convolutional networks. In ICCV,
2015. 6

[7] Kevin Duarte, Yogesh Rawat, and Mubarak Shah. Video-
In

capsulenet: A simpliﬁed network for action detection.
NeurIPS, 2018. 2

[8] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
A learned representation for artistic style. In ICLR, 2017. 1
[9] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.
Convolutional two-stream network fusion for video action
recognition. In CVPR, 2016. 2, 4

[10] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent
Dumoulin, and Jonathon Shlens. Exploring the structure of
a real-time, arbitrary neural artistic stylization network. In
BMVC, 2017. 1

[11] Rohit Girdhar, Jo˜ao Carreira, Carl Doersch, and Andrew
arXiv preprint

Zisserman. A better baseline for ava.
arXiv:1807.10066, 2018. 2

[12] Ross Girshick. Fast r-cnn. In ICCV, 2015. 4
[13] Georgia Gkioxari and Jitendra Malik. Finding action tubes.

In CVPR, 2015. 1, 2

[14] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Car-
oline Pantofaru, David A Ross, George Toderici, Yeqing Li,
Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, et al.
Ava: A video dataset of spatio-temporally localized atomic
visual actions. In CVPR, 2018. 2

[15] Jiawei He, Zhiwei Deng, Mostafa S Ibrahim, and Greg Mori.
Generic tubelet proposals for action localization. In WACV,
2018. 2

[16] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolu-
tional neural network (t-cnn) for action detection in videos.
In ICCV, 2017. 2

[17] Xun Huang and Serge J Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. In ICCV,
2017. 1

[19] Mihir Jain, Jan C. van Gemert, Herv´e J´egou, Patrick
Bouthemy, and Cees G. M. Snoek. Tubelets: Unsupervised
action proposals from spatiotemporal super-voxels.
IJCV,
124(3):287–311, 2017. 2

[20] Hueihan Jhuang,

Juergen Gall, Silvia Zufﬁ, Cordelia
Schmid, and Michael J Black. Towards understanding ac-
tion recognition. In ICCV, 2013. 4

[21] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari,
and Cordelia Schmid. Action tubelet detector for spatio-
temporal action localization. In ICCV, 2017. 1, 2, 4, 5, 6,
8

[22] Alexander Klaser, Marcin Marszałek, and Cordelia Schmid.
In

A spatio-temporal descriptor based on 3d-gradients.
BMVC, 2008. 2

[23] Till Kroeger, Radu Timofte, Dengxin Dai, and Luc Van Gool.
Fast optical ﬂow using dense inverse search. In ECCV, 2016.
6

[24] Tian Lan, Yang Wang, and Greg Mori. Discriminative ﬁgure-
centric models for joint action localization and recognition.
In ICCV, 2011. 2, 4

[25] Dong Li, Zhaofan Qiu, Qi Dai, Ting Yao, and Tao Mei. Re-
current tubelet proposal and recognition networks for action
detection. In ECCV, 2018. 2

[26] Zhenyang Li, Kirill Gavrilyuk, Efstratios Gavves, Mihir Jain,
and Cees G. M. Snoek. Videolstm convolves, attends and
ﬂows for action recognition. CVIU, 166:41–50, 2018. 2

[27] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In ECCV, 2016.
2, 4

[28] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. Ef-
ﬁcient action localization with approximately normalized
ﬁsher vectors. In CVPR, 2014. 2

[29] Xiaojiang Peng and Cordelia Schmid. Multi-region two-
In ECCV, 2016. 1, 2,

stream r-cnn for action detection.
8

[30] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In AAAI, 2018. 1

[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 4

[32] Mikel D Rodriguez, Javed Ahmed, and Mubarak Shah. Ac-
tion mach a spatio-temporal maximum average correlation
height ﬁlter for action recognition. In CVPR, 2008. 4

[33] Suman Saha, Gurkirt Singh, and Fabio Cuzzolin. Amtnet:
Action-micro-tube regression by end-to-end trainable deep
architecture. In ICCV, 2017. 8

[34] Suman Saha, Gurkirt Singh, Michael Sapienza, Philip HS
Torr, and Fabio Cuzzolin. Deep learning for detecting multi-
ple space-time action tubes in videos. In BMVC, 2016. 2, 4,
8

[35] Karen Simonyan and Andrew Zisserman. Two-stream con-
In

volutional networks for action recognition in videos.
NIPS, 2014. 1, 2

[18] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015. 1

[36] Gurkirt Singh, Suman Saha, and Fabio Cuzzolin. Tramnet-
transition matrix network for efﬁcient action tube proposals.
In ACCV, 2018. 1, 2, 8

9943

[37] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip HS
Torr, and Fabio Cuzzolin. Online real-time multiple spa-
tiotemporal action localisation and prediction.
In ICCV,
2017. 1, 2, 4, 5, 6, 8

[38] Khurram Soomro, Haroon Idrees, and Mubarak Shah. Pre-
dicting the where and what of actors and actions through on-
line action localization. In CVPR, 2016. 4

[39] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402, 2012. 4

[40] Rastislav ˇSr´amek, Broˇna Brejov´a, and Tom´aˇs Vinaˇr. On-line
viterbi algorithm and its relationship to random walks. arXiv
preprint arXiv:0704.0062, 2007. 2

[41] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Mur-
phy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric
relation network. In ECCV, 2018. 2

[42] Yicong Tian, Rahul Sukthankar, and Mubarak Shah. Spa-
tiotemporal deformable part models for action detection. In
CVPR, 2013. 2

[43] Du Tran and Junsong Yuan. Max-margin structured output
regression for spatio-temporal action localization. In NIPS,
2012. 2

[44] Du Tran, Junsong Yuan, and David Forsyth. Video event
detection: From subvolume localization to spatio-temporal
path search. TPAMI, 2014. 2

[45] Jan C Van Gemert, Mihir Jain, Ella Gati, and Cees G. M.
Snoek. Apt: Action localization proposals from dense tra-
jectories. In BMVC, 2015. 2

[46] Limin Wang, Yuanjun Xiong, Zhe Wang, and Yu Qiao. To-
wards good practices for very deep two-stream convnets.
arXiv preprint arXiv:1507.02159, 2015. 5

[47] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In CVPR, 2018. 1

[48] Philippe Weinzaepfel, Zaid Harchaoui,

and Cordelia
Schmid. Learning to track for spatio-temporal action local-
ization. In ICCV, 2015. 1, 2, 4

[49] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
Kevin Murphy. Rethinking spatiotemporal feature learning:
Speed-accuracy trade-offs in video classiﬁcation. In ECCV,
2018. 2

[50] Zhenheng Yang, Jiyang Gao, and Ram Nevatia. Spatio-
temporal action detection with cascade proposal and location
anticipation. In BMVC, 2017. 2

[51] Gang Yu and Junsong Yuan. Fast action proposals for human

action detection and search. In CVPR, 2015. 2

[52] Junsong Yuan, Zicheng Liu, and Ying Wu. Discriminative
video pattern search for efﬁcient action detection. TPAMI,
33(9):1728–1743, 2011. 2

[53] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In CVPR, 2016. 7

9944

