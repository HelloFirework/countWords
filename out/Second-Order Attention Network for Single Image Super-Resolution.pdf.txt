Second-order Attention Network for Single Image Super-Resolution

Tao Dai1

2

,

,∗,‡ , Jianrui Cai3

,∗ , Yongbing Zhang1, Shu-Tao Xia1

,

2, Lei Zhang3

4

,§

,

1Graduate School at Shenzhen, Tsinghua University, Shenzhen, China

2 PCL Research Center of Networks and Communications, Peng Cheng Laboratory, Shenzhen, China

3Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China

4DAMO Academy, Alibaba Group

{dait14, zhang.yongbing, xiast}@sz.tsinghua.edu.cn, {csjcai, cslzhang}@comp.polyu.edu.hk

Abstract

Recently, deep convolutional neural networks (CNNs)
have been widely explored in single image super-resolution
(SISR) and obtained remarkable performance. However,
most of the existing CNN-based SISR methods mainly focus
on wider or deeper architecture design, neglecting to ex-
plore the feature correlations of intermediate layers, hence
hindering the representational power of CNNs. To address
this issue, in this paper, we propose a second-order atten-
tion network (SAN) for more powerful feature expression
and feature correlation learning. Speciﬁcally, a novel train-
able second-order channel attention (SOCA) module is de-
veloped to adaptively rescale the channel-wise features by
using second-order feature statistics for more discrimina-
tive representations. Furthermore, we present a non-locally
enhanced residual group (NLRG) structure, which not only
incorporates non-local operations to capture long-distance
spatial contextual information, but also contains repeated
local-source residual attention groups (LSRAG) to learn in-
creasingly abstract feature representations. Experimental
results demonstrate the superiority of our SAN network over
state-of-the-art SISR methods in terms of both quantitative
metrics and visual quality.

1. Introduction

Single image super-resolution (SISR) [5] has recently re-
ceived much attention. In general, the purpose of SISR is
to produce a visually high-resolution (HR) output from its
low-resolution (LR) input. However, this inverse problem

∗The ﬁrst two authors contribute equally to this work.
§Corresponding author: Lei Zhang
‡This work is supported in part by the National Natural Science Foun-
dation of China under Grant 61771273, the R&D Program of Shenzhen un-
der Grant JCYJ20180508152204044, and the research fund of PCL Future
Regional Network Facilities for Large-scale Experiments and Applications
(PCL2018KP001).

is ill-posed since multiple HR solutions can map to any LR
input. Therefore, a great number of SR methods have been
proposed, ranging from early interpolation-based [37] and
model-based [4], to recent learning-based methods [32, 39].
The early developed interpolated-based methods (e.g.,
bilinear and bicubic methods) are simple and efﬁcient but
limited in applications. For more ﬂexible SR methods, more
advanced model-based methods are proposed by exploiting
powerful image priors, such as non-local similarity prior
[34] and sparsity prior [4]. Although such model-based
methods are ﬂexible to produce relative high-quality HR
images, they still suffer from some drawbacks: (1) such
methods often involve a time-consuming optimization pro-
cess; (2) the performance may degrade quickly when image
statistics are biased from the image prior.

Deep convolution neural networks (CNNs) have re-
cently achieved unprecedented success in various problems
[7, 25]. The powerful feature representation and end-to-end
training paradigm of CNN makes it a promising approach
to SISR. In the last several years, a ﬂurry of CNN-based
SISR methods have been proposed to learn a mapping func-
tion from an interpolated or LR input to its corresponding
HR output. By fully exploiting the image statics inherent in
training datasets, CNNs have achieved state-of-the-art re-
sults in SISR [2, 12, 14, 36, 39, 38]. Although considerable
progress has been achieved in image SR, existing CNN-
based SR models are still faced with some limitations: (1)
most of CNN-based SR methods do not make full use of
the information from the original LR images, thereby result-
ing in relatively-low performance; (2) most existing CNN-
based SR models focus mainly on designing a deeper or
wider network to learn more discriminative high-level fea-
tures, while rarely exploiting the inherent feature correla-
tions in intermediate layers, thus hindering the representa-
tional ability of CNNs.

To address these problems, we propose a deep second-
order attention network (SAN) for more powerful feature
expression and feature correlation learning. Speciﬁcally, we

111065

(a) HR

(b) FSRCNN (c) LapSRN [14]

(d) SRMD [36]

(e) EDSR [36]

(f) DBPN [20]

(g) RDN [6]

(h) Ours

Figure 1. Zoom visual results for 4× SR on “img 092” from Urban100. Our method obtains better visual quality and recovers more image
details compared with other state-of-the-art SR methods

propose a second-order channel attention (SOCA) mech-
anism for better feature correlation learning. Our SOCA
adaptively learns feature inter-dependencies by exploiting
second-order feature statistics instead of ﬁrst-order ones.
Such SOCA mechnism makes our network focus on more
informative feature and improve discriminative learning
ability. Moreover, a non-locally enhanced residual group
(NLRG) structure is presented to further incorporates non-
local operations to capture long-distance spatial contextual
information. By stacking the local-source residual attention
groups (LSRAG) structure, we can exploit the information
from the LR images and allow the abundant low-frequency
information to be bypassed. As shown in Fig. 1, our method
obtains better visual quality and recovers more image de-
tails compared with other state-of-the-art SR methods.

In summary, the main contributions of this paper are

listed as follows:

• We propose a deep second-order attention network
(SAN) for accurate image SR. Extensive experiments
on public datasets demonstrate the superiority of our
SAN over state-of-the-art methods in terms of both
quantitive and visual quality.

• We propose second-order channel attention (SOCA)
mechanism to adaptively rescale features by consid-
ering feature statistics higher than ﬁrst-order. Such
SOCA mechanism allows our network to focus on
more informative features and enhance discriminative
learning ability. Besides, we also utilize an iterative
method for covariance normalization to speed up the
training of our network.

• We propose non-locally enhanced residual group
(NLRG) structure to build a deep network, which fur-
ther incorporates non-local operations to capture spa-
tial contextual information, and share-source residual
group structure to learn deep features. Besides, the
share-source residual group structure through share-
source skip connections could allow more abundant in-
formation from the LR input to be bypassed and ease
the training of the deep network.

2. Related Work

During the past decade, a plenty of image SISR meth-
ods have been proposed in the computer vision community,

including interpolation-based [37], model-based [34], and
CNN-based methods [2, 29, 14, 13, 29, 17, 30, 39, 38]. Due
to space limitation, we here brieﬂy review works related to
CNN-based SR methods and attention mechanism, which is
close to our method.

CNN-based SR models. Recently, CNN-based methods
have been extensively studied in image SR, due to their
strong nonlinear representational power. Generally, such
methods cast SR as an image-to-image regression problem,
and learn an end-to-end mapping from LR to HR directly.
Most existing CNN-based methods mainly focus on design-
ing a deeper or wider network structure [2, 12, 13, 6, 39, 38].
For example, Dong et al. [2] ﬁrst introduced a shallow
three-layer convolutional network (SRCNN) for image SR,
which achieves impressive performance. Later, Kim et al.
designed deeper VDSR [12] and DRCN [13] with more than
16 layers based on residual learning. To further improve the
performance, Lim et al. [20] proposed a very deep and wide
network EDSR by stacking modiﬁed residual blocks. The
signiﬁcant performance gain indicates the depth of repre-
sentation plays a key role in image SR. Other recent works
like MemNet [30] and RDN [39], are based on dense blocks
[10] to form deep networks and focus on utilizing all the
hierarchical features from all the convolutional layers. In
addition to focusing on increasing the depth of the network,
some other networks, such as NLRN [22] and RCAN [38],
improve the performance by considering feature correla-
tions in spatial or channel dimension.

Attention mechanism. Attention in human perception gen-
erally means that human visual systems adaptively process
visual information and focus on salient areas [16]. In re-
cent years, several trials have embeded attention process-
ing to improve the performance of CNNs for various tasks,
such as image and video classiﬁcation tasks [9, 33]. Wang
et al. [33] proposed non-local neural network to incorpo-
rate non-local operations for spatial attention in video clas-
siﬁcation. On the contrary, Hu et al. [9] proposed SENet
to exploit channel-wise relationships to achieve signiﬁcant
performance gain for image classiﬁcation.

Recently, SENet was introduced to deep CNNs to fur-
ther improve SR performance [38]. However, SENet only
explores ﬁrst-order statistics (e.g., global average pooling),
while ignoring the statistics higher than ﬁrst-order, thus hin-
dering the discriminative ability of the network.
In im-

11066

age SR, features with more high-frequency information are
more informative for HR reconstruction. To this end, we
propose a deep second-order attention network (SAN) by
exploring second-order statistics of features.

3. Second-order Attention Network (SAN)

3.1. Network Framework

As shown in Fig. 2, our SAN mainly consists of four
parts:
shallow feature extraction, non-locally enhanced
residual group (NLRG) based deep feature extraction, up-
scale module, and reconstruction part. Given ILR and ISR
as the input and output of SAN. As explored in [20, 39], we
apply only one convolutional layer to extract the shallow
feature F0 from the LR input

F0 = HSF (ILR),

(1)

where HSF (·) stands for convolution operation. Then the
extracted shallow feature F0 is used for NLRG based deep
feature extraction, which thus produces the deep feature as

FDF = HN LRG(F0),

(2)

where HN LRG represents the NLRG based deep feature ex-
traction module, which consists of several non-local mod-
ules to enlarge receptive ﬁeld and G local-source residual
attention group (LSRAG) modules (see Fig. 2). So our pro-
posed NLRG obtains very deep depth and thus provides
very large receptive ﬁeld size. Then the extracted deep fea-
ture FDF is upscaled via the upscale module via

F↑ = H↑(FDF ),

(3)

where H↑(·) and F↑ are a upscale module and upscaled
feature respectively. There are some choices to act as up-
scale part, such as transposed convolution [3], ESPCN [28].
The way of embedding upscaling feature in the last few lay-
ers obtains a good trade off between computational burden
and performance, and thus is preferable to be used in recent
CNN-based SR models [3, 6, 39]. The upscaled feature is
then mapped into SR image via one convolution layer

training SAN is to optimize the L1 loss function:

L(Θ) =

1
N

N

(cid:2)

i=1

||HSAN (Ii

LR) − Ii

HR||1,

(5)

where Θ denotes the parameter set of SAN. The loss func-
tion is optimized by stochastic gradient descent algorithm.

3.2. Non-locally Enhanced Residual Group (NLRG)

We now show our non-locally enhanced residual group
(NLRG) (see Fig. 2), which consists of several region-level
non-local (RL-NL) modules and one share-source residual
group (SSRG) structure. The RL-NL exploits the abundant
structure cues in LR features and the self-similarities in HR
nature scenes. The SSRG is composed of G local-source
residual attention groups (LSRAG) with share-source skip
connections (SSC). Each LSRAG further contains M sim-
pliﬁed residual blocks with local-source skip connection,
followed by a second-order channel attention (SOCA) mod-
ule to exploit feature interdependencies.

It has been veriﬁed that stacking residual blocks is help-
ful to form a deep CNN in [20, 39]. However, very deep
network built in such way would suffer from training dif-
ﬁculty and performance bottleneck due to the problem of
gradient vanishing and exploding in deep network. Inspired
by the work in [15], we propose local-source residual atten-
tion group (LSRAG) as the fundamental unit. It is known
that simply stacking repeated LSRAGs would fail to obtain
better performance. To address this issue, the share-source
skip connection (SSC) is introduced in NLRG to not only
facilitate the training of our deep network, but also to by-
pass abundant low-frequency information from LR images.
Then a LSRAG in the g-th group is represented as:

Fg = WSSC F0 + Hg(Fg−1),

(6)

where WSSC denotes the weight to the convolution layer,
and is initialized as 0, and then gradually learns to assign
more weight to the shallow feature. The bias term is omitted
for simplicity. Hg(·) is the function of the g-th LSRAG.
Fg, Fg−1 denote the input and output of the g-th LSRAG.
The deep feature is then obtained as:

ISR = HR(F↑) = HSAN (ILR),

(4)

FDF = WSSC F0 + FG.

(7)

where HR(·), H↑(·) and HSAN are the reconstruction layer,
upscale layer and the function of SAN, respectively.

Then SAN will be optimized with a certain loss func-
tion. Some loss functions have been widely used, such as
L2 [2, 12, 29, 30], L1 [14, 15, 20, 39], perceptual losses
[11, 26]. To verify the effectiveness of our SAN, we adopt
the same loss functions as previous works (e.g., L1 loss
function). Given a training set with N LR images and their
HR counterparts denoted by {Ii
i=1, the goal of

HR}N

LR, Ii

Such SSRG structure can not only ease the ﬂow of informa-
tion across LSRAGs, but also make it possible to train very
deep CNN for image SR with high performance.
Region-level non-local module (RL-NL). The proposed
NLRG also exploits the abundant structure cues in LR fea-
tures and the self-similarities in HR nature scenes by RL-
NL modules plugged before and after the SSRG. The non-
local neural network [33] is proposed to capture the com-
putation of long-range dependencies throughout the entire

11067

LR

LSRAG-1

LSRAG-g

LSRAG-G

RL-
NL

Fg-1

Fg

RL-
NL

HR

Share-source Residual Group(SSRG)

Non-locally Enhanced  Residual Group (NLRG)

Fg,m-1

Fg,m

Fg-1

Fg

SOCA

H× W×C

H× W×C

H× W×C

LSRAG

Conv

RELU

HGCP

WD

WU

f

LSRAG module

Residual 

block

Upscale 
module

SOCA module

H× W×C

W1×1

W1×1
W1×1
W1×1

RL-NL module

Figure 2. Framework of the proposed second-order attention network (SAN) and its sub-modules.

image for high-level tasks. However, traditional global-
level non-local operations may be limited for some reasons:
1) global-level non-local operations require unacceptable
computational burden, especially when the size of feature
is large; 2) it is empirically shown that non-local operations
at a proper neighborhood size are preferable for low-level
tasks (e.g., image super-resolution) [22]. Thus for feature
with higher spatial resolution or degradation, it is natural
to perform region-level non-local operations. For such rea-
sons, we divide the feature map into a grid of regions (see
Fig. 2, the k × k RL-NL indicates the input feature is ﬁrst
divided into a grid of k2 blocks with the same size.), each
of which is then processed by the subsequent layers.

After non-local operations, the feature representation is
non-locally enhanced before fedding into the subsequent
layers via exploiting the spatial correlations of features.
Local-source residual attention group (LSRAG). Due
to our share-source skip connections, the abundant low-
frequency information can be bypassed. To go a further
step to residual learning, we stack M simpliﬁed residual
blocks to form a basic LSRAG. The m-th residual block
(see Fig. 2) in the g-th LSRAG can be represented as

Fg,m = Hg,m(Fg,m−1),

(8)

where Hg,m(·) denotes the function of m-th residual block
in g-th LSRAG, and Fg,m−1, Fg,m are the corresponding
input and output. To make our network focus on more in-
formative features, a local-source skip connection is used to
produce the block output via

Fg = WgFg−1 + Fg,M ,

(9)

where Wg is the corresponding weight. Such local-source
and share-source skip connections allow more abundant
low-frequency information to be bypassed during training.
For more discriminative representations, we propose SOCA
mechnism embedded at the tail of each LSRAG. Our SOCA
mechnism learns to adaptively rescale channel-wise fea-
tures by considering second-order statistics of features.

3.3. Second-order Channel Attention (SOCA)

Most previous CNN-based SR models do not consider
the feature interdependencies. To utilize such information,
SENet [9] was introduced in CNNs to rescale the channel-
wise features for image SR. However, SENet only exploits
ﬁrst-order statistics of features by global average pooling,
while ignoring statistics higher than ﬁrst-order, thus hinder-
ing the discriminative ability of the network. On the other
hand, recent works [19, 21] have shown that second-order
statistics in deep CNNs are more helpful for more discrimi-
native representations than ﬁrst-order ones.

Inspired by the above observations, we propose a
second-order channel attention (SOCA) module to learn
feature interdependencies by considiering second-order
statistics of features. Now we will describe how to exploit
such second-order information next.
Covariance normalization. Given a H × W × C feature
map F = [f1, · · · , fC] with C feature maps with size of
H × W . We reshape the feature map to a feature matrix X
with s = W H features of C-dimension. Then the sample
covariance matrix can be computed as

Σ = X¯IXT ,

(10)

where ¯I = 1
and matrix of all ones, respectively.

s (I − 1

s 1), I and 1 are the s × s identity matrix

It is shown in [27, 19] that covariance normalization
plays a critical role for more discriminative representations.
For this reason, we ﬁrst perform covariance normalization
for the obtained covariance matrix Σ, which is symmetric
positive semi-deﬁnite and thus has eigenvalue decomposi-
tion (EIG) as follows

Σ = UΛUT ,

(11)

=
where U is
diag(λ1, · · · , λC) is diagonal matrix with eigenvalues
in non-increasing order. Then convariance normalization

orthogonal matrix

and Λ

an

11068

can be converted to the power of eigenvalues:

3.4. Covariance Normalization Acceleration

ˆY = Σα = UΛαUT ,

(12)

1 , · · · , λα

and Λα =
where α is a positive real number,
diag(λα
C). When α = 1, there is no normaliza-
tion; when α < 1, it nonlinearly shrinks the eigenvalues
larger than 1.0 and streches those less than 1.0. As explored
in [19], α = 1/2 works well for more discriminative repre-
sentations. Thus, we set α = 1/2 in the following.
Channel attention. The normalized covariance matrix
characterizes the correlations of channel-wise features. We
then take such normalized covariance matrix as a channel
descriptor by global covariance pooling. As illustrated in
Fig. 2, let ˆY = [y1, · · · , yC], the channel-wise statistics
z ∈ RC×1 can be obtained by shrinking ˆY. Then the c-th
dimension of z is computed as

zc = HGCP (yc) =

1
C

C

(cid:2)

i

yc(i),

(13)

where HGCP (·) denotes the global covariance pooling
function. Compared with the commonly used ﬁrst-order
pooling (e.g., global average pooling), our global covari-
ance pooling explores the feature distribution and captures
the feature statistics higher than ﬁrst-order for more dis-
criminative representations.

To fully exploit feature interdependencies from the ag-
gregated information by global covariance pooling, we ap-
ply a gating mechanism. As explored in [9], the simple sig-
moid function can serve as a proper gating function

To date, fast implementation of EIG on GPU is still an
open problem. Inspired by [18], we utilize Newton-Schulz
iteration to speed up the computation of covariance normal-
ization. Speciﬁcally, from Equ. (11), the Σ has square root
i )UT . Given Y0 = Σ, Z0 = I,
as Σ1/2 = Y = Udiag(λ1/2
for n = 1, · · · , N , as shown in [18], the Newton-Schulz
iteration is then updated alternately as follows:

Yn = 1
Zn = 1

2 Yn−1(3I − Zn−1Yn−1),
2 (3I − Zn−1Yn−1)Zn−1.

(16)

After enough iterations, Yn and Zn quadratically converges
to Y and Y−1. Such iterative operation is suitable for par-
allel implementation on GPU. In practice, one can achieve
approximate solution with few iterations, e.g., no more than
5 iterations in our method.

Since Newton-Schulz iteration only converges locally, to

guarantee the convergence, we pre-normalize Σ ﬁrst via

ˆΣ =

1

tr(Σ)

Σ,

(17)

where tr(Σ) = (cid:3)C
i λi denotes the trace of Σ. In such case,
it can be inferred that the ||Σ − I||2 equals to the largest
singular value of (Σ − I), i.e., 1 − λi
(cid:2)i λi) less than 1, which
thus satisﬁes the convergence condition.

After Newton-Schulz iteration, we apply a post-
compensation procedure to compensate the data magnitude
caused by pre-normalization, thus producing the ﬁnal nor-
malized covariance matrix

ˆY = (cid:4)tr(Σ)YN .

(18)

w = f (WU δ(WDz)),

(14)

3.5. Implementations

where WD and WU are the weight set of convolution layer,
which set channel dimension of features to C/r and C, re-
spectively. f (·) and δ(·) are the function of sigmoid and
RELU. Finally, we obtain the channel attention map w to
rescale the input

ˆfc = wc · fc,

(15)

where wc and fc denote the scaling factor and feature map in
the c-th channel. With such channel attention, the residual
component in the LSRAG is rescaled adaptively.

As is shown above, covariance normalization plays a vi-
tal role in our SOCA. However, such covariance normal-
ization relies heavily on eigenvalue decoomposition, which
is not well supported on GPU platform, thus leading to in-
efﬁcient training. To solve this issue, as explored in [18],
we also apply a fast matrix normalization method based on
Newton-Schulz iteration [8]. In the next section, we brieﬂy
describe the covariance normalization.

We set LSRAG number as G = 20 in the SSRG struc-
ture, and embed RL-NL modules (k = 2) at the head and
tail of SSRG. In each LSRAG, we use m = 10 residual
blocks plus single SOCA module at the tail. In SOCA mod-
ule, we use 1 × 1 convolution ﬁlter with reduction ratio
r = 16. For other convolution ﬁlter outside SOCA, the
size and number of ﬁlter are set as 3 × 3 and C = 64,
respectively. For upscale part H↑(·), we follow the works
in [20, 39] and apply ESPCNN [28] to upscale the deep
features, followed by one ﬁnal convolution layer with three
ﬁlters to produce color images (RGB channels).

3.6. Discussions

Difference to Non-local RNN (NLRN). NLRN [22] in-
troduces non-local operations to capture long-distance spa-
tial contextual information in image restoration. There
are some differences between NLRN and our SAN. First,
NLRN embeds non-local operations in a recurrent neural

11069

network (RNN) for image restoration, while our SAN incor-
porates non-local operations in deep convolutional neural
network (CNN) framework for image SR. Second, NLRN
only considers spatial feature correlations between each
location and its neighborhood, but ignores the channel-
wise feature correlations. While our SAN mainly focuses
on learning such channel-wise feature correlations with
second-order statistics of features for more powerful rep-
resentational ability.
Difference to Residual Dense Network (RDN). We sum-
marize the main differences between RDN [39] and our
SAN. The ﬁrst one is the design of basic block. RDN
mainly combines dense blocks with local feature fusion by
using local residual learning, while our SAN is built on the
basis of residual blocks. The second one is the way of en-
hancing discriminative ability of the network. Channel at-
tention [9, 38] has been shown to be effective for better dis-
criminative representations. However, RDN does not con-
sider such information, but pays attention to exploiting the
hierarchical features from all the convolutional layers. On
the contrary, our SAN heavily relies on channel attention
for better discriminative representations. Thus, we propose
second-order channel attention (SOCA) mechanism to ef-
fectively learn channel-wise feature interdependencies.
Difference to Residual Channel Attention Network
(RCAN). Zhang et al.
[38] proposed a residual in resid-
ual structure to form a very deep network. RCAN is close
to our SAN, and the main differences lie in the following as-
pects. First, RCAN consists of several residual groups with
long skip connections. While, SAN stacks repeated resid-
ual groups through share-source skip connections, which
allows more abundant low-frequency information to be by-
passed. Second, RCAN can only exploit the contextual in-
formation in a local receptive ﬁeld, but is unable to exploit
the information outside of the local region. While SAN can
alleviate this problem by incorporating non-local operations
to not only capture long-distance spatial contextual infor-
mation, but enlarge the receptive ﬁeld. Third, to enhance
the discriminative ability of the network, RCAN only con-
siders channel attention based ﬁrst-order feature statistics
by global average pooling. While our SAN learns channel
attention based on second-order feature statistics.

To the best of our knowledge, it is the ﬁrst attempt to in-
vestigate the effect of such attention based on second-order
feature statistics for image SR. More analysis about the ef-
fect of such attention mechanism are shown next.

4. Experiments

4.1. Setup

Following [20, 6, 39, 38], we use 800 high-resolution
images from DIV2K dataset [31] as training set. For test-
ing, we adopt 5 standard benchmark datasets: Set5, Set14,

BSD100, Urban100 and Manga109, each of which has dif-
ferent characteristics. We carry out experiments with Bicu-
bic (BI) and Blur-downscale (BD) degradation models [36].
All the SR results are evaluated by PSNR and SSIM metrics
on Y channel of transformed YCbCr space.

During training, we augment the training images by ran-
domly rotating 90◦, 180◦, 270◦ and horizontally ﬂipping. In
each min-batch, 8 LR color patches with size 48 × 48 are
provided as inputs. Our model is trained by ADAM optimi-
zor with β1 = 0.9, β2 = 0.99, and ε = 10−8. The learning
rate is initialized as 10−4 and then reduced to half every 200
epochs. Our proposed SAN has been implemented on the
Pytorch framework [23] on an Nvidia 1080Ti GPU.

4.2. Ablation Study

As discussed in Section 3, our SAN contains two main
components including non-locally enhanced residual group
(NLRG) and second-order channel attention (SOCA).
Non-locally Enhanced Residual Group (NLRG). To ver-
ify the effectiveness of different modules, we compare
NLRG with its variants trained and tested on Set5 dataset.
The speciﬁc performance is listed in Table 1.

Base refers to a very basic baseline which only con-
tains the convolution layers with 20 LSRAGs and 10 resid-
ual blocks in each LSRAG, thus resulting in deep network
with over 400 convolution layers. As in [38], we also add
long and short skip connections in Base model. From Ta-
ble 1 we can see that Base reaches PSNR=32.00 dB on
Set5 (×4). Results from Ra to Re verify the effectiveness
of individual module, since the module used alone improves
the performance over Base model. Speciﬁcally, Ra and
Rb that add a single RL-NL in shallow (before SSRG) or
deep layers (after SSRG) obtain similar SR results and out-
perform Base, which veriﬁes the effectiveness of RL-NL.
When share-source skip connection (SSC) is added alone
(Rc), the performance can be improved from 32.00 dB to
32.07 dB. The main reason lies in that share-source skip
connections allows more abundant low-frequency informa-
tion from the LR images to be bypassed. When both of Ra
and Rb are used (leading to Rf ), the performance can be
further improved. It is found more RL-NL modules cannot
obtain much better performance than Rf in our method, and
thus we apply Rf in our method to balance the performance
and efﬁciency.
Second-order channel attention (SOCA). We also show
the effect of our SOCA from the results of Rd, Re, Rh and
Ri. Speciﬁcally, Rd means that channel attention is based
on ﬁrst-order feature statistics by global average pooling,
thus leading to ﬁrst-order channel attention (FOCA). Re
means that channel attention is based on second-order fea-
ture statistics, thus leading to our second-order channel at-
tention (SOCA). It can be found that both of Rd and Re
obtain better performance than methods of Ra to Rc with-

11070

Table 1. Effects of different modules. We report the best PSNR (dB) values on Set5 (4×) in 5.6 × 105 iterations.

RL-NL(before SSRG)
RL-NL(after SSRG)
share-source skip connection (SSC)
First-order channel attention (FOCA)
Second-order channel attention (SOCA)

Base

Ra
!

Rb

Rc

Rd

Re

!

!

!

Rf
!
!

Rg
!
!
!

Rh
!
!
!
!

32.00

32.04

32.06

32.07

32.12

!
32.16

32.08

32.10

32.14

Ri
!
!
!

!
32.20

HR

PSNR/SSIM

Bicubic

17.02/0.7101

SRCNN [2]
18.39/0.8023

FSRCNN [3]
18.21/0.7994

LapSRN [14]
18.66/0.8406

Urban100 (4×):

img 067

EDSR [20]
21.17/0.9052

DBPN [6]

20.31/0.8910

RDN [39]

20.87/0.9023

RCAN [38]
21.29/0.9127

SAN

21.34/0.9081

HR

PSNR/SSIM

Bicubic

21.59/0.6325

SRCNN [2]

22.5619/0.7316

FSRCNN [3]
22.0382/0.6807

LapSRN [14]
22.03/0.6948

Urban100 (4×):

img 076

EDSR [20]
23.95/0.7750

DBPN [6]

23.21/0.7455

RDN [39]

24.08/0.7801

RCAN [38]
24.30/0.7896

SAN

24.53/0.7925

Figure 3. Visual comparison for 4× SR with BI model on Urban100 dataset. The best results are highlighted

out channel attention. This indicates that channel atten-
tion plays a more important role in determining the per-
formance. Furthermore, compared with FOCA, our SOCA
achieves consistently better results, no matter if combined
with other modules (e.g., RL-NL and SSC). These observa-
tions demonstrate the superiority of our SOCA.

4.3. Results with Bicubic Degradation (BI)

To test the effectiveness of our SAN, we compare our
SAN with 11 state-of-the-art CNN-based SR methods: SR-
CNN [1], FSRCNN [3], VDSR [12], LapSRN [14], Mem-
Net [30], EDSR [20], SRMD [36], NLRN [22], DBPN [6],
RDN [39] and RCAN [38]. As in [20, 39, 38], we also
adopt self-ensemble method to further improve our SAN
denoted as SAN+. All the quantitative results for various
scaling factors are reported in Table 2. Compared with other
methods, our SAN+ performs the best results on all the
datasets on various scaling factors. Without self-ensemble,
SAN and RCAN obtain very similar results and outperform
other methods. This is mainly because both of them adopt
channel attention to learn feature interdependencies, thus
making the network focus on more informative features.

Compared with RCAN, our SAN obtains better results for
datasets (e.g., such as Set5, Set14 and BSD100) with rich
texture information, while obtaining a little worse results
for datasets(e.g., Urban100 and Manga109) with rich re-
peated edge information. It is known that textures are high-
order patterns and have more complex statistic character-
istics, while edges are ﬁrst-order patterns that can be ex-
tracted by ﬁrst-order gradient operators. Thus our SOCA
based on second-order feature statistics works better on im-
ages with more high-order information (e.g., textures).

Visual quality. We also show the zoomed results of var-
ious methods in Fig. 3, from which we can see that most
compared SR models cannot reconstruct the lattices accu-
rately and suffer from serious blurring artifact. In contrast,
our SAN obtains sharper results and recovers more high-
frequency details, such as high contrast and sharp edges.
Take “img 076” for example, most compared methods out-
put heavy blurring artifacts. The early developed bicubic,
SRCNN, FSRCNN and LapSRN even lose the main struc-
ture. More recent methods (e.g., EDSR, DBPN and RDN)
can recover the main outlines but fail to recover more im-
age details. Compared with the ground-truth, RCAN and

11071

Table 2. Quantitative results with BI degradation model.

Method

Bicubic
2
SRCNN 2
FSRCNN 2
VDSR
2
LapSRN 2
2
MemNet
2
EDSR
SRMD
2
2
NLRN
2
DBPN
2
RDN
2
RCAN
2
SAN
SAN+
2

Bicubic
3
SRCNN 3
FSRCNN 3
VDSR
3
LapSRN 3
MemNet
3
3
EDSR
3
SRMD
3
NLRG
3
RDN
3
RCAN
SAN
3
3
SAN+

Bicubic
4
SRCNN 4
FSRCNN 4
VDSR
4
LapSRN 4
4
MemNet
4
EDSR
4
SRMD
4
DBPN
4
NLRG
4
RDN
RCAN
4
4
SAN
SAN+
4

Bicubic
8
SRCNN 8
FSRCNN 8
SCN
8
VDSR
8
LapSRN 8
8
MemNet
8
MSLap
8
EDSR
DBPN
8
8
SAN
SAN+
8

Set5

Set14

PSNR/ SSIM

PSNR/ SSIM

BSD100
PSNR/ SSIM

Urban100 Manga109
PSNR/ SSIM
PSNR/ SSIM

33.66/.9299 30.24/.8688 29.56/.8431 26.88/.8403
36.66/.9542 32.45/.9067 31.36/.8879 29.50/.8946
37.05/.9560 32.66/.9090 31.53/.8920 29.88/.9020
37.53/.9590 33.05/.9130 31.90/.8960 30.77/.9140
37.52/.9591 33.08/.9130 31.08/.8950 30.41/.9101
37.78/.9597 33.28/.9142 32.08/.8978 31.31/.9195
38.11/.9602 33.92/.9195 32.32/.9013 32.93/.9351
37.79/.9601 33.32/.9159 32.05/.8985 31.33/.9204
38.00/.9603 33.46/.9159 32.19/.8992 31.81/.9246 −−/ −−
38.09/.9600 33.85/.9190 32.27/.9000 32.55/.9324
38.24/.9614 34.01/.9212 32.34/.9017 32.89/.9353
38.27/.9614 34.11/.9216 32.41/.9026 33.34/.9384
38.31/.9620 34.07/.9213 32.42/.9028 33.10/.9370
38.35/.9619 34.44/.9244 32.50/.9038 33.73/.9416

30.80/.9339
35.60/.9663
36.67/.9710
37.22/.9750
37.27/.9740
37.72/.9740
39.10/.9773
38.07/.9761

38.89/.9775
39.18/.9780
39.43/.9786
39.32/.9792
39.72/.9797

30.39/.8682 27.55/.7742 27.21/.7385 24.46/.7349
32.75/.9090 29.30/.8215 28.41/.7863 26.24/.7989
33.18/.9140 29.37/.8240 28.53/.7910 26.43/.8080
33.67/.9210 29.78/.8320 28.83/.7990 27.14/.8290
33.82/.9227 29.87/.8320 28.82/.7980 27.07/.8280
34.09/.9248 30.01/.8350 28.96/.8001 27.56/.8376
34.65/.9280 3.52/ .8462 29.25/.8093 28.80/.8653
34.12/.9254 30.04/.8382 28.97/.8025 27.57/.8398
34.27/.9266 30.16/.8374 29.06/.8026 27.93/.8453 −−/ −−
34.71/.9296 30.57/.8468 29.26/.8093 28.80/.8653
34.74/.9299 30.64/.8481 29.32/.8111 29.08/.8702
34.75/.9300 30.59/.8476 29.33/.8112 28.93/.8671
34.89/.9306 30.77/.8498 29.38/.8121 29.29/.8730

26.95/.8556
30.48/.9117
31.10/.9210
32.01/.9340
32.21/.9350
32.51/.9369
34.17/.9476
33.00/.9403

34.13/.9484
34.43/.9498
34.30/.9494
34.74/.9512

28.42/.8104 26.00/.7027 25.96/.6675 23.14/.6577
30.48/.8628 27.50/.7513 26.90/.7101 24.52/.7221
30.72/.8660 27.61/.7550 26.98/.7150 24.62/.7280
31.35/.8830 28.02/.7680 27.29/.0726 25.18/.7540
31.54/.8850 28.19/.7720 27.32/.7270 25.21/.7560
31.74/.8893 28.26/.7723 27.40/.7281 25.50/.7630
32.46/.8968 28.80/.7876 27.71/.7420 26.64/.8033
31.96/.8925 28.35/.7787 27.49/.7337 25.68/.7731
32.47/.8980 28.82/.7860 27.72/.7400 26.38/.7946
31.92/.8916 28.36/.7745 27.48/.7346 25.79/.7729 −−/ −−
32.47/.8990 28.81/.7871 27.72/.7419 26.61/.8028
32.62/.9001 28.86/.7888 27.76/.7435 26.82/.8087
32.64/.9003 28.92/.7888 27.78/.7436 26.79/.8068
32.70/.9013 29.05/.7921 27.86/.7457 27.23/.8169

24.89/.7866
27.58/.8555
27.90/.8610
28.83/.8870
29.09/.8900
29.42/.8942
31.02/.9148
30.09/.9024
30.91/.9137

31.00/.9151
31.21/.9172
31.18/.9169
31.66/.9222

24.40/.6580 23.10/.5660 23.67/.5480 20.74/.5160
25.33/.6900 23.76/.5910 24.13/.5660 21.29/.5440
20.13/.5520 19.75/.4820 24.21/.5680 21.32/.5380
25.59/.7071 24.02/.6028 24.30/.5698 21.52/.5571
25.93/.7240 24.26/.6140 24.49/.5830 21.70/.5710
26.15/.7380 24.35/.6200 24.54/.5860 21.81/.5810
26.16/.7414 24.38/.6199 24.58/.5842 21.89/.5825
26.34/.7558 24.57/.6273 24.65/.5895 22.06/.5963
26.96/.7762 24.91/.6420 24.81/.5985 22.51/.6221
27.21/.7840 25.13/.6480 24.88/.6010 22.73/.6312
27.22/.7829 25.14/.6476 24.88/.6011 22.70/.6314
27.30/.7849 25.23/.6493 24.97/.6031 22.91/.6369

21.47/.6500
22.46/.6950
22.39/.6730
22.68/.6963
23.16/.7250
23.39/.7350
23.56/.7387
23.90/.7564
24.69/.7841
25.14/.7987
24.85/.7906
25.17/.7964

Table 3. Quantitative results with BD degradation model. Best and
second best results are highlighted and underlined

Method

3
Bicubic
SPMSR
3
SRCNN 3
FSRCNN 3
3
VDSR
IRCNN
3
3
SRMD
3
RDN
3
RCAN
3
SAN
SAN+
3

Set5

Set14

PSNR/ SSIM

PSNR/ SSIM

BSD100
PSNR/ SSIM

Urban100 Manga109
PSNR/ SSIM
PSNR/ SSIM

28.78/.8308 26.38/.7271 26.33/.6918 23.52/.6862
32.21/.9001 28.89/.8105 28.13/.7740 25.84/.7856
32.05/.8944 28.80/.8074 28.13/.7736 25.70/.7770
26.23/.8124 24.44/.7106 24.86/.6832 22.04/.6745
33.25/.9150 29.46/.8244 28.57/.7893 26.61/.8136
33.38/.9182 29.63/.8281 28.65/.7922 26.77/.8154
34.01/.9242 30.11/.8364 28.98/.8009 27.50/.8370
34.58/.9280 30.53/.8447 29.23/.8079 28.46/.8582
34.70/.9288 30.63/.8462 29.32/.8093 28.81/.8645
34.75/.9290 30.68/.8466 29.33/.8101 28.83/.8646
34.86/.9297 30.77/.8481 29.39/.8112 29.03/.8674

25.46/.8149
29.64/.9003
29.47/.8924
23.04/.7927
31.06/.9234
31.15/.9245
32.97/.9391
33.97/.9465
34.38/.9483
34.46/.9487
34.76/.9501

Table 4. Computational and parameter comparison (2× Set5).

Para.
PSNR

EDSR MemNet
43M
38.11

677k
37.78

NLRG
330k
38.00

DBPN
10M
38.09

RCAN

RDN
22.3M 16M
38.24
38.27

SAN
15.7M
38.31

compare our method with 8 state-of-the-art SR methods:
SPMSR [24], SRCNN [2], FSRCNN [3], VDSR [12], IR-
CNN [35], SRMD [36], RDN [39], and RCAN [38]. All
the results on 3× are shown in Table 3, from which we
can observe that our SAN achieves consistently better per-
formance than other methods even without self-ensemble.
Speciﬁcally, the PSNR gain of SAN over RDN is up to 0.4
dB on Urban100 and Manga109 datasets.

4.5. Model Size Analyses

The Table 4 shows the performance and model size of
recent deep CNN-based SR methods. Among these meth-
ods, MemNet and NLRG contain much less parameters at
the cost of performance degradation. Instead, our SAN has
lless parameters than RDN and RCAN, but obtains higher
performance, which implies that our SAN can have a good
trade-off between performance and model complexity.

5. Conclusions

SAN obtain more faithful results and recover more image
details, but SAN has sharper results. These observations
verify the superiority of SAN with more powerful represen-
tational ability. Although the recovery of high-frequency
information is difﬁcult due to limited information available
in LR input (scaling factor > 4×), our SAN can still make
full use of the limited LR information through share-source
skip connections, and simultaneously utilize both spatial
and channel feature correlations for more powerful feature
expressions, thus producing more ﬁner results.

4.4. Results with Blur-downscale Degradation (BD)

Following [36, 39], we also compare various SR meth-
ods on image with blur-down degradation (BD) model. We

We propose a deep second-order attention network
(SAN) for accurate image SR. Speciﬁcally, the non-locally
enhanced residual group (NLRG) structure allows SAN to
capture the long-distance dependencies and structural infor-
mation by embedding non-local operations in the network.
Meanwhile, NLRG allows abundant low-frequency infor-
mation from the LR images to be bypassed through share-
source skip connections. In addition to exploiting the spa-
tial feature correlations, we propose second-order channel
attention (SOCA) module to learn feature interdependen-
cies by global covariance pooling for more discriminative
representations. Extensive experiments on SR with BI and
BD degradation models show the effectiveness of our SAN
in terms of quantitative and visual results.

11072

References

[1] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In ECCV, 2014. 7

[2] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. TPAMI, 2016. 1, 2, 3, 7, 8

[3] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-
ating the super-resolution convolutional neural network. In
ECCV. Springer, 2016. 3, 7, 8

[4] Weisheng Dong, Lei Zhang, Guangming Shi, and Xiaolin
Wu.
Image deblurring and super-resolution by adaptive
sparse domain selection and adaptive regularization. TIP,
2011. 1

[5] William T Freeman, Egon C Pasztor, and Owen T
Carmichael. Learning low-level vision. IJCV, 40(1):25–47,
2000. 1

[6] Muhammad Haris, Greg Shakhnarovich, and Norimichi
Ukita. Deep backprojection networks for super-resolution.
In CVPR, 2018. 2, 3, 6, 7

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 1

[8] Nicholas J Higham. Functions of matrices: theory and com-

putation. SIAM, 2008. 5

[9] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. In CVPR, 2018. 2, 4, 5, 6

[10] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR, 2017. 2

[11] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016. 3

[12] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR, 2016. 1, 2, 3, 7, 8

[13] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In CVPR, 2016. 2

[14] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-
Hsuan Yang. Deep laplacian pyramid networks for fast and
accurate superresolution. In CVPR, 2017. 1, 2, 3, 7

[15] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-
Fast and accurate image super-resolution
arXiv preprint

Hsuan Yang.
with deep laplacian pyramid networks.
arXiv:1710.01992, 2017. 3

[16] Christof Koch Laurent Itti and Ernst Niebur. A model
of saliency-based visual attention for rapid scene analysis.
PAMI, 1998. 2

[17] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, 2017. 2

[18] Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao. To-
wards faster training of global covariance pooling networks

by iterative matrix square root normalization.
2018. 5

In CVPR,

[19] Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo.
Is second-order information helpful for large-scale visual
recognition. In ICCV, 2017. 4, 5

[20] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPRW, 2017. 2, 3, 5, 6, 7

[21] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.
Bilinear cnn models for ﬁne-grained visual recognition. In
ICCV, 2015. 4

[22] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and
Thomas S Huang. Non-local recurrent network for image
restoration. In NIPS, 2018. 2, 4, 5, 7

[23] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 6

[24] Tomer Peleg and Michael Elad. A statistical prediction
model based on sparse representations for single image
super-resolution. TIP, 2014. 8

[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 1

[26] Mehdi SM Sajjadi, Bernhard Sch¨olkopf, and Michael
Hirsch. Enhancenet: Single image super-resolution through
automated texture synthesis. In ICCV, 2017. 3

[27] Jorge S´anchez, Florent Perronnin, Thomas Mensink, and
Jakob Verbeek. Image classiﬁcation with the ﬁsher vector:
Theory and practice. IJCV. 4

[28] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016. 3, 5

[29] Ying Tai, Jian Yang, and Xiaoming Liu.

resolution via deep recursive residual network.
2017. 2, 3

Image super-
In CVPR,

[30] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration. In
CVPR, pages 4539–4547, 2017. 2, 3, 7

[31] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-
Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon
Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017 chal-
lenge on single image super-resolution: Methods and results.
In CVPRW, 2017. 6

[32] Shenlong Wang, Lei Zhang, Yan Liang, and Quan Pan.
Semi-coupled dictionary learning with applications to image
super-resolution and photo-sketch synthesis. In CVPR, 2012.
1

[33] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-

ing He. Non-local neural networks. In CVPR, 2018. 2, 3

[34] K. Zhang, X. Gao, D. Tao, and X. Li. Single image super-
resolution with non-local means and steering kernel regres-
sion. TIP, 21(11):4544–4556, 2012. 1, 2

[35] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep cnn denoiser prior for image restoration. In
CVPR, 2017. 8

11073

[36] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a
single convolutional super-resolution network for multiple
degradations. In CVPR, 2018. 1, 2, 6, 7, 8

[37] Lei Zhang and Xiaolin Wu. An edge-guided image interpo-
lation algorithm via directional ﬁltering and data fusion. TIP,
2006. 1, 2

[38] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In ECCV, 2018. 1, 2, 6,
7, 8

[39] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
In CVPR, 2018. 1, 2, 3, 5, 6, 7, 8

11074

