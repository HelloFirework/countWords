Exact Adversarial Attack to Image Captioning

via Structured Output Learning with Latent Variables

Yan Xu‡†∗, Baoyuan Wu†♯∗, Fumin Shen‡, Yanbo Fan†, Yong Zhang†, Heng Tao Shen‡, Wei Liu†♯

†Tencent AI Lab, ‡University of Electronic Science and Technology of China
{xuyan5533,wubaoyuan1987,fumin.shen,fanyanbo0124,zhangyong201303}@gmail.com,

shenhengtao@hotmail.com, wl2223@columbia.edu

Abstract

In this work, we study the robustness of a CNN+RNN
based image captioning system being subjected to adversar-
ial noises. We propose to fool an image captioning system
to generate some targeted partial captions for an image pol-
luted by adversarial noises, even the targeted captions are
totally irrelevant to the image content. A partial caption in-
dicates that the words at some locations in this caption are
observed, while words at other locations are not restricted.
It is the ﬁrst work to study exact adversarial attacks of tar-
geted partial captions. Due to the sequential dependencies
among words in a caption, we formulate the generation of ad-
versarial noises for targeted partial captions as a structured
output learning problem with latent variables. Both the gen-
eralized expectation maximization algorithm and structural
SVMs with latent variables are then adopted to optimize the
problem. The proposed methods generate very successful at-
tacks to three popular CNN+RNN based image captioning
models. Furthermore, the proposed attack methods are used
to understand the inner mechanism of image captioning sys-
tems, providing the guidance to further improve automatic
image captioning systems towards human captioning.

1. Introduction

It has been shown [29] that deep neural networks (DNNs)
[18] are vulnerable to adversarial images, which are visu-
ally similar to benign images. Most of these works focus
on convolutional neural networks (CNNs) [17] based tasks
(e.g., image classiﬁcation [15, 33, 32, 31], object detection
[9], or object tracking [38, 19]), of which the loss functions
are factorized to independent (i.e., unstructured) outputs, so
that the gradient can be easily computed to generate adver-
sarial noises. However, if the output is structured, it may be

∗indicates equal contributions. ♯indicates corresponding authors. This

work was done when Yan Xu was an intern at Tencent AI Lab.

Figure 1. Examples of adversarial attacks to the image captioning
model of Show-Attend-and-Tell [35], using the proposed attack
methods dubbed GEM (the top row) and latent SSVMs (the bottom
row), respectively. In each targeted partial caption (i.e., targeted),
the red ‘_’ indicates one latent word.
In each predicted caption
(i.e., result), the value at the end denotes the norm of adversarial
noises kǫk2. All targeted captions are successfully attacked, while
adversarial noises are invisible to human perception.

diﬃcult to derive the gradient of the corresponding struc-
tured loss. One popular deep model with structured outputs
is the combination of CNNs and recurrent neural networks
(RNNs) [26], where the visual features extracted by CNNs
are fed into RNNs to generate a sequential output. We call
this combination as a CNN+RNN architecture in this pa-
per. One typical task utilizing the CNN+RNN architecture
is image captioning [30], which describes the image content
using a sentence. In this work, we present adversarial attacks
to image captioning, as an early attempt of the robustness of
DNNs with structured outputs.

Given a trained CNN+RNN image captioning model and
an benign image, we want to fool the model to produce a tar-
geted partial caption, which may be totally irrelevant to the
image content, through adding adversarial noises to that im-

4135

(Original) A double decker bus is driving down the street.(Targeted) A bird is flying over a body of water. (Result) A bird is flying over a body of water. (4.2249)(Targeted) A bird is         over a         of water.(Result) A bird is sitting over a truck of water. (5.0674)(Targeted) A market with a variety of fruit and vegetables. (Result) A market with a variety of fruit and vegetables. (5.6272)(b)(c)(a)Benign ImageTargeted Complete CaptionsTargeted Partial Captions(Original) A group of sheep standing on top of a lush green field.(Targeted) A                 a variety of fruit         vegetables. (Result) A group of a variety of fruit and vegetables. (5.2710)age. This task is called exact adversarial attack of targeted
partial captions, which has never been studied in previous
work. As shown in Fig. 1(b), a targeted partial caption in-
dicates that the words at some locations are observed, while
the words at other locations are not speciﬁed, i.e., latent.
When the words at all locations are observed, it becomes a
targeted complete caption (see Fig. 1(c)). To this end, the
marginal posterior probability of the targeted partial caption
should be maximized, while minimizing the norm of adver-
sarial noises. It could be formulated as a structured output
learning problem with latent variables [3, 37]. Speciﬁcally,
we present two formulations. One is maximizing the log
marginal likelihood of the targeted partial caption, which can
be optimized by the generalized expectation maximization
(GEM) algorithm [4]. The other is maximizing the margin
of the log marginal likelihood between the targeted partial
caption and all other possible partial captions at the same
locations, which can be optimized by the structural support
vector machines with latent variables (latent SSVMs) [37].
Note that the proposed formulations are not coupled with
any speciﬁc CNN+RNN architecture. Thus, we evaluate
the proposed methods on three popular image captioning
models, including Show-and-Tell [30], Show-Attend-and-
Tell [35] and self-critical sequence training (SCST) utilizing
reinforcement learning [24]. Experiments on MS-COCO
[20] demonstrate that the proposed methods can generate
successful adversarial attacks. As shown in Fig. 1(b, c),
the targeted captions are successfully attacked, while the
adversarial noises are invisible to human perception.

It should be emphasized that the value of this work is not
just exploring the robustness the image captioning system,
but also understanding its inside mechanism. The analyses
about untargeted captions and the style of targeted captions
could reveal the diﬀerences between automatic captioning
and human captioning, as shown in Section 6. Moreover, the
proposed formulation based on structured output learning
is independent with any speciﬁc task.
It provides a new
perspective for exact attacks to deep neural networks with
structured outputs, which has not been well studied.

The main contributions of this work are four-fold. (1) We
are the ﬁrst to study the adversarial attack of targeted par-
tial captions to image captioning systems. (2) We formulate
this attack problem as structured output learning with latent
variables. (3) Extensive experiments show that state-of-the-
art image captioning models can be easily attacked by the
proposed methods. (4) We utilize the attack method to un-
derstand the inner mechanism of image captioning systems.

2. Related Work

Deep neural networks (DNNs) were ﬁrstly shown in [29]
to be vulnerable to adversarial examples, and many seminal
methods have been developed in this literature. According
to the information about the attacked model accessible to

the attacker, existing works can be generally partitioned into
three categories, including white-box, gray-box, and black-
box attacks. We refer the readers to the survey of adversarial
examples in [1] for more details. In this section, we catego-
rize existing works according to the outputs of the attacked
model, including independent and structured outputs.

Adversarial attacks to DNNs with independent outputs.
Since DNNs (especially CNNs) show very encouraging re-
sults on many visual tasks (e.g., image classiﬁcation [15],
object detection [9], and semantic segmentation [21]), many
previous works have also studied the robustness of these
DNN-based visual tasks. For example, image classiﬁcation
is a typical successful visual application of CNNs, and it is
also widely studied to verify the newly developed adversarial
attack methods, such as box-constrained L-BFGS [29], fast-
gradient-sign method (FSGM) [10], iterative FSGM [16],
momentum iterative FSGM[7], Carlini and Wagner attack
[5], DeepFool [23], etc. These works demonstrate that image
classiﬁcation based on popular CNN models (e.g., ResNet
[11] or Inception-v3 [28]) is very vulnerate to adversarial
examples. The robustness of other typical visual tasks, e.g.,
object detection and semantic segmentation, is also studied
in [8, 22, 34] and [34, 8, 2], respectively. A common trait of
above works is that they focus on CNNs and their loss func-
tions are factorized to independent outputs. Consequently,
the gradients of the loss function with respect to the input
image can be easily computed to generate adversarial noises.

Adversarial attacks to DNNs with structured outputs.
However, the outputs of some deep models are structured.
One typical model is the CNN+RNN architecture, of which
the output is a temporally dependent sequence. It has been
the main-stream model in some visual tasks, such as image
captioning [30] and visual question answering [27]. Due
to the dependencies among words in the sequence, it may
be diﬃcult to compute the gradient of the attack loss func-
tion with respect to the noise. An early attempt to attack
CNN+RNN based tasks was proposed in [36]. However,
it can only implement attacking of targeted complete sen-
tences, and treat structured outputs as single outputs. A
recent attack to the CNN+RNN based image captioning sys-
tem is called Show-and-Fool [6]. It presents two types of
attacks, including targeted captions and targeted keywords.
Its attack of targeted captions is a special case of our stud-
ied attack of targeted partial captions. Its attack of targeted
keywords encourages the predicted sentence to include the
targeted keywords, but their locations cannot be speciﬁed. In
contrast, our attack of targeted partial captions could enforce
the targeted keywords to occur at speciﬁc locations, which is
more restricted. Moreover, the formulations and optimiza-
tion methods of Show-and-Fool are totally diﬀerent with
ours. Its formulations of targeted captions and keywords are
diﬀerent, while the proposed structured output learning with
latent variables provides a systematic formulation for both

4136

attacks of targeted partial and complete captions.

4.1. Maximizing Log Marginal Likelihood via Gen-

Yt=1

3. Structured Outputs of CNN+RNN based Im-

age Captioning Systems

Given a trained CNN+RNN based captioning model with
parameters θ, and an perturbed image I = I0 + ǫ ∈ [0, 1],
the posterior probability of a caption S is formulated as

N

P (S|I0, ǫ; θ) =

P (St|S<t, I0, ǫ; θ),

(1)

where I0 represents the benign image, and ǫ denotes the
adversarial noise. S = {S1, . . . , St, . . . , SN } indicates a
sequence of N variables. St indicates the output variable
of t-step, and its state could be one from the candidate set
V = {1, 2, . . . , |V|}, corresponding to the set of candidate
words, i.e., V. S<t = {S1, . . . , St−1}; when t = 1, we
deﬁne S<t = ∅. Note that we do not specify the formula-
tion of P (St|S<t, I0, ǫ; θ), and it can be speciﬁed as any
CNN+RNN model (e.g., Show-and-Tell [30]). For clarity,
we ignore the notations I0 and θ hereafter.

Besides, a partial caption is denoted as SO, which means
that the variables at the speciﬁc places O are observed, while
other variables are unobserved, i.e., latent. Speciﬁcally, we
deﬁne O ⊂ {1, 2, . . . , N } and SO = {St|t ∈ O}, where
St = st with st ∈ V being the observed state. All observed
states are summarized as an ordered set SO = {st|t ∈
O}. The latent variables are deﬁned as SH = {St|t ∈
H ≡ {1, 2, . . . , N } \ O} = S \ SO. Then, the posterior
probability of the partial caption SO is formulated as:

P (SO|ǫ) =XSH

P (SO, SH|ǫ),

(2)

indicates the summation over all possible con-

wherePSH

ﬁgurations of latent variables SH.

4. Adversarial Attack of Targeted Partial Cap-

tions to Image Captioning

Learning ǫ. The goal of the targeted partial caption at-
tack is to enforce the predicted caption S to be compatible
with SO, meaning that the predicted words at O are exactly
SO. To this end, while minimizing the norm of adver-
sarial noises, either of the following two criterion can be
adopted. (1) The log marginal likelihood ln P (SO = SO|ǫ)
(2) The margin of the
is maximized (see Section 4.1).
log marginal likelihood between the targeted caption (i.e.,
ln P (SO = SO|ǫ)) and all other possible partial captions
(i.e., ln P (ˆSO 6= SO|ǫ)) is maximized. It is formulated as
structural SVMs with latent variables (see Section 4.2).
Inference. Given the optimized ǫ, the caption of the image
perturbed by ǫ is inferred as follows:

eralized EM Algorithm

According to the ﬁrst criterion, the adversarial noise ǫ for
the targeted partial caption is derived by the maximization of
log marginal likelihood, while minimizing kǫk2
2, as follows:

arg max

ǫ

ln P (SO|ǫ) − λkǫk2

2

(4)

≡ arg max

ǫ

lnXSH

P (SO, SH|ǫ) − λkǫk2
2,

subject to the constraint I0 + ǫ ∈ [0, 1]. This constraint
can be easily satisﬁed by clipping. For clarity, we ignore
it hereafter. λ denotes the trade-oﬀ parameter. Due to the
summation over all possible conﬁgurations of SH, the above
problem is diﬃcult. To tackle it, the generalized expecta-
tion maximization (GEM) algorithm [4] is adopted. The
core idea of GEM is introducing the factorized posterior

q(SH) = Qt∈H q(St) to approximate the posterior proba-

bility P (SH|SO, ǫ). Then, we have the following equation,

ln P (SO|ǫ) = L(q, ǫ) + KL(q k P (SH|SO, ǫ)),

(5)

(6)

L(q, ǫ) =XSH

q(SH) ln

P (SO, SH|ǫ)

q(SH)

,

q(SH) ln

q(SH)

P (SH|SO, ǫ)

.

KL(cid:0)q(SH) k P (SH|SO, ǫ)(cid:1) =XSH

(7)
According to the property of the KL divergence that
KL(q(SH) k P (SH|SO, ǫ)) > 0, we obtain that L(q, ǫ) 6
ln P (SO|ǫ). Consequently, the maximization problem (4)
can be optimized through the following two alternative sub-
problems, until convergence.

E step: Given ǫ, q(SH) is updated by minimizing the fol-
lowing equation

N

|V|

q(Sk

q(S<t,H) ln P (Sk

q(SH) ln q(SH) (8)

P (SO, SH|ǫ)(cid:3) =
t )− XS<t,H
t |S<t, ǫ)(cid:3),
q(SH)(cid:2) lnPSH P (SO, SH|ǫ)(cid:3) in

KL(cid:0)q(SH) k P (SH|SO, ǫ)(cid:1) =XSH
−XSH
q(SH)(cid:2) ln P (SO, SH|ǫ) − lnXSH
Xk=1
Xt=1
t )(cid:2) ln q(Sk
where the constantPSH
k, and Pk∈V q(Sk

the last formula is ignored. q(Sk
t ) = q(St = k) in-
dicates the probability of the variable St with the state
t ) = 1. S<t = {S1, . . . , St−1} and
S<t,H = S<t ∩ SH. When t = 1 and t ∈ H, we deﬁne
S<t,H = ∅. Due to the sequential dependency among S, the
probability q(St) can be updated in an ascending order (i.e.,
from 1 to N ). Speciﬁcally, with ﬁxed q(S<t,H), the update
of q(Sk
t ) is derived by setting its gradient to 0, as follows:

S∗

ǫ = arg max

S

P (S|I0 + ǫ).

(3)

1 + ln q(Sk

t ) − XS<t,H

q(S<t,H) ln P (Sk

t |S<t,H, ǫ) = 0

4137

N

+

Xt=1(cid:20) XS1∼t,H
−PSH

q(S<t,H) ln P (Sk

t |S<t,H, ǫ) − 1(cid:19)

⇒ q(Sk

⇒ q(Sk

t ) = exp(cid:18) XS<t,H
t )/(cid:0)P|V |

t ) ← q(Sk

k q(Sk

t )(cid:1)

M step: Given q(SH), ǫ is updated as follows:

(9)

(10)

arg max

L(q, ǫ) − λkǫk2

2

ǫ

= const − λkǫk2

q(SH) ln P (SO, SH|ǫ) = const

2 +XSH
q(S1∼t,H) ln P (St|S<t, ǫ)(cid:21) − λkǫk2

2,

where S1∼t,H = {S1, . . . , St} ∩ SH, and const =
q(SH) ln q(SH). It can be easily optimized by any
gradient based method for training deep neural networks,
such as stochastic gradient descent (SGD) [25] or adaptive
moment estimation (ADAM) [13]. It will be speciﬁed in our
experiments. However, the number of all possible conﬁgu-
rations of S1∼t,H is |V||S1∼t,H|. It could be very large even
for moderate |S1∼t,H|. Fortunately, since q(Sk
t ) ∈ [0, 1]
t ) = 1 for any t ∈ {1, . . . , N }, the values of
q(S1∼t,H) for most conﬁgurations of S1∼t,H are so small
that they can be numerically ignored. Thus, we only con-
sider the conﬁgurations of top-3 probabilities of q(St) for
each latent variable St. Consequently, the number of all
conﬁgurations is reduced to 3|S1∼t,H|, over which the sum-
mation becomes tractable.

andP|V|

k q(Sk

4.2. Structural SVMs with Latent Variables

According to the second criteria, the adversarial noise ǫ
is generated by structural SVMs with latent variables [37],

arg min

ǫ

λkǫk2

2 − max
SH

ln P (SO, SH|ǫ)

(11)

+ max

ˆSO , ˆSH(cid:2) ln P (ˆSO, ˆSH|ǫ) + △(SO, ˆSO)(cid:3),

△(St, ˆSt), △ (St, ˆSt) =(ζ, St 6= ˆSt

0, St = ˆSt,

(12)

△(SO, ˆSO) = Xt∈O

where the scalar ζ > 0 will be speciﬁed in experiments. This
problem can be optimized by the following two alternative
sub-problems, until convergence.

(1) Latent variable completion with ﬁxed ǫ:

S∗

H = arg max
SH

ln P (SO, SH|ǫ).

(13)

It is solved by sequential inference in an ascending order.

(2) Optimizing ǫ via Structural SVMs with ﬁxed S∗

H:

arg min

ǫ

λkǫk2

2 + max

ˆSO , ˆSH(cid:2) ln P (ˆSO, ˆSH|ǫ) + △(SO, ˆSO)(cid:3)

(14)

− ln P (SO, S∗

H|ǫ).

This problem is also optimized by two alternative steps.
(2.1) Loss augmented inference with ﬁxed ǫ:

ˆS∗
O, ˆS∗

H = arg max
ˆSO , ˆSH

ln P (ˆSO, ˆSH|ǫ) + △(SO, ˆSO). (15)

This inference problem is also sequentially solved in an
ascending order. Speciﬁcally, given the inferred conﬁgura-
tions ˆS∗

<t, the inference over ˆSt is solved as follows:

1. When t ∈ O, ˆS∗

△(St, ˆSt)(cid:3).

t = arg max ˆSt(cid:2) ln P (ˆSt|ˆS∗

<t, ǫ) +

2. When t ∈ H, ˆS∗

t = arg max ˆSt

ln P (ˆSt|ˆS∗

<t, ǫ).

(2.2) Update ǫ with ﬁxed ˆS∗

O, ˆS∗
H:

arg min

ǫ

λkǫk2

2 + ln P (ˆS∗

O, ˆS∗

H|ǫ) − ln P (SO, S∗

H|ǫ). (16)

Similar to M step (see Eq. (10)) in GEM, as the gradients of
all three terms in the above objective function with respect to
ǫ can be easily computed, any gradient based optimization
method for training deep neural networks can be used. It
will be speciﬁed in our experiments.

Remarks on both Sections 4.1 and 4.2. Unlike the gen-
eral structured output learning with a repeated inference
process (e.g., MRFs [14]),
the proposed GEM and la-
tent SSVMs are based on CNN+RNN architecture (i.e.,
P (S|I0, ǫ; θ) in Eq.
(1)), which requires only one pass
along the prediction sequence of RNNs. Excluding the
forward and backward through CNNs, the complexities of

GEM and latent SSVMs are O(cid:0)T (|V|N 2 + 3N d)(cid:1) and
O(cid:0)Touter(|V|NH + Tinner(|V|N + 2N d))(cid:1), respectively,

with T being the iteration number, and d being the output
dimension of RNNs.

5. Experiments

5.1. Experimental Setup

In this section, we evaluate the attack performance of the
proposed two methods on three CNN+RNN based image
captioning models, including Show-Attend-and-Tell (SAT)
[35], self-critical sequence training (SCST) [24], and Show-
and-Tell (ST) [30]. We also compare with the only related
method (to the best of our knowledge), called Show-and-
Fool [6], that also attacks the Show-and-Tell model.

Database and targeted captions. Our experiments are con-
ducted on the benchmark database for image captioning, i.e.,
Microsoft COCO 2014 (MSCOCO) [20]. We adopt the split
of MSCOCO in [12], including 113, 287 training, 5, 000 val-
idation and 5, 000 test images. Following the setting of [6],
we randomly select 1, 000 from 5, 000 validation images
as the attacked images. Using each attacked model (i.e.,
SAT, SCST, or ST), we predict the captions of the remaining

4138

4, 000 benign validation images. We randomly choose 5 dif-
ferent targeted complete captions from these 4, 000 captions
for each attacked image. Based on each targeted complete
caption, we also generate 6 targeted partial captions, includ-
ing the partial captions with 1 to 3 latent words (all other
words are observed), and those with 1 to 3 observed words
(all other words are latent), respectively. Latent or observed
words are randomly chosen from each targeted caption. As
the ﬁrst word in most targeted captions is ‘a’, we keep it
as observed, and skip it when choosing latent or observed
words. Due to the memory limit of GPUs, observed words
are randomly chosen from the second to the 7th location in
each targeted caption. The selected 1, 000 images and corre-
sponding 5, 000 targeted complete captions of each attacked
model will be released along with our codes in early future.
Evaluation metrics. Given one targeted caption SO for the
benign image I0, the adversarial noise ǫ is measured by its
ℓ2 norm, i.e., kǫk2; the predicted caption S∗
ǫ (see Eq. (3))
for I0 + ǫ is evaluated by the following three metrics. First,
the success sign is deﬁned as follows:

succ-sign =(1,

0,

if S∗
if S∗

ǫ,O ≡ SO
ǫ,O 6≡ SO

,

(17)

ǫ,O ⊂ S∗
where ≡ exactly compares two sequences, and S∗
denotes the sub-sequence of S∗
ǫ at observed locations O. As
S∗
ǫ may be too short to include all observed locations, we
know that |S∗
ǫ,O| 6 |SO|, with | · | calculating the length of
sequence. However, succ-sign cannot measure how many
inconsistent words in S∗
ǫ with SO. Thus, we also deﬁne the
following two metrics:

ǫ

Precision =

|S∗

ǫ,O ∩ SO|
|S∗

ǫ,O|

, Recall =

|S∗

ǫ,O ∩ SO|

|SO|

,

(18)

where the operator ∩ between two sequences returns a sub-
sequence including the same words at the same locations.
If succ-sign is 1, then both Precision and Recall are 1; if
succ-sign is 0, then Precision and Recall may be larger than
0. Besides, considering that |S∗
ǫ,O| 6 |SO|, we obtain that
Precision > Recall > succ-sign. We report the average
values of above four metrics over all targeted (partial) cap-
tions of all images, i.e., 5000 captions. The average value of
succ-sign is called as success rate (SR). The lower average
norm kǫk2, while the higher average values of other three
metrics, indicate the better attack performance.

Implementation details. The PyTorch implementations
of three target models are downloaded from an open-source
GitHub project1. We train these models based on the training
set of MSCOCO. We adopt the ResNet-101 architecture
[11] as the CNN part in SAT and SCST. Besides, to fairly
compare with the Show-and-Fool algorithm [6], we adopt
the Inception-v3 [28] architecture as the CNN part in the ST

GEM

method metric
kǫk2 ↓
SR ↑
Prec ↑
Rec ↑
kǫk2 ↓
Latent SR ↑
SSVMs Prec ↑
Rec ↑

0 latent
4.2767
0.9926
0.9953
0.9953
5.1678
0.9806
0.9892
0.9889

1 latent
4.4976
0.9154
0.9575
0.9528
5.4558
0.9126
0.955
0.9524

2 latent
4.6942
0.759
0.9092
0.8855
5.7074
0.8466
0.9197
0.9151

3 latent
4.858
0.5604
0.856
0.8
5.8706
0.7526
0.8868
0.8792

1 obser
3.0304
0.8908
0.8908
0.8908
5.2509
0.85
0.85
0.85

2 obser
3.5611
0.862
0.8897
0.8876
5.6838
0.731
0.8092
0.7896

3 obser
3.6583
0.892
0.9236
0.9234
5.8681
0.708
0.8096
0.7917

Table 1. Results of adversarial attack to the Show-Attend-and-
Tell model. ‘1 obser’ indicates the targeted partial caption of one
observed word.
‘Prec’ indicates Precision, while ‘Rec’ means
Recall. ↓ means that the lower value of that metric is the better
attack performance, while ↑ means that the higher value of that
metric is the better attack performance.

model. For the GEM based attack method, the maximum
number of iterations between E and M step is set to 50; for the
latent SSVM based attack method, the maximum numbers
of both outer and inner iterations are set to 10. In the M step
(see Eq. (10)) of GEM, and the (2.2) step (see Eq. (16)) of
latent SSVMs, we adopt the ADAM optimization algorithm
[13] to update the noise ǫ, with the learning rate 0.001, while
all other hyper-parameters are set to the default values in the
master branch of PyTorch2. If without speciﬁc illustrations,
the trade-oﬀ parameters λ in both Eq. (4) and (11) are set to
0.1 in experiments. The scalar ζ of the structured loss (see
Eq. (12)) in latent SSVMs is set to 1.

5.2. Attack Results of Three State-of-the-Art Image

Captioning Models

Attack results of the Show-Attend-and-Tell model [35] are
presented in Table 1. (1) In terms of the attacks of targeted
complete captions (i.e., ‘0 latent’ in the third column of Ta-
ble 1), the SR of GEM is up to 0.9926, while means that only
37 targeted captions out of 5, 000 targeted captions are not
successfully predicted after generating adversarial noises.
And, the corresponding Precision and Recall of GEM are
up to 0.9953.
It means that even in failed attacks, many
words are also successfully predicted. The average noise
norm kǫk2 of GEM is 4.2767. As shown in Fig. 2, such
small noises are invisible to human perception. In contrast,
the results of latent SSVMs are slightly worse than those of
GEM. (2) In terms of the attacks of targeted partial captions
with 1 to 3 latent words, along the increase of the number of
latent words, the results of both GEM and latent SSVMs get
worse, with decreasing (SR, Precision, Recall) and increas-
ing kǫk2. The reason is that more latent words bring in more
uncertainties on predictions of these latent locations. Then,
the observed words after latent locations will be inﬂuenced
by these uncertainties. (3) In terms of the attacks of targeted
partial captions with 1 to 3 observed words, there is not a
clear relationship between the attack performance and the
number of observed words. The reason is that there is a
trade-oﬀ between satisfying observed words and the uncer-

2https://github.com/PyTorch/PyTorch/blob/master/torch/

1https://github.com/ruotianluo/self-critical.PyTorch

optim/adam.py

4139

Figure 2. Some qualitative examples of adversarial attacks to the Show-Attend-and-Tell [35] model, using the proposed GEM method.
Attacks of (a) targeted complete captions; (a) targeted partial captions with two latent words; (a) targeted partial captions with two observed
words. All targeted partial/complete captions are successfully attacked, while the adversarial noises are invisible to human perception.

method metric

GEM

kǫk2 ↓
SR ↑
Prec ↑
Rec ↑
kǫk2 ↓
Latent SR ↑
SSVMs Prec ↑
Rec ↑

0.001
8.6353
0.9956
0.9973
0.9972
9.2682
0.985
0.9919
0.9917

0.01
7.67
0.9952
0.9969
0.9969
8.2134
0.9818
0.99
0.9897

λ

1
1.6862
0.9402
0.9595
0.9589
2.5074
0.9252
0.9588
0.9574

0.1
4.2767
0.9926
0.9953
0.9953
5.1678
0.9806
0.9892
0.9889

10
0.7513
0.4126
0.5832
0.5754
1.023
0.4144
0.6172
0.6092

100
0.2701
0.0118
0.2128
0.2011
0.2939
0.012
0.227
0.2161

Table 2. Attack results of targeted complete captions to the Show-
Attend-and-Tell model, with diﬀerent trade-oﬀ parameters λ (see
Eqs. (4) and (11)).

tainty from the latent words. (4) In comparison of GEM and
latent SSVMs, the average norm kǫk2 of adversarial noises
produced by GEM is always lower than that produced by la-
tent SSVMs at all cases. The attack performance (evaluated
by SR, Precision and Recall) of GEM is also better than that
of latent SSVMs at most cases, excluding two cases of 2 and
3 latent words. However, based on these results, we cannot
simply conclude which method is better for adversarial at-
tacks to image captioning. Because these two methods are
inﬂuenced by the trade-oﬀ parameter λ, and latent SSVMs
is also aﬀected by the parameter ζ deﬁned in Eq. (12).

(4) and (11) are ﬁxed at 0.1.

In the above analysis, the trade-oﬀ parameters λ in both
In the following, we
Eqs.
explore the inﬂuence of λ to the attack performance. When
λ becomes larger, the norm of adversarial noises is expected
to be smaller, while the loss gets larger, leading to weaker
attack performance. This point is fully veriﬁed by the results
in Table 2. When λ = 0.001, the SR value of GEM is up
to 0.9956, and kǫk2 is 8.6353; when λ = 100, the SR value
of GEM is up to 0.0118, and kǫk2 is 0.2701. With the
same λ, GEM performs slightly better than latent SSVMs in
most cases, with lower kǫk2 and higher SR, Precision, and
Recall. However, the performance of latent SSVMs may be
also inﬂuenced by ζ (see Eq. (12)). Due to the space limit,
it will be studied in the supplementary material.

GEM

method metric
kǫk2 ↓
SR ↑
Prec ↑
Rec ↑
kǫk2 ↓
Latent SR ↑
SSVMs Prec ↑
Rec ↑

0 latent
5.1978
0.992
0.9956
0.9956
4.7005
0.9804
0.9926
0.9924

1 latent
5.5643
0.9168
0.9549
0.9528
5.0926
0.916
0.9684
0.967

2 latent
5.8561
0.7438
0.8847
0.872
5.5109
0.8576
0.934
0.9306

3 latent
6.1171
0.5178
0.7788
0.7503
5.8674
0.7598
0.8835
0.8784

1 obser
4.3749
0.6344
0.6344
0.6344
5.989
0.569
0.6538
0.6502

2 obser
4.8465
0.6372
0.7328
0.7319
5.7939
0.7066
0.7835
0.7809

3 obser
4.8419
0.7838
0.8543
0.8543
5.4646
0.8294
0.8815
0.8801

Table 3. Results of adversarial attacks to the SCST model.

3. The phenomenon behind these results is similar with that
behind the results of the Show-Attend-and-Tell model. The
reason is that the model structures of Show-Attend-and-Tell
and SCST are similar that the visual features extracted by
CNNs are fed into RNNs at each step.

Attack results of the Show-and-Tell model [30] are re-
ported in Table 4. It is found that the attack performance of
Show-and-Tell is much worse than that of Show-Attend-and-
Tell (see Table 1) and SCST (see Table 3). The main reason
is that the model structure of Show-and-Tell is signiﬁcantly
diﬀerent with the structures of the other two models. Specif-
ically, the visual features extracted by CNN is only fed into
the starting step of RNNs, while they are fed into RNNs at ev-
ery step in Show-Attend-and-Tell and SCST. Consequently,
the gradients of observed words in targeted partial captions
can be directly back-propagated to the input image in Show-
Attend-and-Tell and SCST. In contrast, the gradients of both
observed words and latent words are ﬁrstly multiplied, then
are back-propagated to the input image. Obviously, the in-
ﬂuence of observed words becomes much weaker. Thus, it is
expected that the observed word closer to the end of one cap-
tion is more diﬃcult to be successfully attacked. To verify
this point, we summarize the success rate of observed words
at each location. As the lengths of targeted captions vary
signiﬁcantly, we only summarize the words at the ﬁrst 7 lo-
cations. As shown in Fig. 3, in both targeted partial captions
with one observed word and targeted complete captions, as
well as using both GEM and latent SSVMs, the SR value
decreases along the increasing of locations.

Attack results of the SCST model [24] are shown in Table

Due to the space limit, we will present: (1) attack results

4140

Benign ImageAdversarial NoiseAdversarial ImageAdversarial NoiseAdversarial ImageAdversarial NoiseAdversarial Image(a)(b)(c)(Original) A red and white airplane flying in the sky.(Original) A group of people sitting at a table eating food.(Result) A woman sitting at a table with a cell phone. (Result) A baseball player is swinging a bat on a field. (Targeted) A baseball player is         a bat on         field. (4.9876)(Targeted) A baseball player is holding a bat on a field. (4.8603)(Result) A baseball player is holding a bat on a field. (Targeted) A woman sitting at a table with a cell phone. (2.6841)(Result) A woman sitting at a table with a cell phone.(Targeted) A woman sitting at         table         a cell phone. (3.6306)(Targeted)                  sitting         a                         . (2.0917)(Result) A man sitting on a chair with a plate of food. (Targeted)                          is holding                         . (3.9415)(Result) A small airplane is holding a red and yellow kite.GEM

method metric
kǫk2 ↓
SR ↑
Prec ↑
Rec ↑
kǫk2 ↓
Latent SR ↑
SSVMs Prec ↑
Rec ↑

0 latent
4.5959
0.4404
0.6758
0.6635
1.7635
0.4924
0.7438
0.7318

1 latent
3.4488
0.5034
0.7475
0.7344
4.5913
0.5808
0.7982
0.7862

2 latent
3.3999
0.4094
0.691
0.6763
4.6584
0.4634
0.7257
0.7122

3 latent
3.3783
0.3408
0.6455
0.626
4.7369
0.3978
0.6697
0.6545

1 obser
2.2588
0.4606
0.4606
0.4606
4.5513
0.287
0.287
0.287

2 obser
2.5779
0.4248
0.5468
0.5468
4.8617
0.2118
0.3609
0.3459

3 obser
2.7472
0.4962
0.6403
0.6403
4.933
0.227
0.4065
0.3898

Table 4. Results of adversarial attacks to the Show-and-Tell model.

Figure 3. Statistics of success rates of observed words at diﬀerent
locations, on attacking the Show-and-Tell model. (Left): the attack
of targeted partial captions with one observed word; (Right): the
attack of targeted complete captions.

of the Show-and-Tell model with the ResNet-101 architec-
ture as the CNN part; (2) results of transfer attacks among
three captioning models; (3) more qualitative results (like
Fig. 2) of attacks of targeted partial captions on above
three image captioning models, in supplementary materi-
als. We also report the average runtime of attacking one
image in the case of targeted complete captions, using the
proposed two methods. On the four attacked models, in-
cluding Show-Attend-and-Tell, SCST, Show-and-Tell with
Inception-v3 and Show-and-Tell with ResNet-101, the re-
spective average runtime (seconds) of the GEM method is
95, 81, 36 and 61, and of latent SSVMs is 28, 25, 15 and 24.
As GEM requires more back-propagation (see the summa-
in Eq. (10)) than latent SSVMs,

tion term PN

t=1PS1∼t,H

its runtime is larger.

5.3. Comparison with Show-and-Fool [6]

In this section, we compare with the only related work
called Show-and-Fool [6].
Its attack of targeted captions
is a special case of our studied attack of targeted partial
captions, i.e., targeted complete captions. The derivations of
two methods proposed in Show-and-Fool also start from the
joint probability of a caption given a pre-trained CNN+RNN
model, which is same with our derivations. However, the
derived objective functions of Show-and-Fool are totally
diﬀerent with our objective functions. Speciﬁcally,

• Maximizing logits in Show-and-Fool (see Eq.

(6)
in [6]) vs. our maximizing log likelihood (see Eq.
(4)). Show-and-Fool directly removes the normaliza-
tion term of the Softmax function of RNNs, as they

thought this term is a constant with respect to the input
adversarial noises. Actually, this normalization term
depends on adversarial noises. Thus, maximizing log-
its and maxiziming log likelihood are diﬀerent.

• Max margin of logits in Show-and-Fool (see Eq. (7)
in [6]) vs. our max margin of log likelihood (see Eq.
(11)). Show-and-Fool maximizes the logit margin be-
tween each observed word and all other possible words
at the same location. When inferring the word from all
other possible words at one location, the corresponding
logit exploits the observed word at its previous location
as the condition, rather than the inferred word of its
previous location. This objective function is standard
SVMs factorized at each location, while our objective
function is structural SVMs of the whole caption.

In the following, we present some experimental com-
parisons between Show-and-Fool and our methods, on the
attack of targeted complete captions. Show-and-Fool is im-
plemented using Tensorﬂow3, and attacks the Show-and-
Tell model [30], of which the CNN part is the Inception-v3
model [28]. To fairly compare with Show-and-Fool, we re-
implement our methods using Tensorﬂow, based on the im-
plementation codes of Show-and-Fool, and attack the same
checkpoint of the Show-and-Tell model. Besides, Show-
and-Fool adopts the arctanh function to transform I0 and
I0 + ǫ to y = arctanh(I0), w = arctanh(I0 + ǫ), to satisfy
the requirement that I0, I0 + ǫ ∈ [−1, 1]. In this section, our
method also adopt this setting. However, when computing
the norm kǫk2 for evaluation, we still transform I0 and ǫ into
the range I0, I0 + ǫ ∈ [0, 1]. The trade-oﬀ parameter λ is
set to 1 for both Show-and-Fool and our methods. The slack
constant ζ (see Eq. (12)) in max margin of Show-and-Fool is
set to 10, 000 (the default value in the provided code), while
1 in our SSVM method. The experiments are also conducted
on the selected 1000 images and 5000 targeted captions (see
Section 5.1). The results are shown in Table 5. Our GEM
method shows the best attack performance. However, due
to the diﬀerences on objective functions and optimization
methods, these results with similar hyper-parameters may
not give a clear conclusion that which method is better. But,
(1) Using logits or log
we still obtain two observations:
probabilities in the loss term can aﬀect the attack perfor-
mance; (2) The comparison between Table 4 and Table 5
tells that diﬀerent checkpoints of the same attacked model
(i.e., Show-and-tell) will inﬂuence the attack performance.
Show-and-Fool [6] also presented the attack of targeted
keywords, requiring that the targeted keywords should occur
in the predicted caption, but their locations cannot be deter-
mined. In contrast, our attack of targeted partial captions
can enforce the targeted words to occur at speciﬁc locations.
Besides, the formulations for attacks of targeted captions and

3 https://github.com/huanzhang12/ImageCaptioningAttack

4141

00.20.40.60.811.202004006008001000120023456success ratenumber of wordsindex of word locationGEMsuccessfailurerate00.20.40.60.811.201000200030004000500060001234567success ratenumber of wordsindex of word locationGEMsuccessfailurerate00.20.40.60.811.201000200030004000500060001234567success ratenumber of wordsindex of word locationLatent SSVMsuccessfailurerate00.20.40.60.811.20100200300400500600700234567success ratenumber of wordsindex of word locationLatent SSVMsuccessfailureratemetric

kǫk2 ↓
SR ↑
Prec ↑
Rec ↑

Show-and-Fool [6]

Our methods

max logits
1.5202
0.5226
0.7239
0.7135

max margin of logits GEM
1.7423
0.6586
0.8009
0.7926

2.3494
0.7134
0.8933
0.886

latent SSVMs
4.7854
0.4996
0.7335
0.7215

Table 5.Comparisons between Show-and-Fool [6] and our methods.

Figure 4. Two examples of attacks diﬀerent styles of targeted cap-
tions using GEM. (1), (2) and (3) represent the results of Show-
And-Tell, Show-Attend-And-Tell and SCST model, respectively.

Figure 5. Examples of untargeted attacks to Show-Attend-and-Tell.

targeted keywords are diﬀerent in Show-and-Fool, while the
proposed structured output learning with latent variables
provides a systematic formulation of both targeted attacks
of complete and partial captions.

6. Extended Discussions

What style of targeted captions can be successfully at-
tacked? As demonstrated in Section 5.1, the targeted cap-
tions are selected from the captions of 4000 benign vali-
dation images. It is found that most of these captions are
active sentences, due to that most training captions are ac-
tive. Can the image captioning system produce other styles
of captions through adversarial attacking? To answer it, we
run a simple test of using passive sentences and attributive
clauses as targeted captions. As shown in Fig. 4 (left),
only the Show-Attend-and-Tell model can produce passive
sentences, while other two models still keep active. Fig. 4
(right) shows that all three models fail to produce attributive
clauses. It demonstrates that current image captioning sys-
tems are not ﬂexible enough to produce diﬀerent styles of
captions like humans.

Untargeted caption attack. Until now, we have only pre-

sented targeted caption attacks. In the following, we present
a brief analysis about the untargeted caption attack, which
can be formulated as follows:

arg min

ln P (S0|ǫ) + λkǫk2

2, s.t. I0 + ǫ ∈ [0, 1],

(19)

ǫ

where S0 denotes the predicted caption on the benign im-
age I0. This problem can be easily solved by the projected
gradient descent algorithm. Two attack results to the Show-
Attend-and-Tell model are shown in Fig. 5. The predicted
captions after attacking are non-meaningful, i.e., violating
the grammar of natural language. It is not diﬃcult to explain
this observation. In image classiﬁcation, the classiﬁcation
space is continuous and closed, and the prediction will jump
from one to another label if the image is attacked. However,
the distributions of meaningful captions are not continuous
in image captioning. There are massive non-meaningful
captions around every meaningful caption. Consequently,
we think it makes no sense to calculate how many captions
are fooled by untargeted attacks. However, this simple anal-
ysis reveals an important information that state-of-the-art
DNN-based image captioning systems have not learned or
understood the grammar of natural language very well.

A brief summary. The above two studies demonstrate that
state-of-the-art CNN+RNN image captioning systems are
still far from human captioning. The proposed methods can
be used as a probe tool to check what grammars have been
learned by the automatic image captioning system, thus to
guide the improvement towards human captioning.

7. Conclusions

In this paper, we have fooled the CNN+RNN based im-
age captioning system to produce targeted partial captions
by generating adversarial noises added onto benign images.
We formulate the attack of targeted partial captions as a
structured output learning problem. We further present two
structured methods, including the generalized expectation
maximization and the structural SVMs with latent variables.
Extensive experiments demonstrate that state-of-the-art im-
age captioning models can be easily attacked by the pro-
posed methods. Furthermore, the proposed methods have
been used to explore the inner mechanism of image caption
systems, revealing that current automatic image captioning
systems are far from human captioning. In our future work,
we plan to use the proposed methods to guide the improve-
ment of automatic image captioning systems towards human
captioning, and enhance the robustness.

Acknowledgement. The involvements of Yan Xu, Fumin
Shen and Heng Tao Shen in this work were supported in
part by the National Natural Science Foundation of China
under Project 61502081, Sichuan Science and Technology
Program (No. 2019YFG0003, 2018GZDZX0032).

4142

Passive Sentence (Targeted) A frisbee is played by a young boy on the beach. (1) A man is playing frisbee with a dog (on the beach). (3.5022) (2) A frisbee is played by a young boy on the beach. (4.6760)   (3) A frisbee is playing with a frisbee on the beach. (6.3204) Attributive Clause   (Targeted) A young boy is playing with a   frisbee which is orange on the beach.    (1) A young boy is playing with a frisbee       (which is orange on the beach). (3.2735)   (2) A young boy is playing with a frisbee     (which is orange) on the beach. (4.7610)   (3) A young boy is playing with a frisbee   (which is orange) on the beach. (5.3805)Passive Sentence (Targeted) A bike is ridden by a man with a dog.  (1) A dog is riding a horse on the beach. (3.6312) (2) A bike is ridden by a man with a dog. (5.6850) (3) A bike is driving down a street with a dog.  (6.8397) Attributive Clause   (Targeted) A toilet that is white in a                 bathroom with a table.    (1) A small bird sitting on a rock in the                   middle of a field. (4.2003)   (2) A toilet (that) is in the middle of a           bathroom (with a table). (5.8707)   (3) A toilet (that) is standing in a         bathroom with a table. (7.0076)  (Original caption) A man is playing tennis on a tennis court. (Untargeted caption) A girl green green green green green green green green green green green green green green. (5.8350)(Original caption) A man riding a wave on top of a surfboard. (Untargeted caption) A woman is holding a dog is playing a blue bird in the water. (7.1830)Benign ImageAdversarial ImageCaptionsReferences

[1] N. Akhtar and A. Mian. Threat of adversarial attacks on
deep learning in computer vision: A survey. IEEE Access,
6:14410–14430, 2018. 2

[2] A. Arnab, O. Miksik, and P. H. S. Torr. On the robustness
of semantic segmentation models to adversarial attacks. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2018. 2

[3] G. BakIr, T. Hofmann, B. Schölkopf, A. J. Smola, B. Taskar,
and S. Vishwanathan. Predicting Structured Data. MIT press,
2007. 2

[4] C. Bishop. Pattern Recognition and Machine Learning.

Springer, 2006. 2, 3

[5] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In Proceedings of the IEEE Symposium
on Security and Privacy, pages 39–57. IEEE, 2017. 2

[6] H. Chen, H. Zhang, P.-Y. Chen, J. Yi, and C.-J. Hsieh. Attack-
ing visual language grounding with adversarial examples: A
case study on neural image captioning. In Proceedings of the
Association for Computational Linguistics, volume 1, pages
2587–2597, 2018. 2, 4, 5, 7, 8

[7] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li.
Boosting adversarial attacks with momentum. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016. 2

[8] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati,
F. Tramer, A. Prakash, T. Kohno, and D. Song. Physical
adversarial examples for object detectors. arXiv preprint
arXiv:1807.07769, 2018. 2

[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and semantic
segmentation.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 580–587,
2014. 1, 2

[10] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In Proceedings of the In-
ternational Conference on Learning Representations, 2015.
2

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 770–778,
2016. 2, 5

[12] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments
for generating image descriptions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3128–3137, 2015. 4

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 4, 5

[14] D. Koller, N. Friedman, and F. Bach. Probabilistic Graphical

Models: Principles and Techniques. MIT press, 2009. 4

[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Proceedings of the Advances in Neural Information Process-
ing Systems, pages 1097–1105, 2012. 1, 2

[16] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533,
2016. 2

[17] Y. LeCun, Y. Bengio, et al. Convolutional networks for im-
ages, speech, and time series. The Handbook of Brain Theory
and Neural Networks, 3361(10):1995, 1995. 1

[18] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

521(7553):436, 2015. 1

[19] X. Li, C. Ma, B. Wu, Z. He, and M.-H. Yang. Target-aware
deep tracking. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2019. 1

[20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollár, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context.
In Proceedings of the European
Conference on Computer Vision, pages 740–755. Springer,
2014. 2, 4

[21] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015. 2

[22] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to worry
about adversarial examples in object detection in autonomous
vehicles. arXiv preprint arXiv:1707.03501, 2017. 2

[23] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: a simple and accurate method to fool deep neural net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2574–2582, 2016. 2

[24] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2017. 2, 4, 6

[25] H. Robbins and S. Monro. A stochastic approximation
In Herbert Robbins Selected Papers, pages 102–

method.
109. Springer, 1985. 4

[26] J. Schmidhuber. Deep learning in neural networks: An

overview. Neural Networks, 61:85–117, 2015. 1

[27] A. Stanislaw, A. Aishwarya, L. Jiasen, M. Margaret,
B. Dhruv, Z. C. Lawrence, and P. Devi. Vqa: Visual ques-
tion answering.
In Proceedings of the IEEE International
Conference on Computer Vision, 2015. 2

[28] C. Szegedy, V. Vanhoucke, S. Ioﬀe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2818–2826, 2016. 2, 5, 7

[29] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199, 2013. 1, 2

[30] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3156–3164, 2015. 1, 2, 3, 4, 6, 7

[31] B. Wu, W. Chen, Y. Fan, Y. Zhang, J. Hou, J. Huang, W. Liu,
and T. Zhang. Tencent ml-images: A large-scale multi-
label image database for visual representation learning. arXiv
preprint arXiv:1901.01703, 2019. 1

[32] B. Wu, W. Chen, P. Sun, W. Liu, B. Ghanem, and S. Lyu.
Tagging like humans: Diverse and distinct image annotation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7967–7975, 2018. 1

4143

[33] B. Wu, F. Jia, W. Liu, and B. Ghanem. Diverse image anno-
tation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2559–2567, 2017. 1

[34] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille.
Adversarial examples for semantic segmentation and object
detection. In Proceedings of the IEEE International Confer-
ence on Computer Vision. IEEE, 2017. 2

[35] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural image
caption generation with visual attention. In Proceedings of
the International Conference on Machine Learning, pages
2048–2057, 2015. 1, 2, 4, 5, 6

[36] X. Xu, X. Chen, C. Liu, A. Rohrbach, T. Darrell, and D. Song.
Fooling vision and language models despite localization and
attention mechanism. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, June 2018. 2
[37] C.-N. J. Yu and T. Joachims. Learning structural svms with
latent variables. In Proceedings of the International Confer-
ence on Machine Learning, pages 1169–1176. ACM, 2009.
2, 4

[38] T. Zhang, B. Ghanem, S. Liu, and N. Ahuja. Robust visual
tracking via multi-task sparse learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2042–2049, 2012. 1

4144

