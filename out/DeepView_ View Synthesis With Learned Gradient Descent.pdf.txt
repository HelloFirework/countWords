DeepView: View synthesis with learned gradient descent

John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe,

Ryan Overbeck, Noah Snavely, Richard Tucker

{jflynn,broxton,debevec,matthewduvall,fyffe,rover,snavely,richardt}@google.com

Google Inc.

Figure 1: The DeepView architecture. (a) The network takes a sparse set of input images shot from different viewpoints. (b, c) The scene
is reconstructed using learned gradient descent, producing a multi-plane image (a series of fronto-parallel, RGBA textured planes). (d)
The multi-plane image is suitable for real-time, high-quality rendering of novel viewpoints. The result above uses four input views in a
30cm × 20cm rectangular layout. The novel view was rendered with a virtual camera positioned at the centroid of the four input views.

Abstract

We present a novel approach to view synthesis using
multiplane images (MPIs). Building on recent advances
in learned gradient descent, our algorithm generates an
MPI from a set of sparse camera viewpoints. The resulting
method incorporates occlusion reasoning, improving perfor-
mance on challenging scene features such as object bound-
aries, lighting reﬂections, thin structures, and scenes with
high depth complexity. We show that our method achieves
high-quality, state-of-the-art results on two datasets: the
Kalantari light ﬁeld dataset, and a new camera array dataset,
Spaces, which we make publicly available.

1. Introduction

Light ﬁelds offer a compelling way to view scenes from
a continuous, dynamic set of viewpoints. The recently intro-

duced multiplane image (MPI) representation approximates
a light ﬁeld as a stack of semi-transparent, colored layers
arranged at various depths [32, 34] allowing real-time syn-
thesis of new views of real scenes. MPIs are a powerful
representation that can model complex appearance effects
such as transparency and alpha matting.

Reconstructing an MPI from a sparse set of input views
is an ill-posed inverse problem, like computed tomography
(CT) or image deblurring, where we want to estimate a
model whose number of parameters is much larger than the
effective number of measurements. Such inverse problems
can be solved using gradient descent based optimization
methods that solve for the parameters that best predict the
measurements via a forward model (such as a renderer in
the case of view synthesis). However, for ill-posed problems
these methods can overﬁt the measurements, necessitating
the use of priors that are difﬁcult to design and often data
dependent. In the case of view synthesis, such overﬁtting
results in synthesized views with noticeable visual artifacts.

12367

Here we present DeepView, a new method for estimat-
ing a multiplane image from sparse views that uses learned
gradient descent (LGD). Our method achieves high quality
view synthesis results even on challenging scenes with thin
objects or reﬂections. Building on recent ideas from the
optimization community [2, 3, 5] this method replaces the
simple gradient descent update rule with a deep network
that generates parameter updates. The use of a learned up-
date rule allows for a learned prior on the model parameters
through manipulation of the gradients—effectively, the up-
date network learns to generate representations that stay on
the manifold of natural scenes. Additionally, the network
learns to take larger, parameter-speciﬁc steps compared to
standard gradient descent, leading to convergence in just a
few iterations.

Our method, illustrated in Figure 1, takes as input a sparse
set of views, e.g., from a camera rig (Figure 1(a)). It then
processes the input images using learned gradient descent
to produce an MPI (Figure 1(b,c)). Internally, this module
uses a convolution neural network to predict an initial MPI
from the input views, then iteratively improves this MPI
via learned updates that take into account the current esti-
mate. We show that for this problem the gradients have a
particularly intuitive form in that they encode the visibility
information between the input views and the MPI layers. By
explicitly modeling visibility, our method shows improved
performance in traditionally difﬁcult areas such as edges and
regions of high depth complexity. The resulting MPI can
then be used for real-time view synthesis (Figure 1(d)).

In addition to the method itself, we also introduce a large,
challenging dataset of light ﬁeld captures called Spaces,
suitable for training and testing view synthesis methods. On
Spaces as well the standard light ﬁeld dataset of Kalantari
et al. [16], we show that our method produces high-quality
view synthesis results that outperform recent approaches.

2. Background and related work

View synthesis and image-based rendering are classic
problems in vision and graphics [8]. Given a set of input
views, one way to pose view synthesis is to reconstruct a light
ﬁeld, a 4D function that directly represents all of the light
rays that pass through a volume of space, from which one
can generate any view within an appropriate region [19, 13].
However, it is rarely practical to measure a densely sampled
light ﬁeld. Instead, scenes are most often recorded using
a limited number of viewpoints that sparsely sample the
light ﬁeld [20]. View synthesis research has focused on
developing priors that aid in recovery of a dense light ﬁeld
from such sparse measurements.

Some prior view synthesis methods place explicit pri-
ors on the light ﬁeld itself. For example, some degree of
smooth view interpolation is made possible by assuming
Lambertian reﬂectance [18], or that the light ﬁeld is sparse
in the Fourier [27] or Shearlet transform domain [30]. How-
ever, these approaches that do not explicitly model scene
geometry can have difﬁculty correctly distinguishing be-
tween a scene’s 3D structure and its reﬂectance properties.

Other approaches explicitly reconstruct 3D geometry, such
as a global 3D reconstruction [15] or a collection of per-
input-view depth maps [37, 6, 20] using multi-view stereo
algorithms. However, such methods based on explicit 3D re-
construction struggle with complex appearance effects such
as specular highlights, transparency, and semi-transparent or
thin objects that are difﬁcult to reconstruct or represent.

In contrast, we use a scene representation called the mul-
tiplane image (MPI) [34] that provides a more ﬂexible scene
representation than depth maps or triangle meshes. Any
method that predicts an MPI from a set of input images
needs to consider the visibility between the predicted MPI
and the input views so that regions of space that are occluded
in some input views are correctly represented. In this regard,
the task is similar to classic work on voxel coloring [26],
which computes colored (opaque) voxels with an occlusion-
aware algorithm. Similarly, Soft3D [21] improves an initial
set of depth maps by reasoning about their relative occlusion.
One approach to generate an MPI would be to iteratively
optimize its parameters with gradient descent, so that when
the MPI is rendered it reproduces the input images. By it-
eratively improving an MPI, such an approach intrinsically
models visibility between the MPI and input images. How-
ever, given a limited set of input views the MPI parameters
are typically underdetermined so simple optimization will
lead to overﬁtting. Hence, special priors or regularization
strategies must be used [32], which are difﬁcult to devise.

Alternatively, recent work has demonstrated the effective-
ness of deep learning for view synthesis. Earlier methods,
such as the so-called DeepStereo method [12] and the work
of Kalantari et al. [16] require running a deep network for ev-
ery desired output view, and so are less suitable for real-time
view synthesis. In contrast, Zhou et al. predict an MPI from
two input images directly via a learned feed-forward net-
work [34]. These methods typically pass the input images to
the network as a plane sweep volume (PSV) [11, 29] which
removes the need to explicitly supply the camera pose, and
also allows the network to more efﬁciently determine corre-
spondences between images. However, such networks have
no intrinsic ability to understand visibility between the input
views and the predicted MPI, instead relying on the network
layers to learn to model such geometric computations. Since
distant parts of the scene may occlude each other the number
of network connections required to effectively model this
visibility can become prohibitively large.

In this work we adopt a hybrid approach combining the
two directions—estimation and learning—described above:
we model MPI generation as an inverse problem to be solved
using a learned gradient descent algorithm [2, 3]. At infer-
ence time, this algorithm iteratively computes gradients of
the current MPI with regard to the input images and pro-
cesses the gradients with a CNN to generate an updated MPI.
This update CNN learns to (1) avoid overﬁtting, (2) take
large steps, thus requiring only a few iterations at inference
time, and (3) reason about occlusions without dense connec-
tions by leveraging visibility computed in earlier iterations.
This optimization can be viewed as a series of gradient de-
scent steps that have been ‘unrolled’ to form a larger network

2368

where each step reﬁnes the MPI by incorporating the vis-
ibility computed in the previous iteration. From another
perspective, our network is a recurrent network, that at each
step is augmented with useful geometric operations (such
as warping and compositing the MPI into different camera
views) that help to propagate visibility information.

3. Method

We represent a three-dimensional scene using the recently
introduced multiplane image representation [34]. An MPI,
M, consists of D planes, each with an associated H × W ×4
RGBA image. The planes are positioned at ﬁxed depths with
respect to a virtual reference camera, equally spaced ac-
cording to inverse depth (disparity), in back-to-front order,
d1, d2, ..., dD, within the view frustum of the reference cam-
era. The reference camera’s pose is set to the centroid of
the input camera poses, and its ﬁeld of view is chosen such
that it covers the region of space visible to the union of these
cameras. We refer to the RGB color channels of the image
for plane d as cd and the corresponding alpha channel as αd.
To render an MPI to an RGB image ˜Ik at a target view k we
ﬁrst warp the MPI images and then over-composite [22] the
warped images from back to front:

˜Ik = O(Wk(M)),

(1)

where the warp operator Wk(M) warps each of the D MPI
images into view k’s image space via a homography that is a
function of the MPI reference camera, the depth of the plane,
and the target view k [34]. The repeated over operator [22]
O has a compact form, assuming premultiplied alpha cd

1:

where Lk(M) = L(Ik, O(Wk(M))) is a loss function mea-
suring the disagreement between predicted and observed
measurements, and Φ is a prior on M. This non-linear opti-
mization can be solved via iterative methods such as gradient
descent, where the update rule (with step size λ) is given by:

Mn+1 = Mn − λ" K
Xk=1

∂Lk(Mn)

∂Mn

+

∂Φ(Mn)

∂Mn #.

(4)

Recent work on learned gradient descent (LGD) [2, 3]
combines classical gradient descent and deep learning by
replacing the standard gradient descent rule with a learned
component:

, Mn(cid:19) ,

Mn+1 = Mn + Nω(cid:18) ∂L1(Mn)

∂Mn

, . . . ,

∂LK(Mn)

∂Mn

(5)
where Nω is a deep network parameterized by a set of
weights ω. (In practice, as described in Section 3.2, we
do not need to explicitly specify L or compute full gradi-
ents.) The network processes the gradients to generate an
update to the model’s parameters. Notice that λ and Φ have
been folded into Nω. This enables Nω to learn a prior on M
as well an adaptive, parameter-speciﬁc step size.

To train the network we unroll the N -iteration network
obtaining the full network, denoted Mω. We compute a
loss by rendering the ﬁnal MPI MN to a held out view
and comparing to the corresponding ground truth image
Igt, under some training loss Lf . The optimization of the
network’s parameters ω is performed over many training
tuples (I1, ..., IK , Igt), across a large variety of scenes, using
stochastic gradient descent:

O(M ) =

cd

D

Xd=1

D

Yi=d+1

(1 − αi),

(2)

argmin

E [Lf (Igt, O(Wgt(Mω(I1, ..., IK))))] .

(6)

ω

where M is an MPI warped to a view. We call the under-
bracketed term the net transmittance at the depth plane d, as
it represents the fraction of the color that will remain after
attenuation through the planes in front of d.

In this paper, we seek to solve the inverse problem asso-
ciated with Eq. 1. That is, we wish to compute an MPI M
that not only reproduces the input views, but also generates
realistic novel views. Since the number of MPI planes is
typically larger than the number of input images, the number
of variables in an MPI will be much larger than the number
of measurements in the input images.

3.1. Learned gradient descent for view synthesis

Inverse problems are often solved by minimization, e.g.:

argmin

M

K

Xk=1

Lk(M) + Φ(M),

(3)

1With premultiplied color the color channels are assumed to have been
multiplied by αfront, so the two-image over operation reduces to cover =
cfront + (1 − αfront)cback.

After training, the resulting network can be applied to new,
unseen scenes.

3.2. View synthesis gradients

LGD requires the gradients of the loss L(˜Ik, Ik) at each
iteration. We show that for our problem, for any loss L, the
gradients have a simple interpretation as a function of a small
set of components that implicitly encode scene visibility. We
can pass these gradient components directly into the update
networks, avoiding deﬁning the loss explicitly.

The gradient of the kth rendered image ˜Ik = O(Wk(M))
is the combination, through the chain rule, of the gradient
of the warping operation Wk and the gradient of the over
operation O. The warp operator’s gradient is well approxi-
mated2 by the inverse warp, W −1
k . The gradients of the over
operation reduce to a particularly simple form:

∂O(M )

∂cd

=

D

Yi=d+1

(1 − αi);

(7)

2They are exactly equivalent if the warp is perfectly anti-aliased.

2369

Figure 2: Our learned gradient descent network. (a) An initialization CNN generates an initial MPI based on the plane sweep volumes
of the input images. A sequence of update CNNs generate updates to the MPI based on computed gradient components (detailed in (b)).
All CNNs share the same core architecture (see Fig. 3) but are trained with different weights. (b) In place of the explicit loss gradient, we
compute per-view gradient components, deﬁned in Section 3.2. (The black circle represents channel-wise concatenation.)

∂O(M )

∂αd

= −" d−1
Xi=1

ci

d−1

Yj=i+1

(1 − αj)#" D
Yi=d+1

(1 − αi)#.

(8)
We refer to the ﬁrst bracketed term as the accumulated over
at the depth slice d. It represents the repeated over of all
depth slices behind the current depth slice. The second brack-
eted term is again the net transmittance. These per slice
expressions can be stacked to form a 3D tensor containing
the gradient w.r.t. all slices. We denote the operator that com-
putes the accumulated over as A and the corresponding net
transmittance operator as T . Deﬁne the accumulated over
computed in view k as Ak = A(Wk(M)) and the corre-
sponding computed net transmittance as Tk = T (Wk(M)).
The gradient of any loss function w.r.t. to M will nec-
essarily be some function of ˜Ik, Ik, the accumulated over
Ak, and the net transmittance Tk. Thus, without explicitly
deﬁning the loss function, we can write its gradient as some
function of these inputs:

∂Lk(M)

∂M

= F(W −1

k ([I↑

k, ˜I↑

k, Ak, Tk])),

(9)

where the tensors between the square brackets are channel-
wise concatenated and ↑ represents the broadcast operator
that repeats a 2D image to generate a 3D tensor. We deﬁne
the gradient components ∇n,k = W −1
k, Ak, Tk])
and note that W −1
k) are the familiar plane
sweep volumes of the respective images.

k) and W −1

k ([I↑

k (˜I↑

k (I↑

k, ˜I↑

In LGD the computed gradients are passed directly into
the network Nω; thus F is redundant—if needed it could be
replicated by Nω. Instead, we pass the gradient components
∇n,k directly into Nω:

Mn+1 = Mn + Nω (∇n,1, . . . , ∇n,K , Mn) .

(10)

Explicitly specifying the per-iteration loss is thus unneces-
sary. (However, we still must deﬁne a ﬁnal training loss Lf ,
as described in Section 3.3.) Instead, it is sufﬁcient to pro-
vide the network with enough information such that it could
compute the needed gradients. This ﬂexibility, as shown by
our ablation experiments in Section 4.2, allows the network
to learn how to best use the rendered and input images dur-
ing the LGD iterations. We note that this is related to the
primal-dual method discussed in [2], where the dual operator
is learned in the measurement space.

Our learned gradient descent network is visualized in
Fig. 2, and may be intuitively explained as follows. We
initialize the MPI by feeding the plane sweep volumes of
the input images through an initialization CNN, similar to
the early layers of other view synthesis networks such as
Flynn et al. [12]. We then iteratively improve the MPI by
computing and feeding the per-view gradient components
into an update CNN. Interestingly, initializing using the
plane sweep volume of the input images is equivalent to
running one update step with the initial MPI slices set to
zero color and α = 0, further motivating the traditional use
of plane sweep volumes in stereo and view synthesis.

Intuitively, since the gradient components contain the net
transmittance and accumulated over they are useful inputs
to the update CNN as they enable propagation of visibil-
ity information between the MPI and the input views. For
example if a pixel in an MPI slice is occluded in a given
view, as indicated by the value of the net transmittance, then
the update network can learn to ignore this input view for
that pixel. Similarly, the accumulated over for a view at a
particular pixel within an MPI slice indicates how well the
view is already explained by what is behind that slice. We
note that by iteratively improving the visibility information
our method shares similarities with the Soft3D method.

2370

Mn

rn,1

rn,2

…

rn,K

1
6

 

3
x
3

1
6

 

3
x
3

1
6

 

3
x
3

1
6

 

3
x
3

2x2

space_to_depth

Convert to 

RGBA

2x2

space_to_depth

2x2

space_to_depth

2x2

space_to_depth

3
2

 

3
x
3

3
2

 

3
x
3

3
2

 

3
x
3

6
4

 

3
x
3

6
4

 

3
x
3

…

6
4

 

3
x
3

6
4

 

3
x
3

6
4

 

3
x
3

6
4

 

3
x
3

2
4

 

3
x
3

2
4

 

3
x
3

2
4

 

3
x
3

9
6

 

1
x
1

9
6

 

1
x
1

9
6

 

1
x
1

i

m
a
x
m
u
m
 k<

9
6

 

1
x
1

9
6

 

1
x
1

9
6

 

1
x
1

6
4

 

1
x
1

6
4

 

1
x
1

…

6
4

 

1
x
1

i

m
a
x
m
u
m

 
 

k<

1
2
8

 

1
x
1

1
2
8

 

1
x
1

1
2
8

 

1
x
1

9
6

 

1
x
1

9
6

 

1
x
1

…

9
6

 

1
x
1

i

m
a
x
m
u
m

 
 

k<

6
4
/
1
6

 

3
x
3

6
4
/
1
6

 

3
x
3

d
e
p
t
h
_
t
o
_
s
p
a
c
e

2
x
2

8

 

3
x
3

∆Mn

Figure 3: The DeepView update CNN. Convolutional layers are labeled with the number of ﬁlters followed by the kernel size. All
convolutions use the Elu [10] activation, except the last which uses no activation. “64/16” indicates the use of 64 features for the initialization
step and ﬁrst iteration and 16 features on all subsequent iterations. Similarly, the downsampling operations with dotted outlines are only
used during initialization and the ﬁrst iteration. The maximumk operation computes the element-wise maximum across the k input tensors.
Black circles represent channel-wise concatenation. The “Convert to RGBA” block applies a sigmoid activation to the ﬁrst four channels of
its input to generate valid RGBA values in the range [0, 1], and then premultiplies the RGB channels by alpha. The initialization CNN has
the same architecture, however there is no Mn input, and instead of the gradient components we input the PSVs of the input images.

3.3. Implementation

We now detail how we implement the described learned

gradient descent scheme.

Per-iteration network architecture. For both the initial-
ization and update networks we adopt a 2D convolutional
neural network (CNN). The input to the CNN is the channel-
wise concatenation of the current MPI and the computed
gradient components (or, for the ﬁrst iteration, the PSV of
the input images). Within a single iteration the same 2D
CNN (with the same parameters) runs on each depth slice,
meaning the CNN is fully convolutional in all three of the
MPI dimensions and allows changing both the resolution
and the number of MPI depth planes after training. This
allows performing inference at a high resolution and with a
different number of depth planes.

We adapt recent ideas on aggregating across multiple
inputs [23, 24, 35] to design our core per-iteration CNN
architecture, shown in Fig. 3. We ﬁrst concatenate each
of the per-view gradient components ∇n,k with the current
MPI’s RGBA values and transform it through several convo-
lutional layers into a feature space. The per-view features are
then processed through several stages that alternate between
cross-view max-pooling and further convolutional layers to
generate either an initial MPI or an MPI update. The only
interaction between the views is thus through the max pool
operation, leading to a network design that is independent
of the order of the input views. Further, this network design
can be trained with any number of input views in any layout.
Intriguingly, this also opens up the possibility of a single
network that operates on a variable number of input views
with variable layout, although we have not yet explored this.
We use the same core CNN architecture for both the up-
date and initialization CNNs, however, following [2] we
use different parameters for each iteration. As in [2] we in-
clude extra channels (4 in our experiments) in addition to the

RGBA channels.3 These extra channels are passed from one
iteration to the next, potentially allowing the model to emu-
late higher-order optimization methods such as LBFGS [36].
Additionally, we found that using a single level U-Net style
network [25] (i.e. down-sampling the CNN input and up-
sampling its output) for the ﬁrst two iterations reduced RAM
and execution time with a negligible effect on performance.

A major challenge for the implementation of our network
was RAM usage. We reduce the RAM required by discard-
ing activations within each of the per-iteration networks as
described in [9], and tiling computation within the MPI vol-
ume. Additionally, during training, we only generate enough
of the MPI to render a 32 × 32 pixel crop in the target image.
More details are included in the supplemental material.

Training data. Each training tuple consists of a set of input
views and a crop within a target view. We provide details
of how we generate training tuples from the Spaces dataset
in the supplemental material. For the dataset of Kalantari et
al. [16], we follow the procedure described in their paper.

Our network design allows us to change the number of
planes and their depths after training. However, the network
may over-ﬁt to the speciﬁc inter-plane spacing used during
training. We mitigated this over-ﬁtting by applying a random
jitter to the depths of the MPI planes during training.

Training loss function. We use feature similarity [7, 33] as
our training loss Lf , speciﬁcally the conv1 2, conv2 2 and
conv3 3 layers of a pre-trained VGG-16 network [28], and
adopt the per layer scaling method discussed in [7].

Training parameters. We implemented our model in Ten-
sorFlow [1] and use the ADAM optimizer [17] with learning
rate 0.00015. The supplemental material and Table 1 further
describe the hyperparameters and the training setup used in
our experiments.

3Note that in the discussion below “MPI” refers to the network’s internal

representation of the MPI, which includes these extra channels.

2371

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
a
9
R
0
K
T
r
G
0
H
+
z
2
g
4
i
W
m
B
H
v
z
s
y
S
k
=
"
>
A
A
A
B
9
X
i
c
b
V
D
L
S
g
M
x
F
L
2
p
r
1
p
f
V
Z
d
u
Q
o
v
g
q
s
y
I
o
M
u
C
G
z
d
C
B
f
u
A
d
i
y
Z
N
N
O
G
Z
j
J
D
k
l
H
K
M
P
/
h
x
o
U
i
b
v
0
X
d
/
6
N
m
X
Y
W
2
n
o
g
c
D
j
n
X
u
7
J
8
W
P
B
t
X
G
c
b
1
R
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
H
R
0
l
i
r
I
2
j
U
S
k
e
j
7
R
T
H
D
J
2
o
Y
b
w
X
q
x
Y
i
T
0
B
e
v
6
0
+
v
c
7
z
4
y
p
X
k
k
7
8
0
s
Z
l
5
I
x
p
I
H
n
B
J
j
p
Y
d
B
S
M
z
E
D
9
L
b
b
J
j
K
b
F
i
t
O
w
1
n
D
r
x
K
3
I
L
U
o
U
B
r
W
P
0
a
j
C
K
a
h
E
w
a
K
o
j
W
f
d
e
J
j
Z
c
S
Z
T
g
V
L
K
s
M
E
s
1
i
Q
q
d
k
z
P
q
W
S
h
I
y
7
a
X
z
1
B
k
+
t
c
o
I
B
5
G
y
T
x
o
8
V
3
9
v
p
C
T
U
e
h
b
6
d
j
J
P
q
Z
e
9
X
P
z
P
6
y
c
m
u
P
J
S
L
u
P
E
M
E
k
X
h
4
J
E
Y
B
P
h
v
A
I
8
4
o
p
R
I
2
a
W
E
K
q
4
z
Y
r
p
h
C
h
C
j
S
2
q
Y
k
t
w
l
7
+
8
S
j
r
n
D
d
f
y
u
4
t
6
s
1
b
U
U
Y
Y
T
q
M
E
Z
u
H
A
J
T
b
i
B
F
r
S
B
g
o
J
n
e
I
U
3
9
I
R
e
0
D
v
6
W
I
y
W
U
L
F
z
D
H
+
A
P
n
8
A
B
8
W
S
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
a
9
R
0
K
T
r
G
0
H
+
z
2
g
4
i
W
m
B
H
v
z
s
y
S
k
=
"
>
A
A
A
B
9
X
i
c
b
V
D
L
S
g
M
x
F
L
2
p
r
1
p
f
V
Z
d
u
Q
o
v
g
q
s
y
I
o
M
u
C
G
z
d
C
B
f
u
A
d
i
y
Z
N
N
O
G
Z
j
J
D
k
l
H
K
M
P
/
h
x
o
U
i
b
v
0
X
d
/
6
N
m
X
Y
W
2
n
o
g
c
D
j
n
X
u
7
J
8
W
P
B
t
X
G
c
b
1
R
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
H
R
0
l
i
r
I
2
j
U
S
k
e
j
7
R
T
H
D
J
2
o
Y
b
w
X
q
x
Y
i
T
0
B
e
v
6
0
+
v
c
7
z
4
y
p
X
k
k
7
8
0
s
Z
l
5
I
x
p
I
H
n
B
J
j
p
Y
d
B
S
M
z
E
D
9
L
b
b
J
j
K
b
F
i
t
O
w
1
n
D
r
x
K
3
I
L
U
o
U
B
r
W
P
0
a
j
C
K
a
h
E
w
a
K
o
j
W
f
d
e
J
j
Z
c
S
Z
T
g
V
L
K
s
M
E
s
1
i
Q
q
d
k
z
P
q
W
S
h
I
y
7
a
X
z
1
B
k
+
t
c
o
I
B
5
G
y
T
x
o
8
V
3
9
v
p
C
T
U
e
h
b
6
d
j
J
P
q
Z
e
9
X
P
z
P
6
y
c
m
u
P
J
S
L
u
P
E
M
E
k
X
h
4
J
E
Y
B
P
h
v
A
I
8
4
o
p
R
I
2
a
W
E
K
q
4
z
Y
r
p
h
C
h
C
j
S
2
q
Y
k
t
w
l
7
+
8
S
j
r
n
D
d
f
y
u
4
t
6
s
1
b
U
U
Y
Y
T
q
M
E
Z
u
H
A
J
T
b
i
B
F
r
S
B
g
o
J
n
e
I
U
3
9
I
R
e
0
D
v
6
W
I
y
W
U
L
F
z
D
H
+
A
P
n
8
A
B
8
W
S
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
a
9
R
0
K
T
r
G
0
H
+
z
2
g
4
i
W
m
B
H
v
z
s
y
S
k
=
"
>
A
A
A
B
9
X
i
c
b
V
D
L
S
g
M
x
F
L
2
p
r
1
p
f
V
Z
d
u
Q
o
v
g
q
s
y
I
o
M
u
C
G
z
d
C
B
f
u
A
d
i
y
Z
N
N
O
G
Z
j
J
D
k
l
H
K
M
P
/
h
x
o
U
i
b
v
0
X
d
/
6
N
m
X
Y
W
2
n
o
g
c
D
j
n
X
u
7
J
8
W
P
B
t
X
G
c
b
1
R
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
H
R
0
l
i
r
I
2
j
U
S
k
e
j
7
R
T
H
D
J
2
o
Y
b
w
X
q
x
Y
i
T
0
B
e
v
6
0
+
v
c
7
z
4
y
p
X
k
k
7
8
0
s
Z
l
5
I
x
p
I
H
n
B
J
j
p
Y
d
B
S
M
z
E
D
9
L
b
b
J
j
K
b
F
i
t
O
w
1
n
D
r
x
K
3
I
L
U
o
U
B
r
W
P
0
a
j
C
K
a
h
E
w
a
K
o
j
W
f
d
e
J
j
Z
c
S
Z
T
g
V
L
K
s
M
E
s
1
i
Q
q
d
k
z
P
q
W
S
h
I
y
7
a
X
z
1
B
k
+
t
c
o
I
B
5
G
y
T
x
o
8
V
3
9
v
p
C
T
U
e
h
b
6
d
j
J
P
q
Z
e
9
X
P
z
P
6
y
c
m
u
P
J
S
L
u
P
E
M
E
k
X
h
4
J
E
Y
B
P
h
v
A
I
8
4
o
p
R
I
2
a
W
E
K
q
4
z
Y
r
p
h
C
h
C
j
S
2
q
Y
k
t
w
l
7
+
8
S
j
r
n
D
d
f
y
u
4
t
6
s
1
b
U
U
Y
Y
T
q
M
E
Z
u
H
A
J
T
b
i
B
F
r
S
B
g
o
J
n
e
I
U
3
9
I
R
e
0
D
v
6
W
I
y
W
U
L
F
z
D
H
+
A
P
n
8
A
B
8
W
S
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
g
a
9
R
0
K
T
r
G
0
H
+
z
2
g
4
i
W
m
B
H
v
z
s
y
S
k
=
"
>
A
A
A
B
9
X
i
c
b
V
D
L
S
g
M
x
F
L
2
p
r
1
p
f
V
Z
d
u
Q
o
v
g
q
s
y
I
o
M
u
C
G
z
d
C
B
f
u
A
d
i
y
Z
N
N
O
G
Z
j
J
D
k
l
H
K
M
P
/
h
x
o
U
i
b
v
0
X
d
/
6
N
m
X
Y
W
2
n
o
g
c
D
j
n
X
u
7
J
8
W
P
B
t
X
G
c
b
1
R
a
W
9
/
Y
3
C
p
v
V
3
Z
2
9
/
Y
P
q
o
d
H
H
R
0
l
i
r
I
2
j
U
S
k
e
j
7
R
T
H
D
J
2
o
Y
b
w
X
q
x
Y
i
T
0
B
e
v
6
0
+
v
c
7
z
4
y
p
X
k
k
7
8
0
s
Z
l
5
I
x
p
I
H
n
B
J
j
p
Y
d
B
S
M
z
E
D
9
L
b
b
J
j
K
b
F
i
t
O
w
1
n
D
r
x
K
3
I
L
U
o
U
B
r
W
P
0
a
j
C
K
a
h
E
w
a
K
o
j
W
f
d
e
J
j
Z
c
S
Z
T
g
V
L
K
s
M
E
s
1
i
Q
q
d
k
z
P
q
W
S
h
I
y
7
a
X
z
1
B
k
+
t
c
o
I
B
5
G
y
T
x
o
8
V
3
9
v
p
C
T
U
e
h
b
6
d
j
J
P
q
Z
e
9
X
P
z
P
6
y
c
m
u
P
J
S
L
u
P
E
M
E
k
X
h
4
J
E
Y
B
P
h
v
A
I
8
4
o
p
R
I
2
a
W
E
K
q
4
z
Y
r
p
h
C
h
C
j
S
2
q
Y
k
t
w
l
7
+
8
S
j
r
n
D
d
f
y
u
4
t
6
s
1
b
U
U
Y
Y
T
q
M
E
Z
u
H
A
J
T
b
i
B
F
r
S
B
g
o
J
n
e
I
U
3
9
I
R
e
0
D
v
6
W
I
y
W
U
L
F
z
D
H
+
A
P
n
8
A
B
8
W
S
v
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
q
S
q
p
x
o
c
O
z
7
s
C
T
u
1
Q
C
x
o
N
u
E
L
N
k
I
M
=
"
>
A
A
A
B
/
3
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
6
2
v
q
u
D
G
z
d
A
i
u
C
q
J
C
L
o
s
6
M
K
N
U
M
E
+
o
A
l
h
M
r
1
p
h
0
4
m
Y
W
Y
i
l
N
i
F
v
+
L
G
h
S
J
u
/
Q
1
3
/
o
2
T
N
g
t
t
P
T
B
w
O
O
d
e
7
p
k
T
J
J
w
p
b
d
v
f
V
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
9
0
V
J
x
K
C
m
0
a
8
1
j
2
A
q
K
A
M
w
F
t
z
T
S
H
X
i
K
B
R
A
G
H
b
j
C
+
y
v
3
u
A
0
j
F
Y
n
G
v
J
w
l
4
E
R
k
K
F
j
J
K
t
J
H
8
6
p
F
7
D
V
w
T
7
E
Z
E
j
4
I
w
u
5
3
6
m
Z
h
W
/
G
r
d
b
t
g
z
4
G
X
i
F
K
S
O
C
r
T
8
6
p
c
7
i
G
k
a
g
d
C
U
E
6
X
6
j
p
1
o
L
y
N
S
M
8
p
h
W
n
F
T
B
Q
m
h
Y
z
K
E
v
q
G
C
R
K
C
8
b
J
Z
/
i
k
+
M
M
s
B
h
L
M
0
T
G
s
/
U
3
x
s
Z
i
Z
S
a
R
I
G
Z
z
G
O
q
R
S
8
X
/
/
P
6
q
Q
4
v
v
Y
y
J
J
N
U
g
6
P
x
Q
m
H
K
s
Y
5
y
X
g
Q
d
M
A
t
V
8
Y
g
i
h
k
p
m
s
m
I
6
I
J
F
S
b
y
v
I
S
n
M
U
v
L
5
P
O
W
c
M
x
/
O
6
8
3
q
w
V
d
Z
T
R
M
a
q
h
U
+
S
g
C
9
R
E
N
6
i
F
2
o
i
i
R
/
S
M
X
t
G
b
9
W
S
9
W
O
/
W
x
3
y
0
Z
B
U
7
h
+
g
P
r
M
8
f
i
/
W
V
r
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
q
S
q
p
x
o
c
O
z
7
s
C
T
u
1
Q
C
x
o
N
u
E
L
N
k
I
M
=
"
>
A
A
A
B
/
3
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
6
2
v
q
u
D
G
z
d
A
i
u
C
q
J
C
L
o
s
6
M
K
N
U
M
E
+
o
A
l
h
M
r
1
p
h
0
4
m
Y
W
Y
i
l
N
i
F
v
+
L
G
h
S
J
u
/
Q
1
3
/
o
2
T
N
g
t
t
P
T
B
w
O
O
d
e
7
p
k
T
J
J
w
p
b
d
v
f
V
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
9
0
V
J
x
K
C
m
0
a
8
1
j
2
A
q
K
A
M
w
F
t
z
T
S
H
X
i
K
B
R
A
G
H
b
j
C
+
y
v
3
u
A
0
j
F
Y
n
G
v
J
w
l
4
E
R
k
K
F
j
J
K
t
J
H
8
6
p
F
7
D
V
w
T
7
E
Z
E
j
4
I
w
u
5
3
6
m
Z
h
W
/
G
r
d
b
t
g
z
4
G
X
i
F
K
S
O
C
r
T
8
6
p
c
7
i
G
k
a
g
d
C
U
E
6
X
6
j
p
1
o
L
y
N
S
M
8
p
h
W
n
F
T
B
Q
m
h
Y
z
K
E
v
q
G
C
R
K
C
8
b
J
Z
/
i
k
+
M
M
s
B
h
L
M
0
T
G
s
/
U
3
x
s
Z
i
Z
S
a
R
I
G
Z
z
G
O
q
R
S
8
X
/
/
P
6
q
Q
4
v
v
Y
y
J
J
N
U
g
6
P
x
Q
m
H
K
s
Y
5
y
X
g
Q
d
M
A
t
V
8
Y
g
i
h
k
p
m
s
m
I
6
I
J
F
S
b
y
v
I
S
n
M
U
v
L
5
P
O
W
c
M
x
/
O
6
8
3
q
w
V
d
Z
T
R
M
a
q
h
U
+
S
g
C
9
R
E
N
6
i
F
2
o
i
i
R
/
S
M
X
t
G
b
9
W
S
9
W
O
/
W
x
3
y
0
Z
B
U
7
h
+
g
P
r
M
8
f
i
/
W
V
r
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
q
S
q
p
x
o
c
O
z
7
s
C
T
u
1
Q
C
x
o
N
u
E
L
N
k
I
M
=
"
>
A
A
A
B
/
3
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
6
2
v
q
u
D
G
z
d
A
i
u
C
q
J
C
L
o
s
6
M
K
N
U
M
E
+
o
A
l
h
M
r
1
p
h
0
4
m
Y
W
Y
i
l
N
i
F
v
+
L
G
h
S
J
u
/
Q
1
3
/
o
2
T
N
g
t
t
P
T
B
w
O
O
d
e
7
p
k
T
J
J
w
p
b
d
v
f
V
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
9
0
V
J
x
K
C
m
0
a
8
1
j
2
A
q
K
A
M
w
F
t
z
T
S
H
X
i
K
B
R
A
G
H
b
j
C
+
y
v
3
u
A
0
j
F
Y
n
G
v
J
w
l
4
E
R
k
K
F
j
J
K
t
J
H
8
6
p
F
7
D
V
w
T
7
E
Z
E
j
4
I
w
u
5
3
6
m
Z
h
W
/
G
r
d
b
t
g
z
4
G
X
i
F
K
S
O
C
r
T
8
6
p
c
7
i
G
k
a
g
d
C
U
E
6
X
6
j
p
1
o
L
y
N
S
M
8
p
h
W
n
F
T
B
Q
m
h
Y
z
K
E
v
q
G
C
R
K
C
8
b
J
Z
/
i
k
+
M
M
s
B
h
L
M
0
T
G
s
/
U
3
x
s
Z
i
Z
S
a
R
I
G
Z
z
G
O
q
R
S
8
X
/
/
P
6
q
Q
4
v
v
Y
y
J
J
N
U
g
6
P
x
Q
m
H
K
s
Y
5
y
X
g
Q
d
M
A
t
V
8
Y
g
i
h
k
p
m
s
m
I
6
I
J
F
S
b
y
v
I
S
n
M
U
v
L
5
P
O
W
c
M
x
/
O
6
8
3
q
w
V
d
Z
T
R
M
a
q
h
U
+
S
g
C
9
R
E
N
6
i
F
2
o
i
i
R
/
S
M
X
t
G
b
9
W
S
9
W
O
/
W
x
3
y
0
Z
B
U
7
h
+
g
P
r
M
8
f
i
/
W
V
r
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
q
S
q
p
x
o
c
O
z
7
s
C
T
u
1
Q
C
x
o
N
u
E
L
N
k
I
M
=
"
>
A
A
A
B
/
3
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
6
2
v
q
u
D
G
z
d
A
i
u
C
q
J
C
L
o
s
6
M
K
N
U
M
E
+
o
A
l
h
M
r
1
p
h
0
4
m
Y
W
Y
i
l
N
i
F
v
+
L
G
h
S
J
u
/
Q
1
3
/
o
2
T
N
g
t
t
P
T
B
w
O
O
d
e
7
p
k
T
J
J
w
p
b
d
v
f
V
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
9
0
V
J
x
K
C
m
0
a
8
1
j
2
A
q
K
A
M
w
F
t
z
T
S
H
X
i
K
B
R
A
G
H
b
j
C
+
y
v
3
u
A
0
j
F
Y
n
G
v
J
w
l
4
E
R
k
K
F
j
J
K
t
J
H
8
6
p
F
7
D
V
w
T
7
E
Z
E
j
4
I
w
u
5
3
6
m
Z
h
W
/
G
r
d
b
t
g
z
4
G
X
i
F
K
S
O
C
r
T
8
6
p
c
7
i
G
k
a
g
d
C
U
E
6
X
6
j
p
1
o
L
y
N
S
M
8
p
h
W
n
F
T
B
Q
m
h
Y
z
K
E
v
q
G
C
R
K
C
8
b
J
Z
/
i
k
+
M
M
s
B
h
L
M
0
T
G
s
/
U
3
x
s
Z
i
Z
S
a
R
I
G
Z
z
G
O
q
R
S
8
X
/
/
P
6
q
Q
4
v
v
Y
y
J
J
N
U
g
6
P
x
Q
m
H
K
s
Y
5
y
X
g
Q
d
M
A
t
V
8
Y
g
i
h
k
p
m
s
m
I
6
I
J
F
S
b
y
v
I
S
n
M
U
v
L
5
P
O
W
c
M
x
/
O
6
8
3
q
w
V
d
Z
T
R
M
a
q
h
U
+
S
g
C
9
R
E
N
6
i
F
2
o
i
i
R
/
S
M
X
t
G
b
9
W
S
9
W
O
/
W
x
3
y
0
Z
B
U
7
h
+
g
P
r
M
8
f
i
/
W
V
r
w
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
H
F
D
k
o
U
z
F
w
P
E
Q
S
X
m
e
G
r
u
B
g
1
v
Q
w
c
=
"
>
A
A
A
B
6
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
K
k
k
I
t
R
j
w
Y
v
H
F
u
w
H
t
K
F
s
t
p
N
2
7
W
Y
T
d
j
d
C
C
f
0
F
X
j
w
o
4
t
W
f
5
M
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
d
e
Y
N
E
c
G
1
c
9
9
s
p
b
G
3
v
7
O
4
V
9
0
s
H
h
0
f
H
J
+
X
T
s
4
6
O
U
8
W
w
z
W
I
R
q
1
5
A
N
Q
o
u
s
W
2
4
E
d
h
L
F
N
I
o
E
N
g
N
p
n
e
L
e
v
c
J
l
e
a
x
f
D
C
z
B
P
2
I
j
i
U
P
O
a
P
G
W
q
3
p
s
F
x
1
a
+
5
S
Z
B
O
8
H
K
q
Q
q
z
k
s
f
w
1
G
M
U
s
j
l
I
Y
J
q
n
X
f
c
x
P
j
Z
1
Q
Z
z
g
T
O
S
4
N
U
Y
0
L
Z
l
I
6
x
b
1
H
S
C
L
W
f
L
R
e
d
k
0
v
r
j
E
g
Y
K
/
u
k
I
U
v
3
9
0
R
G
I
6
1
n
U
W
A
7
I
2
o
m
e
r
2
2
M
P
+
r
9
V
M
T
3
v
o
Z
l
0
l
q
U
L
L
V
R
2
E
q
i
I
n
J
4
m
o
y
4
g
q
Z
E
T
M
L
l
C
l
u
d
y
V
s
Q
h
V
l
x
m
Z
T
s
i
F
4
6
y
d
v
Q
u
e
6
5
l
l
u
3
V
Q
b
l
T
y
O
I
l
x
A
B
a
7
A
g
z
o
0
4
B
6
a
0
A
Y
G
C
M
/
w
C
m
/
O
o
/
P
i
v
D
s
f
q
9
a
C
k
8
+
c
w
x
8
5
n
z
/
I
5
4
z
P
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
y
J
b
e
A
c
v
o
D
d
l
P
+
m
H
O
4
g
F
j
X
P
6
L
3
F
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
E
F
K
I
o
I
e
C
1
4
8
V
r
A
f
0
I
Q
y
2
W
7
a
p
Z
t
N
2
N
0
I
J
f
R
v
e
P
G
g
i
F
f
/
j
D
f
/
j
d
s
2
B
2
1
9
Y
e
H
h
n
R
l
m
9
g
1
T
w
b
V
x
3
W
+
n
t
L
G
5
t
b
1
T
3
q
3
s
7
R
8
c
H
l
W
P
T
z
o
6
y
R
R
l
b
Z
q
I
R
P
V
C
1
E
x
w
y
d
q
G
G
8
F
6
q
W
I
Y
h
4
J
1
w
8
n
d
v
N
5
9
Y
k
r
z
R
D
6
a
a
c
q
C
G
E
e
S
R
5
y
i
s
Z
b
v
S
w
w
F
D
n
J
5
6
c
0
G
1
b
r
b
c
B
c
i
6
+
A
V
U
I
d
C
r
U
H
1
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
a
r
+
J
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
5
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
C
o
2
B
G
/
1
y
+
v
Q
u
W
p
4
l
h
+
u
6
8
1
a
E
U
c
Z
z
q
A
G
F
+
D
B
D
T
T
h
H
l
r
Q
B
g
o
p
P
M
M
r
v
D
m
Z
8
+
K
8
O
x
/
L
1
p
J
T
z
J
z
C
H
z
m
f
P
6
L
q
k
U
4
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
y
J
b
e
A
c
v
o
D
d
l
P
+
m
H
O
4
g
F
j
X
P
6
L
3
F
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
E
F
K
I
o
I
e
C
1
4
8
V
r
A
f
0
I
Q
y
2
W
7
a
p
Z
t
N
2
N
0
I
J
f
R
v
e
P
G
g
i
F
f
/
j
D
f
/
j
d
s
2
B
2
1
9
Y
e
H
h
n
R
l
m
9
g
1
T
w
b
V
x
3
W
+
n
t
L
G
5
t
b
1
T
3
q
3
s
7
R
8
c
H
l
W
P
T
z
o
6
y
R
R
l
b
Z
q
I
R
P
V
C
1
E
x
w
y
d
q
G
G
8
F
6
q
W
I
Y
h
4
J
1
w
8
n
d
v
N
5
9
Y
k
r
z
R
D
6
a
a
c
q
C
G
E
e
S
R
5
y
i
s
Z
b
v
S
w
w
F
D
n
J
5
6
c
0
G
1
b
r
b
c
B
c
i
6
+
A
V
U
I
d
C
r
U
H
1
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
a
r
+
J
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
5
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
C
o
2
B
G
/
1
y
+
v
Q
u
W
p
4
l
h
+
u
6
8
1
a
E
U
c
Z
z
q
A
G
F
+
D
B
D
T
T
h
H
l
r
Q
B
g
o
p
P
M
M
r
v
D
m
Z
8
+
K
8
O
x
/
L
1
p
J
T
z
J
z
C
H
z
m
f
P
6
L
q
k
U
4
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
y
J
b
e
A
c
v
o
D
d
l
P
+
m
H
O
4
g
F
j
X
P
6
L
3
F
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
E
F
K
I
o
I
e
C
1
4
8
V
r
A
f
0
I
Q
y
2
W
7
a
p
Z
t
N
2
N
0
I
J
f
R
v
e
P
G
g
i
F
f
/
j
D
f
/
j
d
s
2
B
2
1
9
Y
e
H
h
n
R
l
m
9
g
1
T
w
b
V
x
3
W
+
n
t
L
G
5
t
b
1
T
3
q
3
s
7
R
8
c
H
l
W
P
T
z
o
6
y
R
R
l
b
Z
q
I
R
P
V
C
1
E
x
w
y
d
q
G
G
8
F
6
q
W
I
Y
h
4
J
1
w
8
n
d
v
N
5
9
Y
k
r
z
R
D
6
a
a
c
q
C
G
E
e
S
R
5
y
i
s
Z
b
v
S
w
w
F
D
n
J
5
6
c
0
G
1
b
r
b
c
B
c
i
6
+
A
V
U
I
d
C
r
U
H
1
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
a
r
+
J
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
5
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
C
o
2
B
G
/
1
y
+
v
Q
u
W
p
4
l
h
+
u
6
8
1
a
E
U
c
Z
z
q
A
G
F
+
D
B
D
T
T
h
H
l
r
Q
B
g
o
p
P
M
M
r
v
D
m
Z
8
+
K
8
O
x
/
L
1
p
J
T
z
J
z
C
H
z
m
f
P
6
L
q
k
U
4
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
y
J
b
e
A
c
v
o
D
d
l
P
+
m
H
O
4
g
F
j
X
P
6
L
3
F
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
0
u
L
4
E
F
K
I
o
I
e
C
1
4
8
V
r
A
f
0
I
Q
y
2
W
7
a
p
Z
t
N
2
N
0
I
J
f
R
v
e
P
G
g
i
F
f
/
j
D
f
/
j
d
s
2
B
2
1
9
Y
e
H
h
n
R
l
m
9
g
1
T
w
b
V
x
3
W
+
n
t
L
G
5
t
b
1
T
3
q
3
s
7
R
8
c
H
l
W
P
T
z
o
6
y
R
R
l
b
Z
q
I
R
P
V
C
1
E
x
w
y
d
q
G
G
8
F
6
q
W
I
Y
h
4
J
1
w
8
n
d
v
N
5
9
Y
k
r
z
R
D
6
a
a
c
q
C
G
E
e
S
R
5
y
i
s
Z
b
v
S
w
w
F
D
n
J
5
6
c
0
G
1
b
r
b
c
B
c
i
6
+
A
V
U
I
d
C
r
U
H
1
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
a
r
+
J
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
5
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
C
o
2
B
G
/
1
y
+
v
Q
u
W
p
4
l
h
+
u
6
8
1
a
E
U
c
Z
z
q
A
G
F
+
D
B
D
T
T
h
H
l
r
Q
B
g
o
p
P
M
M
r
v
D
m
Z
8
+
K
8
O
x
/
L
1
p
J
T
z
J
z
C
H
z
m
f
P
6
L
q
k
U
4
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
/
d
f
3
X
w
o
G
0
Y
+
c
/
o
R
w
9
o
V
S
n
1
k
A
d
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
i
q
D
H
g
h
e
P
F
e
w
H
N
K
F
M
t
p
t
2
6
W
Y
T
d
j
d
C
C
f
0
b
X
j
w
o
4
t
U
/
4
8
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
Z
f
c
N
U
c
G
1
c
9
9
v
Z
2
N
z
a
3
t
k
t
7
Z
X
3
D
w
6
P
j
i
s
n
p
x
2
d
Z
I
q
y
N
k
1
E
o
n
o
h
a
i
a
4
Z
G
3
D
j
W
C
9
V
D
G
M
Q
8
G
6
4
e
R
u
X
u
8
+
M
a
V
5
I
h
/
N
N
G
V
B
j
C
P
J
I
0
7
R
W
M
v
3
J
Y
Y
C
B
7
m
8
a
s
w
G
l
Z
p
b
d
x
c
i
6
+
A
V
U
I
N
C
r
U
H
l
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
Y
r
+
5
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
F
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
M
o
2
B
G
/
1
y
+
v
Q
a
d
Q
9
y
w
/
X
t
W
a
1
i
K
M
E
5
1
C
F
S
/
D
g
B
p
p
w
D
y
1
o
A
4
U
U
n
u
E
V
3
p
z
M
e
X
H
e
n
Y
9
l
6
4
Z
T
z
J
z
B
H
z
m
f
P
6
R
v
k
U
8
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
/
d
f
3
X
w
o
G
0
Y
+
c
/
o
R
w
9
o
V
S
n
1
k
A
d
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
i
q
D
H
g
h
e
P
F
e
w
H
N
K
F
M
t
p
t
2
6
W
Y
T
d
j
d
C
C
f
0
b
X
j
w
o
4
t
U
/
4
8
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
Z
f
c
N
U
c
G
1
c
9
9
v
Z
2
N
z
a
3
t
k
t
7
Z
X
3
D
w
6
P
j
i
s
n
p
x
2
d
Z
I
q
y
N
k
1
E
o
n
o
h
a
i
a
4
Z
G
3
D
j
W
C
9
V
D
G
M
Q
8
G
6
4
e
R
u
X
u
8
+
M
a
V
5
I
h
/
N
N
G
V
B
j
C
P
J
I
0
7
R
W
M
v
3
J
Y
Y
C
B
7
m
8
a
s
w
G
l
Z
p
b
d
x
c
i
6
+
A
V
U
I
N
C
r
U
H
l
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
Y
r
+
5
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
F
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
M
o
2
B
G
/
1
y
+
v
Q
a
d
Q
9
y
w
/
X
t
W
a
1
i
K
M
E
5
1
C
F
S
/
D
g
B
p
p
w
D
y
1
o
A
4
U
U
n
u
E
V
3
p
z
M
e
X
H
e
n
Y
9
l
6
4
Z
T
z
J
z
B
H
z
m
f
P
6
R
v
k
U
8
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
/
d
f
3
X
w
o
G
0
Y
+
c
/
o
R
w
9
o
V
S
n
1
k
A
d
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
i
q
D
H
g
h
e
P
F
e
w
H
N
K
F
M
t
p
t
2
6
W
Y
T
d
j
d
C
C
f
0
b
X
j
w
o
4
t
U
/
4
8
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
Z
f
c
N
U
c
G
1
c
9
9
v
Z
2
N
z
a
3
t
k
t
7
Z
X
3
D
w
6
P
j
i
s
n
p
x
2
d
Z
I
q
y
N
k
1
E
o
n
o
h
a
i
a
4
Z
G
3
D
j
W
C
9
V
D
G
M
Q
8
G
6
4
e
R
u
X
u
8
+
M
a
V
5
I
h
/
N
N
G
V
B
j
C
P
J
I
0
7
R
W
M
v
3
J
Y
Y
C
B
7
m
8
a
s
w
G
l
Z
p
b
d
x
c
i
6
+
A
V
U
I
N
C
r
U
H
l
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
Y
r
+
5
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
F
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
M
o
2
B
G
/
1
y
+
v
Q
a
d
Q
9
y
w
/
X
t
W
a
1
i
K
M
E
5
1
C
F
S
/
D
g
B
p
p
w
D
y
1
o
A
4
U
U
n
u
E
V
3
p
z
M
e
X
H
e
n
Y
9
l
6
4
Z
T
z
J
z
B
H
z
m
f
P
6
R
v
k
U
8
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
n
/
d
f
3
X
w
o
G
0
Y
+
c
/
o
R
w
9
o
V
S
n
1
k
A
d
M
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
i
q
D
H
g
h
e
P
F
e
w
H
N
K
F
M
t
p
t
2
6
W
Y
T
d
j
d
C
C
f
0
b
X
j
w
o
4
t
U
/
4
8
1
/
4
7
b
N
Q
V
t
f
W
H
h
4
Z
4
a
Z
f
c
N
U
c
G
1
c
9
9
v
Z
2
N
z
a
3
t
k
t
7
Z
X
3
D
w
6
P
j
i
s
n
p
x
2
d
Z
I
q
y
N
k
1
E
o
n
o
h
a
i
a
4
Z
G
3
D
j
W
C
9
V
D
G
M
Q
8
G
6
4
e
R
u
X
u
8
+
M
a
V
5
I
h
/
N
N
G
V
B
j
C
P
J
I
0
7
R
W
M
v
3
J
Y
Y
C
B
7
m
8
a
s
w
G
l
Z
p
b
d
x
c
i
6
+
A
V
U
I
N
C
r
U
H
l
y
x
8
m
N
I
u
Z
N
F
S
g
1
n
3
P
T
U
2
Q
o
z
K
c
C
j
Y
r
+
5
l
m
K
d
I
J
j
l
j
f
o
s
S
Y
6
S
B
f
3
D
w
j
F
9
Y
Z
k
i
h
R
9
k
l
D
F
u
7
v
i
R
x
j
r
a
d
x
a
D
t
j
N
G
O
9
W
p
u
b
/
9
X
6
m
Y
l
u
g
5
z
L
N
D
N
M
0
u
W
i
K
B
P
E
J
G
Q
e
A
B
l
y
x
a
g
R
U
w
t
I
F
b
e
3
E
j
p
G
h
d
T
Y
m
M
o
2
B
G
/
1
y
+
v
Q
a
d
Q
9
y
w
/
X
t
W
a
1
i
K
M
E
5
1
C
F
S
/
D
g
B
p
p
w
D
y
1
o
A
4
U
U
n
u
E
V
3
p
z
M
e
X
H
e
n
Y
9
l
6
4
Z
T
z
J
z
B
H
z
m
f
P
6
R
v
k
U
8
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
I
9
N
M
A
R
i
d
t
U
d
T
Q
K
j
F
i
M
O
i
j
9
y
z
K
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
R
N
B
j
w
Y
v
g
p
Y
L
9
g
C
a
U
y
X
b
T
L
t
1
s
w
u
5
G
K
K
F
/
w
4
s
H
R
b
z
6
Z
7
z
5
b
9
y
2
O
W
j
r
C
w
s
P
7
8
w
w
s
2
+
Y
C
q
6
N
6
3
4
7
a
+
s
b
m
1
v
b
p
Z
3
y
7
t
7
+
w
W
H
l
6
L
i
t
k
0
x
R
1
q
K
J
S
F
Q
3
R
M
0
E
l
6
x
l
u
B
G
s
m
y
q
G
c
S
h
Y
J
x
z
f
z
u
q
d
J
6
Y
0
T
+
S
j
m
a
Q
s
i
H
E
o
e
c
Q
p
G
m
v
5
v
s
R
Q
Y
D
+
X
F
/
f
T
f
q
X
m
1
t
2
5
y
C
p
4
B
d
S
g
U
L
N
f
+
f
I
H
C
c
1
i
J
g
0
V
q
H
X
P
c
1
M
T
5
K
g
M
p
4
J
N
y
3
6
m
W
Y
p
0
j
E
P
W
s
y
g
x
Z
j
r
I
5
z
d
P
y
Z
l
1
B
i
R
K
l
H
3
S
k
L
n
7
e
y
L
H
W
O
t
J
H
N
r
O
G
M
1
I
L
9
d
m
5
n
+
1
X
m
a
i
m
y
D
n
M
s
0
M
k
3
S
x
K
M
o
E
M
Q
m
Z
B
U
A
G
X
D
F
q
x
M
Q
C
U
s
X
t
r
Y
S
O
U
C
E
1
N
q
a
y
D
c
F
b
/
v
I
q
t
C
/
r
n
u
W
H
q
1
q
j
W
s
R
R
g
l
O
o
w
j
l
4
c
A
0
N
u
I
M
m
t
I
B
C
C
s
/
w
C
m
9
O
5
r
w
4
7
8
7
H
o
n
X
N
K
W
Z
O
4
I
+
c
z
x
/
K
b
J
F
o
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
I
9
N
M
A
R
i
d
t
U
d
T
Q
K
j
F
i
M
O
i
j
9
y
z
K
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
R
N
B
j
w
Y
v
g
p
Y
L
9
g
C
a
U
y
X
b
T
L
t
1
s
w
u
5
G
K
K
F
/
w
4
s
H
R
b
z
6
Z
7
z
5
b
9
y
2
O
W
j
r
C
w
s
P
7
8
w
w
s
2
+
Y
C
q
6
N
6
3
4
7
a
+
s
b
m
1
v
b
p
Z
3
y
7
t
7
+
w
W
H
l
6
L
i
t
k
0
x
R
1
q
K
J
S
F
Q
3
R
M
0
E
l
6
x
l
u
B
G
s
m
y
q
G
c
S
h
Y
J
x
z
f
z
u
q
d
J
6
Y
0
T
+
S
j
m
a
Q
s
i
H
E
o
e
c
Q
p
G
m
v
5
v
s
R
Q
Y
D
+
X
F
/
f
T
f
q
X
m
1
t
2
5
y
C
p
4
B
d
S
g
U
L
N
f
+
f
I
H
C
c
1
i
J
g
0
V
q
H
X
P
c
1
M
T
5
K
g
M
p
4
J
N
y
3
6
m
W
Y
p
0
j
E
P
W
s
y
g
x
Z
j
r
I
5
z
d
P
y
Z
l
1
B
i
R
K
l
H
3
S
k
L
n
7
e
y
L
H
W
O
t
J
H
N
r
O
G
M
1
I
L
9
d
m
5
n
+
1
X
m
a
i
m
y
D
n
M
s
0
M
k
3
S
x
K
M
o
E
M
Q
m
Z
B
U
A
G
X
D
F
q
x
M
Q
C
U
s
X
t
r
Y
S
O
U
C
E
1
N
q
a
y
D
c
F
b
/
v
I
q
t
C
/
r
n
u
W
H
q
1
q
j
W
s
R
R
g
l
O
o
w
j
l
4
c
A
0
N
u
I
M
m
t
I
B
C
C
s
/
w
C
m
9
O
5
r
w
4
7
8
7
H
o
n
X
N
K
W
Z
O
4
I
+
c
z
x
/
K
b
J
F
o
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
I
9
N
M
A
R
i
d
t
U
d
T
Q
K
j
F
i
M
O
i
j
9
y
z
K
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
R
N
B
j
w
Y
v
g
p
Y
L
9
g
C
a
U
y
X
b
T
L
t
1
s
w
u
5
G
K
K
F
/
w
4
s
H
R
b
z
6
Z
7
z
5
b
9
y
2
O
W
j
r
C
w
s
P
7
8
w
w
s
2
+
Y
C
q
6
N
6
3
4
7
a
+
s
b
m
1
v
b
p
Z
3
y
7
t
7
+
w
W
H
l
6
L
i
t
k
0
x
R
1
q
K
J
S
F
Q
3
R
M
0
E
l
6
x
l
u
B
G
s
m
y
q
G
c
S
h
Y
J
x
z
f
z
u
q
d
J
6
Y
0
T
+
S
j
m
a
Q
s
i
H
E
o
e
c
Q
p
G
m
v
5
v
s
R
Q
Y
D
+
X
F
/
f
T
f
q
X
m
1
t
2
5
y
C
p
4
B
d
S
g
U
L
N
f
+
f
I
H
C
c
1
i
J
g
0
V
q
H
X
P
c
1
M
T
5
K
g
M
p
4
J
N
y
3
6
m
W
Y
p
0
j
E
P
W
s
y
g
x
Z
j
r
I
5
z
d
P
y
Z
l
1
B
i
R
K
l
H
3
S
k
L
n
7
e
y
L
H
W
O
t
J
H
N
r
O
G
M
1
I
L
9
d
m
5
n
+
1
X
m
a
i
m
y
D
n
M
s
0
M
k
3
S
x
K
M
o
E
M
Q
m
Z
B
U
A
G
X
D
F
q
x
M
Q
C
U
s
X
t
r
Y
S
O
U
C
E
1
N
q
a
y
D
c
F
b
/
v
I
q
t
C
/
r
n
u
W
H
q
1
q
j
W
s
R
R
g
l
O
o
w
j
l
4
c
A
0
N
u
I
M
m
t
I
B
C
C
s
/
w
C
m
9
O
5
r
w
4
7
8
7
H
o
n
X
N
K
W
Z
O
4
I
+
c
z
x
/
K
b
J
F
o
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
I
9
N
M
A
R
i
d
t
U
d
T
Q
K
j
F
i
M
O
i
j
9
y
z
K
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
f
t
b
6
V
f
X
o
Z
W
k
R
P
E
h
J
R
N
B
j
w
Y
v
g
p
Y
L
9
g
C
a
U
y
X
b
T
L
t
1
s
w
u
5
G
K
K
F
/
w
4
s
H
R
b
z
6
Z
7
z
5
b
9
y
2
O
W
j
r
C
w
s
P
7
8
w
w
s
2
+
Y
C
q
6
N
6
3
4
7
a
+
s
b
m
1
v
b
p
Z
3
y
7
t
7
+
w
W
H
l
6
L
i
t
k
0
x
R
1
q
K
J
S
F
Q
3
R
M
0
E
l
6
x
l
u
B
G
s
m
y
q
G
c
S
h
Y
J
x
z
f
z
u
q
d
J
6
Y
0
T
+
S
j
m
a
Q
s
i
H
E
o
e
c
Q
p
G
m
v
5
v
s
R
Q
Y
D
+
X
F
/
f
T
f
q
X
m
1
t
2
5
y
C
p
4
B
d
S
g
U
L
N
f
+
f
I
H
C
c
1
i
J
g
0
V
q
H
X
P
c
1
M
T
5
K
g
M
p
4
J
N
y
3
6
m
W
Y
p
0
j
E
P
W
s
y
g
x
Z
j
r
I
5
z
d
P
y
Z
l
1
B
i
R
K
l
H
3
S
k
L
n
7
e
y
L
H
W
O
t
J
H
N
r
O
G
M
1
I
L
9
d
m
5
n
+
1
X
m
a
i
m
y
D
n
M
s
0
M
k
3
S
x
K
M
o
E
M
Q
m
Z
B
U
A
G
X
D
F
q
x
M
Q
C
U
s
X
t
r
Y
S
O
U
C
E
1
N
q
a
y
D
c
F
b
/
v
I
q
t
C
/
r
n
u
W
H
q
1
q
j
W
s
R
R
g
l
O
o
w
j
l
4
c
A
0
N
u
I
M
m
t
I
B
C
C
s
/
w
C
m
9
O
5
r
w
4
7
8
7
H
o
n
X
N
K
W
Z
O
4
I
+
c
z
x
/
K
b
J
F
o
<
/
l
a
t
e
x
i
t
>
4. Evaluation

We evaluate our method on two datasets: the Lytro dataset
of Kalantari et al. [16], and our own Spaces dataset. The
Lytro dataset is commonly used in view synthesis research,
but the baseline of each capture is limited by the small diame-
ter of Lytro Illum’s lens aperture. In contrast, view synthesis
problems that stem from camera array captures are more
challenging due to the larger camera separation and sparsely
sampled viewpoints. We therefore introduce our new dataset,
Spaces, to provide a more challenging shared dataset for
future view synthesis research.

Spaces consists of 100 indoor and outdoor scenes, cap-
tured using a 16-camera rig (see Fig. 4). For each scene,
we captured image sets at 5-10 slightly different rig posi-
tions (within ∼10cm of each other). This jittering of the rig
position provides a ﬂexible dataset for view synthesis, as
we can mix views from different rig positions for the same
scene during training. We calibrated the intrinsics and the
relative pose of the rig cameras using a standard structure
from motion approach [14], using the nominal rig layout as
a prior. We corrected exposure differences using a method
similar to [4]. For our main experiments we undistort the
images and downsampl them to a resolution of 800 × 480.
For our ablation experiments, we used a lower resolution
of 512 × 300 for expedience. We use 90 scenes from the
dataset for training and hold out 10 for evaluation.

4.1. Quantitative Results

We compare our method to Soft3D and to a more direct
deep learning approach based on Zhou et al. [34]. We also
ran ablations to study the importance of different compo-
nents of our model as well as the number of LGD iterations.

Input View

Eval View

3

2

1

0

4

5

6

7

8

12

11

10

9

13

14

15

12-view

small baseline

medium baseline

large baseline

4-view

Figure 4: The Spaces dataset was captured using an array of 16
camera. The horizontal and diagonal distance between cameras is
approximately 10cm. We experimented with different input and
evaluation views, as shown. See Sec. 4 for more details.

Table 1: Experimental conﬁgurations. A check in the U-Net row
indicates that the core CNN ran at a lower resolution for the ﬁrst
two LGD iterations. Other than “Iterations”, all experiments used 3
LGD iterations. For some experiments we ﬁne-tuned with a larger
batch size for the last 20K iterations.

Kalantari

4-View 12-View Ablation Iterations

Input Views
U-Net
# Planes training
# Planes inference
Training iterations
300K
Batch size (ﬁne tune) 32 (64)

4
✗
28
28

4
✓
64
80

12
✓
64
80

100K
20 (40)

100K
20 (40)

4
✓
20
20
60K
32

4
✗
20
20
80K
32

Table 2: SSIM on the Kalantari Lytro data. (Higher is better.)

Scene

Soft3D DeepView

Scene

Soft3D DeepView

Flowers1
Flowers2
Cars

0.9581
0.9616
0.9705

0.9668
0.9700
0.9725

Rock
Leaves
(average)

0.9595
0.9525
0.9604

0.9669
0.9609
0.9677

We provide details on the different experimental setups in
Table 1. In all experiments we measure image quality by
comparing with the ground truth image using SSIM [31],
which ranges between −1 and 1 (higher is better). For the
ablation experiments we include the feature loss used during
training, which is useful for relative comparisons (lower is
better).

Results on the dataset of Kalantari et al. We train a
DeepView model on the data from Kalantari et al. [16] using
their described train-test split and evaluation procedure. As
shown in Table 2, our method improves the average SSIM
score by 18% (0.9674 vs. 0.9604) over the previous state-of-
the-art, Soft3D [21].

Results on Spaces. The Spaces dataset can be evaluated
using different input camera conﬁgurations. We trained three
4-view networks using input views with varying baselines,
and a dense, 12-view network (as shown in Fig. 4). We found
that our method’s performance improves with the number
of input views. When training all networks, target views
were selected from all nearby jittered rig positions, although
we evaluated only on views from the same rig as the input
views, as shown in Fig. 5. For efﬁciency in some of our
experiments we train on a lower number of planes than we
used during inference, as shown in Fig. 5. The near plane
depth for these experiments was set to 0.7m.

For comparison, the authors of Soft3D ran their algorithm
on Spaces. We show these results in Table 3. We also
compare DeepView results to those produced with a variant
of the network described in [34]. We adapt their network to
use 4 wider baseline input views by concatenating the plane
sweep volumes of the 3 secondary views onto the primary
view (the top left of the 4 views), increasing the number of
planes to 40, and increasing the number of internal features
by 50%. Due to the RAM and speed constraints caused by
the dense across-depth-plane connections it was not feasible

2372

062

063

056

Ground Truth

Soft3D

DeepView

Ground Truth

Soft3D

DeepView

Ground Truth

Soft3D

DeepView

(a)

(b)

(c)

Figure 5: DeepView results on the Spaces dataset, 12-view conﬁguration. (Top row) Synthesized views using 12 input views shown
for evaluation camera viewpoint 7 (cf. Fig. 4). The top left index denotes the Spaces scene number. (Middle row) Depth visualizations
produced from DeepView MPIs show the recovery of depth information in ﬁne structures such as wires and foliage. (Bottom row) Each
triplet shows a crop view comparing ground truth (left), Soft3D (center), and DeepView (right).

Table 3: SSIM on the Spaces dataset. [34]+ represents the network
adapted from [34].

Conﬁguration

Soft3D[21]

[34]+

40 Plane

80 Plane

4-view (small baseline)
4-view (medium baseline)
4-view (large baseline)
12-view

0.9260
0.9300
0.9315
0.9402

0.8884
0.8874
0.8673

n/a

0.9541
0.9544
0.9485
0.9630

0.9561
0.9579
0.9544
0.9651

DeepView

to increase these parameters further, or to train a 12-view
version of this model.

In all experiments our method yields signiﬁcantly higher
SSIM scores than both Soft3D and the Zhou et al.-based
model [34]. On Spaces, we improve the average SSIM score
of Soft3D by 39% (0.9584 vs. 0.9319). Additionally, our
method maintains good performance at wider baselines and
shows improvement as the number of input images increases
from 4 to the 12 cameras used in the 12-view experiments.
The model based on [34] did not perform as well as
DeepView, even when both algorithms were conﬁgured with
the same number of MPI planes (see Table 3). This may
because their original model was designed for a narrow-
baseline stereo camera pair and relies on network connec-
tions to propagate visibility throughout the volume. As the
baseline increases, an increasing number of connections are
needed to propagate the visibility and performance suffers,

as shown by the results for the largest baseline conﬁguration.

4.2. Ablation and iteration studies

Gradient components. In this experiment we set one or
more of the gradient components to zero to measure their
importance to the algorithm. We also tested the effect of
passing the gradient of an explicit L2 loss during the LGD
iterations, instead of the gradient components. Results are
shown in Table 4. For experimental details see Table 1.

The best performance (in terms of feature loss) is
achieved when all gradient components are included. When
all components are removed the full network is equivalent
to a residual network that operates on each depth slice in-
dependently, with no interaction across the depth slices. As
expected, in this conﬁguration the model performs poorly.
Between these two extremes the performance decreases, as
shown by both the SSIM and feature loss as well as the exam-
ple images provided in the supplemental material. We note
that our training optimizes the feature loss, and the drop in
performance as measured by this loss, as opposed to SSIM,
is signiﬁcantly larger.
Number of iterations. We also measured the effect of vary-
ing the number of LGD iterations from 1 to 4. A single
iteration corresponds to just the initialization network and
the performance as expected is poor. The results improve
as the number of iterations increases. We note that four it-
erations does show an improvement over three iterations. It

2373

Table 4: Ablation study. Each run represents an experiment in-
cluding the gradient components labeled as rendered image (R),
transmittance (T), and accumulated over (A) with 3 iterations. The
run labeled “∇L2” uses the true gradients of an L2 loss instead
of gradient components. Experiments labeled “N ” use RTA with
the indicated number of iterations. Runs are sorted in order of
descending SSIM; feature loss (Lf ) (lower is better) is also shown.

Run

SSIM Lf

Run

SSIM Lf

N SSIM Lf

1.196
R-A 0.9461
RTA 0.9446
1.179
RT- 0.9434 1.232
1.238
-TA 0.9435
R-- 0.9409
1.243

--A 0.9397
∇L2
0.9389
-T- 0.9320
--- 0.9075

1.271
1.250
1.390
1.765

4
3
2
1

0.9461
0.9445
0.9417
0.8968

1.146
1.202
1.242
2.003

would be interesting to test even more iterations. However,
due to memory constraints, we were unable to test beyond
four iterations, and even four iterations requires too much
memory to be practical at higher resolutions.

4.3. Qualitative Results

We visually compare our method to both ground truth
and Soft3D in Fig. 5 and in the supplemental material.We
notice a general softness in Soft3D results as well as artifacts
around edges. In contrast, DeepView produces convincing
results in areas that are traditionally difﬁcult for view syn-
thesis including edges, reﬂections, and areas of high depth
complexity. This is best seen in the interactive image compar-
ison tool included in the supplemental material, which allows
close examination of the differences between DeepView’s
and Soft3D’s results.

In Fig. 5a our model produces plausible reﬂections on
the table and convincing leaves on the plant where the depth
complexity is high. In Fig. 5b our method reproduces the ﬁne
horizontal railings on the stairs which challenge previous
work. Fig. 5c shows the crisp reconstruction of the complex
foliage within the tree. Interestingly, DeepView can even
render diffuse reﬂections, for example in Fig. 5b (dotted box).
The way this is achieved can be seen in the corresponding
depth map—transparent alpha values in the MPI permit the
viewer to “see through” to a reﬂection that is rendered at a
more distant plane.

The crop in Fig. 5c shows a difﬁcult scene area for both
our method and Soft3D. We note that occluding specular
surfaces are particularly difﬁcult to represent in an MPI. In
this example, the MPI places the table surface on the far
plane in order to mimic its reﬂective surface. However, the
same surface should also occlude the chair legs which are
closer. The end result is that the table surface becomes
partially transparent, and the chair legs are visible through it.
Finally, we include a depth visualization in Fig. 5 pro-
duced by replacing the MPI’s color channels with a false
color while retaining the original α values.4 This visualiza-
tion shows the sharpness of the MPI around edges, even in
complex areas such as tree branches.

4An interactive version of this visualization is included in the supple-

mental material.

5. Discussion

We have shown that a view synthesis model based on
MPIs solved with learned gradient descent produces state-
of-the-art results. Our approach can infer an MPI from a set
of 4 input images in approximately 50 seconds on a P100
GPU. The method is ﬂexible and allows for changing the
resolution and the number and depth of the depth planes
after training. This enables a model trained with medium
distance scene objects to perform well even on scenes where
more depth planes are needed to capture near objects.

Drawbacks and limitations: A drawback of our approach
is the complexity of implementation and the RAM require-
ments and speed of training, which takes several days on
multiple GPUs. The MPIs produced by our model share
the drawback associated with all plane sweep volume based
methods, in that the number of depth planes needs to in-
crease with the maximum disparity. To model larger scenes
it may be advantageous to use multiple MPIs and transition
between them. Finally, our current implementation can only
train models with a ﬁxed number of input views, although
our use of max-pooling to aggregate across views suggests
the possibility of removing this restriction in the future.

Future work: Although our model is not trained to ex-
plicitly produce depth we were surprised by the quality of
the depth visualizations that our model produces, especially
around object edges. However, in smooth scene areas the
visualization appears less accurate, as nothing in our training
objective guides it towards the correct result in these areas.
An interesting direction would be to include a ground truth
depth loss during training.

The MPI is very effective at producing realistic synthe-
sized images and has proven amenable to deep learning.
However, it is over-parameterized for real scenes that consist
of large areas of empty space. Enforcing sparsity on the
MPI, or developing a more parsimonious representation with
similar qualities, is another interesting area for future work.

6. Conclusion

We have presented a new method for inferring a mul-
tiplane image scene representation with learned gradient
descent. We showed that the resulting algorithm has an
intuitive interpretation: the gradient components encode vis-
ibility information that enables the network to reason about
occlusion. The resulting method exhibits state-of-the-art
performance on a difﬁcult, real-world dataset. Our approach
shows the promise of learned gradient descent for solving
complex, non-linear inverse problems.

Acknowledgments We would like to thank Jay Busch and
Matt Whalen for designing and building our camera rig, Eric Pen-
ner for help with Soft3D comparisons and Oscar Beijbom for some
swell ideas.

2374

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Van-
houcke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden,
M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5

[2] J. Adler and O. ¨Oktem. Learned primal-dual reconstruction.
IEEE Transactions on Medical Imaging, 37:1322–1332, 2018.
2, 3, 4, 5

[3] J. Adler and O. ktem. Solving ill-posed inverse problems
Inverse Problems,

using iterative deep neural networks.
33(12):124007, 2017. 2, 3

[4] R. Anderson, D. Gallup, J. T. Barron, J. Kontkanen,
N. Snavely, C. H. Esteban, S. Agarwal, and S. M. Seitz. Jump:
Virtual reality video. 2016. 6

[5] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoff-
man, D. Pfau, T. Schaul, and N. de Freitas. Learning to learn
by gradient descent by gradient descent. In NIPS, 2016. 2

[6] G. Chaurasia, S. Duchˆene, O. Sorkine-Hornung, and G. Dret-
takis. Depth synthesis and local warps for plausible image-
based navigation. Trans. on Graphics, 32:30:1–30:12, 2013.
2

[7] Q. Chen and V. Koltun. Photographic image synthesis with
cascaded reﬁnement networks. CoRR, abs/1707.09405, 2017.
5

[8] S. E. Chen and L. Williams. View interpolation for image syn-
thesis. In Proceedings of SIGGRAPH 93, Annual Conference
Series, 1993. 2

[9] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep
nets with sublinear memory cost. CoRR, abs/1604.06174,
2016. 5

[10] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accu-
rate deep network learning by exponential linear units (elus).
CoRR, abs/1511.07289, 2015. 5

[11] R. T. Collins. A space-sweep approach to true multi-image

matching. In CVPR, 1996. 2

[12] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. DeepStereo:
Learning to predict new views from the world’s imagery. In
CVPR, 2016. 2, 4

[13] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen.
The lumigraph. In Proceedings of SIGGRAPH 96, Annual
Conference Series, 1996. 2

[14] R. Hartley and A. Zisserman. Multiple View Geometry in
Computer Vision. Cambridge University Press, New York,
NY, USA, 2 edition, 2003. 6

[15] P. Hedman, S. Alsisan, R. Szeliski, and J. Kopf. Casual 3d
photography. ACM Trans. Graph., 36(6):234:1–234:15, 2017.
2

[16] N. K. Kalantari, T.-C. Wang, and R. Ramamoorthi. Learning-
based view synthesis for light ﬁeld cameras. ACM Trans.
Graph., 35(6):193:1–193:10, 2016. 2, 5, 6

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 5

[18] A. Levin and F. Durand. Linear view synthesis using a dimen-

sionality gap light ﬁeld prior. In CVPR, 2010. 2

[19] M. Levoy and P. Hanrahan. Light ﬁeld rendering. In Pro-
ceedings of SIGGRAPH 96, Annual Conference Series, 1996.
2

[20] R. S. Overbeck, D. Erickson, D. Evangelakos, M. Pharr, and
P. E. Debevec. A system for acquiring, processing, and ren-
dering panoramic light ﬁeld stills for virtual reality. CoRR,
abs/1810.08860, 2018. 2

[21] E. Penner and L. Zhang. Soft 3D reconstruction for view
synthesis. ACM Trans. Graph., 36(6):235:1–235:11, 2017. 2,
6, 7

[22] T. Porter and T. Duff. Compositing digital images. SIG-

GRAPH Comput. Graph., 18(3):253–259, 1984. 3

[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
In CVPR, 2017. 5

[24] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, 2017. 5

[25] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional
networks for biomedical image segmentation. In Medical
Image Computing and Computer-Assisted Intervention (MIC-
CAI), volume 9351 of LNCS, pages 234–241. Springer, 2015.
(available on arXiv:1505.04597 [cs.CV]). 5

[26] S. M. Seitz and C. R. Dyer. Photorealistic scene reconstruc-

tion by voxel coloring. IJCV, 35:151–173, 1997. 2

[27] L. Shi, H. Hassanieh, A. Davis, D. Katabi, and F. Durand.
Light ﬁeld reconstruction using sparsity in the continuous
fourier domain. Trans. on Graphics, 34(1):12:1–12:13, Dec.
2014. 2

[28] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 5

[29] R. Szeliski and P. Golland. Stereo matching with transparency

and matting. IJCV, 32(1), 1999. 2

[30] S. Vagharshakyan, R. Bregovic, and A. Gotchev. Light ﬁeld
reconstruction using shearlet transform. IEEE Trans. PAMI,
40(1):133–147, Jan. 2018. 2

[31] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: From error visibility to structural
similarity. Trans. Img. Proc., 13(4):600–612, Apr. 2004. 6

[32] G. Wetzstein, D. Lanman, W. Heidrich, and R. Raskar. Lay-
ered 3D: Tomographic image synthesis for attenuation-based
light ﬁeld and high dynamic range displays. ACM Trans.
Graph., 30(4):95:1–95:12, 2011. 1, 2

[33] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.
The unreasonable effectiveness of deep features as a percep-
tual metric. CoRR, abs/1801.03924, 2018. 5

[34] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely. Stereo
magniﬁcation: Learning view synthesis using multiplane im-
ages. ACM Trans. Graph., 37(4):65:1–65:12, 2018. 1, 2, 3, 6,
7

[35] Y. Zhou and O. Tuzel. VoxelNet: End-to-end learning for

point cloud based 3d object detection. CVPR, 2017. 5

2375

[36] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. L-bfgs-b - fortran
subroutines for large-scale bound constrained optimization.
Technical report, ACM Trans. Math. Software, 1994. 5

[37] C. L. Zitnick, S. B. Kang, M. Uyttendaele, S. Winder, and
R. Szeliski. High-quality video view interpolation using a
layered representation. ACM Trans. Graph., 23(3):600–608,
2004. 2

2376

