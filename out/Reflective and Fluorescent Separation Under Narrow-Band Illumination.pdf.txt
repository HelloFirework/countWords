Reﬂective and Fluorescent Separation under Narrow-Band Illumination

Koji Koyamatsu

Daichi Hidaka

Takahiro Okabe

Kyushu Institute of Technology

Hendrik P. A. Lensch
University of T¨ubingen

okabe@ai.kyutech.ac.jp

hendrik.lensch@uni-tuebingen.de

Abstract

In this paper, we address the separation of reﬂective
and ﬂuorescent components in RGB images taken under
narrow-band light sources such as LEDs. First, we show
that the ﬂuorescent color per pixel can be estimated from
at least two images under different light source colors, be-
cause the observed color at a surface point is represented
by a convex combination of the light source color and the
illumination-invariant ﬂuorescent color. Second, we pro-
pose a method for robustly estimating the ﬂuorescent color
via MAP estimation by taking the prior knowledge with re-
spect to ﬂuorescent colors into consideration. We conducted
a number of experiments by using both synthetic and real
images, and conﬁrmed that our proposed method works bet-
ter than the closely related state-of-the-art method and en-
ables us to separate reﬂective and ﬂuorescent components
even from a single image. Furthermore, we demonstrate
that our method is effective for applications such as image-
based material editing and relighting.

1. Introduction

Fluorescence is a very common phenomenon observed
both in natural objects such as minerals and plants and in
man-made objects such as papers and clothes [2]. Fluo-
rescent materials emit light with longer wavelengths than
those of the absorbed light, in contrast to reﬂective materials
which reﬂect light with the same wavelengths as those of the
incident light. Separating reﬂective and ﬂuorescent compo-
nents in images is important for preprocessing of various
computer vision techniques that assume reﬂective compo-
nents such as diffuse and specular ones.

Recently, due to the progress of light-emitting diodes
(LEDs), narrow-band light sources are often used for illu-
minating an object of interest and capturing its images in
the ﬁelds of computer vision and computer graphics [21, 1,
8, 17, 15, 16]. The application domain of images taken un-
der narrow-band light sources includes image-based spec-
tral relighting [21, 16], raw material classiﬁcation [8], bidi-
rectional texture function (BTF) classiﬁcation [17], and sur-

face normal and reﬂectance recovery [15]. Therefore, sepa-
rating reﬂective and ﬂuorescent components under narrow-
band illumination is useful for preprocessing of such appli-
cations.

In this paper, we address the separation of reﬂective
and ﬂuorescent components in RGB images taken under
narrow-band light sources such as LEDs. First, we show
that the ﬂuorescent color is on the plane spanned by the light
source color and the observed pixel color in the RGB color
space, and then the ﬂuorescent color per pixel can be esti-
mated from at least two images under different light source
colors as the intersection of the two planes. This is because
the pixel color is represented by a convex combination of
the light source color and the illumination-invariant ﬂuo-
rescent color. Second, we take the prior knowledge with
respect to ﬂuorescent colors, i.e. the distribution of ﬂuores-
cent colors in the r-g chromaticity space into consideration,
and propose a method for robustly estimating the ﬂuores-
cent color via maximum a posteriori (MAP) estimation.

Through a number of experiments by using both syn-
thetic and real images, we conﬁrm that our proposed ap-
proach works better than the closely related state-of-the-
art method [26] based on independent component analysis
(ICA) [12]. In addition, we show that our method taking
account of the prior knowledge with respect to ﬂuorescent
colors enables us to separate reﬂective and ﬂuorescent com-
ponents even from a single image. Furthermore, we demon-
strate that our method is effective for applications such as
image-based material editing and relighting.

The main contribution of this study is threefold. First,
we explore the novel problem of separating reﬂective and
ﬂorescent components under narrow-band illumination, and
show that the ﬂuorescent color per pixel can be estimated
from at least two images. Second, we achieve the reﬂec-
tive and ﬂuorescent separation from a single image by tak-
ing the known space of potential ﬂuorescent colors into
consideration. Third, we show that our approach with
the improved model results in better ﬂuorescent separa-
tion than closely related state-of-the-art methods, e.g. ICA-
based method [26], and is effective for applications such as
image-based material editing and relighting.

7577

The rest of this paper is organized as follows. In Sec-
tion 2, we brieﬂy summarize related work. In Section 3, we
show that the ﬂuorescent color per pixel can be estimated
from at least two images, and then propose a method for
reﬂective-ﬂuorescent separation via MAP estimation. We
report the experimental results and the applications in Sec-
tion 4. We present concluding remarks in Section 5.

2. Related Work

2.1. Properties of ﬂuorescence

The spectral property of a ﬂuorescent surface is de-
scribed in a bispectral manner by using the Donaldson ma-
trix [3]. Each element of the Donaldson matrix is the
amount of reﬂected/emitted light when the wavelengths of
the outgoing light (row) and the incoming light (column) are
given. The diagonal elements, where the outgoing wave-
length is the same as the incoming wavelength, are reﬂec-
tive components, and the lower-left elements, where the
outgoing wavelength is longer than the incoming wave-
length, are ﬂuorescent components. The spectral property
of ﬂuorescent components, i.e. a pure ﬂuorescent material
without reﬂective components, is often described more sim-
ply by using the absorption spectrum and the emission spec-
trum [13]. The former is the fraction of incoming light ab-
sorbed by the material, and the latter is the amount of out-
going light emitted from the material at each wavelength.

The amount of reﬂected/emitted light from a ﬂuorescent
surface depends not only on the wavelengths but also on the
directions of the incoming light and the outgoing light. In
the graphics community, Glassner [7] and Wilkie et al. [25]
model the angular property of ﬂuorescent components in
the same manner as the Lambert model, i.e. the ﬂuorescent
radiance is proportional to the irradiance. In general, the
angular property of a ﬂuorescent surface including reﬂec-
tive components is described by a bispectral bidirectional
reﬂectance and reradiation distribution function (bispectral
BRRDF). Hullin et al. [11] propose an image-based method
for acquiring the bispectral BRRDFs of ﬂuorescent surfaces
by extending the image-based method for BRDF acquisi-
tion [18] to bispectal measurement. They report that the
ﬂuorescent components have weak angular dependency.

One of the interesting behaviors of ﬂuorescence is wave-
length shift called the Stokes shift; a ﬂuorescent material ab-
sorbs incident light at a certain wavelength, and then emits
light at longer wavelengths than the incident one. Hullin
et al. [10] exploit the fact that multiple scattering is sup-
pressed in a ﬂuorescent liquid due to the Stokes shift, and
then propose immersion range scanning for shape recov-
ery of transparent objects. Sato et al. [23] and Treibitz et
al. [24] show that ﬂuorescent components approximately
obeys the Lambert model with respect to light source di-
rections, and then propose bispectral photometric stereo

based on ﬂuorescence; an object of interest is illuminated
by shorter-wavelength light sources and is observed by a
longer-wavelength camera band. They show that the bispec-
tral photometric stereo is robust against specular reﬂection
components and interreﬂections due to the Stokes shift.

Another interesting behavior of ﬂuorescence is illumina-
tion invariance of ﬂuorescent colors, i.e the chromaticity of
ﬂuorescent components observed on a ﬂuorescent surface
is constant independent of the spectral intensity of incident
light. This is because the ﬂuorescent color depends only on
the emission spectra and is independent of the absorption
spectra. Han et al. [9] makes use of the illumination in-
variance for calibrating the spectral sensitivity of a camera
under unknown illumination. The illumination invariance
is used also for reﬂective and ﬂuorescent separation as de-
scribed in the next subsection.

2.2. Reﬂective and ﬂuorescent separation

Zhang and Sato [26] propose a method for separating re-
ﬂective and ﬂuorescent components under the assumption
that the spectral sensitivity of a camera is narrow-band as is
often assumed in color constancy algorithms. Speciﬁcally,
their method is based on the channel-wise linear model;
grayscale images in each channel under varying spectral in-
tensities are represented by convex combinations of a re-
ﬂective grayscale image and a ﬂuorescent grayscale image.
Then, those basis images are computed by using ICA in the
high-dimensional image space in a similar manner to re-
ﬂection removal [4]. Unfortunately, however, such channel-
wise linearity does not hold for images taken under narrow-
band illumination1, and then we cannot use their method
when not the cameras but the light sources are narrow-band.
In this study, we show that our pixel-wise linear model can
be used for narrow-band illumination instead.

Fu et al. [6] propose a method for separating reﬂective
and ﬂuorescent components by using high-frequency illu-
mination in the spectral domain on the basis that the ab-
sorption spectra are usually low-frequency. Their method
is analogous to the direct-global separation by using high-
frequency illumination in the spatial domain [20]. Unfor-
tunately, however, their method requires special and expen-
sive devices: a hyperspectral camera and a programmable
illumination in the spectral domain. On the other hand,
our proposed method uses a usual color camera and LEDs.
In addition, Fu et al. [5] propose a method for estimating
the spectral properties of reﬂectance and ﬂuorescence from
several RGB images taken under varying spectral intensi-
ties. They make use of low-dimensional linear model of
spectral reﬂectance [22] and absorption spectrum, and es-

1Speciﬁcally, N grayscale images in each channel under N narrow-
band light sources are represented by the convex combinations of (N + 1)
basis images, i.e. N for reﬂectance and one for ﬂuorescence in general.
Therefore, ICA-based method is ill-posed under narrow-band illumination.

7578

timate their coefﬁcients of the linear combinations. Their
method is also related to ours in the sense that the reﬂective
and ﬂuorescent components can be recovered from the ba-
sis functions and the estimated coefﬁcients. Unfortunately,
however, their method requires 9 images taken under differ-
ent spectral intensities, and thus the separation from a single
image is impossible.

under the narrow-band light source with the wavelength λ is
described by the spectral sensitivity at the wavelength. This
means that the color of the reﬂective component is the same
as the light source color.

It is known that a pure ﬂuorescent material absorbs light
at a certain wavelength λ′ and then emits light at a longer
wavelength λ′′. The ﬂuorescent component is described as

3. Proposed Method

In this section, we explore the reﬂective and ﬂuorescent
model under narrow-band illumination, and derive the in-
tersection method for reﬂective-ﬂuorescent separation. Fur-
thermore, we exploit the typical color distribution for ﬂuo-
rescent materials [19], and then extend it via MAP estima-
tion.

3.1. Reﬂective and ﬂuorescent model

We assume that an object of interest is observed by using
a color camera. For reﬂective and ﬂuorescent surfaces, the
pixel value i = (iR, iG, iB)⊤ at a point on the object sur-
face consists of a reﬂective component r = (rR, rG, rB)⊤
and a ﬂuorescent component f = (fR, fG, fB)⊤ in general:

i = r + f .

(1)

According to the dichromatic reﬂection model, the re-
ﬂective component is described by the sum of a diffuse re-
ﬂection component and a specular reﬂection component as

rc = gd(cid:2) l(λ′)ρ(λ′)sc(λ′)dλ′ + gs(cid:2) l(λ′)sc(λ′)dλ′.

(2)
Here, λ′, l(λ′), ρ(λ′), and sc(λ′) are the wavelength of an
incident light, the spectral intensity of a light source, the
spectral reﬂectance of the surface, and the spectral sensitiv-
ity of the c channel (c = R, G, B) of the camera respec-
tively. The geometric term of the diffuse reﬂection compo-
nent gd depends only on the lighting direction, while that of
the specular reﬂection component gs depends on both the
lighting and viewing directions.

We assume that the object is illuminated by a narrow-
band light source whose spectral intensity is approximately
represented by using the Dirac delta function δ( ) as

l(λ′) = lδ(λ′ − λ).

Substituting Eq.(3) into Eq.(2), we obtain

rc = gdlρ(λ)sc(λ) + gslsc(λ),
sR(λ)
sG(λ)

r = l(gdρ(λ) + gs)⎛
⎝

sB(λ) ⎞
⎠ .

Therefore, the color of the reﬂective component, i.e. the re-
ﬂective component normalized by its L1 norm ˆr = r/||r||1

(3)

(4)

(5)

fc = gf(cid:2) l(λ′)a(λ′)dλ′(cid:2) e(λ′′)sc(λ′′)dλ′′,

(6)

where gf , a(λ′), and e(λ′′) are the geometric term of the
ﬂuorescent component, the absorption and emission spectra
of the ﬂuorescent material.

Substituting Eq.(3) into Eq.(6), we obtain

fc = gf la(λ)(cid:2) e(λ′′)sc(λ′′)dλ′′,
(cid:7) e(λ′′)sR(λ′′)dλ′′
(cid:7) e(λ′′)sB(λ′′)dλ′′ ⎞
f = gf la(λ)⎛
(cid:7) e(λ′′)sG(λ′′)dλ′′
⎠ .
⎝

(7)

(8)

Therefore, the color of the ﬂuorescent component ˆf =
f /||f ||1 under the narrow-band light source is independent
of the wavelength λ of the light source. The light source
wavelength affects only the scale of the ﬂuorescent compo-
nent through a(λ).

3.2. Intersection method

We denote the pixel value observed under the n-th
narrow-band light source (n = 1, 2, 3, ..., N ) by in, and
denote the corresponding reﬂective and ﬂuorescent compo-
nents by rn and fn respectively. Then, the pixel value in is
represented by the convex combination of two unit vectors,
i.e. the light source color ˆrn and the ﬂuorescent color ˆfn as

in = rn + fn = αn ˆrn + βn ˆf ,

(9)

where αn and βn are the non-negative coefﬁcients of the
convex combination. As shown in the previous subsection,
αn, βn, and ˆrn depend on the light source, but the ﬂuores-
cent color ˆf is independent of it.

Hereafter, we consider the RGB color space and the r-g
chromaticity space deﬁned by the spectral sensitivity of a
camera, where r = R/(R + G + B) and g = G/(R + G +
B) respectively. This is because the linearity in Eq.(9) is
lost in nonlinear color spaces such as the CIE-L*a*b*. Our
proposed method exploits the planar structure in the RGB
color space and the linear structure in the r-g chromaticity
space as described below.

Let us consider the case when N = 2. As shown in Fig-
ure 1 (a), for a certain pixel in the ﬁrst image, Eq.(9) means
that the ﬂuorescent color ˆf is on the plane spanned by the
light source color ˆr1 and the pixel color ˆi1 = i1/||i1||1.

7579

Figure 1. The illustration of the intersection method. The ﬂores-
cent color ˆf is on (a) the plane spanned by the light source color
ˆr1 and the pixel color ˆi1 and (b) the plane spanned by ˆr2 and ˆi2 at
the same time. Therefore, the ﬂuorescent color is given by (c) the
intersection of those two planes.

Similarly, as shown in Figure 1(b), the ﬂuorescent color ˆf
is on the plane spanned by the light source color ˆr2 and the
pixel color ˆi2 in the second image at the same time. There-
fore, the ﬂuorescent color ˆf is given by the intersection of
those two planes as shown in Figure 1(c). Thus, we can
separate the reﬂective components rn and ﬂuorescent com-
ponents fn per pixel from at least two images under the
assumption that the light source colors ˆrn are known. Note
that we can compute the coefﬁcients of the convex combi-
nation αn and βn by using the least squares method with
non-negativity constraints when the light source colors are
known and the ﬂuorescent color is given as the above.

In the general case when N ≥ 2, the ﬂuorescent color
ˆf is given by the intersection of the N planes. Speciﬁcally,
the intersection method results in

...

(ˆin × ˆrn)⊤

...

⎛
⎜⎜⎝

⎞
⎟⎟⎠

ˆf =⎛
⎜⎜⎝

...
0
...

⎞
⎟⎟⎠

,

(10)

where × stands for the cross product. In practice, the ﬂu-
orescent color ˆf is computed by using the least squares
method with non-negativity constraints.

Light source colors: Note that we can relax the assump-
tion that the light source colors ˆrn are known. The pixel
color ˆin at a pure reﬂective pixel, where fn = 0, is equal to
the light source color ˆrn from Eq.(9). In addition, the pixel
color at a reﬂective-ﬂuorescent pixel is shifted towards red

Figure 2. The prior and likelihood of MAP estimation. (a) The ﬂu-
orescent colors computed from the actual emission spectra in the
McNamara dataset [19]. The prior probability density of the ﬂu-
orescent color is superimposed; darker has higher probability. (b)
The sketch of the likelihood function. We assume that the distance
dn between the observed pixel color ˆin and the line connecting
from the light source color ˆrn to the ﬂuorescent color ˆf obeys the
zero-mean Gaussian distribution.

due to the Stokes shift. Therefore, we can consider the most
bluish pixel as a pure reﬂective pixel and consider its color
as the light source color, if there is at least one pure reﬂec-
tive pixel in each image.

3.3. MAP estimation

The intersection method in Eq.(10) does not work well
under the following two conditions. First, there is at most a
single pair of ˆin and ˆrn such that ˆin (cid:3)= ˆrn, i.e. ˆin × ˆrn (cid:3)= 0.
Second, there is no pair of (ˆin × ˆrn) and (ˆim × ˆrm) such
that (ˆin × ˆrn) (cid:3)= (ˆim × ˆrm). From the geometric point of
view, the former means that there is at most a single plane
and the latter means that the planes are parallel to each other
in the RGB color space in Figure 1.

To cope with such limitations of the intersection method,
our proposed method makes use of the prior knowledge
with respect to ﬂuorescent colors. In Figure 2 (a), we plot
the ﬂuorescent colors computed from the actual emission
spectra in the McNamara dataset [19] in the chromatic-
ity space2. We can see that the distribution of ﬂuores-
cent colors has arch-like structure, in particular reddish col-
ors distribute near the spectrum locus, i.e. the locus of the
monochromatic (single-wavelength) color. This is because
the emission spectra are relatively narrow-band.

Accordingly, we formulate the estimation of the ﬂuores-

cent color via MAP estimation as

max

ˆf

N

(cid:10)n=1

P (ˆin| ˆf ; ˆrn)P ( ˆf ).

(11)

2Here, we assume the x-y chromaticity space deﬁned by the CIE-XYZ
for display purpose, where x = X/(X +Y +Z) and y = Y /(X +Y +Z)
respectively. Our proposed method assumes that the spectral sensitivity
of a camera is known and considers the prior probability distribution of
ﬂuorescent colors in the r-g chromaticity space deﬁned by the camera.

7580

Here, P (ˆin| ˆf ; ˆrn) is the likelihood function, i.e. the prob-
ability density that the pixel color ˆin is observed when the
ﬂuorescent color ˆf is given and the light source color ˆrn
is known, and P ( ˆf ) is the prior probability density of the
ﬂuorescent color ˆf . Taking the natural log of the above ob-
jective function, we obtain

max

ˆf (cid:11) N
(cid:12)n=1

log P (ˆin| ˆf ; ˆrn) + log P ( ˆf )(cid:13) .

(12)

It is clear from Eq.(9) that the light source color ˆrn, the
ﬂuorescent color ˆf , and the pixel color ˆin are on a single
line in the r-g chromaticity space3. In practice, however,
the observed pixel color is not on the line connecting from
the light source color ˆrn to the ﬂuorescent color ˆf due to
the noise in the observed pixel value. We assume that the
distance dn between the observed pixel color ˆin and the line
in the chromaticity space (see Figure 2 (b)) obeys the zero-
mean Gaussian distribution4 as

Since the above optimization is non-linear, we ﬁnd the op-
timal ﬂuorescent color via coarse-to-ﬁne search in the chro-
maticity space. Note that the ﬂuorescent color is more red-
dish than the pixel color due to the Stokes shift and because
the pixel color is represented by a convex combination of
the light source color and the ﬂuorescent color. Thus, we
constrain the search area to ˆfr + ˆfg ≥ ˆir + ˆig within the
spectrum locus.

Single-image method: Note that our proposed method
based on the MAP estimation enables us to separate reﬂec-
tive and ﬂuorescent components even from a single image.
Intuitively, a single image constrains the ﬂuorescent color
up to the neighborhood of the line connecting from the light
source color towards the pixel color from Eq.(9). Then, the
prior knowledge further constrains the ﬂuorescent color up
to the neighborhood of the ﬁtted smoothing spline curve.
Therefore, the ﬂuorescent color estimated from a single im-
age is located near their intersection.

4. Experiments

(13)

4.1. Synthetic images

P (ˆin| ˆf ; ˆrn) =

1

2πσ2 exp(cid:14)−

d2
n

2σ2(cid:15) .

Then, the ﬁrst term in Eq.(12) results in

−

1
2σ2

d2
n,

N

(cid:12)n=1

(14)

where we omit constant terms independent of ˆf .

Figure 2 (a) shows that the McNamara dataset has a bias;
the number of ﬂuorescent materials with bluish emission
spectra is larger than the number of those with greenish and
reddish spectra. To prevent the bias from propagating to
the result of the MAP estimation, we ﬁt a smoothing spline
curve to the distribution of ﬂuorescent colors in Figure 2
(a)5, and approximate the prior probability density of the
ﬂuorescent color P ( ˆf ) by using the Gaussian distributions
whose centers are at the points on the spline curve. The vari-
ances of the Gaussian distributions are computed around the
points on the spline curve. The computed prior probability
density P ( ˆf ) is superimposed in Figure 2 (a); darker has
higher probability

Thus, our proposed method based on the MAP estima-

tion results in

min

ˆf (cid:11) 1

2

d2

n − σ2 log P ( ˆf )(cid:13) .

N

(cid:12)n=1

(15)

3For the sake of simplicity, we denote an unit vector in the RGB color
space and its projection to the r-g chromaticity space by the same symbol.
4Although σ could depend on the pixel value i, we ﬁxed it to 0.01 in

all of our experiments.

We compared the performances of our proposed methods
with that of the closely related state-of-the-art method [26]
by using synthetic images. Speciﬁcally, we compared the
following four methods.

• Zhang and Sato [26] (ICA) assumes a narrow-band
camera and is based on the channel-wise linear model.
The ordering ambiguity in ICA is resolved by using
known light source colors in our experiments6.

• Intersection method (IS) assumes narrow-band illu-
mination and is based on the pixel-wise linear model.
As described in Subsection 3.2, the ﬂuorescent color is
given by the intersection of the two planes.

• MAP estimation (MAP) extends the intersection
method by taking the prior knowledge with respect to
ﬂuorescent colors into consideration as described in
Subsection 3.3. Multiple images are used as input.

• MAP estimation from a single image (MAP single)
is the same as the above MAP estimation in its formu-
lation, but a single image is used as input.

We synthesized the images of spheres with different
spectral properties taken by cameras with different spectral
sensitivities under light sources with different peak wave-
lengths and widths. We used the spectral reﬂectances of
matte Munsell color chips [22], the absorption and emis-
sion spectra in the McNamara dataset [19], and the dataset
of camera spectral sensitivity [14] for synthesizing realis-
tic images. We assumed that the spectral intensities of light

5We considered a ﬂuorescent color of (0.34, 0.26) as an outlier and

6The reﬂective and ﬂuorescent components estimated by using ICA can

removed it from the computation.

be negative. We clipped them to zeros in our experiments.

7581

Table 1. The quantitative comparison using the ﬁrst object under varying σw and σn. The numerical value in each cell stands for the RMS
error of four result images.

σn\σw

0
1
2
4

ICA

5

39.3
38.9
39.1
38.8

10

38.5
38.2
38.4
38.4

15

38.3
38.5
38.3
38.4

20

38.9
38.8
38.6
38.7

5

0.8
2.6
5.1
10.4

IS

10

0.8
2.6
5.1
10.2

15

1.0
2.6
5.1
10.2

MAP

20

1.2
2.8
5.2
10.1

5

0.9
2.5
4.7
10.0

10

0.9
2.4
4.7
9.7

15

1.0
2.3
4.5
9.5

20

1.4
2.4
4.4
8.9

5

6.4
7.9
10.3
15.3

MAP single
15
10

4.9
6.3
8.7
13.5

4.1
5.3
7.6
12.3

Table 2. The quantitative comparison using the second object under varying σw and σn.

σn\σw

0
1
2
4

ICA

5

17.7
18.3
17.7
17.9

10

18.0
17.9
18.5
18.2

15

18.6
18.6
18.6
19.1

20

19.4
19.4
19.5
20.0

5

2.3
7.4
13.0
19.6

IS

10

2.4
7.3
12.7
19.2

15

2.5
7.1
12.4
19.0

MAP

20

2.7
7.0
12.3
19.1

5

4.3
4.6
6.9
10.7

10

4.3
4.7
6.8
10.8

15

4.4
4.8
7.0
11.0

20

4.5
4.9
7.1
11.2

5

4.4
4.6
5.6
9.1

MAP single
10
15

4.5
4.7
5.6
9.3

4.6
4.9
5.9
9.4

20

5.2
8.1
8.9
12.0

20

4.8
5.1
6.2
9.9

Figure 3. The qualitative comparison using the ﬁrst object.
(a)
(d) are the input images. (b) (e) and (c) (f) are the corresponding
reﬂective and ﬂuorescent images: the ground truth and the results
using ICA, IS, MAP, and MAP single from left to right.

sources obey the Gaussian distributions with the standard
deviation σw. In order to evaluate the robustness of those
methods against noises, we artiﬁcially added the zero-mean
Gaussian noise with the standard deviation σn to each pixel.
Figure 3 shows the qualitative comparison using the ﬁrst
object. (a) (d) are the input images; the peak wavelengths
of light sources are 475 nm and 525 nm respectively and
the camera is Nikon D300s. (b) (e) and (c) (f) are the corre-
sponding reﬂective and ﬂuorescent images: the ground truth
and the results using ICA, IS, MAP, and MAP single from
left to right. Here, σw is 10 nm and σn is 2 for 8-bit im-
ages. Table 1 shows the quantitative comparison using the
ﬁrst object under varying σw and σn. The numerical value
in each cell stands for the root mean square (RMS) error
of four result images: the reﬂective component and the ﬂu-
orescent component for the ﬁrst input image and those for
the second input image. Figure 4 and Table 2 show the re-
sults using the second object. The peak wavelengths of light
sources are 425 nm and 475 nm respectively and the camera

Figure 4. The qualitative comparison using the second object.

is Canon 5D Mark II.

ICA vs. IS, MAP, and MAP single: Figure 3, Figure 4,
Table 1, and Table 2 show both qualitatively and quantita-
tively that our proposed methods, i.e. IS, MAP, and MAP
single work better than the state-of-the art method [26]
(ICA) for images under narrow-band illumination. In addi-
tion, they are robust when the widths of spectral intensities
σw increase from 5 nm to 20 nm. Note that the full width at
half maximum (FWHM) is about 2.35 × σw.

IS vs. MAP: Figure 3 and Table 1 show that both of IS
and MAP work well, but Figure 4 and Table 2 show that
MAP works better than IS in particular when σn increases.
In the former case, the light source colors of the input im-
ages7 are different. Therefore, as shown in Figure 1 (c), we
can obtain two planes such that (ˆi1 × ˆr1) (cid:3)= (ˆi2 × ˆr2), and
then IS itself works well. On the other hand, in the latter
case, the light source colors of the input images are similar
to each other. Therefore, the two planes in the RGB color
space are also similar, and then IS is not stable and the prior
knowledge with respect to ﬂuorescent colors is effective.

MAP vs. MAP single: Figure 4 and Table 2 show that

7See the ground truth of the reﬂective images in (b) and (e).

7582

Figure 5. The qualitative comparison using the real images of a
water pistol under the ﬁrst and the second light source colors. (a)
(d) are the input images. (b) (e) and (c) (f) are the corresponding
reﬂective and ﬂuorescent images: the results using ICA, IS, MAP,
and MAP single from left to right.

Figure 7. The qualitative comparison using the real images of a
tennis ball under the second and the third light source colors.

Figure 8. The qualitative comparison using the real images of
erasers under the second and the third light source colors.

Figure 6. The qualitative comparison using the real images of a
water pistol under the ﬁrst and the fourth light source colors.

constraints are effective.

4.2. Real images

the performance of MAP single is almost the same as that of
MAP using two input images, when the light source color is
different from the ﬂuorescent color. In the case where they
are similar (Figure 3 and Table 1), the single image method
has its limits. The separation is not satisfying, because the
linear constraint in Eq.(9) could be sensitive to noises.

As described in Section 3, we have two different clues
for estimating ﬂuorescent colors; one is a single linear con-
straint in Eq.(9) per input image, and the other is the con-
straint due to the prior knowledge with respect to ﬂuores-
cent colors. Speciﬁcally, for two input images, IS has two
linear constraints, and MAP has two linear constraints and
the constraint due to the prior. MAP single has one linear
constraint and the constraint due to the prior for a single
input image. The experimental results show that our pro-
posed methods work well when at least two independent

We compared the performances of our proposed meth-
ods (IS, MAP, and MAP single) with that of the closely re-
lated state-of-the-art method (ICA) [26] by using real im-
ages. The target objects are a water pistol, a tennis ball,
and erasers. The images of those objects were captured by
using a Point Grey Chameleon camera under four different
LEDs whose peak wavelengths are 405, 460, 520, and 635
nm respectively and FWHMs are from 12 to 36 nm.

Figure 5 shows the qualitative comparison using the real
images of the water pistol under the ﬁrst (405 nm) and the
second (460 nm) LEDs. (a) (d) are the input images. (b) (e)
and (c) (f) are the corresponding reﬂective and ﬂuorescent
images: the results using ICA, IS, MAP, and MAP single
from left to right. Figure 6, Figure 7, and Figure 8 show the
qualitative comparisons using the real images of the water
pistol, the tennis ball, and the erasers respectively. We used
the ﬁrst and the fourth (635 nm) LEDs in Figure 6, and the

7583

Figure 9. The spectral radiances observed at the points A, B, and
C on the water pistol and the point D on the tennis ball under the
ﬁrst light source.

Table 3. The quantitative comparison using real images: the dot
products between the ground truth and estimated ﬂuorescent col-
ors.

Figure 10. The application of our reﬂective-ﬂuorescent separation
to image-based material editing and relighting.

ICA
IS

MAP

MAP single

A

0.919
0.982
0.983
0.983

B

0.931
0.989
0.991
0.992

C

0.992
0.996
0.999
0.999

D

0.960
0.996
0.998
0.998

second and the third (520 nm) LEDs in Figure 7 and Fig-
ure 8.

ICA vs. IS, MAP, and MAP single: We can see that our
proposed methods (IS, MAP, and MAP single) work better
than ICA. In particular, the colors of the reﬂective compo-
nents estimated by ICA are different from the light source
colors in Figure 6 (b) and Figure 8 (b) (e). In addition, the
ﬂuorescence of the orange eraser disappears from the ﬂuo-
rescent components estimated by ICA in Figure 8 (c) (f).

IS vs. MAP: We can see that MAP is more robust than IS.
In particular, the effectiveness of MAP, i.e. taking account
of the prior knowledge with respect to ﬂuorescent colors
is clear when one of the input images has no clue about
ﬂuorescent colors as shown in Figure 6 (b) (c) and (e) (f).
Figure 7 also shows that MAP works better than IS.

MAP vs. MAP single: We can see that MAP single
works as well as MAP as shown in Figure 5 (b) (c) and (e)
(f), Figure 6 (b) (c), Figure 7 (b) (c), and Figure 8 (b) (c).
On the other hand, when the light source colors are similar
to the ﬂuorescent colors, MAP single is not robust as shown
in Figure 7 (e) (f) and Figure 8 (e) (f), and does not work
well as shown in Figure 6 (e) (f).

To conﬁrm the effectiveness of our proposed methods
quantitatively, we conducted numerical comparison using a
hyperspectral camera. Speciﬁcally, as shown in Figure 9,
we measured the spectral radiances under the ﬁrst light
source (405 nm) at the points A, B, and C on the water
pistol and at the point D on the tennis ball. Since the emis-
sion spectra are separated from the reﬂection spectra, we
can compute the ﬂuorescent colors there by using the spec-
tral sensitivity of the color camera, and consider them as
their ground truths. Table 3 shows the dot products between
the ground truth and estimated colors in Figure 5 (c) and
Figure 7 (c). Here, both colors are represented by unit 3D
vectors, and then the dot product is 1 if the colors are the

same. These results show quantitatively that our proposed
methods (IS, MAP, and MAP single) work better than ICA.

4.3. Applications

As a direct application of reﬂective-ﬂuorescent separa-
tion, we conducted image-based material editing and re-
lighting. Speciﬁcally, we captured three images of the
erasers under B (the ﬁrst), G (the third), and R (the fourth)
light sources, and separated the reﬂective and ﬂuorescent
components of those images by using our proposed method
(MAP). Then, we synthesized various images by linearly
combining those 6 images with different weights. In Fig-
ure 10, their average is shown at the center (the second row
and the third column), and the ﬂuorescent components are
increased/decreased at the upper/lower images. Similarly,
we change the light source colors from bluish (left) to red-
dish (right). We can see that our method is effective for
photorealistic image-based material editing and relighting.

5. Conclusion and Future Work

In this paper, we showed that the ﬂuorescent color per
pixel can be estimated from at least two images under differ-
ent light source colors. Furthermore, exploiting the known
space of potential ﬂuorescent colors, we obtain more ro-
bust MAP estimates even from a single input image un-
der narrow-band illumination. Through a number of ex-
periments using both synthetic and real images, we con-
ﬁrmed qualitatively and quantitatively that our proposed
method works better than the closely related state-of-the-
art method. In addition, we demonstrated that our method
is effective for image-based material editing and relighting.
Incorporating the spectral-spatial correlation into our pixel-
wise approach is one of the future directions of this study.

Acknowledgments

This work was partially supported by JSPS KAK-
ENHI Grant Numbers JP18H05011, JP17H01766, and
JP16H01676.

7584

[18] S. Marschner, S. Westin, E. Lafortune, and K. Torrance.
Image-based bidirectional reﬂectance distribution function
measurement. Applied Optics, 39(16):2592–2600, 2000. 2

[19] G. McNamara, A. Gupta, J. Reynaert, T. Coates, and C.
Boswell. Spectral imaging microscopy web sites and data.
Cytometry Part A, 69A(8):863–871, 2006. 3, 4, 5

[20] S. Nayar, G. Krishnan, M. Grossberg, and R. Raskar. Fast
separation of direct and global components of a scene us-
ing high frequency illumination. In Proc. ACM SIGGRAPH
2006, pages 935–944, 2006. 2

[21] J.-I. Park, M.-H. Lee, M. Grossberg, and S. Nayar. Mul-
tispectral imaging using multiplexed illumination. In Proc.
IEEE ICCV2007, pages 1–8, 2007. 1

[22] J. Parkkinen, J. Hallikainen, and T. Jaaskelainen. Character-
istic spectra of munsell colors. JOSA A, 6(2):318–322, 1989.
2, 5

[23] I. Sato, T. Okabe, and Y. Sato. Bispectral photometric stereo
In Proc. IEEE CVPR2012, pages

based on ﬂuorescence.
270–277, 2012. 2

[24] T. Treibitz, Z. Murez, G. Mitchell, and D. Kriegman. Shape
In Proc. ECCV2012, pages 292–306,

from ﬂuorescence.
2012. 2

[25] A. Wilkie, A. Weidlich, C. Larboulette, and W. Purgathofer.
A reﬂectance model for diffuse ﬂuorescent surfaces. In Proc.
GRAPHITE2006, pages 321–331, 2006. 2

[26] C. Zhang and I. Sato. Image-based separation of reﬂective
and ﬂuorescent components using illumination variant and
invariant color. IEEE Trans. PAMI, 35(12):2866–2877, 2013.
1, 2, 5, 6, 7

References

[1] B. Ajdin, M. Finckh, C. Fuchs, J. Hanika, and H. Lensch.
Compressive higher-order sparse and low-rank acquisition
with a hyperspectral light stage. Technical report, Eberhard
Karls Universit¨at T¨ubingen, 2012. WSI-2012-01. 1

[2] K. Barnard. Color constancy with ﬂuorescent surfaces. In

Proc. CIC1999, pages 257–261, 1999. 1

[3] R. Donaldson. Spectrophotometry of ﬂuorescent pigments.

British Journal of Applied Physics, 5(6):210–214, 1954. 2

[4] H. Farid and E. Adelson. Separating reﬂections and light-
ing using independent components analysis. In Proc. IEEE
CVPR1999, pages I–262–267, 1999. 2

[5] Y. Fu, A. Lam, I. Sato, T. Okabe, and Y. Sato. Reﬂectance
and ﬂuorescence spectral recovery via actively lit RGB im-
ages. IEEE Trans. PAMI, 38(7):1313–1326, 2016. 2

[6] Y. Fu, A. Lam, I. Sato, T. Okabe, and Y. Sato. Separating
reﬂective and ﬂuorescent components using high frequency
illumination in the spectral domain.
IEEE Trans. PAMI,
38(5):965–978, 2016. 2

[7] A. Glassner. A model for ﬂuorescence and phosphorescence.
In Proc. the 5th Eurographics Workshop on Rendering, pages
57–68, 1994. 2

[8] J. Gu and C. Liu. Discriminative illumination: per-pixel clas-
siﬁcation of raw materials based on optimal projections of
spectral BRDF. In Proc. IEEE CVPR2012, pages 797–804,
2012. 1

[9] S. Han, Y. Matsushita, I. Sato, T. Okabe, and Y. Sato. Cam-
era spectral sensitivity estimation from a single image under
unknown illumination by using ﬂuorescence. In Proc. IEEE
CVPR2012, pages 805–812, 2012. 2

[10] M. Hullin, M. Fuchs, I. Ihrke, H.-P. Seidel, and H. Lensch.
Fluorescent immersion range scanning. In Proc. ACM SIG-
GRAPH2008, 2008. Article No.87. 2

[11] M. Hullin, J. Hanika, B. Ajdin, H.-P. Seidel, J. Kautz, and H.
Lensch. Acquisition and analysis of bispectral bidirectional
reﬂectance and reradiation distribution functions.
In Proc.
ACM SIGGRAPH2010, 2010. Article No.97. 2

[12] A. Hyv¨arinen and E. Oja. Independent component analysis:
algorithms and applications. Neural Networks, 13(4–5):411–
430, 2000. 1

[13] D. Jameson. Introduction to ﬂuorescence. CRC Press, 2014.

2

[14] J. Jiang, D. Liu, J. Gu, and S. S¨usstrunk. What is the space
of spectral sensitivity functions for digital color cameras? In
Proc. IEEE WACV2013, pages 168–179, 2013. 5

[15] M. Kitahara, T. Okabe, C. Fuchs, and H. Lensch. Simulta-
neous estimation of spectral reﬂectance and normal from a
small number of images. In Proc. VISAPP2015, pages 303–
313, 2015. 1

[16] C. LeGendre, X. Yu, D. Liu, J. Busch, A. Jones, S. Pattanaik,
and P. Debevec. Practical multispectral lighting reproduc-
tion. In Proc. ACM SIGGRAPH2016, 2016. Article No.32.
1

[17] C. Liu, G. Yang, and J. Gu. Learning discriminative illumi-
nation and ﬁlters for raw material classiﬁcation with optimal
projections of bidirectional texture functions. In Proc. IEEE
CVPR2013, pages 1430–1437, 2013. 1

7585

