12126

network that can be trained in an end-to-end fashion to rep-
resent all the relevant information about the social and scene
context. The Multi-Agent Tensor representation, illustrated
in Fig. 1, spatially aligns an encoding of the scene with en-
codings of the past trajectory of every agent in the scene,
which maintains the spatial relationships between agents
and scene features. Next, a fused Multi-Agent Tensor en-
coding is formed via a fully convolutional mapping (see Fig.
2), which naturally learns to capture the spatial locality of
interactions between multiple agents and the environment,
as in agent-centric approaches, and preserves the spatial lay-
out of all agents within the fused Multi-Agent Tensor in a
spatial-centric manner.

Our model decodes the comprehensive social and con-
textual information encoded by the fused Multi-Agent Ten-
sor into predictions of the trajectories of all agents in the
scene simultaneously. Real-world behavior is not deter-
ministic – agents can perform multiple maneuvers from the
same context (e.g. follow lane or change lane), and the same
maneuver can vary in execution in terms of velocity and ori-
entation proﬁle. We use conditional generative adversarial
training [12, 23] to capture this uncertainty over predicted
trajectories, representing the distribution over trajectories
with a ﬁnite set of samples.

We conduct experiments on both driving datasets and
pedestrian crowd datasets. Experimental results are re-
ported on the publicly available NGSIM driving dataset [7],
Stanford Drone pedestrian crowd dataset [25], ETH-UCY
crowd datasets [21, 27], and a private recently-collected
Massachusetts driving dataset. Quantitative and qualitative
ablative experiments are conducted to show the contribu-
tion of each part of the model, and quantitative comparisons
with recent approaches show that the proposed approach
achieves state-of-the-art accuracy in both highway driving
and pedestrian trajectory prediction.

2. Related Work

Traditional methods for predicting or classifying trajec-
tories model various kinds of interactions and constraints
by hand-crafted features or cost functions [3, 5, 6, 8, 15, 22,
32]. Early methods based on inverse optimal control also
use hand-crafted cost features, and learn linear weighting
functions to rationalize trajectories which are assumed to be
generated by optimal control [18]. Recent data-driven ap-
proaches based on deep networks [1, 4, 9, 10, 13, 19, 20, 24,
28, 29, 31] outperform traditional approaches. Most of this
work focuses either on modeling constraints from the scene
context [29] or on modeling social interactions among mul-
tiple agents [1, 9, 10, 13, 31]; a smaller fraction of work
considers both aspects [4, 20, 28].

Agent-centric NN-based approaches integrate informa-
tion from multiple agents by applying aggregation func-
tions on multiple agents’ feature vectors output from re-

current units. Social LSTM [1] runs max pooling over
state vectors of nearby agents within a predeﬁned distance
range, but does not model social interaction with far-away
agents. Social GAN [13] contributes a new pooling mech-
anism over all the agents involved in a scene globally, and
by using adversarial training to learn a stochastic, genera-
tive model of human behavior [12]. Although these kinds
of max pooling aggregation functions handle varying num-
bers of agents well, permutation invariant functions may
discard information when input agents lose their unique-
ness [28]. In contrast, Social Attention [31] and Sophie [28]
address the heterogeneity of social interaction among dif-
ferent agents by attention mechanisms [2, 30], and spatial-
temporal graphs [17]. Attention mechanisms encode which
other agents are most important to focus on when predict-
ing the trajectory of a given agent. However, attention-
based approaches are very sensitive to the number of agents
included — predicting n agents has O(n2) computational
complexity. In contrast, our approach captures multiagent
interactions while maintaining O(n) computational com-
plexity.

The agent-centric approaches discussed above do not
make use of spatial relationships among agents directly. As
an alternative, spatial-centric approaches retain the spatial
structure of agents and the scene context throughout their
representations. Convolutional Social Pooling [9] partially
retains the spatial structure of agents’ locations by forming
a social tensor which is similar to our Multi-Agent Tensor
representation, but much of this spatial information is later
aggregated by several bottleneck layers. This approach does
not encode the scene context, and only a single agent’s tra-
jectory can be predicted with each forward pass — poten-
tially too slow for real-time trajectory prediction of multiple
agents. Chauffeur Net [4] proposes a novel method to retain
the spatial structure of agents and the scene by directly op-
erating on the spatial feature map of agents and the scene
context. In this approach, agents are represented as bound-
ing boxes and do not have independent recurrent encoding
units. In contrast, our model encodes multiple agents’ fea-
ture vectors via recurrent units while simultaneously retain-
ing the spatial structure of agents and the scene throughout
the reasoning process.

Many data-driven approaches learn to predict determin-
istic future trajectories of agents by minimizing recon-
struction loss [1, 29]. However, human behavior is inher-
ently stochastic. Recent approaches address this by pre-
dicting a distribution over future trajectories by combin-
ing Variational Auto-Encoders [11] and Inverse Optimal
Control [20], or with conditional Generative Adversarial
Nets [13, 28]. GAIL-GRU [19] uses generative adversar-
ial imitation learning [16] to learn a stochastic policy that
reproduces human expert driving behavior. R2P2 [24] pro-
poses a novel cost function to encourage enhancement in

12127

12128

1 , x′′

n + x′′

2 , .., x′′

2 , ..., x′

each agent {x′′
n} are sliced out according to their
coordinates from the fused Multi-Agent Tensor output c′′
(Fig. 2). These agent-speciﬁc representations are then
added as a residual [14] to the original encoded agent vec-
tors to form ﬁnal agent encoding vectors {x′
2 +
x′′
n}, which encode all the information from the
past trajectories of the agents themselves, the static scene
context, and the interaction features among multiple agents.
In this way, our approach allows each agent to get a different
social and contextual embedding focused on itself. Impor-
tantly, the model gets these embeddings for multiple agents
using shared feature extractors instead of operating n times
for n agents.

1 + x′′

1 , x′

Finally, for each agent in the scene,

its ﬁnal vector
i + x′′
x′
is decoded to future trajectory prediction ˆyi by
i
LSTM decoders. Similar to the encoders for each agent, pa-
rameters are shared to guarantee that the network can gen-
eralize well when the number of agents in the scene varies.
The whole architecture is fully differentiable and can
be trained end-to-end to minimize reconstruction loss
between predicted future trajectories {ˆy1, ˆy2, .., ˆyn} and
observed ground-truth future trajectories {y1, y2, .., yn}:
LL2/L1(ˆyi, yi) = PT ′
t=1 L2/L1(ˆyit, yit), where L2/L1 in-
dicates that we can use either the L2 or L1 distance between
two positions for reconstruction error.

3.3. Adversarial Loss

We use conditional generative adversarial training [12,
23] to learn a stochastic generative model that captures the
multimodal uncertainty of our predictions. GANs consist of
two networks, a generator G and a discriminator D compet-
ing against each other. G learns the distribution of the data
and generates samples, while D learns to distinguish the
feasibility or infeasibility of the generated samples. These
networks are simultaneously trained in a two player min-
max game framework.

In our setting, we use a conditional G to generate fu-
ture trajectories of multiple agents, conditioning on all the
agents’ past trajectories, the static scene context, and ran-
dom noise input to create stochastic outputs. Simultane-
ously, we use D to distinguish whether the generated tra-
jectories are real (ground truth) or fake (generated). Both G
and D share exactly the same architecture in their encoding
parts with the deterministic model presented in Section 3.1,
to reason about static scene context and interaction among
multiple agents spatially. Both G and D are initialized with
parameters from the trained deterministic model introduced
in previous subsections. Detailed architectures and losses
are described below.

Generator (G) G observes past trajectories of all the
agents in a given scene {x1, x2, .., xn}, and the static scene
context c.
It jointly outputs the predicted future trajec-
tories {ˆy1, ˆy2, .., ˆyn} by decoding the ﬁnal agent vectors

1 , x′

1 + x′′

2 + x′′

2 , ..., x′

n + x′′

{x′
n} described in Section 3.2,
concatenated with Gaussian white noise vector z. The ar-
chitecture is exactly the same as presented in previous sub-
sections, except that in the deterministic model, the ﬁnal
encoding for a given agent x′
is concatenated with
z = 0 vector to decode into its future trajectory; while in G,
z is sampled from a Gaussian distribution.

i + x′′
i

Discriminator (D) D observes the ground truth past
trajectories of all the agents in a given static scene con-
text, combined either with all generated future trajecto-
ries {x1, x2, .., xn, ˆy1, ˆy2, ..., ˆyn} or all ground truth future
trajectories {x1, x2, .., xn, y1, y2, .., yn}. It outputs real or
fake labels for the future trajectory of each agent in the
scene, such that D(y) = 0 if trajectory y is fake, and
D(y) = 1 if trajectory y is real. D shares nearly the same
architecture as presented in previous subsections, except for
the following differences: (1) Its single agent LSTM en-
coders take in past and future trajectories as input instead of
just past trajectories; (2) As a classiﬁer, it does not use an
LSTM to decode the ﬁnal agent vector x′
i to a future
trajectory. Instead, ﬁnal agent encodings are fed into fully
connected layers to be classiﬁed as real or a fake.

i + x′′

Losses The adversarial loss LGAN for a given scene is:

LGAN (scene) =

min

G

D X
max

i∈scene

log D(yi) + log(1 − D(ˆyi)),

(1)

where {i|i ∈ scene} is the set of agents in a given scene, yi
and ˆyi denote ground truth (real) and generated (fake) tra-
jectories, respectively, and G denotes the generative MATF
network which we are optimizing.

To train the MATF GAN, we use the following losses:

Θ∗ = arg max

Escene[LGAN (scene)

Θ

+ λXi∈scene

LL2/L1(ˆyi, yi)],

(2)

where Θ is the set of parameters of the model and λ weights
the contribution of reconstruction loss versus adversarial
loss.

4. Experiments

In the Experiments and Results sections, we evaluate our
model on both driving datasets [7] and pedestrian crowd
datasets [21, 27, 25]. We construct different baseline vari-
ants of our models for ablative studies, and compare with
state-of-the-art alternative methods quantitatively [1, 8, 9,
13, 15, 19, 20, 28]. Qualitative results are also presented for
further analysis.

4.1. Datasets

We use the publicly available NGSIM dataset [7], a re-
cently collected Massachusetts driving dataset, the pub-

12129

licly available ETH-UCY datasets [21, 27], and the publicly
available Stanford Drone dataset [25] for training and eval-
uation.

NGSIM. A driving dataset consisting of trajectories of
real freeway trafﬁc over a time span of 45 minutes. Data
were recorded by ﬁxed bird’s-eye view cameras placed over
a 640-meter span of US101. Trajectories of all the vehi-
cles traveling through the area within this 45 minutes are
annotated. The dataset consists of various trafﬁc conditions
(mild, moderate and congested), and contains around 6k ve-
hicles in total.

ETH-UCY. A collection of relatively small benchmark
pedestrian crowd datasets. There are 5 datasets with 4 dif-
ferent scenes, including 1.5k pedestrian trajectories in total.
We use the same cross-validation training-test split metrics
as reported in previous work [13, 28].

Stanford Drone. A large-scale pedestrian crowd dataset
consisting of 20 unique scenes in which pedestrians, bicy-
clists, skateboarders, carts, cars, and buses navigate on a
university campus. Raw, static scene context images are
provided from bird’s-eye view, and coordinates of multi-
ple agents’ trajectories are provided in pixels. These scenes
contain rich human-human interactions, often taking place
within high density crowds, and diverse physical landmarks
such as buildings and roundabouts that must be avoided. We
use the standard test set for quantitative evaluation. Some
scenes from the standard training set are not used for our
training process, but left out for qualitative evaluation in-
stead.

4.2. Baseline Models

We construct a set of baseline variants of our model for

ablative studies.

LSTM: A simple deterministic LSTM encoder-decoder.
It shares exactly the same architecture as the single-agent
LSTM encoders and decoders introduced in Section 3 for
fair comparison.

Single Agent Scene: This deterministic model shares
exactly the same architecture as introduced in Section 3,
except that it only takes in one agent history xi with scene
representation c and outputs only ˆyi each time, so the model
reasons about scene-agent interaction, but is completely un-
aware of multi-agent interaction.

Multi Agent: This deterministic model has the same
details as the model described in Section 3, except that
the scene representation c is not provided as input. The
model only reasons about multi-agent interactions absent
from scene context information.

Multi Agent Scene: The deterministic model introduced

in Section 3.

GAN: The stochastic model introduced in Section 3.3.
Similar to Social GAN [13], we sample N times and report
the best trajectory in the L2 sense for fair comparison with

stochastic models, with N = 3 in Section 5.1, and N = 20
as adopted by [13] in Section 5.2.

See Supplementary Materials for implementation de-

tails.

5. Results

5.1. Driving Datasets

NGSIM Dataset. We adopt

the same experimen-
the presented results as
tal setting and directly report
in [9]: We split
the trajectories into segments of 8s,
and all agents appearing in the 640-meter span are con-
sidered in the reasoning and prediction process. We
use 3s of trajectory history and a 5s prediction hori-
zon. LSTMs operate at 0.2s. As in [9], we report the
Root Mean Square Error in meters with respect to each
timestep t within the prediction horizon: RM SE(t) =
q 1
n Pi=1,2,..,n((ˆxit − xit)2 + (ˆyit − yit)2) , where n is
the total number of agents in the validation set, xit de-
notes the x coordinate of the i-th car in the dataset at future
timestep t, and yit the y coordinate at t.

Method

CV [9]
LSTM Baseline
C-VGMM + VIM [8]
MATF Multi Agent

GAIL-GRU [19]

Social Conv [9]
MATF GAN

1s

0.73
0.66
0.66
0.67

0.69

0.61
0.66

2s

1.78
1.62
1.56
1.51

1.51

1.27
1.34

3s

3.13
2.94
2.75
2.51

2.55

2.09
2.08

4s

4.78
4.63
4.24
3.71

3.65

3.10
2.97

5s

6.68
6.63
5.99
5.12

4.71

4.37
4.13

Table 1. Quantitative results on NGSIM [7] dataset. RMSEs in
meters with respect to each future timestep in the prediction hori-
zon are reported.

Quantitative results are shown in Table 1. Our deter-
ministic model MATF Multi Agent outperforms the state-of-
the-art deterministic model C-VGMM + VIM [8], a recent
vehicle interaction approach based on variational Gaussian
mixture models with Markov random ﬁelds. We include a
comparison with GAIL-GRU [19]; however, note that this
model has access to the future ground-truth trajectories of
other agents when predicting a given agent, while MATF
and other models do not, so these results are not fully com-
parable. We compare our stochastic model, MATF GAN,
with Social Conv [9], an approach that captures the distri-
bution over future trajectories by representing maneuvers.
MATF GAN performs at the state-of-the-art level, with par-
ticularly improved performance at longer prediction hori-
zons (3-5s). Note that Social Conv has access to auxiliary
supervision from maneuver labels, while MATF does not
require this information. Multi Agent Scene does not outper-
form Multi Agent on NGSIM, because lanes in the NGSIM

12130

12131

in [13], we report the Average Displacement Error and Final
Displacement Error in pixels with respect to each time-step
t within the prediction horizon:

proaches due to the iterative process of IOC and O(n2)-
based attention mechanisms, respectively. In contrast, our
model is more efﬁcient in computational complexity with
our shared convolution operations.

ADE(i) =

1
T ′ X

j=1,2,..,T ′

q(ˆxij − xij )2 + (ˆyij − yij )2

ADE =

1
n X

i=1,2,..,n

ADE(i)

F DE(i) = q(ˆxiT ′ − xiT ′ )2 + (ˆyiT ′ − yiT ′ )2

F DE =

1
n X

i=1,2,..,n

F DE(i),

where n is the total number of agents in the validation set,
xij and yij denote the coordinates of the i-th agent in the
dataset at future timestep j, and T ′ denotes the ﬁnal future
timestep. Table 2 shows our results. MATF performs the
best both in deterministic and stochastic settings.

Dataset

Deterministic

Stochastic

S-LSTM

MATF

S-GAN

MATF GAN

1.09 / 2.35
ETH
HOTEL 0.79 / 1.76
0.67 / 1.40
UNIV
0.47 / 1.00
ZARA1
0.56 / 1.17
ZARA2
0.72 / 1.54
AVG

1.33 / 2.49
0.51 / 0.95
0.56 / 1.19
0.44 / 0.93
0.34 / 0.73
0.64 / 1.26

0.81 / 1.52
0.67 / 1.37
0.60 / 1.26
0.34 / 0.68
0.42 / 0.84
0.57 / 1.13

1.01 / 1.75
0.43 / 0.80
0.44 / 0.91
0.26 / 0.45
0.26 / 0.57
0.48 / 0.90

Table 2. Quantitative results on ETH-UCY datasets. ADE / FDE
of world coordinates in meters at 4.8s prediction horizon are re-
ported. Our deterministic MATF model outperforms Social LSTM,
and our stochastic MATF GAN outperforms Social GAN. We di-
rectly report the Social LSTM and Social GAN results presented
in [13].

Stanford Drone Dataset. We adopt the same exper-
imental setting and directly report the results presented
in [28]: We split the trajectories into segments of 8s, and
all agents appearing in the scene are considered in the rea-
soning and prediction process. We use 3.2s of trajectory
history and a 4.8s prediction horizon. LSTMs operate at
0.4s per timestep. As in [25], we report ADE and FDE.

Fig. 5 shows qualitative ablative results using determin-
istic models; only the full MATF Multi Agent Scene model
captures the range of behaviors in the data. Quantitative re-
sults for deterministic and stochastic models are shown in
Table 3. MATF Multi Agent Scene outperforms other deter-
ministic models in ADE, and MATF GAN performs close
to the state-of-the-art level. Among the deterministic mod-
els, Social LSTM achieves the best performance in FDE.
Among the stochastic models, Desire gains strength from
using Variational Auto-Encoders [11] and Inverse Optimal
Control to generate and rank trajectories; Sophie performs
the best with its strong attention-based social and physi-
cal reasoning modules. However, the computational com-
plexity of these approaches is higher than that of other ap-

Method

c LSTM Baseline

Social Force [15]
Social LSTM [1]
MATF Multi Agent
MATF Multi Agent Scene

c Social GAN [13]

Desire [20]
Sophie [28]
MATF GAN

i
t
s
i
n
i
m
r
e
t
e
D

i
t
s
a
h
c
o
t
S

ADE

37.35
36.38
31.19
30.75
27.82

27.25
19.25
16.27
22.59

FDE

Complexity

77.13
58.14
56.97
65.90
59.31

41.44
34.05
29.38
33.53

O(n)
O(n)
O(n)
O(n)
O(n)

O(n)
O(nK)
O(n2)
O(n)

Table 3. Quantitative results on Stanford Drone [25] dataset. Av-
erage and Final Displacement Errors are reported. Computational
complexity w.r.t agents number n in a given scene is presented.

We also analyze the factors inﬂuencing performance in
our model—particularly the impact of the spatial resolu-
tion of the Multi-Agent Tensor. Table 4 shows that there
is a U-shaped performance curve due to under/overﬁtting at
low/high resolution, respectively, and that the ideal resolu-
tion is 32 × 32, the setting we report.

Spatial Grid Resolution

42

82

Deterministic

Stochastic

ADE
FDE
ADE
FDE

322

32.08 32.36
68.08 66.46
24.57 23.55
39.44 36.46 33.45 33.53

642
162
30.26 27.82 29.47
62.73 59.31 62.60
22.69 22.59 23.50
35.72

Table 4. Effect of spatial grid resolution on prediction accuracy.
Results reported on Stanford Drone Dataset of 4.8s horizon.

6. Discussion

We proposed an architecture for trajectory prediction
which models scene context constraints and social interac-
tion while retaining the spatial structure of multiple agents
and the scene, unlike the purely agent-centric approaches
more commonly used in the literature. Our motivation was
that scene context constraints and social interaction patterns
are invariant to the absolute coordinates where they take
place; these patterns only depend on the relative positions
among agents and scenes. Convolutional layers are suited
to modeling these kinds of position-invariant spatial interac-
tions by sharing parameters across agents and space, while
recent approaches like Social Pooling [1, 13] or Attention
mechanisms [31] cannot explicitly reason about spatial rela-
tionships among agents and cannot reason about these rela-
tionships at multiple spatial scales. Our Multi-Agent tensor
fusion architecture models this naturally. To the best of our
knowledge, MATF is the ﬁrst approach which fuses infor-
mation from a static scene context with multiple dynamic

12132

Figure 5. Ablative results on Stanford Drone dataset. From left to right are results from MATF Multi Agent Scene, MATF Multi Agent,
and LSTM, all deterministic models. Blue lines show past trajectories, red ground truth, and green predicted. All results come from the
qualitative validation dataset. All the agent trajectories shown in this ﬁgure are predicted jointly via one forward pass. The closer the green
predicted trajectory is to the red ground truth future trajectory, the more accurate the prediction. Our model predicts that (1) two agents
entering the roundabout from the top will exit to the left; (2) one agent coming from the left on the pathway above the roundabout is turning
left to move toward the top of the image; (3) one agent is decelerating at the door of the building above and to the right of the roundabout.
(4) In one interesting failure case, an agent on the top-right of the roundabout is turning right to move toward the top of the image; the
model predicts the turn, but not how sharp it will be. These and various other qualitative patterns are correctly predicted by our Multi Agent
Scene model, and some of them are approximated by our Multi Agent model, but most are not predicted by the baseline LSTM model.

agent states, while retaining their spatial structure through-
out the reasoning process to bridge the gap between agent-
centric and spatial-centric trajectory prediction paradigms.

We applied our model to two different trajectory predic-
tion tasks to demonstrate its ﬂexibility and capacity to learn
different types of behaviors, agent types, and scenarios from
data. In the vehicle prediction domain, our model achieved
state-of-the-art results at long-range prediction of vehicle
trajectories in the NGSIM dataset. Our adversarially trained
stochastic prediction model performed best relative to the
maneuver-based approach of [9], suggesting that a repre-
sentation of the distribution over maneuvers was necessary
– whether explicit as in [9] or implicit as in our work. Our
ablative studies on a Massachusetts driving dataset showed
that representations of both the scene and multiagent in-
teractions were necessary for accurate trajectory prediction
in more complex scene contexts than NGSIM (greater lane
curvature, more entrances and exits, etc.).

to

application
[25]

a
demonstrated

state-of-the-art
comparable

pedestrian
Our
performance
dataset
with previously published results. Although some recent
models achieved greater accuracy than ours [28, 20], all
used dramatically different architectures; it is interesting
to ﬁnd that a novel spatial-centric architecture can also
achieve a high standard of performance.
Future work

should examine the factors that inﬂuence performance, and
the advantages and disadvantages of different architectures.
In future work, we plan to integrate unsupervised learn-
ing of structured maneuver representations into our frame-
work. This will increase the interpretability of our model
predictions, while enabling our model to better capture mul-
timodal structure in the distribution over agent-scene and
agent-agent interactions.

Social trajectory prediction is a complex task, which de-
pends on the ability to extract structure from the scene and
the history of agents’ joint motions. Our central goal here
has been to combine the strengths of agent- and spatial-
centric approaches to this problem. Beyond achieving more
accurate multi-agent trajectory predictions, our belief is that
the work of engineering better models will continue to yield
further insights into the structure of human interaction.

7. Acknowledgements

This work was mainly conducted at ISEE, Inc. with the
support of the ISEE team and ISEE data platform. This
work was supported in part by NSFC-61625201, 61527804.

References

[1] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei,
and S. Savarese. Social lstm: Human trajectory prediction in

12133

crowded spaces. In Proceedings of the IEEE International
Conference on Computer Vision and Pattern Recognition,
2016. 2, 4, 7

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine trans-
In ICLR.

lation by jointly learning to align and translate.
2015. 2

[3] M. Bahram, C. Hubmann, A. Lawitzky, M. Aeberhard,
and D. Wollherr. A combined model and learning based
framework for interaction-aware maneuver prediction. IEEE
Transactions on Intelligent Transportation Systems, 2016. 2
[4] M. Bansal, A. Krizhevsky, and A. S. Ogale. Chauffeurnet:
Learning to drive by imitating the best and synthesizing the
worst. CoRR, abs/1812.03079, 2018. 2

[5] W. Choi and S. Savarese. A uniﬁed framework for multi-
target tracking and collective activity recognition. Computer
VisionECCV, 2012. 2

[6] W. Choi and S. Savarese. Understanding collective activi-
ties of people from videos.
IEEE Transactions on Pattern
Analysis and Machine Intelligence, 36(6):1242–1257, 2014.
2

[7] J. Colyar and J. Halkias. Us highway dataset. Federal High-
way Administration (FHWA), Tech. Rep. FHWA-HRT-07-
030. 2, 4, 5

[8] N. Deo, A. Rangesh, and M. M. Trivedi. How would sur-
round vehicles move? A uniﬁed framework for maneu-
ver classiﬁcation and motion prediction. arXiv:1801.06523,
2018. 2, 4, 5

[9] N. Deo and M. M. Trivedi. Convolutional social pooling for
vehicle trajectory prediction. In IEEE Computer Vision and
Pattern Recognition Workshop on Joint Detection, Tracking,
and Prediction in the Wild, 2018. 2, 3, 4, 5, 8

[10] N. Deo and M. M. Trivedi. Multi-modal trajectory prediction
of surrounding vehicles with maneuver based lstms. In IEEE
Intelligent Vehicles Symposium (IV), 2018. 2, 3

[11] K. Diederik and W. Max. Auto-encoding variational bayes.

In ICLR. 2014. 2, 7

[12] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680. 2014. 2, 4

[13] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi.
Social gan: Socially acceptable trajectories with generative
adversarial networks. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion, 2018. 2, 4, 5, 6, 7

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. Proceedings of the IEEE Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion, 2016. 4

[15] D. Helbing and P. Molnar. Social force model for pedestrian

dynamics. Physical review E, 51(5):4282, 1995. 2, 4, 7

[16] J. Ho and S. Ermon. Generative adversarial imitation learn-
ing. In Advances in Neural Information Processing Systems
29, pages 4565–4573. Curran Associates, Inc., 2016. 2

[17] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. Structural-
rnn: Deep learning on spatio-temporal graphs. In Proceed-
ings of the International Conference on Robotics and Au-
tomation (ICRA) 2018, pages 5308–5317, 2016. 2

[18] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert.

Activity forecasting. In ECCV, 2012. 2

[19] A. Kueﬂer, J. Morton, T. Wheeler, and M. Kochenderfer. Im-
itating driver behavior with generative adversarial networks.
Intelligent Vehicles Symposium (IV), 2017. 2, 4, 5

[20] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, and
M. Chandraker. DESIRE: distant future prediction in dy-
namic scenes with interacting agents.
In 2017 IEEE Con-
ference on Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, July 21-26, 2017, pages 2165–
2174, 2017. 2, 4, 7, 8

[21] A. Lerner, Y. Chrysanthou, and D. Lischinski. Crowds by
example. IEEE 12th International Conference on Computer
Vision (ICCV), 2009. 2, 4, 5

[22] R. Mehran, A. Oyama, and M. Shah. Abnormal crowd be-
IEEE Transac-

havior detection using social force model.
tions on CVPR, 2009. 2

[23] M. Mirza and S. Osindero. Conditional generative adversar-

ial nets. arxiv:1411.1784, 2014. 2, 4

[24] N. Rhinehart, P. Vernaza, and K. Kitani. R2p2: A reparam-
eterized pushforward policy for diverse, precise generative
path forecasting. In European Conference on Computer Vi-
sion (ECCV 2018), Part of the Lecture Notes in Computer
Science book series (LNCS, volume 11217), pages 794 – 811,
October 2018. 2

[25] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese.
Learning social etiquette: Human trajectory prediction in
crowded scenes. European Conference on Computer Vision
(ECCV), 2016. 2, 4, 5, 7, 8

[26] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. CoRR,
abs/1505.04597, 2015. 3

[27] K. S. S. Pellegrini, A. Ess and L. V. Gool. Youll never
walk alone: Modeling social behavior for multi-target track-
ing. IEEE 12th International Conference on Computer Vi-
sion (ICCV), 2009. 2, 4, 5

[28] A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, and
S. Savarese. Sophie: An attentive GAN for predicting paths
compliant to social and physical constraints. arXiv, CoRR,
abs/1806.01482, 2018. 2, 4, 5, 7, 8

[29] A. Sadeghian, F. Legros, M. Voisin, R. Vesel, A. Alahi, and
S. Savarese. Car-net: Clairvoyant attentive recurrent net-
work. arXiv:1711.10061, 2017. 2

[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is
all you need. In Advances in Neural Information Process-
ing Systems 30, pages 5998–6008. Curran Associates, Inc.,
2017. 2

[31] A. Vemula, K. Muelling, and J. Oh. Social attention: Mod-
eling attention in human crowds. In Proceedings of the In-
ternational Conference on Robotics and Automation (ICRA)
2018, May 2018. 2, 7

[32] K. Yamaguchi, A. C. Berg, L. E. Ortiz, and T. L. Berg. Who
are you with and where are you going? IEEE Transactions
on CVPR, 2011. 2

12134

