Learning to Reduce Dual-level Discrepancy for Infrared-Visible Person

Re-identiÔ¨Åcation

Zhixiang Wang1 Zheng Wang2 ‚àó Yinqiang Zheng2 Yung-Yu Chuang1

Shin‚Äôichi Satoh2

3

,

1National Taiwan University

2National Institute of Informatics

3The University of Tokyo

Abstract

Infrared-Visible person RE-IDentiÔ¨Åcation (IV-REID) is
a rising task.
Compared to conventional person re-
identiÔ¨Åcation (re-ID), IV-REID concerns the additional
modality discrepancy originated from the different imaging
processes of spectrum cameras, in addition to the person‚Äôs
appearance discrepancy caused by viewpoint changes, pose
variations and deformations presented in the conventional
re-ID task. The co-existed discrepancies make IV-REID
more difÔ¨Åcult to solve. Previous methods attempt to reduce
the appearance and modality discrepancies simultaneously
using feature-level constraints.
It is however difÔ¨Åcult to
eliminate the mixed discrepancies using only feature-level
constraints. To address the problem, this paper introduces a
novel Dual-level Discrepancy Reduction Learning (D2RL)
scheme which handles the two discrepancies separately.
For reducing the modality discrepancy, an image-level sub-
network is trained to translate an infrared image into its
visible counterpart and a visible image to its infrared ver-
sion. With the image-level sub-network, we can unify the
representations for images with different modalities. With
the help of the uniÔ¨Åed multi-spectral images, a feature-level
sub-network is trained to reduce the remaining appearance
discrepancy through feature embedding. By cascading the
two sub-networks and training them jointly, the dual-level
reductions take their responsibilities cooperatively and at-
tentively. Extensive experiments demonstrate the proposed
approach outperforms the state-of-the-art methods.

1. Introduction

Person re-identiÔ¨Åcation (re-ID) has recently received in-
creasing attention in the computer vision community [18,
17, 6, 7, 20, 28] because of its great importance in video
surveillance. Most current re-ID methods rely on person‚Äôs
appearance under good visible light conditions. Under poor
illumination conditions, due to poor appearance, conven-
tional re-ID models could become ‚Äúblind in dark‚Äù. In prac-

‚àóCorresponding Author

Figure 1. Illustration of the difÔ¨Åculty in the IV-REID task. (Left)
The modality discrepancy Œ¥m is much larger than Œ¥a, the ap-
pearance discrepancy. Thus, the modality discrepancy could re-
sult in the intra-person distance D( , ) being larger than the
). (Right) The performance obtained
inter-person distance D( ,
by [23] on the RegDB dataset using only the feature-level con-
straints. ‚ÄúV-V‚Äù and ‚ÄúIR-IR‚Äù respectively denote the performance
of visible-visible and infrared-infrared re-ID. The red gap indi-
cates the performance gap between visible-visible and infrared-
infrared single-modality re-ID. The black gap denotes the perfor-
mance gap between cross-modality re-ID and single-modality re-
ID. It is clear that the re-ID problem across modalities is much
more difÔ¨Åcult than the one with the same modality.

tice, for dealing with poor illumination, most surveillance
cameras automatically switch from the visible mode to the
infrared mode in the dark [21]. Consequently, this raises
a new task in which, given a visible (or infrared) image of
a speciÔ¨Åc person, the goal is to Ô¨Ånd the corresponding in-
frared (or visible) images of the person captured by other
spectrum cameras [21, 22, 23, 2]. This cross-modality im-
age matching task is named Infrared-Visible person RE-
IDentiÔ¨Åcation (IV-REID).

Compared to the conventional re-ID task, IV-REID en-
counters additional modality discrepancy resulting from the
differences between imaging processes of different spec-
trum cameras, in addition to the person‚Äôs appearance dis-
crepancy caused by viewpoint changes, pose variations,
scale changes and deformations. The modality discrepancy
is often more signiÔ¨Åcant than the appearance discrepancy.

618

ùõøùëéùõøùëöIntra personInter personVisible domainInfrared domainmAP110250050000.00.20.40.60.81.0Training stepsLeft GapInitial GapInitial GapLeft GapV‚ÄìVIR‚ÄìIRIV-REIDing (D2RL). We separate the modality discrepancy apart,
and alleviate it through the image-level discrepancy reduc-
tion sub-network TI which uniÔ¨Åes image representations by
synthesizing multi-spectral images from the given visible or
infrared images. The appearance discrepancy is then han-
dled by the feature-level discrepancy reduction sub-network
TF , where feature embedding is more effective with the uni-
Ô¨Åed representation. These two sub-networks are cascaded
and jointly optimized in an end-to-end manner. To this end,
TF beneÔ¨Åts TI to generate spectral images more discrimina-
tively, and meanwhile TI provides TF with more translated
samples. The contributions of this paper are summarized
below:

‚Ä¢ A novel dual-level discrepancy reduction learning
scheme is introduced. We are the Ô¨Årst to decompose the
mixed modality and appearance discrepancies and handle
them separately.

‚Ä¢ Our end-to-end scheme enforces these two sub-networks
beneÔ¨Åt each other. The balance between them affects the
performance.

‚Ä¢ Extensive experiments on two datasets demonstrate the
superior performance of our proposed approach com-
pared to the state-of-the-art methods.

2. Related Work

Single-modality re-ID. The conventional re-ID researches
mainly focus on the challenges of appearance changes in
the single visible modality, such as image misalignment [6],
viewpoint variations [11] and scale changes [16, 19]. Li et
al. [6] formulated a harmonious attention CNN model for
joint learning of pixel and regional attention to optimize re-
ID performance with misaligned images. Liu et al. [11] pro-
posed a pose transferrable framework for generating sam-
ples with rich pose variations. Wang et al. [16] combined
effective embedding schemes built on multiple layers from
high- and low-level details. Wang et al. [19] cascaded
multiple super-resolution networks to overcome the resolu-
tion misalignment problem. Existing state-of-the-art single-
modality re-ID methods are very effective in reducing the
appearance discrepancy as their retrieval accuracy has al-
ready surpassed the accuracy of human [24].
Infrared-visible re-ID. For the IV-REID problem, in ad-
dition to the appearance discrepancy, the modality discrep-
ancy needs to be addressed. Existing methods attempt to re-
duce the mixed appearance and modality discrepancies us-
ing feature embedding frameworks similar to conventional
re-ID methods. Wu et al. [21] proposed a deep zero-padding
framework for shared feature learning under two different
modalities. Ye et al. [22] introduced a two-step framework
for feature learning and metric learning. They [23] also

619

Figure 2. A high-level overview of our approach. Previous meth-
ods reduce the total discrepancy by converting images to the fea-
ture space and using only the feature-level constraint (the blue
dashed line). Our dual-level solution Ô¨Årst converts images into
a uniÔ¨Åed space (the green arrows) and then embeds them into the
feature space (the orange arrow). After the image-level modal-
ity reduction by unifying image representations, the gap becomes
much smaller than the one in the original image space. Thus, the
feature-level embedding can be effective in reducing the remaining
appearance discrepancy.

The intra-person (images from the same person) distance
across visible and infrared cameras is often larger than the
inter-person (images from different persons) distance of the
same type of cameras. While the main goal of IV-REID is
still to maximize the inter-person distance and meanwhile to
minimize the intra-person distance. The co-existed modal-
ity and appearance discrepancies make IV-REID difÔ¨Åcult
(as Figure 1 shows). As far as we know, all previous meth-
ods [21, 22, 23, 2] regard the modality discrepancy Œ¥m as a
part of the appearance discrepancy Œ¥a and attempt to reduce
the mixed discrepancy Œ¥m+Œ¥a using feature-level constraints
employed by most conventional re-ID methods. Note that
the performance gap between single-modality re-ID (‚ÄúV-V‚Äù
or ‚ÄúIR-IR‚Äù) and cross-modality IV-REID is extremely large.
It indicates that the modality discrepancy cannot be effec-
tively eliminated using only feature-level constraints.

Figure 2 gives the main idea of the proposed method.
Because they are taken with different modalities, infrared
images and visible images have quite different appearances.
Thus, direct mappings of them into the feature space can not
be effective. For alleviating the problem, we propose to re-
duce the modality discrepancy Ô¨Årst by unifying the image
representations using image-level conversion. More specif-
ically, we form a multi-spectral image by augmenting an in-
frared image with its visible counterpart or a visible image
with its infrared version. In the uniÔ¨Åed space, the appear-
ance discrepancy is greatly alleviated. Thus, conventional
re-ID methods can be effective in reducing the remaining
appearance discrepancy by feature-level constraints.

With the idea in mind, we propose a novel dual-level
solution, named Dual-level Discrepancy Reduction Learn-

Large GapImage spaceUnified spaceFeature spaceImage-level reductionFeature-level reductionMixed discrepencyFeature-level discrepencyStateLegendxyproposed an end-to-end dual-path network to learn com-
mon representations. Dai et al. [2] designed a network to
learn discriminative representations from different modali-
ties. Due to the modality discrepancy brought in by differ-
ent spectrum with dramatic image-level unbalance, all these
feature-level methods cannot obtain satisfactory results.
Image generation meets re-ID. Recently developed GANs
provide a powerful tool for image translation [29, 12, 1].
A lot of researches attempted to utilize GANs to generate
more training samples and then facilitate solving the con-
ventional re-ID problem. Ma et al. [13] manipulated the
foreground, background and pose information, and gener-
ated images based on manipulated information. Li et al. [7]
used the GAN to generate target-like images. There is an-
other category of researches trying to deal with the prob-
lem of the domain gap using GANs. For the conventional
re-ID task, the domain gap mainly lies in the camera style
or illumination differences. Zhong et al. [28, 27] utilized
the CycleGAN with label smooth regularization to gener-
ate person images with different camera styles. Deng et
al. [4] also exploited the CycleGAN with self-similarity and
domain-dissimilarity constraints. Li et al. [7] exploited the
CycleGAN to generate images under different illumination
conditions. With the similar idea, Wei et al. [20] proposed
a person transfer GAN to bridge the domain gap. However,
working towards the problem of different poses, illumina-
tions, and camera styles, all these methods focus on gener-
ating visible images based on visible images.

3. Proposed Method

Let X = {x | x ‚àà RH√óW√ó3} and Y = {y | y ‚àà RH√óW√ó1}
denote the visible image set and the infrared image set re-
spectively, where H and W are the height and the width of
images. Each image x ‚àà X or y ‚àà Y corresponds to a label
l ‚àà {1, 2, . . . , Np}, and Np is the number of person identi-
ties. Given an infrared (or visible) query image y (or x) and
a visible (or infrared) gallery set X (or Y ), the goal of the
IV-REID task is to propose a ranking list R of the gallery
set, in which the images with the same identity as the query
image should be ranked to the top. A common strategy is
projecting x and y to a feature space through feature em-
bedding, fx = hx(x) and fy = hy(y), where fx ‚àà Rd and
fy ‚àà Rd, and then generating the ranking list R using the
distance between them, f T

x fy.

We propose a new strategy to replace the direct map-
ping function hx and hy. Figure 3 shows the framework
of our proposed method. It consists of two sub-networks:
(1) the image-level discrepancy reduction sub-network TI
for reducing the modality discrepancy, and (2) the feature-
level discrepancy reduction sub-network TF for reducing
the appearance discrepancy. These two sub-networks are
cascaded and jointly optimized in an end-to-end manner. In
the following, we describe their details.

3.1. Image level discrepancy reduction ‚Äî TI

To reduce the modality discrepancy, TI exploits two
variational autoencoders (VAEs) for style disentanglement
followed by two GANs for domain speciÔ¨Åc image genera-
tion. TI translates the visible (infrared) image x (y) to its
infrared (visible) counterpart ÀÜx ( ÀÜy). Together, they form the
multi-spectral image [x, ÀÜx] (or [ ÀÜy, y]) to provide a uniÔ¨Åed
representation for reducing the modality discrepancy.
Style disentanglement. It consists of two encoder-decoder
pairs: VAEv = {Ev, Gv} and VAEi = {Ei, Gi}, that are re-
sponsible for visible and infrared modality disentanglement
respectively. For VAEv, given a visible input x ‚àà X, the
encoder Ev Ô¨Årst maps x to the latent vector z, and then the
decoder Gv reconstructs the input from the latent vector z.
The reconstructed image is ÀÜxv‚Üív = Gv(zv ‚àº qv(zv|x)),
where qv(zv|x) is the distribution of latent information zv.
The loss for VAEv is deÔ¨Åned as

LVAEv (Ev, Gv) = Œª0KL(qv(zv|x)||pŒ∑(z))‚àí

Œª1E

zv‚àºqv(zv|x) [kx ‚àí Gv(zv)k1] ,

(1)

where the hyper-parameters Œª0 and Œª1 control the weights
of the objective terms, and the Kullback-Leibler divergence
term (KL) penalizes deviation between the distribution of
the latent information and the prior pŒ∑(z) which is a zero-
mean Gaussian distribution. The ‚Ñì1 loss penalizes dissimi-
larity between the image and the reconstructed image, and
also encourages sharp output images.
Domain speciÔ¨Åc image generation. Two generative adver-
sarial networks GANv = {Gv, Dv} and GANi = {Gi, Di}
are employed to generate domain speciÔ¨Åc images from the
In GANv, the generator Gv is
style-free latent vector z.
expected to generate realistic visible images from the latent
vector z that can fool the discriminator Dv, while the dis-
criminator Dv is expected to discriminate real and synthetic
visible images. Adversarial losses are utilized to play the
minimax game, which can be expressed as

LGANv (Ei, Gv, Dv) = Œª2Ex‚àºPX [log Dv(x)]+

Œª2E

zi‚àºqi(zi|y)[log(1‚àíDv(Gv(zi)))] ,

(2)

where the hyper-parameter Œª2 controls the impact of GAN.
The discriminator is trained to maximize Equation (2) while
the generator tries to minimize it. The loss is used to en-
sure the translated images resemble images in the visible
domain.
Cycle-consistency. The cycle consistency is used to fur-
ther regularize the ill-posed unsupervised image-to-image
translation problem. Similar to CycleGAN [29], our cycle-
consistency loss is deÔ¨Åned as

LCCv (Ev, Gv, Ei, Gi) =

Œª3E

zi‚àºqi(zi|xv‚Üíi) [kx ‚àí Gv(zi)k1] ,

(3)

620

Figure 3. The framework of our proposed method. The image-level discrepancy reduction sub-network TI Ô¨Årst projects inputs from the
image space (visible or infrared modality) to the uniÔ¨Åed space, where the modality discrepancy is alleviated. Then, the feature-level
discrepancy reduction sub-network TF is utilized to eliminate the remaining appearance discrepancy. The two sub-networks are cascaded
and jointly optimized in an end-to-end manner.

where the negative log-likelihood objective term ensures
a twice translated image resembles the input. The hyper-
parameter Œª3 controls the weight of this objective term.

The losses LVAEi (Ei, Gi), LGANi (Ev, Gi, Di) and
LCCi (Ei, Gi, Ev, Gv) can be similarly deÔ¨Åned. More
speciÔ¨Åcally, they are deÔ¨Åned by substituting the subscript
i for v, v for i, and y for x in Equation (1), Equation (2)
and Equation (3).
Modality uniÔ¨Åcation. There are three possible options for
modality uniÔ¨Åcation, i.e., unifying images to the infrared
modality, the visible modality or the multi-spectral modal-
ity. We choose to generate the multi-spectral images for
modality uniÔ¨Åcation for two reasons. First, the infrared and
visible images are two representations of the same reÔ¨Çected
light of the same person due to different imaging processes.
They are potentially related and it is likely for them to re-
construct each other. Second, if we unify the images to the
visible or infrared modality, some distinctive information in
the visible or infrared modality may be lost.
Objective for training TI . The total loss is a combination
of the VAE loss, the GAN loss and the CC loss:

LI = LVAEv +LVAEi +LGANv +LGANi +LCCv +LCCi . (4)

By optimizing the above loss, we obtain a network TI which
is able to translate a visible image x into its infrared counter
part ÀÜx and translate an infrared image y into its visible
counter part ÀÜy. Thus, we can form a training set S by con-
structing multi-spectral images, uv = [x, ÀÜx] and ui = [ ÀÜy, y],
as the uniÔ¨Åed representations. This way, all images includ-

ing both query and gallery images are represented in the
same way and the modality discrepancy is greatly reduced.

3.2. Feature level discrepancy reduction ‚Äî TF

Since TI has uniÔ¨Åed all the images to the same modality,
a feature embedding network could be sufÔ¨Åcient for reduc-
ing the appearance discrepancy. For each batch, we use TI
to generate a sample set S. The feature-level discrepancy
reduction network TF plays a role of feature learning on
the uniÔ¨Åed multi-spectral images generated by TI . Given a
multi-spectral image u, sampled from S, the deep feature
extractor F : u ‚Üí f maps it to the person descriptor f . In
particular, we use ResNet-50 as the backbone network of
F and follow the training strategy in [27]. The last 1000-
d fully connected (FC) layer is replaced with a new layer
named as ‚ÄúFC-1024‚Äù. The person descriptor f ‚àà R1024 uses
the output feature vector of ‚ÄúFC-1024‚Äù followed by Batch
Normalization, ReLU and Dropout. The output f of the
FC-1024 layer is then fed to two independent FC layers Ht
and Hc for generating two feature vectors ft ‚àà R128 and
fc ‚àà RNp . Two types of loss functions are utilized to su-
pervise the training of TF . One is the triplet loss, employed
for identity information learning, and the other is the cross-
entropy loss, used for similarity learning. The triplet loss is
coupled with ft while the cross-entropy loss is tied with fc.

Triplet loss. It is used for similarity learning. It tries to re-
duce the feature distances between images of the same per-
son and expand the distances between images of different

621

Ev‚ÅûImage SpaceUnified SpaceFeature SpacezÀÜxxyCrossTripletN/YDeep feature extractorEncoderGeneratorDiscriminatorVisible imageInfrared imageMulti-spectral imageFeature vectorxyÀÜyorÀÜÀÜ[,]  [,]xxyyfftfcFDGEN/Ypeople. The triplet loss can be formulated as following:

F = X
LT
t ,f n

t ,f p
f a

t ‚ààS

[D(f a

t , f p

t ) ‚àí D(f a

t , f n

t ) + Œæ]+ ,

(5)

t is the anchor point; f p
t ; and f n

where f a
t is a positive sample with
the same identity with f a
t is a negative sample with
the different identity from f a
t . Œæ is a
margin parameter. D(¬∑) calculates the Euclidean distance,
and [d]+ = max(d, 0) truncates negative numbers to zero
while keeping positive numbers the same.
Cross-entropy loss. It is employed for identity learning and
is written as

t . Note that f a

t 6= f p

LC

F = ‚àí

1
Nb

Nb

X

j=1

log pj ,

(6)

where Nb = |S| is the number of images in the training mini-
batch; p is the predicted probability of the input belonging
to the ground-truth class with p = softmax(Wf + b),
where W and b are the trainable weight and bias of Hc.
Objective for training TF . The loss is a combination of
the cross-entropy and triplet losses as follows:

LF = Œª4LC

F + Œª5LT

F .

(7)

3.3. End to end joint training

We optimize our network in an end-to-end manner, by

cascading TI and TF and minimizing the combined loss:

arg min
Œ∏TI ,Œ∏TF

(1 ‚àí Œ≥)LI + Œ≥LF ,

(8)

where 0 < Œ≥ < 1 and it is a trade-off parameter for balancing
the contributions of two sub-networks TI and TF .

4. Experiments

This section reports the experiment settings, implemen-
tation details, comparisons with other methods, the ablation
study and analysis of our method.

4.1. Experiment settings

Datasets. We evaluated our method on two publicly avail-
able datasets: RegDB [15] and SYSU-MM01 [21].
‚Ä¢ RegDB [15].

It was collected from two aligned cam-
eras (one visible and one far-infared).
It contains to-
tally 412 persons. Each person has 10 visible images and
10 far-infrared images. We follow the evaluation proto-
col in [23] to randomly split the dataset into two halves,
which are used for training and testing respectively.

‚Ä¢ SYSU-MM01 [21]. It is a large-scale dataset collected
by six cameras (four visible and two near-infared), in-
cluding both indoor and outdoor environments. It con-
tains in total 491 persons, and each person was captured

by at least two different cameras. Following [21], we
adopt the most challenging single-shot all-search mode
evaluation protocol. The training set contains 395 per-
sons, with 22,258 visible images and 11,909 infrared im-
ages. The testing set contains 96 persons, with 3,803 in-
frared images for query and 301 randomly selected visi-
ble images as the gallery set.

Evaluation metrics. The standard Cumulative Matching
Characteristics (CMC) curve and mean Average Precision
(mAP) are adopted to evaluate the performance. Note that
there is a slight difference with the conventional re-ID prob-
lem [25]. Images from one modality are used as the gallery
set while the ones from the other modality as the probe set
during testing.

4.2. Implementation details

Network architecture. The architecture of our proposed
method is shown in Figure 3. The sub-network TI is based
on UNIT1. The size of inputs and outputs is resized to
228√ó228√ó3, for both visible and infrared images. For in-
frared images, the three channels are the same. TF is based
on Open-reid2 with the difference that our inputs have four
channels.
Training strategy.
In order to avoid mode collapse and
over-Ô¨Åtting, we pretrained the sub-network TI and TF re-
spectively with the Market-1501 dataset [25], where we
used the original images as the visible input, and the decom-
posed illuminations as the infrared input. Then, we jointly
trained them in an end-to-end manner. Note that the SYSU-
MM01 [21] dataset includes outdoor and indoor scenes. We
trained them separately. We set weight parameters of the
losses in TI as Œª0 = 0.1, Œª1 = 100, Œª2 = 10, Œª3 = 100 by fol-
lowing [12]. For the sub-network TF , we set Œª4 = Œª5 = 10.
The pre-deÔ¨Åned margin for the triplet loss is set as Œæ = 0.8.
The model is optimized using Adam [5] with a learning rate
of 0.0002 and the momentum terms Œ≤1 = 0.5, Œ≤2 = 0.999.

4.3. Comparison with the state of the art methods

To demonstrate the effectiveness of our method, we
compare our method with most of the related methods
for IV-REID. These methods include Zero-Padding [21],
TONE [22], HCML [22], BDTR [23] and cmGAN [2]. In
addition, several other learning-based methods are also in-
cluded for comparisons. The additional competing methods
contain some feature learning methods including HOG [3],
LOMO [8], one-stream and two-stream networks [21]. The
one-stream and two-stream networks are modiÔ¨Åcations of
the IDE method [26] under IV-REID settings. Their detailed
descriptions can be found in [21]. In addition, two match-
ing model learning methods, MLAPG [9] and GSM [10],

1UNIT code: https://github.com/mingyuliutw/UNIT
2Open-reid code: https://github.com/Cysu/open-reid

622

Table 1. Comparison with the state-of-the-art IV-REID methods on two different datasets, RegDB and SYSU-MM01.

Approach

LOMO [8]
MLBP [9]
HOG [3]
GSM [10]
One-stream [21]
Two-stream [21]
Zero-Padding [21]
TONE [22]
HCML [22]
BDTR [23]
cmGAN [2]
Proposed D2RL

Constraints

RegDB

SYSU-MM01

Feature-level

Image-level

CMC-1

CMC-10

CMC-20

mAP

CMC-1

CMC-10

CMC-20

mAP

‚úó

‚úó

‚úó

‚úó

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úó

‚úó

‚úó

‚úó

‚úó

‚úó

‚úó

‚úó

‚úó

‚úó

‚úó

‚úì

0.85
2.02
13.49
17.28
13.11
12.43
17.75
16.87
24.44
33.47

‚Äì

43.4

2.47
7.33
33.22
34.47
32.98
30.36
34.21
34.03
47.53
58.42

‚Äì

66.1

4.10
10.90
43.66
45.26
42.51
40.96
44.35
44.10
56.78
67.52

‚Äì

76.3

2.28
6.77
10.31
15.06
14.02
13.42
18.90
14.92
20.80
31.83

‚Äì

44.1

1.75
2.12
2.76
5.29
12.04
11.65
14.80
12.52
14.32
17.01
26.97
28.9

14.14
16.23
18.25
33.71
49.68
47.99
54.12
50.72
53.16
55.43
67.51
70.6

26.63
28.32
31.91
52.95
66.74
65.50
71.33
68.60
69.17
71.96
80.56
82.4

3.48
3.86
4.24
8.00
13.67
12.85
15.95
14.42
16.16
19.66
27.80
29.2

Table 2. Ablation study on the RegDB dataset.

Method

Components

RegDB

VAE CC CE triplet CMC-1 (%) mAP (%)

Baseline
D2RL (no VAE)
D2RL (no CC)
D2RL (no CE)
D2RL (no triplet)
D2RL

‚úì

‚úó

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úó

‚úì

‚úì

‚úì

‚úó

‚úì

‚úì

‚úó

‚úì

‚úì

‚úó

‚úì

‚úì

‚úì

‚úó

‚úì

28.5
34.8
33.7
41.7
39.5
43.4

23.8
31.3
29.9
40.6
37.4
44.1

are also included for comparisons. Table 1 presents the re-
sults of all the methods. The methods specially designed
for IV-REID generally perform much better than the ones
that are not designed for IV-REID. Our method signiÔ¨Åcantly
outperforms the state-of-the-art IV-REID methods on both
the RegDB and SYSU-MM01 datasets.

4.4. Ablation study

Our method consists of two sub-networks, the image-
level discrepancy reduction sub-network TI and the feature-
level discrepancy reduction sub-network TF , respectively
taking GAN and ResNet-50 as their backbones. TI is
mainly conÔ¨Ågured with the VAE and cycle-consistency
(CC) losses while TF is optimized with the cross-entropy
(CE) and triplet losses. For the ablation study, Table 2 re-
ports the resultant CMC-1 and mAP values on the RegDB
dataset by removing one loss at a time. Note that the
baseline is obtained by only using TF without image-level
modality uniÔ¨Åcation.

Note that the Ô¨Årst two losses, VAE (for modality disen-
tanglement) and cycle-consistency (for modality transfer),
are responsible for the image-level modality uniÔ¨Åcation.
Removing either of them affects the image generation, thus
degrading the performance more signiÔ¨Åcantly. When re-
moving both of them (the baseline), the performance drops
dramatically to 28.5% in CMC-1 as it can only rely on fea-
ture embedding across modalities. The triplet loss is slightly
more effective than the cross-entropy loss.

4.5. Discussions

Why reducing discrepancy separately? For IV-REID,
previous methods try to reduce the appearance and modal-
ity discrepancies together from the feature-level view. Our
method aims at reducing the appearance and modality dis-
crepancies separately. We compare a feature-level method
BDTR [23] with our proposed dual-level discrepancy re-
duction method D2RL for investigating which strategy is
more effective. We evaluate both BDTR and our method
on the RegDB dataset. First, we plot the 1024-d person de-
scriptor in the 2D feature space for visualization using the
t-SNE method [14]. Testing samples of 20 persons were
randomly selected from the RegDB dataset. Figure 4(a)
and Figure 4(b) stand for visualizations of the initial and
best results of the BDTR model respectively. We can ob-
serve that single-modality intra-person samples get closer
to each other after training, but cross-modality intra-person
samples relatively do not change too much. Figure 4(c) and
Figure 4(d) are visualizations of the initial and best results
of our proposed D2RL network respectively. From these
Ô¨Ågures, we can Ô¨Ånd that not only the single-modality intra-
person samples get closer to each other after training, but
also some cross-modality intra-person samples move closer
(as indicated by the red cycles in Figure 4(d)).

For further validating the effectiveness of reducing dis-
crepancy separately, we conduct experiments to see how
effective the embedded features are by again comparing
BDTR and our method on the RegDB dataset. Figure 4(e)
shows the initial distribution of distances of the inter-person
and intra-person pairs for BDTR. Figure 4(f) shows the dis-
tribution after 4,800 training steps. Figure 4(g) and (h) show
the distributions before and after training for our method.
It is clear that, after training, our method can separate the
inter-person and intra-person pairs more further apart than
BDTR. It indicates that feature embedding in the uniÔ¨Åed
space is more effective than the one in the image space.
Which modality to unify? We evaluate three options for
modality uniÔ¨Åcation, i.e., the visible modality, the infrared

623

(a) BDTR initial

(b) BDTR best

(c) Ours initial

(d) Ours best

(e) BDTR initial

(f) BDTR best

(g) Ours initial

(h) Ours best

Figure 4. (Top) Visualization of the feature space. A total of 20 persons are randomly selected from the testing set. Samples with the same
color indicate they are of the same person. The markers ‚Äúdot‚Äù and ‚Äúcross‚Äù denote images from the visible and infrared domain respectively.
(a-b) are obtained using BDTR [23] on the RegDB datasetm which uses only feature-level constraints; (c-d) are obtained by our method.
(Bottom) Histogram of the feature distances. (e-f) are obtained by BDTR; (g-h) are obtained by our method.

Table 3. Comparison of different modality uniÔ¨Åcation options.

Table 4. Comparison between joint and separate training.

Metrics (%) mAP

CMC-1 mAP

RegDB

SYSU-MM01
CMC-1

Metrics (%) mAP

CMC-1 mAP

RegDB

SYSU-MM01
CMC-1

D2RL(v)
D2RL(i)
D2RL

36.4
43.6
44.1

39.1
42.9
43.4

28.4
27.8
29.2

28.1
27.4
28.9

Separate
Joint

40.7
44.1

39.9
43.4

25.7
29.2

26.1
28.9

modality and the multi-spectral modality. We respectively
use D2RL(v), D2RL(i) and D2RL to denote these three op-
tions. Table 3 shows the results and there are several ob-
servations. First, uniÔ¨Åcation to the multi-spectral modal-
ity, D2RL, performs the best. Second, compared with other
methods in Table 1, modality uniÔ¨Åcation helps no mat-
ter which modality is chosen for uniÔ¨Åcation. Finally, we
Ô¨Ånd that D2RL(i) performs better than D2RL(v) on the
RegDB dataset, while performing worse on the SYSU-
MM01 dataset. We attribute this phenomenon to the set-
ting for dataset evaluation. For the RegDB dataset, the
gallery consists of infrared images, implying that the in-
frared modality plays an important role. The majority of
original infrared images makes D2RL(i) more effective on
the dataset. As for the SYSU-MM01 dataset, the gallery
consists of visible images, the results go the other way
round. Our uniÔ¨Åcation to the multi-spectral modality takes
advantages of both domains and is thus more robust.
Why joint training? The whole framework consists of
an image-level discrepancy reduction sub-network and a
feature-level discrepancy reduction sub-network. They play
different roles. They can be trained separately or jointly.
Table 4 compares these two options. First, joint train-
ing provides signiÔ¨Åcant performance boost as the two sub-

networks beneÔ¨Åts each other. Second, when comparing with
other methods in Table 1, even with the separate training,
our method outperforms the state-of-the-art methods.
How to balance sub-networks TI and TF ? In the total
loss of the proposed method deÔ¨Åned in Equation (8), we
use the weight Œ≥ to balance the contributions of TI and TF .
As TI focuses on the modality discrepancy reduction and
TF pays attention to the appearance discrepancy reduction,
the larger Œ≥ is, the more contribution will attribute to the
appearance reduction, in other words, the feature-level dis-
crepancy reduction sub-network.

Figure 5 shows the results of the mAP and CMC-1 val-
ues on RegDB dataset by varying the weight Œ≥. We can Ô¨Ånd
that the re-identiÔ¨Åcation accuracy varies when the weight Œ≥
changes, and there exists a suitable value to balance the con-
tributions of TI and TF . Although TI alleviates the modal-
ity discrepancy, it could also brings in noisy information.
Thus, the balance between TI and TF is important.

4.6. Visualization of results

The ability of modality uniÔ¨Åcation. To demonstrate the
effectiveness of our image-level discrepancy reduction sub-
network TI , we show some visual results of image transla-
tion in Figure 6. For each of the RegDB and SYSU-MM01
datasets, we show six groups of images. Each group has

624

0.00.20.40.60.81.01.21.41.6dist.0100200300400500600700#pairsintra-personinter-person0.00.20.40.60.81.01.21.41.6dist.0100200300400500600700#pairsintra-personinter-person0.00.20.40.60.81.01.21.41.6dist.0100200300400500600700#pairsintra-personinter-person0.00.20.40.60.81.01.21.41.6dist.0100200300400500600700#pairsintra-personinter-person)

%

(
 
P
A
m

50

45

40

35

30

25

20

44.1

43.443

41.6

41.2

39.7

40.7

Baseline
Ours

36.6

33.9

23.8

0

0.2

0.4
Parameter 

0.6

0.8

1

)

%

(
 

1
-
p
o
t
 

C
M
C

50

45

40

35

30

25

20

43.4

41.7

41

42.9

41.8

40.239.4

36.4

32.8

28.5

Baseline
Ours

0

0.2

0.4
Parameter 

0.6

0.8

1

Figure 5. Curves of mAP (left) and CMC-1 (right) with respect to
the hyper-parameter Œ≥ on RegDB dataset.

(a) RegDB

(b) SYSU-MM01

Figure 6. Examples of translated images generated by TI on (a)
RegDB and (b) SYSU-MM01. For each dataset, from left to right,
the four images of a row are the original visible image, the gener-
ated infrared image, the original infrared image, and the generated
visible image respectively. The original visible and infrared im-
ages of the same row have the same identity.

four images: the original visible image, the generated in-
frared image, the original infrared image and the generated
visible images. From the visual examples, we can observe
that the sub-network TI is good at translating visible im-
ages to infrared ones, and the effectiveness of translating
infrared images to visible ones is acceptable. However,
some generated images could have color distortion, such as
the sixth person of Figure 6(a). We can also Ô¨Ånd that the
translation results of the RegDB dataset looks better than
those of the SYSU-MM01 dataset. It is because the SYSU-

(Top) The query set is
Figure 7. Visualization of failure cases.
[ ÀÜy; y] and the gallery set is [x; ÀÜx]; (Bottom) The query set is [x; ÀÜx]
and the gallery set is [ ÀÜy; y].

MM01 dataset is more colorful, and the person images are
not aligned well with different postures and scales. It may
lead to difÔ¨Åculties for the image-level discrepancy reduc-
tion sub-network to be trained well. However, note that the
Ô¨Ånal goal is not to generate images with good visual appear-
ances, but to have good retrieval results. From the results in
Table 1, the translated images do help a lot on IV-REID.
Failure cases. We select the two worst query results (in
which none of the top-10 results is correct) for illustrating
the failure cases. For each query, the two rows respectively
show the ranking list of generated images and the list of cor-
responding original images. It shows that, for some cases,
the generated images could be bad and image-level modal-
ity uniÔ¨Åcation can not work well on these queries.

5. Conclusions

In this paper, we present Dual-level Discrepancy Reduc-
tion Learning network (D2RL) for the IV-REID task that
exhibits both modality discrepancy and appearance discrep-
ancy. Unlike previous IV-REID methods, instead of han-
dling the mixed discrepancy with feature embedding, we
propose to handle the discrepancies separately. We propose
an image-level sub-network for modality uniÔ¨Åcation, which
generates a uniÔ¨Åed multi-spectral representation by image
translation. With the uniÔ¨Åed representations, a feature-level
sub-network can better reduce the appearance discrepancy
by feature embedding. The proposed method shows signif-
icant improvement against the state-of-the-art methods.
Acknowledgements. This work was Ô¨Ånished when Zhix-
iang Wang visited the Optical Sensing and Camera Sys-
tem Laboratory, National Institute of Informatics, Japan,
through the NII International Internship Program. This
work was supported by JST CREST (JPMJCR1686), Grant-
in-Aid for JSPS Fellows (18F18378), NSFC (61801335),
MOST (107-2221-E-002-147-MY3), and Microsoft Re-
search Asia.

625

                Top-10 RankingQueryÀÜxxÀÜyyGTÀÜxxÀÜyyReferences

[1] Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung-
Yu Chuang. Deep photo enhancer: Unpaired learning for im-
age enhancement from photographs with GANs. In Proceed-
ings of Conference on Computer Vision and Pattern Recog-
nition, 2018.

[2] Pingyang Dai, Rongrong Ji, Haibin Wang, Qiong Wu, and
Yuyu Huang. Cross-modality person re-identiÔ¨Åcation with
generative adversarial training.
In Proceedings of Interna-
tional Joint Conferences on ArtiÔ¨Åcial Intelligence, 2018.

[3] Navneet Dalal and Bill Triggs. Histograms of oriented gra-
dients for human detection. In Proceedings of Conference on
Computer Vision and Pattern Recognition, 2005.

[4] Weijian Deng, Liang Zheng, and Jianbin Jiao.

Image-
image domain adaptation with preserved self-similarity and
domain-dissimilarity for person reidentiÔ¨Åcation. In Proceed-
ings of Conference on Computer Vision and Pattern Recog-
nition, 2018.

[5] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. arXiv:1412.6980, 2014.

[6] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious at-
tention network for person re-identiÔ¨Åcation. In Proceedings
of Conference on Computer Vision and Pattern Recognition,
2018.

[7] Xiang Li, Ancong Wu, and Wei-Shi Zheng. Adversarial
open-world person re-identiÔ¨Åcation. In Proceedings of Eu-
ropean Conference on Computer Vision, 2018.

[8] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z. Li. Per-
son re-identiÔ¨Åcation by local maximal occurrence represen-
tation and metric learning. In Proceedings of Conference on
Computer Vision and Pattern Recognition, 2015.

[9] Shengcai Liao and Stan Z. Li. EfÔ¨Åcient PSD constrained
asymmetric metric learning for person re-identiÔ¨Åcation. In
Proceedings of International Conference on Computer Vi-
sion, 2015.

[10] Liang Lin, Guangrun Wang, Wangmeng Zuo, Xiangchu
Feng, and Lei Zhang. Cross-domain visual matching via
generalized similarity measure and feature learning.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
39(6):1089‚Äì1102, 2017.

[11] Jinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo
Pose transferrable person re-
In Proceedings of Conference on Computer

Cheng, and Jianguo Hu.
identiÔ¨Åcation.
Vision and Pattern Recognition, 2018.

[12] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsuper-
vised image-to-image translation networks. In Proceedings
of Neural Information Processing Systems Conference, 2017.
[13] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc
Van Gool, Bernt Schiele, and Mario Fritz. Disentangled
person image generation. In Proceedings of Conference on
Computer Vision and Pattern Recognition, 2018.

[14] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of Machine Learning Research,
9(Nov):2579‚Äì2605, 2008.

[15] Dat Tien Nguyen, Hyung Gil Hong, Ki Wan Kim, and
Kang Ryoung Park. Person recognition system based on a
combination of body images from visible light and thermal
cameras. Sensors, 17(3):605, 2017.

[16] Yan Wang, Lequn Wang, Yurong You, Xu Zou, Vincent
Chen, Serena Li, Gao Huang, Bharath Hariharan, and Kil-
ian Q. Weinberger. Resource aware person re-identiÔ¨Åcation
across multiple resolutions. In Proceedings of Conference on
Computer Vision and Pattern Recognition, 2018.

[17] Zheng Wang, Ruimin Hu, Chen Chen, Yi Yu, Junjun Jiang,
Chao Liang, and Shin‚Äôichi Satoh. Person reidentiÔ¨Åcation via
discrepancy matrix and matrix metric. IEEE Transactions on
Cybernetics, 2018.

[18] Zheng Wang, Ruimin Hu, Chao Liang, Yi Yu, Junjun Jiang,
Mang Ye, Jun Chen, and Qingming Leng. Zero-shot person
re-identiÔ¨Åcation via cross-view consistency. IEEE Transac-
tions on Multimedia, 18(2):260‚Äì272, 2016.

[19] Zheng Wang, Mang Ye, Fan Yang, Xiang Bai, and Shin‚Äôichi
Satoh. Cascaded SR-GAN for scale-adaptive low resolu-
tion person re-identiÔ¨Åcation. In Proceedings of International
Joint Conferences on ArtiÔ¨Åcial Intelligence, 2018.

[20] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Per-
son transfer GAN to bridge domain gap for person re-
identiÔ¨Åcation.
In Proceedings of Conference on Computer
Vision and Pattern Recognition, 2018.

[21] Ancong Wu, Wei-Shi Zheng, Hong-Xing Yu, Shaogang
Gong, and Jianhuang Lai. RGB-Infrared cross-modality per-
son re-identiÔ¨Åcation. In Proceedings of International Con-
ference on Computer Vision, 2017.

[22] Mang Ye, Xiangyuan Lan, Jiawei Li, and Pong C. Yuen. Hi-
erarchical discriminative learning for visible thermal person
re-identiÔ¨Åcation. In Proceedings of AAAI Conference on Ar-
tiÔ¨Åcial Intelligence, 2018.

[23] Mang Ye, Zheng Wang, Xiangyuan Lan, and Pong C. Yuen.
Visible thermal person re-identiÔ¨Åcation via dual-constrained
top-ranking. In Proceedings of International Joint Confer-
ences on ArtiÔ¨Åcial Intelligence, 2018.

[24] Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun,
Qiqi Xiao, Wei Jiang, Chi Zhang, and Jian Sun. Aligne-
dreid: Surpassing human-level performance in person re-
identiÔ¨Åcation. arXiv:1711.08184, 2017.

[25] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiÔ¨Åcation:
A benchmark. In Proceedings of International Conference
on Computer Vision, 2015.

[26] Liang Zheng, Yi Yang,

and Alexander G Haupt-
mann. Person re-identiÔ¨Åcation: Past, present and future.
arXiv:1610.02984, 2016.

[27] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-
alizing a person retrieval model hetero- and homogeneously.
In Proceedings of European Conference on Computer Vision,
2018.

[28] Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li,
Camera style adaptation for person re-
In Proceedings of Conference on Computer

and Yi Yang.
identiÔ¨Åcation.
Vision and Pattern Recognition, 2018.

[29] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of Interna-
tional Conference on Computer Vision, 2017.

626

