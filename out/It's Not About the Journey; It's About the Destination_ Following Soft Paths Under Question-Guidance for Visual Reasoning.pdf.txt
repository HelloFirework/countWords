It’s not about the Journey; It’s about the Destination:

Following Soft Paths under Question-Guidance for Visual Reasoning

Monica Haurilet

Alina Roitberg

Rainer Stiefelhagen

Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany
{haurilet, alina.roitberg, rainer.stiefelhagen}@kit.edu

Abstract

Visual Reasoning remains a challenging task, as it has
to deal with long-range and multi-step object relationships
in the scene. We present a new model for Visual Reason-
ing, aimed at capturing the interplay among individual ob-
jects in the image represented as a scene graph. As not
all graph components are relevant for the query, we intro-
duce the concept of a question-based visual guide, which
constrains the potential solution space by learning an opti-
mal traversal scheme. The ﬁnal destination nodes alone are
then used to produce the answer. We show, that ﬁnding rel-
evant semantic structures facilitates generalization to new
tasks by introducing a novel problem of knowledge transfer:
training on one question type and answering questions from
a different domain without any training data. Furthermore,
we achieve state-of-the-art results for Visual Reasoning on
multiple query types and diverse image and video datasets.

1. Introduction

Interpreting and answering subsequent questions about
the semantic relationships of the complex and noisy en-
vironment is a key trait of our cognition. Extraordinary
progress linked to the rise of deep learning in the core vision
tasks [36, 23, 9, 26] (e.g. object recognition) has created a
solid basis for the new research direction of higher level
visual reasoning. Going beyond the conventional recogni-
tion, Visual Reasoning [37, 48] decides about the neces-
sary future actions [16], which is crucial for artiﬁcial in-
telligence applications. The compositional structure of our
world makes this task especially hard, as merely recogniz-
ing individual building blocks at a lower level is not enough.
Such models require precise relational reasoning about the
entities present in the scene and their interactions with each
other.

Visual Reasoning tasks are often posed in the form of
Visual Question Answering (VQA) [37, 48, 16], which lies
in the intersection of vision and language and attempts to

Figure 1: Visual reasoning example, where object inter-
play is crucial for the correct answer and an overview of
our graph neural network-based approach. The visual guide
learns to give question-dependent directions to follow on
the scene graph. The ﬁnal answer is then produced solely
from the embeddings of the reached destination nodes.

answer a speciﬁc question about the scene. Complex se-
mantic associations between both, language query and the
visual scene entities (Figure 1) are characteristic to this task.
Despite the exceedingly structured nature of the vi-
sual information needed to answer open-ended questions,
the majority of previous works focus on spatial feature
maps obtained from a pre-trained CNN and further com-
bined with an attention mechanism on parts of the im-
age [47, 50, 39]. While pre-trained CNNs offer excellent
object embeddings, they face problems in relational rea-
soning about their large-scale interactions. An excellent
way to model such multi-step associations in an image are
scene graphs [46], where the nodes represent the object-
and the connecting edges specify their relationship embed-
dings. We notice that even though the relations between ob-
jects are indispensable for the complete scene understand-
ing, only a portion of the graph is relevant for answering a
speciﬁc question. We therefore leverage the visual graph in
a selective way through a question-dependent visual guide.
We aim at unifying graph-based inference with question-
speciﬁc visual guidance, in order to identify paths with rel-
evant information ﬂow and present a new model for Visual

1930

DSTravelerDestinationMetal…RubberAnswerPredictionModuleVisual GuideQuestionScene GraphWhatisthematerialofthespherethatisbehindthetinybrownthingtotherightofthegreenobject?Reasoning. Given an image-question pair, we ﬁrst use the
visual guide to create question-speciﬁc directions to follow
in the graph. Next, the graph traveler traverses the visual
graph guided by these directions and computes the probabil-
ity distributions over the nodes being the ﬁnal destination.
Finally, we compute our answer prediction solely from the
expected destination node as visual representation for our
prediction module. While conventional graphical models
for VQA follow the graph-reﬁnement paradigm (i.e. reﬁned
embeddings of all components are used for the prediction),
we maintain the original node representations, identify the
key paths and answer the question solely from the expected
ﬁnal destination nodes, hence: It’s not about the journey;
It’s about the destination.

We demonstrate the effectiveness of our model on three
well-known datasets for different visual reasoning tasks:
question-answering on video data (COG [48]), composi-
tional reasoning on 3D synthetic images (CLEVR [15])
as well as diagram question-answering, with real-life ﬁg-
ures extracted from textbooks (AI2D [17]), which is much
noisier while having less training data. Our model consis-
tently outperforms previous approaches on the AI2D and
COG benchmarks and shows strong performance on the
CLEVR dataset.

As our model operates on semantic structures inside the
scene graph, it has two beneﬁcial properties: interpretabil-
ity and generalization to new tasks. An ablation study illus-
trates that we can easily shed light on the internal choices
our model made to produce the answer by following the ﬁ-
nal soft path. To evaluate the generalization capabilities,
we propose a new task of knowledge transfer for VQA, by
splitting the training and test set based on question types
(e.g. query attributes questions for training and counting for
testing). Through knowledge obtained from training on one
kind of questions, our model is able to derive the answers
for queries, for which type it has never seen before.

2. Related Work

Graph Neural Networks.
Current models are con-
ventionally formed through convolution operations in a
local neighborhood and address long-range dependencies
merely through large receptive ﬁelds. Rich structure of
the scene can be targeted in a more efﬁcient way through
graphs, which have been utilized in a wide range of applica-
tions, such as language [24], social interaction [21, 38, 49],
knowledge representation [3, 29, 42] and chemistry [33].
This is achieved by either generating graphs directly from
the CNN feature maps [25, 18, 46] or by combining the
existing graph representations with the previously acquired
knowledge base [45, 4].

We distinguish three groups of knowledge-base guided
algorithms: approaches using graph-reﬁnement through the

network either for better node representation [20, 43, 5, 41],
or for reﬁning the edges [40, 37], and the graph traversal
approaches [45, 4]. The ﬁrst group performs feature pool-
ing for the node itself and its neighborhood (e.g. through
a recurrent neural network (RNN) [41]).
In contrast, the
second group combines the edges e.g. through average-,
sum-pooling [37] or a weighted combination [20]. Un-
fortunately, a graph representation of an image which has
been strongly modiﬁed e.g. through an RNN looses its inter-
pretability for the human eye. Our proposed approach falls
into the third category, as the graph representation built once
at the beginning remains ﬁxed throughout the process. The
questions are subsequently answered by exploring various
paths of the graph without any further feature reﬁnement
(e.g. depending on the question). The decision is based
solely on the destination node embeddings and the reason,
why our model has favored one answer over the other can
be easily understood through the found graph trails.

In this work, we introduce a model based on a graph
traversal scheme for Visual Reasoning. Of particular rele-
vance are recent works of Xiong et al. [45] and Go et al. [4]
in the ﬁeld of language-based question answering. The au-
thors represent text-based knowledge as a graph and per-
form training with the REINFORCE [44] paradigm in order
to traverse it. However, these procedures are constrained by
the query paths being discrete. In comparison, our model
is trained on visual entities and follows soft paths, as we
obtain a continuous conﬁdence over the nodes in each step
(i.e. as opposed to the paths weighted either by 0 or 1 in
previous work).

Visual Question Answering (VQA). VQA has rapidly
gained popularity over the past years [1, 52, 22, 10], mostly
being addressed through image feature maps extracted with
a pre-trained CNN and subsequent question-related atten-
tion module [50, 51]. In general, the ways of addressing
this problem can be divided into four categories: 1) Global
embedding methods [31, 1, 28, 34, 35, 30] that use a joint
embedding of the global image representation and the ques-
tion to produce an answer; 2) Models that attend to parts of
the image are able to improve performance [50, 51, 6, 47];
3) Compositional models [2, 13, 16] use a modular rep-
resentation of the neural networks; 4) Graph-based VQA
models [37, 41, 11, 17, 18], where a graph representation of
the image or the question is used to produce the answer.

The latter category has emerged recently and is by de-
sign well-suited for relational reasoning, as object connec-
tions are explicitly represented through the edges. Such
approaches mostly follow the graph-reﬁnement paradigm.
Teney et al. [41] reﬁne the features of each node using
an RNN by pooling based on the similarity of to the cur-
rent node. In [17], an RNN is applied on the edges which
are subsequently ﬁltered through a question-based atten-

1931

tion, while in [18] an end-to-end version is proposed, where
the edges are learned inside the model. Finally, the models
in [37, 11] represent the graphs as an unordered set of edges
using weighted average to get a ﬁxed image representation
for answering the question.

Our model falls into the graph neural network category,
leveraging the object- and their relationships embedding as
the scene graph components. Other than previous graph-
based approaches for VQA [41, 17, 18, 37, 11], our model
is not based on graph-reﬁnement. While conventional meth-
ods reﬁne the embeddings of all graph components and use
them to compute the ﬁnal answer, we hold the original node
representations, identify the key paths through the question-
based visual guide and answer the question solely from the
ﬁnal destination nodes.

3. Visual Reasoning via Guided Soft Paths

We present a new model for visual reasoning that deals
with the composite object relationships in the scene as a
graph traversal problem. The challenge is that the space of
potential paths in a visual graph is very large. When asked
‘What is the material of the sphere that is to the left of the
(Figure 1), a
tiny brown thing behind the green object?’
human would immediately look for the green object, there-
after, at the tiny brown sphere, then, select the sphere left
of it. Likewise, our idea is to greatly constrain the solu-
tion space by learning the optimal graph traversal strategies
based on question-speciﬁc decisions.

Conceptually, our visual reasoning model is composed
of three main components: 1) the visual guide, 2) the graph
traveler and 3) the prediction module. The visual guide
takes as input the question and produces direction embed-
dings. The graph traveler follows these directions and
computes the soft paths – probability distributions over the
nodes of being in the route to the nodes that include rel-
evant information to produce the answer. The ﬁnal deci-
sion is made by the prediction module, which exploits the
found destinations as weights for the graph nodes and infers
the ﬁnal answer. We want to highlight, that the prediction
module operates exclusively on the destination node rep-
resentations, dismissing the preceding components of the
paths. While the visual guide and the prediction module
can be viewed as individual neural networks connected by
the graph traveler, they are optimized jointly in an end-to-
end training fashion.

An overview of our model is illustrated in Figure 2.
Next, we give a general deﬁnition of our model’s build-
ing blocks (Section 3.1); provide a mathematical founda-
tion for computing the soft paths (Section 3.2); and, ﬁnally,
we present our complete graph-based neural architecture for
Visual Reasoning (Section 3.3).

3.1. Data Structures

Graph. We deﬁne as a visual graph G = (V, F, R) a
structure with the following properties:

1. V – a set of N vertices representing the object in-

stances present in the image.

2. F ∈ RN ×D – a D-dimensional representation for each
of the N visual nodes. These can be one-hot vectors
representing the object instance or features extracted
from a pre-trained CNN.

3. R ∈ RN ×N ×E – an E-dimensional relation represen-
tation for each pair of nodes (n, m) ∈ V ×V . One way
to deﬁne the representation R is a one-hot embedding
of predicates (e.g. ‘on top’, ‘holding’), which can be
obtained as in [27], or features extracted from a CNN
on the image crop surrounding both objects. A simpler
method is to represent each edge by concatenating the
node pair representations F .

Path. We call an ordered set of nodes of length T in graph
G a path:

τ = [nτ

1 , nτ

2 , . . . , nτ

T ].

We note that this deﬁnition of path assumes a discrete as-
signment of each node in each time step t.

Soft Path. A soft path does not return discrete associations
of each of a node with the path but softens its inclusion.
Formally, for each time step t and node n in graph G we
have an association score pt(n) ∈ [0, 1]. As we aim to
model a probability distribution, we require that the sum
over all nodes in time step t in the graph is one:

pt(n) = 1.

X

n∈V

Thus, a soft path is described by the two dimensional
array τ = [p1(V ), p2(V ), . . . , pT (V )], where we use:
pt : RN → [0, 1]N element-wise on each node.

Starting Node. The starting node of path τ is the node at
the ﬁrst time step: nτ
1 . In case of a soft path it is deﬁned by
a probability distribution over all nodes n.

Destination. A destination n is a node in path τ that
occurs in time step T , while for the soft paths it is equal to
the probability in the last time step.

3.2. Reaching the Destinations

Our model is built upon the assumption that by traversing
the scene graph in a controlled way, we are able to identify

1932

Figure 2: Proposed graph neural network architecture which learns traversal strategies for the scene graph (simpliﬁed for
path length T = 3). While the visual guide, the graph traveler and the prediction module, are individual neural network
components, they are optimized jointly in an end-to-end fashion. The visual guide takes as input the question and provides
direction embeddings for the traveler to follow. Prediction module gives the ﬁnal answer based only on the question and the
destination nodes embeddings, the predecessors are therefore dismissed: it’s not about the journey; it’s about the destination.

the information relevant for the speciﬁc question. We there-
fore compute the probability of the node n being a desti-
nation, which is equal to the sum of the probabilities of all
paths ending in n:

P (τ ) = P (n1) · P (n2|n1) ·

P (nT = n) = X

P (τ ) · 1[nτ

T = n].

(1)

τ

= P (n2) ·

T

Y

t=3

T

Y

t=3

P (nt|nt−1)

P (nt|nt−1)

(4)

According to the marginalization rule, the probability of

the path τ is then equal to:

P (τ ) = P (n1, . . . , nT ) = P (n1) ·

T

Y

t=2

P (nt|nt−1, . . . , n1)

(2)
Our approach models a discrete Markov Chain (i.e. we
assume the Markov Property) with the set of states equal to
the nodes V in our graph G. We obtain the probability of
each path as:

P (τ ) ≈ P (n1) ·

T

Y

t=2

P (nt|nt−1).

(3)

Thus, the new estimation lies in the calculation of each
probability P (nt). For this, we make use of the function
τ t(n) which computes the probability of each node n being
in the path in an iterative way using the formulation:

τ t(n) = X

P t(n|m) · τ t−1(m).

(5)

m∈N

We stop the calculation at time step T and the ﬁnal values
become the probability of each node being the destination
i.e. the node has information relevant for the question. Next,
we show the models for obtaining the start- and transition
probabilities.

3.3. Neural Graph Architecture

In case of t = 1, it is straight forward to compute the
probability of the nodes in the path (i.e. P (n1 = n)).
For t > 1 we have to consider the transition probabili-
ties P (nt|nt−1). Since the number of possible path options
grows exponentially with the path length, we further refor-
mulate this calculation for time steps larger than one. We
iteratively transform the path probability to the probability
of each node lying in each time step e.g.:

In conventional graph neural networks for VQA, node
features F change depending on their neighbors in each
training time step, becoming a mixture of the initial and
foreign object representations (i.e. graph-reﬁnement) [17,
41, 37]. In comparison, our model keeps the semantic node
representations and focuses on the network topology, learn-
ing to ﬁnd relationships of the scene entities relevant for the
current question (see Figure 2). We can easily shed light

1933

RRVisual Graph2. Traveler3. PredictionImage1. GuideQuestionWhatshapeisthe...smallcylinder1111111D ConvsAttentionfor each tD1D3·D2DirectionRepresentation[]Node Rep. FEdge Rep.  FCFC[]·[]FC·P(n1)RP(n2)Whatshapeistheobjectontheleftsideoftheobjectthatisbehindthesmallcylinder?P(nT)·[]FCSphereCylinder…CubeP(n2|n1)QQupon the choices of our model, as we retain the initial in-
terpretation of its nodes and highlight the key links between
them.

1. Visual Guide.
The visual guide considers the static
graph as a map to be traversed using the question as the
reference. That is, the guide takes as input the question,
embeds it e.g. using an LSTM [12] or a one dimensional
CNN with self-attention [8] and produces direction embed-
dings D for the traveler to follow on the graph.
In case
of an LSTM, we represent the question as the ﬁnal hid-
den state, while we use weighted average over the fea-
ture maps in case of a CNN. Predicted directions at a time
step t are then obtained through learned fully connected lay-
D ∈ R|Dt|×|H| with the
ers: Dt = W t
size of the direction embeddings |Dt| chosen empirically.

D, where W t

D · H + bt

2. Graph Traveler. The graph traveler traverses the visual
graph based on the directions suggested by the guide. Thus,
it produces prior probabilities (i.e. the conﬁdence of each
node being the ﬁrst one visited) and computes the transition
probabilities (i.e. conﬁdence of traversing one node to the
next).

For the ﬁrst node of a path, we obtain the conﬁdence by
training a fully connected layer on top of the node represen-
tations F from the visual graph and the ﬁrst direction D1
given by the guide:

Pθ(n1) = sof tmax(Wp1 · [D1, F ] + bp1 ),

(6)

where θ is the collection of all the learnable parameters in
the model and the sof tmax function normalizes over the
nodes:

sof tmax(X)i = exp(xi)/ X

exp(xj).

(7)

j∈V

In case of the transition probabilities, we make use of the

edge features R between each pair of nodes:

Dataset

Type

# Imgs

# Inst

# Q

Videos

COG
AI2D
CLEVR 3D-Synthetic

Diagrams

11M
5K

100K

9.6
9.1
6.5

44M
15K
700K

Table 1: Visual Reasoning benchmarks used to evaluate
our model (by task type, number of images/videos, average
amount of instances per example and number of questions).

questions about the shape, color etc. of an object), the solu-
tion is determined from the destination nodes i.e. soft path
probabilities τ (n) at time step T as:

gH = X

τ T (n) · Fn,

n∈V

(10)

where Fn is the nth row of the matrix F (i.e. the feature rep-
resentation of each node in V ). We concatenate this visual
global representation gH with the question embedding Q.
Then, a fully connected layer is used to produce the ﬁnal
prediction over all possible answers. For existence ques-
tions, we answer the question with ‘yes’, in case that any
of the destinations has a probability over 0.5. In the task
counting, we estimate the number of destinations that round
to one. For tasks, where the sum of the ﬁnal soft path prob-
abilities may be larger than one, as multiple destinations
could be applicable (e.g. counting or existence), we use sig-
moid function instead of softmax for edge normalization.

Model Conﬁguration. We train the network end-to-end by
minimizing the cross entropy using Adam [19] with an ini-
tial learning rate of 0.00025 without any weight or learning
rate decay. We choose a maximal path length T empirically
on the validation data. The question-based guide uses mul-
tiple 1D convolution layers with 32 hidden units, while the
ﬁnal fully connected layers of the graph traveler have the
size of 128 (we include a detailed description of the param-
eters in the supplemental material).

Pθ(nt|nt−1) = sof tmaxsource(Wpt · [Dt, R] + bpt ). (8)

4. Evaluation

Here, the sof tmax operation normalizes over the rows,
such as the sum over the outputs is equal to one:

Pθ(n|m) = 1.

(9)

X

n∈V

In the last time step T the graph traveler computes the
probability of a node being the ﬁnal destination τ T (n) (as
introduced in Equation 5).

3. Prediction Module. The prediction module differen-
tiates between the problem types and generates the answer
leveraging the probability distribution over the destinations
(see step 3 in Figure 2). In case of query-type questions (i.e.

We perform comprehensive studies on three challeng-
ing datasets for Visual Reasoning with diverse query types
(overview in Table 1). All datasets cover visual examples,
task queries with the ground-truth solutions (open-ended or
multiple choice form), as well as annotations for the scene
graph. In Section 4.1, we evaluate our model on video se-
quences, then, in the task of diagram question answering
(Section 4.2) and on highly compositional reasoning prob-
lems on 3D synthetic images (Section 4.3). We further dis-
cuss how different path lengths T impact the performance
(Section 4.4), evaluate how well our model generalizes to
previously unseen tasks ( Section 4.5) and, ﬁnally, visualize
concrete examples of soft paths (Section 4.6).

1934

4.1. Visual Reasoning on Videos

4.2. Diagram Question Answering

Dataset.
In this section, we use the COG [48] dataset as
a test bed for both, spatial and temporal reasoning. The
dataset comprises over 11 Million questions on videos.
While the videos are of synthetic 2D scenes, it speciﬁcally
targets temporal memory and logical deductive reasoning
about video input, being difﬁcult for humans [48]. The
task is to deduce the correct answer while taking into ac-
count changes of the scene in three different query types:
pointing, yes/no, conditional and attribute-related ques-
tions. Higher number of scene entities is also characteristic
for the dataset.

Results. We demonstrate the effectiveness of our model
in Table 2. Additionally to the original Working Mem-
ory [48] approach, we compare our model to three base-
lines: 1) random performance, 2) a question-only model
consisting of a 1D CNN over the question words followed
by fully-connected layers, and 3) a graph-based approach,
where instead of computing the answer from the destination
nodes of the found paths, we use a joint embedding of the
question and all of the nodes in the graph as input and use
fully-connected layers to make a prediction.

Approach

Atts. Condit.

Point Yes/No

All

Baselines

Random
Question-only

1.9
1.6

8.4
2.3

17.5
19.4

50.0
49.7

26.6
27.4

Work. Memory† [48]

–

–

–

–

93.7

Memory Networks

Graph-based Methods

Question+Nodes
Ours

73.7
99.2

63.5
98.4

92.5
100.0

57.9
95.0

63.3
97.2

Table 2: Results for visual reasoning on videos on the
test set of COG for different tasks: pointing, existence,
conditional questions and questions about object attributes.
† Best model selected from 50 trained networks.

Our model yields the best recognition rates in all query
types. The distinction from the natural-language-based
benchmarks becomes obvious, as the question-only ap-
proach exceeds the random baseline by less than 1%. Vi-
sual reasoning is therefore decisive for this benchmark. The
yes/no questions have been the major source of our model’s
unreliability. Our analysis of these confusions indicates oc-
casional difﬁculties in case of ‘and’ connections in the ques-
tion (e.g. ‘Shape of last magenta object equal shape of last
lavender object and shape of now mint object equal shape
of last olive object?’). Nonetheless, our model achieves ex-
cellent performance of 100% for pointing questions, and es-
tablishes new state-of-the-art overall accuracy of 97.2%.

Dataset. Next, we evaluate our approach on real-life im-
ages in the diagram understanding task. AI2D [17] dataset
contains images extracted from school textbooks of various
subjects and evaluates understanding of causal relations in
these ﬁgures. As middle school pupils are required to learn
from such diagrams, reason and answer questions about
them, this dataset represents an excellent realistic testbed
for visual reasoning. As we are dealing with real-life data,
AI2D is smaller and noisier than other datasets we used
for testing, with 666 lessons of total 5K diagrams and 15K
questions.

Approach

Random

Baselines

All

25.00

Classical VQA Methods

VQA [1]

32.90

Graph Neural Networks

DQA-Net [DSDP] [17]
DQA-Net [DGGN] [18]
DQA-Net [18]
Ours

38.47
39.73
41.55
43.45

Table 3: Diagram Question Answering results on real im-
ages extracted from school textbooks (AI2D dataset) [17]

Results.
In Table 3, we compare our model with a
multitude of published approaches, including three graph-
based methods. As AI2D is evaluated in multiple choice
form with four possible options, random choice perfor-
mance is 25%. Overall, there is a clear beneﬁt of using
structured approaches. Our graph-traversal based model
consistently outperforms state-of-the-art graph neural net-
works and therefore conﬁrms the effectiveness of focusing
on traversal schemes and the found destination nodes, in-
stead of the message-passing paradigm.

4.3. VQA on 3D Synthetic Images

Dataset.
The Compositional Language and Elementary
Visual Reasoning dataset (CLEVR) [15] is a widely used
diagnostic benchmark for compositional understanding of
3D scenes for different tasks, such as counting, ﬁnding at-
tributes of objects based on their relations with other in-
stances and comparison between object attributes. Long
reasoning chains, demanding memory-related tasks and ab-
sence of question-based biases are distinctive for this bench-
mark. Although it is comprised of synthetic scenes, con-
ventional VQA models often face signiﬁcant difﬁculties on
CLEVR as they tend to focus on the dataset bias [16, 37, 7].

1935

Trained on: counting (source)

ours
random

Trained on: existence (source)

Trained on: query attributes (source)

)
t
e
g
r
a
t
(
 
e
p
y
t
 

n
o

i
t
s
e
u
q
d
e

 

t

l

a
u
a
v
E

query
attributes
existence

counting

query
attributes
existence

counting

query
attributes
existence

counting

100

50

0

50

Accuracy %

Figure 4: Generalization to unseen tasks: our model is
trained on one query type is evaluated on a different task.

4.4. Impact of the path length on performance

As we explicitly focus on relations in the scene, we com-
pare variants of our model to measure the effect of differ-
ent restrictions of the soft path at length T . Figure 3 il-
lustrates changes of accuracy in relation to T for different
COG and CLEVR tasks. The model beneﬁts immensely
from considering paths of length two or more, e.g. for the
query attributes task, percentage of correct answers rises
from 53.1% (T = 1) to 98.1% (T = 2), further improv-
ing to 99.8% (T = 3), conﬁrming the signiﬁcance of causal
connections in the scene. Starting at T = 4 for CLEVR
and T = 3 for COG, we observe a slight decline in overall
performance, which we link to the extend of chained ques-
tions in the datasets. For example, in a question ‘What is
the material of the sphere behind the tiny brown thing to the
right of the green object?’ (Figure 1) the reasoning chain
consists of two pairwise relationship. In general, enforcing
longer paths than necessary for the question is not a problem
in our architecture, as it permits self-loops. However, the
option of including more nodes than required might result
in higher level of noise, as the overall search space becomes
larger. This slight accuracy drop should be viewed with cau-
tion, as it is also connected to the nature of the questions in
the dataset i.e. it is expected to increase with the amount
of entities mentioned in the question. Nonetheless, when
further increasing the path length to a higher path length
the performance stabilizes e.g. for COG the model achieves
95.6% at T = 8.

4.5. Performance on unseen tasks

Humans have an impressive ability to address new tasks
of increasing difﬁculty by transferring solutions from fa-
miliar problems. Similarly, our motivation for focusing on
the scene structure, is to develop a model which processes

1936

Figure 3: Performance for different maximal path lengths T
on the validation set of COG (top) and CLEVR (bottom).

Results. We report results on all ﬁve problem types of the
CLEVR benchmark: counting, existence, query attributes
and questions about comparing numbers and attributes of
objects. A high number of novel methods have been re-
cently proposed to tackle CLEVR reasoning tasks, which
we group based on their way of addressing object relations
and compare to our model in Table 4.

Approach

Reference Count Exist

Comp. Query Comp.
Attrs.

Attrs.

Nrs.

Human [16]
Qtype [16]

LSTM [16]
CNN [16]
CNN+SA [37]
QGHC [7]
FiLM [32]

–
–

–
–

86.7
34.6

96.6
50.2

86.5
51.0

Classical VQA Methods

41.7
43.7
64.4
91.2
94.3

61.1
65.2
82.7
78.1
99.1

69.8
67.1
77.4
79.2
96.8

ECCV’16
ECCV’18
AAAI’18

Compositional Models

N2NMN* [13]
PG(9K)* [16]
PG(700K)* [16]

ICCV’17
ICCV’17
ICCV’17

68.5
79.7
92.7

85.7
89.7
97.1

Memory Networks

Work. Mem. [48]
MAC† [14]

ECCV’18
ICLR’18

91.7
97.1

99.0
99.3

84.9
79.1
98.7

95.5
96.8

Graph Neural Networks

CNN+RN‡ [37]
Ours

NIPS’17

–

90.1
91.3

97.8
98.6

93.6
99.6

95.0
36.0

36.8
49.3
82.6
89.7
99.1

90.0
92.6
98.1

98.5
99.1

97.9
99.5

96.0
51.3

51.8
53.0
75.4
86.8
99.1

88.7
96.0
98.9

98.8
99.1

97.1
99.8

All

92.6
41.8

46.8
52.3
76.6
86.3
97.7

83.7
88.6
96.9

96.8
98.9

95.5
97.5

Table 4: Visual reasoning results for different tasks on the
CLEVR test set [15]. (*) denotes the use of extra supervi-
sion in form of program labels, ‡ denotes the use of data
augmentation, † denotes the use of pre-trained models.

We achieve state-of-the-art accuracy of over 99% on
three tasks (comparing numbers and two attribute-related
problems) and report a strong overall performance (97.5%),
surpassing humans (92.6%) and the recent graph-based
method based on edge representation sum [37] (95.5%).

1234Path Length707580859095100AccuracyOverallAttributesConditionPointYes/No1234Path Length5060708090100AccuracyOverallCountExistComp. Nrs.Query Atts.Comp. Atts.queries by decomposing them into granular tasks, which
then could be easily re-used to answer questions our model
has never seen before.

To evaluate our assumption, we propose a new challeng-
ing benchmark for visual reasoning on problems not pre-
viously seen during training. We regard three tasks from
the CLEVR dataset: query attributes, existence and object
counting. In our proposed evaluation setup, the model is
trained on one of these tasks and is intended to solve another
one. Consider the existence task, where we output ‘yes’ if
in the last time step T there is at least one destination node
with probability over 0.5 (see Section 3.3). As the node rep-
resentations are not reﬁned throughout the process, we can
extend our model to counting without additional training,
by merely using the counting prediction module version,
i.e. summing the number of destinations with an activation
over 0.5, as described in Section 3.3. For the query attribute
task, we select the node with the maximal activation.

We report the performance of our model on previously
unseen tasks in Figure 4. Our approach successfully ap-
plies the knowledge it had acquired from counting or ex-
istence to previously unseen query types. These two tasks
are especially re-usable as they involve a universal granular
question: whether objects are present in the scene, or not.
In case of learning on the attribute-based questions, we as-
sume that the destinations are always available (as we ques-
tion speciﬁc attributes of the node and not their presence).
Re-usability of the learned information is therefore lower.
Training on the counting queries turned out to be most ben-
eﬁcial for solving new problems. We assume, this is due
to counting being a more composite task as it covers both,
checking for object presence and determining, whether the
objects have certain properties (e.g. ‘What number of brown
balls are the same size as the metal object?’). Our model
trained on the counting task was able to solve the query
attribute problem in 56.4% of times, surpassing random
chance (30.6%) by 25.8%.

Obviously, solving previously unseen tasks is per design
a much harder problem than conventional supervised Vi-
sual Reasoning and the recognition rates are considerably
lower. Apart from the lack of supervision, language expres-
sions not present during training pose an additional chal-
lenge (e.g. ‘how many’ if the model was trained on the ex-
istence task and evaluated on counting). Still, our model
consistently outperforms the random chance baseline, be-
ing able to address new tasks without costly annotations of
training examples.

4.6. Qualitative Results

An important property of our model is the ability to trace
back the underlying reasoning behind the ﬁnal answer. In
Figure 5, we revisit the ﬁnal soft paths of our model on two
examples from CLEVR and AI2D benchmarks. We visu-

There is a green metal object that is
behind the thing on the left side of
the metal block; what shape is it?
Answer: Sphere

What is between mantle and in-
ner core? Answer: Outer Core

Figure 5: Example visualizations of the ﬁnal soft path. Or-
ange circles mark the highest activation at each time step t.

alize the nodes with the maximal probability at each time
step t: e.g. in the left image the starting node ‘S’ points at
the violet cube. Edges which belong to the path are marked
with red arrows, starting with the source node ‘S’ and end-
ing in the ﬁnal destinations, which are the only graph com-
ponents used as input in the prediction modules.
In case
of CLEVR (left), we have a very long and strongly compo-
sitional question on which we produce a path of length 3:
traversing from the cube to the small sphere until we ﬁnally
reach the destination: the large sphere in the right side of
the image (see more examples in the supp. material). In
case of AI2D textbook diagram question (right), our model
solves the query ‘What is between mantle and inner core’
with a soft path of length 2 by starting at the mantle and,
next, choosing the destination and also the correct answer:
‘outer core’.

5. Conclusion

We presented a new approach for compositional visual
reasoning, where we employ a graph neural network archi-
tecture to tackle far-reaching relationships in the scene. Our
framework learns how to traverse the graph in a controlled
way and, then answers the question based on the reached
destination nodes of the found paths. Our model exceeds
state-of-the-art methods on two challenging datasets for Vi-
sual Reasoning: on Videos (COG) and Diagram Question-
Answering (AI2D), as well in the three tasks on the 3D syn-
thetic data (CLEVR). At the same time, our model is highly
interpretable as the graph trails directly shed light on the un-
derlying reasoning, showing that our model breaks complex
instructions into smaller tasks. Furthermore, we demon-
strate the positive impact of focusing on relevant semantic
structures on the ability to reuse the acquired knowledge for
novel tasks. In this new benchmark setting, our model was
trained on a certain question type (e.g. existence) and could
successfully handle tasks of a different kind (e.g. counting)
without any further training. Our experiments show encour-
aging evidence that modern visual recognition approaches
could beneﬁt further from structured methods especially in
high-level understanding of global causal relations.

1937

DSmantleouter corecrustSDinner coreReferences

[1] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick,
D. Parikh, and D. Batra. Vqa: Visual question answering.
International Journal of Computer Vision, 2017.

[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Deep
compositional question answering with neural module net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition, 2016.

[3] G. Bouchard, S. Singh, and T. Trouillon. On approxi-
mate reasoning capabilities of low-rank vector spaces. AAAI
Spring Syposium on Knowledge Representation and Reason-
ing (KRR): Integrating Symbolic and Neural Approaches,
2015.

[4] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar,
A. Krishnamurthy, A. Smola, and A. McCallum. Go for
a walk and arrive at the answer: Reasoning over paths in
knowledge bases using reinforcement learning. International
Conference on Learning Representations, 2017.

[5] K. Do, T. Tran, T. Nguyen, and S. Venkatesh. Attentional
multilabel learning over graphs-a message passing approach.
arXiv preprint arXiv:1804.00293, 2018.

[6] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling for
visual question answering and visual grounding. Confer-
ence on Empirical Methods in Natural Language Processing,
2016.

[7] P. Gao, P. Lu, H. Li, S. Li, Y. Li, S. Hoi, and X. Wang.
Question-guided hybrid convolution for visual question an-
swering. IEEE conference on computer vision and pattern
recognition, 2018.

[8] J. Gehring, M. Auli, D. Grangier, and Y. N. Dauphin. A
convolutional encoder model for neural machine translation.
arXiv preprint arXiv:1611.02344, 2016.

[9] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.

[10] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman,
J. Luo, and J. P. Bigham. Vizwiz grand challenge: Answer-
ing visual questions from blind people. IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[11] M. Haurilet, Z. Al-halah, and R. Stiefelhagen. Moqa - a
In Workshop on

multi-modal question answering model.
Shortcomings in Vision and Language, 2018.

[12] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[13] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.
Learning to reason: End-to-end module networks for vi-
sual question answering. IEEE International Conference on
Computer Vision, 2017.

[14] D. A. Hudson and C. D. Manning. Compositional attention
networks for machine reasoning. International Conference
on Learning Representations, 2018.

[15] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L.
Zitnick, and R. Girshick. Clevr: A diagnostic dataset for
compositional language and elementary visual reasoning. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1988–1997. IEEE, 2017.

[16] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman,
L. Fei-Fei, C. L. Zitnick, and R. B. Girshick. Inferring and
executing programs for visual reasoning. In IEEE Interna-
tional Conference on Computer Vision, pages 3008–3017,
2017.

[17] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi,
In

and A. Farhadi. A diagram is worth a dozen images.
European Conference on Computer Vision, 2016.

[18] D. Kim, Y. Yoo, J. Kim, S. Lee, and N. Kwak. Dynamic
graph generation network: Generating relational knowledge
from diagrams. Conference on Computer Vision and Pattern
Recognition, 2017.

[19] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[20] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation
International Confer-

with graph convolutional networks.
ence on Learning Representations, 2017.

[21] S. Kok and P. Domingos. Statistical predicate invention. In
Proceedings of the 24th international conference on Machine
learning, pages 433–440. ACM, 2007.

[22] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations.
2016.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[24] M. Kuhlmann and S. Oepen. Towards a catalogue of linguis-
tic graph banks. Computational Linguistics, 42(4):819–827,
2016.

[25] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia.
Learning deep generative models of graphs. arXiv preprint
arXiv:1803.03324, 2018.

[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015.

[27] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual re-
lationship detection with language priors. In European Con-
ference on Computer Vision, 2016.

[28] L. Ma, Z. Lu, and H. Li. Learning to answer questions
from image using convolutional neural network. In Associa-
tion for the Advancement of Artiﬁcial Intelligence, volume 3,
page 16, 2016.

[29] F. Mahdisoltani, J. Biega, and F. M. Suchanek. Yago3: A
In Confer-

knowledge base from multilingual wikipedias.
ence on Innovative Data Systems Research, 2013.

[30] M. Malinowski and M. Fritz. A multi-world approach to
question answering about real-world scenes based on uncer-
tain input. In Advances in neural information processing sys-
tems, pages 1682–1690, 2014.

[31] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neu-
rons: A deep learning approach to visual question answering.
International Journal of Computer Vision, 2017.

1938

[32] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and
A. Courville. Film: Visual reasoning with a general condi-
tioning layer. Association for the Advancement of Artiﬁcial
Intelligence, 2017.

[33] P. Radivojac, W. T. Clark, T. R. Oron, A. M. Schnoes, T. Wit-
tkop, A. Sokolov, K. Graim, C. Funk, K. Verspoor, A. Ben-
Hur, et al. A large-scale evaluation of computational protein
function prediction. Nature methods, 10(3):221, 2013.

[34] M. Ren, R. Kiros, and R. Zemel. Exploring models and data
for image question answering. In Advances in neural infor-
mation processing systems, pages 2953–2961, 2015.

[35] M. Ren, R. Kiros, and R. Zemel. Exploring models and data
for image question answering. In Advances in neural infor-
mation processing systems, pages 2953–2961, 2015.

[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision, 115(3):211–252, 2015.

[37] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. Lillicrap. A simple neu-
ral network module for relational reasoning. In Advances in
neural information processing systems, 2017.

[38] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and
T. Eliassi-Rad. Collective classiﬁcation in network data. AI
magazine, 29(3):93, 2008.

[39] K. J. Shih, S. Singh, and D. Hoiem. Where to look: Focus re-
gions for visual question answering. In IEEE conference on
computer vision and pattern recognition, pages 4613–4621,
2016.

[40] M. Simonovsky and N. Komodakis.

Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017.

[41] D. Teney, L. Liu, and A. v. d. Hengel. Graph-structured rep-
resentations for visual question answering. IEEE Conference
on Computer Vision and Pattern Recognition, 2016.

[42] K. Toutanova, D. Chen, P. Pantel, H. Poon, P. Choudhury,
and M. Gamon. Representing text for joint embedding of

text and knowledge bases. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language Process-
ing, pages 1499–1509, 2015.

[43] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio,
and Y. Bengio. Graph attention networks. International Con-
ference on Learning Representations, 2017.

[44] R. J. Williams. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.

[45] W. Xiong, T. Hoang, and W. Y. Wang. Deeppath: A rein-
forcement learning method for knowledge graph reasoning.
Empirical Methods in Natural Language Processing, 2017.
[46] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph
generation by iterative message passing. IEEE Conference
on Computer Vision and Pattern Recognition, 2017.

[47] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
ing.
In European Conference on Computer Vision, pages
451–466. Springer, 2016.

[48] G. R. Yang, I. Ganichev, X.-J. Wang, J. Shlens, and D. Sus-
sillo. A dataset and architecture for visual reasoning with
a working memory. European Conference on Computer Vi-
sion, 2018.

[49] S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng,
and H. Zha. Like like alike:
joint friendship and interest
propagation in social networks. In Proceedings of the 20th
international conference on World wide web, pages 537–546.
ACM, 2011.

[50] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
attention networks for image question answering. In IEEE
Conference on Computer Vision and Pattern Recognition,
2016.

[51] D. Yu, J. Fu, T. Mei, and Y. Rui. Multi-level attention net-
works for visual question answering. In IEEE Conference on
Computer Vision and Pattern Recognition, 2017.

[52] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7w:
Grounded question answering in images. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2016.

1939

