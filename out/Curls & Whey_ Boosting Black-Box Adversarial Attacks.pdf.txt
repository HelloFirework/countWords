Curls & Whey: Boosting Black-Box Adversarial Attacks

Yucheng Shi, Siyu Wang, Yahong Han
College of Intelligence and Computing

Tianjin University, Tianjin, China

{yucheng, syuwang, yahong}@tju.edu.cn

Abstract

(cid:1876) (cid:3003)  

Image classiﬁers based on deep neural networks suffer
from harassment caused by adversarial examples. Two de-
fects exist in black-box iterative attacks that generate ad-
versarial examples by incrementally adjusting the noise-
adding direction for each step. On the one hand, existing
iterative attacks add noises monotonically along the direc-
tion of gradient ascent, resulting in a lack of diversity and
adaptability of the generated iterative trajectories. On the
other hand, it is trivial to perform adversarial attack by
adding excessive noises, but currently there is no reﬁne-
ment mechanism to squeeze redundant noises. In this work,
we propose Curls & Whey black-box attack to ﬁx the above
two defects. During Curls iteration, by combining gradi-
ent ascent and descent, we ‘curl’ up iterative trajectories to
integrate more diversity and transferability into adversar-
ial examples. Curls iteration also alleviates the diminishing
marginal effect in existing iterative attacks. The Whey op-
timization further squeezes the ‘whey’ of noises by exploit-
ing the robustness of adversarial perturbation. Extensive
experiments on Imagenet and Tiny-Imagenet demonstrate
that our approach achieves impressive decrease on noise
magnitude in ℓ2 norm. Curls & Whey attack also shows
promising transferability against ensemble models as well
as adversarially trained models. In addition, we extend our
attack to the targeted misclassiﬁcation, effectively reducing
the difﬁculty of targeted attacks under black-box condition.

1. Introduction

The output of deep neural networks (DNNs) is highly
images [23, 5].
sensitive to tiny perturbation on input
Among all methods that generate adversarial examples, it-
erative attacks [9, 4, 27] strike a better balance between
attack effect and efﬁciency of adversarial example gener-
ation. However, there are two severe drawbacks in current
mainstream black-box iterative attacks based on substitute
model [16]. In the ﬁrst place, decision boundaries between

snow bird

(cid:1876) 

(cid:1876) 

snail

(cid:1876) (cid:3002) (cid:3398) (cid:1876)

= 4.11 

(cid:2870)

(cid:1876) (cid:1499) 

(cid:1876) (cid:3002) 

snail

(cid:1876) (cid:3003) (cid:3398) (cid:1876)

= 6.38

(cid:2870)

snail

(cid:1876) (cid:1499) (cid:3398) (cid:1876) (cid:2870) = 2.52

Figure 1.
Iterative trajectory of Curls. Background is contour of
cross entropy. The redder the color, the lower the loss. The con-
secutive black curve represents decision boundary between cate-
gory ‘snow bird’ and ‘snail’. Green and purple polylines represent
trajectories with simply gradient ascend and Curls iteration with
binary search, respectively. Blue and red rings represent the orig-
inal image x and adversarial example found after binary search.
Original image and three adversarial examples on both sides cor-
respond to four rings with the same color as the image border.

models in black-box scenario are far apart [11]. Iterative
trajectories have difﬁculties crossing decision boundary of
target model with a small noise magnitude, because they are
based on monotonic search along the gradient ascent direc-
tion of substitute model. This impairs adversarial examples’
transferability[11].
In the second place, although noise
magnitude determines the performance of attack methods,
adversarial examples generated by iterative attacks contain
a certain amount of redundant noises that cannot be com-
pletely removed by simply increasing the iteration number.
A post-iteration reﬁnement mechanism is needed to squeeze
out the ‘whey’ of adversarial noises.

In this paper, we propose Curls & Whey black-box at-
tack. During Curls iteration, we iterate along both the gradi-
ent ascent and descent directions of substitute model’s loss
function, as demonstrated by green and purple polylines in
Fig. 1. The dual-direction setting ‘curls’ up the iterative
trajectories and is hence more likely to cross target model’s
decision boundary at a closer distance, which effectively en-
hances the diversity as well as transferability of adversarial
examples. Diminishing marginal effect caused by monoton-

6519

ically adding noises along the direction of gradient ascent
is also weakened. Mechanisms to reﬁne adversarial noises
(red arc in Fig. 1) and guide initial direction are included at
the end and beginning of Curls iteration, respectively.

Whey optimization is applied to further squeeze the mag-
nitude of noise by exploiting adversarial perturbation’s ro-
bustness. We ﬁrstly divide adversarial perturbation into
groups according to pixel value and attempt to ﬁlter out
the noises of each group. Then we distill each pixel in ad-
versarial example stochastically to squeeze out redundant
noises little by little. Experiments on Imagenet [18] and
Tiny-Imagenet [3] verify that our method generates adver-
sarial examples with higher transferability and smaller per-
turbation in ℓ2 norm under the same query limitation. We
also systematically investigate the inﬂuence of each itera-
tive parameter on the performance of the proposed method.
In addition, our method shows strong transferability against
ensemble models and adversarially trained models [24].

Targeted misclassiﬁcation in black-box scenario has long
been considered intractable [11], for differences on deci-
sion boundaries and classiﬁcation spaces between substitute
and target model hampers adversarial examples’ penetration
from source class to target class. Most existing iterative at-
tacks try to solve this problem by simply replacing gradient
descent in untargeted misclassiﬁcation with gradient ascent
towards the target class [9, 4]. In this paper, by integrating
interpolation to iterative process, we boost original image
into the direction towards the target category and signiﬁ-
cantly decrease the difﬁculty of targeted misclassiﬁcation.

We summarize our contributions as follows:
(1) We bring forward Curls iteration, a black-box attack
method aiming at improving diversity of iterative trajecto-
ries and transferability of adversarial examples by combin-
ing both gradient ascent and gradient descent directions.

(2) We propose Whey optimization,

the ﬁrst noise-

squeezing method exploiting robustness of perturbations.

(3) We expand our iterative method to targeted attacks
and signiﬁcantly improve attack effect of iterative methods
under black-box scenario.

2. Related Work

In black-box attack, attackers can only query target
model and get the score of each category [14]. One prac-
tical solution exploits transferability between two models,
i.e., phenomenon that adversarial examples generated by lo-
cal substitute model can fool the target model [16]. Four
existing attacks are introduced in the following.

Fast Gradient Sign Method (FGSM). As a classical
one-step attack, FGSM [5] ﬁnds the noise’s direction by cal-
culating the gradient of cross-entropy loss J(x, yT ):

x′ = x + ε · sign(▽J(x, yT )).

(1)

Iterative FGSM (I-FGSM). I-FGSM [9] splits uppper
bound of noise ε into several small step size α and adds
noises step by step:

x′
t+1 = Clipx,ε{x′

t + α · sign(▽J(x′

t, yT ))}.

(2)

I-FGSM possesses the highest attack effect among all
current iterative attacks in white-box scenario.
Its main
drawback is the diminishing marginal effect of iterative
steps. In other words, as the number of iterations t increases
and the step size α decreases, keeping adding the iteration
step has little improvement on attack effect.

Momentum Iterative FGSM (MI-FGSM). MI-FGSM
[4] introduced a momentum term to make the adjustment
of the noise-adding direction smoother, but the impact of
diminishing marginal effect on iteration number still exists:

mt+1 = µ · mt +

x′
t+1 = Clipx,ε{x′

t, yT ))
t, yT ))k

▽J(x′
k ▽ J(x′
t + α · sign(gt+1)}.

,

(3)

(4)

Variance-Reduced Iterative FGSM (vr-IGSM). Vr-
IGSM [27] uses an averaged gradient of original image with
gaussian noises to eliminate local ﬂuctuation in substitute
model and therefore improves the transferability.

Gt+1 =

1
m

m

Xi=1

▽J(xt + ξi),

ξi ∼ N (0, σ2I), (5)

x′
t+1 = Clipx,ε{x′

t + α · sign(Gt+1)}.

(6)

A series of defense methods have been proposed to im-
prove robustness of target models [15, 10, 12]. Among
them, adversarial training [24] and model ensemble are two
most widely-used methods. Adversarial training vaccinates
against adversarial examples by including them into the
training set of target model, while model ensemble reduces
speciﬁc error made by single model.

3. Curls & Whey Attack

3.1. Notation

An image classiﬁer based on DNN can be represented
as N : X W ×H×C → Y K , where X represents the input
space with dimension of W idth × Height × Channel and
Y represents the classiﬁcation space with K categories. A
successful adversarial attack changes the original classiﬁ-
cation result of image classiﬁer, i.e., the target model, after
adding as little noise as possible to the original image [26]:

min kx′ − xkv,

s.t. N (x) 6= N (x′) ,

(7)

where v refers to the norm used to measure the noise
magnitude including ℓ1, ℓ2 and ℓ∞ norm.
In this paper
we discuss noise magnitude in ℓ2 norm. Some existing

6520

2

3

1

4

5

Figure 2. Diminishing marginal effect on iteration number T . The
small blue ring at the bottom left represents the original image.
Five polylines marked 1(cid:13) -
5(cid:13) are iterative trajectories for T =
1, 2, 3, 5, ∞ cross the decision boundary.

works [28, 29, 4] compare the misclassiﬁcation rate with
a ﬁxed ℓ∞ norm, but we concentrate on the quality of ad-
versarial noises generated by different attacks on one im-
age. Here the black-box attack using substitute model [16]
is used to solve the problem that the target model cannot be
back propagated. The gradient information at step t refers
to the gradient value of the substitute model’s loss function
Jsub, i.e., cross-entropy loss, to adversarial example x′
t.

3.2. Diminishing Marginal Effect on Iteration Steps

Iterative attacks perform well in white-box scenarios,
where the transferability is guaranteed to be 100% [13].
However, when attacking against a black-box target model,
the drawbacks of iterative attacks gradually expose. First
of all, discrepancy on decision boundary burdens transfer-
ability between substitute model and target model [25]. It-
erative attacks always step toward the direction in which
loss function of substitute model increases. But there is a
huge gap on classiﬁcation spaces between different models.
Their gradient directions may be even orthogonal to each
other [11]. Therefore, simply searching for adversarial ex-
amples along the gradient ascent direction of the substitute
model may no longer be suitable for black-box attacks.

What’s more, diminishing marginal effect on the num-
ber of iterations exists. Now assume that in order to min-
imize the noise magnitude, the step size α of each step is
inversely proportional to the total iteration numbers. In I-
FGSM, when the number of iterations T increases by 1, the
marginal gain for the decrease in the noise magnitude is

1

T + 1

T +1

Xt=1

· ▽Jsub(xt) −

1
T

T

Xt=1

· ▽Jsub(xt).

(8)

In general, as T increases and the single step size
shortens, the iterative trajectory tends to be consistent and
smooth and gradually converges, as shown in Fig. 2. Con-
sidering that the number of queries to the target model in

black-box attack is also limited, increasing the iteration
number has little effect on adversarial noise reducing if the
iteration number is already high.

3.3. Curls Iteration

Iterative trajectories of current iterative attacks in black-
box scenario are monotonic. First, monotonically employ-
ing gradient ascent along substitute model’s loss function
is more likely to bring iterative trajectories into local opti-
mum of substitute model, rather than passing through the
decision boundary of target model. Second, simply rely-
ing on transferability between substitute model and target
model, but ignoring the feedback of target model after each
query makes the iterative trajectories lack adaptability.

To ‘curl’ up and diversify the iterative trajectory may be
a more cost-effective solution [19]. Fig. 1 shows one pos-
sible distribution of target model loss function. In the case
that loss function rises slowly along the direction of gradi-
ent ascend, like the green trajectory, it may be possible to
ﬁnd a shortcut across the decision boundary from a nearby
starting point, as shown by the purple polyline in Fig. 1. We
abandon the monotonic search strategy base on gradient as-
cend to increase the diversity of iterative trajectories:

x′

0 = x, x′

1 = Clipx,ε{x′

0 − α · ▽Jsub(x′

0)}, (9)

gt+1 = ( − ▽ Jsub(x′

▽Jsub(x′

t) J(x′
t) J(x′
t + α · gt+1},

t) < J(x′
t) ≥ J(x′

t−1),
t−1),

(10)

(11)

x′
t+1 = Clipx,ε{x′
t) and J(x′
where Jsub(x′
t) represent the cross entropy loss
of adversarial example x′
t on the substitute model and the
target model, respectively. First, update the original image
for one step along the direction of gradient descent. When
the cross entropy loss of current adversarial example on tar-
get model is lower than the previous step, usually the ‘valley
ﬂoor’, i.e., the local minimum of loss function has not yet
been reached. Therefore, when the loss on the target model
is still declining, continue to update along the direction of
gradient descend, and vice versa. We regard this ‘ﬁrst go
down then go up’ iterative method as Curls iteration.

On the basis of Curls, we introduce two heuristic strate-
gies before and after each round of iteration. For an im-
age, the closest adversarial examples are more likely to dis-
tribute in roughly the same direction in the feature space.
Therefore, we record and update the average direction of
all adversarial examples of one image, ¯R, and add a vector
pointing to this direction in the ﬁrst step when calculating
gradients for each round:

¯R =

1
K

K

Xi=1

x′,

s.t. N (x) 6= N (x′) ,

x′

1 = Clipx,ε{x′

0 + α · ▽J(x′

0 + α · ¯R)}.

(12)

(13)

6521

6:

7:

8:

9:

10:

11:

12:

13:

14:

Algorithm 1 Curls Iteration
Input: Target DNN N (x), substitute model Sub(x)

Original image x and label y
Initial noise magnitude limit ε
Iteration step T and variance of gaussian noise s
Step size α and binary search step bs

Output: Adversarial example x′
1: Initialize ¯R and two starting points
2: ¯R = 0, xA
3: downhill = T rue // Set the gradient descend ﬂag to True
4: for t = 0 to T do
5:

0 = x, xB

0 = x

t ∼ N (0, s2I)

t , ξB
ξA
Calculate gradient on substitute model
gA
t = ▽Jsub(xA
gB
t = ▽Jsub(xB

t + α · ¯R)
t + α · ¯R)

t + ξA
t + ξB

xA

t+1 =(cid:26) Clipx,ε{xA

Clipx,ε{xA

t − α · gA
t }
t + α · gA
t }

downhill = T rue
downhill 6= T rue

xB
t+1 = Clipx,ε{xB
t + α · gB
t }
if downhill = T rue and J(xA

t+1) > J(xA

t ) then

downhill = F alse

end if
if N (xA

t+1) 6= N (x) or N (xB

t+1) 6= N (x) then

update ¯R by Eqn. (12)

end if

15:
16: end for
17: if N (xA

T ) 6= N (x) or N (xB

T ) 6= N (x) then

x′ =(cid:26) xA

T
xB
T

T − xk2 < kxB

T − xk2

kxA
else

reﬁne x′ by Eqn. (15)

18:
19: end if
20: return x′

Since the iterative trajectory cannot be a straight line in
the high-dimensional feature space, situation shown in the
red arcs in Fig. 1 exists:
there are adversarial examples
with smaller ℓ2 distance between the adversarial example
found and original image. We perform binary search be-
tween original image x and adversarial example x′ after
each round to fully exploit the potential of this round:

L = x, R = x′,

BS(L, R) =


BS(L, (L + R)/2),
if N (x) 6= N ((L + R)/2),
BS((L + R)/2, R),

if N (x) = N ((L + R)/2).

(14)

(15)

In the actual implementation of Curls iteration, in order
to prevent the oscillation of adversarial noise update, we do

not directly determine the gradient symbol on account of
target model’s loss function, but divide each iterative round
into two stages.
In the ﬁrst stage, carry out gradient de-
scend to the original image. Once the cross entropy on tar-
get model is lower than the previous step, the second stage
starts and carries out gradient ascend until the last step. At
the same time, the normal iterative trajectory of direct gra-
dient ascent is performed simultaneously. In addition, in-
spired by vr-IGSM [27], we add gaussian noise to image in
gradient calculation process to improve the transferability.
Algorithm 1 details Curls iteration.

3.4. Whey Optimization

Usually an iterative attack ends as soon as it ﬁnds adver-
sarial example or runs out of iteration number. However,
adversarial examples generated may still contain redundant
‘whey’ noises after iteration. Or the maximum extent to
which noises can be reduced, while ensuring the adversar-
ial example can still fool the target model [1]:

max(k x′ −x k2 − k x◦ −x k2),

s.t. N (x′) = N (x◦),

where x, x′ and x◦ refers to original image, adversarial ex-
ample found by now and the closest adversarial example to
the original image, respectively.

Since binary search between x and x′ is already per-
formed, adversarial examples with less redundant noises
are more likely to exist in a linearly independent direction
with respect to x′ − x. We propose Whey optimization
to squeeze out the remaining ‘whey’ of redundant noises
in black-box attack. Whey optimization maintains a bal-
ance between noise-squeezing amplitude and the number of
squeezes. Squeezing excessive noises at a time may return
adversarial examples to the original category. Nevertheless,
an incremental squeeze makes it impossible for optimiza-
tion to complete within a limited number of queries. A com-
promise solution is to divide adversarial noises into groups
ﬁrst, then try to reduce noise magnitude group by group:

z0 = x′ − x,
zwhc
t+1 = zwhc
/2,

t

(16)
t = L(V (z0), t), (17)

s.t. zwhc

where z is the noise, L(V, t) represents number with the
tth largest absolute value in pixel value set V :

V (z) = {v | v = zwhc, w ∈ [0, W ], h ∈ [0, H], c ∈ [0, C]}

W, H, C represents the width, height and channel of orig-
inal image x, respectively. Whey optimization divides
noise z into several groups according to the pixel value, se-
lects one group each time in descending order, reduces all
pixel value in z which equals to L(V, t) by half and check
whether the trimmed noises can still fool the target model.
After squeezing in groups, we perform more ﬁne-grained
squeeze. The last step of Whey optimization set the value

6522

Algorithm 2 Whey Optimization
Input: Target DNN N (x) and adversarial example x′

Original image x and label y
Max attempt number for two squeeze steps, T1, T2
Pixel value set of x′ − x, P
Random number generator over [0, 1], random()

Output: Reﬁned adversarial example x∗
1: z = x′ − x
2: t1 = 0, t2 = 0
3: for p in P and t1 < T1 do // Step 1: Squeeze in groups
4:

Reduce the pixel value by half

z [z = p] / = 2
if N (z) = y then

Cancel the update of this step

end if
t1 = t1 + 1

9:
10: end for
11: while t2 < T2 do // Step 2: Squeeze stochastically
12:

Generate a random mask same shape as the image

maskwhc =(cid:26) 0

1

random() ≤ 0.01,
else.

z = z · mask // Element-wise product
if N (z) = y then

Cancel the update of this step

5:

6:

7:

8:

13:

14:

15:

16:

end if
t2 = t2 + 1

17:
18: end while
19: x∗ = z + x
20: return x∗

of each pixel to 0 with probability of δ:

zt+1 = zt · maskt,

maskwhc = (cid:26) 0

1

random() ≤ δ,
else,

(18)

(19)

where mask is the same shape as z. Algorithm 2 gives the
detail of Whey optimization.

3.5. Targeted Attack

Unlike untargeted attack, targeted attack requires not
only the adversarial example be misclassiﬁed by the tar-
get model, but also it can be misclassiﬁed into the speci-
ﬁed category. This is especially difﬁcult in black-box attack
because the decision boundaries between different models
vary greatly, and the gradient direction are even orthogo-
nal to each other [11]. Even if the update of each step is
changed from gradient ascend with respect to the original
category ▽Jsub(x′, yori) to gradient descend with respect
to the target category − ▽ Jsub(x′, ytarget) [4], an iterative
trajectory from original image is almost impossible to reach

the target category space, due to the difference in gradient
values between target model and substitute model.

We abandon the ‘start from scratch’ strategy and inte-
grate interpolation to the iterative attack to get a better ini-
tial update direction. First, we collect a legitimate image xT
that can be classiﬁed into the target category by the target
model. Second, we use binary search to ﬁnd an image x′
0
between the original image x and xT , making sure that x′
0
can also be classiﬁed into the target category. After that, we
use x′
0 to guide the ﬁrst gradient ascent step starting from x:

x′
x′
t+1 = Clipx,ε{x′
x′

0 = (1 − s) · x + s · xT ,
1 = Clipx,ε{x − α · ▽J(x′
t − α · ▽J(x′

0)},
t)}, t ≥ 1,

(20)

(21)

(22)

where 0 < s < 1 indicates the interpolation coefﬁcient
determined by binary search. In this way, we boost original
example into the direction towards the target category. After
the ﬁrst boosting step, we continue to apply Curls&Whey
attack as in untargeted attacks.

4. Experiments

4.1. Experiment Settings

All our experiments are performed on Tiny-Imagenet
used in NIPS 2018 Adversarial Vision Challenge [3] and
Imagenet [18], with image shape of 64 × 64 × 3 and
224 × 224 × 3, respectively. Imagenet contains 1000 im-
age categories. We picked 10000 images from its valida-
tion set that can be correctly classiﬁed by all target models,
10 images for each category. As for Tiny-Imagenet with
200 image categories, we choose 2000 images, 10 images
for each category. 8 neural network models with different
structures are compared: resnet-18 [6], resnet-101, incep-
tion v3 [22], inception-resnet v2 [21], nasnet [30], densenet-
161 [8], vgg19-bn [20], senet-154 [7].

We implement our black-box iterative attack on Foolbox
[17] framework. In order to accurately measure the attack
effect of each method, a large loop for determining ε is
added outside the iterative process. For evaluation criterion,
we choose the median and average size of adversarial per-
turbation transferred from substitute model to target model,
as applied in NIPS 2018 Adversarial Vision Challenge [3]:

mid(Sub, N ) = median({d(x, x∗) | x ∈ X}),(23)

avg(Sub, N ) =

1
N

N

Xi=1

({d(x, x∗) | x ∈ X}), (24)

d(x, x∗) = kx − x∗k2,

(25)

where sub and N represent substitute model and target
model, respectively. x is an original image in the test set
X. x∗ is the adversarial example found that is closest to
x. d(x, x∗) returns the ℓ2 distance between x and x∗. A

6523

Table 1. Median and average ℓ2 distance of adversarial perturbation crafted from pairwise attack between four models.

resnet18

inceptionv3

inception resnet v2

nasnet

attack methods median

average median

average median

average median

average

resnet 18

inception v3

inception resnet v2

nasnet

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls

Curls&Whey

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls

Curls&Whey

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls

Curls&Whey

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls

Curls&Whey

0.1321
0.0800
0.0866
0.0941
0.0731
0.0627

0.9944
0.6699
0.8124
0.6072
0.5760
0.5140

1.6729
0.7019
0.8561
0.6463
0.6040
0.5227

3.7356
1.5575
0.9518
0.5659
0.5821
0.5543

0.8893
0.0881

0.1029
0.1120
0.1182
0.1040

3.6262
1.8883
2.2895
1.7973
1.6781
1.4941

5.0270
2.3966
2.8611
2.4453
2.0220
1.2404

6.0550
4.1401
3.0544
2.4410
2.1520
1.8582

4.3085
1.9686
2.3220
1.8737
1.6443
1.1942

0.1521
0.1132

0.1283
0.1297
0.1243
0.1252

4.2482
1.3314
1.6342
1.3166
1.1325
0.8431

3.5277
1.5926
1.8850
1.5006
1.2719
1.0003

7.4580
2.9287
3.4386
2.8228
2.4739
1.7387

1.9010
0.1518

0.1989
0.1834
0.2194
0.9200

6.6191
2.3834
3.0884
2.6256
1.9407
1.3437

7.2388
4.3745
3.9685
3.2440
3.9490
3.6760

3.6764
2.4624
2.9526
2.4803
1.8507
1.4549

2.6171
1.3415
1.6248
1.3214
1.1163
0.9058

0.2855
0.1293

0.1602
0.1640
0.1501
0.1485

3.4829
1.4180
1.6458
1.3066
1.2048
0.9599

5.3257
3.3192
3.9267
3.4085
2.6290
1.9450

4.9078
1.9095
2.4642
2.0991
1.8997
1.7913

4.5974
0.3814
0.5419
0.5197
0.3450
0.3199

7.1657
4.2968
3.7643
3.1112
4.1637
3.6069

3.4187
2.1865
2.0174
1.7991
1.6773
1.3902

2.8729
1.3774
1.6800
1.3569
1.2335
0.9398

4.1107
1.3761
1.6594
1.3292
1.0978
0.8483

0.2008
0.1173

0.1317
0.1371
0.1360
0.1354

4.5589
2.9644
2.9723
2.7645
2.4919
1.9696

4.5217
2.1675
2.7336
2.3010
2.1067
1.9315

5.5487
2.3732
3.0469
2.6710
1.9644
1.4403

6.3891
1.8225
0.3632
0.3197

2.7491
2.5653

smaller ℓ2 distance indicates a stronger attack effect and
higher the transferability of generated adversarial examples.

4.2. Black box Attack on Multiple Models

We report the median and average adversarial perturba-
tion on Tiny-Imagenet in Table 1. In this 4 × 4 matrix, each
element represents the result of substitute model of this row
against the target model of this column over the entire 2000
images. Elements on diagonal are results of white-box at-
tacks (marked in italics). Fig. 4 shows median perturba-
tion on three target models when using vgg19-bn as substi-
tute model. More experiments on Imagenet can be found in
supplemental material. For each pair of substitute and tar-
get model, we compare our methods (Curls&Whey as well
as Curls only) with FGSM [5] and three other iterative at-
tacks, I-FGSM [9], MI-FGSM [4] and vr-IGSM [27]. Since
ℓ2 norm is used to measure noise magnitude, we no longer
use sign function to update adversarial examples. For the
fairness of comparison, the number of queries to the target
model is basically equal for the iterative attacks. Table 2
reports parameters related to query number, including iter-
ative round number T0, iteration step T , binary search step
bs, max attemp number for two squeeze steps in Whey opti-
mization T1 and T2. The total query number for our method

Table 2. Parameter set for experiments on two datasets.

Tiny-

Imagenet

Others
Ours

Imagenet Others
Ours

T0

20
10

24
14

T

10
4

24
7

bs

–
2

–
3

T1

–
40

–

T2

–
40

–

200

100

Total

200
200

576
580

is T0 ×(T +bs)×2+T1 +T2, and T0 ×T for other iterative
methods. The initial noise magnitude ε and stepsize α are
0.3 and 1/2T , respectively. For variance of gaussian noise
in vr-IGSM and our method, we set s = 1.

It can be seen from Table 1 that Curls&Whey achieves
smaller median noise magnitude in ℓ2 norm than all other
methods, and smaller average magnitude than most other
methods, on black-box attacks, i.e., off-diagonal elements.
With the diversiﬁcation of iterative trajectories and squeeze
of redundant noises, noises are reduced by 20%-30%, in
some cases even 40%, over most model combinations.
Curls iteration alone also outperforms existing methods in
almost all black-box attacks. Due to gaussian noises in
gradient-calculating process, noise magnitude of our meth-
ods are slightly higher than I-FGSM in white-box attacks,
where transferability is no need to be considered. However,

6524

2

1.5

1

e
c
n
a
t
s
i
d

2
ℓ
n
a
i
d
e
m

1.2

1

0.8

5

10

15

20

0

Iteration step T

10

Scale s

20

0.87

0.86

0.85

0.84

0.83

2

4

6

8

10

binary search step bs

Curls&Whey

vr-IGSM

I-FGSM

MI-FGSM

Figure 3. Median noise magnitude under different iteration steps (left), T = (4, 8, 12, 16, 20) , gaussian noise variance (middle), s =
(1, 5, 10, 15, 20) and binary search step (right), bs = (2, 4, 6, 8, 10).

e
c
n
a
t
s
i
d

2
ℓ
n
a
i
d
e
m

5

4

3

resnet-101 densenet-161 senet-154

Target Models

I-FGSM MI-FGSM vr-IGSM Curls&Whey

Figure 4. Median ℓ2 distance comparison of adversarial noises
generated using vgg19-bn as substitute model on Imagenet.

white-box noise of our method is still smaller than that of
vr-IGSM, which validates the effectiveness of Whey opti-
mization. Fig. 5 shows adversarial examples crafted on two
datasets. Curls & Whey achieves targeted and untargeted
misclassiﬁcation with nearly imperceptible noises.

4.3. Ablation Study

Here we investigate inﬂuence of iteration step T , binary
search step bs and variance of gaussian noise s to black-
box attack effect. We use inception-resnet v2 and incep-
tion v3 as substitute and target model, respectively. Results
on Tiny-Imagenet under different T , s and bs is shown in
Fig. 3. As discussed in Section 3, although T is negatively
correlated with noise magnitude, diminishing marginal ef-
fect exists. The noise drop of T = 20 relative to T = 16
is obviously not as great as the drop of T = 8 relative to
T = 4. Our method does not simply increase the iteration
number, but improve the diversity of iterative trajectories.
Therefore, Curls&Whey is able to ﬁnd adversarial exam-
ples with smaller ℓ2 norm with equal queries, and use part
of the query to reﬁne adversarial noises.

Variance s is related to the transferability between sub-
stitute and target model. The higher the s, the greater the
likelihood that adversarial example may transfer from one

Table 3. Incremental comparison on each part of Curls&Whey.

Curls

+BS

+Whey(1)

+Whey(2)

median

1.3138

1.1111

0.9354

average

2.3154

1.9039

1.4723

0.8431

1.3437

model to another highly different model. However, as the
variance of gaussian noise increases, the proportion of orig-
inal image in gradient calculation process will gradually de-
crease, resulting in decline in transferability. Therefore, a
local minimum appears in the results on different s. As can
be seen from Fig. 3, when using inception-resnet v2 to at-
tack inception v3, the local optimal value of s is around 10.
As for binary search step, a larger bs means more binary
search between the adversarial example and original image.
As an auxiliary process in Curls iteration, a relatively small
bs is sufﬁcient to reduce the noises.

To verify the effectiveness of each part of our attack
method, we conduct ablation experiment on Curls&Whey.
As can be seen from Table 3, whether it is Curls iteration,
binary search (BS), or two steps in Whey optimization, each
component can effectively reduce the noise magnitude.

4.4. Targeted Attack Results

For targeted attack, we assign 5 target categories for each
image and calculate the ℓ2 distance between original im-
age and adversarial examples of each category. We select
one image from the test set that can be classiﬁed into target
category for interpolation. We choose resnet18 and incep-
tionv3 as our substitute model and three other models as
target models. As shown in Fig. 6, three existing itera-
tive attacks have difﬁculties achieving targeted misclassiﬁ-
cation. Compared to three decision-based attacks, boundary
attack [2], pointwise attack and vanilla interpolation [17],
noise magnitude of our method is also signiﬁcantly reduced.
This conﬁrms the effectiveness of integrating interpolation
method into Curls & Whey attack.

6525

Egyptian cat

bee

gas helmet

water tower

(cid:1876)

(cid:1876)(cid:3021)

(cid:1876) (cid:1499) (cid:3398) (cid:1876)

(cid:1876) (cid:1499)

(a) Targeted Attack

American alligator

loggerhead turtle

tiger beetle

(cid:1876)

(cid:1876) (cid:1499) (cid:3398) (cid:1876)

(b) Untargeted Attack

bubble

(cid:1876) (cid:1499)

Figure 5. Adversarial examples generated by Curls & Whey attack. Targeted attack results on Tiny-Imagenet are shown on subplot (a).
Original image x, image of target category xT , noise x∗ − x and adversarial example x∗ are listed from left to right. Untargeted results on
Imagenet are shown on subplot (b). Classiﬁcation result on target model are shown at the bottom.

inceptionv3 inc-resnet v2

nasnet

2
ℓ
n
a
i
d
e
m

2
ℓ
n
a
i
d
e
m

80
60
40
20
0

80
60
40
20
0

resnet18 inc-resnet v2

nasnet

I-FGSM
pointwise

Curls&Whey

MI-FGSM

vr-IGSM

boundary attack

interpolation

Figure 6. Median ℓ2 distance comparison of targeted adversarial
noises generated using resnet18 (up) and inceptionv3 (down) as
substitute model on Tiny-Imagenet.

4.5. Attack on Defence and Ensemble Models

Adversarial training [24] and model ensemble are two
widely used defend methods. In Table 4, we use resnet18 as
substitute models to attack two adversarially trained models
(inceptionv3 and inception-resnet v2) and ensemble model
consisting of three models. Although defence methods in-
crease the difﬁculty of adversarial attack compared with Ta-
ble 1, the noise magnitude of adversarial examples built by
Curls & Whey is still much lower than other attacks.

Table 4. Median and average ℓ2 distance of adversarial perturba-
tion against adversarially trained models and ensemble model.

target model

attack methods median

average

inceptionv3(adv)

inc-resnet v2(adv)

inceptionv3+
inc-resnet v2+

nasnet

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls&Whey

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls&Whey

FGSM
I-FGSM
MI-FGSM
vr-IGSM

Curls&Whey

6.5812
2.8839
3.8039
3.2752
2.0633

4.7029
3.3195
3.9919
3.3829
2.2852

4.5826
2.7742
3.5819
3.0785
2.0321

9.1681

3.76

4.6529
4.1449
2.6349

6.2954
3.9606
4.9481
4.2706
2.7884

5.9755
3.595
4.5227
4.0499
2.6187

the iterative trajectory and squeeze the adversarial noises
respectively.
In addition, we integrate interpolation to it-
erative attack to reduce the difﬁculty of targeted attacks in
black-box scenario signiﬁcantly. Experimental results on
Tiny-Imagenet and ImageNet demonstrate that compared to
existing iterative attacks, Curls & Whey generates adversar-
ial examples with smaller ℓ2 distance and stronger transfer-
ability against a variety of target models.

5. Conclusion

Acknowledgements

We propose Curls & Whey, a new black-box attack con-
taining Curls iteration and Whey optimization, to diversify

This work is supported by the NSFC (under Grant

61876130, U1509206).

6526

References

[1] Anish Athalye and Ilya Sutskever. Synthesizing robust ad-
versarial examples. arXiv preprint arXiv:1707.07397, 2017.
4

[2] Wieland Brendel, Jonas Rauber, and Matthias Bethge.
Decision-based adversarial attacks: Reliable attacks against
black-box machine learning models.
arXiv preprint
arXiv:1712.04248, 2017. 7

[3] Wieland Brendel, Jonas Rauber, Alexey Kurakin, Nicolas
Papernot, Behar Veliqi, Marcel Salath´e, Sharada P Mohanty,
and Matthias Bethge. Adversarial vision challenge. arXiv
preprint arXiv:1808.01976, 2018. 2, 5

[4] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xi-
aolin Hu, Jianguo Li, and Jun Zhu. Boosting adversarial at-
tacks with momentum. CVPR, 2018. 1, 2, 3, 5, 6

[5] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In ICLR,
2015. 1, 2, 6

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 5

[7] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017. 5

[8] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely
connected convolutional networks. CVPR, pages 2261–
2269, 2017. 5

[9] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adver-
sarial examples in the physical world. ICLR Workshop, 2017.
1, 2, 6

[10] Xin Li and Fuxin Li. Adversarial examples detection in deep
networks with convolutional ﬁlter statistics. In ICCV, pages
5775–5783, 2017. 2

[11] Yanpei Liu, Xinyun Chen, Cheng Chih Liu, and Dawn Xi-
aodong Song. Delving into transferable adversarial exam-
ples and black-box attacks. CoRR, abs/1611.02770, 2016. 1,
2, 3, 5

[12] Dongyu Meng and Hao Chen. Magnet: a two-pronged de-
fense against adversarial examples.
In Proceedings of the
2017 ACM SIGSAC Conference on Computer and Commu-
nications Security, pages 135–147. ACM, 2017. 2

[13] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.
Transferability in machine learning:
from phenomena to
black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016. 3

[14] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. In Secu-
rity and Privacy (EuroS&P), 2016 IEEE European Sympo-
sium on, pages 372–387. IEEE, 2016. 2

[15] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha,
and Ananthram Swami. Distillation as a defense to adver-
sarial perturbations against deep neural networks. In 2016
IEEE Symposium on Security and Privacy (SP), pages 582–
597. IEEE, 2016. 2

[16] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practi-

cal black-box attacks against machine learning. In AsiaCCS,
2017. 1, 2, 3

[17] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Fool-
box v0. 8.0: A python toolbox to benchmark the ro-
bustness of machine learning models.
arXiv preprint
arXiv:1707.04131, 2017. 5, 7

[18] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015. 2, 5

[19] Tegjyot Singh Sethi and Mehmed Kantardzic. Data driven
exploratory attacks on black box classiﬁers in adversarial do-
mains. Neurocomputing, 289:129–143, 2018. 3

[20] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
5

[21] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In AAAI, vol-
ume 4, page 12, 2017. 5

[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2818–2826, 2016. 5

[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fer-
gus.
ICLR,
abs/1312.6199, 2014. 1

Intriguing properties of neural networks.

[24] Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan
Boneh, and Patrick D. McDaniel. Ensemble adversarial
training: Attacks and defenses. CoRR, abs/1705.07204,
2017. 2, 8

[25] Florian Tram`er, Nicolas Papernot, Ian J. Goodfellow, Dan
Boneh, and Patrick D. McDaniel. The space of transferable
adversarial examples. CoRR, abs/1704.03453, 2017. 3

[26] Beilun Wang, Ji Gao, and Yanjun Qi. A theoretical frame-
work for robustness of (deep) classiﬁers against adversarial
examples. ICLR Workshop, 2017. 2

[27] Lei Wu, Zhanxing Zhu, Cheng Tai, et al. Understanding and
enhancing the transferability of adversarial examples. arXiv
preprint arXiv:1802.09707, 2018. 1, 2, 4, 6

[28] Cihang Xie, Zhishuai Zhang, Jianyu Wang, Yuyin Zhou,
Zhou Ren, and Alan Yuille.
Improving transferability of
adversarial examples with input diversity. arXiv preprint
arXiv:1803.06978, 2018. 3

[29] Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xi-
angqi Huang, Xiang Gan, and Yong Yang. Transferable ad-
versarial perturbations.
In Computer Vision–ECCV 2018,
pages 471–486. Springer, 2018. 3

[30] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V.
Le. Learning transferable architectures for scalable image
recognition. CoRR, abs/1707.07012, 2017. 5

6527

