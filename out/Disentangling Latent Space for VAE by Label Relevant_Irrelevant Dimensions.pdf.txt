Disentangling Latent Space for VAE by Label Relevant/Irrelevant Dimensions

Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University

51171214020@stu.ecnu.edu.cn

sunli@ee.ecnu.edu.cn

Zhilin Zheng

Li Sun

Abstract

VAE requires the standard Gaussian distribution as a
prior in the latent space. Since all codes tend to follow
the same prior, it often suffers the so-called ”posterior col-
lapse”. To avoid this, this paper introduces the class spe-
ciﬁc distribution for the latent code. But different from
cVAE, we present a method for disentangling the latent
space into the label relevant and irrelevant dimensions, zs
and zu, for a single input. We apply two separated en-
coders to map the input into zs and zu respectively, and
then give the concatenated code to the decoder to recon-
struct the input. The label irrelevant code zu represent the
common characteristics of all inputs, hence they are con-
strained by the standard Gaussian, and their encoder is
trained in amortized variational inference way, like VAE.
While zs is assumed to follow the Gaussian mixture distri-
bution in which each component corresponds to a particular
class. The parameters for the Gaussian components in zs
encoder are optimized by the label supervision in a global
stochastic way. In theory, we show that our method is actu-
ally equivalent to adding a KL divergence term on the joint
distribution of zs and the class label c, and it can directly
increase the mutual information between zs and the label
c. Our model can also be extended to GAN by adding a
discriminator in the pixel domain so that it produces high
quality and diverse images.

1. Introduction

Learning a deep generative model for the structured im-
age data is difﬁcult because this task is not simply modeling
a many-to-one mapping function such as the classiﬁcation,
instead it is often required to generate diverse outputs for
similar codes sampled from a simple distribution. Further-
more, image x in the high dimension space often lies in a
complex manifold, thus the generative model should cap-
ture the underlying data distribution p(x).

Basically, Variational Auto-Encoder (VAE) [33, 19] and
Generative Adversarial Network (GAN) [12, 24] are two
strategies for structured data generation. In VAE, the en-

coder qφ(z|x) maps data x into the code z in latent space.
The decoder, represented by pθ(x|z), is given a latent code
z sampled from a distribution speciﬁed by the encoder and
tries to reconstruct x. The encoder and decoder in VAE
are trained together mainly based on the data reconstruc-
tion loss. At the same time, it requires to regularize the
distribution qφ(z|x) to be simple (e.g. Gaussian) based on
the Kullback-Leibler (KL) divergence between q(z|x) and
p(z) = N (0, I), so that the sampling in latent space is easy.
Optimization for VAE is quite stable, but results from it are
blurry. Mainly because the posterior deﬁned by qφ(z|x)
is not complex enough to capture the true posterior, also
known for ”posterior collapse”. On the other hand, GAN
treats the data generation task as a min/max game between
a generator G(z) and discriminator D(x). The adversarial
loss computed from the discriminator makes generated im-
age more realistic, but its training becomes more unstable.
In [9, 21, 27], VAE and GAN are integrated together so that
they can beneﬁt each other.

Both VAE and GAN work in an unsupervised way with-
out giving any condition of the label on the generated im-
age. Instead, conditional VAE (cVAE) [36, 3] extends it by
showing the label c for both encoder and decoder. It learns
data distribution conditioned on the given label. Hence,
the encoder and decoder become qφ(z|x, c) and pθ(x|z, c).
Similarly, in conditional GAN (cGAN) [8, 17, 32, 29] la-
bel c is given to both generator G(z, c) and discriminator
D(x, c). Theoretically, feeding label c to either the encoder
in VAE or decoder in VAE or GAN helps increasing the
mutual information between the generated x and the label
c. Thus, it can improve the quality of generated image.

This paper deals with image generation problem in VAE
with two separate encoders. For a single input x, our goal
is to disentangle the latent space code z, computed by en-
coders, into the label relevant dimensions zs and irrelevant
ones zu. We emphasize the difference between zs and zu,
and their corresponding encoders. For zs, since label c is
known during training, it should be more accurate and spe-
ciﬁc. While without any label constraint, zu should be gen-
eral. Speciﬁcally, the two encoders are constrained with dif-
ferent priors on their posterior distributions qφs (zs|x) and

12192

qφu (zu|x). Similar with VAE or cVAE, in which the full
code z is label irrelevant, the prior for zu is also chosen
N (0, I). But different from previous works, the prior p(zs)
becomes complex to capture the label relevant distribution.
From the decoder’s perspective, it takes the concatenation
of zs and zu to reconstruct the input x. Here the distinction
with cVAE and cGAN is that they uses the ﬁxed, one-hot
encoding label, while our work applies zs, which is consid-
ered to be a variational, soft label.

Note that there are two stages for training our model.
First, the encoder for zs gets trained for classiﬁcation task
under the supervision of label c. Here instead of the softmax
cross entropy loss, Gaussian mixture cross entropy loss pro-
posed in [39] is adopted since it accumulates the mean µc
and variance σc for samples with the same label c, and mod-
els it as the Gaussian N (µc, σc), hence zs ∼ N (µc, σc).
The ﬁrst stage speciﬁes the label relevant distribution. In
the second stage, the two encoders and the decoder are
trained jointly in an end-to-end manner based on the recon-
struction loss. Meanwhile, priors of zs ∼ N (µc, σc) and
zu ∼ N (0, I) are also considered.

The main contribution of this paper lies in following as-
pects: (1) for a single input x to the encoder, we provide
an algorithm to disentangle the latent space into label rele-
vant and irrelevant dimensions in VAE. Previous works like
[14, 4, 35] disentangle the latent space in AE not VAE. So it
is impossible to make the inference from their model. More-
over, [26, 4, 22] requires at least two inputs for training. (2)
we ﬁnd the Gaussian mixture loss function is suitable way
for estimating the parameters of the prior distribution, and
it can be optimized in VAE framework. (3) we give both a
theoretical derivation and a variety of detailed experiments
to explain the effectiveness of our work.

2. Related works

Two types of methods for the structured image gener-
ation are VAE and GAN. VAE [19] is a type of paramet-
ric model deﬁned by pθ(x|z) and qφ(z|x), which employs
the idea of variational inference to maximize the evidence
lower bound (ELBO), as is shown in Eq. 1.

log p(x) ≥ E

qφ(z|x)(log pθ(x|z)) − DKL(qφ(z|x)||p(z))
(1)
The right side of the above is the ELBO, which is the lower
bound of maximum likelihood.
In VAE, a differentiable
encoder-decoder are connected, and they are parameterized
by φ and θ, respectively. Eqφ(z|x)(log pθ(x|z)) represents
the end-to-end reconstruction loss, and KL(qφ(z|x)||p(z))
is the KL divergence between the encoder’s output distribu-
tion qφ(z|x) and the prior p(z), which is usually modeled
by standard normal distribution N (0, I). Note that VAE
assumes that the posterior qφ(z|x) is of Gaussian, and the
µ and σ are estimated for every single input x by the en-

coder. This strategy is named amortized variational infer-
ence (AVI), and it is more efﬁciency than stochastic varia-
tional inference (SVI) [16].

VAE’s advantage is that its loss is easy to optimize, but
the simple prior in latent space may not capture the com-
plex data patterns which often leads to the mode collapse
in latent space. Moreover, VAE’s code is hard to be in-
terpreted. Thus, many works focus on improving VAE on
these two aspects. cVAE [36] adds the label vector as the
input for both the encoder and decoder, so that the latent
code and generated image are conditioned on the label, and
potentially prevent the latent collapse. On the other hand, β-
VAE [15, 7] is a unsupervised approach for the latent space
disentanglement. It introduces a simple hyper-parameter β
to balance the two loss term in Eq. 1. A scheme named
inﬁnite mixture of VAEs is proposed and applied in semi-
supervised generation [1]. It uses multiple number of VAEs
and combines them as a non-parametric mixture model. In
[18], the semi-amortized VAE is proposed. It combines AVI
with SVI in VAE. Here the SVI estimates the distribution
parameters on the whole training set, while the AVI in tra-
ditional VAE gives this estimation for a single input.

GAN [12] is another technique to model the data distri-
bution pD(x). It starts from a random z ∼ p(z), where p(z)
is simple, e.g. Gaussian, and trains a transform network
gθ(z) under the help of discriminator Dφ(·) so that pθ(z)
approximates pD(x). The later works [31, 25, 2, 13, 28]
try to stabilize GAN’s training. Traditional GAN works in a
fully supervised manner, while cGAN [17, 32, 29, 6] aims to
generate images conditioned on labels. In cGAN, the label
is given as an input to both the generator and discrimina-
tor as a condition for the distribution. The encoder-decoder
architecture like AE or VAE can also be used in GAN. In
ALI [10] and BiGAN [9], the encoder maps x to z, while
the decoder reverses it. The discriminator takes the pair of z
and x, and is trained to determine whether it comes from the
encoder or decoder in an adversarial manner. In VAE-GAN
[21, 23], VAE’s generated data are improved by a discrimi-
nator. Similar idea also applies to cVAE in [3]. VAE-GAN
also applies in some speciﬁc applications like [4, 11].

Since code z potentially affects the generated data, some
works try to model its effect and disentangle the dimensions
of z. InfoGAN [8] reveals the effect of latent space code c
by maximizing the mutual information between c and the
synthetic data gθ(z, c). Its generator outputs gθ(z, c) which
is inspected by the discriminator Dφ(·). Dφ(·) also tries to
reconstruct the code c. In [26], the latent dimension is dis-
entangled in VAE based on the speciﬁed factors and unspec-
iﬁed ones, which is similar with our work. But its encoder
takes multiple inputs, and the decoder combines codes from
different inputs for reconstruction. The work in [14] mod-
iﬁes [26] by taking a single input. To stabilize training, its
model is built in AE not VAE, hence it can’t perform vari-

12193

c

LGM
GM

μc
μc
Σc(cid:3) 
Σc

μ5

μ1

μ9

μ2

μ4

μ6

μ10

μ3

μ7

μ8

Encoder

S

x

EncoderU

zs
s

Lkl

zu
z

Decoder

Lrec

x(cid:268) 

adv
LC

adv
LE

Discrimi-

nator

adv
LGD

adv
LD

Adversarial

Classifier

Figure 1. The network architecture. We disentangle class rele-
vant dimensions zs and class irrelevant dimensions zu in the la-
tent space. The Encoders maps input image x to zs, and forces
zs to be well classiﬁed while following a Gaussian mixture distri-
bution with learned mean µc and covariance Σc. Meanwhile, the
Encoderu extracts zu from x and pushes it to match the standard
Gaussian N (0, I). The adversarial classiﬁer is added on the top
of zu to distinguish the class of zu, while Encoderu tries to fool
it. Then zs and zu are concatenated and fed into the Decoder to
obtain x′ for reconstruction. The adversarial training in the pixel
domain is also adopted with a discriminator added on the images.
The forward pass process is drawn in solid lines and dashed lines
represent back propagation.

ational inference. Other works in [35, 4, 22] are also built
in AE and more than two inputs. Moreover they only apply
in a particular domain like face [35, 4] or image-to-image
translation [22], while our work is built in VAE and takes
only a single input for a more general case.

3. Proposed method

We propose a image generation algorithm based on VAE
which divides the encoder into two separate ones, one en-
coding label relevant representation zs and the other encod-
ing label irrelevant information zu. zs is learned with su-
pervision of the categorical class label and it is required to
follow a Gaussian mixture distribution, while zu is wished
to contain other common information irrelevant to the label
and is made close to standard Gaussian N (0, I).

3.1. Problem formulation

a

Ds

Given

dataset

labeled

=
{(x1, y1), (x2, y2), · · · , (x(N ), y(N ))}, where x(i)
is
the i-th images and y(i) ∈ {0, 1, · · · , C − 1} is the
corresponding label. C and N are the number of classes
and the size of the dataset, respectively. The goal of VAE
is to maximum the ELBO deﬁned in Eq. 1, so that the
data log-likelihood log p(x) is also maximized. The key
idea is to split the full latent code z into the label relevant
dimensions zs and the irrelevant dimensions zu, which
means zs fully reﬂects the class c but zu dose not. Thus

the objective can be rewritten as (derived in detail
Appendices).

in

log p(x) = logZZ Xc

p(x, zs, zu, c)dzsdzu

≥ E

qψ(zs|x),qφ(zu|x)[log pθ(x|zs, zu)]

(2)

− DKL(qφ(zu|x)||p(zu))
− DKL(qψ(zs, c|x)||p(zs, c))

In Eq. 2, the ELBO becomes 3 terms in our setting. The
ﬁrst term is the negative reconstruction error, where pθ is
the decoder parameterized by θ. It measures whether the
latent code zs and zu are informative enough to recover the
original data. In practice, the reconstruction error Lrec can
be deﬁned as the l2 loss between x and x′. The second
term acts as a regularization term of label irrelevant branch
that pushes qφ(zu|x) to match the prior distribution p(zu),
which is illustrated in detail in Section 3.2. The third term
matches qψ(zs|x) to a class-speciﬁc Gaussian distribution
whose mean and covariance are learned with supervision,
and it will be further introduced in Section 3.3.

3.2. Label irrelevant branch

Intuitively, we want to disentangle the latent code z into
zs and zu, and expect zu to follow a ﬁxed, prior distribu-
tion which is irrelevant to the label. This regularization is
realized by minimizing KL divergence between qφ(zu|x)
and the prior p(zu) as illustrated in Eq. 3. More specif-
ically, qφ(zu|x) is a Gaussian distribution whose mean µ
and diagonal covariance Σ are the output of Encoderu pa-
rameterized by φ. p(zu) is simply set to N (0, I). Hence
the KL regularization term is:

Lkl = DKL[N (µ, Σ)||N (0, I)]

(3)

Note that Eq. 3 can be represented in a closed form, which
is easy to be computed.

To ensure good disentanglement in zu and zs, we intro-
duce adversarial learning in the latent space as in AAE [24]
to drive the label relevant information out of zu. To do this,
an adversarial classiﬁer is added on the top of zu, which is
trained to classify the category of zu with cross entropy loss
as is shown in Eq. 4:

Ladv

C = −E

qφ(zu|x)Xc

I(c = y) log qω(c|zu)

(4)

where I(c = y) is the indicator function, and qω(c|zu) is
softmax probability output by the adversarial classiﬁer pa-
rameterized by ω. Meanwhile, Encoderu is trained to fool
the classiﬁer, hence the target distribution becomes uniform
over all categories, which is 1
C . The cross entropy loss is
deﬁned as Eq. 5.

Ladv

E = −E

qφ(zu|x)Xc

1
C

log qω(c|zu)

(5)

12194

3.3. Label relevant branch

while this loss becomes

Inspired by GM loss [39], we expect zs to follow a Gaus-
sian mixture distribution, expressed in Eq. 6, where µc and
Σc are the mean and covariance of Gaussian distribution
for class c, and p(c) is the prior probability, which is sim-
ply set to 1
C for all categories. For simplicity, we ignore the
correlation among different dimensions of zs, hence Σc is
assumed to be diagonal.

p(zs) = Xc

p(zs|c)p(c) = Xc

N (zs; µc, Σc)p(c)

(6)

Recall that in Eq. 2, the KL divergence between qψ(zs, c|x)
and p(zs, c) is minimized. If zs is formulated as a Gaus-
sian distribution with its Σ → 0 and its mean ˆzs out-
put by Encoders, which is actually a Dirac delta function
δ(zs − ˆzs), the KL divergence turns out to be the likeli-
hood regularization term Llkd in Eq. 7, which is proved in
Appendices. Here µy and Σy are the mean and covariance
speciﬁed by the label y.

Llkd = − log N (ˆzs; µy, Σy)

(7)

Furthermore, we want zs to contain label information as
much as possible, thus the mutual information between zs
and class c is added to the maximization objective func-
tion. We prove in Appendices that it’s equal to minimize
the cross-entropy loss of the posterior probability q(c|zs)
and the label, which is exactly the classiﬁcation loss Lcls in
GM loss as is shown in Eq. 8.

Lcls = −E

qψ(zs|x)Xc

I(c = y) log q(c|zs)

(8)

= − log

N (ˆzs|µy, Σy)p(y)

Pk N (ˆzs|µk, Σk)p(k)

These two terms are added up to form GM loss in Eq. 9.
Here LGM is ﬁnally used to train the Encoders.

LGM = Lcls + λlkdLlkd

(9)

3.4. The decoder and the adversarial discriminator

The latent codes zs and zu output by Encoders and
Encoderu are ﬁrst concatenated together, and then further
given to the decoder to reconstruct the input x by x′. Here
the Decoder is indicated by pθ(x|z) with its parameter θ
learned from the l2 reconstruction error Lrec. To synthesize
a high quality x′, we also employ the adversarial training
in the pixel domain. Speciﬁcally, a discriminator Dθd (x, c)
with adversarial training on its parameter θd is used to im-
prove x′. Here the label c is utilized in Dθd like in [29]. The
adversarial training loss for discriminator can be formulated
as in Eq. 10,

Ladv

D = − Ex∼Pr [log Dθd (x, c)]

− Ezu∼N (0,I),zs∼p(zs)[log(1 − Dθd (G(zs, zu), c))]
(10)

Ladv

GD = −Ezu∼N (0,I),zs∼p(zs)[log(Dθd (G(zs, zu), c))]

for the generator. Note that here G(zs, zu) is the decoder
and p(zs) is deﬁned in Eq. 6.

3.5. Training algorithm

The training detail is illustrated in Algorithm 1. The
Encoders, modeled by qψ, extracts label relevant code zs.
Encoders is trained with LGM and Lrec, encouraging zs to
be label dependent and follow a learned Gaussian mixture
distribution. Meanwhile, the Encoderu represented by qφ
is intended to extract class irrelevant code zu. It’s trained
by Lkl, Ladv
E and Lrec to make zu irrelevant to the label
and be close to N (0, I). The adversarial classiﬁer parame-
terized by ω is learned to classify zu using Ladv
C . Then the
decoder pθ generates reconstruction image using the com-
bined feature of zs and zu with the loss Lrec.

In the training process, a 2-stage alternating training al-
gorithm is adopted. First, Encoders is updated using LGM
to learn mean µc and covariance Σc of the prior p(zs|c).
Then, the two encoders and the decoder are trained jointly
to reconstruct images while the distributions of zs and zu
are considered.

3.6. Application in semi supervised generation

L

extra

Given

unlabeled

data Du

=
{x(N +1), x(N +2), · · · , x(N +L)}, we now use our ar-
chitecture for the semi-supervised generation, in which the
labels y(N +i) of x(N +i) in Du are not presented. Here
we hold the assumption that Du are in the same domain
as the fully supervised Ds, but y(N +i) can be satisﬁed
y(N +i) ∈ {0, 1, · · · , C − 1}, or out of the predeﬁned range.
In other words, if the absent y(N +i) is in the predeﬁned
range, its zs follows the same Gaussian mixture distribution
as in Eq. 6. Otherwise, zs should follow an ambiguous
Gaussian distribution deﬁned in Eq. 11.

µt = Xc
t = Xc
σ2

p(c)µc

p(c)σ2

c + Xc

p(c)(µc)2 − (Xc

p(c)µc)2

(11)

More speciﬁcally, zs is expected to follow N (µt, Σt)
where µt and Σt are the total mean and covariance of all
the class-speciﬁc Gaussian distributions N (µc, Σc) as il-
lustrated in Eq. 6. Here, Σt is diagonal matrix with σ2
t
as its variance vector. σ2
c is also the variance vector of
Σc. Hence, the likelihood regularization term becomes
Llkd = − log N ( ˆzs; µt, Σt). The whole network is trained
in a end-to-end manner using total losses. Note that in this
E and Ladv
setting, the label y is not provided, so LGM , Ladv
are ignored in the training process.

C

12195

Algorithm 1 The training process of our proposed architec-
ture.
Require: ψ, φ, θ, ω, θd initial parameters of Encoders,
Encoderu, Decoder, the adversarial classiﬁer on zu
and the discriminator on x; µc and Σc initial mean
and covariance for Gaussian distribution of zs; ngm,
the number of iterations of LGM per end-to-end itera-
tion; λrec and λkl the weight of Lrec and Lkl;

1: while not converged do
for i = 0 to ngm do
2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

28:

Sample {x, y} a batch from dataset.
ˆzs ← Encoders(x).
LGM ← − log q(y|ˆzs) − λlkd log p(ˆzs|y)
ψ

+←− −∇ψLGM
+←− −∇µc LGM , c ∈ [0, C − 1]
+←− −∇Σc LGM , c ∈ [0, C − 1]

µc

1

2 ǫ + µ
1
C log qω(c|zu)

Σc
end for
Sample {x, y} a batch from dataset.
µ, Σ ← Encoderu(x)
Lkl ← DKL[N (µ, Σ)||N (0, I)]
Sample ǫ ∼ N (0, I)
zu ← Σ
Ladv
E ← −Pc
Ladv
C ← − log qω(y|zu)
ˆzs ← Encoders(x).
Llkd ← − log N (ˆzs; µy, Σy)
x′
Lrec ← 1
Sample zp
x′
p ← Decoder(zp
D ← − log Dθd (x, y) − log(1 − Dθd (x′
Ladv

2 ||x − x′
s ∼ p(zs|y), zp

f ← Decoder(ˆzs, zu)

u ∼ N (0, I)

s, zp
u)

f ||2
2

f , y)) −

f , y)) − log(Dθd (x′

p, y))

log(1 − Dθd (x′

p, y))

Ladv

ψ

φ

ω

θ

GD ← − log(Dθd (x′
+←− −∇ψ(Lrec + λlkdLlkd)
+←− −∇φ(Ladv
+←− −∇ωLadv
+←− −∇θ(Lrec + Ladv
GD)
+←− −∇θd Ladv

C

D

θd

29:
30: end while

E + λklLkl + λrecLrec)

4. Experiments

In this section, experiments are carried out to validate the
effectiveness of the proposed method. A toy example is ﬁrst
designed to show that by disentangling the label relevant
and irrelevant codes, our model has the ability of generating
diverse data samples than cVAE-GAN [3]. We then com-
pare the quality of generated images on real image datasets.
The latent space is also analyzed. Finally, the experiments
of semi-supervised generation and image inpainting show

the ﬂexibility of our model, hence it may have many poten-
tial applications.

4.1. Toy examples

This section demonstrates our method on a toy example,
in which the real data distribution lies in 2D with one di-
mension (x axis) being label relevant and the other (y axis)
being irrelevant. The distribution is assumed to be known.
There are 3 types of data points indicated by green, red and
blue, belonging to 3 classes. The 2D data points and their
corresponding labels are given to our model for variational
inference and the new sample generation.

For comparison, we also give the same training data to
cVAE-GAN for the same purpose. The two compared mod-
els share the similar settings of the network. In our model,
the two encoders are both MLP with 3 hidden layers, and
there are 32, 64, and 64 units in them. In cVAE-GAN, the
encoder is the same, but it only has one encoder. The dis-
criminators are exactly the same, which is also an MLP of
3 hidden layers with 32, 64, and 64 units. Adam is used
as the optimization method in which a ﬁxed learning rate
of 0.0005 is applied for both. Each model is trained for 50
epochs until they all converge. The generated samples of
each model are plotted in Figure 2.

From Figure 2 we can observe that both two models can
capture the underlying data distribution, and our model con-
verges at the similar rate. The advantage of our model is
that it tends to generate diverse samples, while cVAE-GAN
generates samples in a conserving way in which the label
irrelevant dimensions are within the limited value range.

4.2. Analysis on generated image quality

In this section, we compare our method with other gener-
ative models for image generation quality. The experiments
are conducted on two datasets: FaceScrub [30] and CIFAR-
10 [20]. The FaceScrub contains 92k training images from
530 different identities. For FaceScrub, a cascaded ob-
ject detector proposed in [38] is ﬁrst used to detect faces
ﬁrst, and then the face alignment is also conducted based
on SDM proposed in [41]. The detected cropped faces are
resized to the ﬁxed size 64×64.
In the training process,
Adam optimizer with α = 0.0005 is used. The hyper pa-
, and 1
rameter λlkd, λkl, and λrec are set to 0.1,
,
Nzu
respectively. Here, Npixel is the number of image pixels,
and Nzu is the dimension of zu. Since our method incor-
porates the label for training, popular generative networks
conditioned on label, like cVAE [36], cVAE-GAN [3], and
cGAN [29], are chosen for comparison. For cVAE, cVAE-
GAN and cGAN, we randomly generate samples of class c
by ﬁrst sampling z ∼ N (0, I) and then concatenating z and
one hot vector of c as the input of decoder/generator. As
for ours, zs ∼ N (µc, σc) and zu ∼ N (0, I) are sampled
and combined for decoder to generate samples. Some of

Npixel

10

12196

initial

10 epoches

20 epoches

30 epoches

40 epoches

50 epoches

CVAE-GAN

ours

(a) real data distribution

(b) generated distribution

Figure 2. Results on a toy example for our model and cVAE-GAN. We show the generated points at different epochs.

(a) real samples

(b) cVAE

(c) cGAN

(d) CVAE-GAN

(e) ours

b
u
r
c
S
e
c
a
F

0
1
-
R
A
F
I
C

Figure 3. Visualization of generated images of different models.

generated images are visualized in Figure 3. It shows that
samples generated by cVAE are highly blurred, and cGAN
suffers from mode collapse. Samples generated by cVAE-
GAN and our method seem to have similar quality, we refer
to two metrics, Inception Score [34] and intra-class diver-
sity [5] to compare them.

We adopt Inception Score to evaluate realism and inter-
class diversity of images. Generated images that are close
to real images of class y should have a posterior proba-
bility p(y|x) with low entropy. Meanwhile, images of di-
verse classes should have a marginal probability p(y) with
high entropy. Hence, Inception Score, formulated as
exp(ExKL(p(y|x)||p(y))), gets a high value when images
are realistic and diverse.

To get conditional class probability p(y|x), we ﬁrst train
a classiﬁer with Inception-ResNet-v1 [37] architecture on
real data. Then we randomly generate 53k samples(100
for each class) of FaceScrub and 5k samples (500 for each
class) of CIFAR-10, and apply them to the pre-trained

FaceScrub CIFAR-10

cVAE [36]
cGAN [29]
cVAE-GAN [3]
ours

9.55
10.02
16.75
17.91

3.01
6.27
6.99
7.04

Table 1. Inception Score of different methods on two datasets.
Please refer to 4.2 for more details.

classiﬁer. The marginal p(y) is obtained by averaging all
p(y|x). The results are listed in Table 1.

We emphasize that our method will generate more di-
verse samples in one class. Since Inception Score only
measures inter-class diversity, intra-class diversity of sam-
ples should also be taken into account. We adopt the metric
proposed in [5], which measures the average negative MS-
SSIM [40] between all pairs in the generated image set X.
Table 2 shows the inter-class diversity of cVAE-GAN and
our method on FaceScrub and CIFAR-10.

12197

FaceScrub CIFAR-10

cVAE-GAN [3]
ours

0.0141
0.0157

0.0136
0.0149

Table 2. Intra-class diversity of different methods on two datasets.
Please refer to 4.2 for more details.

dintra(X) = 1 −

1

|X|2 X(x′,x)∈X×X

MS-SSIM(x′, x) (12)

4.3. Analysis on disentangled latent space

We now evaluate our proposal on the disentangled latent
space, which is represented by label relevant dimensions zs
and irrelevant ones zu. zs for class c is supposed to capture
the variation unique to training images within the label c,
while zu should contain the variation in common character-
istics for all classes. It’s validated in the following ways: (1)
ﬁxing zu and varying zs. In this setting, we directly sam-
ple a zu ∼ N (0, I), and keep it ﬁxed. Then a set of zs for
class c is obtained by ﬁrst getting a series of random codes
sampled from N (0, I) and then mapping them to class c. In
speciﬁc, we ﬁrst sample z1 ∼ N (0, I) and z2 ∼ N (0, I).
Then a set of random codes z(i) are obtained by linear in-
terpolation, i.e., z(i) = αz1 + (1 − α)z2, α ∈ [0, 1]. We
(i)
s = z(i) ⊙ σc + µc. Finally
map each z(i) to class c with z
is concatenated with the ﬁxed zu and given to the
each z
decoder to get a generated image. (2) ﬁxing zs and varying
zu. Similar to (1), we ﬁrst sample a zs ∼ N (µc, σc) from
a learned distribution and keep it ﬁxed. Then a set of label
irrelevant zu are obtained by linearly interpolating between
z1 and z2, where z1 and z2 are sampled from N (0, I).

(i)
s

We conduct experiments on FaceScrub and the generated
images are shown in Figure 4. In Figure 4 (a), each row
presents samples generated by linearly transformed zs of a
certain class c and a ﬁxed zu. All three rows share the same
zu, and each column shares the same random code z(i) and
(i)
s = z(i) ⊙ σc + µc.
just maps it to different class c with z
It shows that as zs varies, one may change differently for
different identities, e.g., grow a beard, wrinkle, or take off
the make-up. In Figure 4 (b), each row presents samples
with linearly transformed zu a ﬁxed zs of class c, and each
column shares a same zu. We can see that images from
each row change consistently with poses, expressions and
illuminations. These two experiments suggest that zs is rel-
evant to c, while zu reﬂects more common label irrelevant
characteristics.

We are also interested in each dimension in zu and con-
duct an experiment by varying a single element in it. We
ﬁnd three dimensions in zu which reﬂect the meaningful
the common characteristics, such as the expression, eleva-
tion and azimuth.

(a) Fixing zu and varying zs.

(b) Fixing zs and varying zu.

Figure 4. The generated images by ﬁxing one code and varying the
other. In (a), each row shows samples for linearly transformed zs
of a certain class c with a ﬁxed zu. In (b), each row corresponds
to samples for linearly transformed zu with a ﬁxed zs of class c.

4.4. Semi supervised image generation

According to the details in Section 3.6, the experiments
on semi-supervised image generation are conducted. We
ﬁnd our method can learn well disentangled latent repre-
sentation when the unlabeled extra data are available. To
validate that, we randomly select 200 identities of about 21k
images from CASIA [42] dataset and remove their labels to
form unlabeled dataset Du. Note that the identities in Du
are totally different with those in FaceScrub. After training
the whole network on labeled dataset Ds, we ﬁnetune it on
Du using the training algorithm illustrated in Section 3.6.

To demonstrate the semi-supervised generation results,
two different images are given to EncoderS and EncoderU
to generate the code zs and zu, respectively. Then, the de-
coder is required to synthesis a new image based on the
concatenated code from zs and zu. The Figure 6 shows
face synthesis results using images whose identities have
not appeared in Ds. The ﬁrst row and ﬁrst column show
a set of original images providing zu and zs respectively,
while images in the middle are generated ones using zs of
the corresponding row and zu of the corresponding column.
It is obvious that the identity depends on zs, while other
characteristics like the poses, illumination, expressions are
reﬂected on zu. This semi-supervised generation shows zs
and zu can also be disentangled on identities outside the la-
beled training data Ds, which provides the great ﬂexibility
for image generation.

4.5. Image inpainting

Our method can also be applied to image inpainting.
It means that given a partly corrupted image, we can ex-

12198

(a) expression

(b) elevation

(c) azimuth

Figure 5. The generated images by ﬁxing zs for each row and varying single dimensions in zu. Here, we ﬁnd three different dimensions in
zu, which directly causes the variations on expressions in (a), elevation in (b), and azimuth in (c).

Figure 6. Face synthesis using images whose identities have not appeared in Ds. Original images providing zu and zs are given in the ﬁrst
row and the ﬁrst column. The synthesizing images using the combination of zu zs are shown in the corresponding position.

tract meaningful latent code to reconstruct the original im-
age. Note that in cVAE-GAN [3], an extra class label c
should be provided for reconstruction while it’s needless
in our method. In practice, we ﬁrst corrupt some patches
for a image x, namely right-half, eyes, nose and mouth,
and bottom-half regions, then input those corrupted images
into the two encoders to get zs and zu, then the recon-
structed image x′ is generated using a combined zs and
zu. The image inpainting result is obtained by xinp =
M ⊙ x′ + (1 − M ) ⊙ x, where M is the binary mask
for the corrupted patch. Figure 7 shows the results of image
inpainting. cVAE-GAN struggles to complete the images
when it comes to a large part of missing regions (e.g. right-
half and bottom-half parts) or pivotal regions of faces (e.g.
eyes), while our method provides visually pleasing results.

original image  corrupted image CVAE-GAN

(a) right-half face

ours

original image  corrupted image CVAE-GAN

(b) eyes

ours

5. Conclusion

original image  corrupted image CVAE-GAN

(c) nose and mouth

ours

original image  corrupted image CVAE-GAN

(d) bottom-half face

ours

We propose a latent space disentangling algorithm on
VAE baseline. Our model learns two separated encoders
and divides the latent code into label relevant and irrele-
vant dimensions. Together with a discriminator in pixel
domain, we show that our model can generate high qual-
ity and diverse images, and it can also be applied in semi-
supervised image generation in which unlabeled data with
unseen classes are given to the encoders. Future research
includes building more interpretable latent dimensions with
help of more labels, and reducing the correlation between
the label relevant and irrelevant codes in our framework.

Figure 7. Image inpainting results. The original image is corrupted
with different patterns, on the right-half, eyes, nose and mouth, and
bottom-half face. We compare our model with cVAE-GAN.

Acknowledgements

This work was supported by the National Natural Sci-
ence Foundation of China under Project 61302125, and
by Natural Science Foundation of Shanghai under Project
17ZR1408500. Li Sun (sunli@ee.ecnu.edu.cn) is the corre-
sponding author.

12199

References

[1] M Ehsan Abbasnejad, Anthony Dick, and Anton van den
Hengel. Inﬁnite variational autoencoder for semi-supervised
learning. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 781–790. IEEE, 2017.

[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.

Wasserstein gan. stat, 1050:9, 2017.

[3] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang
Hua. Cvae-gan: Fine-grained image generation through
asymmetric training.
In 2017 IEEE International Confer-
ence on Computer Vision (ICCV), pages 2764–2773. IEEE,
2017.

[4] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang
Hua. Towards open-set identity preserving face synthesis.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 6713–6722, 2018.

[5] Matan Ben-Yosef and Daphna Weinshall. Gaussian mix-
ture generative adversarial networks for diverse datasets,
and the unsupervised clustering of images. arXiv preprint
arXiv:1808.10356, 2018.

[6] Konstantinos Bousmalis, Nathan Silberman, David Dohan,
Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-
level domain adaptation with generative adversarial net-
works.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), volume 1, page 7, 2017.

[7] Christopher P Burgess,

Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and Alexan-
der Lerchner. Understanding disentangling in β-vae. arXiv
preprint arXiv:1804.03599, 2018.

[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable repre-
sentation learning by information maximizing generative ad-
versarial nets. In Advances in neural information processing
systems, pages 2172–2180, 2016.

[9] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-
versarial feature learning. arXiv preprint arXiv:1605.09782,
2016.

[10] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier
Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. In ICLR, 2017.

[11] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Xiaogang
Wang, and Hongsheng Li. Fd-gan: Pose-guided feature dis-
tilling gan for robust person re-identiﬁcation. In Advances in
Neural Information Processing Systems, 2018.

[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville.
Improved training of
wasserstein gans. In Advances in Neural Information Pro-
cessing Systems, pages 5767–5777, 2017.

[14] Naama Hadad, Lior Wolf, and Moni Shahar. A two-step
disentanglement method. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
772–780, 2018.

[15] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. β-vae: Learning basic visual concepts
with a constrained variational framework. In ICLR, 2017.

[16] Matthew D Hoffman, David M Blei, Chong Wang, and John
Paisley. Stochastic variational inference. The Journal of Ma-
chine Learning Research, 14(1):1303–1347, 2013.

[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. arXiv preprint, 2017.

[18] Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag,
and Alexander M Rush. Semi-amortized variational autoen-
coders. arXiv preprint arXiv:1802.02550, 2018.

[19] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. stat, 1050:1, 2014.

[20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer, 2009.

[21] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo
Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Confer-
ence on Machine Learning, pages 1558–1566, 2016.

[22] Hsin-Ying Lee, Hung-Yu Tseng,

Jia-Bin Huang, Ma-
neesh Kumar Singh, and Ming-Hsuan Yang. Diverse image-
to-image translation via disentangled representations. In Eu-
ropean Conference on Computer Vision, 2018.

[23] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In Advances in Neural
Information Processing Systems, pages 700–708, 2017.

[24] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian
Goodfellow, and Brendan Frey. Adversarial autoencoders.
arXiv preprint arXiv:1511.05644, 2015.

[25] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares genera-
tive adversarial networks. In Computer Vision (ICCV), 2017
IEEE International Conference on, pages 2813–2821. IEEE,
2017.

[26] Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya
Ramesh, Pablo Sprechmann, and Yann LeCun. Disentan-
gling factors of variation in deep representation using adver-
sarial training. In Advances in Neural Information Process-
ing Systems, pages 5040–5048, 2016.

[27] Lars Mescheder, S Nowozin, and Andreas Geiger. Adversar-
ial variational bayes: Unifying variational autoencoders and
generative adversarial networks. In International Conference
on Machine Learning (ICML), pages 2391–2400. PMLR,
2017.

[28] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. In ICLR, 2018.

[29] Takeru Miyato and Masanori Koyama. cgans with projection

discriminator. arXiv preprint arXiv:1802.05637, 2018.

[30] Hong-Wei Ng and Stefan Winkler. A data-driven approach
to cleaning large face datasets. In Image Processing (ICIP),
2014 IEEE International Conference on, pages 343–347.
IEEE, 2014.

12200

[31] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-
gan: Training generative neural samplers using variational
divergence minimization. In Advances in Neural Information
Processing Systems, pages 271–279, 2016.

[32] Augustus Odena, Christopher Olah, and Jonathon Shlens.
Conditional image synthesis with auxiliary classiﬁer gans.
In International Conference on Machine Learning, pages
2642–2651, 2017.

[33] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-
stra. Stochastic backpropagation and approximate inference
in deep generative models. In International Conference on
Machine Learning, pages 1278–1286, 2014.

[34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in Neural Information Pro-
cessing Systems, pages 2234–2242, 2016.

[35] Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli,
Eli Shechtman, and Dimitris Samaras. Neural face edit-
ing with intrinsic image disentangling. In Computer Vision
and Pattern Recognition (CVPR), 2017 IEEE Conference on,
pages 5444–5453. IEEE, 2017.

[36] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning
structured output representation using deep conditional gen-
erative models. In Advances in Neural Information Process-
ing Systems, pages 3483–3491, 2015.

[37] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In AAAI, vol-
ume 4, page 12, 2017.

[38] Paul Viola and Michael Jones. Rapid object detection using
a boosted cascade of simple features.
In Computer Vision
and Pattern Recognition, 2001. CVPR 2001. Proceedings of
the 2001 IEEE Computer Society Conference on, volume 1,
pages I–I. IEEE, 2001.

[39] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng
Chen. Rethinking feature distribution for loss functions in
image classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 9117–
9126, 2018.

[40] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multi-
scale structural similarity for image quality assessment. In
The Thrity-Seventh Asilomar Conference on Signals, Sys-
tems & Computers, 2003, volume 2, pages 1398–1402. Ieee,
2003.

[41] Xuehan Xiong and Fernando De la Torre. Supervised descent
method and its applications to face alignment. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 532–539, 2013.

[42] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-
arXiv preprint

ing face representation from scratch.
arXiv:1411.7923, 2014.

12201

