Relational Action Forecasting

Chen Sun1, Abhinav Shrivastava2, Carl Vondrick1,

Rahul Sukthankar1, Kevin Murphy1, and Cordelia Schmid1

1Google Research

2University of Maryland

Abstract

This paper focuses on multi-person action forecasting
in videos. More precisely, given a history of H previous
frames, the goal is to detect actors and to predict their fu-
ture actions for the next T frames. Our approach jointly
models temporal and spatial interactions among different
actors by constructing a recurrent graph, using actor pro-
posals obtained with Faster R-CNN as nodes. Our method
learns to select a subset of discriminative relations with-
out requiring explicit supervision, thus enabling us to tackle
challenging visual data. We refer to our model as Discrimi-
native Relational Recurrent Network (DR2N). Evaluation of
action prediction on AVA demonstrates the effectiveness of
our proposed method compared to simpler baselines. Fur-
thermore, we signiﬁcantly improve performance on the task
of early action classiﬁcation on J-HMDB, from the previous
SOTA of 48% to 60%.

1. Introduction

In this paper, we consider the task of forecasting what
high level actions people will perform in the future, given
noisy visual evidence. For example, consider Figure 1:
given the current video frame, and a sequence of past
frames, we would like to detect (localize) the people (man
and woman), and classify their current actions (woman rides
horse, man and woman are talking), as well as predict future
plausible action sequences for each person (woman will get
off horse, man will hold horse reigns).

1:N , a0:T

More formally, our task is to compute the probability
1:N |V −H:0), where V t is the frame at time t
p(N, b0
(where t = 0 is the present), V = V −H:0 is the visual his-
tory of H previous frames, N is the number of predicted
actors, b0
n is the predicted location (bounding box) of actor
n at time 0, and at
n is the predicted action label for actor
n at time t, which we compute for t = 0 : T , where T

Figure 1: Action prediction from a single frame (middle)
is ambiguous, but requires temporal context for the actors
and their interactions.
It only becomes apparent that the
lady will get off the horse, if we know that she was riding
towards the man, and the man is holding the horse.

1:N , b0:T

is the maximum forecasting horizon. This formulation is
closely related to but different than prior work. In particular,
video classiﬁcation focuses on computing a single global
label in the ofﬂine scenario, p(c|V 0:T ); spatio-temporal ac-
tion detection focuses on multi-agent localization and clas-
siﬁcation, but in the ofﬂine scenario, p(a0:T
1:N |V 0:T );
action anticipation focuses on the future, but for a single
class label, p(c0:T |V −H:0); and trajectory forecasting fo-
cuses on multiple agents in the future, but generally fo-
cuses on locations, not actions, and assumes that the past
locations (and hence the number of agents) is observed:
p(b1:T
1:N , N ). By contrast, we only observe
past frames, and want to predict future high level actions of
agents. This could be useful for self-driving cars, human-
robot interaction, etc. See Section 2 for a more detailed
discussion of related work.

1:N |V −H:0, b−H:0

1273

Figure 2: Overview of our approach Discriminative Relational Recurrent Network (DR2N). Given actor proposals and their
spatio-temporal descriptors at a given time T=0, we model their relations by a graph neural network and its recurrence over
time (here with a GRU).

Our proposed approach is to create a graph-structured
recurrent neural network (GRNN), in which nodes corre-
spond to candidate person detections (from an object de-
tector trained on person examples), and edges represent po-
tential interactions between the people. Each node has an
action label and bounding box location associated with it.
(Note that some nodes may be false positive detections aris-
ing from the detector, and should be labeled as such.) We
use a modiﬁed version of graph attention networks [61] ap-
plied to a fully connected pairwise graph to capture inter-
action effects. Nodes are also linked over time via RNNs
to capture temporal patterns in the data. We name our pro-
posed framework Discriminative Relational Recurrent Net-
work (DR2N). See Figure 2 for an illustration.

We train the model in a weakly supervised way, in the
sense that we have a ground truth set of labeled bounding
boxes for people in frame 0, and we have action labels (but
not location labels) for the people for frames 1 : T . How-
ever, we do not observe any edge information. Our model
is able to learn which edges are useful, so as to maximize
node classiﬁcation performance. On the challenging AVA
dataset [18], we show that our model outperforms various
strong baselines at the task of predicting person action la-
bels for up to 5 seconds into the future.

To be more directly comparable to prior work, we also
consider another task, namely “early classiﬁcation” of video
clips, where the task is to compute p(c|V 0:t), where c is the
class label for the video clip of length T , and t < T is
some preﬁx of the clip (we use the ﬁrst 10% to 50% of the
frames). We modify our model for this task, and train it on
the J-HMDB dataset, as used in prior works [54, 55]. We
achieve signiﬁcant improvements over the previous state of
the art, increasing the classiﬁcation accuracy given a 10%
preﬁx from 48% (obtained by [54]) to 60%.

2. Related work

Action recognition. Human action recognition in videos
is dominated primarily by three well-established tasks: ac-
tion classiﬁcation [52, 7, 38, 26, 56, 1, 28, 41, 29], tempo-
ral action localization [7, 24, 75, 53], and spatio-temporal
action detection [30, 76, 48, 26, 56, 71, 40, 18]. Given
a video (a set of frames), the goal of action classiﬁcation
is to assign action labels to the entire video, whereas tem-
poral action localization assigns labels to only a subset of
frames representing the action. Spatio-temporal action de-
tection combines the temporal action localization with ac-
tor detection, i.e., detecting actors per-frame in a subset
of frames and assigning action labels to per-actor spatio-
temporal tubes [30, 76, 26].

The three standard tasks discussed above assume that the
entire video is observed, therefore prediction in any frame
can utilize past or future frames. In contrast, this paper in-
troduces the task of actor-level action prediction, i.e., given
a video predict or anticipate what actions each actor will
perform in the future. This task operates in a more prac-
tical setup where only the past and present frames are ob-
served, and predictions have to be made for unobserved fu-
ture frames. Note that the proposed task inherently requires
spatio-temporal actor detection in the observed frames.

Future prediction. Our work follows a large number of
works studying future prediction [64, 44, 39, 45, 14, 3,
27, 35, 73, 66, 57, 67, 77, 63, 32, 16, 78, 2, 47, 68, 62,
15, 33, 13, 75, 36]. Broadly speaking, the research on fu-
ture prediction follows two main themes: generating future
frame(s) [64, 44, 39, 45, 14, 3, 27, 35, 73, 57] and predicting
future labels or state(s) [67, 77, 63, 32, 16, 78, 2, 47, 62, 15,
33, 13, 75, 36, 66, 49]. For future frame generation, there
is a wide variety of approaches ranging from predicting in-
termediate representations (e.g., optical ﬂow [45, 15, 68],

274

human pose [62, 69]) and using it to generate future pix-
els, to directly generating future pixels by extending gener-
ative models for images to videos [64, 44, 14, 3]. Though
the quality of generated frames has improved over the past
few years [65, 3], it is arguably solving a much harder task
than necessary (after all, humans can predict likely future
actions, but cannot predict likely future pixels).

The second theme in future prediction circumvents pixel
generation and directly predicts future states [67, 77, 63, 32,
16, 78, 2, 47, 62, 15, 33, 13, 75, 36, 66, 49]. These future
states can vary from low-level trajectories [67, 32, 2, 47, 49,
33] for different agents to high-level semantic outputs [36,
13, 75, 37, 58]. The trajectory-based approaches rely on
modeling and predicting agent behavior in the future, where
an agent can be an object (such as human [32, 2, 47] or
car [49]) or an image patch [67]. Most of these methods
require a key assumption that the scene is static; i.e., no
camera motion (e.g., VIRAT dataset [42]) and no movement
of non-agent entities [32, 2, 47, 49]. Our method is similar
to these trajectory-based methods, in the sense that our state
prediction is also about agents. However, we do not assume
anything about the scene.
In particular, the AVA dataset
has signiﬁcant camera motion and scene cuts, in addition to
agent motion and cluttered, dynamic backgrounds.

Much work on future forecasting focuses on the spatop-
temporal extent of detected actions [54, 55, 20, 37]. How-
ever, there is some work on forecasting high-level seman-
tic states, ranging from semantic segmentation masks [36]
to action classes [13, 75]. However, our method has sev-
eral key distinctions. First, many works require the se-
mantic states to be part of the input to the forecasting al-
gorithm, whereas our method detects actors and actions
from past/present frames. Second, most of the state pre-
diction methods operate on MoCap data (e.g., 3D human
keypoints) [16, 25], whereas our approach works from pix-
els. Third, many method assume static cameras and pos-
sibly single agents, whereas we can forecasting labels for
multiple agents in unconstrained video.

Relational reasoning. Our proposed approach builds on
the ﬁeld of relational reasoning [6, 51, 43, 31, 50, 61, 21,
70, 12, 19, 5, 4, 43]. This is natural because the current
and future actions of an actor rely heavily on the dynam-
ics it shares with other actors [2]. Relational reasoning [6],
in general, can capture relationships between a wide array
of entities. For example, relationship between abstract en-
tities or features [51, 70], different objects [4], humans and
objects [10, 9, 17, 74], humans and context [59], humans
and humans [2, 23], etc. Our work aims to capture human-
human relationships to reason about future actions.

In terms of modeling these relationships, the standard
tools include Interaction Network (IN) [5], Relation Net-
work (RN) [51], Graph Neural Network (GNN) [19], Graph
ATtenion networks [61], as well as their contemporary ex-

tensions to videos, such as Actor-centric Relation Network
(ACRN) [59] and Object Relation Network (ORN) [4].
Similar to ACRN and ORN, our proposed DR2N tries
to capture relation between different entity in videos for
spatio-temporal reasoning. However, as opposed to mod-
eling exhaustive relationships (RN [51] and ORN [4]), our
method discovers discriminative relationships for the task
of actor-level action prediction. Compared to ACRN, our
method focuses on forecasting future actor labels given past
visual information. In addition, we model interaction be-
tween the agents, but ignore any objects in the scene.

3. Approach

In this section, we describe our approach in more detail.

3.1. Creating the nodes in the graph

To generate actor proposals and extract the initial actor-
level features for action prediction, we build our system on
top of the two-stage Faster RCNN [46] detector. The ﬁrst
stage is a region proposal network (RPN) that generates
a large set (e.g. hundreds) of candidate actor proposals in
terms of 2D bounding boxes on a single frame. We apply
this RPN module on the last observed frame V 0 to locate ac-
tors whose actions are to be predicted. These become nodes
in the graph.

The second stage associates features with these nodes.
We ﬁrst extract visual features from video inputs, V −H:0,
using a 3D CNN (see 3.6 for details), and then crop out
features inside the bounding boxes for all (candidate) actors
using ROIPooling. Let vi be the visual features for actor i.
The third stage is to connect the nodes together into a
fully connected graph. However, since not all nodes are
equally useful for prediction, in Section 3.3 we explain how
to learn discriminative edge weights.

3.2. Modeling node dynamics

We model the action dynamics of individual actors using
i as the latent representation for actor i at

RNNs. Given ht
time t and at

i as the set of actor labels, we have

i = fRNN(ht−1
ht

i

, at−1

i

)

at
i = fCLS(ht
i)

(1)

(2)

where fRNN(·) is the RNN update function, and fCLS(·)
is an action classiﬁer that decodes the latent states h into
action labels (we use a simple MLP for this, see 3.6 for
details). The initial state h0
i is set to vi, the visual features
extracted for this bounding box.

To make the model scalable over a varying number of
actors, the RNN function fRNN(·) is shared over all actors.
Similarly, the action classiﬁer fCLS(·) is shared over all ac-
tors and all time steps.

275

p(N, b0

1:N , a0:T

1:N |V −H:0) = δ(N, b0

1:N |fRPN(V 0))p(a0

1:N , h0

1:N |b0

1:N , V −H:0)

T

Y

t=1

p(at

1:N , ht

1:N |a1:t−1

1:N , ht−1
1:N )

p(a0

1:N , h0

1:N |b0

1:N , V ) =

p(at

1:N , ht

1:N |at−1

1:N , ht−1

1:N ) =

N

Y

n=1

N

Y

n=1

Cat(a0

n|fCLS(h0

n))δ(h0

n|fROI(fS3D(V −H:0), b0

n))

Cat(at

n|fCLS(ht

n))δ(ht

n|fRNN(˜ht−1

n , at−1

n ))δ(˜ht−1

n |fGNN(ht−1

1:N ))

Table 1: Formal speciﬁcation of DR2N model. ht
n is the hidden state of RNN n at time t; δ(a|b) denotes a deterministic
probability distribution. Here fRPN is a region proposal network applied to frame V 0 which predicts the location of N
1:N . fROI is a region of interest feature extractor applied to S3D features derived from frames V −H:0 at the locations
boxes b0
speciﬁed by the boxes. fCLS is an MLP classiﬁer with softmax output, and Cat(a|p) is a categorical distribution over action
labels a with parameters p. fGNN is a graph neural network where the input nodes are the old RNN hidden states, ht−1
1:N , and
the output nodes are denoted ˜ht−1
1:N ; its deﬁnition is given in Table 2. Finally, fRNN is a recurrent neural net update function
applied to the previous predicted label and GNN output.

˜h1:N = fGNN(h1:N ) =

N

Y

i=1

δ(˜hi|fnode([hi, zi]))δ(zi|X

j

αijhj)

N

Y

j=1

δ(αij|Sj(fattn(ei,1:N ))δ(eij|fedge(hi, hj))

Table 2: Formal speciﬁcation of the graph neural network. fnode is an MLP that computes node states, fedge is an MLP that
computes edge states, fattn is a self-attention network, and Sj(l) is the j’th output of the softmax function with logits l.

3.3. Modeling the edges

mented as

Our current model captures action dynamics for a set
of independent actors. Many actions involve interactions
among actors (e.g. hugging and kissing). To capture such
interactions, it is important to model the relations between
different actors. Motivated by the recent success on rela-
tional reasoning, we combine graph neural networks (GNN)
with recurrent models. We treat each actor as a node and use
the RNN’s latent representation ht

i as node feature.

We ﬁrst consider a general graph network deﬁnition. For
simplicity, we ignore the time step t, and denote hi as the
feature representation for node i. Let’s also denote Ni as the
neighbors of i in the graph. We compute the output repre-
sentation ˜hi from the input features of the other connected
nodes:

eij = fedge(hi, hj)

˜hi = fnode({eij : j ∈ Ni})

(3)

(4)

where eij is the derived features for edge (i, j). Both
fedge(·) and fnode(·) can be implemented with neural net-
works. Note that fnode(·) is a function mapping a set to a
vector. To make it permutation invariant, it is often imple-

˜hi = fnode




1
|Ni| X

j∈Ni

eij


 .

(5)

i.e. the output node feature is the average over all edge fea-
tures connected to the node.

The overall graph-RNN update function can thus be ex-

pressed as

i = fRNN(˜ht−1
ht

i

, at−1

i

)

(6)

i.e. we apply GNN on the hidden states at time t − 1, then
run the standard RNN update function.

In practice, the number of actors are not known before
hand, neither are the graph structures (relations) provided
as supervision. Besides, the outputs from person detec-
tors are typically over-complete and noisy (e.g. hundreds to
thousands of proposals per frame are generated by Faster-
RCNN). One method to handle unknown graph is via “re-
lation networks” [51, 59], which assumes the graph is fully
connected (i.e. Ni = {vj | j 6= i}). According to Equa-
tion 5, this leads to an average over all edge features, and is
sensitive to noisy nodes.

To mitigate this problem, we introduce the concept of
“virtual node” zi for node i. The virtual node is connected

276

to all nodes in Ni, and aggregates the node features with a
weighted sum:

3.6. Implementation details

zi = X

j

αij hj

(7)

The distributions of soft weights for neighboring nodes

are given by

αij = softmax(fattn(eij))

(8)

where fattn(·) is a attention function that measures the im-
portance of node j to node i. fattn(·) can be efﬁciently
implemented as the self-attention mechanism [60, 61] with
neural networks. Its parameters can be jointly learned with
the target task using back propagation, and thus requires no
additional supervision.

Once zi is computed, we assume node i is connected
only to this virtual node, and updates the output feature by

˜hi = fnode([hi; zi])

(9)

The difference from graph attention networks [61] is that
they use ˜hi = fnode(zi). We have found this gives worse
results (see Section 4), perhaps because the model is not
sure if it should focus on features from itself or features
from neighbors.

3.4. Summary of model

We call our overall model Discriminative Relational Re-
current Network or DR2N for short. See Figure 2 for a
sketch, and Tables 1 and 2 for a precise speciﬁcation of the
model.

3.5. Training

The overall framework is jointly optimized end-to-end,

where the loss function is

Ltotal = αLloc +

T

X

t=0

βtLcls
t

(10)

Here Lloc is the box localization loss given by the region
proposal network and the bounding box reﬁnement network
computed for the last observed frame. Lcls
is the action clas-
t
siﬁcation loss at time t. α and βt are scalars which balances
the two sets of losses. In practice, one may want to down-
weight βt for larger t as the prediction task becomes more
challenging.

Note that we do not use teacher forcing during training,
to encourage the model to predict multiple steps into the
future. That is, when computing the predicted labels at
i, we
condition on previous predicted labels a0:t−1
rather than the
ground truth predicted labels. (We use the soft logit scores
for the predictions, to avoid the need to sample from the
model during training.)

1:t

Our implementation of the Faster-RCNN [46, 22] pro-
posal extractor largely follows the design choices of [59].
The region proposal network (RPN) uses a 2D ResNet-50
network and takes a single image as input.
It is jointly
trained with the whole framework, using the human an-
notations from target dataset. We apply RPN on the ﬁnal
observed frame of each example to generate actor propos-
als. To handle temporal context, the feature network uses
an S3D-G [72] backbone, which is a type of “inﬂated” 3D
convolutional neural network [8], and takes sequences of
frames as inputs. We apply the feature network to frames at
−H : 0, where 0 is the frame number of the last observed
frame, and H is the duration of temporal context. Once the
features are extracted, we apply a temporal convolutional
layer after the Mixed 4f layer of S3D-G to aggregate the
3D feature maps into 2D and then apply the standard 2D
ROIPooling to crop out the features inside actor proposals.
Each cropped feature map is passed to the remaining layers
of S3D-G. The ﬁnal outputs are average-pooled into 1024-
dim feature vectors.

The weights of ResNet-50 used by proposal network
are initialized with an ImageNet pre-trained model. We
keep top 300 proposals per image. The weights of S3D-
G used by feature network are pre-trained from Kinetics-
400 [29]. The weights of newly added layers are randomly
initialized from truncated normal distributions. Unlike oth-
erwise mentioned, the inputs to the feature network at 10
RGB frames resized to 400 by 400. We use gated recur-
rent units (GRU) [11] as the particular RNN architecture to
model action dynamics. We set the number of hidden units
to 1024, which is the same dimension as the visual input
features. For the discriminative relation network, we imple-
ment fedge(·) and fattn(·) as single fully-connected layers.

During both training and inference, the input actions at
i
to the GRU are generated by the model rather than provided
by the ground truth. We set the localization weight α = 1.
The classiﬁcation weight β0 is set to 1, we linearly anneal βt
such that βt = 0.5. To compute classiﬁcation loss, we use
softmax cross entropy for J-HMDB and the sum of sigmoid
cross entropies for AVA (since the action labels of AVA are
not mutually exclusive). We optimize the model with syn-
chronous SGD and batch size of 4 per GPU, and disable
batch norm updates during training. We use 10 GPUs in to-
tal. Two techniques are used to stabilize and speed up train-
ing: ﬁrst, we warm-start the learning rate from 0.008 to 0.08
for Tw steps and use cosine learning rate decay [34] starting
from 0.08 for another Tc steps; second, we apply a gradient
multiplier of 0.01 to gradients computed from DR2N to the
feature map. For AVA, we set Tw to 5K and Tc to 300K.
For J-HMDB, we set Tw to 1K and Tc to 40K.

277

Method

Dynamics Model Relation Model

t = 0

t = 1

t = 2

t = 3

t = 4

t = 5

Single-head
Multi-head

GRU

Graph-GRU
Graph-GRU
Graph-GRU

-
-

GRU
GRU
GRU
GRU

-
-
-

RN [51]
GAT [61]
DR2N (Us)

19.1
16.0
18.7
17.3
16.4
20.4

7.8
9.4
13.1
12.3
12.3
14.4

5.3
6.8
10.3
9.9
9.3
11.2

4.2
5.4
8.0
7.7
7.3
9.3

2.6
4.3
6.7
6.5
6.2
7.5

1.8
3.6
5.7
5.3
5.2
6.8

Table 3: Ablation study on the AVA dataset. We report mean AP@.5 for six different time steps. t = 0 corresponds to the
last observed frame, i.e., standard action detection. Each step is one second long.

Figure 3: Change in AP performance from T = 0 to t = 1. The actions with the biggest change are the hardest to predict in
the future.

4. Experiments

In this section, we conduct experiments to study the im-
pact of different design choices for relational reasoning and
temporal dynamics modeling for the action prediction task.

4.1. Experimental setup

In the following we present the two datasets used in
our experiments, Atomic Visual Actions (AVA) [18] and J-
HMDB [26] as well as the metrics used for evaluating action
prediction.

AVA [18] is a recently released large-scale action detec-
tion dataset with 60 action classes. AVA is sparsely labeled
at 1 FPS and each frame may contain multiple actors with
multiple action labels. We use the most recent AVA ver-
sion 2.1, which contains 210K training examples and 57K
validation examples. To get the ground-truth for future
action labels, we use the bounding box identities (tracks)
semi-automatic annotated for this dataset [18]. For actor-

level action prediction on AVA, we measure the IoU of all
the actor detections with the ground-truth boxes. If the IoU
is above 0.5, and the action label is correct, we consider it a
true positive. For prediction, the ground-truth labels come
from future boxes that are linked to the last observed ground
truth boxes. We compute the average precision, and report
per frame-level mean AP for different time steps.

J-HMDB is an action detection dataset with 928 clips
and 21 categories. Clips have an average length of 1.4 sec-
onds. Every frame in J-HMDB contains one annotated ac-
tor with a single action label. There are three train/val splits
for J-HMDB. We report results by averaging over the three
splits, which is the standard practice on this dataset. Since
there is only one action performed in each example, we fol-
low the setup of Soomro et al. [54, 55] and treat it as an
early action prediction problem. We report accuracy@K,
which is the action classiﬁcation accuracy by watching the
ﬁrst K% of the videos.

278

Figure 4: Change in AP performance from adding graph connections at t = 0. The actions with the biggest change beneﬁt
the most from contextual modeling.

Figure 5: Visualizations of top 3 relations (blue boxes) selected for an actor proposal (orange box) by DR2N on AVA. We see
that the attended regions provide useful contextual information.

4.2. Action prediction on AVA

This section presents and discusses quantitative and
qualitative results for action prediction on the AVA dataset.
We consider the following approaches:

• Single-head: for each future step t, train a separate
model that directly classiﬁes the future actions from
visual features vi derived from V 0.

• Multi-head:

similar to the single-head model, but
jointly trains the classiﬁers for all t in the same model.
The visual features vi are shared with all classiﬁcation
heads.

• GRU: future actions are predicted from hidden states
of GRU, where the states are initialized from visual
features of the actor proposals. Model is trained jointly
for all t. However, there are no edges in the graph, so
all nodes evolve independenty.

• Graph-GRU: Same as GRU, but with a fully con-
nected graph. We consider 3 versions:
the Relation
Network (RN) [51], which assigns equal weights to
all pairwise relations; Graph Attention Network [61],

which uses a weighted sum of features from itself and
all its neighbors; and our proposed method, which uses
Equation 5.

The results are shown in Table 3. The ﬁrst three rows
compare the impact of different approaches for dynamics
modeling. Our ﬁrst observation is that, as t grows larger, the
mean AP declines accordingly. This is expected since the
further away in time the prediction is, the harder is the task.
Our second observation is that the single-head baseline per-
forms worse on all t except for t = 0, where the frames are
observed. The lower performance of t = 0 for multi-head
can be explained by the fact that the joint model has less
model capacity compared with 6 independent single-head
models. However, we can see that by sharing the visual
features in a multi-task setting, the multi-head baseline out-
performs its single-head counterpart for future prediction at
t > 0. Our third observation is that using GRU to model
action dynamics offers better future prediction performance
without sacriﬁcing detection performance at t = 0, since it
can capture patterns in the sequence of action labels.

The last three rows of Table 3 compares the impact of
different relational models. We can see DRN outperforms

279

Figure 6: Example predictions on the AVA validation set at t = 1. We show last observed frames at t = 0 and render the
detected actor boxes on the frames. To the right of each example, we also show the unobserved future frames half and one
second ahead. We show top one detections if above threshold of 0.1, and remove the most frequent categories, such as sitting
and standing. The top row shows examples where the model can predict what will happen in the future based on the current
scene context. The second row shows examples where the model can predict what will happen in the future based on the
other actors in the scene. The third row highlights the challenges in action forecasting (e.g. multiple possible futures).

the other two consistently. For RN, one possible explana-
tion for the performance gap is that it assigns equal weights
to all edges, which is prone to noise in our case, since many
nodes correspond to background detections. For GAT, we
notice that the performance at t = 0 is much lower, indicat-
ing that it has difﬁculty distinguishing node features from
neigbhor features.

Figure 3 compares the classes with biggest performance
drops from detection (t = 0) to prediction (t = 1). We
see that it is challenging for the model to capture actions
with short durations, such as kissing or ﬁghting. Actions
with longer durations, sich as talking, are typically easier to
predict. Figure 4 compares the effectiveness of DR2N over
the GRU baseline, without any edges in the graph. We can
see that the categories with the most gains are those with
explicit interactions (e.g. hand clap, dance, martial art), or
where other actors provide useful context (e.g. eat and ride).
In Figure 5, we show the top 3 boxes ( blue) with the high-
est attention weights to the actor being classiﬁed (orange).
We can see that they typically correspond to other actors.
Finally, we visualize example predictions in Figure 6.

4.3. Early action prediction on J HMDB

Finally, we demonstrate the effectiveness of DR2N on
the early clip classiﬁcation. During training, we feed 10
RGB frames to the feature network, and predict one step
into the future. During inference, we feed the ﬁrst K%

Model

10% 20% 30% 40% 50%

Soomro et al. [55] ≈ 5 ≈ 12 ≈ 21 ≈ 25 ≈ 30
Singh et al. [54]
≈ 48 ≈ 59 ≈ 62 ≈ 66 ≈ 66

GRU

GAT [61]

DR2N

52.5
58.1
60.6

56.2
61.8
65.8

61.1
64.4
68.1

65.2
68.7
71.4

65.9
68.8
71.8

Table 4: Early action prediction performance on J-HMDB.

frames to the feature network, and take the most conﬁdent
prediction as the label of the clip. Table 4 shows the results,
we can see that our approach signiﬁcantly outperforms pre-
vious state-of-the-art methods. To study the impact of rela-
tion models, we also compare with the GRU only and the
GAT baselines, and ﬁnd DR2N outperforms both. By in-
specting the edge attentions, we observe that some of the
RPN proposals cover objects in the scene, which are uti-
lized by DR2N to model human-object relations.

5. Conclusion

We address the multi-person action forecasting task in
videos. We propose a model that jointly models temporal
and spatial interactions among different actors with Dis-
criminative Relational Recurrent Network. Quantitative
and qualitative evaluations on AVA and J-HMDB datasets
demonstrate the effectiveness of our proposed method.

280

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. YouTube-8M: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016. 2

[2] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei,
and S. Savarese. Social lstm: Human trajectory prediction in
crowded spaces. In CVPR, 2016. 2, 3

[3] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and
S. Levine. Stochastic variational video prediction. arXiv
preprint arXiv:1710.11252, 2017. 2, 3

[4] F. Baradel, N. Neverova, C. Wolf, J. Mille, and G. Mori.

Object level visual reasoning in videos. In ECCV, 2018. 3

[5] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al.

In-
teraction networks for learning about objects, relations and
physics. In NIPS, 2016. 3

[6] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-
Gonzalez, V. F. Zambaldi, M. Malinowski, A. Tacchetti,
D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song,
A. J. Ballard, J. Gilmer, G. E. Dahl, A. Vaswani, K. R.
Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wier-
stra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pas-
canu. Relational inductive biases, deep learning, and graph
networks. arXiv preprint arXiv:1806.01261, 2018. 3

[7] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles.
ActivityNet: A large-scale video benchmark for human ac-
tivity understanding. In CVPR, 2015. 2

[8] J. Carreira and A. Zisserman. Quo vadis, action recognition?

A new model and the Kinetics dataset. In CVPR, 2017. 5

[9] Y.-W. Chao, Y. Liu, X. Liu, H. Zeng, and J. Deng. Learning

to detect human-object interactions. In WACV, 2018. 3

[10] Y.-W. Chao, Z. Wang, Y. He, J. Wang, and J. Deng. HICO:
A benchmark for recognizing human-object interactions in
images. In ICCV, 2015. 3

[11] K. Cho, B. van Merrienboer, C¸ . G¨ulc¸ehre, F. Bougares,
H. Schwenk, and Y. Bengio. Learning phrase representations
using RNN encoder-decoder for statistical machine transla-
tion. In EMNLP, 2014. 5

[12] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships

with deep relational networks. In CVPR, 2017. 3

[13] D. Damen, H. Doughty, G. M. Farinella, S. Fidler,
A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Per-
rett, W. Price, and M. Wray. Scaling egocentric vision: The
EPIC-KITCHENS dataset. In ECCV, 2018. 2, 3

[14] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learn-
In

ing for physical interaction through video prediction.
NIPS, 2016. 2, 3

[15] K. Fragkiadaki, J. Huang, A. Alemi, S. Vijayanarasimhan,
S. Ricco, and R. Sukthankar. Motion prediction under
multimodality with conditional stochastic networks. arXiv
preprint arXiv:1705.02082, 2017. 2, 3

[16] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik. Recurrent

network models for human dynamics. In ICCV, 2015. 2, 3

[18] C. Gu, C. Sun, S. Vijayanarasimhan, C. Pantofaru, D. A.
Ross, G. Toderici, Y. Li, S. Ricco, R. Sukthankar, C. Schmid,
and J. Malik. AVA: A video dataset of spatio-temporally lo-
calized atomic visual actions. In CVPR, 2018. 2, 6

[19] W. L. Hamilton, R. Ying, and J. Leskovec. Representation
learning on graphs: Methods and applications. IEEE Data
Engineering Bulletin, 2017. 3

[20] M. Hoai and F. De la Torre. Max-margin early event detec-

tors. IJCV, 2014. 3

[21] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks
for object detection. arXiv preprint arXiv:1711.11575, 2017.
3

[22] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 5

[23] M. S. Ibrahim and G. Mori. Hierarchical relational networks
for group activity recognition and retrieval. In ECCV, 2018.
3

[24] H. Idrees, A. R. Zamir, Y. Jiang, A. Gorban, I. Laptev,
R. Sukthankar, and M. Shah. The THUMOS challenge on
action recognition for videos “in the wild”. CVIU, 2017. 2

[25] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. Structural-
RNN: Deep learning on spatio-temporal graphs. In CVPR,
2016. 3

[26] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. Black. To-
wards understanding action recognition. In ICCV, 2013. 2,
6

[27] N. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka,
O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel
networks. arXiv preprint arXiv:1610.00527, 2016. 2

[28] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014. 2

[29] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,
S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev,
M. Suleyman, and A. Zisserman. The Kinetics human action
video dataset. arXiv preprint arXiv:1705.06950, 2017. 2, 5
[30] Y. Ke, R. Sukthankar, and M. Hebert. Efﬁcient visual event

detection using volumetric features. In ICCV, 2005. 2

[31] T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel.
Neural relational inference for interacting systems. In ICML,
2018. 3

[32] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert.

Activity forecasting. In ECCV, 2012. 2, 3

[33] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, and
M. K. Chandraker. DESIRE: distant future prediction in dy-
namic scenes with interacting agents.
In CVPR, 2017. 2,
3

[34] I. Loshchilov and F. Hutter. SGDR: stochastic gradient de-
scent with restarts. arXiv preprint arXiv:1608.03983, 2016.
5

[35] W. Lotter, G. Kreiman, and D. Cox. Deep predictive cod-
ing networks for video prediction and unsupervised learning.
arXiv preprint arXiv:1605.08104, 2016. 2

[17] G. Gkioxari, R. Girshick, P. Doll´ar, and K. He. Detecting
and recognizing human-object intaractions. In CVPR, 2018.
3

[36] P. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun.
Predicting deeper into the future of semantic segmentation.
In ICCV, 2017. 2, 3

281

[37] S. Ma, L. Sigal, and S. Sclaroff. Learning activity progres-
In

sion in lstms for activity detection and early detection.
CVPR, 2016. 3

[55] K. Soomro, H. Idrees, and M. Shah. Online localization and
prediction of actions and interactions. IEEE PAMI, 2018. 2,
3, 6, 8

[38] M. Marszalek, I. Laptev, and C. Schmid. Actions in context.

In CVPR, 2009. 2

[39] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 2

[40] P. Mettes, J. van Gemert, and C. Snoek. Spot On: Action
In ECCV,

localization from pointly-supervised proposals.
2016. 2

[41] M. Monfort, B. Zhou, S. A. Bargal, A. Andonian, T. Yan,
K. Ramakrishnan, L. Brown, Q. Fan, D. Gutfruend, C. Von-
drick, et al. Moments in time dataset: one million videos
for event understanding. arXiv preprint arXiv:1801.03150,
2018. 2

[42] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T.
Lee, S. Mukherjee, J. Aggarwal, H. Lee, L. Davis, et al.
A large-scale benchmark dataset for event recognition in
surveillance video. In CVPR, 2011. 3

[43] R. B. Palm, U. Paquet, and O. Winther. Recurrent relational
networks for complex relational reasoning. arXiv preprint
arXiv:1711.08028, 2017. 3

[44] N. Petrovic, A. Ivanovic, and N. Jojic. Recursive estimation

of generative models of video. In CVPR, 2006. 2, 3

[45] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert,
and S. Chopra. Video (language) modeling: a baseline
for generative models of natural videos.
arXiv preprint
arXiv:1412.6604, 2014. 2

[46] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 3, 5

[47] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese.
Learning social etiquette: Human trajectory understanding
in crowded scenes. In ECCV, 2016. 2, 3

[48] M. Rodriguez, J. Ahmed, and M. Shah. Action MACH: a
spatio-temporal maximum average correlation height ﬁlter
for action recognition. In CVPR, 2008. 2

[49] A. Sadeghian, F. Legros, M. Voisin, R. Vesel, A. Alahi, and
S. Savarese. Car-net: Clairvoyant attentive recurrent net-
work. In ECCV, 2018. 2, 3

[50] A. Santoro, R. Faulkner, D. Raposo,

J. W. Rae,
M. Chrzanowski, T. Weber, D. Wierstra, O. Vinyals, R. Pas-
canu, and T. P. Lillicrap. Relational recurrent neural net-
works. arXiv preprint arXiv:1806.01822, 2018. 3

[51] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. P. Lillicrap. A simple neural
network module for relational reasoning. In NIPS, 2017. 3,
4, 6, 7

[52] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human

actions: a local SVM approach. In ICPR, 2004. 2

[53] G. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and
A. Gupta. Hollywood in homes: Crowdsourcing data collec-
tion for activity understanding. In ECCV, 2016. 2

[56] K. Soomro, A. Zamir, and M. Shah. UCF101: A dataset of
101 human actions classes from videos in the wild. Technical
Report CRCV-TR-12-01, 2012. 2

[57] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsuper-
vised learning of video representations using lstms. In ICML,
2015. 2

[58] S. Su, J. Pyo Hong, J. Shi, and H. Soo Park. Predicting be-
In

haviors of basketball players from ﬁrst person videos.
CVPR, 2017. 3

[59] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Suk-
thankar, and C. Schmid. Actor-centric relation network. In
ECCV, 2018. 3, 4, 5

[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all
you need. In NIPS, 2017. 5

[61] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o,
and Y. Bengio. Graph attention networks. In ICLR, 2018. 2,
3, 5, 6, 7, 8

[62] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee.
Learning to generate long-term future via hierarchical pre-
diction. arXiv preprint arXiv:1704.05831, 2017. 2, 3

[63] C. Vondrick, H. Pirsiavash, and A. Torralba. Anticipating
visual representations from unlabeled video. In CVPR, 2016.
2, 3

[64] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating

videos with scene dynamics. In NIPS, 2016. 2, 3

[65] C. Vondrick and A. Torralba. Generating the future with ad-

versarial transformers. In CVPR, 2017. 3

[66] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An uncer-
tain future: Forecasting from static images using variational
autoencoders. In ECCV, 2016. 2, 3

[67] J. Walker, A. Gupta, and M. Hebert. Patch to the future:

Unsupervised visual prediction. In CVPR, 2014. 2, 3

[68] J. Walker, A. Gupta, and M. Hebert. Dense optical ﬂow pre-

diction from a static image. In ICCV, 2015. 2

[69] J. Walker, K. Marino, A. Gupta, and M. Hebert. The pose
In

knows: Video forecasting by generating pose futures.
ICCV, 2017. 3

[70] X. Wang, R. B. Girshick, A. Gupta, and K. He. Non-local

neural networks. In CVPR, 2018. 3

[71] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
2

[72] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy. Rethinking
spatiotemporal feature learning for video understanding. In
ECCV, 2018. 5

[73] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynam-
ics: Probabilistic future frame synthesis via cross convolu-
tional networks. In NIPS, 2016. 2

[74] B. Yao and L. Fei-Fei. Modeling mutual context of object
In

and human pose in human-object interaction activities.
CVPR, 2010. 3

[54] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin.
Online real-time multiple spatiotemporal action localisation
and prediction. In ICCV, 2017. 2, 3, 6, 8

[75] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori,
and L. Fei-Fei. Every moment counts: Dense detailed label-
ing of actions in complex videos. IJCV, 2017. 2, 3

282

[76] J. Yuan, Z. Liu, and Y. Wu. Discriminative subvolume search

for efﬁcient action detection. In CVPR, 2009. 2

[77] J. Yuen and A. Torralba. A data-driven approach for event

prediction. In ECCV, 2010. 2, 3

[78] Y. Zhou and T. L. Berg. Temporal perception and prediction

in ego-centric video. In ICCV, 2015. 2, 3

283

