CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation

Kuo Du1 Xiangbo Lin2* Yi Sun2 Xiaohong Ma2

Dalian University of Technology, China

1dumyy2728@mail.dlut.edu.cn, 2{linxbo,lslwf,maxh}@dlut.edu.cn

Abstract

This paper focuses on the topic of vision based hand pose
estimation from single depth map using convolutional neu-
ral network (CNN). Our main contributions lie in designing
a new pose regression network architecture named CrossIn-
foNet. The proposed CrossInfoNet decomposes hand pose
estimation task into palm pose estimation sub-task and ﬁn-
ger pose estimation sub-task, and adopts two-branch cross-
connection structure to share the beneﬁcial complementary
information between the sub-tasks. Our work is inspired
by multi-task information sharing mechanism, which has
been few discussed in hand pose estimation using depth
data in previous publications.
In addition, we propose a
heat-map guided feature extraction structure to get better
feature maps, and train the complete network end-to-end.
The effectiveness of the proposed CrossInfoNet is evaluated
with extensively self-comparative experiments and in com-
parison with state-of-the-art methods on four public hand
pose datasets. The code is available in1.

1. Introduction

The research of vision based 3D hand pose estimation
is a hotspot in the ﬁeld of computer vision, virtual real-
ity and robotics. It has been studied for decades and has
made signiﬁcant progress in recent years [3, 6, 19]. Nev-
ertheless, it is still far from a solved problem due to the
challenges of high joint ﬂexibility, local self-similarity and
severe occlusions. Different efforts have been made in vi-
sion based hand pose estimation. The input data changed
from single RGB [2, 7], stereo RGB [24, 27], to depth maps
which have made many achievements [26, 30, 39]. Re-
cently, there seems to be a renewed interest to RGB im-
ages [24, 48, 18, 25]. The published hand pose estima-
tion methods can be categorized into two main categories
as either generative model-based [29, 35] or discriminative
learning-based methods [11, 32, 36, 38]. Beneﬁt from the
increase of data amounts and computational ability, deep

1https://github.com/dumyy/handpose

CNN has showed strong abilities and has become the lead-
ing method at present.

In 2017, Hands in the Million Challenge (HIM2017) [44]
on depth maps based hand pose estimation attracted the at-
tentions of many research teams. The issues discussed in
the competition summary paper [43] are also our concerns.

Firstly, treating depth maps as 2D images and regressing
3D joint coordinates directly is a commonly used hand pose
estimation pipeline. Although converting the 2.5D depth
maps into 3D voxelized forms will reserve more informa-
tion [12, 17], it suffers from heavy parameter loads and still
exists information defect.
In our work, we tend to be in
line with the argument of [39] to leverage the advances of
2D CNNs, and try to excavate more information from 2D
inputs.

Secondly, designing effective networks receives the most
attentions.
In machine learning, by sharing information,
multi-task learning has the advantages of reserving more
intrinsic information than single task learning. Learning
multiple tasks simultaneously will be helpful to enforce a
model with better generalizing ability [28]. However, multi-
task learning has not been paid enough attention in CNN
based hand pose estimation yet. As
[39] claimed, they
did the ﬁrst attempt to fuse the hand pose estimation re-
sults of the holistic regression and the heat-map detection
in a multi-task setup. Inspired by their achievements, we
design a new CNN structure for hand pose estimation in a
multi-task setup. Hierarchical model is one of hand pose es-
timation networks and has shown excellent performance in
competition. It usually divides the pose estimation problem
into sub-tasks by separately dealing with different ﬁngers
or different type of joints [4, 16, 47]. Intuitively, it would
be easily understood that palm joints have closer tie-ups
than those more ﬂexible ﬁnger joints. The global hand pose
will be mostly determined by the status of the palm joints,
while the local hand pose will be reﬂected by the actions
of the ﬁnger joints. Based on these knowledge, we design
a new hierarchical model in a multi-task setup. The pro-
posed architecture has two branches corresponding to the
palm joint regression sub-task and the ﬁnger joint regres-
sion sub-task, respectively. By cross-connections between

9896

the two branches, the noise in one branch becomes supple-
mental enhancement information in the other branch. This
will help each branch to focus on its speciﬁc sub-task as is
done in multi-task information sharing.

Thirdly, the output representations can be classiﬁed into
the probability density map (heat-maps) or the 3D coordi-
nates for each joint. Since the mapping between the 2D
depth maps and the 3D joint coordinates is highly nonlinear,
it will hamper the learning procedure and prevent the net-
work from accurately estimating the joint coordinates. In
contrast, the output representation with the heat-maps can
provide more joint related information than a single joint
location, which will help the network to get better feature
maps. The analysis in [43] has concluded that the heat-
map based method outperforms direct coordinate regres-
sion method. However, in heat-map based method, the ﬁnal
joint coordinates have usually to be inferred by maximum
operation on the heat-maps. Maximum operation is non-
differentiable, and it has to be tailored as a post-processing
step, but not an end-to-end training. Taking into account
of the advantages of the two representations, we propose a
heat-map guided feature extraction network structure.
In
fact, our idea skillfully applies the multi-task parameter
sharing.

In summary, for deep CNN based hand pose estimation
from single depth map, our work has the following contri-
butions:

• A new hand pose regression network in multi-task
setup is proposed. It takes advantage of information shar-
ing mechanism in multi-task learning. We use hierarchical
model to decompose the ﬁnal task into palm joint regression
sub-task and ﬁnger joint regression sub-task. By branch
cross-connection, the generated ‘attention mask’ guides one
branch to focus on palm joint regression, and the other
branch to focus on ﬁnger joint regression. Since the ‘atten-
tion mask’ enhances the sub-task features, the estimation
accuracy is improved effectively.

• A heat-map guided feature extraction structure is pro-
posed.
It transfers more effective features from the heat-
map detection task to the joint regressing task, without los-
ing the end-to-end training advantage.

• We implement several baselines to investigate informa-
tion sharing in a multi-task setup, which will provide valu-
able insights to this problem. We also carry out substantial
experiments on commonly used datasets, and compare the
performance with the state-of-the-art methods.

2. Related works

The achievements in vision based hand pose estima-
tion are very rich. Since our work focuses on deep CNN
based hand pose estimation from single depth map, we will
limit the discussions to those works related closely with our
work. Please refer to [8, 33, 43] for more comprehensive

reviews.

Pose parameterization: The object of hand pose es-
timation is to ﬁnd the joint coordinates. Directly regress-
ing these coordinates is the natural choice in the models for
output pose representation [4, 10, 12, 22, 23, 46]. How-
ever, since only one 3D coordinate for each joint has to be
regressed from the input, the highly non-linear mapping be-
tween the input and the 3D coordinates output hampers the
learning procedure. To cope with this problem, Tompson et
al. [38] ﬁrstly utilized 2D heat-maps for each hand joint as
the pose parameters and then translated them into 3D coor-
dinates by post-processing. They found that the interme-
diate heat-maps representation not only reduced required
learning capacity but also improved generalization perfor-
mance. Ge et al. [11] extended this method by exploiting
multi-view CNN to estimate 2D heat-maps for each view.
Moon et al. [17] adopted 3D heat-maps as the hand pose
parameters. Wan et al. [39] decomposed the pose parame-
ters into 2D heat-maps, 3D heat-maps, and unit 3D direc-
tional vector ﬁelds. Then these different outputs were trans-
lated into 3D joint coordinates by a vote casting scheme
with a variant of mean shift post-processing. Different from
their schemes, our work uses 3D coordinate regression un-
der heat-map constraints. Such strategy can help the model
to learn a better feature map, and get accurate joint coordi-
nates without the need of post-processing.

Model design: Designing a network according to hu-
man hand kinematics or morphology has received compet-
itive results in recent years [44]. Structured methods em-
bed physical hand motion constraints into the model or in
the loss function [16, 31, 46]. Hierarchical models divide
the pose estimation problem into sub-tasks according to the
hand structure. Chen et al. [40] applied constraints per ﬁn-
ger and joint-type (across ﬁngers) in their multiple regions
extraction step, each region containing a subset of joints.
The extracted feature regions were then integrated hierar-
chically and the hand pose were regressed by utilizing an
iterative cascaded method. Madadi et al. [16] designed a hi-
erarchically structured CNN, using ﬁve branches to model
each ﬁnger and an additional branch to model the palm ori-
entation. The ﬁnal layers of all branches were concatenated
into one layer to predict all joints. Zhou et al. [47] designed
a three-branch network according to different ﬁnger func-
tions in daily manipulation, where one branch correlated
with the thumb ﬁnger, one branch modeled the index ﬁn-
ger, and the last branch represented the other three ﬁngers.
These hierarchical models have their distinctive character-
istics. Here we explore a new two-branch model with one
branch for palm joint regression and the other branch for
ﬁnger joint regression. It is a common sense that the ﬁnger
joints are more ﬂexible than the palm joints. If we use two
different parameter sets to represent relatively stable palm
pose and ﬂexible ﬁnger pose separately, the regression task

9897

Palm

Fingers

Multi-task information sharing

T

Fingers

Palm

Feature extraction module

Feature refinement module

Multi-task information sharing

Convolution with 
max-pooling

Residual module 
with max-poling

Residual block Max-pooling

Fully-connected

Regression module

Element-wise sub

Concatenate

Figure 1. Overall network architecture with multi-task information sharing setup. We use the heat maps to highlight the regions with salient
features in the ﬁrst two modules. In the output part, the red dot represents the estimated joint positions.

will be implemented easier. In addition, we design cross-
connections between the two branches, which helps each
branch focus on its own task.

sub-task and ﬁnger joint regression sub-task. By a cross-
connection between the two sub-task regression branches,
the information is shared.

Multi-task information sharing: By sharing informa-
tion between related tasks, multi-task learning will enable
the model to generalize better on the tasks [28]. Multi-
task learning in deep neural networks has led to success in
many applications, such as in human pose estimation. Xia
et al. [41] proposed to jointly solve the two tasks of hu-
man parsing and pose estimation. They trained two fully
convolutional neural networks (FCNs), in which the esti-
mated pose provided object-level shape prior to regularize
part segments while the part-level segments constrained the
variation of pose locations. Finally the two tasks were fused
with a fully-connected conditional random ﬁeld (FCRF).
Nie et al. [20] proposed mutual learning to adapt model
for joint human parsing and pose estimation.
It effec-
tively exploited mutual beneﬁts by incorporating informa-
tion from their counterparts, providing more powerful rep-
resentations. Though effectively used in many applications,
multi-task learning has not been paid enough attention in
CNN based hand pose estimation yet. To our knowledge,
[39] was the ﬁrst one to give clear claim that they did hand
pose estimation in a multi-task setup. In their work, they
decomposed the hand pose parameters into 2D heat maps,
3D heat maps and unit 3D directional vector ﬁelds. The
three representations were treated as three tasks, and were
estimated via multi-task network cascades. Finally they
fused these estimations by mean shift algorithm based post-
processing. Our work is also built in multi-task learning
framework, but it is quite different from [39]. We divide
hand joints into two subsets, one set consisting of the palm
joints, the other set consisting of the ﬁnger joints. The joint
regression task is decomposed into palm joint regression

3. Method

A hand is an articulated object and has high degree of
freedom, and it is not easy to estimate hand pose accurately.
In order to deal with the highly non-linear mapping from
input depth data to output hand joint coordinates, the hand
pose estimation problem can be simpliﬁed into sub-tasks,
each of which is responsible of a sub-part or subset joint es-
timation. This is why designing hierarchical models to im-
plement the task. Here we propose a new hierarchical model
with an information sharing architecture named CrossIn-
foNet, as illustrated in Fig.1. The ﬁrst part is the initial
feature extraction module, where we integrate heat-maps
as constraints to learn better feature maps and get all ini-
tial joint features. The second part is the feature reﬁnement
module, where the task is decomposed into two sub-tasks,
one sub-task estimating the palm joints, the other sub-task
estimating the ﬁnger joints. The information sharing strat-
egy in this module guides the network to exploit useful clues
from counterpart towards effectively improving the perfor-
mance of hand pose estimation. The ﬁnal part is the joint
coordinate regression module.

We describe the details in the following sections. Section
3.1 describes the heat-map guided initial feature extraction
module. Section 3.2 presents the baseline network with two
independent sub-tasks without information sharing. Section
3.3 details how to provide complementary information by
cross information sharing between the two sub-tasks. Loss
function is introduced in Section 3.4 and implementation
details are given in Section 3.5.

9898

2

2

2

T

(cid:1838)(cid:3035)(cid:3047)

T

Feature extraction module guided by heat-maps

Convolution
with max-pooling

Residual
module

Residual 
block

Convolution
layer 

element-wise
sub

2 Up-sampling 

2 times 

Figure 2. The initial feature extraction module. This network takes
2D depth map as input with the size of 96 × 96 and outputs the
feature maps T with the size of 12×12. We use 2D heat maps with
the size of 24 × 24 as supervision to guide the feature extractions.

3.1. Heat map guided feature extraction

When a shallow CNN is used for feature extraction, the
estimated results are usually not satisfactory. Given the
problem, we design a novel feature extraction network with
two stages, named as initial feature extraction module and
feature reﬁnement module. As for the initial feature ex-
traction module, we choose the ResNet-50 [15] backbone
network with four residual modules because it is highly ef-
ﬁcient, as shown in Fig.2. In order to obtain more infor-
mation, we apply the feature pyramid structure to merge
different feature layers. We denote the feature maps for re-
gressing initial joint locations as T . Different from previous
heat-map based detection method, here the heat maps are
only used as the constraints to guide the feature extraction
and will not be passed to the subsequent module. The ob-
tained feature map T with 256 channels will be input to the
feature reﬁnement module. The kernel size of the residual
blocks is 3 × 3, and that of max-pooling layers is 2 × 2 with
stride 2. We use a convolution layer with 3 × 3 ﬁlters to
obtain the heat-map outputs for all joints.

(cid:1842)(cid:2868)

(cid:1832)(cid:2868)

concat

(cid:1858)(cid:3043)

(cid:1858)(cid:3033)

(cid:1838)(cid:3029)(cid:3043)

(cid:1838)(cid:3028)

(cid:1838)(cid:3029)(cid:3033)

Baseline feature refinement module

Regression module

Residual block with 
max-pooling

Full-connected
with dropout

Output layer

Figure 3. The baseline feature reﬁnement module connected with
the joint coordinate regression module. The kernel size of residual
block is set to 3 × 3 and the dimension of full-connected layer is
set to 2048.

(a) ICVL

(b) NYU

(c) MSRA

Figure 4. The palm joints subset (blue boxes) and the ﬁnger joints
subset (red boxes) on different datasets.

T from the initial feature extraction module are input to
the residual block to extract more intrinsic local features
of palm or ﬁngers in different branch. Then the output of
the full-connected layer fp in the palm branch and ff in the
ﬁnger branch are concatenated to estimate all joint coordi-
nates. We denote this architecture as the baseline network.
Since ICVL, NYU and MSRA datasets have different la-
bel protocol, the joints subsets have some differences, as
shown in Fig.4. The partition of HAND 2017 frame-based
challenge dataset is the same as that of MSRA.

3.2. Baseline feature reﬁnement architecture

3.3. New feature reﬁnement architecture

Some existing methods for hand pose estimation design
tree-like branches, each of which is responsible for one in-
dependent sub-task, or extracts hand features from the out-
put of one task to assist the other task at post-processing.
They can neither extract powerful features nor strengthen
the models. To fully utilize the extracted information, we
proposed a novel feature reﬁnement module based on multi-
task information sharing. Before introducing our new multi-
task feature reﬁnement module, we ﬁrst give the baseline
multi-task architecture, which is illustrated in Fig.3.

Among all the joints, the palm joints have a smaller ac-
tivity space compared to the ﬁnger joints, so the regressing
complexity of the two parts is also different. If we use two
different parameter sets to represent palm pose and ﬁnger
pose, the hand pose would be regressed easier. Therefore,
we separate the palm joint regression and ﬁnger joint re-
gression into two independent branches. The feature maps

The baseline network only considers regressing palm
and ﬁnger poses independently from each branch, which
has no essential difference with the universal branch based
network. There is little shared information between them,
except the input features T . However, in the palm regres-
sion branch, there are residual ﬁnger features. These ﬁnger
features may be noise for palm pose regression, but they
are beneﬁcial for ﬁnger pose regression. The same is in the
ﬁnger branch. To make full use of the useful ‘noise’ in-
formation between the two branches, we try to design the
network in a multi-task information sharing setup. Two-
task Cross-stitch Network [28] is a universal multi-task net-
work, as shown in Fig.5(a).
It uses multiple cross-stitch
units to leverage the knowledge of the other task by lazy
fusion. Nevertheless, lazy cross-stitch may cause interfer-
ence between sub-tasks, and lazy cross-stitch has no clear
understanding of the sub-tasks – their similarity and rela-

9899

(cid:23)(cid:131)(cid:149)(cid:141)(cid:3)(cid:883)

(cid:6)(cid:17)(cid:17)(cid:883)

(cid:6)(cid:17)(cid:17)(cid:883)

(cid:6)(cid:17)(cid:17)(cid:883)

(cid:6)(cid:17)(cid:17)(cid:883)

(cid:6)(cid:17)(cid:17)

(cid:6)(cid:17)(cid:17)

(cid:6)(cid:148)(cid:145)(cid:149)(cid:149)(cid:486)(cid:149)(cid:150)(cid:139)(cid:150)(cid:133)(cid:138)(cid:3)(cid:151)(cid:144)(cid:139)(cid:150)(cid:149)

(cid:6)(cid:17)(cid:17)

(cid:6)(cid:17)(cid:17)

(cid:23)(cid:131)(cid:149)(cid:141)(cid:3)(cid:884)

(cid:6)(cid:17)(cid:17)(cid:884)

(cid:6)(cid:17)(cid:17)(cid:884)

(cid:6)(cid:17)(cid:17)(cid:884)

(cid:6)(cid:17)(cid:17)(cid:884)

(cid:4666)(cid:131)(cid:4667)(cid:3)(cid:6)(cid:148)(cid:145)(cid:149)(cid:149)(cid:486)(cid:149)(cid:150)(cid:139)(cid:150)(cid:133)(cid:138)(cid:3)(cid:144)(cid:135)(cid:150)(cid:153)(cid:145)(cid:148)(cid:141)

(cid:22)(cid:141)(cid:139)(cid:146)

(cid:19)(cid:131)(cid:142)(cid:143)

(cid:9)(cid:139)(cid:144)(cid:137)(cid:135)(cid:148)

(cid:11)(cid:131)(cid:144)(cid:134)

(cid:9)(cid:139)(cid:144)(cid:137)(cid:135)(cid:148)

(cid:19)(cid:131)(cid:142)(cid:143)

(cid:4666)(cid:132)(cid:4667)(cid:3)(cid:18)(cid:151)(cid:148)(cid:3)(cid:133)(cid:148)(cid:145)(cid:149)(cid:149)(cid:486)(cid:133)(cid:145)(cid:144)(cid:144)(cid:135)(cid:133)(cid:150)(cid:139)(cid:145)(cid:144)(cid:3)(cid:143)(cid:145)(cid:134)(cid:135)(cid:142)

(cid:9)(cid:139)(cid:144)(cid:137)(cid:135)(cid:148)(cid:3)

(cid:21)(cid:135)(cid:137)(cid:148)(cid:135)(cid:149)(cid:149)(cid:139)(cid:145)(cid:144)

(cid:19)(cid:131)(cid:142)(cid:143)(cid:3)

(cid:21)(cid:135)(cid:137)(cid:148)(cid:135)(cid:149)(cid:149)(cid:139)(cid:145)(cid:144)

Figure 5. Network comparisons with Cross-stitch Network

tionships.

We hope to actively guide how the sub-tasks should in-
teract with each other. By guided information sharing, the
features related to the same targets should be merged and
enhanced. Fig.5(b) illustrates the proposed multi-task in-
formation sharing mechanism. It uses “skip line” to sepa-
rate palm and ﬁngers (Finger) by subtracting the palm fea-
tures from the global hand features, then uses cross line to
concatenate the ﬁnger features from the two branches. It
reduces the interference from the palm and enhances the
ﬁnger features once more, and vice versa.

The detailed network structure is shown in Fig.6. Initial
features T have palm related features and ﬁnger related fea-
tures. By subtraction operation between T and palm pose
dominated features P0 via skip-connection, we get resid-
ual ﬁnger features F , regarded as ﬁnger ‘attention mask’.
This mask may be ‘noise’ for palm pose regression, but it
will be beneﬁcial for ﬁnger pose regression, which helps
guide the branch to extract ﬁner features. In the same way,
we get the palm ‘attention mask’ P . By cross-connection,
P0 are concatenated with P and form the enhanced palm
features P1. The enhanced ﬁnger features F1 are also ob-
tained using the similar process. In this way, our new net-
work architecture establishes associations between the dif-
ferent sub-tasks. The output features F2 and P2 are got from
the followed residual block. In the end, the 3D hand joint
coordinates are estimated through the ﬁnal regression mod-
ule. The network parameters are presented in Fig.6, and the
main pose regression procedure is described in Algorithm
1.

3.4. Loss functions

We adopt the mean square error between the ground-
truth and the estimated joint coordinates as the loss func-
tion. In the initial feature extraction module, we use a heat
map as the constraint to guide the network for a better global

Algorithm 1 joint regression with multi-task information
sharing.
Input:

Symbols:
∗ : spatial convolution operator
⊗ : feature concatenation operator
p0, p1 : convolutional layers for palm feature extraction
in different stages
f0, f1 : convolutional layers for ﬁngers feature extrac-
tion in different stages
fc : Full-connected layers for regressing joint locations.
T ∈ T w×h×c: regression feature

1: P0 = T ∗ p0; F0 = T ∗ f0
2: F = T − P0; P = T − F0
3: P1 = P0 ⊗ P ; F1 = F0 ⊗ F
4: P2 = P1 ∗ p1; F2 = F1 ∗ f1
5: Jp = fc(P2); Jf = fc(F2)
6: J = Jp ⊗ Jf
Output:

J

Preliminary features
Residual features
Enhanced features
The ﬁnal features
The joint coordinates
The ﬁnal joint coordinates

feature extraction, so the detection loss of heat-map is de-
ﬁned as:

Lht = X

A

n=1 Xu,v

kH a∗

n (u, v) − H a

n(u, v)k

2

(1)

where A denotes the joint number of the whole hand. H a∗
n
and H a
n represent the ground-truth heat-map and estimated
heat-map of joint n, respectively.

In the feature reﬁnement module, we introduce two con-
straints, Lbp and Lbf , to extract the preliminary palm fea-
tures P0 and ﬁnger features F0. They are deﬁned as:

2

(2)

(3)

2

Lbp = X

P

n=1 Xu,v

kH p∗

n (u, v) − H p

n(u, v)k

Lbf = X

F

n=1 Xu,v (cid:13)(cid:13)H f ∗

n (u, v) − H f

n (u, v)(cid:13)(cid:13)

n and H f ∗

where H p∗
the nth palm joint and ﬁnger joint, respectively. H p
are the corresponding network outputs.

n represent the ground-truth heat map of
n and H f
n

In the regression module, three losses are used to su-
pervise the ﬁnal outputs of each subtask and the total hand
joints. They are palm joint regression loss Lep, ﬁnger joint
regression loss Lef , and total hand joint regression loss La.

Lep = X

P

n=1

Lef = X

F

n=1

La = X

A

n=1

(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)

J p∗
n − J p

J f ∗
n − J f

J a∗
n − J a

2

2

2

2

n(cid:13)(cid:13)(cid:13)
n(cid:13)(cid:13)(cid:13)
n(cid:13)(cid:13)(cid:13)

2

2

(4)

(5)

(6)

n and J p

where J p∗
3D coordinates of the nth palm joint, J f ∗

n denote the ground-truth and estimated
n are the

n and J f

9900

T

(cid:1842)(cid:2868)

(cid:1838)(cid:3029)(cid:3043)

(cid:1838)(cid:3029)(cid:3033)

(cid:1832)(cid:2868)

(cid:1832)(cid:2879)

(cid:1842)(cid:2879)

(cid:1832)(cid:2869)

(cid:1842)(cid:2869)

concat

(cid:1858)(cid:3033)

(cid:1858)(cid:3043)

(cid:1832)(cid:2870)

(cid:1842)(cid:2870)

(cid:1838)(cid:3032)(cid:3033)
(cid:3033)

(cid:1838)(cid:3028)
(cid:3028)

(cid:1838)(cid:3032)(cid:3043)
(cid:3032)(cid:3043)(cid:3043)(cid:3043)(cid:3043)

Cross-connection feature refinement module

Regression module

Residual block Max-pooling

Full-connected 
with dropout

output

Element-wise sub

Concatenate

Figure 6. The new feature reﬁnement module connected with the joint regression module. It is designed based on multi-task information
sharing mechanism. The kernel size of the residual block is set to 3 × 3 and the dimension of full-connected layer is set to 2048.

ground-truth and estimated 3D coordinates of the nth ﬁnger
joint, J a∗
n denote the ground-truth and estimated 3D
coordinates of the nth hand joint.

n and J a

4. Experiments and results

4.1. Datasets and evaluation metrics

The total loss function is:

L = α × (Lht + Lbp + Lbf ) + β × (Lep + Lef + La) (7)

where α, β are the factors to balance detection loss and re-
gression loss. In our experiments, α and β are set to be 0.01
and 1, respectively.

3.5. Implementation details

A hand area is ﬁrstly cropped from the original image
and resized to a ﬁxed size of 96 × 96. The depth values
within the cropped region are normalized to [-1, 1] and the
labels are also normalized to keep the correspondence with
the cropped depth map. We apply online data augmenta-
tion during training, including random rotation ([-180, 180]
degree), translation ([-10, 10] pixel) and scaling ([0.9, 1.1]).

The proposed network is trained in an end-to-end man-
ner. All weights are initialized from the zero-mean normal
distribution with σ = 0.01. We choose Adam algorithm to
train the model with an initial learning rate 1e-3, batch size
128 and weight decay 1e-5. The learning rate is reduced by
a factor of 0.96 every epoch, and the dropout rate is set to
be 0.6 to prevent over-ﬁtting.

Our network is implemented by Tensorﬂow [1] and the
RTX 2080 TI GPU is used for training and testing. We
trained the model for 110 epochs. The training time of our
model is 15 hours for ICVL dataset, 6.5 hours for NYU
and MSRA datasets, and 3 days for HANDS 2017 chal-
lenge dataset, respectively. While testing, our model runs at
124.5 fps on a single GPU.

ICVL Dataset. The ICVL dataset [34] has 330K frames
for training and 1.5k for testing. The training set consists of
the genuine 22k frames and an additional 300K augmented
frames with in-plane rotations. This dataset has 16 anno-
tated joints. We use complete frames for training, while in
the self-comparisons we only use the genuine 22k.

NYU Dataset. The NYU dataset [38] contains 72k train-
ing frames and 8k testing frames from three different views.
The training set is collected from subject A, while the test-
ing set is collected from subject A and B. Most previous
works only used view 1 and 14 annotated joints for train-
ing and testing, we also use the same setup for comparison
purposes.

MSRA Dataset. The MSRA dataset [32] consists of
76.5k depth images with 21 annotated joints. It has 9 sub-
jects and 17 different gestures for each one. Following the
common evaluation protocol [32], we also use the leave-
one-subject-out method to evaluate the result.

HANDS 2017 Challenge Frame-based Dataset. This
dataset [44] contains 957k training and 295k testing depth
frames, which are sampled from BigHand2.2M [45] and
FHAD [9] datasets. The training set has 5 subjects, while
the testing set has 10 subjects, including 5 unseen subjects.
This dataset has 21 annotated joints.

Evaluation Metrics. We use two metrics to evaluate the
performance of different 3D hand pose estimation methods.
One is the average 3D distance error between the ground
truth and the predicted 3D joint location for each joint, the
other is the percentage of success frames below a threshold,
which is the same as [37].

9901

Base

Strategy

Average 3D distance error (mm)
ICVL
9.28
Base+HM 9.08
8.79
Cross+HM 8.48

NYU
11.17
10.84
10.57
10.08

Cross

Table 1. Self-comparison results on average 3D distance er-
ror (mm). Base: baseline network without the heat-map con-
straints; Base + HM: baseline network with the heat-map con-
straints; Cross: cross-connection network without the heat-map
constraints; Cross + HM: cross-connection network with the heat-
map constraints.

4.2. Self comparisons

We conduct ablation experiments on both ICVL[34] and
NYU[38] datasets. To evaluate the advantages of the heat-
map constraints, we compared the results of baseline net-
work with or without heat-map constraints. To demonstrate
the performance of the multi-task information sharing net-
work, we compared it with baseline network.

As shown in Tab.1, the baseline network with heat-map
constraints reduces the mean 3D distance error by 0.2mm
(from 9.28 to 9.08) on the ICVL dataset and by 0.33mm
(from 11.17 to 10.84) on the NYU dataset, compared to the
one without heat-map constraints. It proves that the heat-
map constraints enforce the model to get better features and
the estimated errors decrease. Then based on the initial fea-
ture extraction network with heat-map constraints, we com-
pared the effect of two different feature reﬁnement modules
on the average 3D distance error. The proposed model with
cross-connection signiﬁcantly lowers the errors by 0.60mm
(from 9.08 to 8.48) on the ICVL dataset and by 0.76mm
(from 10.84 to 10.08) on the NYU dataset, compared to the
one in the baseline model with two separated branches. Ob-
viously, the result of this comparative experiment supports
our viewpoint that multi-task information sharing can get
more accurate hand pose estimation.

Based on the comprehensive self-comparisons, it can be
concluded that the proposed model with multi-task infor-
mation sharing via cross-connected two-branch architecture
and heat-map guided initial feature extraction, has the best
performance in hand pose estimation.

4.3. Comparisons with state of the art methods

We compared the performance of the proposed Cross-
InfoNet on three public 3D hand pose datasets with most
of state-of-the-art methods, including methods using depth
latent random forest (LRF)[34],
maps (2D) as inputs:
model-based method (DeepModel)[46],
feedback loop
training (Feedback) [23], Lie-X [42], DeepPrior with re-
ﬁnement (DeepPrior) [22],
improved DeepPrior (Deep-
Prior++) [21], region ensemble network (Ren-4x6x6 [14],

Mean error (mm)

ICVL NYU MSRA
15.97
14.51

-
-

Methods

Feedback [23]

Lie-X [42]
LRF [34]

DeepModel [46]
DeepPrior [22]
Ren-4x6x6 [14]
Ren-9x6x6 [40]
DeepPrior++ [21]

Pose-Ren [4]
DenseReg [39]

CrossInfoNet(Ours)

3DCNN [12]
SHPR-Net [5]

HandPointNet [10]
Point-to-Point [13]

V2V [17]

-
-
-

17.04
19.73
13.39
12.69
12.24
11.81
10.2
10.08
14.1
10.78
10.54

9.1
8.42

12.58
11.56
10.4
7.63
7.31
8.1
6.79
7.3
6.73

-

7.22
6.94
6.3
6.28

-
-
-
-

9.7
9.5
8.65
7.2
7.86
9.6
7.76
8.5
7.7
7.59

Input

2D
2D
2D
2D
2D
2D
2D
2D
2D
2D
2D
3D
3D
3D
3D
3D

Table 2. Comparisons with state-of-the-art methods on three
datasets. Mean error indicates the average 3D distance error.

Methods
V2V [17]

DenseReg [39]

Point-to-Point [13]
CrossInfoNet (Ours)

Testing on single GPU (fps)

3.5
27.8
41.8
124.5

Table 3. Comparison of inference time while testing.

Ren-9x6x6 [40]), Pose-guided REN (Pose-Ren) [4], dense
regression network (DenseReg) [39], and methods using
point cloud or voxel (3D) as input: 3DCNN [12], SHPR-
Net [5], HandPointNet [10], Point-to-Point [13], V2V [17].
The results of some methods used for comparisons are ob-
tained from the online available prediction labels, others are
extracted from their papers.

As shown in Tab.2 and Fig.7, our results outperform the
results of the state-of-the-art methods whose input is a depth
map on ICVL and NYU datasets. Compared to those meth-
ods using 3D inputs, our results are worse than V2V [17]
and Point-to-Point [13], but have larger improvement than
3DCNN [12] and SHPR-Net [5]. For the MSRA dataset,
our method gets comparable results with the best 3D CNN
method. DenseReg [39] is better than our method on this
dataset. Nevertheless, when the thresthod is below 10mm,
our method is better on percentage of success frames met-
ric. The qualitative results of our method on three datasets
are shown in Fig.8.

Although on ICVL and NYU datasets, V2V and Point-
to-Point methods with 3D input are better, and on MSRA
dataset, DenseReg method with 2D input is better, they have
a higher inference time on test data than our method. The

9902

Figure 7. Comparisons with state-of-the-art methods. Top row: the percentage of good frames over different error thresholds. Bottom row:
3D distance errors per hand joints. Left: NYU [38] dataset. Middle: ICVL [34] dataset. Right: MSRA [32] dataset.

Figure 8. The qualitative results of our method on three datasets. Left: ICVL [34] dataset. Middle: NYU [38] dataset. Right: MSRA [32]
dataset. Ground truth is shown in blue, and the estimated pose is shown in red.

comparisons about inference time are listed in Tab.3.

We also tested the performance of our method on the
HANDS 2017 frame-based challenge dataset [44] on Feb.2,
2019. Our method won the ﬁrst place, and had the best
performance on the Unseen data.

5. Conclusion

sults prove that the proposed strategies are beneﬁcial to get
more accurate results, and the results of our method on three
3D hand pose datasets outperform most of previous works.
Moreover, the proposed method also achieves the best re-
sult in the hand pose estimation challenge, compared to all
previous participants. We hope this work can provide a new
idea of network design for hand pose estimation.

Our work aims at exploring an effective CNN network
to get the hand joint coordinates from depth data input. Our
designed two-branch cross-connection network hierarchi-
cally regresses the palm pose and the ﬁnger pose by infor-
mation sharing in a multi-task setup. It also uses heat-map
guidance to get better feature maps. The experimental re-

References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning.
In OSDI, vol-
ume 16, pages 265–283, 2016.

9903

PalmWrist1Wrist2Thumb.R1Thumb.R2Thumb.TIndex.RIndex.TMid.RMid.TRing.RRing.TPinky.RPinky.TMean024681012141618202224262830Mean Error (mm)DeepPriorDeepModelFeedbackLie-X3DCNNREN-4x6x6REN-9x6x6DeepPrior++Pose-RENSHPR-NetDenseRegHandPointNetPoint-to-PointV2VOurs01020304050607080Maximum allowed distance to GT (mm)0102030405060708090100Fraction of frames within distance (%)DeepPriorDeepModelFeedbackLie-X3DCNNREN-4x6x6REN-9x6x6DeepPrior++Pose-RENSHPR-NetDenseRegHandPointNetPoint-to-PointV2VOursPalmThumb.RThumb.MThumb.TIndex.RIndex.MIndex.TMid.RMid.MMid.TRing.RRing.MRing.TPinky.RPinky.MPinky.TMean02468101214161820Mean Error (mm)LRFDeepModelDeepPriorRen-4x6x6Ren-9x6x6DenseRegSHPR-NetHandPointNetPose-RENPoint-to-PointV2VOurs01020304050607080Maximum allowed distance to GT (mm)0102030405060708090100Fraction of frames within distance (%)LRFDeepModelDeepPriorDeepPrior++Ren-4x6x6Ren-9x6x6DenseRegSHPR-NetHandPointNetPose-RENPoint-to-PointV2VOursWristIndex.MIndex.PIndex.DIndex.TMid.MMid.PMid.DMid.TRing.MRing.PRing.DRing.TPinky.MPinky.PPinky.DPinky.TThumb.MThumb.PThumb.DThumb.TMean0246810121416Mean Error (mm)Ren-9x6x63DCNNPose-RENHandPointNetSHPR-NetPoint-to-PointV2VDenseRegOurs01020304050607080Maximum allowed distance to GT (mm)0102030405060708090100Fraction of frames within distance (%)Ren-9x6x63DCNNPose-RENHandPointNetSHPR-NetPoint-to-PointV2VDenseRegOurs[2] Vassilis Athitsos and Stan Sclaroff. Estimating 3d hand pose
from a cluttered image.
In Computer Vision and Pattern
Recognition, 2003. Proceedings. 2003 IEEE Computer So-
ciety Conference on, volume 2, pages II–432. IEEE, 2003.

[3] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Aug-
mented skeleton space transfer for depth-based hand pose es-
timation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 8330–8339,
2018.

[4] Xinghao Chen, Guijin Wang, Hengkai Guo, and Cairong
Zhang. Pose guided structured region ensemble network for
cascaded hand pose estimation. Neurocomputing, 2018.

[5] Xinghao Chen, Guijin Wang, Cairong Zhang, Tae-Kyun
Kim, and Xiangyang Ji. Shpr-net: Deep semantic hand pose
regression from point clouds. IEEE Access, 6:43425–43439,
2018.

[6] Chiho Choi, Sang Ho Yoon, Chin-Ning Chen, and Karthik
Ramani. Robust hand pose estimation during the interaction
with an unknown object. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3123–3132, 2017.

[7] Martin de La Gorce, David J Fleet, and Nikos Para-
gios. Model-based 3d hand pose estimation from monocular
video. IEEE transactions on pattern analysis and machine
intelligence, 33(9):1793–1805, 2011.

[8] Ali Erol, George Bebis, Mircea Nicolescu, Richard D Boyle,
and Xander Twombly. Vision-based hand pose estimation: A
review. Computer Vision and Image Understanding, 108(1-
2):52–73, 2007.

[9] Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul
Baek, and Tae-Kyun Kim. First-person hand action bench-
mark with rgb-d videos and 3d hand pose annotations. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.

[10] Liuhao Ge, Yujun Cai, Junwu Weng, and Junsong Yuan.
Hand pointnet: 3d hand pose estimation using point sets. In
CVPR, June 2018.

[11] Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann.
Robust 3d hand pose estimation in single depth images: from
single-view cnn to multi-view cnns. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3593–3601, 2016.

[12] Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann.
3d convolutional neural networks for efﬁcient and robust
hand pose estimation from single depth images. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, volume 1, page 5, 2017.

[13] Liuhao Ge, Zhou Ren, and Junsong Yuan. Point-to-point
regression pointnet for 3d hand pose estimation. In ECCV,
September 2018.

[14] Hengkai Guo, Guijin Wang, Xinghao Chen, Cairong Zhang,
Fei Qiao, and Huazhong Yang. Region ensemble network:
Improving convolutional network for hand pose estimation.
In Image Processing (ICIP), 2017 IEEE International Con-
ference on, pages 4512–4516. IEEE, 2017.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-

ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[16] Meysam Madadi, Sergio Escalera, Xavier Bar´o, and
Jordi Gonzalez.
End-to-end global to local cnn learn-
ing for hand pose recovery in depth data. arXiv preprint
arXiv:1705.09606, 2017.

[17] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.
V2v-posenet: Voxel-to-voxel prediction network for accu-
rate 3d hand and human pose estimation from a single depth
map. In CVPR, June 2018.

[18] Franziska Mueller, Florian Bernard, Oleksandr Sotny-
chenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, and
Christian Theobalt. Ganerated hands for real-time 3d hand
tracking from monocular rgb.
In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2018.

[19] Franziska Mueller, Dushyant Mehta, Oleksandr Sotny-
chenko, Srinath Sridhar, Dan Casas, and Christian Theobalt.
Real-time hand tracking under occlusion from an egocentric
rgb-d sensor. In Proceedings of International Conference on
Computer Vision (ICCV), volume 10, 2017.

[20] Xuecheng Nie, Jiashi Feng, and Shuicheng Yan. Mutual
learning to adapt for joint human parsing and pose estima-
tion. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 502–517, 2018.

[21] Markus Oberweger and Vincent Lepetit. Deepprior++: Im-
proving fast and accurate 3d hand pose estimation. In ICCV
Workshops, Oct 2017.

[22] Markus Oberweger, Paul Wohlhart, and Vincent Lepetit.
In

Hands deep in deep learning for hand pose estimation.
Computer Vision Winter Workshop, 2015.

[23] Markus Oberweger, Paul Wohlhart, and Vincent Lepetit.
Training a feedback loop for hand pose estimation. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 3316–3324, 2015.

[24] Paschalis Panteleris and Antonis Argyros. Back to rgb:
3d tracking of hands and hand-object interactions based on
short-baseline stereo. Hand, 2(63):39, 2017.

[25] Paschalis Panteleris, Iasonas Oikonomidis, and Antonis A.
Argyros. Using a single rgb frame for real time 3d hand pose
estimation in the wild. 2018 IEEE Winter Conference on
Applications of Computer Vision (WACV), pages 436–445,
2018.

[26] Chen Qian, Xiao Sun, Yichen Wei, Xiaoou Tang, and Jian
Sun. Realtime and robust hand tracking from depth. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 1106–1113, 2014.

[27] Romer Rosales, Vassilis Athitsos, Leonid Sigal, and Stan
Sclaroff. 3d hand pose reconstruction using specialized map-
pings. In Computer Vision, 2001. ICCV 2001. Proceedings.
Eighth IEEE International Conference on, volume 1, pages
378–385. IEEE, 2001.

[28] Sebastian Ruder. An overview of multi-task learning in deep

neural networks. arXiv preprint arXiv:1706.05098, 2017.

[29] Toby Sharp, Cem Keskin, Duncan Robertson, Jonathan Tay-
lor, Jamie Shotton, David Kim, Christoph Rhemann, Ido Le-
ichter, Alon Vinnikov, Yichen Wei, et al. Accurate, robust,

9904

[42] Chi Xu, Lakshmi Narasimhan Govindarajan, Yu Zhang, and
Li Cheng. Lie-x: Depth image based articulated object pose
estimation, tracking, and action recognition on lie groups. In-
ternational Journal of Computer Vision, 123:454–478, 2017.
[43] Shanxin Yuan, Guillermo Garcia-Hernando, Bj¨orn Stenger,
Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee, Pavlo
Molchanov, Jan Kautz, Sina Honari, Liuhao Ge, et al. Depth-
based 3d hand pose estimation: From current achievements
to future goals. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.

[44] Shanxin Yuan, Qi Ye, Guillermo Garcia-Hernando, and Tae-
Kyun Kim. The 2017 hands in the million challenge on
3d hand pose estimation. arXiv preprint arXiv:1707.02237,
2017.

[45] Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, and Tae-
Kyun Kim. Bighand2.2m benchmark: Hand pose dataset and
state of the art analysis. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), July 2017.

[46] Xingyi Zhou, Qingfu Wan, Wei Zhang, Xiangyang Xue,
and Yichen Wei. Model-based deep hand pose estimation.
In Proceedings of the Twenty-Fifth International Joint Con-
ference on Artiﬁcial Intelligence, pages 2421–2427. AAAI
Press, 2016.

[47] Yidan Zhou, Jian Lu, Kuo Du, Xiangbo Lin, Yi Sun, and Xi-
aohong Ma. Hbe: Hand branch ensemble network for real-
time 3d hand pose estimation. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 501–
516, 2018.

[48] Christian Zimmermann and Thomas Brox. Learning to es-
timate 3d hand pose from single rgb images. In The IEEE
International Conference on Computer Vision (ICCV), Oct
2017.

and ﬂexible real-time hand tracking. In Proceedings of the
33rd Annual ACM Conference on Human Factors in Com-
puting Systems, pages 3633–3642. ACM, 2015.

[30] Jamie Shotton, Ross Girshick, Andrew Fitzgibbon, Toby
Sharp, Mat Cook, Mark Finocchio, Richard Moore, Push-
meet Kohli, Antonio Criminisi, Alex Kipman, et al. Efﬁ-
cient human pose estimation from single depth images. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
35(12):2821–2840, 2013.

[31] Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.
Compositional human pose regression. In The IEEE Inter-
national Conference on Computer Vision (ICCV), volume 2,
page 7, 2017.

[32] Xiao Sun, Yichen Wei, Shuang Liang, Xiaoou Tang, and Jian
Sun. Cascaded hand pose regression. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2015.

[33] James S Supancic, Gregory Rogez, Yi Yang, Jamie Shot-
ton, and Deva Ramanan. Depth-based hand pose estimation:
data, methods, and challenges. In Proceedings of the IEEE
international conference on computer vision, pages 1868–
1876, 2015.

[34] Danhang Tang, Hyung Jin Chang, Alykhan Tejani, and Tae-
Kyun Kim. Latent regression forest: Structured estimation of
3d articulated hand posture. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
3786–3793, 2014.

[35] Danhang Tang, Jonathan Taylor, Pushmeet Kohli, Cem Ke-
skin, Tae-Kyun Kim, and Jamie Shotton. Opening the black
box: Hierarchical sampling optimization for estimating hu-
man hand pose.
In Proceedings of the IEEE international
conference on computer vision, pages 3325–3333, 2015.

[36] Danhang Tang, Tsz-Ho Yu, and Tae-Kyun Kim. Real-
time articulated hand pose estimation using semi-supervised
transductive regression forests. In Proceedings of the IEEE
international conference on computer vision, pages 3224–
3231, 2013.

[37] Jonathan Taylor, Jamie Shotton, Toby Sharp, and Andrew
Fitzgibbon. The vitruvian manifold: Inferring dense corre-
spondences for one-shot human pose estimation.
In Com-
puter Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on, pages 103–110. IEEE, 2012.

[38] Jonathan Tompson, Murphy Stein, Yann Lecun, and Ken
Perlin. Real-time continuous pose recovery of human hands
using convolutional networks. ACM Transactions on Graph-
ics (ToG), 33(5):169, 2014.

[39] Chengde Wan, Thomas Probst, Luc Van Gool, and Angela
Yao. Dense 3d regression for hand pose estimation. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.

[40] Guijin Wang, Xinghao Chen, Hengkai Guo, and Cairong
Zhang. Region ensemble network: towards good practices
for deep 3d hand pose estimation. Journal of Visual Commu-
nication and Image Representation, 2018.

[41] Fangting Xia, Peng Wang, Xianjie Chen, and Alan L Yuille.
Joint multi-person pose estimation and semantic part seg-
mentation. In CVPR, volume 2, page 7, 2017.

9905

