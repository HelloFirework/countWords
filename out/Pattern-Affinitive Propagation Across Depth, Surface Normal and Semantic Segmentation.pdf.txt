Pattern-Afﬁnitive Propagation across

Depth, Surface Normal and Semantic Segmentation

Zhenyu Zhang1†

Yan Yan1†

Zhen Cui1†∗ Chunyan Xu1†
Nicu Sebe2‡
Jian Yang1†∗

1PCA Lab, Nanjing University of Science and Technology
zhangjesse, zhen.cui, cyx, yyan, csjyang@njust.edu.cn

2Multimedia and Human Understanding Group, University of Trento

niculae.sebe@unitn.it

Abstract

In this paper, we propose a novel Pattern-Afﬁnitive Prop-
agation (PAP) framework to jointly predict depth, surface
normal and semantic segmentation. The motivation behind
it comes from the statistic observation that pattern-afﬁnitive
pairs recur much frequently across different tasks as well as
within a task. Thus, we can conduct two types of propaga-
tions, cross-task propagation and task-speciﬁc propagation,
to adaptively diffuse those similar patterns. The former in-
tegrates cross-task afﬁnity patterns to adapt to each task
therein through the calculation on non-local relationships.
Next the latter performs an iterative diffusion in the feature
space so that the cross-task afﬁnity patterns can be widely-
spread within the task. Accordingly, the learning of each
task can be regularized and boosted by the complementary
task-level afﬁnities. Extensive experiments demonstrate the
effectiveness and the superiority of our method on the joint
three tasks. Meanwhile, we achieve the state-of-the-art or
competitive results on the three related datasets, NYUD-v2,
SUN-RGBD and KITTI.

1. Introduction

The predictions of depth, surface normal and semantic
segmentation are important and challenging for scene un-

∗Corresponding authors
†Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan and Jian Yang are
with PCA Lab, Key Lab of Intelligent Perception and Systems for High-
Dimensional Information of Ministry of Education, and Jiangsu Key Lab
of Image and Video Understanding for Social Security, School of Comput-
er Science and Engineering, Nanjing University of Science and Technolo-
gy. Zhenyu Zhang is also a visiting student in University of Trento.

‡Nicu Sebe is the head of Dept. of Information Engineering and Com-
puter Science Leader of Multimedia and Human Understanding Group
(MHUG) University of Trento.

Figure 1. Statistics of matched afﬁnity (or dissimilar) pairs across
depth, surface normal and segmentation maps.
(a) Visual exhi-
bition. The point pairs colored white are the matched afﬁnity
pixels across three tasks at the same positions, while the pairs
of black points correspond to dissimilar pixels across three map-
s. For the similarity metrics, REL/RMSE/Label consistency are
taken respectively for the three maps. (b) Statistical results. We
compute the success ratio of pairs matching across different maps
on NYUD-v2 and SUN-RGBD datasets, and observe that the suc-
cess ratios of pairs matching cross tasks are rather high.

derstanding. Also, they have many potential industrial ap-
plications such as autonomous driving system [4], simul-
taneous localization and mapping (SLAM) [52] and so-
cially interactive robotics [12]. Currently, most method-
s [10, 11, 13, 14, 40, 43] focused on one of the three
tasks, and they also achieved the state-of-the-art perfor-
mance through the technique of deep learning.

In contrast to the single-task methods, recently, several
joint-task learning methods [58, 62, 46, 32] on these tasks
have shown a promising direction to improve the prediction-
s by utilizing task-correlative information to boost for each
other. In a broad sense, the problem of joint-task learning
has been widely studied in the past few decades [3]. But
more recently most approaches took the technique line of
deep learning for possible different tasks [41, 16, 18, 25,
26]. However, most methods aimed to perform feature fu-

14106

Image Depth Segmentation Surface Normal Similar Pairs Dissimilar Pairs 20%40%60%Segmentation &DepthNormal & DepthSegmentation &Normalrate% in NYUDv2rate% in SUNRGBDRate of Matched Similar Pairs 40%50%60%Segmentation &DepthNormal & DepthSegmentation  &Normalrate% in NYUDv2rate% in SUNRGBDRate of Matched Dissimilar Pairs (a) (b) sion or parameter sharing for task interaction. The fusion
or sharing ways may utilize the correlative information be-
tween tasks, but there exist some drawbacks. For examples,
the integration of different features might result into the am-
biguity of information; the fusion does not explicitly model
the task-level interaction where we do not know what infor-
mation are transmitted. Conversely, could we ﬁnd some ex-
plicitly common patterns across different tasks for the joint-
task learning?

We take the three relative tasks: depth estimation, sur-
face normal prediction and semantic segmentation, and then
conduct a statistical analysis on those second-order pat-
terns across different tasks on NYUD-v2 [49] and SUN-
RGBD [51] dataset. First, we deﬁne the metric of any two
pixels in the predicted images. The average relative error
(REL) is used for depth images, the root mean square error
(RMSE) is used for surface normal images, and the label
consistency is for segmentation images. A pair of pixel-
s have an afﬁnity (or similar) relationship when their er-
ror is less than a speciﬁed threshold, otherwise they have a
dissimilar relationship. Next, we accumulate the matching
number of those similar pairs (or dissimilar pairs) with the
same space positions across the three types of correspond-
ing images. As shown in Fig. 1(a), the afﬁnity pairs (colored
white points) at the common positions may exist in differ-
ent tasks. Meantime, there exist some common dissimilar
pairs (colored black points) across tasks. The statistical re-
sults are shown in Fig. 1(b), where REL threshold of depth
is set to 20%, and RMSE threshold of surface normal is set
to 26% according to the performances of some state-of-the-
art works [46, 1, 29]. We can observe that the success ra-
tios of matching pairs across two tasks are rather high, and
around 50% - 60% similar pairs are matched. Moreover, we
have the same observation on the matching dissimilar pairs,
where REL threshold of depth is set to 20%, and RMSR
threshold of surface normal is set to 40%. Anyhow, this ob-
servation of the second-order afﬁnities is great important to
bridge two tasks.

Just motivated by the statistical observation, in this paper
we propose a Pattern-Afﬁnitive Propagation (PAP) frame-
work to utilize the cross-task afﬁnity patterns to jointly es-
timate depth, surface normal and semantic segmentation.
In order to encode long-distance correlations, the PAP uti-
lizes non-local similarities within each task, different from
the literatures [39, 5] only considering local neighbor re-
lationships. These pair-wise similarities are formulated as
an afﬁnity matrix to encode the pattern relationships of
the task. To spread the afﬁnity relationships, we take two
propagation stages, cross-task propagation and task-speciﬁc
propagation. The afﬁnity relationships across tasks are ﬁrst
aggregated and optimized to adapt to each speciﬁc task by
calculating on three afﬁnity matrices. We then conduct an
iterative task-speciﬁc diffusion on each task by leveraging

the optimized afﬁnity information from the corresponding
other two tasks. The diffusion process is performed in the
feature space so that the afﬁnity information of other tasks
can be widely spread into the current task. Finally, the
learning of afﬁnitive patterns and the two-stage propaga-
tions are encapsuled into an end-to-end network to boost
the prediction process of each task.

In summary, our contributions are in three aspects:

i)
Motivated by an observation that pattern-afﬁnitive pairs re-
cur much frequently across different tasks, we propose a
novel Pattern-afﬁnitive Propagation (PAP) method to utilize
the matched non-local afﬁnity information across tasks. i-
i) Two-stage afﬁnity propagations are designed to perform
cross-task and task-speciﬁc learning. An adaptive ensemble
network module is designed for the former while the strat-
egy of graph diffusion is used for the latter. iii) We make
extensive experiments to validate the effectiveness of PAP
method and its modules therein, and achieve the competi-
tive or superior performances on depth estimation, surface
normal prediction and semantic segmentation on NYUD-
v2 [49], SUN-RGBD [51], and KITTI [53] datasets.

2. Related Works

Depth Estimation: Many works have been proposed
for monocular depth estimation [10, 11, 37, 32, 42, 29, 63,
54, 47, 60, 58, 46, 62]. Recently, Xu et al. [59] employed
multi-scale continuous CRFs as a deep sequential network
for depth prediction. Fu et al. [15] tried to consider the
ordinal information in depth maps and designed a ordinal
regression loss function.

RGBD Semantic Segmentation: As the large RGBD
dataset was released, some approaches [17, 21, 48, 8, 22,
34] attempted to fuse depth information for better segmen-
tation. Recently, Qi et al. [45] designed a 3D graph neu-
ral network to fuse the depth information for segmentation.
Cheng et al. [6] computed the important locations from RG-
B images and depth maps for upsampling and pooling.

Surface Normal Estimation: Recent methods designed
for surface normal estimation are mainly based on deep neu-
ral networks [13, 14, 61, 55]. Wang et al. [56] designed a
network to incorporate local, global and vanishing point in-
formation for surface normal prediction.
In work of [1],
a skip-connected architecture was proposed to fuse features
from different layers for surface normal estimation. 3D geo-
metric information was also utilized in [46] to predict depth
and normal maps.

Afﬁnity Learning: Many afﬁnity learning methods
were designed based on physical nature of the problem-
s [19, 28, 30]. Liu et al.
[38] improve the modeling of
pair-wise relationships by incorporating many priors into d-
iffusion process. Recently, work of [2] proposed an convo-
lutional random walk approach to learn the image afﬁnity
by supervision. Wang et al. [57] proposed a non-local neu-

24107

Figure 2. The overview of our Pattern-Afﬁnitive Propagation network for jointly predicting depth, surface normal and semantic segmen-
tation. The initial predictions are produced from each task-speciﬁc network. During cross-task propagation, the network ﬁrstly learns an
afﬁnity matrix by afﬁnity learning layer to represent the pair-wise relationships of each task, then adaptively combines these matrices to
propagate the cross-task afﬁnitive patterns. Note that, the combined afﬁnity matrices is different for each task. Then we use the combined
matrix to conduct task-speciﬁc propagation by a diffusion layer, propagating the afﬁnitive patterns back to the features for each task. Finally
the diffused features are applied to three reconstruction networks to produce the ﬁnal results with higher resolution.

ral network to mine the relationships with long distances.
Some other works [39, 5, 23] tried to learn local pixel-wise
afﬁnity for semantic segmentation or depth completion. Our
method is different from these approaches in the following
aspects: needs no prior knowledge and is data-driven; need-
s no task-speciﬁc supervisons; learns the non-local afﬁni-
ty rather than limited local pair-wise relationships; learns
the cross-task afﬁnity information rather than learning the
single-task afﬁnity for task-level interaction.

matrices for one speciﬁc task, which will be introduced in
the following section.

4. Pattern-Afﬁnitive Propagation

In this section, we introduce the proposed Pattern-
Afﬁnitive Propagation (PAP) method. We efﬁciently imple-
ment the PAP method into a deep neural network through
designing a series of network modules. The details are in-
troduced in the following.

3. Non-Local Afﬁnities

4.1. The Network Architecture

Our aim is to model the afﬁnitive patterns among tasks,
and utilize such complementary information to boost and
regularize the prediction process of each task. According to
our analysis aforementioned, we want to learn the pair-wise
similarities and then propagate the afﬁnity information into
each task. Instead of learning local afﬁnities as literature
[39, 5], we attempt to utilize non-local afﬁnities, which also
recur frequently as illustrated in Fig. 1. Formally, suppose
xi, xj are the feature vectors of the i-th and j-th positions,
we can deﬁne their similarity s(xi, xj) through some func-
tions such as L1 distance kxi − xjk, inner product xT
xj ,
i
and so on. We employ the exponential function (es(·,·) or
e−s(·,·)) to make the similarities non-negative and larger for
those similar pairs than dissimilar pairs. To reduce the in-
ﬂuence of scale, we normalize the similarity matrix M into
Mik, where M is the matrix of pair-wise simi-
larities across all pixel positions.
In these ways, the ma-
trix M is symmetric, has non-negative elements and ﬁnite
Frobenius norm. Accordingly, for the three tasks, we can
compute their similarity matrices Mdepth, Mseg, Mnormal re-
spectively. According to the above statistic analysis, we can
propagate the afﬁnities by integrating the three similarity

Mij/Pk

We implement the proposed method into a deep network
as shown in Fig. 2, which depicts the network architecture.
The RGB image is ﬁrstly fed into a shared encoder (e.g.,
ResNet [20]) to generate hierarchical features. Then we
upsample the features of the last convolutional layer and
feed them to three task-speciﬁc networks. Note that we also
integrate multi-scale features derived from different layer-
s of encoder with each task-speciﬁc network, as shown by
the gray dots. Each task-speciﬁc network has two residu-
al blocks, and produces the initial prediction after a con-
volutional layer. Then we conduct cross-task propagations
to learn the task-level afﬁnitive patterns. Each task-speciﬁc
network ﬁrstly learns an afﬁnity matrix by the afﬁnity learn-
ing layer to capture the pair-wise similarities for each task,
and secondly adaptively combine the matrix with other two
afﬁnity matrices to integrate the task-correlative informa-
tion. Note that, the adaptively combined matrix is different
for each task. After that, we conduct task-speciﬁc propa-
gation via a diffusion layer to spread the learned afﬁnitive
patterns back to the feature space. In each diffusion process,
we diffuse both initial prediction and the last features from
each task-speciﬁc network by the combined afﬁnity matrix.

34108

Up-samplingDepth Sub-NetNormal Sub-NetSegmentation Sub-NetDiffusion Layer Diffusion Layer Diffusion Layer Adaptively CombinationShared upsamplingUp-samplingShared upsamplingUp-samplingShared upsamplingUp-sampling----------------------Shared Encoder----------------------||---Task-specific Net---||-Initial Results-||-Cross-task propagation--|Task-specific propagation|------Reconstruction Net------||------Final Results------|DepthSurface NormalSemantic segmentation|-RGB Image-||321123123123|------|Affinity MatrixAffinity Learning LayerOther pair-wise functions such as e−kXi−Xj k can also be
used, just not shown in the ﬁgure. Note that, differen-
t from non-local blocks [57], our afﬁnity matrix must sat-
isfy the symmetric and nonnegative properties to represent
the pair-wise similarities. Finally, as each row of the ma-
trix M represents the pair-wise relationships between one
position and all other positions, we conduct normalization
along each row of M to reduce the inﬂuence of scale. In
this way, the task-level patterns can be represented in each
M. Note that we add no supervision to learn M as literature
[2], because such supervision will cost extra memories and
be not easy to deﬁne for some tasks. After that, we want to
integrate the cross-task information for each task. Denote
these three tasks as T1, T2, T3, and the corresponding afﬁn-
MT3 , then we can learn weights
ity matrices as MT1
k (k = 1, 2, 3,Pn
αTi
k = 1) to adaptively combine the
matrices as:

MT2
k=1 αTi

ˆMTi = αTi

1 · MT1 + αTi

2 · MT2 + αTi

3 · MT3 .

(1)

In this way, the cross-task afﬁnitive patterns can be propa-
gated into ˆMTi . In practice, we implement afﬁnity learning
layers at decoding process on 1/16, 1/8 and 1/4 input scale
respectively, hence it actually learns non-local patch-level
relationships.

4.3. Task Speciﬁc Propagation

After obtaining the combined afﬁnity matrices, we
spread such afﬁnitive patterns into the feature space of each
task by the task-spaciﬁc propagation. Different from non-
local block [57] and local spatial propagation [39, 5], we
perform an iterative non-local diffusion process in each d-
iffusion layer to capture long-distance similarities, as illus-
trated in Fig. 3(b). The diffusion process is performed on
initial prediction as well as features from task-speciﬁc net-
work. Without loss of generality, assuming feature or initial
prediction P ∈ RH×W ×C is from task-speciﬁc network, we
ﬁrstly reshape it to h ∈ RHW ×C , and perform one step dif-
fusion by using matrix multiplication with ˆM. In this way,
the feature vector of each position is obtained by weight-
ed accumulating feature vectors of all positions using the
learned afﬁnity. Note that such one-step diffusion may not
deeply and effectively propagate the afﬁnity information to
the feature space, we perform the multi-step iterative diffu-
sion as:

ht+1 = ˆMht, t ≥ 0,

(2)

where ht means the diffused feature (or prediction) at step t.
Such diffusion process can be also expressed with a partial
differential equation (PDE):

ht+1 = ˆMht = (I − L)ht,

ht+1 − ht = − Lht,
∂tht+1 = − Lht,

(3)

44109

Figure 3. The detailed information of afﬁnity learning layer and
diffusion process, and each block describes the feature and its
shape. ⊗ represents the matrix multiplication. (a) afﬁnity learn-
ing layer. The dashed box is corresponding to the function for
computing similarities, and we only illustrate the dot-product as
an example. (b) diffusion process. ⊕ represents the weighted sum
with a parameter β. The dashed arrows are only performed when
the iteration is not ﬁnished.

Finally, the diffused features of each task are fed into
a reconstruction network to produce ﬁnal prediction with
higher resolution. We ﬁrstly use a shared and a task-speciﬁc
upsampling block to upscale the feature maps. Each up-
sampling block is built as a up-projection block [29], and
parameters in the shared upsampling block are shared for
every task to capture correlative local details. After the up-
sampling with the two blocks, the features are concatenated
and fed into a residual block to produce ﬁnal prediction-
s. The scale factor of each upsampling block is set to 2,
and the ﬁnal predictions are half of the input scale. This
means that the number of upsampling blocks depends on
the scale on which we want to learn afﬁnity matrix. In ex-
periments, we learn afﬁnity matrices on 1/16, 1/8 and 1/4
input scale, which means there are 3, 2 and 1 upsampling
stages in the reconstruction network respectively. The w-
hole network can be trained in an end-to-end manner, and
the details of the cross-task and task-speciﬁc propagations
will be introduced in the following sections.

4.2. Cross Task Propagation

In this section we elaborate how to conduct cross-task
propagation. Firstly, we learn an afﬁnity matrix by afﬁni-
ty learning layer to represent the pair-wise similarities for
each task. The detailed architecture of the afﬁnity learning
layer can be observed in Fig. 3(a). Assuming the feature
generated by the last layer of each task-speciﬁc network is
F ∈ RH×W ×2C , we ﬁrstly shrink it using a 1 × 1 convo-
lutional layer to get the feature ˜F ∈ RH×W ×C . Then ˜F
is reshaped to X ∈ RHW ×C . We utilize matrix multipli-
cation to compute pair-wise similarities of inner product,
and obtain the afﬁnity matrix M = XX⊺ ∈ RHW ×HW .

weight function(a) affinity learning layerHxWx2CHxWxCHWxCCxHWReshapeReshapeXHWxHWRow-normalizationaffinity matrix(b) diffusion processHxWxCReshapeHWxCXHWxHWHWxCReshapeHxWxCiterationcombined affinity matrix+β·  (1-β)·  where L is the Laplacian matrix. As ˆM is normalized and
has ﬁnite Frobenius norm, the stability of such PDE can be
guaranteed [39]. Assuming we totally perform t∗ steps in
each diffusion layer, in order to prevent the feature devi-
ating too much from the initial one, we use the weighted
accumulation on the initial feature (or prediction) h0 as:

hout = βht∗

+ (1 − β)h0, 0 ≤ β ≤ 1,

(4)

where hout means the ﬁnal output from a diffusion layer. In
this way, the learned afﬁnitive patterns in each ˆMTi can be
effectively propagated into each task Ti.

4.4. The Loss Function

In this section we introduce a pair-wise afﬁnity loss for
our PAP network. As PAP method is designed to learn
task-correlative pair-wise similarities, we also hope our loss
function can enhance the pair-wise constraints. Firstly we
deﬁne the prediction at position i is ˆzi, and the corre-
sponding ground truth is zi. Then we deﬁne the pair-wise
distance in prediction and corresponding ground truth as
ˆdij = |ˆzi − ˆzj| and dij = |zi − zj|. We hope the dis-
tance in prediction to be similar to ground truth, so the pair-
wise loss can be deﬁned as Lpair-wise = P∀i,j | ˆdij − dij|.

As the calculation of the pair-wise loss in each task will
have a high memory burden, so we randomly select S
pairs from each task and then compute the pair-wise loss

Lpair-wise = PS | ˆdij − dij|. As the pairs are random-

ly selected, such pair-wise loss can capture similarities of
various-distance pairs, not only the adjacent pixels in [10].
Meanwhile, we also use berHu loss [29], L1 loss and cross-
entropy loss for depth estimation, surface normal prediction
and semantic segmentation respectively, which are denoted
as LTi (Ti means the i-th task). Finally the total loss of the
joint task learning problem can be deﬁned as:

L = X

Ti

λTi (LTi + ξTi LTi

pair-wise),

(5)

where LTi
pair-wise is the pair-wise loss for the corresponding
i-th task, and λTi and ξTi are two weights for the i-th task.

5. Experiment

5.1. Dataset

NYUD-v2: The NYUD v2 dataset [49] consists of RGB-
D images of 464 indoor scenes. There are 1449 images with
semantic labels, 795 of them are used for training and the
remaining 654 images for testing. We randomly select more
images (12k, same as [29, 62] ) from the raw data of ofﬁcial
training scenes. These images have the corresponding depth
maps but no semantic labels or surface normals. We follow
the procedure in [13] and [46] to generate surface normal
ground truth. In this way, we can use more data to train our
model for jointly depth and surface normal prediction.

SUN RGBD: The SUN RGBD dataset [51] contains
10355 RGBD images with semantic labels of which 5285
for training and 5050 for testing. We use the ofﬁcial train-
ing set with depth and semantic labels to train our network,
and the ofﬁcial testing set for evaluation. There is no surface
normal ground truth on this dataset, so we perform experi-
ments on jointly predicting depth and segmentation on this
dataset.

KITTI: KITTI online benchmark [53] is a widely-used
outdoor dataset for depth estimation. There are 4k images
for training, 1k images for validating and 500 images for
testing on the online benchmark. As it has no semantic la-
bels or surface normal ground truth, we mainly transform
such information using our PAP method to demonstrate that
PAP can distilling knowledge to improve the performance.

5.2. Implementation Details and Metrics

We implement the proposed model using Pytorch [44] on
a single Nvidia P40 GPU. We build our network based on
ResNet-18 and ResNet-50, and each model is pre-trained
on the ImageNet classiﬁcation task [7]. In diffusion pro-
cess, we use a same subsampling strategy as [57] to down-
sample h in Eqn. (2), which can reduce the amount of pair-
wise computation by 1/4. We set the trade-off parameter
β to 0.05. 300 pairs are randomly selected to compute the
pair-wise loss in each task. We simply set λTi = 1
3 and
ξTi = 0.2 to balance the loss functions. Initial learning rate
is set to 10−4 for the pre-trained convolutional layers and
0.01 for the other layers. For NYUD-v2, we train the model
of 795 training images for 200 epochs and ﬁne-tune 100 e-
pochs, and train the model of 12k training images for jointly
depth/normal predicting for 30 epochs and ﬁne-tune for 10
epochs. For SUN-RGBD dataset, we train the model for 30
epochs and ﬁne-tune it for 30 epochs using a learning rate of
0.001. For KITTI, we ﬁrst train the model on NYUD-v2 for
surface normal estimation, and then freeze the surface nor-
mal branch to train depth branch on KITTI for 15 epochs,
ﬁnally we freeze the normal branch and ﬁne-tune the model
on KITTI for 20 epochs.

Similar to the previous works [29, 10, 59], we evaluate
our depth prediction results with the root mean square error
(rmse), average relative error (rel), root mean square error
in log space (rmse-log), and accuracy with threshold (δ): %

xi

, xi
exi

of exi s.t. max( exi

)=δ, δ = 1.25, 1.252, 1.253, where exi

is the predicted depth value at the pixel i, n is the number of
valid pixels and xi is the ground truth. The evaluation met-
rics for surface normal prediction [56, 1, 10] are mean of an-
gle error (mean), medians of the angle error (median), root
mean square error for normal (rmse-n %), and pixel accura-
cy as percentage of pixels with angle error below threshold
η where η ∈ [11.25◦, 22.50◦, 30◦]. For the evaluation of
semantic segmentation results, we follow the recent works
[6] [24] [35] and use the common metrics including pixel

54110

Table 1. Analyses on Joint task learning on NYU Depth V2.

Metric
Depth only
Segmentation only
Normal only
Depth&Seg jointly
Depth&Normal jointly
Segmentation&Normal jointly
Three task jointly

rmse
0.570

-
-

0.556
0.550

-

0.533

iou

-

42.8

-

44.3

-

44.5
46.2

rmse-n

-

28.7

-

28.1
28.3
26.9

Table 2. Comparisons of different network settings and baselines
on NYU Depth v2 dataset.

Method
initial prediction
+ PAP w/o cross-t prop.
+ PAP cross-t prop.
+ PAP cross-t prop. + recon-net
+ PAP cross-t prop + recon-net + pair-loss
+ cross-stich [41]
+ CSPN [5]
aff-matrix on 1/16 input scale
aff-matrix on 1/8 input scale
aff-matrix on 1/4 input scale
Inner product
L1 distance

rmse
0.582
0.574
0.558
0.550
0.543
0.550
0.548
0.543
0.533
0.530
0.543
0.540

IoU
41.3
41.8
43.1
43.8
44.2
43.5
43.8
44.2
46.2
46.5
44.2
44.0

rmse-n

29.6
29.1
28.5
28.2
27.8
28.2
28.0
27.8
26.9
26.7
27.8
27.9

accuracy (pixel-acc), mean accuracy (mean-acc) and mean
intersection over union (IoU).

5.3. Ablation Study

In this section we perform many experiments to analyse

the inﬂuence of different settings in our method.

Effectiveness of joint task learning: We ﬁrst analyse
the beneﬁt of joint predicting depth, surface normal and se-
mantic segmentation using our PAP method. The networks
are trained on NYUD v2 dataset, and we select ResNet-18
as our shared network backbone and only learn the afﬁnity
matrix on 1/8 input scale in each experiment. As illustrat-
ed in Table 1, we can see that joint-task models gets supe-
rior performances than the single task model, and further
jointly learning three tasks obtains best results. It can be
revealed that our PAP method does boost each task in the
jointly learning procedures.

Analysis on network settings: We perform many exper-
iments to analyse the effectiveness of each network mod-
ules.
In each experiment we use ResNet-18 as our net-
work backbone for equally comparing, and each model is
trained on NYUD v2 dataset for the three tasks. The re-
sult can be seen in Table 2. Note that the results of ﬁrst
ﬁve rows are computed from the model with afﬁnity matrix
learned on 1/16 input scale. We can observe that PAP, re-
construction net and pair-wise loss can all contribute to im-
prove the performance. We also compare two approaches
in the same settings, i.e., cross-stich units [41] and convo-
lutional spatial propagation layers [5] which can also fuse

Figure 4. The inﬂuence of the iterations in diffusion process. The
performance and time burden changes can be seen as a trade-off.

Figure 5. Visualization of the single-task and our cross-task afﬁn-
ity maps at the white point for each task. We can see that the pair-
wise similarities at the white point can be improved and corrected
in our PAP method.

and interact cross-task information. We ﬁnd that they ob-
tain weaker performances. It may be attributed to that: a)
cross-stich layer only combines features, but cannot repre-
sent the afﬁnitive patterns between tasks; b) they only use
limited local information. The middle three rows of the Ta-
ble 2 show the inﬂuence on which scale the afﬁnity matrix is
learned. We can ﬁnd that learning afﬁnity matrix on a larger
scale may be beneﬁcial, as the larger afﬁnity matrices can
describe the similarities between more patches. Note that
the improvements of learning matrix on 1/4 input scale are
comparatively smaller, and the reason may be that learning
good non-local pair-wise similarities becomes more difﬁ-
cult with scale increasing. Finally we show the results using
different functions to calculate the similarities. We ﬁnd that
these two functions does produce different performances,
but with little difference. Hence, we mainly use dot prod-
uct as our weight function in the following experiments for
convenience.

Inﬂuence of the iteration: Here we make experiments
to analyse the inﬂuence of the iterative steps in Eqn. (2).
The models are based on ResNet-18 and trained on NYUD
v2 dataset, and the afﬁnity matrices are learned on 1/8 input
scale. While testing, the input size is 480×640. As illustrat-
ed in Fig. 4, we can see that the performances of all tasks
are improved with more iterations, at least in such a range.
These results demonstrate that the pair-wise constraints and
regularization may be enhanced with more iterations in d-
iffusion. But the testing time will also increase with more
steps, which can be seen as a trade-off.

Visualization of the afﬁnity matrices: We show several

64111

0.520.530.540.550.5648121620depth rmseIteration42434445464748121620IoUIteration262728293048121620normalrmse %Iteration10012014016018048121620time msIterationImage Ground Truth Single-task Affinity Cross-task  Affinity in PAP Table 3. Comparisons with the state-of-the-art depth estimation
approaches on NYU Depth V2 Dataset.

Table 4. Comparisons with the state-of-the-art surface normal es-
timation approaches on NYU Depth V2 Dataset.

Method
HCRF [32]
DCNF [37]
Wang [54]
NR forest [47]
Xu [60]
PAD-Net [58]
Eigen [11]
MS-CNN [10]
MS-CRF [59]
FCRN [29]
GeoNet [46]
AdaD-S [42]
DORN [15]
TRL [62]
Ours d+s+n
Ours d+n

data
795
795
795
795
795
795
120k
120k
95k
12k
16k
100k
120k
12k
795
12k

rmse
0.821
0.824
0.745
0.744
0.593
0.582
0.877
0.641
0.586
0.573
0.569
0.506
0.509
0.501
0.530
0.497

rel

0.232
0.230
0.220
0.187
0.125
0.120
0.214
0.158
0.121
0.127
0.128
0.114
0.115
0.144
0.142
0.121

log

-
-

0.262

-
-
-

0.285
0.214

-

0.194

-
-
-

0.181
0.190
0.175

δ1

0.621
0.614
0.605

-

0.806
0.817
0.611
0.769
0.811
0.811
0.834
0.856
0.828
0.815
0.818
0.846

δ2

0.886
0.883
0.890

-

0.952
0.954
0.887
0.950
0.954
0.953
0.960
0.966
0.965
0.962
0.957
0.968

δ3

0.968
0.971
0.970

-

0.986
0.987
0.971
0.988
0.987
0.988
0.990
0.991
0.992
0.992
0.988
0.994

Method
3DP [13]
UNFOLD [14]
Discr. [61]
MS-CNN [10]
Deep3D [56]
SkipNet [1]
SURGE [55]
GeoNet [46]
Ours-VGG16

mean
36.3
35.2
33.5
23.7
26.9
19.8
20.6
19.0
18.6

median

rmse-n

11.25◦

22.50◦

19.2
17.9
23.1
15.5
14.8
12.0
12.2
11.8
11.7

-
-
-
-
-

28.2

-

26.9
25.5

16.4
40.5
27.7
39.2
42.0
47.9
47.3
48.4
48.8

36.6
54.1
49.0
62.0
61.2
70.0
68.9
71.5
72.2

30◦
48.2
58.9
58.7
71.1
68.2
77.8
76.6
79.5
79.8

Figure 7. Visualization of our predicted surface normal. (a) image;
(b) predictions of [10]; (c) predictions of [1] ; (d) predictions of
[46]; (e) our results; (f) ground truth.

Figure 6. Visualization of our predicted depth maps. (a) image;
(b) predictions of [60]; (c) our results; (d) ground truth. We can
ﬁnd that our predictions have obviously ﬁner details and closer to
ground truth.

examples of the learned afﬁnity maps in Fig. 5. Note that
the afﬁnity maps belong to the white point in each image.
We can see that the single-task afﬁnity maps often show
improper pair-wise relationships, while the cross-task afﬁn-
ity maps in our PAP method have closer relationships with
the points which have similar depth, normal direction and
semantic label. As the afﬁnity matrices is non-local and ac-
tually a dense graph, it can well represent the long-distance
similarities. Such observations demonstrate that the cross-
task complementary afﬁnity information can be learned to
reﬁne the single-task similarities in PAP method. Though
without supervision as [2], our PAP method can still learn
good afﬁnity matrices in such task-regularized unsupervised
approach.

5.4. Comparisons with state of the art methods

Depth Estimation: We mainly perform experiments on
NYUD-v2 dataset to evaluate our depth predictions. The
models are based on ResNet-50. As illustrated in Table 3,
our model trained for three tasks (ours d+s+n) obtains com-
petitive results, though only 795 images are used for train-
ing. Such results demonstrate that our PAP method can well
boost each task and beneﬁt joint task learning with limited

Figure 8. Qualitative semantic segmentation results of our method
on NYUD-v2 and SUNRGBD datasets.

training data. For the model trained for depth&normal pre-
diction (ours d+n), with more training data can be used, our
PAP method gets signiﬁcantly best performances in most
of the metrics with more training data, which well proves
the effectiveness of our approach. Qualitative results can
be observed in Fig. 6, compared with the recent work [60],
our predictions are more ﬁne-detailed and closer to ground
truth.

Surface Normal Estimation: We mainly evaluate our
surface normal predictions on NYUD-v2 dataset. As pre-
vious methods mainly build their network based on VGG-
16 [50], we also utilize the same setting in our experiments.
As illustrated in Table 4, our PAP method obtains obviously
superior performances than the previous approaches in all
metrics. Such results well demonstrate that our joint task
learning method can boost and beneﬁt the surface normal
estimation. Qualitative results can be observed in Fig. 7, we
can ﬁnd that our method can produce better or competitive
results.

RGBD Semantic Segmentation: We evaluate our seg-
mentation results on widely-used NYUD-v2 and SUN-
RGBD datasets. The model in each experiment is build

74112

(a) (b) (c) (d)(a)image(b)MS-CNN(c)SkipNet(d)GeoNet(e) Ours(f) GTImage GT Ours Image GT Ours Table 5. Comparisons the state-of-the-art semantic segmentation
methods on NYU Depth v2 dataset.

Method
FCN [40]
Context [36]
Eigen et al. [10]
B-SegNet [24]
ReﬁneNet-101 [35]
PAD-Net [58]
TRL-ResNet50 [62]
Deng et al. [8]
He et al. [22]
LSTM [34]
Cheng et al. [6]
3D-GNN [45]
RDF-50 [48]
Ours-ResNet50

data
RGB
RGB
RGB
RGB
RGB
RGB
RGB
RGBD
RGBD
RGBD
RGBD
RGBD
RGBD
RGB

pixel-acc

mean-acc

60.0
70.0
65.6
68.0
72.8
75.2
76.2
63.8
70.1

-

71.9

-

74.8
76.2

49.2
53.6
45.1
45.8
57.8
62.3
56.3

-

53.8
49.4
60.7
55.7
60.4
62.5

IoU
29.2
40.6
34.1
32.4
44.9
50.2
46.4
31.5
40.1

-

45.9
43.1
47.7
50.4

Table 6. Comparison with the state-of-the-art semantic segmenta-
tion methods on SUN-RGBD dataset.

Method
Context [36]
B-SegNet [24]
ReﬁneNet-101 [35]
TRL-ResNet50 [62]
LSTM [34]
Cheng et al. [6]
CFN [9]
3D-GNN [45]
RDF-152 [48]
Ours-ResNet50

data
RGB
RGB
RGB
RGB
RGBD
RGBD
RGBD
RGBD
RGBD
RGB

pixel-acc

mean-acc

78.4
71.2
80.4
83.6

-
-
-
-

81.5
83.8

53.4
45.9
57.8
58.9
48.1
58.0

-

57.0
60.1
58.4

IoU
42.3
30.7
45.7
50.3

-
-

48.1
45.9
47.7
50.5

Table 7. Comparison with the state-of-the-art methods on KITTI
online benchmark (lower is better).

Method
DORN [15]
VGG16-Unet∗
FUSION-ROB∗
BMMNet∗
DABC [33]
APMoE [27]
CSWS [31]
Ours single
Ours cross-stich [41]
Ours

SILog
11.77
13.41
13.90
14.37
14.49
14.74
14.85
14.58
14.33
13.08

sqErrRel

absErrRel

2.23
2.86
3.14
5.10
4.08
3.88
3.48
3.96
3.85
2.72

8.78
10.60
11.04
10.92
12.72
11.74
11.84
11.50
11.23
10.27

iRMSE
12.98
15.06
15.69
15.51
15.53
15.63
16.38
15.24
15.14
13.95

time
0.5s
0.16s

2s
0.1s
0.7s
0.2s
0.2s
0.1s
0.1s
0.2s

based on ResNet-50 and trained for the three tasks on
NYUD-v2, and jointly depth prediction and semantic seg-
mentation on SUN-RGBD. The performance on NYUD-
v2 dataset is shown in Table 5. We can observe that the
performances of our PAP method are superior or competi-
tive, though using only RGB images as input. Such results
can demonstrate that although depth ground truth is not di-
rectly use, our method can beneﬁt the segmentation from
jointly learning depth information. The performances on
SUN-RGBD dataset are illustrated in Table 6, we can see
that though slightly weaker than RDF-152 [48] in mean-acc
metric, our method can obtain best results in other metric-
s. Such results reveal that our predictions are superior or
at least competitive with state-of-the-art methods. Visual-
ized results can be observed in Fig. 8, we can see that our
predictions are with high quality and close to ground truth.

Figure 9. Qualitative results of our method on KITTI dataset. We
can ﬁnd that our model obtains good depth predictions and normal
estimations.
5.5. Effectiveness On Distilling

Sometimes the ground truth data cannot be always avail-
able for each task, e.g., some widely-used outdoor depth
datasets, such as KITTI [53], has no or very limited sur-
face normal and segmentation ground truth. However, we
can use PAP method to distill the knowledge from other
dataset to boost the target task. We train our model on
NYUD-v2 for depth and normal estimation, and then freeze
the normal branch to train the model on KITTI. We evalu-
ate our predictions on the KITTI online evaluation server,
and the results are shown in Table 7 (∗ means anonymous
method). Our PAP method outperforms our single-task and
cross-stich based model. Compared with the state-of-the-
art methods, though slightly weaker than DORN [15], our
method obtains superior performances than all other pub-
lished or unpublished approaches. Note that our method
runs faster than DORN, which can be seen as a trade-off.
These results demonstrate the effectiveness and potential of
PAP method on task distilling and transferring. Qualitative
results can be seen on Fig. 9, and our predictions on depth
and normal are both with high quality.

6. Conclusion

In this paper, we propose a novel Pattern-afﬁnitive Prop-
agation method for jointly predicting depth, surface normal
and semantic segmentation. Statistic results have shown
that the afﬁnitive patterns among tasks can be modeled in
pair-wise similarities to some extent. The PAP can ef-
fectively learn the pair-wise relationships from each task,
and further utilize such cross-task complementary afﬁnity
to boost and regularize the joint task learning procedure vi-
a the cross-task and task-speciﬁc propagation. Extensive
experiments demonstrate our PAP method obtained state-
of-the-art or competitive results on these three tasks.In the
future, we may generalize and improve the efﬁciency of the
method on more vision tasks.

7. Acknowledgement

This work was supported by the National Science
Fund of China under Grant Nos. U1713208, 61806094,
61772276 and 61602244, Program for Changjiang Schol-
ars, and 111 Program AH92005.

84113

Image Our Depth Our Normal References

[1] Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr
revisited: 2d-3d alignment via surface normal prediction. In
CVPR, pages 5965–5974, 2016.

[2] Gedas Bertasius, Lorenzo Torresani, X Yu Stella, and Jian-
bo Shi. Convolutional random walk networks for semantic
image segmentation. In CVPR, pages 6137–6145, 2017.

[3] Rich Caruana. Multitask learning. Machine Learning,

28(1):41–75, 1997.

[4] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong X-
iao. Deepdriving: Learning affordance for direct perception
in autonomous driving. In ICCV, pages 2722–2730, 2015.

[5] Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth esti-
mation via afﬁnity learned with convolutional spatial propa-
gation network. In ECCV, pages 108–125, 2018.

[6] Yanhua Cheng, Rui Cai, Zhiwei Li, Xin Zhao, and Kaiqi
Huang. Locality-sensitive deconvolution networks with gat-
ed fusion for rgb-d indoor semantic segmentation. In CVPR,
volume 3, pages 1475–1483, 2017.

[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248–255, 2009.

[8] Zhuo Deng, Sinisa Todorovic, and Longin Jan Latecki. Se-
mantic segmentation of rgbd images with mutex constraints.
In ICCV, pages 1733–1741, 2015.

[9] L Di, Chen Guangyong, Cohen-Or Daniel, Heng Pheng-
Ann, and Huang Hui. Cascaded feature network for semantic
segmentation of rgb-d images. In ICCV, pages 1320–1328,
2017.

[10] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale convo-
lutional architecture. In ICCV, pages 2650–2658, 2015.

[11] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In NIPS, pages 2366–2374, 2014.

[12] Terrence Fong, Illah Nourbakhsh, and Kerstin Dautenhahn.
A survey of socially interactive robots. Robotics and au-
tonomous systems, 42(3-4):143–166, 2003.

[13] David F Fouhey, Abhinav Gupta, and Martial Hebert. Data-
driven 3d primitives for single image understanding. In IC-
CV, pages 3392–3399, 2013.

[14] David Ford Fouhey, Abhinav Gupta, and Martial Hebert. Un-
folding an indoor origami world. In ECCV, pages 687–702,
2014.

[15] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In CVPR, pages 2002–
2011, 2018.

[16] Ross Girshick. Fast R-CNN.

In ICCV, pages 1440–1448,

2015.

[17] Saurabh Gupta, Ross Girshick, Pablo Arbelez, and Jitendra
Malik. Learning rich features from rgb-d images for object
detection and segmentation. In ECCV, volume 8695, pages
345–360, 2014.

[18] Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Gir-

shick. Mask R-CNN. ICCV.

[19] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image ﬁl-
tering. IEEE transactions on pattern analysis and machine
intelligence, (6):1397–1409, 2013.

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[21] Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz,
and SI Campus. Std2p: Rgbd semantic segmentation using
spatio-temporal data-driven pooling. In CVPR, pages 7158–
7167, 2017.

[22] Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz,
and SI Campus. Std2p: Rgbd semantic segmentation us-
ing spatio-temporal data-driven pooling. pages 7158–7167,
2017.

[23] Tsung-Wei Ke, Jyh-Jing Hwang, Ziwei Liu, and Stella X.
Yu. Adaptive afﬁnity ﬁelds for semantic segmentation. In
ECCV, 2018.

[24] Alex Kendall, Vijay Badrinarayanan, , and Roberto Cipolla.
Bayesian segnet: Model uncertainty in deep convolutional
encoder-decoder architectures for scene understanding. arX-
iv preprint arXiv:1511.02680, 2015.

[25] Seungryong Kim, Kihong Park, Kwanghoon Sohn, and
Stephen Lin. Uniﬁed depth prediction and intrinsic image
decomposition from a single image via joint convolutional
neural ﬁelds. In ECCV, pages 143–159, 2016.

[26] Iasonas Kokkinos. Ubernet: Training a ‘universal’ convolu-
tional neural network for low-, mid-, and high-level vision
using diverse datasets and limited memory. In CVPR, pages
5454–5463, 2017.

[27] Shu Kong and Charless Fowlkes. Pixel-wise attentional gat-
ing for parsimonious pixel labeling. arXiv preprint arX-
iv:1805.01556, 2018.

[28] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
in fully connected crfs with gaussian edge potentials. In NIP-
S, pages 109–117, 2011.

[29] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In International
Conference on 3D Vison, pages 239–248, 2016.

[30] Anat Levin, Dani Lischinski, and Yair Weiss. A closed-form
solution to natural image matting.
IEEE transactions on
pattern analysis and machine intelligence, 30(2):228–242,
2008.

[31] Bo Li, Yuchao Dai, and Mingyi He. Monocular depth es-
timation with hierarchical fusion of dilated cnns and soft-
weighted-sum inference. Pattern Recognition, 2018.

[32] Bo Li, Chunhua Shen, Yuchao Dai, Anton van den Hengel,
and Mingyi He. Depth and surface normal estimation from
monocular images using regression on deep features and hi-
erarchical crfs. In CVPR, pages 1119–1127, 2015.

[33] Ruibo Li, Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu,
and Lingxiao Hang. Deep attention-based classiﬁcation
network for robust depth prediction. arXiv preprint arX-
iv:1807.03959, 2018.

[34] Zhen Li, Yukang Gan, Xiaodan Liang, Yizhou Yu, Hui
Cheng, and Liang Lin. Lstm-cf: Unifying context model-
ing and fusion with lstms for rgb-d scene labeling. In ECCV,
pages 541–557, 2016.

94114

[51] S. Song, S. P. Lichtenberg, and J. Xiao. Sun RGB-D: A
In CVPR,

RGB-D scene understanding benchmark suite.
pages 567–576, 2015.

[52] Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir
Navab. Cnn-slam: Real-time dense monocular slam with
learned depth prediction. In CVPR, volume 2, 2017.

[53] Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke,
Thomas Brox, and Andreas Geiger. Sparsity invariant c-
nns. In International Conference on 3D Vision, pages 11–20,
2017.

[54] Peng Wang, Xiaohui Shen, Zhe Lin, and Scott Cohen. To-
wards uniﬁed depth and semantic prediction from a single
image. In CVPR, pages 2800–2809, 2015.

[55] Peng Wang, Xiaohui Shen, Bryan Russell, Scott Cohen, Bri-
an Price, and Alan L Yuille. Surge: Surface regularized ge-
ometry estimation from a single image. In NIPS, pages 172–
180, 2016.

[56] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-
ing deep networks for surface normal estimation. In CVPR,
pages 539–547, 2015.

[57] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR, volume 10,
2018.

[58] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.
Pad-net: Multi-tasks guided prediction-and-distillation net-
work for simultaneous depth estimation and scene parsing.
In CVPR, 2018.

[59] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and
Nicu Sebe. Multi-scale continuous CRFs as sequential deep
networks for monocular depth estimation.
In CVPR, vol-
ume 1, pages 161–169, 2017.

[60] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and
Elisa Ricci. Structured attention guided convolutional neu-
ral ﬁelds for monocular depth estimation. In CVPR, pages
3917–3925, 2018.

[61] Bernhard Zeisl, Marc Pollefeys, et al. Discriminatively
In ECCV, pages

trained dense surface normal estimation.
468–484, 2014.

[62] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang
Li, and Jian Yang. Joint task-recursive learning for semantic
segmentation and depth estimation.
In ECCV, pages 235–
251, 2018.

[63] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, volume 2, page 7, 2017.

[35] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Rei-
d. Reﬁnenet: Multi-path reﬁnement networks for high-
resolution semantic segmentation. In CVPR, volume 1, pages
5168–5177, 2017.

[36] Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and
Ian Reid. Efﬁcient piecewise training of deep structured
models for semantic segmentation. In CVPR, pages 3194–
3203, 2016.

[37] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Rei-
d. Learning depth from single monocular images using
deep convolutional neural ﬁelds. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 38(10):2024–2039,
2016.

[38] Risheng Liu, Guangyu Zhong, Junjie Cao, Zhouchen Lin,
Shiguang Shan, and Zhongxuan Luo. Learning to diffuse:
A new perspective to design pdes for visual analysis. IEEE
transactions on pattern analysis and machine intelligence,
38(12):2457–2471, 2016.

[39] Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong,
Ming-Hsuan Yang, and Jan Kautz. Learning afﬁnity via spa-
tial propagation networks. In NIPS, pages 1520–1530, 2017.
[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Ful-
ly convolutional networks for semantic segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
39(4):640–651, 2017.

[41] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-
tial Hebert. Cross-stitch networks for multi-task learning. In
CVPR, pages 3994–4003, 2016.

[42] Jogendra Nath Kundu, Phani Krishna Uppala, Anuj Pahuja,
and R. Venkatesh Babu. Adadepth: Unsupervised content
congruent adaptation for depth estimation. In CVPR, 2018.

[43] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In ICCV, pages 1520–1528, 2015.

[44] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[45] Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel
Urtasun. 3d graph neural networks for rgbd semantic seg-
mentation. In CVPR, pages 5199–5208, 2017.

[46] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun,
and Jiaya Jia. Geonet: Geometric neural network for joint
depth and surface normal estimation. In CVPR, pages 283–
291, 2018.

[47] Anirban Roy and Sinisa Todorovic. Monocular depth esti-
mation using neural regression forest. In CVPR, pages 5506–
5514, 2016.

[48] Park Seong-Jin, Hong Ki-Sang, and Lee Seungyong. Rdfnet:
Rgb-d multi-level residual feature fusion for indoor semantic
segmentation. In ICCV, pages 4990–4999, 2017.

[49] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Indoor segmentation and support inference from

Fergus.
RGBD images. In ECCV, pages 746–760, 2012.

[50] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

104115

