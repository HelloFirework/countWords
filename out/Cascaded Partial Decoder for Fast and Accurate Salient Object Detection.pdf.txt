Cascaded Partial Decoder for Fast and Accurate Salient Object Detection

Zhe Wu1,2, Li Su1,2,3, and Qingming Huang1,2,3

1School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS),

2Key Lab of Big Data Mining and Knowledge Management, UCAS, Beijing, China

3Key Lab of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy

Beijing, China

of Sciences, Beijing, China

zhe.wu@vipl.ict.ac.cn, {suli, qmhuang}@usas.ac.cn

Abstract

Existing state-of-the-art salient object detection net-
works rely on aggregating multi-level
features of pre-
trained convolutional neural networks (CNNs). Compared
to high-level features, low-level features contribute less to
performance but cost more computations because of their
larger spatial resolutions. In this paper, we propose a novel
Cascaded Partial Decoder (CPD) framework for fast and
accurate salient object detection. On the one hand, the
framework constructs partial decoder which discards larg-
er resolution features of shallower layers for acceleration.
On the other hand, we observe that integrating features of
deeper layers obtain relatively precise saliency map. There-
fore we directly utilize generated saliency map to reﬁne
the features of backbone network. This strategy efﬁciently
suppresses distractors in the features and signiﬁcantly im-
proves their representation ability. Experiments conducted
on ﬁve benchmark datasets exhibit that the proposed model
not only achieves state-of-the-art performance but also run-
s much faster than existing models. Besides, the proposed
framework is further applied to improve existing multi-level
feature aggregation models and signiﬁcantly improve their
efﬁciency and accuracy.

1. Introduction

Recently, deep learning has achieved surprising perfor-
mance in salient object detection for it providing abun-
dant and discriminative image representations. The early
deep saliency methods [15, 16, 32] utilize CNNs to predict
saliency scores of image regions and obtain accurate salien-
cy maps with high computational complexity. In the follow-
ing works, fully convolutional network (FCN) [24] based

(cid:19)(cid:17)(cid:26)(cid:27)

(cid:41)
(cid:91)
(cid:68)
(cid:80)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:19)(cid:17)(cid:26)(cid:21)

(cid:25)

(cid:24)

(cid:23)

(cid:22)

(cid:54)(cid:76)(cid:71)(cid:72)(cid:3)(cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)

(cid:11)(cid:68)(cid:12)

(cid:22)(cid:17)(cid:23)
(cid:72)
(cid:80)
(cid:55)

(cid:76)

(cid:21)(cid:17)(cid:21)

(cid:21)

(cid:20)

(cid:20)

(cid:25)

(cid:24)

(cid:23)

(cid:22)

(cid:21)

(cid:20)

(cid:54)(cid:76)(cid:71)(cid:72)(cid:3)(cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)

(cid:11)(cid:69)(cid:12)

Figure 1: (a) Maximum F-measure of six side outputs of the origi-
nal DSS [9] model in PASCAL-S [19] dataset. (b) We set inference
time of backbone network as 1, and show inference time of each
side output here. The performance growth is getting slower and
the inference time rapidly increases when gradually integrating
features from high-level 6 to low-level 1.

encoder-decoder architecture is widely applied for salien-
t object detection. The encoder is the pre-trained image
classiﬁcation model (e.g. VGG [29] and ResNet [8]) which
provides multi-level deep features: the high-level features
with low resolutions represent semantic information, and
the low-level features with high resolutions represent spa-
tial details. In the decoder, these features are combined to
generate accurate saliency maps. Researchers have devel-
oped various decoders [9, 17, 20, 21, 25, 41, 42] to integrate
low-level and high-level features.

However, two drawbacks exist in these deep aggregation
methods. On the one hand, compared to high-level fea-
tures, low-level features contribute less to the performance
of deep aggregation methods. In Fig. 1(a), we present per-
formances of different side outputs of the DSS [9] model.
It is obvious that the performance tends to saturate quick-
ly when gradually aggregating features from high-level to
low-level. On the other hand, due to the large resolutions

3907

Image

Conv1_2

Conv2_2

Conv3_3

Conv4_3

Conv5_3

Figure 2: The original image and ﬁve-level feature maps
from VGG16 [29]. The Conv3 3 feature still retains edge
information. Hence the Conv1 2 and Conv2 2 features with
large resolutions are not under consideration in the pro-
posed framework.

of low-level features, integrating them with high-level fea-
tures obviously enlarges the computational complexity as
shown in Fig. 1(b). However, detecting and segmenting
salient objects should be fast since this process is often a
preprocessing stage to more complex operations [3]. In con-
sequence, it is essential to design a mechanism to eliminate
the impact of low-level features on computational complex-
ity while ensuring the performance.

When CNNs go deep, feature gradually changes from
low-level representation to high-level representation. Hence
deep aggregation models may recover spatial details of
saliency maps when only integrating features of deeper lay-
ers.
In Fig. 2, we show examples of multi-level feature
maps of VGG16 [29]. Compared to low-level features of
Conv1 2 and Conv2 2 layers, the feature of Conv3 3 layer
also reserve edge information. Besides, background regions
in feature maps may result in inaccuracy of saliency maps.
Previous works make use of adaptive attention mechanis-
m [21, 41] to solve this problem. However, the effect of this
mechanism relies on the accuracy of the attention map. S-
ince fusing features of deeper layers will generate relatively
precise saliency map, we can directly use this map to reﬁne
features.

In this paper, we propose a novel cascaded partial de-
coder framework, which discards features of shallower lay-
ers to ensure high computational efﬁciency and then reﬁne
features of deeper layers to improve their representation a-
bility. We modify the standard straight backbone network
to a bifurcated one. This new backbone network contains
two branches with the same architecture. We construct par-
tial decoder which only aggregates features in each branch.
In order to further accelerate the model, we design a fast
and efﬁcient context module to abstract discriminative fea-
tures and integrate them in an upsampling-concatenating
way. Then we propose a cascaded optimization mechanis-
m which utilizes initial saliency map of the ﬁrst branch to

reﬁne features of the second branch. In order to uniformly
segment the whole salient objects, we propose a holistic at-
tention module to allow the initial saliency map cover more
useful information.
In addition, the proposed framework
can be utilized to improve existing deep aggregation mod-
els. When embedding their decoders in our framework, the
accuracy and efﬁciency will be signiﬁcantly improved. Our
contributions are summarized as follows:

(1) We propose a novel cascaded partial decoder frame-
work, which discards low-level features to reduce the
complexity of deep aggregation models, and utilizes
generated relatively precise attention map to reﬁne
high-level features to improve the performance.

(2) Experimental

results on ﬁve benchmark datasets
demonstrate that the proposed model not only achieves
state-of-the-art performance but also runs much faster
than existing models.

(3) Our framework can be applied to improve existing deep
aggregation models. The efﬁciency and accuracy of im-
proved models will both be signiﬁcantly improved com-
pared to the original models.

2. Related Work

Over the past two decades, researchers have developed a
large amount of saliency detection algorithms. Traditional
models extract hand-crafted features and are based on vari-
ous saliency assumptions [2, 6, 11, 46]. More details about
traditional methods are concluded in [3, 4]. Here we mainly
discuss deep learning based saliency detection models.

Early works utilize CNNs to determine whether image
regions are salient or not [15, 16, 32, 44]. Although these
models have achieved much better performance than tra-
ditional methods, it is time-consuming to predict saliency
scores for image regions. Then researchers develop more
effective models based on the successful fully convolution-
al network [24]. Li et al. [18] set up a uniﬁed framework
for salient object detection and semantic segmentation to
effectively learn the semantic properties of salient object-
s. Wang et al. [34] leverage cascaded fully convolutional
networks to continuously reﬁne previous prediction maps.

Recently, researchers have proved that fusing multi-level
features further improves the performance of dense predic-
tion tasks [7, 27].
In CNNs, high-level features provide
semantic information, and low-level features contains spa-
tial details which are helpful for reﬁning object boundaries.
Many works [9, 17, 20, 21, 25, 41, 42] follow this strategy
and precisely segment salient objects. Li et al. [17] directly
integrate multi-level features to obtain more advanced fea-
ture representation. Liu and Han [20] ﬁrst make a coarse
global prediction, and then hierarchically and progressive-
ly reﬁne the details of saliency maps step by step via in-

3908

(cid:11)(cid:68)(cid:12)

(cid:44)(cid:49)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:41)(cid:88)(cid:79)(cid:79)(cid:3)
(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:54)

(cid:11)(cid:69)(cid:12)

(cid:44)(cid:49)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:3) (cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:37)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)

(cid:54)(cid:36)(cid:48)

(cid:43)(cid:82)(cid:79)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)

(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

(cid:54)(cid:78)(cid:76)(cid:83)(cid:3)(cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:40)(cid:79)(cid:72)(cid:80)(cid:72)(cid:81)(cid:87)(cid:90)(cid:76)(cid:86)(cid:72)(cid:3)
(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:54)(cid:75)

(cid:43)(cid:36)(cid:48)

(cid:54)(cid:76)

(cid:51)(cid:68)(cid:85)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)
(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:38)(cid:82)(cid:81)(cid:89)

(cid:51)(cid:68)(cid:85)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)
(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)

(cid:54)(cid:76)

(cid:54)(cid:71)

Figure 3: (a) Traditional encoder-decoder framework, (b) The proposed cascaded partial decoder framework. We use
VGG16 [29] as the backbone network. Traditional framework generates saliency map S by adopting full decoder which
integrates all level features. The proposed framework adopts partial decoder, which only integrates features of deeper layers,
and generates an initial saliency map Si and the ﬁnal saliency map Sd.

tegrating local context information. Hou et al. [9] intro-
duce short connections to the skip-layer structures within
the HED [38] architecture. Luo et al. [25] segment salient
objects by combining local contrast feature and global in-
formation through a multi-resolution 4 × 5 grid network.
Zhang et al. [42] ﬁrst integrate multi-level feature maps in-
to multiple resolutions, which simultaneously incorporate
semantic information and spatial details. Then this work
predicts the saliency map in each resolution and fuses them
to generate the ﬁnal saliency map. In [41], the work extract-
s context-aware multi-level features and then utilizes a bi-
directional gated structure to pass messages between them.
Liu et al. [21] leverage global and local pixel-wise contex-
tual attention network to capture global and local context in-
formation. Then these modules are incorporated with U-Net
architecture to segment salient objects. In this paper, we ar-
gue that low-level features always contribute less than high-
level features. However, they need more computation cost
than high-level features owing to their larger spatial resolu-
tions. Hence we propose a novel cascaded partial decoder
framework for salient object detection, which does not con-
sider low-level features and utilizes generated saliency map
to reﬁne high-level features.

3. The Proposed Framework

In this paper, we propose a novel cascaded partial de-
coder framework which contains two branches.
In each
branch, we design a fast and effective partial decoder. The
ﬁrst branch generates an initial saliency map which is uti-
lized to reﬁne the features of the second branch. Besides,
we propose a holistic attention module to segment the w-
hole objects uniformly.

3.1. Mechanism of the Proposed Framework

We design the proposed model on the basis of VGG16
network, which is the most widely utilized backbone net-
work in deep salient object detection models. For an in-
put image I with size H × W , we can abstract features at
ﬁve levels, which are denoted as {fi, i = 1, ..., 5} with res-

2i−1 , W

olutions [ H
2i−1 ]. The decoders proposed in previous
works [41, 42], which are called full decoder in this paper,
integrate all features to generate saliency map S. A uni-
ﬁed architecture of the full decoder is shown in Fig. 3(a)
and it can be represented by DT = g(f1, f2, f3, f4, f5),
where g(·) denotes a multi-level feature aggregation algo-
rithm. Previous works focus on how to develop a more ef-
fective integration strategy.

3 , f a

In Fig. 3(b), we show the architecture of the proposed
cascaded partial decoder framework. Since that the features
of shallower layers contribute less to performance, we con-
struct a partial decoder that only integrates features of deep-
er layers. In order to utilize generated saliency map to reﬁne
features, we design a bifurcated backbone network. We set
the Conv3 3 layer as an optimization layer, and use the last
two convolutional blocks to construct two branches (an at-
tention one and a detection one). In the attention branch,
we design a partial decoder to integrate three-level features
which are denoted as {f a
i = fi, i = 3, 4, 5}. Hence the par-
tial decoder is represented by Da = ga(f a
5 ) and it
generates an initial saliency map Si. After processing of the
proposed holistic attention module, we obtain an enhanced
attention map Sh which is utilized to reﬁne the feature f3.
Because we can obtain relatively precise saliency map via
integrating features of three top layers, the attention map
Sh effectively eliminates distractors in feature f3. Then
we obtain the reﬁned feature f d
3 for detection branch via
element-wise multiplying the feature and the attention map:
f d
3 = f3 ⊙ Sh. Hence the following two-level features of
the detection branch are denoted as {f d
5 }. Through con-
structing another partial decoder Dd = gd(f d
5 ) for
the detection branch, the proposed model outputs the ﬁnal
saliency map Sd. For convenience, we set ga = gd in this
paper. The details of the proposed holistic attention mod-
ule and the partial decoder are described in Section 3.2 and
Section 3.3 respectively.

4 , f a

4 , f d

3 , f d

4 , f d

We jointly train the two branches with ground truth.
The parameters of the two branches are not shared. Given
{Si, Sd} and the corresponding label l, the total loss Ltotal

3909

is formulated as:

Ltotal = Lce(Si, l|Θi) + Lce(Sd, l|Θd).

(1)

Lce is the sigmoid cross entropy loss:

Lce(Θ) = −

N

(cid:2)

(cid:2)

j=1

c∈{0,1}

δ(lj = c) log p(Sj = c|Θ),

(2)

where N is the pixel number, δ is the indicator function, j
denotes pixel coordinate and Θ = {Θi, Θd} are parameter
sets corresponding to the saliency maps S = {Si, Sd}. It
is obvious that Θi is a proper subset of Θd, which indicates
that the two branches work in an alternating way. On the
one hand, the attention branch provides precise attention
map for the detection branch, which leads to that the de-
tection branch segments more accurate salient objects. On
the other hand, the detection branch could be considered as
an auxiliary loss of the attention branch, which also helps
the attention branch to focus on salient objects. Joint train-
ing the two branches makes our model uniformly highlights
salient objects while suppressing distractors.

In addition, we can leverage the proposed framework to
improve existing deep aggregation models when we inte-
grate the features of each branch by using the aggregation
algorithms of these works. Even though we raise the com-
putation cost of the backbone network and add one more
decoder when compared to the traditional encoder-decoder
architecture, the total computation complexity is still signif-
icantly reduced because of discarding low-level features in
decoders. Moreover, the cascaded optimization mechanism
of the proposed framework promotes the performance, and
the experiments show that the two branches both outperfor-
m the original models.

3.2. Holistic Attention Module

Given the feature map from the optimization layer and
the initial saliency map from attention branch, we can use
a initial attention strategy which means directly multiply-
ing the feature map with the initial saliency map. When we
obtain an accurate saliency map from the attention branch,
this strategy will efﬁciently suppress distractors of the fea-
ture. On the contrary, if distractors are classiﬁed as salient
regions, this strategy results in abnormal segmentation re-
sults. As a result, we need to improve the effectiveness of
the initial saliency map. More specially, the edge infor-
mation of salient objects may be ﬁltered out by the initial
saliency map because it is difﬁcult to be precisely predict-
ed. In addition, some objects in complex scenes are hard to
be completely segmented. Therefore we propose a holistic
attention module which aims to enlarge the coverage area
of the initial saliency map, and it is deﬁned as follows:

Sh = M AX(fmin max(Convg(Si, k)), Si)

(3)

Image

GT

Initial Attention Holistic Attention

Figure 4: GT is the ground truth. As we can see, the pro-
posed holistic attention module is helpful for segmenting the
whole salient objects and reﬁning more precise boundaries.

where Convg is a convolution operation with a Gaussian
kernel k and zero bias, fmin max(·) is a normalization func-
tion to make the blurred map ranges in [0, 1], and M AX(·)
is a maximum function which tends to increase the weight
coefﬁcient of salient regions of Si because that the convo-
lution operation will blur Si. Compared to the initial at-
tention, the proposed holistic attention mechanism hardly
increases computation cost, and it further highlights the w-
hole salient objects as shown in Fig. 4. Moreover, the size
and standard deviation of Gaussian kernel k are initialized
with 32 and 4. Then it is jointly trained with the proposed
model.

3.3. The Proposed Decoder

Since that the proposed framework consists of two de-
coders, we need to construct a fast integration strategy to
ensure low complexity. Meanwhile, we need to generate
saliency map as accurate as possible. Firstly, in order to cap-
ture global contrast information, we design an effective con-
text module which is inspired by the receptive ﬁeld block
(RFB) [22]. Compared to the original RFB, we add one
more branch to enlarge the receptive ﬁeld further. Our con-
text module consists of four branches {bm, m = 1, ..., 4}.
For acceleration, in each branch, we use a 1×1 convolution-
al layer to reduce channel number to 32. For {bm, m > 1},
we add two layers: a (2m−1)×(2m−1) convolutional lay-
er and a 3×3 convolutional layer with (2m−1) dilation [5].
We concatenate the outputs of these branches and reduce
the channel to 32 by an additional 1 × 1 convolutional lay-
er. Then a short connection is added as the original RFB. In
general, given features {f c
i , i ∈ [l, ..., L], c ∈ [a, d]} from
the bifurcated backbone network, we obtain discriminative
features {f c1
i } from the context module. Then we use mul-
tiplication operation to reduce the gap between multi-level
features. Especially, for the top-most feature (i = L), we
set f c2
, i < L}, we update it to

L . For feature {f c1

L = f c1

i

3910

f c2
i via element-wise multiplying itself with all features of
deeper layers. This operation is deﬁned as follows:

f c2
i = f c1

i ⊙ΠL

k=i+1Conv(U p(f c1

k )), i ∈ [l, ..., L−1], (4)

where U p(·) is upsampling feature by a factor 2k−j , and
Conv is a 3 × 3 convolutional layer. At last, we utilize an
upsampling-concatenating strategy to integrate multi-level
features. When we construct a partial decoder and set the
Conv3 3 layer as the optimization layer (l = 3, L = 5),
we obtain a feature map with [ H
4 ] size and 96 channel
number. With 3 × 3 layer and 1 × 1 convolutional layers,
we obtain the ﬁnal feature map and resize it to [H, W ].

4 , W

4. Experiment

4.1. Salient Object Detection

4.1.1 Experimental Setup

Evaluation Datasets. We evaluate the proposed model
on ﬁve benchmark datasets: ECSSD [39], HKU-IS [16],
PASCAL-S [19], DUTS [33], DUT-OMRON [40].
Evaluation Metrics. We adopt two metrics: mean abso-
lute error (MAE) and F-measure (maxF). We adopt mean
absolute error (MAE) and F-measure as our evaluation met-
rics. According to the different ways for saliency map bi-
narization, there exist two ways to compute F-measure [4].
One is maximum F-measure (denoted as maxF), which is
adopted in [9, 21, 25, 41]. The other is average F-measure
(denoted as avgF), which is adopted in [35, 36, 42, 43]. For
fairly comparison, we compute both maxF and avgF.
Implementation Details. We implement the proposed
model based on the Pytorch1 framework and a GTX 1080Ti
GPU is used for acceleration. Following previous work-
s [21, 35, 36, 41, 43], we train the proposed model on the
training set of DUTS [33] dataset. The parameters of the b-
ifurcated backbone network are initialized by VGG16 [29].
We initialize the other convolutional layers using the default
setting of the Pytorch. All training and test images are re-
sized to 352 × 352. Any post-processing procedure (e.g.
CRF [14]) is not applied in this paper. The proposed model
is trained by Adam optimizer [13]. The batch size is set as
10 and the initial learning rate is set as 10−4 and decreased
by 10% when training loss reaches a ﬂat. It takes nearly six
hours for training the proposed model. The code is available
at https://github.com/wuzhe71/CPD.

4.1.2 Comparison with State-of-the-arts

We compare the proposed model with eight state-of-the-art
deep salient object detection algorithms, including NLD-
F [25], Amulet [42], DSS [9], SRM [35], BMPM [41], PA-
GR [43], DGRL [36] and PiCANet [21]. We implement

1https://pytorch.org/

these models with available source codes or directly evalu-
ate saliency maps provided by authors. Especially, NLDF,
Amulet and DSS are originally trained on MSRA10K [6]
dataset or MSRA-B [23] dataset (there is a large overlap
between these two datasets). Hence we re-train these three
models on DUTS dataset as other models for fairly com-
parison. We ﬁnd that training on DUTS dataset will make
deep models work better in complex scenes. Besides, we
also train the proposed model on MSRA-B dataset to com-
pare with these three original models, and the results are
reported in supplementary material.

In Table. 1, we show the quantitative comparison results.
Considering some works use ResNet50 as the backbone, we
also train the proposed model on the basis of this backbone
network. ResNet50 contains four convolutional blocks, and
we set the last layer of the second block as the optimiza-
tion layer. Then we utilize the last two blocks to design
the two branches. In Table. 1, the results of the attention
branch (denoted as “-A”) of the proposed model are also re-
ported. Moreover, we compare the average execution time
with the other models on DUTS dataset, and all scores are
tested on our platform (PAGR only provides saliency map-
s). It is obvious that the proposed model outperforms all
other models in most cases and it runs much faster than ex-
isting models. Only PiCANet-R obtains higher maxF score
than the proposed model on DUT-OMRON dataset. How-
ever, our model runs about 12 times faster than PiCANet-
R. More specially, compared to the improvements on maxF
and MAE, we obtain a larger improvement on avgF. This
demonstrates that the proposed model works much better
in uniformly highlighting salient objects. In addition, we
can ﬁnd that the results of our attention branch also achieve
comparable results with other models. Meanwhile, the pro-
posed model only with the attention branch runs faster. This
indicates that the proposed model provides two-level salien-
cy maps for real-time applications.

In Fig. 5, we show the qualitative comparison on some
challenging cases: small object, complex scenes, multiple
objects and large object. Even though we discard the low-
level features of backbone network, our model still recovers
precise boundaries of salient objects, and the small object
is still accurately segmented. Moreover, the proposed mod-
el segments more uniform salient objects than the compared
models. It is consistency with the results in Table. 1 that our
model achieves more improvement in avgF score than MAE
and maxF. This phenomenon is owing to the joint training s-
trategy of the proposed model. On one hand, the supervised
attention map of the attention branch makes the detection
branch further concentrate on salient objects. On the oth-
er hand, when training the proposed model, the gradient of
the detection branch also back propagates to the attention
branch. This training mechanism gradually promotes the
proposed model to focus on salient objects. More visual

3911

Method

Backbone

FPS

Amulet [42]
NLDF [25]

DSS [9]

BMPM [41]
PAGR [43]

PiCANet [21]
CPD-A (ours)

CPD (ours)

VGG16
VGG16
VGG16
VGG16
VGG19
VGG16
VGG16
VGG16

SRM [35]
DGRL [36]

ResNet50
ResNet50
PiCANet-R [21] ResNet50
CPD-RA (ours)
ResNet50
CPD-R (ours)
ResNet50

21
20
23
28
-
7

105
66

37
6
5

104
62

ECSSD [39]

HKU-IS [16]

DUT-OMRON [40]

DUTS [33]

PASCAL-S [19]

maxF
0.922
0.915
0.928
0.928
0.927
0.931
0.928
0.936

0.917
0.925
0.935
0.934
0.939

avgF MAE maxF
0.909
0.881
0.886
0.908
0.915
0.889
0.920
0.894
0.918
0.894
0.921
0.885
0.918
0.906
0.915
0.924

0.057
0.051
0.051
0.044
0.061
0.046
0.045
0.040

0.892
0.903
0.886
0.907
0.917

0.054
0.043
0.046
0.043
0.037

0.903
0.914
0.919
0.918
0.925

avgF MAE maxF
0.791
0.863
0.871
0.759
0.781
0.867
0.775
0.875
0.771
0.886
0.794
0.870
0.781
0.884
0.896
0.794

0.047
0.041
0.043
0.039
0.048
0.042
0.037
0.033

0.871
0.882
0.870
0.882
0.891

0.047
0.037
0.043
0.038
0.034

0.769
0.779
0.803
0.783
0.797

avgF MAE maxF
0.832
0.699
0.694
0.830
0.858
0.692
0.850
0.693
0.855
0.711
0.851
0.710
0.854
0.721
0.745
0.864

0.072
0.071
0.065
0.063
0.072
0.068
0.061
0.057

0.707
0.709
0.717
0.725
0.747

0.069
0.063
0.065
0.059
0.056

0.827
0.834
0.860
0.852
0.865

avgF MAE maxF
0.839
0.738
0.759
0.840
0.859
0.757
0.862
0.768
0.851
0.788
0.862
0.749
0.859
0.787
0.813
0.866

0.062
0.055
0.050
0.049
0.055
0.054
0.047
0.043

0.757
0.764
0.759
0.776
0.805

0.059
0.051
0.051
0.048
0.043

0.847
0.853
0.863
0.855
0.864

avgF MAE
0.095
0.780
0.792
0.083
0.081
0.796
0.074
0.770
0.092
0.803
0.076
0.796
0.077
0.814
0.825
0.074

0.796
0.807
0.798
0.807
0.824

0.085
0.074
0.075
0.077
0.072

Table 1: Comparison of different methods on ﬁve benchmark datasets and four metrics including FPS, MAE (lower is better),
max F-measure (higher is better) and average F-measure. The comparison is under two settings (with VGG [29] and
ResNet50 [8] backbone netowrk). The best result of each setting is shown in Red. “-R” means using ResNet50 as the
backbone. “-A” means the results of the attention branch. All method are the trained on training set of DUTS [33]. There is
not available code of PAGR [43] and the author only provides the saliency maps.

Image

GT

CPD

CPD-R

CPD-A

CPD-RA PiCANet-R PiCANet

PAGR

DGRL

BMPM

SRM

DSS

Amulet

NLDF

Figure 5: Visual comparisons of the proposed model and existing state-of-the-art algorithms in some challenging cases:
small object, complex scene, multiple objects and large object.

comparison results are shown in supplementary material.

4.1.3 Application in Existing Models

Through integrating features of each branch via using
aggregation algorithms proposed in existing models, our
framework can be utilized to improve these works.
In
this paper, we apply the proposed framework in three
deep aggregation models (BMPM, Amulet, NLDF). NLD-
F adopts a typical U-Net architecture, BMPM proposes a
bi-directional decoder with gate function and Amulet inte-
grates multi-level feature maps in multiple resolutions. We
implement the improved models in their respectively default
deep learning library (tensorﬂow [1] for BMPM and NLD-
F, caffe [12] for Amulet). For BMPM and NLDF, we train
the improved models (denoted as BMPM-CPD and NLDF-
CPD) by using default settings, and it only needs to change
the learning rate from the original 10−6 to 10−5. For A-

mulet, we train the improved model (denoted as Amulet-
CPD) by using the completely same settings as the original
model.

In Table. 2, we show the quantitative results of the orig-
inal models and the improved models (-CPD-A, -CPD) on
ﬁve benchmark datasets. We can see that each improved
model outperform its original model. More specially, the
improved models obtain a large improvement on the two
most challenging DUT-OMRON and DUTS datasets.
In
addition, the improved models (-CPD and -CPD-A) runs
about 2 and 3 times faster than the original models respec-
tively. In conclusion, the proposed cascaded partial decoder
framework can be used to improve deep aggregation mod-
els with different kinds of decoders. In Fig. 6, we show the
qualitative results on challenge cases: multiple objects, s-
mall object, large object and complex scene. The upper two
rows show that the improved models further focus on tar-
get regions and suppress distractions. The under two rows

3912

Method

FPS

BMPM [41]

BMPM-CPD-A

BMPM-CPD

NLDF [25]

NLDF-CPD-A

NLDF-CPD

Amulet [42]

Amulet-CPD-A

Amulet-CPD

28
82
47

21
75
48

21
61
45

ECSSD [39]

HKU-IS [16]

DUT-OMRON [40]

DUTS [33]

PASCAL-S [19]

maxF
0.928
0.932
0.935

0.915
0.918
0.922

0.922
0.925
0.934

avgF MAE maxF
0.894
0.920
0.920
0.901
0.907
0.925

0.044
0.046
0.043

avgF MAE maxF
0.875
0.775
0.796
0.882
0.888
0.804

0.039
0.037
0.035

avgF MAE maxF
0.693
0.850
0.864
0.731
0.740
0.870

0.063
0.057
0.056

avgF MAE maxF
0.768
0.862
0.861
0.799
0.808
0.868

0.049
0.046
0.044

avgF MAE
0.803
0.074
0.074
0.817
0.822
0.072

0.886
0.889
0.896

0.881
0.889
0.901

0.051
0.049
0.044

0.057
0.053
0.047

0.908
0.914
0.916

0.909
0.910
0.920

0.871
0.873
0.880

0.863
0.864
0.878

0.041
0.039
0.036

0.047
0.045
0.040

0.759
0.775
0.781

0.791
0.790
0.805

0.694
0.710
0.721

0.699
0.708
0.735

0.071
0.061
0.060

0.072
0.070
0.063

0.830
0.837
0.842

0.832
0.832
0.845

0.759
0.773
0.786

0.738
0.747
0.771

0.055
0.050
0.048

0.062
0.060
0.055

0.840
0.841
0.843

0.839
0.842
0.851

0.792
0.793
0.800

0.780
0.784
0.801

0.083
0.083
0.080

0.095
0.091
0.085

Table 2: Comparison of the original models and the improved models (-CPD-A and -CPD).

Image

GT

BMPM

BMPM-CPD-A BMPM-CPD

Amulet

Amulet-CPD-A Amulet-CPD

NLDF

NLDF-CPD-A

NLDF-CPD

Figure 6: Visual comparisons of original models (BMPM, Amulet, NLDF) with improved models (-CPD-A, -CPD).

Settings

CPD (with ia)
CPD (with ha)

Amulet-CPD (with ia)
Amulet-CPD (with ha)

BMPM-CPD (with ia)
BMPM-CPD (with ha)

NLDF-CPD (with ia)
NLDF-CPD (with ha)

DUTS [33]

PASCAL-S [19]

maxF
0.862
0.864

0.842
0.845

0.865
0.870

0.838
0.842

avgF MAE maxF
0.862
0.803
0.813
0.866

0.045
0.043

avgF MAE
0.075
0.821
0.825
0.074

0.763
0.771

0.791
0.808

0.777
0.786

0.056
0.055

0.045
0.044

0.051
0.048

0.849
0.851

0.867
0.868

0.840
0.843

0.794
0.801

0.818
0.822

0.793
0.800

0.087
0.085

0.072
0.072

0.084
0.080

Table 4: Comparison of initial attention (ia) and holistic
attention (ha) in four models (the proposed model and three
improved models).

show that the improved model further highlights the whole
objects.

4.1.4 Analysis of the Proposed Framework

Effectiveness of holistic attention. Here we demonstrate
the effectiveness of the proposed holistic attention model
in the proposed model and the three improved models. We
compare these models with holistic attention and the models
with initial attention, and the results are shown in Table.4. It

is shown that holistic attention outperforms initial attention.
Selection of Optimization Layer. In the proposed model,
we set Conv3 3 layer as the optimization layer. Here we
compared the proposed model with different optimization
layers (Conv2 2 and conv4 3). In addition, we also report
the results of no optimization layer, which means integrat-
ing all-level features via the proposed decoder. We do not
test the proposed model with Conv1 2 optimization layer
because this setting will increase the computation cost via
adding one more full decoder; thus requirements of reduc-
ing computation cost will not be achieved. The comparison
results on ﬁve benchmark datasets are shown in Table. 3.
In conclusion, we set the conv3 3 layer as the optimization
layer considering its best performance. When we reﬁne the
shallower feature (Conv2 2), the computation complexity
increases and the performance decreases. The reason might
be that the feature of shallower layer has not been enough
trained. When we reﬁne the deep feature (Conv4 3), the
computation cost and the performance both decrease. This
is because that resolution of the feature in the Conv4 3 layer
is smaller. The accuracy and efﬁciency of settings (Conv2 2
and Conv4 3) both outperform the full decoder, which vali-
dates the effectiveness of the proposed framework.

3913

Settings

FPS

Conv2 2
Conv3 3
Conv4 3

Full Decoder

38
66
90
30

ECSSD [39]

HKU-IS [16]

DUT-OMRON [40]

DUTS [33]

PASCAL-S [19]

maxF
0.936
0.936
0.931
0.922

avgF MAE maxF
0.925
0.903
0.915
0.924
0.920
0.910
0.891
0.911

0.042
0.040
0.041
0.051

avgF MAE maxF
0.884
0.792
0.794
0.896
0.787
0.890
0.873
0.758

0.036
0.033
0.034
0.042

avgF MAE maxF
0.720
0.861
0.864
0.745
0.855
0.737
0.692
0.843

0.063
0.057
0.059
0.070

avgF MAE maxF
0.778
0.865
0.866
0.813
0.863
0.801
0.766
0.853

0.048
0.043
0.045
0.050

avgF MAE
0.810
0.076
0.825
0.074
0.072
0.824
0.807
0.077

Table 3: Comparison of the proposed model with different optimization layers and no optimization layer (full decoder).

Image

GT

CPD-A

CPD

Figure 7: Some Failure examples of the proposed model.
When the attention branch only localizes a small part of
target regions, our model performs poorly.

Failure Examples. The performance of the proposed mod-
el relies on the accuracy of the attention branch. When the
attention branch detects clutters as target regions, our model
will obtain wrong results. In Fig. 7, we show some failure
examples of our model. When a large target region is not
segmented correctly, the proposed model is unable to seg-
ment the whole objects.

4.2. Application in Other Tasks

In this paper, we also evaluate the proposed model on
other two binary segmentation tasks: shadow detection and
portrait segmentation.

Shadow Detection. We re-train our model on the training
set of SBU [30] dataset and test the model on three public
shadow detection datasets: test set of SBU, ISTD [31] and
UCF [45]. Moreover, we apply the widely-used metric BER
(balanced error rate) for quantitative comparison. We com-
pare our method with ﬁve deep shadow detection methods:
JDR [31], DSC [10], DC-DSPF [37], scGAN [26], Stacked-
CNN [30]. In addition, we re-train three salient object de-
tection models for shadow detection: NLDF [25], DSS [9],
BMPM [41]. The results are shown in Table. 5, and the
proposed model outperforms the other models in all cases.

Portrait Segmentation. We use the data from [28]. And
we re-train NLDF, DSS, BMPM on this dataset. The results
are shown in Table 6. It can be seen that the proposed model
outperforms existing algorithms.

SBU [30]

ISTD [31] UCF [45]

Method

NLDF [25]

DSS [9]

BMPM [41]
scGAN [26]

StackedCNN [30]

JDR [31]

DC-DSPF [37]

DSC [10]
CPD (ours)

BER↓
7.02
7.00
6.17
9.10
11.00
8.14
4.90
5.59
4.19

BER↓
7.50
10.48
7.10
8.98
10.45
7.35

-

8.24
6.76

BER↓
7.69
10.56
8.09
11.50
13.00
11.23
7.90
8.10
7.21

Table 5: Comparing the proposed method with state-of-the-
arts for shadow detection (DSC, DC-DSPF, JDR, StackedC-
NN, scGAN), and for salient object detection (Amulet, NLD-
F, BMPM, DSS).

Methods
Mean IoU

PFCN+ [28] NLDF [25] DSS [9] BMPM [42] CPD (Ours)

95.90%

95.60%

96.20%

96.20%

96.60%

Table 6: Quantitative Comparison on Portrait Segmenta-
tion.

5. Conclusion

In this paper, we propose a novel cascaded partial de-
coder framework for fast and accurate salient object detec-
tion. When constructing decoders, the proposed framework
discards features of shallower layers to improve the com-
putational efﬁciency, and utilizes generated saliency map to
reﬁne features to improve the accuracy. We also propose
a holistic attention module to further segment the whole
salient objects and an effective decoder to abstract discrim-
inative features and quickly integrate multi-level features.
The experiments show that our model achieves state-of-the-
art performance on ﬁve benchmark datasets and runs much
faster than existing deep models. To prove the generaliza-
tion of the proposed framework, we apply it to improve ex-
isting deep aggregation models and signiﬁcantly improve
their accuracy and efﬁciency. Besides, we validate the ef-
fectiveness of the proposed model in two tasks of shadow
detection and portrait segmentation.
Acknowledgement. This work was supported by the U-
niversity of Chinese Academy of Sciences,
in part of
National 61472389, 61620106009, 61772494, U1636214,
61771457, 61732007, in part by Key Research Program of
Frontier Sciences, CAS: QYZDJ-SSW-SYS013.

3914

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ﬂow: a system for large-scale machine learning. In OSDI,
volume 16, pages 265–283, 2016.

[2] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk.
In CVPR, pages

Frequency-tuned salient region detection.
1597–1604, 2009.

[3] A. Borji, M.-M. Cheng, H. Jiang, and J. Li. Salient object
detection: A survey. arXiv preprint arXiv:1411.5878, 2014.
[4] A. Borji, M.-M. Cheng, H. Jiang, and J. Li. Salient objec-
IEEE TIP, 24(12):5706–5722,

t detection: A benchmark.
2015.

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE TPAMI, 40(4):834–848, 2018.

[6] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M.
IEEE

Hu. Global contrast based salient region detection.
TPAMI, 37(3):569–582, 2015.

[7] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In CVPR, pages 447–456, 2015.

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016.

[9] Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. S.
Torr. Deeply supervised salient object detection with short
connections. IEEE TPAMI, 41(4):815–828, 2019.

[10] X. Hu, L. Zhu, C.-W. Fu, J. Qin, and P.-A. Heng. Direction-
In

aware spatial context features for shadow detection.
CVPR, pages 7454–7462, 2018.

[11] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
IEEE TPAMI,

visual attention for rapid scene analysis.
20(11):1254–1259, 1998.

[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM MM, pages
675–678, 2014.

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2014.

[14] V. Koltun. Efﬁcient inference in fully connected crfs with

gaussian edge potentials. In NIPS, pages 109–117, 2011.

[15] G. Lee, Y.-W. Tai, and J. Kim. Deep saliency with encoded
In CVPR,

low level distance map and high level features.
pages 660–668, 2016.

[16] G. Li and Y. Yu. Visual saliency based on multiscale deep

features. In CVPR, pages 5455–5463, 2015.

[17] G. Li and Y. Yu. Deep contrast learning for salient object

detection. In CVPR, pages 478–487, 2016.

[18] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang,
H. Ling, and J. Wang. Deepsaliency: Multi-task deep neu-
ral network model for salient object detection.
IEEE TIP,
25(8):3919–3930, 2016.

[19] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. The
secrets of salient object segmentation. In CVPR, pages 280–
287, 2014.

[20] N. Liu and J. Han. Dhsnet: Deep hierarchical saliency net-
work for salient object detection. In CVPR, pages 678–686,
2016.

[21] N. Liu, J. Han, and M.-H. Yang. Picanet: Learning pixel-
wise contextual attention for saliency detection. In CVPR,
pages 3089–3098, 2018.

[22] S. Liu, D. Huang, and Y. Wang. Receptive ﬁeld block net for

accurate and fast object detection. In ECCV, 2018.

[23] T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum.
In CVPR, pages 1–8,

Learning to detect a salient object.
2007.

[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015.

[25] Z. Luo, A. K. Mishra, A. Achkar, J. A. Eichel, S. Li, and
P.-M. Jodoin. Non-local deep features for salient object de-
tection. In CVPR, pages 6593–6601, 2017.

[26] V. Nguyen, T. F. Y. Vicente, M. Zhao, M. Hoai, and D. Sama-
ras. Shadow detection with conditional generative adversar-
ial networks. In ICCV, pages 4520–4528, 2017.

[27] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241, 2015.

[28] X. Shen, A. Hertzmann, J. Jia, S. Paris, B. L. Price,
E. Shechtman, and I. Sachs. Automatic portrait segmentation
for image stylization. Comput. Graph. Forum, 35(2):93–102,
2016.

[29] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[30] T. F. Y. Vicente, L. Hou, C.-P. Yu, M. Hoai, and D. Sama-
ras. Large-scale training of shadow detectors with noisily-
annotated shadow examples.
In ECCV, pages 816–832,
2016.

[31] J. Wang, X. Li, and J. Yang. Stacked conditional generative
adversarial networks for jointly learning shadow detection
and shadow removal. In CVPR, pages 1788–1797, 2018.

[32] L. Wang, H. Lu, X. Ruan, and M.-H. Yang. Deep networks
for saliency detection via local estimation and global search.
In CVPR, pages 3183–3192, 2015.

[33] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and
X. Ruan. Learning to detect salient objects with image-level
supervision. In CVPR, pages 136–145, 2017.

[34] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan. Salien-
cy detection with recurrent fully convolutional networks. In
ECCV, pages 825–841, 2016.

[35] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu. A stage-
wise reﬁnement model for detecting salient objects in im-
ages. In ICCV, pages 4019–4028, 2017.

[36] T. Wang, L. Zhang, S. Wang, H. Lu, G. Yang, X. Ruan, and
A. Borji. Detect globally, reﬁne locally: A novel approach
to saliency detection. In CVPR, pages 3127–3135, 2018.

[37] Y. Wang, X. Zhao, Y. Li, X. Hu, K. Huang, and N. CRIPAC.
Densely cascaded shadow detection network via deeply su-
pervised parallel fusion. In IJCAI, pages 1007–1013, 2018.
In

[38] S. Xie and Z. Tu. Holistically-nested edge detection.

CVPR, pages 1395–1403, 2015.

3915

[39] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detec-

tion. In CVPR, pages 1155–1162, 2013.

[40] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang. Salien-
In CVPR,

cy detection via graph-based manifold ranking.
pages 3166–3173, 2013.

[41] L. Zhang, J. Dai, H. Lu, Y. He, and G. Wang. A bi-
directional message passing model for salient object detec-
tion. In CVPR, pages 1741–1750, 2018.

[42] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan. A-
mulet: Aggregating multi-level convolutional features for
salient object detection. In ICCV, pages 202–211, 2017.

[43] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang. Progressive
attention guided recurrent network for salient object detec-
tion. In CVPR, pages 714–722, 2018.

[44] R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection
by multi-context deep learning. In CVPR, pages 1265–1274,
2015.

[45] J. Zhu, K. G. Samuel, S. Z. Masood, and M. F. Tappen.
Learning to recognize shadows in monochromatic natural
images. In CVPR, pages 223–230, 2010.

[46] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization
from robust background detection. In CVPR, pages 2814–
2821, 2014.

3916

