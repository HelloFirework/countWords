Weakly-Supervised Discovery of Geometry-Aware Representation

for 3D Human Pose Estimation

Xipeng Chen∗1

Kwan-Yee Lin∗ 2,3

1Sun Yat-Sen University

Wentao Liu3
2Peking University

Chen Qian3
3SenseTime Research

Liang Lin1

1chenxp37@mail2.sysu.edu.cn 2linjunyi@pku.edu.cn

3{liuwentao,qianchen}@sensetime.com 4linliang@ieee.org

Abstract

Recent studies have shown remarkable advances in 3D
human pose estimation from monocular images, with the
help of large-scale in-door 3D datasets and sophisticated
network architectures. However, the generalizability to dif-
ferent environments remains an elusive goal.

In this work, we propose a geometry-aware 3D repre-
sentation for the human pose to address this limitation by
using multiple views in a simple auto-encoder model at the
training stage and only 2D keypoint information as super-
vision. A view synthesis framework is proposed to learn the
shared 3D representation between viewpoints with synthe-
sizing the human pose from one viewpoint to the other one.
Instead of performing a direct transfer in the raw image-
level, we propose a skeleton-based encoder-decoder mech-
anism to distil only pose-related representation in the latent
space. A learning-based representation consistency con-
straint is further introduced to facilitate the robustness of
latent 3D representation. Since the learnt representation
encodes 3D geometry information, mapping it to 3D pose
will be much easier than conventional frameworks that use
an image or 2D coordinates as the input of 3D pose esti-
mator. We demonstrate our approach on the task of 3D hu-
man pose estimation. Comprehensive experiments on three
popular benchmarks show that our model can signiﬁcantly
improve the performance of state-of-the-art methods with
simply injecting the representation as a robust 3D prior.

1. Introduction

3D human pose estimation refers to estimating 3D loca-
tions of body parts given an image or a video.  This task is
an active research topic in the computer vision community 
for serving as a key step for many applications, e.g., action
recognition, human-computer interaction, and autonomous

∗Xipeng  Chen  and  Kwan-Yee  Lin  have  contributed  equally  and  assert 
joint  first  authorship. Corresponding author is Liang Lin. The  work  was 
done during the internship at Sense-Time Research. 

Input  Image  

Deep    Network

3D  Pose  Prediction

Input  Image  

2D  Keypoint heat  maps  

Deep    Network 3D  Pose  Prediction

(a)  

(b)	  

G

φ

Input  Image  

Pre-­trained  

2D

2D  Human  Pose  Estimator  

Skeleton

Pre-­trained

Encoder

Learnt

Representation

Shallow    
Network

3D  Pose  
Prediction

(c)	  

Figure 1: Motivation. Most state-of-the-arts usually directly learn
the 3D poses from monocular images (as shown in (a)), or ﬁrst es-
timate 2D poses and then lift 2D poses to 3D poses (as shown
in (b)). Both categories require sophisticated deep network ar-
chitectures and abundant annotated training samples. Instead, we
consider learning a geometry representation from multi-view in-
formation with only 2D annotations as supervision. The learnt
representation could map to 3D pose with a shallow network and
less annotated training samples, as shown in (c).

driving. Signiﬁcant advances in particular datasets have
been achieved in recent years due to the abundant anno-
tations and sophisticated designed deep neural networks.
However, since precise 3D annotation requires large efforts,
and usually subjects to speciﬁc conditions in practice, like
motions, environments, and appearances, etc., the bottle-
neck of generalizability still exists.

Weakly-supervised learning provides an alternative
paradigm for learning robust geometry representation with-
out requiring extensive precise 3D annotation. Most of
approaches [42, 27, 25, 38] leverage knowledge transfor-
mation to learn the robustness by training 3D annotations
with abundant 2D annotations in-the-wild. These meth-
ods face the difﬁculties of large domain shift between con-
strained lab environment for 3D annotations and uncon-
strained in-the-wild environment for 2D annotations. Some
approaches try to represent body shape through multiple
view images acquired by synchronized cameras with the
usage of view-consistency property [27], pre-deﬁned para-
metric 3D model ﬁtting [3, 23, 10], or by sequence with
the usage of time-independent features [13]. Nevertheless,
ﬁtting a pre-deﬁned 3D model or exploiting limited multi-

10895

view information in a particular dataset can hardly capture
all subtle poses of the human body.

The emergence of approaches for novel view synthesis,
e.g., [8, 31], provides an appealing and succinct solution
for capturing geometry representation with multi-view in-
formation. However, despite the success of this ﬁeld on
many generic objects, like chairs, cars, and planes, it is non-
trivial to utilize existing frameworks to learn geometry rep-
resentation for the human body, since the human body is
articulated and much more deformable than rigid objects.

The objective of this paper is to devise a simple yet effec-
tive framework that learns a 3D geometry-aware structure
representation of human pose with only accessible 2D an-
notation as supervision. In particular, we use an encoder-
decoder to generate a novel view pose from a given view
pose. The latent code of the encoder-decoder is regarded as
the desired geometry representation. Instead of generating
the novel view pose on image-level [13, 2], we propose the
use of the 2D skeleton map as a compact medium. Con-
cretely, we ﬁrst map the source and target images into 2D
skeleton maps, then an encoder-decoder is trained to syn-
thesis target skeleton from source skeleton.

Introducing the 2D skeleton as the source/target space of
the encoder-decoder is beneﬁcial for learning a robust ge-
ometry representation. Firstly, 2D skeleton could be easily
obtained from an image with the usage of well-studied 2D
human pose estimator [20, 5, 15], which is accurate and ro-
bust under diverse poses, appearances and environment con-
ditions. This advantage could guarantee body pose and ge-
ometry information are faithfully kept. Secondly, skeleton
representation avoids the variances among datasets, which
could be leveraged to cover pose changes as much as possi-
ble by training existing datasets together and augment sam-
ples on continuous views. Thirdly, the representation in the
latent space could be simply distilled to only pose-related
information without consideration of disentangling shape
with appearance and other unessential nature of encoding
geometry information.

However, the premise of obtaining a robust geometry
representation under an encoder-decoder framework is the
accurate generation of the target view. While, there is no
theoretical assurance for generating the correct one, since
the conventional view synthesis losses (e.g., reconstruction
loss and adversarial loss) do not facilitate semantic infor-
mation. To address the problem, we introduce a representa-
tion consistency loss in latent space to constrain the process
without requiring any other auxiliary information.

We summarize our contributions as follows:

1) We propose a novel weakly-supervised encoder-decoder
framework to learn the geometry-aware 3D representa-
tion for the human pose with multi-view data and only
existing 2D annotation as supervision. To distil the rep-
resentation from unessential factors, and meanwhile in-

crease the training space, a skeleton-based view synthe-
sis is introduced. Our approach allows substantial 3D
pose estimator to generalize well in different conditions.

2) To ensure the robustness of the desired representation,
a representation consistency loss is introduced to con-
strain the learning process of latent space. In contrast to
conventional weakly-supervised methods which require
auxiliary information, our framework is more ﬂexible
and easier to train and implement.

3) A comprehensive quantitative and qualitative evaluation
on public 3D human pose estimation datasets shows the
signiﬁcant improvements of our model applied on state-
of-the-art methods, which demonstrates the effective-
ness of learnt 3D geometry representation to pose es-
timation task.

2. Related Work

Geometry-Aware Representations. To capture the in-
trinsic structure of objects, existing studies [37, 31, 13, 41]
typically disentangle visual content into multiple predeﬁned
factors like camera viewpoints, appearance and motion.
Some works [36, 40] leverage the correspondence among
intra-object instance category to encode the structure rep-
resentation. [40] discovery landmark structure as an inter-
mediate representation for image autoencoding with several
constraints. Other approaches utilize multiple views to ei-
ther directly learn the geometry representation [30, 39, 9]
with object reconstruction, or take advantage of view syn-
thesis [24] to learn the structure with shared latent repre-
sentation between views. For example, [24] learn 3D hand
pose representation by synthesizing depth maps under dif-
ferent views. [13] conditionally generate an image of the ob-
ject from another one, where the generated image differs by
acquisition time or viewpoint, to encourage representation
distilled to object landmarks. These methods mainly focus
on structure representation of generic objects or hand/face
pose. Whereas, the human body is articulated and much
more deformable. How to capture the geometry represen-
tation of the human body with fewer data and simpler con-
straints is still an open question.

3D Human Pose Estimation. Most of the existing stud-
ies for 3D human pose estimation beneﬁt from the availabil-
ity of large-scale datasets and sophisticated deep-net archi-
tectures. These methods could be roughly categorized into
fully-supervised and weakly-supervised manners.

A vast amount of fully-supervised 3D pose estimation
methods via monocular image exist in the literature [17,
19, 4, 33]. Despite the performance these methods achieve,
modeling 3D mapping from a given dataset limits their gen-
eralizability due to the constrained lab environment, limited

10896

Image-Skeleton Mapping

φ

µ

G
i

~
jG

View Synthesis

𝑅"→#

⊗

ijG

⊗

𝑅#→"

~
jiG

𝝍

ν

Representation Consistency Constraint

v
i

jv

𝐼"

𝐼#

Pre-trained  2D Human Pose Estimator 

(Hourglass Architecture)

Encoder

Decoder

Latent 

Representation

Loss

jvˆ

vˆ
i

Backward

Forward

Figure 2: The framework of learning a geometry representation for 3D human pose in a weakly-supervised manner. There are three
main components. (a)Image-skeleton mapping module is used to obtain 2D skeleton maps from raw images. (b)View synthesis module is
in a position to learn the geometry representation in latent space by generating skeleton map under viewpoint j from skeleton map under
viewpoint i. (c) Since there is no explicit constrain to facilitate the representation to be semantic, a representation consistency constrain
mechanism is proposed to further reﬁne the representation.

motion and inter-dataset variation.1

Several works focus on weakly-supervised learning to
increase the diversity of samples and meanwhile restrain the
usage of labeled 3d annotated data. For example, synthesize
training data by deforming a human template model with
known 3D ground truth [32], or generating various fore-
ground/background [18]. [42] proposes to transform knowl-
edge from 2D pose to 3d pose estimation network with
re-projection constraint to 2D results. A converse strat-
egy is employed in [38] to distil 3D pose structure to un-
constrained domain under an adversarial learning frame-
work. [23] proposes to learn the parameters of the statistical
model SMPL [16] to obtain 3D mesh from image with an
end-to-end network, and regresses 3d coordinates from the
mesh. Other approaches [27, 43] exploit views consistency
with the usage of multiple viewpoints of the same person.
Nevertheless, these methods still rely on a large quantity of
3D training samples or auxiliary annotations, like silhou-
ettes [6] and depth [43] to initialize or constrain the models.
In contrast to above approaches, our framework aims at
discovering a robust geometry-aware 3D representation of
human pose in latent space, with only 2D annotation in
hand. This allows us to train the subsequent monocular 3D
pose estimation network with much less labeled 3D data.
Recently, a concurrent work is published in the community
with similar spirits. In contrast to [26] that can only han-
dle one particular dataset due to the dependency of appear-
ance and inter-frame information during the training pro-
cess, our framework tries to break the gap of inter-dataset

1Inter-dataset variation refers to bias among different datasets on view-

points, environments, the deﬁnition of 3D key points, etc.

variation, which permits more practical usages. Moreover,
our framework is complementary to previous 3D pose esti-
mation works, and can use current approaches as the base-
line with the injection of learnt representation as a 3D struc-
ture prior.

3. Weakly-Supervised Geometry Representa-

tion

t , I j

Recall that our goal is to learn a geometry-aware 3D rep-
resentation G for the human pose, which is expected to be
robust to diverse pose changes and can be learnt with less
effort than conventional weakly-supervised methods. To-
ward this end, we propose to discover the geometry relation
between paired images(I i
t ), which are acquired from
synchronized and calibrated cameras, with the only exist-
ing 2D coordinate annotation used for supervision, where i
and j denote different viewpoints, t denotes acquiring time.
The proposed approach is depicted in Figure 2. The frame-
work includes three components: an image-skeleton map-
ping component, a skeleton-based view synthesis compo-
nent, and a representation consistency constraint compo-
nent. The desired representation is encoded in the bottle-
neck of the encoder-decoder on the view synthesis compo-
nent. In the inference phase, the learnt representation will
be obtained by forwarding a single image through the ﬁrst
two components, as illustrated in Figure 1(c). We will detail
each component in the remainder of this section.

3.1. Image skeleton mapping

It is habitual to directly feed forward the raw image
to the network to learn geometry representation [13, 31].
However, under the setting of multiple-view with encoder-

10897

t , I j

decoder framework, we demonstrate that utilizing only 2D
skeleton information is sufﬁcient and better than raw im-
ages to learn the representation, as shown in the Sec 4.
Consequently, given a pair of raw images (I i
t ) with the
size of W × H under different viewpoints of camera i and
camera j respectively, a pre-trained 2D human pose es-
timator2 is ﬁrstly applied to obtain two stacks of K key
point heatmaps C i
t . Then, the corresponding 2D
skeleton maps, regarded as a person tree-structured kine-
matic graph, are constructed from the heatmaps with 8 pix-
els width. Consequently, we are given the binary skeleton
maps pair (Si

t ∈ {0, 1}(K−1)×W ×H .

t ), where S(·)

t , and C j

t, Sj

Intuitively, we could sample (i, j) randomly from ex-
isting cameras. However, such a sampling strategy will
lead to two problems in practice. Firstly, the ﬁnite samples
limit the diversity of the training set. Secondly, the nonuni-
form distribution3 of viewpoints will increase the difﬁculty
of network learning. To solve the above problems, it is
straightforward to utilize virtual cameras-based data aug-
mentation. While, conventional methods can only achieve
in-plane rotations due to image-level inputs [13, 26].
In-
stead, we draw on virtual cameras applied in [7] to increase
training pairs on a torus4. Different from [7] that gener-
ate new 2D coordinates-3D coordinates pairs, we randomly
sample 2D skeleton pairs. Thus, we could obtain inﬁnite
training pairs and calculate their relative rotation matrix in
theory. This augmentation strategy facilitates our model to
be robust to different camera conﬁgurations.

3.2. Geometry representation via view synthesis

t, Sj

t , Ri→j)}NT

Assume that we are given a training set T =
t, Sj
{(Si
t=1 containing pairs of two views of pro-
jection of same 3D skeleton (Si
t ) and relative rotation
matrix Ri→j from coordinate system of camera i to j, af-
ter image-skeleton mapping step. We now turn to discover
the geometry representation G. A straightforward way for
learning representation in unsupervised/weakly-supervised
manner is to utilize autoencoding mechanism reconstruct-
ing input image. Then, the latent codes of the auto-encoder
could be regarded as the features that encode compact in-
formation of the input [40, 14]. While, such a represen-
tation neither contains geometry structure information nor
provides more useful information for 3D pose estimation
than 2D coordinates, as demonstrated in Figure 6.

The proposed

‘skeleton-based view synthesis’ step
draws an idea from novel view synthesis methods, which
usually rely on the encoder-decoder framework to generate
image under a new viewpoint of the same object, given an

SG

D G

GT

Figure 3: An illustration of the effectiveness of representation
consistency constraint. Compared with only applying the ‘image-
skeleton mapping+view synthesis’(SG), the representation consis-
tency constraint(DG) is able to reﬁne the implausible poses, which
is more similar to the ground-truth poses(GT)(better zoom in).

image under the known viewpoint as input. Without the loss
of generality, the input images are regarded as the source
domain, and the generated ones are regarded as the target
domain. We tailor the process to our problem as follows.

t}V

t }V

Let S i = {Si

t ∈ S i into a latent space Gi ∈ G.

i=1 be the source domain, where V de-
notes the amount of viewpoints, and S j = {Sj
j=1 be
the target domain with j 6= i. We are interested in learn-
ing an encoder φ : S i → G that capture the geometry
structure of the human pose. The encoder maps a source
skeleton Si
In order
to learn G, the property of the shared representation be-
tween the source and target domains should be satisﬁed.
Thus, under the control of relative rotation matrix Ri→j ,
Gi should be decoded back to the target view with a decoder
ψ : Ri→j × G → S j . Besides, if G is close to the manifold
of 3D pose coordinates, the learning process of subsequent
monocular 3D pose estimation will be simpliﬁed and less
labeled 3D data will be needed. So far, it is difﬁcult to
demonstrate whether the learnt Gi satisfy the assumption,
since the framework doesn’t contain any explicit constraint
to Gi. To this end, the dimensional space of G should be
constrained at ﬁrst. We formulate Gi as the set of m discrete
points on a 3η-dimensional feature space with the form of
a 3η-dimensional and M -length feature vector in practice,
i.e., G = [g1, g2, · · · , gM ]⊤ with gm = (xm, ym, zm). We
adopt L2 reconstruction loss to the learning process:

Lℓ2 (φ · ψ, θ) =

1
N T X kψ(Ri→j × φ(Si

t)) − Sj

t k2. (1)

While the combination of reconstruction loss, adversar-
ial loss and perceptual loss are widely used in synthesis
tasks [2, 35, 34], the rest two losses will introduce artiﬁcial
noise to our framework. Since skeleton maps only contain
low-frequency information when regarded as the images.

2We follow previous works [42, 19, 17] to train the 2D estimator on

MPII dataset.

3For example, in Human3.6M dataset [11], four cameras are approxi-

mately located at four corners of a rectangle.

4Please refer to the supplemental materials for detail operation.

3.3. Representation consistency constraint

As shown in Figure 3, only applying ‘image-skeleton
mapping+view synthesis’ components may lead to unreal-
istic generation on target pose when there are large self-

10898

occlusions on source view, which will lead the learnt rep-
resentation G misleading the regression of subsequent 3D
pose estimation task. Since there is no explicit constraint
on latent space to facilitate G to be semantic. To this
end, we propose a representation consistency constraint to
the framework. We assume there exists an inverse map-
ping (one-to-one) between source domain and target do-
main, on the condition of the known relative rotation ma-
trix. Then, we could ﬁnd an encoder µ : S j → G maps
t to the latent space ˜Gj ∈ G, and a de-
target skeleton Sj
coder ν : Rj→i × G → Si maps the representation ˜Gj back
to source skeleton Si
t on the condition of Rj→i. Thus, for
t ), Gi and ˜Gj should be the same shared
paired data (Si
representation on G with different rotation-related coefﬁ-
cients. We add this relationship, namely representation con-
sistency, to the network explicitly with the formulation as:

t, Sj

lrc = kf × Gi − ˜Gjk2,

(2)

where f denotes the rotation-related transformation that
map Gi to ˜Gj . This loss function is well-deﬁned when
f is known. To release the constraint, we simply assume
f = Ri→j . In practice, we implement the representation
consistency constraint by designing a bidirectional encoder-
decoder framework, which hinges on two encoder-decoder
networks with same architecture, i.e., generator(φ, ψ) and
generator(µ, ν), to perform view synthesis in the two di-
rections simultaneously. Speciﬁcally, let Gij be the rotated
Gi on generator(φ, ψ)-branch, we enforce normalized Gij
to be close to normalized ˜Gj with modiﬁed Eqn 2:

lrc =

M

X

m=1

kgijm − gjm k2
2.

(3)

The general idea behind the formula is that if the mapping
could be perfectly modeling, the latent codes Gi and ˜Gj
would be the same geometry representation under world
coordinate system mapping to different camera coordinate
systems. In other words, the consistency constraint enforces
the learnt latent codes containing explicit physical mean-
ings. Thus, features of implausible poses could be distilled.
With more robust representation, subsequent pose estima-
tion results will be improved.

Besides, since the latent codes are formulated as the set
of m discrete points on a 3η-dimensional feature space, they
could be regarded as 3D point clouds. In Figure 4, we show
both point clouds interpolations with/without proposed rep-
resentation constraint to illustrate the claim qualitatively.
As can be seen from the ﬁgure, the linear interpolation re-
sults of the one with representation constraint show more
reasonable coverage of the manifold, and better consistency
between decoded 2D skeleton on the target domain and re-
gressed 3D pose. This phenomenon demonstrates the learnt

Pose_1

Interpolations

Pose_2

(a)

(b)

Figure 4:
Illustration of point cloud interpolation. Pose 1 and
Pose 2 are two randomly sampled poses under same camera view-
point. (a) and (b) show the interpolation results of the latent codes
learned without/with representation constraint, respectively. There
are two main differences. First, from ﬁrst rows in (a) and (b), (b)
shows more smooth interpolation results (for example, the change
of arms from the ﬁfth column to the sixth column), than the ones
in (a). Second, the lower part of the body should gradually stand
upright and spraddle from left to right for both 2D skeleton and
3D pose. However, it is inconsistent between the 2D skeleton and
3D pose in (a). Instead, the results in (b) are consistent.

latent codes have extracted better 3D geometry representa-
tions of the human shape with the help of representation
constraint.

We train our bidirectional model in an end-to-end man-

ner, minimizing the following total loss:

L = Lℓ2 (φ · ψ, θ) + Lℓ2 (µ · ν, ζ) + Lrc(φ, µ, θ),

(4)

where θ and ζ denotes the parameters of two encode-
decoder networks, respectively.

3.4. 3D human pose estimation by learnt represen 

tation

Recall that our ultimate goal is to inference 3D human
pose in the form of b = {(xp, yp, zp)}P
p=1 from a monoc-
ular image I, where P denotes the amount of body joint
locations, and b ∈ B. In this section, we discuss how to
ﬁnd a function F : I → B to learn the pose regression.
Above components ﬁrst lift the raw image to a 2D skeleton
representation, then the 2D skeleton is lifted to G, which is
a 3D geometry representation for human body. Thus, we
could split function F into three sub-functions: F2D, FG
and Fregression, with:

F(I) = Fregression(FG(F2D(I))) = Fregression(G),

(5)
where F2D denotes the ﬁrst component, and FG denotes
the second component. Since G ∈ R3×M and b ∈ R3×P ,
Fregression(·) could be a linear function to decode G to
b. In practice, we implement the regression part by sim-
ply constructing a two-layers fully-connected neural net-
work. Speciﬁcally, we ﬁrstly feed forward the raw image

10899

‘image-skeleton mapping+φ’ to
to the ﬁxed components
obtain G, then G is regarded as the input to Fregression(·)
to regress the ﬁnal coordinates. Only leveraging a small set
of labeled samples to train the regression part could lead to
satisﬁed accuracy, as demonstrated in Sec 4.

4. Experiments

Datasets. We evaluate our approach both quantitatively
and qualitatively on popular human pose estimation bench-
marks: Human3.6M [11], MPI-INF-3DHP [18], and MPII
Human Pose [1]. Human3.6M is the largest dataset for
3D human pose estimation, which consists of 3.6 million
poses and corresponding video frames featuring 11 actors
performing 15 daily activities from 4 camera views. MPI-
INF-3DHP is a recently proposed 3D benchmark consists
of both constrained indoor and complex outdoor scenes.
MPII Human Pose dataset is a challenging benchmark for
estimating in-the-wild 2D human pose. Following previous
methods [38, 7, 22, 17], we adopt this dataset for evaluating
the cross-domain generalization qualitatively.

Evaluation Protocols. For Human3.6M dataset, we fol-
low the standard protocol, i.e.,Protocol#1, to use all 4 cam-
era views in subjects 1, 5, 6, 7 and 8 for training, and same
all 4 camera views in 9 and 11 for testing. In some works,
the predictions are further aligned with the ground-truth via
a rigid transformation [38, 7], which is referred as Proto-
col#2. To further validate the robustness of different models
to new subjects and views, we follow [7] to use subjects 1,
5, 6, 7 and 8 in 3 camera views for training, while 9 and
11 in the other camera view for testing. This protocol is re-
ferred as Protocol#3. The evaluation metric is the Mean Per
Joint Position Error (MPJPE), measured in millimeters.

Implementation Details. For

‘image-skeleton map-
ping’ module, we adopt a state-of-the-art 2D pose estima-
tor [20] to perform 2D pose detection. We adopt the net-
work architecture on the U-Net as the backbone of our
generator(·, ·). The skip connections are removed to en-
sure all information can be encoded into the latent codes.
For model acceleration, we also halve the feature channels
and modify the input and output to 15-channel 64 × 64. The
regression module is a two-layer fully-connected network
of dimensions 1024 and 48, which is referred to as Regres-
sion#1. To further validate the ﬂexibility and complemen-
tarity of our proposed framework to other approaches, we
also try to use state-of-the-art 3D pose estimators [17, 29]
as the regression components. The learnt representation G,
behaves as a 3D structure prior, is injected into their frame-
works. These two conﬁgurations are referred to as Regres-
sion#2 and Regression#3 respectively. Note that, in order
to evaluate the robustness and ﬂexibility of the proposed ge-
ometry representation in a straightforward manner, we only
forward the geometry representation G to fully connection
layers to match the feature dimension of baselines, and then

MPJPE	  in	  mm

PMPJPE	  in	  mm

207.5
200

210

190

170

150

130

110

90

70

127.3 122.7
107.9

122

121.5 121.6 117.6 115.3 114.7

96.2

94.5

93.3

91.9

88.5

83.4

80.2

49	  	  	  

496	  	  	  	  	  	  	  	  

2.5k	  	  	  	  	  	  	  	  

5k	  	  	  	  	  	  	  	  

25k	  	  	  	  	  	  	  	  

(0.1%S1)

(1%	  S1)

(5%	  S1)

(10%	  S1)

(50%	  S1)

49k	  	  	  	  	  	  
(S1)

(	  S1	  +	  S5)

129k	  	  	  	  	  	  	  	  	  	  

179k	  	  	  	  	  	  	  	  	  

312k	  	  	  	  	  	  	  	  	  	  
(all	  )

(	  S1	  +	  S5	  +	  

S6)

153.2

135.7

170

150

130

110

90

70

50

85.3
82.1

82.9
71.3

82.4
70.7

81.6

68.6

81.5

68

78.9

62.5

77

60.3

76.1

58.2

49	  	  	  

496	  	  	  	  	  	  	  	  

2.5k	  	  	  	  	  	  	  	  

5k	  	  	  	  	  	  	  	  

25k	  	  	  	  	  	  	  	  

(0.1%S1)

(1%	  S1)

(5%	  S1)

(10%	  S1)

(50%	  S1)

49k	  	  	  	  	  	  
(S1)

(	  S1	  +	  S5)

129k	  	  	  	  	  	  	  	  	  	  

179k	  	  	  	  	  	  	  	  	  

312k	  	  	  	  	  	  	  	  	  	  
(all	  )

(	  S1	  +	  S5	  +	  

S6)

OursShallow

Baseline#1

OursShallow

Baseline#1

(a)

(b)

Figure 5: Evaluation on the Human3.6M using different number
of training data. (a) presents the results under MPJPE metric. (b)
presents the results under PMPJPE metric.

directly do element-wise sum with baselines, instead of de-
signing sophisticated feature fusion mechanism to poten-
tially better fuse the representation with original features.
All the experiments are conducted on Titan X GPUs. Please
refer to the supplemental materials for architecture details.
Results on Human3.6M. We ﬁrstly validate the effec-
tiveness of learnt representation G to 3D human pose esti-
mation task, on the condition of using different amount of
3D annotated samples (under Protocol#1) to train the re-
gression module. We adopt Regression#1 as the regressor
with only G as the input. The conﬁguration is referred as
OursShallow. Since only 2D annotation is utilized to learn
G, we also list the performances of directly regressing 3D
pose coordinates from 2D detections with the same regres-
sor, which is referred to Baseline#1. Figure 5 shows the
results. The phenomenon is consistent on both MPJPE and
PMPJPE metrics. Given only about 500 annotated train-
ing samples, our method achieves 17.98% relative improve-
ments than Baseline#1 on MPJPE, and 3.90% on PMPJPE.
The margin becomes larger when more annotated samples
are used for training. Our general improvements over dif-
ferent setting demonstrate the robustness of the learnt rep-
resentation to different amount of 3D training samples. We
also perform above experiments on Regression#2 and Re-
gression#3 to further verify the effectiveness of the learnt
representation to strong baselines (For space saving, the de-
tail results are shown in the supplementary material). Under
fewer amount of training samples, our proposed represen-
tation could help improve the performance of baselines to
comparable results with the one trained on a larger amount
of samples by themselves.

We then evaluate the models under all three protocols to
demonstrate the effectiveness and ﬂexibility of learnt rep-
resentation G as a robust 3D prior to different 3D human
pose estimation methods. Table 1 reports the comparison
with current state-of-the-arts. We draw two key observa-
tions as follows: (1) Directly regressing 3D poses with only
learnt geometry representation G as input and simple 2-
layer fc architecture (Ours+ Regression#1) could achieves
reasonable 3D pose estimation results. (2) As a 3D geome-
try prior, G could easily help improving the performance
of different backbones coherently, achieving state-of-the-

10900

Protocol #1

Direction Discuss

Martinez et al. (ICCV’17) [17]
Fang et al. (AAAI’18) [7]
Sun et al. (ICCV’17) [28]
Yang et al. (CVPR’18) [38]
Pavlakos et al. (CVPR’18) [21]
Sun et al. (ECCV’18) [29]

Ours + Regression#1 (2 fc layers)
Ours + Regression#2 ( [17])
Ours + Regression#3 ( [29])

51.8
50.1
52.8
51.5
48.5
46.5

63.9
45.9
41.1

56.2
54.3
54.8
58.9
54.4
48.1

73.7
53.5
44.2

Protocol #2

Direction Discuss

Moreno-Noguer (CVPR’17) [19]
Zhou et al. (Arxiv’17) [44]
Sun et al. (ICCV’17) [28]
Martinez et al. (ICCV’17) [17]
Fang et al. (AAAI’18) [7]
Sun et al. (ECCV’18) [29]
Yang et al. (CVPR’18) [38]

Ours + Regression#1 (2 fc layers)
Ours + Regression#2 ([17])
Ours + Regression#3 ([29])

66.1
47.9
42.1
39.5
38.2
40.9
26.9

47.0
36.5
36.9

61.7
48.8
44.3
43.2
41.7
41.4
30.9

51.8
41.0
39.3

Protocol #3

Direction Discuss

Pavlakos et al. (CVPR’17) [22]
Martinez et al. (ICCV’17) [17]
Zhou et al. (ICCV’17) [42]
Fang et al. (AAAI’18) [7]
Sun et al. (ECCV’18) [29]

Ours + Regression#1 (2 fc layers)
Ours + Regression#2 ([17])
Ours + Regression#3 ([29])

79.2
65.7
61.4
57.5
52.4

70.8
60.4
45.9

85.2
68.8
70.7
57.8
50.5

78.3
63.6
48.0

Eat

58.1
57.0
54.2
50.4
54.4
49.9

70.9
50.1
44.9

Eat

84.5
52.7
45.0
46.4
43.7
45.0
36.3

53.3
40.9
40.5

Eat

78.3
92.6
62.2
81.6
45.0

84.9
77.2
48.6

Greet

Phone

Photo

Pose

Purchase

59.0
57.1
54.3
57.0
52.0
51.1

76.1
53.2
45.9

69.5
66.6
61.8
62.1
59.4
47.3

82.6
61.5
46.5

78.4
73.3
67.2
65.4
65.3
43.2

69.5
72.8
39.3

55.2
53.4
53.1
49.8
49.9
45.9

75.1
50.7
41.6

58.1
55.7
53.6
52.7
52.9
57.0

96.1
49.4
54.8

Sit

74.0
72.8
71.7
69.2
65.8
77.6

120.6
68.4
73.2

SitDown

Smoke Wait WalkDog Walk WalkT. Avg.

94.6
88.6
86.7
85.2
71.1
47.9

75.4
82.1
46.2

62.3
60.3
61.5
57.4
56.6
54.9

96.8
58.6
48.7

59.1
57.7
53.4
58.4
52.9
46.9

78.7
53.9
42.1

65.1
62.7
61.6
43.6
60.9
37.1

69.1
57.6
35.8

49.5
47.5
47.1
60.1
44.7
49.8

83.5
41.1
46.6

52.4
50.6
53.4
47.7
47.8
41.2

72.2
46.0
38.5

62.9
60.4
59.1
58.6
56.2
49.8

80.2
56.9
46.3

Greet

Phone

Photo

Pose

Purchase

Sit

SitDown

Smoke Wait WalkDog Walk WalkT. Avg.

73.7
55.0
45.4
47.0
44.9
45.2
39.9

55.3
43.9
41.2

65.2
56.8
51.5
51.0
48.5
42.1
43.9

59.7
45.6
42.0

67.2
65.5
53.0
56.0
55.3
37.6
47.4

48.4
53.8
34.9

60.9
49.0
43.2
41.4
40.2
41.1
28.8

51.7
38.5
38.0

67.3
45.5
41.3
40.6
38.2
52.0
29.4

72.1
37.3
51.2

103.5
60.8
59.3
56.5
54.5
71.4
36.9

90.6
53.0
67.5

74.6
81.1
73.3
69.4
64.4
42.5
58.4

56.6
65.2
42.1

92.6
53.7
51.0
49.2
47.2
47.4
41.5

65.4
44.6
42.5

69.6
51.6
44.0
45.0
44.3
41.6
30.5

55.1
40.9
37.5

71.5
54.8
48.0
49.5
47.3
32.0
29.5

50.2
44.3
30.6

78.0
50.4
38.3
38.0
36.7
42.6
42.5

59.4
32.0
40.2

73.2
55.9
44.8
43.1
41.7
36.9
32.2

53.9
38.4
34.2

74.0
55.3
48.3
47.7
45.7
44.1
37.7

58.2
44.1
41.6

Greet

Phone

Photo

Pose

Purchase

Sit

SitDown

Smoke Wait WalkDog Walk WalkT. Avg.

89.9
79.9
76.9
68.8
57.8

89.2
69.5
50.8

86.3
84.5
71.0
75.1
49.8

89.2
64.8
48.9

87.9
100.4
81.2
85.8
50.3

78.0
96.1
45.1

75.8
72.3
67.3
61.6
46.1

85.6
64.1
46.1

81.8
88.2
71.6
70.4
57.1

116.3
75.0
57.4

106.4
109.5
96.7
95.8
96.3

142.7
87.6
77.3

137.6
130.8
126.1
106.9
47.4

87.0
111.1
49.4

86.2
76.9
68.1
68.5
56.4

114.2
66.6
54.2

92.3
81.4
76.7
70.4
52.1

88.1
67.7
47.2

72.9
85.5
63.3
73.89
45.7

81.5
70.0
39.9

82.3
69.1
72.1
58.5
53.7

92.9
54.8
49.9

77.5
68.2
68.9
59.6
48.7

80.3
57.6
42.9

88.6
84.9
75.6
72.8
53.6

91.4
71.8
50.3

Table 1: Quantitative comparisons of Mean Per Joint Position Error (mm) between the estimated pose and the ground-truth on Hu-
man3.6M under Protocol #1,#2 #3. The best score is marked in bold.

BL+DG+AUG

BL+SG+AUG

BL+I_SG

BL+SG

BL+AE

BL

art results under all three protocols. Even on the strong
baseline like [29], which is the most state-of-the-art, the
model (Ours+Regression#3) could still have 7% inprove-
ments, achieving 46.3 of mm of error.

Ablation Study. We conduct ablation experiments on
the Human3.6M dataset under Protocol#1 to verify the ef-
fectiveness of different components of our method. The
overall results are shown in Figure 6. The notations and
comparison are as follows:

• BL refers to the 3D pose estimator without learnt rep-
resentation G. We regard this model as the baseline
model of our framework. We train the baseline with
its public implementation [29]. The mean error of the
baseline is 49.8mm.

• BL+I SG refers to the use of raw images to train the
generator(·, ·). We observe a drop of performance
(49.8mm → 52.6mm), which is even worse than
the baseline model. This result suggests that the raw
image-based view synthesis mechanism could not fa-
cilitate the encoding of the representation due to the
lack of the distilling step to distill unnecessary factors
(e.g.,appearance, lighting, and background).

• BL+AE refers to the conﬁguration that the source
and target domain are same during the training of
generator(·, ·). The mean error is 49.9mm, which
is almost the same with the baseline. This result sug-
gests that the latent codes of autoencoding could not
provide more valid information than a pure 2D coor-

52.6

49.8

49.9

48.2

47.4

46.3

54

52

50

48

46

44

42

BL

BL+I_SG

BL+AE

BL+SG

BL+SG+AUG BL+DG+AUG

Figure 6: Ablation studies on different components in our
method. The evaluation is performed on Human3.6M under Pro-
tocol#1 with MPJPE metric.

dinate information, if there is no special mechanism
incorporated in.

• BL+SG refers to the model that injecting learnt repre-
sentation G to the baseline network as a 3D structure
prior, where G is learnt without representation consis-
tency constraint. Simply adding the learnt G to the
baseline network by concatenation operation instead
of any sophisticated fusion mechanism, the model re-
duces the error by 3.2%(49.8mm → 48.2mm). This
validates the effectiveness and ﬂexibility of our frame-
work to learn the geometry representation in the ar-
ticulated human body. Moreover, comparing with the
results on BL+I SG, BL+SG shows 2D skeleton maps
could provide sufﬁcient information to learn the geom-
etry representation.

• BL+SG+AUG refers to the use of data augmentation
by virtual cameras. The augmentation provides 1.6%

10901

Figure 7: Qualitative results of our approach on the test split of in-the-wild MPII human pose dataset. Best viewed in color.

lower mean error compared with ‘BL+SG’. In the ab-
lation study that shown in supplemental materials, the
augmentation on other baselines show similar results
of relative improvements.

• BL+DG+AUG refers to the use of representation
consistency constraint. We see a 2.3% error drop
(47.4mm → 46.3mm), showing that our proposed
consistency constraint
indeed increase the robust-
ness of the geometry representation G. The con-
straints that conventionally designed in multi-view ap-
proaches, e.g. epipolar divergence [12] and multi-view
consistency [27], require iterative optimization-based
method, like RANSAC, to initialize the process.
In
contrast, our representation consistency constraint is
straightforward and purely feed-forward, which is eas-
ier to train and implement.

We further illustrate the ablation study on the conﬁgu-
ration of Regression#1 and Regression#2. The observation
is similar to the results shown in Figure 6, while the rel-
ative improvements among different components are more
signiﬁcant. Please refer to supplemental materials.

Cross-Domain Generalization. Here, we perform three
types of cross-dataset evaluation to further verify some mer-
its of our approaches.

We ﬁrst demonstrate the generalization ability of the
learnt representation between domains quantitatively. Ta-
ble 2 reports the results of the conﬁguration that train-
ing on Human3.6M and then testing on INF-3DHP. Fol-
lowing [18, 38], we use AUC and PCK as the evalua-
tion metrics. As can be seen from the results, our model
with different regressors present consistent improvements
to their baselines in most cases, which demonstrates the
learnt geometry representation could improve the general-
ization ability of subsequent pose estimator signiﬁcantly for
its robust to new camera views and unseen poses.

[18]
PCK 64.7
AUC 31.7

[42]
50.1
21.6

[38] R#1 R#2[17] R#3[29] Ours + R#1 Ours + R#2 Ours + R#3
69.0
32.0

41.0
17.1

68.7
34.6

68.0
34.7

68.4
29.4

61.4
29.4

75.9
36.3

Table 2: Cross-dataset comparison with state-of-the-arts on the
MPI-INF-3DHP dataset with PCK and AUC metrics. R#* indi-
cates Regression#*.

We then demonstrate the generalization ability of our
model to the unconstrained environment qualitatively. Fig-
ure 7 shows the sampled results on the test split of MPII
dataset, where the model is trained on Human3.6M dataset.
As can be seen from the ﬁgure, our method is able to accu-
rately predict 3D pose for in-the-wild images.

Finally, we present the beneﬁt of eliminating the inter-
dataset variation to 3D human pose estimation. Since our
framework breaks the gap of inter-dataset variation, differ-
ent 3D human pose benchmarks could be trained together to
increase the diversity. As shown in Figure 8, cross-dataset
training (Human3.6M + MPI-INF-3DHP) shows better ro-
bustness than single-dataset training (Human3.6M) on some
unseen poses of the MPII dataset.

Figure 8: Qualitative comparison on the MPII dataset. The sec-
ond column shows the predictions of training on the Human3.6M
dataset. The third column shows the predictions of cross-dataset
training.

5. Conclusion

We have presented a weakly-supervised method of learn-
ing a geometry-aware representation for 3D human pose es-
timation. Our method is novel in that we take a radically dif-
ferent approach to learn the geometry representation under
multi-view setting. Speciﬁcally, we leverage view synthesis
to distill shared representation in the latent space with only
the usage of 2D annotation and simple representation con-
sistency constraint, which provides a new aspect to learn
the representation with fewer annotation efforts and sim-
pler network architecture. Meanwhile, we bridge different
3D human pose datasets by introducing a skeleton-based
encoder-decoder. Experimental results validate the effec-
tiveness and ﬂexibility of the proposed framework on 3D
human pose estimation task.

10902

References

[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and

Bernt Schiele.

[2] Guha Balakrishnan, Amy Zhao, Adrian V. Dalca, Frdo Du-
rand, and John Guttag. Synthesizing images of humans in
unseen poses. In CVPR, 2018.

[3] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J Black. Keep it smpl:
Automatic estimation of 3d human pose and shape from a
single image. In ECCV, 2016.

[4] Ching-Hang Chen and Deva Ramanan. 3d human pose esti-

mation= 2d pose estimation+ matching. In CVPR, 2017.

[5] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L
Yuille, and Xiaogang Wang. Multi-context attention for
human pose estimation. arXiv preprint arXiv:1702.07432,
2017.

[6] Yu Du, Yongkang Wong, Yonghao Liu, Feilin Han, Yilin
Gui, Zhen Wang, Mohan Kankanhalli, and Weidong Geng.
Marker-less 3d human motion capture with monocular im-
age sequence and height-maps. In ECCV, 2016.

[7] Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, and
Song-Chun Zhu. Learning pose grammar to encode human
body conﬁguration for 3d pose estimation. In AAAI, 2018.

[8] John Flynn, Ivan Neulander, James Philbin, and Noah
Snavely. Deepstereo: Learning to predict new views from
the world’s imagery. In CVPR, 2016.

[9] Po-Han Huang, Kevin Matzen, Narendra Ahuja, and Jia-Bin
Huang. Deepmvs: Learning multi-view stereopsis. In CVPR,
2018.

[10] Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo
Kanazawa, Peter V. Gehler, Javier Romero, Ijaz Akhter,
and Michael J. Black. Towards accurate marker-less human
shape and pose estimation over time. In 3DV, 2017.

[11] Catalin Ionescu, Vlad Olaru, and Cristian Sminchisescu. Hu-
man3.6m: Large scale datasets and predictive methods for 3d
human sensing in natural environments. TPAMI, 2014.

[12] Yasamin Jafarian, Yuan Yao, and Hyun Soo Park. Multiview
semi-supervised keypoint via epipolar divergence. CoRR,
2018.

[13] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea
Vedaldi. Conditional image generation for learning the struc-
ture of visual objects. arXiv preprint arXiv:1806.07823,
2018.

[14] Diederik P. Kingma and Max Welling. Auto-encoding vari-

ational bayes. CoRR, 2013.

[15] Wentao Liu, Jie Chen, Cheng Li, Chen Qian, Xiao Chu, and
Xiaolin Hu. A cascaded inception of inception network with
attention modulated feature fusion for human pose estima-
tion. In AAAI, 2018.

[16] Matthew Loper, Naureen Mahmood, Gerard Pons-Moll, and
Michael J Black. Smpl: A skinned multi-person linear
model. TOG, 2015.

[17] Julieta Martinez, Rayat Hossain, Javier Romero, and James J
Little. A simple yet effective baseline for 3d human pose
estimation. In ICCV, 2017.

[18] Dushyant Mehta, Helge Rhodin, Oleksandr Sotnychenko,
and Christian Theobalt. Monocular 3d human pose estima-
tion in the wild using improved cnn supervision.
In 3DV,
2017.

[19] Francesc Moreno-Noguer. 3d human pose estimation from a
single image via distance matrix regression. In CVPR, 2017.
[20] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
[21] Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
Ordinal depth supervision for 3d human pose estimation. In
CVPR, 2018.

[22] Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpa-
nis, and Kostas Daniilidis. Coarse-to-ﬁne volumetric predic-
tion for single-image 3d human pose. In CVPR, 2017.

[23] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
Daniilidis. Learning to estimate 3d human pose and shape
from a single color image. In CVPR, 2018.

[24] Georg Poier, David Schinagl, and Horst Bischof. Learning
pose speciﬁc representations by predicting different views.
In CVPR, 2018.

[25] Alin-Ionut Popa, Mihai Zanﬁr, and Cristian Sminchisescu.
Deep multitask architecture for integrated 2d and 3d human
sensing. In CVPR, 2017.

[26] Helge Rhodin, Mathieu Salzmann, and Pascal Fua. Unsu-
pervised geometry-aware representation for 3d human pose
estimation. arXiv preprint arXiv:1804.01110, 2018.

[27] Helge Rhodin, J¨org Sp¨orri, Isinsu Katircioglu, Victor Con-
stantin, Fr´ed´eric Meyer, Erich M¨uller, Mathieu Salzmann,
and Pascal Fua. Learning monocular 3d human pose estima-
tion from multi-view images. In CVPR, 2018.

[28] Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.

Compositional human pose regression. In ICCV, 2017.

[29] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen

Wei. Integral human pose regression. In ECCV, 2018.

[30] Supasorn Suwajanakorn, Noah Snavely, Jonathan Tomp-
son, and Mohammad Norouzi. Discovery of latent 3d key-
points via end-to-end geometric reasoning. arXiv preprint
arXiv:1807.03146, 2018.

[31] Shubham Tulsiani, Alexei A Efros, and Jitendra Malik.
Multi-view consistency as supervisory signal for learning
shape and pose prediction. In CVPR, 2018.

[32] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-
mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
Learning from synthetic humans. In CVPR, 2017.

[33] Min Wang, Xipeng Chen, Wentao Liu, Chen Qian, Liang
Lin, and Lizhuang Ma. Drpose3d: Depth ranking in 3d
human pose estimation. arXiv preprint arXiv:1805.08973,
2018.

[34] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai,
and Qiang Zhou. Look at boundary: A boundary-aware face
alignment algorithm. In CVPR, 2018.

[35] Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, and
Chen Change Loy. Learning to reenact faces via boundary
transfer. ECCV, 2018.

[36] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and
Honglak Lee. Perspective transformer nets: Learning single-
view 3d object reconstruction without 3d supervision.
In
NIPS, 2016.

10903

[37] Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak
Lee. Weakly-supervised disentangling with recurrent trans-
formations for 3d view synthesis. In NIPS, 2015.

[38] Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren,
Hongsheng Li, and Xiaogang Wang. 3d human pose esti-
mation in the wild by adversarial learning. In CVPR, 2018.

[39] Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view
In

harmonized bilinear network for 3d object recognition.
CVPR, 2018.

[40] Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He,
and Honglak Lee. Unsupervised discovery of object land-
marks as structural representations. In CVPR, 2018.

[41] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-
lik, and Alexei A Efros. View synthesis by appearance ﬂow.
In ECCV, 2016.

[42] Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, and
Yichen Wei. Towards 3d human pose estimation in the wild:
a weakly-supervised approach. In ICCV, 2017.

[43] Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, and
Qixing Huang. Unsupervised domain adaptation for 3d key-
point prediction from a single depth scan. CoRR, 2017.

[44] Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon
Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis.
Monocap: Monocular human motion capture using a CNN
coupled with a geometric prior. CoRR, 2017.

10904

