Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction

Chen-Hsuan Lin1,2* Oliver Wang2 Bryan C. Russell2

Eli Shechtman2

Vladimir G. Kim2 Matthew Fisher2

Simon Lucey1

1The Robotics Institute, Carnegie Mellon University

2Adobe Research

chlin@cmu.edu

{owang,brussell,elishe,vokim,matfishe}@adobe.com

slucey@cs.cmu.edu

https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/

Abstract

In this paper, we address the problem of 3D object mesh
reconstruction from RGB videos. Our approach combines
the best of multi-view geometric and data-driven methods
for 3D reconstruction by optimizing object meshes for multi-
view photometric consistency while constraining mesh de-
formations with a shape prior. We pose this as a piecewise
image alignment problem for each mesh face projection. Our
approach allows us to update shape parameters from the
photometric error without any depth or mask information.
Moreover, we show how to avoid a degeneracy of zero pho-
tometric gradients via rasterizing from a virtual viewpoint.
We demonstrate 3D object mesh reconstruction results from
both synthetic and real-world videos with our photometric
mesh optimization, which is unachievable with either na¨ıve
mesh generation networks or traditional pipelines of surface
reconstruction without heavy manual post-processing.

Figure 1: Our video-aligned object mesh reconstruction
enforcing multi-view consistency while constraining shape
deformations with shape priors, generating an output mesh
with improved geometry with respect to the input views.

1. Introduction

The choice of 3D representation plays a crucial role in 3D
reconstruction problems from 2D images. Classical multi-
view geometric methods, most notably structure from motion
(Sf M) and SLAM, recover point clouds as the underlying
3D structure of RGB sequences, often with very high ac-
curacy [10, 29]. Point clouds, however, lack inherent 3D
spatial structure that is essential for efﬁcient reasoning. In
many scenarios, mesh representations are more desirable –
they are signiﬁcantly more compact since they have inherent
geometric structures deﬁned by point connectivity, while
they also represent continuous surfaces necessary for many
applications such as robotics (e.g., accurate localization for
autonomous driving), computer graphics (e.g., physical sim-
ulation, texture synthesis), and virtual/augmented reality.

Another drawback of classical multi-view geometric
methods is reliance on hand-designed features and can be

*Work done during CHL’s internship at Adobe Research.

fragile when their assumptions are violated. This happens
especially in textureless regions or when there are changes
in illumination. Data-driven approaches [5, 15], on the other
hand, learn priors to tackle ill-posed 3D reconstruction prob-
lems and have recently been widely applied to 3D prediction
tasks from single images. However, they can only reliably re-
construct from the space of training examples it learns from,
resulting in limited ability to generalize to unseen data.

In this work, we address the problem of 3D mesh recon-
struction from image sequences by bringing together the best
attributes of multi-view geometric methods and data-driven
approaches (Fig. 1). Focusing on object instances, we use
shape priors (speciﬁcally, neural networks) to reconstruct
geometry with incomplete observations as well as multi-view
geometric constraints to reﬁne mesh predictions on the input
sequences. Our approach allows dense reconstruction with
object semantics from learned priors, which is not possible
from the traditional pipelines of surface meshing [21] from
multi-view stereo (MVS). Moreover, our approach general-

1969

izes to unseen objects by utilizing multi-view geometry to
enforce observation consistency across viewpoints.

Given only RGB information, we achieve mesh recon-
struction from image sequences by photometric optimization,
which we pose as a piecewise image alignment problem of
individual mesh faces. To avoid degeneracy, we introduce
a novel virtual viewpoint rasterization to compute photo-
metric gradients with respect to mesh vertices for 3D align-
ment, allowing the mesh to deform to the observed shape.
A main advantage of our photometric mesh optimization is
its non-reliance on any a-priori known depth or mask infor-
mation [19, 33, 36] – a necessary condition to be able to
reconstruct objects from real-world images. With this, we
take a step toward practical usage of prior-based 3D mesh
reconstruction aligned with RGB sequences.

In summary, we present the following contributions:

• We incorporate multi-view photometric consistency
with data-driven shape priors for optimizing 3D meshes
using 2D photometric cues.

• We propose a novel photometric optimization formu-
lation for meshes and introduce a virtual viewpoint
rasterization step to avoid gradient degeneracy.

Finally, we show 3D object mesh reconstruction results from
both synthetic and real-world sequences, unachievable with
either na¨ıve mesh generators or traditional MVS pipelines
without heavy manual post-processing.

2. Related Work

Our work on object mesh reconstruction touches several
areas, including multi-view object reconstruction, mesh opti-
mization, deep shape priors, and image alignment.

Multi-view object reconstruction. Multi-view calibration
and reconstruction is a well-studied problem. Most ap-
proaches begin by estimating camera coordinates using 2D
keypoint matching, a process known as SLAM [10, 28] or
SfM [12, 30], followed by dense reconstruction methods
such as MVS [13] and meshing [21]. More recent works
using deep learning have explored 3D reconstruction from
multiple-view consistency between various forms of 2D ob-
servations [23, 32, 33, 36, 39]. These methods all utilize
forms of 2D supervision that are easier to acquire than 3D
CAD models, which are relatively limited in quantity. Our
approach uses both geometric and image-based constraints,
which allows it to overcome common multi-view limitations
such as missing observations and textureless regions.

Mesh optimization. Mesh optimization dates back to clas-
sical works of Active Shape Models [7] and Active Appear-
ance Models [6, 27], which uses 2D meshes to ﬁt facial
landmarks. In this work, we optimize for 3D meshes using
2D photometric cues, a signiﬁcantly more challenging prob-

lem due to the inherent ambiguities in the task. Similar ap-
proaches for mesh reﬁnement have also been explored [8, 9];
however, a sufﬁciently good initialization is required with
very small vertex perturbations allowed. As we show in our
experiments, we are able to handle larger amount of noise
perturbation by optimizing over a latent shape code instead
of mesh vertices, making it more suitable for practical uses.
Several recent methods have addressed learning 3D re-
construction with mesh representations. AtlasNet [15] and
Pixel2Mesh [34] are examples of learning mesh object re-
constructions from 3D CAD models. Meanwhile, Neural
Mesh Renderer [20] suggested a method of mesh reconstruc-
tion via approximate gradients for 2D mask optimization,
and Kanazawa et al. [19] further advocated learning mesh
reconstruction from 2D supervision of textures, masks, and
2D keypoints. Our approach, in contrast, does not assume
any availability of masks or keypoints and operates purely
via photometric cues across viewpoints.

Shape priors. The use of neural networks as object pri-
ors for reconstruction has recently been explored with point
clouds [40]. However, it requires object masks as addi-
tional constraints during optimization. We eliminate the
need for mask supervision by regularizing the latent code.
Shape priors have also been explored for ﬁnding shape corre-
spondences [14], where the network learns the deformation
ﬁeld from a template shape to match 3D observations. In
our method, we directly optimize the latent shape code to
match 2D cues from multiple viewpoints and do not require
a known shape template for the object. A plane and primitive
prior has been used for the challenging task of multi-view
scene reconstruction [17]. Although the primitive prior does
not need to be learned from an object dataset, the resulting re-
construction can differ signiﬁcantly from the target geometry
when it is not well represented by the chosen primitives.

Image alignment. The most generic form of image align-
ment refers to prediction of inherent geometric misalignment
between a pair of images. Image alignment using simple
warping functions can be dated back to the seminal Lucas-
Kanade algorithm [26] and its recent variants [1, 25]. Recent
work has also explored learning a warp function to align
images from neural networks for applications such as novel
view synthesis [37, 38] and learning invariant representa-
tions [18, 24]. In this work, we pose our problem of mesh
optimization as multiple image alignment problems of mesh
faces, and solve it by optimizing over a latent code from a
deep network rather than the vertices themselves.

3. Approach

We seek to reconstruct a 3D object mesh from an RGB
sequence {(If , Ωf )}, where each frame If is associated
with a camera matrix Ωf . In this work, we assume that
the camera matrices {Ωf } can be readily obtained from off-

970

We deﬁne our optimization problem as follows: given the
RGB image sequence and cameras {(If , Ωf )}, we optimize
a regularized cost consisting of a photometric loss Lphoto for
all pairs of frames over the representation z, formulated as

z X
min

a6=b

Lphoto(Ia, Ib, Ωa, Ωb; G(z)) + Lreg(z) ,

(1)

where Lreg is a regularization term on z. This objective
allows the generated mesh to deform with respect to an
effective shape prior. We describe each term in detail next.

3.2. Piecewise Image Alignment

Optimizing the mesh M with the photometric loss Lphoto
is based on the assumption that a dense 2D projection of the
individual triangular faces of a 3D mesh should be globally
consistent across multiple viewpoints. Therefore, we cast
the problem of 3D mesh alignment to the input views as a
collection of piecewise 2D image alignment subproblems of
each projected triangular face (Fig. 2).

To perform piecewise 2D image alignment between Ia
and Ib, we need to establish pixel correspondences. We ﬁrst
denote Vj(z) ∈ R3×3 as the 3D vertices of triangle j in
mesh M = G(z), deﬁned as column vectors. From triangle
j, we can sample a collection of 3D points Pj = {pi(z)}
that lie within triangle j, related via pi(z) = Vj(z)αi
through the barycentric coordinates αi. For a camera Ω,
let π : R3 → R2 be the projection function mapping a world
3D point pi(z) to 2D image coordinates. The pixel intensity
error between the two views Ωa and Ωb can be compared
at the 2D image coordinates corresponding to the projected
sampled 3D points. We formulate the photometric loss Lphoto
as the sum of ℓ1 distances between pixel intensities at these
2D image coordinates over all triangular faces,

Lphoto(Ia, Ib, Ωa, Ωb; G(z))

(2)

= X

X

j

i:pi∈Pj

kIa (π (pi(z); Ωa)) − Ib (π (pi(z); Ωb))k1 .

As such, we can optimize the photometric loss Lphoto with
pixel correspondences established as a function of z.

Visibility. As a 3D point pi may not be visible in a given
view due to possible object self-occlusion, we handle visibil-
ity by constraining Pj to be the set of samples in triangle j
whose projection is visible in both views. We achieve this
by returning a mesh index map using mesh rasterization, a
standard operation in computer graphics, for each optimiza-
tion step. The photometric gradients of each sampled point
∂pi
∂xi
∂I
in turn backpropagate to the vertices Vj .
∂Vj
∂Vj
∂pi
We obtain ∂I
through differentiable image sampling [18],
∂xi
by taking the derivative of the projection π, and ∂pi
∂xi
∂pi
∂Vj
by associating with the barycentric coordinates αi. We note
that the entire process is differentiable and does not resort to
approximate gradients [20].

= ∂I
∂xi

971

Figure 2: Overview. We perform 3D mesh reconstruction
via piecewise image alignment of triangles to achieve per-
triangle visibility-aware photometric consistency across mul-
tiple views, with mesh vertices optimized over the latent
code of a shape prior learned by deep neural networks.

the-shelf Sf M methods [30]. Fig. 2 provides an overview
– we optimize for object meshes that maximize multi-view
photometric consistency over a shape prior, where we use a
pretrained mesh generator. We focus on triangular meshes
here although our method is applicable to any mesh type.

3.1. Mesh Optimization over Shape Prior

Direct optimization on a 3D mesh M with N vertices
involves solving for 3N degrees of freedom (DoFs) and typi-
cally becomes underconstrained when N is large. Therefore,
reducing the allowed DoFs is crucial to ensure mesh defor-
mations are well-behaved during optimization. We wish to
represent the mesh M = G(z) as a differentiable function G
of a reduced vector representation z.

We propose to use an off-the-shelf generative neural net-
work as the main part of G and reparameterize the mesh with
an associated latent code z ∈ RK , where K ≪ 3N . The net-
work serves as an object shape prior whose efﬁcacy comes
from pretraining on external shape datasets. Shape priors
over point clouds have been previously explored [40]; here,
we extend to mesh representations. We use AtlasNet [15]
here although other mesh generators are also applicable. The
shape prior allows the predicted 3D mesh to deform within a
learned shape space, avoiding many local minima that exist
with direct vertex optimization. To utilize RGB information
from the given sequence for photometric optimization, we
further add a 3D similarity transform to map the generated
mesh to world cameras recovered by Sf M (see Sec. 3.4).

codeFigure 4: Sample sequences composited from ShapeNet
renderings (top: car, bottom: airplane) and SUN360 scenes.

Figure 3: Visualization of the photometric loss Lphoto be-
tween the synthesized appearances at virtual viewpoints ΩV
starting from input images Ia and Ib. The photometric loss
Lphoto encourages consistent appearance syntheses from both
input viewpoints Ωa and Ωb.

by Sf M. Therefore, we need to account for an additional
3D similarity transform T (·) applied to the mesh vertices.
For each 3D vertex v′
k from the prediction, we deﬁne the
similarity transform as

vk = T (v′

k; θ) = exp(s) · R(ω)v′

k + t ∀k ,

(4)

3.3. Virtual Viewpoint Rasterization

We can efﬁciently sample a large number of 3D points
Pj in triangle j by rendering the depth of M from a given
view using mesh rasterization (Sec. 3.2). If the depth were
rasterized from either input view Ωa or Ωb, however, we
would obtain zero photometric gradients. This degeneracy
arises due to the fact that ray-casting from one view and
projecting back to the same view results in ∂I
∂Vj

= 0.

To elaborate, we ﬁrst note that depth rasterization of trian-
gle j is equivalent to back-projecting regular grid coordinates
¯xi to triangle j. We can express each depth point from cam-
era Ω ∈ {Ωa, Ωb} as pi(z) = π−1 (¯xi; Vj(z), Ω), where
π−1 : R2 → R3 is the inverse projection function realized by
solving for ray-triangle intersection with Vj(z). Combining
with the projection equation, we have

xi(z, Ω) = π (cid:0)π−1 (¯xi; Vj(z), Ω) ; Ω(cid:1) = ¯xi ∀¯xi ,

(3)

becoming the identity mapping and losing the dependency of
xi on Vj(z), which in turn leads to ∂xi
= 0. This insight
∂Vj
is in line with the recent observation from Ham et al. [16].
To overcome this degeneracy, we rasterize the depth from
a third virtual viewpoint ΩV /∈ {Ωa, Ωb}. This step allows
correct gradients to be computed in both viewpoints Ωa and
Ωb, which is essential to maintain stability during optimiza-
tion. We can form the photometric loss by synthesizing the
image appearance at ΩV using the pixel intensities from
both Ωa and Ωb (Fig. 3). We note that ΩV can be arbitrar-
ily chosen. In practice, we choose ΩV to be the bisection
between Ωa and Ωb by applying Slerp [31] on the rotation
quaternions and averaging the two camera centers.

3.4. Implementation Details

Coordinate systems. Mesh predictions from a genera-
tive network typically lie in a canonical coordinate sys-
tem [15, 34] independent of the world cameras recovered

where θ = [s; ω; t] ∈ R7 are the parameters and R is a
3D rotation matrix parameterized with the so(3) Lie algebra.
We optimize for z = [z′; θ] together, where z′ is the latent
code associated with the generative network.

Since automated registration of noisy 3D data with un-
known scales is still an open problem, we assume a coarse
alignment of the coordinate systems can be computed from
minimal annotation of rough correspondences (see Sec. 4.3
for details). We optimize for the similarity transform to more
accurately align the meshes to the RGB sequences.

Regularization. Despite neural networks being effective
priors, the latent space is only spanned by the training data.
To avoid meshes from reaching a degenerate solution, we im-
pose an extra penalty on the latent code z′ to ensure it stays
within a trust region of the initial code z0 (extracted from a
pretrained image encoder), deﬁned as Lcode = kz′ − z0k2
2.
We also add a scale penalty Lscale = −s that encourages
the mesh to expand, since the mesh shrinking to inﬁnites-
imal is a trivial solution with zero photometric error. The
regularization Lreg in cost (1) is written as

Lreg(z) = λcode · Lcode(z′) + λscale · Lscale(θ)

(5)

where λcode and λscale are the penalty weights.

4. Experiments

We evaluate the performance of our method on a single
(Sec. 4.1) and multiple (Sec. 4.2) object categories with
synthetic data as well as real-world videos (Sec. 4.3).

Data preparation. We create datasets of 3D CAD model
renderings for training a mesh generation network and eval-
uating our optimization framework. Our rendering pipeline
aims to create realistic images with complex backgrounds so
they could be applied to real-world video sequences. We use

972

photometric errormesh ()warped towarped tomesh ()ShapeNet [3] for the object dataset and normalize all objects
to ﬁt an origin-centered unit sphere. We render RGB im-
ages of each object using perspective cameras at 24 equally
spaced azimuth angles and 3 elevation angles.

To simulate realistic backgrounds, we randomly warp
and crop spherical images from the SUN360 database [35]
to create background images of the same scene taken at
different camera viewpoints. By compositing the foreground
and background images together at corresponding camera
poses, we obtain RGB sequences of objects composited on
realistic textured backgrounds (Fig. 4). Note that we do
not keep any mask information that was accessible in the
rendering and compositing process as such information is
typically not available in real-world examples. All images
are rendered/cropped at a resolution of 224×224.

Shape prior. We use AtlasNet [15] as the base network
architecture for mesh generation, which we retrain on our
new dataset. We use the same 80%-20% training/test split
from Groueix et al. [15] and additionally split the SUN360
spherical images with the same ratio. During training, we
augment background images at random azimuth angles.

Initialization. We initialize the code z0 by encoding an
RGB frame with the AtlasNet encoder. For ShapeNet se-
quences, we choose frames with objects facing 45° sideways.
For real-world sequences, we manually select frames where
objects are center-aligned to the images as much as possible
to match our rendering settings. We initialize the similarity
transform parameters to θ = 0 (identity transform).

Evaluation criteria. We evaluate the result by measuring
the 3D distances between the sampled 3D points from the
predicted meshes and the ground-truth point clouds [15]. We
follow Lin et al. [23] by reporting the 3D error between
the predicted and ground-truth point clouds as η(S1, S2) =
Pi:vi∈S1 minvj ∈S2 kvi − vjk2 for some source and target
point sets S1 and S2, respectively. This metric measures the
prediction shape accuracy when S1 is the prediction and S2
is the ground truth, while it indicates the prediction shape
coverage when vice versa. We report quantitative results in
both directions separately averaged across all instances.

4.1. Single Object Category

We start by evaluating our mesh alignment in a category-
speciﬁc setting. We select the car, chair, and plane categories
from ShapeNet, consisting of 703, 1356, and 809 objects
in our test split, respectively. For each object, we create an
RGB sequence by overlaying its rendering onto a randomly
paired SUN360 scene with the cameras in correspondence.
We retrain each category-speciﬁc AtlasNet model on our new
dataset using the default settings for 500 epochs. During op-
timization, we use the Adam optimizer [22] with a constant
learning rate of 0.003 for 100 iterations. We manually set
the penalty factors to be λcode = 0.05 and λscale = 0.02.

One challenge is that the coordinate system for a mesh
generated by AtlasNet is independent of the recovered world
cameras {Ωf } for a real-world sequence. Determining such
coordinate system mapping (deﬁned by a 3D similarity trans-
form) is required to relate the predicted mesh to the world.
On the other hand, for the synthetic sequences, we know the
exact mapping as we can render the views for AtlasNet and
the input views {If } in the same coordinate system.

For our ﬁrst experiment, we simulate the possibly incor-
rect mapping estimates by perturbing the ground-truth 3D
similarity transform by adding Gaussian noise ε ∼ N (0, σI)
to its parameters, pre-generated per sequence for evaluation.
We evaluate the 3D error metrics under such perturbations.
Note that our method utilizes no additional information other
than the RGB information from the given sequences.

We compare our mesh reconstruction approach against
three baseline variants of AtlasNet: (a) mesh generations
from a single-image feed-forward initialization, (b) gener-
ation from the mean latent code averaged over all frames
in the sequence, and (c) the mean shape where vertices are
averaged from the mesh generation across all frames.

We show qualitative results in Fig. 5 (compared under per-
turbation σ = 0.12). Our method is able to take advantage
of multi-view geometry to resolve large misalignments and
optimize for more accurate shapes. The high photometric
error from the background between views discourages mesh
vertices from staying in such regions. This error serves as
a natural force to constrain the mesh within the desired 3D
regions, eliminating the need of depth or mask constraints
during optimization. We further visualize our mesh recon-
struction with textures that are estimated from all images
(Fig. 6). Note that the ﬁdelity of mean textures increases
while variance in textures decrease after optimization.

We evaluate quantitatively in Fig. 7, where we plot the
average 3D error over mapping noise. This result demon-
strates how our method handles inaccurate coordinate system
mappings to successfully match the meshes against RGB
sequences. We also ablate optimizing the latent code z,
showing that allowing shape deformation improves recon-
struction quality over a sole 3D similarity transform (“ﬁxed
code” in Fig. 7). Note that our method is slightly worse
in shape coverage error (GT→pred.) when evaluated at the
ground-truth mapping. This result is attributed to the limi-
tation of photometric optimization that opts for degenerate
solutions when objects are insufﬁciently textured.

4.2. Multiple Object Categories

We extend beyond a model that reconstructs a single
object category by training a single model to reconstruct
multiple object categories. We take 13 commonly chosen
CAD model categories from ShapeNet [5, 11, 15, 23]. We
follow the same settings as in Sec. 4.1 except we retrain
AtlasNet longer for 1000 epochs due to a larger training set.

973

Figure 5: Qualitative results from category-speciﬁc models, where we visualize two sample frames from each test sequence.
Our method better aligns initial meshes to the RGB sequences while optimizing for more subtle shape details (e.g., car spoilers
and airplane wings) over baselines. The meshes are color-coded by surface normals with occlusion boundaries drawn.

Category

plane bench cabin.

car

chair monit.

lamp speak.

ﬁre.

couch table

cell. water. mean

3.872 4.931 5.708 4.269 4.869 4.687 8.684 7.245 3.864 5.017 4.964 4.571 4.290 5.152
AtlasNet (single)
3.746 4.496 5.600 4.286 4.571 4.634 7.366 6.976 3.632 4.798 4.903 4.286 3.860 4.858
AtlasNet (mean code)
AtlasNet (mean shape) 3.659 4.412 5.382 4.192 4.499 4.424 7.200 6.683 3.547 4.606 4.860 4.196 3.742 4.723
0.704 1.821 2.850 0.597 1.441 1.115 8.855 3.430 1.255 0.983 1.725 1.599 1.743 2.163
Ours

Category

plane bench cabin.

car

chair monit.

lamp

speak.

ﬁre.

couch table

cell. water. mean

(a) 3D error: prediction → ground truth (shape accuracy).

4.430 4.895 5.024 4.461 4.896 4.640
AtlasNet (single)
AtlasNet (mean code)
4.177 4.507 4.962 4.384 4.635 4.143
AtlasNet (mean shape) 4.464 4.915 5.150 4.521 4.940 4.560
Ours

6.994 4.407 4.613 5.350 4.254 4.263 5.164
6.990 4.307 4.463 5.084 4.036 3.718 4.823
7.308 4.528 4.707 5.255 4.299 4.183 5.153
2.237 3.215 1.927 0.734 2.377 2.119 10.764 4.152 2.583 1.735 6.126 1.851 2.926 3.288

8.906
7.292
8.159

(b) 3D error: ground truth → prediction (shape coverage).

Table 1: Average 3D test error for general object categories (numbers scaled by 103). The mean is taken across categories. Our
optimization method is effective on most object categories. Note that our method improves on accuracy of the table category
despite worsening in shape coverage due to insufﬁcient textures in object samples.

We show visual results in Fig. 8 on the efﬁcacy of our
method for multiple object categories (under perturbation
σ = 0.12). Our results show how we can reconstruct a shape
that better matches our RGB observations (e.g., reﬁning hol-
low regions, as in the bench backs and table legs). We also
show category-wise quantitative results in Table 1, compared
under perturbation noise σ = 0.06. We ﬁnd photometric

optimization to perform effectively across most categories
except lamps, which consist of many examples where opti-
mizing for thin structures is hard for photometric loss.

4.3. Real world Videos

Finally, we demonstrate the efﬁcacy of our method on
challenging real-world video sequences orbiting an object.

974

RGBsequenceAtlasNet(single)AtlasNet(mean code)AtlasNet(mean vertices)OursRGBsequenceAtlasNet(single)AtlasNet(mean code)AtlasNet(mean vertices)OursFigure 6: Mesh visualization with textures computed by
averaging projections across all viewpoints. Our method
successfully reduces variance and recovers dense textures
that can be embedded on the surfaces.

Figure 7: Category-speciﬁc performance to noise in coor-
dinate system mapping. Our method is able to resolve for
various extents of mesh misalignments from the sequence.

We use a dataset of RGB-D object scans [4], where we use
the chair model to evaluate on the chair category. We select
the subset of video sequences that are 3D-reconstructible
using traditional pipelines [30] and where Sf M extracts at
least 20 reliable frames and 100 salient 3D points. We retain
82 sequences with sufﬁcient quality for evaluation. We
rescale the sequences to 240 × 320 and skip every 10 frames.
We compute the camera extrinsic and intrinsic matrices
using off-the-shelf Sf M with COLMAP [30]. For evaluation,
we additionally compute a rough estimate of the coordinate
system mapping by annotating 3 corresponding points be-
tween the predicted mesh and the sparse points extracted
from Sf M (Fig. 9), which allows us to ﬁt a 3D similarity
transform. We optimize using Adam with a learning rate
of 2e-3 for 200 iterations, and we manually set the penalty
factors to be λcode = 0.05 and λscale = 0.01.

Figure 8: Qualitative results for general object categories.
Our optimization method recovers subtle details such as back
of benches, watercraft sails, and even starts to reveal cabinet
open spaces which were initially occluded. Our method
tends to fail more frequently with textureless objects (e.g.,
cellphone and ﬁrearm).

Figure 9: We select 3 correspondences between (a) the mesh
vertices and (b) the Sf M points to ﬁnd (c) an estimated coor-
dinate system mapping by ﬁtting a 3D similarity transform.
(d) Alignment result after our photometric optimization.

975

Initial.Afteroptim.meanvariancemeanvariance0.000.030.060.090.12perturbation noise ()03691215183D error (× 0.001)car (pred.GT)AtlasNet (single)AtlasNet (mean code)AtlasNet (mean shape)OursOurs (fixed code)0.000.030.060.090.12perturbation noise ()03691215183D error (× 0.001)car (GTpred.)AtlasNet (single)AtlasNet (mean code)AtlasNet (mean shape)OursOurs (fixed code)0.000.030.060.090.12perturbation noise ()03691215183D error (× 0.001)chair (pred.GT)AtlasNet (single)AtlasNet (mean code)AtlasNet (mean shape)OursOurs (fixed code)0.000.030.060.090.12perturbation noise ()03691215183D error (× 0.001)chair (GTpred.)AtlasNet (single)AtlasNet (mean code)AtlasNet (mean shape)OursOurs (fixed code)0.000.030.060.090.12perturbation noise ()03691215183D error (× 0.001)plane (pred.GT)AtlasNet (single)AtlasNet (mean code)AtlasNet (mean shape)OursOurs (fixed code)0.000.030.060.090.12perturbation noise ()03691215183D error (× 0.001)plane (GTpred.)AtlasNet (single)AtlasNet (mean code)AtlasNet (mean shape)OursOurs (fixed code)RGBsequenceAtlasNet(init.)OurscabinetbenchmonitorspeakerlampcouchfirearmtablewatercraftcellphoneRGBsequenceAtlasNet(init.)Ours(a)(b)(c)(d)Figure 10: Qualitative results on real-world sequences. Given an initialization, our method accurately aligns a generated mesh
to an RGB video. Even when the initial mesh is an inaccurate prediction of the real object, our method is still able to align the
semantic parts (bottom left). We show failure cases in the last two examples in the bottom right, where there is insufﬁcient
background texture as photometric cues and where the initial mesh is insufﬁcient to capture the thin structures. We also show
the result of a traditional reconstruction pipeline [30] after manual cleanup. Due to the difﬁculty of the problem these meshes
still often have many undesirable artifacts.

Dist.

Initial. Optim.

1
2
3
4
6

6.504
9.064
10.984
12.479
14.718

4.990
6.979
8.528
9.788
11.665

Table 2: Average pixel repro-
jection error (scaled by 100)
from real-world videos as a
function of frame distances.

Figure 11: Metric-scale
depth error before and af-
ter optimization (with Sf M
world cameras rescaled).

We demonstrate how our method is applicable to real-
world datasets in Fig. 10. Our method is able to reﬁne
shapes such as armrests and ofﬁce chair legs. Note that our
method is sensitive to the quality of mesh initialization from
real images, mainly due to the domain mismatch between
synthetic and real data during the training/test phases of the
shape prior. Despite this, it is still able to straighten and align
to the desired 3D location. In addition, we report the average
pixel reprojection error in Table 2 and metric depth error in
Fig. 11 to quantify the effect of photometric optimization,
which shows further improvement over coarse initializations.

Finally, we note that surface reconstruction is a chal-
lenging post-processing procedure for traditional pipelines.
Fig. 10 shows sample results for Sf M [30], PatchMatch
Stereo [2], stereo fusion, and Poisson mesh reconstruc-
tion [21] from COLMAP [30]. In addition to the need of
accurate object segmentation, the dense meshing problem
with traditional pipelines typically yields noisy results with-
out laborious manual post-processing.

5. Conclusion

We have demonstrated a method for reconstructing a 3D
mesh from an RGB video by combining data-driven deep
shape priors with multi-view photometric consistency opti-
mization. We also show that mesh rasterization from a vir-
tual viewpoint is critical for avoiding degenerate photometric
gradients during optimization. We believe our photometric
mesh optimization technique has merit for a number of prac-
tical applications. It enables the ability to generate more
accurate models of real-world objects for computer graphics
and potentially allows automated object segmentation from
video data. It could also beneﬁt 3D localization for robot
navigation and autonomous driving, where accurate object
location, orientation, and shape from real-world cameras is
crucial for more efﬁcient understanding.

976

RGBsequenceAtlasNet(init.)OursRGBsequenceAtlasNet(init.)OursTraditional pipelineTraditional pipeline0100200depth error (mm)0255075100percentage (%)InitializationAfter optim.References

[1] Simon Baker and Iain Matthews. Lucas-kanade 20 years on:
A unifying framework. International journal of computer
vision, 56(3):221–255, 2004. 2

[2] Michael Bleyer, Christoph Rhemann, and Carsten Rother.
Patchmatch stereo-stereo matching with slanted support win-
dows. In British Machine Vision Conference, 2011. 8

[3] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis
Savva, Shuran Song, Hao Su, et al. Shapenet: An information-
rich 3d model repository. arXiv preprint arXiv:1512.03012,
2015. 5

[4] Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen
Koltun. A large dataset of object scans. arXiv preprint
arXiv:1602.02481, 2016. 7

[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for
single and multi-view 3d object reconstruction. In European
conference on computer vision, pages 628–644. Springer,
2016. 1, 5

[6] Timothy F Cootes, Gareth J Edwards, and Christopher J Tay-
lor. Active appearance models. IEEE Transactions on Pattern
Analysis & Machine Intelligence, 2001. 2

[7] Timothy F Cootes and Christopher J Taylor. Active shape
modelssmart snakes. In BMVC92, pages 266–275. Springer,
1992. 2

[8] Ama¨el Delaunoy and Marc Pollefeys. Photometric bundle
adjustment for dense multi-view 3d modeling. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1486–1493, 2014. 2

[9] Ama¨el Delaunoy and Emmanuel Prados. Gradient ﬂows for
optimizing triangular mesh-based surfaces: Applications to
3d reconstruction problems dealing with visibility. Interna-
tional journal of computer vision, 95(2):100–123, 2011. 2

[10] Jakob Engel, Thomas Sch¨ops, and Daniel Cremers. Lsd-slam:
Large-scale direct monocular slam. In European Conference
on Computer Vision, pages 834–849. Springer, 2014. 1, 2

[11] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3d object reconstruction from a single
image. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 5

[12] Jorge Fuentes-Pacheco, Jos´e Ruiz-Ascencio, and Juan Manuel
Rend´on-Mancha. Visual simultaneous localization and map-
ping: a survey. Artiﬁcial Intelligence Review, 43(1):55–81,
2015. 2

[13] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and
robust multiview stereopsis. IEEE transactions on pattern
analysis and machine intelligence, 32(8):1362–1376, 2010. 2

[14] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C
Russell, and Mathieu Aubry. 3d-coded: 3d correspondences
by deep deformation. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 230–246, 2018.
2

[15] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C
Russell, and Mathieu Aubry. Atlasnet: A papier-mˆach´e ap-
proach to learning 3d surface generation. In IEEE Conference

on Computer Vision and Pattern Recognition (CVPR), 2018.
1, 2, 3, 4, 5

[16] Christopher Ham, Simon Lucey, and Surya Singh. Proxy
templates for inverse compositional photometric bundle ad-
justment. arXiv preprint arXiv:1704.06967, 2017. 4

[17] Jingwei Huang, Angela Dai, Leonidas Guibas, and Matthias
Nießner. 3dlite: Towards commodity 3d scanning for content
creation. ACM Transactions on Graphics 2017 (TOG), 2017.
2

[18] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in Neural Infor-
mation Processing Systems, pages 2017–2025, 2015. 2, 3

[19] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and Ji-
tendra Malik. Learning category-speciﬁc mesh reconstruction
from image collections. In ECCV, 2018. 2

[20] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-
ral 3d mesh renderer. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 2, 3

[21] Michael Kazhdan and Hugues Hoppe. Screened poisson
surface reconstruction. ACM Transactions on Graphics (ToG),
32(3):29, 2013. 1, 2, 8

[22] Diederik Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 5

[23] Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning
efﬁcient point cloud generation for dense 3d object reconstruc-
tion. In AAAI Conference on Artiﬁcial Intelligence (AAAI),
2018. 2, 5

[24] Chen-Hsuan Lin and Simon Lucey. Inverse compositional
spatial transformer networks. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017. 2

[25] Chen-Hsuan Lin, Rui Zhu, and Simon Lucey. The conditional
lucas & kanade algorithm. In European Conference on Com-
puter Vision (ECCV), pages 793–808. Springer International
Publishing, 2016. 2

[26] Bruce D. Lucas and Takeo Kanade. An iterative image reg-
istration technique with an application to stereo vision. In
Proceedings of the 7th International Joint Conference on Arti-
ﬁcial Intelligence - Volume 2, IJCAI’81, pages 674–679, 1981.
2

[27] Iain Matthews and Simon Baker. Active appearance models
revisited. International journal of computer vision, 60(2):135–
164, 2004. 2

[28] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D
Tardos. Orb-slam: a versatile and accurate monocular slam
system. IEEE Transactions on Robotics, 31(5):1147–1163,
2015. 2

[29] Richard A Newcombe, Steven J Lovegrove, and Andrew J
Davison. Dtam: Dense tracking and mapping in real-time.
In 2011 international conference on computer vision, pages
2320–2327. IEEE, 2011. 1

[30] Johannes Lutz Sch¨onberger and Jan-Michael Frahm.
Structure-from-motion revisited. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2016. 2, 3, 7, 8

[31] Ken Shoemake. Animating rotation with quaternion curves.

In ACM SIGGRAPH computer graphics. ACM, 1985. 4

977

[32] Shubham Tulsiani, Alexei A. Efros, and Jitendra Malik. Multi-
view consistency as supervisory signal for learning shape and
pose prediction. In Computer Vision and Pattern Recognition
(CVPR), 2018. 2

[33] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jiten-
dra Malik. Multi-view supervision for single-view reconstruc-
tion via differentiable ray consistency. In Computer Vision
and Pattern Recognition (CVPR), 2017. 2

[34] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu,
Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating
3d mesh models from single rgb images. arXiv preprint
arXiv:1804.01654, 2018. 2, 4

[35] Jianxiong Xiao, Krista A Ehinger, Aude Oliva, and Antonio
Torralba. Recognizing scene viewpoint using panoramic place
representation. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 2695–2702. IEEE,
2012. 5

[36] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and
Honglak Lee. Perspective transformer nets: Learning single-
view 3d object reconstruction without 3d supervision.
In
Advances in Neural Information Processing Systems, pages
1696–1704, 2016. 2

[37] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, 2017. 2

[38] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik,
and Alexei A Efros. View synthesis by appearance ﬂow. In
European Conference on Computer Vision, pages 286–301.
Springer, 2016. 2

[39] Rui Zhu, Hamed Kiani Galoogahi, Chaoyang Wang, and Si-
mon Lucey. Rethinking reprojection: Closing the loop for
pose-aware shape reconstruction from a single image. In Com-
puter Vision (ICCV), 2017 IEEE International Conference on,
pages 57–65. IEEE, 2017. 2

[40] Rui Zhu, Chaoyang Wang, Chen-Hsuan Lin, Ziyan Wang, and
Simon Lucey. Object-centric photometric bundle adjustment
with deep shape prior. In 2018 IEEE Winter Conference on
Applications of Computer Vision (WACV), pages 894–902.
IEEE, 2018. 2, 3

978

