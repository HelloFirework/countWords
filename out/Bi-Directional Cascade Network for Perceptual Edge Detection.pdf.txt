Bi-Directional Cascade Network for Perceptual Edge Detection

Jianzhong He1,Shiliang Zhang1,Ming Yang2,Yanhu Shan2,Tiejun Huang1,3

1Peking University, 2Horizon Robotics, Inc, 3Peng Cheng Laboratory.

{jianzhonghe,slzhang.jdl,tjhuang}@pku.edu.cn,m-yang4@u.northwestern.edu, yanhu.shan@horizon.ai

Abstract

Exploiting multi-scale representations is critical to im-
prove edge detection for objects at different scales. To ex-
tract edges at dramatically different scales, we propose a
Bi-Directional Cascade Network (BDCN) structure, where
an individual layer is supervised by labeled edges at its spe-
ciﬁc scale, rather than directly applying the same supervi-
sion to all CNN outputs. Furthermore, to enrich multi-scale
representations learned by BDCN, we introduce a Scale En-
hancement Module (SEM) which utilizes dilated convolu-
tion to generate multi-scale features, instead of using deep-
er CNNs or explicitly fusing multi-scale edge maps. These
new approaches encourage the learning of multi-scale rep-
resentations in different layers and detect edges that are
well delineated by their scales. Learning scale dedicated
layers also results in compact network with a fraction of
parameters. We evaluate our method on three datasets, i.e.,
BSDS500, NYUDv2, and Multicue, and achieve ODS F-
measure of 0.828, 1.3% higher than current state-of-the art
on BSDS500. The code has been available1.

1. Introduction

Edge detection targets on extracting object boundaries
and perceptually salient edges from natural images, which
preserve the gist of an image and ignore unintended detail-
s. Thus, it is important to a variety of mid- and high-level
vision tasks, such as image segmentation [1, 41], object de-
tection and recognition [13, 14], etc. Thanks to research
efforts ranging from exploiting low-level visual cues with
hand-crafted features [4, 22, 1, 28, 10] to recent deep mod-
els [3, 30, 23, 47], the accuracy of edge detection has been
signiﬁcantly boosted. For example, on the Berkeley Seg-
mentation Data Set and Benchmarks 500 (BSDS500) [1],
the detection performance has been boosted from 0.598 [7]
to 0.815 [47] in ODS F-measure.

Nevertheless, there remain some open issues worthy of
studying. As shown in Fig. 1, edges in one image stem

1https://www.pkuvmc.com/dataset.html.

Figure 1. Some images and their ground truth edge maps in BSD-
S500 dataset. The scale of edges in one image varies considerably,
like the boundaries of human body and hands.

from both object-level boundaries and meaningful local de-
tails, e.g., the silhouette of human body and the shape of
hand gestures. The variety of scale of edges makes it cru-
cial to exploit multi-scale representations for edge detec-
tion. Recent neural net based methods [2, 42, 49] utilize hi-
erarchal features learned by Convolutional Neural Network-
s (CNN) to obtain multi-scale representations. To generate
more powerful multi-scale representation, some researchers
adopt very deep networks, like ResNet50 [18], as the back-
bone model of the edge detector. Deeper models generally
involve more parameters, making the network hard to train
and expensive to infer. Another way is to build an image
pyramid and fuse multi-level features, which may involve
redundant computations.
In another word, can we use a
shallow or light network to achieve a comparable or even
better performance?

Another issue is about the CNN training strategy for
edge detection, i.e., supervising predictions of different net-
work layers by one general ground truth edge map [49, 30].
For instance, HED [49, 50] and RCF [30] compute edge
prediction on each intermediate CNN output to spot edges
at different scales, i.e., the lower layers are expected to de-
tect more local image patterns while higher layers capture
object-level information with larger receptive ﬁelds. Since
different network layers attend to depict patterns at different
scales, it is not optimal to train those layers with the same
supervision. In another word, existing works [49, 50, 30]
enforce each layer of CNN to predict edges at all scales and
ignore that one speciﬁc intermeadiate layer can only focus
on edges at certain scales. Liu et al. [31] propose to re-
lax the supervisions on intermediate layers using Canny [4]
detectors with layer-speciﬁc scales. However, it is hard to
decide layer-speciﬁc scales through human intervention.

13828

Groundtruth
Edge Maps

ID
Block

pooling
stride:2

ID
Block

pooling
stride:2

ID
Block

pooling
stride:2

ID
Block

pooling
stride:1

ID
Block

deep to 
shallow

shallow 
to deep

Groundtruth
Edge Maps

Figure 2. The overall architecture of BDCN. ID Block denotes the
Incremental Detection Block, which is the basic component of B-
DCN. Each ID Block is trained by layer-speciﬁc supervisions in-
ferred by a bi-directional cascade structure. This structure trains
each ID Block to spot edges at a proper scale. The predictions of
ID Blocks are fused as the ﬁnal result.

Aiming to fully exploit the multiple scale cues with a
shallow CNN, we introduce a Scale Enhancement Mod-
ule (SEM) which consists of multiple parallel convolutions
with different dilation rates. As shown in image segmen-
tation [5], dilated convolution effectively increases the size
of receptive ﬁelds of network neurons. By involving mul-
tiple dilated convolutions, SEM captures multi-scale spatial
contexts. Compared with previous strategies, i.e., introduc-
ing deeper networks and explicitly fusing multiple edge de-
tections, SEM does not signiﬁcantly increase network pa-
rameters and avoids the repetitive edge detection on image
pyramids.

To address the second issue, each layer in CNN shall be
trained by proper layer-speciﬁc supervision, e.g., the shal-
low layers are trained to focus on meaningful details and
deep layers should depict object-level boundaries. We pro-
pose a Bi-Directional Cascade Network (BDCN) architec-
ture to achieve effective layer-speciﬁc edge learning. For
each layer in BDCN, its layer-speciﬁc supervision is in-
ferred by a bi-directional cascade structure, which propa-
gates the outputs from its adjacent higher and lower layers,
as shown in Fig. 2.
In another word, each layer in BD-
CN predicts edges in an incremental way w.r.t scale. We
hence call the basic block in BDCN, which is constructed
by inserting several SEMs into a VGG-type block, as the In-
cremental Detection Block (ID Block). This bi-directional
cascade structure enforces each layer to focus on a speciﬁc
scale, allowing for a more rational training procedure.

By combining SEM and BDCN, our method achieves
consistent performance on three widely used datasets, i.e.,
BSDS500, NYUDv2, and Multicue.
It achieves ODS F-
measure of 0.828, 1.3% higher than current state-of-the art
CED [47] on BSDS500. It achieves 0.806 only using the
trainval data of BSDS500 for training, and outperforms the
human perception (ODS F-measure 0.803). To our best
knowledge, we are the ﬁrst that outperforms human percep-
tion by training only on trainval data of BSDS500. More-

over, we achieve a better trade-off between model compact-
ness and accuracy than existing methods relying on deeper
models. With a shallow CNN structure, we obtain compara-
ble performance with some well-known methods [3, 42, 2].
For example, we outperform HED [49] using only 1/6 of
its parameters. This shows the validity of our proposed
SEM, which enriches the multi-scale representations in C-
NN. This work is also an original effort studying a rational
training strategy for edge detection, i.e., employing the B-
DCN structure to train each CNN layer with layer-speciﬁc
supervision.

2. Related Work

This work is related to edge detection, multi-scale rep-
resentation learning, and network cascade structure. We
brieﬂy review these three lines of works, respectively.

Edge Detection: Most edge detection methods can be
categorized into three groups, i.e., traditional edge oper-
ators, learning based methods, and the recent deep learn-
ing, respectively. Traditional edge operators [22, 4, 45, 34]
detect edges by ﬁnding sudden changes in intensity, col-
or, texture, etc. Learning based methods spot edges by u-
tilizing supervised models and hand-crafted features. For
example, Doll´ar et al. [10] propose structured edge which
jointly learns the clustering of groundtruth edges and the
mapping of image patch to clustered token. Deep learning
based methods use CNN to extract multi-level hierarchical
features. Bertasius et al. [2] employ CNN to generate fea-
tures of candidate contour points. Xie et al. [49] propose
an end-to-end detection model that leverages the output-
s from different intermediate layers with skip-connections.
Liu et al. [30] further learn richer deep representations by
concatenating features derived from all convolutional lay-
ers. Xu et al. [51] introduce a hierarchical deep model to
extract multi-scale features and a gated conditional random
ﬁeld to fuse them.

Multi-Scale Representation Learning: Extraction and fu-
sion of multi-scale features are fundamental and critical for
many vision tasks, e.g.,
[19, 52, 6]. Multi-scale repre-
sentations can be constructed from multiple re-scaled im-
ages [12, 38, 11], i.e., an image pyramid, either by comput-
ing features independently at each scale [12] or using the
output from one scale as the input to the next scale [38, 11].
Recently, innovative works DeepLab [5] and PSPNet [55]
use dilated convolutions and pooling to achieve multi-scale
feature learning in image segmentation. Chen et al. [6]
propose an attention mechanism to softly weight the multi-
scale features at each pixel location.

Like other image patterns, edges vary dramatically in s-
cales. Ren et al. [39] show that considering multi-scale cues
does improve performance of edge detection. Multiple s-
cale cues are also used in many approaches [48, 39, 24, 50,
30, 34, 51]. Most of those approaches explore the scale-

23829

space of edges, e.g., using Gaussian smoothing at multiple
scales [48] or extracting features from different scaled im-
ages [1]. Recent deep based methods employ image pyra-
mid and hierarchal features. For example, Liu et al. [30]
forward multiple re-scaled images to a CNN independently,
then average the results. Our approaches follow a similar
intuition, nevertheless, we build SEM to learn multi-scale
representations in an efﬁcient way, which avoids repetitive
computation on multiple input images.

Network Cascade: Network cascade [21, 37, 25, 46, 26]
is an effective scheme for many vision applications like
classiﬁcation [37], detection [25], pose estimation [46]
and semantic segmentation [26]. For example, Murthy
et al. [37] treat easy and hard samples with different net-
works to improve classiﬁcation accuracy. Yuan et al. [54]
ensemble a set of models with different complexities to pro-
cess samples with different difﬁculties. Li et al. [26] pro-
pose to classify easy regions in a shallow network and train
deeper networks to deal with hard regions. Lin et al. [29]
propose a top-down architecture with lateral connection-
s to propagate deep semantic features to shallow layers.
Different from previous network cascade, BDCN is a bi-
directional pseudo-cascade structure, which allows an inno-
vative way to supervise each layer individually for layer-
speciﬁc edge detection. To our best knowledge, this is an
early and original attempt to adopt a cascade architecture in
edge detection.

3. Proposed Methods

3.1. Formulation

Let (X, Y ) denote one sample in the training set T,
where X = {xj, j = 1, · · · , |X|} is a raw input image
and Y = {yj, j = 1, · · · , |X|}, yj ∈ {0, 1} is the corre-
sponding groundtruth edge map. Considering the scale of
edges may vary considerably in one image, we decompose
edges in Y into S binary edge maps according to the scale
of their depicted objects, i.e.,

Y =

S

(cid:2)

s=1

Ys,

(1)

where Ys contains annotated edges corresponding to a scale
s. Note that, we assume the scale of edges is in proportion
to the size of their depicted objects.

Our goal is to learn an edge detector D(·) capable of de-
tecting edges at different scales. A natural way to design
D(·) is to train a deep neural network, where different layers
correspond to different sizes of receptive ﬁeld. Speciﬁcally,
we can build a neural network N with S convolutional lay-
ers. The pooling layers make adjacent convolutional layers
depict image patterns at different scales.

For one training image X, suppose the feature map gen-
erated by the s-th convolutional layer is Ns(X) ∈ Rw×h×c.

Using Ns(X) as input, we design a detector Ds(·) to spot
edges at scale s. The training loss for Ds(·) is formulated
as

Ls = (cid:2)

X∈T

|Ps − Ys|,

(2)

where Ps = Ds(Ns(X)) is the edge prediction at scale s.
The ﬁnal detector D(·) hence is derived as the ensemble of
detectors learned from scale 1 to S.

To make the training with Eq. (2) possible, Ys is re-
quired. It is not easy to decompose the groundtruth edge
map Y manually into different scales, making it hard to ob-
tain the layer-speciﬁc supervision Ys for the s-th layer. A
possible solution is to approximate Ys based on ground truth
label Y and edges predicted at other layers, i.e.,

Ys ∼ Y − (cid:2)

i(cid:3)=s

Pi.

(3)

However, Ys computed in Eq. (3) is not an appropriate
layer-speciﬁc supervision. In the following paragraph, we
brieﬂy explain the reason.

According to Eq. (3), for a training image, its predict-
ed edges Ps at layer s should approximate Ys, i.e., Ps ∼
Y −(cid:3)i(cid:3)=s Pi. In other words, we can pass the other layers’
predictions to layer s for training, resulting in an equiva-
lent formulation, i.e., Y ∼ (cid:3)i Pi. The training objective
can thus become L = L( ˆY , Y ), where ˆY = (cid:3)i Pi. The
gradient w.r.t the prediction Ps of layer s is

∂(L)
∂(Ps)

=

∂(L( ˆY , Y ))

∂(Ps)

=

∂(L( ˆY , Y ))

∂( ˆY )

·

∂( ˆY )
∂(Ps)

.

(4)

According to Eq. (4), for edge predictions Ps, Pi at any two
layers s and i, s (cid:4)= i, their loss gradients are equal because
∂( ˆY )
∂(Ps) = ∂( ˆY )
∂(Pi) = 1. This implies that, with Eq. (3), the
training process dose not necessarily differentiate the scales
depicted by different layers, making it not appropriate for
our layer-speciﬁc scale learning task.

To address the above issue, we approximate Ys with two
complementary supervisions. One ignores the edges with
scales smaller than s, and the other ignores the edges with
larger scales. Those two supervisions train two edge detec-
tors at each scale. We deﬁne those two supervisions at scale
s as

s = Y − (cid:2)
Y s2d

i<s

Pi

s2d,

s = Y − (cid:2)
Y d2s

i>s

Pi

d2s,

(5)

where the superscript s2d denotes information propagation
from shallow layers to deeper layers, and d2s denotes the
prorogation from deep layers to shallower layers.

33830

ID Block2

3x3-128

conv

3x3-128

conv

l
o
o
p
 
2
x
2

ID Block1

e
g
a
m

i

3x3-64
conv

3x3-64
conv

SEM

1x1-1
conv

(cid:2001)
(cid:1842)(cid:2869)(cid:3031)(cid:2870)(cid:3046)

SEM

1x1-1
conv

(cid:2001)
(cid:1842)(cid:2869)(cid:3046)(cid:2870)(cid:3031)

loss

SEM

1x1-1
conv

(cid:2001)
(cid:1842)(cid:2870)(cid:3031)(cid:2870)(cid:3046)

ID Block3
3x3-256

conv

3x3-256

conv

3x3-256

conv

SEM
input

3x3-32 conv

SEM

SEM

1x1-1
conv

(cid:2001)
(cid:1842)(cid:2871)(cid:3031)(cid:2870)(cid:3046)

SEM

1x1-1
conv

(cid:2001)
(cid:1842)(cid:2871)(cid:3046)(cid:2870)(cid:3031)
up(cid:1618)loss

3x3-32conv

r = r0

…

r = K(cid:1668)r0

1x1-21
conv
output

l
o
o
p
2
x
2

 

1x1-1
conv

SEM

(cid:2001)
(cid:1842)(cid:2870)(cid:3046)(cid:2870)(cid:3031)
loss(cid:1618)up

Figure 3. The detailed architecture of BDCN and SEM. For illus-
tration, we only show 3 ID Blocks and the cascade from shallow
to deep. The number of ID Blocks in our network can be ﬂexibly
set from 2 to 5 (see Fig. 9).

For scale s, the predicted edges Ps

imate to Y s2d
a reasonable approximation to Ys, i.e.,

and Ys

s

d2s approx-
d2s, respectively. Their combination is

s2d and Ps

Ps

s2d + Ps

d2s ∼ 2Y − (cid:2)

i<s

Pi

s2d − (cid:2)

i>s

Pi

d2s,

(6)

where the edges predicted at scales i (cid:4)= s are depressed.
d2s to interpolate the edge
Therefore, we use Ps
prediction at scale s.

s2d + Ps

Because different convolutional layers depict different s-
cales, the depth of a neural network determines the range
of scales it could model. A shallow network may not be
capable to detect edges at all of the S scales. However,
a large number of convolutional layers involves too many
parameters and makes the training difﬁcult. To enable edge
detection at different scales with a shallow network, we pro-
pose to enhance the multi-scale representation learned in
each convolutional layer with the Scale Enhancement Mod-
ule (SEM). The detail of SEM will be presented in Sec. 3.2.

3.2. Architecture of BDCN

Based on Eq. (6), we propose a Bi-Directional Cascade
Network (BDCN) architecture to achieve layer-speciﬁc
training for edge detection. As shown in Fig. 2, our network
is composed of multiple ID Blocks, each of which is learned
with different supervisions inferred by a bi-directional cas-
cade structure. Speciﬁcally, the network is based on the
VGG16 [44] by removing its three fully connected layers
and last pooling layer. The 13 convolutional layers in VG-
G16 are then divided into 5 blocks, each follows a pooling
layer to progressively enlarge the receptive ﬁelds in the next
block. The VGG blocks evolve into ID Blocks by inserting
several SEMs. We illustrate the detailed architecture of B-
DCN and SEM in Fig. 3.

ID Block is the basic component of our network. Each
ID block produces two edge predictions. As shown in
Fig. 3, an ID Block consists of several convolutional lay-
ers, each is followed by a SEM. The outputs of multiple
SEMs are fused and fed into two 1×1 convolutional layers
to generate two edges predictions P d2s and P s2d, respec-
tively. The cascade structure shown in Fig. 3 propagates the

edge predictions from the shallow layers to deep layers. For
the s-th block, P s2d
com-
puted in Eq. (5). P d2s
is trained in a similar way. The ﬁnal
edge prediction is computed by fusing those intermediate
edge predictions in a fusion layer using 1×1 convolution.

is trained with supervision Y s2d

s

s

s

Scale Enhancement Module is inserted into each ID
Block to enrich the multi-scale representations in it. SEM
is inspired by the dilated convolution proposed by Chen
et al. [5] for image segmentation.
For an input two-
dimensional feature map x ∈ RH×W with a convolution
ﬁlter w ∈ Rh×w, the output y ∈ RH ′×W ′
of dilated convo-
lution at location (i, j) is computed by

yij =

h,w

(cid:2)

m,n

x[i+r·m,j+r·n] · w[m,n],

(7)

where r is the dilation rate, indicating the stride for sam-
pling input feature map. Standard convolution can be treat-
ed as a special case with r = 1. Eq. (7) shows that dilated
convolution enlarges the receptive ﬁeld of neurons without
reducing the resolution of feature maps or increasing the
parameters.

As shown on the right side of Fig. 3, for each SEM we
apply K dilated convolutions with different dilation rates.
For the k-th dilated convolution, we set its dilation rate as
rk = max(1, r0 × k), which involves two parameters in
SEM: the dilation rate factor r0 and the number of convolu-
tion layers K. They are evaluated in Sec. 4.3.

3.3. Network Training

Each ID Block in our network is trained with two layer-
speciﬁc side supervisions. Besides that, we fuse the inter-
mediate edge predictions with a fusion layer as the ﬁnal re-
sult. Therefore, BDCN is trained with three types of loss.
We formulate the overall loss L as,

L = wside · Lside + wf use · Lf use(P, Y ),

(8)

Lside =

S

(cid:2)

s=1

L(P d2s

s

, Y d2s

s

) + L(P s2d

s

, Y s2d

s

),

(9)

where wside and wf use are weights for the side loss and fu-
sion loss, respectively. P denotes the ﬁnal edge prediction.
The function L(·) is computed at each pixel with respect
to its edge annotation. Because the distribution of edge/non-
edge pixels is heavily biased, we employ a class-balanced
cross-entropy loss as L(·). Because of the inconsistency of
annotations among different annotators, we also introduce
a threshold γ for loss computation. For a groudtruth Y =
(yj, j = 1, ..., |Y |), yj ∈ (0, 1), we deﬁne Y+ = {yj, yj >
γ} and Y− = {yj, yj = 0}. Only pixels corresponding to
Y+ and Y− are considered in loss computation. We hence

43831

IDB-1

IDB-2

IDB-3

IDB-4

IDB-5

(cid:1842)(cid:3046)(cid:2870)(cid:3031)

(cid:1842)(cid:3031)(cid:2870)(cid:3046)

(cid:1842)(cid:3046)(cid:2870)(cid:3031)

(cid:1842)(cid:3031)(cid:2870)(cid:3046)

(cid:1842)(cid:3046)(cid:2870)(cid:3031)

(cid:1842)(cid:3031)(cid:2870)(cid:3046)

Figure 4. Examples of edges detected by different ID Blocks (IDB
for short). Each ID Block generates two edge predictions, P s2d
and P d2s, respectively.

deﬁne L(·) as

L(cid:4) ˆY , Y (cid:5) = −α (cid:2)

j∈Y−

log(1 − ˆyj) − β (cid:2)

j∈Y+

log(ˆyj), (10)

where ˆY = (ˆyj, j = 1, ..., | ˆY |), ˆyj ∈ (0, 1) denotes a
predicted edge map, α = λ · |Y+|/(|Y+| + |Y−|), β =
|Y−|/(|Y+| + |Y−|) balance the edge/non-edge pixels. λ
controls the weight of positive over negative samples.

Fig. 4 shows edges detected by different ID blocks. We
observe that, edges detected by different ID Blocks corre-
spond to different scales. The shallow ID Blocks produce
strong responses on local details and deeper ID Blocks are
more sensitive to edges at larger scale. For instance, de-
tailed edges on the body of zebra and butterﬂy can be de-
tected by shallow ID Block, but are depressed by deeper ID
Block. The following section tests the validity of BDCN
and SEM.

4. Experiments

4.1. Datasets

We evaluate the proposed approach on three public

datasets: BSDS500 [1], NYUDv2 [43], and Multicue [35].

BSDS500 contains 200 images for training, 100 images
for validation, and 200 images for testing. Each image
is manually annotated by multiple annotators. The ﬁnal
groundtruth is the averaged annotations by the annotators.
We also utilize the strategies in [49, 30, 47] to augment
training and validation sets by randomly ﬂipping, scaling
and rotating images. Following those works, we also adopt
the PASCAL VOC Context dataset [36] as our training set.
NYUDv2 consists of 1449 pairs of aligned RGB and
depth images. It is split into 381 training, 414 validation,
and 654 testing images. NYUDv2 is initially used for scene
understanding, hence is also used for edge detection in pre-
vious works [15, 40, 49, 30]. Following those works, we

augment the training set by randomly ﬂipping, scaling, and
rotating training images.

Multicue [35] contains 100 challenging natural scenes.
Each scene has two frame sequences taken from left and
right view, respectively. The last frame of left-view se-
quence is annotated with edges and boundaries. Follow-
ing [35, 30, 50], we randomly split 100 annotated frames
into 80 and 20 images for training and testing, respective-
ly. We also augment the training data with the same way
in [49].

4.2. Implementation Details

We implement our network using PyTorch. The VG-
G16 [44] pretrained on ImageNet [8] is used to initialize
the backbone. The threshold γ used for loss computation
is set as 0.3 for BSDS500. γ is set as 0.3 and 0.4 for Mul-
ticue boundary and edges datasets, respectively. NYUDv2
provides binary annotations, thus does not need to set γ for
loss computation. Following [30], we set the parameter λ
as 1.1 for BSDS500 and Multicue, set λ as 1.2 for NYUDv2.
SGD optimizer is adopted to train our network. On B-
SDS500 and NYUDv2, we set the batch size to 10 for all
the experiments. The initial learning rate, momentum and
weight decay are set to 1e-6, 0.9, and 2e-4 respectively. The
learning rate decreases by 10 times after every 10k itera-
tions. We train 40k iterations for BSDS500 and NYUDv2,
2k and 4k iterations for Multicue boundary and edge, re-
spectively. wside and wf use are set as 0.5, and 1.1, respec-
tively. Since Multicue dataset includes high resolution im-
ages, we randomly crop 500×500 patches from each image
in training. All the experiments are conducted on a NVIDIA
GeForce1080Ti GPU with 11GB memory.

We follow previous works [49, 30, 47, 51], and perform
standard Non-Maximum Suppression (NMS) to produce the
ﬁnal edge maps. For a fair comparison with other work,
we report our edge detection performance with commonly
used evaluation metrics, including Average Precision (AP),
as well as F-measure at both Optimal Dataset Scale (ODS)
and Optimal Image Scale (OIS). The maximum tolerance
allowed for correct matches between edge predictions and
groundtruth annotations is set to 0.0075 for BSDS500 and
Multicue dataset, and is set to 0.011 for NYUDv2 dataset as
in previous works [30, 35, 50].

4.3. Ablation Study

In this section, we conduct experiments on BSDS500 to
study the impact of parameters and verify each component
in our network. We train the network on the BSDS500 train-
ing set and evaluate on the validation set. Firstly, we test the
impact of the parameters in SEM, i.e., the number of dilated
convolutions K and the dilation rate factor r0. Experimen-
tal results are summarized in Table 1.

Table 1 (a) shows the impact of K with r0=4. Note that,

53832

Table 1. Impact of SEM parameters to the edge detection perfor-
mance on BSDS500 validation set. (a) shows the impact of K with
r0=4. (b) shows the impact of r0 with K=3.

Table 3. Comparison with other methods on BSDS500 test
†indicates trained with additional PASCAL-Context data.
set.
‡indicates the fused result of multi-scale images.

(a)

(b)

K ODS OIS

AP

r0

rate

ODS OIS

AP

0
1
2
3
4

.7728 .7881 .8093
.7733 .7845 .8139
.7738 .7876 .8169
.7748 .7894 .8170
.7745 .7896 .8166

0
1
2
4
8

1,1,1
1,2,3
2,4,6
4,8,12
8,16,24

.7720 .7881 .8116
.7721 .7882 .8124
.7725 .7875 .8132
.7748 .7894 .8170
.7742 .7889 .8169

Table 2. Validity of components in BDCN on BSDS500 validation
set. (a) tests different cascade architectures. (b) shows the validity
of SEM and the bi-directional cascade architecture.

(a)

(b)

Architecture

ODS OIS

AP

baseline

S2D
D2S

S2D+D2S

(BDCN w/o SEM)

.7681 .7751 .7912
.7683 .7802 .7978
.7710 .7816 .8049

.7762 .7872 .8013

Method

baseline

SEM

S2D+D2S

(BDCN w/o SEM)

ODS OIS

AP

.7681 .7751 .7912
.7748 .7894 .8170

.7762 .7872 .8013

BDCN

.7765 .7882 .8091

K=0 means directly copying the input as output. The re-
sults demonstrate that setting K larger than 1 substantially
improves the performance. However, too large K does not
constantly boost the performance. The reason might be that,
large K produces high dimensional outputs and makes edge
extraction from such high dimensional data difﬁcult. Table
1 (b) also shows that larger r0 improves the performance.
But the performance starts to drop with too large r0, e.g.,
r0=8. In our following experiments, we ﬁx K=3 and r0=4.
Table 2 (a) shows the comparison among different cas-
cade architectures, i.e., single direction cascade from shal-
low to deep layers (S2D), from deep to shallow layers
(D2S), and the bi-directional cascade (S2D+D2S), i.e., the
BDCN w/o SEM. Note that, we use the VGG16 network
without fully connected layer as baseline. It can be observed
that, both S2D and D2S structures outperform the baseline.
This shows the validity of the cascade structure in network
training. The combination of these two cascade structures,
i.e., S2D+D2S, results in the best performance. We fur-
ther test the performance of combining SEM and S2D+D2S
and summarize the results in Table 2 (b), which shows that
SEM and bi-directional cascade structure consistently im-
prove the performance of baseline, e.g., improve the ODS F-
measure by 0.7% and 0.8% respectively. Combining SEM
and S2D+D2S results in the best performance. We can con-
clude that, the components introduced in our method are
valid in boosting edge detection performance.

4.4. Comparison with Other Works

Performance on BSDS500: We compare our ap-
proach with recent deep learning based methods includ-
ing CED [47], RCF [30], DeepBoundary [23], DCD [27],
COB [32], HED [49], HFL [3], DeepEdge [2] and Deep-

Method

Human

SCG [40]
PMI [20]
OEF [17]

DeepContour [42]

HFL [3]
HED [49]

CEDN [53] †

COB [32]
DCD [27]

AMH-Net [51]

RCF [30]
RCF [30] †
RCF [30] ‡

Deep Boundary [23]
Deep Boundary [23] ‡

Deep Boundary [23] ‡ + Grouping

CED [47]
CED [47] ‡
LPCB [9]
LPCB [9] †
LPCB [9] ‡

BDCN
BDCN †
BDCN ‡

ODS

.803
.739
.741
.746
.757
.767
.788
.788
.793
.799
.798
.798
.806
.811
.789
.809
.813
.794
.815
.800
.808
.815
.806
.820
.828

OIS

.803
.758
.769
.770
.776
.788
.808
.804
.820
.817
.829
.815
.823
.830
.811
.827
.831
.811
.833
.816
.824
.834
.826
.838
.844

AP

–

.773
.799
.820
.800
.795
.840

–

.859
.849
.869

–
–
–

.789
.861
.866
.847
.889

–
–
–

.847
.888
.890

1

0.9

0.8

0.7

0.6

0.5

0.4

n
o
i
s
i
c
e
r
P

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5
Recall

0.6

0.7

0.8

0.9

1

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8) (cid:9)(cid:10)(cid:11)(cid:12)(cid:13)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:14)(cid:5)(cid:8) (cid:15)(cid:10)(cid:16)(cid:17) (cid:1)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:14)(cid:6)(cid:8) (cid:15)(cid:10)(cid:16)(cid:17)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:18)(cid:19)(cid:8) (cid:20)(cid:21)(cid:22) (cid:1)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:18)(cid:18)(cid:8) (cid:23)(cid:20)(cid:2) (cid:1)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:24)(cid:8) (cid:23)(cid:20)(cid:2)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:26)(cid:7)(cid:8) (cid:20)(cid:15)(cid:27)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:5)(cid:5)(cid:8) (cid:9)(cid:21)(cid:22)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:24)(cid:25)(cid:8) (cid:9)(cid:2)(cid:28)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:19)(cid:25)(cid:8) (cid:22)(cid:29)(cid:29)(cid:30)(cid:20)(cid:31)(cid:13)(cid:32)(cid:31)(cid:10)(cid:16)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:19)(cid:7)(cid:8) (cid:22)(cid:29)(cid:29)(cid:30)(cid:21)(cid:33)(cid:34)(cid:29)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:35)(cid:24)(cid:8) (cid:15)(cid:21)(cid:2)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:35)(cid:7)(cid:8) (cid:36)(cid:21)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:14)(cid:26)(cid:8) (cid:34)(cid:37)(cid:38)(cid:39)(cid:40)(cid:20)(cid:41)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:25)(cid:18)(cid:25)(cid:8) (cid:42)(cid:36)(cid:20)(cid:23)(cid:43)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:24)(cid:18)(cid:35)(cid:8) (cid:21)(cid:44)(cid:27)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:24)(cid:18)(cid:18)(cid:8) (cid:20)(cid:12)(cid:13)(cid:13)(cid:45)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:19)(cid:26)(cid:5)(cid:8) (cid:41)(cid:36)(cid:46)(cid:47)(cid:48)(cid:32)

Figure 5. The precision-recall curves of our method and other
works on BSDS500 test set.

Contour [42], and traditional edge detection methods, in-
cluding SCG [40], PMI [20] and OEF [17]. The comparison
on BSDS500 is summarized in Table 3 and Fig. 5, respec-
tively.

As shown in the results, our method obtains the F-
measure ODS of 0.820 using single scale input, and
achieves 0.828 with multi-scale inputs, both outperform al-
l of these competing methods. Using a single-scale input,
our method still outperforms the recent CED [47] and Deep-
Boundary [23] that use multi-scale inputs. Our method also
outperforms the human perception by 2.5% in F-measure
ODS. The F-measure OIS and AP of our approach are also
higher than the ones of the other methods.

Performance on NYUDv2: NYUDv2 has three types of
inputs, i.e., RGB, HHA, and RGB-HHA, respectively. Fol-

63833

Table 4. Comparison with recent works on NYUDv2.

Method

ODS OIS AP

Table 5. Comparison with recent works on Multicue. ‡indicates
the fused result of multi-scale images.

gPb-UCM [1]
gPb+NG [15]

OEF[17]
SE [10]

SE+NG+ [16]

HED [49]

RCF [30]

AMH-Net-ResNet50 [51]

LPCB [9]

COB-ResNet50[33]

BDCN

1

0.9

0.8

0.7

n
o
i
s
i
c
e
r
P

0.6

0.5

0.4

RGB

.632 .661 .562
.687 .716 .629
.651 .667
.695 .708 .679
.706 .734 .738
.720 .734 .734
.682 .695 .702
RGB-HHA .746 .761 .786

RGB
HHA

–

RGB
HHA

.729 .742
.705 .715
RGB-HHA .757 .771

–
–
–

RGB
HHA

.744 .758 .765
.716 .729 .734
RGB-HHA .771 .786 .802

–
–
–

RGB
HHA

.739 .754
.707 .719
RGB-HHA .762 .778
825
RGB-HHA .784 .805
.748 .763
.770
.707 .719 .731
RGB-HHA .765 .781 .813

RGB
HHA

Cat.

Method

ODS

Boundary

Edge

Human [35]
Multicue [35]

HED [50]
RCF [30]
RCF [30] ‡

BDCN
BDCN ‡

Human [35]
Multicue [35]

HED [50]
RCF [30]
RCF [30] ‡

BDCN
BDCN ‡

.760 (0.017)
.720 (0.014)
.814 (0.011)
.817 (0.004)
.825 (0.008)
.836 (0.001)
.838(0.004)
.750 (0.024)
.830 (0.002)
.851 (0.014)
.857 (0.004)
.860 (0.005)
.891 (0.001)
.894(0.002)

OIS

–
–

AP
–
–

.822 (0.008)
.825 (0.005)
.836 (0.007)
.846(0.003)
.853(0.009)

.869(0.015)

–
–

.893(0.001)
.906(0.005)

–
–

.864 (0.011)
.862 (0.004)
.864 (0.004)
.898 (0.002)
.901(0.004)

–
–
–
–
–

.935(0.002)
.941(0.005)

[F=.765] Ours
[F=.757] RCF
[F=.741] HED
[F=.706] SE+NG+
[F=.695] SE
[F=.687] gPb+NG
[F=.651] OEF
[F=.631] gPb-UCM

image

GT-Boundary

BDCN-Boundary

GT-Edge

BDCN-Edge

Figure 7. Examples of our edge detection results before Non-
Maximum Suppression on Multicue dataset.

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5
Recall

0.6

0.7

0.8

0.9

1

Figure 6. The precision-recall curves of our method and compared
works on NYUDv2.

lowing previous works [49, 30], we perform experiments
on all of them. The results of RGB-HHA are obtained by
averaging the edges detected on RGB and HHA. Table 4
shows the comparison of our method with several recent ap-
proaches, including gPb-ucm [1], OEF [17], gPb+NG [15],
SE+NG+ [16], SE [10], HED [49], RCF [30] and AMH-
Net [51]. Fig. 6 shows the precision-recall curves of our
method and other competitors. All of the evaluation results
are based on a single scale input.

As shown in Table 4 and Fig. 6, our performance is com-
petitive, i.e., outperforms most of the compared works ex-
cept AMH-Net [51]. Note that, AMH-Net applies the deep-
er ResNet50 to construct the edge detector. With a shal-
lower network, our method still outperforms AMH-Net on
the RGB image, i.e., our 0.748 vs. 0.744 of AMH-Net in
F-measure ODS. Compared with previous works, our im-
provement over existing works is actually more substantial,
e.g., on NYUDv2 our gains over RCF [30] and HED [49]
are 0.019 and 0.028 in ODS, higher than the 0.009 gain of

ODS

0.83
0.8
0.77
0.74
0.71
0.68

4

5
Ours

3

2

Ours w/o BDCN 

0.85

0.8

0.75

0.7

OIS

5

4

3

Ours w/o SEB 

0.9
0.86
0.82
0.78
0.74
0.7

AP

5

4

RCF[29]

2

3

2
HED[47]

Figure 8. Comparison of edge detection accuracy as we decrease
the number of ID Blocks from 5 to 2. HED learned with VGG16
is denoted as the solid line for comparison.

RCF [30] over HED [49].

Performance on Multicue: Multicue consists of two sub
datasets, i.e., Multicue boundary and Multicue edge. As
done in RCF [30] and the recent version of HED [50], we
average the scores of three independent experiments as the
ﬁnal result. We show the comparison with recent works
in Table 5, where our method achieves substantially higher
performance than RCF [30] and HED [49]. For boundary
detection task, we outperform RCF and HED by 1.3% and
2.4%, respectively in F-measure ODS. For edge detection
task, our performance is 3.4% and 4.3% higher than the
ones of RCF and HED. Moreover, the performance ﬂuc-
tuation of our method is considerably smaller than those
two methods, which means our method delivers more sta-
ble results. Some edge detection results generated by our
approach on Multicue are presented in Fig. 7.

Discussions: The above experiments have shown the

73834

0.828

0.82

0.812

0.815 0.813 0.811

119.6

0.799 0.798

0.793

0.788 0.788

0.796

0.766

16.3 16.3

8.69

2.26 0.28

21.4

14.7 14.8 14.7

28.8

22

14.7

0.767

0.757

20

0.38

Table 6. The performance (ODS) of each layer in BDCN, R-
CF [30], and HED [49] on BSDS500 test set.

Layer ID.

HED [49]

RCF [30]

BDCN

1
2
3
4
5

fuse

0.595
0.697
0.750
0.748
0.637
0.790

0.595
0.710
0.766
0.761
0.758
0.805

0.727
0.762
0.771
0.802
0.815
0.820

0.83
0.82
0.81
0.8
0.79
0.78
0.77
0.76
0.75
0.74

S
D
O

)

M

(
.

m
a
r
a
P

120

100

80

60

40

20

0

Figure 9. Comparison of parameters and performance with other
methods. The number behind “BDCN” indicates the number of ID
Block. ‡means the multiscale results.

competitive performance of our proposed method. We fur-
ther test the capability of our method in learning multi-
scale representations with shallow networks. We test our
approach and RCF with different depth of networks, i.e.,
using different numbers of convolutional block to construct
the edge detection model. Fig. 8 presents the results on B-
SDS500. As shown in Fig. 8, the performance of RCF [30]
drops more substantially than our method as we decrease
the depth of networks. This veriﬁes that our approach is
more effective in detecting edges with shallow networks.
We also show the performance of our approach without the
SEM and the BDCN structure. These ablations show that
removing either BDCN or SEM degrades the performance.
It is also interesting to observe that, without SEM, the per-
formance of our method drops substantially. This hence
veriﬁes the importance of SEM to multi-scale representa-
tion learning in shallow networks.

Fig. 9 further shows the comparison of parameters vs.
performance of our method with other deep net based meth-
ods on BSDS500. With 5 convolutional blocks in VGG16,
HED [49], RCF [30], and our method use similar number
of parameters, i.e., about 16M. As we decrease the number
of ID Blocks from 5 to 2, our number of parameters de-
creases dramatically, drops to 8.69M, 2.26M, and 0.28M,
respectively. Our method still achieves F-measure ODS of
0.766 using only two ID Blocks with 0.28M parameters. It
also outperforms HED and RCF with a more shallow net-
work, i.e., with 3 and 4 ID Blocks respectively. For exam-
ple, it outperforms HED by 0.8% with 3 ID Blocks and just
1/6 parameters of HED. We thus conclude that, our method
can achieve promising edge detection accuracy even with a
compact shallow network.

To further show the advantage of our method, we evalu-
ate the performance of edge predictions by different inter-
mediate layers, and show the comparison with HED [49]
and RCF [30] in Table 6. It can be observed that, the in-
termediate predictions of our network also consistently out-

image

GT

PMI [19]

HED [47]

RCF [29]

CED [45]

BDCN

Figure 10. Comparison of edge detection results on BSDS500 test
set. All the results are raw edge maps computed with a single scale
input before Non-Maximum Suppression.

perform the ones from HED and RCF, respectively. With
5 ID Blocks, our method runs at about 22fps for edge de-
tection, on par with most DCNN-based methods. With 4, 3
and 2 ID Blocks, it accelerates to 29 fps, 33fps, and 37fps,
respectively. Fig. 10 compares some edge detection results
generated by our approach and several recent ones.

5. Conclusions

This paper proposes a Bi-Directional Cascade Network
for edge detection. By introducing a bi-directional cascade
structure to enforce each layer to focus on a speciﬁc scale,
BDCN trains each network layer with a layer-speciﬁc su-
pervision. To enrich the multi-scale representations learned
with a shallow network, we further introduce a Scale En-
hancement Module (SEM). Our method compares favor-
ably with over 10 edge detection methods on three datasets,
achieving ODS F-measure of 0.828, 1.3% higher than cur-
rent state-of-art on BSDS500. Our experiments also show
that learning scale dedicated layers results in compact net-
works with a fraction of parameters, e.g., our approach out-
performs HED [49] with only 1/6 of its parameters.

6. Acknowledgement

This work is supported in part by Beijing Natu-
in
ral Science Foundation under Grant No.
part by Natural Science Foundation of China under
Grant No.
61620106009, 61572050, 91538111. We
also thank NVIDIA’s generosity for providing DGX-1
super-computer and support through the NVAIL program.

JQ18012,

83835

References

[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour
detection and hierarchical image segmentation. IEEE Trans.
Pattern Anal. Mach. Intell., 33(5):898–916, 2011. 1, 3, 5, 7

[2] G. Bertasius, J. Shi, and L. Torresani. Deepedge: A multi-
scale bifurcated deep network for top-down contour detec-
tion. In CVPR, pages 4380–4389, 2015. 1, 2, 6

[3] G. Bertasius, J. Shi, and L. Torresani. High-for-low and low-
for-high: Efﬁcient boundary detection from deep object fea-
tures and its applications to high-level vision. In ICCV, 2015.
1, 2, 6

[4] J. Canny. A computational approach to edge detection. IEEE
Trans. Pattern Anal. Mach. Intell., 8(6):679–698, June 1986.
1, 2

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint arXiv:1606.00915, 2016. 2, 4

[6] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At-
tention to scale: Scale-aware semantic image segmentation.
In CVPR, pages 3640–3649, 2016. 2

[7] D. Comaniciu and P. Meer. Mean shift: A robust approach
IEEE Trans. Pattern Anal.

toward feature space analysis.
Mach. Intell., 24(5):603–619, 2002. 1

[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255. IEEE, 2009. 5

[9] R. Deng, C. Shen, S. Liu, H. Wang, and X. Liu. Learning to
predict crisp boundaries. In ECCV, pages 562–578, 2018. 6,
7

[10] P. Doll´ar and C. L. Zitnick.

Fast edge detection using
IEEE Trans. Pattern Anal. Mach. Intel-

structured forests.
l., 37(8):1558–1570, 2015. 1, 2, 7

[11] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolution-
al architecture. In ICCV, pages 2650–2658, 2015. 2

[12] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning
hierarchical features for scene labeling. IEEE Trans. Pattern
Anal. Mach. Intell., 35(8):1915–1929, 2013. 2

[13] V. Ferrari, L. Fevrier, F. Jurie, and C. Schmid. Groups of
adjacent contour segments for object detection. IEEE Trans.
Pattern Anal. Mach. Intell., 30(1):36–51, 2008. 1

[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, pages 580–587, 2014. 1

[15] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organiza-
tion and recognition of indoor scenes from rgb-d images. In
CVPR, 2013. 5, 7

[16] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learning
rich features from rgb-d images for object detection and seg-
mentation. In ECCV, pages 345–360. Springer, 2014. 7

[17] S. Hallman and C. C. Fowlkes. Oriented edge forests for
boundary detection. In CVPR, pages 1732–1740, 2015. 6, 7

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016. 1

[19] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and
K. Q. Weinberger. Multi-scale dense convolutional networks
for efﬁcient prediction. arXiv preprint arXiv:1703.09844,
2017. 2

[20] P. Isola, D. Zoran, D. Krishnan, and E. H. Adelson. Crisp
boundary detection using pointwise mutual information. In
ECCV. Springer, 2014. 6

[21] W. Ke, J. Chen, J. Jiao, G. Zhao, and Q. Ye. Srn: Side-output
residual network for object symmetry detection in the wild.
In CVPR, pages 1068–1076, 2017. 3

[22] J. Kittler. On the accuracy of the sobel edge detector. Image

and Vision Computing, 1(1):37–42, 1983. 1, 2

[23] I. Kokkinos. Pushing the boundaries of boundary detection
using deep learning. arXiv preprint arXiv:1511.07386, 2015.
1, 6

[24] S. Konishi, A. L. Yuille, J. M. Coughlan, and S. C. Zhu. S-
tatistical edge detection: Learning and evaluating edge cues.
IEEE Trans. Pattern Anal. Mach. Intell., 25(1):57–74, 2003.
2

[25] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua. A convolu-
tional neural network cascade for face detection. In CVPR,
pages 5325–5334, 2015. 3

[26] X. Li, Z. Liu, P. Luo, C. Change Loy, and X. Tang. Not all
pixels are equal: Difﬁculty-aware semantic segmentation via
deep layer cascade. In CVPR, 2017. 3

[27] Y. Liao, S. Fu, X. Lu, C. Zhang, and Z. Tang. Deep-learning-
based object-level contour detection with ccg and crf opti-
mization. In ICME, 2017. 6

[28] J. J. Lim, C. L. Zitnick, and P. Doll´ar. Sketch tokens: A
learned mid-level representation for contour and object de-
tection. In CVPR, 2013. 1

[29] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In CVPR, volume 1, page 4, 2017. 3

[30] Y. Liu, M.-M. Cheng, X. Hu, K. Wang, and X. Bai. Richer
convolutional features for edge detection. In CVPR, 2017. 1,
2, 3, 5, 6, 7, 8

[31] Y. Liu and M. S. Lew. Learning relaxed deep supervision for

better edge detection. In CVPR, 2016. 1

[32] K. Maninis, J. Pont-Tuset, P. Arbel´aez, and L. V. Gool. Con-

volutional oriented boundaries. In ECCV, 2016. 6

[33] K.-K. Maninis, J. Pont-Tuset, P. Arbel´aez, and L. Van Gool.
Convolutional oriented boundaries: From image segmenta-
tion to high-level tasks. IEEE Trans. on Pattern Anal. Mach
Interll., 40(4):819–833, 2018. 7

[34] D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to de-
tect natural image boundaries using local brightness, color,
and texture cues. IEEE Trans. Pattern Anal. Mach. Intell.,
26(5):530–549, 2004. 2

[35] D. A. M´ely, J. Kim, M. McGill, Y. Guo, and T. Serre. A sys-
tematic comparison between visual cues for boundary detec-
tion. Vision research, 120:93–107, 2016. 5, 7

[36] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-
dler, R. Urtasun, and A. Yuille. The role of context for object
detection and semantic segmentation in the wild. In CVPR.
5

93836

[37] V. N. Murthy, V. Singh, T. Chen, R. Manmatha, and D. Co-
maniciu. Deep decision network for multi-class image clas-
siﬁcation. In CVPR, pages 2240–2248. IEEE, 2016. 3

[38] P. Pinheiro and R. Collobert. Recurrent convolutional neural
networks for scene labeling. In ICML, pages 82–90, 2014. 2
[39] X. Ren. Multi-scale improves boundary detection in natural

images. In ECCV, pages 533–545, 2008. 2

[40] X. Ren and L. Bo. Discriminatively trained sparse code gra-

dients for contour detection. In NIPS, 2012. 5, 6

[41] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interac-
tive foreground extraction using iterated graph cuts. In IEEE
Trans. Graph., volume 23, pages 309–314. ACM, 2004. 1

[42] W. Shen, X. Wang, Y. Wang, X. Bai, and Z. Zhang. Deep-
contour: A deep convolutional feature learned by positive-
In CVPR, 2015. 1, 2,
sharing loss for contour detection.
6

[43] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.

segmentation and support inference from rgbd images.
ECCV, 2012. 5

Indoor
In

[44] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 4, 5

[45] V. Torre and T. A. Poggio. On edge detection. IEEE Trans.

Pattern Anal. Mach. Intell., (2):147–163, 1986. 2

[46] A. Toshev and C. Szegedy. Deeppose: Human pose estima-
tion via deep neural networks. In CVPR, pages 1653–1660,
2014. 3

[47] Y. Wang, X. Zhao, and K. Huang. Deep crisp boundaries. In

CVPR, 2017. 1, 2, 5, 6

[48] A. P. Witkin. Scale-space ﬁltering. In Readings in Computer

Vision, pages 329–332. Elsevier, 1987. 2, 3

[49] S. Xie and Z. Tu. Holistically-nested edge detection.

In

ICCV, 2015. 1, 2, 5, 6, 7, 8

[50] S. Xie and Z. Tu. Holistically-nested edge detection. Inter-

national journal of computer vision, 2017. 1, 2, 5, 7

[51] D. Xu, W. Ouyang, X. Alameda-Pineda, E. Ricci, X. Wang,
and N. Sebe. Learning deep structured multi-scale features
using attention-gated crfs for contour prediction.
In NIPS,
pages 3964–3973, 2017. 2, 5, 6, 7

[52] S. Yan, J. S. Smith, W. Lu, and B. Zhang. Hierarchical multi-
scale attention networks for action recognition. Signal Pro-
cessing: Image Communication, 61:73–84, 2018. 2

[53] J. Yang, B. Price, S. Cohen, H. Lee, and M.-H. Yang. Object
contour detection with a fully convolutional encoder-decoder
network. In CVPR, pages 193–202, 2016. 6

[54] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cascad-

ed embedding. CoRR, abs/1611.05720, 1, 2016. 3

[55] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, pages 2881–2890, 2017. 2

103837

