Why ReLU networks yield high-conﬁdence predictions far away from

the training data and how to mitigate the problem

Matthias Hein

Maksym Andriushchenko

Julian Bitterwolf

University of T¨ubingen

Saarland University

University of T¨ubingen

Abstract

Classiﬁers used in the wild, in particular for safety-
critical systems, should not only have good generaliza-
tion properties but also should know when they don’t
know, in particular make low conﬁdence predictions far
away from the training data. We show that ReLU type
neural networks which yield a piecewise linear classi-
ﬁer function fail in this regard as they produce almost
always high conﬁdence predictions far away from the
training data. For bounded domains like images we
propose a new robust optimization technique similar
to adversarial training which enforces low conﬁdence
predictions far away from the training data. We show
that this technique is surprisingly eﬀective in reducing
the conﬁdence of predictions far away from the training
data while maintaining high conﬁdence predictions and
test error on the original classiﬁcation task compared
to standard training.

1. Introduction

Neural networks have recently obtained state-of-the-
art performance in several application domains like ob-
ject recognition and speech recognition. They have be-
come the de facto standard for many learning tasks.
Despite this great success story and very good predic-
tion performance there are also aspects of neural net-
works which are undesirable. One property which is
naturally expected from any classiﬁer is that it should
know when it does not know or said more directly:
far away from the training data a classiﬁer should not
make high conﬁdence predictions. This is particu-
larly important in safety-critical applications like au-
tonomous driving or medical diagnosis systems where
such an input should either lead to the fact that other
redundant sensors are used or that a human doctor is
asked to check the diagnosis. It is thus an important
property of a classiﬁer which however has not received
much attention despite the fact that it seems to be a
minimal requirement for any classiﬁer.

There have been many cases reported where high
conﬁdence predictions are made far away from the
training data by neural networks, e.g.
fooling images
[30], for out-of-distribution images [15] or in a medical
diagnosis task [23]. Moreover, it has been observed
that, even on the original task, neural networks of-
ten produce overconﬁdent predictions [12]. A related
but diﬀerent problem are adversarial samples where
very small modiﬁcations of the input can change the
classiﬁer decision [34, 11, 32]. Apart from methods
which provide robustness guarantees for neural net-
works [14, 37, 31, 26] which give still only reasonable
guarantees for small networks, up to our knowledge the
only approach which has not been broken again [6, 5, 2]
is adversarial training [25] using robust optimization
techniques.

While several methods have been proposed to adjust
overconﬁdent predictions on the true input distribution
using softmax calibration [12], ensemble techniques [20]
or uncertainty estimation using dropout [10], only re-
cently the detection of out-of-distribution inputs [15]
has been tackled. The existing approaches basically ei-
ther use adjustment techniques of the softmax outputs
[9, 24] by temperature rescaling [12] or they use a gener-
ative model like a VAE or GAN to model boundary in-
puts of the true distribution [22, 36] in order to discrim-
inate in-distribution from out-of-distribution inputs di-
rectly in the training process. While all these ap-
proaches are signiﬁcant steps towards obtaining more
reliable classiﬁers, the approaches using a generative
model have been recently challenged by [28, 16] which
report that generative approaches can produce highly
conﬁdent density estimates for inputs outside of the
class they are supposed to model. Moreover, note that
the quite useful models for conﬁdence calibration on
the input distribution like [10, 12, 20] cannot be used
for out-of-distribution detection as it has been observed
in [23]. Another approach is the introduction of a rejec-
tion option into the classiﬁer [35, 4], in order to avoid
decisions the classiﬁer is not certain about.

In this paper we will show that for the class of ReLU

41

networks, that are networks with fully connected, con-
volutional and residual
layers, where just ReLU or
leaky ReLU are used as activation functions and max
or average pooling for convolution layers, basically any
neural network which results in a piecewise aﬃne classi-
ﬁer function, produces arbitrarily high conﬁdence pre-
dictions far away from the training data. This implies
that techniques which operate on the output of the clas-
siﬁer cannot identify these inputs as out-of-distribution
inputs. On the contrary we formalize the well known
fact that RBF networks produce almost uniform conﬁ-
dence over the classes far away from the training data,
which shows that there exist classiﬁers which satisfy
the minimal requirement of not being conﬁdent in areas
where one has never seen data. Moreover, we propose
a robust optimization scheme motivated by adversarial
training [25] which simply enforces uniform conﬁdence
predictions on noise images which are by construction
far away from the true images. We show that our tech-
nique not only signiﬁcantly reduces conﬁdence on such
noise images, but also on other unrelated image clas-
siﬁcation tasks and in some cases even for adversarial
samples generated for the original classiﬁcation task.
The training procedure is simple, needs no adaptation
for diﬀerent out-of-distribution tasks, has similar com-
plexity as standard adversarial training and achieves
similar or marginally worse generalization performance
on the original classiﬁcation task.

2. ReLU networks produce piecewise

aﬃne functions

We quickly review in this section the fact that ReLU
networks lead to continuous piecewise aﬃne classiﬁers,
see [1, 8], which we brieﬂy summarize in order to set
the ground for our main theoretical result in Section 3.

Deﬁnition 2.1. A function f : Rd → R is called
piecewise aﬃne if there exists a ﬁnite set of polytopes
{Qr}M
r=1 (referred to as linear regions of f ) such that
∪M
r=1Qr = Rd and f is an aﬃne function when re-
stricted to every Qr.

Feedforward neural networks which use piecewise
aﬃne activation functions (e.g. ReLU, leaky ReLU)
and are linear in the output layer can be rewritten as
continuous piecewise aﬃne functions [1]. This includes
fully connected, convolutional, residual layers and even
skip connections as all these layers are just linear map-
pings. Moreover, it includes further average pooling
and max pooling. More precisely, the classiﬁer is a
function f : Rd → RK, where K are the number of
classes, such that each component fi : Rd → R, is a
continuous piecewise aﬃne function and the K compo-
nents (fi)K
i=1 have the same set of linear regions. Note

that explicit upper bounds on the number of linear re-
gions have been given [27].

In the following we follow [8]. For simplicity we just
present fully connected layers. Denote by σ : R →
R, σ(t) = max{0, t}, the ReLU activation function,
by L + 1 the number of layers and W (l) ∈ Rnl×nl−1
and b(l) ∈ Rnl respectively are the weights and oﬀset
vectors of layer l, for l = 1, . . . , L + 1 and n0 = d.
For x ∈ Rd one deﬁnes g(0)(x) = x. Then one can
recursively deﬁne the pre- and post-activation output
of every layer as

f (k)(x) = W (k)g(k−1)(x) + b(k),
g(k)(x) = σ(f (k)(x)),

k = 1, . . . , L,

and

so that
f (L+1)(x) = W (L+1)g(L)(x) + b(L+1).

resulting classiﬁer

the

is obtained as

Let ∆(l), Σ(l) ∈ Rnl×nl for l = 1, . . . , L be diagonal

matrices deﬁned elementwise as

∆(l)(x)ij =(sign(f (l)
Σ(l)(x)ij =(1

0

0

i (x))

if i = j,
else.

,

if i = j and f (l)
else.

i (x) > 0,

.

Note that for leaky ReLU the entries would be 1 and
α instead. This allows to write f (k)(x) as composition
of aﬃne functions, that is

f (k)(x) =W (k)Σ(k−1)(x)(cid:16)W (k−1)Σ(k−2)(x)

×(cid:16) . . .(cid:16)W (1)x + b(1)(cid:17) . . .(cid:17) + b(k−1)(cid:17) + b(k),

We can further simplify the previous expression as
f (k)(x) = V (k)x + a(k), with V (k) ∈ Rnk×d and a(k) ∈
Rnk given by

k−1

Σ(k−l)(x)W (k−l)(cid:17) and
V (k) = W (k)(cid:16)
Yl=1
W (k+1−m)Σ(k−m)(x)(cid:17)b(l).
Xl=1(cid:16)
Ym=1

k−1

k−l

a(k) = b(k) +

spaces given by

The polytope Q(x), the linear region containing x, can
l=1 nl half

be characterized as an intersection of N =PL
i (cid:1) ≥ 0(cid:9),

∆(l)(x)(cid:0)V (l)

Γl,i =(cid:8)z ∈ Rd(cid:12)(cid:12)(cid:12)

for l = 1, . . . , L, i = 1, . . . , nl, namely

i z + a(l)

Q(x) = \l=1,...,L \i=1,...,nl

Γl,i.

42

Note that N is also the number of hidden units of the
network. Finally, we can write

= V (L+1)z + a(L+1),

f (L+1)(z)(cid:12)(cid:12)(cid:12)Q(x)

which is the aﬃne restriction of f to Q(x).

3. Why ReLU networks produce high
conﬁdence predictions far away from
the training data

With the explicit description of the piecewise lin-
ear classiﬁer resulting from a ReLU type network from
Section 2, we can now formulate our main theorem.
It shows that, as long a very mild condition on the
network holds, for any ǫ > 0 one can always ﬁnd for
(almost) all directions an input z far away from the
training data which realizes a conﬁdence of 1 − ǫ on z
for a certain class. However, before we come to this
result, we ﬁrst need a technical lemma needed in the
proof, which uses that all linear regions are polytopes
and thus convex sets.

Lemma 3.1. Let {Qi}R
l=1 be the set of linear regions
associated to the ReLU-classiﬁer f : Rd → RK. For
any x ∈ Rd there exists α ∈ R with α > 0 and t ∈
{1, . . . , R} such that βx ∈ Qt for all β ≥ α.

All the proofs can be found in the supplementary
material. Using Lemma 3.1 we can now state our ﬁrst
main result.

l=1Ql and f (x) = V lx + al
Theorem 3.1. Let Rd = ∪R
be the piecewise aﬃne representation of the output of a
ReLU network on Ql. Suppose that V l does not contain
identical rows for all l = 1, . . . , R, then for almost any
x ∈ Rd and ǫ > 0 there exists an α > 0 and a class
k ∈ {1, . . . , K} such that for z = αx it holds

Moreover,

lim
α→∞

efk (αx)

efr (αx)

efk(z)
r=1 efr(z)

≥ 1 − ǫ.

= 1.

PK
PK

r=1

Please note that the condition that for a region the
linear part V l need not contain two identical rows is
very weak. It is hardly imaginable that this is ever true
for a normally trained network unless the output of the
network is constant anyway. Even if it is true, it just
invalidates the assertion of the theorem for the points
lying in this region. Without explicitly enforcing this
condition it seems impossible that this is true for all
possible asymptotic regions extending to inﬁnity (see
Figure 1). However, it is also completely open how

Figure 1: A decomposition of R2 into a ﬁnite set of polytopes for
a two-hidden layer ReLU network. The outer polytopes extend to
inﬁnity. This is where ReLU networks realize arbitrarily high conﬁ-
dence predictions. The picture is produced with the code of [17].

this condition could be enforced during training of the
network.

l=1

efk (x)/T

PK

The result implies that for ReLU networks there
exist inﬁnitely many inputs which realize arbitrarily
high conﬁdence predictions of the networks. It is easy
to see that the temperature rescaling of the softmax,
efl (x)/T , for temperature T > 0, as used in [24],
will not be able to detect these cases, in particular since
the ﬁrst step of the method in [24] consists of going in
the direction of increasing conﬁdence. Also it is obvious
that using a reject option in the classiﬁer, see e.g. [3],
will not help to detect these instances either. The result
is negative in the sense that it looks like that without
modifying the architecture of a ReLU network it is im-
possible to prevent this phenomenon. Please note that
from the point of view of Bayesian decision theory the
softmax function is the correct transfer function [21]
for the cross-entropy loss turning the classiﬁer output
fk(x) into an estimate P(Y = k |x, f ) = efk (x)
efl (x) for

the conditional probability at x.

While the previous result seems not to be known,
the following result is at least qualitatively known [11]
but we could not ﬁnd a reference for it.
In contrast
to the ReLU networks it turns out that Radial Basis
Function (RBF) networks have the property to pro-
duce approximately uniform conﬁdence predictions far
away from the training data. Thus there exist classi-
ﬁers which satisfy the minimal requirement which we
formulated in Section 1. In the following theorem we
explicitly quantify what “far away” means in terms of
parameters of the RBF classiﬁer and the training data.

l=1

PK

Theorem 3.2. Let fk(x) = PN

2, k =
1, . . . , K be a RBF-network trained with cross-entropy
loss on the training data (xi, yi)N
i=1. We deﬁne rmin =

l=1 αkle−γkx−xlk2

43

min

kx − xlk2 and α = max

l=1 |αrl − αkl|.

l=1,...,N
ǫ > 0 and

r2
min ≥

1
γ

log(cid:16)

then for all k = 1, . . . , K,

r,k PN
log(1 + Kǫ)(cid:17),

α

1
K

− ǫ ≤

efk(x)
r=1 efr(x)

≤

1
K

+ ǫ.

PK

We think that it is a very important open problem
to realize a similar result as in Theorem 3.2 for a class
of neural networks. Note that arbitrarily high conﬁ-
dence predictions for ReLU networks can be obtained
only if the domain is unbounded e.g. Rd. However,
images are contained in [0, 1]d and thus Theorem 3.1
does not directly apply, even though the technique can
in principle be used to produce high-conﬁdence predic-
tions (see experiments in the supplement). In the next
section we propose a novel training scheme enforcing
low conﬁdence predictions on inputs far away from the
training data.

4. Adversarial Conﬁdence Enhanced

Training

In this section we suggest a simple way to adjust
the conﬁdence estimation of a neural network far away
from the training data, not necessarily restricted to
ReLU networks studied in Theorem 3.1. Theorem 3.1
tells us that for ReLU networks a post-processing of the
softmax scores is not suﬃcient to avoid high-conﬁdence
predictions far away from the training data - instead
there seem to be two potential ways to tackle the prob-
lem: a) one uses an extra generative model either for
the in-distribution or for the out-distribution or b)
one modiﬁes directly the network via an adaptation of
the training process so that uniform conﬁdence predic-
tions are enforced far away from the training data. As
recently problems with generative models have been
pointed out which assign high conﬁdence to samples
from the out-distribution [28] and thus a) seems less
promising we explore approach b).

We assume that it is possible to characterize a distri-
bution of data points pout on the input space for which
we are sure that they do not belong to the true dis-
tribution pin resp. the set of the intersection of their
supports has zero or close to zero probability mass.
An example of such an out-distribution pout would be
the uniform distribution on [0, 1]w×h (w × h gray scale
images) or similar noise distributions. Suppose that
the in-distribution consists of certain image classes like
handwritten digits, then the probability mass of all im-
ages of handwritten digits under the pout is zero (if it
is really a low-dimensional manifold) or close to zero.

If

In such a setting the training objective can be writ-

ten as a sum of two losses:

1
N

N

Xi=1

LCE(yi, f (xi)) + λ E(cid:2)Lpout(f, Z)(cid:3),

(1)

where (xi, yi)N
tribution pout and

i=1 is the i.i.d. training data, Z has dis-

LCE(yi, f (xi)) = − log(cid:16)

Lpout(f, z) = max

l=1,...,K

efyi (xi)

k=1 efk(xi)(cid:17)
PK
log(cid:16)
k=1 efk(z)(cid:17).
PK

efl(z)

(2)

(3)

k=1

efl (z)

PK

LCE is the usual cross entropy loss on the original clas-
siﬁcation task and Lpout(f, z) is the maximal log con-
ﬁdence over all classes where the conﬁdence of class l
efk (z) , with the softmax function as
is given by,
the link function. The full loss can be easily minimized
by using SGD with batchsize B for the original data
and adding ⌈λB⌉ samples from pout on which one en-
forces a uniform distribution over the labels. We call
this process in the following conﬁdence enhancing data
augmentation (CEDA). We note that in a concurrent
paper [16] the same scheme has been proposed, where
they use as pout existing large image datasets, whereas
we favor an agnostic approach where pout models a cer-
tain “noise” distribution on images.

The problem with CEDA is that it might take too
many samples to enforce low conﬁdence on the whole
out-distribution. Moreover, it has been shown in the
area of adversarial manipulation that data augmen-
tation is not suﬃcient for robust models and we will
see in Section 5 that indeed CEDA models still pro-
duce high conﬁdence predictions in a neighborhood of
noise images. Thus, we propose to use ideas from ro-
bust optimization similar to adversarial training which
[34, 11, 25] apply to obtain robust networks against
adversarial manipulations. Thus we are enforcing low
conﬁdence not only at the point itself but actively min-
imize the worst case in a neighborhood of the point.
This leads to the following formulation of adversarial
conﬁdence enhancing training (ACET)

1
N

N

Xi=1

LCE(yi, f (xi)) + λ E(cid:2) max

ku−Zkp≤ǫ

Lpout(f, u)(cid:3),

(4)

where in each SGD step one solves (approximately) for
a given z ∼ pout the optimization problem:

max

ku−zkp≤ǫ

Lpout(f, u).

(5)

44

In this paper we use always p = ∞. Note that if the
distributions pout and pin have joint support, the maxi-
mum in (5) could be obtained at a point in the support
of the true distribution. However, if pout is a generic
noise distribution like uniform noise or a smoothed ver-
sion of it, then the number of cases where this happens
has probability mass close to zero under pout and thus
does not negatively inﬂuence in (4) the loss LCE on the
true distribution. The optimization of ACET in (4) can
be done using an adapted version of the PGD method
of [25] for adversarial training where one performs pro-
jected gradient descent (potentially for a few restarts)
and uses the u realizing the worst loss for computing
the gradient. The resulting samples are more infor-
mative and thus lead to a faster and more signiﬁcant
reduction of high conﬁdence predictions far away from
the training data. We use ǫ = 0.3 for all datasets. We
present in Figure 2 and 3 for MNIST and CIFAR-10 a
few noise images together with their adversarial mod-
iﬁcation u generated by applying PGD to solve (5).
One can observe that the generated images have no
structure resembling images from the in-distribution.

5. Experiments

In the evaluation, we follow [15, 24, 22] by train-
ing on one dataset and evaluating the conﬁdence on
other out of distribution datasets and noise images. In
contrast to [24, 22] we neither use a diﬀerent parame-
ter set for each test dataset [24] nor do we use one of
the test datasets during training [22]. More precisely,
we train on MNIST, SVHN, CIFAR-10 and CIFAR-
100, where we use the LeNet architecture on MNIST
taken from [25] and a ResNet architecture [13] for the
other datasets. We also use standard data augmenta-
tion which includes random crops for all datasets and
random mirroring for CIFAR-10 and CIFAR-100. For
the generation of out-of-distribution images from pout
we proceed as follows: half of the images are gener-
ated by randomly permuting pixels of images from the
training set and half of the images are generated uni-
formly at random. Then we apply to these images a
Gaussian ﬁlter with standard deviation σ ∈ [1.0, 2.5] as
lowpass ﬁlter to have more low-frequency structure in
the noise. As the Gaussian ﬁlter leads to a contrast re-
duction we apply afterwards a global rescaling so that
the maximal range of the image is again in [0, 1].
Training: We train each model normally (plain),
with conﬁdence enhancing data augmentation (CEDA)
and with adversarial conﬁdence enhancing training
(ACET). It is well known that weight decay alone re-
duces overconﬁdent predictions. Thus we use weight
decay with regularization parameter 5 · 10−4 for all
models leading to a strong baseline (plain). For CEDA

l=1

efk (x)

PK

(1) and ACET (4) we both use λ = 1, that means 50%
of the samples in each batch are from the original train-
ing set and 50% are noise samples as described before.
For ACET we use p = ∞ and ǫ = 0.3 and optimize
with PGD [25] using 40 iterations and stepsize 0.0075
for all datasets. All models are trained for 100 epochs
with ADAM [18] on MNIST and SGD+momentum
for SVHN/CIFAR-10/CIFAR-100. The initial learning
rate is 10−3 for MNIST and 0.1 for SVHN/CIFAR-10
and it is reduced by a factor of 10 at the 50th, 75th and
90th of the in total 100 epochs. More results and de-
tails can be found in the supplementary material. The
code is available at https://github.com/max-andr/
relu_networks_overconfident.
Evaluation: We report for each model (plain, CEDA,
ACET) the test error and the mean maximal conﬁ-
efl (x) ),
dence (for each point this is maxk=1,...,K
denoted as MMC, on the test set.
In order to eval-
uate how well we reduce the conﬁdence on the out-
distribution, we use four datasets on CIFAR-10 [19]
and SVHN [29] (namely among CIFAR-10, CIFAR-
100, SVHN, ImageNet-, which is a subset of ImageNet
where we removed classes similar to CIFAR-10, and
the classroom subset of LSUN [39] we use the ones on
which we have not trained) and for MNIST we eval-
uate on EMNIST [7], a grayscale version of CIFAR-
10 and Fashion MNIST [38]. Additionally, we show
the evaluation on noise, adversarial noise and adver-
sarial samples. The noise is generated in the same
way as the noise we use for training. For adversar-
ial noise, where we maximize the maximal conﬁdence
over all classes (see Lpout(f, z) in (3)), we use PGD
with 200 iterations and stepsize 0.0075 in the ǫ ball
wrt the k·k∞-norm with ǫ = 0.3 (same as in training).
Note that for training we use only 40 iterations, so that
the attack at test time is signiﬁcantly stronger. Fi-
nally, we check also the conﬁdence on adversarial sam-
ples computed for the test set of the in-distribution
dataset using 80 iterations of PGD with ǫ = 0.3, step-
size 0.0075 for MNIST and ǫ = 0.1, stepsize 0.0025 for
the other datasets. The latter two evaluation modali-
ties are novel compared to [15, 24, 22]. The adversarial
noise is interesting as it actively searches for images
which still yield high conﬁdence in a neighborhood of a
noise image and thus is a much more challenging than
the pure evaluation on noise. Moreover, it potentially
detects an over-adaptation to the noise model used dur-
ing training in particular in CEDA. The evaluation on
adversarial samples is interesting as one can hope that
the reduction of the conﬁdence for out-of-distribution
images also reduces the conﬁdence of adversarial sam-
ples as typically adversarial samples are oﬀ the data

45

Noise

Samples
MNIST

Adversarial

Noise
MNIST
for Plain

Figure 2: Top row: our generated noise images based on uniform noise resp. permuted MNIST together with a Gaussian ﬁlter and contrast
rescaling. Bottom row: for each noise image from above we generate the corresponding adversarial noise image using PGD with 40 iterations
maximizing the second part of the loss in ACET for the plain model. Note that neither in the noise images nor in the adversarially modiﬁed
ones there is structure similar to a MNIST image. For ACET and CEDA it is very diﬃcult to generate adversarial noise images for the fully
trained models thus we omit them.

Noise

Samples
CIFAR-10

Adversarial

Noise

CIFAR-10
for Plain

Adversarial

Noise

CIFAR-10
for CEDA

Adversarial

Noise

CIFAR-10
for ACET

Figure 3: Top row: our generated noise images based on uniform noise resp. permuted MNIST together with a Gaussian ﬁlter and contrast
rescaling (similar to Figure 2). Bottom rows: the corresponding adversarial images for the plain, CEDA, and ACET models. Neither the noise
nor the adversarial noise images show similarity to CIFAR-10 images.

manifold [33] and thus are also out-of-distribution sam-
ples (even though their distance to the true distribution
is small). Note that our models have never seen ad-
versarial samples during training, they only have been
trained using the adversarial noise. Nevertheless our
ACET model can reduce the conﬁdence on adversarial
samples. As evaluation criteria we use the average con-
ﬁdence, the area under the ROC curve (AUC) where
we use the conﬁdence as a threshold for the detection
problem (in-distribution vs. out-distribution). More-
over, we report in the same setting the false positive
rate (FPR) when the true positive rate (TPR) is ﬁxed
to 95%. All results can be found in Table 1.

Main Results: In Table 1, we show the results of
plain (normal training), CEDA and ACET. First of all,
we observe that there is almost no diﬀerence between
the test errors of all three methods. Thus improving
the conﬁdence far away from the training data does

not impair the generalization performance. We also
see that the plain models always produce high conﬁ-
dence predictions on noise images and completely fail
on adversarial noise. CEDA produces low conﬁdence
on noise images but mostly fails on adversarial noise
which was to be expected as similar ﬁndings have been
made for the creation of adversarial samples. Only
ACET consistently produces low conﬁdence predictions
on adversarial noise and has high AUROC. For the
out-of-distribution datasets CEDA and ACET improve
most of the time the maximum conﬁdence and the AU-
ROC, sometimes with very strong improvements like
on MNIST evaluated on FMNIST or SVHN evaluated
on LSUN. However, one observes that it is more dif-
ﬁcult to reduce the conﬁdence for related tasks e.g.
MNIST evaluated on EMNIST or CIFAR-10 evaluated
on LSUN, where the image structure is more similar.
Finally, an interesting outcome is that ACET reduces

46

Trained on
MNIST

MNIST
FMNIST
EMNIST
grayCIFAR-10
Noise
Adv. Noise
Adv. Samples

Trained on

SVHN

SVHN
CIFAR-10
CIFAR-100
LSUN CR
Imagenet-
Noise
Adv. Noise
Adv. Samples

Trained on
CIFAR-10

CIFAR-10
SVHN
CIFAR-100
LSUN CR
Imagenet-
Noise
Adv. Noise
Adv. Samples

Trained on
CIFAR-100

CIFAR-100
SVHN
CIFAR-10
LSUN CR
Imagenet-
Noise
Adv. Noise
Adv. Samples

Plain (TE: 0.51%)

CEDA (TE: 0.74%)

ACET (TE: 0.66%)

MMC AUROC FPR@95 MMC AUROC FPR@95
–
0.991
0.027
0.654
0.821
0.358
0.000
0.492
0.000
0.463
0.002
1.000
0.999
0.953

–
0.994
0.895
1.000
1.000
0.998
0.549

0.987
0.373
0.787
0.105
0.100
0.102
0.987

–
0.121
0.374
0.003
0.000
1.000
0.992

–
0.972
0.883
0.996
0.998
0.031
0.358

MMC AUROC FPR@95
–
0.986
0.003
0.239
0.752
0.313
0.000
0.101
0.000
0.100
0.042
0.162
0.854
0.782

–
0.998
0.912
1.000
1.000
0.992
0.692

Plain (TE: 3.53%)

CEDA (TE: 3.50%)

ACET (TE: 3.52%)

MMC AUROC FPR@95 MMC AUROC FPR@95
–
0.980
0.732
0.209
0.205
0.730
0.084
0.722
0.232
0.725
0.720
0.000
0.940
1.000
1.000
0.994

–
0.960
0.959
0.984
0.955
1.000
0.062
0.009

0.977
0.551
0.527
0.364
0.574
0.100
0.946
0.995

–
0.348
0.350
0.324
0.340
0.325
1.000
1.000

–
0.938
0.935
0.945
0.939
0.943
0.004
0.004

MMC AUROC FPR@95
–
0.978
0.435
0.140
0.139
0.414
0.012
0.148
0.113
0.368
0.100
0.000
0.000
0.101
0.369
0.279

–
0.973
0.971
0.997
0.977
1.000
1.000
0.778

Plain (TE: 8.87%)

CEDA (TE: 8.87%)

ACET (TE: 8.44%)

MMC AUROC FPR@95 MMC AUROC FPR@95
–
0.949
0.146
0.800
0.720
0.764
0.738
0.680
0.704
0.757
0.000
0.825
0.983
1.000
1.000
1.000

–
0.978
0.850
0.864
0.853
1.000
0.032
0.014

–
0.783
0.715
0.667
0.698
0.818
1.000
1.000

0.946
0.327
0.761
0.735
0.749
0.100
0.985
1.000

–
0.850
0.856
0.872
0.858
0.827
0.035
0.034

MMC AUROC FPR@95
–
0.948
0.118
0.263
0.711
0.764
0.745
0.677
0.678
0.744
0.000
0.100
0.008
0.112
0.633
0.590

–
0.981
0.852
0.858
0.859
1.000
0.999
0.512

Plain (TE: 31.97%)

CEDA (TE: 32.74%)

ACET (TE: 32.24%)

MMC AUROC FPR@95 MMC AUROC FPR@95
0.751
–
0.410
0.570
0.855
0.560
0.887
0.592
0.531
0.808
0.000
0.614
0.985
1.000
0.999
1.000

–
0.865
0.856
0.887
0.827
0.928
1.000
1.000

–
0.710
0.718
0.690
0.744
0.672
0.000
0.010

0.734
0.290
0.547
0.581
0.504
0.010
0.985
0.999

–
0.874
0.711
0.678
0.749
1.000
0.015
0.012

MMC AUROC FPR@95
0.728
–
0.345
0.234
0.860
0.530
0.881
0.554
0.492
0.819
0.000
0.010
0.003
0.013
0.863
0.975

–
0.912
0.720
0.698
0.752
1.000
0.998
0.267

Table 1: On the four datasets MNIST, SVHN, CIFAR-10, and CIFAR-100, we train three models: Plain, CEDA and ACET. We evaluate
them on out-of-distribution samples (other image datasets, noise, adversarial noise and adversarial samples built from the test set on which was
trained). We report test error of all models and show the mean maximum conﬁdence (MMC) on the in- and out-distribution samples (lower is
better for out-distribution samples), the AUC of the ROC curve (AUROC) for the discrimination between in- and out-distribution based on
conﬁdence value (higher is better), and the FPR at 95% true positive rate for the same problem (lower is better).

the conﬁdence on adversarial examples, see Figure 4 for
an illustration, and achieves on MNIST very high AU-
ROC values so that one can detect adversarial exam-
ples via thresholding the conﬁdence. Obviously, plain
and CEDA fail on this task. The good performance

of ACET is to some extent unexpected as we just bias
the model towards uniform conﬁdence over all classes
far away from the training data, but adversarial exam-
ples are still close to the original image. In summary,
ACET does improve conﬁdence estimates signiﬁcantly

47

MNIST SVHN CIFAR-10 CIFAR-100 MNIST SVHN CIFAR-10 CIFAR-100

Plain

ACET

Median α
% overconﬁdent

1.5

98.7%

28.1
99.9%

8.1

99.9%

9.9

99.8%

> 106
0.0% 50.2%

49.8

45.3
3.4%

9.9

0.0%

Table 2: We evaluate all trained models on uniform random inputs scaled by a constant α ≥ 1 (note that the resulting inputs will not
constitute valid images anymore, since in most cases they exceed the [0, 1]d box). We ﬁnd the minimum α such that the models outputs 99.9%
conﬁdence on them, and report the median over 10 000 trials. As predicted by Theorem 3.1 we observe that it is always possible to obtain
overconﬁdent predictions just by scaling inputs by some constant α, and for plain models this constant is smaller than for ACET. For MNIST
the value of α was so high that we ran into numerical problems. Second row: we show the percentage of overconﬁdent predictions (higher than
95% conﬁdence) when projecting back the α-rescaled uniform noise images back to [0, 1]d. One observes that there are much less overconﬁdent
predictions for the ACET models compared to standard training.

Plain

CEDA

ACET

Figure 4: Histogram of conﬁdence values (logarithmic scale) of adversarial samples for MNIST test points. ACET is the only model where
most adversarial samples have very low conﬁdence. Note, however that the ACET model has not been trained on adversarial samples of MNIST,
but only on adversarial noise.

compared to the plain but also compared to CEDA, in
particular on adversarial noise and adversarial exam-
ples. In particular, its very good eﬀect also on adver-
sarial examples is an interesting side eﬀect and shows
in our opinion that the models have become more reli-
able.

Far away high conﬁdence predictions: Theo-
rem 3.1 states that ReLU networks always attain high
conﬁdence predictions far away from the training data.
The two network architectures used in this paper are
ReLU networks.
It is thus interesting to investigate
if the conﬁdence-enhanced training CEDA or ACET
makes it harder to reach high conﬁdence than for the
plain model. We do the following experiment: we take
uniform random noise images x and then search for the
smallest α such that the classiﬁer attains 99.9% con-
ﬁdence on αx. This is exactly the construction from
Theorem 3.1 and the result can be found in Table 2.

We observe in Table 2 that indeed the required up-
scaling factor α is signiﬁcantly higher for CEDA and
ACET than for the plain model which implies that
our models also inﬂuence the network far away from
the training data. Moreover, this also shows that even
training methods explicitly aiming at counteracting the
phenomenon of high conﬁdence predictions far away
from the training data, cannot prevent this.

6. Conclusion

We have shown in this paper that the problem of ar-
bitrarily high conﬁdence predictions of ReLU networks
far away from the training data cannot be avoided even
with modiﬁcations like temperature rescaling [12]. It is
an inherent problem of the neural network architecture
and thus can only be resolved by changing the archi-
tecture. On the other hand we have shown that CEDA
and in particular ACET are a good way to reach much
better conﬁdence estimates for image data. CEDA and
ACET can be directly used for any model with lit-
tle implementation overhead. For the future it would
be desirable to have network architectures which have
provably the property that far away from the training
data the conﬁdence is uniform over the classes: the
network knows when it does not know.

Acknowledgements

M.H. and J.B. acknowledge support from the BMBF
through the T¨ubingen AI Center (FKZ: 01IS18039A)
and by the DFG TRR 248, project number 389792660
and the DFG Excellence Cluster “Machine Learning -
New Perspectives for Science”, EXC 2064/1, project
number 390727645.

48

References

[1] R. Arora, A. Basuy, P. Mianjyz, and A. Mukherjee.
Understanding deep neural networks with rectiﬁed lin-
ear unit. In ICLR, 2018. 2

[2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated
gradients give a false sense of security: Circumventing
defenses to adversarial examples. In ICML, 2018. 1

[3] P. Bartlett and M. H. Wegkamp. Classiﬁcation with a
reject option using a hinge loss. JMLR, 9:1823–1840,
2008. 3

[4] A. Bendale and T. Boult. Towards open set deep net-

works. In CVPR, 2016. 1

[5] N. Carlini and D. Wagner. Adversarial examples are
not easily detected: Bypassing ten detection methods.
In ACM Workshop on Artiﬁcial Intelligence and Secu-
rity, 2017. 1

[6] N. Carlini and D. Wagner. Towards evaluating the
In IEEE Symposium

robustness of neural networks.
on Security and Privacy, 2017. 1

[7] G. Cohen, S. Afshar, J. Tapson, and A. van Schaik.
Emnist: an extension of mnist to handwritten letters.
preprint, arXiv:1702.05373v2, 2017. 5

[8] F. Croce and M. Hein. A randomized gradient-free

attack on relu networks. In GCPR, 2018. 2

[9] T. DeVries and G. W. Taylor. Learning conﬁdence
for out-of-distribution detection in neural networks.
preprint, arXiv:1802.04865v1, 2018. 1

[10] Y. Gal and Z. Ghahramani. Dropout as a bayesian ap-
proximation: Representing model uncertainty in deep
learning. In ICML, 2016. 1

[11] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples. In ICLR, 2015.
1, 3, 4

[12] C. Guo, G. Pleiss, Y. Sun, and K. Weinberger. On
calibration of modern neural networks. In ICML, 2017.
1, 8

[13] K. He, X. Zhang, , S. Ren, and J. Sun. Deep residual
learning for image recognition. In CVPR, pages 770–
778, 2016. 5

[14] M. Hein and M. Andriushchenko. Formal guarantees
on the robustness of a classiﬁer against adversarial ma-
nipulation. In NIPS, 2017. 1

[15] D. Hendrycks and K. Gimpel. A baseline for detecting
misclassiﬁed and out-of-distribution examples in neu-
ral networks. In ICLR, 2017. 1, 5

[16] D. Hendrycks, M. Mazeika, and T. Dietterich. Deep
In ICLR,

anomaly detection with outlier exposure.
2019. 1, 4

[17] M. Jordan, U. Lewis, and A. G. Dimakis. Provable cer-
tiﬁcates for adversarial examples: Fitting a ball in the
union of polytopes. arXiv preprint, arXiv:1903.08778,
2019. 3

[18] D. P. Kingma and J. Ba. Adam: A method for stochas-
arXiv preprint arXiv:1412.6980,

tic optimization.
2014. 5

[19] A. Krizhevsky. Learning multiple layers of features

from tiny images. technical report, 2009. 5

[20] B. Lakshminarayanan, A. Pritzel, and C. Blundell.
Simple and scalable predictive uncertainty estimation
using deep ensembles. In NIPS, 2017. 1

[21] M. Lapin, M. Hein, and B. Schiele. Loss functions for
top-k error: Analysis and insights. In CVPR, 2016. 3
[22] K. Lee, H. Lee, K. Lee, and J. Shin. Training
conﬁdence-calibrated classiﬁers for detecting out-of-
distribution samples. In ICLR, 2018. 1, 5

[23] C. Leibig, V. Allken, M. S. Ayhan, P. Berens, and
S. Wahl. Leveraging uncertainty information from
deep neural networks for disease detection. Scientiﬁc
Reports, 7, 2017. 1

[24] S. Liang, Y. Li, and R. Srikant. Enhancing the relia-
bility of out-of-distribution image detection in neural
networks. In ICLR, 2018. 1, 3, 5

[25] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Valdu. Towards deep learning models resistant to
adversarial attacks. In ICLR, 2018. 1, 2, 4, 5

[26] M. Mirman, T. Gehr, and M. Vechev. Diﬀerentiable
abstract interpretation for provably robust neural net-
works. In ICML, 2018. 1

[27] G. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On
the number of linear regions of deep neural networks.
In NIPS, 2014. 2

[28] E. Nalisnick, A. Matsukawa, Y. Whye Teh, D. Gorur,
Do deep generative
preprint,

and B. Lakshminarayanan.
models know what they don’t know?
arXiv:1810.09136v1, 2018. 1, 4

[29] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu,
and A. Y. Ng. Reading digits in natural images with
unsupervised feature learning.
In NIPS Workshop
on Deep Learning and Unsupervised Feature Learning,
2011. 5

[30] A. Nguyen, J. Yosinski, and J. Clune. Deep neural
networks are easily fooled: High conﬁdence predictions
for unrecognizable images. In CVPR, 2015. 1

[31] A. Raghunathan, J. Steinhardt, and P. Liang. Certiﬁed
defenses against adversarial examples. In ICLR, 2018.
1

[32] P. F. S.-M. Moosavi-Dezfooli, A. Fawzi. Deepfool: a
simple and accurate method to fool deep neural net-
works. In CVPR, pages 2574–2582, 2016. 1

[33] D. Stutz, M. Hein, and B. Schiele. Disentangling ad-
In CVPR,

versarial robustness and generalization.
2019. 6

[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-
han, I. Goodfellow, and R. Fergus. Intriguing proper-
ties of neural networks.
In ICLR, pages 2503–2511,
2014. 1, 4

[35] A. Tewari and P. Bartlett. On the consistency of
multiclass classiﬁcation methods. Journal of Machine
Learning Research, 8:1007–1025, 2007. 1

[36] W. Wang, A. Wang, A. Tamar, X. Chen, and
P. Abbeel. Safer classiﬁcation by synthesis. preprint,
arXiv:1711.08534v2, 2018. 1

[37] E. Wong and J. Z. Kolter. Provable defenses against
adversarial examples via the convex outer adversarial
polytope. In ICML, 2018. 1

49

[38] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a
novel image dataset for benchmarking machine learn-
ing algorithms. preprint, arXiv:1708.07747, 2017. 5

[39] F. Yu, A. Seﬀ, Y. Zhang, S. Song, T. Funkhouser, and
J. Xiao. Lsun: Construction of a large-scale image
dataset using deep learning with humans in the loop.
preprint, arXiv:1506.03365v3, 2015. 5

50

