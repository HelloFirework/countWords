Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation

Xiaobing Wang1, Yingying Jiang1, Zhenbo Luo1, Cheng-Lin Liu2, 3, Hyunsoo Choi4, Sungjin Kim4

1Samsung Research China - Beijing, Beijing 100028, China

2National Laboratory of Pattern Recognition

Institute of Automation of Chinese Academy of Sciences, Beijing 100190, China

3University of Chinese Academy of Sciences, Beijing 100049, China

4Samsung Research, Seoul 06765, Korea

{x0106.wang, yy.jiang, zb.luo, hsu.choi, sj9373.kim}@samsung.com, liucl@nlpr.ia.ac.cn

Abstract

Scene text detection attracts much attention in comput-
er vision, because it can be widely used in many applica-
tions such as real-time text translation, automatic informa-
tion entry, blind person assistance, robot sensing and so on.
Though many methods have been proposed for horizontal
and oriented texts, detecting irregular shape texts such as
curved texts is still a challenging problem. To solve the
problem, we propose a robust scene text detection method
with adaptive text region representation. Given an input im-
age, a text region proposal network is ﬁrst used for extract-
ing text proposals. Then, these proposals are veriﬁed and
reﬁned with a reﬁnement network. Here, recurrent neural
network based adaptive text region representation is pro-
posed for text region reﬁnement, where a pair of bound-
ary points are predicted each time step until no new points
are found. In this way, text regions of arbitrary shapes are
detected and represented with adaptive number of bound-
ary points. This gives more accurate description of text
regions. Experimental results on ﬁve benchmarks, namely,
CTW1500, TotalText, ICDAR2013, ICDAR2015 and MSRA-
TD500, show that the proposed method achieves state-of-
the-art in scene text detection.

1. Introduction

Text is the most fundamental medium for communicat-
ing semantic information. It appears everywhere in daily
life: on street nameplates, store signs, product packages,
restaurant menus and so on. Such texts in natural envi-
ronment are known as scene texts. Automatically detecting
and recognizing scene texts can be very rewarding with nu-
merous applications, such as real-time text translation, blind
person assistance, shopping, robots, smart cars and educa-
tion. An end-to-end text recognition system usually consist-

s of two steps: text detection and text recognition. In text
detection, text regions are detected and labeled with their
bounding boxes. And in text recognition, text information
is retrieved from the detected text regions. Text detection
is an important step for end-to-end text recognition, with-
out which texts can not be recognized from scene images.
Therefore, scene text detection attracts much attention these
years.

While traditional optical character reader (OCR) tech-
niques can only deal with texts on printed documents or
business cards, scene text detection tries to detect various
texts in complex scenes. Due to complex backgrounds and
variations of font, size, color, language, illumination con-
dition and orientation, scene text detection becomes a very
challenging task. And its performance was poor when hand
designed features and traditional classiﬁers were used be-
fore deep learning methods become popular. However, the
performance has been much improved in recent years, sig-
niﬁcantly beneﬁtted from the development of deep learning.
Meanwhile, the research focus of text detection has shift-
ed from horizontal scene texts [10] to multi-oriented scene
texts [9] and more challenging curved or arbitrary shape
scene texts [19]. Therefore, arbitrary shape scene text de-
tection is focused on in this paper.

In this paper, we propose an arbitrary shape scene text
detection method using adaptive text region representation,
as shown in Figure 1. Given an input image, a text region
proposal network (Text-RPN) is ﬁrst used for obtaining text
proposals. The Convolutional Neural Network (CNN) fea-
ture maps of the input image are also obtained in this step.
Then, text proposals are veriﬁed and reﬁned with a reﬁne-
ment network, whose input are the text proposal features
obtained by using region of interest (ROI) pooling to the C-
NN feature maps. Here, three branches including text/non-
text classiﬁcation, bounding box reﬁnement and recurrent
neural network (RNN) based adaptive text region represen-
tation exist in the reﬁnement network. In the RNN, a pair

6449

of boundary points are predicted each time step until the
stop label is predicted. In this way, arbitrary shape text re-
gions can be represented with adaptive number of boundary
points. For performance evaluation, the proposed method
is tested on ﬁve benchmarks, namely, CTW1500, TotalTex-
t, ICDAR2013, ICDAR2015 and MSRA-TD500. Experi-
mental results show that the proposed method can process
not only multi-oriented scene texts but also arbitrary shape
scene texts including curved texts. Moreover, it achieves
state-of-the-art performances on the ﬁve datasets.

2. Related work

Traditional sliding window based and Connected com-
ponent (CC) based scene text detection methods had been
widely used before deep learning became the most promis-
ing machine learning tool. Sliding window based method-
s [27, 32] move a multi-scale window over an image and
classify the current patch as text or non-text. CC based
methods, especially the Maximally Stable Extremal Re-
gions (MSER) based methods [26, 30], get character candi-
dates by extracting CCs. And then, these candidate CCs are
classiﬁed as text or non-text. These methods usually adopt
a bottom-up strategy and often need several steps to detec-
t texts (e.g., character detection, text line construction and
text line classiﬁcation). As each step may lead to misclassi-
ﬁcation, the performances of these traditional text detection
methods are poor.

Recently, deep learning based methods have become
popular in scene text detection. These methods can be
divided into three groups, including bounding box regres-
sion based methods, segmentation based methods, and com-
bined methods. Bounding box regression based method-
s [5, 8, 11, 12, 13, 16], which are inspired by general ob-
ject detection methods such as SSD [14] and Faster R-
CNN [23], treat text as a kind of object and directly esti-
mate its bounding box as the detection result. Segmentation
based methods [3, 19, 33] try to solve the problem by seg-
menting text regions from the background and an additional
step is needed to get the ﬁnal bounding boxes. Combined
methods [20] use a similar strategy as Mask R-CNN [4], in
which both segmentation and bounding box regression are
used for better performance. However, its processing time
is increased because more steps are needed than previous
methods. Among the three kinds of methods, bounding box
regression based methods are the most popular in scene text
detection, beneﬁtted from the development of general ob-
ject detection.

For bounding box regression based methods, they can
be divided into one-stage methods and two-stage meth-
ods. One-stage methods including Deep Direct Regres-
sion [5], TextBox [12], TextBoxes++ [11], DMPNet [16],
SegLink [24] and EAST [34], directly estimate bounding
boxes of text regions in one step. Two-stage methods in-

clude R2CNN [8], RRD [13], RRPN [22], IncepText [28]
and FEN [31]. They consist of text proposal generation
stage, in which candidate text regions are generated, and
bounding box reﬁnement stage, in which candidate text re-
gions are veriﬁed and reﬁned to generate the ﬁnal detec-
tion result. Two-stage methods usually achieve higher per-
formances than one-stage methods. Therefore, the idea of
two-stage detection is used in this paper.

While most proposed scene text detection methods can
only deal with horizontal or oriented texts, detecting arbi-
trary shape texts such as curved text attracts more attention
recently.
In CTD [17], a polygon of ﬁxed 14 points are
used to represent text region. Meanwhile, recurrent trans-
verse and longitudinal offset connection (TLOC) is pro-
posed for accurate curved text detection. Though a polygon
of ﬁxed 14 points is enough for most text regions, it is not
enough for some long curve text lines. Besides, 14 points
are too many for most horizontal and oriented texts, while
4 points are enough for these texts. In TextSnake [19], a
text instance is described as a sequence of ordered, over-
lapping disks centered at symmetric axes of text region-
s. Each disk is associated with potentially variable radius
and orientation, which are estimated via a Fully Convolu-
tional Network (FCN) model. Moreover, Mask TextSpot-
ter [20] which is inspired by Mask R-CNN, can handle
text instances of irregular shapes via semantic segmenta-
tion. Though TextSnake and Mask TextSpotter both can
deal with text of arbitrary shapes, pixel-wise predictions are
both needed in them, which need heavy computation.

Considering a polygon of ﬁxed number of points is not
suitable for representing text regions of different shapes, an
adaptive text region representation using different numbers
of points for texts of different shapes is proposed in this
paper. Meanwhile, a RNN is employed to learn the adaptive
representation of each text region, with which text regions
can be directly labeled and pixel-wise segmentation is not
needed.

3. Methodology

It consists of two steps:

Figure 1 shows the ﬂowchart of the proposed method for
arbitrary shape text detection, which is a two-stage detec-
tion method.
text proposal and
proposal reﬁnement. In text proposal, a Text-RPN is used
to generate text proposals of an input image. Meanwhile,
the CNN feature maps of the input image are obtained here,
which can be used in the following. Then, text proposals are
veriﬁed and reﬁned through a reﬁnement network. In this
step, text/non-text classiﬁcation, bounding box regression
and RNN based adaptive text region representation are in-
cluded. Finally, text regions labeled with polygons of adap-
tive number of points are output as the detection result.

6450

Figure 1. Flowchart of the proposed method for arbitrary shape scene text detection. With adaptive text region representation used, detected
text regions can be labeled with adaptive number of pairwise points.

3.1. Adaptive text region representation

region representation.

The existing scene text detection methods use polygons
of ﬁxed number of points to represent text regions. For hori-
zontal texts, 2 points (left-top point and bottom-right point)
are used to represent the text regions. For multi-oriented
texts, the 4 points of their bounding boxes are used to rep-
resent these regions. Moreover, for curved texts, 14 points
are adopted in CTW1500 [17] for text region representa-
tion. However, for some very complex scene texts, such
as curved long text, even 14 points may be not enough to
represent them well. While for most scene texts such as
horizontal texts and oriented texts, less than 14 points are
enough and using 14 points to represent these text regions
is a waste.

Therefore, it is reasonable to consider using polygons of
adaptive numbers of points to represent text regions. Easi-
ly, we can imagine that corner points on the boundary of a
text region can be used for region representation, as shown
in Figure 2 (a). And this is similar as the method for an-
notating general objects [1]. However, the points in this
way are not arranged in a direction and it may be difﬁcult
to learn the representation.
In the method for annotating
general objects, human correction may be needed for ac-
curate segmentation. Considering text regions usually have
approximate symmetry top boundary and down boundary as
shown in Figure 3, using the pairwise points from the two
boundaries for text region representation may be more suit-
able. It is much easier to learn the pairwise boundary points
from one end to the other end of text region, as shown in
Figure 2 (b). In this way, different scene text regions can
be represented by different numbers of points precisely, as
shown in Figure 3. Moreover, to our knowledge, we are
the ﬁrst to use adaptive numbers of pairwise points for text

Figure 2. Two methods for adaptive text region representation. (a)
text region represented by corner points; (b) text region represent-
ed by pairwise points on its top and down boundaries.

Figure 3. Samples of text regions with adaptive representation. (a)
text region represented by 4 points; (b) text region represented by
6 points; (c) text region represented by 12 points.

6451

3.2. Text proposal

When an input image is given, the ﬁrst step of the pro-
posed method is text proposal, in which text region can-
didates called text proposals are generated by Text-RPN.
The Text-RPN is similar as RPN in Faster R-CNN [23] ex-
cept different backbone networks and anchor sizes. In the
proposed method, the backbone network is SE-VGG16 as
shown in Table 1, which is obtained by adding Squeeze-
and-Excitation (SE) blocks [7] to VGG16 [25]. As shown
in Figure 4, SE blocks adaptively recalibrate channel-wise
feature responses by explicitly modelling interdependencies
between channels, which can produce signiﬁcant perfor-
mance improvement. Here, FC means fully connected lay-
er and ReLU means Rectiﬁed Linear Unit function. More-
over, because scene texts usually have different sizes, an-
chor sizes are set as {32, 64, 128, 256, 512} for covering
more texts while aspect ratios {0.5, 1, 2} are kept.

Layer
Conv1
Pool1
SE1

Conv2
Pool2
SE2

Conv3
Pool3
SE3

Conv4
Pool4
SE4

Conv5
Pool5
SE5

Kernel

[3 × 3, 64] × 2
2 × 2, stride 2

4, 64

[3 × 3, 128] × 2
2 × 2, stride 2

8, 128

[3 × 3, 256] × 3
2 × 2, stride 2

16, 256

[3 × 3, 512] × 3
2 × 2, stride 2

32, 512

[3 × 3, 512] × 3
2 × 2, stride 2

32, 512

Table 1. The architecture of SE-VGG16 network. For SE block,
its kernel means the channel numbers of the two FC layers in it.

branches: text/non-text classiﬁcation, bounding box regres-
sion and RNN based adaptive text region representation.
Here, text/non-text classiﬁcation and bounding box regres-
sion are similar as other two-stage text detection methods,
while the last branch is proposed for arbitrary shape text
representation.

For the proposed branch, the input are the features of
each text proposal, which are obtained by using ROI pool-
ing to the CNN feature maps generated with SE-VGG16.
The output target of this branch is the adaptive number of
boundary points for each text region. Because the output
length changes for different text regions, it is reasonable to
use RNN to predict these points. Therefore, Long Short-
Term Memory (LSTM) [6] is used here, which is a kind of
RNN and popular for processing sequence learning prob-
lem, such as machine translation, speech recognition, image
caption and text recognition.

Though it is proposed that pairwise boundary points are
used for text region representation, different ways can be
used for pairwise points representation. Easily, we can
imagine that using the coordinates of two pairwise points
(xi, yi, xi+1, yi+1) to represent them. In this way, the co-
ordinates of pairwise points are used as the regression tar-
gets as shown in Figure 5. However, pairwise points can be
represented in a different way, using the coordinate of their
center point (xc
i ), the distance from the center point to
them hi, and their orientation θi. However, the angle target
is not stable in some special situations. For example, angle
near 90◦ is very similar to angle near −90◦ in spatial, but
their angles are quite different. This makes the network hard
to learn the angle target well. Besides, the orientation can
be represented by sin θi and cos θi, which can be predicted
stably. However, more parameters are needed. Therefore,
the coordinates of points (xi, yi, xi+1, yi+1) are used as the
regression targets in the proposed method.

i , yc

Figure 4. The architecture of SE block.

Figure 5. LSTM used for learning text region representation. The
input of each time step in the LSTM are the ROI pooling features
of the corresponding text proposals.

3.3. Proposal reﬁnement

After text proposal, text region candidates in the input
image are generated, which will be veriﬁed and reﬁned in
this step. As shown in Figure 1 a reﬁnement network is em-
ployed for proposal reﬁnement, which consists of several

The inputs of all time steps in the LSTM used here are
the same, which are the ROI pooling features of the cor-
responding text proposals. And the output of each time
step are the coordinates of the pairwise points on text re-
gion boundary. Meanwhile, as adaptive numbers of points
are used for different text regions, a stop label is needed

6452

to represent when the predicting network stops. Because
stop label prediction is a classiﬁcation problem while coor-
dinates prediction is a regression problem, it is not appropri-
ate to put them in the same branch. Therefore, there are two
branches in each time step of the LSTM: one for point coor-
dinate regression and one for stop label prediction. At each
time step the coordinates of two pairwise boundary points
of text region and the label stop/continue are predicted. If
the label is continue, the coordinates of another two points
and a new label are predicted in the next time step. Other-
wise, the prediction stops and text region is represented with
the points predicted before. In this way, text regions in the
input image can be detected and represented with different
polygons made up by the predicted pairwise points.

While Non-Maximum Suppression (NMS) is extensively
used to post-process detection candidates by general object
detection methods, it is also needed in the proposed method.
As the detected text regions are represented with polygons,
normal NMS which is computed based on the area of hori-
zontal bounding box is not suitable here. Instead, a polygon
NMS is used, which is computed based on the area of the
polygon of text region. After NMS, the remaining text re-
gions are output as the detection result.

3.4. Training objective

As Text-RPN in the proposed method is similar as the
RPN in Faster R-CNN [23], the training loss of Text-RPN
is also computed in the similar way as it. Therefore, in this
section, we only focus on the loss function of reﬁnement
network in proposal reﬁnement. The loss deﬁned on each
proposal is the sum of a text/non-text classiﬁcation loss, a
bounding box regression loss, a boundary points regression
loss and a stop/continue label classiﬁcation loss. The multi-
task loss function on each proposal is deﬁned as:

Lsum =Lcls(p, t) + λ1tXi∈{x,y,w,h}
+ λ2tXi∈{x1,y1,x2,y2,...,xn,yn}
+ λ3tXi∈{l1,l2,...,xn/2}

Lcls(li, l∗
i )

Lreg(vi, v∗
i )

Lreg(ui, u∗
i )

(1)

λ1, λ2 and λ3 are balancing parameters that control the
trade-off between these terms and they are set as 1 in the
proposed method.

For the text/non-text classiﬁcation loss term, t is the in-
dicator of the class label. Text is labeled as 1 (t = 1), and
background is labeled as 0 (t = 0). The parameter p =
(p0, p1) is the probability over text and background classes
computed after softmax. Then, Lcls(p, t) = − log pt is the
log loss for true class t.

For the bounding box regression loss term, v =
(vx, vy, vw, vh) is a tuple of true bounding box regression

targets including coordinates of the center point and its
width and height, and v∗ = (v∗
h) is the predict-
ed tuple for each text proposal. We use the parameteriza-
tion for v and v∗ given in Faster R-CNN [23], in which
v and v∗ specify scale-invariant translation and log-space
height/width shift relative to an object proposal.

w, v∗

x, v∗

y, v∗

For the boundary points regression loss term, u =
(ux1 , uy1 , . . . , uxn , uyn ) is a tuple of true coordinates of
boundary points, and u∗ = (u∗
yn ) is the
predicted tuple for the text label. To make the points learned
suitable for text of different scales, the learning targets
should also be processed to make them scale invariant. The
parameters (u∗

yi ) are processed as following:

y1 , . . . , u∗

xn , u∗

xi , u∗

x1 , u∗

u∗
xi = (x∗

i − xa)/wa, u∗

yi = (y∗

i − ya)/ha,

(2)

i and y∗

where x∗
i denote the coordinates of the boundary
points, xa and ya denote the coordinates of the center point
of the corresponding text proposal, wa and ha denote the
width and height of this proposal.
Let (w, w∗) indicates (vi, v∗

i ), Lreg(w, w∗)
is deﬁned as the smooth L1 loss as in Faster R-CNN [23]:

i ) or (ui, u∗

Lreg(w, w∗) = smoothL1(w − w∗),

smoothL1(x) = (cid:26)

0.5x2
|x| − 0.5

if |x| < 1
otherwise

(3)

(4)

For the stop/continue label classiﬁcation loss term, it is
also a binary classiﬁcation and its loss is formatted similar
as text/non-text classiﬁcation loss.

4. Experiments

4.1. Benchmarks

Five benchmarks are used in this paper for performance

evaluation, which are introduced in the following:

• CTW1500: The CTW1500 dataset [17] contains 500
test images and 1000 training images, which contain
multi-oriented text, curved text and irregular shape tex-
t. Text regions in this dataset are labeled with 14 scene
text boundary points at sentence level.

• TotalText: The TotalText dataset [2] consists of 300
test images and 1255 training images with more than 3
different text orientations: horizontal, multi-oriented,
and curved. The texts in these images are labeled at
word level with adaptive number of corner points.

• ICDAR2013: The ICDAR2013 dataset [10] contains
focused scene texts for ICDAR Robust Reading Com-
petition. It includes 233 test images and 229 training
images. The scene texts are horizontal and labeled
with horizontal bounding boxes made up by 2 points
at word level.

6453

• ICDAR2015: The ICDAR2015 dataset [9] focuses on
incidental scene text in ICDAR Robust Reading Com-
petition. It includes 500 testing images and 1000 train-
ing images. The scene texts have different orientation-
s, which are labeled with inclined boxes made up by 4
points at word level.

• MSRA-TD500: The MSRA-TD500 dataset [29] con-
tains 200 test images and 300 training images, that
contain arbitrarily-oriented texts in both Chinese and
English. The texts are labeled with inclined boxes
made up by 4 points at sentence level. Some long s-
traight text lines exist in the dataset.

The evaluation for text detection follows the ICDAR e-
valuation protocol in terms of Recall, Precision and Hmean.
Recall represents the ratio of the number of correctly de-
tected text regions to the total number of text regions in the
dataset while Precision represents the ratio of the number
of correctly detected text regions to the total number of de-
tected text regions. Hmean is single measure of quality by
combining recall and precision. A detected text region is
considered as correct if its overlap with the ground truth text
region is larger than a given threshold. The computation of
the three evaluation terms is usually different for differen-
t datasets. While the results on ICDAR 2013 and ICDAR
2015 can be evaluated through ICDAR robust reading com-
petition platform, the results of the other three datasets can
be evaluated with the given evaluation methods correspond-
ing to them.

4.2. Implementation details

Our scene text detection network is initialized with pre-
trained VGG16 model for ImageNet classiﬁcation. When
the proposed method is tested on the ﬁve datasets, differ-
ent models are used for them, which are trained using only
the training images of each dataset with data augmentation.
All models are trained 10 × 104 iterations in total. Learn-
ing rates start from 10−3, and are multiplied by 1/10 after
2 × 104, 6 × 104 and 8 × 104 iterations. We use 0.0005
weight decay and 0.9 momentum. We use multi-scale train-
ing, setting the short side of training images as {400, 600,
720, 1000, 1200}, while maintaining the long side at 2000.
Because adaptive text region representation is used in
the proposed method,
it can be simply used for these
datasets with text regions labeled with different numbers of
points. As ICDAR 2013, ICDAR 2015 and MSRA-TD500
are labeled with quadrilateral boxes, they are easy to be
transformed into pairwise points. However, for CTW1500
dataset and TotalText dataset, some operations are needed
to transform the ground truthes into the form we needed.

Text regions in CTW1500 are labeled with 14 points,
which are needed to be transformed into adaptive number
of pairwise points. First, the 14 points are grouped into 7

point pairs. Then, we compute the intersection angle for
each point, which is the angle of the two vectors from cur-
rent point to its nearby two points. And for each point pair,
the angle is the smaller one of the two points. Next, point
pairs are sorted according to their angles in descending or-
der and we try to remove each point pair in the order. If
the ratio of the polygon areas after removing operation to
the original area is larger than 0.93, this point pair can be
removed. Otherwise, the operation stops and the remaining
points are used in the training for text region representation.
text regions in TotalText are labeled with
adaptive number of points, but these points are not pairwise.
For text regions labeled with even number of points, it is
easy to process by group them into pairs. For text regions
labeled with odd number of points, the start two points and
the end two points should be found ﬁrst, and then the corre-
sponding points to the remaining points are found based on
their distances to the start points on the boundary.

Moreover,

The results of the proposed method are obtained on sin-
gle scale input image with one trained model. Because test
image scale has a deep impact on the detection results, such
as FOTS [15] uses different scales for different datasets, we
also use different test scales for different datasets for best
performance. In our experiments, the scale for ICDAR 2013
is 960 × 1400, the scale for ICDAR 2015 is 1200 × 2000
and the scales for other datasets are all 720 × 1280.

The proposed method is implemented in Caffe and the

experiments are ﬁnished using a Nvidia P40 GPU.

4.3. Ablation study

In the proposed method the backbone network is SE-
VGG16, while VGG16 is usually used by other state-of-
the-art methods. To verify the effectiveness of the back-
bone network, we test the proposed method with different
backbone networks (SE-VGG16 vs VGG16) on CTW1500
dataset and ICDAR 2015 dataset as shown in Table 2. The
results show that SE-VGG16 is better than VGG16, with
which better performances achieved on the two datasets.

Backbone Recall Precision Hmean

CTW1500

VGG16

SE-VGG16

VGG16

SE-VGG16

79.7
80.1

79.1
80.2
ICDAR2015
83.3
86.0

90.4
89.2

79.4
80.1

86.8
87.6

Table 2. Ablation study on backbone network.

Meanwhile, an adaptive text region representation is pro-
posed for text of arbitrary shapes in this paper. To vali-
date its effectiveness for scene text detection, we add an
ablation study on text region representation on CTW1500

6454

dataset. For comparison, ﬁxed text region representation
directly uses the ﬁxed 14 points as the regression targets in
the experiment. Table 3 shows the experimental results of
different text region representation methods on CTW1500
dataset. The recall of the method with adaptive represen-
tation is much higher than ﬁxed representation (80.2% vs
76.4%). It justiﬁes that the adaptive text region representa-
tion is more suitable for texts of arbitrary shapes.

Method

Recall Precision Hmean

SegLink [24]
EAST [34]

DeconvNet [2]

Mask Textspotter [20]

TextSnake [19]

Proposed

23.8
36.2
44.0
55.0
74.5
76.2

30.3
50.0
33.0
69.0
82.7
80.9

26.7
42.0
36.0
61.3
78.4
78.5

Table 5. Results on TotalText.

Representation Recall Precision Hmean

Fixed

Adaptive

76.4
80.2

80.0
80.1

78.2
80.1

Table 3. Ablation study on text region representation.

4.4. Comparison with State of the arts

To show the performance of the proposed method for
different shape texts, we test it on several benchmarks.
We ﬁrst compare its performance with state-of-the-arts on
CTW1500 and TotalText which both contains challenging
multi-oriented and curved texts. Then we compare the
methods on the two most widely used benchmarks:
IC-
DAR2013 and ICDAR2015. At last we compare them on
MSRA-TD500 which contains long straight text lines and
multi-language texts (Chinese+English).

Table 4 and Table 5 compare the proposed method with
state-of-the-art methods on CTW1500 and TotalText, re-
spectively. The propose method is much better than all other
methods on CTW1500 including the methods designed for
curved texts such as CTD, CTD+TLOC and TextSnake (H-
mean: 80.1% vs 69.5%, 73.4% and 75.6%). Meanwhile, it
also achieves better performance (Hmean: 78.5%) than all
other methods on TotalText. The performances on the two
datasets containing challenging multi-oriented and curved
texts mean that the proposed method can detect scene text
of arbitrary shapes.

scale input image with single model, only the results gen-
erated in this situation are used here. The results show that
the proposed method can also process horizontal text well.

Method

Recall Precision Hmean

TextBoxes [12]
SegLink [24]
He et al. [5]
Lyu et al. [21]

FOTS [15]
RRPN [22]
FEN [31]

Mask Textspotter [20]

Proposed

83.0
83.0
81.0
84.4

-

87.9
89.1
88.6
89.7

88.0
87.7
92.0
92.0

-

94.9
93.6
95.0
93.7

Table 6. Results on ICDAR2013.

85.0
85.3
86.0
88.0
88.2
91.3
91.3
91.7
91.7

Table 7 shows the experimental results on ICDAR 2015
dataset and the proposed method achieve the second best
performance, which is only a little lower than FOTS (H-
mean: 87.6% vs 88.0%). While FOTS is trained end-to-end
by combining text detection and recognition, the proposed
method is only trained for text detection, which is much
easier to train than FOTS. And the results tested on single
scale input image with single mode are used here. The re-
sults show that the proposed method achieves comparable
performance with state-of-the-arts, which means it can also
process multi-oriented text well.

Method

Recall Precision Hmean

Method

Recall Precision Hmean

SegLink [24]
EAST [34]

DMPNet [16]

CTD [17]

CTD+TLOC [17]

TextSnake [19]

Proposed

40.0
49.1
56.0
65.2
69.8
85.3
80.2

42.3
78.7
69.9
74.3
77.4
67.9
80.1

40.8
60.4
62.2
69.5
73.4
75.6
80.1

Table 4. Results on CTW1500.

Table 6 shows the experimental results on ICDAR2013
dataset. The proposed method achieves the best perfor-
mance same as Mask Textspotter, whose Hmean both are
91.7%. Because the proposed method is tested on single

SegLink [24]
RRPN [22]
He et al. [5]
R2CNN [8]

TextSnake [19]
PixelLink [3]
InceptText [28]

Mask Textspotter [20]

Proposed
FOTS [15]

76.8
77.0
81.0
79.7
80.4
82.0
80.6
81.0
86.0

-

73.1
84.0
92.0
85.6
84.9
85.5
90.5
91.6
89.2

-

Table 7. Results on ICDAR2015.

75.0
80.0
86.0
82.5
82.6
83.7
85.3
86.0
87.6
88.0

Table 8 shows the results on MSRA-TD500 dataset and

6455

it shows that our detection method can support long straight
text line detection and Chinese+English detection well. It
achieves Hmean of 83.6% and is better than all other meth-
ods.

Method

Recall Precision Hmean

EAST [34]
SegLink [24]
PixelLink [3]
TextSnake [19]
InceptText [28]

MCN [18]
Proposed

67.4
70.0
73.2
73.9
79.0
79.0
82.1

87.3
86.0
83.0
83.2
87.5
88.0
85.2

76.1
77.0
77.8
78.3
83.0
83.0
83.6

Table 8. Results on MSRA-TD500.

4.5. Speed

The speed of the proposed method is compared with two
other methods as shown in Table 9, which are all able to
deal with arbitrary shape scene text. From the results, we
can see that the speed of the proposed method is much faster
than the other two methods. While pixel-wise prediction is
needed in Mask Textspotter and TextSnake, it is not needed
in the proposed method and less computation is needed.

Method

TextSnake [19]

Mask Textspotter [20]

Proposed

Scale
768
720
720

Speed
1.1 fps
6.9 fps
10.0 fps

Table 9. Speed compared on different detection methods support-
ing arbitrary shape texts.

4.6. Qualitative results

Figure 6 illustrates qualitative results on CTW1500, To-
talText, ICDAR2013, ICDAR2015 and MSRA-TD500. It
shows that the proposed method can deal with various texts
of arbitrarily oriented or curved, different languages, non-
uniform illuminations and different text lengths at word lev-
el or sentence level.

5. Conclusion

In this paper, we propose a robust arbitrary shape scene
text detection method with adaptive text region representa-
tion. After text proposal using a Text-RPN, each text region
is veriﬁed and reﬁned using a RNN for predicting adaptive
number of boundary points. Experiments on ﬁve bench-
marks show that the proposed method can not only detect
horizontal and oriented scene texts but also work well for ar-
bitrary shape scene texts. Particularly, it outperforms exist-
ing methods signiﬁcantly on CTW1500 and MSRA-TD500,

Figure 6. Results on different datasets. (a) results on CTW1500;
(b) results on TotalText; (c) results on ICDAR2013; (d) results on
ICDAR2015; (e) results on MSRA-TD500.

which are typical of curved texts and multi-oriented texts,
respectively.

In the future, the proposed method can be improved in
several aspects. First, arbitrary shape scene text detection
may can be improved by using corner point detection. This
will require easier annotations for training images. Second,
to fulﬁll the ﬁnal goal of text recognition end-to-end text
recognition for arbitrary shape scene text will be consid-
ered.

6456

References

[1] Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, and Sanja
Fidler. Annotating object instances with a polygon-rnn. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion(CVPR), pages 5230–5238, 2017.

[2] Chee Kheng Chng and Chee Seng Chan. Total-text: A
comprehensive dataset for scene text detection and recogni-
tion. In International Conference on Document Analysis and
Recognition(ICDAR), pages 935–942, 2017.

[3] Dan Deng, Haifeng Liu, Xuelong Li, and Deng Cai. Pix-
ellink: Detecting scene text via instance segmentation.
In
Proceedings of the Thirty-Second AAAI Conference on Arti-
ﬁcial Intelligence, 2018.

[4] Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Gir-
In IEEE International Conference on

shick. Mask r-cnn.
Computer Vision (ICCV), pages 2980–2988, 2017.

[5] Wenhao He, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu.
Deep direct regression for multi-oriented scene text detec-
tion.
In IEEE International Conference on Computer Vi-
sion(ICCV), pages 745–753, 2017.

[6] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term

memory. Neural Computation, 9(8):1735–1780, 1997.

[7] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition(CVPR), pages 7132–7141, 2018.

[8] Yingying Jiang, Xiangyu Zhu, Xiaobing Wang, Shuli Yang,
Wei Li, Hua Wang, Pei Fu, and Zhenbo Luo. R2cnn: Rota-
tional region cnn for arbitrarily-oriented scene text detection.
In International Conference on Pattern Recognition(ICPR),
pages 3610–3615, 2018.

[9] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos
Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-
mura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-
drasekhar, Shijian Lu, Faisal Shafait, Seiichi Uchida, and
Ernest Valveny. Icdar 2015 competition on robust reading. In
International Conference on Document Analysis and Recog-
nition(ICDAR), pages 1156–1160, 2015.

[10] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,
Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles
Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-
mazan, and Lluis Pere de las Heras. Icdar 2013 robust read-
ing competition. In International Conference on Document
Analysis and Recognition(ICDAR), pages 1484–1493, 2013.
[11] Minghui Liao, Baoguang Shi, and Xiang Bai. Textboxes++:
IEEE Transac-

A single-shot oriented scene text detector.
tions on Image Processing, 27(8):3676–3690, 2018.

[12] Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang,
and Wenyu Liu. Textboxes: A fast text detector with a sin-
gle deep neural network. In Proceedings of the Thirty-First
AAAI Conference on Artiﬁcial Intelligence, pages 4161–
4167, 2017.

[13] Minghui Liao, Zhen Zhu, Baoguang Shi, Gui-Song Xia, and
Xiang Bai. Rotation-sensitive regression for oriented scene
text detection. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition(CVPR), pages 5909–5918, 2018.

[14] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.

Berg. Ssd: Single shot multibox detector. In European Con-
ference on Computer Vision(ECCV), pages 21–37, 2016.

[15] Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and
Junjie Yan. Fots: Fast oriented text spotting with a uniﬁed
network. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition(CVPR), pages 5676–5685.

[16] Yuliang Liu and Lianwen Jin. Deep matching prior net-
work: Toward tighter multi-oriented text detection.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition(CVPR), pages 3454–3461.

[17] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and
Sheng Zhang. Curved scene text detection via transverse
and longitudinal sequence connection. Pattern Recognition,
90(6):337–345, 2018.

[18] Zichuan Liu, Guosheng Lin, Sheng Yang, Jiashi Feng, Weisi
Lin, and Wang Ling Goh. Learning markov clustering net-
works for scene text detection. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition(CVPR), 2018.

[19] Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He,
Wenhao Wu, and Cong Yao. Textsnake: A ﬂexible repre-
sentation for detecting text of arbitrary shapes. In European
Conference on Computer Vision(ECCV), pages 19–35, 2018.
[20] Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, and
Xiang Bai. Mask textspotter: An end-to-end trainable neural
network for spotting text with arbitrary shapes. In European
Conference on Computer Vision(ECCV), pages 71–88, 2018.
[21] Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, and
Xiang Bai. Multi-oriented scene text detection via corner
localization and region. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition(CVPR), pages 7553–
7563, 2018.

[22] Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Y-
ingbin Zheng, and Xiangyang Xue. Arbitrary-oriented scene
text detection via rotation proposals. IEEE Transactions on
Multimedia, 20(11):3111–3122, 2018.

[23] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: towards real-time object detection with region
proposal networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 39(6):1137–1149, 2017.

[24] Baoguang Shi, Xiang Bai, and Serge Belongie. Detect-
ing oriented text in natural images by linking segments. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion(CVPR), pages 3482–3490, 2017.

[25] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Inter-
national Conference on Learning Representations, 2015.

[26] Lei Sun, Qiang Huo, and Wei Jia. A robust approach for text
detection from natural scene images. Pattern Recognition,
48(9):2906–2920, 2015.

[27] Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai
Yu, and Chew Lim Tan. Text ﬂow: A uniﬁed text detec-
tion system in natural scene images. In IEEE International
Conference on Computer Vision (ICCV), pages 4651–4659,
2015.

[28] Qiangpeng Yang, Mengli Cheng, Wenmeng Zhou, Yan
Chen, Minghui Qiu, and Wei Lin.
Inceptext: A new
inception-text module with deformable psroi pooling for
multi-oriented scene text detection. pages 1071–1077, 2018.

6457

[29] Cong Yao, Xiang Bai, Wenyu Liu, Yi Ma, and Zhuowen Tu.
Detecting texts of arbitrary orientations in natural images. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion(CVPR), pages 1083–1090, 2012.

[30] Xu-Cheng Yin, Xuwang Yin, Kaizhu Huang, and Hong-Wei
Hao. Robust text detection in natural scene images. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
36(5):970–983, 2014.

[31] Sheng Zhang, Yuliang Liu, Lianwen Jin, and Canjie Luo.
Feature enhancement network: A reﬁned scene text detec-
tor. In Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, pages 2612–2619, 2017.

[32] Zheng Zhang, Wei Shen, Cong Yao, and Xiang Bai.
Symmetry-based text line detection in natural scenes.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition(CVPR), pages 2558–2567, 2015.

[33] Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao,
Wenyu Liu, and Xiang Bai. Multi-oriented text detection
with fully convolutional networks.
In IEEE Conference
on Computer Vision and Pattern Recognition(CVPR), pages
4159–4167, 2016.

[34] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang
Zhou, Weiran He, and Jiajun Liang. East: An efﬁcient and
accurate scene text detector. In IEEE International Confer-
ence on Computer Vision (ICCV), pages 2642–2651, 2017.

6458

