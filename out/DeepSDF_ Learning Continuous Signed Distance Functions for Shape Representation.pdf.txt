DeepSDF: Learning Continuous Signed Distance Functions

for Shape Representation

Jeong Joon Park1,3

Peter Florence 2,3

Julian Straub3

Richard Newcombe3

Steven Lovegrove3

1University of Washington

2Massachusetts Institute of Technology

3Facebook Reality Labs

Figure 1: DeepSDF represents signed distance functions (SDFs) of shapes via latent code-conditioned feed-forward decoder networks.
Above images are raycast renderings of DeepSDF interpolating between two shapes in the learned shape latent space. Best viewed digitally.

Abstract

1. Introduction

Computer graphics, 3D computer vision and robotics
communities have produced multiple approaches to rep-
resenting 3D geometry for rendering and reconstruction.
These provide trade-offs across ﬁdelity, efﬁciency and com-
pression capabilities. In this work, we introduce DeepSDF,
a learned continuous Signed Distance Function (SDF) rep-
resentation of a class of shapes that enables high qual-
ity shape representation, interpolation and completion from
partial and noisy 3D input data. DeepSDF, like its clas-
sical counterpart, represents a shape’s surface by a con-
tinuous volumetric ﬁeld: the magnitude of a point in the
ﬁeld represents the distance to the surface boundary and the
sign indicates whether the region is inside (-) or outside (+)
of the shape, hence our representation implicitly encodes a
shape’s boundary as the zero-level-set of the learned func-
tion while explicitly representing the classiﬁcation of space
as being part of the shapes’ interior or not. While classical
SDF’s both in analytical or discretized voxel form typically
represent the surface of a single shape, DeepSDF can repre-
sent an entire class of shapes. Furthermore, we show state-
of-the-art performance for learned 3D shape representation
and completion while reducing the model size by an order
of magnitude compared with previous work.

Work performed while Park and Florence were interns at Facebook.

Deep convolutional networks which are a mainstay of
image-based approaches grow quickly in space and time
complexity when directly generalized to the 3rd spatial di-
mension, and more classical and compact surface repre-
sentations such as triangle or quad meshes pose problems
in training since we may need to deal with an unknown
number of vertices and arbitrary topology. These chal-
lenges have limited the quality, ﬂexibility and ﬁdelity of
deep learning approaches when attempting to either input
3D data for processing or produce 3D inferences for object
segmentation and reconstruction.

In this work, we present a novel representation and ap-
proach for generative 3D modeling that is efﬁcient, expres-
sive, and fully continuous. Our approach uses the concept
of a SDF, but unlike common surface reconstruction tech-
niques which discretize this SDF into a regular grid for eval-
uation and measurement denoising [14], we instead learn a
generative model to produce such a continuous ﬁeld.

The proposed continuous representation may be intu-
itively understood as a learned shape-conditioned classiﬁer
for which the decision boundary is the surface of the shape
itself, as shown in Fig. 2. Our approach shares the genera-
tive aspect of other works seeking to map a latent space to
a distribution of complex shapes in 3D [52], but critically
differs in the central representation. While the notion of an

1165

Decision 
boundary
of implicit 

surface

(a)

(b)

(c)

Figure 2: Our DeepSDF representation applied to the Stanford
Bunny: (a) depiction of the underlying implicit surface SDF = 0
trained on sampled points inside SDF < 0 and outside SDF > 0
the surface, (b) 2D cross-section of the signed distance ﬁeld, (c)
rendered 3D surface recovered from SDF = 0. Note that (b) and
(c) are recovered via DeepSDF.

implicit surface deﬁned as a SDF is widely known in the
computer vision and graphics communities, to our knowl-
edge no prior works have attempted to directly learn contin-
uous, generalizable 3D generative models of SDFs.

Our contributions include: (i) the formulation of gen-
erative shape-conditioned 3D modeling with a continuous
implicit surface, (ii) a learning method for 3D shapes based
on a probabilistic auto-decoder, and (iii) the demonstration
and application of this formulation to shape modeling and
completion. Our models produce high quality continuous
surfaces with complex topologies, and obtain state-of-the-
art results in quantitative comparisons for shape reconstruc-
tion and completion. As an example of the effectiveness
of our method, our models use only 7.4 MB (megabytes)
of memory to represent entire classes of shapes (for exam-
ple, thousands of 3D chair models) – this is, for example,
less than half the memory footprint (16.8 MB) of a single
uncompressed 5123 3D bitmap.

2. Related Work

We review three main areas of related work: 3D rep-
resentations for shape learning (Sec. 2.1), techniques for
learning generative models (Sec. 2.2), and shape comple-
tion (Sec. 2.3).

2.1. Representations for 3D Shape Learning

Representations for data-driven 3D learning approaches
can be largely classiﬁed into three categories: point-based,
mesh-based, and voxel-based methods. While some appli-
cations such as 3D-point-cloud-based object classiﬁcation
are well suited to these representations, we address their
limitations in expressing continuous surfaces with complex
topologies.
Point-based. A point cloud is a lightweight 3D representa-

tion that closely matches the raw data that many sensors (i.e.
LiDARs, depth cameras) provide, and hence is a natural ﬁt
for applying 3D learning. PointNet [36], for example, uses
max-pool operations to extract global shape features, and
the technique is widely used as an encoder for point genera-
tion networks [55, 1]. There is a sizable list of related works
for learning on point clouds [37, 51, 56]. A primary limi-
tation, however, of learning with point clouds is that they
do not describe topology and are not suitable for producing
watertight surfaces.

Mesh-based. Various approaches represent classes of sim-
ilarly shaped objects, such as morphable human body parts,
with predeﬁned template meshes and some of these models
demonstrate high ﬁdelity shape generation results [2, 31].
Other recent works [3] use poly-cube mapping [48] for
shape optimization. While the use of template meshes is
convenient and naturally provides 3D correspondences, it
can only model shapes with ﬁxed mesh topology.

Other mesh-based methods use existing [45, 33] or
learned [19, 20] parameterization techniques to describe 3D
surfaces by morphing 2D planes. The quality of such repre-
sentations depends on parameterization algorithms that are
often sensitive to input mesh quality and cutting strategies.
To address this, recent data-driven approaches [55, 19] learn
the parameterization task with deep networks. They report,
however, that (a) multiple planes are required to describe
complex topologies but (b) the generated surface patches
are not stitched, i.e. the produced shape is not closed. To
generate a closed mesh, sphere parameterization may be
used [19, 20], but the resulting shape is limited to the topo-
logical sphere. Other works related to learning on meshes
propose to use new convolution and pooling operations for
meshes [16, 50] or general graphs [8].

Voxel-based. Voxels, which non-parametrically describe
volumes with 3D grids of values, are perhaps the most natu-
ral extension into the 3D domain of the well-known learning
paradigms (i.e., convolution) that have excelled in the 2D
image domain. The most straightforward variant of voxel-
based learning is to use a dense occupancy grid (occupied /
not occupied). Due to the cubically growing compute and
memory requirements, however, current methods are only
able to handle low resolutions (1283 or below). As such,
voxel-based approaches do not preserve ﬁne shape details
[54, 13], and additionally voxels visually appear signiﬁ-
cantly different than high-ﬁdelity shapes, since when ren-
dered their normals are not smooth. Octree-based methods
[49, 41, 22] alleviate the compute and memory limitations
of dense voxel methods, extending for example the ability to
learn at up to 5123 resolution [49], but even this resolution
is far from producing shapes that are visually compelling.

Aside from occupancy grids, and more closely related to
our approach, it is also possible to use a 3D grid of vox-
els to represent a signed distance function. This inherits

2166

from the success of fusion approaches that use a truncated
SDF (TSDF), pioneered in [14, 35], to combine noisy depth
maps into a single 3D model. Voxel-based SDF represen-
tations have been extensively used for 3D shape learning
[57, 15, 46], but their use of discrete voxels is expensive
in memory. As a result, these approaches generally present
low resolution shapes. Wavelet transform-based methods
[27] and dimensionality reduction techniques [27] for dis-
tance ﬁelds were reported, but they encode the SDF volume
of each individual scene rather than a dataset of shapes.

Recently, concurrently to our work, binary implicit sur-
face representations were used by [12, 34], where they train
deep networks across classes of shapes to classify 3D points
as inside or outside of a shape. Note that the binary occu-
pancy function is a special case of SDF, considering only
‘sign’ of SDF values. As DeepSDF models metric signed
distance to the surface, it can be used to raycast against sur-
faces and compute surface normals with its gradients.

2.2. Representation Learning Techniques

Modern representation learning techniques aim at auto-
matically discovering a set of features that compactly but
expressively describe data. For a more extensive review of
the ﬁeld, we refer to Bengio et al. [4].
Generative Adversial Networks. GANs [18] and their
variants [11, 39, 26, 28] learn deep embeddings of target
data, from which realistic images are sampled, by training
discriminators adversarially against generators. In the 3D
domain, Wu et al. [52] trains a GAN to generate objects in a
voxel form, while Hamu et al. [20] uses multiple parameter-
ization planes to generate shapes of topological spheres. On
the downside, training for GANs is known to be unstable.
Auto-encoders. The ability of auto-encoders as a represen-
tation learning tool has been evidenced by the vast variety of
3D shape learning works in the literature [15, 46, 2, 19, 53]
who adopt auto-encoders for representation learning. Re-
cent 3D vision works [5, 2, 31] often adopt a variational
auto-encoder (VAE) learning scheme, in which bottleneck
features are perturbed with Gaussian noise to encourage
smooth and complete latent spaces. The regularization on
the latent vectors enables exploring the embedding space
with gradient descent or random sampling.
Optimizing Latent Vectors. Instead of using the full auto-
encoder, an alternative is to learn compact data represen-
tations by training decoder-only networks. This idea goes
back to at least the work of Tan et al. [47] which simulta-
neously optimizes the latent vectors assigned to each data
point and the decoder weights through back-propagation.
For inference, an optimal latent vector is searched to match
the new observation with ﬁxed decoder parameters. Similar
approaches have been extensively studied in [40, 7, 38], for
applications including noise reduction, missing measure-
ment completions, and fault detections. Recent approaches

[6, 17] extend the technique by applying deep architectures.
Throughout the paper we refer to this class of networks as
auto-decoders, for they are trained with self-reconstruction
loss on decoder-only architectures.

2.3. Shape Completion

3D shape completion related works aim to infer unseen
parts of the original shape given sparse or partial input ob-
servations. This task is analogous to image-inpainting in 2D
computer vision.

Classical surface reconstruction methods complete a
point cloud into a dense surface by ﬁtting radial basis func-
tion (RBF) [9] to approximate implicit surface functions, or
by casting the reconstruction from oriented point clouds as
a Poisson problem [29]. These methods only model a single
shape rather than a dataset.

Various recent methods use data-driven approaches for
the 3D completion task. Most of these methods adopt
encoder-decoder architectures to reduce partial inputs of oc-
cupancy voxels [54], discrete SDF voxels [15], depth maps
[42], RGB images [13, 53] or point clouds [46] into a la-
tent vector and subsequently generate a prediction of full
volumetric shape based on learned priors.

3. Modeling SDFs with Neural Networks

In this section we present DeepSDF, our continuous
shape learning approach. We describe modeling shapes
as the zero iso-surface decision boundaries of feed-forward
networks trained to represent SDFs. A signed distance func-
tion is a continuous function that, for a given spatial point,
outputs the point’s distance to the closest surface, whose
sign encodes whether the point is inside (negative) or out-
side (positive) of the watertight surface:

SDF (x) = s : x ∈ R3, s ∈ R .

(1)

The underlying surface is implicitly represented by the iso-
surface of SDF (·) = 0. A view of this implicit surface can
be rendered through raycasting or rasterization of a mesh
obtained with, for example, Marching Cubes [32].

Our key idea is to directly regress the continuous SDF
from point samples using deep neural networks. The result-
ing trained network is able to predict the SDF value of a
given query position, from which we can extract the zero
level-set surface by evaluating spatial samples. Such sur-
face representation can be intuitively understood as a spa-
tial classiﬁer for which the decision boundary is the surface
of the shape itself (Fig. 2). As a universal function approxi-
mator [24], deep feed-forward networks in theory can learn
continuous SDFs with arbitrary precision. Yet, the preci-
sion of the approximation in practice is limited by the ﬁnite
number of point samples that guide the decision boundaries
and the ﬁnite capacity of the network due to restricted com-
pute power.

3167

(a) Single Shape DeepSDF

(b) Coded Shape DeepSDF

Figure 3: DeepSDF network outputs SDF value at a 3D query lo-
cation. While (a) the network can memorize a single shape, (b)
conditioning the network with a code vector allows DeepSDF to
model a large space of shapes, where the shape information is con-
tained in the code vector that is concatenated with the query point.

The most direct application of this approach is to train a
single deep network for a given target shape as depicted in
Fig. 3a. Given a target shape, we prepare a set of pairs X
composed of 3D point samples and their SDF values:

X := {(x, s) : SDF (x) = s} .

(2)

We train the parameters θ of a multi-layer fully-connected
neural network fθ on the training set S to make fθ a good
approximator of the given SDF in the target domain Ω:

fθ(x) ≈ SDF (x), ∀x ∈ Ω .

(3)

The training is done by minimizing the sum over losses
between the predicted and real SDF values of points in X
under the following L1 loss function:

L(fθ(x), s) = | clamp(fθ(x), δ) − clamp(s, δ) |,

(4)

where clamp(x, δ) := min(δ, max(−δ, x)) introduces the
parameter δ to control the distance from the surface over
which we expect to maintain a metric SDF. Larger values
of δ allow for fast ray-tracing since each sample gives in-
formation of safe step sizes [23]. Smaller δ can be used
to concentrate network capacity on details near the surface.
We used δ = 0.1 in practice. (see supplementary for details)
Once trained, the surface is implicitly represented as the
zero iso-surface of fθ(x), which can be visualized through
raycasting or marching cubes. Another nice property of this
approach is that accurate normals can be analytically com-
puted by calculating the spatial derivative ∂fθ(x)/∂x via
back-propogation through the network.

4. Learning the Latent Space of Shapes

Training a speciﬁc neural network for each shape is nei-
ther feasible nor very useful. Instead, we want a model that
can represent a wide variety of shapes, discover their com-
mon properties, and embed them in a low dimensional latent
space. To this end, we introduce a latent vector z, which can
be thought of as encoding the desired shape, as a second in-
put to the neural network as depicted in Fig. 3b. Concep-
tually, we map this latent vector to a 3D shape represented

(a) Auto-encoder

(b) Auto-decoder

Figure 4: Different from an auto-encoder whose latent code is
produced by the encoder, an auto-decoder directly accepts a la-
tent vector as an input. A randomly initialized latent vector is
assigned to each data point in the beginning of training, and the la-
tent vectors are optimized along with the decoder weights through
standard backpropagation. During inference, decoder weights are
ﬁxed, and an optimal latent vector is estimated.

by a continuous SDF. Formally, for some shape indexed by
i, fθ is now a function of a latent code zi and a query 3D
location x, which outputs the shape’s approximate SDF:

fθ(zi, x) ≈ SDF i(x).

(5)

By conditioning the network output on a latent vector, this
formulation allows modeling multiple SDFs with a single
neural network. Given the decoding model fθ, the contin-
uous surface associated with a latent vector z is similarly
represented with the zero iso-surface of fθ(z, x), and the
shape can again be discretized for visualization by, for ex-
ample, raycasting or Marching Cubes.

Throughout the paper we use the coded shape DeepSDF
model of Fig. 3b whose decoder is a feed-forward network
composed of eight fully connected layers, each of them ap-
plied with dropouts. All internal layers are 512-dimensional
and have ReLU non-linearities. The output non-linearity
regressing the SDF value is tanh. We found training with
batch-normalization [25] to be unstable and applied the
weight-normalization technique instead [43]. For training,
we use the Adam optimizer [30].

In the next section we explain training the decoding
model fθ(z, x) and introduce the ‘auto-decoder’ formula-
tion for encoder-less training of shape-coded DeepSDF.

4.1. Motivating Encoder less Learning

Auto-encoders

and encoder-decoder networks

are
widely used for representation learning as their bottleneck
features tend to form natural latent variable representations.
Recently, in applications such as modeling depth maps
[5], faces [2], and body shapes [31] a full encoder-decoder
network is trained but only the decoder is retained for infer-
ence, where they search for an optimal latent vector given
some input observation. Since the trained encoder is unused
at test time, it is unclear whether (1) training encoder is an
effective use of computational resources and (2) it is neces-

4168

(x,y,z)SDFCode(x,y,z)SDFCodeInputOutputCodesOutputBackprogatesary for researchers to design encoders for various 3D input
representations (e.g. points, meshes, octrees, etc).

This motivates us to use an auto-decoder for learning a
shape embedding without an encoder as depicted in Fig. 4b.
We show that learning continuous SDFs with auto-decoder
leads to high quality 3D generative models. Further, we
develop a probabilistic formulation for training and test-
ing the auto-decoder that naturally introduces latent space
regularization for improved generalization. To the best of
our knowledge, this work is the ﬁrst to introduce the auto-
decoder learning method to the 3D learning community.

4.2. Auto decoder based DeepSDF Formulation

To derive the auto-decoder-based shape-coded DeepSDF
formulation we adopt a probabilistic perspective. Given a
dataset of N shapes represented with signed distance func-
tion SDF iN
i=1, we prepare a set of K point samples and
their signed distance values:

Xi = {(xj, sj) : sj = SDF i(xj)} .

(6)

For an auto-decoder, as there is no encoder, each latent
code zi is paired with training shape Xi. The posterior over
shape code zi given the shape SDF samples Xi can be de-
composed as:

pθ(zi|Xi) = p(zi)Q(xj ,sj )∈Xi

pθ(sj|zi; xj) ,

(7)

where θ parameterizes the SDF likelihood.
In the latent
shape-code space, we assume the prior distribution over
codes p(zi) to be a zero-mean multivariate-Gaussian with
a spherical covariance σ2I. This prior encapsulates the no-
tion that the shape codes should be concentrated and we
empirically found it was needed to infer a compact shape
manifold and to help converge to good solutions.

In the auto-decoder-based DeepSDF formulation we ex-
press the SDF likelihood via a deep feed-forward network
fθ(zi, xj) and, without loss of generality, assume that the
likelihood takes the form:

pθ(sj|zi; xj) = exp(−L(fθ(zi, xj), sj)) .

(8)

The SDF prediction ˜sj = fθ(zi, xj) is represented using a
fully-connected network. L(˜sj, sj) is a loss function penal-
izing the deviation of the network prediction from the actual
SDF value sj . One example for the cost function is the stan-
dard L2 loss function which amounts to assuming Gaussian
noise on the SDF values. In practice we use the clamped L1
cost from Eq. 4 for reasons outlined previously.

At training time we maximize the joint log posterior over
all training shapes with respect to the individual shape codes
{zi}N

i=1 and the network parameters θ:

arg min
θ,{zi}N

i=1

N

X

i=1

K


X


j=1

L(fθ(zi, xj), sj) +

1
σ2 ||zi||2

2


 .

(9)

Figure 5: Compared to car shapes memorized using OGN [49]
(right), our models (left) preserve details and render visually pleas-
ing results as DeepSDF provides oriented surace normals.

At inference time, after training and ﬁxing θ, a shape
code zi for shape Xi can be estimated via Maximum-a-
Posterior (MAP) estimation as:

ˆz = arg min

z X

(xj ,sj )∈X

L(fθ(z, xj), sj) +

1
σ2 ||z||2

2 . (10)

Crucially, this formulation is valid for SDF samples X
of arbitrary size and distribution because the gradient of the
loss with respect to z can be computed separately for each
SDF sample. This implies that DeepSDF can handle any
form of partial observations such as depth maps. This is
a major advantage over the auto-encoder framework whose
encoder expects a test input similar to the training data, e.g.
shape completion networks of [15, 56] require preparing
training data of partial shapes.

To incorporate the latent shape code, we stack the code
vector and the sample location as depicted in Fig. 3b and
feed it into the same fully-connected NN described previ-
ously at the input layer and additionally at the 4th layer. We
again use the Adam optimizer [30]. The latent vector z is
initialized randomly from N (0, 0.012).

Note that while both VAE and the proposed auto-decoder
formulation share the zero-mean Gaussian prior on the la-
tent codes, we found that the the stochastic nature of the
VAE optimization did not lead to good training results.

5. Data Preparation

To train our continuous SDF model, we prepare the SDF
samples X (Eq. 2) for each mesh, which consists of 3D
points and their SDF values. While SDF can be computed
through a distance transform for any watertight shapes from
real or synthetic data, we train with synthetic objects, (e.g.
ShapeNet [10]), for which we are provided complete 3D
shape meshes. To prepare data, we start by normalizing
each mesh to a unit sphere and sampling 500,000 spatial
points x’s: we sample more aggressively near the surface
of the object as we want to capture a more detailed SDF
near the surface. For an ideal oriented watertight mesh,
computing the signed distance value of x would only in-
volve ﬁnding the closest triangle, but we ﬁnd that human
designed meshes are commonly not watertight and con-
tain undesired internal structures. To obtain the shell of a

5169

Method

Type

3D-EPN [15]
OGN [49]
AtlasNet
-Sphere [19]
AtlasNet
-25 [19]
DeepSDF
(ours)

Voxel SDF

Octree

Discretization
323 voxels
2563 voxels

Parametric

1 patch

mesh

Parametric

25 patches

mesh

Continuous

none

SDF

X

X

X

X

Complex
topologies

Closed
surfaces

Surface
normals

Model

Inf.

size (GB)

time (s)

X

X

X

X

0.42
0.54
0.015

-

0.32
0.01

Eval.
tasks

C
K

K, U

0.172

0.32

K, U

X

X

0.0074

9.72

K, U, C

Table 1: Overview of the benchmarked methods. AtlasNet-Sphere can only describe topological-spheres, voxel/octree occupancy methods
(i.e. OGN) only provide 8 directions for normals, and AtlasNet does not provide oriented normals. Our tasks evaluated are: (K) representing
known shapes, (U) representing unknown shapes, and (C) shape completion.

mesh with proper orientation, we set up equally spaced vir-
tual cameras around the object, and densely sample surface
points, denoted Ps, with surface normals oriented towards
the camera. Double sided triangles visible from both orien-
tations (indicating that the shape is not closed) cause prob-
lems in this case, so we discard mesh objects containing too
many of such faces. Then, for each x, we ﬁnd the closest
point in Ps, from which the SDF (x) can be computed. We
refer readers to supplementary material for further details.

6. Results

We conduct a number of experiments to show the repre-
sentational power of DeepSDF, both in terms of its ability
to describe geometric details and its generalization capabil-
ity to learn a desirable shape embedding space. Largely, we
propose four main experiments designed to test its ability to
1) represent training data, 2) use learned feature representa-
tion to reconstruct unseen shapes, 3) apply shape priors to
complete partial shapes, and 4) learn smooth and complete
shape embedding space from which we can sample new
shapes. For all experiments we use the popular ShapeNet
[10] dataset.

We select a representative set of 3D learning approaches
to comparatively evaluate aforementioned criteria: a recent
octree-based method (OGN) [49], a mesh-based method
(AtlasNet) [19], and a volumetric SDF-based shape comple-
tion method (3D-EPN) [15] (Table 1). These works show
state-of-the-art performance in their respective representa-
tions and tasks, so we omit comparisons with the works
that have already been compared: e.g. OGN’s octree model
outperforms regular voxel approaches, while AtlasNet com-
pares itself with various points, mesh, or voxel based meth-
ods and 3D-EPN with various completion methods.

6.1. Representing Known 3D Shapes

First, we evaluate the capacity of the model to represent
known shapes, i.e. shapes that were in the training set, from
only a restricted-size latent code — testing the limit of ex-
pressive capability of the representations.

Method \metric

OGN
AtlasNet-Sph.
AtlasNet-25
DeepSDF

CD,
mean

0.167
0.210
0.157
0.084

CD,

median

0.127
0.185
0.140
0.058

EMD,
mean

0.043
0.046
0.060
0.043

EMD,
median

0.042
0.045
0.060
0.042

Table 2: Comparison for representing known shapes (K) for cars
trained on ShapeNet. CD = Chamfer Distance (30, 000 points)
multiplied by 103, EMD = Earth Mover’s Distance (500 points).

Quantitative comparison in Table 2 shows that the pro-
posed DeepSDF signiﬁcantly beats OGN and AtlasNet in
Chamfer distance against the true shape computed with a
large number of points (30,000). The difference in earth
mover distance (EMD) is smaller because 500 points do not
well capture the additional precision. Figure 5 shows a qual-
itative comparison of DeepSDF to OGN.

6.2. Representing Test 3D Shapes (auto encoding)

For encoding unknown shapes, i.e. shapes in the held-out
test set, DeepSDF again signiﬁcantly outperforms AtlasNet
on a wide variety of shape classes and metrics as shown
in Table 3. Note that AtlasNet performs reasonably well
at classes of shapes that have mostly consistent topology
without holes (like planes) but struggles more on classes
that commonly have holes, like chairs. This is shown in
Fig. 6 where AtlasNet fails to represent the ﬁne detail of the
back of the chair. Figure 7 shows more examples of detailed
reconstructions on test data from DeepSDF as well as two
example failure cases.

6.3. Shape Completion

A major advantage of the proposed DeepSDF approach
for representation learning is that inference can be per-
formed from an arbitrary number of SDF samples. In the
DeepSDF framework, shape completion amounts to solving
for the shape code that best explains a partial shape obser-
vation via Eq. 10. Given the shape-code a complete shape
can be rendered using the priors encoded in the decoder.

6170

(a) Ground-truth

(b) Our Result

(c) [19]-25 patch

(d) [19]-sphere

(e) Our Result

(f) [19]-25 patch

Figure 6: Reconstruction comparison between DeepSDF and AtlasNet [19] (with 25-plane and sphere parameterization) for test shapes.
Note that AtlasNet fails to capture the ﬁne details of the chair, and that (f) shows holes on the surface of sofa and the plane.

Figure 7: Reconstruction of test shapes. From left to right alternating: ground truth shape and our reconstruction. The two right most
columns show failure modes of DeepSDF. These failures are likely due to lack of training data and failure of minimization convergence.

CD, mean
AtlasNet-Sph.
AtlasNet-25
DeepSDF

CD, median
AtlasNet-Sph.
AtlasNet-25
DeepSDF

EMD, mean
AtlasNet-Sph.
AtlasNet-25
DeepSDF

Mesh acc., mean
AtlasNet-Sph.
AtlasNet-25
DeepSDF

chair
0.752
0.368
0.204

0.511
0.276
0.072

0.071
0.064
0.049

0.033
0.018
0.009

plane
0.188
0.216
0.143

0.079
0.065
0.036

0.038
0.041
0.033

0.013
0.013
0.004

table
0.725
0.328
0.553

0.389
0.195
0.068

0.060
0.073
0.050

0.032
0.014
0.012

lamp
2.381
1.182
0.832

2.180
0.993
0.219

0.085
0.062
0.059

0.054
0.042
0.013

sofa
0.445
0.411
0.132

0.330
0.311
0.088

0.050
0.063
0.047

0.017
0.017
0.004

Table 3: Comparison for representing unknown shapes (U) for
various classes of ShapeNet. Mesh accuracy as deﬁned in [44]
is the minimum distance d such that 90% of generated points are
within d of the ground truth mesh. Lower is better for all metrics.

We test our completion scheme using single view depth
observations which is a common use-case and maps well
to our architecture without modiﬁcation. Note that we cur-
rently require the depth observations in the canonical shape
frame of reference.

To generate SDF point samples from the depth image ob-
servation, we sample two points for each depth observation,
each of them located η distance away from the measured

Method
\Metric

chair
3D-EPN
DeepSDF
plane
3D-EPN
DeepSDF
sofa
3D-EPN
DeepSDF

lower is better
CD,
CD,
med. mean

EMD

Mesh
acc.

higher is better
Mesh
Cos
sim.
comp.

2.25
1.28

1.63
0.37

2.03
0.82

2.83
2.11

2.19
1.16

2.18
1.59

0.084
0.071

0.059
0.049

0.209
0.500

0.752
0.766

0.063
0.049

0.040
0.032

0.165
0.722

0.710
0.823

0.071
0.059

0.049
0.041

0.254
0.541

0.742
0.810

Table 4: Comparison for shape completion (C) from partial range
scans of unknown shapes from ShapeNet.

surface point (along surface normal estimate). With small
η we approximate the signed distance value of those points
to be η and −η, respectively. We solve for Eq. 10 with
loss function of Eq. 4 using clamp value of η. Additionally,
we incorporate free-space observations, (i.e. empty-space
between surface and camera), by sampling points along
the freespace-direction and enforce larger-than-zero con-
straints. The freespace loss is |fθ(z, xj)| if fθ(z, xj) < 0
and 0 otherwise.

Given the SDF point samples and empty space points,
we similarly optimize the latent vector using MAP estima-
tion. Tab. 4 and Figs. (8, 9) respectively shows quantitative
and qualitative shape completion results. Compared to one
of the most recent completion approaches [15] using volu-

7171

(a) Input Depth

(b) Completion (ours)

(c) Second View (ours)

(d) Ground truth

(e) 3D-EPN

Figure 8: For a given depth image visualized as a green point cloud, we show a comparison of shape completions from our DeepSDF
approach against the true shape and 3D-EPN.

(a) Noisy Input Point Cloud

(b) Shape Completion

Figure 9: Demonstration of DeepSDF shape completion from a
partial noisy point cloud. Input here is generated by perturbing the
3D point cloud positions generated by the ground truth depth map
by 1.5% of the plane length. We provide a comprehensive analysis
of robustness to noise in the supplementary material.

metric shape representation, our continuous SDF approach
produces more visually pleasing and accurate shape recon-
structions. While a few recent shape completion methods
were presented [21, 53], we could not ﬁnd the code to run
the comparisons, and their underlying 3D representation is
voxel grid which we extensively compare against.

6.4. Latent Space Shape Interpolation

To show that our learned shape embedding is complete
and continuous, we render the results of the decoder when
a pair of shapes are interpolated in the latent vector space
(Fig. 1). The results suggests that the embedded continuous
SDF’s are of meaningful shapes and that our representation
extracts common interpretable shape features, such as the
arms of a chair, that interpolate linearly in the latent space.

7. Conclusion & Future Work

DeepSDF signiﬁcantly outperforms

the applicable
benchmarked methods across shape representation and
completion tasks and simultaneously addresses the goals
of representing complex topologies, closed surfaces, while
providing high quality surface normals of the shape. How-
ever, while point-wise forward sampling of a shape’s SDF is
efﬁcient, shape completion (auto-decoding) takes consider-
ably more time during inference due to the need for explicit
optimization over the latent vector. We look to increase per-
formance by replacing ADAM optimization with more ef-
ﬁcient Gauss-Newton or similar methods that make use of
the analytic derivatives of the model.

DeepSDF models enable representation of more com-
plex shapes without discretization errors with signiﬁcantly
less memory than previous state-of-the-art results as shown
in Table 1, demonstrating an exciting route ahead for 3D
shape learning. The clear ability to produce quality latent
shape space interpolation opens the door to reconstruction
algorithms operating over scenes built up of such efﬁcient
encodings. However, DeepSDF currently assumes models
are in a canonical pose and as such completion in-the-wild
requires explicit optimization over a SE(3) transformation
space increasing inference time. Finally, to represent the
true space-of-possible-scenes including dynamics and tex-
tures in a single embedding remains a major challenge, one
which we continue to explore.

8172

References

[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas.
Learning representations and generative models for 3d point
clouds. 2018.

[2] T. Bagautdinov, C. Wu, J. Saragih, P. Fua, and Y. Sheikh.

Modeling facial geometry using compositional vaes. 1:1.

[3] P. Baqu´e, E. Remelli, F. Fleuret, and P. Fua. Geodesic
arXiv preprint

shape optimization.

convolutional
arXiv:1802.04016, 2018.

[4] Y. Bengio, A. Courville, and P. Vincent.

Representa-
tion learning: A review and new perspectives. TPAMI,
35(8):1798–1828, 2013.

[5] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and
A. J. Davison. Codeslam-learning a compact, optimis-
able representation for dense visual slam. arXiv preprint
arXiv:1804.00874, 2018.

[6] P. Bojanowski, A. Joulin, D. Lopez-Pas, and A. Szlam. Op-
timizing the latent space of generative networks.
In J. Dy
and A. Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 600–609. PMLR, 10–
15 Jul 2018.

[7] M. Bouakkaz and M.-F. Harkat. Combined input training and
radial basis function neural networks based nonlinear princi-
pal components analysis model applied for process monitor-
ing. In IJCCI, pages 483–492, 2012.

[8] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. arXiv
preprint arXiv:1312.6203, 2013.

[9] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R.
Fright, B. C. McCallum, and T. R. Evans. Reconstruction
and representation of 3d objects with radial basis functions.
In SIGGRAPH, pages 67–76. ACM, 2001.

[10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.

[11] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. Infogan: Interpretable representation learning
by information maximizing generative adversarial nets.
In
NIPS, pages 2172–2180, 2016.

[12] Z. Chen and H. Zhang. Learning implicit ﬁelds for generative

shape modeling. arXiv preprint arXiv:1812.02822, 2018.

[13] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In ECCV, pages 628–644. Springer, 2016.

[14] B. Curless and M. Levoy. A volumetric method for building
complex models from range images. In SIGGRAPH, pages
303–312. ACM, 1996.

[15] A. Dai, C. Ruizhongtai Qi, and M. Niessner. Shape comple-
tion using 3d-encoder-predictor cnns and shape synthesis. In
CVPR, pages 5868–5877, 2017.

[16] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-
tional neural networks on graphs with fast localized spectral
ﬁltering. In NIPS, pages 3844–3852, 2016.

[17] J. Fan and J. Cheng. Matrix completion by deep matrix fac-

torization. Neural Networks, 98:34–41, 2018.

[18] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, pages 2672–2680, 2014.

[19] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. Atlasnet: A papier-m\ˆ ach\’e approach to learn-
ing 3d surface generation. arXiv preprint arXiv:1802.05384,
2018.

[20] H. B. Hamu, H. Maron, I. Kezurer, G. Avineri, and Y. Lip-
arXiv

man. Multi-chart generative surface modeling.
preprint arXiv:1806.02143, 2018.

[21] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
resolution shape completion using deep neural networks for
global structure and local geometry inference.

[22] C. H¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-
diction for 3d object reconstruction.
In 3D Vision (3DV),
2017 International Conference on, pages 412–420. IEEE,
2017.

[23] J. C. Hart. Sphere tracing: A geometric method for the an-
tialiased ray tracing of implicit surfaces. The Visual Com-
puter, 12(10):527–545, 1996.

[24] K. Hornik, M. Stinchcombe, and H. White. Multilayer feed-
forward networks are universal approximators. Neural net-
works, 2(5):359–366, 1989.

[25] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.

[26] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.

Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017.

[27] M. W. Jones. Distance ﬁeld compression. Journal of WSCG,

12(2):199–204, 2004.

[28] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
arXiv preprint arXiv:1710.10196, 2017.

[29] M. Kazhdan and H. Hoppe. Screened poisson surface recon-

struction. ACM TOG, 32(3):29, 2013.

[30] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[31] O. Litany, A. Bronstein, M. Bronstein, and A. Makadia. De-
formable shape completion with graph convolutional autoen-
coders. CVPR, 2017.

[32] W. E. Lorensen and H. E. Cline. Marching cubes: A high
resolution 3d surface construction algorithm. In SIGGRAPH,
volume 21, pages 163–169. ACM, 1987.

[33] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym,
E. Yumer, V. G. Kim, and Y. Lipman. Convolutional neu-
ral networks on surfaces via seamless toric covers. 2017.

[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and
A. Geiger. Occupancy networks: Learning 3d reconstruction
in function space. arXiv preprint arXiv:1812.03828, 2018.

[35] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and
A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In ISMAR, pages 127–136. IEEE, 2011.

[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
In CVPR, pages 652–660, 2017.

9173

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, pages 5099–5108, 2017.

[55] Y. Yang, C. Feng, Y. Shen, and D. Tian. Foldingnet: In-
terpretable unsupervised learning on 3d point clouds. arXiv
preprint arXiv:1712.07262, 2017.

[56] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert. Pcn:

Point completion network. In 3DV, 2018.

[57] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and
T. Funkhouser. 3dmatch: Learning local geometric descrip-
tors from rgb-d reconstructions. In CVPR, pages 199–208.
IEEE, 2017.

[38] Z. Qunxiong and L. Chengfei. Dimensionality reduction
with input training neural network and its application in
chemical process modelling. Chinese Journal of Chemical
Engineering, 14(5):597–603, 2006.

[39] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015.

[40] V. Reddy and M. Mavrovouniotis. An input-training neu-
ral network approach for gross error detection and sensor
replacement. Chemical Engineering Research and Design,
76(4):478–489, 1998.

[41] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning
deep 3d representations at high resolutions. In CVPR, pages
6620–6629. IEEE, 2017.

[42] J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, and
D. Hoiem. Completing 3d object shape from one depth im-
age. In CVPR, pages 2484–2493, 2015.

[43] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NIPS, pages 901–909, 2016.

[44] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and
R. Szeliski. A comparison and evaluation of multi-view
stereo reconstruction algorithms.
pages 519–528. IEEE,
2006.

[45] A. Sinha, J. Bai, and K. Ramani. Deep learning 3d shape
surfaces using geometry images. In ECCV, pages 223–240.
Springer, 2016.

[46] D. Stutz and A. Geiger. Learning 3d shape completion from
In CVPR, pages

laser scan data with weak supervision.
1955–1964, 2018.

[47] S. Tan and M. L. Mayrovouniotis. Reducing data dimen-
sionality through optimizing neural network inputs. AIChE
Journal, 41(6):1471–1480, 1995.

[48] M. Tarini, K. Hormann, P. Cignoni, and C. Montani.
Polycube-maps. In ACM TOG, volume 23, pages 853–860.
ACM, 2004.

[49] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3d outputs. In ICCV, 2017.

[50] N. Verma, E. Boyer, and J. Verbeek. Feastnet: Feature-

steered graph convolutions for 3d shape analysis.

[51] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and
J. M. Solomon. Dynamic graph cnn for learning on point
clouds. arXiv preprint arXiv:1801.07829, 2018.

[52] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.
Learning a probabilistic latent space of object shapes via
3d generative-adversarial modeling. In NIPS, pages 82–90,
2016.

[53] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman,
and J. B. Tenenbaum. Learning shape priors for single-
view 3d completion and reconstruction.
arXiv preprint
arXiv:1809.05068, 2018.

[54] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In CVPR, pages 1912–1920, 2015.

10174

