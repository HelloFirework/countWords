Viewport Proposal CNN for 360° Video Quality Assessment

Chen Li†, Mai Xu†‡*, Lai Jiang†, Shanyi Zhang† and Xiaoming Tao§

† School of Electronic and Information Engineering, Beihang University, Beijing, China
‡ Hangzhou Innovation Institute (HZII), Beihang University, Hangzhou, Zhejiang, China

§ Department of Electronic Engineering, Tsinghua University, Beijing, China

†{jnlichen123, maixu, jianglai.china, shanyi zhang}@buaa.edu.cn

Abstract

Recent years have witnessed the growing interest in vi-
sual quality assessment (VQA) for 360° video. Unfortu-
nately, the existing VQA approaches do not consider the
facts that: 1) Observers only see viewports of 360° video,
rather than patches or whole 360° frames. 2) Within the
viewport, only salient regions can be perceived by observers
with high resolution. Thus, this paper proposes a viewport-
based convolutional neural network (V-CNN) approach for
VQA on 360° video, considering both auxiliary tasks of
viewport proposal and viewport saliency prediction. Our
V-CNN approach is composed of two stages, i.e., viewport
proposal and VQA. In the ﬁrst stage, the viewport proposal
network (VP-net) is developed to yield several potential
viewports, seen as the ﬁrst auxiliary task.
In the second
stage, a viewport quality network (VQ-net) is designed to
rate the VQA score for each proposed viewport, in which
the saliency map of the viewport is predicted and then uti-
lized in VQA score rating. Consequently, another auxiliary
task of viewport saliency prediction can be achieved. More
importantly, the main task of VQA on 360° video can be
accomplished via integrating the VQA scores of all view-
ports. The experiments validate the effectiveness of our V-
CNN approach in signiﬁcantly advancing the state-of-the-
art performance of VQA on 360° video. In addition, our
approach achieves comparable performance in two auxil-
iary tasks. The code of our V-CNN approach is available at
https://github.com/Archer-Tatsu/V-CNN .

1. Introduction

Nowadays, 360° video has become part of our daily
life. Typically, people watch 360° video through a head-
mounted display (HMD), resulting in that only the content
within the viewport is visible. They can obtain immer-
sive experience by changing their viewports in the range of

*Mai Xu is the corresponding author of this paper.

Figure 1. Brief framework of the proposed V-CNN for 360° video
quality assessment with multiple tasks.

360 × 180°. Therefore, 360° video covers the 360 × 180°
content, requiring extremely high resolution for clear rep-
resentation. To deliver high-resolution 360° video over the
bandwidth-limited channel, compression is urgently needed
[4, 37] to save encoding bit-rates, meanwhile causing dra-
matic degradation on visual quality. Moreover, the quality
degradation in the viewport can be more noticeable, since
people only watch a small part of the original frame. It is
thus necessary to study objective visual quality assessment
(VQA) for 360° video, which can be used as the guidance
for processing 360° video.

Recently, there has been increasing interest in assess-
ing the visual quality of 360° image/video [19, 21, 38, 39,
32, 28]. For example, based on peak signal-to-noise ratio
(PSNR), Yu et al. [38] proposed sphere-based PSNR (S-
PSNR) to calculate PSNR by considering uniform sampling
on sphere. They also provide an alternative VQA approach
that calculates PSNR in the ground-truth viewports; how-
ever, it is not practical as the viewport is not available when

10177

1Viewport proposal2Viewport saliency prediction3VQA score ratingTasks:VQA scoreSaliency maps0~10~11Viewport proposalsAlignmentViewport quality360 video frame123Output:Input:0~10~1[19] proposed a
processing 360° video ofﬂine. Li et al.
model with convolutional neural networks (CNNs), which
directly utilizes the data of head and eye movement (HM
and EM) in VQA on 360° video. Based on the HM and EM
data, the patches are extracted for assessing visual quality of
360° video. Unfortunately, the existing practical VQA ap-
proaches for 360° video ignore the fact that the subjects can
only access the viewports, rather than the patches or original
frames. Thus, a viewport-based VQA approach is more rea-
sonable to reﬂect the subjective quality of 360° video. Note
that when 360° video is compressed ofﬂine or broadcasted,
the viewports of multiple subjects need to be predicted.

The procedure of watching 360° video can be divided
into two stages. In the ﬁrst stage, the subjects control the
HM to select the attractive viewport visible to them, while
the region outside the viewport is invisible. Thus, the posi-
tion of viewport is determined by the HM of the subjects. In
the second stage, the subjects may control the EM to focus
on a small region (normally 2-3 visual degrees) of the view-
port, i.e., salient region that is clearly captured by eyes. Due
to this, it is reasonable to assess the visual quality of 360°
video by taking both potential viewport (HM) and saliency
within viewport (EM) into consideration. Recall that the
region-based CNN (R-CNN) approaches achieve the great
success in object detection [9, 24, 12], with the designs of
region proposal structure. Inspired by these key factors, it
is intuitive to design a viewport-based CNN approach with
viewport proposal for VQA on 360° video.

In this paper, we propose a viewport-based CNN (V-
CNN) approach for VQA on 360° video, the framework of
which is brieﬂy shown in Figure 1. This approach consists
of two stages, i.e., viewport proposal and VQA. In the stage
of viewport proposal, several potential viewports are pro-
posed for the next stage, as the result of predicting HM. In
the stage of VQA, after viewport alignment, the saliency
map and VQA score are generated for each proposed view-
port. At last, the ﬁnal VQA score is averaged over VQA
scores of all viewport proposals. As is shown in Figure 1,
there are three tasks in our V-CNN approach.
(1) View-
port proposal: This task is to propose potential viewports
containing attractive content for subjects. (2) Saliency pre-
diction in viewport: This task is to predict saliency map of
each proposed viewport. (3) VQA score rating: This task is
to rate the VQA score of the 360° video based on each pro-
posed viewport with the predicted saliency map. Although
the main task is VQA, our V-CNN approach is able to obtain
comparable results in other two auxiliary tasks, i.e., view-
port proposal and viewport saliency prediction.

The main contributions of this paper are: (1) We propose
a viewport-based approach for VQA on 360° video1, on the
basis of real human behaviors when watching 360° video.

1The code of our V-CNN approach is available at https://

github.com/Archer-Tatsu/V-CNN.

(2) We embed auxiliary tasks to boost VQA, by predict-
ing the HM and EM of subjects. (3) Our approach outper-
forms the state-of-the-art approaches in VQA, and achieves
comparable performance in the auxiliary tasks, i.e., view-
port proposal and viewport saliency prediction.

2. Related Works

VQA on 360° video. In addition to the traditional VQA
approaches for 2D video, the spherical characteristic has
been taken into account in the latest VQA works for 360°
video [28, 32, 38, 39], which advance the metric of PSNR.
For example, considering the nonuniformity of mapping
spherical content to plane, both weighted-to-spherically-
uniform PSNR (WS-PSNR) [28] and area weighted spheri-
cal PSNR (AW-SPSNR) [32] apply weight allocation in cal-
culating PSNR to balance the nonuniform sampling den-
sity.
In addition, there are some other VQA approaches
[38, 39] aiming to solve the nonuniform sampling in 360°
video. In [38], Yu et al. proposed S-PSNR that resamples
the original frame with a set of uniformly distributed points
on sphere. In [39], Zakharchenko et al. proposed apply-
ing Craster parabolic projection (CPP) to calculate PSNR,
named CPP-PSNR. The effectiveness of these approaches
[28, 32, 38, 39] has been veriﬁed through VQA experiments
on different 360° video datasets [30, 29, 22]. Recently, there
have also emerged several deep learning based approaches
[19, 21] for VQA on 360° video. Li et al. [19] proposed uti-
lizing both HM and EM data in a CNN model for VQA on
360° video. Lim et al. [21] proposed a 360° video VQA ap-
proach with adversarial learning. Both [19] and [21] apply
the same patch-based approach [17] as VQA on 2D video,
and learned to sample and allocate weights on patches for
VQA on 360° video.

Saliency models on 360° video. Recently, there has
been increasing interest in studying attention models for
360° video. Several 360° video datasets [6, 31, 7] were
built, including HM data [6, 31] and even EM data [7].
These datasets enable further research of analysis and even
prediction on human attention. Based on the datasets, the
human attention of watching 360° video was thoroughly
studied [23, 26], by investigating consistency and ﬁxation
bias among subjects. In addition, several works [34, 35, 3]
were proposed to predict the HM and EM of subjects when
watching 360° video. For predicting HM, Cheng et al. [3]
developed a spatial-temporal deep neural network (DNN)
to predict saliency maps of HM on 360° video.
In [3],
the original 360° videos are remapped to cubemap pro-
jection, followed by cube padding before being fed to the
network. Most recently, Xu et al.
[34] have also pro-
posed predicting viewports on 360° video, achieved by de-
veloping a new framework of deep reinforcement learning
(DRL). For predicting EM, a DNN was developed in [35]
to predict saliency maps of EM on 360° video. In addition,

10178

some works [20, 1] focused on predicting the scan-paths of
HM/EM in 360° image/video.

However, none of above VQA approaches for 360° video
considers the fact that the subjective visual quality highly
depends on the viewports in 360° video. Additionally, there
is no existing VQA work for 360° video that beneﬁts from
embedding auxiliary tasks. Thus, in this paper, we propose
a viewport-based approach for VQA on 360° video, which
considers predicting the real human behaviors, i.e., HM and
EM, when watching 360° video.

3. The Proposed V-CNN Approach

3.1. Framework

Since the visual quality of an impaired 360° frame only
depends on the viewports seen by the subjects, it is intuitive
to propose some potential viewports for VQA. In our V-
CNN approach, there are two stages, i.e., viewport proposal
(Stage I) and VQA (Stage II), for assessing full-reference
(FR) VQA score of the t-th impaired sequence frame Ft.
The framework of the V-CNN approach is shown in Figure
2. As is seen in this ﬁgure, the input to V-CNN includes the
t-th impaired frame Ft, the t-th reference frame Fref
and
the (t − ∆t)-th impaired frame Ft−∆t.

t

At stage I, the potential viewports on the impaired se-
quence frames are proposed, which also performs as the
ﬁrst auxiliary task of our V-CNN approach. Speciﬁcally, we
propose a viewport proposal network (VP-net) to generate
the viewport candidates and their corresponding importance
weights. Then, a viewport softer non maximum suppres-
sion (NMS) is developed to ﬁlter the viewport candidates
for proposal, according to the importance weights.

At stage II, the proposed viewports from stage I as well
as their corresponding error maps are ﬁrst converted from
spherical signals to plane, through our developed viewport
alignment method. Then, we develop a viewport quality
network (VQ-net) to assess the VQA score of each view-
port.
In the VQ-net, the saliency map of each viewport
is predicted by a mini-DenseNet, which also performs as
the second auxiliary task of our V-CNN. Given the error
map and the predicted saliency map, the VQA score of each
viewport is rated by a shallow CNN structure. At last, the
ﬁnal VQA score of the impaired 360° frame is obtained, by
averaging over all VQA scores of the proposed viewports.
This accomplishes the main task of our V-CNN, i,e, FR
VQA on impaired 360° frames. Note that we follow [38] to
yield the ﬁnal score of FR VQA on an impaired 360° video
sequence, given the corresponding reference sequence.

3.2. Stage I: Viewport proposal

In this stage, we take impaired frame Ft and its tempo-
ral residual (Ft − Ft−∆t) as the inputs. Subsequently, the
VP-net and viewport softer NMS are developed to propose

potential viewports. These viewport proposals can be seen
as the results of the ﬁrst auxiliary task for modelling general
HM of subjects on 360° video sequence. More details are
presented as follows.

VP-net. The architecture of the VP-net is shown in Fig-
ure 3. Speciﬁcally, the inputs of the impaired frame and
temporal residual are resampled with a widely used spher-
ical grid [14], and then concatenated together to ﬂow into
the following layers. Since 360° video sequence is a spher-
ical signal, we apply the spherical CNN [5] in our VP-net
to extract spherical features. Note that there are two types
of convolutional layers in spherical CNN, which implement
convolution on the unit sphere, denoted as S2, and 3D rota-
tion group, denoted as SO(3), respectively. The outputs of
both these two types of layers are feature maps on SO(3).
Additionally, the outputs of the 1st, 3rd and 5th layers are
down-sampled and then concatenated with the outputs of
the 7th to 9th layers, in order to fuse the low-level infor-
mation into the deep layers. After the last spherical con-
volutional layer, the spherical feature maps on the S2 are
obtained, denoted as T, by being converted from SO(3).

Given the spherical features T on the S2, we further ap-
ply the anchor implementation [24] on sphere for viewport
proposal. For each pixel location in T, there exists a cor-
responding anchor location on the sphere. This anchor lo-
cation can be represented as va = (φa, θa), where φa and
θa are the longitude and latitude, respectively. Accordingly,
the spherical feature maps T can be regarded as feature vec-
tors at the anchor locations. Since the size of the viewport
is ﬁxed when subjects are watching 360° video, each view-
port can be represented as v = (φ, θ), where φ and θ are
the longitude and latitude of its center, respectively. To be
more speciﬁc, for each of the I anchor locations {va
i=1,
a viewport candidate is generated by predicting the offset
∆vi = (∆φi, ∆θi) to va
i and its corresponding importance
weight ˆwi. This is achieved via two convolutional layers
with kernel size of 1. Subsequently, a softmax function is
applied to the importance weights. Finally, I viewport can-
didates can be obtained by

i }I

vi = va

i + ∆vi = (φa

i + ∆φi, θa

i + ∆θi) , i ∈ [1, I].

(1)

Viewport softer NMS. Here, We develop a viewport
softer NMS for ﬁltering viewport candidates, which is sum-
marized in Algorithm 1. Note that our viewport softer NMS
is modiﬁed and simpliﬁed from the traditional softer NMS
on 2D regions [13]. Speciﬁcally, among a set of viewport
candidates V, the viewport with the maximum importance
weight is found. Then, this viewport as well as all view-
port candidates within a threshold great-circle distance [16]
of dth, are weightily merged according to their importance
weights. Similarly, the corresponding importance weights
are also merged. These viewport candidates are then ex-
cluded from V. Finally, after our viewport softer NMS, the

10179

Figure 2. Framework of the V-CNN.

Figure 3. Architecture of the VP-net.

Algorithm 1: Viewport softer NMS.

Input: Sets of viewport candidates V = {v1, . . . , vI } and the

corresponding predicted importance weights
W = { ˆw1, . . . , ˆwI } at all anchor locations, great-circle
distance threshold dth, proposal quantity threshold K th.

Output: Sets of viewport proposals Vp and the corresponding

predicted importance weights Wp.

1 k ← 1, Vp ← ∅, Wp ← ∅
2 while V 6= ∅ and k ≤ K th do
ι ← arg max{ι| ˆwι∈W} ˆwι
3
I′ ← {ι′|d(vι′ , vι) < dth, vι′ ∈ V}, where d(vι′ , vι) is

4

the great-circle distance between vι′ and vι
wp
k ← Pι′∈I′ ˆwι′
k ← (cid:0)Pι′∈I′ ˆwι′ · vι′ (cid:1) /wp
vp
Vp ← Vp S{vp
V ← V \ {vι′ |ι′ ∈ I′}, W ← W \ { ˆwι′ |ι′ ∈ I′}
k ← k + 1

k}, Wp ← Wp S{wp
k }

k

5

6

7

8

9

10 end
11 return Vp, Wp

ﬁltered viewports are proposed for the next stage.

3.3. Stage II: VQA

In this stage, the proposed viewports are ﬁrst projected
to 2D plane by our viewport alignment method. Then, the
VQA score of each aligned viewport is rated by the pro-
posed VQ-net. Finally, the VQA scores of the proposed
viewports at all frames are combined to output the VQA
score of an impaired 360° video sequence. More details are
presented as follows.

Viewport alignment. Assume that the locations of pro-
k=1, where K is the

posed viewports are {vp

k = (φp

k, θp

k)}K

total number of proposed viewports. At the beginning of
stage II, viewport alignment is developed to project the
spherical content of the proposed viewport at vp
k to the 2D
plane, denoted as Ck. For each pixel in Ck, the alignment
is implemented by bilinear interpolation on the impaired
sequence frame Ft at the corresponding location. Addi-
tionally, the 2D error map of each viewport Cerr
is also
obtained by the same interpolation on the frame error map
Ferr

k

(Fref

t − Ft).

t

To be more speciﬁc, in the 2D plane, we denote the width
and height of the viewport as W and H (numbers of pix-
els), respectively. Given a pixel location (x, y) (x ∈ [1, W ],
y ∈ [1, H]) in Ck, the inverse of gnomonic projection [27]
is conducted to ﬁnd the corresponding spherical location
(φx,y, θx,y). Firstly, the pixel location (x, y) is converted
to (fx, fy), which has the same scale as the unit sphere:

fx =

2x−1−W

W

·tan

aW
2

, fy = −

2y−1−H

H

·tan

aH
2

, (2)

where aW and aH are the angular ranges of the viewport,
corresponding to W and H, respectively. Then, the corre-
sponding spherical location (φx,y, θx,y) is obtained by

φx,y = φp

k +arctan(cid:16)

ρ cos θp

θx,y = arcsin(cid:16)cos c sin θp

fx sin c

k cos c−fy sin θp
k + fy sin c cos θp

k

ρ

k sin c(cid:17),
(cid:17) ,

where

ρ =qf 2

x + f 2
y ,

c = arctan ρ.

Next, the corresponding pixel locations of (φx,y, θx,y) on
the Ft and Ferr
are determined by the projection type of
the frame. Taking the equirectangular projection (ERP) as
an example, the corresponding pixel location (px,y, qx,y) of
(φx,y, θx,y) can be obtained by:

t

(3)

(4)

(5)

px,y =(cid:18)φx,y

360°

+

1

2(cid:19)WF+

1
2

, qx,y =(cid:18)1

2

−

θx,y

180°(cid:19)HF+

1
2

, (6)

where WF and HF are the width and height of the 360°
sequence frame, respectively. Finally, the pixel values at

10180

Viewport proposals(with importance weights)Referencesequence(frame t)Viewport 1Viewporterror map 1Impairedsequence(frame t)Impairedsequence(frame t-Δt)21kSaliencymapViewportsofter NMSTemporalresidualViewportalignmentVQ-netWeightedaverageVQAscoreErrormapViewportVQA scoreViewportVQA scoreVQ-netViewport kwith its error map Mini-DenseNetCNNiiiiiiiViewport proposaliiSaliency predictioniiiVQA score ratingTasks:Stage IStage IIVP-netImpairedframeTemporalresidualSO3Conv10, 128SO3Conv9, 64SO3Conv5, 128SO3Conv3, 64SO3Conv4, 64, /2SO3Conv8, 128SO3Conv7, 256Notation: Layer name, number of output channels(, /resize ratio)SO3Conv6, 128, /2S2Conv1, 32, /2ResamplingSO(3)→S2Viewportimportanceweights {w}Viewportoffsets {Δv}SoftmaxConv11_2, 2Conv11_1, 1DownsamplingSO3Conv2, 32, /2(x, y) in Ck and Cerr
at (px,y, qx,y) in Ft and Ferr
proposed viewports.

t

k are obtained by bilinear interpolation
, yielding the 2D content of

VQ-net. The architecture of the VQ-net is shown in Fig-
ure 4. Both the content Ck and error map Cerr
k of each
proposed viewport are fed into the VQ-net. These 2 inputs
are concatenated after going through 2 convolutional layers,
respectively. Then, a mini-DenseNet with 3 dense blocks, 3
transpose convolutional layers and a softmax activation are
developed to predict the saliency map ˆMk of each proposed
viewport. These saliency maps can be seen as the results
of the second auxiliary task for modelling EM of subjects
in viewports of 360° video. Next, ˆMk is up-sampled and
then used to weight the input error map Cerr
k by Hadamard
product. Subsequently, the weighted error map is fed into a
shallow CNN structure with 3 convolutional layers, 2 max
pooling layers, a global average pooling layer and 2 fully
connected layers. Consequently, the VQA score sk of each
proposed viewport can be obtained.

Final VQA score of 360° video. Given the predicted
VQA scores {sk}K
k=1 of the proposed viewports, the over-
all VQA score of one impaired frame (denoted as ˜s) can be
obtained as the weighted average, according to the impor-
tance weights {wp

k=1 of the proposed viewports:

k}K

k=1 wp

k · sk

k=1 wp

k

˜s = PK
PK

.

(7)

At last, the VQA score of the input impaired 360° video
sequence can be obtained by averaging VQA scores of all
frames. This is the ﬁnal output of our V-CNN, as an FR
VQA approach on 360° video.

3.4. Training implementation

Now, we focus on the training implementation of our V-
CNN approach, in which the VP-net and VQ-net are two
CNN models to be trained.

Training VP-net. When watching 360° video, the HM
positions of subjects are also the centers of the correspond-
ing viewports. Hence, we use the HM of subjects as the su-
pervision of the predicted importance weights ˆwi and view-
port offsets ∆vi in the training of VP-net. Assume that
{vh
j=1 are the ground truth HM positions of
J subjects. Then, for the i-th anchor location va
i , the impor-
tance weight wi can be deﬁned by the following Gaussian
distribution and normalization:

j = (cid:0)φh

j(cid:1)}J

j , θh

˜wi =

exp(−

J

Xj=1

d2(va

i , vh
j )

2σ2 ), wi =

.

(8)

˜wi
i=1 ˜wi

PI

In (8), σ is the standard deviation; I is the quantity of an-
chor locations; d(va
j ) is the great-circle distance [16]
between va
i and vh
j . According to (8), importance weights

i , vh

Table 1. Values of some key hyper-parameters.
dth = 13.75°
K th = 20
σ = 18.33°
1 × 10−3
5 × 10−5
λw = 1
λv = 5

Great-circle distance threshold
Proposal quantity threshold
Standard deviation in (8)
Initial learning rate
Weight decay
Coefﬁcient for Lw in (12)
Coefﬁcient for Lv in (12)

Stage I

Stage II

Viewport size (number of pixels) W, H = 540, 600
Angular range
Initial learning rate
Weight decay
Coefﬁcient for LMk in (15)
Coefﬁcient for Lsk in (15)

aW , aH = 71°, 74°
1 × 10−3
5 × 10−5
λM = 10
λs = 1 × 103

Figure 4. Architecture of the VQ-net.

over all anchor locations can be regraded as the probability
distribution that whether the corresponding viewport can be
watched by subjects. Hence, we use the Kullback-Leibler
(KL) divergence as the loss function for predicting the im-
portance weights. To be more speciﬁc, we measure the
KL divergence DKL between the distribution of importance
weights {wi}I
i=1 at all I
anchor locations as follows,

i=1 and their predicted values { ˆwi}I

Lw = DKL({wi}I

i=1k{ ˆwi}I

i=1) =

I

Xi=1

wi log(cid:18) wi

ˆwi(cid:19) . (9)

i is deﬁned as the difference between va

i , the ground truth viewport offset
i and its closest

At each anchor location va
∆vg
HM position:

∆vg

i = arg min

{vh

j }J

j=1

d(va

i , vh

j ) − va
i .

(10)

Then, a smooth L1 loss [9], denoted as Lsmooth
for the predicted viewport offset at each anchor location:

, is applied

1

Lv =

I

Xi=1

wi · Lsmooth

1

(∆vi, ∆vg

i ).

(11)

Note that in (11), the importance weights {wi}I
i=1 are uti-
lized to weight Lsmooth
of different anchor locations, such
that the loss Lv is more sensitive to the loss at more impor-
tant anchor locations. Finally, the loss function for training
the VP-net is a combination of Lw and Lv:

1

LI = λwLw + λvLv.

(12)

Training VQ-net. For training the VQ-net, both the
outputs of the saliency map and VQA score need to be

10181

ViewportViewporterror mapNotation: Layer name, number of output channels(, /stride)    DenseBlock name, number of DenseLayersSaliencymap MˆVQAscore sSoftmaxConv10, 32, /2FC, 16FC, 1Max pooling, /2Max pooling, /2Up samplingMax pooling, /2Global averagepoolingConv11, 64, /2Conv12, 128, /2Conv1_2, 16DenseBlock4, 4DenseBlock5, 8DenseBlock6, 16TransposeConv7, 128, /2TransposeConv8, 16, /2TransposeConv9, 1, /2Conv2_2, 32Conv3, 96, /2Conv1_1, 16Conv2_1, 32Mini-DenseNetTable 2. Comparison on VQA performance between our and other approaches, over all test sequences.
Evaluation on VQA-ODV [19]

Attributes

Approaches

For 360° video

Full reference Deep learning Re-trained*

S-PSNR

WS-PSNR
CPP-PSNR
BP-QAVR

Li et al.

VR-IQA-NET

DeepQA

WaDIQaM-FR
V-CNN (Ours)

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

PLCC SROCC KROCC
0.4981
0.6929
0.4860
0.6721
0.4912
0.6812
0.6588
0.4780
0.5902
0.7821
0.2260
0.3713
0.5213
0.6936
0.4206
0.6207
0.8740
0.7137

0.6976
0.6839
0.6896
0.6801
0.7953
0.3379
0.7296
0.6162
0.8962

RMSE MAE
6.6810
8.5407
6.9089
8.7707
6.7932
8.6718
8.9112
7.0823
5.7793
7.3817
9.1016
10.9984
6.7720
8.5325
7.4574
9.2868
5.7551
4.4893

* The training code of the deep learning approaches checked in this column is available. Thus, they are re-trained on the same
training set as our approach before evaluation. Other approaches are evaluated with the pre-trained model provided by the authors.

supervised for each input viewport. For the supervision
of saliency prediction, we ﬁrst follow [15] to generate the
ground truth saliency map Mk using the EM points of sub-
jects within the k-th input viewport. Then, the loss of
saliency prediction is calculated for the k-th input viewport
by the following KL divergence,

LMk = DKL(Mkk ˆMk) =X(x′,y′)

Mk(x′, y′)log(Mk(x′, y′)
ˆMk(x′, y′)),

(13)
where Mk(x′, y′) and ˆMk(x′, y′) refer to the saliency val-
ues at location (x′, y′) of Mk and ˆMk, respectively. For the
supervision of VQA, the following loss is applied for the
k-th input viewport:

Lsk = (s − sk)2 ,

(14)

where sk is the VQA score rated by the VQ-net. In addi-
tion, s is the ground truth of difference mean opinion score
(DMOS) rated by subjects. Note that it is impractical to
obtain the DMOS value of each input viewport. Therefore,
in (14), the DMOS value of the 360° sequence is used as
the ground truth for supervising the VQA scores of view-
ports. Finally, the loss function for training the VQ-net is a
combination of LMk and Lsk for the k-th input viewport:

LII

k = λMLMk + λsLsk .

(15)

Training protocol. In training both VP-net and VQ-net,
the parameters are updated using the stochastic gradient de-
scent algorithm with the Adam optimizer. Weight decay is
also applied for regularization. Table 1 lists the values of
some key hyper-parameters in our approach. Among them,
the proposal quantity threshold K th is set to 20, according
to the recommended number of subjects in subjective VQA
on 360° video [33]. Since the typical resolution of view-
port in HMD is 1080 × 1200 with 110 × 113° angular range
[8], the standard deviation σ in (8) is set to 18.33◦, such
that the range of 3σ can correspond to the angular range of
viewport. However, in stage II, the width W and height H
of the input viewports are set as half of the resolution of

HMD, corresponding to an angular range around 71 × 74°.
It is because the subjects hardly ﬁx their eyes beyond this
range [23]. Moreover, other hyper-parameters are obtained
by tuning over the training set.

4. Experimental results

4.1. Experiment settings

In this section, we present the experimental results to val-
idate the performance of the proposed V-CNN approach in
the main task of VQA on 360° video and in the auxiliary
tasks of viewport proposal and viewport saliency predic-
tion. In our experiments, we evaluate the performance in
the dataset of VQA-ODV proposed in [19]. Note that the
VQA-ODV [19] is the largest VQA dataset for 360° video,
which includes the DMOS, HM and EM data of 200+ sub-
jects in watching 540 impaired sequences and 60 reference
sequences. We follow the standard training/test set split
provided in [19]. That is, our V-CNN model is trained on
432 impaired sequences, and the remaining 108 impaired
sequences form the test set. It is worth mentioning that the
DMOS values of the VQA-ODV [19] are all reversed in this
paper, so that the larger values of DMOS indicate better vi-
sual quality.

Then, the performance of VQA can be evaluated by mea-
suring the agreement between the subjective and objective
quality scores2 on the test set. To this end, the correla-
tion between the DMOS values and objective scores of each
VQA approach is measured for all test sequences, in terms
of Pearson linear correlation coefﬁcient (PLCC), Spearman
rank-order correlation coefﬁcient (SROCC), Kendall rank-
order correlation coefﬁcient (KROCC), root-mean-square
error (RMSE) and mean absolute error (MAE). We fol-
low [25] to apply a logistic function for ﬁtting the objec-
tive VQA scores to their corresponding DMOS values, such
that the ﬁtted scores of all VQA approaches are in the same
scale as DMOS. Then, PLCC, SROCC, KROCC, RMSE
and MAE are calculated for all pairs of the DMOS values
and the ﬁtted objective scores. Note that the larger values

2Here, objective scores are the predicted results of VQA approaches.

10182

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

30

35

40
Model Prediction

(a) S-PSNR

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

25

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

20

15

10

5

0

30

35

40

Model Prediction

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

20

15

10

5

0

30

35

40

Model Prediction

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

20

15

10

5

0

30

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

25

20

15

10

5

0

55

40

20

15

10

5

0

60

45

55
Model Prediction

50

35

40

50
Model Prediction

45

(b) CPP-PSNR

(c) WS-PSNR

(d) VR IQA NET

(e) BP-QAVR

20

15

10

5

0

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

20

15

10

5

0

50

40

45

Model Prediction

50

20

30
Model Prediction

35

40

45

30

40

50

60

Model Prediction

)
d
e
s
r
e
v
e
R

(
 
S
O
M
D

70

60

50

40

30

20

15

10

5

0

70

30

14

12

10

8

6

4

2

0

60

40

50

Model Prediction

(f) WaDIQaM-FR

(g) DeepQA

(h) Li et al.

(i) Our

Figure 5. Scatter plots for the pairs of DMOS values and objective VQA scores of different approaches, over all test sequences.

of PLCC, SROCC and KROCC indicate higher correlation,
while the smaller values of RMSE and MAE mean less error
of using the objective scores to predict their corresponding
DMOS values.

4.2. Evaluation on VQA

Here, we compare the performance between our V-CNN
approach and other state-of-the-art approaches for VQA
on 360° video, including S-PSNR [38], WS-PSNR [28],
CPP-PSNR [39], BP-QAVR [36], VR-IQA-NET [21] and
Li [19]. Among them, S-PSNR, WS-PSNR and CPP-
PSNR are PSNR-related VQA approaches for 360° video,
approved in the standard [37] of the moving picture ex-
perts group (MPEG); BP-QAVR, VR-IQA-NET and Li are
DNN-based VQA approaches for 360° video. Additionally,
we compare two latest DNN-based FR approaches for 2D
image: DeepQA [17] and WaDIQaM-FR [2]. For more de-
tails about the attributes of these approaches, see Table 2.

Scatter plots. Figure 5 shows the scatter plots of the
objective VQA scores versus the DMOS values over all
108 impaired sequences in the test set. The logistic ﬁt-
ting curves are also shown in this ﬁgure. In addition, the
color of the scatter points represents the absolute errors to
the ﬁtting curves. Generally speaking, the intensive scatter
points close to the ﬁtting curves are of little error, indicating
the high correlation between the objective VQA scores and
their corresponding DMOS values. It can be obviously seen
from Figure 5 that the VQA scores of our V-CNN approach
have much higher correlation with the DMOS values, com-
pared with all other VQA approaches. Therefore, we can
conclude that our V-CNN approach performs much better
than other approaches.

Quantitative results. Table 2 tabulates the results of
PLCC, SROCC, KROCC, RMSE and MAE for our V-CNN

approach and 8 other state-of-the-art approaches. The re-
sults of this table are obtained over all test sequences. We
can see from Table 2 that our V-CNN approach performs
signiﬁcantly better than all other approaches in terms of 5
metrics. To be more speciﬁc, our approach achieves at least
0.09, 0.10, 0.12, 1.62 and 1.28 improvements in PLCC,
SROCC, KROCC, RMSE and MAE, respectively.

4.3. Ablation study

Ablation on viewport proposal. We investigate the
inﬂuence of viewport proposal in our V-CNN approach
through the following ablation experiments. The experi-
ments include four types of viewports: (1) the ground truth
viewports, (2) viewports proposed by our VP-net, (3) view-
ports at random locations and (4) re-sized frame (regarded
as whole video frame but no viewport). Figure 6(a) shows
the ablation results on viewports. It can be seen from Figure
6(a) that the performance of VQA is signiﬁcantly degraded,
when the V-CNN approach uses the random viewports or
does not incorporate any viewport. In addition, the perfor-
mance of our V-CNN approach almost remains unchanged,
when replacing proposed viewports with the ground-truth
viewports. This indicates the effectiveness of the viewport
proposal for VQA on 360° video.

Ablation on saliency prediction. We further conduct
the ablation experiments to analyze the impact of view-
port saliency prediction on VQA. In our experiments, we
train the V-CNN with different loss weights on saliency
prediction, i.e., λM = 0, 100, 101, 102, 103 in (15). Note
that larger λM means more inﬂuence of saliency prediction
on VQA task. Figure 6(b) shows the VQA performance
at different values of λM. We can see that the V-CNN
without saliency prediction (i.e., λM = 0) performs worst.
Hence, we can conclude that saliency prediction is effec-

10183

t
n
e
i
c
i
f
f
e
o
C
n
o
i
t
a
l
e
r
r
o
C

 

1.0

0.8

0.6

0.4

t
n
e
i
c
i
f
f
e
o
C
n
o
i
t
a
l
e
r
r
o
C

 

0.9

0.8

0.7

0.6

0.5

PLCC

SROCC

KROCC

RMSE

MAE

Ground truth

viewports

Proposed
viewports

Random
viewports

No viewport

(re-sized frame)

(a) Ablation results on viewport proposal.

9

7

5

3

8

7

6

5

r
o
r
r
E

r
o
r
r
E

0

100

101

102

103

Ground truth

4

(b) Ablation results on saliency prediction.

Figure 6. Results of the ablation experiments.

tive for VQA in our V-CNN approach. On the other hand,
the VQA performance slightly reduces, when the V-CNN is
trained with smaller or larger λM than 101. This implies
that the under- or over-weighted loss on saliency prediction
degrades the performance of VQA in our V-CNN approach.
In addition, Figure 6(b) also plots the VQA performance
when replacing the predicted saliency by its ground truth.
As can be seen in this ﬁgure, the ground truth saliency leads
to a slight reduction on VQA performance, in comparison
with that with predicted saliency.
It is probably because
the Mini-DenseNet of VQ-net learns the shared features for
both saliency prediction and VQA, as a typical characteris-
tic of multi-task learning. In summary, the ablation exper-
iments show the saliency prediction has positive impact on
our V-CNN approach for VQA.

4.4. Evaluation on auxiliary tasks

In our V-CNN approach, the auxiliary tasks of viewport
proposal and viewport saliency prediction can be used to
to model the HM and EM of subjects on 360° video, re-
spectively. Here, we verify the performance of our V-CNN
approach in these two auxiliary tasks.

Evaluation on viewport proposal. The center of each
proposed viewport can be seen as an HM position. We thus
evaluate the performance of viewport proposal by generat-
ing the heat maps of HM, the same as that in [34]. Then,
the accuracy of predicting HM heat maps is measured in
three metrics: normalized scanpath saliency (NSS), corre-
lation coefﬁcient (CC) and KL divergence. The larger val-
ues of NSS/CC and smaller value of KL divergence mean
higher accuracy of HM prediction. For comparison, we
evaluate the performance of 4 state-of-the-art HM predic-
tion approaches for 360° video, i.e., Cheng [3], DHP [34],
GBVS360 [18] and BMS360 [18]. The experimental results

Table 3. Performance comparison on the auxiliary tasks.

(a) Results of HM modelling

(b) Results of EM modelling

Approaches
Cheng et al.

DHP

GBVS360
BMS360

V-CNN (Ours)

NSS
1.96
1.98
1.16
1.85
2.65

CC
0.35
0.37
0.22
0.34
0.63

KL
2.50
2.41
2.31
1.88
2.38

Approaches

DeepVS

BMS
GBVS
PQFT

V-CNN (Ours)

NSS
1.10
0.89
0.81
0.73
0.97

CC
0.36
0.26
0.30
0.19
0.37

KL
1.95
1.42
1.34
1.60
1.33

are shown in Table 3(a). It can be seen from this table that
the NSS and CC results of our V-CNN approach are higher
than those of all compared approaches, verifying the effec-
tiveness of our V-CNN approach in the ﬁrst auxiliary task
of viewport proposal.

Evaluation on viewport saliency prediction. We fur-
ther validate the performance of our V-CNN approach in the
auxiliary task of viewport saliency prediction, via compar-
ing with 4 state-of-the-art saliency prediction approaches:
BMS [40], GBVS [11], PQFT [10] and DeepVS [15]. Sim-
ilarly, the metrics of NSS, CC and KL are evaluated. The
experimental results are reported in Table 3(b). As is shown
in this table, our V-CNN approach achieves the best per-
formance in the metrics of CC and KL. Therefore, we can
conclude that our V-CNN approach is also effective in the
second auxiliary task of viewport saliency prediction.

5. Conclusion

In this paper, we have proposed a V-CNN approach that
advances VQA on 360° video by exploring the potential
viewports and predicting viewport saliency. Our approach
is in accordance with the human’s way of watching 360°
video, since the observers can only see viewports. Our V-
CNN approach consists of two stages, i.e., viewport pro-
posal and VQA. In the ﬁrst stage, the VP-net was developed
with a viewport softer NMS for proposing potential view-
ports. In this stage, the ﬁrst auxiliary task of viewport pro-
posal is achieved. In the second stage, we designed a VQ-
net to rate the VQA score of each proposed viewport. In the
VQ-net, the saliency map of each proposed viewport is pre-
dicted, as the second auxiliary task; then it is used for VQA
of the viewport. Subsequently, the main task of VQA on
360° video is accomplished by integrating the VQA scores
of all proposed viewports. Finally, the experimental results
veriﬁed that the V-CNN approach signiﬁcantly outperforms
8 other state-of-the-art approaches in VQA on 360° video,
and that it also performs well in the auxiliary tasks of view-
port proposal and viewport saliency prediction.

Acknowledgement

This work was supported by NSFC under Grants
61876013 and 61573037, and by the Fok Ying-Tung Ed-
ucation Foundation under Grant 151061.

10184

References

[1] Marc Assens, Xavier Giro-i Nieto, Kevin McGuinness, and
Noel E O’Connor. Saltinet: Scan-path prediction on 360 de-
gree images using saliency volumes. In IEEE International
Conference on Computer Vision Workshop, pages 2331–
2338. IEEE, 2017.

[2] Sebastian Bosse, Dominique Maniry, Klaus-Robert M¨uller,
Thomas Wiegand, and Wojciech Samek. Deep neural
networks for no-reference and full-reference image qual-
ity assessment.
IEEE Transactions on Image Processing,
27(1):206–219, 2018.

[3] Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-
Kai Wen, Tyng-Luh Liu, and Min Sun. Cube padding for
weakly-supervised saliency prediction in 360° videos.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 1420–1429. IEEE, 2018.

[4] B Choi, YK Wang, MM Hannuksela, Y Lim, and A Mur-
Information technology–coded representation of im-
taza.
mersive media (MPEG-I)–part 2: Omnidirectional media
format. ISO/IEC, pages 23090–2, 2017.

[5] Taco S. Cohen, Mario Geiger, Jonas K¨ohler, and Max
Welling. Spherical CNNs. In International Conference on
Learning Representations, 2018.

[6] Xavier Corbillon, Francesca De Simone, and Gwendal Si-
mon. 360-degree video head movement dataset. In ACM In-
ternational Conference on Multimedia Systems, pages 199–
204. ACM, 2017.

[7] Erwan J David,

Jes´us Guti´errez, Antoine Coutrot,
Matthieu Perreira Da Silva, and Patrick Le Callet. A
dataset of head and eye movements for 360◦ videos. In ACM
International Conference on Multimedia Systems, pages
432–437. ACM, 2018.

[8] Alex Davies. Oculus Rift vs. HTC Vive vs. PlaySta-
https://www.tomshardware.co.uk/

tion VR.
vive-rift-playstation-vr-comparison,
review-33556-3.html/, 2016.

[9] Ross Girshick. Fast R-CNN. In IEEE International Confer-

ence on Computer Vision, pages 1440–1448, 2015.

[10] Chenlei Guo, Qi Ma, and Liming Zhang. Spatio-temporal
saliency detection using phase spectrum of quaternion
fourier transform. In IEEE Conference on Computer Vision
and Pattern Recognition. Citeseer, 2008.

[11] Jonathan Harel, Christof Koch, and Pietro Perona. Graph-
In Advances in Neural Information

based visual saliency.
Processing Systems, pages 545–552, 2007.

[12] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask R-CNN. In IEEE International Conference on
Computer Vision, pages 2980–2988. IEEE, 2017.

[13] Yihui He, Xiangyu Zhang, Marios Savvides, and Kris Ki-
tani. Softer-NMS: Rethinking bounding box regression for
accurate object detection. arXiv preprint arXiv:1809.08545,
2018.

[14] Dennis M Healy, Daniel N Rockmore, Peter J Kostelec,
and Sean Moore. FFTs for the 2-sphere-improvements and
variations. Journal of Fourier Analysis and Applications,
9(4):341–385, 2003.

[15] Lai Jiang, Mai Xu, Tie Liu, Minglang Qiao, and Zulin Wang.
DeepVS: A deep learning based video saliency prediction
approach.
In European Conference on Computer Vision,
pages 625–642. Springer, 2018.

[16] Lyman M Kells. Plane and Spherical Trigonometry with Ta-
bles by Lyman M. Kells, Willis F. Kern, James R. Bland. US
Armed Forces Institute, 1940.

[17] Jongyoo Kim and Sanghoon Lee. Deep learning of human
visual sensitivity in image quality assessment framework. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017.

[18] Pierre Lebreton and Alexander Raake. GBVS360, BMS360,
ProSal: Extending existing saliency prediction models from
2D to omnidirectional images. Signal Processing: Image
Communication, 2018.

[19] Chen Li, Mai Xu, Xinzhe Du, and Zulin Wang. Bridge the
gap between VQA and human behavior on omnidirectional
video: A large-scale dataset and a deep learning model. In
ACM Multimedia Conference, pages 932–940. ACM, 2018.
[20] Feng Li, Huihui Bai, and Yao Zhao. Visual attention guided
eye movements for 360 degree images. In IEEE Asia-Paciﬁc
Signal and Information Processing Association Annual Sum-
mit and Conference, pages 506–511. IEEE, 2017.

[21] Heountaek Lim, Hak Gu Kim, and Yang Man Ra. VR IQA
NET: Deep virtual reality image quality assessment using
adversarial learning. In IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 6737–6741,
April 2018.

[22] Francisco Lopes, Jo˜ao Ascenso, Ant´onio Rodrigues, and
Maria Paula Queluz. Subjective and objective quality assess-
ment of omnidirectional video.
In Applications of Digital
Image Processing XLI, volume 10752, page 107520P. Inter-
national Society for Optics and Photonics, 2018.

[23] Yashas Rai, Patrick Le Callet, and Philippe Guillotel. Which
saliency weighting for omni directional image quality assess-
ment? In IEEE International Conference on Quality of Mul-
timedia Experience, pages 1–6. IEEE, 2017.

[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, (6):1137–1149, 2017.

[25] Kalpana Seshadrinathan, Rajiv Soundararajan, Alan Conrad
Bovik, and Lawrence K Cormack. Study of subjective and
objective quality assessment of video. IEEE Transactions on
Image Processing, 19(6):1427–1441, 2010.

[26] Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh
Agrawala, Diego Gutierrez, Belen Masia, and Gordon Wet-
zstein. Saliency in VR: How do people explore virtual en-
vironments? IEEE Transactions on Visualization and Com-
puter Graphics, 24(4):1633–1642, 2018.

[27] John Parr Snyder. Map projections–A working manual, vol-

ume 1395. US Government Printing Ofﬁce, 1987.

[28] Yule Sun, Ang Lu, and Lu Yu. Weighted-to-spherically-
uniform quality evaluation for omnidirectional video. IEEE
Signal Processing Letters, 24(9):1408–1412, 2017.

[29] Huyen TT Tran, Nam Pham Ngoc, Cuong Manh Bui,
Minh Hong Pham, and Truong Cong Thang. An evalua-
tion of quality metrics for 360 videos. In IEEE International

10185

Conference on Ubiquitous and Future Networks, pages 7–11.
IEEE, 2017.

[30] Evgeniy Upenik, Martin Rerabek, and Touradj Ebrahimi. On
the performance of objective metrics for omnidirectional vi-
sual content. In IEEE International Conference on Quality
of Multimedia Experience, pages 1–6. IEEE, 2017.

[31] Chenglei Wu, Zhihao Tan, Zhi Wang, and Shiqiang Yang. A
dataset for exploring user behaviors in VR spherical video
streaming. In ACM International Conference on Multimedia
Systems, pages 193–198. ACM, 2017.

[32] Xiaoyu Xiu, Yuwen He, Yan Ye, and Bharath Vishwanath.
An evaluation framework for 360-degree video compression.
In IEEE International Conference on Visual Communica-
tions and Image Processing, pages 1–4. IEEE, 2017.

[33] Mai Xu, Chen Li, Yufan Liu, Xin Deng, and Jiaxin Lu. A
subjective visual quality assessment method of panoramic
videos.
In IEEE International Conference on Multimedia
and Expo, pages 517–522. IEEE, 2017.

[34] Mai Xu, Yuhang Song, Jianyi Wang, MingLang Qiao,
Liangyu Huo, and Zulin Wang. Predicting head movement in
panoramic video: A deep reinforcement learning approach.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2018.

[35] Yanyu Xu, Yanbing Dong, Junru Wu, Zhengzhong Sun,
Zhiru Shi, Jingyi Yu, and Shenghua Gao. Gaze prediction
in dynamic 360° immersive videos.
In IEEE Conference
on Computer Vision and Pattern Recognition, pages 5333–
5342, 2018.

[36] Shu Yang, Junzhe Zhao, Tingting Jiang, Jing Wang Tariq
Rahim, Bo Zhang, Zhaoji Xu, and Zesong Fei. An ob-
jective assessment method based on multi-level factors for
panoramic videos.
In IEEE International Conference on
Visual Communications and Image Processing, pages 1–4.
IEEE, 2017.

[37] Y Ye, E Alshina, and J Boyce. Algorithm descriptions of
projection format conversion and video quality metrics in
360Lib.
Joint Video Exploration Team of ITU-T SG, 16,
2017.

[38] Matt Yu, Haricharan Lakshman, and Bernd Girod. A frame-
work to evaluate omnidirectional video coding schemes. In
IEEE International Symposium on Mixed and Augmented
Reality, pages 31–36. IEEE, 2015.

[39] Vladyslav Zakharchenko, Kwang Pyo Choi, and Jeong Hoon
Park. Quality metric for spherical panoramic video. In Op-
tics and Photonics for Information Processing X, volume
9970, page 99700C. International Society for Optics and
Photonics, 2016.

[40] Jianming Zhang and Stan Sclaroff. Exploiting surrounded-
ness for saliency detection: a boolean map approach. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
(5):889–902, 2016.

10186

