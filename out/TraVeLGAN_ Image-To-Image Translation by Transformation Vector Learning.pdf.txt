TraVeLGAN: Image-to-image Translation by Transformation Vector Learning

Matthew Amodio
Yale University

Smita Krishnaswamy

Yale University

matthew.amodio@yale.edu

smita.krishnaswamy@yale.edu

Abstract

Interest in image-to-image translation has grown sub-
stantially in recent years with the success of unsupervised
models based on the cycle-consistency assumption. The
achievements of these models have been limited to a partic-
ular subset of domains where this assumption yields good
results, namely homogeneous domains that are character-
ized by style or texture differences. We tackle the chal-
lenging problem of image-to-image translation where the
domains are deﬁned by high-level shapes and contexts, as
well as including signiﬁcant clutter and heterogeneity. For
this purpose, we introduce a novel GAN based on preserv-
ing intra-domain vector transformations in a latent space
learned by a siamese network. The traditional GAN sys-
tem introduced a discriminator network to guide the gener-
ator into generating images in the target domain. To this
two-network system we add a third: a siamese network that
guides the generator so that each original image shares se-
mantics with its generated version. With this new three-
network system, we no longer need to constrain the gen-
erators with the ubiquitous cycle-consistency restraint. As
a result, the generators can learn mappings between more
complex domains that differ from each other by large differ-
ences - not just style or texture.

1. Introduction

Learning to translate an image from one domain to an-
other has been a much studied task in recent years [36,
17, 15, 38, 13]. The task is intuitively deﬁned when we
have paired examples of an image in each domain, but
unfortunately these are not available in many interesting
cases. Enthusiasm has grown as the ﬁeld has moved to-
wards unsupervised methods that match the distributions
of the two domains with generative adversarial networks
(GANs) [18, 11, 32, 35, 26]. However, there are inﬁnitely
many mappings between the two domains [24], and there is
no guarantee that an individual image in one domain will
share any characteristics with its representation in the other
domain after mapping.

Figure 1: The TraVeLGAN architecture, which adds a siamese
network S to the traditional generator G and discriminator D and
trains to preserve vector arithmetic between points in the latent
space of S.

Other methods have addressed this non-identiﬁability
problem by regularizing the family of generators in various
ways, including employing cross-domain weight-coupling
in some layers [26] and decoding from a shared embedding
space [25]. By far the most common regularization, ﬁrst in-
troduced by the CycleGAN and the DiscoGAN, has been
forcing the generators to be each other’s inverse, known

18983

Figure 2: Examples of TraVeLGAN generated output on Imagenet domains that are too different and diverse for cycle-consistent GANs
to map between. The TraVeLGAN successfully generates images that are both fully realistic in the output domain (shape of object, color,
background) and have preserved semantics learned by the siamese network.

as the cycle-consistency property [16, 39, 20, 31, 27, 2, 9,
4, 37]. Recent ﬁndings have shown that being able to in-
vert a mapping at the entire dataset level does not neces-
sarily lead to the generation of related real-generated image
pairs [23, 3, 11].

Not only do these dataset-level regularizations on the
generator not provide individual image-level matching, but
also by restricting the generator,
they prevent us from
learning mappings that may be necessary for some do-
mains. Previous work continues to pile up regularization
after regularization, adding restrictions on top of the gen-
erators needing to be inverses of each other. These in-

clude forcing the generator to be close to the identity func-
tion [39], matching population statistics of discriminator ac-
tivations [20], weight sharing [26], penalizing distances in
the latent space [31], perceptual loss on a previously trained
model [25], or more commonly, multiple of these.

Instead of searching for yet another regularization on
the generator itself, we introduce an entirely novel ap-
proach to the task of unsupervised domain mapping:
the
Transformation Vector Learning GAN (TraVeLGAN).

The TraVeLGAN uses a third network, a siamese net-
work, in addition to the generator and discriminator to pro-
duce a latent space of the data to capture high-level seman-

8984

tics characterizing the domains. This space guides the gen-
erator during training, by forcing the generator to preserve
vector arithmetic between points in this space. The vector
that transforms one image to another in the original domain
must be the same vector that transforms the generated ver-
sion of that image into the generated version of the other
image. Inspired by word2vec embeddings [14] in the nat-
ural language space, if we need to transform one original
image into another original image by moving a foreground
object from the top-left corner to the bottom-right corner,
then the generator must generate two points in the target
domain separated by the same transformation vector.

In word2vec, semantic vector transformations are a prop-
erty of learning a latent space from known word contexts. In
TraVeLGAN, we train to produce these vectors while learn-
ing the space.

Domain mapping consists of two aspects: (a) transfer the
given image to the other domain and (b) make the translated
image similar to the original image in some way. Previous
work has achieved (a) with a separate adversarial discrimi-
nator network, but attempted (b) by just restricting the class
of generator functions. We propose the natural extension to
instead achieve (b) with a separate network, too.

The TraVeLGAN differs from previous work in several

substantial ways.

1. It completely eliminates the need for training on cycle-
consistency or coupling generator weights or other-
wise restricting the generator architecture in any way.

2. It introduces a separate network whose output space
is used to score similarity between original and gen-
erated images. Other work has used a shared latent
embedding space, but differs in two essential ways: (a)
their representations are forced to overlap (instead of
preserving vector arithmetic) and (b) the decoder must
be able to decode out of the embedding space in an au-
toencoder fashion [25, 31] ([25] shows this is in fact
equivalent to the cycle consistency constraint.

3. It

is entirely parameterized by neural networks:
nowhere are Euclidean distances between images as-
sumed to be meaningful by using mean-squared error.

4. It adds interpetability to the unsupervised domain
transfer task through its latent space, which explains
what aspects of any particular image were used to gen-
erate its paired image.

As a consequence of these differences, the TraVeLGAN is
better able to handle mappings between complex, hetero-
geneous domains that require signiﬁcant and diverse shape
changing.

By avoiding direct regularization of the generators, the
TraVeLGAN also avoids problems that these regularizations
cause. For example, cycle-consistency can unnecessarily
prefer an easily invertible function to a possibly more co-
herent one that is slightly harder to invert (or preventing us

from mapping to a domain if the inverse is hard to learn).
Not only must each generator learn invertible mappings,
but it further requires that the two invertible mappings be
each other’s inverses. Furthermore, cycle-consistency is en-
forced with a pixel-wise MSE between the original and re-
constructed image: other work has identiﬁed the problems
caused by using pixelwise MSE, such as the tendency to
bias towards the mean images [7].

Our approach bears a resemblance to that of the Dis-
tanceGAN [6], which preserves pairwise distances between
images after mapping. However, they calculate distance di-
rectly on the pixel space, while also not preserving any no-
tion of directionality in the space between images. In this
paper, we demonstrate the importance of not performing
this arithmetic in the pixel space.

Many of these previous attempts have been developed
speciﬁcally for the task of style transfer, explicitly assuming
the domains are characterized by low-level pixel differences
(color, resolution, lines) as opposed to high-level semantic
differences (shapes and types of speciﬁc objects, composi-
tion) [7, 37, 13]. We demonstrate that these models do not
perform as well at the latter case, while the TraVeLGAN
does.

2. Model

i=1 ∈ X and {yi}Ny

We denote two data domains X and Y , consisting of ﬁ-
nite (unpaired) training points {xi}Nx
i=1 ∈
Y , respectively. We seek to learn two mappings, GXY :
X → Y and GY X : Y → X, that map between the do-
mains. Moreover, we want the generators to do more than
just mimic the domains at an aggregate level. We want there
to be a meaningful and identiﬁable relationship between the
two representations of each point. We claim that this task of
unsupervised domain mapping consists of two components:
domain membership and individuality. Without loss of
generality, we deﬁne these terms with respect to GXY here,
with GY X being the same everywhere but with opposite do-
mains.

Domain membership The generator should output points
in the target domain, i.e. GXY (X) ∈ Y . To enforce this, we
use the standard GAN framework of having a discriminator
DY that tries to distinguish the generator’s synthetic output
from real samples in Y . This yields the typical adversarial
loss term Ladv:

Ladv = EX [DY (GXY (X))]

Individuality
In addition, our task has a further require-
ment than just two different points in X each looking like
they belong to Y . Given xi, xj ∈ X, i 6= j, we want there
to be some relationship between xi and GXY (xi) that jus-
tiﬁes why GXY (xi) is the representation in domain Y for

8985

Figure 3: Examples of TraVeLGAN generated output on traditional datasets for unsupervised domain transfer with cycle-consistent
GANs. Little change to the original image is necessary in these problems, and TraVeLGAN generates the expected, minimally changed
image in the other domain.

xi and not for xj . Without this requirement, the generator
could satisfy its objective by ignoring anything substantive
about its input and producing arbitrary members of the other
domain.

While other methods try to address this by regularizing
GXY (by forcing it to be close to the identity or to be in-
verted by GY X ), this limits the ability to map between do-
mains that are too different. So instead of enforcing simi-
larity between the point xi and the point GXY (xi) directly
in this way, we do so implicitly by matching the relation-
ship between the xi’s and the relationship between the cor-
responding GXY (xi)’s.

We introduce the notion of a transformation vector be-
tween two points.
In previous natural language process-
ing applications [14], there is a space where the vector that
would transform the word man to the word woman is similar
to the vector that would transform king to queen. In our ap-
plications, rather than changing the gender of the word, the

transformation vector could change the background color,
size, or shape of an image. The crucial idea, though, is that
whatever transformation is necessary to turn one original
image into another original image, an analogous transfor-
mation must separate the two generated versions of these
images.

Formally, given xi, xj ∈ X, deﬁne the transforma-
tion vector between them ν(xi, xj) = xj − xi. The
generator must learn a mapping such that ν(xi, xj) =
ν(GXY (xi), GXY (xj)). This is a more powerful property
than even preserving distances between points, as it requires
the space to be organized such that the directions of the vec-
tors as well as the magnitudes be preserved. This property
requires that the vector that takes xi to xj , be the same vec-
tor that takes GXY (xi) to GXY (xj).

As stated so far, this framework would only be able to de-
ﬁne simple transformations, as it is looking directly at the
input space. By analogy, the word-gender-changing vec-

8986

tor transformation does not hold over the original one-hot
encodings of the words, but instead holds in some reduced
semantic latent space. So we instead redeﬁne the transfor-
mation vector to be ν(xi, xj) = S(xj) − S(xi), where S is
a function that gives a representation of each point in some
latent space. Given an S that learns high-level semantic
representations of each image, we can use our notion of pre-
serving the transformation vectors to guide generation. We
propose to learn such a space with an analogue to the ad-
versarial discriminator D from the traditional GAN frame-
work: a cooperative siamese network S.

The goal of S is to map images to some space where
the relationship between original images is the same as the
relationship between their generated versions in the target
domain:

LT raV eL = ΣΣi6=jDist(νij, ν′

ij)

νij = S(xi) − S(xj)
ν′
ij = S(GXY (xi)) − S(GXY (xj))

where Dist is a distance metric, such as cosine similarity.
Note this term involves the parameters of G, but G needs
this space to learn its generative function in the ﬁrst place.
Thus, these two networks depend on each other to achieve
their goals. However, unlike in the case of G and D, the
goals of G and S are not opposed, but cooperative. They
both want LT raV eL to be minimized, but G will not learn a
trivial function to satisfy this goal, because it also is trying
to fool the discriminator. S could still learn a trivial function
(such as always outputting zero), so to avoid this we add one
further requirement and make its objective multi-task.
It
must satisfy the standard siamese margin-based contrastive
objective [28, 29] LSc , that every point is at least δ away
from every other point in the latent space:

LSc = ΣΣi6=jmax(0, (δ − ||νij||2))

This term incentivizes S to learn a latent space that identi-
ﬁes some differences between images, while LT raV eL in-
centivizes S to organize it. Thus, the ﬁnal objective terms
of S and G are:

LS = LSc + LT raV eL
LG = Ladv + LT raV eL

G and S are cooperative in the sense that each is trying to
minimize LT raV eL, but each has an additional goal speciﬁc
to its task as well. We jointly train these networks such that
together G learns to generate images that S can look at and
map to some space where the relationships between original
and generated images are preserved.

3. Experiments

Our experiments are designed around intentionally dif-
ﬁcult image-to-image translation tasks. These translations

Figure 4:
It is hard to learn mappings between domains that
are each other’s inverse when the domains are asymmetric (e.g.
crossword conﬁgurations are more complex than abacus conﬁg-
(a) G1 can change the background (red selection) or
urations).
black beads (orange circles) in hard-to-invert ways. (b) The cycle-
consistency assumption forced every black bead to a white cross-
word square and every blank space to a black crossword square,
even though the result is not a realistic crossword pattern. The
background is also not fully changed because it could not learn
that more complicated inverse function.

turning a picture into a cartoon) [31, 19].

are much harder than style or texture transfer problems,
where the domain transformation can be reduced to repeat-
ing a common patch transformation on every local patch
of every image without higher-level semantic information
(e.g.
Instead,
we choose domains where the differences are higher-level
and semantic. For example, when mapping from horses to
birds, any given picture of a horse might solely consist of
style, texture, and patches that appear in other pictures of
real birds (like blue sky, green grass, sharp black outlines,
and a brown exterior). Only the higher-level shape and con-
text of the image eventually reveal which domain it belongs
to. Additionally, because we use datasets that are designed
for classiﬁcation tasks, the domains contain signiﬁcant het-
erogeneity that makes ﬁnding commonality within a domain
very difﬁcult.

We compare the TraVeLGAN to several previous meth-
ods that ﬁrst regularize the generators by enforcing cycle-
consistency and then augment this with further regular-
izations [39, 20, 3, 31, 27, 2, 9, 4]. Namely, we com-
pare to a GAN with just the cycle-consistency loss (cy-
cle GAN) [39], with cycle-consistency loss plus the iden-
tity regularization (cycle+identity GAN) [39], with cycle-
consistency loss plus a correspondence loss (cycle+corr
GAN) [3], with cycle-consistency loss plus a feature match-
ing regularization (cycle+featmatch GAN) [20], and with

8987

cycle-consistency loss plus a shared latent space regulariza-
tion (cycle+latent GAN) [25]. The TraVeLGAN utilizes a
U-net architecture with skip connections [30] for the gener-
ator. The discriminator network is a standard stride-2 con-
volutional classiﬁer network that doubles the number of ﬁl-
ters at each layer until the layer is 4x4 and outputs a sin-
gle sigmoidal probability. The siamese network is iden-
tical except rather than outputting one node like the dis-
criminator it outputs the number of nodes that is the size
of the latent space, without any nonlinear activation. For
the cycle-consistent GANs we compare to, we optimized
the hyperparameters to get the best achievement we could,
since our focus is on testing our different loss formulation.
This involved trying both Resnet and U-Net architectures
for the models from [39]: the U-Net performed much bet-
ter than the Resnet at these tasks, so we use that here. We
also had to choose a value of the cycle-consistent coefﬁcient
that largely de-emphasized it in order to get them to change
the input image at all (0.1). Even so, we were not able to
achieve nearly as convincing results with any of the baseline
models as with the TraVeLGAN.

3.1. Similar domains

The datasets we ﬁrst consider are traditional cases for
unsupervised domain mapping with cycle-consistent net-
works, where little change is necessary. These are:
Apples to oranges The photos of apples and oranges
from [39] (Figure 3a).
The TraVeLGAN successfully
changes not only the color of the fruit, but also the shape
and texture. The stem is removed from apples, for example,
and the insides of oranges aren’t just colored red but fully
made into apples. In the last row, the TraVeLGAN changes
the shape of the orange to become an apple and correspond-
ingly moves its shadow down in the frame to correspond.

Van Gogh to landscape photo The portraits by Van Gogh
and photos of landscapes, also from [39] (Figure 3b). Here
the prototypical Van Gogh brush strokes and colors are suc-
cessfully applied or removed. Notably, in the last row, the
portrait of the man is changed to be a photo of a rocky
outcrop with the blue clothes of the man changing to blue
sky and the chair becoming rocks, rather than becoming a
photo-realistic version of that man, which would not belong
in the target domain of landscapes.
Ukiyoe to landscape photo Another dataset from [39],
paintings by Ukiyoe and photos of landscapes (Figure 3c).
It is interesting to note that in the generated Ukiyoe images,
the TraVeLGAN correctly matches reﬂections of mountains
in the water, adding color to the top of the mountain and the
corresponding bottom of the reﬂection.
CelebA glasses The CelebA dataset ﬁltered for men with
and without glasses [8] (Figure 3d). As expected, the TraV-
eLGAN produces the minimal change necessary to trans-

fer an image to the other domain, i.e. adding or remov-
ing glasses while preserving the other aspects of the image.
Since the TraVeLGAN learns a semantic, rather than pixel-
wise, information preserving penalty, in some cases aspects
not related to the domain are also changed (like hair color
or background). In each case, the resulting image is still
a convincingly realistic image in the target domain with a
strong similarity to the original, though.

CelebA hats The CelebA dataset ﬁltered for men with
and without hats [8] (Figure 3e). As before, the TraVeL-
GAN adds or removes a hat while preserving the other se-
mantics in the image.

Sketch to shoe
Images of shoes along with their sketch
outlines, from [33] (Figure 3f). Because this dataset is
paired (though it is still trained unsupervised as always),
we are able to quantify the performance of the TraVeLGAN
with a heuristic: the pixel-wise mean-squared error (MSE)
between the TraVeLGAN’s generated output and the true
image in the other domain. This can be seen to be a heuris-
tic in the fourth row of Figure 3c, where the blue and black
shoe matches the outline of the sketch perfectly, but is not
the red and black color that the actual shoe happened to be.
However, even as an approximation it provides information.
Table 2 shows the full results, and while the vanilla cycle-
consistent network performs the best, the TraVeLGAN is
not far off and is better than the others. Given that the TraV-
eLGAN does not have the strict pixel-wise losses of the
other models and that the two domains of this dataset are
so similar, it is not surprising that the more ﬂexible TraVeL-
GAN only performs similarly to the cycle-consistent frame-
works. These scores provide an opportunity to gauge the
effect of changing the size of the latent space learned by the
siamese network. We see that our empirically chosen de-
fault value of 1000 slightly outperforms a smaller and lower
value. This parameter controls the expressive capability of
the model, and the scores suggest providing it too small of a
space can limit the complexity of the learned transformation
and too large of a space can inhibit the training. The scores
are all very similar, though, suggesting it is fairly robust to
this choice.

Quantitative results Since the two domains in these
datasets are so similar, it is reasonble to evaluate each model
using structural similarity (SSIM) between the real and gen-
erated images in each case. These results are presented in
Table 1. There we can see that the TraVeLGAN performs
comparably to the cycle-consistent models. It is expected
that the baselines perform well in these cases, as these are
the standard applications they were designed to succeed on
in the ﬁrst place; namely, domains that require little change
to the original images. Furthermore, it is expected that
the TraVeLGAN changes the images slightly more than the
models that enforce pixel-wise cycle-consistency. That the

8988

SSIM

Apple

Van Gogh

Ukiyoe

Glasses

Hats

TraVeLGAN
Cycle
Cycle+ident
Cycle+corr
Cycle+featmatch
Cycle+latent

0.302
0.424
0.305
0.251
0.114
0.245

0.183
0.216
0.327
0.079
0.117
0.260

0.222
0.252
0.260
0.072
0.125
0.144

0.499
0.463
0.608
0.230
0.086
0.442

0.420
0.437
0.358
0.204
0.209
0.382

Table 1: Real/generated SSIM on the similar-domains datasets.

Pixel MSE

Sketches

Shoes

TraVeLGAN
TraVeLGAN (Dlatent=100)
TraVeLGAN (Dlatent=2000)
Cycle
Cycle+corr
Cycle+featmatch
Cycle+latent

0.060
0.069
0.064
0.047
0.427
0.077
0.072

0.267
0.370
0.274
0.148
0.603
0.394
0.434

Table 2: Per-pixel MSE on the shoes-to-sketch dataset.

TraVeLGAN performs so similarly demonstrates quantita-
tively that the TraVeLGAN can preserve the main qualities
of the image when the domains are similar.

3.2. Imagenet: diverse domains

The previous datasets considered domains that were very
similar to each other. Next, we map between two domains
that are not only very different from each other, but from
classiﬁcation datasets where the object characterizing the
domain is sometimes only partially in the frame, has many
different possible appearances, or have substantial clutter
around it.
In this most difﬁcult task, we present arbi-
trary chooses two classes from the Imagenet [10] dataset.
These images are much higher-resolution (all images are
rescaled to 128x128), making it easier to learn a transfer that
only needs local image patches (like style/texture transfer)
than entire-image solutions like TraVeLGAN’s high-level
semantic mappings.

We chose classes arbitrarily because we seek a frame-
work that is ﬂexible enough to make translations between
any domains, even when those classes are very different and
arbitrarily picked (as opposed to speciﬁc domains contrived
to satisfy particular assumptions). The pairs are: 1. aba-
cus and crossword (Figures 2a and S2) 2. volcano and jack-
-o-lantern (Figures 2b and S5) 3. clock and hourglass (Fig-
ures 2c and S4) 4. toucan and rock beauty (Figures 2d and
S3).

Asymmetric domains Learning to map between the do-
mains of abacus and crossword showcase a standard prop-
erty of arbitrary domain mapping: the amount and nature
of variability in one domain is larger than in the other. In
Figure 4, we see that the TraVeLGAN learned a seman-
tic mapping from an abacus to a crossword by turning the
beads of an abacus into the white squares in a crossword
and turning the string in the abacus to the black squares.
However, in an abacus, the beads can be aligned in any

Figure 5:
(a) A real crossword image artiﬁcially manipulated
to move a white square around the frame. (b) The TraVeLGAN,
which has not seen any of these images during training, has learned
a semantic mapping between the domains that moves an abacus
bead appropriately with the crossword square.

Figure 6: The CycleGAN generates images such that pairwise
L2-distances in pixel space are strongly preserved. The TraVeL-
GAN generated images are virtually uncorrelated in pixel space,
but the siamese network learns a latent space where pairwise dis-
tances are preserved.

shape, while in crosswords only speciﬁc grids are feasible.
To turn the abacus in Figure 4 (which has huge blocks of
beads that would make for a very difﬁcult crossword in-
deed!)
into a realistic crossword, the TraVeLGAN must
make some beads into black squares and others into white
squares. The cycle-consistency loss ﬁghts this one-to-many
mapping because it would be hard for the other generator,
which is forced to also be the inverse of this generator, to
learn the inverse many-to-one function. So instead, it learns
a precise, rigid bead-to-white-square and string-to-black-
square mapping at the expense of making a realistic cross-
word (Figure 4b). Even though the background is an unim-
portant part of the image semantically, it must recover all of
the exact pixel-wise values after cycling. We note that the
TraVeLGAN automatically relaxed the one-to-one relation-
ship of beads to crossword squares to create realistic cross-
words. On the other hand, any real crossword conﬁguration
is a plausible abacus conﬁguration. In the next section, we
show that the TraVeLGAN also automatically discovered
this mapping can be one-to-one in white-squares-to-beads,
and preserves this systematically.

Manipulated images study Next we examine the degree
to which the TraVeLGAN has learned a meaningful seman-

8989

FID score

TraVeLGAN
Cycle
Cycle+identity
Cycle+corr
Cycle+featmatch
Cycle+latent

(a)

1.026
1.350
1.535
1.519
1.357
1.221

(b)

0.032
1.281
0.917
0.527
1.331
0.485

(c)

0.698
1.018
1.297
0.727
1.084
1.104

(d)

0.206
0.381
1.067
0.638
0.869
0.543

Table 3: FID scores for each of the models on each of the Ima-
genet datasets. Column labels correspond to Figure 2.

Discriminator score

(a)

TraVeLGAN
Cycle
Cycle+identity
Cycle+corr
Cycle+featmatch
Cycle+latent

0.035
0.014
0.011
0.009
0.002
0.009

(b)

0.206
0.008
0.044
0.191
0.029
0.069

(c)

0.074
0.033
0.040
0.026
0.066
0.047

(d)

0.145
0.008
0.064
0.001
0.014
0.039

Table 4: Discriminator scores for each of the models on each of
the Imagenet datasets. Column labels correspond to Figure 2.

tic mapping between domains. Since the Imagenet classes
are so cluttered and heterogeneous and lack repetition in the
form of two very similar images, we create similar images
with a manipulation study. We have taken one of the real
images in the crossword domain, and using standard photo-
editing software, we have created systematically related im-
ages. With these systematically related images, we can test
to see whether the TraVeLGAN’s mapping preserves the se-
mantics in the abacus domain in a systematic way, too.

In Figure 5, we started with a crossword and created a
regular three-by-three grid of black squares by editing an
image from Figure S2. Then, systematically, we move a
white square around the grid through each of the nine po-
sitions.
In each case, the TraVeLGAN generates an aba-
cus with a bead moving around the grid appropriately. Re-
markably, it even colors the bead to ﬁt with the neighboring
beads, which differ throughout the grid. Given that none
of the speciﬁc nine images in Figure 5 were seen in train-
ing, the TraVeLGAN has clearly learned the semantics of
the mapping rather than memorizing a speciﬁc point.

Pairwise distance preservation The DistanceGAN [6]
has shown that approximately maintaining pairwise dis-
tances between images in pixel space achieves similar suc-
cess to the cycle-consistent GANs.
In fact, they show
that cycle-consistent GANs produce images that preserve
the pixel pairwise distance between images with extremely
highly correlation. On the toucan to rock beauty dataset,
we observe the same phenomenon (r2 = 0.82 in Figure 6).
While this produced plausible images in some cases, main-
taining pixel-wise distance between images could not gen-
erate realistic toucans or rock beauties. The TraVeLGAN
pairwise distances are virtually uncorrelated in pixel space
(r2 = 0.17). However, we understand the role of the
siamese network when we look at the pairwise distances

between real images in latent space and the correspond-
ing pairwise distances between generated images in latent
space. There we see a similar correlation (r2 = 0.72).
In other words, the TraVeLGAN simultaneously learns a
mapping with a neural network to a space where distances
can be meaningfully preserved while using that mapping to
guide it in generating realistic images.

Quantitative results Lastly, we add quantitative evidence
to the qualitative evidence already presented that the TraV-
eLGAN outperforms existing models when the domains are
very different. While we used the SSIM and pixel-wise
MSE in the previous sections to evaluate success, neither
heuristic is appropriate for these datasets. The goal in these
mappings is not to leave the image unchanged and as simi-
lar to the original as possible, it is to fully change the image
into the other domain. Thus, we apply use two different
metrics to evaluate the models quantitatively on these Ima-
genet datasets.

In general, quantifying GAN quality is a hard task [5].
Moreover, here we are speciﬁcally interested in how well
a generated image is paired or corresponding to the origi-
nal image, point-by-point. To the best of our knowledge,
there is no current way to measure this quantitatively for
arbitrary domains, so we have pursued the qualitative eval-
uations in the previous sections. However, in addition to
those qualitative evaluation of the correspondence aspect,
we at least quantify how well the generated images resem-
ble the target domain, at a population level, with heuristic
scores designed to measure this under certain assumptions.
The ﬁrst, the Fr´echet Inception Distance (FID score) [12]
is an improved version of the Inception Score (whose ﬂaws
were well articulated in [5]) which compares the real and
generated images in a layer of a pre-trained Inception net-
work (Table 3). The second, the discriminator score, trains
a discriminator from scratch, independent of the one used
during training, to try to distinguish between real and gen-
erated examples (Table 4). The TraVeLGAN achieved bet-
ter scores than any of the baseline models with both metrics
and across all datasets.

4. Discussion

In recent years, unsupervised domain mapping has
been dominated by approaches building off of the cycle-
consistency assumption and framework. We have identi-
ﬁed that some cluttered, heterogeneous, asymmetric do-
mains cannot be successfully mapped between by genera-
tors trained on this cycle-consistency approach. Further im-
proving the ﬂexibility of domain mapping models may need
to proceed without the cycle-consistent assumption, as we
have done here.

8990

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ﬂow: a system for large-scale machine learning. In OSDI,
volume 16, pages 265–283, 2016. 17

[2] A. Almahairi, S. Rajeswar, A. Sordoni, P. Bachman, and
Augmented cyclegan: Learning many-
arXiv preprint

A. Courville.
to-many mappings from unpaired data.
arXiv:1802.10151, 2018. 2, 5

[3] M. Amodio and S. Krishnaswamy. Magan: Aligning biolog-
ical manifolds. arXiv preprint arXiv:1803.00385, 2018. 2,
5, 17

[4] A. Anoosheh, E. Agustsson, R. Timofte, and L. Van Gool.
Combogan: Unrestrained scalability for image domain trans-
lation. arXiv preprint arXiv:1712.06909, 2017. 2, 5

[5] S. Barratt and R. Sharma. A note on the inception score.

arXiv preprint arXiv:1801.01973, 2018. 8

[6] S. Benaim and L. Wolf. One-sided unsupervised domain
mapping. In Advances in neural information processing sys-
tems, pages 752–762, 2017. 3, 8

[7] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks.
In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 1, page 7, 2017. 3

[8] Large-scale

celebfaces

attributes

(celeba)

dataset.

http://mmlab.ie.cuhk.edu.hk/projects/
CelebA.html. Accessed: 2018-10-20. 6

[9] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo.
Stargan: Uniﬁed generative adversarial networks for multi-
domain image-to-image translation. arXiv preprint, 1711,
2017. 2, 5

[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. Ieee, 2009. 7

[11] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. arXiv preprint arXiv:1606.00704, 2016.
1, 2

[12] Fr´echet inception distance (ﬁd score) in pytorch. https://
github.com/mseitzer/pytorch-fid. Accessed:
2018-10-20. 8, 17

[13] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In 2016 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 2414–2423. IEEE, 2016. 1, 3

[14] Y. Goldberg and O. Levy. word2vec explained: deriving
mikolov et al.’s negative-sampling word-embedding method.
arXiv preprint arXiv:1402.3722, 2014. 3, 4

[15] J. Hoffman, E. Rodner,

J. Donahue, T. Darrell, and
K. Saenko. Efﬁcient learning of domain-invariant image rep-
resentations. arXiv preprint arXiv:1301.3224, 2013. 1

[16] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adver-
sarial domain adaptation. arXiv preprint arXiv:1711.03213,
2017. 2

[17] X. Huang and S. J. Belongie. Arbitrary style transfer in real-
time with adaptive instance normalization. In ICCV, pages
1510–1519, 2017. 1

[18] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.

Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017. 1

[19] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution.
In European
Conference on Computer Vision, pages 694–711. Springer,
2016. 5

[20] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. arXiv preprint arXiv:1703.05192, 2017. 2, 5, 17

[21] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 17

[22] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
17

[23] C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and
L. Carin. Alice: Towards understanding adversarial learn-
ing for joint distribution matching. In Advances in Neural
Information Processing Systems, pages 5495–5503, 2017. 2
[24] T. Lindvall. Lectures on the coupling method. Courier Cor-

poration, 2002. 1

[25] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In Advances in Neural Informa-
tion Processing Systems, pages 700–708, 2017. 1, 2, 3, 6

[26] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-
In Advances in neural information processing sys-

works.
tems, pages 469–477, 2016. 1, 2

[27] Y. Lu, Y.-W. Tai, and C.-K. Tang. Conditional cyclegan
for attribute guided face image generation. arXiv preprint
arXiv:1705.09966, 2017. 2, 5

[28] I. Melekhov, J. Kannala, and E. Rahtu. Siamese network fea-
tures for image matching.
In Pattern Recognition (ICPR),
2016 23rd International Conference on, pages 378–383.
IEEE, 2016. 5

[29] E.-J. Ong, S. Husain, and M. Bober. Siamese network of
arXiv

deep ﬁsher-vector descriptors for image retrieval.
preprint arXiv:1702.00338, 2017. 5

[30] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015. 6, 17

[31] A. Royer, K. Bousmalis, S. Gouws, F. Bertsch, I. Moressi,
F. Cole, and K. Murphy. Xgan: Unsupervised image-
to-image translation for many-to-many mappings.
arXiv
preprint arXiv:1711.05139, 2017. 2, 3, 5

[32] P. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo. From
source to target and back: symmetric bi-directional adaptive
gan. arXiv preprint arXiv:1705.08824, 2017. 1

[33] igan.

https://github.com/junyanz/iGAN/
tree/master/train_dcgan. Accessed: 2019-02-01.
6

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.

8991

In Proceedings of the
Going deeper with convolutions.
IEEE conference on computer vision and pattern recogni-
tion, pages 1–9, 2015. 17

[35] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-
domain image generation. arXiv preprint arXiv:1611.02200,
2016. 1

[36] T. Yao, Y. Pan, C.-W. Ngo, H. Li, and T. Mei. Semi-
supervised domain adaptation with subspace learning for vi-
sual recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2142–2150,
2015. 1

[37] Z. Yi, H. R. Zhang, P. Tan, and M. Gong. Dualgan: Unsuper-
vised dual learning for image-to-image translation. In ICCV,
pages 2868–2876, 2017. 2, 3

[38] W. Zhang, C. Cao, S. Chen, J. Liu, and X. Tang. Style trans-
IEEE Transactions on

fer via image component analysis.
multimedia, 15(7):1594–1601, 2013. 1

[39] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. arXiv preprint, 2017. 2, 5, 6, 17

8992

