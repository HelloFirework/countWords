Blind Super-Resolution With Iterative Kernel Correction

Jinjin Gu1∗, Hannan Lu2∗, Wangmeng Zuo2, Chao Dong3

1The School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen
2School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
3ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab,

Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

jinjingu@link.cuhk.edu.cn, {hannanlu, wmzuo}@hit.edu.cn, chao.dong@siat.ac.cn

Abstract

Deep learning based methods have dominated super-
resolution (SR) ﬁeld due to their remarkable performance in
terms of effectiveness and efﬁciency. Most of these methods
assume that the blur kernel during downsampling is prede-
ﬁned/known (e.g., bicubic). However, the blur kernels in-
volved in real applications are complicated and unknown,
resulting in severe performance drop for the advanced SR
methods. In this paper, we propose an Iterative Kernel Cor-
rection (IKC) method for blur kernel estimation in blind SR
problem, where the blur kernels are unknown. We draw the
observation that kernel mismatch could bring regular ar-
tifacts (either over-sharpening or over-smoothing), which
can be applied to correct inaccurate blur kernels. Thus
we introduce an iterative correction scheme – IKC that
achieves better results than direct kernel estimation. We fur-
ther propose an effective SR network architecture using spa-
tial feature transform (SFT) layers to handle multiple blur
kernels, named SFTMD. Extensive experiments on synthetic
and real-world images show that the proposed IKC method
with SFTMD can provide visually favorable SR results and
the state-of-the-art performance in blind SR problem.

1. Introduction

As a fundamental low-level vision problem, single image
super-resolution (SISR) is an active research topic and has
attracted increasingly attention. SISR aims to reconstruct
the high-resolution (HR) image from its low-resolution
(LR) observation. Since the seminal work of employing
convolutional neural networks (CNNs) for SR [6], various
deep learning based methods with different network archi-
tectures [15, 16, 18, 29, 41, 10, 40] and training strategies
[19, 34, 27, 5] have been proposed to continuously im-
prove the SR performance. Most of the existing advanced

∗This work was done when they were interns at SenseTime.

LR image

ZSSR [27]

SR without kernel correction

Iterative Kernel Correction (ours)

Figure 1. SISR results on image “img 017” with SR factor 4. Be-
fore bicubic downsamping, the HR image is blurred by a Gaussian
kernel with σ = 1.8

SR methods assume that the downsampling blur kernel is
known and pre-deﬁned, but the blur kernels involved in real
applications are typically complicated and unavailable. As
has been revealed in [9, 36], learning-based methods will
suffer severe performance drop when the pre-deﬁned blur
kernel is different from the real one. This phenomenon of
kernel mismatch will introduce undesired artifacts to output
images, as shown in Figure 2. Thus the problem with un-
known blur kernels, also known as blind SR, has failed most
of deep learning based SR methods and largely limited their
usage in real-world applications.

Most existing blind SR methods are model-based [3, 32,
11, 12, 14], which usually involve complicated optimization
procedures. They predict the underlying blur kernel using
self-similarity properties of natural images [23]. However,
their predictions are easily affected by input noises, lead-
ing to inaccurate kernel estimation. A few deep learning
based methods have also tried to make progress for blind
SR. For example, in CAB [25] and SRMD [39], the net-

11604

work can take the blur kernel as an additional input and
generate different results according to the provided kernel.
They achieve satisfactory performance if the input kernel
is close to the ground truth. However, these methods still
cannot predict the blur kernel for every image on hand, thus
are not applicable in real applications. Although deep learn-
ing based methods have dominated SISR, they have limited
progress on blind SR problem.

In this paper, we focus on using deep learning methods
to solve the blind SR problem. Our method stems from the
observation that artifacts caused by kernel mismatch have
regular patterns. Speciﬁcally, if the input kernel is smoother
than the real one, then the output image will be blurry/over-
smoothing. Conversely, if the input kernel is sharper than
the correct one, then the results will be over-shapened with
obvious ringing effects (see Figure 2). This asymmetry of
kernel mismatch effect provides us an empirical guidance
on how to correct an inaccurate blur kernel. In practical, we
propose an Iterative Kernel Correction (IKC) method for
blind SR based on predict-and-correct principle. The esti-
mated kernel is iteratively corrected by observing the previ-
ous SR results, and gradually approaches the ground truth.
Even the predicted blur kernel is slightly different from the
real one, the output image can still get rid of those regular
artifacts caused by kernel mismatch.

By further diving into the SR methods proposed for
multiple blur kernels (i.e., SRMD [39]), we ﬁnd that tak-
ing the concatenation of image and blur kernel as input is
not the optimal choice. To make a step forward, we em-
ploy spatial feature transform (SFT) layers [33] and pro-
pose an advanced CNN structure for multiple blur kernels,
namely SFTMD. Experiments demonstrate that the pro-
posed SFTMD is superior to SRMD by a large margin. By
combining the above components – SFTMD and IKC, we
achieve state-of-the-art (SOTA) performance on blind SR
problem.

We summarize our contributions as follows: (1) We pro-
pose an intuitive and effective deep learning framework for
blur kernel estimation in single image super resolution. (2)
We propose a new non-blind SR network using the spa-
tial feature transform layers for multiple blur kernels. We
demonstrate the superior performance of the proposed non-
blind SR network. (3) We test the blind SR performance on
both carefully selected blur kernels and real images. Exten-
sive experiments show that the combination of SFTMD and
IKC achieves the SOTA performance in blind SR problem.

2. Related Work

Super-Resolution Neural Networks.

In the past few
years, neural networks have shown remarkable capability
on improving SISR performance. Since the pioneer work
of using CNN to learn the end-to-end mapping from LR to
HR images [6], plenty of CNN architectures have been pro-

posed for SISR [7, 26, 18, 10, 16, 28]. In order to go deeper
in network depth and achieve better performance, most
of the existing high-performance SR networks have resid-
ual architecture [15]. SRGAN [19] ﬁrst introduce residual
blocks into SR networks. EDSR [20] improve it by remov-
ing unnecessary batch normalization layer in residual block
and expanding the model size. DenseSR [41] present an ef-
fective residual dense block and ESRGAN [34] further use
a residual-in-residual dense block to improve the perceptual
quality of SR results. Zhang et al. [40] introduce the chan-
nel attention component in residual blocks. Some networks
are speciﬁcally designed for the SR task in some special
scenarios, e.g., Wang et al. [33] use a novel spatial feature
transform layer to introduce the semantic prior as an addi-
tional input of SR network. Moreover, Riegler et al. [25]
propose conditioned regression models can effectively ex-
ploit the additional kernel information during training and
inference. SRMD [39] propose a stretching strategy to inte-
grate non-image degradation information in a SR network.
Blind Super-Resolution. Blind SR assume that the
degradation kernels are unavailable.
In recent years, the
community has paid relatively less research attention to
blind SR problem. Michaeli and Irani [23] estimate the
optimal blur kernel based on the property that small im-
age patches will re-appear in images. There are also re-
search works trying to employ deep learning in blind SR
task. Yuan et al. [37] propose to learn not only SR mapping
but also the degradation mapping using unsupervised learn-
ing. Shocher et al. [27] exploit the internal recurrence of
information inside an image and propose an unsupervised
SR method to super-resolve images with different blur ker-
nels. They train a small CNN on examples extracted from
the input image itself, the trained image-speciﬁc CNN is
appropriate for super-resolving this image. Different from
the previous works, our method employs the correlation be-
tween SR results and kernel mismatch. Our method uses the
intermediate SR results to iteratively correct the estimation
of blur kernels, thus provide artifact-free ﬁnal SR results.

3. Method

3.1. Problem Formulation

The blind super-resolution problem is formulated as fol-
lows. Mathematically, the HR image I HR and LR image
I LR are related by a degradation model

I LR = (k ⊗ I HR) ↓s +n,

(1)

where ⊗ denotes convolution operation. There are three
main components in this model, namely the blur kernel k,
the downsampling operation ↓s and the additive noise n. In
literature, the most widely adopted blur kernel is isotropic
Gaussian blur kernel [8, 36, 39]. Besides, the anisotropic
blur kernels also appear in some works [25, 39], which can

1605

be regarded as the combination of a motion blur and an
isotropic blur kernel. For simplicity, we mainly focus on
the isotropic blur kernel without motion effect in this pa-
per. Following most recent deep learning based SR methods
[39], we adopt the combination of Gaussian blur and bicu-
bic downsampling. In real-world use cases, the LR images
are often accompanied with additive noises. As in SRMD
[39], we assume that the additive noise follows Gaussian
distribution in real world application. Note that the formu-
lation of blind SR in this paper is different with the previous
works [23, 37] . Although deﬁned as blind SR problem, our
method focuses on a limited variety of kernels and noise.
But the kernel estimated according to our assumptions can
handle most of the real world images.

3.2. Motivation

We then review the importance of using correct blur ker-
nel during SISR based on the settings described above. In
order to obtain the LR images I LR, the HR images I HR
are ﬁrst blurred by the isotropic Gaussian kernel with ker-
nel width σLR and then downsampled by bicubic interpola-
tion. Assume that the mapping F(I LR, k) is a well-trained
SR model with the kernel information as input (e.g., SRMD
[39]). Then the output image is artifact-free with correct
kernel k. The blind SR problem is equivalent to ﬁnding the
kernel k that helps SR model generate visual pleasing re-
sult I SR. A straightforward solution is to adopt a predictor
function k′ = P(I LR) that estimates k from the LR input
directly. The predictor can be optimized by minimizing the
l2 distance as

θP = arg min

kk − P(I LR; θP )k2
2,

(2)

θP

where θP is the parameter of P. By employing the predictor
function and the SR model together, we are able to build an
end-to-end blind SR model.

However, accurate estimation of k is impossible. As the
inverse problem is ill-posed, there exists multiple candi-
dates of k for a single input. Meanwhile, the SR models
are very sensitive to the estimation error. If the inaccurate
kernel is used for SR directly, then the ﬁnal SR results will
contain obvious artifacts. Figure 2 shows the sensitivity of
the SR results to kernel mismatch, where σSR denotes the
kernel width used for SR. As shown in the upper-right re-
gion of Figure 2, where the kernel used for SR are sharper
than the real one (σSR < σLR), the SR results are over-
smoothing and the the high frequency textures are signiﬁ-
cantly blurred. In the lower-left region of Figure 2, where
the kernel used for SR are smoother than the correct one
(σSR > σLR), the SR results show unnatural ringing arti-
facts caused by over-enhancing high-frequency edges. In
contrast, the results on the diagonal, which use correct blur
kernels, look natural without artifacts and blurring. The

σLR = 1.5

σLR = 2.0

σLR = 2.5

σLR = 3.0

5
.
1
=
R
S
σ

0
.
2
=
R
S
σ

5
.
2
=
R
S
σ

0
.
3
=
R
S
σ

Figure 2. SR sensitivity to the kernel mismatch. Where σLR de-
notes the kernel used for downsampling and σSR denotes the ker-
nel used for SR.

above phenomenon illustrates that the estimation error of
k will be signiﬁcantly magniﬁed by the SR model, resulting
in unnatural output images. To address the kernel mismatch
problem, we propose to iteratively correct the kernel until
we obtain an artifact-free SR results.

To correctly estimate k, we build a corrector function C
that measures the difference between the estimated kernel
and the ground truth kernel. In the core of our idea is to
adopt the intermediate SR results. The corrector function
can be obtained by minimizing the l2 distance between the
corrected kernel and the ground truth as

θC = arg min

kk − (C(I SR; θC) + k′)k2
2,

(3)

θC

where θC is the parameter of C and I SR is the SR result
using the last estimated kernel. This corrector adjusts the
estimated blur kernel based on the features of the SR image.
After correction, the SR results using adjusted kernel are
supposed to approach natural images with less artifacts.

However, if we train our model with only one time of
correction, the corrector may provide inadequate correc-
tion or over-correct the kernel, leading to unsatisfactory
SR results. A possible solution is to use smaller correc-
tion steps that gradually reﬁne the kernel until it reaches
ground truth. When the SR result does not contain seri-
ous over-smoothing or over-sharpening effects, the correc-
tor will make little changes to the estimated kernel to en-
sure convergence. Then we are able to get a high-quality
SR image by iteratively applying kernel correction. Experi-
ments also demonstrate our assumption. Figure 3 shows the
PSNR and SSIM results using different iteration numbers.

1606

)

B
d
(
R
N
S
P

28

27.7

27.4

27.1

26.8

26.5

26.2

I

M
S
S

0.76

0.75

0.74

0.73

0.72

0.71

0.7

0

1

2

3

4
Iterations

5

6

7

0

1

2

3

4

5

6

7

Iterations

Figure 3. The curves of PSNR and SSIM vs. iterations. The ex-
periments are conducted using IKC method. The test set is Set14
and the SR factor is 4.

It can be observed that correcting only once is not sufﬁ-
cient. When the number of iterations increases, both PSNR
and SSIM increase gradually until convergence.

3.3. Proposed Method

Overall framework. The proposed Iterative Kernel Cor-
rection (IKC) framework consists of a SR model F , a pre-
dictor P and a corrector C, and the pseudo-code is shown
in Algorithm 1. Suppose the LR image I LR is of size
C × H × W , where C denotes the number of channels,
H and W denote the height and width of the image. We as-
sume that blur kernel is of size l × l and the kernel space is a
l2-dimensional linear space. In order to save computation,
we ﬁrst reduce the dimensionality of the kernel space by
principal component analysis (PCA). The kernels are pro-
jected onto a b-dimensional linear space by a dimension re-
duction matrix M ∈ Rb×l2
. Thus we only need to perform
estimation in this low dimensional space, which is more ef-
fective in calculation. The kernel after the dimension reduc-
tion is denoted by h, where h = M k, h ∈ Rb. At the start of
the algorithm, an initial estimation h0 is given by the predic-
tor function h0 = P(I LR), and then used to get the ﬁrst SR
result I SR
0 = F(I LR, h0). After obtaining the initial esti-
mation, we proceed to the correction phase of the estimated
kernel. At the ith iteration, given the previous estimation
hi−1, the correcting update ∆hi, the new estimation hi and
the new SR result I SR

can be written as

i

∆hi = C(I SR

i

, hi−1)

hi = hi−1 + ∆hi
I SR
i = F(I LR, hi).

(4)

(5)

(6)

After t iterations, the I SR

t

is the ﬁnal output of IKC.

Network architecture of SR model F . As the most suc-
cessful SR method for multiple blur kernels, SRMD [39]
propose a simple yet efﬁcient stretching strategy for CNN
to process non-image input directly. SRMD stretches the
input h into kernel maps H of size b × H × W , where all
the elements of the ith map are equal to the ith element of h.
SRMD takes the concatenated LR image and kernel maps of
size (b+C)×H ×W as input. Then, a cascade of 3×3 con-
volution layers and one pixel-shufﬂe upsampling layer are
applied to perform super-resolution. However, to exploit the

Algorithm 1 Iterative Kernel Correction
Require: the LR image I LR
Require: the max iteration number t
1: h0 ← P(I LR) (Initialize the kernel estimation)
2: I SR
3: i ← 0 (Initialize counter)
4: while i < t do
i ← i + 1
5:
∆hi ← C(I SR

0 ← F(I LR, h0) (The initial SR result)

6:

i−1, hi−1) (Estimate the kernel error us-

ing the intermediate SR results)

7:

hi ← hi−1 + ∆hi (Update kernel estimation)
i ← F(I LR, hi) (Update the SR result)
I SR

8:
9: return I SR

t

(Output the ﬁnal SR result)

kernel information, concatenating the image and the trans-
formed kernel as input is not the only or best choice. On the
one hand, the kernel maps do not actually contain the infor-
mation of the image. Processing the kernel maps and the
image at the same time with convolution operation will in-
troduce interference that is not related to the image. Using
this concatenation strategy with residual blocks can inter-
fere with image processing, making it difﬁcult to employ
residual structure to improve performance. On the other
hand, the inﬂuence of kernel information is only consid-
ered at the ﬁrst layer. When applying the same strategy in a
deeper network, the deeper layers are difﬁcult to be affected
by the kernel information input at the ﬁrst layer. To address
above problems, we proposed a new SR model for multiple
kernels using spatial feature transform (SFT) layers [33],
namely SFTMD. In SFTMD, the kernel maps inﬂuence the
output of network by applying an afﬁne transformation to
the feature maps in each middle layer by SFT layers. This
afﬁne transformation is not involved in the process of input
image directly, thus providing better performance.

Figure 4 illustrates the network architecture of SFTMD.
We employ the high level architecture of SRResNet [19]
and extend it to handle multiple kernels by SFT layers.
The SFT layer provides afﬁne transformation for the fea-
ture maps F conditioned on the kernel maps H by a scaling
and shifting operation:

SFT(F, H) = γ ⊙ F + β,

(7)

where γ and β is the parameters for scaling and shifting,
⊙ present Hadamard product. The transformation param-
eters γ and β are obtained by small CNN. Suppose that
the output feature maps of the previous layer F are of size
Cf × H × W , where Cf is the number of feature maps,
and the kernel maps are of size b × H × W . The CNN
takes the concatenated feature maps and kernel maps (total
size is (b + Cf ) × H × W ) as input and output γ and β. We
use SFT layers after all convolution layers in residual blocks

1607

ILR

Conv

Stretch

H

h

F

!
γ
Sigmoid

Convs

+
β

Convs

Single SFT Layer

SFT Layer

+

Conv Conv

Residual

Block

k
c
o
l
B

 
l
a
u
d
i
s
e
R

k
c
o
l
B

 
l
a
u
d
i
s
e
R

+

Pixel Shuffle

Upsample

ISR

Figure 4. The architecture of the proposed SFTMD network. The design of the proposed SFT layer is shown in pink box.

The Predictor Network P

The Corrector Network C

I LR

Conv Layers

Estimation

Maps

Global
Pooling

h0

I SR
i

hi−1

FC Layers

Conv Layers

fh

Stretch

FSR

t
a
C

Fh

Conv Layers

Estimation

Maps

Global
Pooling

∆hi

Figure 5. The network architecture of the proposed predictor and corrector.

and after the global residual connection. It is worth pointing
out that the code maps are spatially uniform, thus the SFT
layers do not actually provide spatial variability according
to the code maps. This is different from its application in
semantic super resolution [33]. We only employ the trans-
formation characteristic of SFT layers.

Network architecture of predictor P and corrector
C. The network designs of the predictor and corrector are
shown in Figure 5. For the predictor P, we use four convo-
lution layers with Leaky ReLU activations and a global av-
erage pooling layer. The convolution layers give the estima-
tion of the kernel h spatially and form the estimation maps.
Then the global average pooling layer gives the global esti-
mation by taking the mean value spatially.

For the corrector C, we take not only the SR image I SR
but also the previous estimation h as inputs. Similar to Eq.
(3), the new corrector can be obtained by solving the fol-
lowing optimization problem:

θC = arg min

kk − (C(I SR, h; θC) + k′)k2
2.

(8)

θC

The input SR result is ﬁrst processed to feature maps FSR
by ﬁve convolution layers with Leaky ReLU activations.
Note that the previous SR result may contain artifacts (e.g.,
ringing and blurry) caused by kernel mismatch, which can
be extracted by these convolution layers. At the same time,
we use two fully-connected layers with Leaky ReLU activa-
tions to extract the inner correlations of the previous kernel
estimation. We then stretch the output vector fh to feature
maps Fh using the same strategy used in SFTMD. The Fh
and FSR are then concatenated to predict the ∆h. We use
three convolution layers with kernel size 1 × 1 and Leaky

ReLU activations to give the estimation for ∆h spatially.
Same as the predictor, a global average pooling operation is
used to get the global estimation of ∆h.

4. Experiments

4.1. Data Preparation and Network Training

We synthesize the training image pairs according to the
problem formulation described in section 3.1. For the
isotropic Gaussian blur kernels used for training, the ker-
nel width ranges are set to [0.2, 2.0], [0.2, 3.0] and [0.2, 4.0]
for SR factors 2, 3 and 4, respectively. We uniformly sample
the kernel width in the above ranges. The kernel size is ﬁxed
to 21×21. When applying on real world images, we use the
additive Gaussian noise with covariance σ = 15. We also
provide noise-free version for comparison on the synthetic
test images. The HR images are collected from DIV2K [1]
and Flickr2K [30], then the training set consists of 3450
high-quality 2K images. The training dataset is augmented
with random horizontal ﬂips and 90 degree rotations. All
models are trained and tested on RGB channels.

The SFTMD and IKC are both trained on the synthetic
training image pairs and their corresponding blur kernels.
First, the SFTMD is pre-trained using mean square error
(MSE) loss. We then train the predictor network and the
corrector network alternately. The parameters of the trained
SFTMD are ﬁxed during training the predictor and the cor-
rector. The order of training can refer to Algorithm 1.
For every mini-batch data {I LR
i=1, where N de-
notes the mini-batch size, we ﬁrst update the parameters of
the predictor according to Eq. (2). We then update the cor-

, hi}N

, I HR

i

i

1608

Table 1. Quantitative comparison of SRCNN-CAB [25], SRMDNF [39] and the proposed SFTMD. The comparison is conducted using
three different isotropic Gaussian kernels on Set5, Set14 and BSD100 dataset. The best two results are highlighted in red and blue colors.

Method

SRCNN-CAB [25]
SRMDNF [39]
SRResNet, concatenate at the ﬁrst layer
SRResNet, replace SFT layer by direct concatenation
SFTMD (ours)
SRCNN-CAB [25]
SRMDNF [39]
SRResNet, concatenate at the ﬁrst layer
SRResNet, replace SFT layer by direct concatenation
SFTMD (ours)
SRCNN-CAB [25]
SRMDNF [39]
SRResNet, concatenate at the ﬁrst layer
SRResNet, replace SFT layer by direct concatenation
SFTMD (ours)

Kernel Width

Set5 [4]

Set14 [38]

BSD100 [21]

×2

×3

×4

×2

×3

×4

×2

×3

×4

0.2

1.3

2.6

33.27
37.79
31.74
37.69
38.00
33.42
37.44
30.88
37.01
38.00
32.21
34.12
24.22
27.75
38.00

31.03
34.13
30.90
34.01
34.57
31.14
34.17
30.33
34.02
34.57
30.82
33.02
28.44
32.71
34.57

29.31
31.96
29.40
31.64
32.39
29.50
32.00
29.11
31.69
32.39
28.81
31.77
28.64
31.35
32.39

30.29
33.33
27.57
33.26
33.68
30.51
33.20
27.16
32.96
33.68
29.74
30.25
22.99
25.67
33.68

28.29
30.04
26.40
30.04
30.47
28.34
30.08
25.84
30.13
30.47
27.83
29.33
24.19
29.28
30.47

26.91
28.35
26.18
28.23
28.77
27.02
28.42
25.93
28.29
28.77
26.15
28.26
25.63
28.07
28.77

28.98
32.05
27.24
31.83
32.09
29.02
31.98
26.84
31.58
32.09
28.35
29.23
23.07
25.57
32.09

27.65
28.97
26.43
28.81
29.09
27.91
29.03
25.92
28.89
29.09
26.63
28.35
24.42
28.19
29.09

25.51
27.49
26.34
27.26
27.58
25.66
27.53
26.20
27.29
27.58
25.13
27.43
25.99
27.15
27.58

rector according to Eq. (8) with a ﬁxed iteration number
t = 7. For optimization, we use Adam [17] with β1 = 0.9,
β2 = 0.999 and learning rate 1 × 10−4. We implement our
models with the PyTorch framework and train them using
NVIDIA Titan Xp GPUs.

We also propose a test kernel set for the quantitative
evaluation of blind SR methods, namely Gausssian8. As
declared by the name, Gausssian8 consists eight selected
isotropic Gaussian blur kernels for each SR factor 2, 3 and
4 (twenty four kernels in total). The ranges of kernel width
are set to [0.80, 1.60], [1.35, 2.40] and [1.80, 3.20] for SR
factors 2, 3 and 4, respectively. The HR images are ﬁrst
blurred by the selected blur kernels and then downsampled
by bicubic interpolation. By determining the blur kernels
for testing, we can compare and analyze the performance
of blind SR methods. Although it only contains isotropic
Gaussian kernels, it can still be used to test the basic perfor-
mance of a blind SR method.

4.2. Experiments of SFTMD

We evaluate the performance of the proposed SFTMD
on different Gaussian kernels. The kernel settings are given
in Table 1. We compare the SFTMD with the SOTA non-
blind SR methods SRCNN-CAB [25] and SRMD [39]. As
SFTMD adopts SRResNet as the main network, which is
different from SRMD and SRCNN-CAB, we provide two
additional baselines that have same network structures but
different concatenation strategies: (1) SRResNet with con-
catenating H at the ﬁrst layer, (2) SFTMD with SFT layer
replaced by direct concatenation1.Table 1 shows the quan-
titative comparison results. Comparing with the SOTA SR
methods – SRCNN-CAB and SRMD, the proposed SFTMD
achieves signiﬁcantly better performance on all settings and
dataset. Comparing with two additional baselines that all
use SRResNet as the main network, SFTMD could also ob-
tain the best results. This further demonstrated the effect

1Direct concatenation means concatenating the kernel maps with fea-
ture maps directly. This is different from the afﬁne transformation in the
SFT layer.

of SFT layers. It is worth noting that directly concatenat-
ing H in SRResNet will cause severe performance drop. As
the combination of direct concatenation strategy and resid-
ual structure will interfere with image processing and cause
severe artifacts.

4.3. Experiments on Synthetic Test Images

We evaluate the performance of the proposed method on
the synthetic test images. Figure 7 shows the intermediate
results during correction. As one can see that the SR results
using the kernel estimated by the predictor directly (the ini-
tial prediction in Figure 7) are unsatisfactory and contain
either blurry or ringing artifacts. As the number of itera-
tions increases, artifacts and blurring are gradually allevi-
ated. The quantitative results (PSNR) also prove the neces-
sity of the iterative correction strategy. We can see at the
4th iteration, the SR results using corrected kernels are able
to show good visual quality.

We then conduct thorough comparisons with the SOTA
non-blind and blind SR methods using Gaussian8 kernels.
We also provide the comparison with the solutions using
the SOTA deblurring method. We perform blind debluring
method Pan et al. [24] before and after the non-blind SR
method CARN [2]. Table 2 shows the PSNR and SSIM
[35] results on ﬁve widely-used datasets. As one can see,
despite the remarkable performance under bicubic down-
sampling setting, the non-blind SR methods suffer severe
performance drop when the downsampling kernel is differ-
ent from the predeﬁned bicubic kernel. The ZSSR [27]
takes the effect of blur kernel into account, and provides
better SR performance compared with non-blind SR meth-
ods. Performing blind deblurring on the LR images makes
the SR images sharper, but lost in image quality The ﬁ-
nal SR results have severe distortion. Deblurring on the
blurred super-resolved images provides better results, but
fails to reconstruct textures and details. Although the SR re-
sults without kernel correction (denoted by “P+SFTMD”)
achieves comparable quantitative performance with the ex-
isting methods, the SR performance can still be greatly im-

1609

A+ [31]

CARN [2]

CARN + Pan et al.[24]

ZSSR [27]

P +SFTMD

IKC (ours)

Figure 6. SISR performance comparison of different methods with SR factor 4 and kernel width 1.8 on image “Img 050” from Urban100.

Table 2. Quantitative comparison of the SOTA SR methods and IKC method. The best two results are highlighted in red and blue colors,
respectively. Note that the methods marked with “*” is not designed for blind SR, thus the comparison with these methods is unfair.

Method

Bicubic
CARN∗ [2]
ZSSR [27]
Pan et al. [24] + CARN [2]
CARN [2] + Pan et al. [24]
P+ SFTMD
IKC (ours)
Bicubic
CARN∗ [2]
ZSSR [27]
Pan et al. [24] + CARN [2]
CARN [2] + Pan et al. [24]
P+ SFTMD
IKC (ours)
Bicubic
CARN∗ [2]
ZSSR [27]
Pan et al. [24] + CARN [2]
CARN [2] + Pan et al. [24]
P+ SFTMD
IKC (ours)

Scale

Set5 [4]

Set14 [38]

BSD100 [21]

PSNR

SSIM

PSNR

SSIM

PSNR

SSIM

Urban100 [13]
SSIM

PSNR

Manga109 [22]
PSNR
SSIM

×2

×3

×4

28.82
30.99
31.08
24.20
31.27
35.44
36.62
26.21
27.26
28.25
19.05
30.13
31.26
32.16
24.57
26.57
26.45
18.10
28.69
29.29
31.52

0.8577
0.8779
0.8786
0.7496
0.8974
0.9617
0.9658
0.7766
0.7855
0.7989
0.5226
0.8562
0.9291
0.9420
0.7108
0.7420
0.7279
0.4843
0.8092
0.9014
0.9278

26.02
28.10
28.35
21.12
29.03
31.27
32.82
24.01
25.06
26.11
17.61
27.57
28.41
29.46
22.79
24.62
24.78
16.59
26.40
26.40
28.26

0.7634
0.7879
0.7933
0.6170
0.8267
0.8676
0.8999
0.6662
0.6676
0.6942
0.4558
0.7531
0.7811
0.8229
0.6032
0.6226
0.6268
0.3994
0.6926
0.7137
0.7688

25.92
26.78
27.92
22.69
28.72
30.54
31.36
24.25
25.85
26.06
20.51
27.14
27.37
28.56
23.29
24.79
24.97
18.46
26.10
26.16
27.29

0.7310
0.7286
0.7632
0.6471
0.8033
0.8946
0.9097
0.6356
0.6566
0.6633
0.5331
0.7152
0.8102
0.8493
0.5786
0.5963
0.5989
0.4481
0.6528
0.7648
0.8014

23.14
25.27
25.25
18.89
25.62
27.80
30.36
21.39
22.67
23.26
16.72
24.45
24.57
25.94
20.35
22.17
22.11
15.47
23.46
22.97
25.33

0.7258
0.7630
0.7618
0.5895
0.7981
0.8464
0.8949
0.6203
0.6323
0.6534
0.4578
0.7241
0.7458
0.8165
0.5532
0.5865
0.5805
0.3872
0.6597
0.6722
0.7760

25.60
26.86
28.05
21.54
29.58
30.75
36.06
22.98
23.84
25.19
18.38
27.67
26.29
28.21
21.50
21.85
23.53
16.78
25.84
24.24
29.90

0.8498
0.8606
0.8769
0.7496
0.9134
0.9074
0.9474
0.7576
0.7620
0.7914
0.6118
0.8592
0.8399
0.8739
0.6933
0.6834
0.7240
0.5371
0.8035
0.7950
0.8793

The Initial
Prediction
(23.60dB)

1st

2nd

3rd

4th

Iteration
(25.65dB)

Iteration
(27.75dB)

Iteration
(27.94dB)

Iteration
(27.97dB)

The Initial
Prediction
(22.22dB)

1st

2nd

3rd

4th

Iteration
(24.60dB)

Iteration
(27.30dB)

Iteration
(27.88dB)

Iteration
(27.90dB)

Figure 7. The intermediate SR results during kernel correction.

proved by using the proposed IKC method. An example is
shown in Figure 6. The PSNR values of different methods
on different blur kernels are shown in Figure 9. As can be
seen, when the kernel width becomes larger, the SR perfor-
mance of the previous methods decreases. Meanwhile, the
proposed IKC method achieves superior performance under
all blur kernels.

To further show the generalization ability of the pro-
posed IKC method, we test our method on another widely-
used degradation setting [36], which involves Gaussian ker-
nels and direct downsampler. When the downsampling

Table 3. Quantitative performance of the proposed IKC method on
other downsampling settings.

Method

CARN [2]
ZSSR [27]
CARN [2]+Pan et al. [24]
P+ SFTMD
IKC, w/o PCA
IKC (ours)
CARN [2]
ZSSR [27]
CARN [2]+Pan et al. [24]
P+ SFTMD
IKC, w/o PCA
IKC (ours)

Kernel
Width

BSD100 [21]

BSD100 [21]

PSNR

SSIM

PSNR

SSIM

2.0

3.0

26.05
25.64
25.71
23.42
26.85
27.06
24.20
24.19
25.62
23.30
26.75
26.98

0.6970
0.6771
0.7115
0.6812
0.7694
0.7704
0.6066
0.6045
0.6678
0.6799
0.7685
0.7694

25.92
25.64
25.94
25.01
26.30
26.35
24.53
24.53
25.52
24.41
26.28
26.58

0.6601
0.6446
0.6804
0.7231
0.7812
0.7838
0.5812
0.5796
0.6293
0.7214
0.7849
0.7994

function is different, the LR images obtained by the same
blur kernel are also different. Table 3 shows the quantitative
results of the proposed IKC method under different down-
sampling settings. The proposed IKC method has main-
tained its performance, which indicates that IKC is able
to generalize to a downsampling setting that is inconsis-
tent with the training settings. An important reason why
the IKC method has such generalization ability is that IKC
learns the kernel after PCA rather than the kernel parameter-
ized by kernel width. PCA provides a feature representation

1610

LR image

A+ [31]

ZSSR [27]

CARN [2]

IKC (ours)

Figure 8. SISR performance comparison of different methods with SR factor 4 on a real historic image ‘1967 Vietnam war protest’.

R
N
S
P
e
g
a
r
e
v
A

29

27

25

23

21

19

IKC (ours)
P+SFTMD
ZSSR
CARN
A+

1.35

1.50

1.65

1.80
Kernel Width σ

1.95

2.10

2.25

2.40

Figure 9. The PSNR performance of different methods on BSD100
[21] with different kernel width. The test SR factor is 3.

for the kernels. IKC learns the relationship between the SR
images and these features rather than the Gaussian kernel
width. In Table 3, we provide the comparison with the IKC
method that adopts kernels parameterized by Gaussian ker-
nel width. Experiments prove that the use of PCA helps to
improve the generalization performance of IKC.

LR image

ZSSR [27]

SRMD with hand-craft kernel

IKC (Ours)

Figure 10. SR results of the real image “Chip” with SR factor 4.
The hand-craft kernel width suggested by SRMD is 1.5.

4.4. Experiments on Real Images Set

natural visual effects.

Besides the above experiments on synthetic test images,
we also conduct experiments on real images to demonstrate
the effectiveness of the proposed IKC and SFTMD. Since
there are no ground-truth HR images, we only provide the
visual comparison. Figure 8 shows the SISR results on
real world image from the Historic dataset. For compari-
son, the A+ [31] and CARN [2] are used as the represen-
tative SR methods with bicubic downsampling, and ZSSR
[27] is used as the representative blind SR method. For a
real-world image, the downsampling kernel is unknown and
complicated, thus performance of the non-blind SR meth-
ods are severely affected. The SOTA blind method – ZSSR
also fails to provide satisfactory results. In comparison, IKC
provides artifact-free SR result with sharp edges.

We also compare the proposed IKC method with the
non-blind SR method using ‘hand-craft’ kernel on real-
world image ‘Chip’. We super-resolve the LR image us-
ing SRMD with the ‘hand-craft’ kernel suggested by [39].
They use a grid search strategy to ﬁnd the kernel parameters
with good visual quality. The visual comparison is shown
in Figure 10. We can see that the result of SRMD has harper
edges and higher contrast, but also looks a little artiﬁcial. At
the same time, IKC could provide visual pleasing SR results
automatically. Although the contrast of IKC result is not as
high as SRMD result, it still provides sharp edges and more

5. Discussion

In this paper, we explore the relationship between blur
kernel mismatch and the SR results, then propose an iter-
ative blind SR method – IKC. We also propose SFTMD,
a new SR network architecture for multiple blur kernels.
In this paper, our experiments are mainly conducted on the
isotropic kernels. However, the isotropic kernels don’t seem
to be applicable in some real world applications. As in most
cases, there are some slightly motion blurs that affect the
kernel.
It is worth noting that the asymmetry of the ker-
nel mismatch effect that IKC relies on can still be observed
in the case of slightly motion blur (anisotropy blur kernels).
For example, the artifacts and blur of a SR image in a certain
direction is related to the width of the kernel in the same di-
rection. This indicates that, by employing such asymmetry
of the kernel mismatch in each direction, the IKC method
can also be applied to more realistic cases with slightly mo-
tion blur, which will be our future work.

Acknowledgements. This work is partially supported
by SenseTime Group Limited, National Key Research
and Development Program of China (2016YFC1400704),
Shenzhen Research Program (JCYJ20170818164704758,
JCYJ20150925163005055, CXB201104220032A),
and
Joint Lab of CAS-HK.

1611

References

[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge
on single image super-resolution: Dataset and study. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) Workshops, volume 3, page 2, 2017. 5

[2] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast,
accurate, and lightweight super-resolution with cascading
residual network. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 252–268, 2018. 6,
7, 8

[3] Isabelle Begin and FR Ferrie. Blind super-resolution using
a learning-based approach.
In Pattern Recognition, 2004.
ICPR 2004. Proceedings of the 17th International Confer-
ence on, volume 2, pages 85–89. IEEE, 2004. 1

[4] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie Line Alberi-Morel. Low-complexity single-image
super-resolution based on nonnegative neighbor embedding.
2012. 6, 7

[5] Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To
learn image super-resolution, use a gan to learn how to do
image degradation ﬁrst.
In Proceedings of the European
Conference on Computer Vision (ECCV), pages 185–200,
2018. 1

[6] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE transactions on pattern analysis and machine
intelligence, 38(2):295–307, 2016. 1, 2

[7] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-
ating the super-resolution convolutional neural network. In
European Conference on Computer Vision, pages 391–407.
Springer, 2016. 2

[8] Weisheng Dong, Lei Zhang, Guangming Shi, and Xin
Li. Nonlocally centralized sparse representation for im-
age restoration.
IEEE Transactions on Image Processing,
22(4):1620–1630, 2013. 2

[9] Netalee Efrat, Daniel Glasner, Alexander Apartsin, Boaz
Nadler, and Anat Levin. Accurate blur models vs. image pri-
ors in single image super-resolution. In Proceedings of the
IEEE International Conference on Computer Vision, pages
2832–2839, 2013. 1

[10] Muhammad Haris, Greg Shakhnarovich, and Norimichi
Ukita. Deep backprojection networks for super-resolution.
In Conference on Computer Vision and Pattern Recognition,
2018. 1, 2

[11] He He and Wan-Chi Siu. Single image super-resolution us-
ing gaussian process regression. In Computer Vision and Pat-
tern Recognition (CVPR), 2011 IEEE Conference on, pages
449–456. IEEE, 2011. 1

[12] Yu He, Kim-Hui Yap, Li Chen, and Lap-Pui Chau. A soft
map framework for blind super-resolution image reconstruc-
tion. Image and Vision Computing, 27(4):364–373, 2009. 1
[13] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Sin-
gle image super-resolution from transformed self-exemplars.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5197–5206, 2015. 7

[14] Neel Joshi, Richard Szeliski, and David J Kriegman. Psf es-
In Computer Vision

timation using sharp edge prediction.

and Pattern Recognition, 2008. CVPR 2008. IEEE Confer-
ence on, pages 1–8. IEEE, 2008. 1

[15] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1646–1654, 2016. 1, 2

[16] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 1637–1645, 2016. 1, 2

[17] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 6

[18] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-
Hsuan Yang. Deep laplacian pyramid networks for fast and
accurate superresolution. In IEEE Conference on Computer
Vision and Pattern Recognition, volume 2, page 5, 2017. 1,
2

[19] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, volume 2, page 4, 2017. 1, 2,
4

[20] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for sin-
gle image super-resolution. In The IEEE conference on com-
puter vision and pattern recognition (CVPR) workshops, vol-
ume 1, page 4, 2017. 2

[21] David Martin, Charless Fowlkes, Doron Tal, and Jitendra
Malik. A database of human segmented natural images and
its application to evaluating segmentation algorithms and
measuring ecological statistics. In Computer Vision, 2001.
ICCV 2001. Proceedings. Eighth IEEE International Con-
ference on, volume 2, pages 416–423. IEEE, 2001. 6, 7, 8

[22] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,
Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.
Sketch-based manga retrieval using manga109 dataset. Mul-
timedia Tools and Applications, 76(20):21811–21838, 2017.
7

[23] Tomer Michaeli and Michal Irani. Nonparametric blind
super-resolution. In Proceedings of the IEEE International
Conference on Computer Vision, pages 945–952, 2013. 1, 2,
3

[24] Jinshan Pan, Deqing Sun, Hanspeter Pﬁster, and Ming-
Hsuan Yang. Deblurring images via dark channel prior. IEEE
transactions on pattern analysis and machine intelligence,
40(10):2315–2328, 2018. 6, 7

[25] Gernot Riegler, Samuel Schulter, Matthias Ruther, and Horst
Bischof. Conditioned regression models for non-blind sin-
gle image super-resolution. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, pages 522–530,
2015. 1, 2, 6

[26] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In

1612

[39] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a
single convolutional super-resolution network for multiple
degradations. In IEEE Conference on Computer Vision and
Pattern Recognition, volume 6, 2018. 1, 2, 3, 4, 6, 8

[40] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
286–301, 2018. 1, 2

[41] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 1, 2

Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1874–1883, 2016. 2

[27] Assaf Shocher, Nadav Cohen, and Michal Irani. Zero-shot
super-resolution using deep internal learning. In Conference
on computer vision and pattern recognition (CVPR), 2018.
1, 2, 6, 7, 8

[28] Ying Tai, Jian Yang, and Xiaoming Liu.

Image super-
resolution via deep recursive residual network. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, volume 1, page 5, 2017. 2

[29] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4539–4547, 2017. 1

[30] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-
Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon
Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017
challenge on single image super-resolution: Methods and re-
sults.
In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2017 IEEE Conference on, pages 1110–
1121. IEEE, 2017. 5

[31] Radu Timofte, Vincent De Smet, and Luc Van Gool. A+:
Adjusted anchored neighborhood regression for fast super-
resolution. In Asian Conference on Computer Vision, pages
111–126. Springer, 2014. 7, 8

[32] Qiang Wang, Xiaoou Tang, and Harry Shum. Patch based
blind image super resolution.
In Computer Vision, 2005.
ICCV 2005. Tenth IEEE International Conference on, vol-
ume 1, pages 709–716. IEEE, 2005. 1

[33] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 606–615, 2018. 2, 4, 5

[34] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
European Conference on Computer Vision, pages 63–79.
Springer, 2018. 1, 2

[35] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004. 6

[36] Chih-Yuan Yang, Chao Ma, and Ming-Hsuan Yang. Single-
image super-resolution: A benchmark. In European Confer-
ence on Computer Vision, pages 372–386. Springer, 2014. 1,
2, 7

[37] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,
Chao Dong, and Liang Lin. Unsupervised image super-
resolution using cycle-in-cycle generative adversarial net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops, pages 701–710,
2018. 2, 3

[38] Roman Zeyde, Michael Elad, and Matan Protter. On sin-
gle image scale-up using sparse-representations. In Interna-
tional conference on curves and surfaces, pages 711–730.
Springer, 2010. 6, 7

1613

