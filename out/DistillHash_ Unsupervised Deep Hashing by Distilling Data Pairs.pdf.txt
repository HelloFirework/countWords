DistillHash: Unsupervised Deep Hashing by Distilling Data Pairs

Erkun Yang1,2, Tongliang Liu2, Cheng Deng1 ∗, Wei Liu3∗, Dacheng Tao2

1 School of Electronic Engineering, Xidian University, Xian 710071, China

2 UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney,

Darlington, NSW 2008, Australia, 3 Tencent AI Lab, Shenzhen, China

ekyang@stu.xidian.edu.cn, tongliang.liu@sydney.edu.au, chdeng.xd@gmail.com,

wl2223@columbia.edu, dacheng.tao@sydney.edu.au

Abstract

Due to the high storage and search efﬁciency, hashing
has become prevalent for large-scale similarity search. Par-
ticularly, deep hashing methods have greatly improved the
search performance under supervised scenarios.
In con-
trast, unsupervised deep hashing models can hardly achieve
satisfactory performance due to the lack of reliable super-
visory similarity signals. To address this issue, we propose
a novel deep unsupervised hashing model, dubbed Distill-
Hash, which can learn a distilled data set consisted of data
pairs, which have conﬁdence similarity signals. Specif-
ically, we investigate the relationship between the initial
noisy similarity signals learned from local structures and
the semantic similarity labels assigned by a Bayes optimal
classiﬁer. We show that under a mild assumption, some
data pairs, of which labels are consistent with those as-
signed by the Bayes optimal classiﬁer, can be potentially
distilled. Inspired by this fact, we design a simple yet effec-
tive strategy to distill data pairs automatically and further
adopt a Bayesian learning framework to learn hash func-
tions from the distilled data set. Extensive experimental re-
sults on three widely used benchmark datasets show that the
proposed DistillHash consistently accomplishes the state-
of-the-art search performance.

1. Introduction

The explosive growth of visual data (e.g., photos and
videos) has led to renewed interest in efﬁcient indexing
and searching algorithms [6, 9, 19, 44, 48, 53, 57, 58, 65–67],
among which, hashing-based approximate nearest neighbor
(ANN) searching, which balances retrieval quality and com-
putational cost, has attracted increasing attention.

Generally, hashing methods can be divided into super-
vised and unsupervised models. The supervised hashing

∗Corresponding author

models [7, 35, 40, 62], which aim to learn hash functions
with semantic labels, have shown remarkable performance.
However, existing supervised hashing methods, especially
deep hashing rely on massive labeled data examples to train
their models. Thus, when there exist no enough training
examples, their performance may be dramatically degraded
caused by over-ﬁtting to those training examples.

To address this challenge, unsupervised hashing meth-
ods usually adopt learning frameworks without requiring
any supervised information. Traditional unsupervised hash-
ing methods [3,21,25,39] with hand-crafted features cannot
well preserve the similarities of real-world data samples due
to the low model capacity and the separated representation
and binary codes optimization processes. To take advan-
tages of the recent progress of deep learning [31,56,68], un-
supervised deep hashing methods [11, 15, 17, 30, 36], which
adopt neural networks as hash functions, have also been
proposed. These deep hashing models are usually trained by
minimizing either the quantization loss or data reconstruc-
tion loss. However, since these objectives fail to exploit the
semantic similarities between data points, they can hardly
achieve satisfactory results.

In this paper, we propose a novel unsupervised deep
hashing model, dubbed DistillHash, which addresses the
absence of supervisory signals by distilling data pairs with
conﬁdent semantic similarity relationships.
In particular,
we ﬁrst exploit the local structure of training data points
to assign an initial similarity label for each data pair.
If
we treat the semantic similarity labels as true labels, these
initial similarity labels then contain label- and instance-
dependent label noise, because many of them fail to rep-
resent semantic similarities. By assuming that we know the
probability of a semantic similarity label given a pair of the
data points, the Bayes optimal classiﬁer will assign the se-
mantic similarity label to the data pair which has a higher
probability (or has a probability greater than 0.5). Based on
these results, we give strict analysis on the relationship be-
tween the noisy labels and the labels assigned by the Bayes

12946

Inspired by the framework of [8], we
optimal classiﬁer.
show that, under a mild assumption, data pairs with con-
ﬁdent semantic labels can be potentially distilled. Further-
more, we theoretically give the criteria to select distilled
data pairs and also provide a simple but effective method to
collect distilled data pairs automatically. Finally, given the
distilled data pair set, we design a deep neural network and
adopt a Bayesian learning framework to perform the repre-
sentation and hash code learning simultaneously.

Our main contributions can be summarized as follows:

• By considering signals learned from deep features as
noisy pairwise labels, we successfully apply noisy la-
bel learning techniques to our method. This shows that
data pairs, of which labels are consistent with those
assigned by the Bayes optimal classiﬁer, can be poten-
tially distilled.

• We theoretically give the criteria to select distilled data
pairs for hash learning and further provide a simple but
effective method to collect distilled data pairs automat-
ically.

• Experiments on three popular benchmark datasets
show that our method can outperform current state-of-
the-art unsupervised hashing methods.

The rest of this paper is organized as follows. We review
the relevant literature in Section 2. We present our novel
DistillHash in Section 3. Section 4 details the experiments,
after which concluding remarks are presented in Section 5.

2. Related Work

Recently, the amount of literatures have grown up con-
siderably around the theme of hashing [12, 13, 32, 33, 42, 42,
43]. According to whether supervised information are in-
volved in the learning phase, existing hashing models can
be divided into two categories: supervised hashing methods
and unsupervised hashing methods.

Supervised hashing methods [5, 14, 22, 35, 40, 49, 55, 61,
64] aim to learn hash functions that can map data points
to Hamming space where the semantic similarity can be
preserved. Kernel-based supervised hashing (KSH) [40]
uses inner products to approximate the Hamming distance
and learns hash functions by perserving semantic similar-
ities in Hamming space. Fast supervised discrete hashing
(FSDH) [22] uses a simple yet effective regression from the
class labels of training data points to the corresponding hash
code to accelerate the learning process. Convolutional neu-
ral networks-based hashing (CNNH) [61] decomposes the
hash function learning into two stages. Firstly, a pairwise
similarity matrix is constructed and decomposed into the
product of approximate hash codes. Secondly, CNNH si-
multaneously learns representations and hash functions by

training the model to predict the learned hash codes as well
as the discrete image class labels. Deep Cauchy hashing
(DCH) [5] adopts Cauchy distribution to continue to opti-
mize data pairs in a relatively small Hamming ball.

Unsupervised hashing methods [3, 21, 25, 39, 41] try to
encode original data into binary codes by training with un-
labeled data points. Iterative quantization (ITQ) [21] ﬁrst
uses principal component analysis (PCA) to map the data
to a low dimensional space and then exploits an alternating
minimization scheme to ﬁnd a rotation matrix, which maps
the data to binary codes with minimum quantization error.
Discrete graph hashing (DGH) [39] casts the graph hashing
problem into a discrete optimization framework and explic-
itly deals with the discrete constraints, so it can directly out-
put binary codes. Spherical hashing (SpH) [25] minimizes
the spherical distance between the original real-valued fea-
tures and the learned binary codes. Anchor graph hashing
(AGH) [41] utilizes anchor graphs to obtain tractable low-
rank adjacency matrices and approximate the data structure.
Though current traditional unsupervised hashing methods
have made great progress, they usually depend on pre-
deﬁned features and cannot simultaneously optimize the
feature and hash code learning processes, thus missing an
opportunity to learn more effective hash codes.

Unsupervised deep hashing methods [11, 17, 30, 36, 37,
52, 63] adopt deep architectures to extract features and per-
form hash mapping. Semantic hashing [52] uses pre-trained
restricted Boltzmann machines (RBMs) [46] to construct an
auto-encoder network, which is then used to generate efﬁ-
cient hash codes and reconstruct the original inputs. Deep
binary descriptors (DeepBit) [36] considers original im-
ages and the corresponding rotated images as similar pairs
and tries to learn hash codes to preserve this similarity.
Stochastic generative hashing (SGH) [11] utilizes a gener-
ative mechanism to learn hash functions through the mini-
mum description length principle. The hash codes are op-
timized to maximally compress the dataset as well as to re-
generate the inputs. Semantic structure-based unsupervised
deep hashing (SSDH) [63] takes advantage of the seman-
tic information in deep features and learns semantic struc-
tures based on the pairwise distances and a Gaussian esti-
mation. The semantic structure is then used to guide the
hash code learning process. By integrating the feature and
hash code learning processes, deep unsupervised hashing
methods usually produce better results.

Training classiﬁers from noisy labels is also a closely re-
lated task. We refer the noisy labels to the setting where
the labels of data points are corrupted [4, 23, 24, 69]. Since
in many situations, it is both expensive and difﬁcult to
obtain reliable labels, a growing body of literature has
been devoted to learning with noisy labels. Those meth-
ods can be organized into two major groups: label noise-
tolerant classiﬁcation [2,45] and label noise cleansing meth-

2947

ods [8, 38, 47, 50]. The former adopts the strategies like de-
cision trees or boosting-based ensemble techniques, while
the latter tries to ﬁlter the label noise by exploiting the prior
information from training samples. For a comprehensive
understanding, we recommend readers to read [20]. By
treating the initial similarity relationship as noisy labels, our
method can explicitly model the relationship between noisy
labels and labels assigned by the Bayes optimal classiﬁers,
which then enables us to extract data pairs with conﬁdent
similarity signals.

3. Approach

Let X = {xi}N

i=1 denote the training set with N in-
stances, deep hashing aims to learn nonlinear hash func-
tions h : x 7→ b ∈ {−1, 1}K , which can encode original
data points x to compact K-bit hash codes.

Traditional supervised deep hashing methods usually
accept data pairs {(xi, xj), Sij} as inputs, where Sij ∈
{+1, −1} is a binary label to indicate whether xi and xj
are similar or not. However, due to the laborious labeling
process and the need of requisite domain knowledge, it’s
not feasible to directly acquire labels in many tasks. Thus,
in this paper, we study the hashing problem under unsuper-
vised settings.

Inspired by the Bayesian classiﬁer theory [18], reliable
labels for data pairs can be conﬁdently assigned by an Bayes
optimal classiﬁer, i.e.,

η(xi, xj) as

˜η(xi, xj) = P ( ˜Sij = +1|xi, xj)
= P ( ˜Sij = +1|xi, xj, Sij = +1)P (Sij = +1|xi, xj)

+ P ( ˜Sij = +1|xi, xj, Sij = −1)P (Sij = −1|xi, xj)

= (1 − ρ+1(xi, xj))η(xi, xj)

+ ρ−1(xi, xj)(1 − η(xi, xj)),

(3)
where ρSij (xi, xj) = P ( ˜Sij = −Sij|xi, xj, Sij) denotes
the ﬂip rate between true labels and noisy labels on given
data pair (xi, xj) and their label Sij . If we know the values
of ρSij (xi, xj) and ˜η(xi, xj), the values of η(xi, xj) can
be easily inferred. However, the values of ρSij (xi, xj) are
unknown. From Eq. 3 we can further get that, when the ﬂip
rates ρ+1(xi, xj) and ρ−1(xi, xj) are relatively small, if
˜η(xi, xj) is large, η(xi, xj) should also be large, and vice
versa. In the following subsection, we show that it is possi-
ble to infer whether η(xi, xj) is smaller or larger than 0.5
based on some weak information1 of ρSij (xi, xj), which
means we may potentially achieve the reliable labels for
some data pairs. We deﬁne those data pairs of which re-
liable labels can be recovered from ˜S as distilled data pairs.
In the following subsection, we theoretically prove that
distilled data pairs can be extracted under a mild assump-
tion. And we further provide a method to collect distilled
data pairs automatically.

3.1. Collecting Distilled Data Pairs Automatically

Sij =(1,

−1,

if η(xi, xj) ≥ 0.5,
if η(xi, xj) < 0.5,

(1)

To collect distilled data pairs, we ﬁrst give the following

assumption.

where η(xi, xj) = P (Sij = +1|xi, xj). This Bayes op-
timal classiﬁer implies that if we have access to η(xi, xj),
we can infer the true data labels with Eq. 1. However, under
unsupervised settings, we cannot access η(xi, xj).

For unsupervised learning, some recent works [34, 51,
63] demonstrate that local structures learned from original
features can help to capture the similarity relationship be-
tween points. Motivated by this, we can roughly label the
training data pairs based on their local structures and con-
struct a similarity matrix ˜S as

˜Sij =(1,

−1,

if d(i, j) ≤ t1,
if d(i, j) > t2,

(2)

where d(i, j) denotes the distance of extracted features for
xi and xj , t1 and t2 are the thresholds for the distance.
However, since ˜S is only constructed from local structures,
they are unreliable and may contain label noise.

Note that, based on ˜S we can learn an estimation of the
conditional probability ˜η(xi, xj) = P ( ˜Sij = +1|xi, xj).
there exists a relationship between ˜η(xi, xj) and
And,

Assumption 1. For any data pairs {(xi, xj), i, j =
1, ...N }, we have

0 ≤ ρ+1(xi, xj) + ρ−1(xi, xj) ≤ 1.

(4)

This assumption implies that label noise is not too heavy.
Note that, if the number of correctly labeled data pairs is
considered larger than that of mislabeled data pairs, the ﬂip
rate ρSij (xi, xj) will be bounded by 0.5. We can see that
Assumption 1 is much weaker than the above assumption.

It is hard to prove that the noisy labels constructed by
exploiting local structures satisfy Assumption 1. However,
the experimental results on three widely used benchmark
datasets empirically verify that the assumption applies well
to the constructed noisy labels. In the rest of this paper, we
always suppose Assumption 1 holds.

We then extend the noisy label learning techniques in [8]
to pairwise labels and present the following key theorem,
which gives the basic criteria to collect distilled data pairs.

1As many of the labels in ˜S are correct, we show later that upper bounds

for ρSij (xi, xj ) can be easily obtained.

2948

Theorem 1. For any data pairs {(xi, xj), i, j = 1, ...N },
we have

Proposition 1. Given the conditional probability ˜η(xi, xj),
the following inequations holds

if ˜η(xi, xj) < 1−ρ+1(xi,xj )

is a distilled data pair;

if ˜η(xi, xj) > 1+ρ−1(xi,xj )

2

2

is a distilled data pair.

, then {(xi, xj), sij = −1}

, then {(xi, xj), sij = +1}

ρ−1(xi, xj) ≤ ˜η(xi, xj),
ρ+1(xi, xj) ≤ 1 − ˜η(xi, xj).

data
Proof. According
{(xi, xj)|η(xi, xj) ≥ 0.5, i, j = 1, ..., N }, we have

to Eq.

any

for

3,

˜η(xi, xj) = (1 − ρ+1(xi, xj))η(xi, xj)

+ ρ−1(xi, xj)(1 − η(xi, xj))

= η(xi, xj)(1 − ρ+1(xi, xj) − ρ−1(xi, xj))

Proof. According to Eq. 3, we can get

pair

˜η(xi, xj) = (1 − ρ+1(xi, xj))η(xi, xj)

+ ρ−1(xi, xj)(1 − η(xi, xj))

= η(xi, xj)(1 − ρ+1(xi, xj) − ρ−1(xi, xj))

+ ρ−1(xi, xj) ≥ ρ−1(xi, xj).

(6)

(7)

+ ρ−1(xi, xj)
1 − ρ+1(xi, xj) + ρ−1(xi, xj)

(5)

The inequality holds because ρ+1(xi, xj) + ρ−1(xi, xj) ≤
1. Similarly, it gives ρ+1(xi, xj) ≤ 1 − ˜η(xi, xj).

≥

≥

2
1 − ρ+1(xi, xj)

2

.

inequality holds since η(xi, xj) ≥ 0.5 and
The ﬁrst
ρ+1(xi, xj) + ρ−1(xi, xj) ≤ 1. Based on Eq. 5, we have

η(xi, xj) ≥ 0.5 ⇒ ˜η(xi, xj) ≥

1 − ρ+1(xi, xj)

2

,

which implies that

˜η(xi, xj) <

1 − ρ+1(xi, xj)

2

⇒ η(xi, xj) < 0.5.

Combining this result with Eq. 1, we can label data pair
(xi, xj) with Sij = −1, if ˜η(xi, xj) < 1−ρ+1(xi,xj )
.
Similarly, we can prove that data pairs (xi, xj) with
˜η(xi, xj) > 1+ρ−1(xi,xj )
can be labeled with Sij =
+1.

2

2

The trade-off for selecting distilled data pairs is the need
of estimating the conditional probability ˜η and the ﬂip rate
ρSij (xi, xj). To estimate ˜η, we adopt a probabilistic classi-
ﬁcation method. Speciﬁcally, we design a deep network to
map data pairs to probabilities. Since this objective is sim-
ilar to the hash code learning, we explore the same archi-
tecture for the estimation of ˜η and hash code learning. The
detailed description of this deep network will be presented
in the next subsection.

For the estimation of the ﬂip rate ρSij (xi, xj), most
existing works [38, 50] assume the noise to be label- and
instance-independent or instance-independent. While in
our method, the ﬂip rate should be label- and instance-
dependent, so most existing methods are not suitable for
the current problem. Considering the difﬁculty to directly
estimate the ﬂip rate, we alternatively propose a method to
obtain an upper bound. Formally, we give the following
proposition.

However, if we directly combine Proposition 1 and The-
orem 1, no distilled data pairs can be selected. So, here we
further assume the ﬂip rate to be local invariant, and thus
obtain the ﬂip rate upper bounds as

ρ−1max(xi, xj)
= min{˜η(xk, xl)|, xk ∈ nno(xi), xl ∈ nno(xj)},
ρ+1max(xi, xj)
= min{(1 − ˜η(xk, xl))|, xk ∈ nno(xi), xl ∈ nno(xj)},
(8)
where nno(xi) indicates the set of top o nearest neighbors
for xi.

With the ﬂip rate upper bounds ρ+1max(xi, xj) and

ρ−1max(xi, xj), we have

1 − ρ+max(xi, xj)

2

1 + ρ−max(xi, xj)

2

≤

≥

1 − ρ+1(xi, xj)

2

1 + ρ−1(xi, xj)

2

(9)

.

The conditional probability ˜η(xi, xj) can be estimated by
the adopted deep networks, and the noisy rate upper bound
can be acquired with Eq. 8. Combining these results with
Eq. 9 and Theorem 1, we can ﬁnd that the distilled data
pairs can be successfully collected by picking out every
pairs (xi, xj) that satisfy ˜η(xi, xj) > 1+ρ−1max(xi,xj )
and
assigning label Sij = +1, and picking out every pairs
(xi, xj) that satisfy ˜η(xi, xj) < 1−ρ+1max(xi,xj )
and as-
signing label Sij = −1. The distilled data pair set can be
represented as {(xi, xj, Sij), i, j = 1, ...m}, where m is
the number of distilled data pairs.

2

2

After obtaining the distilled data pair set, we can perform
hash code learning, which is similar to the learning process
for supervised hashing. Speciﬁcally, we adopt a Bayesian
learning framework, which is elaborated in the following
subsection.

2949

3.2. Bayesian Learning Framework

In this subsection, we propose a Bayesian learning
framework to perform deep hashing learning and also es-
timate the conditional probability ˜η(xi, xj). We ﬁrst intro-
duce the framework for hash code learning and then show
how to apply it for the estimation of ˜η(xi, xj).

By representing the hash codes for distilled data as B =
[b1, ..., bm], the Maximum Likelihood (ML) estimation of
the hash codes can be deﬁned as:

log P (S|B) =

1
m2

m

m

Xi=1

Xj=1

log P (Sij|bi, bj),

(10)

where P (Sij|bi, bj) is the conditional probability of simi-
larity label Sij given the hash codes bi and bj , which can
be naturally approximated by a pairwise logistic function

log P (Sij|bi, bj) =( σ(hbi, bji)

1 − σ(hbi, bji) Sij = −1,

Sij = 1,

(11)

where σ(x) = 1
1+e−x is the sigmoid function and hbi, bji
denotes the inner product of the hash codes bi and bj .
Here, we adopt the inner product, since as indicated in [40],
the Hamming distance distH (·, ·) of hash codes can be in-
ferred from the inner product h·, ·i as: distH (bi, bj) =
1
2 (K − hbi, bji). Hence, the inner product can well reﬂect
the Hamming distance for binary hash codes.

Similar to logistic regression, we can ﬁnd that the smaller
the Hamming distance distH (bi, bj) is, the larger the in-
ner product results hbi, bji and the conditional probability
P (1|bi, bj) will be. Otherwise, the larger the conditional
probability P (−1|bi, bj) will be. These results imply that
similar data points will be enforced to have small Hamming
distance and dissimilar data points will be enforced to have
large Hamming distance, which are expected for Hamming
space similarity search. So, learning with Eq. 10, effective
hash codes can be obtained.

After training the model, given a data point, we can
obtain its hash codes by directly forward propagating it
through the adopted network, and obtain the ﬁnal binary
codes by the following sign function

sign(x) =(1

−1

if x ≥ 0,
if x < 0.

(12)

The whole learning algorithm is summarized in Algo-
rithm 1.

Since this framework maps data pairs to similarity prob-
abilities, we can also use it to estimate the conditional prob-
ability. The main difference lies in that, for hash code learn-
ing we use distilled data pairs as inputs, while for con-
ditional probability estimation, we use the data pairs con-
structed from local structures as inputs.

Algorithm 1: DistillHash

Training Stage

Input: Training images X, code length K, mini-batch
size t, hyper-parameters o and p.
Procedure:
1. Construct initial noisy similarity labels with Eq. (2).
2. Estimate the conditional noisy label probability
˜η(·, ·) for all training data pairs.
3. Estimate the ﬂip rate upper bounds for all training
data pairs with Eq. (8).
4. Distill data pairs with Theorem 1.
repeat

3.1 Randomly sample t data pairs from the
distilled data pair set as inputs.
3.2 Calculate the outputs by forward-propagation
through the adopted networks.
3.3 Update parameters of the network by
minimizing Eq. (10).

until convergence;

Testing Stage

Input: Image query qi, parameters for the adopted
network.
Procedure:
1. Calculate the output of the neural network by
directly forward-propagating the input images.
2. Obtain hash codes with the sign function.

4. Experiments

We evaluate our method on three popular benchmark
datasets, FLICKR25K, NUSWIDE, and CIFAR10, and
provide extensive evaluations to demonstrate its perfor-
mance. In this section, we ﬁrst introduce the datasets and
then present our experimental results.

4.1. Datasets

FLICKR25K [26] contains 25,000 images collected
from the Flickr website. Each image is manually annotated
with at least one of the 24 unique labels provided. We ran-
domly select 2,000 images as a test set; the remaining im-
ages are used as a retrieval set, from which we randomly
select 5,000 images as a training set. NUSWIDE [10] con-
tains 269,648 images, each of the images is annotated with
multiple labels referring to 81 concepts. The subset con-
taining the 10 most popular concepts is used here. We ran-
domly select 5,000 images as a test set; the remaining im-
ages are used as a retrieval set, and 10,500 images are ran-
domly selected from the retrieval set as the training set. CI-
FAR10 [29] is a popular image dataset containing 60,000
images in 10 classes. For each class, we randomly select
1,000 images as queries and 500 as training images, result-

2950

ing in a query set containing 10,000 images and a training
set made up of 5,000 images. All images except for the
query set are used as the retrieval set.

4.2. Baseline Methods

The proposed method is compared with six state-of-
the-art traditional unsupervised hashing methods (LSH [3],
SH [60], ITQ [21], PCAH [59], DSH [28], and SpH [25])
and three recently proposed deep unsupervised hashing
methods (DeeBit [36], SGH [11], and SSDH [63]). All
the codes for these methods have been kindly provided by
the authors. LSH, SH, ITQ, PCAH, DSH, and SpH are
implemented with MATLAB, SGH and SSDH are imple-
mented with TensorFlow [1], and DeepBit is implemented
with Caffe [27]. We use TensorFlow when write our code,
and run the algorithm in a machine with one Titan X Pascal
GPU.

4.3. Evaluation.

To evaluate the performance of our proposed method, we
adopt three evaluation criteria: mean of average precision
(MAP), topN-precision, and precision-recall. The ﬁrst two
criteria are based on Hamming ranking, which ranks data
points based on their Hamming distances to the query; for
its part, precision-recall is based on hash lookup. More de-
tailed introductions are given as follows.

MAP is one of the most widely-used criteria for evaluat-
ing retrieval accuracy. Given a query and a list of R ranked
retrieval results, the average precision (AP) for this query
can be computed. MAP is then deﬁned as the average of
APs for all queries. For all three datasets, we set R as the
number of the retrieval set. TopN-precision is deﬁned as
the average ratio of similar instances among the top N re-
trieved instances for all queries in terms of Hamming dis-
tance. In our experiments, N is set to 1,000. Precision-
recall reveals the precisions at different recall levels and
is a good indicator of overall performance. Typically, the
area under the precision-recall curve is computed. A larger
Precision-recall value always indicates better performance.

4.4. Implementation Details

To initialize the noisy similarity matrix in Eq. (2), we se-
lect the cosine distance as the distance to measure the lo-
cal structure of training examples. The threshold t1 and
t2 are selected as indicated in [63]. For the adopted deep
networks, we use the VGG16 architecture [54] and replace
the last fully-connected layer with a new fully-connected
layer with K units for hash code learning. For the estima-
tion of the conditional probability ˜η, we set the dimensions
of the last fully-connected layer as p, which is 48 in our
experiments. To obtain the upper bound of the ﬂip rate,
we set o as 4. The parameter sensitivity of our algorithm
with regard to o and p are analyzed in Subsection 4.6. Pa-

rameters for the new fully connected layer are learned from
scratch, while parameters for the preceding layers are ﬁne-
tuned from the model pre-trained on ImageNet [16]. We
employ the standard stochastic gradient descent algorithm
with 0.9 momentum for optimization, min-batch size is set
to 64, and the learning rate is ﬁxed to 10−3. Two data points
are considered neighbors if they share the same label (for
CIFAR10) or share at least one common label (for the multi-
label datasets FLICKR25K and NUSWIDE).

For a fair comparison, we adopt

the deep features
extracted from the last fully-connected layer from the
VGG16 network pre-trained on ImageNet for all shallow
architecture-based baseline methods. These deep features
are also used for the construction of ˜S. Since VGG16 ac-
cepts images of size 224 × 224 as inputs, we resize all im-
ages to be 224 × 224 before inputting them into the VGG16
network. Random rotation and ﬂipping are also used for
data augmentation.

4.5. Results and Discussion

We ﬁrst present the MAP values for all methods with
different hash bit lengths, then draw precision-recall and
TopN-precision curves for all methods with 32 and 64 hash
code lengths to give a more comprehensive comparison.

Table 1 presents the MAP results for DistillHash and
all baseline methods on FLICKR25K, NUSWIDE, and CI-
FAR10, with hash code numbers varying from 16 to 128.
By comparing the data-independent method LSH with other
data-dependent methods, we can see that data-dependent
hashing methods outperform the data-independent hashing
method in most cases. This may be because that data-
dependent methods learn hash functions from data, so can
better capture the used data structures. By comparing deep
hashing methods and no-deep hashing methods, we ﬁnd that
no-deep hashing methods can surpass deep hashing meth-
ods DeepBit and SGH in some cases. This may be be-
cause that, without proper supervisory signals, deep hash-
ing methods cannot fully exploit the representation ability
of deep networks, and may achieve unsatisfactory perfor-
mance by over-ﬁtting to bad local minima. While, by ex-
ploiting local structures, deep hashing methods (SSDH and
DistillHash) achieve more promising results.

three datasets.

lengths for all

Concretely, from the MAP results, we can see that Dis-
tillHash consistently obtains the best results across dif-
ferent hash bit
Specif-
ically, compared to one of the best non-deep hashing
methods, i.e, ITQ, we achieve absolute improvements of
6.89%, 13.97%, and 7.73% in the average MAP for dif-
ferent bits on FLICKR25K, NUSWIDE, and CIFAR10 re-
spectively. Compared to the state-of-the-art deep hash-
ing method SSDH, we achieve absolute improvements of
3.08%, 4.01%, and 2.86% in average MAP for different
bits on the three datasets respectively. Note that DeepBit,

2951

Table 1. Comparison with baselines in terms of MAP. The best accuracy is shown in boldface.

FLICKR25K

NUSWIDE

CIFAR10

16 bits

32 bits

64 bits

128 bits

16 bits

32 bits

64 bits

128 bits

16 bits

32 bits

64 bits

128 bits

0.5831
0.5919
0.6192
0.6091
0.6074
0.6108
0.5934
0.6162
0.6621
0.6964

0.5885
0.5923
0.6318
0.6105
0.6121
0.6029
0.5933
0.6283
0.6733
0.7056

0.5933
0.6016
0.6346
0.6033
0.6118
0.6339
0.6199
0.6253
0.6732
0.7075

0.6014
0.6213
0.6477
0.6071
0.6154
0.6251
0.6349
0.6206
0.6771
0.6995

0.4324
0.4458
0.5283
0.4625
0.5200
0.4532
0.4542
0.4936
0.6231
0.6667

0.4411
0.4537
0.5323
0.4531
0.5227
0.4597
0.4625
0.4829
0.6294
0.6752

0.4433
0.4926
0.5319
0.4635
0.5345
0.4958
0.47616
0.4865
0.6321
0.6769

0.4816
0.5000
0.5424
0.4923
0.5370
0.5127
0.4923
0.4975
0.6485
0.6747

0.1319
0.1605
0.1942
0.1432
0.1616
0.1439
0.2204
0.1795
0.2568
0.2844

0.1580
0.1583
0.2086
0.1589
0.1876
0.1665
0.2410
0.1827
0.2560
0.2853

0.1673
0.1509
0.2151
0.1730
0.1918
0.1783
0.2521
0.1889
0.2587
0.2867

0.1794
0.1538
0.2188
0.1835
0.2055
0.1840
0.2530
0.1904
0.2601
0.2895

method

LSH [3]
SH [60]
ITQ [21]

PCAH [59]
DSH [28]
SpH [25]

DeepBit [36]

SGH [11]
SSDH [63]
DistillHash

(a) TopN-precision with 16bits

(b) TopN-precision with 32bits

(c) Precision-recall with 16bits

(d) Precision-recall with 32bits

Figure 1. TopN-precision and precision-recall curves on FLICKR25K with 16 and 32 hash bits.

(a) TopN-precision with 16bits

(b) TopN-precision with 32bits

(c) Precision-recall with 16bits

(d) Precision-recall with 32bits

Figure 2. TopN-precision and precision-recall curves on NUSWIDE with 16 and 32 hash bits.

(a) TopN-precision with 16bits

(b) TopN-precision with 32bits

(c) Precision-recall with 16bits

(d) Precision-recall with 32bits

Figure 3. TopN-precision and precision-recall curves on CIFAR10 with 16 and 32 hash bits.

SGH, SSDH, and DistillHash are both deep hashing meth-
ods, only SSDH and DistillHash can exploit and preserve
the similarity of different data points, thus they can achieve
better performance than the other two. Moreover, Distill-
Hash learns more accurate similarity relationships by dis-
tilling some data pairs, so can obtain a further performance
improvement than SSDH.

The left two subﬁgures of Figure 1, 2, and 3 present
the TopN-precision curves for all methods on each of the
three datasets with hash bit lengths of 16 and 32. Consistent
with MAP results, we can observe that DistillHash achieves
the best results among all approaches. Since MAP val-
ues and TopN-precision curves are both Hamming ranking-
based metrics, an overview of the above analysis reveals

2952

02004006008001000Number of top ranked samples0.600.650.700.750.80PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash02004006008001000Number of top ranked samples0.650.700.750.800.85PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash0.00.20.40.60.81.0Recall0.60.70.80.9PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash0.00.20.40.60.81.0Recall0.60.70.80.91.0PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash02004006008001000Number of top ranked samples0.450.500.550.600.650.700.75PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash02004006008001000Number of top ranked samples0.500.550.600.650.700.750.80PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash0.00.20.40.60.81.0Recall0.40.50.60.70.80.9PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash0.00.20.40.60.81.0Recall0.40.50.60.70.80.91.0PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash02004006008001000Number of top ranked samples0.200.250.300.35PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash02004006008001000Number of top ranked samples0.250.300.350.400.45PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash0.00.20.40.60.81.0Recall0.10.20.30.40.5PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHash0.00.20.40.60.81.0Recall0.20.40.60.8PrecisionLSHSHITQPCAHDSHSpHDeepBitSGHSSDHDistillHashTable 2. MAP results for DistillHash* and DistillHash. The best accuracy is shown in boldface.

FLICKR25K

NUSWIDE

CIFAR10

method

16 bits

32 bits

64 bits

128 bits

16 bits

32 bits

64 bits

128 bits

16 bits

32 bits

64 bits

128 bits

DistillHash*
DistillHash

0.6653
0.6964

0.6633
0.7056

0.6726
0.7075

0.6784
0.6995

0.6322
0.6667

0.6357
0.6752

0.6480
0.6769

0.6451
0.6747

0.2547
0.2844

0.2538
0.2853

0.2573
0.2867

0.2583
0.2895

(a) FLICKR25K

(b) NUSWIDE

(c) CIFAR10

Figure 4. Losses of DistillHash through the training process.

that DistillHash can achieve superior performance for Ham-
ming ranking-based evaluations. Moreover, to illustrate the
hash lookup results, we plot the precision-recall curves for
all methods with hash bit lengths of 16 and 32 in the right
two subﬁgures of Figure 1, 2, and 3. From the results, we
can again observe that DistillHash consistently achieves the
best performance, which further demonstrates the superior-
ity of our proposed method.

To investigate the change of loss values through the train-
ing process, we display the loss values in Figure 4. The
results reveal that our methods can converge in all cases
within 1,000 iterations.

(a) MAP w.r.t. different o.

(b) MAP w.r.t. different p.

Figure 5. Parameter sensitive analysis for o and p on NUSWIDE.

for the NUSWIDE dataset, and 2.97%, 3.15%, 2.94% and
3.12% for the CIFAR10 dataset at hash bit lengths of 16, 32,
64, and 128 respectively. Note that the only difference be-
tween DistillHash and DistillHash* lies in that DistillHash
is trained with distilled data set and DistillHash* is trained
with the initial data set. The performance improvements
clearly demonstrate the superiority of the proposed distilled
data pair learning.

4.6. Parameter Sensitivity

5. Conclusions

We next investigate the sensitivity of hyper-parameters
o and p. Figure 5 shows the effect of these two hyper-
parameters on NUSWIDE dataset with hash code lengths
of 16, 32, 64, and 128. We ﬁrst ﬁx p to 48 and evaluate the
MAP by varying o between 2 and 20, the results are pre-
sented in Figure 5(a). The performance shows that the algo-
rithm is not sensitive to parameter o in the range of [2, 20],
and we can set o as any number in the range of [2, 20]. In
our experiments, we set o as 4. Figure 5(b) shows the MAP
by varying p between 16 and 128 with o ﬁxed to 4. The
performance of DistillHash ﬁrst increases and then keeps at
a relatively high level. The result is also not sensitive to p in
the range of [32, 128] . For other experiments in this paper,
we select p as 48.

This work presented a new unsupervised deep hash-
ing approach for image search, namely DstilHash. Firstly,
we theoretically investigated the relationship between the
Bayes optimal classiﬁer and noisy labels learned from local
structures, showing that distilled data pairs can be poten-
tially collected. Secondly, with the above understanding,
we provided a simple yet effective scheme to automatically
distill data pairs. Thirdly, leveraging a distilled data set,
we designed a deep hashing model and adopted a Bayesian
learning framework to perform the hash code learning. The
experimental results on three benchmark datasets demon-
strated that the proposed DistillHash surpasses other state-
of-the-art methods.

4.7. Ablation Study

6. Acknowledgements

In this subsection, we go deeper to study the efﬁcacy
of the proposed distilled data pair learning. More speciﬁ-
cally, we investigate DistillHash*, a variant of DistillHash
with the same Bayesian learning framework but trained with
the initial similarity label ˜S. The MAP results of Distill-
Hash* and DistillHash are shown in Table 2, from which we
can see that DistillHash consistently outperforms Distill-
Hash* by margins of 3.11%, 4.23%, 3.49% and 2.11% for
the FLICKR25K dataset, 3.45%, 3.95%, 2.89% and 2.96%

This work was also supported in part by the Na-
tional Natural Science Foundation of China under Grant
61572388 and 61703327,
in part by the Key R&D
Program-The Key Industry Innovation Chain of Shaanxi
under Grant 2017ZDCXL-GY-05-04-02, 2017ZDCXL-
GY-05-02 and 2018ZDXM-GY-176,
in part by the
National Key R&D Program of China under Grant
2017YFE0104100, and in part by the Australian Research
Council Projects DP-180103424, DE-1901014738, and FL-
170100117.

2953

02505007501000Iterations0.00.20.40.6Loss16bits32bits64bits128bits02505007501000Iterations0.20.40.6Loss16bits32bits64bits128bits02505007501000Iterations0.00.20.40.6Loss16bits32bits64bits128bits5101520o0.560.580.600.620.640.660.680.70MAP16bits32bits64bits128bits255075100125p0.560.580.600.620.640.660.680.70MAP16bits32bits64bits128bitsReferences

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning.
In OSDI, vol-
ume 16, pages 265–283, 2016. 6

[2] Kamal M Ali and Michael J Pazzani.

Error reduction
through learning multiple descriptions. Machine Learning,
24(3):173–202, 1996. 2

[3] Alexandr Andoni and Piotr Indyk. Near-optimal hashing al-
gorithms for approximate nearest neighbor in high dimen-
sions. In FOCS, pages 459–468, 2006. 1, 2, 6, 7

[4] Battista Biggio, Blaine Nelson, and Pavel Laskov. Support
vector machines under adversarial label noise. In Asian Con-
ference on Machine Learning, pages 97–112, 2011. 2

[5] Yue Cao, Mingsheng Long, Bin Liu, Jianmin Wang, and
MOE KLiss. Deep cauchy hashing for hamming space re-
trieval. In CVPR, pages 1229–1237, 2018. 2

[6] Xinyuan Chen, Chang Xu, Xiaokang Yang, and Dacheng
Tao. Attention-gan for object transﬁguration in wild images.
In ECCV, pages 164–180, 2018. 1

[7] Zhixiang Chen, Xin Yuan, Jiwen Lu, Qi Tian, and Jie Zhou.
Deep hashing via discrepancy minimization. In CVPR, June
2018. 1

[8] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamoha-
Learning with bounded
arXiv preprint

narao,
instance-and label-dependent label noise.
arXiv:1709.03768, 2017. 2, 3

and Dacheng Tao.

[9] Lianhua Chi, Bin Li, Xingquan Zhu, Shirui Pan, and
Ling Chen. Hashing for adaptive real-time graph stream
classiﬁcation with concept drifts.
IEEE Trans. Cybern.,
48(5):1591–1604, 2018. 1

[10] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip-
ing Luo, and Yantao Zheng. Nus-wide: a real-world web im-
age database from national university of singapore. In CIVR,
page 48, 2009. 5

[11] Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, and Le Song.

Stochastic generative hashing. In ICML, 2017. 1, 2, 6, 7

[12] Cheng Deng, Zhaojia Chen, Xianglong Liu, Xinbo Gao, and
Dacheng Tao. Triplet-based deep hashing network for cross-
modal retrieval. IEEE Trans. Image Process., 27(8):3893–
3903, 2018. 2

[13] Cheng Deng, Huiru Deng, Xianglong Liu, and Yuan Yuan.
Adaptive multi-bit quantization for hashing. Neurocomput-
ing, 151:319–326, 2015. 2

[14] Cheng Deng, Xu Tang, Junchi Yan, Wei Liu, and Xinbo Gao.
Discriminative dictionary learning with common label align-
ment for cross-modal retrieval.
IEEE Trans. Multimedia,
18(2):208–218, 2016. 2

[15] Cheng Deng, Erkun Yang, Tongliang Liu, Wei Liu, Jie Li,
and Dacheng Tao. Unsupervised semantic-preserving adver-
sarial hashing for image search. IEEE Transactions on Image
Processing, 2019. 1

[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248–255. Ieee, 2009. 6

[17] Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi
Nourabadi, Yanhua Yang, Cheng Deng, and Heng Huang.
Unsupervised deep generative adversarial hashing network.
In CVPR, 2018. 1, 2

[18] Pedro Domingos and Michael Pazzani. On the optimality of
the simple bayesian classiﬁer under zero-one loss. Machine
learning, 29(2-3):103–130, 1997. 3

[19] Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng
Tao. The expressive power of parameterized quantum cir-
cuits. arXiv preprint arXiv:1810.11922, 2018. 1

[20] Benoˆıt Fr´enay and Michel Verleysen. Classiﬁcation in the
presence of label noise: a survey. IEEE Trans. Neural Netw.
Learn. Syst., 25(5):845–869, 2014. 3

[21] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and
Iterative quantization: A procrustean
large-scale im-
IEEE Trans. Pattern Anal. Mach. Intell.,

Florent Perronnin.
approach to learning binary codes for
age retrieval.
35(12):2916–2929, 2013. 1, 2, 6, 7

[22] Jie Gui, Tongliang Liu, Zhenan Sun, Dacheng Tao, and Tie-
niu Tan. Fast supervised discrete hashing. IEEE Trans. Pat-
tern Anal. Mach. Intell., 40(2):490–496, 2018. 2

[23] Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor
Tsang, Ya Zhang, and Masashi Sugiyama. Masking: A new
perspective of noisy supervision. In Advances in Neural In-
formation Processing Systems, pages 5841–5851, 2018. 2

[24] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao
Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-
teaching: Robust training of deep neural networks with ex-
tremely noisy labels.
In Advances in Neural Information
Processing Systems, pages 8536–8546, 2018. 2

[25] Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu Chang,
In CVPR, pages

and Sung-Eui Yoon. Spherical hashing.
2957–2964, 2012. 1, 2, 6, 7

[26] Mark J. Huiskes and Michael S. Lew. The mir ﬂickr retrieval

evaluation. In ICMR, 2008. 5

[27] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding.
In ACM Multimedia, pages 675–678,
2014. 6

[28] Zhongming Jin, Cheng Li, Yue Lin, and Deng Cai. Density
sensitive hashing. IEEE Trans. Cybern., 44(8):1362–1371,
2014. 6, 7

[29] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, 2009.
5

[30] Alex Krizhevsky and Geoffrey E Hinton. Using very deep
autoencoders for content-based image retrieval. In ESANN,
2011. 1, 2

[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1097–1105, 2012. 1

[32] Chao Li, Cheng Deng, Ning Li, Wei Liu, Xinbo Gao, and
Dacheng Tao. Self-supervised adversarial hashing networks
for cross-modal retrieval. In CVPR, pages 4242–4251, 2018.
2

2954

[33] Ning Li, Chao Li, Cheng Deng, Xianglong Liu, and Xinbo
In IJCAI,

Gao. Deep joint semantic-embedding hashing.
pages 2397–2403, 2018. 2

[51] Pedro O Pinheiro and AI Element. Unsupervised domain
adaptation with similarity learning. In CVPR, pages 8004–
8013, 2018. 3

[34] Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi,
Qin Huang, and C-C Jay Kuo. Instance embedding transfer
to unsupervised video object segmentation. In CVPR, pages
6526–6535, 2018. 3

[35] Wu-Jun Li, Sheng Wang, and Wang-Cheng Kang. Feature
learning based deep supervised hashing with pairwise labels.
In IJCAI, pages 1711–1717, 2016. 1, 2

[36] Kevin Lin, Jiwen Lu, Chu-Song Chen, and Jie Zhou. Learn-
ing compact binary descriptors with unsupervised deep neu-
ral networks.
In CVPR, pages 1183–1192, 2016. 1, 2, 6,
7

[37] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, Jie
Zhou, et al. Deep hashing for compact binary codes learning.
In CVPR, volume 1, page 3, 2015. 2

[38] Tongliang Liu and Dacheng Tao. Classiﬁcation with noisy
labels by importance reweighting. IEEE Trans. Pattern Anal.
Mach. Intell., 38(3):447–461, 2016. 3, 4

[39] Wei Liu, Cun Mu, Sanjiv Kumar, and Shih-Fu Chang. Dis-
crete graph hashing. In NIPS, pages 3419–3427, 2014. 1,
2

[40] W. Liu, J. Wang, R. Ji, Y. Jiang, and S.-F. Chang. Supervised
hashing with kernels. In CVPR, pages 2074–2081, 2012. 1,
2, 5

[41] Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang.

Hashing with graphs. In ICML, pages 1–8, 2011. 2

[42] Xianglong Liu, Cheng Deng, Bo Lang, Dacheng Tao, and
Xuelong Li. Query-adaptive reciprocal hash tables for near-
est neighbor search. IEEE Trans. Image Process., 25(2):907–
919, 2016. 2

[43] Xianglong Liu, Bowen Du, Cheng Deng, Ming Liu, and
Bo Lang. Structure sensitive hashing with adaptive prod-
uct quantization. IEEE Trans. Cybern., 46(10):2252–2264,
2016. 2

[44] Yu Liu, Jingkuan Song, Ke Zhou, Lingyu Yan, Li Liu, Fuhao
Zou, and Ling Shao. Deep self-taught hashing for image
retrieval. IEEE Trans. Cybern., 2018. 1

[45] Prem Melville, Nishit Shah, Lilyana Mihalkova, and Ray-
mond J Mooney. Experiments on ensembles with missing
and noisy data. In MCS Workshop, pages 293–302, 2004. 2
[46] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In ICML, pages 807–
814, 2010. 2

[47] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Raviku-
mar, and Ambuj Tewari. Learning with noisy labels. In NIPS,
pages 1196–1204. 2013. 3

[48] Wing WY Ng, Xing Tian, Witold Pedrycz, Xizhao Wang,
and Daniel S Yeung. Incremental hash-bit learning for se-
mantic image retrieval in nonstationary environments. IEEE
Trans. Cybern., (99), 2018. 1

[49] M. Norouzi and D. M Blei. Minimal loss hashing for com-

pact binary codes. In ICML, pages 353–360, 2011. 2

[52] Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hash-

ing. Int. J. Approx. Reasoning., 50(7):969–978, 2009. 2

[53] Yuming Shen, Li Liu, Fumin Shen, and Ling Shao. Zero-shot
sketch-image hashing. In CVPR, pages 3598–3607, 2018. 1
[54] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 6

[55] Jingkuan Song, Yi Yang, Xuelong Li, Zi Huang, and Yang
Yang. Robust hashing with local models for approximate
similarity search.
IEEE Trans. Cybern., 44(7):1225–1236,
2014. 2

[56] Chaoyue Wang, Chang Xu, Xin Yao, and Dacheng Tao. Evo-
lutionary generative adversarial networks. IEEE Trans. Evol.
Comput., 2019. 1

[57] Hao Wang, Yanhua Yang, Erkun Yang, and Cheng Deng. Ex-
ploring hybrid spatio-temporal convolutional networks for
human action recognition. Multimedia Tools and Applica-
tions, 76(13):15065–15081, 2017. 1

[58] Shengnan Wang, Chunguang Li, and Hui-Liang Shen. Dis-
IEEE Trans. Cybern., (99):1–13,

tributed graph hashing.
2018. 1

[59] Xin-Jing Wang, Lei Zhang, Feng Jing, and Wei-Ying Ma.
In CVPR,

Annosearch: Image auto-annotation by search.
volume 2, pages 1483–1490, 2006. 6, 7

[60] Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral

hashing. In NIPS, pages 1753–1760, 2009. 6, 7

[61] Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu, and
Shuicheng Yan. Supervised hashing for image retrieval via
image representation learning. In AAAI, volume 1, page 2,
2014. 2

[62] Erkun Yang, Cheng Deng, Chao Li, Wei Liu, Jie Li, and
Dacheng Tao. Shared predictive cross-modal deep quantiza-
tion. IEEE Trans. Neural Netw. Learn. Syst., 2018. 1

[63] Erkun Yang, Cheng Deng, Tongliang Liu, Wei Liu, and
Dacheng Tao. Semantic structure-based unsupervised deep
hashing. In IJCAI, pages 1064–1070, 2018. 2, 3, 6, 7

[64] Erkun Yang, Cheng Deng, Wei Liu, Xianglong Liu, Dacheng
Tao, and Xinbo Gao. Pairwise relationship guided deep hash-
ing for cross-modal retrieval.
In AAAI, pages 1618–1625,
2017. 2

[65] Erkun Yang, Tongliang Liu, Cheng Deng, and Dacheng Tao.
IEEE

Adversarial examples for hamming space search.
Trans. Cybern., 2018. 1

[66] Xu Yang, Cheng Deng, Xianglong Liu, and Feiping Nie.
New l2, 1-norm relaxation of multi-way graph cut for clus-
tering. In AAAI, pages 4374–4381, 2018. 1

[67] Shan You, Chang Xu, Yunhe Wang, Chao Xu, and Dacheng
Tao. Privileged multi-label learning. In IJCAI, pages 3336–
3342, 2017. 1

[68] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning
In ACM SIGKDD, pages

from multiple teacher networks.
1285–1294. ACM, 2017. 1

[50] Curtis G Northcutt, Tailin Wu, and Isaac L Chuang. Learning
with conﬁdent examples: Rank pruning for robust classiﬁca-
tion with noisy labels. In UAI, 2017. 3, 4

[69] Xiyu Yu, Tongliang Liu, Mingming Gong, Kun Zhang, and
Dacheng Tao. Transfer learning with label noise. arXiv
preprint arXiv:1707.09724, 2017. 2

2955

