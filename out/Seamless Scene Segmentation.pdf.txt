Seamless Scene Segmentation

Lorenzo Porzi, Samuel Rota Bul`o, Aleksander Colovic, Peter Kontschieder

Mapillary Research

research@mapillary.com

Abstract

In this work we introduce a novel, CNN-based archi-
tecture that can be trained end-to-end to deliver seam-
less scene segmentation results. Our goal is to predict
consistent semantic segmentation and detection results by
means of a panoptic output format, going beyond the sim-
ple combination of independently trained segmentation and
detection models. The proposed architecture takes advan-
tage of a novel segmentation head that seamlessly inte-
grates multi-scale features generated by a Feature Pyramid
Network with contextual information conveyed by a light-
weight DeepLab-like module. As additional contribution we
review the panoptic metric and propose an alternative that
overcomes its limitations when evaluating non-instance cat-
egories. Our proposed network architecture yields state-of-
the-art results on three challenging street-level datasets, i.e.
Cityscapes, Indian Driving Dataset and Mapillary Vistas.

1. Introduction

Scene understanding is one of the grand goals for au-
tomated perception that requires advanced visual compre-
hension of tasks like semantic segmentation (Which seman-
tic category does a pixel belong to?)
and detection or
instance-speciﬁc semantic segmentation (Which individual
object segmentation mask does a pixel belong to?). Solv-
ing these tasks has large impact on a number of applica-
tions, including autonomous driving or augmented reality.
Interestingly, and despite sharing some obvious commonali-
ties, both these segmentation tasks have been predominantly
handled in a disjoint way ever since the rise of deep learn-
ing, while earlier works [49, 50, 56] already approached
them in a joint manner. Instead, independent trainings of
models, with separate evaluations using corresponding per-
formance metrics, and ﬁnal fusion in a post-processing step
based on task-speciﬁc heuristics have seen a revival.

The work in [24] introduces a so-called panoptic eval-
uation metric for joint assessment of semantic segmenta-
tion of stuff and instance-speciﬁc thing object categories,
to encourage further research on this topic. Stuff is deﬁned
as non-countable, amorphous regions of similar texture or
material while things are enumerable, and have a deﬁned
shape. Few works have started adopting the panoptic metric

in their methodology yet, but reported results remain sig-
niﬁcantly below the ones obtained from fused, individual
models. All winning entries on designated panoptic Seg-
mentation challenges like e.g. the Joint COCO and Mapil-
lary Recognition Workshop 20181, were based on combina-
tions of individual (pre-trained) segmentation and instance
segmentation models, rather than introducing streamlined
integrations that can be successfully trained from scratch.

The use of separate models for semantic segmentation
and detection obviously comes with the disadvantage of
signiﬁcant computational overhead. Furthermore, and due
to a lack of cross-pollination of models, there is no way
of enforcing labeling consistency between individual mod-
els. Moreover, we argue that individual models supposedly
spend signiﬁcant amounts of their capacity on modeling re-
dundant information, whereas sensible architectural choices
in a joint setting are leading to favorable or on par results,
but at much reduced computational costs.

In this work we introduce a novel, deep convolutional
neural network based architecture for seamless scene seg-
mentation. Our proposed network design aims at jointly
addressing the tasks of semantic segmentation and instance
segmentation. We present ideas for interleaving information
from segmentation and instance-segmentation modules and
discuss model modiﬁcations over vanilla combinations of
standard segmentation and detection building blocks. With
our ﬁndings, we are able to train high-quality, seamless
scene segmentation models without the need of pre-trained
recognition models. As result, we obtain a state-of-the-
art, single model that jointly produces semantic segmen-
tation and instance segmentation results, at a fraction of
the computational cost required when combining indepen-
dently trained recognition models.

We provide the following contributions in our work:
• Streamlined architecture based on a single network
backbone to generate complete semantic scene seg-
mentation for stuff and thing classes

• A novel segmentation head integrating multi-scale fea-
tures from Feature Pyramid Network, with contex-
tual information provided by a light-weight, DeepLab-
inspired module

1http://cocodataset.org/workshop/coco-

mapillary-eccv-2018.html

18277

• Re-evaluation of the panoptic segmentation metric and
reﬁnement for more adequate handling of stuff classes
• Comparisons of the proposed architecture against in-
dividually trained and fused segmentation models, in-
cluding analyses of model parameters and computa-
tional requirements

• Experimental results on challenging driving scene
Indian Driving
datasets
Dataset [51], and Mapillary Vistas [39], demon-
strating state-of-the-art performance.

like Cityscapes

[10],

2. Related Works

Semantic segmentation is a long-standing problem in
computer vision research [3, 26, 27, 48] that has signiﬁ-
cantly improved over the past ﬁve years, thanks in great part
to advances in deep learning. The works in [2, 38] have in-
troduced encoder/decoder CNN architectures for providing
dense, pixel-wise predictions by taking e.g. a fully convo-
lutional approach. The more recent DeepLab [5] exploits
multi-scale features via parallel ﬁlters from convolutions
with different dilation factors, together with globally pooled
features. Another recent Deeplab extension [9] integrates a
decoder module for reﬁning object boundary segmentation
results. In [8], a meta-learning technique for dense predic-
tion tasks is introduced, that learns how to design a decoder
for semantic segmentation. The pyramid scene parsing net-
work [59] employs i) a pyramidal pooling module to cap-
ture sub-region representations at different scales, followed
by upsampling and stacking with respective input features
and ii) an auxiliary loss applied after the conv4 block of
a ResNet-101 backbone. The works in [57, 58] propose
aggregation of multi-scale contextual information using di-
lated convolutions, which have proven to be particularly
effective for dense prediction tasks, and are a generaliza-
tion of the conventional convolution operator to expand its
receptive ﬁeld. ReﬁneNet [32] proposes a multi-path re-
ﬁnement network to exploit multiple abstraction levels of
features for enhancing the segmentation quality of high-
resolution images. Other works like [45, 53] are address-
ing the problem of class sample imbalance by introducing
loss-guided, pixel-wise gradient reweighting schemes.

Instance-speciﬁc semantic segmentation has recently
gained large attention in the ﬁeld, with early, random ﬁeld-
based works in [21, 49].
In [17] a simultaneous detec-
tion and segmentation algorithm is developed that classi-
ﬁes and reﬁnes CNN features obtained from regions under
R-CNN [16] bounding box proposals. The work in [18] em-
phasizes on reﬁning object boundaries for binary segmenta-
tion masks initially generated from bounding box propos-
als. In [12] a multi-task network cascade is introduced that,
beyond sharing features from the encoder in all following
tasks, subsequently adds blocks for i) bounding box gen-

eration, ii) instance mask generation and iii) mask catego-
rization. Another approach [11] introduces instance fully
convolutional networks that assemble segmentations from
position-sensitive score maps, generated by classifying pix-
els based on their relative positions. The follow-up work
in [31] builds upon Faster R-CNN [43] for proposal gen-
eration and additionally includes position-sensitive outside
score maps.
InstanceCUT [25] obtains instance segmen-
tations by solving a Multi-Cut problem, taking instance-
agnostic semantic segmentation masks and instance-aware,
probabilistic boundary masks as inputs, provided by a CNN.
The work in [1] also introduces an approach where an in-
stance CRF provides individual instance masks based on
exploiting box, global and shape cues as unary poten-
tials, together with instance-agnostic semantic information.
In [36], sequential grouping networks are presented that
run a sequence of simple networks for solving increas-
ingly complex grouping problems, eventually yielding in-
stance segmentation masks. DeepMask [40] ﬁrst produces
an instance-agnostic segmentation mask for an input patch,
which is then assigned to a score corresponding to how
likely this patch it to contain an object. At inference, their
approach generates a set of ranked segmentation propos-
als. The follow-up work SharpMask [41] augments the
networks with a top-down reﬁnement approach. Mask R-
CNN [19] forms the basis of current state-of-the-art in-
stance segmentation approaches. It is a conceptually simple
extension of Faster R-CNN, adding a dedicated branch for
object mask segmentation in parallel to the existing ones for
bounding box regression and classiﬁcation. Due to its im-
portance in our work, we provide a more thorough review in
the next section. The work in [37] proposes to improve lo-
calization quality of objects in Mask R-CNN via integration
of multi-scale information as bottom-up path augmentation.

Joint
segmentation and instance-segmentation ap-
proaches date back to [50], introducing a Bayesian ap-
proach for scene representation by establishing a scene
parsing graph to explain both, segmentation of stuff and
things. Other works before the era of deep learning of-
ten built upon CRFs where [49] alternatingly reﬁned pixel
labelings and object instance predictions, and [56] framed
holistic scene understanding as a structure prediction prob-
lem in a graphical model, deﬁned over hierarchies of re-
gions, scene types, etc. The recently proposed work in [22]
addresses automated loss balancing in a multi-task learning
problem based on analysing the homoscedastic uncertainty
of each task. Even though their work addresses three tasks
at the same time (semantic segmentation, instance segmen-
tation and depth estimation), it fails to demonstrate consis-
tent improvements over semantic segmentation and instance
segmentation alone and lacks of comparisons to comparable
baselines. The supervised variant in [30] generates panop-
tic segmentation results but i) requires separate (external)

8278

input for bounding box proposals and ii) exploits a condi-
tional random ﬁeld during inference, increasing the com-
plexity of the model. The work in [14] attempts to introduce
a uniﬁed architecture related to our ideas, however, the re-
ported results remain signiﬁcantly below those of reported
state-of-the-art methods. Independently and simultaneously
to our paper, a number of works [23, 29, 35, 54, 55] have
proposed panoptic segmentation provided by a single deep
network, conﬁrming the importance of this task to the ﬁeld.
While comparable in complexity and architecture, we ob-
tain improved performance on challenging street-level im-
age datasets like Cityscapes and Mapillary Vistas.

3. Proposed Architecture

The proposed architecture consists of a backbone work-
ing as feature extractor and two task-speciﬁc branches ad-
dressing semantic segmentation and instance segmentation,
respectively. Hereafter, we provide details about each com-
ponent and refer to Fig. 1 for an overview.

3.1. Shared Backbone

The backbone that we use throughout this paper is a
slightly modiﬁed ResNet-50 [20] with a Feature Pyramid
Network (FPN) [33] on top. The FPN network is linked to
the output of the modules conv2, conv3, conv4 and conv5
of ResNet-50, which yield different downsampling factor,
namely ×4, ×8, ×16 and ×32, respectively. Akin to the
original FPN architecture, we have a variable number of
additional lower resolution scales covering downsampling
factors of ×64 and ×128, depending on the dataset. The
main modiﬁcation in ResNet-50 is the replacement of all
Batch Normalization (BN) + ReLU layers with synchro-
nized Inplace Activated Batch Normalization (iABNsync)
proposed in [46], which uses LeakyReLU with slope 0.01
as activation function due to the need of invertible activation
functions. This modiﬁcation gives two important advan-
tages: we gain up to 50% additional GPU memory since the
layer performs in-place operations, and the synchronization
across GPUs ensures a better estimate of the gradients in
multi-GPU trainings with positive effects on convergence.

3.2. Instance Segmentation Branch

The instance segmentation branch follows the state-of-
the-art Mask R-CNN [19] architecture, but with some mod-
iﬁcations described next. This branch is structured into a
region proposal head and a region segmentation head.

Region Proposal Head (RPH). The RPH introduces the
notion of an anchor. An anchor is a reference bounding
box (a.k.a. region), centered on one of the available spatial
locations of the RPH’s input and having pre-deﬁned dimen-
sions. The set of pre-deﬁned dimensions is chosen in ad-
vance, depending on the dataset and the scale of the FPN

output (see details in Sec. 5). We denote by A all anchors
that can be constructed by combining a position on an avail-
able, spatial location and a dimension from the pre-deﬁned
set, and which are entirely contained in the image. Given
an anchor a we denote its position (in the image coordi-
nate system) by (ua, va) and its dimensions by (wa, ha).
The role of RPH is to apply a transformation to each an-
chor in order to obtain a new bounding box proposal to-
gether with an objectness score, that assesses the validity
of the region. To this end, RPH applies a 3 × 3 convolu-
tion with 256 output channels and stride 2 to the outputs
of the backbone, followed by iABNsync, and a 1 × 1 con-
volution with 5Nanchors channels, which provide a bounding
box proposal with an objectness score for each anchor in
A. In more details, for each anchor a ∈ A the transformed
bounding box has center (ˆu, ˆv) = (ua + ouwa, va + ovha),
dimensions ( ˆw, ˆh) = (wa eow , ha eoh ) and objectness score
ˆs = σ(os), where (ou, ov, ow, oh, os) represents the output
from the 1 × 1 convolution for anchor a, and σ(·) is the sig-
moid function. The resulting set of bounding boxes are then
fed to the region segmentation head, with distinct ﬁltering
steps for training and test time.

Region Segmentation Head (RSH). Each region proposal
ˆr = (ˆu, ˆv, ˆw, ˆh) obtained from RPH is fed to RSH, which
applies ROIAlign [19], pooling features directly from the
kth output of the backbone within region ˆr with a 14 × 14
spatial resolution, where k is selected based on the scale
of ˆr according to the formula k = max(1, min(4, ⌊3 +

log2(p ˆwˆh/224)⌋)) [19]. The result is forwarded to two

parallel sub-branches: one devoted to predicting a class la-
bel (or void) for the region proposal together with class-
speciﬁc corrections of the proposal’s bounding box, and the
other devoted to providing class-speciﬁc mask segmenta-
tions. The ﬁrst sub-branch of RSH is composed of two
fully-connected layers with 1024 channels, each followed
by Group Normalization (GN) [52] and LeakyReLU with
slope 0.01, and a ﬁnal fully-connected layer with 5Nclasses +
1 output units. The output units encode, for each possi-
ble class c, class-speciﬁc correction factors (oc
w, oc
h)
that are used to compute a new bounding box centered
ˆh) with dimensions
in (ˆuc, ˆvc) = (ˆuo + oc
u ˆw, ˆvr + oc
v
( ˆwc, ˆhc) = ( ˆw eoc
w , ˆh eoc
h ). This operation generates from
ˆr and for each class c a new class-speciﬁc region propos-
als given by ˆrc = (ˆuc, ˆvc, ˆwc, ˆhc).
In addition, we have
Nclasses + 1 units providing logits for a softmax layer that
gives a probability distribution over classes and void, the
latter label assessing the invalidity of the proposal. The
probability associated to class c is used as score function ˆsc
for the class-speciﬁc region proposal ˆrc. The second sub-
branch applies four 3 × 3 convolution layers each with 256
output channels. As for the ﬁrst sub-branch each convolu-
tion is followed by GN and LeakyReLU. This is followed

u, oc

v, oc

8279

Figure 1: Comparison of two architectures for panoptic segmentation. Left: Separate models (including bodies) for detection
and segmentation. Both predictions are fused to obtain the ﬁnal panoptic prediction. Right: Shared body between the heads.

by a 2 × 2 deconvolution layer with output stride 2 and 256
output channels, GN, LeakyReLU, and a ﬁnal 1 × 1 con-
volution with Nclasses output channels. This yields, for each
class, 28 × 28 logits that provide class-speciﬁc mask fore-
ground probabilities for the given region proposal via a sig-
moid. The resulting mask prediction is combined with the
output of the segmentation branch described below.

3.3. Semantic Segmentation Branch

The semantic segmentation branch takes in input the out-
puts of the backbone corresponding to the ﬁrst four scales of
FPN. We apply independently to each input (not sharing pa-
rameters) a variant of the DeepLabV3 head [6] that we call
Mini-DeepLab (MiniDL, see Fig. 2) followed by an upsam-
pling operation that yields an output downsampling factor
of ×4 and 128 output channels. All the resulting streams are
concatenated and the result is fed to a ﬁnal 1×1 convolution
layer with Nclasses output channels. The output is bilinearly
upsampled to the size of the input image. This provides
the logits for a ﬁnal softmax layer that provides class prob-
abilities for each pixel of the input image. Note that each
convolution in the semantic segmentation branch, including
MiniDL, is followed by iABNsync akin to the backbone.

MiniDL. The MiniDL module consists of 3 parallel sub-
branches. The ﬁrst two apply a 3 × 3 convolution with 128
output channels with dilations 1 and 6, respectively. The
third one applies a 64 × 64 average pooling operation with
stride 1 followed by a padding with boundary replication
to recover the spatial resolution of the input and a 1 × 1
convolution with 128 output channels. The outputs of the
3 sub-branches are concatenated and fed into a 3 × 3 con-
volution layer with 128 output channels, which delivers the
ﬁnal output of the MiniDL module.

As opposed to DeepLabV3, we do not perform the global
pooling operation in our MiniDL module for two reasons:
i) it breaks translation equivariance if we change the input
resolution at test time, which is typically the case and ii)
since we work with large input resolutions, it is preferable
to limit the extent of contextual information. Instead, we re-

placed the global pooling operation with average pooling in
the 3rd sub-branch with a ﬁxed large kernel size and stride
1, but without padding. The lack of padding yields an output
resolution which is smaller than the input resolution and we
re-establish the input resolution by replicating the boundary
of the resulting tensor, i.e. we employ a padding layer with
boundary replication. By doing so, we generalize the solu-
tion originally implemented in DeepLabV3, for we obtain
the same output at training time if we keep the kernel size
equal to the training input resolution, but we preserve trans-
lation equivariance at test time, and can reduce the extent of
contextual information by properly ﬁxing the kernel size.

Figure 2: Segmentation Head (top) and the architecture of
the Mini Deeplab (MiniDL) module (bottom), which is used
in the head.

3.4. Training losses

The two branches of the architecture are supported with
distinct losses, which are detailed below. We denote by

8280

ROIAlignRPNHeadDetSegMask R-CNNFusePanROIAlignRPNHeadDetSegMask R-CNNFusePanImgBody BBody ABodyImgMaskHeadSeg.HeadSeg.HeadMaskHead↑Up↑Up↑UpMiniDeep-LabMiniDLMiniDLMiniDLMiniDLCFPNx4FPNx32FPNx16FPNx81x163x31283x3 / 612864x64AVG1x11283x3128CMiniDLCConcatenationKxK/DCK...Kernel Size, C...Channels, D...DilationY = {1, . . . , Nclasses} the set of class labels, and assume
for simplicity input images with ﬁxed resolution H × W .
Semantic segmentation branch. Let Yij ∈ Y be the se-
mantic segmentation ground truth for a given image and
pixel position (i, j) and let Pij(c) denote the predicted
probability for the same pixel to be assigned class c ∈
Y. The per-image segmentation loss that we employ is a
weighted per-pixel log-loss that is given by

Lss(P, Y ) = −Xij

ωij log Pij(Yij) .

The weights are computed following the simplest version
of [45] with p = 1 and τ = 4
W H . This corresponds to
having a pixel-wise hard negative mining, which selects the
25% worst predictions, i.e. ωij = τ for all (i, j) within
the 25% pixels yielding the lowest probability Pij(Yij), and
ωij = 0 otherwise.
Instance segmentation branch. The losses for the instance
segmentation branch and the training procedure are derived
from the ones proposed in Mask R-CNN [19]. We refer
to [42] for additional details due to the lack of space.

3.5. Testing and Panoptic Fusion

At test time, given an input image I we extract features
F with the backbone and generate region proposals with
corresponding objectness scores by applying RPH. We ﬁl-
ter the resulting set of bounding boxes with Non-Maxima
Suppression (NMS) guided by the objectness scores. The
surviving proposals are fed to the RSH (ﬁrst sub-branch)
together with F in order to generate class-speciﬁc region
proposals with corresponding class probabilities. A second
NMS pass is applied on the resulting set of bounding boxes,
this time independently per class guided by the class prob-
abilities. The resulting class-speciﬁc bounding boxes are
fed again to RSH together with F , but this time through the
second sub-branch which provides the corresponding mask
predictions. The extracted features F are fed in parallel to
the segmentation branch, which provides class probabilities
for each pixel. The output of RSH and the segmentation
branch are ﬁnally fused using the strategy given below, in
order to deliver the ﬁnal panoptic segmentation.

Fusion. The fusion operation is inspired by the one pro-
posed in [24]. We start iterating over predicted instances
in reverse classiﬁcation score order. For each instance we
mark the pixels in the ﬁnal output that belong to it and are
still unassigned, provided that the latter number of pixels
covers at least 50% of the instance. Otherwise we discard
the instance thus resembling a NMS procedure. Remaining
unassigned pixels take the most likely class according to the
segmentation head prediction, if it belongs to stuff, or void
if it belongs to thing. Finally, if the total amount of pixels
of any stuff class is smaller than a given threshold (4096 in
our case) we mark all those pixels to void.

4. Revisiting Panoptic Segmentation

In this section we review the panoptic segmentation met-
ric [24] (a.k.a. PQ metric), which evaluates the performance
of a so-called panoptic segmentation, and discuss a limita-
tion of this metric when it comes to stuff classes.

PQ metric. A panoptic segmentation assigns each pixel
a stuff class label or an instance ID. Instance IDs are fur-
ther given a thing class label (e.g. pedestrian, car, etc.). As
opposed to AP metrics used in detection, instances are not
overlapping. The PQ metric is computed for each class
independently and averaged over classes (void class ex-
cluded). This makes the metric insensitive to imbalanced
class distributions. Given a set of ground truth segments Sc
and predicted segments ˆSc for a given class c, the metric
collects a set of True Positive matches as TPc = {(s, ˆs) ∈
Sc × ˆSc : IoU(s, ˆs) > 0.5} . This set contains all pairs of
ground truth and predicted segments that overlap in terms
of IoU more than 0.5. By construction, every ground truth
segment can be assigned at most one predicted segment and
vice versa. The PQ metric for class c is given by

PQc = P(s,ˆs)∈TPc

|TPc| + 1

2 |FPc| + 1

2 |FNc|

,

IoU(s, ˆs)

where FPc is the set False Positives, i.e. unmatched pre-
dicted segments for class c, and FNc is the set False Nega-
tives, i.e. unmatched segments from ground truth for class c.
The metric allows also speciﬁcation of void classes, both in
ground truth and actual predictions. Pixels labeled as void
in the ground truth are not counted in IoU computations
and predicted segments of any class c that overlap with void
more than 50% are not counted in FPc. Also, ground truth
segments for class c that overlap with predicted void pixels
more than 50% are not counted in FNc. The ﬁnal PQ metric
is obtained by averaging the class-speciﬁc PQ scores:

PQ =

1

Nclasses Xc∈Y

PQc .

We further denote by PQTh and PQSt the average of thing-
speciﬁc and stuff-speciﬁc PQ scores, respectively.

The issue with stuff classes. One limitation of the PQ met-
ric is that it over-penalizes errors related to stuff classes,
which are by deﬁnition not organized into instances. This
derives from the fact that the metric does not distinguish
stuff and thing classes and applies indiscriminately the rule
that we have a true positive if the ground truth and the pre-
dicted segment have IoU greater than 0.5. De facto it re-
gards all pixels in an image belonging to a stuff class as a
single big instance. To give an example of why we think
this is sub-optimal, consider a street scene with two side-
walks and assume that the algorithm confuses one of the
two with road (say the largest) then the segmentation quality

8281

5. Experimental Results

We assess the beneﬁts of our proposed network ar-
chitecture on multiple street-level image datasets, namely
Cityscapes [10], Mapillary Vistas [39] and the Indian Driv-
ing Dataset (IDD) [51]. All experiments were designed to
provide a fair comparison between baseline reference mod-
els and our proposed architecture design choices. To in-
crease transparency of our proposed design contributions,
we deliberately leave out model extensions like path ag-
gregation network extensions [7, 37], deformable convolu-
tions [13] or Cascade R-CNN [4]. We do not apply test time
data augmentation (multi-scale testing or horizontal ﬂip-
ping) or explicit use of model ensembles, etc., as we assume
that such bells and whistles approximately equally increase
recognition performances for all methods. All models were
only pre-trained on ImageNet [47]. We use the following
terminology in the remainder of this section: Ours Indepen-
dent refers to fused, but individually trained models (Fig. 1
left) each following the proposed design, and Ours Com-
bined refers to the uniﬁed architecture in Fig. 1 (right).

Model and Training Hyperparameters. Unless other-
wise noted, we take all the hyperparameters of the in-
stance segmentation branch from [19]. These hyperparam-
eters are shared by all the models we evaluate in our ex-
periments, and are exhaustively listed in [42]. We initial-
ize our backbone model with weights extracted from Py-
Torch’s ImageNet-pretrained ResNet-50 despite using a dif-
ferent activation function, motivated by ﬁndings in our prior
work [46]. We train all our networks with SGD, using a
ﬁxed schedule of 48k iterations and learning rate 10−2, de-
creasing the learning rate by a factor 10 after 36k and 44k
iterations. At the beginning of training we perform a warm-
up phase where the learning rate is linearly increased from
1
3 · 10−2 to 10−2 in 200 iterations.2 During training the
networks receive full images as input, randomly ﬂipped in
the horizontal direction, and scaled such that their shortest
side measures ⌊1024 · t⌋ pixels, where t is randomly sam-
pled from [0.5, 2.0]. Training is performed on batches of
8 images using a computing node equipped with 8 Nvidia
V100 GPUs. At test time, images are scaled such that their
shortest size measures 1024 pixels (preserving aspect ratio).

5.1. Cityscapes

Cityscapes [10] is a street-level driving dataset with im-
ages from 50 central-European cities. All images were
recorded with a single camera type, image resolution of
1024 × 2048, and during comparable weather and lighting
conditions. It has a total of 5,000 pixel-speciﬁcally anno-
tated images (2,975/500/1,525 for training, validation and
test, respectively), and additionally provides 19,998 images

2Note that the warm-up phase is not strictly needed for convergence.

Instead, we adopt it for compatibility with [19].

8282

Figure 3: Prediction on a Cityscapes validation set image,
where light colored areas highlight conducted errors. Sev-
eral classes, e.g. pole (IoU 0.49) and trafﬁc light (IoU 0.46),
are just below the PQ acceptance threshold, while the side-
walk class (IoU 0.62) is just above it. Thus, the former will
be overly penalized (PQ → 0), while the latter will con-
tribute positively (PQ → 0.62), even if they look qualita-
tively similar. Best viewed in color and with digital zoom.

on sidewalk for that image becomes 0. A real-world exam-
ple is provided in Fig. 3, where several stuff segments are
severely penalized by the PQ metric, not reﬂecting the real
quality of the segmentation. The >0.5-IoU rule for thing
classes is convenient because it renders the matching be-
tween predicted and ground truth instances easy, but this is
a problem to be solved only for thing classes. Indeed, pre-
dicted and ground truth segments belonging to stuff classes
can be directly matched independently from their IoU be-
cause each image has at most one instance of them.

Suggested alternative. We propose to maintain the PQ
metric only for thing classes, but change the metric for stuff
classes. Speciﬁcally, let Sc be the set of ground truth seg-
ments of a given class c and let ˆSc be the set of predicted
segments for class c. Note that each image can have at most
1 ground truth segment and at most 1 predicted segment
of the given stuff class. Let Mc = {(s, ˆs) ∈ Sc × ˆSc :
IoU(s, ˆs) > 0} be the set of matching segments, then the
updated metric for class c becomes:

PQ†

c =( 1

|Sc|P(s,ˆs)∈Mc

PQc ,

IoU(s, ˆs) ,

if c is stuff class

otherwise.

Furthermore, we denote by PQ† the ﬁnal version of the pro-
posed panoptic metric, which averages PQ†
c over all classes,
i.e.

PQ† =

1

Nclasses Xc∈Y

PQ†
c .

Similarly to PQ, the proposed metric is bounded in [0, 1] and
implicitly regards a stuff segment of an image as a single
instance. However, we do not require the prediction of stuff
classes to have IoU>0.5 with the ground truth.

Figure 4: Qualitative results obtained by our proposed combined architectures. Top row: Cityscapes. Middle row: IDD.
Bottom row: Vistas. Best viewed in color and with digital zoom.

forming the coarse extra set, where only coarse annotations
per image are available (which we have not used in our ex-
periments). Images are annotated into 19 object classes (11
stuff and 8 instance-speciﬁc).

For Ours Independent, we trained each recognition
model independently, using the hyperparameter settings de-
scribed above (again, each with a ResNet-50+FPN back-
bone). For the semantic segmentation model, we obtain a
baseline segmentation result of 73.8%(mean Intersection-
over-Union [15]), which is comparable to 75.2% reported
in [28] (using a DenseNet-169 backbone), 73.6% using
DeepLab2 in combination with a ResNet-101 backbone
as reported in [45], or 74.6% with a ResNet-152 in [53].
The instance-segmentation APM (mean average precision
on masks) results of our single model baseline are 31.9%,
which is slightly above the reported baseline score in Mask
R-CNN [19] (31.5% w/o COCO [34] pre-training).

Fusing the results of our individually trained models
(Ours Independent) delivers PQ = 59.8%, PQSt = 64.5%,
PQTh = 64.5% and PQ† = 59.0%. We furthermore pro-
vide results of Ours Combined in Tab. 1, performing equally
well on PQ and PQ†. This is remarkable, given the signiﬁ-
cantly reduced number of model parameters (see discussion
in Section 5.4) and when assuming that the fusion of in-
dividually trained models could lead to an ensemble effect
(often deliberately used to improve test results, at the cost
of increased computational complexity).

In addition, we show results of jointly trained networks
from independent, concurrently appearing works [14, 23,
29, 54, 55], with focus on comparability of network archi-
tectures and data used for pre-training.
In Tab. 1 we ab-
breviate the network backbones as R50, R101 or X71 for
ResNet50, ResNet101 or Xception Net71, respectively, and
provide datasets used for pre-training (I = ImageNet and
C = COCO). All our proposed variants outperform the di-
rect competitors by a considerable margin, i.e., our baseline
models as well as jointly trained architectures are better.

The last entry in Tab. 1 shows results for another variant
of our network where we deactivated freezing of all param-
eters and dropped weight decay on the batch normalization
parameters (keeping the rest as described above). We can
see that this gives another boost in terms of PQ. Finally, the
top row in Fig. 4 shows some qualitative seamless segmen-
tation results obtained with our architecture.

5.2. Indian Driving Dataset (IDD)

IDD [51] was introduced for testing perception algo-
rithm performance in India.
It comprises 10,003 images
from 182 driving sequences, divided in 6,993/981/2,029 im-
ages for training, validation and test, respectively. Images
are either of 720p or 1080p resolution and were obtained
from a front-facing camera mounted on a car roof. The
dataset is annotated into 26 classes (17 stuff and 9 instance-
speciﬁc), and we report results for level 3 labels.

8283

Method

de Geus et al. [14]
Supervised in [30]
FPN-Panoptic [23]
TASCNet [29]
UPSNet [54]
DeeperLab [55]

R50
Ours Combined
Ours Combined no freeze|decay BN R50

I
I

Body Data

PQ PQSt

PQTh

PQ† APM IoU

PQ PQSt

PQTh

PQ† APM IoU

Cityscapes

Vistas

R50
R101
R50
R50
R50
X71

I
I
I
I+C
I
I

-

47.3
57.7
59.2
59.3
56.3

59.8
60.2

-

52.9
62.2
61.5
62.7

-

63.6
63.6

-

39.6
51.6
56.0
54.6

-

54.6
55.6

-
-
-
-
-
-

59.0
59.6

-

24.3
32.0
37.6
33.3

-

33.0
33.3

71.6
75.0
77.8
75.2

-

76.2
74.9

-

32.0

36.2
35.8

-

17.6

27.5

10.0

-
-

-
-

-
-

32.6

34.4

31.1

-
-

-
-

-
-
-
-
-
-

-
-
-

18.5

-
-

40.0
39.8

33.6
33.0

37.5
37.2

16.5
16.2

34.7

-
-
-
-

55.3

45.8
45.6

Table 1: Comparison of validation set results on Cityscapes and Vistas with related works. Used network bodies include
R101, R50 and X71 for ResNet-101, ResNet-50 and Xception-71, respectively. Data indicates datasets used for pre-training
where I = ImageNet and C = COCO. All results in [%].

The recognition models for Ours Independent obtained
segmentation and instance segmentation results of IoU =
67.2% and APM = 29.8%, respectively. The numbers re-
ported as baselines in [51] for semantic segmentation are
55.4% using ERFNet [44] and 66.6% for dilated residual
nets [58] and again Mask R-CNN for instance-speciﬁc seg-
mentation on a ResNet-101 body yielding APM = 26.8%.
Those numbers supposedly belong to the test set, while no
numbers are reported for validation. Moreover, Ours In-
dependent yields PQ = 47.2%, PQSt = 46.6%, PQTh =
48.3% and PQ† = 48.8%. For Ours Combined we ob-
tain PQ = 46.9%, PQSt = 45.9%, PQTh = 48.7%,
PQ† = 48.5%, APM = 29.8% and IOU = 67.5%. In the
key metrics PQ and PQ† the results differ by ≤ 0.3 points,
and we again stress that the numbers for Ours Combined are
provided from network architectures with signiﬁcantly less
parameters.

The middle row in Fig. 4 shows seamless segmentation

results obtained by our combined architecture.

5.3. Mapillary Vistas

Mapillary Vistas [39] is one of the richest, publicly
available street-level image datasets today.
It comprises
25k high-resolution (on average 8.6 MPixels) images, split
into sets of 18k/2k/5k images for training, validation and
test, respectively. We only used the training set during
model training while evaluating on the validation set. Vis-
tas shows street-level images from all over the world, with
images captured from driving cars as well as pedestrians
taken them on a sidewalk. It also has large variability in
terms of weather, lighting, capture time during day and sea-
son, sensor type, etc., making it a very challenging road
scene segmentation benchmark. Accounting for this, we
modify some of our model’s hyper-parameters and training
schedule as follows: we use anchors with aspect ratios in
{0.2, 0.5, 1, 2, 5} and area (2 × D)2, where D is the FPN
level’s downsampling factor; we train on images with short-
est side scaled to ⌊1920 · t⌋, where t is randomly sampled
from [0.8, 1.25]; we train for a total of 192k iterations, de-

creasing the learning rate after 144k and 176k iterations.

Scores for both, Ours Combined and its slightly modi-
ﬁed variant discussed at the end of Section 5.1 are given in
Tab. 1. We obtain +4.2% and +3.6% PQ score over Deep-
erLab [55] and TASCNet [29], respectively. More details
are given in [42]. We also show seamless scene segmenta-
tion results in the bottom row of Fig. 4.

5.4. Computational Aspects

Here, we discuss computational aspects when compar-
ing two individually trained recognition models against our
combined model architecture. When fused, the two task-
speciﬁc models have ≈ 78.06M parameters, which are ≈
51.8% more than our combined architecture (≈ 51.43M ).
The majority of saved parameters belong to the backbone.
The amount of computation is similarly reduced, i.e. the
combined, independently trained models require ≈ 50.4%
more FLOPs due to two inference steps per test image.
In absolute terms, the individual models require ≈ 0.864
TFLOP while our combined architectures requires ≈ 0.514
TFLOP on 1024 × 2048 image resolution, respectively.

6. Conclusions

In this work we have introduced a novel CNN architec-
ture for producing seamless scene segmentation results, i.e.
jointly acting semantic segmentation and instance segmen-
tation modules operating on top of a single network back-
bone. We depart from the prevailing approach of train-
ing individual recognition models, and instead introduce a
multi-task architecture that beneﬁts from interleaving net-
work components as well as a novel segmentation mod-
ule. Moreover, we revisit the panoptic metric used to as-
sess combined segmentation and detection results, and pro-
pose a relaxed alternative for handling stuff segments. Our
ﬁndings include that we can generate state-of-the-art recog-
nition results that are signiﬁcantly more efﬁcient in terms
of computational effort and model sizes, when compared to
combined, individual models.

8284

References

[1] Anurag Arnab and Philip H.S. Torr. Pixelwise instance
In

segmentation with a dynamically instantiated network.
(CVPR), 2017. 2

[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for image segmentation. CoRR, abs/1511.00561, 2015. 2

[3] Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and
Roberto Cipolla. Segmentation and recognition using struc-
ture from motion point clouds.
In (ECCV), pages 44–57.
2008. 2

[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving
into high quality object detection. In (CVPR), June 2018. 6
[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected CRFs. CoRR, abs/1606.00915,
2016. 2

[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for semantic
image segmentation. CoRR, abs/1706.05587, 2017. 4

[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-
rian Schroff, and Hartwig Adam. Encoder-decoder with
atrous separable convolution for semantic image segmenta-
tion. CoRR, abs/1802.02611, 2018. 6

[8] Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, George
Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam,
and Jonathon Shlens. Searching for efﬁcient multi-scale ar-
chitectures for dense image prediction. (NIPS), 2018. 2

[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
(ECCV), September 2018. 2

[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The Cityscapes
dataset for semantic urban scene understanding. In (CVPR),
2016. 2, 6

[11] Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun
In

Instance-sensitive fully convolutional networks.

Sun.
(ECCV), 2016. 2

[12] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In

mantic segmentation via multi-task network cascades.
CVPR, 2016. 2

[13] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In (ICCV), Oct 2017. 6

[14] Daan de Geus, Panagiotis Meletis, and Gijs Dubbelman.
Panoptic segmentation with a joint semantic and instance
segmentation network. CoRR, abs/1809.02110, 2018. 3, 7, 8
[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The Pascal visual object classes (VOC)
challenge. (IJCV), 88(2):303–338, 2010. 7

[16] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In (CVPR), 2014. 2

[17] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Ji-
tendra Malik. Simultaneous detection and segmentation. In
(ECCV), pages 297–312, 2014. 2

[18] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.
In (CVPR), 2017.

Boundary-aware instance segmentation.
2

[19] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B.

Girshick. Mask R-CNN. In (ICCV), 2017. 2, 3, 5, 6, 7

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Identity mappings in deep residual networks. CoRR,
abs/1603.05027, 2016. 3

[21] Xuming He and Stephen Gould. An exemplar-based crf for

multi-instance object segmentation. In (CVPR), 2014. 2

[22] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In (CVPR), June 2018. 2

[23] Alexander Kirillov, Ross B. Girshick, Kaiming He, and Pi-
otr Doll´ar. Panoptic feature pyramid networks. CoRR,
abs/1901.02446, 2018. 3, 7, 8

[24] Alexander Kirillov, Kaiming He, Ross B. Girshick, Carsten
Rother, and Piotr Doll´ar. Panoptic segmentation. CoRR,
abs/1801.00868, 2018. 1, 5

[25] Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bog-
Instancecut: From

dan Savchynskyy, and Carsten Rother.
edges to instances with multicut. In (CVPR), 2017. 2

[26] Peter Kontschieder, Samuel Rota Bul`o, Marcello Pelillo, and
Horst Bischof. Structured labels in random forests for se-
mantic labelling and object detection. (PAMI), 36, 2014. 2

[27] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
In

in fully connected crfs with gaussian edge potentials.
(NIPS), 2011. 2

[28] Ivan Kreso, Sinisa Segvic, and Josip Krapac. Ladder-style
densenets for semantic segmentation of large natural images.
In The IEEE International Conference on Computer Vision
(ICCV) Workshops, Oct 2017. 7

[29] Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa,
and Adrien Gaidon. Learning to fuse things and stuff. CoRR,
abs/1812.01192, 2018. 3, 7, 8

[30] Qizhu Li, Anurag Arnab, and Philip H.S. Torr. Weakly- and
In (ECCV), 2018.

semi-supervised panoptic segmentation.
2, 8

[31] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
CoRR, abs/1611.07709, 2016. 2

[32] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian
Reid. Reﬁnenet: Multi-path reﬁnement networks for high-
resolution semantic segmentation. In (CVPR), 2017. 2

[33] Tsung-Yi Lin, Piotr Doll´ar, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection. CoRR, abs/1612.03144, 2016.
3

[34] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft
COCO: Common objects in context. CoRR, abs/1405.0312,
2014. 7

[35] Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu
Liu, Gang Yu, and Wei Jiang. An end-to-end network for
panoptic segmentation. CoRR, abs/1903.05027, 2019. 3

[36] Shu Liu, Jiaya Jia, Sandra Fidler, and Raquel Urtasun. Sgn:
Sequential grouping networks for instance segmentation. In
(ICCV), 2017. 2

[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.

8285

Path aggregation network for instance segmentation.
(CVPR), June 2018. 2, 6

In

[38] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
(CVPR), pages 3431–3440, 2015. 2

[39] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o, and
Peter Kontschieder. The mapillary vistas dataset for semantic
understanding of street scenes. In (ICCV), October 2017. 2,
6, 8

Hwang, Ting Liu, Xiao Zhang, Vivienne Sze, George Pa-
pandreou, and Liang-Chieh Chen. Deeperlab: Single-shot
image parser. CoRR, abs/1902.05093, 2019. 3, 7, 8

[56] Jian Yao, Sanja Fidler, and Raquel Urtasun. Describing the
scene as a whole: Joint object detection, scene classiﬁcation
and semantic segmentation. In (CVPR), 2012. 1, 2

[57] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-
tion by dilated convolutions. Int. Conf. on Learning Repre-
sentations (ICLR), 2016. 2

[40] Pedro H. O. Pinheiro, Ronan Collobert, and Piotr Doll´ar.

[58] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated

Learning to segment object candidates. 2015. 2

residual networks. In (CVPR), 2017. 2, 8

[59] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. CoRR,
abs/1612.01105, 2016. 2

[41] Pedro H. O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, and
Piotr Doll´ar. Learning to reﬁne object segments. In (ECCV),
2016. 2

[42] Lorenzo Porzi, Samuel Rota Bul`o, Aleksander Colovic, and
Peter Kontschieder. Seamless scene segmentation. CoRR,
2019. 5, 6, 8

[43] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In (NIPS), 2015. 2

[44] E. Romera, J. M. lvarez, L. M. Bergasa, and R. Arroyo.
Erfnet: Efﬁcient residual factorized convnet for real-time
semantic segmentation.
IEEE Transactions on Intelligent
Transportation Systems, 19(1):263–272, 2018. 8

[45] S. Rota Bul`o, G. Neuhold, and P. Kontschieder. Loss max-
pooling for semantic image segmentation. In (CVPR), July
2017. 2, 5, 7

[46] Samuel Rota Bul`o, Lorenzo Porzi, and Peter Kontschieder.
In-place activated batchnorm for memory-optimized training
of DNNs. In (CVPR), 2018. 3, 6

[47] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S.
Ma, Z. Huang, A. Karphathy, A. Khosla, M. Bernstein, A. C.
Berg, and L. Fei-Fei. Imagenet large scale visual recognition
challenge. (IJCV), 2015. 6

[48] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost
for image understanding: Multi-class object recognition and
segmentation by jointly modeling texture, layout, and con-
text. (IJCV), 81(1):2–23, 2007. 2

[49] J. Tighe, M. Niethammer, and S. Lazebnik. Scene parsing
In (CVPR),

with object instances and occlusion ordering.
2014. 1, 2

[50] Z. Tu, X. Chen, A.L. Yuille, and S.-C. Zhu. Image parsing:
Unifying segmentation, detection, and recognition. (IJCV),
2005. 1, 2

[51] Girish Varma, Anbumani Subramanian, Anoop Nambood-
iri, Manmohan Chandraker, and C V Jawahar. Indian driv-
ing dataset (IDD): A dataset for exploring problems of au-
tonomous navigation in unconstrained environments.
In
(WACV), 2019. 2, 6, 7, 8

[52] Yuxin Wu and Kaiming He. Group normalization.

In

(ECCV), September 2018. 3

[53] Zifeng Wu, Chunhua Shen, and Anton van den Hengel.
High-performance semantic segmentation using very deep
fully convolutional networks. CoRR, abs/1604.04339, 2016.
2, 7

[54] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min
Bai, Ersin Yumer, and Raquel Urtasun. Upsnet: A uni-
ﬁed panoptic segmentation network. CoRR, abs/1901.03784,
2019. 3, 7, 8

[55] Tien-Ju Yang, Maxwell D. Collins, Yukun Zhu, Jyh-Jing

8286

