Multi-granularity Generator for Temporal Action Proposal

Yuan Liu‚ôØ‚àó

Lin Ma‚ôÆ‚Ä†

Yifeng Zhang‚ôØ‚Ä†

Wei Liu‚ôÆ

Shih-Fu Chang¬ß

‚ôÆTencent AI Lab

‚ôØSoutheast University

¬ßColumbia University

{lhy19930911,forest.linma}@gmail.com

yfz@seu.edu.cn

{wl2223,sc250}@columbia.edu

Abstract

Video Input

Start

End

Temporal action proposal generation is an important
task, aiming to localize the video segments containing hu-
man actions in an untrimmed video. In this paper, we pro-
pose a multi-granularity generator (MGG) to perform the
temporal action proposal from different granularity per-
spectives, relying on the video visual features equipped with
the position embedding information. First, we propose to
use a bilinear matching model to exploit the rich local infor-
mation within the video sequence. Afterwards, two compo-
nents, namely segment proposal producer (SPP) and frame
actionness producer (FAP), are combined to perform the
task of temporal action proposal at two distinct granular-
ities. SPP considers the whole video in the form of feature
pyramid and generates segment proposals from one coarse
perspective, while FAP carries out a Ô¨Åner actionness eval-
uation for each video frame. Our proposed MGG can be
trained in an end-to-end fashion. By temporally adjust-
ing the segment proposals with Ô¨Åne-grained frame action-
ness information, MGG achieves the superior performance
over state-of-the-art methods on the public THUMOS-14
and ActivityNet-1.3 datasets. Moreover, we employ existing
action classiÔ¨Åers to perform the classiÔ¨Åcation of the propos-
als generated by MGG, leading to signiÔ¨Åcant improvements
compared against the competing methods for the video de-
tection task.

1. Introduction

Temporal action proposal [10, 14] aims at capturing
video temporal intervals that are likely to contain an ac-
tion in an untrimmed video. This task plays an important
role in video analysis and can thus be applied in many
areas, such as action recognition [3, 19‚Äì21], summariza-
tion [45,47], grounding [6,7] and captioning [39,40]. Many
methods [13,43] have been proposed to handle this task, and
have shown that, akin to object proposals for object detec-

‚àóThis work was done while Yuan Liu was a Research Intern with Ten-

cent AI Lab.

‚Ä†Corresponding authors.

Segment Proposals

Frame Actionness

Search Space

1

0.5

0

Score: 0.82

Score: 0.53

Score: 0.33

Time(s)

starting
ending

Search Space

Refined Proposals

Score: 0.82

Figure 1: Our proposed MGG can generate segment propos-
als and frame actionness simultaneously, which helps dis-
cover information about possible actions at both the coarse
and Ô¨Åne levels. By temporally adjusting the boundaries of
the segment within the search space determined by the com-
puted frame actionness, MGG can yield reÔ¨Åned action pro-
posals with both high recall and precision.

tion [30], temporal action proposal has a crucial impact on
the quality of action detection.

High-quality action proposal methods should capture
temporal action instances with both high recall and high
temporal overlapping with ground-truths, meanwhile pro-
ducing proposals without many false alarms. One type of
existing methods focuses on generating segment propos-
als [14, 35], where the initial segments are regularly dis-
tributed or manually deÔ¨Åned over the video sequence. A bi-
nary classier is thereafter trained to evaluate the conÔ¨Ådence
scores of the segments. Such methods are able to gener-
ate proposals of various temporal spans. However, since
the segments are regularly distributed or manually deÔ¨Åned,
the generated proposals naturally have imprecise boundary
information, even though boundary regressors are further
applied. Another thread of work, like [33, 43, 50], tackles
the action proposal task in the form of evaluating frame ac-
tionness. These methods densely evaluate the conÔ¨Ådence
score for each frame and group consecutive frames together
as candidate proposals. The whole video sequence is ana-
lyzed at a Ô¨Åner level, in contrast with the segment proposal
based methods. As a result, the boundaries of the generated

3604

proposals are of high precision. However, such methods of-
ten produce low conÔ¨Ådence scores for long video segments,
resulting in misses of true action segments and thus low re-
calls.

Obviously, these two types of methods are complemen-
tary to each other. Boundary sensitive network (BSN) [26]
adopts a ‚Äúlocal to global‚Äù scheme for action proposal, which
locally detects the boudary information and globally ranks
the candidate proposals. Complementary temporal action
proposal (CTAP) [13] consists of three stages, which are
initial proposal generation, complementary proposal collec-
tion, and boundary adjustment and proposal ranking, re-
spectively. However, both of these two methods are multi-
stage models with the modules in different stages trained
independently, without overall optimization of the models.
Another drawback is the neglect of the temporal position
information, which conveys the temporal ordering informa-
tion of the video sequence and is thereby expected to be
helpful for precisely localizing the proposal boundary.

In order to address the aforementioned drawbacks, we
propose a multi-granularity generator (MGG) by taking full
advantage of both segment proposal and frame actionness
based methods. At the beginning, the frame position em-
bedding, realized with cosine and sine functions of differ-
ent wavelengths, is combined with the video frame features.
The combined features are then fed to MGG to perform the
temporal action proposal. SpeciÔ¨Åcally, a bilinear matching
model is Ô¨Årst proposed to exploit the rich local informa-
tion of the video sequence. Afterwards, two components,
namely segment proposal producer (SPP) and frame action-
ness producer (FAP), are coupling together and responsible
for generating coarse segment proposals and evaluating Ô¨Åne
frame actionness, respectively. SPP uses a U-shape archi-
tecture with lateral connections to generate candidate pro-
posals of different temporal spans with high recall. For FAP,
we densely evaluate the probabilities of each frame being
the starting point, ending point, and inside a correct pro-
posal (middle point). During the inference, MGG can fur-
ther temporally adjust the segment boundaries with respect
to the frame actionness information as shown in Fig. 1, and
consequently produce reÔ¨Åned action proposals.

In summary, the main contributions of our work are four-

fold:

‚Ä¢ We propose an end-to-end multi-granularity generator
(MGG) for temporal action proposal, using a novel
representation integrating video features and the po-
sition embedding information. MGG simultaneously
generates coarse segment proposals by perceiving the
whole video sequence, and predicts the frame action-
ness by densely evaluating each video frame.

which is thereafter harnessed by the following SPP and
FAP.

‚Ä¢ SPP is realized in a U-shape architecture with lateral
connections, capturing temporal proposals of various
spans with high recall, while FAP evaluates the prob-
abilities of each frame being the stating point, ending
point, and middle point.

‚Ä¢ Through temporally adjusting the segment proposal
boundaries using the complementary information in
the frame actionness, our proposed MGG achieves the
state-of-the-art performances on the THUMOS-14 and
ActivityNet-1.3 datasets for the temporal action pro-
posal task.

2. Related Work

A large number of existing approaches have been pro-
posed to tackle the problem of temporal action detection
[24, 31, 34, 48, 49, 51]. Inspired by the success of two-stage
detectors like RCNN [17], many recent methods adopt a
proposal-plus-classiÔ¨Åcation framework [5, 9, 33, 44], where
classiÔ¨Åers are applied on a smaller number of class agnos-
tic segment proposals for detection. The proposal stage and
classiÔ¨Åcation stage can be trained separately [33, 35, 51] or
jointly [5, 44], and demonstrate very competitive results.
Regarding temporal action proposal, DAP [10] and SST
[1] introduce RNNs to process video sequences in a sin-
gle pass. However, LSTM [18] and GRU [8] fail to han-
dle video segments with long time spans. Alternatively,
[9,35,41] directly generate proposals from sliding windows.
R-C3D [44] and TAL-Net [5] follow the Faster R-CNN [30]
paradigm to predict locations of temporal proposals and
the corresponding categories. These methods perceive the
whole videos in a coarser level, while the pre-deÔ¨Åned tem-
poral intervals may limit the accuracy of generated propos-
als. Methods like temporal action grouping (TAG) [43] and
CDC [33] produce Ô¨Ånal proposals by densely giving evalu-
ation to each frame. Analyzing videos in a Ô¨Åner level, the
generated proposals are quite accurate in boundaries. In our
work, MGG tackles the problem of temporal action pro-
posal in both coarse and Ô¨Åne perspectives, being better at
both recall and overlapping.

3. Our Approach

Given an untrimmed video sequence s = {sn}ls

n=1 with
its length as ls, temporal action proposal aims at detecting
action instances œïp = {Œæn = [ts,n, te,n]}Ms
n=1, where Ms
is the total number of action instances, and [ts,n, te,n] de-
note the starting and ending points of an action instance Œæn,
respectively.

‚Ä¢ A bilinear matching model is proposed to exploit
the rich local information within the video sequence,

We propose one novel neural network, namely MGG
shown in Fig. 2, which analyzes the video and performs

3605

Position Embedding

Feature Representation

Segment Proposals

Segment Proposal

Producer

l

e
d
o
M

 
r
a
e
n

i
l
i

B

Frame Actionness

Producer

Temporal
Boundary
Adjustment

v
n
o
C

H1

H2

ùíçùíî

ùíçùíî

L

BaseNet

Figure 2: The architecture of our proposed MGG. The video
visual features are Ô¨Årst combined with the position embed-
ding information to form the video representations. The
proposed BaseNet relies on a blinear model to exploit the
rich local information within the sequential video represen-
tations. Segment proposal producer (SPP) is realized by us-
ing a U-shape architecture with lateral connections to gen-
erate proposals of different temporal lengths, while frame
actionness producer (FAP) evaluates each frame whether it
is the starting point, ending point, or middle point. With the
temporal boundary adjustment (TBA) module, boundaries
of the segment proposals are temporally adjusted based on
computed frame actionness, and the reÔ¨Åned accurate action
proposals are therefore generated.

temporal action proposal at different granularities. SpeciÔ¨Å-
cally, our proposed MGG consists of four components. The
video visual features are Ô¨Årst combined with the position
embedding information to yield the video representations.
The subsequent BaseNet relies on a blinear model to ex-
ploit the rich local information within the sequential video
representations. Afterwards, SPP and FAP are used to pro-
duce the action proposals from the coarse (segment) and
Ô¨Åne (frame) perspectives, respectively. Finally, the tem-
poral boundary adjustment (TBA) module adjusts the seg-
ment proposal boundaries regarding the frame actionness
and therefore generates action proposals of both high recall
and precision.

3.1. Video Representation

First, we need to encode the video sequence and gener-
ate the corresponding representations. Same as the previous
work [13, 26], one convolutional neural network (CNN) is
used to convert one video sequence s = {sn}ls
n=1 into one
visual feature sequence f = {fn}ls
n=1 with fn ‚àà Rdf . df
is the dimension of each feature representation. However,
the temporal ordering information of the video sequence is
not considered. Inspired by [15, 38], we embed the position
information to explicitly characterize the ordering informa-
tion of each visual feature, which is believed to beneÔ¨Åt the
action proposal generation. The position information of the
n-th (n ‚àà [1, ls]) visual feature fn is embedded into a fea-
ture pn with a dimension dp by computing cosine and sine

functions of different wavelengths:

pn(2i) = sin(n/100002i/dp ),
pn(2i + 1) = cos(n/100002i/dp ),

(1)

where i is the index of the dimension. The generated
position embedding pn will be equipped with the visual
feature representation fn via concatenation, denoted by
ln = [fn, pn]. As such, the Ô¨Ånal video representations
L = {ln}ls
n=1 ‚àà Rls√ódl are obtained, where dl = df + dp
denotes the dimension of the fused representations.

3.2. BaseNet

Based on the video representations, we propose a novel
BaseNet to exploit the rich local behaviors within the video
sequence. As shown in Fig. 2, two temporal convolutional
layers are Ô¨Årst stacked to exploit video temporal relation-
ships. A typical temporal convolutional layer is denoted as
Conv(nf , nk, ‚Ñ¶), where nf , nk, and ‚Ñ¶ are Ô¨Ålter numbers,
kernel size, and activation function, respectively. In our pro-
posed BaseNet, the two convolutional layers are of the same
architecture, speciÔ¨Åcally Conv(dh, k, ReLU), where dh is
set to 512, k is set to 5, and ReLU refers to the activation of
rectiÔ¨Åed linear units [29]. The outputs of these two tempo-
ral convolutional layers are denoted as H1 and H2, respec-
tively.

The intermediate representations H1 and H2 express the
semantic information of the video sequence at different lev-
els, which are rich in characterizing the local information.
We propose a bilinear matching model [28] to capture the
interaction behaviors between H1 and H2. Due to a large
number of parameters contained in a traditional bilinear
matching model, which result in an increased computational
complexity and a higher convergence difÔ¨Åculty, we turn to
pursue a factorized bilinear matching model [11, 23]:

ÀÜH n
1 =H n
ÀÜH n
2 =H n
i = ÀÜH n
T n
1
1 ‚àà R1√ódh and H n

1 Wi + bi,
2 Wi + bi,
ÀÜH n‚ä§

,

2

(2)

2 ‚àà R1√ódh denote the cor-
where H n
responding representations at the n-th location of H1 and
H2, respectively. Wi ‚àà Rdh√óg and bi ‚àà R1√óg are the
parameters to be learned, with g denoting a hyperparam-
eter and being much smaller than dh. Due to the smaller
value of g, fewer parameters are introduced, which are eas-
ier for training. As such, the matching video representations
T = [T 1, .., T ls ], with T n = [T n
] denoting the
n-th feature, is obtained and used as the input to the follow-
ing SPP and FAP for proposal generation.

2 , .., T n
dh

1 , T n

3.3. Segment Proposal Producer

Due to large variations of action duration, capturing pro-
posals of different temporal lengths with high recall is a big

3606

ùëá#

Max pool

Conv

Max pool

ùëá	

Conv

&
ùëì%

Conv

‚Äô
ùëì%

Deconv

Deconv

Anchor Predict

Anchor Predict

Anchor Predict

&
ùëì)

‚Äô
ùëì)

(
ùëì)

Conv

(
ùëì%

(a) The architecture of SPP

v
n
o
C

v
n
o
C

v
n
o
C

v
n
o
C

sigmoid cross

entropy with tIoU

smooth L1 loss

(b) The anchor predict module

Figure 3: (a) Overview of SPP with pyramid levels M = 3.
With a U-shape architecture and lateral connections, the
generated feature pyramid FH is helpful for capturing pro-
posals with different temporal durations.
(b) The anchor
predict module has two branches which are used for classi-
Ô¨Åcation and boundary regression, respectively.

challenge. Xu et al. [44] used one feature map to locate
proposals of various temporal spans, yielding low average
recall. SSAD [24] and TAL-Net [5] use a feature pyramid
network, with each layer being responsible for proposal lo-
calization with speciÔ¨Åc time spans. However, each pyramid
layer, especially the lower ones being unaware of high-level
semantic information, is unable to localize temporal propos-
als accurately. To deal with this issue, we adopt a U-shape
architecture with lateral connections between the convolu-
tional and deconvolutional layers, as shown in Fig. 3.

With yielded matching video representations T as input,
SPP Ô¨Årst stacks three layers, speciÔ¨Åcally one temporal con-
volutional layer and two max-pooling layers, to reduce the
temporal dimension and hence increase the size of the re-
ceptive Ô¨Åeld accordingly. As a result, the temporal feature
Tc with temporal dimension ls/8 is taken as the input of the
U-shape architecture.

Same as the previous work, such as Unet [32], FPN [27],
and DSSD [12], our U-shape architecture also consists of a
contracting path and an expansive path as well as the lateral
connections. Regarding the contracting path, with repeated
temporal convolutions with stride 2 for downsampling, the
feature pyramid (FP) FL = {f (0)
} is ob-
tained, where f (n)
L is the n-th level feature map of FL with
temporal dimension ls
8‚àó2n . M denotes the total number of
pyramid levels. For the expansive path, temporal decon-
volutions are adopted on multiple layers with an upscal-
ing factor of 2. Via lateral connections, high-level features
from the expansive path are combined with the correspond-
ing low-level features, with the fused features denoted as
f (n)
H . Repeating this operation, the fused feature pyramid is

L , ...f (M ‚àí1)

L , f (1)

L

H , f (1)

deÔ¨Åned as FH = {f (0)
}. Different levels
of feature pyramids are of different receptive Ô¨Åelds, which
are responsible for locating proposals of different temporal
spans.

H , ...f (M ‚àí1)

H

A set of anchors are regularly distributed over each level
of feature pyramid FH , based on which segment proposals
are produced. As shown in Fig. 3, each fH is followed by
two branches, with each branch realized by stacking two
layers of temporal convolutions. SpeciÔ¨Åcally, one branch
is the classiÔ¨Åcation module to predict the probability of a
ground-truth proposal being present at each temporal loca-
tion for each of the œÅ anchors, where œÅ is the number of an-
chors per location of the feature pyramid. The other branch
is the boundary regression module to yield the relative off-
set between the anchor and the ground-truth proposal.

3.4. Frame Actionness Producer

Based on the yielded matching video representations T ,
the frame actionness producer (FAP) is proposed to evaluate
the actionness of each frame. SpeciÔ¨Åcally, three two-layer
temporal convolutional networks are used to generate the
starting point, ending point, and middle point probabilities
for each frame, respectively. Please note that two-layer tem-
poral convolutional networks share the same conÔ¨Åguration,
where the Ô¨Årst one is deÔ¨Åned as Conv(df , k, ReLU) and the
second one is Conv(1, k, Sigmoid). df is set to 64, while
k, as the kernel size, is set to 3. And their weights are not
shared. As a result, we obtain three probability sequences,
namely the starting probability sequence Ps = {ps
n=1,
the ending probability sequence Pe = {pe
n=1, and the
n }ls
middle probability sequence Pm = {pm
n, pe
n,
and pm
n denoting the starting, ending, and middle probabil-
ities of the n-th feature, respectively. Compared with the
generated segment proposals by SPP, the frame actionness
yielded by FAP densely evaluates each frame in a Ô¨Åner man-
ner.

n}ls
n=1, with ps

n}ls

4. Training and Inference

In this section, we will Ô¨Årst introduce how to train our
proposed MGG network, which can subsequently generate
segment proposals and frame actionness. During the infer-
ence, we propose one novel fusion strategy by temporally
adjusting the segment boundary information with respect to
the frame actionness.

4.1. Training

As introduced in Sec. 3, our proposed MGG considers
both the SPP and FAP together with a shared BaseNet. Dur-
ing the training process, these three components cooperate
with each other and are jointly trained in an end-to-end fash-
ion. SpeciÔ¨Åcally, the objective function of our proposed

3607

MGG is deÔ¨Åned as:

LM GG = LSP P + Œ≤LF AP ,

(3)

where LSP P and LF AP are the objective functions deÔ¨Åned
for SPP and FAP, respectively. Œ≤ is a parameter to adjust
their relative contributions, which is empirically set to 0.1.
Detailed information about LSP P and LF AP will be intro-
duced in what follows.

4.1.1 SPP Training

Our proposed SPP produces a set of anchor segments for
each level of the fused feature pyramids FH . We Ô¨Årst in-
troduce how to assign labels to the corresponding anchor
segments. Subsequently, the objective function by referring
to the assigned labels is introduced.

Label Assignment. Same as Faster RCNN [30], we as-
sign a binary class label to each anchor segment. A positive
label is assigned if it overlaps with some ground-truth pro-
posals with temporal Intersection-over-Union (tIoU) higher
than 0.7, or has the highest tIoU with a ground-truth pro-
posal. Anchors are regarded as negative if the maximum
tIoU with all ground-truth proposals is lower than 0.3. An-
chors that are neither positive nor negative are Ô¨Åltered out.
To ease the issue of class imbalance, we sample the positive
and negative examples with a ratio of 1:1 for training.

Objective Function. As shown in Fig. 3 (b), we per-
form a multi-task training for SPP, which not only predicts
the actionness of each anchor segment but also regresses its
boundary information. For actionness prediction, the cross-
entropy function is used, while the smooth L1 loss func-
tion, as introduced in [16], is used for boundary regression.
SpeciÔ¨Åcally, the objective function is deÔ¨Åned as:

LSP P =

Lcls(pi, p‚àó

i )+

1

Ncls Xi
Nreg Xi

1

Œ≥

[p‚àó

i > 1]Lreg(Wi, W ‚àó

i ),

(4)

where Œ≥ is a trade-off parameter, which is set to 0.001
empirically. Ncls is the total number of training exam-
ples. pi stands for the yielded score. p‚àó
i is the label, 1 for
positive samples and 0 for negative samples. Lcls is the
cross-entropy loss function between pi and p‚àó
i . The smooth
L1 loss function Lreg is activated only when the ground-
truth label p‚àó
is positive, and disabled otherwise. Nreg
i
is the number of training examples whose p‚àó
is positive.
i
Wi = {tc, tl} represents the predicted relative offsets of
anchor segments. W ‚àó
l } indicates the relative off-
sets between ground-truth proposals and the anchors, which
can be computed:

i = {t‚àó

c , t‚àó

where ci and li indicate the center and length of anchor
segments, respectively. c‚àó
i represent the center and
length of the ground-truth action instances.

i and l‚àó

4.1.2

FAP Training

FAP takes the matching video representations with their
length as ls as input and outputs three probability se-
quences, namely the starting probability sequence Ps =
{ps
n=1,
and the middle probability sequence Pm = {pm

n=1, the ending probability sequence Pe = {pe
n=1.

n }ls

n}ls

n}ls

Label Assignment.
The ground-truth annotations
temporal action proposals are denoted as œÄ =
of
{œàn = [ts,n, te,n]}Ma
n=1, where Ma is the total number of an-
notations. For each action instance œàn ‚àà œÄ, we deÔ¨Åne the
starting, ending, and middle regions as [ts,n ‚àídd,n/Œ∑, ts,n +
dd,n/Œ∑], [te,n ‚àí dd,n/Œ∑, te,n + dd,n/Œ∑], and [ts,n, te,n], re-
spectively, where dd,n = te,n ‚àí ts,n is the duration of the
annotated action instance and Œ∑ is set to 10 empirically.
For each visual feature, if it lies in the starting, ending,
or middle regions of any action instances, its correspond-
ing starting, ending, or middle label will be set to 1, oth-
erwise 0. In this way, we obtain the ground-truth label for
the three sequences, which are denoted as Gs = {gs
n=1,
Ge = {ge

n=1, and Gm = {gm

n=1, respectively.

n }ls

n}ls

n}ls

Objective Function. Given the predicted probability se-
quences and ground-truth labels, the objective function for
FAP is deÔ¨Åned as:

Lall

F AP = ŒªsLs

F AP + ŒªeLe

F AP + ŒªmLm

F AP .

(6)

F AP , Le

F AP , and Lm

The cross-entropy loss function is used for calculating all
the three losses Ls
F AP , where a weight-
ing factor set by an inverse class frequency is introduced
to address class imbalance. Lall
F AP is the sum of the start-
ing loss Ls
F AP , and middle loss Lm
F AP ,
where Œªs, Œªe, and Œªm are the weights specifying the rel-
ative importance of each part. In our experiments, we set
Œªs = Œªe = Œªm = 1.

F AP , ending loss Le

4.2. Inference

As aforementioned, SPP aims to locate segment propos-
als of various temporal spans, thus yielding segment pro-
posals with inaccurate boundary information. On the con-
trary, FAP gives an evaluation of each video frame in a Ô¨Åner
level, which makes it sensitive to boundaries of action pro-
posals. Obviously, SPP and FAP are complementary to each
other. Therefore, during the inference phase, we propose
the temporal boundary adjustment (TBA) module realized
in a two-stage fusion strategy to improve the boundary ac-
curacy of segment proposals with respect to the frame ac-
tionness.

c = (c‚àó
(cid:26)t‚àó
l = log(l‚àó
t‚àó

i ‚àí ci)/li,
i /li),

(5)

Stage I. We Ô¨Årst use non-maximum suppression (NMS)
to post-process the segment-level action instances detected

3608

by SPP. The generated results are denoted as œïp = {Œæn =
[ts,n, te,n]}Ms
n=1, where Ms is the total number of the de-
tected action instances, and ts,n and te,n denote the corre-
sponding starting and ending times of an action instance Œæn,
respectively. We will adjust ts,n and te,n by referring to
the starting and ending scores detected in FAP. Firstly, we
set two context regions Œæs
n, which are named as the
searching space:

n and Œæe

Œæs
n = [ts,n ‚àí dd,n/Œµ, ts,n + dd,n/Œµ],
Œæe
n = [te,n ‚àí dd,n/Œµ, te,n + dd,n/Œµ],

(7)

n=1.

n}Ms

n or ce

n and tmax

n and tmax

e,n , respectively. If cs

e,n and yield the reÔ¨Åned action instance Œæ‚ãÜ

where dd,n = te,n ‚àí ts,n is the duration of Œæn. Œµ which
controls the size of the searching space is set to 5 . The max
starting score and the corresponding time in the region of
Œæs
n are deÔ¨Åned as cs
s,n , respectively , and the max
ending score and the corresponding time in the region of
Œæe
n are deÔ¨Åned as ce
n is
higher than a threshold œÉ ‚àà [0, 1], which is set manually for
each speciÔ¨Åc dataset, we adjust the starting or ending point
of Œæn with a weighting factor Œ¥ to control the contribution of
s,n and tmax
tmax
n. As
such, the new segment-level action instance set is reÔ¨Åned to
be œï‚ãÜ

p = {Œæ‚ãÜ
Stage II. The middle probability sequence illustrates
the probability of each frame whether it is inside one ac-
tion proposal or not. We use the grouping scheme simi-
lar to TAG [43] to group the consecutive frames with high
middle probability into regions as the candidate action in-
stances. Such generated action instances are denoted by
œïtag = {œÜn}Mt
n=1 with Mt indicating the total number of
grouped action instances. We propose to make a further po-
sition adjustment by considering both œïtag and œï‚ãÜ
p. Specif-
ically, for each action instance Œæ‚ãÜ
p, its tIoU with all
the action instances in œïtag are computed. If the maximum
tIoU is higher than 0.8, the boundaries of Œæ‚ãÜ
n will be re-
placed by the corresponding action instance œÜn in œïtag. Via
such an operation, the substituted proposals are sensitive to
boundaries and the overall boundary accuracy is improved
accordingly.

n in œï‚ãÜ

5. Experiments

5.1. Datasets

THUMOS-14 [22]. It includes 1,010 videos and 1,574
videos with 20 action classes in the validation and test sets,
respectively. There are 200 and 212 videos with temporal
annotations of actions labeled in the validation and testing
sets, respectively. We conduct the experiments on the same
public split as [13, 43].

ActivityNet-1.3 [2]. The whole dataset consists of
19,994 videos with 200 classes annotated, with 50% for
training, 25% for validation, and the rest 25% for testing.

Table 1: Performance comparisons with DAPs [10], SCNN-
prop [35], SST [1], TURN [14], BSN [26], TAG [43], and
CTAP [13] on THUMOS-14 in terms of AR@AN.

Feature

Method

@50

@100 @200 @500 @1000

Flow

2-Stream
2-stream
2-Stream
2-Stream

C3D
C3D
C3D
C3D
C3D
C3D

TURN
TAG
CTAP

BSN+NMS

MGG

DAPs

SCNN-prop

SST

TURN

BSN+NMS

MGG

21.86
18.55
32.49
35.41
39.93

13.56
17.22
19.90
19.63
27.19
29.11

31.89
29.00
42.61
43.55
47.75

23.83
26.17
28.36
27.96
35.38
36.31

43.02
39.61
51.97
52.23
54.65

33.96
37.01
37.90
38.34
43.61
44.32

57.63

64.17

-
-

61.35
61.36

49.29
51.57
51.58
53.52
53.77
54.95

-
-

65.10
64.06

57.64
58.20
60.27
60.75
59.50
60.98

We train our model on the training set and perform evalua-
tions on the validation and testing sets, respectively.

5.2. Temporal Proposal Generation

In this section, we compare our proposed MGG against
the existing state-of-the-art methods on both THUMOS-14
and ActivityNet-1.3 datasets.

Following traditional practice,

For temporal action proposal, Average Recall (AR) com-
puted with different tIoUs is usually adopted for perfor-
mance evaluation.
tIoU
thresholds set from 0.5 to 0.95 with a step size of 0.05 are
used on ActivityNet-1.3, while tIoU thresholds set from 0.5
to 1.0 with a step size of 0.05 are used on THUMOS-14. We
also measure AR with different Average Numbers (ANs)
of proposals, denoted as AR@AN. Moreover, the area un-
der the AR-AN curve (AUC) is also used as one metric on
ActivityNet-1.3, where AN ranges from 0 to 100.

Table 1 illustrates the performance comparisons on the
testing set of THUMOS-14. Different feature representa-
tions will signiÔ¨Åcantly affect the performances. As such,
we adopt the two-stream [36] and C3D [37] features for fair
comparisons. Taking the two-stream features as input, the
AR@AN performances are consistently improved for AN
ranging from 50 to 500, while BSN+NMS achieves a bet-
ter performance with AN equal to 1000. While the C3D
features are adopted, the AR@AN of MGG is higher than
those of the other methods, with AN ranging from 50 to
1000. Such experiments clearly indicate the effectiveness
of MGG in temporal proposal generation.

Furthermore, Fig. 4 illustrates

the AR-AN and
recall@100-tIoU curves of different models on the testing
split of THUMOS-14. It can be observed that our proposed
MGG outperforms the other methods in terms of AR-AN
curves. SpeciÔ¨Åcally, when AN equals 40, MGG signiÔ¨Å-
cantly improves the performance from 33.02% to 37.01%.
For recall@100-tIoU, MGG gains a signiÔ¨Åcantly higher re-
call when tIoU ranges from 0.5 to 1, indicating high accu-
racy of our proposal results.

3609

Figure 4: AR-AN and recall@AN=100 curves of differ-
ent temporal action proposal methods on the testing set of
THUMOS-14.

Performance comparisons with TCN [9],
Table 2:
MSRA [46], Prop-SSAD [25], CTAP [13], and BSN [26]
on the validation and testing splits of ActivityNet-1.3.

Method

TCN

MSRA

Prop-SSAD

CTAP

BSN MGG

AUC (val)
AUC (test)
AR@100

59.58
61.56

-

63.12
64.18

-

64.40
64.80
73.01

65.72

-

73.17

66.17
66.26
74.16

66.43
66.47
74.54

Table 2 illustrates the performance comparisons on the
ActivityNet-1.3 dataset, where a two-stream InÔ¨Çated 3D
ConvNet (I3D) model [4] is used to extract features. Specif-
ically, we compare our proposed MGG with the state-
of-the-art methods, namely TCN [9], MSRA [46], Prop-
SSAD [25], CTAP [13], and BSN [26], in terms of AUC and
AR@100. It can be observed that the proposed MGG out-
performs the other methods on both the validation and test-
ing sets. SpeciÔ¨Åcally, MGG improves AR@100 on the vali-
dation set from 74.16 of the state-of-the-art method BSN to
74.54.

Fig. 5 illustrates some qualitative results of the gener-
ated proposals by MGG on ActivityNet-1.3 and THUMOS-
14. Each is composed of a sequence of frames sampled
from a full video. By analyzing videos from both coarse
and Ô¨Åne perspectives, MGG generates the reÔ¨Åned propos-
als, with high overlapping with ground-truth proposals.

5.3. Ablation Study

In this subsection, the effect of each component in MGG
is studied in detail. We ablate the studies on the validation
set of ActivityNet-1.3. SpeciÔ¨Åcally, in order to verify the
component effectiveness of MGG: position embedding, bi-
linear matching, U-shape architecture in SPP, FAP, and SPP,
we perform the ablation studies as follows:
MGG-P: We discard the position information of the input
video sequence and directly feed the visual feature repre-
sentations into MGG.
MGG-B: We discard the bilinear matching model which
exploits the interactions between the two temporal convo-
lutions within BaseNet, and instead feed the output of the

17.3 17.7 18.0

55.3

56.5 56.9

17.3 18.1 18.3

136.9 137.4

138.1

Time(s)

Time(s)

Time(s)

504.4 504.6 505.1

Ground-Truth

Segment Proposals

510.3 510.6

Refined Proposals

Figure 5: Qualitative results of the proposals generated by
MGG on ActivityNet-1.3 (top and middle) and THUMOS-
14 (bottom). It can be observed that the boundary informa-
tion of the segment proposals generated by SPP is further
adjusted using FAP, resulting in more precise proposals.

Table 3:
ActivityNet-1.3 in terms of AUC and AR@AN.

Ablation studies on the validation set of

Method

AUC (val) @30 @50 @80 @100

MGG-P
MGG-B
MGG-U
MGG-F
MGG-S
MGG

65.59
65.88
65.02
64.31
59.91
66.43

65.21
65.56
64.85
63.76
59.53
66.21

69.93
70.41
69.41
67.91
63.05
70.97

72.88
73.19
72.95
71.04
67.18
73.87

73.92
73.89
73.71
72.24
68.96
74.54

second convolutional layer to the following SPP and FAP.
MGG-U: We discard the U-shape architecture which is pro-
posed in SPP to increase semantic information of the lower
layers. Correspondingly, only the expansive path of the fea-
ture pyramid is used.
MGG-F: We only consider SPP to generate the Ô¨Ånal pro-
posals, without considering FAP and the following TBA
module.
MGG-S: We only consider FAP to generate the Ô¨Ånal pro-
posals, without considering SPP and the following TBA
module.

As shown in Table 3, our full model MGG outperforms
all its variants, namely MGG-P, MGG-B, MGG-U, MGG-F,
and MGG-S, which veriÔ¨Åes the effectiveness of the compo-
nents. In order to examine the detailed effectiveness of the
U-shape architecture, we compare the recall rate of gener-
ated proposals in different lengths. As shown in Table 4,
the recall rate of short proposals drops dramatically, when
the U-shape architecture is removed. The reason is that the
U-shape architecture transfers higher semantic information
to the lower layers, which can perceive global information
of the video sequence, and is thus helpful for capturing pro-
posals with short temporal extents.

3610

Table 4: Recall rates of MGG-U and MGG on generated
proposals of different temporal extents on the validation set
of ActivityNet-1.3, where AN and tIoU thresholds are set to
100 and 0.75, respectively.

Table 6: Performance comparisons between MGG and the
other proposal generation methods in terms of video detec-
tion on the testing set of THUMOS-14, where mAP is re-
ported with tIoU set from 0.3 to 0.7.

Method

0-5s

5-10s

10-15s

15-20s

25-30s

35-40s

40-45s

Proposal Method

ClassiÔ¨Åer

MGG-U
MGG

0.15
0.21

0.63
0.73

0.73
0.82

0.80
0.90

0.91
0.93

0.93
0.93

0.94
0.92

Table 5: Performance comparisons of the two-stage TBA
on the validation set of ActivityNet-1.3 in both end-to-end
training and stagewise training manners.

MGG-F
Stage I
Stage II
AUC(val)
AR@100

Stagewise

End-to-end

X

X
X

X
X
X

X

X
X

X
X
X

64.12
72.05

65.40
73.41

66.28
74.19

64.31
72.24

65.54
73.48

66.43
74.54

Moreover, it can be observed that MGG-F and MGG-S
both perform inferiorly to our full MGG. The main reason
is that SPP and FAP generate proposals at different granu-
larities. Our proposed TBA can exploit their complemen-
tary behaviors and fuse them together to produce proposals
with more precise boundary information. As introduced in
Sec. 4.2, TBA performs in two stages:
Stage I: The starting and ending probability sequences gen-
erated by FAP are used to adjust boundaries of segment pro-
posals from SPP.
Stage II: The middle probability sequence is grouped into
proposals with the method similar to [43] and gives a Ô¨Ånal
adjustment to boundaries of proposals from Stage I.

Table 5 illustrates the effectiveness of each stage in TBA.
It can be observed that the two stages of TBA can both
reÔ¨Åne boundaries of segment proposals, thus consistently
improving the performances, with AUC increasing from
64.31% to 66.43%.

Training: Stagewise v.s. End-to-end. MGG is de-
signed to jointly optimize SPP and FAP in an end-to-end
fashion.
It is also possible to train SPP and FAP sepa-
rately, in which they do not work together. Such a train-
ing scheme is referred to as the stagewise training. Ta-
ble 5 illustrates the performance comparisons between end-
to-end training and stagewise training. It can be observed
that models trained in an end-to-end fashion can outper-
form those learned with stagewise training under the same
settings. It clearly demonstrates the importance of jointly
optimizing SPP and FAP with BaseNet as a shared block to
provide intermediate video representations.

SST [1]

TURN [14]
CTAP [13]
BSN [26]

MGG

SST [1]

TURN [14]
BSN [26]

MGG

SCNN-cls
SCNN-cls
SCNN-cls
SCNN-cls
SCNN-cls

UNet
UNet
UNet
UNet

0.7

-

7.7

-

15.0
15.8

4.7
6.3
20.0
21.3

0.6

0.5

0.4

0.3

-

14.6

-

22.4
23.6

10.9
14.1
28.4
29.5

23.0
25.6
26.9
29.4
29.9

20.0
24.5
36.9
37.4

-

-

34.9

44.1

-

36.6
37.8

31.5
35.3
45.0
46.8

-

43.1
44.9

41.2
46.3
53.5
53.9

5.4. Action Detection

In order to further examine the quality of generated pro-
posals by MGG, we feed the detected proposals into the
state-of-the-art action classiÔ¨Åers, including SCNN [35] and
UntrimmedNet [42]. For fair comparisons, the same clas-
siÔ¨Åers are also used for other proposal generation methods,
including SST [1], TURN [14], CTAP, and BSN. We adopt
the conventional mean Average Precision (mAP) metric,
where Average Precision (AP) reports the performance of
each activity category. SpeciÔ¨Åcally, mAP with tIoU thresh-
olds {0.3, 0.4, 0.5, 0.6, 0.7} is used on THUMOS-14.

Table 6 illustrates the performance comparisons, which
are evaluated on the testing set of THUMOS-14. With the
same classiÔ¨Åer, MGG achieves better performance than the
other proposal generators, and outperforms the state-of-the-
art proposal methods, namely CTAP [13] and BSN [26],
thus demonstrating the effectiveness of our proposed MGG.

6. Conclusion

In this paper, we proposed a novel architecture, namely
MGG, for the temporal action proposal generation. MGG
holds two branches: one is SPP perceiving the whole
video in a coarse level and the other is FAP working in
a Ô¨Åner level. SPP and FAP couple together and integrate
into MGG, which can be trained in an end-to-end fashion.
By analyzing whole videos from both coarse and Ô¨Åne
perspectives, MGG generates proposals with high recall
and more precise boundary information. As such, MGG
achieves better performance than the other state-of-the-art
methods on the THUMOS-14 and ActivityNet-1.3 datasets.
The superior performance of video detection relying on the
generated proposals further demonstrates the effectiveness
of the proposed MGG.

Acknowledgements. This work was supported in part by the Nat-
ural Science Foundation of Jiangsu under Grant BK20151102, in
part by the State Key Laboratory for Novel Software Technology,
Nanjing University under Grant KFKT2017B17, and in part by
the Natural Science Foundation of China under Grant 61673108.

3611

References

[1] S. Buch, V. Escorcia, C. Shen, B. Ghanem, and J. C. Niebles.
In CVPR,

Sst: Single-stream temporal action proposals.
pages 6373‚Äì6382, 2017.

[2] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark
for human activity understanding. In CVPR, pages 961‚Äì970,
2015.

[3] L. Cao, R. Ji, Y. Gao, W. Liu, and Q. Tian. Mining spatiotem-
poral video patterns towards robust action retrieval. Neuro-
computing, 105:61‚Äì69, 2013.

[4] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. In CVPR, pages 4724‚Äì
4733. IEEE, 2017.

[5] Y.-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross,
J. Deng, and R. Sukthankar. Rethinking the faster r-cnn ar-
chitecture for temporal action localization. In CVPR, pages
1130‚Äì1139, 2018.

[6] J. Chen, X. Chen, L. Ma, Z. Jie, and T.-S. Chua. Temporally

grounding natural sentence in video. In EMNLP, 2018.

[7] J. Chen, L. Ma, X. Chen, Z. Jie, and J. Luo. Localizing

natural language in videos. In AAAI, 2019.

[8] K. Cho, B. Van Merri¬®enboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase
representations using rnn encoder-decoder for statistical ma-
chine translation. arXiv preprint arXiv:1406.1078, 2014.

[9] X. Dai, B. Singh, G. Zhang, L. S. Davis, and Y. Q. Chen.
Temporal context network for activity localization in videos.
In ICCV, pages 5727‚Äì5736, 2017.

[10] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.
In

Daps: Deep action proposals for action understanding.
ECCV, pages 768‚Äì784, 2016.

[11] Y. Feng, L. Ma, W. Liu, T. Zhang, and J. Luo. Video re-

localization. In ECCV, pages 51‚Äì66, 2018.

[12] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
Dssd: Deconvolutional single shot detector. arXiv preprint
arXiv:1701.06659, 2017.

[13] J. Gao, K. Chen, and R. Nevatia. Ctap: Complemen-
tary temporal action proposal generation. arXiv preprint
arXiv:1807.04821, 2018.

[14] J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia. Turn
tap: Temporal unit regression network for temporal action
proposals. In ICCV, pages 3648‚Äì3656, 2017.

[15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N.
Dauphin. Convolutional sequence to sequence learning.
arXiv preprint arXiv:1705.03122, 2017.

[16] R. Girshick. Fast r-cnn. In ICCV, pages 1440‚Äì1448, 2015.
[17] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, pages 580‚Äì587, 2014.

[18] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735‚Äì1780, 1997.

[19] W. Huang, L. Fan, M. Harandi, L. Ma, H. Liu, W. Liu, and
C. Gan. Towards efÔ¨Åcient action recognition: Principal back-
propagation for training two-stream networks. IEEE Trans-
actions on Image Processing, 28(4):1773‚Äì1782, 2019.

[20] Y. Jiang, Q. Dai, W. Liu, X. Xue, and C. Ngo. Human
action recognition in unconstrained videos by explicit mo-
tion modeling.
IEEE Transactions on Image Processing,
24(11):3781‚Äì3795, 2015.

[21] Y. Jiang, Q. Dai, X. Xue, W. Liu, and C. Ngo. Trajectory-
based modeling of human actions with motion reference
points. In ECCV, pages 425‚Äì438, 2012.

[22] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Action
recognition with a large number of classes. In ECCV Work-
shop, 2014.

[23] Y. Li, N. Wang, J. Liu, and X. Hou. Factorized bilinear mod-

els for image recognition. arXiv preprint, 2017.

[24] T. Lin, X. Zhao, and Z. Shou. Single shot temporal action

detection. In ACM MM, pages 988‚Äì996, 2017.

[25] T. Lin, X. Zhao, and Z. Shou. Temporal convolution based
arXiv

action proposal: Submission to activitynet 2017.
preprint arXiv:1707.06750, 2017.

[26] T. Lin, X. Zhao, H. Su, C. Wang, and M. Yang. Bsn: Bound-
ary sensitive network for temporal action proposal genera-
tion. arXiv preprint arXiv:1806.02964, 2018.

[27] T. Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In CVPR, pages 936‚Äì944, 2017.

[28] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for Ô¨Åne-grained visual recognition. In ICCV, pages 1449‚Äì
1457, 2015.

[29] V. Nair and G. E. Hinton. RectiÔ¨Åed linear units improve
In ICML, pages 807‚Äì814,

restricted boltzmann machines.
2010.

[30] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, pages 91‚Äì99, 2015.

[31] A. Richard and J. Gall. Temporal action detection using a sta-
tistical language model. In CVPR, pages 3131‚Äì3140, 2016.
[32] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, pages 234‚Äì241, 2015.

[33] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang.
Cdc: Convolutional-de-convolutional networks for precise
temporal action localization in untrimmed videos. In CVPR,
pages 1417‚Äì1426, 2017.

[34] Z. Shou, H. Gao, L. Zhang, K. Miyazawa, and S.-F. Chang.
Autoloc: Weaklysupervised temporal action localization in
untrimmed videos. In ECCV, pages 162‚Äì179, 2018.

[35] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
ization in untrimmed videos via multi-stage cnns. In CVPR,
pages 1049‚Äì1058, 2016.

[36] K. Simonyan and A. Zisserman. Two-stream convolutional
In NIPS, pages

networks for action recognition in videos.
568‚Äì576, 2014.

[37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, pages 4489‚Äì4497, 2015.

[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all
you need. In NIPS, pages 5998‚Äì6008, 2017.

3612

[39] B. Wang, L. Ma, W. Zhang, and W. Liu. Reconstruction

network for video captioning. In CVPR, 2018.

[40] J. Wang, W. Jiang, L. Ma, W. Liu, and Y. Xu. Bidirectional
attentive fusion with context gating for dense video caption-
ing. In CVPR, 2018.

[41] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1(2):2, 2014.

[42] L. Wang, Y. Xiong, D. Lin, and L. V. Gool. Untrimmednets
for weakly supervised action recognition and detection. In
CVPR, pages 6402‚Äì6411, 2017.

[43] Y. Xiong, Y. Zhao, L. Wang, D. Lin, and X. Tang. A pur-
suit of temporal accuracy in general activity detection. arXiv
preprint arXiv:1703.02716, 2017.

[44] H. Xu, A. Das, and K. Saenko. R-c3d: region convolutional
3d network for temporal activity detection. In ICCV, pages
5794‚Äì5803, 2017.

[45] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In ICCV, pages 199‚Äì211, 2015.

[46] T. Yao, Y. Li, Z. Qiu, F. Long, Y. Pan, D. Li, and T. Mei.
Msr asia msm at activitynet challenge 2017: Trimmed action
recognition, temporal action proposals and densecaptioning
events in videos. In CVPR Workshop, 2017.

[47] T. Yao, T. Mei, and Y. Rui. Highlight detection with pairwise
deep ranking for Ô¨Årst-person video summarization. In CVPR,
pages 982‚Äì990, 2016.

[48] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In CVPR, pages 2678‚Äì2687, 2016.

[49] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal action
localization with pyramid of score distribution features. In
CVPR, pages 3093‚Äì3102, 2016.

[50] Z.-H. Yuan, J. C. Stroud, T. Lu, and J. Deng. Temporal action
localization by structured maximal sums. In CVPR, page 7,
2017.

[51] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin.
Temporal action detection with structured segment networks.
In ICCV, pages 2933‚Äì2942, 2017.

3613

