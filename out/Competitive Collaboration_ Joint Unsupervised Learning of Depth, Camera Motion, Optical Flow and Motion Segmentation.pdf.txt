Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera

Motion, Optical Flow and Motion Segmentation

Anurag Ranjan1 Varun Jampani2

Kihwan Kim 2 Deqing Sun 2

Jonas Wulff 1
1Max Planck Institute for Intelligent Systems

Lukas Balles1
3 Michael J. Black1

,

2NVIDIA 3MIT

{aranjan, lballes, jwulff, black}@tuebingen.mpg.de

{vjampani, kihwank, deqings}@nvidia.com

Abstract

We address the unsupervised learning of several intercon-
nected problems in low-level vision: single view depth predic-
tion, camera motion estimation, optical ﬂow, and segmenta-
tion of a video into the static scene and moving regions. Our
key insight is that these four fundamental vision problems are
coupled through geometric constraints. Consequently, learn-
ing to solve them together simpliﬁes the problem because the
solutions can reinforce each other. We go beyond previous
work by exploiting geometry more explicitly and segment-
ing the scene into static and moving regions. To that end,
we introduce Competitive Collaboration, a framework that
facilitates the coordinated training of multiple specialized
neural networks to solve complex problems. Competitive
Collaboration works much like expectation-maximization,
but with neural networks that act as both competitors to ex-
plain pixels that correspond to static or moving regions, and
as collaborators through a moderator that assigns pixels to
be either static or independently moving. Our novel method
integrates all these problems in a common framework and
simultaneously reasons about the segmentation of the scene
into moving objects and the static background, the camera
motion, depth of the static scene structure, and the optical
ﬂow of moving objects. Our model is trained without any su-
pervision and achieves state-of-the-art performance among
joint unsupervised methods on all sub-problems.

.

1. Introduction

Deep learning methods have achieved state-of-the-art re-
sults on computer vision problems with supervision using
large amounts of data [9, 18, 21]. However, for many vision
problems requiring dense, continuous-valued outputs, it is ei-

This project was formerly referred by Adversarial Collaboration: Joint
Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion
Segmentation

Figure 1: Unsupervised Learning of Depth, Camera Mo-
tion, Optical Flow and Motion Segmentation. Left, top to
bottom: sample image, soft masks representing motion seg-
mentation, estimated depth map. Right, top to bottom: static
scene optical ﬂow, segmented ﬂow in the moving regions
and combined optical ﬂow.

ther impractical or expensive to gather ground truth data [6].
We consider four such problems in this paper: single view
depth prediction, camera motion estimation, optical ﬂow,
and motion segmentation. Previous work has approached
these problems with supervision using real [5] and synthetic
data [4]. However there is always a realism gap between
synthetic and real data, and real data is limited or inaccurate.
For example, depth ground truth obtained using LIDAR [6]
is sparse. Furthermore, there are no sensors that provide
ground truth optical ﬂow, so all existing datasets with real
imagery are limited or approximate [1, 6, 13]. Motion seg-
mentation ground truth currently requires manual labeling
of all pixels in an image [26].

Problem. Recent work has tried to address the problem
of limited training data using unsupervised learning [14, 24].
To learn a mapping from pixels to ﬂow, depth, and camera
motion without ground truth is challenging because each
of these problems is highly ambiguous. To address this,
additional constraints are needed and the geometric relations
between static scenes, camera motion, and optical ﬂow can
be exploited. For example, unsupervised learning of depth

12240

and camera motion has been coupled in [38, 22]. They use
an explainability mask to exclude evidence that cannot be
explained by the static scene assumption. Yin et al. [37]
extend this to estimate optical ﬂow as well and use forward-
backward consistency to reason about unexplained pixels.
These methods perform poorly on depth [38] and optical
ﬂow [37] benchmarks. A key reason is that the constraints
applied here do not distinguish or segment objects that move
independently, such as people and cars. More generally,
not all the data in the unlabeled training set will conform
to the model assumptions, and some of it might corrupt
the network training. For instance, the training data for
depth and camera motion should not contain independently
moving objects. Similarly, for optical ﬂow, the data should
not contain occlusions, which disrupt the commonly used
photometric loss.

Idea. A typical real-world scene consists of static regions,
which do not move in the physical world, and moving ob-
jects [36]. Given depth and camera-motion, we can reason
about the static scene in a video sequence. Optical ﬂow, in
contrast, reasons about all parts of the scene. Motion segmen-
tation classiﬁes a scene into static and moving regions. Our
key insight is that these problems are coupled by the geome-
try and motion of the scene; therefore solving them jointly is
synergistic. We show that by learning jointly from unlabeled
data, our coupled networks can partition the dataset and use
only the relevant data, resulting in more accurate results than
learning without this synergy.

Approach. To address the problem of joint unsupervised
learning, we introduce Competitive Collaboration (CC), a
generic framework in which networks learn to collaborate
and compete, thereby achieving speciﬁc goals. In our spe-
ciﬁc scenario, Competitive Collaboration is a three player
game consisting of two players competing for a resource that
is regulated by a third player, the moderator. As shown in
Figure 2, we introduce two players in our framework, the
static scene reconstructor, R = (D, C), that reasons about
the static scene pixels using depth, D, and camera motion,
C; and a moving region reconstructor, F , that reasons about
pixels in the independently moving regions. These two play-
ers compete for training data by reasoning about static-scene
and moving-region pixels in an image sequence. The compe-
tition is moderated by a motion segmentation network, M ,
that segments the static scene and moving regions, and dis-
tributes training data to the players. However, the moderator
also needs training to ensure a fair competition. Therefore,
the players, R and F , collaborate to train the moderator, M ,
such that it classiﬁes static and moving regions correctly in
alternating phases of the training cycle. This general frame-
work is similar in spirit to expectation-maximization (EM)
but is formulated for neural network training.

Contributions. In summary our contributions are: 1)
We introduce Competitive Collaboration, an unsupervised

learning framework where networks act as competitors and
collaborators to reach speciﬁc goals. 2) We show that jointly
training networks with this framework has a synergistic ef-
fect on their performance. 3) To our knowledge, our method
is the ﬁrst to use low level information like depth, camera
motion and optical ﬂow to solve a segmentation task without
any supervision. 4) We achieve state-of-the-art performance
on single view depth prediction and camera motion estima-
tion among unsupervised methods. We achieve state of art
performance on optical ﬂow among unsupervised methods
that reason about the geometry of the scene, and introduce
the ﬁrst baseline for fully unsupervised motion segmenta-
tion. We even outperform competing methods that use much
larger networks [37] and multiple reﬁnement steps such as
network cascading [24]. 5) We analyze the convergence
properties of our method and give an intuition of its gener-
alization using mixed domain learning on MNIST [19] and
SVHN [25] digits. All our models and code are available at
https://github.com/anuragranj/cc.

2. Related Work

Our method is a three-player game, consisting of two
competitors and a moderator, where the moderator takes the
role of a critic and two competitors collaborate to train the
moderator. The idea of collaboration can also be seen as
neural expectation maximization [8] where one model is
trained to distribute data to other models. For unsupervised
learning, these ideas have been mainly used to model the data
distribution [8] and have not been applied to unsupervised
training of regression or classiﬁcation problems.

There is signiﬁcant recent work on supervised training of
single image depth prediction [5], camera motion estimation
[16] and optical ﬂow estimation [4]. However, as labeling
large datasets for continuous-valued regression tasks is not
trivial, and the methods often rely on synthetic data [4, 23,
28]. Unsupervised methods have tried to independently solve
for optical ﬂow [14, 24, 35] by minimizing a photometric
loss. This is highly underconstrained and thus the methods
perform poorly.

More recent works [22, 32, 33, 37, 38] have approached
estimation of these problems by coupling two or more prob-
lems together in an unsupervised learning framework. Zhou
et al. [38] introduce joint unsupervised learning of ego-
motion and depth from multiple unlabeled frames. To ac-
count for moving objects, they learn an explainability mask.
However, these masks also capture model failures such as
occlusions at depth discontinuities, and are hence not useful
for motion segmentation. Mahjourian et al. [22] use a more
explicit geometric loss to jointly learn depth and camera mo-
tion for rigid scenes. Yin et al. [37] add a reﬁnement network
to [38] to also estimate residual optical ﬂow. The estimation
of residual ﬂow is designed to account for moving regions,
but there is no coupling of the optical ﬂow network with

12241

Figure 2: The network R = (D, C) reasons about the scene by estimating optical ﬂow over static regions using depth, D, and
camera motion, C. The optical ﬂow network F estimates ﬂow over the whole image. The motion segmentation network, M ,
masks out static scene pixels from F to produce composite optical ﬂow over the full image. A loss, E, using the composite
ﬂow is applied over neighboring frames to train all these models jointly.

the depth and camera motion networks. Residual optical
ﬂow is obtained using a cascaded reﬁnement network, thus
preventing other networks from using ﬂow information to
improve themselves. Therefore, recent works show good
performance either on depth and camera motion [22, 37, 38]
or on optical ﬂow [24], but not on both. Zou et al. [39]
exploit consistency between depth and optical ﬂow to im-
prove performance. The key missing piece that we add is
to jointly learn the segmentation of the scene into static and
independently-moving regions. This allows the networks
to use geometric constraints where they apply and generic
ﬂow where they do not. Our work introduces a framework
where motion segmentation, ﬂow, depth and camera motion
models can be coupled and solved jointly to reason about
the complete geometric structure and motion of the scene.

Competitive Collaboration can be generalized to prob-
lems in which the models have intersecting goals where
they can compete and collaborate. For example, modeling
multi-modal distributions can be accomplished using our
framework, whereby each competitor learns the distribution
over a mode. In fact, the use of expectation-maximization
(EM) in computer vision began with the optical ﬂow problem
and was used to segment the scene into “layers” [15] and
was then widely applied to other vision problems.

3. Competitive Collaboration

In our context, Competitive Collaboration is formulated
as a three-player game consisting of two players compet-
ing for a resource that is regulated by a moderator as illus-
trated in Figure 3. Consider an unlabeled training dataset
D = {Di : i ∈ N}, which can be partitioned into two dis-
joint sets. Two players {R, F} compete to obtain this data
as a resource, and each player tries to partition D to mini-
mize its loss. The partition is regulated by the moderator’s

output m = M (Di), m ∈ [0, 1]Ω, and Ω is the output do-
main of the competitors. The competing players minimize
their loss function LR, LF respectively such that each player
optimizes for itself but not for the group. To resolve this
problem, our training cycle consists of two phases. In the
ﬁrst phase, we train the competitors by ﬁxing the moderator
network M and minimizing

m· LR(R(Di)) + (1− m)· LF (F (Di)), (1)

E1 =Xi XΩ
where · is used to represent elementwise product throughout
the paper. However, the moderator M also needs to be
trained. This happens in the second phase of the training
cycle. The competitors {R, F} form a consensus and train
the moderator M such that it correctly distributes the data
in the next phase of the training cycle. In the collaboration
phase, we ﬁx the competitors and train the moderator by
minimizing,

E2 = E1 +Xi XΩ

LM (Di, R, F )

(2)

where LM is a loss that denotes a consensus between the
competitors {R, F}. Competitive Collaboration can be ap-
plied to more general problems of training multiple task
speciﬁc networks. In the Appendix A.1, we show the gener-
alization of our method using an example of mixed domain
learning on MNIST and SVHN digits, and analyze its con-
vergence properties.

In the context of jointly learning depth, camera mo-
tion, optical ﬂow and motion segmentation, the ﬁrst player
R = (D, C) consists of the depth and camera motion net-
works that reason about the static regions in the scene. The
second player F is the optical ﬂow network that reasons
about the moving regions. For training the competitors, the

12242

Figure 3: Training cycle of Competitive Collaboration: The
moderator M drives two competitors {R, F} (ﬁrst phase,
left). Later, the competitors collaborate to train the moder-
ator to ensure fair competition in the next iteration (second
phase, right).

motion segmentation network M selects networks (D, C)
on pixels that are static and selects F on pixels that belong
to moving regions. The competition ensures that (D, C) rea-
sons only about the static parts and prevents moving pixels
from corrupting its training. Similarly, it prevents any static
pixels from appearing in the training loss of F , thereby im-
proving its performance in the moving regions. In the second
phase of the training cycle, the competitors (D, C) and F
now collaborate to reason about static scene and moving
regions by forming a consensus that is used as a loss for
training the moderator, M . In the rest of this section, we
formulate the joint unsupervised estimation of depth, camera
motion, optical ﬂow and motion segmentation within this
framework.

Notation. We use {Dθ, Cφ, Fψ, Mχ}, to denote the net-
works that estimate depth, camera motion, optical ﬂow
and motion segmentation respectively. The subscripts
{θ, φ, ψ, χ} are the network parameters. We will omit the
subscripts in several places for brevity. Consider an im-
age sequence I−, I, I+ with target frame I and temporally
neighboring reference frames I−, I+. In general, we can
have many neighboring frames. In our implementation, we
use 5-frame sequences for Cφ and Mχ but for simplicity use
3 frames to describe our approach. We estimate the depth of
the target frame as

d = Dθ(I).

(3)

We estimate the camera motion, e, of each of the reference
frames I−, I+ w.r.t. the target frame I as

e−, e+ = Cφ(I−, I, I+).

(4)

Similarly, we estimate the segmentation of the target image
into the static scene and moving regions. The optical ﬂow
of the static scene is deﬁned only by the camera motion and
depth. This generally refers to the structure of the scene. The
moving regions have independent motion w.r.t. the scene.
The segmentation masks corresponding to each pair of target
and reference image are given by

where m−, m+ ∈ [0, 1]Ω represent the probabilities of re-
gions being static in spatial pixel domain, Ω. Finally, the net-
work Fψ estimates the optical ﬂow. Fψ works with 2 images
at a time, and its weights are shared while estimating u−, u+,
the backward and forward optical ﬂow1 respectively.

u− = Fψ(I, I−),

u+ = Fψ(I, I+).

(6)

learn the parameters of

Loss. We
{Dθ, Cφ, Fψ, Mχ} by jointly minimizing the energy
E = λRER + λF EF + λM EM + λC EC + λSES,

the networks

(7)

where {λR, λF , λM , λC , λS} are the weights on the respec-
tive energy terms. The terms ER and EF are the objectives
that are minimized by the two competitors reconstructing
static and moving regions respectively. The competition for
data is driven by EM . A larger weight λM will drive more
pixels towards the static scene reconstructor. The term EC
drives the collaboration, and ES is a smoothness regularizer.
The static scene term, ER minimizes the photometric loss
on the static scene pixels given by

ER = Xs∈{+,−}XΩ

ρ(cid:16)I, wc(Is, es, d)(cid:17) · ms

(8)

where Ω is the spatial pixel domain, ρ is a robust error func-
tion, and wc warps the reference frames towards the target
frame according to depth d and camera motion e. Similarly,
EF minimizes photometric loss on moving regions

EF = Xs∈{+,−}XΩ

ρ(cid:16)I, wf (Is, us)(cid:17) · (1 − ms)

(9)

where wf warps the reference image using ﬂow u. We show
the formulations for wc, wf in the Appendix A.2 and A.3
respectively. We compute the robust error ρ(x, y) as

ρ(x,y)=λρ√(x−y)2+ǫ2+(1−λρ)"1−

(2µx µy+c1 )(2µxy+c2 )
(µ2

y+c1 )(σx+σy+c2 )# (10)

x+µ2

where λρ is a ﬁxed constant and ǫ = 0.01. The second term
is also known as the structure similarity loss (SSIM) [34]
that has been used in previous work [22, 37], and µx, σx are
the local mean and variance over the pixel neighborhood
with c1 = 0.012 and c2 = 0.032.

The loss EM minimizes the cross entropy, H, between

the masks and a unit tensor regulated by λM

EM = Xs∈{+,−}XΩ

H(1, ms).

(11)

A larger λM gives preference to the static scene reconstructor
R, biasing the scene towards being static.

1Note that this is different from the forward and backward optical ﬂow

m−, m+ = Mχ(I−, I, I+),

(5)

in the context of two-frame estimation.

12243

Let ν(e, d) represent the optical ﬂow induced by camera
motion e and depth d, as described in the Appendix A.2. The
consensus loss EC drives the collaboration and constrains
the masks to segment moving objects by taking a consensus
between ﬂow of the static scene given by ν(e, d) and optical
ﬂow estimates from Fψ. It is given by

EC = Xs∈{+,−}XΩ
H(cid:0)IρR<ρF ∨ I||ν(es,d)−us||<λc , ms(cid:1) (12)
where I ∈ {0, 1} is an indicator function and equals 1 if
the condition in the subscript is true. The ﬁrst indicator
function favors mask assignments to the competitor that
achieves lower photometric error on a pixel by comparing
ρR = ρ(I, wc(Is, es, d)) and ρF = ρ(I, wf (Is, us)). In the
second indicator function, the threshold λc forces I = 1,
if the static scene ﬂow ν(e, d) is close to the optical ﬂow
u, indicating a static scene. The symbol ∨ denotes logical
OR between indicator functions. The consensus loss EC
encourages a pixel to be labeled as static if R has a lower
photometric error than F or if the induced ﬂow of R is
similar to that of F . Finally, the smoothness term ES acts as
a regularizer on depth, segmentations and ﬂow,

ES =XΩ

||λe∇d||2 + ||λe∇u−||2 + ||λe∇u+||2
+||λe∇m−||2 + ||λe∇m+||2,

(13)

where λe = e−∇I (elementwise) and ∇ is the ﬁrst deriva-
tive along spatial directions [29]. The term λe ensures that
smoothness is guided by edges of the images.

Inference. The depth d and camera motion e are directly
inferred from network outputs. The motion segmentation
m∗ is obtained by the output of mask network Mχ and the
consensus between the static ﬂow and optical ﬂow estimates
from Fχ. It is given by

m∗ = Im+·m−>0.5 ∨ I||ν(e+,d)−u+||<λc .

(14)

The ﬁrst term takes the intersection of mask probabilities in-
ferred by Mχ using forward and backward reference frames.
The second term takes a consensus between ﬂow estimated
from R = (Dθ, Cφ) and Fψ to reason about the masks.
The ﬁnal masks are obtained by taking the union of both
terms. Finally, the full optical ﬂow, u∗, between (I, I+) is
a composite of optical ﬂows from the static scene and the
independently moving regions given by

u∗ = Im∗>0.5 · ν(e+, d) + Im∗≤0.5 · u+.

(15)

The loss in Eq. (7) is formulated to minimize the reconstruc-
tion error of the neighboring frames. Two competitors, the
static scene reconstructor R = (Dθ, Cφ) and moving region
reconstructor Fψ minimize this loss. The reconstructor R

reasons about the static scene using Eq. (8) and the recon-
structor Fψ reasons about the moving regions using Eq. (9).
The moderation is achieved by the mask network, Mχ using
Eq. (11). Furthermore, the collaboration between R, F is
driven using Eq. (12) to train the network Mχ.

If the scenes are completely static, and only the camera
moves, the mask forces (Dθ, Cφ) to reconstruct the whole
scene. However, (Dθ, Cφ) are wrong in the independently
moving regions of the scene, and these regions are recon-
structed using Fψ. The moderator Mχ is trained to segment
static and moving regions correctly by taking a consensus
from (Dθ, Cφ) and Fψ to reason about static and moving
parts on the scene, as seen in Eq. (12). Therefore, our train-
ing cycle has two phases. In the ﬁrst phase, the moderator
Mχ drives competition between two models (Dθ, Cφ) and
Fψ using Eqs. (8, 9). In the second phase, the competitors
(Dθ, Cφ) and Fψ collaborate together to train the moderator
Mχ using Eqs. (11,12).

4. Experiments

Network Architecture. For the depth network, we experi-
ment with DispNetS [38] and DispResNet where we replace
convolutional blocks with residual blocks [10]. The net-
work Dθ takes a single RGB image as input and outputs
depth. For the ﬂow network, Fψ, we experiment with both
FlowNetC [4] and PWC-Net [31]. The PWC-Net uses the
multi-frame unsupervised learning framework from Janai et
al. [12]. The network Fψ computes optical ﬂow between
a pair of frames. The networks Cφ, Mχ take a 5 frame
sequence (I−−, I−, I, I+, I++) as input. The mask net-
work Mχ has an encoder-decoder architecture. The encoder
consists of stacked residual convolutional layers. The de-

Result: Trained Network Parameters, (θ, φ, ψ, χ)
Deﬁne λ = (λR, λF , λM , λC);
Randomly initialize (θ, φ, ψ, χ);
Update (θ, φ) by jointly training (Dθ, Cφ) with
λ = (1.0, 0.0, 0.0, 0.0);
Update ψ by training Fψ with λ = (0.0, 1.0, 0.0, 0.0);
Update χ by jointly training (Dθ, Cφ, Fψ, Mχ) with
λ = (1.0, 0.5, 0.0, 0.0);
Loop

Competition Step

Update θ, φ by jointly training (Dθ, Cφ,
Fψ, Mχ) with λ = (1.0, 0.5, 0.05, 0) ;
Update ψ by jointly training (Dθ, Cφ, Fψ, Mχ)
with λ = (0.0, 1.0, 0.005, 0) ;

Collaboration Step

Update χ by jointly training (Dθ, Cφ, Fψ, Mχ)
with λ = (1.0, 0.5, 0.005, 0.3) ;

EndLoop

Algorithm 1: Network Training Algorithm

12244

Figure 4: Visual results. Top to bottom: Sample image, estimated depth, soft consensus masks, motion segmented optical
ﬂow and combined optical ﬂow.

coder has stacked upconvolutional layers to produce masks
(m−−, m−, m+, m++) of the reference frames. The cam-
era motion network Cφ consists of stacked convolutions
followed by adaptive average pooling of feature maps to
get the camera motions (e−−, e−, e+, e++). The networks
Dθ, Fψ, Mχ output their results at 6 different spatial scales.
The predictions at the ﬁnest scale are used. The highest scale
is of the same resolution as the image, and each lower scale
reduces the resolution by a factor of 2. We show the network
architecture details in the Appendix A.4.

Network Training. We use raw KITTI sequences [6] for
training using Eigen et al.’s split [5] that is consistent across
related works [5, 20, 22, 37, 38, 39]. We train the net-
works with a batch size of 4 and learning rate of 10−4 us-
ing ADAM [17] optimization. The images are scaled to
256 × 832 for training. The data is augmented with random
scaling, cropping and horizontal ﬂips. We use Algorithm
1 for training. Initially, we train (Dθ, Cφ) with only pho-
tometric loss over static pixels ER and smoothness loss
ES while other loss terms are set to zero. Similarly, we
train Fψ independently with photometric loss over all pix-
els and smoothness losses. The models (Dθ, Cφ), Fψ at
this stage are referred to as ‘basic’ models in our exper-
iments. We then learn Mχ using the joint loss. We use
λR = 1.0, λF = 0.5 for joint training because the static
scene reconstructor R uses 4 reference frames in its loss,
whereas the optical ﬂow network F uses 2 frames. Hence,
these weights normalize the loss per neighboring frame. We
iteratively train (Dθ, Cφ), Fψ, Mχ using the joint loss while
keeping the other network weights ﬁxed. The consensus
weight λC = 0.3 is used only while training the mask net-
work. Other constants are ﬁxed with λS = 0.005, and
threshold in Eq. (14), λc = 0.001. The constant λρ = 0.003
regulates the SSIM loss and is chosen empirically. We it-
eratively train the competitors (Dθ, Cφ), Fψ and moderator
Mχ for about 100,000 iterations at each step until validation
error saturates.

Monocular Depth and Camera Motion Estimation. We
obtain state of the art results on single view depth prediction
and camera motion estimation as shown in Tables 1 and 3.
The depth is evaluated on the Eigen et al. [5] split of the raw
KITTI dataset [6] and camera motion is evaluated on the
KITTI Odometry dataset [6]. These evaluation frameworks
are consistent with previous work [5, 20, 22, 37]. All depth
maps are capped at 80 meters. As shown in Table 1, by train-
ing our method only on KITTI [6], we get similar or better
performance than competing methods like [37, 39] that use
a much bigger Resnet-50 architecture [10] and are trained
on the larger Cityscapes dataset [3]. Using Cityscapes in our
training further improves our performance on depth estima-
tion benchmarks (cs+k in Table 1).

Ablation studies on depth estimation are shown in Table
2. In the basic mode, our network architecture, DispNet for
depth and camera motion estimation is most similar to [38]
and this is reﬂected in the performance of our basic model.
We get some performance improvements by adding the SSIM
loss [34]. However, we observe that using the Competitive
Collaboration (CC) framework with a joint loss results in
larger performance gains in both tasks. Further improve-
ments are obtained by using a better network architecture,
DispResNet. Greater improvements in depth estimation are
obtained when we use a better network for ﬂow, which shows
that improving on one task improves the performance of the
other in the CC framework (row 4 vs 5 in Table 2).

The camera motion estimation also shows similar perfor-
mance trends as shown in Table 3. Using a basic model,
we achieve similar performance as the baseline [38], which
improves with the addition of the SSIM loss. Using the CC
framework leads to further improvements in performance.

In summary, we show that joint training using CC boosts
performance of single view depth prediction and camera mo-
tion estimation. We show qualitative results in Figure 4.
In the Appendix, we show additional evaluations using
Make3D dataset [30] (A.6) and more qualitative results (A.5).

12245

Method
Eigen et al. [5] coarse
Eigen et al. [5] ﬁne
Liu et al. [20]
Zhou et al. [38]
Mahjourian et al. [22]
Geonet-Resnet [37]
DF-Net [39]
CC (ours)
Zhou et al.* [38]
Mahjourian et al. [22]
Geonet-VGG [37]
Geonet-Resnet [37]
Godard et el. [7]
DF-Net [39]
CC (ours)

k
k
k

cs+k
cs+k
cs+k
cs+k
cs+k

Data AbsRel
0.214
0.203
0.202
0.198
0.159
0.153
0.146
0.139
0.183
0.163
0.164
0.155
0.154
0.150
0.140

k
k
k
k
k
k
k

Error

Accuracy, δ

SqRel RMS RMSlog <1.25 <1.252 <1.253
0.957
1.605
0.958
1.548
1.614
0.965
0.960
1.836
0.970
1.231
0.972
1.328
0.978
1.182
1.032
0.977
0.959
1.595
0.968
1.240
0.968
1.303
1.296
0.973
0.973
1.218
0.973
1.124
0.975
1.070

0.292
0.282
0.275
0.275
0.243
0.232
0.213
0.213
0.270
0.250
0.247
0.233
0.231
0.223
0.217

6.563
6.307
6.523
6.565
5.912
5.737
5.215
5.199
6.709
6.220
6.090
5.857
5.699
5.507
5.326

0.884
0.890
0.895
0.901
0.923
0.934
0.943
0.943
0.902
0.916
0.919
0.931
0.932
0.933
0.941

0.673
0.702
0.678
0.718
0.784
0.802
0.818
0.827
0.734
0.762
0.765
0.793
0.798
0.806
0.826

Table 1: Results on Depth Estimation. Supervised methods are shown in the ﬁrst rows. Data refers to the training set:
Cityscapes (cs) and KITTI (k). Zhou el al.* shows improved results from their github page.

Error

Accuracy, δ

Net D
DispNet
DispNet
DispNet

Method
Basic
Basic + ssim
CC + ssim
CC + ssim
CC + ssim
CC + ssim

Data

k
k
k
k
k

DispResNet
DispResNet
cs+k DispResNet

Net F

-
-

FlowNetC
FlowNetC
PWC Net
PWC Net

AbsRel
0.184
0.168
0.148
0.144
0.140
0.139

SqRel RMS RMSlog <1.25 <1.252 <1.253
0.967
1.476
0.971
1.396
1.149
0.973
0.973
1.284
0.975
1.070
1.032
0.977

6.325
6.176
5.464
5.716
5.326
5.199

0.259
0.244
0.226
0.226
0.217
0.213

0.732
0.767
0.815
0.822
0.826
0.827

0.910
0.922
0.935
0.938
0.941
0.943

Table 2: Ablation studies on Depth Estimation. Joint training using Competitive Collaboration and better architectures improve
the results. The beneﬁts of CC can be seen when depth improves by using a better network for ﬂow (row 4 vs 5).

Method
ORB-SLAM (full)
ORB-SLAM (short)
Mean Odometry
Zhou et al. [38]
Mahjourian et al. [22]
Geonet [37]
DF-Net [39]
Basic (ours)
Basic + ssim (ours)
CC + ssim (ours)

Sequence 09

Sequence 10

0.014 ± 0.008
0.064 ± 0.141
0.032 ± 0.026
0.016 ± 0.009
0.013 ± 0.010
0.012 ± 0.007
0.017 ± 0.007
0.022 ± 0.010
0.017 ± 0.009
0.012 ± 0.007

0.012 ± 0.011
0.064 ± 0.130
0.028 ± 0.023
0.013 ± 0.009
0.012 ± 0.011
0.012 ± 0.009
0.015 ± 0.009
0.018 ± 0.011
0.015 ± 0.009
0.012 ± 0.008

Table 3: Results on Camera Pose Estimation.

Optical Flow Estimation. We compare the performance
of our approach with competing methods using the KITTI
2015 training set [6] to be consistent with previous work
[24, 37]. We obtain state of the art performance among joint
methods as shown in Table 4. Unsupervised ﬁne tuning

(CC-uft) by setting λM = 0.02 gives more improvements
than CC as masks now choose the best ﬂow between R and
F without being overconstrained to choose R. In contrast,
UnFlow-CSS [24] uses 3 cascaded networks to reﬁne optical
ﬂow at each stage. Geonet [37] and DF-Net [39] are more
similar to our architecture but use a larger ResNet-50 archi-
tecture. Back2Future [12] performs better than our method
in terms of outlier error, but not in terms of average end point
error due to use of additional data.

In Table 5, we observe that training the static scene recon-
structor R or moving region reconstructor F independently
leads to worse performance. This happens because R can not
reason about dynamic moving objects in the scene. Similarly
F is not as good as R for reasoning about static parts of the
scene, especially in occluded regions. Using them together,
and compositing the optical ﬂow from both as shown in
Eq. (15) leads to a large improvement in performance. More-
over, using better network architectures further improves the
performance under the CC framework. We show qualitative
results in Figure 4 and in the Appendix A.5.

12246

Method
FlowNet2 [11]
SPyNet [27]
UnFlow-C [24]
UnFlow-CSS [24]
Back2Future [12]
Back2Future* [12]
Geonet [37]
DF-Net [39]
CC (ours)
CC-uft (ours)

EPE
10.06
20.56
8.80
8.10
6.59
7.04
10.81
8.98
6.21
5.66

Train

Fl

Test
Fl
-
-

30.37 %
44.78%
28.94% 29.46%
23.27%

-

-

22.94%

24.21%

-

-
-

26.01% 25.70%
26.41%
20.93% 25.27%

-

Table 4: Results on Optical Flow. We also compare with
supervised methods (top 2 rows) that are trained on synthetic
data only; unsupervised methods specialized for optical ﬂow
(middle 3 rows) and joint methods that solve more than one
task (bottom 4 rows). * refers to our Pytorch implementation
used in our framework which gives slightly lower accuracy.

Average EPE

Method

R

F

CC
CC

Net D

DispNet

-

DispNet

DispResNet

Net F

SP

-

7.51
FlowNetC 15.32
6.35
FlowNetC
PWC Net
5.67

MP

32.75
6.20
6.16
5.04

Total

13.54
14.68
7.76
6.21

Table 5: Ablation studies on Flow estimation. SP, MP refer
to static scene and moving region pixels. EPE is computed
over KITTI 2015 training set. R, F are trained independently
without CC.

Motion Segmentation. We evaluate the estimated motion
segmentations using the KITTI 2015 training set [6] that pro-
vides ground truth segmentation for moving cars. Since our
approach does not distinguish between different semantic
classes while estimating segmentation, we evaluate segmen-
tations only on car pixels. Speciﬁcally, we only consider car
pixels and compute Intersection over Union (IoU) scores for
moving and static car pixels. In Table 6, we show the IoU
scores of the segmentation masks obtained using our tech-
nique under different conditions. We refer to the masks ob-

as ‘MaskNet’ and refer to the masks obtained with ﬂow

tained with the motion segmentation network(cid:0)Im−m+>0.5(cid:1)
consensus (cid:0)I||ν(e+,d)−u+||<λc(cid:1) as ‘Consensus’. The ﬁnal

motion segmentation masks m∗ obtained with the intersec-
tion of the above two estimates are referred to as ‘Joint’
(Eq. 14). IoU results indicate substantial IoU improvements
with ‘Joint’ masks compared to both ‘MaskNet’ and ‘Con-
sensus’ masks, illustrating the complementary nature of dif-
ferent masks. Qualitative results are shown in Figure 4 and
in the Appendix A.5.

We thank Frederik Kunstner for verifying the proofs, Clément Pinard
for his code, Georgios Pavlakos for paper revisions, Joel Janai for optical
ﬂow visualizations, and Clément Gorard for Make3d evaluation code. MJB
is a part-time employee of Amazon; has ﬁnancial interests in Amazon and

Overall

Static Car Moving Car

MaskNet
Consensus
Joint

41.64
51.52
56.94

30.56
47.30
55.77

52.71
55.74
58.11

Table 6: Motion Segmentation Results. Intersection Over
Union (IoU) scores on KITTI2015 training dataset images
computed over car pixels.

5. Conclusions and Discussion

Typically, learning to infer depth from a single image
requires training images with ground truth depth scans, and
learning to compute optical ﬂow relies on synthetic data,
which may not generalize to real image sequences. For static
scenes, observed by a moving camera, these two problems
are related by camera motion; depth and camera motion
completely determine the 2D optical ﬂow. This holds true
over several frames if the scene is static and only the camera
moves. Thus by combining depth, camera, and ﬂow estima-
tion, we can learn single-image depth by using information
from several frames during training. This is particularly
critical for unsupervised training since both depth and op-
tical ﬂow are highly ill-posed. Combining evidence from
multiple tasks and multiple frames helps to synergistically
constrain the problem. This alone is not enough, however,
as real scenes contain multiple moving objects that do not
conform to static scene geometry. Consequently, we also
learn to segment the scene into static and moving regions
without supervision. In the independently moving regions, a
generic ﬂow network learns to estimate the optical ﬂow.

To facilitate this process we introduce Competitive Col-
laboration in which networks both compete and cooperate.
We demonstrate that this results in top performance among
unsupervised methods for all subproblems. Additionally, the
moderator learns to segment the scene into static and moving
regions without any direct supervision.

Future Work. We can add small amounts of supervised
training, with which we expect to signiﬁcantly boost perfor-
mance on benchmarks, cf. [24]. We could use, for example,
sparse depth and ﬂow from KITTI and segmentation from
Cityscapes to selectively provide ground truth to different
networks. A richer segmentation network together with
semantic segmentation should improve non-rigid segmenta-
tion. For automotive applications, the depth map formula-
tion should be extended to a world coordinate system, which
would support the integration of depth information over long
image sequences. Finally, as shown in [36], the key ideas
of using layers and geometry apply to general scenes be-
yond the automotive case and we should be able to train this
method to work with generic scenes and camera motions.

Meshcapde GmbH; and received research gift funds from Intel, Nvidia,
Adobe, Facebook, and Amazon. MJB’s research was performed solely at,
and funded solely by MPI. This project was supported by NVIDIA grants.

12247

References

[1] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for opti-
cal ﬂow. International Journal of Computer Vision, 92(1):1–
31, 2011. 1

[2] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation. In
European Conference on Computer Vision, pages 611–625,
2012. 16

[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
Cityscapes dataset for semantic urban scene understanding.
In Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 6

[4] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In Proceedings of the IEEE International Conference on Com-
puter Vision, pages 2758–2766, 2015. 1, 2, 5, 14

[5] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network. In
Advances in neural information processing systems, pages
2366–2374, 2014. 1, 2, 6, 7

[6] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? the KITTI vision benchmark suite. In Con-
ference on Computer Vision and Pattern Recognition (CVPR),
2012. 1, 6, 7, 8

[7] C. Godard, O. Mac Aodha, and G. Brostow. Digging into
self-supervised monocular depth estimation. arXiv preprint
arXiv:1806.01260, 2018. 7, 16

[8] K. Greff, S. van Steenkiste, and J. Schmidhuber. Neural
expectation maximization. In Advances in Neural Information
Processing Systems, pages 6694–6704, 2017. 2

[9] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-
cnn. In Computer Vision (ICCV), 2017 IEEE International
Conference on, pages 2980–2988. IEEE, 2017. 1

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 770–778,
2016. 5, 6, 14

[11] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), volume 2, 2017. 8

[12] J. Janai, F. Güney, A. Ranjan, M. Black, and A. Geiger. Unsu-
pervised learning of multi-frame optical ﬂow with occlusions.
In Proceedings of the European Conference on Computer
Vision (ECCV), pages 690–706, 2018. 5, 7, 8, 14

[13] J. Janai, F. Güney, J. Wulff, M. Black, and A. Geiger. Slow
ﬂow: Exploiting high-speed cameras for accurate and diverse
optical ﬂow reference data. In Proceedings IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) 2017,
Piscataway, NJ, USA, July 2017. IEEE. 1

[14] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to
basics: Unsupervised learning of optical ﬂow via brightness
constancy and motion smoothness. In European Conference
on Computer Vision, pages 3–10. Springer, 2016. 1, 2

[15] A. Jepson and M. J. Black. Mixture models for optical ﬂow
computation. In Proceedings of IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 760–761. IEEE,
1993. 3

[16] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
Proceedings of the IEEE international conference on com-
puter vision, pages 2938–2946, 2015. 2

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105, 2012. 1

[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278–2324, 1998. 2, 11

[20] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from
single monocular images using deep convolutional neural
ﬁelds. IEEE transactions on pattern analysis and machine
intelligence, 38(10):2024–2039, 2016. 6, 7

[21] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recognition,
pages 3431–3440, 2015. 1

[22] R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised
learning of depth and ego-motion from monocular video using
3d geometric constraints. arXiv preprint arXiv:1802.05522,
2018. 2, 3, 4, 6, 7

[23] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Doso-
vitskiy, and T. Brox. A large dataset to train convolutional
networks for disparity, optical ﬂow, and scene ﬂow estimation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4040–4048, 2016. 2, 14

[24] S. Meister, J. Hur, and S. Roth. UnFlow: Unsupervised
learning of optical ﬂow with a bidirectional census loss. arXiv
preprint arXiv:1711.07837, 2017. 1, 2, 3, 7, 8, 14, 20

[25] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and
A. Y. Ng. Reading digits in natural images with unsupervised
feature learning. In NIPS workshop on deep learning and
unsupervised feature learning, volume 2011, page 5, 2011. 2,
11

[26] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv preprint arXiv:1704.00675,
2017. 1

[27] A. Ranjan and M. J. Black. Optical ﬂow estimation using a
spatial pyramid network. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 2, 2017. 8

[28] A. Ranjan, J. Romero, and M. J. Black. Learning human
optical ﬂow. In 29th British Machine Vision Conference, Sept.
2018. 2

[29] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interactive
foreground extraction using iterated graph cuts.
In ACM
transactions on graphics (TOG), volume 23, pages 309–314.
ACM, 2004. 5

12248

[30] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from
single monocular images. In Advances in neural information
processing systems, pages 1161–1168, 2006. 6, 16

[31] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs
for optical ﬂow using pyramid, warping, and cost volume.
arXiv preprint arXiv:1709.02371, 2017. 5

[32] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Doso-
vitskiy, and T. Brox. Demon: Depth and motion network for
learning monocular stereo. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 5, 2017. 2

[33] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar,
learning of structure and

and K. Fragkiadaki. SfM-Net:
motion from video. abs/1704.07804, 2017. 2

[34] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE transactions on image processing, 13(4):600–
612, 2004. 4, 6

[35] J. Wulff and M. J. Black. Temporal interpolation as an un-
supervised pretraining task for optical ﬂow estimation. In
German Conference on Pattern Recognition (GCPR), Oct.
2018. 2

[36] J. Wulff, L. Sevilla-Lara, and M. J. Black. Optical ﬂow in
mostly rigid scenes. In IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), July 2017. 2, 8

[37] Z. Yin and J. Shi. Geonet: Unsupervised learning of dense
depth, optical ﬂow and camera pose. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1983–1992, 2018. 2, 3, 4, 6, 7, 8, 14, 16, 18,
20

[38] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsuper-
vised learning of depth and ego-motion from video. In CVPR,
volume 2, page 7, 2017. 2, 3, 5, 6, 7, 16, 18

[39] Y. Zou, Z. Luo, and J.-B. Huang. Df-net: Unsupervised
joint learning of depth and ﬂow using cross-task consistency.
In European Conference on Computer Vision, pages 38–55.
Springer, 2018. 3, 6, 7, 8, 14, 16, 18, 20

12249

