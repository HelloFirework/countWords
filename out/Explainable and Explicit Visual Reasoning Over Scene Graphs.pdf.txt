Explainable and Explicit Visual Reasoning over Scene Graphs

Jiaxin Shi1 ∗

Hanwang Zhang2

Juanzi Li1

1Tsinghua University

2Nanyang Technological University

shijx12@gmail.com; hanwangzhang@ntu.edu.sg; lijuanzi@tsinghua.edu.cn

Abstract

We aim to dismantle the prevalent black-box neural ar-
chitectures used in complex visual reasoning tasks, into
the proposed eXplainable and eXplicit Neural Modules
(XNMs), which advance beyond existing neural module net-
works towards using scene graphs — objects as nodes and
the pairwise relationships as edges — for explainable and
explicit reasoning with structured knowledge. XNMs al-
low us to pay more attention to teach machines how to
“think”, regardless of what they “look”. As we will show
in the paper, by using scene graphs as an inductive bias,
1) we can design XNMs in a concise and ﬂexible fash-
ion, i.e., XNMs merely consist of 4 meta-types, which sig-
niﬁcantly reduce the number of parameters by 10 to 100
times, and 2) we can explicitly trace the reasoning-ﬂow
in terms of graph attentions. XNMs are so generic that
they support a wide range of scene graph implementations
with various qualities. For example, when the graphs are
detected perfectly, XNMs achieve 100% accuracy on both
CLEVR and CLEVR CoGenT, establishing an empirical
performance upper-bound for visual reasoning; when the
graphs are noisily detected from real-world images, XNMs
are still robust to achieve a competitive 67.5% accuracy on
VQAv2.0, surpassing the popular bag-of-objects attention
models without graph structures.

1. Introduction

The prosperity of A.I. — mastering super-human skills
in game playing [22], speech recognition [1], and image
recognition [8, 20] — is mainly attributed to the “winning
streak” of connectionism, more speciﬁcally, the deep neu-
ral networks [15], over the “old-school” symbolism, where
their controversy can be dated back to the birth of A.I. in
1950s [18]. With massive training data and powerful com-
puting resources, the key advantage of deep neural networks
is the end-to-end design that generalizes to a large spec-

∗The work was done when Jiaxin Shi was an intern at Nanyang Tech-

nological University.

!purple cylinder
large metal "

Scene Graph

left, front
right, behind

r

i

g

l

e

f

t
,
 

b

e

h

t
,
 
f

r

o

n

t

h

i

n

d

right, behind

left, front

!brown cylinder
large metal "

cube

! red
large rubber"

AttendNode

AttendEdge

Transfer

Logic

And, Or, Not

X Neural Modules

Q: How many brown metal 
cylinders have the same size 
as the purple cylinder?

A:1

A:horse

Q: What is the man riding 
on?

Figure 1: The ﬂowchart of using the proposed XNMs rea-
soning over scene graphs, which can be represented by de-
tected one-hot class labels (left) or RoI feature vectors (col-
ored bars on the right). Feature colors are consistent with
the bounding box colors. XNMs have 4 meta-types. Red
nodes or edges indicate attentive results. The ﬁnal mod-
ule assembly can be obtained by training an off-the-shelf
sequence-to-sequence program generator [13].

trum of domains, minimizing the human efforts in domain-
speciﬁc knowledge engineering. However, large gaps be-
tween human and machines can be still observed in “high-
level” vision-language tasks such as visual Q&A [4, 6, 12],
which inherently requires composite reasoning (cf. Fig-
ure 1). In particular, recent studies show that the end-to-end
models are easily optimized to learn the dataset “shortcut
bias” but not reasoning [12].

18376

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
Neural module networks (NMNs) [3, 12, 10, 17, 9, 26]
show a promising direction in conferring reasoning abil-
ity for the end-to-end design by learning to compose the
networks on-demand from the language counterpart, which
implies the logical compositions. Take the question “How
many objects are left of the red cube?” as an example, we
can program the reasoning path into a composition of func-
tional modules [17]: Attend[cube], Attend[red],
Relate[left], and Count, and then execute them with
the input image. We attribute the success of NMNs to the
eXplainable and eXplicit (dubbed X) language understand-
ing. By explicitly parsing the question into an explainable
module assembly, NMNs effectively prevent the language-
to-reasoning shortcut, which are frequent when using the
implicit fused question representations [4, 6] (e.g., the an-
swer can be directly inferred according to certain language
patterns).

However, the vision-to-reasoning shortcut still exists as
an obstacle on the way of NMNs towards the real X visual
reasoning. This is mainly because that the visual percep-
tion counterpart is still attached to reasoning [17], which
is inevitably biased to certain vision patterns. For exam-
ple, on the CLEVR CoGenT task, which provides novel
object attributes to test the model’s generalization ability
(e.g., cubes are blue in the training set but red in the test
set), we observe signiﬁcant performance drop of existing
NMNs [13, 17] (e.g., red cubes in the test set cannot be
recognized as “cube”). Besides, the reusability of the cur-
rent module design is limited. For example, the network
structure of the Relate module in [17] must be carefully
designed using a series of dilated convolutions to achieve
good performance. Therefore, how to design a complete
inventory of X modules is still an tricky engineering.

In this paper, we advance NMN towards X visual reason-
ing by using the proposed eXplainable and eXplicit Neural
Modules (XNMs) reasoning over scene graphs. By do-
ing this, we can insulate the “low-level” visual perception
from the modules, and thus can prevent reasoning shortcut
of both language and vision counterpart. As illustrated in
Figure 1, a scene graph is the knowledge representation of
a visual input, where the nodes are the entities (e.g., cylin-
der, horse) and the edges are the relationships between en-
tities (e.g., left, ride).
In particular, we note that scene
graph detection per se is still a challenging task in com-
puter vision [28], therefore, we allow XNMs to accept scene
graphs with different detection qualities. For example, the
left-hand side of Figure 1 is one extreme when the visual
scene is clean and closed-vocabulary, e.g., in CLEVR [12],
we can have almost perfect scene graphs where the nodes
and edges can be represented by one-hot class labels; the
right-hand side shows another extreme when the scene is
cluttered and open-vocabulary in practice, the best we have
might be merely a set of object proposals. Then, the nodes

are RoI features and the edges are their concatenations.

Thanks to scene graphs, our XNMs only have 4 meta-
types: 1) AttendNode, ﬁnding the queried entities,
2) AttendEdge, ﬁnding the queried relationships, 3)
Transfer, transforming the node attentions along the at-
tentive edges, and 4) Logic, performing basic logical op-
erations on attention maps. All types are fully X as their
outputs are pure graph attentions that are easily traceable
and visible. Moreover, these meta modules are only spe-
ciﬁc to the generic graph structures, and are highly reusable
to constitute different composite modules for more complex
functions. For example, we do not need to carefully design
the internal implementation details for the module Relate
as in [17]; instead, we only need to combine AttendEdge
and Transfer in XNMs.

We conduct extensive experiments on two visual Q&A
benchmarks and demonstrate the following advantages of
using XNMs reasoning over scene graphs:

1. We achieve 100% accuracy by using the ground-truth
scene graphs and programs on both CLEVR [12]
and CLEVR-CoGent, revealing the performance upper-
bound of XNMs, and the beneﬁts of disentangling
“high-level” reasoning from “low-level” perception.

2. Our network requires signiﬁcantly less parameters while
achieves better performance than previous state-of-the-
art neural module networks, due to the conciseness and
high-reusability of XNMs.

3. XNMs are ﬂexible to different graph qualities, e.g., it
achieves competitive accuracy on VQAv2.0 [6] when
scene graphs are noisily detected.

4. We show qualitative results to demonstrate that our

XNMs reasoning is highly explainable and explicit.

2. Related Work

It

Visual Reasoning.

is the process of analyzing
visual
information and solving problems based on it.
The most representative benchmark of visual reasoning is
CLEVR [12], a diagnostic visual Q&A dataset for compo-
sitional language and elementary visual reasoning. The ma-
jority of existing methods on CLEVR can be categorized
into two families: 1) holistic approaches [12, 21, 19, 11],
which embed both the image and question into a feature
space and infer the answer by feature fusion; 2) neural mod-
ule approaches [3, 10, 13, 17, 9, 26], which ﬁrst parse the
question into a program assembly of neural modules, and
then execute the modules over the image features for visual
reasoning. Our XNM belongs to the second one but replaces
the visual feature input with scene graphs.

Neural Module Networks. They dismantle a complex
question into several sub-tasks, which are easier to answer
and more transparent to follow the intermediate outputs.
Modules are pre-deﬁned neural networks that implement
the corresponding functions of sub-tasks, and then are as-

28377

sembled into a layout dynamically, usually by a sequence-
to-sequence program generator given the input question.
The assembled program is ﬁnally executed for answer pre-
diction [10, 13, 17]. In particular, the program generator is
trained based on the human annotations of desired layout
or with the help of reinforcement learning due to the non-
differentiability of layout selection. Recently, Hu et al. [9]
proposed StackNMN, which replaces the hard-layout with
soft and continuous module layout and performs well even
without layout annotations at all. Our XNM experiments on
VQAv2.0 follows their soft-program generator.

Recently, NS-VQA [26] ﬁrstly built the reasoning over
the object-level structural scene representation, improving
the accuracy on CLEVR from the previous state-of-the-art
99.1% [17] to an almost perfect 99.8%. Their scene struc-
ture consists of objects with detected labels, but lacked the
relationships between objects, which limited its application
on real-world datasets such as VQAv2.0 [6].
In this pa-
per, we propose a much more generic framework for visual
reasoning over scene graphs, including object nodes and re-
lationship edges represented by either labels or visual fea-
tures. Our scene graph is more ﬂexible and more powerful
than the table structure of NS-VQA.

Scene Graphs. This task is to produce graph represen-
tations of images in terms of objects and their relationships.
Scene graphs have been shown effective in boosting several
vision-language tasks [14, 24, 27, 5]. To the best of our
knowledge, we are the ﬁrst to design neural module net-
works that can reason over scene graphs. However, scene
graph detection is far from satisfactory compared to object
detection [25, 28, 16]. To this end, our scene graph im-
plementation also supports cluttered and open-vocabulary
in real-world scene graph detection, where the nodes are
merely RoI features and the edges are their concatenations.

3. Approach

We build our neural module network over scene graphs
to tackle the visual reasoning challenge. As shown in Fig-
ure 2, given an input image and a question, we ﬁrst parse
the image into a scene graph and parse the question into
a module program, and then execute the program over the
scene graph.
In this paper, we propose a set of generic
base modules that can conduct reasoning over scene graphs
— eXplainable and eXplicit Neural Modules (XNMs) —
as the reasoning building blocks. We can easily assemble
these XNMs to form more complex modules under speciﬁc
scenarios. Besides, our XNMs are totally attention-based,
making all the intermediate reasoning steps transparent.

3.1. Scene Graph Representations

We formulate the scene graph of an image as (V, E),
where V = {v1, · · · , vN } are graph nodes corresponding
to N detected objects, and vi denotes the feature represen-
tation of the i-th object. E = {eij|i, j = 1, · · · , N } are

graph edges corresponding to relations between each object
pairs, and eij denotes the feature representation of the rela-
tion from object i to object j (Note that edges are directed).
Our XNMs are generic for scene graphs of different qual-
ity levels of detection. We consider two extreme settings in
this paper. The ﬁrst is the ground-truth scene graph with la-
bels, denoted by GT, that is, using ground-truth objects as
nodes, ground-truth object label embeddings as node fea-
tures, and ground-truth relation label embeddings as edge
features.
In this setting, scene graphs are annotated with
ﬁxed-vocabulary object labels and relationship labels, e.g.,
deﬁned in CLEVR dataset [12]. We collect all the C labels
into a dictionary, and use an embedding matrix D ∈ RC×d
to map a label into a d-dimensional vector. We represent
the nodes and edges using the concatenation of their corre-
sponding label embeddings.

The second setting is totally detected and label-agnostic,
denoted by Det, that is, using detected objects as nodes, RoI
visual features as node features, and the fusion of two node
features as the edge features. For example, the edge fea-
tures can be represented by concatenating the two related

node features, i.e., eij = (cid:2)vi; vj(cid:3). As an another example,

in CLEVR where the edges are only about spatial relation-
ships, we use the difference between detected coordinates
of object pairs as the edge embedding. More details are in
Section 4.

We use the GT setting to demonstrate the performance
upper-bound of our approach when a perfect scene graph
detector is available along with the rapid development of
visual recognition, and use the Det setting to demonstrate
the practicality in open domains.

3.2. X Neural Modules

As shown in Figure 1, our XNMs have four meta-types
and are totally attention-based. We denote the node atten-
tion weight vector by a ∈ [0, 1]N and the weight of the i-th
node by ai. The edge attention weight matrix is denoted by
W ∈ [0, 1]N ×N , where Wij represents the weight of edge
from node i to node j.

AttendNode[query]. This most basic and intuitive
operation is to ﬁnd the relevant objects given an input query
(e.g., ﬁnd all [“cubes”]). For the purpose of semantic com-
putation, we ﬁrst encode the query into a vector q. This X
module takes the query vector as input, and produces the
node attention vector by the following function:

a = f (V, q).

(1)

The implementation of f is designed according to a speciﬁc
scene graph representation, as long as f is differentiable and
range(f ) = [0, 1].

AttendEdge[query]. Though object attention is a
widely-used mechanism for better visual understanding, it
is unable to capture the interaction between objects and thus

38378

!purple cylinder
large metal "

scene graph

parsing

left, front
right, behind

r

i

g

l

e

f

t
,
 

h

t
,
 
f

r

b

o

e

n

t

h

i

n

d

right, behind

left, front

!brown cylinder
large metal "

cube

! red
large rubber"

Attend[cylinder]

Attend[metal]

Attend[brown]

program

Relate[right]

generation

Q: How many objects are right 
of the brown metal cylinder and 
left of the red cube?

Attend[cube]

Attend[red]

Relate[left]

And

Count

Scene

Attend[cube]

Attend[red]

Relate[left]

Scene

Attend[cylinder]

Attend[metal]

Attend[brown]

Relate[right]

0 + 1 + 0 = 1

And

Count

Figure 2: To answer a question about an image, we need to 1) parse the image into a scene graph, 2) parse the question into
a module program, and 3) reasoning over the scene graph. Here, we show the reasoning details of an example from CLEVR.
The nodes and edges in red are attended. Scene is a dummy placeholder module that attends all nodes. All intermediate
steps of our XNMs are explainable and explicit.

is weak in the complex visual reasoning [29]. This X mod-
ule aims to ﬁnd the relevant edges given an input query (e.g.,
ﬁnd all edges that are [“left”]). After encoding the query
into q, we compute the edge attention matrix by the follow-
ing function:

W = g(E, q),

(2)

where g is deﬁned according to a speciﬁc scene graph rep-
resentation, as long as g is differentiable and range(g) =
[0, 1].

Transfer. With the node attention vector a and the
edge attention matrix W, we can transfer the node weights
along the attentive relations to ﬁnd new objects (e.g., ﬁnd
objects that are [“left”] to the [“cube”]). Thanks to the graph
structure, to obtain the updated node attention a′, we merely
need to perform a simple matrix multiplication:

a′ = norm(W⊤a),

(3)

where norm assert the values in [0, 1] by dividing the max-
imum value if any entry exceeds 1. Here, Wij indicates
how many weights will ﬂow from object i to object j, and
a′
j=1 Wjiaj is the total received weights of object i.
This module reallocates node attention in an efﬁcient and
fully-differentiable manner.

i = PN

Logic. Logical operations are crucial in complex reason-
ing cases. In XNM, logical operations are performed on one
or more attention weights to produce a new attention. We
deﬁne three logical X modules: And, Or, and Not. With-
out loss of generality, we discuss all these logical modules
on node attention vectors, and the extension to edge atten-
tion is similar. The And and Or modules are binary, that

is, take two attentions as inputs, while the Not module is
unary. The implementation of these logical X modules are
as follows:

And(a1, a2) = min(a1, a2), Not(a) = 1 − a,
Or(a1, a2) = max(a1, a2).

(4)

These four meta-types of XNMs constitute the base of
our graph reasoning. They are explicitly executed on at-
tention maps, and all intermediate results are explainable.
Besides, these X modules are totally differentiable. We can
ﬂexibly assemble them into composite modules for more
complex functions, which can be still trained end-to-end.

3.3. Implementations

To apply XNMs in practice, we need to consider these
questions: (1) How to implement the attention functions
f, g in Eq. (1) and Eq. (2)? (2) How to compose our X mod-
ules into composite reasoning modules? (3) How to predict
the answer according to the attentive results? (4) How to
parse the input question to an executable module program?

3.3.1 Attention Functions

We use different attention functions for different scene
graph settings. In the GT setting, as annotated labels are
mostly mutually exclusive (e.g., “red” and “green”), we
compute the node attention using the softmax function over
the label space. Speciﬁcally, given a query vector q ∈ Rd,
we ﬁrst compute its attention distribution over all labels by
b = softmax(D · q), where length(b) = C and bc rep-
resents the weight of the c-th label. Then we capture the

48379

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Z
V
8
Q
T
F
G
5
R
T
Z
/
G
4
q
R
/
y
u
4
F
+
Z
g
n
u
4
=
"
>
A
A
A
C
W
3
i
c
b
V
F
N
S
8
N
A
E
N
2
k
a
m
v
8
q
o
o
n
L
6
t
F
8
V
Q
S
E
f
R
Y
9
O
J
R
w
a
r
Q
l
L
L
Z
T
O
v
S
z
S
b
s
T
q
Q
l
5
E
9
6
0
o
N
/
R
d
z
W
I
F
p
9
y
8
L
j
z
R
t
m
9
m
2
U
S
W
H
Q
9
9
8
c
t
7
a
0
v
F
J
v
r
H
p
r
6
x
u
b
W
8
3
t
n
X
u
T
5
p
p
D
l
6
c
y
1
Y
8
R
M
y
C
F
g
i
4
K
l
P
C
Y
a
W
B
J
J
O
E
h
G
l
/
N
6
g
/
P
o
I
1
I
1
R
1
O
M
+
g
n
b
K
T
E
U
H
C
G
V
h
o
0
d
R
j
B
S
K
g
i
S
h
h
q
M
S
k
9
a
h
E
i
T
L
D
I
c
p
1
J
K
O
l
x
e
D
A
7
l
c
q
n
d
l
Q
M
u
q
R
h
+
M
M
s
m
R
5
Z
7
4
I
5
A
W
S
y
p
F
4
I
K
v
4
e
M
W
i
2
/
L
Y
/
B
/
1
L
g
o
q
0
S
I
W
b
Q
f
M
l
j
F
O
e
J
6
C
Q
S
2
Z
M
L
/
A
z
7
B
d
M
o
+
B
2
Q
S
/
M
D
W
S
M
j
9
k
I
e
p
Y
q
l
o
D
p
F
/
N
s
S
n
p
k
l
Z
g
O
U
2
2
v
Q
j
p
X
f
3
Y
U
L
D
F
m
m
k
T
W
a
f
d
7
M
o
u
1
m
f
h
f
r
Z
f
j
8
K
J
f
C
J
X
l
C
I
p
/
D
R
r
m
k
m
J
K
Z
0
H
T
W
G
j
g
K
K
e
W
M
K
6
F
3
Z
X
y
J
6
Y
Z
R
/
s
d
n
g
0
h
W
H
z
y
X
3
J
/
2
g
4
s
v
z
1
r
d
S
6
r
O
B
p
k
n
x
y
S
E
x
K
Q
c
9
I
h
1
+
S
G
d
A
k
n
r
+
T
D
q
T
s
N
5
9
2
t
u
Z
6
7
/
m
V
1
n
a
p
n
l
/
y
C
u
/
c
J
P
W
a
z
W
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
C
i
J
G
k
U
z
V
J
Y
1
U
X
7
t
3
q
O
p
K
M
F
c
J
T
o
=
"
>
A
A
A
C
W
n
i
c
b
V
F
d
S
8
M
w
F
E
2
r
b
r
N
+
z
Y
8
3
X
6
J
D
8
W
m
0
I
u
i
j
6
I
u
P
E
5
w
K
6
x
h
p
e
j
e
D
a
V
q
T
W
3
W
U
/
k
l
f
R
P
C
v
C
G
a
z
i
E
5
P
C
B
z
O
P
Z
d
7
c
x
J
l
U
h
j
0
/
T
f
H
n
Z
t
f
q
N
U
b
i
9
7
S
8
s
r
q
W
n
N
9
4
9
q
k
u
e
b
Q
5
a
l
M
9
W
3
E
D
E
i
h
o
I
s
C
J
d
x
m
G
l
g
S
S
b
i
J
7
s
8
n
9
Z
t
H
0
E
a
k
6
g
r
H
G
f
Q
T
N
l
J
i
K
D
h
D
K
w
2
a
D
2
E
E
I
6
G
K
K
G
G
o
x
X
P
p
U
Y
s
Q
4
R
m
L
S
K
d
P
q
q
T
7
4
c
7
k
V
C
I
f
2
0
k
x
6
J
K
G
4
Q
+
v
Z
H
o
E
J
Z
0
x
J
4
B
M
l
t
Q
L
Q
c
X
f
E
w
b
N
l
t
/
2
p
6
B
/
S
V
C
R
F
q
n
Q
G
T
R
f
w
j
j
l
e
Q
I
K
u
W
T
G
9
A
I
/
w
3
7
B
N
A
o
u
o
f
T
C
3
E
D
G
+
D
0
b
Q
c
9
S
x
R
I
w
/
W
I
a
T
U
n
3
r
B
L
T
Y
a
r
t
V
U
i
n
6
s
+
O
g
i
X
G
j
J
P
I
O
u
1
+
d
2
a
2
N
h
H
/
q
/
V
y
H
J
7
0
C
6
G
y
H
E
H
x
r
0
H
D
X
F
J
M
6
S
R
n
G
g
s
N
H
O
X
Y
E
s
a
1
s
L
t
S
f
s
c
0
4
2
h
/
w
7
M
h
B
L
N
P
/
k
u
u
D
9
u
B
5
Z
d
H
r
d
O
z
K
o
4
G
2
S
a
7
5
I
A
E
5
J
i
c
k
g
v
S
I
V
3
C
y
S
v
5
c
G
p
O
3
X
l
3
X
X
f
R
X
f
q
y
u
k
7
V
s
0
l
+
w
d
3
6
B
D
3
r
s
u
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
l
b
R
p
z
+
e
Y
C
r
m
K
p
c
p
G
c
b
x
d
l
x
S
N
n
M
=
"
>
A
A
A
C
V
X
i
c
b
V
F
d
S
8
M
w
F
E
3
r
d
/
2
a
+
u
h
L
d
C
g
+
S
S
u
C
P
g
5
9
8
V
H
B
O
W
E
d
I
0
n
v
Z
j
B
N
S
3
I
r
j
t
I
/
u
R
f
x
n
/
g
i
m
M
0
i
u
n
l
C
4
H
D
u
u
c
n
N
C
c
+
V
t
B
i
G
7
5
6
/
s
L
i
0
v
L
K
6
F
q
x
v
b
G
5
t
N
3
Z
2
H
2
x
W
G
A
F
t
k
a
n
M
P
H
J
m
Q
U
k
N
b
Z
S
o
4
D
E
3
w
F
K
u
o
M
O
f
r
y
f
1
z
g
s
Y
K
z
N
9
j
6
M
c
e
i
k
b
a
j
m
Q
g
q
G
T
+
g
0
V
c
x
h
K
X
f
K
U
o
Z
G
v
V
U
A
d
Y
o
R
X
L
A
0
k
F
T
2
O
D
y
a
r
l
k
T
B
o
a
J
x
/
M
u
l
m
B
k
6
b
c
Z
o
C
s
7
B
V
D
S
I
Q
S
c
/
h
/
c
b
z
f
A
0
n
I
L
O
k
6
g
m
T
V
L
j
t
t
8
Y
x
0
k
m
i
h
Q
0
C
s
W
s
7
U
Z
h
j
r
2
S
G
Z
R
C
Q
R
X
E
h
Y
W
c
i
W
c
2
h
K
6
j
m
q
V
g
e
+
U
0
l
Y
o
e
O
S
W
h
g
8
y
4
r
Z
F
O
1
d
8
d
J
U
u
t
H
a
X
c
O
d
1
8
T
3
a
2
N
h
H
/
q
3
U
L
H
F
z
2
S
q
n
z
A
k
G
L
7
4
s
G
h
a
K
Y
0
U
n
E
N
J
E
G
B
K
q
R
I
0
w
Y
6
W
a
l
4
o
k
Z
J
t
B
9
R
O
B
C
i
G
a
f
P
E
8
e
z
k
4
j
x
+
/
O
m
6
2
r
O
o
5
V
s
k
8
O
y
Q
m
J
y
A
V
p
k
R
t
y
S
9
p
E
k
D
H
5
8
D
z
P
9
9
6
8
T
3
/
R
X
/
6
2
+
l
7
d
s
0
f
+
w
N
/
+
A
l
7
B
s
X
U
=
<
/
l
a
t
e
x
i
t
>
node and edge attention by summing up corresponding la-
bel weights:

ai = f (V, q)i = Xc∈Ci

bc, Wij = g(E, q)ij = Xc∈Cij

bc,

(5)
where Ci and Cij denote the (multi-) labels of node i and
edge ij respectively.

In the Det setting, we use the sigmoid function to com-
pute the attention weights. Given the query q ∈ Rd, the
node and edge attentions are:

color, M2 ¯v represents the shape, etc.), where K is a hyper-
parameter related to the speciﬁc scene graph vocabulary.
The output feature is computed by

Describe(a, q) =

K

Xk=1

ck(Mk ¯v),

(8)

where c = Softmax(MLP(q)) represents a probability dis-
tribution over these K aspects, and ck denotes the k-th prob-
ability. The mapping matrixes can be learned end-to-end
automatically.

(6)

Table 1: Our composite modules (the top section) and out-
put modules (the bottom section). MLP() consists of sev-
eral linear and ReLU layers.

ai = f (V, q)i = sigmoid(cid:0)MLP(vi)⊤q(cid:1) ,
Wij = g(E, q)ij = sigmoid(cid:0)MLP(eij)⊤q(cid:1) ,

where the MLP maps vi and eij to the dimension d.

3.3.2 Composite Reasoning Modules

We list our composite reasoning modules and their imple-
mentations (i.e., how they are composed by basic X mod-
ules) in the top section of Table 1. For example, Same mod-
ule is to ﬁnd other objects that have the same attribute value
as the input objects (e.g., ﬁnd other objects with the same
[“color”]).
In particular, Describe used in Same is to
obtain the corresponding attribute value (e.g., describe one
object’s [“color”]), and will be introduced in the following
section.

3.3.3 Feature Output Modules

Besides the above reasoning modules, we also need an-
other kind of modules to map the intermediate attention
to a hidden embedding h for feature representation, which
is fed into a softmax layer to predict the ﬁnal answer, or
into some modules for further reasoning. We list our out-
put modules in the bottom section of Table 1. Exist and
Count sum up the node attention weights to answer yes/no
and counting questions. Compare is for attribute or num-
ber comparisons, which takes two hidden features as in-
puts. Describe[query] is to transform the attentive
node features to an embedding that describes the speciﬁed
attribute value (e.g., what is the [“color”] of attended ob-
jects).

To implement the Describe module, we ﬁrst obtain

the “raw” attentive node feature by

¯v =

N

Xi=1

aivi.

N

Xi=1

ai,

(7)

and then project it into several “ﬁne-grained” sub-spaces
— describing different attribute aspects such as color and
shape — using different transformation matrices. Specif-
ically, we deﬁne K projection matrices M1, · · · , MK to
map ¯v into different aspects (e.g., M1 ¯v represents the

Modules

In → Out

Intersect

Union

Filter

Same

Relate

Exist

Count

, a2
, a2

→ a′
a1
→ a′
a1
a, q → a′
a, q → a′
a, q → a′

Implementation
And(a1
, a2)
, a2)
Or(a1

And(a, AttendNode(q))

Filter(Not(a), Describe(a, q))

Transfer(a, AttendEdge(q))

a → h

MLP(Pi ai)

Compare

h1

, h2

→ h′

MLP(h1

− h2)

Describe

a, q → h

Eq. (8)

3.3.4 Program Generation & Training

For datasets that have ground-truth program annotations
(e.g., CLEVR), we directly learn an LSTM sequence-to-
sequence model [23] to convert the word sequence into the
module program. However, there is no layout annotations in
most real-world datasets (e.g., VQAv2.0). In this case, fol-
lowing StackNMN [9], we make soft module selection with
a differentiable stack structure. Please refer to their papers
for more details.

We feed our output features from modules (cf. Table 1)
into a softmax layer for the answer prediction. We use
the cross entropy loss between our predicted answers and
ground-truth answers to train our XNMs.

4. Experiments

4.1. CLEVR

Settings. The CLEVR dataset [12] is a synthetic diag-
nostic dataset that tests a range of visual reasoning abilities.
In CLEVR, images are annotated with ground-truth object
positions and labels, and questions are represented as func-
tional programs that consists of 13 kinds of modules. Ex-
cept the “Unique” module, which does not have actual op-
eration, all the remaining 12 modules can correspond to our
modules in Table 1. CLEVR modules “Equal attribute”,
“Equal integer”, “Greater than” and “Less than” have the
same implementation as our Compare, but with different

58380

Table 2: Comparisons between neural module networks on the CLEVR dataset. Top section: results of the ofﬁcial test set;
Bottom section: results of the validation set (we can only evaluate our GT setting on the validation set since the annotations
of the test set are not public [12]). The program option “scratch” means totally without program annotations, “supervised”
means using trained end-to-end parser, and “GT” means using ground-truth programs. Our reasoning modules are composed
with highly-reusable X modules, leading to a very small number of parameters. Using the ground-truth scene graphs and
programs, we can achieve a perfect reasoning on all kinds of questions.

Method

Program

#Modules

#Param.

Overall

Count

Compare

Numbers

Exist

Query

Compare

Attribute

Attribute

Human [12]

-

N2NMN [10]

scratch

N2NMN [10]

supervised

PG+EE [13]

supervised

TbD-net [17]

supervised

StackNMN [9]

scratch

StackNMN [9]

supervised

XNM-Det

supervised

NS-VQA [26]

supervised

XNM-Det

supervised

XNM-Det

GT

XNM-GT

supervised

XNM-GT

GT

-

12

12

39

39

9

9

12

12

12

12

12

12

-

-

-

40.4M

115M

7.32M

7.32M

0.55M

-

0.55M

0.55M

0.22M

0.22M

92.6

69.0

83.7

96.9

99.1

93.0

96.5

97.7

99.8

97.8

97.9

99.9

100

86.7

86.4

96.6

95.0

96.0

-

-

92.7

97.6

-

-

96.0

99.7

96.0

96.2

99.9

100

-

-

98.7

99.4

-

-

98.0

99.9

98.1

98.1

99.9

100

-

-

97.1

99.2

-

-

98.7

99.9

98.6

98.8

99.8

100

-

-

98.1

99.5

-

-

98.4

99.8

98.7

98.7

99.8

100

-

-

98.9

99.6

-

-

97.6

99.8

97.8

97.8

99.9

100

parameters. There are 4 attribute categories in CLEVR, so
we set the number of mapping matrixes K = 4.

We reused the trained sequence-to-sequence program
generator of [13, 17], which uses preﬁx-order traversal to
convert the program trees to sequences. Note that their
modules are bundled with input, e.g., they regard Filter[red]
and Filter[green] as two different modules. This will cause
serious sparseness in the real-world case. We used their pro-
gram generator, but unpack the module and the input (e.g.,
Filter[red] and Filter[green] are the same module with dif-
ferent input query).

In the GT setting, we performed reasoning over the
ground-truth scene graphs. In the Det setting, we built the
scene graphs by detecting objects and using RoI features
as node embeddings and the differences between detected
coordinates as edge embeddings. Since CLEVR does not
provide the bounding box or segmentation annotations of
objects, it is hard to directly train an object detector. NS-
VQA [26] trained a Mask R-CNN [7] for object segmenta-
tion by “hacking” the rendering process [12], which could
perform very well due to the simplicity of visual scenes of
CLEVR. However, as we expected to explore X modules
in a noisier case, we chose the trained attention modules of
TbD-net [17] as our object detector. Speciﬁcally, we enu-
merated all possible combinations of object attributes (e.g.,
red, cube, metal, large), and tried to ﬁnd corresponding ob-
jects using their attention modules (e.g., intersection of the

output mask of Attend[red], Attend[cube], Attend[metal]
and Attend[large], and then regarded each clique as a single
object). The detected results have some frequent mistakes,
such as inaccurate position, wrongly merged nodes (two ad-
jacent objects with the same attribute values are recognized
as one). These detection noises allow us to test whether our
XNMs are robust enough.

Goals. We expect to answer the following questions ac-
cording to the CLEVR experiments: Q1: What is the upper
bound of our X reasoning when both the vision and lan-
guage perceptions are perfect? Q2: Are our XNMs robust
for noisy detected scene graphs and parsed programs? Q3:
What are the parameter and data efﬁciency, and the conver-
gence speed of XNMs? Q4: How is the explainability of
XNMs?

Results. Experimental results are listed in Table 2.
A1: When using the ground-truth scene graphs and pro-
grams, we can achieve 100% accuracy, indicating an in-
spiring upper-bound of visual reasoning. By disentangling
“high-level” reasoning from “low-level” perception and us-
ing XNMs, we may eventually conquer the visual reasoning
challenge with the rapid development of visual recognition.

A2: With noisy detected scene graphs, we can still
achieve a competitive 97.9% accuracy using the ground-
truth programs, indicating that our X reasoning are robust
to different quality levels of scene graphs. When replacing
the ground-truth programs with parsed programs, the accu-

68381

racy drops by 0.1% in both GT and Det settings, which is
caused by minor errors of the program parser.

A3: Due to the conciseness and high-reusability of X
modules, our model requires signiﬁcantly less parameters
than existing models. Our GT setting only needs about
0.22M parameters, taking about 500MB memory with batch
size of 128, while PG+EE [13] and TbD-net [17] bundle
modules and inputs together, leading to a large number of
modules and parameters.

Figure 3: Comparison of data efﬁciency and convergence
speed.

To explore the data efﬁciency, we trained our model with
a partial training set and evaluated on the complete valida-
tion set. Results are displayed in the left part of Figure 3.
We can see that our model performs much better than other
baselines when the training set is small. Especially, our GT
setting can still achieve a 100% accuracy even with only
10% training data. The right part shows the accuracy at each
training epoch. We can see that our X reasoning converges
very fast.

A4: As our XNMs are attention-based, the reasoning
process is totally transparent and we can easily show inter-
mediate results. Figure 4 displays two examples of CLEVR.
We can see all reasoning steps are clear and intuitive.

4.2. CLEVR CoGenT

Settings. The CLEVR-CoGenT dataset is a benchmark
to study the ability of models to recognize novel combi-
nations of attributes at test-time, which is derived from
CLEVR but has two different conditions: in Condition A
all cubes are colored one of gray, blue, brown, or yellow,
and all cylinders are one of red, green, purple, or cyan; in
Condition B the color palettes are swapped. The model is
trained using the training set of Condition A, and then is
tested using Condition B to check whether it can generalize
well to the novel attribute combinations. We train our model
on the training set of Condition A, and report the accuracy
of both conditions.

Goals. Q1: Can our model perform well when meeting
the novel attribute combinations? Q2: If not, what actually

Table 3: Comparisons between NMNs on CLEVR-
CoGenT. Top section: results of the test set; Bottom sec-
tion: results of the validation set. Using the ground-truth
scene graphs, our XNMs generalize very well and do not
suffer from shortcuts at all.

Method

Program

Condition A Condition B

PG+EE [13]

supervised

TbD-net [17]

supervised

XNM-Det

supervised

NS-VQA [26]

supervised

XNM-Det

supervised

XNM-Det

GT

XNM-GT

supervised

XNM-GT

GT

96.6

98.8

98.1

99.8

98.2

98.3

99.9

100

73.7

75.4

72.6

63.9

72.1

72.2

99.9

100

causes the reasoning shortcut?

Results. Results of CLEVR-CoGenT are displayed in
Table 3. A1: When using the ground-truth scene graphs,
our XNMs perform perfectly on both Condition A and Con-
dition B. Novel combinations of attributes in Condition B
do not cause the performance drop at all. However, when
using the detected scene graphs, where node embeddings
are RoI features that fuse all attribute values, our generaliza-
tion results on Condition B drops to 72.1%, suffering from
the dataset shortcut just like other existing models [13, 17].

Filter
[purple]

Filter
[cube]

Filter

[cylinder]

Query

[shape]

cylinder

(a)

(b)

(c)

Figure 5: Failure cases of our Det setting on Condition B of
CoGenT.

A2: Figure 5 shows some typical failure cases of our
Det setting on Condition B. In case (a), our model cannot
recognize purple cubes as “cube” because all cubes are col-
ored one of gray, blue, brown, or yellow in the training data.
Similarly, in case (b) and (c), whether an object is recog-
nized as a “cube” or a “cylinder” by our model is actually
determined by its color. However, in our GT setting, which
is given the ground-truth visual labels, we can achieve a
perfect performance. This gap reveals that the challenge of
CLEVR-CoGenT mostly comes from the vision bias, rather
than the reasoning shortcut.

78382

0.10.20.30.40.5trainingratio5060708090100accuracy(%)Accuracywhentrainedwithdiﬀerentratiosoftrainingdata.12345678910epoch5060708090100accuracy(%)Accuracyatdiﬀerentepochs.TbD-netStackNMNXNMs-DetXNMs-GTFilter
[large]

Filter
[gray]

Filter
[cube]

Relate
[behind]

Filter
[large]

Filter
[small]

Filter
[brown]

Filter
[rubber]

Filter
[sphere]

Union

Count

Anser: 3

Anser:cyan

Filter
[small]

Filter
[yellow]

Filter
[metal]

Same

[shape]

Filter
[small]

Filter
[metal]

Query
[color]

Figure 4: Reasoning visualizations of two CLEVR samples. Question 1: What number of objects are either big objects that
are behind the big gray block or tiny brown rubber balls? Question 2: The other small shiny thing that is the same shape
as the tiny yellow shiny object is what color? We plot a dot for each object and darker (red) dots indicate higher attention
weights.

4.3. VQAv2.0

Settings. VQAv2.0 [6] is a real-world visual Q&A
dataset which does not have annotations about scene graphs
and module programs. We used the grounded visual fea-
tures of [2] as node features, and concatenated node em-
beddings as edge features. We set K = 1 and fused the
question embedding with our output feature for answer pre-
diction. Following [2], we used softmax over objects for
node attention computation.

Goals. We used VQAv2.0 to demonstrate the generality

and robustness of our model in the practical case.

Results. We list the results in Table 4. We follow Stack-
NMN [9] to build the module program in a stacked soft
manner, but our model can achieve better performance as
our reasoning over scene graphs is more powerful than their
pixel-level operations.

that

Recall

[13, 17] are not applicable in open-
vocabulary input, and [26] relies on the ﬁxed label represen-
tation, so it is hard to apply them on practical datasets. In
contrast, our XNMs are ﬂexible enough for different cases.

5. Conclusions

In this paper, we proposed X neural modules (XNMs)
that allows visual reasoning over scene graphs, represented
by different detection qualities. Using the ground-truth

Table 4: Single-model results on VQAv2.0 validation set
and test set. †: values reported in the original papers.

Method

expert layout

validation(%)

test(%)

Up-Down [2]

N2NMN [10]

StackNMN [9]

XNMs

no

yes

no

no

63.2†

-

-

64.7

66.3
63.3†
64.1†

67.5

scene graphs and programs on CLEVR, we can achieve
100% accuracy with only 0.22M parameters. Compared
to existing neural module networks, XNMs disentangle the
“high-level” reasoning from the “low-level” visual percep-
tion, and allow us to pay more attention to teaching A.I. how
to “think”, regardless of what they “look”. We believe that
this is an inspiring direction towards explainable machine
reasoning. Besides, our experimental results suggest that vi-
sual reasoning beneﬁts a lot from high-quality scene graphs,
revealing the practical signiﬁcance of the scene graph re-
search.

Acknowledgments. The work is supported by NSFC
key projects (U1736204, 61661146007, 61533018), Min-
istry of Education and China Mobile Research Fund (No.
20181770250), THUNUS NExT Co-Lab, and Alibaba-
NTU JRI.

88383

[18] M. L. Minsky. Logical versus analogical or symbolic versus
connectionist or neat versus scruffy. AI magazine, 12(2):34,
1991. 1

[19] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and
A. Courville. Film: Visual reasoning with a general con-
ditioning layer. AAAI, 2018. 2

[20] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1

[21] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural
network module for relational reasoning. In NIPS, 2017. 2

[22] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,
G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, et al. Mastering the game
of go with deep neural networks and tree search. nature,
529(7587):484, 2016. 1

[23] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence

learning with neural networks. In NIPS, 2014. 5
[24] D. Teney, L. Liu, and A. van den Hengel.

Graph-
structured representations for visual question answering.
arXiv preprint, 2017. 3

[25] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph

generation by iterative message passing. In CVPR, 2017. 3

[26] K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenen-
baum. Neural-symbolic vqa: Disentangling reasoning from
vision and language understanding. NIPS, 2018. 2, 3, 6, 7, 8
[27] X. Yin and V. Ordonez. Obj2text: Generating visually de-
In EMNLP, 2017.

scriptive language from object layouts.
3

[28] R. Zellers, M. Yatskar, S. Thomson, and Y. Choi. Neural
motifs: Scene graph parsing with global context. In CVPR,
2018. 2, 3

[29] Y. Zhang, J. Hare, and A. Pr¨ugel-Bennett. Learning to
count objects in natural images for visual question answer-
ing. arXiv preprint arXiv:1802.05766, 2018. 4

References

[1] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai,
E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng,
G. Chen, et al. Deep speech 2: End-to-end speech recogni-
tion in english and mandarin.
In International Conference
on Machine Learning, 2016. 1

[2] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down atten-
tion for image captioning and visual question answering. In
CVPR, 2018. 8

[3] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural

module networks. In CVPR, 2016. 2

[4] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. Lawrence Zitnick, and D. Parikh. Vqa: Visual question
answering. In ICCV, 2015. 1, 2

[5] L. Chen, H. Zhang, J. Xiao, X. He, S. Pu, and S.-
F. Chang. Scene dynamics: Counterfactual critic multi-
agent training for scene graph generation. arXiv preprint
arXiv:1812.02347, 2018. 3

[6] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and
D. Parikh. Making the v in vqa matter: Elevating the role of
image understanding in visual question answering. In CVPR,
2017. 1, 2, 3, 8

[7] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

In ICCV, 2017. 6

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1

[9] R. Hu, J. Andreas, T. Darrell, and K. Saenko. Explainable
In

neural computation via stack neural module networks.
ECCV, 2018. 2, 3, 5, 6, 8

[10] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.
Learning to reason: End-to-end module networks for visual
question answering. In ICCV, 2017. 2, 3, 6, 8

[11] D. A. Hudson and C. D. Manning. Compositional attention

networks for machine reasoning. ICLR, 2018. 2

[12] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L.
Zitnick, and R. Girshick. Clevr: A diagnostic dataset for
compositional language and elementary visual reasoning. In
CVPR, 2017. 1, 2, 3, 5, 6

[13] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman,
L. Fei-Fei, C. L. Zitnick, and R. B. Girshick. Inferring and
executing programs for visual reasoning. In ICCV, 2017. 1,
2, 3, 6, 7, 8

[14] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma,
Image retrieval using scene

M. Bernstein, and L. Fei-Fei.
graphs. In CVPR, 2015. 3

[15] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,

521(7553):436, 2015. 1

[16] Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang.
Factorizable net: an efﬁcient subgraph-based framework for
scene graph generation. In ECCV, 2018. 3

[17] D. Mascharka, P. Tran, R. Soklaski, and A. Majumdar.
Transparency by design: Closing the gap between perfor-
mance and interpretability in visual reasoning.
In CVPR,
2018. 2, 3, 6, 7, 8

98384

