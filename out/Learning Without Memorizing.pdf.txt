Learning without Memorizing

Prithviraj Dhar*1, Rajat Vikram Singh*2, Kuan-Chuan Peng2, Ziyan Wu2, Rama Chellappa1

1University of Maryland, College Park, MD

2Siemens Corporate Technology, Princeton, NJ

{prithvi,rama}@umiacs.umd.edu, {singh.rajat,kuanchuan.peng,ziyan.wu}@siemens.com

Abstract

Incremental learning (IL) is an important task aimed at
increasing the capability of a trained model, in terms of
the number of classes recognizable by the model. The key
problem in this task is the requirement of storing data (e.g.
images) associated with existing classes, while teaching the
classiﬁer to learn new classes. However, this is imprac-
tical as it increases the memory requirement at every incre-
mental step, which makes it impossible to implement IL
algorithms on edge devices with limited memory. Hence, we
propose a novel approach, called ‘Learning without Memo-
rizing (LwM)’, to preserve the information about existing
(base) classes, without storing any of their data, while
making the classiﬁer progressively learn the new classes. In
LwM, we present an information preserving penalty: Atten-
tion Distillation Loss (LAD), and demonstrate that penal-
izing the changes in classiﬁers’ attention maps helps to
retain information of the base classes, as new classes are
added. We show that adding LAD to the distillation loss
which is an existing information preserving loss consis-
tently outperforms the state-of-the-art performance in the
iILSVRC-small and iCIFAR-100 datasets in terms of the
overall accuracy of base and incrementally learned classes.

1. Introduction

Most state-of-the-art solutions to visual recognition
tasks use models that are speciﬁcally trained for these tasks
[6, 13]. For the tasks involving categories (such as object
classiﬁcation, segmentation), the complexity of the task
(i.e. the number of target classes) limits the ability of these
trained models. For example, a trained model aimed for
object recognition can only classify object categories on
which it has been trained. However, if the number of target
classes increases, the model must be updated in such a way
that it performs well on the original classes on which it has

*These authors have contributed equally to this work, which was

partially done during PD’s internship at Siemens Corporate Technology.

Figure 1: Our problem setup does not store data/model pertaining
to information about classes learned in previous incremental steps.

been trained, also known as base classes, while it incremen-
tally learns new classes as well.

If we retrain the model only on new, previously unseen
classes, it would completely forget the base classes, which
is known as catastrophic forgetting [9, 10], a phenomenon
which is not typically observed in humane learning. There-
fore, most existing solutions [4, 14, 18] explore incre-
mental learning (IL) by allowing the model to retain a
fraction of the training data of base classes, while incre-
mentally learning new classes. Yu et al.
[18] proposed
retaining trained models encoding base class information, to
transfer their knowledge to the model learning new classes.
However, this process is not scalable. This is because
storing base class data or models encoding base class infor-
mation is a memory expensive task, and hence is cumber-
some when used in a lifelong learning setting. Also, in an
industrial setting, when a trained object classiﬁcation model
is delivered to the end user, the training data is kept private
for proprietary reasons. Therefore, the end user will be
unable to update the trained model to incorporate new target
classes in the absence of base class data.

Moreover,

storing base class data for

incremen-
tally learning new classes is not biologically inspired.
For example, when a toddler learns to recognize new
shapes/objects, it is observed that it does not completely
forget the shapes or objects it already knows. It also does
not always need to revisit the old information when learning

5138

Inspired by this, we aim to explore incre-
new entities.
mental learning in object classiﬁcation by adding a stream
of new classes without storing data belonging to classes
that the classiﬁer has already seen. While IL solutions
that do not require base class data, such as [1, 9] have
been proposed, these methods mostly aim at incrementally
learning new tasks, which means that at test time the model
cannot confuse the incrementally learned tasks with tasks it
has already learned, making the problem setup much easier.
We explore the problem of incrementally learning object
classes, without storing any data or model associated with
the base classes (Figure 1) in the previous steps, while
allowing the model to confuse new classes with old ones.
In our problem setup, an ideal incremental learner should
have the following properties:

i It should help a trained model to learn new classes
obtained from a stream of data, while preserving the
model’s knowledge of base class information.

ii At testing time, it should enable the model to consider
all the classes it has learned when the model makes a
prediction.

iii The size of the memory footprint should not grow at all,

irrespective of the number of classes seen thus far.

the model trained with only

An existing work targeting the same problem is LwF-MC,
which is one of the baselines in [14].
In the following
sections, we use the following terminology (introduced in
[19]) at incremental step t :
Teacher model, Mt−1, i.e.
base classes.
Student model, Mt, i.e.
the model which incrementally
learns new classes, while emulating the teacher model for
maintaining performance on base classes.
Information Preserving Penalty (IPP), i.e.
the loss to
penalize the divergence between Mt−1 and Mt.
Ideally,
this helps Mt to be as proﬁcient in classifying base classes
as Mt−1.

Initialized using Mt−1, Mt is then trained to learn new
classes using a classiﬁcation loss, LC . However, an IPP
is also applied to Mt so as to minimize the divergence
between the representations of Mt−1 and Mt. While LC
helps Mt to learn new classes, IPP prevents Mt from
diverging too much from Mt−1. Since Mt is already initial-
ized as Mt−1, the initial value of IPP is expected to be close
to zero. However, as Mt keeps learning new classes with
LC , it starts diverging from Mt−1, which leads the IPP to
increase. The purpose of the IPP is to prevent the divergence
of Mt from Mt−1. Once Mt is trained for a ﬁxed number of
epochs, it is used as a teacher in the next incremental step,
using which a new student model is initialized.

In LwF-MC [14], the IPP is the knowledge distillation
loss. The knowledge distillation loss LD, in this context,

It captures the divergence
was ﬁrst introduced in [12].
between the prediction vectors of Mt−1 and Mt.
In an
incremental setup, when an image belonging to a new class
(In) is fed to Mt−1, the base classes which have some
resemblance in In are captured. LD enforces Mt to capture
the same base classes. Thus, LD essentially makes Mt
learn ‘what’ are the possible base classes in In, as shown
in Figure 1. Pixels that have signiﬁcant inﬂuence on the
models’ prediction constitute the attention region of the
network. However, LD does not explicitly take into account
the degree of each pixel inﬂuencing the models predictions.
For example, in Figure 2, in the ﬁrst row, it is seen that
at step n, even though the network focuses on an incor-
rect region while predicting ‘dial telephone’, the numerical
value of LD (0.09) is same as that when the network focuses
on the correct region in step n, in the bottom row.

We hypothesize that attention regions encode the
models’ representation more precisely. Hence, constraining
the attention regions of Mt and Mt−1 using an Attention
Distillation Loss (LD, explained in Sec. 4.1), to mini-
mize the divergence of the representations of Mt from that
of Mt−1 is more meaningful. This is because, instead
of ﬁnding which base classes are resembled in the new
data, attention maps explain ‘why’ hints of a base class are
present (as shown in Figure 1). Using these hints, LD, in an
attempt to make the attention maps of Mt−1 and Mt equiva-
lent, helps to encode some visual knowledge of base class in
Mt. We show the utility of LAD in Figure 2, where although
the model correctly predicts the image as ’dial telephone’,
the value of LD in step n increases if the attention regions
diverge too much from the region in Step 0.

We propose an approach where an Attention Distilla-
tion Loss (LAD) is applied to Mt to prevent its divergence
from Mt−1, at incremental step t. Precisely, we propose to
constrain the L1 distance between the attention maps gener-
ated by Mt−1 and Mt in order to preserve the knowledge of
base classes. The reasoning behind this strategy is described
in Sec 4.1. This is applied in addition to the distillation loss
LD and a classiﬁcation loss for the student model to incre-
mentally learn new classes.

The main contribution of this work is to provide an
termed as ‘Learning without
attention-based approach,
Memorizing (LwM)’, that helps a model to incrementally
learn new classes by restricting the divergence between
student and teacher model. LwM does not require any data
of the base classes when learning new classes. Different
from contemporary approaches that explore the same
problem, LwM takes into account the gradient ﬂow infor-
mation of teacher and student models by generating atten-
tion maps using these models. It then constrains this infor-
mation to be equivalent for teacher and student models,
thus preventing the student model to diverge too much from
the teacher model. Finally, we show that LwM consis-

5139

Sample

Step 0

Step 1

Step n

…

0.08
0.12

0.09
0.82

…

0.08

0.12

0.09

0.15

-
-

-
-

Knowledge Distillation Loss
Attention Distillation Loss

Knowledge Distillation Loss
Attention Distillation Loss

Figure 2: (Top) Example of a case where attention regions degrade
in later incremental steps.(Bottom) Example of a case where atten-
tion regions do not vary across incremental steps. Distillation loss
is seen to be unaffected by degrading attention regions, whereas
Attention Distillation Loss is sensitive to the attention regions

tently outperforms the state-of-the-art performance in the
iILSVRC-small [14] and iCIFAR-100 [14] datasets.

2. Related work

Task incremental (TI) methods:

In object classiﬁcation, Incremental learning (IL) is the
process of increasing the breadth of an object classiﬁer,
by training it to recognize new classes, while retaining its
knowledge of the classes on which it has been trained orig-
inally. In the past couple of years, there has been consid-
erable research efforts in this ﬁeld [9, 12]. Moreover, there
exist several subsets of this research problem which impose
different constraints in terms of data storage and evaluation.
We can divide existing methods based on their constraints:
In this problem, a
model trained to perform object classiﬁcation on a speciﬁc
dataset is incrementally trained to classify objects in a new
dataset. A key characteristic of these experiments is that
during evaluation, the ﬁnal model is tested on different
datasets (base and incrementally learned) separately. This
is known as multi-headed evaluation [4]. In such an eval-
uation, the classes belonging to two different tasks have
no chance to confuse with one another. One of the earlier
works in this category is LwF [12], where a distillation loss
is used to preserve information of the base classes. Also,
the data from base classes is used during training, while
the classiﬁer learns new classes. A prominent work in
this area is EWC [9], where at each incremental task the
weights of the student model are set to those of their corre-
sponding teacher model, according to their importance of

network weights. Aljundi et al. present MAS [1], a tech-
nique to train the agents to learn what information should
not be forgotten. All experiments in this category use multi-
headed evaluation, which is different from the problem
setting of this paper where we use single-headed evalua-
tion, deﬁned explicitly in [4]. Single-headed evaluation
is another evaluation method wherein the model is evalu-
ated on both base and incrementally learned classes jointly.
Multi-headed evaluation is easier than single-headed evalu-
ation, as explained in [4].

Class incremental (CI) methods: In this problem, a
model trained to perform object classiﬁcation on speciﬁc
classes of a dataset is incrementally trained to classify new
unseen classes in the same dataset. Most of the existing
work exploring this problem use single-headed evaluation.
This makes the CI problem more difﬁcult than the TI
problem because the model can confuse the new class with
a base class in the CI problem. iCaRL [14] belongs to this
category. In iCaRL [14], Rebufﬁ et al. propose a technique
to jointly learn feature representation and classiﬁers. They
also introduce a strategy to select exemplars which is used
in combination with the distillation loss to prevent catas-
trophic forgetting. In addition, a new baseline: LwF-MC
is introduced in [14], which is a class incremental version
of LwF [12]. LwF-MC uses the distillation loss to preserve
the knowledge of base classes along with a classiﬁcation
loss, without storing the data of base classes and is evalu-
ated using single-headed evaluation. Another work aiming
to solve the CI problem is [4], which evaluates using both
single-headed and multi-headed evaluations and highlights
their difference. Chaudhry et al. [4] introduce metrics to
quantify forgetting and intransigence, and also propose the
Riemannian walk to incrementally learn classes.

A key factor of most incremental learning frameworks is
whether or not they allow storing the data of base classes
(i.e. classes on which the classiﬁer is originally trained).
We can also divide existing methods based on this factor:

Methods which use base class data: Several exper-
iments have been proposed to use a small percentage of
the data of base classes while training the classiﬁer to
learn new classes. iCaRL [14] uses the exemplars of base
classes, while incrementally learning new classes. Simi-
larly, Chaudhry et al.
[4] also use a fraction of the data
[4] also show that this
of base classes. Chaudhry et al.
is especially useful for alleviating intransigence, which is
a problem faced in single-headed evaluation. However,
storing data for base classes increases memory requirement
at each incremental step, which is not feasible when the
memory budget is limited.

Methods which do not use base class data: Several
TI methods described earlier (such as [1, 9] ) do not use
the information about base classes while training the clas-
siﬁer to learn new classes incrementally. To the best of

5140

Constraints

Use base class data

No base class data

CI methods

iCaRL [14], [4], [18]

LwF-MC [14], LwM

TI methods

LwF [12]

IMM [10], EWC [9],

MAS [1], [2], [8]

Table 1: Categorization of recent related works in incremental
learning. We focus on the class incremental (CI) problems where
base class data is unavailable when learning new classes.

our knowledge, LwF-MC [14] is the only CI method which
needs no base class data but uses single-headed evaluation.

Table 1 presents a taxonomy of previous works in this
ﬁeld. We propose a technique to solve the CI problem,
without using any base class data. We can infer from the
discussion above that LwF-MC [14] is the only existing
work which uses single-headed evaluation, and hence use
it as our baseline. We intend to use attention maps in an
incremental setup, instead of only knowledge distillation,
to transfer more comprehensive knowledge of base classes
from teacher to student model. Although in [19], enforcing
equivalence of attention maps of teacher and student models
has been explored previously for transferring knowledge
from teacher to student models, the same approach cannot
be applied to an incremental learning setting. In our incre-
mental problem setup, due to the absence of base class
data, we intend to utilize the attention region in the new
data which resembles one of the base classes. But these
regions are not prominent since the data does not belong to
any of the base classes, thus making class-speciﬁc atten-
tion maps a necessity. Class-speciﬁcity is required to
mine out base class regions in a more targeted fashion,
which is why generic attention maps such as activation-
based attention maps in [19] are not applicable as they
can not provide a class-speciﬁc explanation about relevant
patterns corresponding to the target class. We deﬁne class-
speciﬁc interpretation as how a network understands the
spatial locations of speciﬁc kinds of object. Such loca-
tions are determined by computing Grad-CAM [16] atten-
tion maps. Also, in LwM, by using class-speciﬁc atten-
tion map, we can enforce the consistency on class-speciﬁc
interpretation between teacher and student models. More-
over, our problem setup is different from knowledge distil-
lation because at incremental step t, we freeze Mt−1 while
training Mt, and do not allow Mt to access data from the
base classes, and therefore Mt−1 and Mt are trained using a
completely different set of classes. This makes the problem
more challenging as the output of Mt on feeding data from
unseen classes is the only source of base class data. This is
further explained in Sec. 4.1.

We intend to explore the CI problem by proposing to
constrain the attention maps of the teacher and student
models to be equivalent (in addition to their prediction

vectors), to improve the information preserving capability
of LwF-MC [14]. In LwF-MC and our proposed method
LwM, storing teacher models trained in previous incre-
mental steps is not allowed since it would not be feasible
to accumulate models from all the previous steps when the
memory budget is limited.

3. Background

Before we discuss LwM, it is important to introduce
distillation loss LD, which is our baseline IPP, as well as
how we generate attention maps.

3.1. Distillation loss (LD)

LD was ﬁrst introduced in [12] for incremental learning.

It is deﬁned as follows:

LD(y, ˆy) =  

y

N

Xi=1

0

i. log( ˆy 0

i),

(1)

0

where y and ˆy are prediction vectors (composed of prob-
ability scores) of Mt−1 and Mt for base classes at incre-
mental step t, each of length N (assuming that Mt−1
i = σ(yi) and
is trained on N base classes). Also, y
ˆy 0
i = σ( ˆyi) (where σ(·) is sigmoid activation). This deﬁ-
nition of LD is consistent with that deﬁned in LwF-MC
[14]. Essentially, LD enforces the base class prediction of
Mt and Mt−1 to be equivalent, when an image belonging
to one of the incrementally added classes is fed to each
of them. Moreover, we believe that there exist common
visual semantics or patterns in both base and new class
data. Therefore, it makes sense to encourage the feature
responses of Mt and Mt−1 to be equivalent, when new class
data is given as input. This helps to retain the old class
knowledge (in terms of the common visual semantics).

3.2. Generating attention maps

We describe the technique employed to generate atten-
tion maps. In our experiments we use the Grad-CAM [16]
for this task. In [15], Grad-CAM maps have been shown
to encode information to learn new classes, although not in
an incremental setup. For using the Grad-CAM, the image
is ﬁrst forwarded to the model, obtaining a raw score for
every class. Following this, the gradient of score yc for a
desired class c is computed with respect to each convolu-
tional feature map Ak. For each Ak, global average pooling
is performed to obtain the neuron importance αk of Ak. All
the Ak weighted by αk are passed through a ReLU activa-
tion function to obtain a ﬁnal attention map for class c.

let αk = ∂yc

More precisely,

Let α =
[α1, α2, . . . , αK] and A = [A1, A2, . . . , AK], where K is
the number of convolutional feature maps in the layer using
which attention map is to be generated. The attention map
Q can be deﬁned as

∂Ak

.

5141

an important step for student training. Hence we perform
this step while computing LAD. During training of Mt, an
image belonging to one of the new classes to be learned
(denoted as In), is given as input to both Mt−1 and Mt.
Let b be the top base class predicted by Mt (i.e. base class
having the highest score) for In. For this input, LAD is
deﬁned as the sum of element wise L1 difference of the
normalized, vectorized attention map:

LAD =

l

Xj=1

k

QIn,b
t−1,j
kQIn,b
t−1 k2

 

QIn,b
t,j
kQIn,b

t

k2

k1

(6)

From the explanation above, we know that for training Mt,
Mt−1 is fed with the data from the classes that it has not
seen before (In). Essentially, the attention regions gener-
ated by Mt−1 for In, represent the regions in the image
If Mt and Mt−1 have
which resemble the base classes.
equivalent knowledge of base classes, they should have
a similar response to these regions, and therefore QIn,b
should be similar to QIn,b
t−1 . This implies that the attention
outputs of Mt−1 are the only traces of base data, which
guides Mt’s knowledge of base classes. We use the L1
distance between QIn,b
as a penalty to enforce
their similarity. We experimented with both L1 and L2
distance in this context. However, as we obtained better
results with L1 distance on held-out data, we chose L1 over
L2 distance.

t−1 and QIn,b

t

t

According to Eq. 2, attention maps encode gradient
of the score of class b, yb with respect to convolutional
feature maps A. This information is not explicitly captured
by the distribution of class scores (used by LD). By
encouraging QIn,b
to be equivalent, we are

t−1 and QIn,b

t

restricting the divergence between " ∂yb

∂A#t−1

and " ∂yb
∂A#t

.

This ensures the consistency on class-speciﬁc interpretation
between teacher and student. We know that every feature
map in A encodes a visual feature. While there can be
several factors that can cause changes to yb, LAD forces
the changes with respect to a speciﬁc visual feature encap-
sulated in A to be equivalent for Mt and Mt−1. Hence, we
hypothesize that combining LD, which captures the score
distribution of the model for base classes (y, ˆy), with a loss
that captures the gradient ﬂow information of the model,
would result in a more wholesome information preserving
penalty. Moreover, the attention maps are a 2D manifes-
tation of the prediction vectors (y, ˆy), which means that
they capture more spatial information than these vectors,
and hence it is more advantageous to use attention maps
than using only prediction vectors.

5. Experiments

We ﬁrst explain our baseline, which is LwF-MC [14].
Following that, we provide information about the datasets

5142

Figure 3: At incremental step t, LwM accepts images belonging
to one of the new classes. Three losses (LC , LD and LAD) are
applied to Mt while Mt−1 remains frozen. The new classes are
depicted in the lower part of the classiﬁer of Mt.

Q = ReLU (αT A)

(2)

4. Proposed approach

We introduce an information preserving penalty (LAD)
based on attention maps. We combine LAD with distillation
loss LD and a classiﬁcation loss LC to construct LwM, an
approach which encourages attention maps of teacher and
student to be similar. Our LwM framework is shown in
Figure 3. The loss function of LwM is deﬁned below:

LLwM = LC + βLD + γLAD

(3)

Here β, γ are the weights used for LD, LAD respectively. In
comparison to LwM, LwF-MC [14] only uses a classiﬁca-
tion loss combined with distillation loss and is our baseline.

4.1. Attention distillation loss (LAD)

At incremental step t, we deﬁne student model Mt,
initialized using a teacher model Mt−1. We assume Mt is
proﬁcient in classifying N base classes. Mt is required to
recognize N + k classes, where k is the number of previ-
ously unseen classes added incrementally. Hence, the sizes
of the prediction vectors of Mt−1 and Mt are N and N + k
respectively. For any given input image i, we denote the
vectorized attention maps generated by Mt−1 and Mt, for
class c as Qi,c
, respectively. We generate these
maps using Grad-CAM [16], as explained above.

t−1 and Qi,c

t

Qi,c

t−1 = vector(Grad-CAM(i, Mt−1, c))

Qi,c

t = vector(Grad-CAM(i, Mt, c))

(4)

(5)

We assume that the lengths of each vectorized attention map
is l.
In [19], it has been mentioned that normalizing the
attention map by dividing it by the L2 norm of the map is

input 

M

0 

step 1 

step 2 

step 3 

step 4 

input 

M

0 

step 1 

step 2 

step 3 

step 4 

 

C

 

C
M
-
F
w
L

 

M
w
L

 

C

 

C
M
-
F
w
L

 

M
w
L

Figure 4: The example attention maps generated by the following experiment IDs (Table 3): C, LwF-MC, and LwM. All the input images
belong to the initial base classes. The column M0 represents the corresponding base-class attention maps generated by the initial teacher
model, and the columns step 1∼4 represent the corresponding base-class attention maps generated in four different incremental steps in
temporal order. These examples show that the attention maps generated by LwM are closer to those in the column M0 over time compared
with C and LwF-MC, which demonstrates the efﬁcacy of LAD in LwM.

Dataset

iILSVRC iCIFAR

CUB200

Caltech

5.2. Datasets

-small

-100

-2011

-101

100

100

# classes

100

# training images 500
100
# testing images
# classes/batch
10

100

500
100

10, 20, 50

eval. metric

top-5

top-1

80% of data 80% of data
20% of data 20% of data

10

top-1

10

top-1

Table 2: The statistics of the datasets used in our experiments, in
accordance with [14]. Additionally, we also perform experiments
on the CUB-200-2011 [17] dataset.

used in our experiments. After that, we describe the iterative
protocol to perform classiﬁcation at every incremental step.
We also provide implementation details including architec-
tural information.

5.1. Baseline

As our baseline is LwF-MC [14], we ﬁrstly implement
its objective function, which is a sum of a classiﬁcation loss
and distillation loss (LC + LD). In all our experiments, we
use a cross entropy loss for LC to be consistent with [14].
However, it should be highlighted that the ofﬁcial imple-
mentation of LD in LwF-MC by [14] is different from the
deﬁnition of LD in [12]. As LwF-MC (but not LwF) is our
baseline, we use iCaRL’s implementation of LwF-MC in
our work. LwF cannot handle CI problems where no base
class training data is available (according to Table 1), which
is the reason why we choose LwF-MC as the baseline and
iCaRL’s implementation.

We use two datasets used in LwF-MC [14] for our exper-
iments. Additionally, we also perform experiments on
Caltech-101 [5] as well as CUBS-200-2011 [17] datasets.
The details for the datasets are provided in Table 2. These
datasets are constructed by randomly selecting a batch of
classes at every incremental step.
In both datasets, the
classes belonging to different batches are disjoint. For a
fair comparison, the data preparation for all the datsets and
evaluation strategy are the same as that for LwF-MC [14].

5.3. Experimental protocol

We now describe the protocol using which we iteratively
train Mt, so that it preserves the knowledge of the base
classes while incrementally learning new classes.

Initialization: Before the ﬁrst incremental step (t = 1),
we train a teacher model M0 on 10 base classes, using a
classiﬁcation loss for 10 epochs. The classiﬁcation loss is a
cross entropy loss LC . Following this, for t = 1 to t = k
we initialize student Mt using Mt−1 as its teacher, and feed
data from a new batch of images that is to be incrementally
learned, to both of these models. Here k is the number of
incremental steps.

Applying IPP and classiﬁcation loss to student model:
Given the data from new classes as inputs, we generate
the output of Mt and Mt−1 with respect to base class
having the highest score. These outputs can either be class-
speciﬁc attention maps (required for computing LAD) or
class-speciﬁc scores (required for computing LD). Using
these outputs we compute an IPP which can either be LAD
or LD.
In addition, we apply a classiﬁcation loss to Mt

5143

Experiment ID\loss LC

LD

LAD

# Classes

FT

LwM (ours)

FT

LwM (ours)

Finetuning

LwF-MC [14]

LwM

3

3

3

7

3

3

7

7

3

Table 3: Experiment conﬁgurations used in this work, identiﬁed
by their respective experiment IDs.

based on its outputs with respect to the new classes which
are to be learned incrementally. We jointly apply classiﬁ-
cation loss and IPP to Mt and train it for 10 epochs. Once
Mt is trained, we use it as a teacher model in the next incre-
mental step, and follow the aforementioned steps iteratively,
until all the k incremental steps are completed.

5.4. Implementation details

We use the ResNet-18 [7] architecture for training
student and teacher models on the iILSVRC-small, Caltech-
101 and CUBS-200-2011 datasets, and the ResNet-34 [7]
for training models on the iCIFAR-100 dataset. This is
consistent with the networks and datasets used in [14].
We used a learning rate of 0.01. The feature maps of
the ﬁnal convolutional layer are used to generate attention
maps using Grad-CAM, as these maps are highly inter-
pretable. [16]. The combinations of classiﬁcation loss and
IPP, along with their experiment IDs are provided in Table
3. The experiment conﬁgurations will be referred to as their
respective experiment IDs from now on.

6. Results

Before discussing the quantitative results and advantages
of our proposed penalties, we show some qualitative results
to demonstrate the advantage of using LAD. We show that
we can retain attention regions of base classes for a longer
time when more classes are incrementally added to the clas-
siﬁer by using LwM as compared to LwF-MC [14]. Before
the ﬁrst incremental step t = 1, we have M0 trained on 10
base classes. Now, following the protocol in Sec. 5.3, we
incrementally add 10 classes at each incremental step. At
every incremental step t, we train Mt with 3 conﬁgurations:
C, LwF-MC [14], and LwM. We use Mt to generate the
attention maps for the data from base classes (using which
M0 was trained), which it has not seen, and show the results
in Figure 4. Additionally, we also generate corresponding
attention maps using M0 (i.e.
the ﬁrst teacher model),
which can be considered ‘ideal’ (as target maps) as M0 was
given full access to base class data. For the Mts trained
with C, it is seen that attention regions for base classes are
quickly forgotten after every incremental step. This can
be attributed to catastrophic forgetting [9, 10]. Mt trained
with LwF-MC [14] have slightly better attention preserving
ability but as the number of incremental steps increases, the
attention regions diverge from the ‘ideal’ attention regions.

Dataset

Caltech-101

CUBS-200-2011

10 (base)

20
30
40
50
60
70
80
90
100

97.78
59.55
52.65
44.51
35.52
31.18
32.99
27.45
28.55
28.26

97.78
75.34
71.78
67.49
59.79
56.62
54.62
48.71
46.21
48.42

99.17
57.92
41.11
35.42
32.33
29.03
22.14
22.27
20.52
17.4

99.17
78.75
70.83
58.54
53.67
47.92
43.79
43.83
39.85
34.52

Table 4: Results obtained on Caltech-101 [5] and CUBS-200-2011
[17]. Here FT refers to ﬁnetuning. The ﬁrst step refers to the
training of ﬁrst teacher model using 10 classes.

# Classes / Conﬁg LC + LAD

LwM (ours)

20
30
40
50
60

84.95
55.82
43.46
36.36
26.78

99.55
99.18
98.72
98.10
97.22

Table 5: Top-5 accuracy comparison of LC + LAD and LwM.
The LwM accuracies are in accordance to that of Figure 5. Not
designed to be used alone, LAD is used to ensure the consistency
on class-speciﬁc interpretation between teacher and student, by
enforcing the gradients of class-speciﬁc score w.r.t. feature maps
to be equivalent.

Interestingly, the attention maps generated by Mt trained
with LwM conﬁguration retain the attention regions for
base classes for all incremental steps shown in Figure 4,
and are most similar to the target attention maps. These
examples support that LwM delays forgetting of base class
knowledge.

We now present the quantitative results of the following
conﬁgurations: C , LwF-MC [14] and LwM. To show the
efﬁcacy of LwM across, we evaluate these conﬁgurations
on multiple datasets. The results on the iILSVRC-small
and iCIFAR-100 datasets are presented in Figure 5. For
the iILSVRC-small dataset, the performance of LwM is
better than that of the baseline LwF-MC [14]. LwM outper-
forms the baseline by a margin of more than 30% when the
number of classes is 40 or more. Especially for 100 classes,
LwM achieves an improvement of more than 50% over
the baseline LwF-MC [14]. In addition, LwM outperforms
iCaRL [14], at every incremental step, even though iCaRL
has the unfair advantage of storing the exemplars of base
classes while training the student model for the iILSVRC-
small dataset.

To be consistent with the LwF-MC experiments in [14],
we perform experiments by constructing the iCIFAR-100

5144

top-5 accuracy on iILSVRC-small

)

%

(
y
c
a
r
u
c
c
a

5
-
p
o
t

100

80

60

40

20

0

0 10 20 30 40 50 60 70 80 90 100

number of classes

iCaRL

ﬁnetuning
LwF-MC

LwM (ours)

top-1 accuracy on iCIFAR-100 (10-class batches)

top-1 accuracy on iCIFAR-100 (20-class batches)

top-1 accuracy on iCIFAR-100 (50-class batches)

)

%

(

y
c
a
r
u
c
c
a

1
-
p
o
t

100

80

60

40

20

0

100

80

60

40

20

)

%

(

y
c
a
r
u
c
c
a

1
-
p
o
t

100

80

60

40

20

)

%

(
y
c
a
r
u
c
c
a

1
-
p
o
t

0 10 20 30 40 50 60 70 80 90 100

0

0

20

40

60

80

100

0

0

number of classes

number of classes

50

100

number of classes

Figure 5: Performance comparison between our method, LwM, and the baselines. LwM outperforms LwF-MC [14] and “using only
classiﬁcation loss with ﬁnetuning” on the iILSVRC-small and iCIFAR-100 datasets [14]. LwM even outperforms iCaRL [14] on the
iILSVRC-small dataset given that iCaRL has the unfair advantage of accessing the base-class data.

datasets by using batches of 10, 20, and 50 classes at
each incremental step. The results are provided in Figure
5. It can be seen that LwM outperforms LwF-MC for all
three sizes of incremental batches in iCIFAR-100 dataset.
Hence, we conclude that LwM consistently outperforms
LwF-MC [14] in iILSVRC-small and iCIFAR-100 datasets.
Additionally, we also perform these experiments using the
Caltech-101 and CUBS-200-2011 dataset [5] by adding a
batch of 10 classes at every incremental step and compare
it with ﬁnetuning. The results for these two datasets are
shown in Table 4.In Table 5, we also provide the results
obtained using only a combination of LC and LAD, on a
few incremental steps in iILSVRC-small dataset.

The advantage of incrementally adding every loss on top
of LC is demonstrated in Figure 5, where we show that the
performance with only C is poor due to the catastrophic
forgetting [9, 10]. We achieve some improvement when LD
is added as an IPP in LwF-MC. The performance further
improves with the addition of LAD in LwM conﬁguration.

7. Conclusion and future work

We explored the IL problem for the task of object clas-
siﬁcation, and proposed a technique: LwM by combining
LD with LAD, for utilizing attention maps to transfer the
knowledge of base classes from the teacher to student
model, without requiring any data of base classes during

training. This technique outperforms the baseline in all
the scenarios that we investigate. Regarding future appli-
cations, LwM can be used in many real world scenarios.
While we explore IL problem for classiﬁcation in this
work, we believe that the proposed approach can also be
extended for segmentation. Incremental segmentation is a
challenging problem due to the absence of abundant ground
truth maps. The importance of incremental segmentation
has already been underscored in [3]. As visual attention is
also meaningful for segmentation (as shown in [11]), we
intend to extend LwM to incremental segmentation in the
near future.

Acknowledgment

Interior/Interior Business Center

Mostly done when Prithviraj was an intern at Siemens,
this work was partially supported by the Intelligence
Advanced Research Projects Activity (IARPA) via Depart-
ment of
(DOI/IBC)
contract number D17PC00345. The U.S. Government is
authorized to reproduce and distribute reprints for Govern-
mental purposes not withstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as neces-
sarily representing the ofﬁcial policies or endorsements,
either expressed or implied of IARPA, DOI/IBC or the U.S.
Government.

5145

Technical
The Caltech-UCSD Birds-200-2011 Dataset.
Report CNS-TR-2011-001, California Institute of Tech-
nology, 2011.

[18] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, Z. Zhang,
and Y. Fu.
Incremental classiﬁer learning with genera-
tive adversarial networks. arXiv preprint arXiv:1802.00853,
2018.

[19] S. Zagoruyko and N. Komodakis. Paying more attention
Improving the performance of convolutional

to attention:
neural networks via attention transfer. In ICLR, 2017.

References

[1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and
T. Tuytelaars. Memory aware synapses: Learning what (not)
to forget. In The European Conference on Computer Vision
(ECCV), 2018.

[2] R. Aljundi, M. Rohrbach, and T. Tuytelaars. Selﬂess sequen-

tial learning. arXiv preprint arXiv:1806.05421, 2018.

[3] C. Baweja, B. Glocker, and K. Kamnitsas. Towards continual
In Medical Imaging meets

learning in medical imaging.
NIPS Workshop, 2018.

[4] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S. Torr.
Riemannian walk for incremental learning: Understanding
forgetting and intransigence. In The European Conference
on Computer Vision (ECCV), September 2018.

[5] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of
IEEE Transactions on Pattern Analysis

object categories.
and Machine Intelligence, 28:594–611, 2006.

[6] R. Girshick. Fast R-CNN. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, pages 770–
778, 2016.

[8] H. Jung, J. Ju, M. Jung, and J. Kim. Less-forgetting learning

in deep neural networks. In AAAI, 2018.

[9] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness,
G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,
A. Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the national
academy of sciences, page 201611835, 2017.

[10] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang.
Overcoming catastrophic forgetting by incremental moment
matching.
In Advances in Neural Information Processing
Systems, pages 4652–4662, 2017.

[11] K. Li, Z. Wu, K.-C. Peng, J. Ernst, and Y. Fu. Tell me
where to look: Guided attention inference network. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2018.

[12] Z. Li and D. Hoiem. Learning without forgetting.

IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2017.

[13] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015.

[14] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert.
iCaRL: Incremental classiﬁer and representation learning. In
Proc. CVPR, 2017.

[15] R. R. Selvaraju, P. Chattopadhyay, M. Elhoseiny, T. Sharma,
D. Batra, D. Parikh, and S. Lee. Choose your neuron: Incor-
porating domain knowledge through neuron-importance.
In Proceedings of the European Conference on Computer
Vision (ECCV), pages 526–541, 2018.

[16] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, D. Batra, et al. Grad-CAM: Visual explana-
tions from deep networks via gradient-based localization. In
ICCV, pages 618–626, 2017.

[17] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.

5146

