Compressing Unknown Images with Product Quantizer for Efﬁcient Zero-Shot

Classiﬁcation

Jin Li1

,

2, Xuguang Lan1

,

2 ∗, Yang Liu3, Le Wang1

,

2, Nanning Zheng1

2

,

1 Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University, China

2 National Engineering Laboratory for Visual Information

Processing and Applications, Xi’an Jiaotong University, China,

3 State Key Laboratory of Integrated Services Networks, Xidian University, China

{j.lixjtu, liuyangxidian}@gmail.com, {xglan, lewang, nnzheng}@mail.xjtu.edu.cn

Abstract

For Zero-Shot Learning (ZSL), the Nearest Neighbor (N-
N) search is generally conducted for classiﬁcation, which
may cause unacceptable computational complexity for
large-scale datasets. To compress zero-shot classes by the
trained quantizer for efﬁcient search, it tends to induce
large quantization error because distributions between seen
and unseen classes are different. However, as semantic
attributes of classes are available in ZSL, both seen and
unseen classes have the same distribution for one speciﬁc
property, e.g., animals have or do not have spots. Based
on this intuition, a Product Quantization Zero-Shot Learn-
ing (PQZSL) method is proposed to learn embeddings as
well as quantizers to compress visual features into compact
codes for Approximate NN (ANN) search. Particularly, vi-
sual features are projected into an orthogonal semantic s-
pace, and then the Product Quantization (PQ) is utilized to
quantize individual properties. Experimental results on ﬁve
benchmark datasets demonstrate that unseen classes are
represented by the Cartesian product of quantized proper-
ties with little quantization error. As classes in orthogonal
common space are more discriminative, the classiﬁcation
based on PQZSL achieves state-of-the-art performance in
Generalized Zero-Shot Learning (GZSL) task, meanwhile,
the speed of ANN search is 10-100 times higher than tradi-
tional NN search.

1. Introduction

As deep learning-based architectures achieve many suc-
cesses [23] [40], large-scale labeled training images are
more and more important for computer vision application-
s recently. However, large-scale well-annotated datasets

∗Corresponding author: X. Lan. (xglan@mail.xjtu.edu.cn)

are difﬁcult to be established. One of the main reasons is
that the frequencies of natural images obtained by people
follow long-tailed distributions [47]. The number of new-
ly deﬁned visual concepts and products is growing rapid-
ly. Meanwhile, some objects are simply rare by nature
and there are little labeled samples for training. Besides,
sometimes image annotation requires options from expert-
s, which is expensive. Alternatively, Zero-Short Learning
(ZSL) has been proposed as a feasible solution [24]. Im-
ages in limited classes are used in the training phase while
the learned model aims to recognize images in totally new
classes without additional data collections. Aiming at this
goal, ZSL utilizes semantic auxiliary information such as
word descriptions and attributes to preserve inter-class asso-
ciations between seen and unseen classes [24] [10] [11] [48]
[14]. In the training phase, embeddings and compatibility
functions are learned given images and attributes of seen
classes. In the testing phase, the task of unseen class recog-
nition becomes a typical classiﬁcation problem, which can
be solved by employing the Nearest Neighbor (NN) search
with learned compatibility functions.

Nowadays, as the number of obtained images and new
concepts grows rapidly, very large space and long time
are required to store and annotate images. For ZSL, to
achieve high efﬁciency of classiﬁcation in the large-scale
dataset such as ImageNet [9] is a big challenge. Com-
pressing data into compact code is a powerful technique
for enabling efﬁcient Approximate Nearest Neighbor (AN-
N) search. Particularly, the methods of generating compact
code can be divided into two categories, i.e., hashing-based
and quantization-based methods. Hashing learns embed-
dings mapping high-dimension vectors into binary codes,
and thus can signiﬁcantly save storage space [42] [36] [26].
More importantly, in the binary space, the Euclidean dis-
tance is replaced by the Hamming distance, which can be ef-
ﬁciently calculated by the bit-wise XOR operation. Howev-

15463

color

sub-codebook 1

color

color

red

shape

circle

square

shape

sub-codebook 2

blue

red square

shape

bule circle

(a) seen classes

(b) learning product quantizer

(c) quantizing unseen classes

Figure 1. Illustration of PQZSL. (a) Samples in two seen clusters with two properties, i.e., shape and color. (b) Product quantizer trained to
quantize individual property. The sub-codebook of shape contains two codewords, circle and square; the sub-codebook of color contains
red and blue. (c) Using Cartesian product of properties to represent new classes, i.e., bule circles and red squares.

er, there are two main disadvantages in hashing-based ANN
search. One is that the value range of Hamming distance is
limited, which may decrease the accuracy when the nearest
neighbors are sorted by Hamming distance. For example,
as vectors are compressed into 8-bits codes, distances only
vary in 8 integer values. The other one is that hashing can
only use the Symmetric Distance Computation (SDC)[19],
which has been proved to be worse than Asymmetric Dis-
tance Computation (ADC) in ANN search [19].

Quantization-based methods aim to learn a set of code-
books that contain codewords to reconstruct vectors or sub-
vectors [19] [16] [5].
In particular, original vectors are
mapped into integer codes that index codewords for recon-
struction. For ANN search, distances between the query
and codewords are pre-computed and stored in look-up ta-
bles, then they are read from these tables by their codes.
When vectors are compressed into 8-bits codes, quantiza-
tion methods can learn a codebook containing 28 code-
words, thus the range of distance has 256 values.
In ad-
dition, quantization methods support the ADC, where only
the database is compressed. In order to quantize zero-shot
classes, the problem is that only distributions of seen class-
es can be obtained, which may be very different from test
unseen classes. Therefore, if the codebook trained by fea-
tures in seen classes are used to quantize unseen classes, it
tends to cause large quantization error that may signiﬁcantly
decrease the search accuracy.

Since attributes of classes are introduced, we assume that
there is a semantic space, where the value range of each se-
mantic property is the same in both seen and unseen class-
es. As illustrated in Figure 1 (a), a semantic space con-
tains two seen classes, varying in the color and shape. Al-
though seen classes cluster in small regions of the semantic
space, all kinds of colors and shapes are included. Based on
this intuition, we propose a Product Quantization Zero-Shot
Learning (PQZSL), which compresses large-scale database
into compact codes for efﬁcient ANN search. To reconcile
the visual and semantic space, a common space is deﬁned,
where dimensions of projected features are required to be

orthogonal to satisfy the independence condition of Product
Quantization (PQ). In the common space, we train a sub-
codebook to quantize speciﬁc properties. For example, two
sub-codebooks are learned from the color and shape indi-
vidually in Figure 1 (b). After sub-codebooks are generat-
ed, new classes can be quantized by Cartesian products of
sub-codewords with little errors, which is shown in Figure
1 (c). Our intuition is demonstrated by experimental results
on both synthesized dataset and ImageNet [9]. In the ﬁnal
classiﬁcation, ANN search is utilized to compute distances
between projected features and class-level attributes, which
comes from two different modalities. Consequently, it is d-
ifﬁcult to train a uniﬁed codebook to simultaneously quan-
tizer features and attributes with little quantization errors.
To solve this problem, ADC search strategy is introduced,
where only the large-scale database is compressed while at-
tributes are uncompressed in the testing phase. In practical
applications, when the number of databases is much larger
than that of classes, ADC has similar efﬁciency but achieves
higher accuracy than SDC.

2. Related Works

In this section, we brieﬂy review related work in zero-
shot learning, vector quantization and approximate nearest
neighbor search.

2.1. Zero-Shot Learning

In order to train a model from seen classes that can be
generalized to classify unseen classes, ZSL generally re-
quires a set of high-dimension vectors in visual space and
semantic space to describe images and their characteristic-
s, which are named features and attributes in this paper. In
the training phase, features and class-level attributes in seen
classes are obtained to learn embeddings or compatibility
functions. In the testing phase, test images are required to
be classiﬁed into the correct unseen classes identiﬁed by
their attributes via the NN search. For Generalized Zero-
Shot Learning (GZSL) [7], test images may come from ei-
ther seen or unseen classes, which is more reasonable for

25464

the real-world task.

To unify visual features and semantic attributes into the
same space, the ﬁrst idea is mapping features to seman-
tic space[24] [2] [43] [38] [13] [34] [4], which is similar
to the multi-label classiﬁcation. However, this direction of
the projection may induce the hubness problem [37], where
much features may be classiﬁed into one class in the test-
ing phase. In contrast, mapping attributes into visual space
can reduce the hubness problem, where the projected at-
tributes are regarded as centroids of classes [33] [29]. Se-
mantic Auto-Encoder (SAE) requires features and attributes
to mutually reconstruct others using the same parameters,
which can reduce the domain shift problem [22]. Moreover,
a low-rank constraint is added in [27] to preserve the intrin-
sic structure in attributes.

Another group of methods map both visual features and
semantic attributes into intermediate spaces [6] [45] [1]
[46].
In Semantic Similarity Embedding (SSE) [45], the
mixture of seen class proportions is regarded as the com-
mon space, where images belong to the same class should
have a similar mixture pattern. Preserving Semantic Rela-
tions (PSR) [3] deﬁnes a common space which preserves
semantic relations between classes. In Joint Latent Simi-
larity Embedding (JLSE) [46], features and attributes are
mapped into two separate latent spaces, and compatibility
function is learned to measure their similarities. More ex-
actly, projected class-level attributes in common space are
also named prototypes, which represent centroids of classes
in the NN search. For classiﬁcation, the class label of test
sample ui is deﬁned as

ˆy(ui) = arg min

d(ui, vj),

(1)

j

i=1 and V = {vj}T

where d(·, ·) is the measure of similarity, which is gener-
ally deﬁned as the Euclidean or Cosine distance in ZSL.
Assume U = {ui}N
j=1 denote features
and attributes in the testing phase respectively, where N
is the number of images and T is the number of candi-
date classes. As similarities between features and attributes
are exhaustively computed, the computational complexity
is O(D2N T ), where D is the dimension of common space.
For large-scale classiﬁcation task, the complexity is very
large so that the quantization-based ANN search [19] is pro-
posed to improve the efﬁciency.

2.2. Vector Quantization

Vector Quantization (VQ) [25] has been widely used in
data compression and fast retrieval applications. For a D-
dimension vector ui ∈ RD, a quantizer is a function map-
ping ui to a vector q(ui) ∈ C, where C = [c1, c2, · · · , cK]
is the codebook that contains K D-dimension codewords
ci. Then the mapping is deﬁned as

q(ui) = arg min
ck

||ui − ck||2
F ,

(2)

where the D-dimension vector ui is compressed into a s-
calar. Given a training set U, the objective function is de-
ﬁned as

min

N

(cid:2)i=1

||ui − Cbi||2
F ,

(3)

s.t. bi = {0, 1}K , ||bi||1 = 1,

where bi is the code (a one-hot vector) to represent sample
ui. Lloyd’s algorithm [28] iteratively updates the codebook
and codes to minimize the loss. However, the size of the
codebook in VQ rises exponentially with the length of the
code. PQ [19] is proposed to overcome this limitation.

In PQ, a vector ui

into M sub-vectors
ui1, ui2, · · · , uiM , and then the m-th sub-codebooks is
trained to quantize corresponding sub-vectors,

is split

qm(uim) = cmk, cmk ∈ Cm,

(4)

where Cm = [cm1, cm2, · · · , cmK]
is the m-th sub-
codebooks and k is the k-th codeword in Cm. And the
codebook is deﬁned as the Cartesian product of M sub-
codebooks

C =

⎡

⎢⎢⎢⎣

0
C1
0 C2
...
...
0
0

0
· · ·
0
· · ·
...
. . .
· · · CM

⎤
⎥⎥⎥⎦

.

(5)

Let bim denote a sub-vector in bi, which indexes a code-
word in sub-codebook Cm to represent uim, thus the objec-
tive function of PQ can be deﬁned as

N

min

s.t.

||ui − Cbi||2
F ,

(cid:9)
i=1
bi = [bT
bim = {0, 1}K ,

i1, · · · , bT

iM ]T ,

||bim||1 = 1.

(6)

To reduce the quantization error in PQ, we can increase
the number of sub-codebooks M . However, PQ assumes
that the splits of ui are independent of others. To satisfy
this condition, a transformation that reduces correlation a-
mong dimensions in ui is learned before training the prod-
uct quantizer[16]. In this paper, we add the orthogonal re-
striction among dimensions in ui to satisfy the independent
condition, which is discussed in Section 3.1 in detail.

2.3. Approximate Nearest Neighbor Search

The quantization-based ANN search method is proved to
be an efﬁcient way to reduce the cost of distance computa-
tion [19]. After the vectors in the database are compressed

35465

via the learned quantizer, there are two strategies to approx-
imate the NN search. The ﬁrst way is SDC, where the sim-
ilarity distance measure in NN search can be approximated
as

ˆd(u, v) = d(q(u), q(v)) = d(ck, cl),

(7)

where d(ck, cl) can be pre-computed and stored in a look-
up table. The second way is ADC that can be represented
as

˜d(u, v) = d(q(u), v) = d(ck, v),

(8)

where the query v is not compressed. Similar to SDC, we
ﬁrst compute store d(ck, v) (k = 1, · · · , K) in a look-up
table. After pre-computation, ANN search only requires to
look up the table for N times to compute distances between
v and U = {ui}N
i=1. When the time consumption of quan-
tizing v is also considered, SDC and ADC have the same
computational complexity in ANN search, which has been
proved in [19].

Seen classes
Unseen classes

 

150

100

50

0

-50

-100

 

-150

-100

-50

0

50

100

150

(a)

d
e
e
p
s

l

a
m
n
a

i

:
t
s
a

f

100
90
80

70
60
50

40
30
20

10
0

-10

Seen classes
Unseen classes

(cid:3)

(cid:3)

-10 0 10 20 30 40 50 60 70 80 90 100

big: animal size

(b)

Figure 2. Visualization of distributions of seen and unseen classes
in dataset Animal with Attributes (AwA). (a) 2-D t-SNE result of
features. (b) Two properties class-level attributes: size and speed
of animals

query v in Eq. (8). Asymmetric distances between features
and attributes are computed via Eq. (8) for classiﬁcation.

3. Approach

3.1.1 Product Quantization

i , ys

i , as

i )Ns

The training dataset of ZSL is deﬁned by a series of
triplets (xs
i=1 ∈ Xs × Ys × As, where Ns is
the number of training samples. Xs ∈ Rdx×Ns repre-
sents the set of dx dimensional visual features of images.
Ys ∈ RL×Ns denotes class labels, where each column is
a one-hot vector. Moreover, in ZSL, only L classes are
available in training, which are regarded as seen classes.
As ∈ Rda×Ns is the da-dim semantic representation such
as attributes, which is the augment of class-level attributes
A, i.e., As = AYs. In this paper, features and attributes are
projected into a common space. f (xs
i ) indicate
the visual embedding and semantic embedding, respective-
ly. In the testing phase, visual and semantic samples of un-
seen classes are given, i.e., (xt
i=1 ∈ Xt ×At. Similar to
seen classes, Xt and At contain Nt features and attributes,
respectively. Moreover, ZSL requires that As ∩ At = ∅.
The NN search is employed to predict the label ˆy of a sam-
ple xt
i, which can be deﬁned in Eq. (1). Then, embeddings
are learned for efﬁcient zero-shot classiﬁcation.

i ) and g(as

i)Nt

i, at

3.1. Product Quantization Zero-Shot Learning

Our method structure an orthogonal common space,
where each dimension is independent to others. In the com-
mon space, a codebook (like C in Eq. (5)) is trained, where
each sub-codebook is trained to represent corresponding
dimensions. Then visual and semantic embeddings are
learned to project features and attributes into the common
space. By this way, the ﬁnal objective function contains
a loss containing three parts: quantization loss, visual em-
bedding loss, and semantic embedding loss, meanwhile all
parameters are optimized simultaneously. In the test phase,
projected features are compressed into compact codes vi-
a the codebook while a projected attribute is regarded as a

Distributions of visual features in seen and unseen class-
es are different in zero-shot classiﬁcation problem, which
can be demonstrated by the t-SNE result [30] of features in
Animal with Attributes (AwA) dataset [24] [44] shown in
Figure 2 (a). Therefore, if the vector quantizer is trained
using seen classes, it tends to cause large quantization error
when codewords are utilized to represent unseen classes. S-
ince attributes of classes can be obtained, the value range
of each property of seen and unseen classes is similar. For
example, properties “big” and “fast” of 50 kind animals in
AwA dataset are shown in Figure 2 (b), which describe the
size and speed with continuous value. For every single di-
mension, the value of both seen and unseen classes vary in
the same range. This means each property can be complete-
ly seen in the training phase.

In fact, if the learned model is required to recognize all
unseen objects in ZSL, seen classes must contains as much
range as possible of properties. For example, if all training
samples are animals, the model deﬁnitely cannot know air-
plane (just like ancient people). Therefore, we think the as-
sumption of diversity of training classes is existed in all ZSL
problem. Based on this observation, we assume that there
is an orthogonal common space where dimensions are re-
quired to be independent of others. Compared to the seman-
tic space of class-level attributes, the orthogonal common s-
pace achieves two advantages. First, projected features can
be split into independent sub-vectors and quantized individ-
ually. Second, assume the visual embedding f consists of a
family functions f1, f2, · · · , each of them projects features
into one dimension. The independence among dimensions
in common space makes f simpler and more discriminative
because functions will not confuse others, which is pointed
in [18] and [21].

45466

As discussed above, the quantization loss is deﬁned as

Lq =

Ns

(cid:9)

i=1

(cid:4)f (xs

i ) − Cbi(cid:4)2

F = (cid:4)f (Xs) − CB(cid:4)2

F ,

s.t.

f (Xs)f (Xs)T = I.

To train a codebook for ZSL, Le1 and Le2 respectively guar-
antee the independence and semantics in projected points.
Hence, the objective function of PQZSL is

(9)

L = Lq + αLe1 + βLe2.

(14)

Here f (Xs) maps each column in Xs one by one. B =
[b1, b2, · · · , bNs ] represents the set of codes of features. In
quantization method, Eq. (9) is generally optimized by it-
eratively update C and B. After sub-codebooks are trained,
the Cartesian product of sub-codewords can represent un-
seen classes in the common space with little quantization
error for ANN search.

3.1.2 Embedding Learning

In ZSL, the visual embedding f is learned to project fea-
tures into different class centers, which is similar to the clas-
siﬁer. In this paper, the center loss [17] is introduced,

Le1 =

Ns

(cid:9)

i=1

i ) − zys

(cid:10)(cid:10)f (xs

i (cid:10)(cid:10)

2

F

= (cid:4)f (Xs) − Zs(cid:4)2

F ,

(10)

s.t.

f (Xs)f (Xs)T = I.

i

in common space.

Here zys
represents the center of projected features of
class indexed by ys
In the training
i
phase, visual embedding maps features into L centers Z =
[z1, z2, · · · , zL], which represent L seen classes. Then Zs
(10) is the augment of Z by class labels Ys, i.e.,
in Eq.
Zs = ZYs.

In Eq. (9), the orthogonal constraint makes f difﬁcult
to be optimized. Therefore, we relaxed this constraint by
letting dimensions in the class center be orthogonal. And
then the objective function Eq. (10) can be modiﬁed to

Le1 = (cid:4)f (Xs) − Zs(cid:4)2
s.t. Zs = ZYs, ZZT = I.

F ,

(11)

The semantic embedding maps class-level attributes in-
to class centers in the common space. In order to preserve
semantic inter-class associations among class centers while
avoiding the domain shift problem, the auto-encoder frame-
work is introduced like [22], i.e.

Ns

Le2 =

s.t.

(cid:9)
i − g−1(g(as
(cid:10)(cid:10)as
i=1
zs
i = g(as

i ),

i ))(cid:10)(cid:10)

2

F

,

(12)

where g−1(·) is a decoder to reconstruct attributes. Accord-
ing to SAE [22]. The constraint in Le2 can be relaxed and
the compact form is represented as

Le2 = (cid:10)(cid:10)As − g−1(Zs)(cid:10)(cid:10)

2

F + λ (cid:4)Zs − g(As)(cid:4)2
F .

(13)

3.2. Optimization

In this paper, both visual and semantic embeddings are
deﬁned as linear matrixes. Similar to SAE, the same projec-
tion matrix is simultaneously used for encoder and decoder.
The objective function is deduced as

min

P,Q,C,B,Zs,Z

(cid:4)PXs − CB(cid:4)2

F + α (cid:4)PXs − Zs(cid:4)2

F +

β (cid:10)(cid:10)(cid:10)

As − QT Zs(cid:10)(cid:10)(cid:10)

s.t. Zs = ZYs, ZZT = I.

2

F

+ αλ (cid:4)QAs − Zs(cid:4)2

F ,

(15)

3.2.1 Parameter Updating

To optimize this function, embeddings, codebook, codes
and class centers are updated iteratively.

Update P: Fixing all parameters except P, and let the
derivative with respect to P be 0, the visual embedding is
updated by

P = (CB + αZs)XT

s [(1 + α)(XsXT

s )]−1.

(16)

Update Q: To learn the semantic embedding Q, the other
parameters are ﬁxed. Let the derivative with respect to Q be
0, a Sylvester Equation [39] is obtained,

βZsZT

s Q + λQAsAT

s = (β + λ)ZsAT
s ,

(17)

which can be solved in a single line in Matlab or Python 1.
In fact, divide both sides of Eq. (17) by β, there is only one
hyper-parameter that inﬂuence Q.

Update C: In PQ, sub-codebooks can be optimized inde-
pendently. Let P = [PT
M ]T , the sub-codebook Cm
refers to

1 , · · · , PT

(cid:4)PmXs − CmBm(cid:4)2

F ,

(18)

where Bm = [b1m, b2m, · · · , bNsm]. Similar to Eq. (16),
Cm can be updated by

Cm = (PmXBT

m)(BmBT

m)−1.

(19)

Update B: Given a sample xs
i , its code bim of each sub-
codebook Cm is also independent. We update bim by ex-
haustively search sub-codewords in Cm, which achieves the
minimal quantization error in (cid:4)Pmxs

i − Cm(cid:4)2
F .

Update Zs: When Zs is updated, we relax the constraint

1For example, Matlab can use sylvester() or lyap() function.

55467

Zs = ZYs by adding an additional term (cid:4)Zs − ZYs(cid:4)2
F ,
and the objective function can be re-written as

α (cid:4)PXs − Zs(cid:4)2
+λ (cid:4)QAs − Zs(cid:4)2

F + β (cid:10)(cid:10)(cid:10)

As − QT Zs(cid:10)(cid:10)(cid:10)

F + γ (cid:4)Zs − ZYs(cid:4)2

F
F .

2

(20)

Let the derivative with respect to Zs be 0, it can be updated
by

Zs =(cid:11)(α + λ + γ) I + βQQT(cid:12)−1

[(β + λ) QAs + αPXs + γZYs] .

(21)

here I is an identity matrix with the same size as QQT . To
guarantee the independence among dimensions in Zs, we
set γ ≫ α, β, λ.

to train the uniﬁed codebook to quantize projected features
and attributes, because there is a bias between the projec-
tions from two different modalities. Following ADC strate-
gy, visual features are ﬁrstly projected into common space
by Ut = PXt, which are further quantized as q(Ut) = CB
and stored. Semantic attributes are also projected into com-
mon space, where Vt = QAt is regarded as the set of un-
compressed queries. Then the ANN-based classiﬁcation is
conducted by using Eqs.
(1) and (8). Speciﬁcally, when
d(·, ·) is the Euclidean distance, Eq. (8) can be re-written as

˜d(u, v) = (cid:13)(cid:4)q(u)(cid:4)2 + (cid:4)v(cid:4)2 − 2q(u)T v

=

(cid:14)(cid:15)(cid:15)(cid:16)

M

(cid:9)

m=1

(cid:4)qm(um)(cid:4)2 + (cid:4)vm(cid:4)2 − 2qm(um)T vm.

(23)

Update Z: To optimize Z, we solve the objective function

When d(·, ·) is deﬁned as cosine distance, Eq. (8) be-

(cid:4)Zs − ZYs(cid:4)2

F ,

s.t. ZZT = I.

(22)

Notice that there is no constraint in the number of dimen-
sions in common space. To solve Eq. (22), we restrict Z to
be a square matrix with size L × L. It can be easily proved
that if rows of Z are orthogonal, and then columns of Z are
also orthogonal [15], i.e., ZT Z = ZZT = I. According to
[16], Eq. (22) has a closed-form solution. Firstly, we ap-
s = RSWT , and
ply Singular Value Decomposition to YsZT
Z = WRT .

3.2.2 Hyper-parameters and Initialization

There are many hyper-parameters in Eq. (15). However, C,
B and Z only present in one term thus are not inﬂuenced by
hyper-parameters. As we set γ ≫ α, β, λ to satisfy the in-
dependence constraint, Zs is also slightly inﬂuenced by all
the hyper-parameters. More importantly, when updating P
and Q, they are only depended on α and λ respectively. By
this means, we individually consider these parameters based
on the performance in cross-validation in training data. For
ImageNet [9], γ = 104, α = 300, λ = 10 and β = 1. The
hyper-parameters in other datasets can be obtained in same
way.

To initialize parameters, we generate a real orthogonal
matrix Z by orthogonalizing a random matrix and let Zs =
ZY. Q can be calculated by solving Eq. (17). The visual
embedding is initialized without considering quantization
loss, i.e., P = ZsXT
s )−1, and then Lloyd’s method
is utilized to compute C and B given PXs.

s (XsXT

3.3. ANN Search-based Classiﬁcation

In the testing phase, the ADC strategy is employed. The
main reason is that asymmetric distance estimation is more
accurate than symmetric distance. Moreover, it is difﬁcult

comes

˜d(u, v) = 1 −

2q(u)T v

(cid:4)q(u)(cid:4)2 (cid:4)v(cid:4)2

M

(cid:2)m=1

qm(um)T vm

(24)

= 1 −

M

(cid:2)m=1

(cid:4)qm(um)(cid:4)2

.

M

(cid:2)m=1

(cid:4)vm(cid:4)2

Notice that each-codebook contains K=256 codewords,
thus qm(um) only has 256 possible values. In Eqs. (23)
and (24), terms qm(um)T vm and (cid:4)qm(um)(cid:4)2
F also has 256
values that can be pre-computed and stored in look-up ta-
bles, and then the computational complexity is independent
from the size of database.

3.4. Computational Complexity Analysis

Assume N features are required classiﬁed into T class-
es, then the computational complexity is O(D2N T ) when
the ﬁnal classiﬁcation is conducted in the D-dim common
space. As ADC strategy is introduced, the computational
complexity to establish tables is O(D2KT ). The time of
table lookup is O(M N T ), where M is the number of sub-
codebooks. Let D = δM, 1 (cid:2) δ (cid:2) D, the ratio of the
complexity of NN and ANN search can be represented as

O(D2N T )

O(D2KT ) + O(M N T )

O(δDM N T )

O(δDM KT ) + O(M N T )

O(δDN )

O(δDK) + O(N )

.

=

=

(25)

where K is generally ﬁxed to 256, which is smaller than the
number of features N . D is the dimension of uncompressed

65468

vectors. δ is depended on M , which is a hyper-parameter
inﬂuencing the quantization error and search speed. In the
large-scale datasets, N ≫ DK, thus the time consump-
tion can be signiﬁcantly reduced according to Eq. (25). For
medium-scale datasets, the main cost of ANN is establish-
ing look-up tables. And the ratio of the speed of ANN and
NN search is depended on the size of the database.

4. Experiments

The proposed method is mainly evaluated in large-
scale dataset ImageNet 21K dataset and four medium-scale
datasets, where classiﬁcation in the generalized zero-shot
task is tested. Moreover, details about product quantization
are shown, including quantization error and search efﬁcien-
cy. For fair comparison, are features and settings are same
as that in [44], which provides a standard of ZSL and GZSL
task.

4.1. Datasets and Evaluation

The large-scale dataset ImageNet [9] contains 21,841
classes with more than 10 million images collected from the
real-world, where 1K seen classes containing 1.2 million
images are employed to learn embeddings. There are dif-
ferent splits for the test. 2-hops/3-hops refers to test classes
belonging to 2/3 tree hops away from 1K train classes in the
WordNet hierarchy[6], which contains 1,509/7,678 unseen
classes. Classes that contain the top 500/1K/5K maximum
images as well as top 500/1K/5K minimum images are al-
so used for test splits respectively. Finally, all 20K classes
are tested, which is very challenging. Four medium-scale
datasets are Attribute Pascal and Yahoo (aPY) [12], Animal-
s with Attributes (AWA) [44], Caltech-UCSD-Birds (CUB)
[41] and SUN attributes (SUN) [32]. To split the dataset for
training and testing, we follow the same settings with [44],
where unseen classes are not included in the sets of deep
neural network training. Hence, these unseen classes are
really “unknown” for the trained model2.

As quantization method is used, the quantization loss and
its inﬂuence on accuracy are shown. And the time consump-
tion of ANN search is evaluated to show the improvement
of classiﬁcation efﬁciency. To evaluate the accuracy of clas-
siﬁcation, the average of per-class precision (AP) is mea-
sured. In GZSL task where features come from either seen
classes or unseen classes, ‘tr’ represents AP of test features
In contrast, ‘ts’ is the AP
which belong to seen classes.
that unseen features are classiﬁed into all classes. ‘H’ is the
harmonic mean of tr and ts.

4.2. Main Results

We ﬁrst train product quantizer in a synthesized dataset
to demonstrate the effectiveness in representing zero-shot

2All

image feautures and standard splites are published here:

http://www.mpi-inf.mpg.de/zsl-benchmark

12

10

8

6

4

2

0

-2

-4

(cid:3)
-2

(cid:3)

Seen classes
Unseen classes

0

2

4

6

8

10

12

VQ distortion of Unseen: 13.4031

(a)

12

10

8

6

4

2

0

-2

-4

(cid:3)

Seen classes
Unseen classes

-2

0

2

4

6

8

10

12

PQ distortion of Unseen: 1.0473

(b)

Figure 3. Training codebooks with different quantization method
in ZSL, where codewords are denoted by black squares. (a) Vector
Quantization. (b) Product Quantization

25

20

15

10

5

%
y
c
a
r
u
c
c
a
0
1
p
o

t

0

(cid:3)
0

0.01

2 hops
3 hops

(cid:3)

(cid:3)

2 hops
3 hops

M=10

1

1
10

1
100

N
N
o
t

N
N
A

r
u
o
f
o
n
o
i
t
p
m
u
s
n
o
c
e
m

i
t

f

0.03

0.02
average quantization loss

0.04

0.05

0.06

0.07

(a)

o
o

i
t

a
R

1

1000

(cid:3)
0

10

20

30

40

50

60

70

80

90

100

number of sub-codebook M

(b)

Figure 4. Accuracy and efﬁciency of quantization (a) Accuracy
VS. quantization loss under different M. (b) Ratio of time con-
sumption of 2/3 hops classes under different M.

classes. In detailed, we sample 4000 2-D vectors from a
Gaussian Mixture Model, which has 9 clusters. Vectors
drawn from 4 clusters are treated as seen classes, while oth-
ers are unseen. The comparison between VQ and PQ is
shown in Figure 3, where 9 codewords are trained. If VQ is
directly used, it tends to minimize the quantization loss of
known classes, which cause over-ﬁtting when unseen class-
es are introduced. Since the information in each dimension
is completely obtained from seen classes, the sub-codebook
can accurately represent both seen and unseen classes.

To present the relationship between classiﬁcation accu-
racy and quantization loss, we vary the number of sub-
codebooks to adjust the loss. GZSL performance (ts) in
2/3 hops under different quantization loss are evaluated in
Figure 4 (a). When quantization loss is larger than 0.05
(M = 10), the ts accuracy decreases rapidly. In Figure 4
(b), the ratio of time consumption of ANN and NN search
via the different number of sub-codebooks is compared.
The efﬁciency of ANN is 10-100 times higher than that of
NN search in the same common space. According to the
requirement of accuracy, we can set M from 10 to 50.

Finally, the overall results of PQZSL are compared with
state-of-the-art baselines. The GZSL accuracy on ImageNet
is evaluated in Table 1. To quantize projected features, we
train 50 sub-codebooks where each of them contains 256
codewords. In this way, one visual feature is compressed

75469

Table 1. Generalized Zero-Shot Learning comparisons on ImageNet dataset. We measure Top-10 accuracy in %.

Hierarchy

Most populated

Least populated

Method

2-hops

3-hops

CONSE [31]

CMT [38]

LATEM [43]

ALE [1]

DEVISE [13]

SJE [2]

ESZSL [35]
SYNC [6]
SAE [22]
PQZSL

0.86
7.80
16.99
17.79
17.59
17.46
19.24
14.55
13.55
21.80

7.14
2.77
6.28
6.34
6.28
6.21
6.81
5.62
4.82
7.41

1K

500
23.47 18.38
9.65
7.73
23.61 18.65
24.93 19.37
24.66 19.11
23.61 18.45
26.52 20.56
16.33 13.82
20.76 16.60
29.30 23.75

5K
9.92
3.83
8.73
9.12
8.99
8.79
9.72
7.87
7.60
11.3

500
0.00
3.37
8.73
10.38
10.11
9.85
9.12
2.77
3.43
9.42

1K
0.00
2.71
7.60
8.46
8.26
8.00
7.73
2.44
2.57
7.87

5K
0.66
1.45
3.50
3.63
3.63
3.50
3.76
1.78
1.58
3.72

All
20K
3.43
1.25
2.71
2.77
2.71
2.71
3.10
2.64
2.24
3.45

Table 2. Generalized Zero-Shot Learning results on SUN, CUB, AWA and aPY. We measure the AP of Top-1 accuracy in %.

Method

CMT [38]
SSE [45]

LATEM [43]

ALE [1]

DEVISE [13]

SJE [2]

ESZSL [35]
SYNC [6]
SAE [22]

LESAE [27]

PSR [3]

SP-ANE[8]
CDL [20]
PQZSL

SUN

CUB

AWA

H

tr
ts
28.0 13.3
8.7
2.1
36.4
4.0
14.7 28.8 19.5
21.8 33.1 26.3
16.9 27.4 20.9
14.7 30.5 19.8
11.0 27.9 15.8
43.3 13.4
7.9
17.8 32.0 22.8
21.9 34.7 26.9
20.8 37.2 26.7
24.9 38.6 30.3
21.5 34.7 26.5
35.1 35.3 35.2

H
tr
ts
60.1
8.7
4.7
46.9 14.4
8.5
15.2 57.3 24.0
23.7 62.8 34.4
23.8 53.0 32.8
23.5 59.2 33.6
12.6 63.8 21.0
11.5 70.9 19.8
18.8 58.5 29.0
24.3 53.0 33.3
24.6 54.3 33.9
34.7 70.6 46.6
23.5 55.2 32.9
43.2 51.4 46.9

H

tr
ts
89.0 15.9
8.7
8.1
82.5 14.8
11.5 77.3 20.0
14.0 81.8 23.9
17.1 74.7 27.8
8.0
73.9 14.4
5.9
77.8 11.0
10.0 90.5 18.0
16.7 82.5 27.8
21.8 70.6 33.3
20.7 73.8 32.3
23.3 90.9 37.1
28.1 73.5 40.6
31.7 70.9 43.8

aPY
tr

H

ts
10.9 74.2 19.0
78.9
0.4
0.2
73.0
0.1
0.2
73.7
8.7
4.6
76.9
9.2
4.9
55.7
6.9
3.7
70.1
2.4
4.6
7.4
66.3 13.3
12.3 72.5 20.9
12.7 56.1 20.1
13.5 51.4 21.4
13.7 63.4 22.6
19.8 48.6 28.1
27.9 64.1 38.8

into a 50-Bytes code and the compression ratio is about 160.
Compared with plenty of baselines, PQZSL achieves the
best performance in most splits. The comparison demon-
strates that compact codes can be used for replacing orig-
inal features for zero-shot classiﬁcation. Results in Table
2 show our advantages in the other four datasets. Accord-
ing to the dimension of common space in different datasets,
the M is set to 30, 129, 10 and 5 for CUB, SUN, AWA
and aPY respectively. Compared to other baselines, our ap-
proach obtains highest ts and H values in all the datasets.
As we deﬁne the orthogonal semantic space, class centers
are more discriminative. This is the main reason to achieve
improvement. More importantly, the product quantization
can quantize unseen classes with little errors, which slight-
ly decreases the search accuracy.

5. Conclusions

In this paper, we propose a novel Product Quantization
Zero-Shot Learning method, which learns product quantiz-

er from seen classes to quantize unseen classes.
In this
way, the database can be compressed and stored as com-
pact codes for efﬁcient nearest neighbor search, which is
helpful to large-scale classiﬁcation. Given visual features
and semantic attributes, the quantizer as well as embed-
dings, are learned iteratively. Experimental results in syn-
thesized datasets demonstrate that codebook can well repre-
sent unseen classes. More importantly, the search speed is
improved when ANN search is employed. The proposed
method also achieves the state-of-the-art performance in
GZSL in ImageNet and four medium-scale datasets.

Acknowledgements

This work was supported in part by the key project of Trico-
Robot plan of NSFC under Grant No.91748208, Nation-
al Science and Technology Major Project under Grant No.
2018ZX01028-101, key Project of Shaanxi province un-
der Grant No.2018ZDCXLGY0607, and NSFC under Grant
No.61573268.

85470

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-
embedding for image classiﬁcation.
IEEE Transactions
on Pattern Analysis and Machine Intelligence, 38(7):1425–
1438, 2016. 3, 8

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, pages 2927–2936, 2015. 3, 8

[3] Y. Annadani and S. Biswas. Preserving semantic relations
for zero-shot learning. In CVPR, pages 7603–7612, 2018. 3,
8

[4] L. J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Pre-
dicting deep zero-shot convolutional neural networks using
textual descriptions. In ICCV, pages 4247–4255, 2015. 3

[5] A. Babenko and V. Lempitsky. Additive quantization for ex-
treme vector compression. In CVPR, pages 931–938, 2014.
2

[6] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Syn-
thesized classiﬁers for zero-shot learning. In CVPR, pages
5327–5336, 2016. 3, 7, 8

[7] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-
ical study and analysis of generalized zero-shot learning for
object recognition in the wild. In ECCV, pages 52–68, 2016.
2

[8] L. Chen, H. Zhang, J. Xiao, W. Liu, and S.-F. Chang. Zero-
shot visual recognition using semantics-preserving adversar-
ial embedding network. In CVPR, pages 1043–1052, 2018.
8

[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255, 2009. 1, 2, 6, 7

[10] M. Elhoseiny, B. Saleh, and A. Elgammal. Write a classiﬁ-
er: Zero-shot learning using purely textual descriptions. In
CVPR, pages 2584–2591, 2013. 1

[11] M. Elhoseiny, Y. Zhu, H. Zhang, and A. Elgammal. Link
the head to the” beak”: Zero shot learning from noisy text
description at part precision.
In CVPR, pages 6288–6297,
2017. 1

[12] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing
objects by their attributes. In CVPR, pages 1778–1785, 2009.
7

[13] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In NIPS, pages 2121–2129, 2013. 3, 8

[14] Y. Fu and L. Sigal. Semi-supervised vocabulary-informed

learning. In CVPR, pages 5337–5346, 2016. 1

[15] F. R. Gantmacher. Matrix theory. Chelsea, New York, 21,

1959. 6

[16] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quanti-
zation. IEEE transactions on pattern analysis and machine
intelligence, 36(4):744–755, 2014. 2, 3, 6

[17] Y. Guo, G. Ding, J. Han, and Y. Gao. Sitnet: Discrete simi-
larity transfer network for zero-shot hashing. In IJCAI, pages
1767–1773, 2017. 5

[18] D. Jayaraman, F. Sha, and K. Grauman. Decorrelating se-
In

mantic visual attributes by resisting the urge to share.
CVPR, pages 1629–1636, 2014. 4

[19] H. Jegou, M. Douze, and C. Schmid. Product quantization
for nearest neighbor search.
IEEE transactions on pattern
analysis and machine intelligence, 33(1):117–128, 2011. 2,
3, 4

[20] H. Jiang, R. Wang, S. Shan, and X. Chen. Learning class
prototypes via structure alignment for zero-shot recognition.
In ECCV, pages 118–134, 2018. 8

[21] P. Kankuekul, A. Kawewong, S. Tangruamsub,

and
O. Hasegawa. Online incremental attribute-based zero-shot
learning. In CVPR, pages 3657–3664, 2012. 4

[22] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder
for zero-shot learning. In CVPR, pages 3174–3183, 2017. 3,
5, 8

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, pages 1097–1105, 2012. 1

Imagenet
In

[24] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 36(3):453–465, 2014. 1, 3, 4

[25] Y. Linde, A. Buzo, and R. Gray. An algorithm for vec-
tor quantizer design. IEEE Transaction on Communication,
28(1):84–95, 1980. 3

[26] L. Liu, M. Yu, and L. Shao. Latent structure preserving hash-
ing. International Journal of Computer Vision, 122(3):439–
457, 2017. 1

[27] Y. Liu, Q. Gao, J. Li, J. Han, and L. Shao. Zero shot learn-
ing via low-rank embedded semantic autoencoder. In IJCAI,
pages 2490–2496, 2018. 3, 8

[28] S. Lloyd. Least squares quantization in pcm. IEEE transac-

tions on information theory, 28(2):129–137, 1982. 3

[29] Y. Long, L. Liu, L. Shao, F. Shen, G. Ding, and J. Han. From
zero-shot learning to conventional supervised classiﬁcation:
Unseen visual data synthesis. In CVPR, pages 1627–1636,
2017. 3

[30] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.
Journal of machine learning research, 9(Nov):2579–2605,
2008. 4

[31] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning
by convex combination of semantic embeddings. 2013. 8

[32] G. Patterson, C. Xu, H. Su, and J. Hays. The sun attribute
database: Beyond categories for deeper scene understanding.
International Journal of Computer Vision, 108(1-2):59–81,
2014. 7

[33] M. Radovanovi´c, A. Nanopoulos, and M. Ivanovi´c. Hubs in
space: Popular nearest neighbors in high-dimensional data.
Journal of Machine Learning Research, 11(Sep):2487–2531,
2010. 3

[34] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep
representations of ﬁne-grained visual descriptions. In CVPR,
pages 49–58, 2016. 3

[35] B. Romera-Paredes and P. Torr. An embarrassingly simple
approach to zero-shot learning. In ICML, pages 2152–2161,
2015. 8

[36] F. Shen, C. Shen, W. Liu, and H. Tao Shen. Supervised dis-

crete hashing. In CVPR, pages 37–45, 2015. 1

95471

[37] Y. Shigeto, I. Suzuki, K. Hara, M. Shimbo, and Y. Mat-
sumoto. Ridge regression, hubness, and zero-shot learn-
ing. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 135–151, 2015. 3
[38] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-
shot learning through cross-modal transfer. In NIPS, pages
935–943, 2013. 3, 8

[39] J. J. Sylvester. Sur l’´equation en matrices px=xq. CR Acad.

Sci. Paris, 99(2):67–71, 1884. 5

[40] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9, 2015.
1

[41] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.

The caltech-ucsd birds-200-2011 dataset. 2011. 7

[42] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In

NIPS, pages 1753–1760, 2009. 1

[43] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In CVPR, pages 69–77, 2016. 3, 8

[44] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-
shot learning-a comprehensive evaluation of the good, the
bad and the ugly. arXiv preprint arXiv:1707.00600, 2017. 4,
7

[45] Z. Zhang and V. Saligrama. Zero-shot learning via semantic
similarity embedding. In ICCV, pages 4166–4174, 2015. 3,
8

[46] Z. Zhang and V. Saligrama. Zero-shot learning via join-
t latent similarity embedding. In CVPR, pages 6034–6042,
2016. 3

[47] X. Zhu, D. Anguelov, and D. Ramanan. Capturing long-tail
distributions of object subcategories. In CVPR, pages 915–
922, 2014. 1

[48] Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal. A
generative adversarial approach for zero-shot learning from
noisy texts. In CVPR, pages 1004–1013, 2018. 1

105472

