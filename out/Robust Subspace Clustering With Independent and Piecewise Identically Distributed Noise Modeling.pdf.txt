Robust Subspace Clustering with Independent and Piecewise Identically

Distributed Noise Modeling

Yuanman Li, Jiantao Zhou, Xianwei Zheng, Jinyu Tian, Yuan Yan Tang

Department of Computer and Information Science, University of Macau, Macau, China

{yuanmanli, jtzhou, yb47430, yb77405, yytang}@um.edu.mo

Abstract

Most of the existing subspace clustering (SC) frame-
works assume that the noise contaminating the data is gen-
erated by an independent and identically distributed (i.i.d.)
source, where the Gaussianity is often imposed. Though
these assumptions greatly simplify the underlying problem-
s, they do not hold in many real-world applications. For
instance, in face clustering, the noise is usually caused by
random occlusions, local variations and unconstrained il-
luminations, which is essentially structural and hence sat-
isﬁes neither the i.i.d. property nor the Gaussianity. In this
work, we propose an independent and piecewise identically
distributed (i.p.i.d.) noise model, where the i.i.d. property
only holds locally. We demonstrate that the i.p.i.d. model
better characterizes the noise encountered in practical sce-
narios, and accommodates the traditional i.i.d. model as
a special case. Assisted by this generalized noise model,
we design an information theoretic learning (ITL) frame-
work for robust SC through a novel minimum weighted error
entropy (MWEE) criterion. Extensive experimental result-
s show that our proposed SC scheme signiﬁcantly outper-
forms the state-of-the-art competing algorithms.

1. Introduction

Many practical high-dimensional data usually lie in a
low-dimensional structure, rather than being uniformly dis-
tributed over the ambient space [30, 36, 3, 5, 44]. Some rep-
resentative examples include feature trajectories of rigidly
moving objects in a video [30], face images of one subjec-
t [36], and the spectra of one instance in a hyperspectral
image [3]. As a result, a collection of data from multiple
categories can be regarded as the ones lying in a union of
low-dimensional subspaces [5]. Subspace clustering (SC)
refers to the problem of separating the data points accord-
ing to their underlying subspaces, and has found numerous
applications in motion segmentation [22, 43], image clus-
tering [20, 21], data representation [14], etc.

There are many different types of SC approach proposed,

e.g., the algebraic [17], the statistical [10], the iterative [41],
and the spectral clustering based [5, 20] algorithms. In this
work, we focus on the SC approaches based on the spec-
tral clustering [5, 20], due to their state-of-the-art perfor-
mance provided. Within the framework of spectral cluster-
ing based methods, an afﬁnity matrix indicating the similar-
ity between pairs of the data points is ﬁrst built, and then the
data points are separated by applying the spectral clustering
[25] on this afﬁnity matrix. The primary difference of vari-
ous spectral clustering-based algorithms lies in how to learn
a robust subspace representation (SR) of each data point,
which seriously affects the clustering performance. Typi-
cally, the task of learning a robust SR is cast into a certain
optimization problem, usually consisting of two terms: the
ﬁdelity term as well as the regularization term. The majority
of the previous efforts along this line focused on designing
the regularization functions with desirable properties, such
as sparsity [5, 19], low-rankness [20, 33], manifold struc-
tures [28], or a combination of them [37, 42].

On the other hand, the studies on the ﬁdelity term es-
sentially accounting for the noise effect to robust SR are
relatively limited. For the analytical tractability and the
low complexity, most SC approaches simply adopted mean
square error (MSE) criterion, which provides the optimal-
ity only when the noise is i.i.d. Gaussian [2]. Because of
this limitation, MSE-based frameworks are very sensitive to
the non-Gaussian noise [8, 29, 39]. Besides, MSE criteri-
on only considers the second order statistics and may fail to
capture sufﬁcient statistical information of the noise signal.
To remedy these drawbacks, information theoretic learning
(ITL) [15, 29, 23, 7, 8] has been recently suggested to han-
dle non-Gaussian noise, and successfully applied to image
recognition [12, 34]. Speciﬁcally, ITL aims to ﬁnd the solu-
tion that produces the coding residual with the minimal in-
formation [7, 34]. To this end, ITL replaces the MSE crite-
rion with the one based on information theoretic measures,
e.g., correntropy [23] and R´enyi’s entropy [6, 40]. Com-
pared with MSE, ITL does not make Gaussianity assump-
tion, and can exploit higher orders of statistical information
of the signal [15]. Despite these desirable properties, all the

8720

(a)

(b)

(c)

(d)

Figure 1. Four images corrupted with the additive white Gaussian
noise (standard deviation 50). They have the same backgrounds
and the same number of black pixels. The black pixels in (a)-(c)
exhibit structural patterns, while the ones in (d) is purely random.

existing ITL-based algorithms still imposed i.i.d. assump-
tion on the noise, namely, all the noise samples are generat-
ed from the same underlying distribution and no correlation
exists among them. Unfortunately, such i.i.d. assumption
often does not hold in reality. In many practical settings,
different portions of the noise could have different statis-
tical behaviors, exhibiting certain structures. A persuasive
example is shown in Fig. 1. If we naively model the sig-
nals in Figs. 1(a)-(d) with an i.i.d. source, then all these
four signals would have the same amount of information in
terms of the traditional entropy [40]. Apparently, this i.i.d.
noise model leads to inaccurate information estimations as
the signal in Fig. 1(d) should have much larger amount of
information from the view of information theory [11]. The
aforementioned phenomenon calls for a more generic noise
model that can better characterize the statistical behavior of
the noise encountered in various practical scenarios.

In this work, we present a new robust SC algorithm
through a more generic noise model called independent and
piecewise identically distributed (i.p.i.d.) model, where we
use a union of distributions, rather than a single one, to char-
acterize the statistical behavior of the underlying noise. To
the best of our knowledge, this is probably the ﬁrst SC ap-
proach explicitly built upon a generic non-i.i.d. noise mod-
eling. The major contributions of our work are as follows:

1. Our framework makes neither the i.i.d. nor Gaussian-
ity assumptions on the noise, leading to the essential
difference from the existing SC approaches.

2. We develop a novel minimum weighted error entropy
(MWEE) criterion for the robust SC, through an i.p.i.d.
noise model. We demonstrate its effectiveness in ex-
ploiting the inherent statistical information of the noise
(including structural and purely random ones).

3. We design a relaxation technique to solve the optimiza-
tion problem for the robust SC under the MWEE crite-
rion, and an efﬁcient implementation can be achieved.

4. The proposed MWEE criterion could be regarded as a
general technique and readily incorporated into many
existing learning systems to improve the robustness a-
gainst various types of practical noise.

The rest of the paper is organized as follows. Section 2
reviews the spectral clustering-based SC. Section 3 presents
the i.p.i.d. noise model. Section 4 introduces the MWEE-
based SC algorithm and its optimization. Experimental re-
sults are given in Section 5 and Section 6 concludes.

2. Review of the Spectral Clustering-based SC

Let {Sk}K

k=1 be a union of K linear subspaces of RN ,

and {xi}n

i=1 be a collection of n observed data. Deﬁne

X = [x1, ..., xn] = [X1, ..., XK]P

(1)

where P is a permutation matrix, and Xi ∈ RN×nk con-
tains the nk data points lying in the subspace Si. Given
the data matrix X, the goal of SC is to correctly separate
the data points {xi}n
i=1 into their underlying subspaces. In
this work, we focus on the methods based on the spectral
clustering [5, 20], which generally comprise two steps: i)
learning an afﬁnity matrix indicating the similarity between
pairs of the data; and ii) obtaining the clustering results by
applying the spectral clustering to the learned afﬁnity ma-
trix. The crucial difference among various SC algorithms
lies in the techniques on how to learn the afﬁnity matrix.

The state-of-the-art methods for learning the afﬁnity ma-
trix are based on the robust SR. According to the subspace
learning, each data point can be effectively represented as a
linear combination of the other points in X, i.e.,

X = XZ, diag(Z) = 0,

(2)

where X is the self-expressive dictionary and Z serves as
the representation coefﬁcient matrix. Generally, the solu-
tion of Z is not unique, due to the fact that rank(Xk) < nk.
To tackle this challenge, a commonly used technique is to
incorporate the prior-domain knowledge, and solve the fol-
lowing regularized optimization problem

min

Z

R(Z), s.t. X = XZ, diag(Z) = 0,

(3)

where R(·) is a certain regularization function. In practice,
X is often observed with various kinds of noise, i.e.,

X = Xo + Eo,

(4)

where Xo is the noise-free data matrix and Eo denotes the
noise term. Then (2) can be rewritten as

X = XZ + E, diag(Z) = 0,

(5)

where E = Eo − EoZ. In the presence of noise, we usually
consider the following problem

min

Z

R(Z), s.t. L(X − XZ) < ǫ, diag(Z) = 0.

(6)

Here L(·) is the ﬁdelity function designed according to the
noise behavior, and R(·) represents the regularization func-
tion, which has been devised in many forms with different

8721

priors on the subspace structures. For example, ℓ1-norm
leads to the subspace-sparse representation [36, 5], and nu-
clear norm results in the subspace-low-rank representation
[20]. Once obtaining Z, the afﬁnity matrix M can be in-
duced from Z, e.g., M = |Z| + |ZT |.

At step ii), we apply the spectral clustering [25] to M,

and eventually obtain the clustering results.

3. Construction of the i.p.i.d. Noise Model

Compared with R(·), the studies on the ﬁdelity func-
tion L(·) to characterize the noise behavior are relatively
limited. For simplicity, most of the existing SC approach-
es adopted MSE for the ﬁdelity term, naively modeling the
noise with i.i.d. Gaussian distribution. Though i.i.d. Gaus-
sianity assumption greatly simpliﬁes the underlying prob-
lems, it does not hold in many real-world scenarios. In this
section, we present a generic noise model i.p.i.d., where the
i.i.d. property is satisﬁed only in a piecewise fashion. Later,
we will show that the proposed i.p.i.d. noise model leads to
a new design of L(·) under the framework of SC given in
(6). As neither i.i.d. nor Gaussianity assumptions are im-
posed, the resultant SC scheme exhibits superior robustness
against various types of noise encountered in practice.

3.1. Deﬁnition of the i.p.i.d. source and its properties

We deﬁne the 1-D i.p.i.d. source as follows.

Deﬁnition 3.1 Suppose that x = [x1, ..., xN ] ∈ RN is a
sequence of N independent samples. Let {Pi}L
i=1 be a
non-overlapping, sequential partition of the index vector
[1, 2, · · · , N ], i.e.,
Pi = {ni−1 + 1, ni−1 + 2, · · · , ni}, i ∈ {1, · · · , L}, (7)
where n0 = 0, ni < ni+1, and nL = N . The sequence x
is said to be generated by an i.p.i.d. source, if there exists a
union of probability density functions {fi}L

i=1, such that

xni−1+1, xni−1+2, · · · , xni

i.i.d.
∼ fi, i ∈ {1, · · · , L}.

(8)

The above deﬁnition can be readily extended to signal-
s in higher dimensional space, e.g., images and videos. A
somewhat similar deﬁnition was also given in [35] for the
binary source coding. With the Deﬁnition 3.1, the i.p.i.d.
source has the following properties. Locality: the i.p.i.d.
source can well exploit the local behavior of a signal, which
is different from a purely i.i.d. source; Fine-description,
the i.p.i.d. source characterizes a signal using a union of
density functions rather than a single one, providing it more
powerful descriptive capability to describe a complex sig-
nal. Generalization: the traditional i.i.d. source is a special
case of the i.p.i.d. source with L = 1.

Owning to these desirable properties, an i.p.i.d. source
can describe both structural signals (as shown in Figs. 1(a)-
(c)) and purely random ones (as shown in Fig. 1(d)). For ex-
ample, Fig. 1(a) can be satisfactorily modeled by an i.p.i.d.

Figure 2. An illustration of the illumination noise. (a) observed
image, (b) latent image, (c) noise, (d) an example of the synthetic
noise generated by an i.p.i.d. source.

source with L = 2, where the dark region and the back-
ground originate from two different distributions. A more
illustrative example is given in Figs. 2(a)-(c), where the
noise is dominated by the unconstrained illumination. Ob-
viously, such noise cannot be appropriately modeled with
any i.i.d. source; but it can be well characterized by the
i.p.i.d. model. To clarify this point, we show a synthetic
image generated by an i.p.i.d. source in Fig. 2(d). This syn-
thetic image is produced in a way that each disjoint 8 × 8
patch is generated by a Gaussian distribution, with the mean
gradually decreased by a constant 0.5 from left to right. We
can observe that the synthetic image can well approximate
the behavior of the illumination noise shown in Fig. 2(c).

3.2. R´enyi’s entropy of an i.p.i.d. Source

We now discuss how to estimate the information of a
signal under the i.p.i.d. model, which is crucial for the pro-
posed robust SC scheme. We ﬁrst review the traditional
R´enyi’s entropy of an i.i.d. source. Let E be a random
variable, and its R´enyi’s entropy with the order α (α > 0
and α 6= 1) is deﬁned as

Hα(E) =

1

1 − α

log(cid:16)Z (fE(e))αde(cid:17).

(9)

In practice, the probability density function fE(e) is gen-
erally unknown. Parzen window estimation [27] is a com-
monly adopted algorithm to approximate fE(e) using ﬁnite
samples {ei}N

i=1, which is given by

ˆfE(e) =

1
N

N

Xi=1

κσ(e − ei),

(10)

where κσ(·) is the Gaussian kernel and σ is the kernel size.
Note that an important assumption of the Parzen window
estimation is that the samples {ei}N

i=1 are i.i.d.

To differentiate from the above traditional R´enyi’s en-
tropy estimator deﬁned under the i.i.d. assumption, we call
our estimator under the i.p.i.d. assumption as the piecewise
R´enyi’s entropy (PRE). As a well-known fact, the tradition-
al entropy reﬂects the smallest number of bits on average, to
represent a symbol generated by an i.i.d. source. To be con-
sistent with this rule for an i.p.i.d. source, we deﬁne PRE as
follows:

8722

Deﬁnition 3.2 Suppose that e = [e1, e2, .., eN ] is a se-
quence generated by an i.p.i.d. source with the index par-
tition {Pq}L
q=1. Its information estimator PRE is given by

ˆHα(e) =

1

1 − α Xq

|Pq|
N

logZ (cid:16)fEq (e)(cid:17)α

de,

(11)

where fEq (e) is the probability density function estimated
with the samples indexed by the partition Pq, i.e.,

fEq (e) =

1

|Pq| Xi∈Pq

κσ(e − ei).

(12)

It can be shown that the PRE reduces to the traditional
entropy Hα(e) when the signal is actually i.i.d.; otherwise
if the signal is i.p.i.d., PRE can more precisely exploit the
entropy information. Without loss of generality, we set α =
2, and the resulting estimator is denoted by ˆH2(e).

It should be noted that calculating ˆH2(e) is not straight-
forward in reality, because the partitions {Pq}L
q=1 are gen-
erally unknown. In fact, we believe that it is rather chal-
lenging, if possible, to estimate the partitions from the given
data. This is typically true when the signal is very complex
or gradually varied (see e.g., Fig. 2). Fortunately, thanks to
the properties of the i.p.i.d. source, we can still approximate
ˆH2(e) without explicitly knowing the partitions {Pq}L
q=1.
Speciﬁcally, for the data samples in a sufﬁciently small lo-
cal region, they can reasonably be assumed as i.i.d., accord-
ing to the locality property of the i.p.i.d. source. Then we
can estimate the probability density function for each small
local region, and approximate ˆH2(e) by taking the average
over all the local regions by resorting to Deﬁnition 3.2.

Speciﬁcally, let Iq be the location of eq in the original
data space 1. For each location Iq, we ﬁrst construct a local
region, denoted by ΩIq, centered at Iq. Then we estimate
the probability density function for ΩIq as

fEIq (e) =

1

|ΩIq| Xi∈ΩIq

κσ(e − ei),

(13)

Note that (12) and (13) are equivalent if ΩIq happens to
be the actual partition Pq. In the sequel, we use the notation
fIq (e) instead of fEIq (e) for simplicity.

However, the number of samples in a small local region
is often insufﬁcient for the density estimation. Alternative-
ly, we propose to estimate fIq (e) by using all the samples in
e by introducing a weighting function, potentially achieving
more accurate estimation. A somewhat similar strategy was
employed in [1] for density estimation with a few samples.
To preserve the locality property of the i.p.i.d. source, when
estimating fIq (e), we assign larger weights to the samples
with smaller distances to Iq. Concretely, we deﬁne the dis-
tance of two locations Ii and Ij in the data space as

1For the 1-D signal (e.g., voice), Iq is a scalar. For the 2-D signal (e.g.,

image), Iq is a 2-D index.

Di,j = Dis(Ii, Ij),

(14)

where Dis(·) is a certain distance function, e.g., ||·||2. Then
the probability density function for ΩIq is estimated by

ˆfIq (e) =

N

Xi=1

c(Dq,i)κσ(e − ei).

(15)

Here c(·) is an appropriately designed weighting function.
In our work, we simply choose c(·) as a Gaussian function

c(Dq,i) =

(Dq,i )2
σ2
w ,

e−

1
Q

(16)

where Q is the normalizer such that PIi
w is empirically set as N
σ2
Parzen window (WPW) estimation.

c(Dq,i) = 1, and
1000 . We then call (15) the weighted

Upon estimating the density ˆfIq(e) for each ΩIq, the
PRE ˆH2(e) can then be approximated by ¯H2(e) through
taking the average over all the locations, i.e.,

¯H2(e) = −

1

N XIq

logZ (cid:16) ˆfIq (e)(cid:17)2

de

= −

1

N XIq

log

N

Xi,j=1

c(Dq,i)c(Dq,j)κ√2σ(ei − ej). (17)

3.3. Relationship among H2(e), ˆH2(e) and ¯H2(e)

Compared with ˆH2(e) deﬁned in (11), ¯H2(e) does not
need to know the partitions {Pq}L
q=1 explicitly. Further-
more, it can be proved below that ¯H2(e) derived under
the i.p.i.d. assumption is still equivalent to the traditional
R´enyi’s entropy H2(e) under the i.i.d. setting.

Theorem 3.1 Suppose that the signal elements in e =
[e1, e2, .., eN ] are independently sampled from the same dis-
tribution f (e). Then the PRE estimator ¯H2(e) deﬁned in
(17) is equivalent to the traditional R´enyi’s entropy H2(e)
given by (9) and (10).

The proof is given in the supplementary ﬁle. It provides a
fundamental theoretical basis for the capability of ¯H2(e) to
characterize the i.i.d. source.

To further show the relationship among the traditional
R´enyi’s entropy H2(e), the PRE ˆH2(e) and its approxima-
tion ¯H2(e), we give a toy example here by generating an
i.p.i.d. sequence e = [e1, e2, ..., eN ] (N = 2000) with L
partitions. For the q-th partition, the samples are indepen-
dently generated by a Gaussian distribution

fEq (e) =

1

p2πq2

exp(cid:16)−

(e − µ)2

2q2

(cid:17),

(18)

where µ = 200
L (q −1). For simplicity, each partition has the
same number of samples, i.e., ⌊N/L⌋. Obviously, L = 1

8723

L=1
L=10

250

200

150

100

50

0

0

200

400

600

800

1000 1200 1400 1600 1800 2000

 Indices of generated samples

Figure 3. Two examples of the i.p.i.d. sequence.

8

7

6

5

4

3

y
p
o
r
t

n
E

 

2

1

3

5

7

 L

9

11

Figure 4. H2(e), ˆH2(e), ¯H2(e) and ˜H2(e) w.r.t. L

implies that the sequence e is generated by a standard nor-
mal distribution; while when L > 1, e is a 1-D signal with
structures. Fig. 3 shows two examples of the sequences
when L = 1 (red) and L = 10 (blue). Fig. 4 plots the curves
of different estimators with various number of partitions.
We can observe that under the i.i.d. case, i.e., L = 1, H2(e),
ˆH2(e) and ¯H2(e) are the same, which coincides with The-
orem 3.1. When L > 1, H2(e) becomes much larger than
the PRE ˆH2(e) and ¯H2(e). This is because H2(e) totally
ignores the structural information of e, and simply treats all
the signal elements as i.i.d. From Fig. 4, we can also ob-
serve that ¯H2(e) can well approximate ˆH2(e), even without
knowing the partitions.

4. Proposed SC Method and Its Optimization

Based on the proposed i.p.i.d. noise model, we now sug-

gest a new ﬁdelity function L(·) in (6) for the robust SC.

4.1. MWEE based sparse SC

In this work, we focus on the popular Sparse Subspace
Clustering (SSC) method [5], which adopts ℓ1-norm for
R(·) to achieve a subspace-sparse representation, while de-
signing L(·) under MSE criterion. Namely,

argmin

Z

||Z||1, s.t. ||X − XZ||2

F < ǫ, diag(Z) = 0. (19)

As aforementioned, the MSE criterion has many serious
limitations. Motivated by the great success of ITL to handle
non-Gaussian noise, we suggest to design a new ITL-type
ﬁdelity function L(·) through the proposed i.p.i.d. noise
model. Speciﬁcally, given a data point and a dictionary, the
ITL-based framework aims to ﬁnd a representation produc-
ing the coding residual with minimal information [12, 34].

In light of this motivation, we replace the Frobenius norm
in (19) with our proposed PRE, and we have

argmin

||Z||1, s.t. Φ(X − XZ) < ǫ, diag(Z) = 0, (20)

Z

where

Φ(X − XZ) =

n

Xi=1

¯H2(xi − Xzi),

(21)

and zi is the i-th column of Z.
In this work, we name
the criterion of minimizing ¯H2(e) as Minimum Weighted
Error Entropy (MWEE) criterion, due to the weighted na-
ture of ¯H2(e). Different from MSE and all the existing
ITL criteria, such as the ones based on correntropy [12]
and R´enyi’s entropy [40, 34], MWEE built upon the i.p.i.d.
model makes neither the i.i.d. nor Gaussianity assumptions.
The minimization target PRE can better reﬂect the inherent
information of the noise, no matter it is structural or pure-
ly random. As expected and will be shown experimentally,
our algorithm is very robust against various kinds of noise.
The problem (20) can be decomposed into n independent

subproblems, with the i-th one expressed as

||zi||1, s.t. ¯H2(xi − Xzi) < ǫi, zi,i = 0,

(22)

argmin
zi∈Rn

where ǫ = Pn

ﬁrst solve

i=1 ǫi. To handle the problem (22), we can

||z′i||1, s.t. ¯H2(xi − ˆXz′i) < ǫi.

(23)

argmin
i∈Rn−1
z′

where ˆX = [x1, .., xi−1, xi+1, .., xn]. Then the solution zi

of (22) is eventually computed by

zi = [z′i,1, ..., z′i,i−1, 0, z′i,i, ..., z′i,n−1].

Nevertheless, it is very difﬁcult to solve the problem
(23), since 1) ¯H2(e) deﬁned in (17) is a summation of
complex logarithmic functions; and 2) the kernel function
κ√2σ(·) is highly non-convex [12]. To tackle this challenge,
we now propose a relaxation technique for ¯H2(e) such that
the resulting problem can be efﬁciently solved.

4.2. Relaxation of ¯H2(e)

Deﬁne

cq
i,j = c(Dq,i)c(Dq,j),
where Dq,i is given in (14). Since PIq

N = 1, by the
convexity of the negative log function and the Jassen’s in-
equality, we have

(24)

1

¯H2(e) ≥ −logXIq

1
N

N

Xi,j=1

N

cq
i,jκ√2σ(ei − ej)

(25)

= −logXIq

Xi,j=1

cq
i,jκ√2σ(ei − ej) + logN.

8724

Figure 5. Illustration of the face clustering. Images are from the Extended Yale B database [18].

Algorithm 1 Half-quadratic algorithm for the problem (32)
Input: The data matrix A = [x1, .., xi−1, xi+1, .., xn], a data
point y = xi, the parameter λ and t = 0.
1: Calculate ˜y = [˜y1, ..., ˜yN ]T and ˜X = [˜xT
2: While ‘not converged’, do
3:

2σ2 κ√2σ(˜yi − ˜xizt), i = 1, 2, · · · , N

1 , ..., ˜xT

N ]T .

ut+1
i = 1
zt+1 = argmin

4:

(˜y − ˜Xz)T diag(ut+1)(˜y − ˜Xz) + λ||z||1

z

t = t + 1

5:
6: end while

Output: The representation vector z.

Algorithm 2 MWEE-based SC (MWEE-S)
Input: The data matrix X = [x1, ..., xn], the number of sub-

spaces K, the parameter λ.

1: Normalize the columns of X to have unit l2 norm.
2: Compute the representation matrix Z by solving (20) to deal

with linear subspaces , or solving (33) for afﬁne subspaces.

3: Construct the similarity matrix M = |Z| + |Z|T .
4: Apply the spectral clustering algorithm [25] to M.

Output: K clusters.

Let

where

˜H2(e) = −logS(e) + logN,

(26)

N

S(e) =

Xi,j=1

wi,jκ√2σ(ei − ej), wi,j = XIq

cq
i,j.

(27)

From (25), we can see that ˜H2(e) is a lower bound of
¯H2(e). It can be shown that when the signal elements in
e are i.i.d., the inequality in (25) holds with equality. Fur-
ther, ¯H2(e) and ˜H2(e) have the same minimizer e = c1 for
some constant c. In Fig. 4, we also plot the curve of ˜H2(e)
with different number of partitions. Motivated by the work
[16], we suggest to substitute ¯H2(e) with ˜H2(e) in (23).

Noticing that ˜H2(e) is monotonically decreasing w.r.t.
S(e), minimizing ˜H2(e) is equivalent
to minimizing
−S(e). By replacing ¯H2(e) with −S(e), and introducing a
Lagrange multiplier λ, we reformulate the problem (23) as

− S(xi − ˆXz′i) + λ||z′i||1.

(28)

argmin

z′
i

For the notation simplicity, we further let y = xi, A = ˆX
and z = z′i. Then, the problem (28) becomes

argmin

− S(y − Az) + λ||z||1.

(29)

z

By resorting to a similar strategy proposed in [34], we

approximate S(e) with

˜S(e) =

N

Xi=1

κ√2σ 


N

Xj=1

wi,j(ei − ej)
 .

(30)

Denote the i-th entry of y by yi and the i-th row of A by ai.

we have

Since Pj wi,j = 1 (proof given in the supplementary ﬁle),
wi,j ((yi − aiz) − (yj − aj z))!

˜S(y − Az) =

N

Xi=1
κ√2σ yi −

κ√2σ  N
Xj=1
wi,j yj − ai −
Xj=1

N

=

N

Xi=1

wi,j aj! z! .

N

Xj=1

(31)

Substituting S(y − Az) by ˜S(y − Az), the problem (29)
ﬁnally becomes

N

z

−

argmin

κ√2σ(˜yi − ˜xiz) + λ||z||1,

Xi=1
where ˜yi = yi −PN
j=1 wi,jyj and ˜xi = ai −PN

j=1 wi,j aj .
(32) can be efﬁciently solved via the half-quadratic theory
[26]. Algorithm 1 shows the optimization procedures, and
the detailed derivation is given in the supplementary ﬁle.

(32)

Remark: Note that in many cases, data points lie in a u-
nion of afﬁne subspaces rather than linear subspaces, as will
be discussed in Section 5.2. To deal with afﬁne subspaces,
we adopt the strategy suggested in [5], by introducing addi-
tional linear equality constraints in (20), i.e.,

argmin

||Z||1,

Z

s.t. Φ(X − XZ) < ǫ, ZT 1 = 1, diag(Z) = 0.

(33)

This problem can be efﬁciently solved by using a simi-
lar technique described in Algorithm 1, incorporating with
the Alternating Direction Method of Multipliers (ADMM)
method [5].

Upon having the representation matrix Z, we then build
the afﬁnity matrix by M = |Z| + |Z|T . Finally, we apply
spectral clustering [25] to M, and obtain the clustering re-
sults. The whole SC algorithm is summarized in Algorithm
2. In this work, we refer the MWEE-based SC algorithm to
as MWEE-S for short.

8725

Table 1. Clustering accuracy (%) of different algorithms on the
Extended Yale B.

Methods

LSA SSC0 SSC1 LRR

TSC L2-G S3C MWEE-S

2 subjects
4 subjects
6 subjects
8 subjects
10 subjects

71.09
42.58
45.05
33.98
32.50

99.22
75.39
85.94
60.35
53.75

96.89
92.97
94.01
93.75
87.19

97.66
93.75
96.62
75.59
76.56

97.66
91.80
93.49
90.43
86.41

98.44
98.44
98.44
97.66
96.56

99.22
99.22
95.83
94.92
94.69

100.0
100.0
100.0
100.0
99.84

Figure 6. Some simulated examples. Images from left to right are
randomly occluded by 0%, 10%, 20%, 30% and 40%, respectively.

Figure 7. Average clustering accuracy of different algorithms on
the Extended Yale B against 25% contiguous occlusion.

5. Experimental Results

We evaluate our proposed SC algorithm MWEE-S, in
dealing with two practical problems: face clustering and
motion segmentation. To embrace the concept of repro-
ducible research [32], the code of our paper will be available
upon the acceptance.

5.1. Face clustering

Given a set of facial images from multiple subjects, the
goal of face clustering is to separate them according to their
underlying subjects. An example is shown in Fig. 5. The
Extended Yale B dataset [18] is adopted in this exper-
iment, which contains 2432 frontal face images from 38
subjects, with 64 instances for each subject. Images in this
dataset are captured under various lighting conditions. For
efﬁciency, we resize all the images to 96 × 84. We compare
the performance of our MWEE-S in (20) with SSC0 [4], SS-
C1 [5], LRR [20], TSC [13], LSA [38], S3C [19] and L2-G
[28]. We use the codes provided by their authors with the
default parameter settings. More speciﬁcally, for S3C, we
adopt the soft S3C implementation, since it leads to the best
performance among all the variants [19]. For LRR, we use
the code newly updated [20]. The difference between SSC0
and SSC1 is that SSC0 adopts || · ||2
F as the ﬁdelity function
while SSC1 uses || · ||1. For our proposed MWEE-S, the
parameter λ is consistently set as 10−4.

5,

As shown in Fig.

the noise in the Extended
Yale B is mainly caused by the unconstrained illumina-
tion, which obviously satisﬁes neither the i.i.d. assumption
nor the Gaussianity. Table 1 reports the clustering accuracy
of different algorithms over the Extended Yale B, for
the ﬁrst 2, 4, 6, 8 and 10 subjects. We can see that when the
number of subjects increases, the performance of the MSE-

Figure 8. Average clustering accuracy of different algorithms a-
gainst various levels of occlusion.

based algorithms (such as SSC0 and LRR) drops rapidly.
This is because MSE criterion simply treats the noise as
i.i.d. Gaussian. By resorting to the MWEE-based criteri-
on derived under the i.p.i.d. noise model, our MWEE-S
outperforms the competing methods for all the cases. No-
tably, MWEE-S obtains 100% clustering accuracy when the
number of subjects is below 10, while only miss-clustering
one image when the number of subjects is 10. It can also
be seen that the recent works S3C [19] and L2-G [28] also
achieved rather good performance on this dataset. However,
S3C adopts more complex regularization functions, while
L2-G applies a post-processing on the representation coef-
ﬁcients. Furthermore, as will be clear soon, they are not
robust under more complex noise scenarios.

Effect of the contiguous block occlusion: We now test
the effectiveness of the proposed MWEE-S in the presence
of contiguous occlusions. For each facial image, we ﬁrst
randomly select a region, and then substitute it with an unre-
lated image patch. Speciﬁcally, the image ‘Baboon’ is used
for the occlusion simulation, which was adopted in [36, 34]
as well. Some examples are given in Fig. 6.

Note that the noise in this scenario can hardly be as-
sumed to be i.i.d., since it is the combination of the illu-
mination and the unrelated image ‘Baboon’, both of which
are highly structural. Fig. 7 plots the clustering accuracy of
different methods against 25% occlusion, over the facial im-
ages of various number of subjects. To alleviate the impact
of random occlusion positions, all the results are averaged
over 10 random runs. We can observe that our MWEE-S
achieves considerably better performance than the competi-
tors, and the gain margin becomes larger when the number
of subjects increases. Compared with the results in Table
1, it can be noticed that the performance drops severely,
for all the competing methods against 25% occlusions. In
contrast, the performance degradation of WMEE-S is much
more graceful. Such phenomenon further demonstrates that
the WMEE criterion designed under the i.p.i.d. can better
characterize the noise behavior.

Effect of the occlusion level: To investigate the impact
of different occlusion levels on the clustering performance,
we vary the occlusion level from 0 to 40%, while ﬁxing the
number of subjects to be 4. Fig. 8 depicts the results of dif-
ferent algorithms. As can be observed, our method achieves
the best clustering performance for all the occlusion level-

8726

Figure 9. Example frames with tracked features from three videos in Hopkins 155 [31]. Given feature points on multiple rigidly moving
objects tracked in a video (top), motion segmentation aims to separate the feature trajectories according to moving objects (bottom).

Table 2. Clustering error (%) of different algorithms on the
Hopkins155 database.

Methods

LSR SSC0 SSC1 LRR LRSC L2-G S3C MWEE-S

2F

4K

Avg.
Med.
Std.
Avg.
Med.
Std.

2.98
0.30
7.48
3.21
0.38
7.79

6.97
0.21
12.69
7.05
0.21
12.82

2.18
0.00
7.24
2.42
0.00
7.51

1.60
0.00
4.66
2.35
0.00
7.30

3.42
0.00
8.83
3.35
0.00
8.76

5.54
0.00
11.18
5.81
0.00
11.59

2.20
0.00
6.89
2.33
0.00
6.98

1.22
0.00
4.20
1.77
0.00
6.12

s. The performance gaps of our scheme over the competing
methods are quite remarkable, especially when the occlu-
sion level is between 10% to 30%.

5.2. Motion segmentation

Motion segmentation aims to segment a video sequence
with multiple rigidly moving objects into several spatiotem-
poral regions, each of which corresponds to one moving ob-
ject in the scene. Fig. 9 shows some examples of three video
sequences, where we only draw two frames with tracked
points for each video. Let F be the number of frames in a
video sequence. Generally, the motion segmentation prob-
lem can be solved by ﬁrst tracking the spatial positions of
n feature points xf,i ∈ R2 (f = [1, .., F ], i = [1, .., n])
across the frames of the video, and then clustering the fea-
ture point trajectories according to their underlying motions
[5, 19]. Speciﬁcally, the trajectory of the i-th feature point is
formed by stacking its spatial positions in the video, namely

xi = [xT

1,i, xT

2,i, .., xT

F,i]T ∈ R2F .

(34)

Then all the trajectories of a video can be represented by
a matrix X = [x1, x2, ..., xn]. Since the trajectories of a
rigid motion lie in an afﬁne subspace of R2F of dimension
at most 3 [5], the feature trajectories of K rigid motions lie
in a union of K subspaces of R2F .

We adopt the Hopkins155 database [31] for this exper-
iment, which provides an extensive benchmark for testing
many subspace segmentation methods. The Hopkins155
database consists of 156 video sequences (hence 156 sub-
space clustering tasks), with 2 or 3 motions in each video.
The feature points are extracted and tracked across frames.
On average, each video of 2 motions has 266 trajectories
and 30 frames, while each video of 3 motions has 398 tra-
jectories and 29 frames. Some example frames are given
in Fig. 9, where the feature points from one moving object

are marked in the same color (bottom). For the motion seg-
mentation task, we adopt MWEE-S in (33) tailored for the
afﬁne subspace, and compare it with LSR[24], SSC0 [4],
SSC1 [5], LRR [20], LRSC [9], L2-G [28], S3C[19]. The
difference between SSC0 and SSC1 is that SSC1 uses the
afﬁne constraint ZT 1 = 1, while SSC0 does not. For LSR,
we utilize the LSR1 implementation.

The ﬁrst experiment is conducted on the original 2F -
dimensional data. Since the rank of each linear subspace
is at most 4 [5], we also do the experiment on the 4K-
dimensional data, where K is the number of motions in each
sequence. PCA is adopted for the dimension reduction. The
clustering results are summarized in Table 2, where we re-
port the average, median and standard deviation of the clus-
tering errors. Generally, the clustering error of each method
increases when the data dimension is reduced from 2F to
4K, due to the information loss. However, in both cases,
the proposed method signiﬁcantly outperforms all the com-
peting algorithms, in terms of both the average clustering
error and standard deviation. This implies that modeling
the noise of trajectories with an i.p.i.d. source, other than
an i.i.d. Gaussian, indeed helps for motion segmentation.

6. Conclusions

This paper has presented a new robust SC approach via
the i.p.i.d. noise modeling. Different from the tradition-
al SC algorithms, our method makes neither the i.i.d. nor
Gaussianity assumptions on the noise. Based on the pro-
posed i.p.i.d. model, we have developed a novel optimiza-
tion criterion MWEE, which can well characterize the in-
herent information of the noise, despite it is structural or
purely random. Such desirable properties make the devel-
oped SC method very robust against various kinds of noise
encountered in practice. In fact, the proposed MWEE crite-
rion can be readily integrated into many learning systems,
which could provide better performance than the state-of-
the-art methods, as will be demonstrated in our future work.
Acknowledgments: This work was supported in part by
the Macau Science and Technology Development Fund un-
der Grants FDCT/022/2017/A1 and FDCT/077/2018/A2,
and in part by the Research Committee at the Univer-
sity of Macau under Grant MYRG2016-00137-FST and
MYRG2018-00029-FST.

8727

References

[1] G. A. Babich and O. I. Camps. Weighted parzen windows
for pattern classiﬁcation. IEEE Trans. Pattern Anal. Mach.
Intell., 18(5):567–570, 1996. 4

[2] C. M. Bishop. Pattern Recognition and Machine Learning.

Springer, 2006. 1

[3] Y. Chen, N. M. Nasrabadi, and T. D. Tran. Hyperspectral im-
age classiﬁcation using dictionary-based sparse representa-
tion. IEEE Trans. Geosci. Remote Sens., 49(10):3973–3985,
2011. 1

[4] E. Elhamifar and R. Vidal. Sparse subspace clustering. In
Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 2790–
2797, 2009. 7, 8

[5] E. Elhamifar and R. Vidal. Sparse subspace clustering: Al-
gorithm, theory, and applications. IEEE Trans. Pattern Anal.
Mach. Intell., 35(11):2765–2781, 2013. 1, 2, 3, 5, 6, 7, 8

[6] D. Erdogmus, K. E. Hild, J. C. Principe, M. Lazaro, and
I. Santamaria. Adaptive blind deconvolution of linear chan-
nels using renyi’s entropy with parzen window estimation.
IEEE Trans. Signal Process., 52(6):1489–1498, 2004. 1

[7] D. Erdogmus and J. C. Principe. An error-entropy minimiza-
tion algorithm for supervised training of nonlinear adaptive
systems.
IEEE Trans. Signal Process., 50(7):1780–1786,
2002. 1

[8] D. Erdogmus and J. C. Principe. From linear adaptive ﬁl-
tering to nonlinear information processing - the design and
analysis of information processing systems.
IEEE Signal
Process. Mag., 23(6):14–33, 2006. 1

[9] P. Favaro, R. Vidal, and A. Ravichandran. A closed form so-
lution to robust subspace estimation and clustering. In Proc.
IEEE Conf. Comput. Vis. Pattern Recogn., pages 1801–1807,
2011. 8

[10] A. Gruber and Y. Weiss. Multibody factorization with un-
certainty and missing data using the em algorithm. In Proc.
IEEE Conf. Comput. Vis. Pattern Recogn., volume 1, pages
707–714. IEEE, 2004. 1

[11] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of

Statistical Learning. Springer, New York, 2009. 2

[12] R. He, W. S. Zheng, and B. G. Hu. Maximum correntropy
IEEE Trans. Pattern

criterion for robust face recognition.
Anal. Mach. Intell., 33(8):1561–1576, 2011. 1, 5

[13] R. Heckel and H. B¨olcskei. Robust subspace clustering vi-
a thresholding. IEEE Trans. Signal Process., 61(11):6320–
6342, 2015. 7

[14] W. Hong, J. Wright, K. Huang, and Y. Ma. Multiscale hybrid
linear models for lossy image representation. IEEE Trans.
Image Process., 15(12):3655–3671, 2006. 1

[15] T. Hu, J. Fan, Q. Wu, and D.-X. Zhou. Learning theory ap-
proach to minimum error entropy criterion. J. Mach. Learn.
Res., 14:377–397, 2013. 1

[16] T. Huang, Y. Liu, H. Meng, and X. Wang. Adaptive com-
IEEE

pressed sensing via minimizing cramer?rao bound.
Sig. Process. Lett., 21(3):270–274, 2014. 6

[18] K.-C. Lee, J. Ho, and D. J. Kriegman. Acquiring linear sub-
spaces for face recognition under variable lighting.
IEEE
Trans. Pattern Anal. Mach. Intell., 27(5):684–698, 2005. 6,
7

[19] C. G. Li, C. You, and R. Vidal. Structured sparse subspace
clustering: A joint afﬁnity learning and subspace clustering
framework. IEEE Trans. Image Process., 26(6):2988–3001,
2017. 1, 7, 8

[20] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. Ro-
bust recovery of subspace structures by low-rank represen-
tation. IEEE Trans. Pattern Anal. Mach. Intell., 35(1):171–
184, 2013. 1, 2, 3, 7, 8

[21] G. Liu, Z. Lin, and Y. Yu. Robust subspace segmentation
by low-rank representation. In Proc.Int. Conf. Mach. Learn,
pages 663–670, 2010. 1

[22] G. Liu and S. Yan. Latent low-rank representation for sub-
In Proc. IEEE

space segmentation and feature extraction.
Int. Conf. Comput. Vis., pages 1615–1622. IEEE, 2011. 1

[23] W. Liu, P. P. Pokharel, and J. C. Principe. Correntropy:
Properties and applications in non-gaussian signal process-
ing. IEEE Trans. Signal Process., 55(11):5286–5298, 2007.
1

[24] C.-Y. Lu, H. Min, Z.-Q. Zhao, L. Zhu, D.-S. Huang, and
S. Yan. Robust and efﬁcient subspace segmentation via least
squares regression. In Proc. Eur. Conf. Comput. Vis.,, pages
347–360. Springer, 2012. 8

[25] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral cluster-
ing: Analysis and an algorithm. In Proc. Neural Information
Processing Systems,, pages 849–856, 2002. 1, 3, 6

[26] M. Nikolova and M. K. Ng. Analysis of half-quadratic min-
imization methods for signal and image recovery. SIAM J.
Sci. Comput., 27(3):937–966, 2005. 6

[27] E. Parzen. On estimation of a probability density func-
The Annals of Mathematical Statistics,

tion and mode.
33(3):1065–1076, 1962. 3

[28] X. Peng, Z. Yu, Z. Yi, and H. Tang. Constructing the l2-
graph for robust subspace learning and subspace clustering.
IEEE Trans. on Cybern., 47(4):1053–1066, 2017. 1, 7, 8

[29] J. C. Principe. Information Theoretic Learning: Renyi’s En-
tropy and Kernel Perspectives. Springer, New York, 1st edi-
tion, 2010. 1

[30] C. Tomasi and T. Kanade. Shape and motion from image
streams under orthography: a factorization method. Int. J.
Comput. Vis.,, 9(2):137–154, 1992. 1

[31] R. Tron and R. Vidal. A benchmark for the comparison of
In Proc. IEEE Conf.

3-d motion segmentation algorithms.
Comput. Vis. Pattern Recogn., pages 1–8, 2007. 8

[32] P. Vandewalle, J. Kovacevic, and M. Vetterli. Reproducible
research in signal processing. IEEE Signal Process. Mag.,
26(3):37–47, 2009. 7

[33] R. Vidal and P. Favaro. Low rank subspace clustering

(LRSC). Pattern Recognit. Lett., 43:47 – 61, 2014. 1

[34] Y. Wang, Y. Y. Tang, and L. Li. Robust face recognition via
minimum error entropy-based atomic representation. IEEE
Trans. Image Process., 24(12):5868–5878, 2015. 1, 5, 6, 7

[17] K.-i. Kanatani. Motion segmentation by subspace separation
and model selection. In Proc. IEEE Int. Conf. Comput. Vis.,
volume 2, pages 586–591. IEEE, 2001. 1

[35] F. M. J. Willems. Coding for a binary independent piecewise-
identically-distributed source. IEEE Trans. Signal Process.,
42(6):2210–2217, 1996. 3

8728

[36] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma.
Robust face recognition via sparse representation.
IEEE
Trans. Pattern Anal. Mach. Intell., 31(2):210–227, 2009. 1,
3, 7

[37] Y. Xu, X. Fang, J. Wu, X. Li, and D. Zhang. Discriminative
transfer subspace learning via low-rank and sparse represen-
tation. IEEE Trans. Image Process., 25(2):850–863, 2016.
1

[38] J. Yan and M. Pollefeys. A general framework for motion
segmentation: Independent, articulated, rigid, non-rigid, de-
generate and non-degenerate. In Proc. European Conf. Com-
puter Vision, pages 94–106. Springer, 2006. 7

[39] M. Yang, L. Zhang, J. Yang, and D. Zhang. Regularized
robust coding for face recognition. IEEE Trans. Image Pro-
cess., 22(5):1753–1766, 2013. 1

[40] X.-T. Yuan and B.-G. Hu. Robust feature extraction via infor-
mation theoretic learning. In Proc. Int. Conf. Mach. Learn.,
pages 1193–1200. ACM, 2009. 1, 2, 5

[41] T. Zhang, A. Szlam, and G. Lerman. Median k-ﬂats for hy-
brid linear modeling with many outliers. In Proc. IEEE Int.
Conf. Comput. Vis., pages 234–241. IEEE, 2009. 1

[42] Y. Zhang, D. Shi, J. Gao, and D. Cheng. Low-rank-sparse
subspace representation for robust regression. In Proc. IEEE
Conf. Comput. Vis. Pattern Recogn., pages 7445–7454, 2017.
1

[43] Y. Zhang, Z. Sun, R. He, and T. Tan. Robust subspace clus-
In Proc. IEEE Int.

tering via half-quadratic minimization.
Conf. Comput. Vis., pages 3096–3103, 2013. 1

[44] Q. Zhao, D. Meng, Z. Xu, W. Zuo, and L. Zhang. Robust
principal component analysis with complex noise. In Proc.
Int. Conf. Mach. Learn., pages 55–63. ACM, 2014. 1

8729

