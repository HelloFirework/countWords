Unifying Heterogeneous Classiﬁers with Distillation

Jayakorn Vongkulbhisal1, Phongtharin Vinayavekhin1, Marco Visentini-Scarzanella2

1IBM Research, Tokyo, Japan

2Amazon, Tokyo, Japan

jayakornv@ibm.com, pvmilk@jp.ibm.com, marcovs@amazon.com

Abstract

In this paper, we study the problem of unifying knowl-
edge from a set of classiﬁers with different architectures and
target classes into a single classiﬁer, given only a generic
set of unlabelled data. We call this problem Unifying Het-
erogeneous Classiﬁers (UHC). This problem is motivated by
scenarios where data is collected from multiple sources, but
the sources cannot share their data, e.g., due to privacy con-
cerns, and only privately trained models can be shared. In
addition, each source may not be able to gather data to train
all classes due to data availability at each source, and may
not be able to train the same classiﬁcation model due to dif-
ferent computational resources. To tackle this problem, we
propose a generalisation of knowledge distillation to merge
HCs. We derive a probabilistic relation between the out-
puts of HCs and the probability over all classes. Based on
this relation, we propose two classes of methods based on
cross-entropy minimisation and matrix factorisation, which
allow us to estimate soft labels over all classes from unla-
belled samples and use them in lieu of ground truth labels
to train a uniﬁed classiﬁer. Our extensive experiments on
ImageNet, LSUN, and Places365 datasets show that our ap-
proaches signiﬁcantly outperform a naive extension of dis-
tillation and can achieve almost the same accuracy as clas-
siﬁers that are trained in a centralised, supervised manner.

1. Introduction

The success of machine learning in image classiﬁcation
tasks has been largely enabled by the availability of big
datasets, such as ImageNet [32] and MS-COCO [25]. As
the technology becomes more pervasive, data collection is
transitioning towards more distributed settings where the
data is sourced from multiple entities and then combined
to train a classiﬁer in a central node (Fig. 1a). However,
in many cases, transfer of data between entities is not pos-
sible due to privacy concerns (e.g., private photo albums
or medical data) or bandwidth restrictions (e.g., very large

(a)

s
e
c
r
u
o
S
 
a
t
a
D

(b)

s
e
c
r
u
o
S
 
a
t
a
D

Central Processing Node

Cat, Dog

Merged Dataset

Giraffe, Cat, Doll

Fruit, Cat

Cat, Dog

Giraffe, Cat, Doll

Fruit, Cat

Cat, Dog, Giraffe, 

Doll, Fruit

Central Processing Node

Unlabelled Data

HCs

Cat, Dog, Giraffe, 

Doll, Fruit

Figure 1. Unifying Heterogeneous Classiﬁers. (a) Common train-
ing approaches require transferring data from sources to a cen-
tral processing node where a classiﬁer is trained. (b) We propose
to train a uniﬁed classiﬁer from pre-trained classiﬁers from each
source and an unlabelled set of generic data, thereby preserving
privacy. The individual pre-trained classiﬁers may have different
sets of target classes, hence the term Heterogeneous Classiﬁers
(HCs).

datasets), hampering the uniﬁcation of knowledge from dif-
ferent sources. This has led to multiple works that pro-
pose to learn classiﬁers without directly sharing data, e.g.,
distributed optimisation [4], consensus-based training [12],
and federated learning [20]. However, these approaches
generally require models trained by each entity to be the
same both in terms of architecture and target classes.

In this paper, we aim to remove these limitations and
propose a system for a more general scenario consisting of
an ensemble of Heterogeneous Classiﬁers (HCs), as shown
in Fig. 1b. We deﬁne a set of HCs as a set of classiﬁers
which may have different architectures and, more impor-
tantly, may be trained to classify different sets of target
classes. To combine the HCs, each entity only needs to for-

13175

ward their trained classiﬁers and class names to the central
processing node, where all the HCs are uniﬁed into a single
model that can classify all target classes of all input HCs.
We refer to this problem as Unifying Heterogeneous Clas-
siﬁers (UHC). UHC has practical applications for the cases
when it is not possible to enforce every entity to (i) use the
same model/architecture; (ii) collect sufﬁcient training data
for all classes; or (iii) send the data to the central node, due
to computational, data availability, and conﬁdentiality con-
straints.

To tackle UHC, we propose a generalisation of knowl-
edge distillation [8, 17]. Knowledge distillation was origi-
nally proposed to compress multiple complex teacher mod-
els into a single simpler student one. However, distillation
still assumes that the target classes of all teacher and stu-
dent models are the same, whereas in this work we relax
this limitation. To generalise distillation to UHC, we de-
rive a probabilistic relationship connecting the outputs of
HCs with that of the uniﬁed classiﬁer. Based on this rela-
tionship, we propose two classes of methods, one based on
cross-entropy minimisation and the other on matrix factori-
sation with missing entries, to estimate the probability over
all classes of a given sample. After obtaining the probabil-
ity, we can then use it to train the uniﬁed classiﬁer. Our
approach only requires unlabelled data to unify HCs, thus
no labour is necessary to label any data at the central node.
In addition, our approach can be applied to any classiﬁers
which can be trained with soft labels, e.g., neural networks,
boosting classiﬁers, random forests, etc.

We evaluated our proposed approach extensively on Im-
ageNet, LSUN, and Places365 datasets in a variety of set-
tings and against a natural extension of the standard distilla-
tion. Through our experiments we show that our approach
outperforms standard distillation and can achieve almost the
same accuracy as the classiﬁers that were trained in a cen-
tralised, supervised manner.

2. Related Work

There exists a long history of research that aims to har-
ness the power of multiple classiﬁers to boost classiﬁcation
result. The most well-known approaches are arguably en-
semble methods [19, 23, 30] which combine the output of
multiple classiﬁers to make a classiﬁcation. Many tech-
niques, such as voting and averaging [23], can merge pre-
diction from trained classiﬁers, while some train the clas-
siﬁers jointly as part of the technique, e.g., boosting [13]
and random forests [6]. These techniques have been suc-
cessfully used in many applications, e.g., multi-class clas-
siﬁcation [15], object detection [34, 27], tracking [1], etc.
However, ensemble methods require storing and running all
models for prediction, which may lead to scalability issues
when complex models, e.g., deep networks, are used.
In
addition, ensemble methods assume all base classiﬁers are

Figure 2. UHC problem and approach overview. An input image x
is drawn from an unlabelled set U and input to a set of pre-trained
classiﬁers {C1, · · · , CN }, where each Ci returns soft label pi over
classes in Li. Here, the classes Li may be different for each Ci.
The goal of UHC is to train a classiﬁer CU that can classify all
target classes in LU using the prediction of Ci on x ∈ U instead of
labelled data. Our approach to UHC involves using pi to estimate
q, the soft label of x over all classes in LU , then using x and q to
train CU .

trained to classify all classes, which is not suitable for the
scenarios addressed by UHC.

To the best of our knowledge, the closest class of meth-
ods to UHC is knowledge distillation [8, 17]. Distilla-
tion approaches operate by passing unlabelled data to a
set of pretrained teacher models to obtain soft predictions,
which are used to train a student model. Albeit origi-
nally conceived for compressing complex models into sim-
pler ones by matching predictions, distillation has been
further extended to, for instance, matching intermediate
features [31], knowledge transfer between domains [14],
combining knowledge using generative adversarial-based
loss [35], etc. More related to UHC, Lopes et al. [26] pro-
pose to distill teacher models trained by different entities
using their metadata rather than raw inputs. This allows the
student model to be trained without any raw data transfer,
thus preserving privacy while also not requiring any data
collection from the central processing node. Still, no for-
mulation of distillation can cope with the case where each
teacher model has different target classes, which we tackle
in this paper. We describe how distillation can be gener-
alised to UHC in the next section.

3. Unifying Heterogeneous Classiﬁers (UHC)

We deﬁne the Unifying Heterogeneous Classiﬁers
(UHC) problem in this paper as follows (see Fig. 2). Let
U be an unlabelled set of images (“transfer set”) and let
C = {Ci}N
i=1 be a set of N Heterogeneous Classiﬁers
(HCs), where each Ci is trained to predict the probabil-
ity pi(Y = lj) of an image belonging to class lj ∈ Li.
Given U and C, the goal of this work is to learn a uni-

3176

ﬁed classiﬁer CU that estimates the probability q(Y = lj)
of an input image belonging to the class lj ∈ LU where
LU = SN
i=1 Li = {l1, l2, . . . , lL}. Note that Ci might be
trained to classify different sets of classes, i.e., we may have
Li 6= Lj or even |Li| 6= |Lj| for i 6= j.

Our approach to tackle UHC involves three steps:
(i) passing the image x ∈ U to Ci to obtain pi, ∀i, (ii) esti-
mating q from {pi}i, then (iii) using the estimated q to train
CU in a supervised manner. We note that it is possible to
combine (ii) and (iii) into a single step for neural networks
(see Sec. 3.5.1), but this 3-step approach allows it to be ap-
plied to other classiﬁers, e.g., boosting and random forests.
To accomplish (ii), we derive probabilistic relationship be-
tween each pi and q, which we leverage to estimate q via the
following two proposed methods: cross-entropy minimisa-
tion and matrix factorisation. In the rest of this section, we
ﬁrst review standard distillation, showing why it cannot be
applied to UHC. We then describe our approaches to esti-
mate q from {pi}i. We provide a discussion on the compu-
tation cost in the supplementary material.

3.1. Review of Distillation

Overview Distillation [8, 17] is a class of algorithms
used for compressing multiple trained models Ci into a sin-
gle uniﬁed model CU using a set of unlabelled data U 1.
Referring to Fig. 2, standard distillation corresponds to the
case where Li = Lj , ∀(i, j). The uniﬁed CU is trained
by minimising the cross-entropy between outputs of Ci and
CU as

J(q) = −X

i

X

l∈LU

pi(Y = l) log q(Y = l).

(1)

Essentially, the outputs of Ci are used as soft labels for the
unlabelled U in training CU . For neural networks, class
probabilities are usually computed with softmax function:

p(Y = l) =

exp(zl/T )

Pk∈LU exp(zk/T )

,

(2)

where zl is the logit for class l, and T denotes an adjustable
temperature parameter. In [17], it was shown that minimis-
ing (1) when T is high is similar to minimising the ℓ2 error
between the logits of p and q, thereby relating the cross-
entropy minimising to logit matching.

Issues The main issue with standard distillation stems
from its inability to cope with the more general case of Li 6=
Lj . Mathematically, Eq. (1) assumes CU and Ci’s share
the same set of classes. This is not true in our case since
each Ci is trained to predict classes in Li, thus pi(Y = l) is
2. A naive solution to this issue would
undeﬁned for l ∈ L−i
be to simply set pi(Y = l) = 0 for l ∈ L−i. However,

1Labelled data can also be used in a supervised manner.
2We deﬁne L−i as the set of classes in LU but outside Li.

this could incur serious errors, e.g., one may set pi(Y =
cat) of a cat image to zero when Ci does not classify cats,
which would be an improper supervision. We show that this
approach does not provide good results in the experiments.
It is also worth mentioning that Ci in UHC is different
from the Specialised Classiﬁers (SC) in [17]. While SCs
are trained to specialise in classifying a subset of classes,
they are also trained with data from other classes which are
grouped together into a single dustbin class. This allows
SCs to distinguish dustbin from their specialised classes,
enabling student model to be trained with (1). Using the
previous example, the cat image would be labelled as dust-
bin class, which is an appropriate supervision for SCs that
do not classify cat. However, the presence of a dustbin class
imposes a design constraint on the Ci’s, as well as requiring
the data source entities to collect large amounts of generic
data to train it. Conversely, we remove these constraints
in our formulation, and Ci’s are trained without a dustbin
class. Thus, given data from L−i, Ci will only provide
pi only over classes in Li, making it difﬁcult to unify C
with (1).

3.2. Relating outputs of HCs and uniﬁed classiﬁer

To overcome the limitation of standard distillation, we
need to relate the output pi of each Ci to the probability q
over LU . Since pi is deﬁned only in the subset Li ⊆ LU ,
we can consider pi(Y = l) as the probability q of Y = l
given that Y cannot be in L−i. This leads to the following
derivation:

pi(Y = l) = q(Y = l|Y /∈ L−i)

= q(Y = l|Y ∈ Li)
q(Y = l, Y ∈ Li)

=

=

q(Y ∈ Li)
q(Y = l)

Pk∈Li q(Y = k)

.

(3)

(4)

(5)

(6)

We can see that pi(Y = l) is equivalent to q(Y = l) nor-
malised by the classes in Li. In the following sections, we
describe two classes of methods that utilise this relationship
for estimating q from {pi}i.

3.3. Method 1: Cross entropy approach

Recall that the goal of (1) is to match q to pi by min-
imising the cross-entropy between them. Based on the re-
lation in (6), we generalise (1) to tackle UHC by matching

q(Y =l)

q(Y =k) to pi(Y = k), resulting in:

J(q) = −X

i

X

l∈Li

pi(Y = l) log ˆqi(Y = l),

(7)

ˆqi(Y = l) =

q(Y = l)

Pk∈Li q(Y = k)

.

(8)

Pk∈Li

where:

3177

We can see that the difference between (1) and (7) lies in
the normalisation of q. Speciﬁcally, the cross-entropy of
each Ci (i.e., the second summation) is computed between
pi(Y = l) and ˆqi(Y = l) over the classes in Li. With
this approach, we do not need to arbitrarily deﬁne values
for pi(Y = l) whenever l ∈ L−i, thus not causing spurious
supervision. We now outline optimality properties of (7).

Proposition 1 (Sufﬁcient condition for optimality) Sup-
pose there exists a probability ¯p over LU , where pi(Y = l) =

¯p(Y =l)

Pk∈Li

¯p(Y =k) , ∀i, then q = ¯p is a global minimum of (7).
Sketch of proof Consider ˜Ji(˜qi) = −Pl∈Li pi(Y =
l) log ˜qi(Y = l) (Note ˜Ji is a function of ˜qi whereas J
˜Ji(˜qi) achieves its minimum when
is a function of q).
˜qi = pi, with the a value of ˜Ji(pi). Thus, the minimum
˜Ji(pi). This is a lower bound of
˜Ji(˜qi) is Pi
value of Pi
˜Ji(pi) ≤ J(q), ∀q. However, we can see that
(7), i.e., Pi
by setting q = ¯p, we achieve equality in the bound, i.e.,
˜Ji(pi) = J(¯p), and so ¯p is a global minimum of (7). (cid:3)
Pi
The above result establishes the form of a global min-
imum of (7), and that minimising (7) may obtain the true
underlying probability ¯p if it exists. However, there are
cases where the global solution may not be unique. A sim-
ple example is when there are no shared classes between the
HCs, e.g., N = 2 with L1 ∩ L2 = ∅. It may be possible
to show uniqueness of the global solution in some cases de-
pending on the structure of shared classes between Li’s, but
we leave this as future work.

Optimisation Minimisation of (7) can be transformed
into a geometric program (see supplementary material),
which can then be converted to a convex problem and ef-
ﬁciently solved [3]. In short, we deﬁne ul ∈ R for l ∈ LU
and replace q(Y = l) with exp(ul). Thus, (7) transforms to

ˆJ({ul}l) = −X

i

X

l∈Li

pi(Y = l)

ul − log 

 X

k∈Li

exp(uk)



 ,

(9)
which is convex in {ul}l since it is a sum of scaled and
log-sum-exps of {ul}l [5]. We minimise it using gradient
descent. Once the optimal {ul}l is obtained, we transform
it to q with the softmax function (2).

3.4. Method 2: Matrix factorisation approaches

Our second class of approaches is based on low-rank ma-
trix factorisation with missing entries. Indeed, it is possible
to cast UHC as a problem of ﬁlling an incomplete matrix of
soft labels. During the last decade, low-rank matrix comple-
tion and factorisation [10, 11] have been successfully used
in various applications, e.g., structure from motion [18] and
recommender systems [21]. It has also been used for mul-
tilabel classiﬁcation in a transductive setting [9]. Here, we
will describe how we can use matrix factorisation to recover
soft labels q from {pi}i.

3.4.1 Matrix factorisation in probability space

Consider a matrix P ∈ [0, 1]L×N where we set Pli (the
element in row l and column i) to pi(Y = l) if l ∈ Li
and zero otherwise. This matrix P is similar to the decision
proﬁle matrix in ensemble methods [23], but here we ﬁll in
0 for the classes that Ci’s cannot predict. To account for
these missing predictions, we deﬁne M ∈ {0, 1}L×N as a
mask matrix where Mli is 1 if l ∈ Li and zero otherwise.
Using the relation between pi and q in (6), we can see that
P can be factorised into a masked product of vectors as:

M ⊙ P = M ⊙ (uv⊤),

u =




q(Y = l1)

...

q(Y = lm)




, v =




1
q(Y =l)
Pl∈L1

...
1
Pl∈LN

q(Y =l)




,

(10)

(11)

where ⊙ is the Hadamard product. Here, u is the vector
containing q, and each element in v contains the normali-
sation factor for each Ci. In this form, we can estimate the
probability vector u by solving the following rank-1 matrix
completion problem:

minimise

u,v

kM ⊙ (P − uv⊤)k2
F

subject to u⊤1L = 1

v ≥ 0N , u ≥ 0L,

(12)

(13)

(14)

where k · kF denotes Frobenius norm, and 0k and 1k denote
vectors of zeros and ones of size k. Here, the constraints
ensure that u is a probability vector and that v remains non-
negative so that the sign of probability in u is not ﬂipped.
This formulation can be regarded as a non-negative matrix
factorisation problem [24], which we solve using Alternat-
ing Least Squares (ALS) [2] where we normalise u to sum
to 1 in each iteration3. Due to gauge freedom [7], this nor-
malisation in u does not affect the cost function.

3.4.2 Matrix factorisation in logit space

In Sec. 3.1, we discussed the relationship between minimis-
ing cross-entropy and logit matching under ℓ2 distance. In
this section, we consider applying matrix factorisation in
logit space and show that our formulation is a generalisa-
tion of logit matching between Ci and CU .

Let zi

l be the given logit output of class l of Ci

4, and
ul be that of CU to be estimated. Consider a matrix Z ∈
RL×N where Zli = zi
l if l ∈ Li and zero otherwise. We

3We note there are more effective algorithms for matrix factorisation

than ALS [7, 29, 11]. Here, we use ALS due to ease of implementation.

4For algorithms besides neural networks, we can obtain logits from

probability via zi

l = log pi(Y = l).

3178

can formulate the problem of estimating the vector of logits
u ∈ RL as :

minimise

u,v,c

kM ⊙ (Z − uv⊤ − 1Lc⊤)k2

F + λ(kuk2

2 + kvk2
2)
(15)

subject to v ≥ 0N ,

(16)

where c ∈ RN deals with shift in logits5, and λ ∈ R is
a hyperparameter controlling regularisation [7]. Here, op-
timising v ∈ RN is akin to optimising the temperature of
logits [17] from each source classiﬁer, and we constrained it
to be nonnegative to prevent the logit sign ﬂip, which could
affect the probability.

Relation to logit matching The optimisation in (15) has
three variables. Since c is unconstrained, we derive its
closed form solution and remove it from the formulation.
This transforms (15) into:

minimise

u,v

N

X

i=1

(cid:13)(cid:13)P|Li| ([zi − uvi]Li )(cid:13)(cid:13)

subject to v ≥ 0N ,

2

2 + λ(kuk2

2 + kvk2
2)

(17)

(18)

1k1⊤

where zi is the ith column of Z; [x]Li selects the elements
of x which are indexed in Li; and Pk(x) = (Ik − 1
k )x
k
is the orthogonal projector that removes the mean from the
vector x ∈ Rk. This transformation simpliﬁes (15) to con-
tain only u and v. We can see that this formulation min-
imises the ℓ2 distance between logits, but instead of consid-
ering all classes in LU , each term in the summation consid-
ers only the classes in Li. In addition, (17) also includes
regularisation and optimises for scaling in v. Thus, we can
say that (15) is a generalisation of logit matching for UHC.
fewer parameters
than (15), it is more complicated to optimise as the ele-
ments in u are entangled due to the projector. Instead, we
solve (15) using ALS over u, v, and c. Here, there is no
constraint on u, so we do not normalise it as in Sec. 3.4.1.

Optimisation While

(17) has

Alternative approach: Setting v as a constant While
setting v as a variable allows (15) to handle different scal-
ings of logits, it also introduces cumbersome issues. Specif-
ically, the gauge freedom in uv⊤ may lead to arbitrary scal-
ing in u and v, i.e., uv⊤ = (u/α)(αv⊤) for α 6= 0. Also,
while the regularisers help prevent the norms of u and v to
become too large, it is difﬁcult to set a single λ that works
well for all data in U . To combat these issues, we propose
another formulation of (15) where we ﬁx v = 1N . With
v ﬁxed, we do not require to regularise u since its scale is
determined by Z. In addition, the new formulation is con-
vex and can be solved to global optimality. We solve this
alternative formulation with gradient descent.

5Recall that a shift in logit values has no effect on the probability out-
put, but we need to account for the different shifts from the Ci’s to cast it
as matrix factorisation.

3.5. Extensions

In Secs. 3.3 and 3.4, we have described methods for esti-
mating q from {pi} then using q as the soft label for training
CU . In this section, we discuss two possible extensions ap-
plicable to all the methods: (i) direct backpropagation for
neural networks and (ii) ﬁxing imbalance in soft labels.

3.5.1 Direct backpropagation for neural networks

Suppose the uniﬁed classiﬁer CU is a neural network. While
it possible to use q to train CU in a supervised manner, we
could also consider an alternative where we directly back-
propagate the loss without having to estimate q ﬁrst. In the
case of cross-entropy (Sec. 3.3), we can think of q as the
probability output from CU , through which we can directly
backpropagate the loss. In the case of matrix factorisation
(Sec. 3.4), we could consider u as the vector of probability
(Sec. 3.4.1) or logit (Sec. 3.4.2) outputs from CU . Once u
is obtained from CU , we plug it in each formulation, solve
for other variables (e.g., v and c) with u ﬁxed, then back-
propagate the loss via u. Directly backpropagating the loss
merges the steps of estimating q and using it to train CU
into a single step.

3.5.2 Balancing soft labels

All the methods we have discussed are based on individual
samples: we estimate q from {pi} of a single x from the
transfer set U and use it to train CU . However, we observe
that the set of estimated q’s from the whole U could be im-
balanced. That is, the estimated q’s may be biased towards
certain classes more than others. To counter this effect, we
apply the common technique of weighting the cross-entropy
loss while training CU [28]. The weight of each class l is
computed as the inverse of the mean of q(Y = l) over all
data from U .

4. Experiments

In this section, we perform experiments to compare dif-
ferent methods for solving UHC. The main experiments on
ImageNet, LSUN, and Places365 datasets are described in
Sec. 4.1, while sensitivity analysis is described in Sec. 4.2.
We use the following abbreviations to denote the meth-
SD for the naive extension of Standard Distilla-
ods.
tion (Sec. 3.1) [17]; CE-X for Cross-Entropy methods
(Sec. 3.3); MF-P-X for Matrix Factorization in Probabil-
ity space (Sec. 3.4.1); and MF-LU-X and MF-LF-X for
Matrix Factorization in Logit space with Unﬁxed and Fixed
v (Sec. 3.4.2), resp. The sufﬁx ‘X’ is replaced with ‘E’ if
we estimate q ﬁrst before using it as soft label to train CU ;
with ‘BP’ if we perform direct backpropagation from the
loss function (Sec. 3.5.1); and with ‘BS’ if we estimate and
balance the soft labels q before training CU (Sec. 3.5.2).

3179

Table 1. HC conﬁgurations for the main experiment

Dataset

#Classes
in LU (L)

#HCs (N )

#Classes for each HC

Random Compl. overlap.

ImageNet

LSUN

Places365

20-50
5-10
20-50

10-20

3-7

10-20

5-15
2-5
5-15

= L
= L
= L

In addition to the mentioned methods, we also include SD-
BS as the SD method with balanced soft labels, and SPV
as the method trained directly in a supervised fashion with
all training data of all Ci’s as a benchmark. For MF-LU-X
methods, we used λ = 0.01. All methods use tempera-
ture T = 3 to smooth the soft labels and logits (See (2)
and [17]).

4.1. Experiment on large image datasets

In this section, we describe our experiment on ImageNet,
LSUN, and Places365 datasets. First, we describe the ex-
periment protocols, providing details on the datasets, archi-
tectures used as Ci and CU , and the conﬁgurations of Ci.
Then, we discuss the results.

4.1.1 Experiment protocols

Datasets We use three datasets for this experiment. (i) Im-
ageNet (ILSVRC2012) [32], consisting of 1k classes with
~700 to 1300 training and 50 validation images per class, as
well as 100k unlabelled test images. In our experiments, the
training images are used as training data for the Ci’s, the un-
labelled test images as U , and the validation images as our
test set to evaluate the accuracy. (ii) LSUN [36], consisting
of 10 classes with ~100k to 3M training and 300 validation
images per class with 10k unlabelled test images. Here,
we randomly sample a set of 1k training images per class to
train the Ci’s, a second randomly sampled set of 20k images
per class also from the training data is used as U , and the
validation data is used as our test set. (iii) Places365 [37],
consisting of 365 classes with ~3k to 5k training and 100
validation images per class, as well as ~329k unlabelled test
images. We follow the same usage as in ImageNet, but with
100k samples from the unlabelled test images as U . We pre-
process all images by centre cropping and scaling to 64×64
pixels.

HC conﬁgurations We test the proposed methods un-
der two conﬁgurations of HCs (see summary in Table 1). (i)
Random classes. For ImageNet and Places365, in each trial,
we sample 20 to 50 classes as LU and train 10 to 20 Ci’s
where each is trained to classify 5 to 15 classes. For LSUN,
in each trial, we sample 5 to 10 classes as LU and train 3 to
7 Ci’s where each is trained to classify 2 to 5 classes. We
use this conﬁguration as the main test for when Ci’s clas-
sify different sets of classes. (ii) Completely overlapping

classes. Here, we use the same conﬁgurations as in (i) ex-
cept all Ci’s are trained to classify all classes in LU . This
case is used to test our proposed methods under the common
conﬁgurations where all Ci and CU share the same classes.
Under both conﬁgurations, U consist of a much wider set
of classes than LU . In other words, a large portion of the
images in U does not fall under any of the classes in LU .

Models Each Ci

is randomly selected from one of
the following four architectures with ImageNet pre-trained
weights: AlexNet [22], VGG16 [33], ResNet18, and
ResNet34 [16].
For AlexNet and VGG16, we ﬁx the
weights of their feature extractor portion, replace their fc
layers with two fc layers with 256 hidden nodes (with
BatchNorm and ReLU), and train the fc layers with their
training data. Similarly in ResNet models, we replace their
fc layers with two fc layers with 256 hidden nodes as
above. In addition, we also ﬁne-tune the last residual block.
As for CU , we use two models, VGG16 and ResNet34, with
similar settings as above.

For all datasets and conﬁgurations, we train each Ci with
50 to 200 samples per class; no sample is shared between
any Ci in the same trial. These Ci’s together with U are
then used to train CU . We train all models for 20 epochs
with SGD optimiser (step sizes of 0.1 and 0.016 for ﬁrst
and latter 10 epochs with momentum 0.9). To control the
variation in results, in each trial we initialise instances of
CU ’s from the same architecture using the same weights
and we train them using the same batch order. In each trial,
we evaluate the CU ’s of all methods on the test data from all
classes in LU . We run 50 trials for each dataset, model, and
HC conﬁguration combination. The results are reported in
the next section.

4.1.2 Results

Table 2 shows the results for this experiment. Each col-
umn shows the average accuracy of each method under each
experiment setting, where the best performing method is
shown in underlined bold. To test statistical signiﬁcance,
we choose Wilcoxon signed-rank test over standard devia-
tion to cater for the vastly different settings (e.g., model ar-
chitectures, number of classes and HCs, etc.) across trials.
We run the test between the best performing method in each
experiment and the rest. Methods where the performance is
not statistically signiﬁcantly different from the best method
at α = 0.01 are shown in bold.

First, let us observe the result for the random classes case
which addresses the main scenario of this paper, i.e., when
each HC is trained to classify different sets of classes. We
can make the following observations.

All proposed methods perform signiﬁcantly better
than SD. We can see that all methods in (A), (B), and (C)

6For MF-P-BP, we use 150× the rates as its loss has a smaller scale.

3180

Table 2. Average accuracy of UHC methods over different combinations of HC conﬁgurations, datasets, and uniﬁed classiﬁer models.
(Underline bold: Best method. Bold: Methods which are not statistically signiﬁcantly different from the best method.)

Methods

ImageNet

LSUN

Places365

ImageNet

LSUN

Places365

VGG16 ResNet34 VGG16 ResNet34 VGG16 ResNet34 VGG16 ResNet34 VGG16 ResNet34 VGG16 ResNet34

Random Classes

Completely Overlapping Classes

SPV (Benchmark)

.7212

SD .5543

(A) Estimate q methods

CE-E .6911
MF-P-E .6819
MF-LV-E .6660
MF-LF-E .6886

(B) Backprop. methods
CE-BP
MF-P-BP
MF-LV-BP
MF-LF-BP

(C) Balanced soft labels
SD-BS
CE-BS
MF-P-BS
MF-LV-BS
MF-LF-BS

.6902
.6945
.6889
.6842

.6629
.6928
.6851
.6772
.6935

.6953

.5562

.6852
.6747
.6609
.6833

.6869
.6872
.6847
.6840

.6574
.6856
.6756
.6682
.6865

.6664

.5310

.6483
.6443
.6348
.6490

.6520
.6480
.6495
.6523

.6343
.6513
.6474
.6388
.6549

.6760

.5350

.6445
.6406
.6330
.6458

.6439
.6417
.6389
.6445

.6345
.6464
.6450
.6357
.6485

.5525

.4390

.5484
.5349
.5199
.5441

.5466
.5471
.5467
.5383

.5283
.5548
.5455
.5346
.5544

.5870

.4564

.5643
.5488
.5414
.5609

.5669
.5609
.5681
.5624

.5433
.5687
.5546
.5497
.5692

.7345

.7275

.7276
.7280
.7231
.7265

.7275
.7277
.7229
.7239

.7217
.7215
.7243
.7168
.7210

.7490

.7292

.7290
.7297
.7242
.7279

.7288
.7287
.7225
.7252

.7214
.7213
.7252
.7173
.7215

.6769

.7004

.7002
.7012
.7031
.7015

.7003
.6999
.7001
.7020

.6979
.6979
.6996
.7014
.6998

.7017

.7041

.7036
.7052
.7043
.7057

.7040
.7019
.7046
.7034

.7017
.7018
.7041
.7028
.7035

.5960

.6163

.6162
.6167
.6129
.6161

.6161
.6146
.6113
.6104

.6094
.6094
.6124
.6063
.6101

.6460

.6402

.6406
.6406
.6374
.6397

.6400
.6384
.6369
.6366

.6320
.6323
.6355
.6301
.6330

of Table 2 outperform SD by a large margin of 9-15%. This
shows that simply setting probability of undeﬁned classes in
each HC to 0 may signiﬁcantly deteriorate the accuracy. On
the other hand, our proposed methods achieve signiﬁcantly
better results and almost reach the same accuracy as SPV
with a gap of 1-4%. This suggests the soft labels from HCs
can be used for unsupervised training at a little expense of
accuracy, even though U contains a signiﬁcant proportion
of images that are not part of the target classes. Still, there
are several factors that may affect the capability of CU from
reaching the accuracy of SPV, e.g., accuracy of Ci, their ar-
chitectures, etc. We look at some of these in the sensitivity
analysis section.

MF-LF-BS performs well in all cases. We can see that
different algorithms perform best under different settings,
but MF-LF-BS always performs best or has no statistical
difference from the best methods. This suggests MF-LF-
BS could be the best method for solving UHC. At the same
time, CE methods offer a good trade-off between high accu-
racy and ease of implementation, which makes them a good
alternative for the UHC problem.

Besides these main points, we also note the following

small but consistent trends.

Balancing soft labels helps improve accuracy. While
the improvement may be marginal (less than 1.5%), we can
see that ‘BS’ methods in (C) consistently outperform their
‘E’ counterparts in (A). Surprisingly, SD-BS, which is SD
with balanced soft labels, also signiﬁcantly improved over
SD by more than 10%. These results indicate that it is a
good practice to use balanced soft labels to solve UHC.
Note that while SD-BS received signiﬁcant boost, it still

generally underperforms compared to CE and MF methods,
suggesting that it is important to incorporate the relation be-
tween {pi} and q into training.

Nonconvex losses perform better with ‘BP’. Methods
with sufﬁxes ‘E’ and ‘BS’ in (A) and (C) are based on es-
timating q before training CU , while ‘BP’ in (B) directly
perform backpropagation from the loss function. As seen
in Sec. 3, the losses of CE and MF-LF are convex in their
variables while MF-P and MF-LV are nonconvex. Here, we
observe a small but interesting effect that methods with non-
convex losses perform better with ‘BP’. We speculate that
this is due to errors in the estimation of q trickling down
to the training of CU if the two steps are separated. Con-
versely in ‘BP’, where the two steps are merged into a single
step, such issue might be avoided. More research would be
needed to conﬁrm this speculation. For convex losses (CE
and MF-LF), we ﬁnd no signiﬁcant patterns between ‘E’ in
(A) and ‘BP’ in (B).

Next, we discuss the completely overlapping case.
All methods perform rather well. We can see that all
methods, including SD, achieve about the same accuracy
(within ~1% range). This shows that our proposed methods
can also perform well in the common cases of all Ci’s being
trained to classify all classes and corroborates the claim that
our proposed methods are generalisations of distillation.

Not balancing soft labels performs better. We note that
balancing soft labels tends to slightly deteriorate the accu-
racy. This is the opposite result from the random classes
case. Here, even SD-BS which receive an accuracy boost in
the random classes case also performs worse than its coun-
terpart SD. This suggests not balancing soft labels may be a

3181

Figure 3. Sensitivity analysis results. (a) Size of unlabelled set. (b) Temperature. (c) Accuracy of HCs.

better option for overlapping classes case.

Distillation may outperform its supervised counter-
parts. For LSUN and Places365 datasets, we see that many
times distillation methods performs better than SPV. Espe-
cially for the case of VGG16, we see SPV consistently per-
form worse than other methods by 1 to 3% in most of the
trials. This shows that it is possible that distillation-based
methods may outperform their supervised counterparts.

4.2. Sensitivity Analysis

In this section, we perform three sets of sensitivity anal-
ysis on the effect of size of the transfer set, temperature pa-
rameter T , and accuracy of HCs. We use the same settings
as the ImageNet random classes experiment in the previous
section with VGG16 as CU . We run 50 trials for each test.
We evaluate the following ﬁve methods as the representa-
tive set of SD and top performing methods from previous
section: SD, SD-BS, MF-P-BP, MF-LF-BS, and CE-BS.

Size of transfer set We use this test to evaluate the effect
of the number of unlabelled samples in the transfer set U .
We vary the number of samples from 103 to 105. The result
is shown in Fig. 3a. As expected, we can see that all meth-
ods deteriorate as the size of transfer set decreases. In this
test, MF-P-BP is the most affected by the decrease as its ac-
curacy drops fastest. Still, all other methods perform better
than SD in the whole test range, illustrating the robustness
to transfer sets with different sizes.

Temperature In this test, we vary the temperature T
used for smoothing the probability {pi} (see (2) or [17]) be-
fore using them to estimate q or train CU . The values evalu-
ated are T = 1, 3, 6, and 10. The result is shown in Fig. 3b.
We can see that the accuracies of SD and SD-BS drop sig-
niﬁcantly when T is set to high and low values, resp. On
the other hand, the other three methods are less affected by
different values of T .

HCs’ accuracies In this test, we evaluate the robustness
of UHC methods against varying accuracy of Ci. The test
protocol is as follows. In each trial, we vary the accuracy of
all Ci’s to 40-80%, obtain pi from the Ci’s, and use them to

perform UHC. To vary the accuracy of each Ci, we take 50
samples per class from training data as the adjustment set,
completely train each Ci from the remaining training data,
then inject increasing Gaussian noise into the last fc layer
until its accuracy on the adjustment set drops to the desired
value. If the initial accuracy of Ci is below the desired value
then we simply use the initial Ci. The result of this evalu-
ation is shown in Fig. 3c. We can see that the accuracy of
all methods increase as the Ci’s perform better, illustrating
that the accuracy of Ci is an important factor for the per-
formance of UHC methods. We can also see that MF-P-BP
is most affected by low accuracy of Ci while MF-LF-BS is
the most robust.

Based on the sensitivity analysis, we see that MF-LF-BS
is the most robust method against the number of samples
in the transfer set, temperature, and accuracy of the HCs.
This result provides further evidence that MF-LF-BS should
be the suggested method for solving UHC. We provide the
complete sensitivity plots with all methods in the supple-
mentary material.

5. Conclusion

In this paper, we formalise the problem of unifying
knowledge from heterogeneous classiﬁers (HCs) using only
unlabelled data. We proposed cross-entropy minimisation
and matrix factorisation methods for estimating soft labels
of the unlabelled data from the output of HCs based on a
derived probabilistic relationship. We also proposed two
extensions to directly backpropagate the loss for neural net-
works and to balance estimated soft labels. Our extensive
experiments on ImageNet, LSUN, and Places365 show that
our proposed methods signiﬁcantly outperformed a naive
extension of knowledge distillation. The result together
with additional three sensitivity analysis suggest that an ap-
proach based on matrix factorization in logit space with bal-
anced soft labels is the most robust approach to unify HCs
into a single classﬁer.

3182

(a)(b)(c)1031041050.30.40.50.60.7Accuracy ofSDSD-BSMF-P-BPCE-BSMF-LF-BS246810Temperature0.30.40.50.60.7Accuracy of0.40.50.60.70.8Accuracy of0.30.40.50.60.7Accuracy of#unlabelled samples inReferences

[1] Shai Avidan. Ensemble tracking. IEEE TPAMI, 29(2):261–

271, 2007. 2

[2] Michael W. Berry, Murray Browne, Amy N. Langville,
Paul V. Pauca, and Robert J. Plemmons. Algorithms and
applications for approximate nonnegative matrix factoriza-
tion. Computational statistics & data analysis, 52(1):155–
173, 2007. 4

[3] Stephen Boyd, Seung-Jean Kim, Lieven Vandenberghe, and
Arash Hassibi. A tutorial on geometric programming. Opti-
mization and engineering, 8(1):67, 2007. 4

[4] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and
Jonathan Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122,
Jan. 2011. 1

[5] Stephen Boyd and Lieven Vandenberghe. Convex optimiza-

tion. Cambridge university press, 2004. 4

[6] Leo Breiman. Random forests. Machine learning, 45(1):5–

32, 2001. 2

[7] Aeron M. Buchanan and Andrew W. Fitzgibbon. Damped
newton algorithms for matrix factorization with missing
data. In CVPR, 2005. 4, 5

[8] Cristian Bucila, Rich Caruana, and Alexandru Niculescu-
Mizil. Model compression. In ACM SIGKDD, pages 535–
541, 2006. 2, 3

[9] Ricardo S. Cabral, Fernando De la Torre, Jo˜ao P. Costeira,
and Alexandre Bernardino. Matrix completion for multi-
label image classiﬁcation. In NIPS, 2011. 4

[10] Emmanuel J. Cand`es and Benjamin Recht. Exact matrix
completion via convex optimization. Foundations of Com-
putational mathematics, 9(6):717, 2009. 4

[11] Alessio Del Bue, Joao Xavier, Lourdes Agapito, and Marco
Paladini. Bilinear modeling via augmented lagrange multi-
pliers (BALM). IEEE TPAMI, 34(8):1496–1508, 2012. 4

[12] Pedro A. Forero, Alfonso Cano, and Georgios B. Gian-
nakis. Consensus-based distributed support vector machines.
JMLR, 11:1663–1707, Aug. 2010. 1

[13] Yoav Freund and Robert Schapire. A short introduction to
Journal-Japanese Society For Artiﬁcial Intelli-

boosting.
gence, 14(771-780):1612, 1999. 2

[14] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross
modal distillation for supervision transfer. In CVPR, 2016. 2
[15] Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-
class adaboost. Statistics and its Interface, 2(3):349–360,
2009. 2

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 6

[17] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Dis-
tilling the knowledge in a neural network.
In NIPS Deep
Learning and Representation Learning Workshop, 2015. 2,
3, 5, 6, 8

[18] Qifa Ke and Takeo Kanade. Robust l1 norm factorization
in the presence of outliers and missing data by alternative
convex programming. In CVPR, 2005. 4

[19] Josef Kittler, Mohamad Hatef, Robert P. W. Duin, and Jiri
Matas. On combining classiﬁers. IEEE TPAMI, 20(3):226–
239, 1998. 2

[20] Jakub Koneˇcn´y, Brendan H. McMahan, Felix X. Yu, Peter
Richtarik, Ananda Theertha Suresh, and Dave Bacon. Fed-
erated learning: Strategies for improving communication ef-
ﬁciency. In NIPS Workshop on Private Multi-Party Machine
Learning, 2016. 1

[21] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix fac-
torization techniques for recommender systems. Computer,
(8):30–37, 2009. 4

[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 6

[23] Ludmila I. Kuncheva. Combining pattern classiﬁers: meth-

ods and algorithms. John Wiley & Sons, 2004. 2, 4

[24] Daniel D Lee and Sebastian H. Seung. Algorithms for non-

negative matrix factorization. In NIPS, 2001. 4

[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 1

[26] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner.
Data-free knowledge distillation for deep neural networks. In
NIPS workshop on learning with limited labeled data, 2017.
2

[27] Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros.
Ensemble of exemplar-SVMs for object detection and be-
yond. In ICCV, 2011. 2

[28] Andrew Ng. Machine Learning Yearning, chapter 39,

page 76. deeplearning.ai, 2018. 5

[29] Takayuki Okatani, Takahiro Yoshida, and Koichiro Deguchi.
Efﬁcient algorithm for low-rank matrix factorization with
missing components and performance comparison of latest
algorithms. In ICCV, 2011. 4

[30] Robi Polikar. Ensemble based systems in decision making.

IEEE Circuits and Systems Magazine, 6(3):21–45. 2

[31] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets:
Hints for thin deep nets. In ICLR, 2015. 2

[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Imagenet large
Aditya Khosla, Michael Bernstein, et al.
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 1, 6

[33] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 6

[34] Paul Viola and Michael Jones. Rapid object detection using

a boosted cascade of simple features. In CVPR, 2001. 2

[35] Zheng Xu, Yen-Chang Hsu, and Jiawei Huang. Learning
loss for knowledge distillation with conditional adversarial
networks. In ICLR workshop, 2017. 2

[36] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. LSUN: Construction of
a large-scale image dataset using deep learning with humans
in the loop. arXiv preprint arXiv:1506.03365, 2015. 6

3183

[37] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE transactions on pattern analysis
and machine intelligence, 40(6):1452–1464, 2018. 6

3184

