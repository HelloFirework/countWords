Scene Parsing via Integrated Classiﬁcation Model and Variance-Based

Regularization

Hengcan Shi, Hongliang Li ∗, Qingbo Wu, Zichen Song
University of Electronic Science and Technology of China

Chengdu, China

shihc@std.uestc.edu.cn, hlli@uestc.edu.cn, qbwu@uestc.edu.cn, szc.uestc@gmail.com

Abstract

Input

Output

Scene Parsing is a challenging task in computer vision,
which can be formulated as a pixel-wise classiﬁcation prob-
lem. Existing deep-learning-based methods usually use one
general classiﬁer to recognize all object categories. How-
ever, the general classiﬁer easily makes some mistakes in
dealing with some confusing categories that share similar
appearances or semantics.
In this paper, we propose an
integrated classiﬁcation model and a variance-based reg-
ularization to achieve more accurate classiﬁcations. On
the one hand, the integrated classiﬁcation model contains
multiple classiﬁers, not only the general classiﬁer but al-
so a reﬁnement classiﬁer to distinguish the confusing cat-
egories. On the other hand, the variance-based regular-
ization differentiates the scores of all categories as large
as possible to reduce misclassiﬁcations. Speciﬁcally, the
integrated classiﬁcation model includes three steps. The
ﬁrst is to extract the features of each pixel. Based on the
features, the second step is to classify each pixel across
all categories to generate a preliminary classiﬁcation re-
sult. In the third step, we leverage a reﬁnement classiﬁer
to reﬁne the classiﬁcation result, focusing on differentiating
the high-preliminary-score categories. An integrated loss
with the variance-based regularization is used to train the
model. Extensive experiments on three common scene pars-
ing datasets demonstrate the effectiveness of the proposed
method. †

(a) general-classifier-based model

…

…

ground

floor

×

Input

Output

(b) integrated classification model

ground

floor

Ground Truth

…

…

√

building

ground

car

floor

light

background

(c)  object categories and ground truth

Figure 1. A comparison of general-classiﬁer-based model and
our proposed integrated classiﬁcation model.
(a) The general-
classiﬁer-based model. (b) The proposed integrated classiﬁcation
model. (c) The ground truth and object categories. The general-
classiﬁer-based model misclassiﬁes the ground object as the ﬂoor
object, whereas this misclassiﬁcation is avoided by the integrated
classiﬁcation model. Moreover, the general-classiﬁer-based mod-
el often predicts close scores for similar categories, while these
scores are more different in the proposed method, beneﬁting from
the variance-based regularization.

1. Introduction

Scene parsing expects to segment an entire image in-
to multiple objects, which acts as a crucial component for
many higher-level computer vision tasks, such as scene un-
derstanding [8, 20], object extraction [15, 26] and language-
based vision analysis [11, 35]. The scene parsing task is

∗Corresponding author.
†Code: https://github.com/shihengcan/ICM-matcaffe

usually formulated as a pixel-wise classiﬁcation problem.

State-of-the-art scene parsing methods [1, 3, 5, 7, 10, 18,
23–25, 31, 36–38, 42, 44–46] mostly leverage deep neural
network (DNN) to tackle this pixel-wise classiﬁcation prob-
lem. These DNN-based methods encode the features of ev-
ery pixel in an image and then classify these pixels by a
general classiﬁer, which focuses on classifying each pixel
across all object categories by one step. However, as inves-

5307

tigated in [2], this strategy usually fails to distinguish the
categories with similar appearances. For example, in Fig.
1, the general-classiﬁer-based model mislabels the ground
object as the ﬂoor object, which shares the similar shape
and textures.

In this paper, we try to solve this problem from two as-
pects. Firstly, we propose an integrated classiﬁcation model
for scene parsing, which contains not only the general clas-
siﬁcation to recognize all object categories but also a re-
ﬁnement classiﬁcation to distinguish confusing categories.
Secondly, we observed that the scores of the confusing cat-
egories are usually close to each other within a classiﬁer,
which is also easy to result in misclassiﬁcations, such as the
example in Fig. 1. Therefore, we propose a variance-based
regularization to differentiate the scores of all categories as
large as possible.

Speciﬁcally, the proposed scene parsing method can be
divided into three steps. In the ﬁrst step, we encode the fea-
tures of all pixels by a deep learning network. Based on
the extracted features, the second step is a general classiﬁ-
cation, which gives preliminary classiﬁcation scores across
all categories. In the preliminary scores, there may be more
than one categories with the high score, which are confus-
ing with respect to the general classiﬁer. Therefore, in the
third step, a reﬁnement classiﬁer is used to reﬁne the scores,
focusing on discriminating these confusing categories. To
reduce the error accumulation between the two classiﬁer-
s, we implement our general classiﬁer with multiple binary
classiﬁers rather than the commonly used multinomial clas-
siﬁer. An integrated classiﬁcation loss with the variance-
based regularization is used to train the integrated classiﬁ-
cation model to enhance its ability of differentiating similar
categories. The proposed method is validated on three com-
mon scene parsing datasets, including the NYU Depth v2,
Pascal-Context and SUN-RGBD datasets. The results show
that our proposed method outperforms many state-of-the-art
methods on these datasets.

This paper is organized as follows. The related work is
introduced in Section 2. In Section 3, we detail the integrat-
ed classiﬁcation model and variance-based regularization.
Experimental results are reported in Section 4 to demon-
strate the effectiveness of our method. Finally, Section 5
concludes this paper.

2. Related Work

In this section, we review recent advances in the scene
parsing task. The existing methods [1, 3–7, 9, 10, 12–14,
18, 21–25, 27, 28, 30–32, 36–38, 42–47] formulate the scene
parsing task as a pixel-wise classiﬁcation problem and tack-
le this problem with the deep neural network (DNN). Long
et al. [28] proposed a fully convolutional network (FCN)
[28]. They leveraged a DNN to directly encode features
for every pixel and then used a general classiﬁer to clas-

sify these pixels. However, since too many downsampling
operations are involved in the DNN, the ﬁnal predictions
generated by the FCN [28] usually lose some details, such
as the small objects and exact object edges.

Many works [3, 5, 30, 43] attempt to enhance the res-
olution of predictions to retain more detailed information.
Chen et al. [5] and Yu et al. [43] replaced a part of down-
sampling layers with atrous convolutions and dilated con-
volutions, respectively. Noh et al. [30] trained a decon-
volutional network to restore the details in the prediction-
s, which is the mirror of a convolutional neural network.
Bilinski et al. [3] changed the connections in the decon-
volutional network into dense connections to enable a fu-
sion between different output resolutions. To further seg-
ment an image on different resolutions, many method-
s [6, 10, 18, 23, 24, 31, 37, 42, 45] proposed to use multi-scale
strategies, including multi-scale averaging [24, 25, 31, 42]
and scale learning [6, 10, 18, 23, 37, 45]. These methods are
able to provide more detailed scene parsing results and thus
reduce the over- and under-segmentations. However, the
highly diverse relationships among the objects in the scene
are ignored, which is useful to constrain the semantic con-
sistency among the scene and every object.

Some approaches [1, 7, 10, 21, 22, 24, 25, 27, 32, 36, 38,
44, 46, 47] model the relationships among the scene and
objects by context models. Zheng et al. [47] and Lin et
al. [25] leveraged the conditional random ﬁelds (CRFs) to
model the relationships between each pair of pixels.
In
[7, 27, 46], convolutions with multi-size perspectives were
used to model the hierarchical object relationships. Lin
et al. [24] changed the size of input image instead of the
size of convolution perspective to achieve the same goal
as [7,27,46]. Zhang et al. [44] and Ding et al. [10] turned to
use dictionary learning context and context contrasted local
features to model these relationships, respectively. RNN-
based context models were proposed by [21, 22, 32, 36, 38]
to model the relationships including the relative positions of
objects. Abdulnabi et al. [1] combined the RNN and atten-
tion model to learn more speciﬁc context. These methods
also achieve remarkable performance for scene parsing.

However, these existing methods only adopt a general
classiﬁer, which is easy to confuse some categories with
similar appearances or semantics.
In order to solve this
problem, this paper proposes an integrated classiﬁcation
model and a variance-based regularization to achieve more
accurate classiﬁcations for the confusing categories.

3. Proposed Method

3.1. Integrated Classiﬁcation Model for Scene Pars 

ing

The scene parsing task can be formulated as a pixel-wise
multinomial classiﬁcation problem. Given an input image

5308

Input

Input

Input

Input

…

…

…

…

…

…

…

…

ground

floor

ground

floor

ground

floor

ground

floor

(a) Case1: the general  classifier 

predicts similar scores for 

confusing categories 

(b) Case2: the general classifier 
predicts very high score for a 

incorrect category in the 

confusing categories

• correct category:    ground
• incorrect category:    floor

Figure 2. Comparison of the multinomial classiﬁer and multiple
binary classiﬁers in misclassiﬁcation cases. (a) In the case of that
the classiﬁer generates similar scores for the correct and incorrect
categories, the two types of classiﬁers show similar probability s-
core distributions. (b) In the case of that the classiﬁer predicts a
very high score for the incorrect category, the multinomial classiﬁ-
er generate a lower score for the correct category than the multiple
binary classiﬁers, due to the across-category competition.

I, our goal is to predict a conditional probability distribu-
tion P (O|I, r) for each pixel r in the image. The random
variable O can take the value in the set {oc}c=1,...,C , where
C is the number of object categories in the dataset and
P (O = oc|I, r) denotes the probability of that the r-th pix-
el belongs to the c-th category of objects. Once P (O|I, r)
is generated, the object category of the r-th pixel can be
deﬁned as follows:

misclassiﬁcations can be corrected by the reﬁnement clas-
siﬁer. The second case can be found in Fig 2(b), where the
classiﬁer predicts a very high score for the incorrect catego-
ry. In this case, the multinomial classiﬁer would yield a very
low score for the correct category because of the across-
category competitions. This probability score distribution is
hard to be corrected by the reﬁnement classiﬁer. In contrast,
in the multiple binary classiﬁers, the scores of categories do
not affect each other. Hence, the binary classiﬁers generate
a relatively higher score for the correct category than the
multinomial classiﬁer, which is more favorable to the next
reﬁnement.

To leverage multiple binary classiﬁers as the general
classiﬁer, it is necessary to convert the multinomial classiﬁ-
cation problem into multiple binary classiﬁcation problems.
We rewrite each probability P (O = oc|I, r) in the distribu-
tion as an equivalent form P (Y1 = 0, ..., Yc−1 = 0, Yc =
1, Yc+1 = 0, ..., YC = 0|I, r), where Yi ∈ {0, 1}(i =
1, ..., C) denotes whether the pixel r belongs to the i-th
object category. Based on this probability form, we de-
compose this multinomial classiﬁcation problem into C bi-
nary classiﬁcation problems, in which each binary classi-
ﬁcation problem determines the probabilities of whether
the r-th pixel belongs to the c-th object category, namely
P (Yc = 1|I, r) and P (Yc = 0|I, r). For simplicity, we use
pr,c, pf g

r,c to represent these probabilities, i.e.:

r,c and pbg

pr,c =P (Y1 = 0, ..., Yc−1 = 0, Yc = 1,

Yc+1 = 0, ..., YC = 0|I, r)

pf g
r,c = P (Yc = 1|I, r)

pbg
r,c = P (Yc = 0|I, r).

(2)

(3)

(4)

ˆc = arg max

P (O = oc|I, r)

(1)

c

Then, the general classiﬁcation can be formulated as fol-
lows:

where ˆc is the predicted object category of the r-th pixel in
the image I.

In the proposed integrated classiﬁcation model, we ﬁrst
leverage a general classiﬁer to predict a preliminary proba-
bility distribution across all object categories and then use a
reﬁnement classiﬁer to distinguish the high-score categories
in the preliminary distribution.

To reduce the error accumulation between the general
classiﬁer and the reﬁnement classiﬁer, we adopt multiple
binary classiﬁers as the general classiﬁer instead of using a
multinomial classiﬁer. Since there are across-category com-
petitions in the multinomial classiﬁer, the misclassiﬁcations
from the multinomial classiﬁer result in more severe conse-
quences than those from the binary classiﬁers. The reason
is given as follows. Generally, there are two main cases of
misclassiﬁcations. The ﬁrst is that the classiﬁer predicts a
sightly higher score for the incorrect category than for the
correct category, such as shown in Fig 2(a). In this case,

{pf g

r,1, ..., pf g

r,C, pbg

r,1, ..., pbg

r,C} = gcls(I, r)

(5)

where, gcls(·) denotes the general classiﬁer.

After predicting general classiﬁcation probabilities pf g
r,c
and pbg
r,c for each category, a reﬁnement classiﬁer then gen-
erates the ﬁnal probability distribution P (O|I, r) as fol-
lows:

P (O|I, r) = rcls(I, r, pf g

r,1, ..., pf g
= {pr,1, pr,2, ..., pr,C}

r,C, pbg

r,1, ..., pbg

r,C)

(6)

where rcls(·) is the reﬁnement classiﬁer. We employ a
multinomial classiﬁer as the reﬁnement classiﬁer, which
takes the features of the r-th pixel in the image I and
the general classiﬁcation probabilities {pf g
r,c}c=1,...,C
as inputs, and then outputs the ﬁnal probability distribu-
tion. The reﬁnement classiﬁer focuses on differentiating the
categories with high general classiﬁcation scores (i.e., high

r,c, pbg

5309

input image I 

scene parsing 

result

Figure 3. The proposed integrated classiﬁcation model contains three parts: (1) a feature extraction network that encodes features of
each pixel, (2) a general classiﬁcation network that generates a preliminary probability distribution across all object categories, and (3) a
reﬁnement classiﬁcation network that differentiates the high-preliminary-score categories and reﬁnes the probability distribution.

pf g
r,c), but is not limited to this. It also has the ability of the
second general classiﬁcation to avoid the error accumula-
tion caused by misclassiﬁcations from the general classiﬁer.
We next illustrate how to implement the proposed inte-

grated classiﬁcation model with the deep neural network.

appearance and semantic information of a region around the
r-th pixel; and d is the dimensionality of each feature vec-
tor. The feature extraction deep network can be implement-
ed with any network structure, such as the commonly used
VGG [40] and ResNet [16].

3.2. Integrated Classiﬁcation Model Implemented

with Deep Neural Network

The network structure of the proposed integrated classi-
ﬁcation model contains three parts, as illustrated in Fig. 3:
(1) a feature extraction network that encodes features of the
input image, (2) a general classiﬁcation network that pre-
dicts a preliminary probability distribution, and (3) a reﬁne-
ment classiﬁcation network that distinguishes the confusing
categories and generates the ﬁnal probability distribution.

3.2.1 Feature Extraction Network

Consider the input image I ∈ RH×W ×D, where H and W
are the height and width of the image, respectively; and D
is the number of channels. We ﬁrst use the feature extrac-
tion deep neural network to encode features of this image as
follows:

F = DN N (I)

= {f1, f2, ..., fHW }

(7)

where F ∈ RH×W ×d is the encoded feature map, in which
each feature vector fr ∈ Rd (r = 1, ..., HW ) encodes the

3.2.2 General Classiﬁcation Network

Based on the feature map F , we employ a series of binary
classiﬁers as a general classiﬁer to determine a preliminary
probability distribution. Each binary classiﬁer predicts a
pair of foreground and background scores for each object
category c as follows:

sf g
r,c = (wf g

c )T fr + bf g

c

sbg
r,c = (wbg

c )T fr + bbg

c

(8)

(9)

c , wbg

c ∈ Rd and bf g

where c = 1, .., C. Here, C is the number of object cate-
gories in the dataset. For the c-th category, sf g
r,c ∈ R de-
notes the predicted score of the r-th pixel belonging to this
category, while sbg
r,c ∈ R is the score of the opposite case.
wf g
c ∈ R are the parameters in the
binary classiﬁer. The scores for all pixels and all categories
can be grouped into a score map Sgcls ∈ RH×W ×2C , and
the corresponding binary classiﬁers can be efﬁciently im-
plemented with a convolutional layer, as shown in Fig. 3(a).
The foreground and background scores are then normal-
ized into the form of probability by a binary logistic regres-

c , bbg

5310

sion:

pf g
r,c =

pbg
r,c =

exp(sf g
r,c)
r,c) + exp(sbg

r,c)

exp(sf g

r,c)

exp(sbg
r,c) + exp(sbg

r,c)

exp(sf g

(10)

(11)

r,c and pbg

where pf g
r,c are the normalized foreground and back-
ground probabilities, respectively. Similar to the score map
S, the probabilities can be grouped into a probability map
P gcls ∈ RH×W ×2C . In the general classiﬁcation network,
the predictions of a category do not compete with ones of
another category.

3.2.3 Reﬁnement Classiﬁcation Network

The reﬁnement classiﬁcation network reﬁnes probability
distribution by differentiating the high-preliminary-score
categories and the second general classiﬁcation. We em-
ploy a multinomial classiﬁer to achieve this goal.

The input of the reﬁnement classiﬁcation network is a
concatenation of the binary classiﬁcation probability map
P gcls, the image feature map F and the orignal image I,
where the feature map F and the orignal image I are ref-
erences to assist with the classiﬁcation. Note that since the
value ranges of P gcls, F , and I may be different, we L2-
normalize them before the concatenation.

The concatenated multi-modal input is then transformed
by a series of convolutional layers. We use 3 × 3 convolu-
tions in these layers to model contextual information among
multiple pixels. Based on the transformed feature map, we
employ an 1 × 1 convolution to generate the reﬁned classi-
ﬁcation score map Srcls ∈ RH×W ×C , where each element
sr,c indicates the score of the r-th pixel belonging to the
c-th category. The reﬁned scores are normalized by a multi-
nomial logistic regression (i.e., softmax) as follows:

variance-based regularization to further reduce misclassiﬁ-
cations; λrcls and λvbr are the factors controlling the rela-
tive importance among these losses and the regularization.
As the general classiﬁer is composed of multiple binary
classiﬁers, we train it in terms of an average loss of multi-
ple binary cross entropy losses, in which each binary cross
entropy loss corresponds to an object categories:

Lgcls = −

1
N

N

∑

i=1

1

HW

HW

∑

r=1

1
C

C

∑

[yi,r,c × log(pf g

i,r,c)

c=1

+ (1 − yi,r,c) × log(pbg

i,r,c)]

(14)
where N is the number of images in the training set; and
yi,r,c ∈ {0, 1} is the scene parsing label, which indicates
whether the r-th pixel in the i-th image belongs to the c-th
object category.

The loss Lrcls for the reﬁnement classiﬁer is formulated

as a multinomial cross entropy loss as follows:

Lrcls = −

1
N

N

∑

i=1

1

HW

HW

C

∑

∑

r=1

c=1

yi,r,c × log(pi,r,c).

(15)

Since similar probability scores of multiple categories
may lead to misclassiﬁcations, we propose a variance-based
regularization Lvbr to avoid this case. The variance-based
regularization Lvbr constrains the scores of different cate-
gories to be as variant as possible. In this paper, inspired by
the Herﬁndahl-Hirschman Index (HHI) [33] in economics,
we adopt the second-order moment as the variance-based
regularization Lvbr:

Lvbr = 1 −

1
N

N

∑

i=1

1

HW

HW

C

∑

∑

r=1

c=1

(pi,r,c)2

(16)

where Lvbr ∈ [0, 1 − 1/C] decreases with increasing vari-
ances among the probabilities {pi,r,c}c=1,...,C .

pr,c =

exp(sr,c)
t=1 exp(sr,t)

∑C

(12)

4. Experiments

where pr,c is the ﬁnal probability of the r-th pixel belong-
ing to the c-th category. The probability set {pr,c}c=1,..,C
denotes the desired probability distribution P (O|I, r).

3.3. Loss Function and Variance Based Regulariza 

tion

We use an integrated classiﬁcation loss with the
variance-based regularization to end-to-end train our full
model:

L = Lgcls + λrclsLrcls + λvbrLvbr

(13)

where Lgcls and Lrcls denote the losses for the general clas-
siﬁer and the reﬁnement classiﬁer, respectively; Lvbr is the

In this section, we validated the proposed integrated clas-
siﬁcation model and variance-based regularization on mul-
tiple scene parsing datasets, including the NYU Depth v2
dataset [39], the PASCAL-Context dataset [29] and the
SUN-RGBD dataset [41].

Datasets. The NYU Depth v2 dataset [39] contains 1449
pairs of RGB and depth images, where 795 pairs for train-
ing and 654 pairs for testing. We use 40 categories object
labels as the same as [13]. Only RGB images and scene
parsing labels are used to train the proposed model. The
PASCAL-Context dataset [29] contains 10103 images. It is
split into training and testing sets, including 4998 and 5105
images, respectively. We use 60 object category labels pro-
vided by [29]. The SUN-RGBD dataset [41] includes 37

5311

Method

pAcc.(%) mIoU(%)

Method

pAcc.(%) mIoU(%)

FCN [28]
DilatedNet [43]
Context [25]
DeepLab v2 [5]
ReﬁneNet [24]
PSPNet [46]
LoopNet [18]
Dense Decoder [3]
DeepLab v3+ [7]
Ours

60.0
65.4
70.0
71.7
73.6
73.6
72.1
73.8
73.8
75.4

29.2
33.7
40.6
42.3
46.5
46.9
44.5
48.1
47.4
50.7

Table 1. Comparison with state-of-the-art methods on the NYU
Depth v2 dataset. LoopNet [18] is trained by both scene parsing
and depth prediction labels, and all other methods are trained only
by the scene parsing labels.

object categories. There are 10335 pairs of RGB and depth
images in this dataset, 5285 pairs for training and 5050 pairs
for testing. Here, we only use the RGB images in the exper-
iments.

Evaluation Metrics. We adopted the pixel-wise accura-
cy (pAcc.) and the mean intersection-over-union (mIoU)
metrics to evaluate the scene parsing performance. The
pixel-wise accuracy is the percentage of correctly classi-
ﬁed pixels in the entire dataset. The mean intersection-over-
union is the average of the intersection-over-union between
the predictions and ground-truths over all categories in the
dataset.

Details of Implementation. We implement the pro-
posed method with the Caffe [17] deep learning toolkit. We
employed PSPNet [46] as the feature extraction deep net-
work, which is implemented with the ResNet [16]. Mean-
while, a three-layer neural network is designed as the reﬁne-
ment classiﬁcation network. Note that we used PSPNet [46]
and the three-layer reﬁnement classiﬁcation network as a
running example, which does not mean that the proposed
model is limited to these networks. The feature extraction
deep network is initialized from the weights pre-trained on
ImageNet dataset [34], and other parts are initialized from
random weights. We trained the proposed model end-to-
end with the stochastic gradient descent (SGD) and the in-
tegrated classiﬁcation loss including the variance-based reg-
ularization. The base learning rate was set to 0.00025, and
the learning rates of the randomly initialized layers were 10
times higher than those of the pre-trained layers. The loss
factors λrcls and λvbr were set to 1 and 0.2, respectively.

4.1. Comparison with State of the art Methods

We compare the proposed method with nine state-of-the-
art scene parsing methods on the NYU Depth v2 dataset.
The results are shown in Table 1. All these methods on-
ly use a general classiﬁer to classify each pixel, in which
Dense Decoder [3] shows the best performance. Compared
with Dense Decoder [3] , the proposed integrated classi-

FCN [28]
DilatedNet [43]
Episodic CAMN [1]
Context [25]
DeepLab v2 [5]
ReﬁneNet [24]
PSPNet [46]
Dense Decoder [3]
EncNet [44]
CCL&GMA [10]
DeepLab v3+ [7]
Ours

65.9
66.4
72.1
71.5
73.6
75.1
75.1
74.9
78.2
78.4
75.5
80.5

35.1
37.0
41.2
43.3
44.5
47.3
47.0
47.8
51.7
51.6
47.4
52.6

Table 2. Comparison with state-of-the-art methods on the
PASCAL-Context dataset.

Method

pAcc.(%) mIoU(%)

FCN [28]
Context [25]
DeepLab v2 [5]
ReﬁneNet [24]
PSPNet [46]
CCL&GMA [10]
DeepLab v3+ [7]
Ours

68.2
78.4
71.9
80.6
79.7
81.4
80.5
82.4

27.4
42.3
32.1
45.9
46.2
47.1
46.7
50.6

Table 3. Comparison with state-of-the-art methods on the SUN-
RGBD dataset.

Input

GroundTruth

PSPNet [46] DeepLab v3+ [7]

Ours

person

cat

bird

tree

bedclothes

sky

grass

bed

wall

building

ground

sofa

car

floor

road

background

Figure 4. Visual comparison on the Pascal-Context dataset. Left to
right: input images, ground truth, and results from PSPNet [46],
DeepLab v3+ [7] and our method.

ﬁcation model achieves improvements of 1.6% and 2.6%
in terms of the pixel-wise accuracy and mean IoU, respec-
tively. Even LoopNet [18] is trained by both scene parsing
and depth prediction labels, the proposed model also out-
performs it by 3.3% and 6.2% in terms of the pixel-wise

5312

Method

Baseline model [46]
Baseline model [46] + DSN [19]
Baseline model [46] + more layers
Integrated classiﬁcation model

Baseline model [46]

Integrated classiﬁcation model

Variance-based
regularization

pAcc.(%) mIoU(%)

Training
Speed(Hz)

Testing

Speed(Hz)

Params(×106)

73.6
73.8
73.3
75.0

73.8
75.4

46.9
46.5
47.1
50.3

47.8
50.7

4.3
4.2
3.9
3.2

4.2

3.2

9.6
9.7
9.0
9.0

9.6

9.0

65.7
65.7
80.8
80.7

65.7

80.7

!
!

Table 4. The effects of main components in the proposed method on the NYU Depth v2 dataset.

Method

Type of general classiﬁer

pAcc.(%) mIoU(%) Mean difference

Baseline model [46]
Integrated classiﬁcation model
Integrated classiﬁcation model Multiple Binary Classiﬁers

Multinomial Classiﬁer
Multinomial Classiﬁer

73.6
74.8
75.4

46.9
49.4
50.7

0.596
0.579
0.414

Table 5. Comparison of different general classiﬁers on the NYU Depth v2 dataset. “Mean difference” is the the mean difference between
the correct and incorrect probability scores when incorrect predictions are generated.

accuracy and mean IoU, respectively. This superior perfor-
mance demonstrates the effectiveness of the proposed inte-
grated classiﬁcation model and variance-based regulariza-
tion.

The results of comparative experiments conducted on
the PASCAL-Context dataset and the SUN-RGBD dataset
are shown in Table 2 and Table 3, respectively. On
the PASCAL-Context dataset, compared with the previous
state-of-the-art results, the proposed method achieves im-
provements of 2.1% and 0.9% in terms of the pixel-wise
accuracy and mean IoU, respectively. On the SUN-RGBD
dataset, our proposed method outperforms the previous s-
tate of the art by 1.0% pixel-wise accuracy and 3.5% mean
IoU.

We depict some visualized scene parsing results in Fig.
4. It can be observed that the general-classiﬁer-based meth-
ods mislabel some objects. For example, in the third image
in Fig. 4, the sky object is mislabeled as the wall object; in
the fourth image in Fig. 4, the road object is misclassiﬁed
as the ground object. The proposed method successfully
avoids such misclassiﬁcations, beneﬁting from the integrat-
ed classiﬁcation and the variance-based regularization.

4.2. Ablation Study

In this section, we conduct a series of ablation experi-
ments to further evaluate the effectiveness of our proposed
method.

Effects of main components. We give the effects of
our main components in Table 4. Without the variance-
based regularization, compared with the baseline general-
classiﬁer-based model [46], the proposed integrated classi-
ﬁcation model improves pixel-wise accuracy and mean IoU
by 1.4% and 3.4%, respectively. This demonstrates the ef-
fectiveness of the proposed integrated classiﬁcation model.
DSN [19] adds multiple classiﬁers to different layer-

s as supervision in the training stage. Our method out-
performs “Baseline model [46] + DSN [19]”, because our
method not only uses the classiﬁcation results as supervi-
sion but also corrects these results in the test stage to pro-
mote the classiﬁcation accuracy. In “Baseline model [46] +
more layers”, we add more parameters to the baseline [46].
Compared with this method, our method achieves gains of
1.7% and 3.2%, respectively, in terms of pixel-wise accu-
racy and mean IoU. This result demonstrates that the pro-
posed method improves parsing accuracy mainly through
integrating multiple classiﬁers rather than adding more pa-
rameters.

Moreover, the baseline model [46] and proposed model
trained with the variance-based regularization both achieve
the better pixel-wise accuracy and mean IoU than them
trained without the regularization. Ultimately, our ful-
l method (the integrated classiﬁcation model and variance-
based regularization) outperforms the baseline model [46]
by 1.8% and 3.8% in terms of the pixel-wise accuracy and
mean IoU, respectively.

Computation costs. Table 4 shows the computation cost
of our method. It can be seen that the proposed method im-
proves the scene parsing accuracy with acceptable compu-
tation overhead.

Effects of different general classiﬁers. The effects of
different general classiﬁers are shown in Table 5. It can be
observed that whether a multinomial classiﬁer or multiple
binary classiﬁers are used as our general classiﬁer, the pro-
posed integrated classiﬁcation model provides better perfor-
mance than the baseline model [46]. Using multiple binary
classiﬁers achieves more improvements. The reason is that
the multiple binary classiﬁers are easier to avoid error accu-
mulation than the multinomial classiﬁer, as explained in the
Section 3.1. In Table 5, we depict the mean difference be-
tween the correct and incorrect scores when predictions are

5313

The number Dimensionality of

of layers

hidden layers

pAcc.(%) mIoU(%)

1
2
2
2
3
3
3
3
3

/

256
512
4096
256
512
1024
2048
4096

74.1
73.9
74.1
74.9
74.5
75.0
75.4
75.5
75.2

49.0
48.9
49.3
49.9
49.8
50.6
50.7
50.6
51.0

Table 6. Comparison of different numbers of layers and different
hidden layer dimensionalities of the reﬁnement classiﬁcation net-
work on the NYU Depth v2 dataset.

General classiﬁcation

Image

probability map

feature map

Original
image

pAcc.(%) mIoU(%)

!
!
!
!

!

!

!
!

74.7

74.8

75.1
75.4

50.1

50.2

49.9
50.7

Table 7. The effects of different inputs of the reﬁnement classiﬁ-
cation network on the NYU Depth v2 dataset.

incorrect. It can be seen that the mean difference obviously
decreases by using multiple binary classiﬁers.

Structures of the reﬁnement classiﬁcation network.
Table 6 shows the effects of different structures of the re-
ﬁnement classiﬁcation network.
It can be seen that the
best pixel-wise accuracy and mean IoU are achieved by the
three-layer networks with 2048- and 4096-dimensional hid-
den layers, respectively. Compared with these structures,
the three-layer network with 1024-dimensional hidden lay-
ers shows comparable performance but less computation
costs. To balance the performance with the computation
costs, we ﬁnally adopt the three-layer network with 1024-
dimensional hidden layers in other experiments.

Inputs of the reﬁnement classiﬁcation network. The
effects of different inputs of the reﬁnement classiﬁcation
network are listed in Table 7. From Tables 7 and 4, it can
be observed that when we only input the general classiﬁ-
cation probability map to the reﬁnement classiﬁcation net-
work, the proposed model outperforms the baseline mod-
el [46] by 1.1% and 3.2% in terms of the pixel-wise accura-
cy and mean IoU, respectively. Inputting the image feature
map and original image further improves the scene pars-
ing accuracy, because they can be regard as references to
assist with the classiﬁcation. The best pixel-wise accuracy
and mean IoU are achieved when the general classiﬁcation
probability map, image feature map and original image are
input together.

Effects of different λvbr. Table 8 shows how the scene
parsing performance is affected by the weight λvbr of the

λvbr

0

0.1 0.2 0.5

1

2

pAcc.(%) 75.0 75.1 75.4 75.3 74.8 74.9
mIoU(%) 50.3 50.1 50.7 50.1 50.0 49.7

Table 8. The effects of different factor λvbr in the integrated loss
function on the NYU Depth v2 dataset.

otherstructure
otherstructure

Input

GroundTruth

Ours

wall

ceiling

door

shelves

window

sofa

floor

picture

floor mat

otherprop

otherstructure

Figure 5. Parsing failures of the proposed method on the NYU
Depth v2 dataset.

variance-based regularization in the integrated loss func-
tion. It can be observed that the best pixel-wise accuracy
and mean IoU are achieved when λvbr is 0.2.

Analysis of failure cases. We display some parsing fail-
ures of the proposed method in Fig. 5. The ﬁrst type of
parsing errors is over-segmentation when objects contains
various colors, such as the otherstructure (ﬁreplace) object
in the ﬁrst image in Fig. 5. Another type of parsing errors
is imprecise segmentation of delicate object edges, such as
the otherprop (plant) object in the third image in Fig. 1.
These problems may be alleviated by parsing the scene on
different scales.

5. Conclusion

In this paper, we have presented an integrated classiﬁca-
tion model and a variance-based regularization for the scene
parsing task. The integrated classiﬁcation model ﬁrst en-
codes features of each pixel. Then a series of binary clas-
siﬁers are used to classify these pixels across all object cat-
egories. Based on the results of the general classiﬁcation,
we ﬁnally leverage a reﬁnement classiﬁer to discriminate
the confusing categories. The variance-based regularization
is used to train the proposed integrated classiﬁcation mod-
el to differentiate the classiﬁcation scores of all categories
to be as large as possible. We have demonstrated the ef-
fectiveness of our method on three common scene parsing
datasets. In the future, we hope to fuse multi-scale methods
with our model to reduce over-segmentations and imprecise
object edges.

Acknowledgement.
supported in
part by National Natural Science Foundation of China
(No.61831005, 61525102, 61601102 and 61871078).

This work was

5314

References

[1] A. H. Abdulnabi, B. Shuai, S. Winkler, and G. Wang.
Episodic camn: Contextual attention-based memory net-
works with iterative feedback for scene labeling.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5561–5570, 2017.

[2] K. Ahmed, M. H. Baig, and L. Torresani. Network of experts
for large-scale image categorization. In Proceedings of the
European Conference on Computer Vision, pages 516–532.
Springer, 2016.

[3] P. Bilinski and V. Prisacariu. Dense decoder shortcut con-
nections for single-pass semantic segmentation. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6596–6605, 2018.

[4] H. Caesar, J. Uijlings, and V. Ferrari. Region-based semantic
segmentation with end-to-end training. In Proceedings of the
European Conference on Computer Vision, pages 381–397.
Springer, 2016.

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 40(4):834–848, 2018.

[6] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At-
tention to scale: Scale-aware semantic image segmentation.
In Proceedings of the IEEE conference on Computer Vision
and Pattern Recognition, pages 3640–3649, 2016.

[7] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam.
Encoder-decoder with atrous separable convolution for se-
mantic image segmentation. In Proceedings of the European
Conference on Computer Vision, 2018.

[8] Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented
adaptation for semantic segmentation of urban scenes.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7892–7901, 2018.

[9] J. Dai, K. He, and J. Sun. Convolutional feature masking for
joint object and stuff segmentation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3992–4000, 2015.

[10] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang. Con-
text contrasted feature and gated multi-scale aggregation for
scene segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2393–
2402, 2018.

[11] C. Gan, Y. Li, H. Li, C. Sun, and B. Gong. Vqs: Linking seg-
mentations to questions and answers for supervised attention
in vqa and question-focused semantic segmentation. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 1811–1820, 2017.

[12] S. Gupta, P. Arbel´aez, R. Girshick, and J. Malik.

Indoor
scene understanding with rgb-d images: Bottom-up segmen-
tation, object detection and semantic segmentation. Interna-
tional Journal of Computer Vision, 112(2):133–149, 2015.

[13] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization
and recognition of indoor scenes from rgb-d images. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 564–571, 2013.

[14] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learning
rich features from rgb-d images for object detection and seg-
mentation. In Proceedings of the European Conference on
Computer Vision, pages 345–360. Springer, 2014.

[15] J. Han, L. Yang, D. Zhang, X. Chang, and X. Liang. Rein-
forcement cutting-agent learning for video object segmenta-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 9080–9089, 2018.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolution-
al architecture for fast feature embedding.
In Proceedings
of the 22nd ACM international conference on Multimedia,
pages 675–678, 2014.

[18] S. Kong and C. C. Fowlkes. Recurrent scene parsing with
perspective understanding in the loop. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2018.

[19] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-
supervised nets. In Proceedings of the Artiﬁcial Intelligence
and Statistics, pages 562–570, 2015.

[20] X. Li, Z. Jie, W. Wang, C. Liu, J. Yang, X. Shen, Z. Lin,
Q. Chen, S. Yan, and J. Feng. Foveanet: Perspective-aware
urban scene parsing. In Proceedings of the IEEE Internation-
al Conference on Computer Vision, pages 784–792, 2017.

[21] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan. Semantic ob-
ject parsing with graph lstm. In Proceedings of the European
Conference on Computer Vision, pages 125–143. Springer,
2016.

[22] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan.
Semantic object parsing with local-global long short-term
memory. In Proceedings of the IEEE Conference on Comput-
er Vision and Pattern Recognition, pages 3185–3193, 2016.
[23] D. Lin, G. Chen, D. Cohen-Or, P.-A. Heng, and H. Huang.
Cascaded feature network for semantic segmentation of rgb-
d images. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 1311–1319, 2017.

[24] G. Lin, A. Milan, C. Shen, and I. Reid. Reﬁnenet: Multi-path
reﬁnement networks for high-resolution semantic segmenta-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1925–1934, 2017.

[25] G. Lin, C. Shen, A. Van Den Hengel, and I. Reid. Exploring
context with deep structured models for semantic segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(6):1352–1366, 2018.

[26] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path aggregation
network for instance segmentation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 8759–8768, 2018.

[27] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. CoRR, abs/1506.04579, 2015.

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015.

5315

[44] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal. Context encoding for semantic segmentation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2018.

[45] R. Zhang, S. Tang, Y. Zhang, J. Li, and S. Yan. Scale-
adaptive convolutions for scene parsing.
In Proceedings
of the IEEE International Conference on Computer Vision,
pages 2031–2039, 2017.

[46] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
In Proceedings of the IEEE Conference
parsing network.
on Computer Vision and Pattern Recognition, pages 2881–
2890, 2017.

[47] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random
ﬁelds as recurrent neural networks.
In Proceedings of the
IEEE International Conference on Computer Vision, pages
1529–1537, 2015.

[29] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-
dler, R. Urtasun, and A. Yuille. The role of context for object
detection and semantic segmentation in the wild. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 891–898, 2014.

[30] H. Noh, S. Hong, and B. Han. Learning deconvolution net-
work for semantic segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1520–
1528, 2015.

[31] S.-J. Park, K.-S. Hong, and S. Lee. Rdfnet: Rgb-d multi-
level residual feature fusion for indoor semantic segmenta-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 4980–4989, 2017.

[32] Z. Peng, R. Zhang, X. Liang, X. Liu, and L. Lin. Geomet-
ric scene parsing with hierarchical lstm. In Proceedings of
the Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence, pages 3439–3445, 2016.

[33] S. A. Rhoades. The herﬁndahl-hirschman index. Fed. Res.

Bull., 79:188, 1993.

[34] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015.

[35] H. Shi, H. Li, F. Meng, and Q. Wu. Key-word-aware network
for referring expression image segmentation. In Proceedings
of the Proceedings of the European Conference on Computer
Vision, pages 38–54, 2018.

[36] H. Shi, H. Li, F. Meng, Q. Wu, L. Xu, and K. N. Ngan. Hi-
erarchical parsing net: Semantic scene parsing from global
scene to objects. IEEE Transactions on Multimedia, 2018.

[37] H. Shi, H. Li, Q. Wu, F. Meng, and K. N. Ngan. Boosting
scene parsing performance via reliable scale prediction. In
2018 ACM Multimedia Conference on Multimedia Confer-
ence, pages 492–500. ACM, 2018.

[38] B. Shuai, Z. Zuo, B. Wang, and G. Wang. Scene segmenta-
tion with dag-recurrent neural networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 40(6):1480–
1493, 2018.

[39] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.

Indoor
In
segmentation and support inference from rgbd images.
Proceedings of the European Conference on Computer Vi-
sion, pages 746–760. Springer, 2012.

[40] K. Simonyan and A. Zisserman. Very deep convolutional
International

networks for large-scale image recognition.
Conference on Learning Representations, 2015.

[41] S. Song, S. P. Lichtenberg, and J. Xiao. Sun rgb-d: A rgb-d
scene understanding benchmark suite. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 567–576, 2015.

[42] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. Learn-
ing a discriminative feature network for semantic segmenta-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018.

[43] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In International Conference on Learning
Representations, 2016.

5316

