Towards High-ﬁdelity Nonlinear 3D Face Morphable Model

Luan Tran, Feng Liu, Xiaoming Liu

Department of Computer Science and Engineering
Michigan State University, East Lansing MI 48824
{tranluan, liufeng6, liuxm}@msu.edu

Input

Reconstruction

Albedo

Shape

O
u
r

T
e
w
a
r
i

e
t
a
l
.

[
3
9
]

Figure 1: With novel enhancements in both learning objective as well as the network architecture, our proposed nonlinear 3D
morphable model enables, for the ﬁrst time, regressing high-ﬁdelity facial shape (geometry) and albedo (skin reﬂectence) by
directly estimating model latent representations.

Abstract

1. Introduction

Embedding 3 D morphable basis functions into deep
neural networks opens great potential for models with bet-
ter representation power. However, to faithfully learn those
models from an image collection, it requires strong regu-
larization to overcome ambiguities involved in the learning
process. This critically prevents us from learning high ﬁ-
delity face models which are needed to represent face im-
ages in high level of details. To address this problem, this
paper presents a novel approach to learn additional prox-
ies as means to side-step strong regularizations, as well as,
leverages to promote detailed shape/albedo. To ease the
learning, we also propose to use a dual-pathway network,
a carefully-designed architecture that brings a balance be-
tween global and local-based models. By improving the
nonlinear 3 D morphable model in both learning objective
and network architecture, we present a model which is su-
perior in capturing higher level of details than the linear
or its precedent nonlinear counterparts. As a result, our
model achieves state-of-the-art performance on 3 D face re-
construction by solely optimizing latent representations.

Project website: http://cvlab.cse.msu.edu/

project-nonlinear-3dmm.html

Computer vision and computer graphics ﬁelds have had
much interest in the longstanding problem of 3D face recon-
struction — creating a detailed 3D model of a person’s face
from a collection or a single photograph. The problem is
important with many applications, including but not limited
to face recognition [1, 26, 50], video editing [12, 41], avatar
puppeteering [8, 10, 51] or virtual make-up [13, 24].

Recently, an incredible amount of attention is drawn into
the simplest but most challenging form of this problem:
monocular face reconstruction. Inferring a 3D face mesh
from a single 2D photo is arduous and ill-posed since the
image formation process blends multiple facial components
(shape, albedo) as well as environment (lighting) into a
single color for each pixel. To better handle the ambigu-
ity, one must rely on additional prior assumptions, such as
constraining faces to lie in a restricted subspace, e.g., 3D
Morphable Models (3DMM) [6] learned from a small 3D
scans collection. Many state-of-the-art approaches, either
learning-based [33, 34] or optimization-based face recon-
struction [5,14], heavily rely on such priors. While yielding
impressive results, these algorithms do not generalize well
beyond the underlying model’s restricted low-dimensional
subspace. As a consequence, the reconstructed 3D face may

11126

fail to recover important facial features, contain incorrect
details or not well aligned to the input face.

Recently, with the ﬂourishing in neural network, a few
attempts have tried to use deep neural networks to replace
the 3DMM basis functions [39, 44]. This increases the
model representation power and learns model directly from
unconstrained 2D images to better capture in-the-wild vari-
ations. However, even with better representation powers,
these models still rely on many constraints [39] to regu-
larize the model learning. Hence, their objectives involve
the conﬂicting requirements of a strong regularization for a
global shape vs. a weak regularization for capturing higher
level details. E.g., in order to faithfully separate shading
and albedo, albedo is usually assumed to be piecewise con-
stant [22, 36], which prevents learning albedo with high
level of details.
In this work, besides learning the shape
and albedo, we propose to learn additional shape and albedo
proxies, on which we can enforce regularizations. This also
allows us to ﬂexibly pair the true shape with strongly reg-
ularized albedo proxy to learn the detailed shape or vice
versa. As a result, each element can be learned with high
ﬁdelity without sacriﬁcing the other element’s quality.

On a different note, many 3DMM models fail to repre-
sent small details because of their parameterization. Many
global 3D face parameterizations have been proposed to
overcome the ambiguities associated with single image face
ﬁtting such as noise or occlusion. However, because they
are designed to model the whole face at once, it is chal-
lenging to use them to represent small details. Meanwhile,
local-based models can be more expressive than global ap-
proaches but with the cost of being less constrained to
realistically represent human faces. We propose using
dual-pathway networks to provide a better balance between
global and local-based models. From the latent space, there
is a global pathway focusing on the inference of global face
structure and multiple local pathways generating details of
different semantic facial parts. Their corresponding features
are then fused together for successive process generation of
the ﬁnal shape and albedo. This network also helps to spe-
cialize ﬁlters in local pathways for each facial part which
both improves the quality and saves the computation power.
In this paper, we improve the nonlinear 3D face mor-

phable model in both learning objective and architecture:

• We solve the conﬂicting objective problem by learning

shape and albedo proxies with proper regularization.

• The novel pairing scheme allows learning both de-

tailed shape and albedo without sacriﬁcing one.

• The global-local-based network architecture offers

more balance between robustness and ﬂexibility.

• Our model allows high-ﬁdelity 3D face reconstruction

by solely optimizing latent representations.

2. Prior Work

Linear 3DMM. The ﬁrst generic 3D face model is built
by Blanz and Vetter [6] using principal component analy-
sis (PCA) on 3D scans. Since this seminal work, there has
been a large amount of effort on improving 3DMM model-
ing mechanism. Paysan et al. [30] replace the previous UV
space alignment [6] by Nonrigid Iterative Closest Point [2]
to directly align 3D scans. Vlasic et al. [49] use a multi-
linear model to describe the combined effect of expression
and identity variation on the facial geometry. On the texture
side, Booth et al. [7] explore feature-based texture model to
represent in-the-wild texture variations.
Nonlinear face model. Recently, there is a great interest
to use deep neural networks to present the 3DMM. Early
work by Duong et al. [29] use Deep Boltzmann Machines
to present 2D Active Appearance Models. Bagautdinov et
al. [3] use Variational Autoencoder (VAE) to learn to model
facial geometry directly from 3D scans. On another direc-
tion, Tewari et al. [39] and Tran and Liu [44] attempt to
learn 3DMM models from a 2D image collection. Tewari et
al. [39] embed shape and albedo bases in multi-layer per-
ceptions. Meanwhile, Tran and Liu [44] use convolution
neural networks by representing both geometry and skin re-
ﬂectance in UV space. Despite having greater representa-
tion power, these models still have difﬁculty in recovering
small details in the input images due to strong regulariza-
tions in their learning objectives.
Global/local-based facial parameterization. Although,
global 3D face parameterizations [23, 49] can remedy the
vagueness associated with monocular face tracking [4, 11];
they can’t represent small geometry details without mak-
ing them exceedingly large and unwieldy. Hence, region or
local-based models are proposed to overcome this problem.
Blanz and Vetter [6] and Tena et al. [37] learn a region-
based PCA, where Blanz and Vetter [6] segment the face
into semantic subregions (eyes, nose, mouth), while Tena et
al. [37] further split into smaller regions to increase the
model’s expressiveness. Other approaches include a region-
based blendshape [18] or localized multilinear model [9].
All these models bring more ﬂexibility than the global one
but at the cost of being less constrained on realistically rep-
resenting human faces. Our approach offers a balance be-
tween global and local models by using a dual-pathway net-
work architecture. Bagautdinov et al. [3] try to achieve
a similar objective with compositional VAE by introduc-
ing multiple layers of hidden variables, but at a cost of ex-
tremely large numbers of hidden variables.
Residual learning.
Residual learning has been used in
many vision tasks. In super resolution, Kim et al. [21] pro-
pose to learn the difference between the high-resolution tar-
get and the low-resolution input rather than estimating the
target itself. In face alignment [19], or missing data impu-
tation task [46], residual learning is used in many cascade

1127

of networks to iteratively reﬁne their estimation by learning
the difference with the true target. In this work, we lever-
age residual learning idea but with a different purpose to
overcome conﬂicting objectives in learning 3D models.

3. Proposed Method

For completeness, we start by brieﬂy summarizing the
traditional linear 3DMM, the recently proposed nonlinear
3DMM learning method including their limitations. Then
we introduce our proposed improvements in both learning
objective and network architecture.

3.1. Linear 3DMM

The 3D Morphable Model (3DMM) [6] provides para-
metric models representing faces using two components:
shape (geometry) and albedo (skin reﬂectance). Blanz et
al. [6] describe the 3D face space with PCA. The 3D face
mesh S ∈ R3Q with Q vertices is computed as:

S = FS(fS|ΘS) = ΘS fS,

(1)

where FS(fS|ΘS) is a function of fS ∈ RlS , parameterized
by ΘS. In linear model, FS is simply a matrix multiplica-
tion (the mean shape is omitted for clarity).

The albedo of the face A ∈ R3Q is deﬁned within a tem-
plate shape, describing the R, G, B colors of Q correspond-
ing vertices. A is also formulated in a similar fashion:

A = FA(fA|ΘA) = ΘAfA.

(2)

To synthesize 2D face images, the 3D mesh is projected
onto the image plan with the weak perspective projection
model. Then, the texture and 2D image is rendered using an
illumination model, i.e., Spherical Harmonics [32].

3.2. Nonlinear 3DMM

Recently, Tewari et al. [39], Tran and Liu [44, 45] con-
currently propose to use deep neural network to present
3DMM bases. Essentially, mappings FS and FA are now
represented as neural networks with parameters ΘS, ΘA re-
spectively. Tewari et al. [39] straightforwardly use multi-
layer perceptron as their networks. Meanwhile, Tran and
Liu [44] leverage spatial relation of vertices by presenting
both S and A in a UV space, denoted SUV, AUV. Mappings
F∗ are convolution neural networks (CNNs) with an extra
sampling step converting from RUV to R3Q. To make the
framework end-to-end trainable, they also learn a model ﬁt-
ting module, E, which is another CNN. Beside estimating
shape, albedo latent vectors fS, fA, the encoder E also esti-
mates projection matrix M as well as lighting coefﬁcients
L. The objective of the whole network is to reconstruct the
original input image via a differentiable rendering layer R:

arg min

E,DS ,DAXI

Lrec(ˆI, I),

(3)

ˆI = R (EM (I), EL(I), FS(ES(I)), FA(EA(I))) .

Reconstruction loss. There are many design options for
the reconstruction loss. The straightforward choice is com-
paring images in the pixel space, with typical l1 or l2 loss.
To better handle outliers, the robust l2,1 is adopted, where
the distance in the RGB color space is based on l2 and the
summation is based on l1-norm to enforce sparsity [41, 42]:

Li

rec =

1

|V| Xq∈V(cid:13)(cid:13)(cid:13)

ˆI(q) − I(q)(cid:13)(cid:13)(cid:13)2

,

(4)

where V is the set of pixels covered by the estimated mesh.
The closeness between images ˆI and I can also be en-

forced in the feature space (perceptual loss):

Lf

rec =

1

|C| Xj∈C

1

WjHjCj

||ϕj(ˆI) − ϕj(I)||2
2.

(5)

The loss is summed over C, a subset of layers of the network
ϕ. Here ϕj(I) is the activations of the j-th layer of ϕ with
dimension Wj × Hj × Cj obtained when processing I.

The ﬁnal reconstruction loss is a weighted average be-

tween the image and feature reconstruction losses:

Lrec(ˆI, I) = Li

rec(ˆI, I) + λf Lf

rec(ˆI, I).

(6)

Sparse Landmark Alignment.
To help achieve better
model ﬁtting, which in turn helps to improve the model
learning itself, the landmark alignment loss is used as an
auxiliary task. The loss is deﬁned by Euclidean distance
between estimated and groundtruth landmarks:

Llan =(cid:13)(cid:13)(cid:13)(cid:13)

M ∗(cid:20)S(:, d)

1 (cid:21) − U(cid:13)(cid:13)(cid:13)(cid:13)

2

2

,

(7)

where U ∈ R2×68 is the manual labels of 2D landmark
locations, d stores the indexes of 68 vertices corresponding
to the sparse 2D landmarks in the 3D face mesh. In [44,45],
the landmark loss is only applied on E to prevent learning
implausible shapes as the loss only affects a tiny subsets of
vertices related to the keypoints.
Different regularization.
To overcome ambiguity and
faithfully recover different elements (shape, albedo, light-
ing), many regularizations are needed.

Albedo Symmetry:

Lsym(A) = kAuv − ﬂip(Auv)k1 ,

(8)

where ﬂip() is a horizontal image ﬂip operation.
Albedo Constancy:

Lcon(A) = Xvuv

∈Ni

j

ω(vuv

i , vuv

j )(cid:13)(cid:13)

Auv(vuv

i ) − Auv(vuv

j )(cid:13)(cid:13)

p
2 .

(9)

The weight ω(vuv
helps to penalize more on pixels with the same chromaticity

i ) − c(vuv

j ) = exp(cid:0)−α(cid:13)(cid:13)

j )(cid:13)(cid:13)(cid:1),

i , vuv

c(vuv

1128

Figure 2: The proposed framework. Each shape or albedo decoder consist of two branches to reconstruct the true element and its proxy.
Proxies free shape and albedo from strong regularizations, allow them to learn models with high level of details.

(i.e., c(x) = I(x)/|I(x)|), where the color is referenced
from the input image using the current estimated projection.
Ni denotes a set of 4-pixel neighborhood of pixel vuv
i .

Shape Smoothness: This is a Laplacian regularization on

By pairing two shapes S, ˜S and two albedos A, ˜A, we
can render four different output images (Fig. 2). Any of
them can be used to compare with the original input image.
We rewrite our reconstruction loss as:

the vertex locations.

Lsmo(S) = Xvuv

∈Suv

i

Suv(vuv

i ) −

1

|Ni| Xvuv

∈Ni

j

Suv(vuv

. (10)

rec = Lrec(ˆI(˜S, ˜A), I)
L∗
+ Lrec(ˆI(˜S, A), I)
+ Lrec(ˆI(S, ˜A), I).

(15)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

j )(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

The overall objective can be summarized as:

L = Lrec(ˆI, I) + Llan + Lreg,

(11)

with Lreg = Lsym(A) + λconLcon(A) + λsmoLsmo(S). (12)

3.3. Nonlinear 3DMM with Proxy and Residual

Proxy and Residual Learning.
Strong regularization
has been shown to be critical in ensuring the plausibility
of the learned models [39, 45]. However, the strong regular-
ization also prevents the model from recovering high-level
details in either shape or albedo. Hence, this prevents us
from achieving the ultimate goal of learning a high-ﬁdelity
3DMM model.

In this work, we propose to learn additional proxy
shape (˜S) and proxy albedo ( ˜A), on which we can apply
the regularization. All presented regularizations will now
be moved to proxies:

L∗

reg = Lsym( ˜A) + λconLcon( ˜A) + λsmoLsmo(˜S).

(13)

There will be no regularization applied directly to the ac-
tual shape S and albedo A, other than a weak regularization
encouraging each to be close to its proxy:

Lres = k∆Sk1+k∆Ak1 = (cid:13)(cid:13)(cid:13)

S − ˜S(cid:13)(cid:13)(cid:13)1

+(cid:13)(cid:13)(cid:13)

A − ˜A(cid:13)(cid:13)(cid:13)1

. (14)

Pairing strongly regularized proxies and weakly regular-
ized components is a critical point in our approach. Using
proxies allows us to learn high-ﬁdelity shape and albedo
without sacriﬁcing quality of either component. This pair-
ing is inspired by the observation that Shape from Shad-
ing techniques are able to recover detailed face mesh by
assuming over regularized albedo or even using the mean
albedo [34]. Here, Lrec(ˆI(S, ˜A), I) loss promotes S to re-
cover more details as ˜A is constrained by piece-wise con-
stant Lcon( ˜A) objective. Vice versa, Lrec(ˆI(˜S, A), I) aims
to learn better albedo.
In order for these two losses to
work as desired, proxies ˜S and ˜A should perform well
enough to approximate the input images by themselves.
Without Lrec(ˆI(˜S, ˜A), I), a valid solution that minimizes
Lrec(ˆI(S, ˜A), I) is combination of a constant albedo proxy
and noisy shape creating surface normal with dark shading
in necessary regions, i.e., eyebrows.

Another notable design choice is that we intentionally
left out the loss function on ˆI(S, A), even though this the-
oretically is the most important objective. This is to avoid
the case that the shape S learns an in-between solution that
works well with both ˜A, A and vice versa.
Occlusion Imputation. With proposed objective func-
tion, our model is able to faithfully reconstruct input im-
ages. However, we empirically found that besides high-

1129

4x 

Figure 3: The proposed global-local-based network architecture.

ﬁdelity visible regions, the model tends to keep invisible
region smooth. The reason might be that, there is no su-
pervision on those areas other than the residual magnitude
loss pulling the shape and albedo closer to their proxies. To
learn a more meaningful model, which is beneﬁcial to other
applications, i.e., face editing or face synthesis, we propose
to use a soft symmetry loss [43] on occluded regions:

Lres-sym(S) = kT ⊙ (∆Suv

z − ﬂip(∆Suv

z ))k1 ,

(16)

where T is a visibility mask of each pixel in UV space,
approximated based on estimated surface normal direction.
Even though the shape itself is not symmetric, i.e., face with
asymmetric expression; we enforce symmetrical property
on its depth residual ∆Sz (only use shape’s z-dimension).

3.4. Global Local Based Network Architecture

While global-based models are usually robust to noise
and mismatches, they are usually over-constrained and do
not provide sufﬁcient ﬂexibility to represent high-frequency
deformations as local-based models.
In order to take the
best of both worlds, we propose to use dual-pathway net-
works for our shape and albedo decoders.

Here, we transfer the success of combining local and
global models in image synthesis [15, 28] to 3D face mod-
eling. The general architecture of a decoder is shown in
Fig. 3. From the latent vector, there is a global pathway fo-
cusing on inferring the global structure and a local pathway
with four small sub-networks generating details of different
facial parts, including eyes, nose and mouth. The global
pathway is built from fractional strided convolution layers
with ﬁve up-sampling steps. Meanwhile, each sub-network
in the local pathway has the similar architecture but shal-
lower with only three up-sampling steps. Using different
small sub-networks for each facial part offers two beneﬁts:
i) with less up-sampling steps, the network is better able
to represent high-frequency details in early layers; ii) each
sub-network can learn part-speciﬁc ﬁlters, which is more
computationally efﬁcient than applying across global face.
As shown in Fig. 3, to fuse two pathways’ features, we
ﬁrstly integrate four local pathways’ outputs into one sin-
gle feature tensor. Different from other works that synthe-
size face images with different yaw angles [20, 47, 48] with
no ﬁxed keypoints’ locations, our 3DMM generates facial
albedo as well as 3D shape in UV space with predeﬁned

Input

l2,1

l2,1+

Grad. diff.

l2,1+

Perceptual

Figure 4: Reconstruction results with different loss functions.

topology. Merging these local feature tensors is efﬁciently
done with the zero padding operation. The max-pooling fu-
sion strategy is also used to reduce the stitching artifacts on
the overlapping areas. Then the resultant feature is simply
concatenated with the global pathway’s feature, which has
the same spatial resolution. Successive convolution layers
integrate information from both pathways and generate the
ﬁnal albedo/shape (or their proxies).

4. Experimental Results

We study different aspects of the proposed framework,
in terms of framework design, model representation power,
and applications to facial analysis.

The training is similar to [45], which also include a pre-
train stage with supervised losses. Adopting Basel Face
Model (BFM) [30]’s facial mesh triangle topology, we use a
subset of Q = 39, 111 vertices on the face region only. The
model is trained on 300W-LP dataset [52], which contains
122, 450 in-the-wild face images, in a wide pose range.

The model is optimized using Adam optimizer with a
learning rate of 0.001. We set the following parameters:
U = 192, V = 224, lS = lA = 320. λ values are selected
to bring losses to similar magnitudes.

4.1. Ablation Study

Reconstruction Loss Functions. We study effects of dif-
ferent reconstruction losses on quality of the reconstructed
images (Fig. 4). As expected, the model trained with l2,1
loss only results in blurry reconstruction, similar to other
lp loss. To make the reconstruction more realistic, we ex-
plore other options such as gradient difference [27] or per-
ceptual loss [17]. While adding the gradient difference loss
creates more details in the reconstruction, combining per-
ceptual loss with l2,1 gives the best results with high level
of details and realism. For the rest of the paper we will refer

1130

Input

ˆI(˜S, ˜A)

ˆI(S, ˜A)

ˆI(˜S, A)

ˆI(S, A)

Input

Linear

Nonlinear

+GL

+GL & Proxy

˜S

S

˜A

A

Figure 5: Image reconstruction with our 3DMM model using the
proxy and the true shape and albedo. Our shape and albedo can
faithfully recover details of the face. Note: for the shape, we show
the shading in UV space – a better visualization than the raw SUV.

Without Lres-sym

With Lres-sym

Figure 6: Affect of soft symmetry loss on our shape model.

to the model trained using this combination.
Understanding image pairing.
Fig. 5 shows ﬁtting re-
sults of our model on a 2D face image. By using the proxy
or the ﬁnal components (shape or albedo) we can render
four different reconstructed images with different quality
and characteristics. The image generated by two proxies
˜S, ˜A is quite blurry but is still be able to capture major vari-
ations in the input face. By pairing S and the proxy ˜A, S
is enforced to capture high level of details to bring the im-
age closer to the input. Similarly, A is also encouraged to
capture more details by pairing with the proxy ˜S. The ﬁnal
image ˆI(S, A) inherently achieves high level of details and
realism even without direct optimization.
Residual Soft Symmetry Loss. We study effects of the
residual soft symmetry loss on recovering details on oc-
cluded face region. As shown in Fig. 6, without Lres-sym, the
learned model can result in an unnatural shape, in which
one side of the face is over-smooth, on occluded regions,
while the other side still has high level of details. Our model
learned with Lres-sym can consistently create details across
the face, even in occluded areas.

4.2. Representation Power

We compare the representation power of the proposed
nonlinear 3DMM with Basel Face Model [30], the most
commonly used linear 3DMM. We also make comparisons
with the recently proposed nonlinear 3DMM [44].
Texture. We evaluate our model’s power to represent

Figure 7: Qualitative comparisons on texture representation
power. Our model can better reconstruct in-the-wild facial texture.

Table 1: Texture representation power quantitative comparison
(Average reconstruction error on non-occluded face portion.)

Method

Reconstruction error (l2,1)

Linear [52]
Nonlinear [45]
Nonlinear + GL (Ours)
Nonlinear + GL + Proxy (Ours)

0.1287

0.0427

0.0386

0.0363

in-the-wild facial texture on AFLW2000-3D dataset [52].
Given a face image, also with the groundtruth geometry
and camera projection, we can jointly estimate an albedo
parameter fA and a lighting parameter L whose decoded
texture can reconstruct the original image. To accomplish
this, we use SGD on fA and L with the initial parameters
estimated by our encoder E. For the linear model, Zhu et
al. [52] ﬁtting results of Basel albedo using Phong illumi-
nation model [31] is used. As in Fig. 7, nonlinear model sig-
niﬁcantly outperforms the Basel Face model. Despite, being
close to the original image, Tran and Liu [45] model recon-
struction results are still blurry. Using global-local-based
network architecture (“+GL”) with the same loss functions
helps to bring the image closer to the input. However,
these models are still constrained by regularizations on the
albedo. By learning using proxy technique (“+Proxy”), our
model can learn more realistic albedo with more high fre-
quency details on the face. This conclusion is further sup-
ported with quantitative comparison in Tab. 1. We report the
averaged l2,1 reconstruction error over the face portion of
each image. Our model achieves the lowest averaged recon-
struction error among four models, 0.0363, which is a 15%
error reduction of the recent nonlinear 3DMM work [45].
Shape.
Similarly, we also compare models’ power to
represent real-world 3D scans. Using ten 3D face meshes

1131

Origin

Linear [30] Nonlinear [45]

Our

Input

Overlay

Albedo

Shape

Shading

NME

0.0241

0.0146

0.0139

Figure 8: Shape representation power comparison. Given a 3D
shape, we optimize the feature fS to approximate the original one.

Our 

Tran and Liu 

Linear 3DMM 

Figure 9: The distance between the input images and their recon-
struction from three models. For better visualization, images are
sorted based on their distance to our model’s reconstructions.

provided by [30], which share the same triangle topology
with us, we can optimize the shape parameter to gener-
ate, through the decoder, shapes matching the groundtruth
scans. The optimization objective is deﬁned based on ver-
tex distances (Euclidean) as well as surface normal direc-
tion (cosine distance), which empirically improves recon-
structed meshes’ ﬁdelity compared to optimizing the former
only. Fig. 8 shows the visual comparisons between different
reconstructed meshes. Our reconstructions closely match
the face shapes details. To make quantitative comparisons,
we use NME — averaged per-vertex Euclidean distances
between the recovered and groundtruth meshes, normalized
by inter-ocular distances. The proposed model has a signif-
icantly smaller reconstruction error than the linear model,
and is also smaller than the nonlinear model by Tran and
Liu [45] (0.0139 vs. 0.0146 [45], and 0.0241 [30]).

4.3. Identity Preserving

We explore the effect of our proposed 3DMM on pre-
serving identity when reconstructing face images. Using
DR-GAN [48], a pretrained face recognition network, we
can compute the cosine distance between the input and its
reconstruction from different models. Fig. 9 shows the plot
of these score distributions. At each horizontal mark, there

Figure 10: Model ﬁtting on faces with diverse skin color, pose,
expression, lighting. Our model faithfully recovers these cues.

Input

Our

Tewari17

Figure 11: 3D reconstruction comparison to Tewari et al. [40].

are exactly three points presenting distances between an im-
age with its reconstructions from three models. Images are
sorted based on the distance to our reconstruction. For the
majority of the cases (77.2%), our reconstruction has the
smallest difference to the input in the identity space.

4.4. 3D Reconstruction

Using our model FS, FA, together with the model ﬁtting
CNN E, we can decompose a 2D photograph into different
components: 3D shape, albedo and lighting (Fig. 10). Here
we compare our 3D reconstruction results with different
lines of works: linear 3DMM ﬁtting [40], nonlinear 3DMM
ﬁtting [39, 45] and approaches beyond 3DMM [16, 35].
Comparisons are made on CelebA dataset [25].

For linear 3DMM model, the representative work, MoFA
by Tewari et al. [38, 40], learns to regress 3DMM parame-
ters in an unsupervised fashion. Even being trained on in-
the-wild images, it is still limited to the linear subspace,
with limited power to recovering in-the-wild texture. This
results in the surface shrinkage when dealing with challeng-
ing texture, i.e., facial hair as discussed in [39, 44, 45]. Be-
sides, even with regular skin texture their reconstruction is

1132

Input

Our

Tewari18

Tran18a

Figure 12: 3D reconstruction comparisons to nonlinear 3DMM approaches by Tewari et al. [39] or Tran and Liu [45]. Our model can
reconstruct face images with higher level of details. Please zoom-in for more details. Best view electronically.

Input

Our

Tran18b

Sela17

Figure 13: 3D reconstruction comparisons to Sela et al. [35] or
Tran et al. [43], which go beyond latent space representations.

The most related work to our proposed model

still blurry and has less details compared to ours (Fig. 11).
is
Tewari et al. [39], Tran and Liu [45], in which 3DMM bases
are embedded in neural networks. With more representation
power, these models can recover details that the traditional
3DMM usually can’t, i.e. make-up, facial hair. However,
the model learning process is attached with strong regular-
ization, which limits their ability to recover high-frequency
details of the face. Our propose model enhances the learn-
ing process in both learning objective and network architec-
ture to allow higher-ﬁdelity reconstructions (Fig. 12).

To improve 3D reconstruction quality, many approaches
also try to move beyond the 3DMM such as Richardson et
al. [34], Sela et al. [35] or Tran et al. [43]. The current
state-of-the-art 3D monocular face reconstruction method
by Sela et al. [35] using a ﬁne detail reconstruction step
to help reconstructing high ﬁdelity meshes. However, their
ﬁrst depth map regression step is trained on synthetic data
generated by the linear 3DMM. Besides domain gap be-
tween synthetic and real, it faces a more serious problem
of lacking facial hair in the low-dimension texture. Hence,
this network’s output tends to ignore these unexplainable
regions, which leads to failure in later steps. Our net-
work is more robust in handling these in-the-wild varia-
tions (Fig. 13). The approach of Tran et al. [43] shares a
similar objective with us to be both robust and maintain high
level of details in 3D reconstruction. However, they use an

Figure 14: Adding stickers to faces. The sticker is naturally
added into faces following the surface normal or lighting.

over-constrained foundation, which loses personal charac-
teristics of the each face mesh. As a result, the 3D shapes
look similar across different subjects (Fig. 13).

4.5. Facial Editting

With more precise 3D face mesh reconstruction, the
quality of successive tasks is also improved. Here, we show
an application of our model on face editing: adding stick-
ers or tattoos onto faces. Using the estimated shape as well
as the projection matrix, we can unwrap the facial texture
into the UV space. Thanks to the lighting decomposition,
we can also remove the shading from the texture to get the
detailed albedo. From here we can directly edit the albedo
by adding sticker, tattoo or make-up. Finally, the edited
images can be rendered using the modiﬁed albedo together
with other original elements. Fig. 14 shows our editing re-
sults by adding stickers into different people’s face.

5. Conclusions

In realization that the strong regularization and global-
based modeling are the roadblocks to achieve high-ﬁdelity
3DMM model, this work presents a novel approach to im-
prove the nonlinear 3DMM modeling in both learning ob-
jective and network architecture. Hopefully, with insights
and ﬁndings discussed in the paper, this work can be a step
toward unlocking the possibility to build a model which
can capture mid and high-level details in the face. Through
which, high-ﬁdelity 3D face reconstruction can be achieved
solely by doing model ﬁtting.

1133

References

[1] Brian Amberg, Reinhard Knothe, and Thomas Vetter. Ex-
pression invariant 3D face recognition with a morphable
model. In FG, 2008. 1

[2] Brian Amberg, Sami Romdhani, and Thomas Vetter. Opti-
mal step nonrigid ICP algorithms for surface registration. In
CVPR, 2007. 2

[3] Timur Bagautdinov, Chenglei Wu, Jason Saragih, Pascal
Fua, and Yaser Sheikh. Modeling facial geometry using com-
positional VAEs. In CVPR, 2018. 2

[4] Michael J Black and Yaser Yacoob. Tracking and recogniz-
ing rigid and non-rigid facial motions using local parametric
models of image motion. In ICCV, 1995. 2

[5] Volker Blanz, Curzio Basso, Tomaso Poggio, and Thomas
Vetter. Reanimating faces in images and video. In Computer
graphics forum. Wiley Online Library, 2003. 1

[6] Volker Blanz and Thomas Vetter. A morphable model for
In Proceedings of the 26th an-
the synthesis of 3D faces.
nual conference on Computer graphics and interactive tech-
niques, 1999. 1, 2, 3

[7] James Booth,

Epameinondas Antonakos,

Ploumpis, George Trigeorgis, Yannis Panagakis,
Stefanos Zafeiriou.
wild”. In CVPR, 2017. 2

Stylianos
and
3D face morphable models “In-the-

[8] Soﬁen Bouaziz, Yangang Wang, and Mark Pauly. Online
modeling for realtime facial animation. ACM TOG, 2013. 1

[9] Alan Brunton, Timo Bolkart, and Stefanie Wuhrer. Multilin-
ear wavelets: A statistical shape space for human faces. In
ECCV, 2014. 2

[10] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic
expression regression for real-time facial tracking and ani-
mation. ACM TOG, 2014. 1

[11] Douglas DeCarlo and Dimitris Metaxas. The integration of
optical ﬂow and deformable models with applications to hu-
man face shape and motion estimation. In CVPR, 1996. 2

[12] Pablo Garrido, Levi Valgaerts, Hamid Sarmadi, Ingmar
Steiner, Kiran Varanasi, Patrick Perez, and Christian
Theobalt. Vdub: Modifying face video of actors for plau-
sible visual alignment to a dubbed audio track. In Computer
Graphics Forum. Wiley Online Library, 2015. 1

[13] Pablo Garrido, Levi Valgaerts, Chenglei Wu, and Christian
Theobalt. Reconstructing detailed dynamic face geometry
from monocular video. ACM TOG, 2013. 1

[14] Pablo Garrido, Michael Zollh¨ofer, Dan Casas, Levi Val-
gaerts, Kiran Varanasi, Patrick P´erez,
and Christian
Theobalt. Reconstruction of personalized 3D face rigs from
monocular video. ACM TOG, 2016. 1

[15] Rui Huang, Shu Zhang, Tianyu Li, Ran He, et al. Beyond
face rotation: Global and local perception gan for photoreal-
istic and identity preserving frontal view synthesis. In ICCV,
2017. 5

[16] Aaron S Jackson, Adrian Bulat, Vasileios Argyriou, and
Georgios Tzimiropoulos. Large pose 3D face reconstruction
from a single image via direct volumetric CNN regression.
In ICCV, 2017. 7

[17] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016. 5

[18] Pushkar Joshi, Wen C Tien, Mathieu Desbrun, and Fr’ed’eric
Pighin. Learning controls for blend shape based realistic fa-
cial animation. In ACM Siggraph 2006 Courses, 2006. 2

[19] Amin Jourabloo and Xiaoming Liu. Pose-invariant 3D face

alignment. In ICCV, 2015. 2

[20] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. In ICLR, 2018. 5

[21] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR, 2016. 2

[22] Edwin H Land and John J McCann. Lightness and retinex

theory. Josa, 1971. 2

[23] Manfred Lau, Jinxiang Chai, Ying-Qing Xu, and Heung-
Yeung Shum. Face poser: Interactive modeling of 3D facial
expressions using facial priors. ACM TOG, 2009. 2

[24] Chen Li, Kun Zhou, and Stephen Lin. Simulating makeup
through physics-based manipulation of intrinsic image lay-
ers. In CVPR, 2015. 1

[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
In ICCV, 2015.

Deep learning face attributes in the wild.
7

[26] Iacopo Masi, Anh Tun Trn, Tal Hassner, Jatuporn Toy Lek-
sut, and G´erard Medioni. Do we really need to collect mil-
lions of faces for effective face recognition? In ECCV, 2016.
1

[27] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error.
arXiv:1511.05440, 2015. 5

[28] Umar Mohammed, Simon JD Prince, and Jan Kautz. Visio-
lization: generating novel facial images. ACM TOG, 2009.
5

[29] Chi Nhan Duong, Khoa Luu, Kha Gia Quach, and Tien D
Bui. Beyond principal components: Deep Boltzmann Ma-
chines for face modeling. In CVPR, 2015. 2

[30] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3D face model for pose
and illumination invariant face recognition. In AVSS, 2009.
2, 5, 6, 7

[31] Bui Tuong Phong. Illumination for computer generated pic-

tures. Communications of the ACM, 1975. 6

[32] Ravi Ramamoorthi and Pat Hanrahan. An efﬁcient represen-
tation for irradiance environment maps.
In Proceedings of
the 28th annual conference on Computer graphics and inter-
active techniques, 2001. 3

[33] Elad Richardson, Matan Sela, and Ron Kimmel. 3D face re-
construction by learning from synthetic data. In 3DV, 2016.
1

[34] Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel.
Learning detailed face reconstruction from a single image.
In CVPR, 2017. 1, 4, 8

[35] Matan Sela, Elad Richardson, and Ron Kimmel. Unre-
stricted facial geometry reconstruction using image-to-image
translation. In ICCV, 2017. 7, 8

1134

[36] Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli,
Eli Shechtman, and Dimitris Samaras. Neural face editing
with intrinsic image disentangling. In CVPR, 2017. 2

[37] J Rafael Tena, Fernando De la Torre, and Iain Matthews. In-
teractive region-based linear 3D face models. In ACM TOG,
2011. 2

[38] Ayush Tewari, Michael Zollhoefer, Florian Bernard, Pablo
Garrido, Hyeongwoo Kim, Patrick Perez, and Christian
Theobalt. High-ﬁdelity monocular face reconstruction based
on an unsupervised model-based face autoencoder. TPAMI,
2018. 7

[39] Ayush Tewari, Michael Zollh¨ofer, Pablo Garrido, Florian
Bernard, Hyeongwoo Kim, Patrick P´erez, and Christian
Theobalt. Self-supervised multi-level face model learning
for monocular reconstruction at over 250 Hz.
In CVPR,
2018. 1, 2, 3, 4, 7, 8

[40] Ayush Tewari, Michael Zollh¨ofer, Hyeongwoo Kim, Pablo
Garrido, Florian Bernard, Patrick P´erez, and Christian
Theobalt. MoFA: Model-based deep convolutional face au-
toencoder for unsupervised monocular reconstruction.
In
ICCV, 2017. 7

[41] Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Face2face: Real-time
face capture and reenactment of RGB videos.
In CVPR,
2016. 1, 3

[42] Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. FaceVR: Real-time fa-
cial reenactment and eye gaze control in virtual reality. ACM
TOG, 2018. 3

[43] Anh Tuan Tran, Tal Hassner, Iacopo Masi, Eran Paz, Yuval
Nirkin, and Gerard Medioni. Extreme 3D face reconstruc-
tion: Looking past occlusions. In CVPR, 2018. 5, 8

[44] Luan Tran and Xiaoming Liu. Nonlinear 3D morphable

model. In CVPR, 2018. 2, 3, 6, 7

[45] Luan Tran and Xiaoming Liu. On learning 3D face mor-
phable model from in-the-wild images. arXiv:1808.09560,
2018. 3, 4, 5, 6, 7, 8

[46] Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. Miss-
ing modalities imputation via cascaded residual autoencoder.
In CVPR, 2017. 2

[47] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre-
sentation learning GAN for pose-invariant face recognition.
In CVPR, 2017. 5

[48] Luan Tran, Xi Yin, and Xiaoming Liu. Representation learn-

ing by rotating your faces. TPAMI, 2018. 5, 7

[49] Daniel Vlasic, Matthew Brand, Hanspeter Pﬁster, and Jovan
In ACM

Popovi´c. Face transfer with multilinear models.
TOG, 2005. 2

[50] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Man-
mohan Chandraker. Towards large-pose face frontalization
in the wild. In ICCV, 2017. 1

[51] Eduard Zell, JP Lewis, Junyong Noh, Mario Botsch, et al.
Facial retargeting with automatic range of motion alignment.
ACM TOG, 2017. 1

[52] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and
Stan Z Li. Face alignment across large poses: A 3D solu-
tion. In CVPR, 2016. 5, 6

1135

