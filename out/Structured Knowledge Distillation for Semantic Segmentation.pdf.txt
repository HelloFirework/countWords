Structured Knowledge Distillation for Semantic Segmentation

Yifan Liu1∗ Ke Chen2

Chris Liu2

Zengchang Qin3

4

,

Zhenbo Luo5

Jingdong Wang2†

1The University of Adelaide

2Microsoft Research Asia

3Beihang University

4Keep Labs, Keep Inc.

5Samsung Research China

Abstract

In this paper, we investigate the knowledge distillation
strategy for training small semantic segmentation networks
by making use of large networks. We start from the straight-
forward scheme, pixel-wise distillation, which applies the
distillation scheme adopted for image classiﬁcation and
performs knowledge distillation for each pixel separately.
We further propose to distill the structured knowledge from
large networks to small networks, which is motivated by that
semantic segmentation is a structured prediction problem.
We study two structured distillation schemes: (i) pair-wise
distillation that distills the pairwise similarities, and (ii)
holistic distillation that uses GAN to distill holistic knowl-
edge. The effectiveness of our knowledge distillation ap-
proaches is demonstrated by extensive experiments on three
scene parsing datasets: Cityscapes, Camvid and ADE20K.

1. Introduction

Semantic segmentation is the problem of predicting the
category label of each pixel in an input image. It is a fun-
damental task in computer vision and has many real-world
applications, such as autonomous driving, video surveil-
lance, virtual reality, and so on. Deep neural networks
have been the dominant solutions for semantic segmenta-
tion since the invention of fully-convolutional neural net-
works (FCNs) [38]. The subsequent approaches, e.g.,
DeepLab [5, 6, 7, 48], PSPNet [56], OCNet [50], Re-
ﬁneNet [23] and DenseASPP [46] have achieved signiﬁcant
improvement in segmentation accuracy, often with cumber-
some models and expensive computation.

Recently, neural networks with small model size, light
computation cost and high segmentation accuracy, have at-
tracted much attention because of the need of applications
on mobile devices. Most current efforts have been devoted
to designing lightweight networks specially for segmenta-
tion or borrowing the design from classiﬁcation networks,

∗Part of this work was done when Y. Liu was an intern at Microsoft

Research, Beijing, China.
†Corresponding author.

e.g., ENet [31], ESPNet [31], ERFNet [34] and ICNet [55].
The interest of this paper lies in compact segmentation net-
works, with a focus on training compact networks with the
help of cumbersome networks for improving the segmenta-
tion accuracy.

We study the knowledge distillation strategy, which has
been veriﬁed valid in classiﬁcation tasks [15, 35], for
training compact semantic segmentation networks. As a
straightforward scheme, we simply view the segmentation
problem as many separate pixel classiﬁcation problems, and
then directly apply the knowledge distillation scheme to
pixel-level. This simple scheme, we call pixel-wise distil-
lation, transfers the class probability of the corresponding
pixel produced from the cumbersome network (teacher) to
the compact network (student).

Considering that semantic segmentation is a structured
prediction problem, we present structured knowledge dis-
tillation and transfer the structure information with two
schemes, pair-wise distillation and holistic distillation. The
pair-wise distillation scheme is motivated by the widely-
studied pair-wise Markov random ﬁeld framework [22] for
enforcing spatial labeling contiguity, and the goal is to align
the pair-wise similarities among pixels computed from the
compact network and the cumbersome network.

The holistic distillation scheme aims to align higher-
order consistencies, which are not characterized in the
pixel-wise and pair-wise distillation, between segmentation
maps produced from the compact segmentation network
and the cumbersome segmentation network. We adopt the
adversarial training scheme, encouraging the holistic em-
beddings of the segmentation maps produced from the com-
pact segmentation network not to be distinguished from the
output of the cumbersome segmentation network.

To this end, we optimize an objective function that com-
bines a conventional multi-class cross-entropy loss with the
distillation terms. The main contributions of this paper can
be summarized as follows.

• We study the knowledge distillation strategy for train-
ing accurate compact semantic segmentation net-
works.

2604

)

 

%
U
o
I
m

(
 
y
c
a
r
u
c
c
A

85

80

75

70

65

60

55

50
100

(cid:90)(cid:18)(cid:3)(cid:82)ur distillation 
(cid:3)(cid:90)/o distillation 

PSPNet
OCNet

RefineNet

ResNet18(1.0)

FCN

MobilenNetV2Plus

ERFNet

ESPNet

ENet

ESPNet-C
101

ResNet18(0.5)

85

80

75

70

65

60

55

(cid:90)(cid:18)(cid:3)(cid:82)ur distillation 
(cid:3)(cid:90)/o distillation 

PSPNet
OCNet

RefineNet

MobilenNetV2Plus

ERFNet

ResNet18 (1.0)

ESPNet

ENet

ESPNet-C

ResNet18 (0.5)

FCN

SegNet

)

 

%
U
o
I
m

(
 

y
c
a
r
u
c
c
A

102

103

50
10-1

  FLOPs (B)

100

101

102

103

#Parameters (M)

Figure 1: The complexity, parameters and the mIoU for different networks on the Cityscapes test set. The FLOPs is calculated with the resolution of
512 × 1024. The red triangles are the results of our distillation method while others are without distillation. Blue circles are collected from FCN* [38],
ReﬁneNet [23], SegNet [3], ENet [31], PSPNet [56], ERFNet [34], ESPNet [28], MobileNetV2Plus [25], and OCNet [50]. We can see that with our proposed
distillation method, we can achieve a higher mIoU, but no extra FLOPs and #Parameters.

• We present

two structured knowledge distillation
schemes, pair-wise distillation and holistic distilla-
tion, enforcing pair-wise and high-order consistency
between the outputs of the compact and cumbersome
segmentation networks.

• We demonstrate the effectiveness of our approach
by improving recently-developed state-of-the-art com-
pact segmentation networks, ESPNet, MobileNetV2-
Plus and ResNet18 on three benchmark datasets:
Cityscapes [10], CamVid [4] and ADE20K [58], which
is illustrated in Figure 1.

2. Related Work

Semantic segmentation. Deep convolutional neural net-
works have been the dominant solution to semantic seg-
mentation since the pioneering works, fully-convolutional
network [38], DeConvNet [30], U-Net [36]. Various
schemes [47] have been developed for improving the net-
work capability and accordingly the segmentation perfor-
mance. For example, stronger backbone networks, e.g.,
GoogleNets [39], ResNets [14], and DenseNets [17], have
shown better segmentation performance. Improving the res-
olution through dilated convolutions [5, 6, 7, 48] or multi-
path reﬁne networks [23] leads to signiﬁcant performance
gain. Exploiting multi-scale context, e.g., dilated convolu-
tions [48], pyramid pooling modules in PSPNet [56], atrous
spatial pyramid pooling in DeepLab [6], object context [50],
also beneﬁts the segmentation. Lin et al. [24] combine deep
models with structured output learning for semantic seg-
mentation.

In addition to cumbersome networks for highly accu-
rate segmentation, highly efﬁcient segmentation networks
have been attracting increasingly more interests due to the

need of real applications, e.g., mobile applications. Most
works focus on lightweight network design by acceler-
ating the convolution operations with factorization tech-
niques. ENet [31], inspired by [40], integrates several ac-
celeration factors, including multi-branch modules, early
feature map resolution down-sampling, small decoder size,
ﬁlter tensor factorization, and so on. SQ [41] adopts the
SqueezeNet [18] ﬁre modules and parallel dilated convo-
lution layers for efﬁcient segmentation. ESPNet [28] pro-
poses an efﬁcient spatial pyramid, which is based on ﬁlter
factorization techniques: point-wise convolutions and spa-
tial pyramid of dilated convolutions, to replace the standard
convolution. The efﬁcient classiﬁcation networks, e.g., Mo-
bileNet [16], ShufﬂeNet [54], and IGCNet [53], are also ap-
plied to accelerate segmentation. In addition, ICNet (image
cascade network) [55] exploits the efﬁciency of processing
low-resolution images and high inference quality of high-
resolution ones, achieving a trade-off between efﬁciency
and accuracy.

Knowledge distillation. Knowledge distillation [15] is a
way of transferring knowledge from the cumbersome model
to a compact model to improve the performance of com-
pact networks. It has been applied to image classiﬁcation
by using the class probabilities produced from the cum-
bersome model as soft targets for training the compact
model [2, 15, 42] or transferring the intermediate feature
maps [35, 51].

There are also other applications, including object de-
tection [21], pedestrian re-identiﬁcation [9] and so on.
The very recent and independently-developed application
for semantic segmentation [45] is related to our approach.
It mainly distills the class probabilities for each pixel
separately (like our pixel-wise distillation) and center-
surrounding differences of labels for each local patch

2605

(termed as a local relation in [45]). In contrast, we focus on
distilling structured knowledge: pairwise distillation, which
transfers the relation among all pairs of pixels other than the
relation in a local patch [45], and holistic distillation, which
transfers the holistic knowledge that captures high-order in-
formation.

Adversarial learning. Generative adversarial networks
(GANs) have been widely studied in text generation [43, 49]
and image synthesis [12, 20]. The conditional version [29]
is successfully applied to image-to-image translation, in-
cluding style transfer [19], image inpainting [32], image
coloring [26] and text-to-image [33].

The idea of adversarial learning is also adopted in pose
estimation [8], encouraging the human pose estimation re-
sult not to be distinguished from the ground-truth; and se-
mantic segmentation [27], encouraging the estimated seg-
mentation map not to be distinguished from the ground-
truth map. One challenge in [27] is the mismatch between
the generator’s continuous output and the discrete true la-
bels, making the discriminator in GAN be of very limited
success. Different from [27], in our approach, the employed
GAN does not have this problem as the ground truth for
the discriminator is the teacher network’s logits, which are
real valued. We use adversarial learning to encourage the
alignment between the segmentation maps produced from
the cumbersome network and the compact network.

3. Approach

Image semantic segmentation is a task of predicting a
category label to each pixel in the image from C categories.
A segmentation network takes an RGB image I of size W ×
H × 3 as the input, then it computes a feature map F of size
W ′ × H ′ × N , where N is the number of channels. Finally,
a classiﬁer is applied to compute the segmentation map Q
of size W ′ × H ′ × C from F, which is upsampled to the
spatial size W × H of the input image as the segmentation
results.

3.1. Structured Knowledge Distillation

We apply the knowledge distillation [15] strategy to
transfer the knowledge of the cumbersome segmentation
network T to a compact segmentation network S for better
training the compact segmentation network. In addition to a
straightforward scheme, pixel-wise distillation, we present
the two structured knowledge distillation schemes, pair-
wise distillation and holistic distillation, to transfer struc-
tured knowledge from the cumbersome network to the com-
pact network. The pipeline is illustrated in Figure 2.

Pixel-wise distillation. We view the segmentation prob-
lem as a collection of separate pixel labeling problems, and
directly use knowledge distillation to align the class prob-
ability of each pixel produced from the compact network.

(c)

(b)

 

p
a
m
e
r
o
c
S

(a)

p
a
m
e
r
u

 

t

a
e
F

n
o
i
t
i
d
n
o
C

Wasserstein

loss

Real embedding

Fake embedding

Holistic loss

Discriminator

net

Pixel-wise 

loss

Pixel 
labeling

Pixel 
labeling

Pair-wise

loss

Similarity map

Teacher 

net

Student 

net

Input image

Cross entropy

loss

Distillation loss

Segmentation loss

Discrimination loss

Figure 2: Our distillation framework. (a) Pair-wise distillation. (b) Pixel-
wise distillation. (c) Holistic distillation. In the training process, we ﬁx
the cumbersome network as our teacher net, and only the student net and
the discriminator net will be optimized. The student net with a compact
architecture will be trained with three distillation terms and a cross-entropy
term.

We adopt an obvious way [15]: use the class probabilities
produced from the cumbersome model as soft targets for
training the compact network.

The loss function is given as follows,

ℓpi(S) =

1

W ′ × H ′ (cid:2)

i∈R

KL(qs

i (cid:2)qt

i),

(1)

where qs
i represent the class probabilities of the ith pixel
produced from the compact network S, qt
i represent the
class probabilities of the ith pixel produced from the cum-
bersome network T, KL(·) is the Kullback-Leibler diver-
gence between two probabilities, and R = {1, 2, . . . , W ′ ×
H ′} denotes all the pixels.
Pair-wise distillation.
Inspired by the pair-wise Markov
random ﬁeld framework that is widely adopted for improv-
ing spatial labeling contiguity, we propose to transfer the

2606

pair-wise relations, specially pair-wise similarities in our
approach, among pixels.

Let at

ij denote the similarity between the ith pixel and
the jth pixel produced from the cumbersome network T and
as
ij denote the similarity between the ith pixel and the jth
pixel produced from the compact network S. We adopt the
squared difference to formulate the pair-wise similarity dis-
tillation loss,

ℓpa(S) =

1

(W ′ × H ′)2 (cid:2)

i∈R

(as

ij − at

ij)2.

(2)

(cid:2)

j∈R

In our implementation, the similarity between two pixels is
simply computed from the features fi and fj as

aij = f ⊤

i fj/((cid:2)fi(cid:2)2(cid:2)fj(cid:2)2),

which empirically works well.

We adopt conditional generative adversarial

Holistic distillation. We align the high-order relations be-
tween the segmentation maps produced from the cumber-
some and compact networks. The holistic embeddings of
the segmentation maps are computed as the representations.
learn-
ing [29] for formulating the holistic distillation problem.
The compact net is regarded as a generator conditioned on
the input RGB image I, and the predicted segmentation map
Qs is regarded as a fake sample. We expect that Qs is as
similar to Qt, which is the segmentation map predicted by
the teacher and is regarded as the real sample, as possible.
Wasserstein distance [13] is employed to evaluate the dif-
ference between the real distribution and fake distribution,
which is written as the following,

ℓho(S, D) = E

Qs∼ps(Qs)[D(Qs|I)]

− E

Qt∼pt(Qt)[D(Qt|I)],

(3)

where E[·] is the expectation operator, and D(·) is an
embedding network, acting as the discriminator in GAN,
which projects Q and I together into a holistic embedding
score. The Lipschitz requirement is satisﬁed by the gradient
penalty.

The segmentation map and the conditional RGB image
are concatenated as the input of the embedding network
D. D is a fully convolutional neural network with ﬁve
convolutions. Two self-attention modules are inserted be-
tween the ﬁnal three layers to capture the structure infor-
mation [52, 57]. Such a discriminator is able to produce a
holistic embedding representing how well the input image
and the segmentation map match.

3.2. Optimization

The whole objective function consists of a conventional
multi-class cross-entropy loss ℓmc(S) with pixel-wise and

structured distillation terms 1

ℓ(S, D) = ℓmc(S) + λ1(ℓpi(S) + ℓpa(S))

− λ2ℓho(S, D),

(4)

where λ1 and λ2 are set as 10 and 0.1, making these loss
value ranges comparable. We minimize the objective func-
tion with respect to the parameters of the compact segmen-
tation network S, while maximize it with respect to the pa-
rameters of the discriminator D, which is implemented by
iterating the following two steps:

• Train the discriminator D. Training the discriminator
is equivalent to minimizing ℓho(S, D). D aims to give
a high embedding score for the real samples from the
teacher net and a low embedding score for the fake
samples from the student net.

• Train the compact segmentation network S. Given
the discriminator network, the goal is to minimize the
multi-class cross-entropy loss and the distillation loss
relevant to the compact segmentation network:

ℓmc(S) + λ1(ℓpi(S) + ℓpa(S)) − λ2ℓs

ho(S),

where

ℓs
ho(S) = E

Qs∼ps(Qs)[D(Qs|I)]

is a part of ℓho(S, D) given in Equation 3, and we ex-
pect S to achieve a higher score under the evaluation
of D.

4. Implementation Details

Network structures. We adopt state-of-the-art segmenta-
tion architecture PSPNet [56] with a ResNet101 [14] as the
cumbersome network (teacher) T.

We study recent public compact networks, and employ
several different architectures to verify the effectiveness of
the distillation framework. We ﬁrst consider ResNet18 as
a basic student network and conduct ablation studies on it.
Then, we employ an open source MobileNetV2Plus [25],
which is based on a pretrained MobileNetV2 [37] model on
the ImageNet dataset. We also test the structure of ESPNet-
C [28] and ESPNet [28] that are very compact and have low
complexity.

Training setup. Most segmentation networks in this paper
are trained by mini-batch stochastic gradient descent (SGD)
with the momentum (0.9) and the weight decay (0.0005)
for 40000 iterations. The learning rate is initialized as 0.01
max−iter )0.9. We random cut
and is multiplied by (1 −

iter

1The objective function is the summation of the losses over the mini-
batch of training samples. For description clarity, we ignore the summation
operation.

2607

the the images into 512 × 512 as the training input. Nor-
mal data augmentation methods are applied during training,
such as random scaling (from 0.5 to 2.1) and random ﬂip-
ping. Other than this, we follow the settings in the corre-
sponding publications [28] to reproduce the results of ES-
PNet and ESPNet-C, and train the compact networks under
our distillation framework.

5. Experiments

5.1. Datasets

Cityscapes. The Cityscapes dataset [10] is collected for ur-
ban scene understanding and contains 30 classes with only
19 classes used for evaluation. The dataset contains 5, 000
high quality pixel-level ﬁnely annotated images and 20, 000
coarsely annotated images. The ﬁnely annotated 5, 000 im-
ages are divided into 2, 975/ 500/ 1, 525 images for train-
ing, validation and testing. We only use the ﬁnely annotated
dataset in our experiments.

CamVid. The CamVid dataset [4] is an automotive dataset.
It contains 367 training and 233 testing images. We evaluate
the performance over 11 different classes such as building,
tree, sky, car, road, etc. and ignore the 12th class that con-
tains unlabeled data.

ADE20K. The ADE20K dataset [58] is used in ImageNet
scene parsing challenge 2016. It contains 150 classes and
under diverse scenes. The dataset is divided into 20K/2K/
3K images for training, validation and testing.

5.2. Evaluation Metrics

We use the following metrics to evaluate the segmenta-

tion accuracy, as well as the model size and the efﬁciency.

The Intersection over Union (IoU) score is calculated
as the ratio of interval and union between the ground truth
mask and the predicted segmentation mask for each class.
We use the mean IoU of all classes (mIoU) to study the dis-
tillation effectiveness. We also report the class IoU to study
the effect of distillation on different classes. Pixel accuracy
is the ratio of the pixels with the correct semantic labels to
the overall pixels.

The model size is represented by the number of network
parameters. and the Complexity is evaluated by the sum of
ﬂoating point operations (FLOPs) in one forward on a ﬁxed
input size.

5.3. Ablation Study

The effectiveness of distillations. We look into the effect
of enabling and disabling different components of our dis-
tillation system. The experiments are conduct on ResNet18
with its variant ResNet18 (0.5) representing a width-halved
version of ResNet18 on the Cityscapes dataset. In Table 1,
the results of different settings for the student net are the
average results from three runs.

Table 1: The effect of different components of the loss in the proposed
method. PI = pixel-wise distillation, PA = pair-wise distillation, HO =
holistic distillation, ImN = initial from the pretrain weight on the Ima-
geNet.

Method

Teacher

ResNet18 (0

.

5)

+ PI

+ PI + PA

+ PI + PA + HO

ResNet18 (1

.

0)

+ PI

+ PI + PA

Validation mIoU (%)

Training mIoU (%)

78

.

56

55

37 ± 0

.

.

25

57

07 ± 0

.

.

69

61

03 ± 0

.

.

49

86

.

09

60

67 ± 0

.

.

37

62

33 ± 0

.

.

66

65

73 ± 0

.

.

38

61

63 ± 0

.

.

99

66

13 ± 0

.

.

70

57

50 ± 0

.

.

49

58

63 ± 0

.

.

31

62

48 ± 0

.

.

23

62

98 ± 0

.

.

45

64

32 ± 0

.

.

32

68

77 ± 0

.

.

37

+ PI + PA + HO

63

24 ± 0

.

.

74

69

93 ± 0

.

.

86

+ ImN

+ PI + ImN

+ PI + PA + ImN

69

10 ± 0

.

.

21

70

51 ± 0

.

.

37

71

37 ± 0

.

.

12

74

12 ± 0

.

.

19

75

10 ± 0

.

.

37

76

42 ± 0

.

.

20

+ PI + PA + HO + ImN

72

67 ± 0

.

.

57

78

03 ± 0

.

.

51

From Table 1, we can see that distillation can improve
the performance of the student network, and distilling the
structure information helps the student learn better. With
the three distillation terms, the improvements for ResNet18
(0.5), ResNet18 (1.0) and ResNet18 (1.0) with weights pre-
trained from the ImageNet dataset are 6.26%, 5.74% and
2.9%, respectively, which indicates that the effect of dis-
tillation is more pronounced for the smaller student net-
work and networks without initialization with the weight
pre-trained from the ImageNet. Such an initialization is
also a way to transfer the knowledge from other source
(ImageNet). The best mIoU of the holistic distillation for
ResNet18 (0.5) reaches 62.7% on the validation set.

On the other hand, one can see that each distillation
scheme lead to higher mIoU score. This implies that the
three distillation schemes make complementary contribu-
tions for better training the compact network.

Furthermore, we illustrate that GAN is able to distill the
holistic knowledge. For each image, we feed three segmen-
tation maps, output by the teacher net, the student net w/o
holistic distillation, and the student net w/ holistic distil-
lation, into the discriminator D, and compare the embed-
ding scores of the student net to the teacher net. Figure 3a
shows the difference of embedding scores, with holistic dis-
tillation, the segmentation maps produced from student net
can achieve a similar score to the teacher, indicating that
GAN helps distill the holistic structure knowledge. Figure
3b, 3c and 3d are segmentation maps and their correspond-
ing embedding scores of a randomly-selected image. The
well-trained D can assign a higher score to a high quality
segmentation maps, and the student net with the holistic dis-
tillation can generate segmentation maps with higher scores
and better quality. The self-attention modules in the dis-
criminator are useful for capturing the structure information
and beneﬁt the holistic distillation. The gain of using two

2608

self-attention modules is around 1%, from 71.6% to 72.67%
for ResNet18 (1.0).

(a) Score difference

(b) Teacher score: 7.78

(c) w/o HO score: 7.17

(d) with HO score: 7.75

Figure 3: Illustrations of that GAN is able to distill the holistic structure
with ResNet18 (1.0) as an example student net. (a) shows the score differ-
ence of 100 samples between the teacher and the student with and without
the adversarial holistic distillation. (b), (c) and (d) present the segmentaion
maps and the embedding scores of a randomly-selected sample.

Feature and local pair-wise distillation. We compare the
variants of the pair-wise distillation:

• Feature distillation by MIMIC [35, 21]: We follow
[21] to align the features of each pixel between T and
S through a 1 × 1 convolution layer to match the di-
mension of the feature

• Feature distillation by attention transfer [51]: We ag-
gregate the response maps into a so-called attention
map (single channel), and then transfer the attention
map from the teacher to the student.

• Local pair-wise distillation [45]: We distill a local sim-
ilarity map, which represents the similarities between
each pixel and the 8-neighborhood pixels.

We replace our pair-wise distillation by the above three
distillation schemes to verify the effectiveness of our global
pair-wise distillation. From Table 2, we can see that our
pair-wise distillation method outperforms all the other dis-
tillation methods. The superiority over feature distillation
schemes: MIMIC [21] and attention transfer [51], which
transfers the knowledge for each pixel separately, comes
from that we transfer the structured knowledge other than
aligning the feature for each individual pixel. The superior-
ity to the local pair-wise distillation shows the effectiveness

Table 2: Empirical comparison of feature transfer MIMIC [35, 21], atten-
tion transfer [51], and local pair-wise distillation [45] to our global pair-
wise distillation. The segmentation is evaluated by mIoU (%). PI: pixel-
wise distillation. MIMIC: using a 1× 1 convolution for feature distillation.
AT: attention transfer for feature distillation. LOCAL: The local similarity
distillation method. PA: our pair-wise distillation. ImN: initializing the
network from the weights pretrained on ImageNet dataset.

Method

ResNet18 (0

5)

.

ResNet18 (1

.

0) + ImN

w/o distillation

+ PI

+ PI + MIMIC

+ PI + AT

+ PI + LOCAL

+ PI + PA

55

57

58

57

58

61

37

07

44

93

62

03

.

.

.

.

.

.

69

70

71

70

70

71

10

51

03

70

86

37

.

.

.

.

.

.

Table 3: The segmentation results on the testing, validation (Val.), training
(Tra.) set of Cityscapes.

Method

#Params (M) FLOPs (B) Test § Val. Tra.

Current state-of-the-art results

ENet [31] †
ERFNet [48] ‡
FCN [38] ‡
ReﬁneNet [23] ‡
OCNet [50]‡
PSPNet [56] ‡

0

.

3580

2

.

067

134

5

.

118

1

.

62

70

.

.

58

43

3

.

612

25

60

.

333

9

.

525

7

.

548

5

.

574

9

.

58

3 n/a

.

68

0 n/a

.

65

3 n/a

.

73

6 n/a

.

80

1 n/a

.

78

4 n/a

.

n/a

n/a

n/a

n/a

n/a

n/a

Results w/ and w/o distillation schemes

MD [45] ‡
MD (Enhanced) [45] ‡
ESPNet-C [28] †
ESPNet-C (ours) †
ESPNet [28] †
ESPNet (ours) †
5) †
5) (ours) †
0) †
0) (ours) †

ResNet18 (0

ResNet18 (0

ResNet18 (1

.

.

.

.

ResNet18 (1
ResNet18 (1.0) ‡
ResNet18 (1.0) (ours) ‡
MobileNetV2Plus [25] ‡
MobileNetV2Plus (ours) ‡

14

35

.

14

35

.

0

.

3492

0

.

3492

0

.

3635

0

.

3635

3

.

835

3

.

835

15

15

15

15

.

.

.

.

24

24

24

24

8

.

301

8

.

301

64

64

48

48

.

.

3

.

468

3

.

468

4

.

422

4

.

422

33

33

.

.

35

35

128

2

.

128

2

.

128

2

.

128

2

.

86

86

.

.

14

14

n/a 67

3 n/a

.

n/a 71

9 n/a

.

51

57

60

62

.

.

.

.

1 53

6 59

.

.

3 65

9

.

9 70

0

.

3 61

4 n/a

.

0 63

8 73

8

.

.

54

60

56

62

67

71

68

74

.

.

.

.

.

.

.

.

1 55

.

4 60

7

.

5 61

.

6 66

1

.

0 57

.

5 63

0

.

1 63

.

2 69

9

.

6 69

.

1 74

1

.

4 72

.

7 77

4

.

9 70

.

1 n/a

0 74

.

5 83

1

.

† Train from scratch
‡ Initialized from the weights pretrained on ImageNet
§ We test all our models on single scale. Some cumbersome networks are

test on multiple scales, such as OCNet and PSPNet.

of our global pare-wise distillation which is able to transfer
the whole structure information other than a local boundary
information [45].

5.4. Results

Cityscapes. We apply our structure distillation method to
several compact networks: MobileNetV2Plus [25] which is
based on a MobileNetV2 model, ESPNet-C [28] and ES-
PNet [28] which are carefully designed for mobile appli-
cations. Table 3 presents the segmentation accuracy, the

2609

)

 

%
U
o
I
m

(
 
y
c
a
r
u
c
c
A

W/o distillation
Pixel distillation
Our method

90
80
70
60
50
40

Figure 4: Illustrations of the effectiveness of pixel-wise and structured distillation schemes in terms of class IoU scores on the network MobileNetV2Plus
[25] over the Cityscapes test set. Both pixel-level and structured distillation are helpful for improving the performance especially for the hard classes with
low IoU scores. The improvement from structured distillation is more signiﬁcant for structured objects, such as bus and truck.

(a) Image

(b) W/o distillation

(c) Pixel-wise distillation

(d) Our method

(e) Ground truth

Figure 5: Qualitative results on the Cityscapes testing set produced from MobileNetV2Plus: (a) initial images, (b) w/o distillation, (c) only w/ pixel-wise
distillation, (d) Our distillation schemes: both pixel-wise and structured distillation schemes. The segmentation map in the red box about four structured
objects: trunk, person, bus and trafﬁc sign are zoomed in. One can see that the structured distillation method (ours) produces more consistent labels.

model complexity and the model size. GLOPs2 is calcu-
lated on the resolution 512 × 1024 to evaluate the com-
plexity. #parameters is the number of network parame-
ters. We can see that our distillation approach can im-
prove the results over 5 compact networks: ESPNet-C and
ESPNet [28], ResNet18 (0.5), ResNet18 (1.0), and Mo-
bileNetV2Plus [25]. For the networks without pre-training,
such as ResNet18 (0.5), ResNet18 (1.0) and ESPNet-C,
the improvements are very signiﬁcant with 6.2%, 5.74%
and 6.6%, respectively. Compared with MD (Enhanced)
[45] that uses the pixel-wise and local pair-wise distillation
schemes over MobileNet, our approach with the similar net-
work MobileNetV2Plus achieves higher segmentation qual-
ity (74.5 vs 71.9 on the validation set) with a little higher
computation complexity and much smaller model size.

Figure 4 shows the IoU scores for each class over Mo-

bileNetV2Plus. Both the pixel-wise and structured distilla-
tion schemes improve the performance, especially for the
categories with low IoU scores. In particular, the structured
distillation (pair-wise and holistic) has signiﬁcant improve-
ment for structured objects, e.g., 17.23% improvement for
Bus and 10.03% for Truck. The qualitative segmentation
results in Figure 5 visually demonstrate the effectiveness
of our structured distillation for structured objects, such as
trucks, buses, persons, and trafﬁc signs.

CamVid. Table 4 shows the performance of the student
networks w/o and w/ our distillation schemes and state-of-
the-art results. We train and evaluate the student networks
w/ and w/o distillation at the resolution 480×360 following
the setting of ENet. Again we can see that the distillation
scheme improves the performance. Figure 6 shows some
samples on the CamVid test set w/o and w/ the distillation
produced from ESPNet.

2The FLOPs is calculated with the pytorch version implementation [1]

We also conduct an experiment by using an extra unla-

2610

Table 4: The segmentation performance on the test set of CamVid. ImN =
ImageNet dataset, and unl = unlabeled street scene dataset sampled from
Cityscapes.

Extra data mIoU (%)

#Params (M)

Method

ENet[31]

FC-DenseNet56[11]

SegNet[3]

DeepLab-LFOV[5]

FCN-8s[38]

ESPNet-C[28]

ESPNet-C (ours)

ESPNet-C (ours)

ESPNet[28]

ESPNet (ours)

ESPNet (ours)

ResNet18

ResNet18 (ours)

no

no

ImN

ImN

ImN

no

no

unl

no

no

unl

ImN

ImN

ResNet18 (ours)

ImN+unl

51

3

.

58

9

.

55

6

.

61

6

.

57

0

.

56

7

.

60

3

.

64

1

.

57

8

.

61

4

.

65

1

.

70

3

.

71

0

.

72

3

.

0

.

3580

1

.

550

29

37

46

32

.

.

134

5

.

0

.

3492

0

.

3635

15

24

.

(a) Image

(b) W/o dis.

(c) Our method (d) Ground truth

Figure 6: Qualitative results on the CamVid test set produced from ESP-
Net. W/o dis. represents for the baseline student network trained without
distillation.

beled dataset, which contains 2000 unlabeled street scene
images collected from the Cityscapes dataset, to show that
the distillation schemes can transfer the knowledge of the
unlabeled images. The experiments are done with ESPNet
and ESPNet-C. The loss function is almost the same ex-
cept that there is no cross-entropy loss over the unlabeled
dataset. The results are shown in Figure 7. We can see that
our distillation method with the extra unlabeled data can
signiﬁcantly improve mIoU of ESPNet-c and ESPNet for
13.5% and 12.6%.

ADE20K. The ADE20K dataset
is a very challenging
dataset and contains 150 objects. The frequency of objects
appearing in scenes and the pixel ratios of different objects
follow a long-tail distribution. For example, the stuff classes
like wall, building, ﬂoor, and sky occupy more than 40% of
all the annotated pixels, and the discrete objects, such as
vase and microwave at the tail of the distribution, occupy
only 0.03% of the annotated pixels.

)

 

%
U
o
I
m

(
 
y
c
a
r
u
c
c
A

0.75

0.7

0.65

0.6

0.55

0.5

W/o distillaton
+ Pixel level distillation
(cid:50)(cid:88)(cid:85)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)

ESPNet

ESPNet (Unlabel)

ESPNet - C ESPNet - C (Unlabel)

Figure 7: The effect of structured distillation on CamVid. We can see
that distillation can improve the results in two cases: trained over only the
labeled data and over both the labeled and extra unlabeled data.

Table 5: mIoU and pixel accuracy on validation set of ADE20K.

Method

SegNet [3]

DilatedNet50 [44]

PSPNet (teacher) [56]

FCN [38]

ESPNet [28]

ESPNet (ours)

MobileNetV2Plus [25]

MobileNetV2Plus (ours)

ResNet18 [44]

ResNet18 (ours)

mIoU(%) Pixel Acc. (%) #Params (M)

21

34

42

29

20

23

33

35

33

36

.

.

.

.

.

.

.

.

.

.

64

28

19

39

13

91

64

51

82

55

71

76

80

71

70

73

74

76

76

77

.

.

.

.

.

.

.

.

.

.

00

35

59

32

54

94

38

20

05

77

29

62

70

46

74

43

.

.

.

134

5

.

0

.

3635

0

.

3635

8

.

301

8

.

301

15

15

24

24

.

.

the

We

and

report

results

for ResNet18

the
MobileNetV2Plus which are trained with the initial
weights pretrained on the ImageNet dataset, and ESPNet
which is trained from scratch in Table 5. All the results are
tested on single scale. For ESPNet, with our distillation,
we can see that the mIoU score is improved by 3.78%,
and it achieves a higher accuracy with samller #parameters
compared to SegNet. For ResNet18, after the distillation,
we have a 2.73% improvement over the one without
distillation reported in [44]. We check the result for each
class and ﬁnd that the improvements are mainly from the
discrete objects.

6. Conclusion

We study knowledge distillation for training compact
semantic segmentation networks with the help of cumber-
some networks.
In addition to the pixel-level knowledge
distillation, we present two structural distillation schemes:
pair-wise distillation and holistic distillation. We demon-
strate the effectiveness of our proposed distillation schemes
on several recently-developed compact networks on three
benchmark datasets.

2611

References

[1] https://github.com/warmspringwinds/

pytorch-segmentation-detection/blob/
master/pytorch_segmentation_detection/
utils/flops_benchmark.py, 2018.

[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be
deep? In Proc. Advances in Neural Inf. Process. Syst., pages
2654–2662, 2014.

[3] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for image segmentation. IEEE Trans. Pattern Anal. Mach.
Intell., (12):2481–2495, 2017.

[4] Gabriel J Brostow, Jamie Shotton, Julien Fauqueur, and
Roberto Cipolla. Segmentation and recognition using struc-
ture from motion point clouds.
In Proc. Eur. Conf. Comp.
Vis., pages 44–57. Springer, 2008.

[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan Yuille. Semantic image segmenta-
tion with deep convolutional nets and fully connected crfs.
In Proc. Int. Conf. Learn. Representations, 2015.

[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs.
IEEE Trans. Pattern Anal.
Mach. Intell., 40(4):834–848, 2018.

[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-
rian Schroff, and Hartwig Adam. Encoder-decoder with
atrous separable convolution for semantic image segmenta-
tion. Proc. Eur. Conf. Comp. Vis., 2018.

[8] Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and
Jian Yang. Adversarial PoseNet: A structure-aware convo-
lutional network for human pose estimation. In Proc. IEEE
Int. Conf. Comp. Vis., pages 1212–1221, 2017.

[9] Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. Dark-
rank: Accelerating deep metric learning via cross sample
similarities transfer. Proc. Eur. Conf. Comp. Vis., 2018.

[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding.
In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2016.

[11] Simon J´egou1 Michal Drozdzal, David Vazquez, and Adri-
ana Romero1 Yoshua Bengio. The one hundred layers
tiramisu: Fully convolutional densenets for semantic seg-
mentation. Proc. Workshop of IEEE Conf. Comp. Vis. Patt.
Recogn., 2017.

[12] Ian J Goodfellow, Jean Pougetabadie, Mehdi Mirza, Bing
Xu, David Wardefarley, Sherjil Ozair, Aaron Courville,
Yoshua Bengio, Zoubin Ghahramani, and Max Welling.
Generative adversarial nets. Proc. Advances in Neural Inf.
Process. Syst., 3:2672–2680, 2014.

[13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville.
Improved training of
wasserstein gans. In Proc. Advances in Neural Inf. Process.
Syst., pages 5767–5777, 2017.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. Proc. IEEE
Conf. Comp. Vis. Patt. Recogn., pages 770–778, 2016.

[15] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
CoRR,

Distilling the knowledge in a neural network.
abs/1503.02531, 2015.

[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv:
Comp. Res. Repository, abs/1704.04861, 2017.

[17] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. CVPR, pages 2261–2269, 2017.

[18] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf,
Song Han, William J. Dally, and Kurt Keutzer. Squeezenet:
Alexnet-level accuracy with 50x fewer parameters and
<1mb model size.
arXiv: Comp. Res. Repository,
abs/1602.07360, 2016.

[19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. Proc.
Eur. Conf. Comp. Vis., pages 694–711, 2016.

[20] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. Proc. Int. Conf. Learn. Representations, 2018.
[21] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking
very efﬁcient network for object detection. Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., pages 7341–7349, 2017.

[22] Stan Z Li. Markov random ﬁeld modeling in image analysis.

Springer Science & Business Media, 2009.

[23] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D.
Reid. Reﬁnenet: Multi-path reﬁnement networks for high-
resolution semantic segmentation.
In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., pages 5168–5177, 2017.

[24] Guosheng Lin, Chunhua Shen, Anton van den Hengel, and
Ian Reid. Exploring context with deep structured models for
semantic segmentation.
IEEE Trans. Pattern Anal. Mach.
Intell., 2017.
[25] Huijun Liu.

Lightnet: Light-weight networks for se-
mantic image segmentation. https://github.com/
ansleliu/LightNet, 2018.

[26] Yifan Liu, Zengchang Qin, Tao Wan, and Zhenbo Luo. Auto-
painter: Cartoon image generation from sketch by using con-
ditional wasserstein generative adversarial networks. Neuro-
computing, 311:78–87, 2018.

[27] Pauline Luc, Camille Couprie, Soumith Chintala, and Jakob
Verbeek. Semantic segmentation using adversarial networks.
arXiv: Comp. Res. Repository, abs/1611.08408, 2016.

[28] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda
Shapiro, and Hannaneh Hajishirzi. Espnet: Efﬁcient spatial
pyramid of dilated convolutions for semantic segmentation.
Proc. Eur. Conf. Comp. Vis., 2018.

[29] Mehdi Mirza and Simon Osindero. Conditional gener-
arXiv: Comp. Res. Repository,

ative adversarial nets.
abs/1411.1784, 2014.

[30] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In Proc. IEEE Int. Conf. Comp. Vis., pages 1520–1528, 2015.

2612

[45] Jiafeng Xie, Bing Shuai, Jian-Fang Hu, Jingyang Lin, and
Wei-Shi Zheng. Improving fast segmentation with teacher-
student learning. Proc. British Machine Vis. Conf., 2018.

[46] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan
Yang. Denseaspp for semantic segmentation in street scenes.
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018.

[47] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,
Gang Yu, and Nong Sang. Learning a discriminative fea-
ture network for semantic segmentation. In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., 2018.

[48] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-
tion by dilated convolutions. Proc. Int. Conf. Learn. Repre-
sentations, 2016.

[49] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan:
Sequence generative adversarial nets with policy gradient. In
Proc. AAAI Conf. Artiﬁcial Intell., pages 2852–2858, 2017.
[50] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-
work for scene parsing. In arXiv: Comp. Res. Repository,
volume abs/1809.00916, 2018.

[51] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer. Proc. Int. Conf.
Learn. Representations, 2017.

[52] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-
tus Odena. Self-attention generative adversarial networks. In
CoRR, volume abs/1805.08318, 2018.

[53] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang.
In Proc. IEEE Int. Conf.

Interleaved group convolutions.
Comp. Vis., pages 4383–4392, 2017.

[54] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., 2018.

[55] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping
Shi, and Jiaya Jia.
Icnet for real-time semantic segmenta-
tion on high-resolution images. Proc. Eur. Conf. Comp. Vis.,
2018.

[56] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network.
In
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 2881–
2890, 2017.

[57] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen
Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise
spatial attention network for scene parsing. In ECCV, pages
267–283, 2018.

[58] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Scene parsing through
In Proc. IEEE Conf. Comp. Vis. Patt.

Barriuso, and Antonio Torralba.
ade20k dataset.
Recogn., 2017.

[31] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Euge-
nio Culurciello. Enet: A deep neural network architecture for
real-time semantic segmentation. arXiv: Comp. Res. Repos-
itory, abs/1606.02147, 2016.

[32] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A. Efros. Context encoders: Feature
learning by inpainting. Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., pages 2536–2544, 2016.

[33] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. Proc. Int. Conf. Mach.
Learn., pages 1060–1069, 2016.

[34] Eduardo Romera, Jos´e M Alvarez, Luis M Bergasa, and
Roberto Arroyo. Efﬁcient convnet for real-time semantic
segmentation.
In IEEE Intelligent Vehicles Symp., pages
1789–1794, 2017.

[35] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fit-
nets: Hints for thin deep nets. arXiv: Comp. Res. Repository,
abs/1412.6550, 2014.

[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In Proc. Medical Image Computing and Computer-Assisted
Intervention, MICCAI, pages 234–241, 2015.

[37] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., 2018.

[38] E Shelhamer, J Long, and T Darrell. Fully convolutional
IEEE Trans. Pattern

networks for semantic segmentation.
Anal. Mach. Intell., 39(4):640, 2017.

[39] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,
pages 1–9, 2015.

[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., pages 2818–2826, 2016.

[41] Michael Treml, Jos´e Arjona-Medina, Thomas Unterthiner,
Rupesh Durgesh, Felix Friedmann, Peter Schuberth, An-
dreas Mayr, Martin Heusel, Markus Hofmarcher, Michael
Widrich, et al. Speeding up semantic segmentation for au-
tonomous driving. In Proc. Workshop of Advances in Neural
Inf. Process. Syst., 2016.

[42] Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou,
Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman
Mohamed, Matthai Philipose, and Matt Richardson. Do deep
convolutional nets really need to be deep (or even convolu-
tional)? In Proc. Int. Conf. Learn. Representations, 2016.

[43] Heng Wang, Zengchang Qin, and Tao Wan. Text generation
based on generative adversarial nets with latent variables. In
Proc. Paciﬁc-Asia Conf. Knowledge discovery & data min-
ing, pages 92–103, 2018.

[44] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Uniﬁed perceptual parsing for scene understand-
ing. In Proc. Eur. Conf. Comp. Vis., 2018.

2613

