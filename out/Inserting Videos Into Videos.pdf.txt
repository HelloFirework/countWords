Inserting Videos into Videos

Donghoon Lee1,2

Tomas Pﬁster2

Ming-Hsuan Yang2,3

1Electrical and Computer Engineering and ASRI, Seoul National University

2Google Cloud AI

3Electrical Engineering and Computer Science, University of California at Merced

Abstract

directly [10, 14, 19].

In this paper, we introduce a new problem of manipu-
lating a given video by inserting other videos into it. Our
main task is, given an object video and a scene video, to in-
sert the object video at a user-speciﬁed location in the scene
video so that the resulting video looks realistic. We aim to
handle different object motions and complex backgrounds
without expensive segmentation annotations. As it is difﬁ-
cult to collect training pairs for this problem, we synthe-
size fake training pairs that can provide helpful supervisory
signals when training a neural network with unpaired real
data. The proposed network architecture can take both real
and fake pairs as input and perform both supervised and un-
supervised training in an adversarial learning scheme. To
synthesize a realistic video, the network renders each frame
based on the current input and previous frames. Within this
framework, we observe that injecting noise into previous
frames while generating the current frame stabilizes train-
ing. We conduct experiments on real-world videos in object
tracking and person re-identiﬁcation benchmark datasets.
Experimental results demonstrate that the proposed algo-
rithm is able to synthesize long sequences of realistic videos
with a given object video inserted.

1. Introduction

Object insertion in images aims to insert a new object
into a given scene such that the manipulated scene looks
realistic. In recent years, there has been increasing inter-
est in this problem as it can be applied to numerous vision
tasks, including but not limited to training data augmenta-
tion for object detection [19], interactive image editing [10],
and manipulating semantic layouts [14]. However, there re-
mains a signiﬁcant gap between its potential and real-world
applications since existing methods focus on modifying a
single image while either requiring carefully pre-processed
inputs, e.g. segmented objects without backgrounds [16],
or generating objects from a random vector which makes
it difﬁcult to control the resulting appearance of the object

In this paper, we introduce a new problem of insert-
ing existing videos into other videos. More speciﬁcally,
as shown in Figure 1, a user can select a video of an ob-
ject of interest, e.g. a walking pedestrian, and put it at a
desired location in other videos, e.g. surveillance scenes.
Then, an algorithm composes the object seamlessly while
it moves in the scene video. Note that unlike previous
approaches [10, 14, 19], we do not assume that the input
videos have expensive segmentation annotations. This not
only allows users to edit videos more directly and intu-
itively, but also opens the door to numerous applications
from training data augmentation for object tracking, video
person re-identiﬁcation, and video object segmentation, to
video content generation for virtual reality or movies.

We pose the problem as a video-to-video synthesis task
where the synthesized video containing an object of in-
terest should follow the distribution of existing objects in
the scene video. This falls into an unsupervised video-
to-video translation problem since we do not have paired
data in general, i.e. we do not observe exactly the same
motion of the same object at the location we want to in-
sert in different videos. Nevertheless, without any super-
vision, we face challenging issues such as handling differ-
ent backgrounds, occlusions, lighting conditions and object
sizes. Existing methods are limited to addressing such is-
sues when there exists a number of moving objects and
complex backgrounds. For example, the performance of
an algorithm that relies on object segmentation methods,
which often fails to crop foreground objects accurately in
a complex scene, will be bounded by the accuracy of the
segmentation algorithm.

To address the problem, we ﬁrst address the related prob-
lems in the image domain, i.e. we study how to insert a
given object image into other frames from different videos.
To alleviate the issue of unpaired data, we propose a simple
yet effective way to synthesize fake data that can provide su-
pervisory signals for object insertion. The key idea of this
supervision approach using the fake data is, when training a
network, the fake data is carefully rendered to closely match

110061

Figure 1: Given two videos, our algorithm aims to insert an object from one video to the other video. Red arrows point the
inserted object originally exists in video A. We cast the problem as a video-to-video synthesis task. The proposed algorithm
does not rely on any external domain knowledge such as the semantic segmentation mask or body pose.

the distribution of real data so that back-propagated gradi-
ent signals from the supervised fake data can help training
the network with the unsupervised real data. In this work,
the fake data is generated by blending an object image and a
random background patch from each video. Then, the net-
work learns how to reconstruct the object from the blended
data. As the reconstruction errors provide strong supervi-
sory signals, this approach facilitates the learning process
of the generative adversarial framework [9] using unpaired
real data. During inference, a new object is blended into a
target location of the scene video and then fed to the trained
network.

To extend the above-described algorithm to videos, we
discuss how to utilize a history of synthesized frames to ob-
tain a temporally consistent video. We observe that if we
simply add a history of previous frames as a new source of
input to the object insertion network trained on images, the
network will easily collapse by relying only on the (clean)
previous frames instead of the (blended) current frame. To
avoid this pitfall, we use an idea from the denoising autoen-
coder [31]: a random noise is injected into previous frames
before synthesizing the current frame. It forces the network
to learn semantics between previous frames and the current
input instead of blindly copy-and-pasting most of the infor-
mation from the previous frames.

We conduct extensive experiments with strong baseline
methods to evaluate the effectiveness of the proposed algo-

rithm on real-world data. Experimental results show that
the proposed algorithm can insert challenging objects, e.g.
moving pedestrians under the cluttered backgrounds, into
other videos. For quantitative evaluation, we carry out three
experiments. First, we measure the recall of the state-of-
the-art object detector [22] for the inserted object.
It as-
sesses the overall appearance of the inserted object given
the surrounding context. Second, given the state-of-the-art
segmentation algorithm [4], we measure pixel-level preci-
sion and recall of the inserted object. Third, we perform
a human subjective study for evaluating the realism of in-
serted objects.

The main contributions of this work are summarized as

follows:

• We introduce an important and challenging problem
which broadens the domain of object insertion from
images to videos.

• We propose a novel approach to synthesize supervised
fake training pairs that can help a deep neural network
to learn to insert objects without supervised real pairs.
• We develop a new conditional GAN model to facilitate
the joint training of both unsupervised real and super-
vised fake training pairs.

• We demonstrate that the proposed algorithm can syn-
thesize realistic videos based on challenging real-
world input videos.

10062

2. Related Work

Inserting objects into images. Given a pair of an object
image and a scene image, the ST-GAN approach [16] learns
a warping of the object conditioned on the scene. Based
on the warping, the object is transformed to a new location
without changing its appearance. As it focuses on geometric
realism, they use carefully segmented object as an input.

Other approaches aim to insert an object by rendering
its appearance. In [10], an object in a target category is in-
serted into a scene given a location and a size of a bounding
box. It ﬁrst predicts a shape of the object in the semantic
space, after which an output image is generated from the
predicted semantic label map and an input image. A similar
approach is proposed in [19] without using a semantic label
map. A bounding box of a pedestrian is replaced by random
noise and then inﬁlled with a new pedestrian based on the
surrounding context.

To learn both placement and shape of a new object, the
method in [5] removes existing objects from the scene us-
ing an image in-painting algorithm. Then, a network is
trained to recover the existing objects. The results of this
method rely signiﬁcantly run script on whether the adopted
image in-painting algorithm performs well, e.g. not generat-
ing noisy pixels. This issue is alleviated in [14] by learning
the joint distribution of the location and shape of an object
conditioned on the semantic label map. This method aims
to ﬁnd plausible locations and sizes of a bounding box by
learning diverse afﬁne transforms that warp a unit bound-
ing box into the scene. Then, objects of different shapes
are synthesized conditioned on the predicted location and
its surrounding context.

In contrast to existing methods, our algorithm allows a
user to specify both the appearance of an object to insert
and its location. In addition, our algorithm does not require
a segmentation map for training or test.

Conditional video synthesis. The future frame prediction
task conditions on previous frames to synthesize image con-
tent [18, 7, 32, 6, 15, 29, 30]. Due to the future uncertainty
and accumulated error in the prediction process, it typically
can generate only short video sequences. On the other hand,
we synthesize long video sequences by inserting one video
into other videos.

The contents of a video can be transferred to other videos
to synthesize new videos. In [3], given a source video of a
person, the method transfers one’s motion to another person
in the target video. This method estimates the object motion
using a detected body pose and trains a network to render
a person conditioned on the pose. The trained network ren-
ders a new video as if the target subject follows the motion
of the source video. Instead of following exactly the same
motion, the approach in [1] transfers an abstract content of
the source video while the style of the target video is pre-

served. A cyclic spatio-temporal constraint is proposed to
address the task in an unsupervised manner. It translates a
source frame to a target domain and predicts the next frame.
Then, the predicted frame is translated back to the source
domain. This work also forms a cyclic loop which can im-
prove the video quality.

The dynamic contents/textures in a video can also be
used for conditional video synthesis. In [28], dynamic tex-
tures in a video such as water ﬂow or ﬁre ﬂame are captured
by learning a two-stream network. Then, the work animates
an input image to a video with realistic dynamic motions.
Artistic styles of a video is transferred to edit a target video
while preserving its contents [11, 25].

For more generic video-to-video translations, the scheme
in [33] formulates conditional generative adversarial net-
works (GANs) to synthesize photorealistic videos given a
sequence of semantic label maps, sketches or human pose
as an input. During training, the network takes paired data
as input, e.g. sequences of a semantic label map and the
corresponding RGB image sequence. The network is con-
strained to preserve the content of the input sequence in the
output video.

3. Proposed Algorithm

In this work, we consider the problem where a user se-
lects an object in video A and wants to insert it at a desired
location in video B. We assume that each video has annota-
tions for bounding boxes and IDs of objects at every frame.
From the bounding boxes of the selected object in A, we
obtain a video uA consisting of cropped images. The goal
is to translate uA to vA so that the translated video is re-
alistic when inserted into B. We ﬁrst tackle this problem’s
image counterpart and then extend it to videos.

3.1. Inserting images into images

Let uA denote a frame in uA which will be inserted into a
user-deﬁned region rB in B. We train a generator network
GI which takes uA and rB as inputs to render an output
vA. Note that this is different from existing image-to-image
translation tasks [12, 13, 17, 35, 36] since they aim to pre-
serve the content of an input image while changing it to
different attributes or styles, e.g. a semantic map is trans-
lated to RGB images that have the same semantic layout.
In contrast, we need to translate two different images into
a single image while learning which part of the content in
each image should be preserved.

One challenging issue is that we do not have a train-
ing tuple (uA, rB, vA). To address this issue, we ﬁrst cast
the problem as a conditional image in-painting task. More
speciﬁcally, we corrupt rB by blending uA using pixel-wise
multiplications with a ﬁxed binary mask m, i.e. uA ⊕ rB =
uAm/2+rB(1−m/2), as shown in the Figure 2. Then, the

10063

Figure 2: Main steps of the proposed algorithm for inserting an object into an image. Given the video A and B, we crop
objects and backgrounds from each video and synthesize blended images uA ⊕ rB, uB ⊕ rA, and uB ⊕ rB. By learning how
to reconstruct uB from uB ⊕ rA and uB ⊕ rB, we can guide the network to insert uA into the center of rB. In addition to
reconstruction losses from fake pairs, we have additional objective functions as described in (6).

(a) uA

(b) rB

(c) (1) + (2)

(d) (2) + (3)

(e) (6)

Figure 3: Object insertion results using different objective
functions. Inputs are uA and rB.

generator learns a mapping GI : (uA ⊕ rB) → vA to syn-
thesize realistic vA. To this end, the generator learns how
to render the object while suppressing mismatched back-
grounds based on the context of surrounding non-blended
regions. The key advantage of this formulation is that it
is easy to synthesize fake training pairs that are similar to
(uA ⊕ rB, vA).

In this paper, we propose two types of fake pairs (uB ⊕
rA, uB) and (uB ⊕ rB, uB) to learn object insertion. The
intuition behind it is that these pairs contain two separate
tasks that the generator has to perform during inference:
rendering consistent backgrounds based on the context, and
recovering the object region overlapped with rB. We de-
sign two objective functions for fake pairs using GI and an
image discriminator DI . First,

Lf ake
A (GI , DI ) = E(uB ,rA)[log DI (uB, uB ⊕ rA)]
+ E(uB ,rB )[log DI (uB, uB ⊕ rB)]
+ E(uB ,rA)[log(1 − DI (GI (uB ⊕ rA), uB ⊕ rA))]
+ E(uB ,rB )[log(1 − DI (GI (uB ⊕ rB), uB ⊕ rB))],

(1)

is a conditional adversarial loss to make the reconstructed
image sharper and realistic1. Second,

LR(GI ) = kuB − GI (uB ⊕ rA)k + kuB − GI (uB ⊕ rB)k,
(2)

is a content loss to reconstruct uB.

We present results on the real pair using a network
trained with fake pairs in Figure 3(c). Although some parts
are blurry, the overall shape and appearance of inserted ob-
jects are preserved.
In addition, most of the background
pixels from A are removed and replaced by rB, showing
that fake pairs provide meaningful signals to the network to
insert unseen objects. Thus, we expect that the network can
be trained well with both of real and fake pairs. We update
the adversarial loss to consider real pairs as follows:

LA(GI , DI ) = Lf ake
+ E(uA,rB )[log(1 − DI (GI (uA ⊕ rB), uA ⊕ rB))].

A (GI , DI )

(3)

However, as shown in Figure 3(d), the synthesized re-
sults become unstable when we naively train the network
using (2) and (3). We attribute this to different distribu-
tions of the fake pair and real pair. Although their similar
distributions make it possible to generalize the network to
unseen images, when the network actually learns with both
pair types, it is able to distinguish between them, thus limit-
ing generalization. We address this issue by making it more
difﬁcult for the network to distinguish these pairs. In partic-
ular, we make it uncertain about whether the input is sam-
pled from the fake pair or real pair. To this end, we add
a discriminator DE that aims to distinguish the input type

1We denote E(·)

, E(·)∼pdata(·) for notational simplicity.

10064

two major modiﬁcations. First, when rendering the current
frame, we also look up previous frames. Second, we add a
new term in the objective function to synthesize temporally
consistent videos.

Let GV denote a video generator that learns a mapping
2. One simple mapping is to apply
GV : (uA ⊕ rB) → vA
GI for each frame. However, as the mapping of a frame
is independent from neighboring frames, the resulting se-
quence becomes temporally inconsistent. Therefore, we let
GV to additionally look up N previous frames while syn-
thesizing each frame from the blended input. This Markov
assumption is useful for generating long sequence videos
[33]. Figure 4 shows the proposed U-net [24] style encoder-
decoder network architecture. If the network operates with-
out blue layers, which correspond to the feature maps of
previous frames, then it is identical to GI in Section 3.1.
The network encodes all previous frames using a shared en-
coder. Then, the feature map is linearly combined with a
scalar weight wn which represents the importance of each
frame. We use N = 2 and w1 = w2 = 0.5 for experiments
in this work.

To learn GV , we calculate an error signal for the gener-

ated sequence using the following objective function:

L(GV , DI , DV , DE) = LA(GV , DI ) + LA(GV , DV )

+ LA(GV , DE) + LR(GV ),

(7)

where DV is a video discriminator. The ﬁrst term is de-
ﬁned similarly to (5) while we select a random frame from
the generated sequence to calculate the loss; this term fo-
cuses on the realism of the selected frame. The second term
assesses the rendered sequence as follows:

LA(GV , DV ) = E(uB ,rA)[log DV (uB, euB ⊕rA )]
+ E(uB ,rB )[log DV (uB, euB ⊕rB )]
+ E(uB ,rA)[log(1 − DV (GV (uB ⊕ rA), euB ⊕rA ))]
+ E(uB ,rB )[log(1 − DV (GV (uB ⊕ rB), euB ⊕rB ))]
+ E(uA,rB )[log(1 − DV (GV (uA ⊕ rB), euA⊕rB ))].

(8)

(5)

The third and fourth terms are deﬁned similarly to (4) and
(2), respectively.

In addition, while training the network, we observe that
the predicted frame vt
A heavily relies on the previous frames
rather than the current input. The main reason is that the
current input is corrupted by a blending ut
B which
makes it more difﬁcult to process. Therefore, instead of
learning to recover the current frame, the network gradually
ignores the current input and depends more on the previous
frame. It is a critical problem when generating long videos
as the error from the previous frame is accumulated. As a
result, the generated sequence contains severe artifacts af-
ter a number of frames. To address this issue, we degrade

A ⊕ rt

2We denote (uA ⊕ rB) as a sequence of blended inputs ((u1
A

⊕

r1
B , . . . , (uT

A

⊕ rT

B)) where T is the number of frames.

10065

Figure 4: Network structure of the video insertion network
GV . As an illustrative example, we show the case where the
number of layers is four. The network takes previous frames
(vt−N
B as an input
to render vt
A. Each square denotes a layer in the network.
The dashed lines indicates shared weights, and layers next
to each other represent channel concatenations.

A ) and a blended image ut

A , . . . , vt−1

A ⊕ rt

based on its embedded vector as follows:

LA(GI , DE) = E(uB ,rA)[log DE(euB ⊕rA )]
+ E(uB ,rB )[log DE(euB ⊕rB )]
+ E(uA,rB )[log(1 − DE(euA⊕rB ))],

(4)

where ex denotes an embedded vector from the encoder in
GI with an input x. The encoder is trained to fool the dis-
criminator by embedding the fake pair and real pair into the
same space. This embedding vector is fed to discriminators
as a conditional input. We tile the vector to the same size of
the input image and concatenate them to the input channel.
The objective function LA(GI , DI ) is modiﬁed as follows:

LA(GI , DI ) = E(uB ,rA)[log DI (uB, euB ⊕rA )]
+ E(uB ,rB )[log DI (uB, euB ⊕rB )]
+ E(uB ,rA)[log(1 − DI (GI (uB ⊕ rA), euB ⊕rA ))]
+ E(uB ,rB )[log(1 − DI (GI (uB ⊕ rB), euB ⊕rB ))]
+ E(uA,rB )[log(1 − DI (GI (uA ⊕ rB), euA⊕rB ))].

Finally, the overall objective function for object insertion

on the image domain is formulated as follows:

L(GI , DI , DE) = LA(GI , DI )+LA(GI , DE)+LR(GI ). (6)

Figure 3(e) shows that the inserted objects using the loss
function in (6) are sharp and realistic.

3.2. Inserting videos into videos

In this section, we discuss how to extend the object in-
sertion model from images to videos. To this end, we make

previous frames as well using random noise before render
the current frame. By blocking this easy cheating route, the
network has to learn semantic relationships between the two
inputs instead of relying on one side. It makes the network
signiﬁcantly stable during training.

4. Experimental Results

We evaluate our method on the multi-target tracking or
person re-identiﬁcation databases such as the DukeMTMC
[23], TownCenter [2], and UA-DETRAC [34] to show ap-
plicability of our algorithm on real-world examples. These
datasets record challenging scenarios where pedestrians or
cars move naturally. We split 20% of the data as a test set
and present experimental results on the test set. Additional
results, including sample generated videos and a user study,
are included in the supplementary material.

Implementation details. For all experiments,
the net-
work architecture, parameters, and initialization are similar
to DCGAN [21]. We use transposed convolutional layers
with 64 as a base number of ﬁlters for both of the generator
and discriminator. The batch size is set to 1 and instance
normalization is used instead of batch normalization. Input
videos are resized to 1024 × 2048 pixels. We crop u(·) and
r(·) from the video and resize to 256 × 128 pixels. Then, we
render an object on the 256 × 128 pixels patch. It is trans-
formed to 512×256 pixels image or video for visualization.
For each iteration, we pick a random location in A to put a
new object since we want to cover various location and size
input of a user.

Baseline models and qualitative evaluations. As the
problem introduced in this paper is a new problem, we de-
sign strong baselines for performance evaluation.

For object insertion in images, we present six baseline
models. First, we apply the state-of-the-art semantic seg-
mentation algorithm [4] to segment the interested object
region in video A, e.g. a pedestrian in the DukeMTMC
dataset. Then, object pixels are copied to a region in video
B using the predicted segmentation mask as shown in Fig-
ure 5(c). However, the predicted segmentation mask is in-
accurate due to the complex background and articulated
human pose. Therefore, some parts of the object are of-
ten missing and undesired background pixels from video
A are included in the synthesized frame. In addition, the
brightness of the inserted pixels does not match with sur-
rounding pixels in video B. Second, we apply the Pois-
son blending [20] method to the predicted object mask as
shown in Figure 5(d). Although the boundary of object be-
comes smoother, the blended image still contains artifacts.
In addition, the results depend on the performance of the
segmentation algorithm.

Third, we design four GAN-based methods. One naive
approach focuses on synthesizing realistic example using

the following objective function:

Lbase

A (G, D) = EuB [log D(uB)]

+ E(uA,rB )[log D(G(uA ⊕ rB))].

(9)

In this case, the generator easily collapses as it is not guided
to preserve the content of the input object as shown in Fig-
ure 5(e). To alleviate this issue, we add an objective func-
tion that checks the content in the generated image, e.g. a
pixel-wise reconstruction loss or the perceptual loss [8] as
shown in Figure 5(f) and Figure 5(g). The objective func-
tions are deﬁned as follows:

Lbase

pixel(G, D) = Lbase

A (G, D) + kuAm − vAmk,

Lbase

perceptual(G, D) = Lbase

A (G, D)

+ X

l

1

ClHlWl

kφl(uAm) − φl(vAm)k2
2,

(10)

(11)

where φl is l-th activation map of the VGG19 network [27]
with a shape of Cl × Hl × Wl. We use activation maps
of relu2 2 and relu3 3 layers of the VGG19 network which
is pre-trained on the ImageNet dataset [26] to calculate the
perceptual loss. The main limitation of these approaches
is that the network is trained to preserve all pixels around
the object in uA. As a result, a large number of undesired
background pixels appear in vA.

The ﬁnal baseline model uses the cycle consistency
loss [35] which has been used to train networks with un-
paired training data. For the cyclic loss, we learn two map-
ping functions G : (uA, rB) → vA and F : (uB, rA) → vB.
By taking the conditional inputs into account, the objective
function is deﬁned by:

Lbase
cyc (G, F, DA, DB) = LA(G, DB) + LA(F, DA)
+ E(uA,rB )[kF (G(uA, rB), uA(1 − m)) − uAk1]
+ E(uB ,rA)[kG(F (uB, rA), uB(1 − m)) − uBk1]
+ E(uA,rB )[kG(uA, rB)(1 − m) − rB(1 − m)k1]
+ E(uB ,rA)[kF (uB, rA)(1 − m) − rA(1 − m)k1],

(12)

where DA and DB are discriminators for each video and
LA(G, DB) and LA(F, DA) are typical adversarial losses.
The last two terms are added to force the network to insert
an object at a given rA or rB. Although the formulation
has the potential to learn unpaired mappings, it still cannot
guide the network to preserve the same object while trans-
lating images as shown in Figure 5(h). In addition, we ob-
serve that this makes the network unstable during training.
In contrast, the proposed algorithm inserts an object with
its sharp shape and renders less noisy background pixels as
shown in Figure 5(i).

For video object insertion, we consider two baseline
models. First, frames are synthesized without using previ-
ous frames. As the model only processes the current frame
as an input, the overall video may contain ﬂickering or in-
consistent content. Second, a video is generated without

10066

(a) uA

(b) rB

(c) [4]

(d) [4] + [20]

(e) (9)

(f) (10)

(g) (11)

(h) (12)

(i) Ours (6)

Figure 5: Object insertion results for different baseline models given input uA and rB.

Table 1: Recall of the state-of-the-art object detector [22] on
the DukeMTMC database. B1: Adobe Premiere blending
mode. B2: Segmentation-based composition [4].

Table 2: Object insertion score on the DukeMTMC dataset.
B1: Adobe Premiere blending mode.

Method

B1

B2

(9)

(10)

(11) Our

Recall

0.39

0.76

0.73

0.80

0.78

0.86

injecting noise into previous frames. In such cases, as small
errors in each frame accumulate over frame, the synthesized
images are likely noisy.

Figure 6 shows video object insertion results with base-
line comparisons. We use an automatic blending mode of
a commercial video editing software (Adobe Premier CC
Pro) as one baseline. The other baseline uses DeepLabv3+
[4] to copy and paste the predicted segment along frames.
It shows that the proposed algorithm can synthesize more
realistic videos than other baseline methods.
In addition,
as shown in Figure 7, our algorithm is capable of inserting
videos across databases and different objects such as a car.

Quantitative evaluations. To quantify the realism of the
inserted object, an object detector is often used to locate the
inserted object [14, 19, 5]. The premise is that a detector
is likely to locate only well-inserted objects since state-of-

Method

B1

(9)

(10)

(11) Our

Precision

Recall

0.32
0.28

0.61
0.26

0.70
0.47

0.76
0.61

0.85
0.72

OIS

0.30

0.36

0.56

0.68

0.78

the-art methods take both of the object and its surrounding
context into account. We use the YOLOv3 detector [22] to
determine whether it can correctly detect the inserted object
or not. We ﬁx the detection threshold and measure the recall
of the detector by calculating the intersection over union
(IoU) between the inserted object and detected bounding
boxes, using an IoU threshold of 0.5. Table 1 shows the
average recall using a network trained with ﬁve different
iterations. For each experiment, we sample one thousand
images at random.
It shows that the proposed algorithm
achieves the highest recall value on average. In addition, we
accidentally found an interesting corner case of this experi-
ment. While (9) generates non-realistic images in a similar
mode as shown in Figure 5(e), this method once achieves
the highest recall value. It reveals one limitation of assess-

10067

Figure 6: Inserting a pedestrian video on the DukeMTMC dataset. Click the image to play the video.

where ⊙ is an element-wise multiplication, |s| is an area
of non-zero region in s, and OIS is deﬁned using the F1
score. We calculate the score based on randomly generated
one thousand samples and segmentation masks are obtained
by the DeepLabv3+ [4] method. Table 2 shows that the
proposed algorithm achieves the highest OIS against other
baseline algorithms. We also note that the OIS of the base-
line model based on (9) is the lowest.

In order to show potential application for data augmenta-
tion, we train a detector using synthesized objects by our al-
gorithm. We detect pedestrians on the DukeMTMC dataset
using YOLOv3 initialized on the ImageNet. For training
and evaluation, we pick 100 and 1,000 frames at random
from the video of camera 5 in the dataset. In addition, 3,000
frames are augmented by inserting pedestrians from camera
1. It boosts mAP from 53.1% to 68.3%.

5. Conclusion

In this paper, we have introduced an algorithm to a new
problem: manipulating a given video by inserting other
videos into it.
It is a challenging task as it is inherently
an unsupervised (unpaired) problem. Unlike existing ap-
proaches, we propose an algorithm that converts the prob-
lem to a paired problem by synthesizing fake training pairs
and corresponding loss functions. We conducted experi-
ments on real-world videos and demonstrated that the pro-
posed algorithm is able to render long realistic videos with a
given object video inserted. As a future work, it is interest-
ing to make the inserted object interact with the new video,
e.g., path navigation or occlusion handling.

10068

Figure 7: Results of cross dataset pedestrian insertion (from
the DukeMTMC dataset to TownCenter dataset) and a car
video insertion on the UA-DETRAC dataset.

ing the synthesized image using a detector, i.e., if a trained
detector mistakenly returns positive detection result for a
non-realistic fake image, then it is highly likely that other
non-realistic images in the same mode will be detected as
positive samples as well.

While detection results give an idea of how realistic (or
at least, detectable) the inserted object is, it does not indi-
cate pixel-level accuracy of the object insertion, i.e. whether
the object pixels in the input are preserved in the output. To
this end, we introduce a new metric based on pixel-level
precision and recall for object insertion. Given a semantic
segmentation algorithm, let sA denote a binary segmenta-
tion mask of the input object image. Also let s∆ be a binary
mask where s∆(i, j) = 1 when vA(i, j) is closer to uA(i, j)
than rB(i, j). Thus, s∆ represents pixel locations of the in-
serted object. We then deﬁne the precision P , recall R, and
object insertion score (OIS) as follows:

P =

|sA ⊙ s∆|

|s∆|

, R =

|sA ⊙ s∆|

|sA|

, OIS = 2

P R

P + R

, (13)

References

[1] Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser
Sheikh. Recycle-GAN: Unsupervised video retargeting. In
European Conference on Computer Vision, 2018. 3

[2] Ben Benfold and Ian Reid. Stable multi-target tracking in
real-time surveillance video. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2011. 6

[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou,

and
arXiv preprint

Alexei A Efros. Everybody dance now.
arXiv:1808.07371, 2018. 3

[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
European Conference on Computer Vision, 2018. 2, 6, 7, 8

[5] Jui-Ting Chien, Chia-Jung Chou, Ding-Jie Chen, and
Hwann-Tzong Chen. Detecting nonexistent pedestrians. In
IEEE International Conference on Computer Vision, 2017.
3, 7

[6] Emily Denton and Vighnesh Birodkar. Unsupervised learn-
In Neural

ing of disentangled representations from video.
Information Processing Systems, 2017. 3

[7] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsuper-
vised learning for physical interaction through video predic-
tion. In Neural Information Processing Systems, 2016. 3

[8] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
age style transfer using convolutional neural networks.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2016. 6

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Neural In-
formation Processing Systems, 2014. 2

[10] Seunghoon Hong, Xinchen Yan, Thomas Huang, and
Honglak Lee. Learning hierarchical semantic image manip-
ulation through structured representations. In Neural Infor-
mation Processing Systems, 2018. 1, 3

[11] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao
Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-time
neural style transfer for videos. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2017. 3

[12] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
Multimodal unsupervised image-to-image translation. In Eu-
ropean Conference on Computer Vision, 2018. 3

[13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-
In IEEE Conference on Computer Vision

Efros.
sarial networks.
and Pattern Recognition, 2017. 3

[14] Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-
Hsuan Yang, and Jan Kautz. Context-aware synthesis and
placement of object instances. In Neural Information Pro-
cessing Systems, 2018. 1, 3, 7

[15] Xiaodan Liang, Lisa Lee, Wei Dai, and Eric P Xing. Dual
motion gan for future-ﬂow embedded video prediction.
In
IEEE International Conference on Computer Vision, 2017. 3

adversarial networks for image compositing. In IEEE Con-
ference on Computer Vision and Pattern Recognition, 2018.
1, 3

[17] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In Neural Information
Processing Systems, 2017. 3

[18] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error. In In-
ternational Conference on Learning Representations, 2016.
3

[19] Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, and
Pan Zhou. Pedestrian-Synthesis-GAN: Generating pedes-
trian data in real scene and beyond.
arXiv preprint
arXiv:1804.02047, 2018. 1, 3, 7

[20] Patrick P´erez, Michel Gangnet, and Andrew Blake. Poisson
image editing. ACM Transactions on Graphics, 22(3):313–
318, 2003. 6, 7

[21] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
tional generative adversarial networks.
arXiv preprint
arXiv:1511.06434, 2015. 6

[22] Joseph Redmon and Ali Farhadi. Yolov3: An incremental
improvement. arXiv preprint arXiv:1804.02767, 2018. 2, 7
[23] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,
and Carlo Tomasi. Performance measures and a data set for
multi-target, multi-camera tracking. In European Conference
on Computer Vision Workshops, 2016. 6

[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In Proc. of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, 2015. 5

[25] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos and spherical images.
In-
ternational Journal of Computer Vision, pages 1–21, 2018.
3

[26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015. 6

[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 6

[28] Matthew Tesfaldet, Marcus A. Brubaker, and Konstanti-
nos G. Derpanis. Two-stream convolutional networks for dy-
namic texture synthesis. In IEEE Conference on Computer
Vision and Pattern Recognition, 2018. 3

[29] Ruben Villegas, Jimei Yang, Seunghoon Hang, Xunyu Lin,
and Honglak Lee. Decomposing motion and content for nat-
ural video sequence prediction. In International Conference
on Learning Representations, 2017. 3

[30] Ruben Villegas, Jimei Yang, Yuliang Zou, Seunghoon Hang,
Xunyu Lin, and Honglak Lee. Learning to generate long-
term future via hierarchical prediction. In International Con-
ference on Machine Learning, 2017. 3

[16] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,
and Simon Lucey. ST-GAN: Spatial transformer generative

[31] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. Extracting and composing robust

10069

features with denoising autoencoders. In International Con-
ference on Machine Learning, 2008. 2

[32] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial
Hebert. An uncertain future: Forecasting from static images
using variational autoencoders. In European Conference on
Computer Vision, 2016. 3

[33] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
video synthesis. In Neural Information Processing Systems,
2018. 3, 5

[34] Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-
Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan
Yang, and Siwei Lyu. UA-DETRAC: A new benchmark
and protocol for multi-object detection and tracking. arXiv
preprint arXiv:1511.04136, 2015. 6

[35] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In IEEE International Con-
ference on Computer Vision, 2017. 3, 6

[36] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In Neural In-
formation Processing Systems, 2017. 3

10070

