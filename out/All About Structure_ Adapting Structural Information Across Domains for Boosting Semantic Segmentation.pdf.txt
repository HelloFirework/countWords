All about Structure: Adapting Structural Information across Domains for

Boosting Semantic Segmentation

Wei-Lun Chang* Hui-Po Wang* Wen-Hsiao Peng Wei-Chen Chiu

National Chiao Tung University, Taiwan

{luckchang.ee06g, a88575847.cs06g, wpeng, walon}@nctu.edu.tw

Abstract

Source Domain

In this paper we tackle the problem of unsupervised
domain adaptation for the task of semantic segmentation,
where we attempt to transfer the knowledge learned upon
synthetic datasets with ground-truth labels to real-world
images without any annotation. With the hypothesis that
the structural content of images is the most informative and
decisive factor to semantic segmentation and can be read-
ily shared across domains, we propose a Domain Invariant
Structure Extraction (DISE) framework to disentangle im-
ages into domain-invariant structure and domain-speciﬁc
texture representations, which can further realize image-
translation across domains and enable label transfer to im-
prove segmentation performance. Extensive experiments
verify the effectiveness of our proposed DISE model and
demonstrate its superiority over several state-of-the-art ap-
proaches.

1. Introduction

Semantic segmentation is to predict pixel-level semantic
labels for an image. It is considered one of the most chal-
lenging tasks in computer vision. Due to the renaissance
of deep learning in recent years, we witness a great leap
brought to this task. Since the inception of Fully Convolu-
tional Network (FCN), which is built upon pre-trained clas-
siﬁcation models (e.g. VGG [21] and ResNet [7]) and de-
convolutional layers, numerous techniques have been pro-
posed to advance semantic segmentation, such as enlarging
receptive ﬁelds [2, 27] and better preserving contextual in-
formation [28], to name a few. However, these approaches
rely largely on supervised learning, thereby calling for ex-
pensive pixel-level annotations.

To circumvent this issue, one solution is to train seg-
mentation models on synthetic data. The computer graph-
ics technology nowadays is able to synthesize high-quality,
photo-realistic images for a virtual scene. It is thus possi-

*Both authors contribute equally

Target Domain  

Domain-Invariant

Feature

(a) Conventional domain adaptation

Source Domain

Source-Specific Texture

Target Domain  

Domain-Invariant

Structure

Target-Specific Texture

(b) The proposed method

Figure 1. Comparison of the conventional domain adaptation for
semantic segmentation and our proposed method. Instead of mak-
ing the entire feature representation domain invariant, we align
only the distributions of the structure component across domains.

ble to build up a dataset for supervised semantic segmenta-
tion (e.g. GTA5 [17] and SYNTHIA [18]) based on these
synthetic images. During the rendering process, their pixel-
level semantic labels are readily available. Nevertheless,
segmentation models trained on synthetic datasets often
have difﬁculty achieving satisfactory performance in real-
world scenes due to a phenomenon known as domain shift
– i.e. synthetic and real-world images can still exhibit con-
siderable difference in their low-level texture appearance.

Domain adaptation is thus proposed to transfer the
knowledge learned from a source domain (e.g. synthetic
images) to another target domain (e.g. real images). One
common approach is to learn a domain-invariant feature
space across domains by matching their feature distribu-
tions, where different matching criteria have been explored,
e.g. minimizing the second order statistics [23] and domain
adversarial training [6, 8, 25] . There is also a recent re-
search work [24] which introduces distribution alignment

11900

directly in the structural output space for the task of se-
mantic segmentation. However, these approaches are all
driven by a strong assumption that the entire feature or out-
put space of two domains can be well aligned (see Figure
1 (a)) to yield a domain-invariant representation that is also
discriminative for the tasks in question.

In this paper, we propose a Domain Invariant Structure
Extraction (DISE) framework to address unsupervised do-
main adaptation for semantic segmentation. We hypothe-
size that the high-level structure information of an image
would be the most effective for its segmentation prediction.
Thus, our DISE aims to discover a domain-invariant struc-
ture feature by learning to disentangle domain-invariant
structure information of an image from its domain-speciﬁc
texture information, as illustrated in Figure 1 (b).

Our method distinguishes from similar prior works in
(1) learning an image representation comprising explic-
itly a domain-invariant structure component and a domain-
speciﬁc texture component, (2) making only the structure
component domain invariant, and (3) allowing image-to-
image translation across domains which further enables la-
bel transfer, with all achieved within one single framework.
Although DISE shares some parallels with domain separa-
tion networks [1] and DRIT [13], its emphasis on the sep-
aration of structure and texture information and the ability
to translate images across domains and meanwhile maintain
structures clearly highlight the novelties. Extensive experi-
ments on standardized datasets conﬁrm its superiority over
several state-of-the-art baselines.

2. Related Work

In comparison to image classiﬁcation where there exist
many prior works addressing the domain adaptation prob-
lem, semantic segmentation is considered a much more
challenging task to apply domain adaptation, since its out-
put is a segmentation map full of highly structured and con-
textual semantic information. We review several related
works here and categorize them according to the use of
three widely utilized strategies: distribution alignment, im-
age translation, and label transfer. Different works may
differ in their choice and conducting order of these strate-
gies, as contrasted in Table 1.

Firstly, similar to the case of domain adaptation for
image classiﬁcation, different criteria may be applied to
match distributions across domains in the feature space (e.g.
[9, 20, 26, 30]) or in the output space. The representative
work of the latter is proposed by Tsai et al. [24], where ad-
versarial learning is applied on segmentation maps, based
on spatial contextual similarities between the source and tar-
get domains . However, the assumption that the whole fea-
ture or output space of the two domains can be well aligned
often proves impractical, considering the substantial differ-
ence in appearance (namely, texture) between synthetic and

Table 1. Different strategies adopted by prior works on domain
adaptation for semantic segmentation. IT, DA, LT stand for Im-
age Translation, Distribution Alignment, and Label Transfer, re-
spectively. Order denotes the order in which these strategies are
applied.

IT DA LT

X

X X

Methods
Sankaranarayanan
et al. [20]
Hong et al. [9]
Wu et al. [26]
Tsai et al. [24]
Chen et al. [3]
Hoffman et al. [8] X X
X X
Zhu et al. [30]
Our DISE

X X

X X

X

X

X

X

X

Order

IT→DA

--

IT→DA→LT

--
--

IT→LT, DA

IT→DA

DA→IT→LT

real-world images in some applications.

Secondly, the recent advance in image-to-image transla-
tion and style transfer [10, 12, 29] has motivated the trans-
lation of source images to gain texture appearance of target
images, or vice versa. On the one hand, this translation pro-
cess allows segmentation models to use translated images
as augmented training data [8, 26]; on the other hand, the
common feature space learned in the course of image trans-
lation can facilitate learning a domain-invariant segmenta-
tion model [20, 30].

Finally, the image-to-image translation makes possible
the transfer of labels from the source domain to the target
domain, providing additional supervised signals to learn a
model applicable to target-domain images [8, 26]. How-
ever, the direct image-translation may be harmful to learn-
ing, due to the risk of carrying over source speciﬁc informa-
tion to the target domain.

Our proposed DISE makes use of all three strategies but
differs from these prior works in several signiﬁcant ways.
We hypothesize that the high-level structure information of
an image would be the most informative for its semantic
segmentation. Thus, the DISE is to disentangle high-level,
domain-invariant structure information of an image from
its low-level, domain-speciﬁc texture information through a
set of common and private encoders.

3. Method

In this paper, we propose a Domain Invariant Structure
Extraction (DISE) framework to address the problem of un-
supervised domain adaptation for semantic segmentation.
The emphasis on explicitly regularizing the common and
private encoders towards capturing structure and texture in-
formation, along with the ability to translate images from
one domain to another for label transfer, underlines the nov-
elties of our method. The following gives a formal treat-
ment of the DISE. We begin by an overview of its frame-

1901

Notation

Source domain

Target  domain

Loss

#%

Common encoder

#$

Private encoder

"

Decoder

!

Pixel-wise classifier

Domain Invariant Structure Extraction (DISE)

0 &

0'

&
#$

#%

'
#$

'2&

ℒ'3-4&_-./

(b)

10'2&

&
($

'
(%

"

&
($

&
(%

'
(%

'
($

&
(%
'
($

!

&

ℒ&*+

ℒ&*+_-./

"

"

(a)

"

&2'

ℒ'3-4&_-./

(c)

10 &2'

10 &2&

10'2'

&

ℒ3*%

'

ℒ3*%

(d)

#%

!

&2'
ℒ&*+

'2&

ℒ'3-4&_&'3

'2&

ℒ'3-4&_'*5

&2'

ℒ'3-4&_&'3

&2'

ℒ'3-4&_'*5

Figure 2. An overview of the proposed domain-invariant structure extraction (DISE) framework for semantic segmentation. The DISE
framework is composed of a common encoder Ec shared across domains, two domain-speciﬁc private encoders, E s
p, a pixel-wise
classiﬁer T , and a shared decoder D. It encodes an image, source-domain or target-domain, into a domain-speciﬁc texture component zp
and a domain-invariant structure component zc, as shown in part (a). With this disentanglement, it can translate an image xs (respectively,
xt) in one domain to another image ˆxs2t (respectively, ˆxt2s) in the other domain by combining the structure content of xs (respectively,
xt) with the texture appearance of xt (respectively, xs), as shown in parts (b) and (c). This further enables the transfer of ground-truth
labels from the source domain to the target domain, as illustrated in part (d).

p, E t

work. Next, we present in detail the loss functions used,
followed by a description of implementation details.

3.1. Domain Invariant Structure Extraction

i , ys

i )}Ns

i=1, with each image xs

The DISE aims to learn an image representation com-
prising a domain-invariant structure component and a
domain-speciﬁc texture component. The setting assumes
access to Ns annotated source-domain images X s =
i ∈ RH×W ×3 having
{(xs
height H, width W and C-way per-pixel label of object
i ∈ {0, 1}H×W ×C , and Nt unannotated target-
categories ys
domain images X t = {xt
i=1. As shown in Figure 2 (a),
there are ﬁve sub-networks in DISE, namely, the common
encoder Ec shared across domains, the domain-speciﬁc pri-
vate encoders (cid:8)Es
p(cid:9), the shared decoder D, and the
pixel-wise classiﬁer T . They are parameterized by θc, θs
p,
θt
p, θd and θt, respectively.

p, Et

i}Nt

Given a source-domain image xs as input, the com-
mon encoder Ec produces zs
c = Ec(xs; θc) to character-
ize its domain-invariant, high-level structure information
while the source-speciﬁc private encoder Es
p =
Es
p) for capturing its remnant aspects that are largely
related to domain-speciﬁc, low-level texture information.
These two components (cid:8)zs
p(cid:9) are complementary to each
other; when combined together, they allow the decoder D

p generates zs

p(xs; θs

c , zs

p(xt; θt

to minimize a reconstruction loss Ls
rec between the input
xs and its reconstruction ˆxs2s = D(zs
c , zs
p; θd). Likewise, a
target-domain image xt can be encoded and decoded sim-
ilarly to minimize Lt
c = Ec(xt; θc), zt
p =
Et
p; θd), where the pri-
vate encoder Et
p, extracts the target-
speciﬁc texture information. It is the structure components
zs
c , zt
c that will be used by classiﬁer T to predict segmen-
tation maps, ˆys = T (zs
c, θt) in source and
target domains accordingly.

rec, yielding zt
c, zt
p, like its counterpart Es

p) and xt ≈ ˆxt2t = D(zt

c , θt), ˆyt = T (zt

c , zs

p} and xt = {zt

The disentanglement between structure and texture in-
formation is realized by the regularization coming from im-
age translation with domain adversarial training [14] and
perceptual loss minimization [12]. As illustrated in Fig-
ure 2 (b) and (c), we consider any pair of source- and
target-domain images with their respective representations
xs = {zs
p}. We ﬁrst interchange
their domain-speciﬁc components, and then decode them
into two unseen, translated images ˆxs2t = D(zs
p; θd)
and ˆxt2s = D(zt
p; θd). If the common and private en-
coders behave as we expect them to capture the structure
and texture information, respectively, the translated image
ˆxs2t (respectively, ˆxt2s) should hold the high-level struc-
ture the same as xs (respectively, xt) while exhibiting simi-
lar low-level texture appearance to xt (respectively, xs). To

c, zs

c , zt

c, zt

1902

trans str, Ls2t

trans str, Lt2s

trans tex, Ls2t

trans adv, Ls2t

this end, we train our networks by imposing domain ad-
versarial losses Lt2s
trans adv [14] and perceptual
losses Lt2s
trans tex [12] at
the output of the decoder D in order to ensure the do-
main and perceptual similarities between these translated
images and their counterparts in the source or target do-
mains. This image translation functionality of DISE further
allows the transfer of ground-truth labels from the source
domain to the target domain. More speciﬁcally, since the
target-domain-like images ˆxs2t share the same structure
component as xs, we consider the ground-truth labels ys
of xs to be the pseudo labels for ˆxs2t on grounds of our
hypothesis that the segmentation prediction for an image
depends solely on its structure information.

Finally, we make the structure components zs

c invari-
ant to the domain from which they are extracted by mini-
mizing another domain adversarial loss Lseg adv at the out-
put of the classiﬁer T , as well as the negative log-likelihood
functions of the ground-truth labels ys with respect to xs
and ˆxs2t, i.e. Ls

seg (see Figure 2 (d)).

seg and Ls2t

c , zt

3.2. Learning

The training of the proposed DISE is to minimize a
weighted combination of the aforementioned loss functions
with respect to the parameters {θc, θs
p, θd θt} of the ﬁve
sub-networks:

p, θt

L =λs

seg + λseg adv Lseg adv + λrec Lrec

seg Ls
+ λtrans str Ltrans str + λtrans tex Ltrans tex
+ λtrans adv Ltrans adv + λs2t

seg Ls2t
seg,

(1)

where the combination weights λ’s are chosen empirically
to strike a balance among the model capacity, reconstruc-
tion/translation quality, and prediction accuracy. In the fol-
lowing, we elaborate on each of these loss functions.

Segmentation Loss. The segmentation loss Ls
seg(θc, θt)
given by the typical cross-entropy based on the source-
domain ground truths ys is to train supervisedly the com-
mon encoder Ec and the classiﬁer T in order to predict seg-
mentation maps ˆys for source-domain images xs.

Output Space Adversarial Loss. Inspired by Tsai et al.
[24], we introduce an adversarial loss Lseg adv(θc, θt) at
the output of the classiﬁer T , in the hopes of making the
common encoder Ec and the classiﬁer T generalize well on
target-domain images. Speciﬁcally, we ﬁrst train a discrim-
inator Dseg
adv to distinguish between the source prediction ˆys
and the target prediction ˆyt at the patch level [11] by min-
imizing a supervised domain loss (i.e. Dseg
adv should ideally
output 1 for each patch in the source prediction ˆys and 0 for
that in the target prediction ˆyt). We then update the com-
mon encoder Ec and the classiﬁer T to fool the discrimina-
tor Dseg
adv by inverting its output for ˆyt from 0 to 1, that is,

by minimizing

Lseg adv(θc, θt) = −

1

H ′W ′ X

h′,w′

log(Dseg

adv(ˆyt)h′,w′ ), (2)

where h′, w′ are patch coordinates and H ′ = H/16, W ′ =
W/16 with the factor 16 accounting for the downsampling
in the discriminator Dseg
adv.

The

p, θt

reconstruction

p, θd) is to ensure that

Reconstruction Loss.
loss
Lrec(θc, θs
the two domain-
invariant and domain-speciﬁc components zc, zp of an
image representation together form a nearly complete
summary of the image. To encourage the reconstruction to
be perceptually similar to the input image, we follow the
notion of perceptual loss [12] to deﬁne our quality metric
Lperc(x, y; w) as a weighted sum of L1 differences be-
tween feature representations extracted from a pre-trained
VGG network [22]. In symbols, we have

Lperc(x, y; w) = X

l∈L

w(l)

N (l) (cid:13)(cid:13)(cid:13)

ψ(l)(x) − ψ(l)(y)(cid:13)(cid:13)(cid:13)1

,

(3)

is

the l-th layer of

(respectively, ψ(l)(y))

where ψ(l)(x)
the activa-
the pre-trained VGG net-
tions of
work for input x (respectively, y), N (l)
is the num-
l, w(l) gives a separate
ber of activations in layer
l, and L refers to
weighting to the loss in layer
{relu1 1, relu2 1, relu3 1, relu4 1, relu5 1} of
the VGG network. As pointed out in [12], the higher layers
of VGG network tend to represent the high-level structure
content of an image while the lower layers generally de-
scribe its low-level texture appearance. Equation 3 is then
used to regularize the reconstruction of both source- and
target-domain images by minimizing the sum of their re-
spective perceptual losses:

p, θd)

p, θt
rec + Lt

Lrec(θc, θs
= Ls
= Lperc(ˆxs2s, xs; wrec) + Lperc(ˆxt2t, xt; wrec),

rec

(4)

where the weighting wrec is set to weight more on higher
layers.

p, θt

Translation Structure Loss. As motivated previously in
Section 3.1, an image produced by translation across do-
mains should keep its structure unchanged. The translation
structure loss Ltrans str(θc, θs
p, θd) as deﬁned in Equa-
tion 5 measures the differences in high-level structure be-
tween the translated image ˆxs2t and the image xs from
which the structure component of ˆxs2t is derived, and like-
wise, between ˆxt2s and xt. This is achieved by choosing for
the perceptual metric a weighting wstr that again stresses on
the feature reconstruction losses in higher layers of the pre-
trained VGG network. Our goal is to penalize the translated

1903

images which differ signiﬁcantly in structure from the im-
ages with which they share the same structure component
zc, thereby getting zc to encode explicitly the structure as-
pect of an image.

p, θd)

p, θt
trans str + Lt2s

Ltrans str(θc, θs
= Ls2t
= Lperc(ˆxs2t, xs; wstr) + Lperc(ˆxt2s, xt; wstr)

trans str

(5)

p, θt

Translation Texture Loss. The translation texture loss
Ltrans tex(θc, θs
p, θd) further requires that the translated
image ˆxs2t (respectively, ˆxt2s) should resemble closely in
texture the image xt (respectively, xs), since they share the
In doing so, zp has to en-
same texture component zp.
code explicitly the texture aspect of an image.
Inspired
by the work of AdaIN [10], we propose a weighted met-
ric Ltex(x, y; w) to measure channel-wisely the difference
in the mean value of their activations extracted from a pre-
trained VGG network:

Ltex(x, y; w)

= X

l∈L

w(l)
C (l) X

c

(cid:13)(cid:13)(cid:13)

µc(ψ(l)(x)) − µc(ψ(l)(y))(cid:13)(cid:13)(cid:13)1

,

(6)

where C (l) is the number of channels in layer l of the VGG
network, w(l) speciﬁes the weighting given to layer l, and
µc(·) returns the mean activation of channel c. Like the
translation structure loss, the translation texture loss also
involves the two types of translation:

p, θd)

p, θt
trans tex + Lt2s

Ltrans tex(θc, θs
= Ls2t
= Ltex(ˆxs2t, xt; wtex) + Ltex(ˆxt2s, xs; wtex),

trans tex

where the weighting wtex of the perceptual metric is now
chosen to emphasize more on early layers.

Translation Adversarial Loss.
In addition to the afore-
mentioned perceptual losses, we also employ adversarial
losses Ltrans adv(θc, θs
p, θd) to adapt the translated im-
ages ˆxs2t and ˆxt2s to appear as if they were images out of
the target and source domains, respectively. To this end, we
adopt LSGAN [16] and Patch Discriminator [11].

p, θt

Label Transfer Loss. The label transfer loss Ls2t
seg(θc, θt) is
given by a typical cross-entropy loss that trains supervisedly
the common encoder Ec and the classifer T on translated
images ˆxs2t with pseudo labels ys.

3.3. Implementation

Networks. For experiments, we use a base model, refer-
ring collectively to the common encoder Ec and the pixel-
wise classiﬁer T , similar to the segmentation network in

p, Et

[24], which is built on DeepLab-v2 [2] with ResNet-101
[7]. We obtain initial weights by pre-training on PASCAL
VOC [5] dataset, and at training time, reuse the pre-trained
batchnorm layer. The common encoder Ec outputs the fea-
ture maps of the last residual layer (layer4) as zc. For the
private encoders Es
p, we adopt a convolutional neural
network containing 4 convolution blocks, followed by one
global pooling layer and one fully-connected layer. The
output of the private encoder Es
p) is an
8-dimensional representation zs
p). For the
shared decoder D, we use three residual blocks and three
deconvolution layers. The input to the decoder is a concate-
nation of the private code zp, the feature maps zc, and a ﬂag
indicating the domain of the private code.

p (respectively, Et

p (respectively, zt

Training Details. We implement DISE with Pytorch on
a single Tesla V100 with 16 GB memory. The full train-
ing takes 88 GPU hours. Due to limited memory, at train-
ing time, we resize input images to 512×1024 and perform
random cropping with a crop size of 256×512. However,
at test time, the input images are of size 512×1024. For
fair comparison, we follow Tsai et al. [24] and resize the
output predictions from 512×1024 to 1024×2048 at evalu-
ation time. We train our model for 250,000 iterations with a
batch size of 2. We use the SGD solver with an initial learn-
ing rate of 2.5 × 10−4 for the common encoder Ec and the
classiﬁer T ; the Adam solver with an initial learning rate of
1.0 × 10−3 for the decoder D; and the Adam solver with
an initial learning rate of 1.0 × 10−4 for the others. All the
learning rates decrease according to the polynomial decay
policy. The momentum is set to 0.9 and 0.99.

(7)

4. Experimental Results

In this section, we perform experiments on typical
datasets for semantic segmentation. We compare the perfor-
mance of our proposed method with several state-of-the-art
baselines and conduct an ablation study to understand the
effect of various combinations of loss functions on segmen-
tation performance. The code and pre-trained models are
available online1.

4.1. Datasets

that is,

For experiments, we follow the common protocol
adopted by most prior works;
taking synthetic
dataset GTA5 [17] or SYNTHIA [18] with ground-truth an-
notations as the source domain, and Cityscapes dataset [4]
as the target domain where no annotation is available dur-
ing training. At test time, the evaluation is conducted on the
validation set of Cityscapes. The details of these datasets
are described as follows.

1https://github.com/a514514772/

DISE-Domain-Invariant-Structure-Extraction

1904

Cityscapes [4] is a real-world dataset composed of street-
view images captured in 50 different cities. Its data split
includes 2975 training images and 500 validation images,
with each having a spatial resolution of 2048 × 1024 and
19 semantic labels at the pixel level. Note again that no
ground-truth label is used in model training.

GTA5 [17] is a synthetic dataset containing 24996 images
of size 1914 × 1052. These images are collected from com-
puter game Grand Theft Auto V (GTAV) and come with
pixel-level semantic labels that are fully compatible with
Cityscapes [4].

SYNTHIA is another synthetic dataset composed of 9400
annotated synthetic images with the resolution 1280 × 960.
Like GTA5, it has semantically compatible annotations with
Cityscapes [4]. Following the prior works [9, 20, 24, 26],
we use the SYNTHIA-RAND-CITYSCAPE subset [18].

4.2. Performance Comparison

We compare the performance of our method against sev-
eral baselines, including the models of [3, 9, 19, 20, 24, 26].
Of these, the works [3, 9, 24] are representative of the con-
ventional adaptation that matches distributions of feature or
output spaces across domains based on adversarial training;
the works [20, 26] are typical of those that map source-
domain images to the target domain at the pixel level by im-
age translation or style transfer; and Saleh et al. [19] stands
out from the others by object detection-based method for
foreground instances. More details of these works can be
found in Section 2.

GTA5 to Cityscapes. Table 2 shows that as compared to the
baselines, our method achieves the state-of-the-art perfor-
mance of 45.4 in mean intersection-over-union (mIoU). A
breakdown analysis further reveals that it outperforms most
of the baselines by a large margin in predicting ”Road”,
”Sidewalk, ”Wall”, ”Fence”, ”Building”, and ”Sky” classes.
These are classes that often appear concurrently in an im-
age and tend to be spatially connected. Moreover, some of
them, e.g. ”Road” and ”Sidewalk”, exhibit highly similar
texture appearance. We thus attribute the good performance
of our scheme to its ability to ﬁlter out the domain-speciﬁc
texture information in forming a domain-invariant structure
representation for semantic segmentation.

In Figure 3, we show qualitative results comparing our
method against ”Source Only” (i.e. no adaptation) and
”Conventional Adaptation” (i.e. without disentanglement
of structure and texture). For the latter, we present results
of [24]. It is clear that the segmentation predictions made
by our method look most similar to the ground truths. On
closer examination, we see that our model can better discern
the difference between ”Sidewalk” and ”Road” as compared
to the baselines. It also does a good job at identifying rare
classes such as ”Pole” and ”Trafﬁc Sign”. These obser-

vations suggest that our structure-based representations are
indeed more discriminative than other representations that
may have encoded both structure and texture information as
with the ”Conventional Adaptation”.

SYNTHIA to Cityscapes. We also evaluate all models on
the more challenging SYNTHIA dataset. Speciﬁcally, we
follow [24] to compare results based on semantic predic-
tions for only 16 classes. Table 3 presents quantitative re-
sults in terms of per-class IoU and mIoU. It is seen that most
of the aforementioned discussions made with GTA5 dataset
can be carried over to SYNTHIA. Although the prior work
[9] performs closely to our model in terms of mIoU, the su-
periority of our method in classes like ”Road”, ”Sidewalk”,
”Building”, ”Sky” still remains.

4.3. Ablation Study

The following presents a study of four variants of our
model by comparing their performance with four distinct
training objectives:

• Source Only:

dataset [17] by minimizing Ls
domain adaptation.

Training with annotated GTA5
seg only, i.e. without any

• Seg-map Adaptation: Training with annotated GTA5
dataset [17] together with domain adaptation at the
output space by minimizing Ls
seg and Lseg adv. This
corresponds to the method in [24], which aligns seg-
mentation predictions across domains.

• DISE w/o Label Transfer: Training with all loss
functions except label transfer loss, i.e. the setting for
seg-map adaptation plus disentanglement of structure
and texture components.

• DISE: Training with all loss functions.

Table 4 compares the performance of these settings in
terms of mIoU. As expected, without any domain adapta-
tion, ”Source Only” shows the worst performance with a
39.8 mIoU. The performance improves by 2.8 with Seg-
map Adaptation”, arriving at a 42.6 mIoU, when introduc-
ing domain adaptation at the output space. An even higher
gain of 4.3 over ”Source Only” is seen for the setting of
”DISE w/o Label Transfer”, conﬁrming the beneﬁt of dis-
entangling the structure and texture components. Finally,
with additional augmented data due to label transfer, the
DISE achieves the best performance.

4.4. Image to Image Translation

In Figure 4, we show qualitative results of image-to-
image translation with DISE for two settings, S2T and T2S.
With S2T (respectively, T2S), we combine the structure
content of images in GTA5 (respectively, Cityscapes) in col-
umn (a) with the texture appearance of images in Cityscapes

1905

Table 2. Comparison results on Cityscapes when adapted from GTA5 in terms of per-class IoU and mIoU over 19 classes.

k
l
a
w
e
d
i

S

g
n
i
d
l
i
u
B

d
a
o
R

l
l
a

W

e
c
n
e
F

e
l
o
P

t
h
g
i
L
c
ﬁ
f
a
r
T

n
g
i
S
c
ﬁ
f
a
r
T

n
o
i
t
a
t
e
g
e
V

n
i
a
r
r
e
T

y
k
S

n
o
s
r
e
P

r
e
d
i
R

r
a
C

k
c
u
r
T

s
u
B

n
i
a
r
T

e
k
i
b
r
o
t
o
M

e
l
c
y
c
i
B

U
o
I
m

Base Model

FCN8s [15]

88.0 30.5 78.6 25.2 23.5 16.7 23.5 11.3 78.7 27.2 71.9 51.3 19.5 80.4 19.8 18.3 0.9 20.8 18.4 37.1

FCN8s [15]
FCN8s [15]
PSPNet [28]
PSPNet [28]
Deeplab v2 [2]
Deeplab v2 [2]
Deeplab v2 [2]
Deeplab v2 [2]

88.5 37.4 79.3 24.8 16.5 21.3 26.3 17.4 80.8 30.9 77.6 50.2 19.2 77.7 21.6 27.1 2.7 14.3 18.1 38.5
89.2 49.0 70.7 13.5 10.9 38.5 29.4 33.7 77.9 37.6 65.8 75.1 32.4 77.8 39.2 45.2 0.0 25.5 35.4 44.5
76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6 39.4
85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.5 26.9 11.6 41.7
85.4 31.2 78.6 27.9 22.2 21.9 23.7 11.4 80.7 29.3 68.9 48.5 14.1 78.0 19.1 23.8 9.4
0.0 35.9
86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4
79.8 29.3 77.8 24.2 21.6 6.9 23.5 44.2 80.5 38.0 76.2 52.7 22.2 83.0 32.3 41.3 27.0 19.3 27.7 42.5
91.5 47.5 82.5 31.3 25.6 33.0 33.7 25.8 82.7 28.8 82.7 62.4 30.8 85.2 27.7 34.5 6.4 25.2 24.4 45.4

8.3

Methods
Sankaranarayanan
et al. [20]
Wu et al. [26]
Hong et al. [9]
Chen et al. [3]
Wu et al. [26]
Chen et al. [3]
Tsai et al. [24]
Saleh et al. [19]
Ours

Table 3. Comparison results on Cityscapes when adapted from SYNTHIA in terms of per-class IoU and mIoU over 16 classes.

k
l
a
w
e
d
i
S

g
n
i
d
l
i
u
B

d
a
o
R

l
l
a

W

e
c
n
e
F

e
l
o
P

t
h
g
i
L
c
ﬁ
f
a
r
T

n
g
i
S
c
ﬁ
f
a
r
T

n
o
i
t
a
t
e
g
e
V

n
o
s
r
e
P

r
e
d
i
R

y
k
S

r
a
C

s
u
B

e
k
i
b
r
o
t
o
M

e
l
c
y
c
i
B

U
o
I
m

Base Model

FCN8s [15]

80.1 29.1 77.5 2.8 0.4 26.8 11.1 18.0 78.1 76.7 48.2 15.2 70.5 17.4

8.7

16.7 36.1

FCN8s [15]
FCN8s [15]
PSPNet [28]
Deeplab v2 [2]
Deeplab v2 [2]
Deeplab v2 [2]

8.6

81.5 33.4 72.4 7.9 0.2 20.0
10.5 71.0 68.7 51.5 18.7 75.3 22.7 12.8 28.1 36.5
85.0 25.8 73.5 3.4 3.0 31.5 19.5 21.3 67.4 69.4 68.5 25.0 76.5 41.6 17.9 29.5 41.2
37.3 38.4
82.8 36.4 75.7 5.1 0.1 25.8 8.04 18.7 74.7 76.9 51.1 15.9 77.7 24.8
77.7 30.0 77.5 9.6 0.3 25.8 10.3 15.6 77.6 79.8 44.5 16.6 67.8 14.5
23.8 36.2
77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3 40.0
84.3 42.7 77.5 9.3 0.2 22.9
91.7 53.5 77.1 2.5 0.2 27.1
78.4 81.2 55.8 19.2 82.3 30.3 17.1 34.3 41.5

4.7
6.2

7.0
7.6

4.1
7.0

Methods
Sankaranarayanan
et al. [20]
Wu et al. [26]
Hong et al. [9]
Wu et al. [26]
Chen et al. [3]
Tsai et al. [24]
Ours

Table 4. Ablation study results on Cityscapes when adapted from
GTA5 in terms of mIoU. We present results for no adaptation
(Source Only), adaptation at the output space only (Seg-map
Adaptation), adaptation at the output space together with struc-
ture and texture disentanglement (DISE w/o Label Transfer), and
adaptation with all losses considered (DISE).

X X

A B
X

C D mIoU
39.8
42.6
44.1
X X X X 45.4

Method
Source Only
Seg-map Adaptation
DISE w/o Label Transfer X X X
DISE
A: Ls
B: Lseg adv
C: Lrec + Ltrans str + Ltrans tex + Ltrans adv
D: Ls2t
seg

seg

(respectively, GTA5) in columns (b) and (d) to produce
translated images in columns (c) and (e), respectively. We
see that DISE is very effective in translating images from
one domain to another with high quality. In all cases, the
translated images preserve well the structure content while
producing the desired texture appearance. This also val-
idates our use of the ground-truth labels of the source-
domain images as pseudo labels for their translated images
with texture appearance similar to target-domain images.

5. Conclusion

In this paper, we hypothesize that the high-level structure
information of an image is most decisive to semantic seg-
mentation and can be made invariant across domains. Based
on this hypothesis, we propose a novel framework, Domain
Invariant Structure Extraction (DISE), to disentangle the
representation of an image into a domain-invariant struc-
ture component and a domain-speciﬁc texture component,
where the former is used to advance domain adaptation for
semantic segmentation. The DISE also allows transfer of
ground-truth labels from the source domain to the target
domain, providing additional supervision for learning a seg-
mentation network suitable for target-domain images. Ex-
tensive simulation results on typical datasets conﬁrms the
superiority of DISE over several state-of-the-art methods,
justifying our initial hypothesis.

Acknowledgements

is

project

supported

This
by MOST-108-2634-
F-009-013
and
we are grateful
for High-
performance Computing for computer time and facilities.

and MOST-108-2636-E-009-001

to the National Center

1906

(a) Target Image

(b) Ground Truth

(c) Source Only

(d) Conventional Adapt.

(e) DISE (ours)

Figure 3. Segmentation results on Cityscapes when adapted from GTA5. From left to right, (a) Target Image, (b) Ground Truth, (c) Source
Only, (d) Conventional Adaptation [24], (e) and DISE.

T
2
S

S
2
T

(a) Structure

(b) Texture

(c) Output

(d) Texture

(e) Output

Figure 4. Sample results of translated images. S2T: the structure content of GTA5 images in (a) are combined with the texture appearance
of Cityscapes images in (b) and (d) to output translated images in (c) and (e), respectively. T2S: the structure content of Cityscapes images
in (a) are combined with the texture appearance of GTA5 images in (b) and (d) to output translated images in (c) and (e), respectively.

1907

References

[1] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In Advances in Neu-
ral Information Processing Systems (NIPS), 2016. 2

[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. In IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 2018. 1, 5, 7

[3] Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented
adaptation for semantic segmentation of urban scenes.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 2, 6, 7

[4] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016. 5, 6

[5] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective.
International Journal of Com-
puter Vision (IJCV), 2015. 5

[6] Y. Ganin and V. Lempitsky. Unsupervised domain adapta-
tion by backpropagation. In Proceedings of the International
Conference on Machine Learning (ICML), 2015. 1

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2016. 1, 5

[8] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
A. A. Efros, and T. Darrell. Cycada: Cycle consistent adver-
sarial domain adaptation. In Proceedings of the International
Conference on Machine Learning (ICML), 2018. 1, 2

[9] W. Hong, Z. Wang, M. Yang, and J. Yuan. Conditional gen-
erative adversarial network for structured domain adaptation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 2, 6, 7

[10] X. Huang and S. J. Belongie. Arbitrary style transfer in real-
time with adaptive instance normalization. In Proceedings
of the IEEE International Conference on Computer Vision
(ICCV), 2017. 2, 5

[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 4, 5

[12] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In Proceedings
of the European Conference on Computer Vision (ECCV),
2016. 2, 3, 4

[13] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, and M.-H.
Yang. Diverse image-to-image translation via disentangled
representations. In Proceedings of the European Conference
on Computer Vision (ECCV), 2018. 2

[14] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In Advances in Neural Informa-
tion Processing Systems (NIPS), 2017. 3, 4

[15] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 7

[16] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In Pro-
ceedings of the IEEE International Conference on Computer
Vision (ICCV), 2017. 5

[17] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for
In Proceedings
data: Ground truth from computer games.
of the European Conference on Computer Vision (ECCV),
2016. 1, 5, 6

[18] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 1, 5, 6

[19] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson,
and J. M. Alvarez. Effective use of synthetic data for urban
scene semantic segmentation. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), 2018. 6, 7

[20] S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and
R. Chellappa. Learning from synthetic data: Addressing do-
main shift for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 2, 6, 7

[21] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proceedings
of the International Conference on Learning Representations
(ICLR), 2014. 1

[22] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 4

[23] B. Sun and K. Saenko. Deep coral: Correlation alignment
for deep domain adaptation. In Proceedings of the European
Conference on Computer Vision (ECCV), 2016. 1

[24] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang,
and M. Chandraker. Learning to adapt structured output
space for semantic segmentation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2018. 1, 2, 4, 5, 6, 7, 8

[25] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversar-
ial discriminative domain adaptation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1

[26] Z. Wu, X. Han, Y.-L. Lin, M. G. Uzunbas, T. Goldstein, S. N.
Lim, and L. S. Davis. Dcan: Dual channel-wise alignment
networks for unsupervised scene adaptation. In Proceedings
of the European Conference on Computer Vision (ECCV),
2018. 2, 6, 7

[27] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. In Proceedings of the International Con-
ference on Learning Representations (ICLR), 2015. 1

[28] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
parsing network. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2017. 1,
7

1908

[29] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017. 2

[30] X. Zhu, H. Zhou, C. Yang, J. Shi, and D. Lin. Penalizing
top performers: Conservative loss for semantic segmentation
adaptation. In Proceedings of the European Conference on
Computer Vision (ECCV), 2018. 2

1909

