End-to-End Multi-Task Learning with Attention

Edward Johns

Shikun Liu
Andrew J. Davison
Department of Computing, Imperial College London
{shikun.liu17, e.johns, a.davison}@imperial.ac.uk

Abstract

We propose a novel multi-task learning architecture,
task-speciﬁc feature-level at-
which allows learning of
tention. Our design,
the Multi-Task Attention Network
(MTAN), consists of a single shared network containing a
global feature pool, together with a soft-attention module
for each task. These modules allow for learning of task-
speciﬁc features from the global features, whilst simulta-
neously allowing for features to be shared across different
tasks. The architecture can be trained end-to-end and can
be built upon any feed-forward neural network, is simple
to implement, and is parameter efﬁcient. We evaluate our
approach on a variety of datasets, across both image-to-
image predictions and image classiﬁcation tasks. We show
that our architecture is state-of-the-art in multi-task learn-
ing compared to existing methods, and is also less sen-
sitive to various weighting schemes in the multi-task loss
function. Code is available at https://github.com/
lorenmt/mtan.

1. Introduction

Convolutional Neural Networks (CNNs) have seen great
success in a range of computer vision tasks, including im-
age classiﬁcation [11], semantic segmentation [1], and style
transfer [13]. However, these networks are typically de-
signed to achieve only one particular task. For more com-
plete vision systems in real-world applications, a network
which can perform multiple tasks simultaneously is far
more desirable than building a set of independent networks,
one for each task. This is more efﬁcient not only in terms
of memory and inference speed, but also in terms of data,
since related tasks may share informative visual features.

This type of learning is called Multi-Task Learning
(MTL) [20, 14, 6], and in this paper we present a novel ar-
chitecture for MTL based on feature-level attention masks,
which add greater ﬂexibility to share complementary fea-
tures. Compared to standard single-task learning, training
multiple tasks whilst successfully learning a shared repre-
sentation poses two key challenges:

Shared Features

Task-Specific

Attention Modules

Task-Specific

Attention Modules

Figure 1: Overview of our proposal MTAN. The shared net-
work takes input data and learns task-shared features, whilst
each attention network learns task-speciﬁc features, by ap-
plying attention modules to the shared network.

i) Network Architecture (how to share): A multi-task
learning architecture should express both task-shared
and task-speciﬁc features. In this way, the network is en-
couraged to learn a generalisable representation (to avoid
over-ﬁtting), whilst also providing the ability to learn
features tailored to each task (to avoid under-ﬁtting).

ii) Loss Function (how to balance tasks): A multi-task
loss function, which weights the relative contributions of
each task, should enable learning of all tasks with equal
importance, without allowing easier tasks to dominate.
Manual tuning of loss weights is tedious, and it is prefer-
able to automatically learn the weights, or design a net-
work which is robust to different weights.

However, most prior MTL approaches focus on only one
of these two challenges, whilst maintaining a standard im-
plementation of the other. In this paper, we introduce a uni-
ﬁed approach which addresses both challenges cohesively,
by designing a novel network which (i) enables both task-
shared and task-speciﬁc features to be learned automati-
cally, and consequently (ii) learns an inherent robustness to
the choice of loss weighting scheme.

The proposed network, which we call the Multi-Task At-
tention Network (MTAN) (see Figure 1), is composed of a
single shared network, which learns a global feature pool
containing features across all tasks. Then for each task,

11871

rather than learning directly from the shared feature pool,
a soft attention mask is applied at each convolution block
in the shared network.
In this way, each attention mask
automatically determines the importance of the shared fea-
tures for the respective task, allowing learning of both task-
shared and task-speciﬁc features in a self-supervised, end-
to-end manner. This ﬂexibility enables much more expres-
sive combinations of features to be learned for generali-
sation across tasks, whilst still allowing for discriminative
features to be tailored for each individual task. Further-
more, automatically choosing which features to share and
which to be task speciﬁc allows for a highly efﬁcient archi-
tecture with far fewer parameters than multi-task architec-
tures which have explicit separation of tasks [26, 20].

MTAN can be built on any feed-forward neural network
depending on the type of tasks. We ﬁrst evaluate MTAN
with SegNet [1], an encoder-decoder network on the tasks
of semantic segmentation and depth estimation on the out-
door CityScapes dataset [4], and then with an additional
task of surface normal prediction on the more challenging
indoor dataset NYUv2 [21]. We also test our approach with
a different backbone architecture, Wide Residual Network
[31], on the recently proposed Visual Decathlon Challenge
[23], to solve 10 individual image classiﬁcation tasks. Re-
sults show that MTAN outperforms several baselines and is
competitive with the state-of-the-art for multi-task learning,
whilst being more parameter efﬁcient and therefore scaling
more gracefully with the number of tasks. Furthermore, our
method shows greater robustness to the choice of weight-
ing scheme in the loss function compared to baselines. As
part of our evaluation of this robustness, we also propose a
novel weighting scheme, Dynamic Weight Average (DWA),
which adapts the task weighting over time by considering
the rate of change of the loss for each task.

2. Related Work

The term Multi-Task Learning (MTL) has been broadly
used in machine learning [2, 8, 6, 17], with similarities to
transfer learning [22, 18] and continual learning [29].
In
computer vision, multi-task learning has been used to for
learning similar tasks such as image classiﬁcation in mul-
tiple domains [23], pose estimation and action recognition
[9], and dense prediction of depth, surface normals, and se-
mantic classes [20, 7]. In this paper, we consider two impor-
tant aspects of multi-task learning: how can a good multi-
task network architecture be designed, and how to balance
feature sharing in multi-task learning across all tasks?

Most multi-task learning network architectures for com-
puter vision are designed based on existing CNN architec-
tures. For example, Cross-Stitch Networks [20] contain one
standard feed-forward network per task, with cross-stitch
units to allow features to be shared across tasks. The self-
supervised approach of [6], based on the ResNet101 archi-

tecture [30], learns a regularised combination of features
from different layers of a single shared network. UberNet
[16] proposes an image pyramid approach to process im-
ages across multiple resolutions, where for each resolution,
additional task-speciﬁc layers are formed top of the shared
VGG-Net [27]. The Progressive Networks [26] uses a se-
quence of incrementally-trained networks to transfer knowl-
edge between tasks. However, architectures such as Cross-
Stitch Networks and Progressive Networks require a large
number of network parameters, and scale linearly with the
number of tasks.
In contrast, our model requires only a
rough 10% increase in parameters for per learning task.

On the balancing of feature sharing in multi-task learn-
ing, there is extensive experimental analysis in [20, 14],
with both papers arguing that different amounts of sharing
and weighting tend to work best for different tasks. One
example of weighting tasks appropriately is with the use
of weight uncertainty [14], which modiﬁes the loss func-
tions in multi-task learning using task uncertainty. Another
method is that of GradNorm [3], which manipulates gradi-
ent norms over time to control the training dynamics. As an
alternative to using task losses to determine task difﬁculties,
Dynamic Task Prioritisation [10] encourages prioritisation
of difﬁcult tasks directly using performance metrics such as
accuracy and precision.

3. Multi-Task Attention Network

We now introduce our novel multi-task learning archi-
tecture, the Multi-Task Attention Network (MTAN). Whilst
the architecture can be incorporated into any feed-forward
network, in the following section we demonstrate how to
build MTAN upon an encoder-decoder network, SegNet
[1]. This example conﬁguration allows for image-to-image
dense pixel-level prediction, such as semantic segmenta-
tion, depth estimation, and surface normal prediction.

3.1. Architecture Design

MTAN consists of two components: a single shared net-
work, and K task-speciﬁc attention networks. The shared
network can be designed based on the particular task, whilst
each task-speciﬁc network consists of a set of attention
modules, which link with the shared network. Each at-
tention module applies a soft attention mask to a particular
layer of the shared network, to learn task-speciﬁc features.
As such, the attention masks can be considered as feature
selectors from the shared network, which are automatically
learned in an end-to-end manner, whilst the shared network
learns a compact global feature pool across all tasks.

Figure 2 shows a detailed visualisation of our network
based on VGG-16 [27], illustrating the encoder half of Seg-
Net. The decoder half of SegNet is then symmetric to VGG-
16. As shown, each attention module learns a soft attention
mask, which itself is dependent on the features in the shared

1872

image

conv conv pool

conv conv pool

conv

conv

conv

pool

conv

conv

conv

pool

conv

conv

conv

pool

Task 1

Task 2

conv

u
Merge
g

conv

conv

conv

conv

conv

conv

conv

Attention
Attention
Module
Module

Attention
Module

conv

conv

conv

conv

pool

Attention
Module

Attention
Module

conv

conv

conv

Attention
Attention
Module
Module

Attention
Attention
Module
Module

conv

pool

Attention
Module

Attention
Attention
Module
Module

conv

conv

pool

h

p

f

[1 x 1] conv

[1 x 1] conv

Batch Norm

Batch Norm

ReLu

Sigmoid

Attention Module for Encoder

a

Element-wise
Multiplication

^

a

[3 x 3] conv

Batch Norm

ReLu

pool

samp

u

f

conv

conv

conv

Merge
g

h

p

[3 x 3] conv

[1 x 1] conv

[1 x 1] conv

Batch Norm

Batch Norm

Batch Norm

a

Element-wise
Multiplication

^

a

ReLu

samp

ReLu

Sigmoid

Attention Module for Decoder

Equivalence Relation

Forward Data Flow

Concatenation

Element-wise
Multiplication

Figure 2: Visualisation of MTAN based on VGG-16, showing the encoder half of SegNet (with the decoder half being
symmetrical to the encoder). Task one (green) and task two (blue) have their own set of attention modules, which link with
the shared network (grey). The middle attention module has its structure exposed for visualisation, which is further expanded
in the bottom section of the ﬁgure, showing both the encoder and decoder versions of the module. All attention modules have
the same design, although their weights are individually learned.

network at the corresponding layer. Therefore, the features
in the shared network, and the soft attention masks, can be
learned jointly to maximise the generalisation of the shared
features across multiple tasks, whilst simultaneously max-
imising the task-speciﬁc performance due to the attention
masks.

3.2. Task Speciﬁc Attention Module

The attention module is designed to allow the task-
speciﬁc network to learn task-related features, by applying
a soft attention mask to the features in the shared network,
with one attention mask per task per feature channel. We
denote the shared features in the jth block of the shared net-
work as p(j), and the learned attention mask in this layer for

. The task-speciﬁc features ˆa(j)

task i as a(j)
in this layer,
are then computed by element-wise multiplication of the at-
tention masks with the shared features:

i

i

ˆa(j)
i = a(j)

i ⊙ p(j) ,

(1)

where ⊙ denotes element-wise multiplication.

As shown in Figure 2, the ﬁrst attention module in the
encoder takes as input only features in the shared network.
But for subsequent attention modules in block j, the input is
formed by a concatenation of the shared features u(j), and
the task-speciﬁc features from the previous layer ˆa(j−1)

:

i

a(j)
i = h(j)

i (cid:16)g(j)

i (cid:16)hu(j); f (j) (cid:16)ˆa(j−1)

i

(cid:17)i(cid:17)(cid:17) , j ≥ 2

(2)

1873

Here, f (j), g(j)

, h(j)

i

i

i

are convolutional layers with batch
normalisation, following a non-linear activation. Both g(j)
and h(j)
are composed of [1 × 1] kernels presenting the ith
task-speciﬁc attention mask in block j. f (j) is composed
of [3 × 3] kernels representing a shared feature extractor for
passing to another attention module, following by a pooling
or sampling layer to match the corresponding resolution.

i

The attention mask, following a sigmoid activation to en-
sure a(j)
i ∈ [0, 1], is learned in a self-supervised fashion
with back-propagation. If a(j)
i → 1 such that the mask be-
comes an identity map, the attended feature maps are equiv-
alent to global feature maps and the tasks share all the fea-
tures. Therefore, we expect the performance to be no worse
than that of a shared multi-task network, which splits into
individual tasks only at the end of the network, and we show
results demonstrating this in Section 4.

3.3. The Model Objective

In general multi-task learning with K tasks, input X and
task-speciﬁc labels Yi, i = 1, 2, · · · , K, the loss function
is deﬁned as,

Ltot(X, Y1:K) =

K

X

i=1

λiLi(X, Yi).

(3)

This is the linear combination of task-speciﬁc losses Li with
task weightings λi. In our experiments, we study the effect
of different weighting schemes on various multi-task learn-
ing approaches.

For image-to-image prediction tasks, we consider each
mapping from input data X to a set of labels Yi as one task
with total three tasks for evaluation. In each loss function,
ˆY represents the network’s prediction, and Y represents the
ground-truth label.

• For semantic segmentation, we apply a pixel-wise cross-
entropy loss for each predicted class label from a depth-
softmax classiﬁer.

L1(X, Y1) = −

1
pq X

p,q

Y1(p, q) log ˆY1(p, q).

(4)

• For depth estimation, we apply an L1 norm comparing
the predicted and ground-truth depth. We use true depth
for the NYUv2 indoor scene dataset, and inverse depth in
CityScapes outdoor scene dataset as standard, which can
more easily represent points at inﬁnite distances, such as
the sky:

L2(X, Y2) =

1
pq X

p,q

|Y2(p, q) − ˆY2(p, q)|.

(5)

• For surface normals (only available in NYUv2), we ap-
ply an element-wise dot product at each normalised pixel
with the ground-truth map:

L3(X, Y3) = −

1
pq X

p,q

Y3(p, q) · ˆY3(p, q).

(6)

For image classiﬁcation tasks, we consider each dataset
as one task for which each dataset represents each individ-
ual classiﬁcation task for one domain. We apply standard
cross-entropy loss for all classiﬁcation tasks.

4. Experiments

In this section, we evaluate our proposed method on two
types of tasks: one-to-many predictions for image-to-image
regression tasks in Section 4.1 and many-to-many predic-
tions for image classiﬁcation tasks (Visual Decathlon Chal-
lenge) in Section 4.2.

4.1. Image to Image Prediction (One to Many)

In this section, we evaluate MTAN built upon SegNet
[1] on image-to-image prediction tasks. We ﬁrst introduce
the datasets used for validation in Section 4.1.1, and sev-
eral baselines for comparison in Section 4.1.2. In Section
4.1.3, we introduce a novel adaptive weighting method, and
in Section 4.1.4 we show the effectiveness of MTAN with
various weighting methods compared with single and multi-
task baseline methods. We explore how the performance
of our method scales with task complexity in Section 4.1.5
and we show visualisations of the learned attention masks
in Section 4.1.6.

4.1.1 Datasets

CityScapes. The CityScapes dataset [4] consists of high
resolution street-view images. We use this dataset for two
tasks: semantic segmentation and depth estimation. To
speed up training, all training and validation images were
resized to [128 × 256]. The dataset contains 19 classes for
pixel-wise semantic segmentation, together with ground-
truth inverse depth labels. We pair the depth estimation
task with three levels of semantic segmentation using 2, 7
or 19 classes (excluding the void group in 7 and 19 classes).
Labels for the 19 classes and the coarser 7 categories are
deﬁned as in the original CityScapes dataset. We then fur-
ther create a 2-class dataset with only background and fore-
ground objects. The details of these segmentation classes
are presented in Table 1. We perform multi-task learning
for 7-class CityScapes dataset in Section 4.1.4. We compare
the 2/7/19-class results in Section 4.1.5, with visualisation
of these attention maps in Section 4.1.6.

NYUv2. The NYUv2 dataset [21] is consisted with
RGB-D indoor scene images. We evaluate performances

1874

on three learning tasks: 13-class semantic segmentation de-
ﬁned in [5], true depth data which is recorded by depth cam-
eras from Microsoft Kinect, and surface normals which are
provided in [7]. To speed up training, all training and vali-
dation images were resized to [288 × 384] resolution.

Compared to CityScapes, NYUv2 contains images of in-
door scenes, which are much more complex since the view-
points can vary signiﬁcantly, changable lighting conditions
are present, and the appearance for each object class shifts
widely in texture and shape. We evaluate performance on
different datasets, together with different numbers of tasks,
and further with different class complexities, in order to at-
tain a comprehensive understanding on how our proposed
method behaves and scales under a range of scenarios.

2-class

7-class

19-class

void

ﬂat

void

road, sidewalk

construction

building, wall, fence

background

foreground

object

nature

sky

human

vehicle

pole, trafﬁc light, trafﬁc sign

vegetation, terrain

sky

person, rider

carm truck, bus, caravan, trailer, train, motorcycle

Table 1: Three levels of semantic classes for the CityScapes
data used in our experiments.

4.1.2 Baselines

Most image-to-image multi-task learning architectures are
designed based on speciﬁc feed-forward neural networks,
or implemented on varying network architectures, and thus
they are typically not directly comparable based on pub-
lished results. Our method is general and can be applied to
any feed-forward neural network, and so for a fair compar-
ison, we implemented 5 different network architectures (2
single-task + 3 multi-task) based on SegNet [1], which we
consider as baselines:

• Single-Task, One Task: The vanilla SegNet for single

task learning.

• Single-Task, STAN: A Single-Task Attention Network,
where we directly apply our proposed MTAN whilst only
performing a single task.

• Multi-Task, Split (Wide, Deep): The standard multi-
task learning, which splits at the last layer for the ﬁnal
prediction for each speciﬁc task. We introduce two ve-
rions of Split: Wide, where we adjusted the number of
convolutional ﬁlters, and Deep, where we adjusted the
number of convolutional layers, until Split had at least
as many parameters as MTAN.

• Multi-Task, Dense: A shared network together with
task-speciﬁc networks, where each task-speciﬁc network
receives all features from the shared network, without
any attention modules.

• Multi-Task, Cross-Stitch: The Cross-Stitch Network
[20], a previously proposed adaptive multi-task learning
approach, which we implemented on SegNet.

Note that all the baselines were designed to have at least
as many parameters than our proposed MTAN, and were
tested to validate that our proposed method’s better perfor-
mance is due to the attention modules, rather than simply
due to the increase in network parameters.

4.1.3 Dynamic Weight Average

For most multi-task learning networks, training multiple
tasks is difﬁcult without ﬁnding the correct balance between
those tasks, and recent approaches have attempted to ad-
dress this issue [3, 14]. To test our method across a range
of weighting schemes, we propose a simple yet effective
adaptive weighting method, named Dynamic Weight Aver-
age (DWA). Inspired by GradNorm [3], this learns to av-
erage task weighting over time by considering the rate of
change of loss for each task. But whilst GradNorm requires
access to the network’s internal gradients, our DWA pro-
posal only requires the numerical task loss, and therefore
its implementation is far simpler.

With DWA, we deﬁne the weighting λk for task k as:

λk(t) :=

K exp(wk(t − 1)/T )
Pi exp(wi(t − 1)/T )

, wk(t − 1) =

Lk(t − 1)
Lk(t − 2)

,

(7)

Here, wk(·) calculates the relative descending rate in the
range (0, +∞), t is an iteration index, and T represents a
temperature which controls the softness of task weighting,
similar to [12]. A large T results in a more even distri-
If T is large enough, we
bution between different tasks.
have λi ≈ 1, and tasks are weighted equally. Finally, the
softmax operator, which is multiplied by K, ensures that
Pi λi(t) = K.
In our implementation, the loss value Lk(t) is calculated
as the average loss in each epoch over several iterations.
Doing so reduces the uncertainty from stochastic gradient
descent and random training data selection. For t = 1, 2,
we initialise wk(t) = 1, but any non-balanced initialisation
based on prior knowledge could also be introduced.

4.1.4 Results on Image-to-Image Predictions

We now evaluate the performance of our proposed MTAN
method in image-to-image multi-task learning, based on
the SegNet architecture. Using the 7-class version of the

1875

CityScapes dataset and 13-class version of NYUv2 dataset,
we compare all the baselines introduced in Section 4.1.2.

Training. For each network architecture, we ran experi-
ments with three types of weighting methods: equal weight-
ing, weight uncertainty [14], and our proposed DWA (with
hyper-parameter temperature T = 2, found empirically to
be optimum across all architectures). We did not include
GradNorm [3] because it requires a manual choice of sub-
set network weights across all baselines, based on their spe-
ciﬁc architectures, which distracts from a fair evaluation of
the architectures themselves. We trained all the models with
ADAM optimiser [15] using a learning rate of 10−4, with
a batch size of 2 for NYUv2 dataset and 8 for CityScapes
dataset. During training, we halve the learning rate at 40k
iterations, for a total of 80k iterations.

Results. Table 2 and 3 shows experimental results for
CityScales and NYUv2 datasets across all architectures, and
across all loss function weighting schemes. Results also
show the number of network parameters for each architec-
ture. Our MTAN method performs similarly to our base-
line Dense in the CityScapes dataset, whilst only having
less than half the number of parameters, and outperforms all
other baselines. For the more challenging NYUv2 dataset,
our method outperforms all baselines across all weighting
methods and all learning tasks.

#P.

Architecture

Weighting

Segmentation

Depth

(Higher Better)
(Lower Better)
mIoU Pix Acc Abs Err Rel Err

2 One Task

3.04 STAN

n.a.
n.a.

51.09
51.90

1.75 Split, Wide

2

Split, Deep

3.63 Dense

Equal Weights
50.17
Uncert. Weights [14] 51.21
DWA, T = 2
50.39

49.85
Equal Weights
Uncert. Weights [14] 48.12
DWA, T = 2
49.67

51.91
Equal Weights
Uncert. Weights [14] 51.89
DWA, T = 2
51.78

50.08
≈2 Cross-Stitch [20] Uncert. Weights [14] 50.31
50.33

Equal Weights

DWA, T = 2

1.65 MTAN (Ours)

Equal Weights
53.04
Uncert. Weights [14] 53.86
DWA, T = 2
53.29

90.69
90.87

90.63
90.72
90.45

88.69
88.68
88.81

90.89
91.22
90.88

90.33
90.43
90.55

91.11
91.10
91.09

0.0158
0.0145

0.0167
0.0158
0.0164

0.0180
0.0169
0.0182

0.0138
0.0134
0.0137

0.0154
0.0152
0.0153

0.0144
0.0144
0.0144

34.17
27.46

44.73
44.01
43.93

43.86
39.73
46.63

27.21
25.36
26.67

34.49
31.36
33.37

33.63
35.72
34.14

Table 2: 7-class semantic segmentation and depth estima-
tion results on CityScapes validation dataset. #P shows
the number of network parameters, and the best perform-
ing combination of multi-task architecture and weighting is
highlighted in bold. The top validation scores for each task
are annotated with boxes.

In particular, our method has two key advantages. First,
due to the efﬁciency of having a single shared feature pool
with attention masks automatically learning which features
to share, our method outperforms other methods without re-
quiring extra parameters (column #P), and even with signif-

icantly fewer parameters in some cases.

Second, our method maintains high performance across
different loss function weighting schemes, and is more ro-
bust to the choice of weighting scheme than other meth-
ods, avoiding the need for cumbersome tweaking of loss
weights. We illustrate the robustness of our method to the
weighting schemes with a comparison to the Cross-Stitch
Network [20], by plotting learning curves in Figure 3 with
respect to the performance of three learning tasks in NYUv2
dataset. We can clearly see that our network follows simi-
lar learning trends across various weighting schemes, com-
pared to the Cross-Stitch Network which produces notably
different behaviour across the different schemes.

Equal Weights

DWA

Weight Uncertainty

)
.
c
c
A

.
x
i
P
(

c
i
t
n
a
m
e
S

55

50

45

55

50

)
.
c
c
A

.
x
i
P
(

c
i
t
n
a
m
e
S

0

0

)
.
r
r
E

.
s
b
A
(

h
t
p
e
D

0.8

0.7

0.6

200

0

200

Epoch

Epoch

)
.
r
r
E

.
s
b
A
(

h
t
p
e
D

0.8

0.7

0.6

Epoch

200

0

200

Epoch

)
.
s
o
C
+
1
(

l
a
m
r
o
N

)
.
s
o
C
+
1
(

l
a
m
r
o
N

0.26

0.24

0.22

0.20

0

0.26

0.24

0.22

0.20

0

 
h
c
t
i
t
S
-
s
s
o
r
C

k
s
a
T

-
i
t
l

u
M

k
r
o
w
t
e
N

k
r
o
w
t
e
N
 
n
o
i
t
n
e
t
t
A

200

Epoch

200

Epoch

Figure 3: Validation performance curves on the NYUv2
dataset, across all three tasks (semantics, depth, normals,
from left to right), showing robustness to loss function
weighting schemes on the Cross-Stitch Network [20] (top)
and our Multi-task Attention Network (bottom).

Figure 4 then shows qualitative results on the CityScapes
validation dataset. We can see the advantage of our multi-
task learning approach over vanilla single-task learning,
where the edges of objects are clearly more pronounced.

4.1.5 Effect of Task Complexity

For further introspection into the beneﬁts of multi-task
learning, we evaluated our implementations on CityScapes
across different numbers of semantic classes, with the depth
labels the same across all experiments. We trained the net-
works with the same settings as in Section 4.1.4, with an
additional multi-task baseline Split (the standard version),
which we found to perform better than the other modiﬁed
versions. All networks are trained with equal weighting.

Table 4 (left) shows the validation performance improve-
ment across all multi-task implementations and the single-
task STAN implementation, plotted relative to the perfor-
mance of the vanilla single-task learning on the CityScapes
dataset. Interestingly, for only a 2-class setup, the single-
task attention network (STAN) performs better than all

1876

Type

#P.

Architecture

Weighting

Single Task

3

4.56

One Task
STAN

n.a.
n.a.

1.75

Split, Wide

2

Split, Deep

Multi Task

4.95 Dense

Equal Weights
Uncert. Weights [14]
DWA, T = 2

Equal Weights
Uncert. Weights [14]
DWA, T = 2

Equal Weights
Uncert. Weights [14]
DWA, T = 2

Equal Weights

≈3

Cross-Stitch [20] Uncert. Weights [14]

1.77 MTAN (Ours)

DWA, T = 2

Equal Weights
Uncert. Weights [14]
DWA, T = 2

Segmentation

Depth

Surface Normal

Angle Distance
(Higher Better)
(Lower Better)
mIoU Pix Acc Abs Err Rel Err Mean Median

(Lower Better)

Within t◦

(Higher Better)

11.25

22.5

30

15.10
15.73

15.89
15.86
16.92

13.03
14.53
13.63

16.06
16.48
16.15

14.71
15.69
16.11

17.72
17.67
17.15

51.54
52.89

51.19
51.12
53.72

41.47
43.69
44.41

52.73
54.40
54.35

50.23
52.60
53.19

55.32
55.61
54.97

0.7508
0.6935

0.6494
0.6040
0.6125

0.7836
0.7705
0.7581

0.6488
0.6282
0.6059

0.6481
0.6277
0.5922

0.5906
0.5927
0.5956

0.3266
0.2891

0.2804
0.2570
0.2546

0.3326
0.3340
0.3227

0.2871
0.2761
0.2593

0.2871
0.2702
0.2611

0.2577
0.2592
0.2569

31.76
32.09

33.69
32.33
32.34

38.28
35.14
36.41

33.58
31.68
32.44

33.56
32.69
32.34

31.44
31.25
31.60

25.51
26.32

28.91
26.62
27.10

36.55
32.13
34.12

28.01
25.68
27.40

28.58
27.26
26.91

25.37
25.57
25.46

22.12
21.49

18.54
21.68
20.69

9.50
14.69
12.82

20.07
21.73
20.53

20.08
21.63
21.81

23.17
22.99
22.48

45.33
44.38

39.91
43.59
42.73

27.11
34.52
31.12

41.50
44.58
42.76

40.54
42.84
43.14

45.65
45.83
44.86

57.13
56.51

52.02
55.36
54.74

39.63
46.94
43.48

53.35
56.65
54.27

51.97
54.45
54.92

57.48
57.67
57.24

Table 3: 13-class semantic segmentation, depth estimation, and surface normal prediction results on the NYUv2 validation
dataset. #P shows the number of network parameters, and the best performing combination of multi-task architecture and
weighting is highlighted in bold. The top validation scores for each task are annotated with boxes.

Input Image

Grouth Truth
(Semantic)

Vanilla

Single-Task

Learning

Multi-Task
Attention
Network

Grouth Truth

(Depth)

Vanilla

Single-Task

Learning

Multi-Task
Attention
Network

Figure 4: CityScapes validation results on 7-class semantic labelling and depth estimation, trained with equal weighting. The
original images are cropped to avoid invalid points for better visualisation. The red boxes are regions of interest, showing the
effectiveness of the results provided from our method and single task method.

multi-task methods since it is able to fully utilise network
parameters in a simple manner for the simple task. How-
ever, for greater task complexity, the multi-task methods
encourage the sharing of features for a more efﬁcient use of

available network parameters, which then leads to better re-
sults. We also observe that, whilst the relative performance
gain increases for all implementations as the task complex-
ity increases, our MTAN method increases at a greater rate.

1877

15

10

5

)
U
o
I
m

(

n
i
a
G
e
c
n
a
m
r
o
f
r
e
P

Single-Task, STAN
Multi-Task, Split
Multi-Task, Dense
Multi-Task, Cross-Stitch
Multi-Task, MTAN (ours)

Method

#P. ImNet. Airc. C100 DPed DTD GTSR Flwr Oglt SVHN UCF Mean Score

Scratch [23]
Finetune [23]

10
10

59.87 57.10 75.73 91.20 37.77 96.55 56.3 88.74 96.63 43.27 70.32 1625
59.87 60.34 82.12 92.82 55.53 97.53 81.41 87.69 96.55 51.20 76.51 2500

1
2

Feature [23]
Res. Adapt.[23]
DAN [25]
Piggyback [19]
Parallel SVD [24] 1.5
MTAN (Ours)

59.67 23.31 63.11 80.33 45.37 68.16 73.69 58.79 43.54 26.8 54.28
544
59.67 56.68 81.20 93.88 50.85 97.05 66.24 89.62 96.13 47.45 73.88 2118
2.17 57.74 64.12 80.07 91.30 56.54 98.46 86.05 89.67 96.77 49.38 77.01 2851
1.28 57.69 65.29 79.87 96.99 57.45 97.27 79.09 87.63 97.24 47.48 76.60 2838
60.32 66.04 81.86 94.23 57.82 99.24 85.74 89.25 96.62 52.50 78.36 3398
1.74 63.90 61.81 81.59 91.63 56.44 98.80 81.04 89.83 96.88 50.63 77.25 2941

0
2-class

7-class

19-class

Table 4: Left: CityScapes performance gain in percentage for all implementations compared with the vanilla single-task
method. Right: Top-1 classiﬁcation accuracy on the Visual Decathlon Challenge online test set. #P is the number of parame-
ters as a factor of a single-task implementation. The upper part of table presents results from single task learning baselines;
lower part of table presents results from multi-task learning baselines.

4.1.6 Attention Masks as Feature Selectors

To understand the role of the proposed attention modules, in
Figure 5 we visualise the ﬁrst layer attention masks learned
with our network based on CityScapes dataset. We can see
a clear difference in attention masks between the two tasks,
with each mask working as a feature selector to mask out
uninformative parts of the shared features, and focus on
parts which are useful for each task. Notably, the depth
masks have a much higher contrast than the semantic masks,
suggesting that whilst all shared features are generally use-
ful for the semantic task, the depth task beneﬁts more from
extraction of task-speciﬁc features.

Input Image

Semantic Mask

Semantic Features

Shared Features

Depth Mask

Depth Features

curacies, and assigns a cumulative score with a maximum
value of 10,000 (1,000 per task) based on these accuracies.
The complete details about the challenge settings, evalua-
tion, and datasets used, can be found at http://www.
robots.ox.ac.uk/˜vgg/decathlon/.

Table 4 (right) shows results for the online test set of the
challenge. As consistent with the prior works, we apply
MTAN built on Wide Residual Network [31] with a depth
of 28, widening factor of 4, and a stride of 2 in the ﬁrst
convolutional layer of each block. We train our model us-
ing a batch size of 100, learning rate of 0.1 with SGD, and
weight decay of 5 · 10−5 for all 10 classiﬁcation tasks. We
halve the learning rate every 50 epochs for a total of 300
epochs. Then, we ﬁne-tune 9 classiﬁcation tasks (all ex-
cept ImageNet) with a learning rate 0.01 until convergence.
The results show that our approach surpasses most of the
baselines and is competitive with the current state-of-the-
art, without the need for complicated regularisation strate-
gies such as applying DropOut [28], regrouping datasets by
size, or adaptive weight decay for each dataset, as required.

Input Image

Semantic Mask

Semantic Features

5. Conclusions

Shared Features

Depth Mask

Depth Features

Figure 5: Visualisation of the ﬁrst layer of 7-class semantic
and depth attention features of our proposed network. The
colours for each image are rescaled to ﬁt the data.

4.2. Visual Decathlon Challenge (Many to Many)

Finally, we evaluate our approach on the recently in-
troduced Visual Decathlon Challenge, consisting of 10 in-
dividual image classiﬁcation tasks (many-to-many predic-
tions). Evaluation on this challenge reports per-task ac-

In this work, we have presented a new method for multi-
task learning, the Multi-Task Attention Network (MTAN).
The network architecture consists of a global feature pool,
together with task-speciﬁc attention modules for each task,
which allows for automatic learning of both task-shared
and task-speciﬁc features in an end-to-end manner. Exper-
iments on the NYUv2 and CityScapes datasets with mul-
tiple dense-prediction tasks, and on the Visual Decathlon
Challenge with multiple image classiﬁcation tasks, show
that our method outperforms or is competitive with other
methods, whilst also showing robustness to the particular
task weighting schemes used in the loss function. Due
to our method’s ability to share weights through atten-
tion masks, our method achieves this state-of-the-art per-
formance whilst also being highly parameter efﬁcient.

1878

References

[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture
for image segmentation. IEEE transactions on pattern anal-
ysis and machine intelligence, 39(12):2481–2495, 2017.

[2] Rich Caruana. Multitask learning.

In Learning to learn,

pages 95–133. Springer, 1998.

[3] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-
drew Rabinovich. Gradnorm: Gradient normalization for
adaptive loss balancing in deep multitask networks. In Inter-
national Conference on Machine Learning, pages 793–802,
2018.

[4] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3213–3223, 2016.

[5] Camille Couprie, Cl´ement Farabet, Laurent Najman, and
Yann Lecun. Indoor semantic segmentation using depth in-
formation. In International Conference on Learning Repre-
sentations (ICLR2013), April 2013, 2013.

[6] Carl Doersch and Andrew Zisserman. Multi-task self-
supervised visual learning. In The IEEE International Con-
ference on Computer Vision (ICCV), Oct 2017.

[7] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale con-
volutional architecture.
In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 2650–2658,
2015.

[8] Theodoros Evgeniou and Massimiliano Pontil. Regular-
ized multi–task learning. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 109–117. ACM, 2004.

[9] Georgia Gkioxari, Bharath Hariharan, Ross Girshick, and Ji-
tendra Malik. R-cnns for pose estimation and action detec-
tion. arXiv preprint arXiv:1406.5212, 2014.

[10] Michelle Guo, Albert Haque, De-An Huang, Serena Ye-
ung, and Li Fei-Fei. Dynamic task prioritization for multi-
task learning. In European Conference on Computer Vision,
pages 282–299. Springer, 2018.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015.

[13] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In
losses for real-time style transfer and super-resolution.
European Conference on Computer Vision, pages 694–711.
Springer, 2016.

[14] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 7482–
7491, 2018.

[15] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[16] Iasonas Kokkinos. Ubernet: Training a universal convolu-
tional neural network for low-, mid-, and high-level vision
using diverse datasets and limited memory.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.

[17] Abhishek Kumar and Hal Daum´e III. Learning task grouping
In Proceedings of the
and overlap in multi-task learning.
29th International Coference on International Conference on
Machine Learning, pages 1723–1730. Omnipress, 2012.

[18] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang
Sun, and Philip S Yu. Transfer feature learning with joint
distribution adaptation.
In Proceedings of the IEEE inter-
national conference on computer vision, pages 2200–2207,
2013.

[19] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-
back: Adapting a single network to multiple tasks by learn-
ing to mask weights. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 67–82, 2018.

[20] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-
tial Hebert. Cross-stitch networks for multi-task learning.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3994–4003, 2016.

[21] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
Indoor segmentation and support inference from

Fergus.
rgbd images. In ECCV, 2012.

[22] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-
ing. IEEE Transactions on knowledge and data engineering,
22(10):1345–1359, 2010.

[23] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. In
Advances in Neural Information Processing Systems, pages
506–516, 2017.

[24] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi.
Efﬁcient parametrization of multi-domain deep neural net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 8119–8127, 2018.

[25] Amir Rosenfeld and John K Tsotsos. Incremental learning
through deep adaptation. IEEE transactions on pattern anal-
ysis and machine intelligence, 2018.

[26] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671, 2016.

[27] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[28] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

[29] Sebastian Thrun and Lorien Pratt.

Learning to learn.

Springer Science & Business Media, 2012.

[30] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.

1879

Residual attention network for image classiﬁcation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3156–3164, 2017.

[31] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In Edwin R. Hancock Richard C. Wilson and William
A. P. Smith, editors, Proceedings of the British Machine Vi-
sion Conference (BMVC), pages 87.1–87.12. BMVA Press,
September 2016.

1880

