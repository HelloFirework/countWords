Co-occurrent Features in Semantic Segmentation

Hang Zhang 1 Han Zhang 2 Chenguang Wang 1

Junyuan Xie 1

1Amazon Web Services 2Google Brain

{hzaws,chgwang,junyuanx}@amazon.com, zhanghan@google.com

Abstract

Recent work has achieved great success in utilizing
global contextual information for semantic segmentation,
including increasing the receptive ﬁeld and aggregating
pyramid feature representations. In this paper, we go be-
yond global context and explore the ﬁne-grained repre-
sentation using co-occurrent features by introducing Co-
occurrent Feature Model, which predicts the distribution
of co-occurrent features for a given target.
To lever-
age the semantic context in the co-occurrent features, we
build an Aggregated Co-occurrent Feature (ACF) Module
by aggregating the probability of the co-occurrent feature
within the co-occurrent context. ACF Module learns a
ﬁne-grained spatial invariant representation to capture co-
occurrent context information across the scene. Our ap-
proach signiﬁcantly improves the segmentation results us-
ing FCN and achieves superior performance 54.0% mIoU
on Pascal Context, 87.2% mIoU on Pascal VOC 2012 and
44.89% mIoU on ADE20K datasets. The source code and
complete system will be publicly available upon publica-
tion1.

1. Introduction

Semantic segmentation provides per-pixel label of object
categories for the given image, which is a challenging task
requiring accurate prediction of the object category, loca-
tion and shape. Successful approaches are usually based
on Fully Convolutional Network (FCN) [31], with a Deep
Convolutional Neural Network (CNN) [22, 23] as the base
network. Recent work achieves great success in leverag-
ing contextual information, including enlarging receptive
ﬁeld size with pyramid-based representations [6,29,53] and
learning category speciﬁc scaling factors using context em-
bedding [51].

Despite the success in incorporating global contextual

1Links can be found at http://hangzh.com/

(a) Image

(b) Ground Truth

(c) FCN (baseline)

(d) CFNet (ours)

(e) legend

Figure 1: Some object categories are difﬁcult to distinguish
based on local appearance and scene context. In this exam-
ple, water, river and sea are visually similar and all ﬁt this
scene context. Human can utilize the presence of the boat to
make the prediction, as it typically co-occurs with the sea.
Motivated by this, we introduce Aggregated Co-occurrent
Feature Module to relook at the relations with all the co-
occurrent features before making the predictions. (More vi-
sual examples in Figure 2)

information, in some challenging scenarios, a rough holis-
tic global context might not be enough for the classiﬁcation
of ambiguous objects in the scene. In addition, we observe
natural scenes usually have reasonable and coherent com-
position of objects. The presence of one object, even in a
spatially disjoint region, can be compelling evidence of the
existence of the other. The co-occurrence property among
objects can improve the robustness of the recognition sys-
tem and help resolve the ambiguity of object labels against
noises such as occlusion and variations in pose and illumi-
nation. For example, as shown in Figure 1, sea, river and
water are very similar in appearance and the global context
as a city scene is not able to disambiguate these three as
they can all exist near a city. But object co-occurrence as-
serts that sea is more likely to appear when boat is around.

548

Moreover, co-occurrence does not only exist between ob-
jects and it can also be generalized to different parts of an
object. As shown in the 1st row of Figure 2, an armchair
is composed of armrests, legs, back and seat. It is difﬁcult
to resolve the ambiguity between a chair with an armchair
without noticing the co-occurring armrest parts. In general,
co-occurrent features play an important role in recognizing
the class labels of image pixels. Therefore, a powerful ap-
proach directly capturing the co-occurrent features and uti-
lizing their dependencies is desirable for semantic segmen-
tation.

Existing approaches are not capable to capture the de-
pendencies between co-occurrent features due to their ﬁxed
spatial structure. The baseline FCN [31] has a relatively lo-
cal receptive ﬁeld and fails to utilize co-occurrent features
in distant portions of the image. Recent work simply en-
larges the receptive ﬁeld by utilizing the multi-scale feature
representations using pyramid pooling method [17, 53] or
different atrous rates of convolutions [6]. So the same pool-
ing or atrous convolution operation is applied everywhere
in the feature map. However, the distribution of crucial fea-
tures for the recognition of different image regions varies
tremendously. Instead of having ﬁxed spatial connection,
the network should be able to capture co-occurrent features
across different relative locations, in a spatial invariant man-
ner.

As the ﬁrst contribution of this work, the feature co-
occurrence is modeled as a probability distribution over
the feature space conditioned on a given target feature,
which we refer to as Co-occurrent Feature Model (CFM).
The CFM learns an inherent co-occurrence representation,
where the similarities between features measure how likely
the features would co-occur with the target in the same im-
age. We therefore deﬁne a probability distribution condi-
tioned on target feature using Softmax of the similarities be-
tween the target and co-occurrent features across the space,
which inherits the spatial invariant nature. Moreover, we
expect the co-occurrent features also capture the scene con-
text. However, we ﬁnd that the limitation in expressive-
ness of the Softmax distribution is a bottleneck for model-
ing the context information. For this, we propose a con-
textual prior as a conditional probability on the scene con-
text. The CFM is then deﬁned as a mixture of Softmaxes
distribution with the contextual prior. With the proposed
CFM, we build Aggregated Co-occurrent Feature (ACF)
Module to integrate the context-aware information within
the co-occurrent features, which allows the network to re-
cap the whole scene before making individual predictions
(overview in Figure 3).

The second contribution of this paper is constructing
Co-occurrent Feature Network (CFNet), the state-of-the-
art semantic segmentation architecture. With the proposed
ACF Module, we build CFNet with pre-trained ResNet [18]

as the base network. The proposed CFNet with ResNet-
101 base network achieves state-of-the-art results 54.0%
mIoU on Pascal Context [33], 87.2% mIoU on Pascal VOC
2012 [12] and 44.89% mIoU on ADE20K [56].

2. Co-occurrent Features

We refer to the features co-occurring with the target
feature within the same input image/featuremap as co-
occurrent features. In this section, we ﬁrst introduce the Co-
occurrent Features Model to capture the distribution of the
co-occurrent features for a given target. We further intro-
duce Aggregated Co-occurrent Feature Module to aggregate
the contextual information of co-occurrent features across
the scene as the output target feature representation.

2.1. Co occurrent Feature Model

We tackle the feature co-occurrences as a probabilistic
problem instead of predicting their presences, since the co-
occurrent features for a given target are usually not deter-
ministic. We build a Co-occurrent Feature Model, which
learns an inherent representation via measuring the simi-
larity between the co-occurrent feature and the target fea-
ture, indicating how likely they would co-occur2. Then the
probability distribution of the co-occurrent features condi-
tioned on target feature can be deﬁned using Softmax of
the similarities across the space. Consider the input CNN
featuremap as N number of channel-dimensional features
X = {x1, ...xN }, and xi for i ∈ {1, ...N } is the input
feature at location i. The probability of the co-occurrent
feature xc for a given target feature xt is:

p(xc|xt) =

es(xc,xt)
i=1 es(xi,xt)

PN

,

(1)

where s(xc, xt) is the similarity between the co-occurrent
feature xc and the target feature xt. A natural parame-
terization for the similarity function s is using dot prod-
uct similarity s(xc, xt) = u⊤
xc vxt , where vxt and uxc are
the target and co-occurrent vector representations for fea-
ture xt and xc. The vector representations are given by
uxc = Φc(xc) and vxt = Φt(xt), where Φc & Φt are
the learnable transformations using feed-forward networks.
The proposed model in Eq. 1 is in the same spirit with the
skip-gram model proposed in [32], which is used to capture
the co-occurrent word representation.

Contextual Prior. We ﬁnd it is difﬁcult to model the co-
occurrent features only using the target information without
knowing the whole scene, because the distribution of the
co-occurrent features for the same target varies in differ-
ent context. For example, we may expect chairs or tables

2Inspired by the distributed hypothesis [16]: the target feature repre-
sentations are modeled to predict well co-occurrent features in its context.

549

(a) Image

(b) Ground Truth

(c) FCN (baseline)

(d) CFNet (ours)

(e) Legend

Figure 2: Some challenging category labels are difﬁcult to distinguish even using global semantic context, which requires
understanding the ﬁne-grained details in the co-occurrent features. In the 1st example, it is hard to know whether the chair
is a armchair without noticing co-occurring arm. For the 2nd example, the baseline FCN fails to predict mirror parts that are
far from sconce. Similarly, FCN fails to utilize the spatial layout to distinguish the cabinet with kitchen island. The proposed
CFNet relooks at the relations with the co-occurrent features before classifying each pixel, which successfully segments the
above mentioned objects and also distinguishes the road from sidewalk and dirt track, segments the mountain as a whole part
in the last two examples. (Visual examples from ADE20K dataset [56].)

co-occurring with a human in the indoor scene, but expect
vehicles and buildings instead in the outdoor scene. Re-
cent study also shows that the Softmax-based models do not
have enough capacity for high-rank problems [49]. We can
hypothesize that predicting co-occurrent features is a high-
rank problem in images, which we can show with empirical
observations. If the co-occurrent features are low-rank, we

could use ﬁnite number of bases to represent all possible
co-occurrent features by a weighted combinations of these
bases. However, this contradicts with our common sense
about the varieties of the real-world images. Therefore, pre-
dicting co-occurrent features is a high-rank problem.

To tackle the above issues, we propose to model the
Inspired by Yang et

scene context as contextual prior.

550

Figure 3: Overview of the proposed CFNet. Given an input image, the convolutional featuremaps are extracted using pre-
trained CNN. Then the featuremaps are transformed into target and co-occurrent vector representations using the feed-forward
networks Φt and Φc. The Co-occurrent Feature Model estimates the probability of the co-occurrent features based on the pair-
wise similarities in the vector spaces. The ACF Module aggregates the co-occurrent context captured using transformation
Ψ with the co-occurrent probability. Another branch captures the global feature using global average pooling followed by
a convolution. Then the upsampled featuremaps are concatenated with ACF model output along the channel dimension.
Finally, the concatenated feature representation is fed into the last convolutional layer to make the per-pixel predictions. (J
represents pairwise dot-product similarity, N means aggregation operation)

al. [49], the contextual prior is deﬁned as a Mixture of
Softmaxes (MoS) to learn a prior distribution for the co-
occurrent features conditioned on the contextual informa-
tion. The MoS formulates the co-occurrent probability of
xc for target xt as:

p(xc|xt) =

K

X

k=1

πk

esk(xc,xt)
i=1 esk(xi,xt)

PN

,

(2)

where zt is the aggregated feature output for target t,
p(xc|xt) is the co-occurrent probability given by Equation 2
and ψc is the co-occurrent context at location c. The co-
occurrent context is given by ψc = Ψ(xc), and Ψ is a learn-
able transformation using feed-forward network. The ACF
Module captures the co-occurrent feature distributions, and
aggregates the contextual information with the co-occurrent
probabilities.

where πk is the prior or mixture weight of the k-th compo-
nent, and sk(xc, xt) is the similarity in k-th component for
k ∈ {1, ...K}. The vector representations uxc and vxt are
chunked into K sub-components, and the similarity of each
component is given by sk(xc, xt) = u⊤
xc,kvxt,k. The prior
of each mixture is conditional on the contextual informa-
tion, which can be parameterized as πk =
,
k′ ¯vx)
vxi
where ¯vx = PN
N captures the contextual information
and wk is a learnable vector. The MoS allows the co-
occurrent features under different semantic context can have
different priors.

k ¯vx)
k′=1 exp(w⊤

exp(w⊤

PK

i

2.2. Aggregated Co occurrent Feature Module

To utilize the co-occurrent features, we build Aggregated
Co-occurrent Feature Module (ACF), which aggregates the
co-occurrent contexts with their co-occurrent probabilities
across the spatial locations in a self-attention [42] or non-
local [3] manner:

zt =

N

X

c=1

p(xc|xt) · ψc,

(3)

Dropout and Multi-head Ensembles. Model combina-
tion almost always improves the performance for machine
learning algorithms. Dropout [40] randomly drops units
during the training, so that it learns “thinned” networks and
averages the logits during the inference. Dropout can avoid
the network adapting too much on the training data for over-
ﬁtting. We apply dropout [40] on the co-occurrent features
and expect the network to make correct predictions even
if some of the concurrent features are missing, so that the
network can generalize from limited patterns appeared in
the training set. Another model combination we explore
is using “multi-head” [42], which concatenates the features
of module outputs using different weights to build a in-
network ensemble. We adapt the multi-head strategy to fur-
ther improve the model capacity.

Global Pooling Feature. Global pooling (GP) feature
is commonly used in modern semantic segmentation ap-
proaches [6, 29, 53], which provides a global receptive ﬁeld
as a strong cue to distinguish category in confusing areas.
The GP feature is captured by a global average pooling, fol-

551

lowed by a 1×1 convolution, and then attached to each fea-
ture location. We extend the proposed Co-occurrent Feature
Module with a global pooling feature branch to leverage the
global context, as shown in Figure 3.

3. Co-occurrent Feature Network

With proposed Co-occurrent Feature Module, we build
Co-occurrent Feature Network (CFNet) as shown in Fig-
ure 3. We use pre-trained ResNet [18] as the base network
and apply dilated network strategy to Res-4 and Res-53 of
ResNet, resulting stride-8 models. The proposed Aggre-
gated Co-occurrent Feature Module and global pooling fea-
ture branch are added on top of the base network. ACF
Module considers the input convolutional featuremap with
the shape of C × H × W as a set of C-dimensional features
X = {x1, ..xN }, where N = H ∗ W is total number of fea-
tures. The input features are transformed into vector spaces
using Φt and Φc. We use shared weights for the target trans-
formation Φt and the co-occurrent transformation Φc, and
apply 3×3 average pooling with stride of 2 before the co-
occurrent transformation Φc to reduce the computation. The
probabilities of co-occurrent features are predicted based on
the similarities in the vector space. Then the proposed ACF
Module aggregates the co-occurrent contexts with the co-
occurrent probabilities. The Co-occurrent Feature Model
learns the co-occurrent feature distribution by learning the
transformation functions Φt and Φc in vector spaces.

In another branch, a global pooling feature is concate-
nated to the output featuremap after convolution and up-
sampling. Finally, the last convolution predicts the per-pixel
prediction of the object categories. We upsample the predic-
tion featuremap by 8 times to make its size equal to input
image size to calculate the segmentation loss. Since the pro-
posed ACF Module is differentiable and can be learned with
the rest of the network, it is compatible with existing FCN
based algorithms.

3.1. Relation to Other Methods

Semantic Segmentation.
CNN based method has
achieved remarkable success in semantic segmentation
and scene parsing. Early work classiﬁes each individual
patches/regions for generating segmentation masks [13,15].
FCN [31] ﬁrst replaces the fully connected layers of pre-
trained network with the convolution layers for seman-
tic segmentation. The adaption of CNN for image clas-
siﬁcation suffers from the loss of spatial resolution. De-
convNet [34] and SegNet [2] learn a decoder to recover
the information from downsampled features. Applying
Atrous/Dilated convolution in pre-trained network produces
larger featuremap [5, 50]. UNet [38] concatenates the lower

3We refer to the network stages with the original strides of 16 and 32

as Res-4 and Res-5.

Network

2242 center

3202 center

top-1

top-5

top-1

top-5

ResNet-50
ResNet-101

78.55
80.24

94.17
95.12

79.33
80.63

94.64
95.50

Table 1: Imagenet [10] pretraining for the base networks.
The top-1 and top-5 accuracy (%) on validation set use cen-
ter crop on image size of 224×224 and 320×320.

level features to the featuremap as skip connections. Prior
work also adapts Dense CRF as post-processing to reﬁne the
FCN prediction boundaries [5,8]. CRF-FCN allows end-to-
end learning of CRF with FCN [55]. Recent work reﬁnes
segmentation boundaries [26, 55] and increases spatial res-
olution [36]. Hwang et al. [20] and Ke et al. [21] also ex-
ploit label co-occurrence and structural label dependencies.
These work mainly focus on the network training regular-
ization, while our approach improves the network represen-
tation by directly model the feature co-occurrence.

Context Aggregation.
Pioneering work demonstrates
that combining global features with local patches can im-
prove the segmentation results [37, 39, 41]. ParseNet [29]
proposes to concatenate a global pooling feature with origi-
nal featuremap to capture global context and increase the re-
ceptive ﬁeld size. Pyramid Pooling Module (PPM) [17, 53]
concatenates the global pooling features from a multi-scale
pyramid. Atrous Spatial Pyramid Pooling (ASPP) [6] uses a
set of different atrous rate convolutions to capture pyramid
feature representations with different receptive ﬁeld sizes.

These methods have the predeﬁned spatial connections.
Consider the convolution operation as a matrix multiplica-
tion y = W x, where x ∈ Rn and y ∈ Rm are the ﬂatten in-
put and output and W ∈ Rmn is a transform matrix depend-
ing on the convolution kernel [11]. Matrix W has kh · kw
non-zero elements in each row for the convolution kernel
with the shape of kh × kw. Combining different atrous rate
of convolutions as in ASPP [6] is adding the non-zero en-
tries to each row, but the overall spatial connections are still
sparse and the representation is spatial sensitive. The pro-
posed ACF Module can also be formulated as y = W x and
W is the co-occurrent probability. Comparing to existing
methods, the ACF Module captures the context across the
whole scene with spatial invariant representation.

Featuremap Attention.
Attention mechanism has
achieved great success in natural language processing [35,
42], which captures the long-range information using a
weighted sum of all the features in a sequence. Non-local
neural network [45] brings the self-attention to the ﬁeld of
video classiﬁcation and object detection in computer vision.
A key difference between Co-occurrent Feature Model with

552

Method BaseNet ACF GP Enc

pixAcc% mIoU%

Method

BaseNet

mIoU%

FCN
FCN
CFNet
CFNet
CFNet

Res50
Res50
Res50
Res50
Res101

X

X

X

X

X

X

X

X

X

76.3
79.0
79.3
79.8
81.1

46.3
49.8
51.6
52.4
54.9

Table 2: Ablation study of CFNet on Pascal Context
dataset. ACF indicates using Aggregated Co-occurrent Fea-
ture Module, GP means including global pooling feature
branch, Enc represents Context Encoding Module [51].
Adding Co-occurrent Feature signiﬁcantly improves the
segmentation results, and including global pooling feature
and Context Encoding can further boost the performance.

pixAcc/mIoU

K=1

H=1
H=2
H=4

77.1/49.4
77.9/49.4
79.2/51.2

K=2

79.3/51.6
79.4/51.8
79.6/52.1

K=4

79.5/51.8
79.3/51.6

−4

Table 3: Ablation Study of Contextual Prior and Multi-
heads. We vary the number of mixtures K and number
of multi-heads H and ﬁnd H=4, K=2 gives the best perfor-
mance.

self-attention or non-local network is that the proposed Co-
occurrent Feature Model learns a prior distribution con-
ditioned on the semantic context.
In semantic segmen-
tation, EncNet [51] calculates the pair-wise similarity be-
tween input and learnable codewords and predicts a set of
channel-wise attention factors, which can be considered as
co-occurrent features in channel dimension. PSANet [54]
learns a long range context aggregation using a location sen-
sitive non-local neural network strategy for semantic seg-
mentation.

4. Experimental Results

In this section, we ﬁrst explain the technical details for
the implementation of the proposed CFNet and baseline
FCN. Then we conduct a comprehensive ablation study of
the proposed ACF Module and CFNet on Pascal Context
dataset [33]. Then we report the performance of CFNet on
Pascal VOC 2012 [12], ADE20K [56] and Cityscapes [9]
datasets.

4.1. Implementation Details

For baseline FCN and proposed CFNet, we use
ResNet [18] as the base network and apply dilation strategy
for the pre-trained networks, resulting in stride-8 models.
Following the prior work [51, 53], we use bilinear interpo-
lation to upsample the network output logits for calculat-
ing the loss. We use standard SGD optimizer and set the
momentum to 0.9 and weight decay to 0.0001. A “poly”

FCN-8s [31]
CRF-RNN [55]
ParseNet [29]
HO CRF [1]
Piecewise [27]
VeryDeep [46]
DeepLab-v2 [5] Res101 + COCO
ReﬁneNet [26]
MSCI [25]
EncNet [51]
CFNet (ours)
CFNet (ours)

Res152
Res152
Res101
Res50
Res101

37.8
39.3
40.4
41.3
43.3
44.5
45.7
47.3
50.3
51.7
51.5
54.0

Table 4: Segmentation results on PASCAL-Context dataset.
(Note: mIoU on 60 classes w/ background.)

like learning rate scheduling [5] is used lr = base lr ∗
(1 − iter
total iter )power. We set the base learning rate as 0.004
for ADE20K and Cityscapes datasets and the power is set
to 0.9. We use base learning rate of 0.004 for COCO pre-
training and reduce it to 0.001 when ﬁne-tuning on Pascal
VOC. We use the “sync-once” implementation of Cross-
GPU Batch Normalization provided by Zhang et al. [51].
As ACF Module is compatible with existing FCN based
approaches, we also study the performance when adding
Context Encoding Module and Semantic Encoding Loss
with default settings in EncNet [51]. Following the prior
work [53], an auxiliary loss is added after Res-4 by adding
an additional FCN head to Res-4, which is applied to all the
experiments.

The networks are trained for 120 epochs for ADE20K
dataset, 180 epochs on Cityscapes dataset, 30 epochs for
COCO pretraining, 50 epochs on Pascal VOC and 80
epochs on Pascal Context dataset. The images and ground
truth masks are randomly ﬂipped and rescaled to the ratio
of 0.5 to 2.0 and randomly cropped into the training sizes
using zero padding if needed. We use the mini-batch size of
16 during the training. The samples are randomly shufﬂed,
and the last batch is discarded if mini-batch size is less than
16.

Evaluation and Metrics. During the evaluation, we fol-
low the best practice [51] to average the network predictions
in multiple scales. We ﬁrst resize the original image into
different scales {0.5, 0.75, 1.0, 1.25, 1.5, 1.75}, then crop
the scaled images into training image size and feed the im-
ages into the network with ﬂipping. Finally, the predicted
logits are averaged across different crops and scales. Since
the multi-size evaluation improves the performance of all
the methods, we adopts this strategy for all the experiments.
We use the standard metrics of pixel accuracy (pixAcc) and
mean intersection of union (mIoU) in this experiments. For

553

the scene parsing results on Pascal Context and ADE20K
validation sets, we ignore the background pixels in calcu-
lating the evaluation metrics, following the standard bench-
mark [56]. For the semantic segmentation results on Pascal
VOC and Cityscapes datasets, we use the public server for
the evaluation.

ImageNet Pretraining.
Similar as in the prior work [51,
53], we modify the standard ResNet [17] by replacing the
ﬁrst 7×7 convolution with 3 consequent 3×3 convolution.
We follow the best practise of ImageNet training [19] to
train our base networks. The Top-1 and Top-5 accuracy on
ImageNet validation set using center crop with the crop size
of 224×224 and 320×320 are shown in Table 1. The pre-
trained models will be included in the public code system.

4.2. Abalation Study on Pascal Context

Pascal Context dataset [33] is a scene parsing dataset,
containing the semantic labels for the entire image, with
4,998 training and 5,105 validation images. Following the
practice in prior work [5, 26, 33, 51], we use the 59 most
frequent categories for this benchmark and consider all the
other classes as background.

Ablation Study of CFNet. We ﬁrst break down the im-
provements of CFNet over FCN, by conducting a set of ex-
periments by adding individual components step-by-step to
the baseline FCN. We use 4 mixtures and 2 multiheads in
ACF Module with atrous-rate of 4 for the transformation Φ
in this study. The baseline FCN achieves 76.3% pixAcc and
46.3% mIoU. Adding the ACF Module improves the pix-
Acc and mIoU by 2.7% and 3.6%. Including global pooling
feature yields 0.9% boost in mIoU. Further improvements
are from adding Context Encoding Module [51] and using
deeper base network (See results in Table 2).

Ablation Study of ACF Module. To explore the best per-
formance of Aggregated Co-occurrent Feature Module, we
conduct the experiments with different hyper-parameters
and settings. We ﬁrst study different instantiations of the
transformation Φ using different feed-forward network ar-
chitectures and empirically ﬁnd using the atrous rate of 4
gives best performance (detailed study in the supplemen-
tary material). We also explore the inﬂuence of contextual
prior and multi-heads in the ACF Module in Table 3. To
keep the comparison fair, we reduce the feature dimension,
when increasing the number of mixtures or the number of
multi-heads, so that the total computation of ACF Module
remains roughly the same. Varying the number of mixtures
K for contextual prior and the number of muli-heads H in
ACF Module, we can see that using contextual prior signif-
icantly improves the expressiveness of the Softmax model,

and empirically ﬁnd K=2 and H=4 gives the best perfor-
mance.

State-of-the-art Comparisons. We consider the back-
ground as one of the categories in order to compare with
prior work (60 classes in total). The results are shown in
Table 4. CFNet with ResNet-50 already outperforms most
of the previous work even using much shallower base net-
work. CFNet (ResNet-101) achieves 54.0% mIoU on vali-
dation set, which surpasses other approaches by a large mar-
gin even without using deeper base network or COCO pre-
training.

4.3. Results on Pascal VOC 2012

Pascal VOC 2012 segmentation dataset [12] is one of the
gold-standard benchmarks for object segmentation. Follow-
ing the work [5, 53], we utilize the augmented set [14] with
10,582, 1,449 and 1,456 images in training, validation and
test set. The CFNet is ﬁrst trained on the train + val sets on
the augmented set and then ﬁnetuned on the original Pascal
VOC 2012 images as in previous work [51]. For fair com-
parision with prior work, we use ResNet-101 as the base
network. CFNet-101 achieves 84.2% mIoU5 on the test set,
which outperforms all the previous work without COCO
pre-training and achieves superior performance on most of
the categories. State-of-the-art approaches typically pre-
train the network using MS-COCO dataset [28]. We follow
the prior work [6, 51] to generate semantic segmentation
mask by merging the instance labels for the 20 categories
shared with Pascal VOC 2012 dataset, and discard the la-
bels for the other categories, which results in around 90K
images with more than 1000 labeled pixels (from the train-
ing set of MS-COCO 2017). We ﬁrst pre-train the CFNet on
COCO dataset using learning rate of 0.004 and then ﬁnetune
on the augmented and original training set. CFNet achieves
the best result of 87.2%6 on the test set. The per-class com-
parision is shown in Table 5, and CFNet achieves superior
performance on many categories. (The entries using larger
base model such as Xeption, or extra than COCO [28] &
ImageNet [10] data for pre-training are not included in this
benchmark [7, 25, 44].)

4.4. Results on ADE20K

ADE20K dataset [56] is a large scale scene parsing
benchmark with 150 object and stuff categories, containing
20K training, 2K validation and 3K test images. We ﬁrst
train the baseline FCN and CFNet on the training set and
evaluate the models on the validation set (results are shown
in Table 6). Our baseline FCN using ResNet-50 achieves
39.28% mIoU using good pre-trained base network and
multi-size evaluation. CFNet outperforms FCN by more

5

6

http://host.robots.ox.ac.uk:8080/anonymous/ZFDFXP.html

http://host.robots.ox.ac.uk:8080/anonymous/SOWG4O.html

554

Method

aero

bike

bird

boat bottle

bus

car

cat

chair

cow table

dog horse mbike person plant sheep sofa

train

tv mIoU

76.8 34.2 68.9 49.4
FCN [31]
84.4 54.5 81.5 63.6
DeepLabv2 [4]
CRF-RNN [55]
87.5 39.0 79.7 64.2
DeconvNet [34] 89.9 39.3 79.7 63.9
85.2 43.9 83.3 65.2
GCRF [43]
DPN [30]
87.7 59.4 78.4 64.9
Piecewise [27]
90.6 37.6 80.0 67.8
94.4 72.9
ResNet38 [47]
94.9 68.8
71.9 94.7 71.2
91.8
PSPNet [53]
76.7
94.1 69.2 96.3
EncNet [51]
CFNet (ours)5
95.7
71.9 95.0
76.3

CRF-RNN [55]
Dilation8 [50]
DPN [30]
Piecewise [27]
DeepLabv2 [5]
ReﬁneNet [26]
ResNet38 [47]
PSPNet [53]
DeepLabv3 [6]
EncNet [51]
CFNet (ours)6

90.4 55.3 88.7 68.4
91.7 39.6 87.8 63.1
89.0 61.6 87.7 66.8
94.1 40.7 84.1 67.8
92.6 60.4 91.6 63.4
95.0 73.2 93.5 78.1
96.2 75.2 95.4
74.4
95.8 72.7
95.0 78.9
96.4 76.6 92.7 77.8
76.9 94.2 80.2
95.3
96.9 79.7 94.3 78.4

60.3
65.9
68.3
68.2
68.3
70.3
74.4
78.4
75.8
86.2
82.8

69.8
71.8
74.7
75.9
76.3
84.8
81.7
84.4
87.6
85.2
83.0

90.0 95.9 37.1 92.6

75.3 74.7 77.6 21.4 62.5 46.8 71.8
85.1 79.1 83.4 30.7 74.1 59.8 79.0
87.6 80.8 84.4 30.4 78.2 60.4 80.5
87.4 81.2 86.1 28.5 77.0 62.0 79.0
89.0 82.7 85.3 31.1 79.5 63.3 80.5
89.3 83.5 86.1 31.7 79.9 62.6 81.9
92.0 85.2 86.2 39.1 81.2 58.9 83.8
90.0 92.1 40.1
90.4 71.7 89.9
90.6
95.2 89.9 95.9 39.3
90.7 71.7
90.5
90.7 73.3 90.0
96.3 90.7 94.2 38.8
73.0 93.4
94.8
With MS-COCO Pre-training
88.3 82.4 85.1 32.6 78.5 64.4 79.6
89.7 82.9 89.8 37.2 84.0 63.0 83.3
91.2 84.3 87.6 36.5 86.3 66.1 84.4
93.4 84.3 88.4 42.5 86.4 64.7 85.4
95.0 88.4 92.6 32.7 88.5 67.6 89.6
95.6 89.8 94.1 43.7 92.0 77.2 90.8
93.7 89.9 92.5 48.2 92.0 79.9 90.1
92.0 95.7 43.1 91.0
94.7
80.3 91.3
96.7
90.2 95.4 47.5 93.4 76.3 91.4
96.5 90.8 96.3 47.9
80.0 92.4
79.6 93.6
96.7

93.9
91.6 96.7 50.1 95.2

63.9
76.1
77.8
80.3
79.3
80.0
83.9
93.7
94.5
92.5
94.6

81.9
89.0
87.8
89.0
92.1
93.4
95.5
96.3
97.2
96.6
97.2

76.5
83.2
83.1
83.6
85.5
83.5
84.3
91.0
88.8
88.8
89.6

86.4
83.8
85.6
85.8
87.0
88.6
91.8
92.3
91.0
90.5
94.2

73.9
80.8
80.6
80.2
81.0
82.3
84.8
89.1
89.6
87.9
88.4

81.8
85.1
85.4
86.0
87.4
88.1
91.2
90.1
92.1
91.5
91.7

45.2
59.7
59.5
58.8
60.5
60.5
62.1
71.3
72.8
68.7
74.9

58.6
56.8
63.6
67.5
63.3
70.1
73.0
71.5
71.3
70.8
78.4

72.4
82.2
82.8
83.4
85.5
83.2
83.2
90.7
89.6
92.6
95.2

82.4
87.6
87.3
90.2
88.3
92.9
90.5
94.4
90.9
93.6
95.4

37.4 70.9 55.1
50.4 73.1 63.7
47.8 78.3 67.1
54.3 80.7 65.0
52.0 77.3 65.1
53.4 77.9 65.0
58.2 80.8 72.3
87.7 78.1
61.3
64.0
85.1 76.3
59.0 86.4 73.4
63.2 89.7 78.2

53.5 77.4 70.1
56.0 80.2 64.7
61.3 79.4 66.4
63.8 80.9 73.0
60.0 86.8 74.5
64.3 87.7 78.8
65.4 88.7 80.6
82.0
66.9 88.8
68.9 90.8 79.3
66.5 87.7 80.8
69.6
90.0 81.4

62.2
71.6
72.0
72.5
73.2
74.1
75.3
82.5
82.6
82.9
84.2

74.7
75.3
77.5
78.0
79.7
84.2
84.9
85.4
85.7
85.9
87.2

Table 5: Per-class results on PASCAL VOC 2012 testing set. CFNet-101 outperforms existing approaches and achieves
84.2% and 87.2% mIoU w/o and w/ pre-training on COCO dataset. (The best two entries in each columns are marked in gray
color. Note: the entries using larger base networks or extra data are not included [7, 25, 44].)

Method

BaseNet mIoU%

ReﬁneNet [26]
UperNet [48]
PSPNet [53]
DSSPN [24]
SAC [52]
EncNet [51]
FCN (baseline)
CFNet (ours)
CFNet (ours)

Res152
Res101
Res101
Res101
Res101
Res101
Res50
Res50
Res101

40.7
42.66
43.29
43.68
44.30
44.65
39.28
42.87
44.89

Table 6: Results on ADE20K validation set. CFNet-101
outperforms all previous methods in mIoU using same base
network.

frames (ﬁne data) and 20K weakly annotated ones (coarse
data). We only use the ﬁne data in this experiment with
2,975, 500 and 1,525 number of images for training, valida-
tion, and testing. 19 object/stuff categories are used in the
evaluation. We use ResNet-101 as the base network, and
train our CFNet on the training set using 768 crop size, then
evaluate it on the validation set. CFNet achieves 79.56%
mIoU on the validation set. For performance on test set, we
retrain CFNet-101 on train and validation set and submit
the prediction on test set to the evaluation server. CFNet
achieves 79.60% mIoU (IoU classes) on the test set only
using ﬁne-label data. We have not used online hard exam-
ple mining (OHEM) strategy in this experiment, which can
further improve the performance.

5. Conclusion

than 3.5% mIoU using same base network. CFNet-101
achieves 44.89% mIoU and outperforms all previous meth-
ods in mIoU using the same base network. Visual compar-
ison examples are shown in Figure 2. The proposed CFNet
successfully captures and utilizes the semantic dependen-
cies of the co-occurrent features across the entire image for
making predictions, while the baseline FCN only utilizes
the local feature representations.

4.5. Results on Cityscapes Dataset

Cityscapes dataset [9] is a high-resolution city street
scene parsing dataset, including 5K high-quality labeled

To capture and utilize the co-occurrent features, we
introduce a Co-occurrent Feature Model, which predicts
the probability distribution of co-occurrent features for the
given target. To further utilize co-occurrent features in
semantic segmentation, we introduce an Aggregated Co-
occurrent Feature Module to aggregate the co-occurrent
context of the co-occurrent features. The proposed ap-
proach outperforms existing contextual modules achieving
superior performance on gold-standard semantic segmenta-
tion benchmarks. We expect the co-occurrent feature rep-
resentation and our state-of-the-art implementations will be
beneﬁcial to the segmentation work in the community.

555

References

[1] A. Arnab, S. Jayasumana, S. Zheng, and P. H. Torr. Higher
order conditional random ﬁelds in deep neural networks. In
European Conference on Computer Vision, pages 524–540.
Springer, 2016. 6

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv preprint arXiv:1511.00561, 2015. 5

[3] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm
for image denoising. In Computer Vision and Pattern Recog-
nition, 2005. CVPR 2005. IEEE Computer Society Confer-
ence on, volume 2, pages 60–65. IEEE, 2005. 4

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 8

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv:1606.00915, 2016. 5, 6, 7, 8

[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-
thinking atrous convolution for semantic image segmenta-
tion. arXiv preprint arXiv:1706.05587, 2017. 1, 2, 4, 5, 7,
8

[7] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and
H. Adam. Encoder-decoder with atrous separable convo-
lution for semantic image segmentation.
arXiv preprint
arXiv:1802.02611, 2018. 7, 8

[8] M. Cimpoi, S. Maji, and A. Vedaldi. Deep ﬁlter banks for
texture recognition and segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3828–3836, 2015. 5

[9] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 3213–3223, 2016. 6, 8

[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09, 2009. 5, 7

[11] V. Dumoulin and F. Visin. A guide to convolution arithmetic
for deep learning. arXiv preprint arXiv:1603.07285, 2016. 5

[12] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010. 2, 6, 7

[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 580–587,
2014. 5

[14] B. Hariharan, P. Arbel´aez, L. Bourdev, S. Maji, and J. Malik.

Semantic contours from inverse detectors. 2011. 7

[15] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Si-
multaneous detection and segmentation. In European Con-
ference on Computer Vision, pages 297–312. Springer, 2014.
5

[16] Z. S. Harris. Distributional structure. Word, 10(2-3):146–

162, 1954. 2

[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
European Conference on Computer Vision, pages 346–361.
Springer, 2014. 2, 5, 7

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. arXiv preprint arXiv:1512.03385,
2015. 2, 5, 6

[19] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li.
Bag of tricks to train convolutional neural networks for im-
age classiﬁcation. arXiv preprint arXiv:1812.01187, 2018.
7

[20] J.-J. Hwang, T.-W. Ke, J. Shi, and S. X. Yu. Adversar-
ial structure matching loss for image segmentation. arXiv
preprint arXiv:1805.07457, 2018. 5

[21] T.-W. Ke, J.-J. Hwang, Z. Liu, and S. X. Yu. Adaptive afﬁnity
ﬁelds for semantic segmentation. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 587–
602, 2018. 5

[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 1

[23] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 1

[24] X. Liang, H. Zhou, and E. Xing. Dynamic-structured seman-
tic propagation network. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
752–761, 2018. 8

[25] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang.
Multi-scale context intertwining for semantic segmentation.
In Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 603–619, 2018. 6, 7, 8

[26] G. Lin, A. Milan, C. Shen, and I. Reid. ReﬁneNet: Multi-
path reﬁnement networks for high-resolution semantic seg-
mentation. In CVPR, July 2017. 5, 6, 7, 8

[27] G. Lin, C. Shen, A. van den Hengel, and I. Reid. Efﬁ-
cient piecewise training of deep structured models for se-
mantic segmentation.
In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pages
3194–3203, 2016. 6, 8

[28] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European Conference on Com-
puter Vision, pages 740–755. Springer, 2014. 7

[29] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking
wider to see better. arXiv preprint arXiv:1506.04579, 2015.
1, 4, 5, 6

[30] Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang. Semantic im-
age segmentation via deep parsing network. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 1377–1385, 2015. 8

[31] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015. 1, 2, 5, 6, 8

556

[47] Z. Wu, C. Shen, and A. v. d. Hengel. Wider or deeper: Revis-
iting the resnet model for visual recognition. arXiv preprint
arXiv:1611.10080, 2016. 8

[48] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun. Uniﬁed
perceptual parsing for scene understanding. arXiv preprint
arXiv:1807.10221, 2018. 8

[49] Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Break-
ing the softmax bottleneck: A high-rank rnn language model.
arXiv preprint arXiv:1711.03953, 2017. 3, 4

[50] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. arXiv preprint arXiv:1511.07122, 2015.
5, 8

[51] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal. Context encoding for semantic segmentation.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018. 1, 6, 7, 8

[52] R. Zhang, S. Tang, Y. Zhang, J. Li, and S. Yan. Scale-
adaptive convolutions for scene parsing. In Proc. 26th Int.
Conf. Comput. Vis., pages 2031–2039, 2017. 8

[53] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
parsing network.
In Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2017. 1,
2, 4, 5, 6, 7, 8

[54] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and
J. Jia. PSANet: Point-wise spatial attention network for
scene parsing. In ECCV, 2018. 6

[55] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional random
ﬁelds as recurrent neural networks.
In Proceedings of the
IEEE International Conference on Computer Vision, pages
1529–1537, 2015. 5, 6, 8

[56] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-
ralba. Scene parsing through ade20k dataset. In Proc. CVPR,
2017. 2, 3, 6, 7

[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In Advances in neural informa-
tion processing systems, pages 3111–3119, 2013. 2

[33] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-
dler, R. Urtasun, and A. Yuille. The role of context for object
detection and semantic segmentation in the wild. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 891–898, 2014. 2, 6, 7

[34] H. Noh, S. Hong, and B. Han. Learning deconvolution net-
work for semantic segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1520–
1528, 2015. 5, 8

[35] A. P. Parikh, O. T¨ackstr¨om, D. Das, and J. Uszkoreit. A de-
composable attention model for natural language inference.
arXiv preprint arXiv:1606.01933, 2016. 5

[36] T. Pohlen, A. Hermans, M. Mathias, and B. Leibe. Fullreso-
lution residual networks for semantic segmentation in street
scenes. arXiv preprint, 2017. 5

[37] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora,
and S. Belongie. Objects in context.
In Computer vision,
2007. ICCV 2007. IEEE 11th international conference on,
pages 1–8. IEEE, 2007. 5

[38] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015. 5

[39] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost
for image understanding: Multi-class object recognition and
segmentation by jointly modeling texture, layout, and con-
text. International Journal of Computer Vision, 81(1):2–23,
2009. 5

[40] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neu-
ral networks from overﬁtting. Journal of machine learning
research, 15(1):1929–1958, 2014. 4

[41] A. Torralba. Contextual priming for object detection. Inter-
national journal of computer vision, 53(2):169–191, 2003.
5

[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need.
In Advances in Neural Information Processing
Systems, pages 5998–6008, 2017. 4, 5

[43] R. Vemulapalli, O. Tuzel, M.-Y. Liu, and R. Chellapa. Gaus-
sian conditional random ﬁeld network for semantic segmen-
tation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3224–3233, 2016. 8

[44] G. Wang, P. Luo, L. Lin, and X. Wang. Learning object in-
teractions and descriptions for semantic image segmentation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5859–5867, 2017. 7, 8

[45] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural
networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 5

[46] Z. Wu, C. Shen, and A. v. d. Hengel. Bridging category-
level and instance-level semantic image segmentation. arXiv
preprint arXiv:1605.06885, 2016. 6

557

