TopNet: Structural Point Cloud Decoder

http://completion3d.stanford.edu

Lyne P. Tchapmi1

Vineet Kosaraju1

S. Hamid Rezatoﬁghi1,2

Ian Reid2

Silvio Savarese1

1Stanford University, 2The University of Adelaide, Australia

Abstract

3D point cloud generation is of great use for 3D scene
modeling and understanding. Real-world 3D object point
clouds can be properly described by a collection of low-
level and high-level structures such as surfaces, geometric
primitives, semantic parts, etc. In fact, there exist many dif-
ferent representations of a 3D object point cloud as a set of
point groups. Existing frameworks for point cloud genera-
tion either do not consider structure in their proposed solu-
tions, or assume and enforce a speciﬁc structure/topology,
e.g. a collection of manifolds or surfaces, for the gener-
ated point cloud of a 3D object.
In this work, we pro-
pose a novel decoder that generates a structured point cloud
without assuming any speciﬁc structure or topology on the
underlying point set. Our decoder is softly constrained to
generate a point cloud following a hierarchical rooted tree
structure. We show that given enough capacity and allow-
ing for redundancies, the proposed decoder is very ﬂexible
and able to learn any arbitrary grouping of points including
any topology on the point set. We evaluate our decoder on
the task of point cloud generation for 3D point cloud shape
completion. Combined with encoders from existing frame-
works, we show that our proposed decoder signiﬁcantly out-
performs state-of-the-art 3D point cloud completion meth-
ods on the Shapenet dataset.

1. Introduction

Generating 3D point clouds using neural networks is in-
creasingly studied for various applications such as 3D re-
construction [9, 13, 37], object point cloud completion [38,
13], and representation learning for point clouds [37, 1, 13].
This work focuses on the common sub-task of producing a
complete 3D point cloud shape, given a feature vector rep-
resenting the shape. The input feature can originate from
various inputs (such as images [9], 2.5D sketches [35], point
clouds [23], etc.), but the scope of evaluation in this work
focuses on the task of 3D object shape completion, where
the input is a partial 3D point cloud and the desired output

Figure 1: Our proposed decoder generates point clouds accord-
ing to a tree structure where each node of the tree represents a
subset of the point cloud. The embedded point cloud structure is
shown by visualizing nodes in the decoder, as a collection of all
its descendant points. We show selected point grouping patterns
emerging from our structural decoder for several object classes.

is the ground-truth completed point cloud. The ability to
infer the full shape of an object from an incomplete scan
acquired by depth camera or LiDAR is indeed an important
task which enables several downstream applications such as
robotics manipulation [31], scene understanding for navi-
gation [7], and virtual manipulation of completed shapes
[8].

Although moderate success has been achieved with per-
forming shape completion on regular representations of 3D
objects such as distance ﬁelds, meshes, and voxel grids
[3, 6, 22, 28, 16, 14], these methods fall short due to in-
efﬁciency of such representations. Recent progresses in
developing highly efﬁcient and powerful point cloud en-
coders [23, 21, 24, 20, 18], have made point clouds a highly
promising representation for 3D object generation and com-
pletion using neural networks. Several state-of-the-art ap-

1383

proaches applicable to shape completion focus on point-
cloud based shape completion [38, 13, 37]. The dominant
paradigm in these frameworks is to use a point cloud en-
coder to embed the partial point cloud input [21], and de-
sign a decoder to generate a completed point cloud from the
embedding of the encoder.

The key focus in most existing point cloud generation
approaches is on the representation of 2D or 3D objects
and also designation of a relevant point cloud based de-
coder for the proposed representation. The earliest method
uses a parallel multilayer perceptron network and a de-
convolution network to decode the latent feature generated
by a 2D encoder [9] while using a permutation invariant
loss such as the earth mover’s ditance [26, 9] or Chamfer
loss [9] to deal with the orderless nature of point clouds.
However, this framework does not explicitly consider any
topology or structure that naturally exists in real-world 3D
objects. The recent successful approaches concentrate on
the designation of decoders which generate structured point
clouds [38, 13, 37]. For example in [13], it is assumed
the point clouds of a 3D object lie on a 2-manifold, for-
mally deﬁned as a topological space that locally resembles
the Euclidean plane. Therefore, the proposed decoder is en-
forced to learn a set of mappings or deformations from the
euclidean plane to the object point cloud. Imposing these
structures into the learning process may result in superior
performance in generating 3D object point clouds compared
to the approaches ignoring structure. However what is usu-
ally ignored is the potential impact that a speciﬁc represen-
tation for grouping the point clouds may have on a learning
process that uses an unstructured (i.e. permutation invari-
ant) loss. Enforcing a single speciﬁc structure during learn-
ing may not be optimal for training as the space of possible
solutions is constrained.

To address this issue, we propose a more general decoder
that can generate structured point clouds by implicitly mod-
eling the point cloud structure in a rooted tree architecture.
We show that given enough capacity and allowing for re-
dundancies, a rooted tree allows us to model structure, in-
cluding any topology on the point set, making our decoder a
more general structured decoder. Since structure is only im-
plicitly modeled in our decoder, our model is not bound to a
pre-determined structure, and therefore can embed arbitrary
structure and/or topologies on the point set, as in Fig. 1.

More speciﬁcally for the shape completion task, we em-
bed the partial input point cloud as a feature vector or code
which is used by our proposed decoder to generate a com-
pleted point cloud. Our decoder has a rooted tree structure
in which the root node embeds the entire point set as the en-
coder feature vector, the leaf nodes are individual points in
the point set, and each internal node embeds a subset of the
entire point cloud made of all its descendant leaves. The set
of point cloud subsets represented by the tree nodes deﬁnes

the generated point cloud structure or topology. This model
choice is inspired by the formal deﬁnition of topology on
ﬁnite discrete point sets as detailed in Section 4.

We evaluate our proposed decoder on the Shapenet
dataset and show a 34% relative improvement over the next-
best performing methods for the task of point cloud shape
completion1. Visualizations of the nodes of our tree de-
coder reveals various non-identical patterns learned by our
decoder.

The main contributions of the paper are summarized as

follows:

• We propose a novel way to model arbitrary struc-
ture/topology on a point cloud using a rooted tree in
which each node embeds a deﬁning element of the
structure.

• We design a novel tree decoder network for point cloud
generation and completion which generates arbitrarily
structured point clouds without explicitly enforcing a
speciﬁc structure.

• We show an intuitive visualization of the structure
learned by our decoder by visualizing a node in the
tree decoder as a set of all its descendants.

• Finally, our network sets a new state-of-the-art on ob-
ject point cloud completion by more than 30% im-
provement over the next-best performing approach.

2. Related Works

Our work follows a long line of frameworks on shape
completion which leverage various representations. We re-
view a subset of these approaches among those leveraging
neural networks.

Volumetric 3D shape completion: Earlier and some
recent works on shape completion leveraging neural net-
works favored voxel grids, and distance ﬁelds representa-
tions [6, 15, 29, 27, 18] which are well suited for process-
ing with 3D convolutional neural networks. These works
have shown great success in the tasks of 3D Reconstruc-
tion [5, 11], shape completion [6, 14, 36, 19], and shape
generation from embeddings [3]. However voxel grids re-
quire large footprints and early works operate on low di-
mensional grids. This issue has been addressed using sparse
representations [30, 32, 33, 17, 12, 25] which makes pro-
cessing voxels more efﬁcient. However the process of vox-
elization still introduces a quantization effect which dis-
cards some details of the data [34] and is not suitable for
representing ﬁne-grained information. To avoid this limita-
tion, recent works generate 3D shapes in point cloud space
which is naturally able to represent ﬁne-grained details, and

1The code for this project and an associated object point cloud com-
pletion benchmark with all evaluated methods are available at http:
//completion3d.stanford.edu.

384

several of these works show superior performance to voxel-
based methods [38, 9, 10].

Multiresolution Point Cloud Generation with Neural
Networks: A few works on point cloud generation con-
sider or introduce a multi-resolution structure in the process
of point cloud generation. Yuan et al. [38] generate point
clouds in 2 stages where the ﬁrst stage is a lower resolution
point cloud and the 2nd stage is the ﬁnal output. Gadelha
et al. [10] represents a point cloud as a 1D list of spatially
ordered points, and generates a point cloud through a tree
network in which each branch aims at representing different
resolutions of the point cloud which are connected through
multiresolution convolution. These works while highly per-
formant, constrain the network to focus on the multiresolu-
tion structure in the point cloud.

Unstructured Point Cloud Generation with Neural
Networks: Fan et al. [9] proposed one of the earliest works
in the literature addressing the task of generating 3D point
clouds from an input image. They proposed an architecture
made of an encoder which encodes the input into a embed-
ding, and a decoder which generates the point cloud from
the embedding. The decoder they propose is a 2 branch ar-
chitecture, one multilayer perceptron (MLP) branch and one
deconvolutional branch. They also introduce the Chamfer
loss, a differentiable loss for point cloud comparison which
we leverage in our work. However, the output generated
by their method is an unstructured point cloud, while real-
world object point clouds are structured and can be repre-
sented as a collection of subsets, e.g. surfaces and parts.
Recent approaches based on imposing one of the structured
representations, have shown to be superior in generating
point clouds of real-world objects. We review them next.

Manifold-based Point Cloud Generation with Neural
Networks: Several state-of-the-art methods on point cloud
generation and completion generate structured point clouds
by assuming and enforcing a 2-manifold topology on the
point cloud. Groueix et al. [13] design a decoder that learns
a manifold by learning a mapping from the Euclidean plane
to the ground-truth point cloud. Yang et al. [37] and Yuan
et al. [38] also learn to generate a point cloud structured as
a manifold through a series of deformation (folding) oper-
ations on the Euclidean plane. While several point clouds
are indeed derived from sampling manifolds, they exhibit
several other structures or topological representations that
can be leveraged during training. Therefore enforcing a
speciﬁc structured representation may constrain the learn-
ing process by limiting the solution search space. To avoid
this limitation, we propose a decoder which is able to rep-
resent arbitrary structures and topologies on a point set.
The decoder has a rooted tree structure in which each node
of the tree represents and generates a subset of the point
cloud deﬁning the point cloud structure. Unlike [13, 37, 38]
which speciﬁcally enforce their decoder to generate a mani-

fold, we do not enforce our decoder to generate any speciﬁc
topology which increases the space of potential topologies
that can be generated by the decoder. Visualization of struc-
tural patterns learned by our decoder suggests that the de-
coder learns patterns which are not necessarily traditional
2-manifolds but occur across several objects.

3. Background and Motivation

We ﬁrst provide some background on concepts relating

to object structure and topology.

2-manifolds and surface topology: Object structure is
commonly modeled as a surface or 2-manifold which for-
mally is a topological space that locally resembles the Eu-
clidean plane. This implies that 2-manifolds have a local
smoothness property. Previous works have attempted to
explicitly enforce this property by learning mappings from
smooth 2D grids to local surfaces of 3D objects[13, 37, 38].
While enforcing local smoothness may be helpful for learn-
ing explicitly smooth representations such as meshes, this
assumption may be less relevant for point clouds due to their
discrete nature which allows for various potentially more
suitable non-smooth representations.

Topology on discrete point sets: Unlike 2-manifold
representations, we do not leverage the local smoothness
assumption due to the discrete nature of point clouds. In-
stead we leverage one of the multiple more general equiva-
lent deﬁnitions of topological space on ﬁnite discrete point
sets which for a set S is deﬁned as a nonempty collection of
subsets of S [2](see Section 4). Note that this deﬁnition of
topology is signiﬁcantly less constrained than 2-manifolds
and does not impose restrictions on smoothness or point
neighborhoods within the topology. Leveraging this deﬁni-
tion allows us to design a point cloud decoder which is less
constrained than previous topological point cloud decoders.
Indeed, we simply design a decoder which is able to group
the point cloud into subsets deﬁning the point cloud struc-
ture. The intuition behind designing a decoder that groups
a point cloud into subsets is that if a topology - deﬁned as
a collection of nonempty subsets is adequate for the learn-
ing process, then the decoder has the ability to generate the
point cloud according to that topology by adequately group-
ing points.

Designing Topological Decoders: Given the more gen-
eral deﬁnition of topology on discrete ﬁnite point sets as a
collection of subsets, how can we design a decoder capa-
ble of generating collections of subsets for a set S? One
straightforward approach is to train N multilayer percep-
trons to generate separate point cloud subsets and merge
them into the ﬁnal point cloud. This method trivially scales
poorly the larger the size of the topology or structure de-
ﬁned on the point set. We build and improve on this basic
idea and instead propose a decoder modeled as a hierarchi-
cal rooted tree in which each node of the tree represents a

385

subset of the point cloud and the root of the tree represents
the entire point cloud (Fig. 1). This rooted tree architecture
has several appealing properties including its ability to rep-
resent arbitrary topologies and structure on discrete point
sets. The hierarchical nature of the decoder is also a more
efﬁcient and compact representation since parts of the neu-
ral network are shared in generating overlapping subsets of
the point cloud (see Section 5). Next, we present the the-
oretical foundation of our work starting with a formal deﬁ-
nition of topology on discrete point sets, and we show that
our proposed rooted tree structure is adequate to represent
several point cloud structures including any arbitrary topol-
ogy.

4. Topology Representation

In this work, we leverage a general deﬁnition of topol-
ogy on point sets and propose a decoder that generates
a structured point cloud. To this end, we ﬁrst provide a
general deﬁnition of a topological space on point sets.
There exists several equivalent deﬁnitions of topological
spaces, and for our purposes, we use the following [2]:

Deﬁnition: Assume S = {s1, s2, · · · , sn} is a set of
points, where si ∈ Rd is a point in d dimensional space,
and T = {S1, S2, · · · , Sk} is a collection of open subsets
of S. Then, T is a topology on S if:

1. The empty set ∅ and S are open,
2. The intersection of any ﬁnite number of subsets of T is

in T, i.e. ∩∀iSi ∈ T,

3. The union of an arbitrary number of subsets of T is in

T, i.e. ∪∀iSi ∈ T,

Any ﬁnite point set can be deﬁned as open and through-
out this work we use that assumption, such that ∅, S, and
all subsets of S are open. From the above deﬁnition, it be-
comes apparent that topology on a ﬁnite discrete point set
can be generally conceived as a collection of subsets of the
point set. Therefore a decoder modeling topology must be
able to generate or model groups of points in order to rep-
resent a topology on the point set. Among several possibil-
ities to accomplish the task of generating a point cloud as
a group of points, we choose to use a rooted tree topology
for two main reasons. The ﬁrst reason for our choice is that
assuming enough capacity and allowing for redundancies,
any topology T on a point set S can be represented as a
rooted tree as shown in Proposition 1. The second reason
for our choice is that any rooted tree with at least three non
leaf nodes can embed at least 2 topologies (Proposition 2)
meaning this representation regardless of capacity, (as long
as there are at least 3 non leaf nodes) can quantitatively en-
code more topologies than previous works in which a single
pre-determined topology is assumed. We now go into the
details and proof of the two propositions above.

Proposition 1: Any topology T on a point set S can be

modeled as a rooted tree structure G in which:

– Every leaf of G represents a singleton {s} where s is

an individual point s ∈ S (P1a)

– Each non-leaf node G(Si) in the tree represents a non-

empty element Si ∈ T (P1b)

– For any pair of nodes G(Si), G(Sj) in G, if G(Sj) is

a child of G(Si), then Si ⊆ Sj (P1c)

Proof by existence: We want to show that for each topology
T on a set S, there exists at least one rooted tree structure
G satisfying the conditions of Proposition 1. Lets deﬁne
T∗ = T ∪ {{s} : s ∈ S}. We create a graph G as follows:

– For each individual point s ∈ S and each non-empty
set Si ∈ T, we create a representative node G(s),
G(Si) in G. (C1a)

– For each non-empty Si, Sj ∈ T∗, if Si * Sj , we add a

directed edge E(Si, Sj) from Si to Sj in G (C1b)

– Since ∀Si ∈ T∗/S, Si * S, E(Si, S) is an edge in
G and we designate G(S) as the root of our graph G
(C1d)

By C1a, all non-empty subsets of T are represented by a
node in G and we now show that G satisﬁes the conditions
in Proposition 1.

P1a proof: Let Gl be a leaf in G representing a subset
Si ⊆ S . By C1a, Si is non-empty which means ∃s ∈
S, s ∈ Si. Therefore s ⊆ Si. By contradiction, lets assume
Si is not a singleton. By C1b, E(Si, {s}) is a directed edge
G which means Gl is not a leaf which is a contradiction. Si
must therefore be a singleton.

P1b proof: This is a direct consequence of the deﬁnition

in C1a

P1c proof: This is a direct consequence of the converse

of C1d
Corollary 1: The set of all leaf descendants of a given node
in G(Si) in G is equal to Si. Based on this corollary, we
can visualize individual nodes in a point set's topological
tree by visualizing all its descendants leaves.

Proposition 2: Given a structured point cloud, repre-
senting a set of points S, and a rooted tree graph G with at
least 3 non-leaf nodes, and the properties deﬁned in Propo-
sition 1, there exist more than one topology that can be rep-
resented by G.

Proof: This proposition can also be proven by exam-
ple. There are two trivial topologies that can be represent
in a rooted tree G of at least 3 non-leaf nodes. The ﬁrst of
which being T = {∅, S} which can be represent in G by
choosing the root node and every other node in the tree to
represent S (our representation allows for duplicate nodes).
The empty set is represented implicitly. The second trivial
topology that can be represented by G is T = {∅, S, S1, S2}

386

cludes an encoder as a ﬁrst stage.

5.1. Design of Rooted Tree Decoder

Our proposed decoder architecture has a rooted tree
structure in which the root node embeds and processes the
global point cloud embedding representing a 3D shape. A
collection of multilayer perceptrons arranged in a tree struc-
ture are used to learn embeddings of child nodes from par-
ent node embeddings concatenated with the global embed-
ding at each node. The decoder architecture is illustrated in
Fig. 2. Our decoder’s architecture features a root node N0
which takes the feature vector from the encoder and uses
M1 MLPs to generate M1 feature vectors of dimension C
corresponding to M1 children node at level 1 of the tree.
Subsequently , the feature vector of each node at level i ≥ 1
of the tree is concatenated with the global feature produced
by the encoder and further processed by Mi+1 MLPs to pro-
duce Mi+1 children features per node for the next level i+1.
All nodes at a given level i are processed by the same Mi
shared MLPs. At the last level of the tree, the feature vec-
tors generated for each leaf node have dimension C = 3,
and represent the individual points in the output point cloud.

Design analysis: In Section 4, we demonstrated that any
topology T on a set S can be embedded in a rooted tree in
which each node of the tree represents a non-empty subset
of T or a singleton of S. We proved this hypothesis for
any arbitrary rooted tree. However, the decoder D which
we propose here, is a speciﬁc rooted tree structure with the
following features:

– Every node in D has at most one parent
– All nodes at the same level have the same number of

children

– All leaves are at the same level.

While G is not guaranteed to meet the conditions above, it
can be transformed into a new equivalent tree G′ for which
the conditions above are met by performing a series of node
duplication on G. We provide more details in the supple-
mentary materials. Our decoder in this case can be seen as
modeling G′ rather than G which implies potential dupli-
cation of nodes and points. This is acceptable since dupli-
cation of points in a point set does not change the point set
overall.

5.2. Point Cloud Loss

The point cloud generated by our decoder needs to be
compared against the ground-truth for learning. An ideal
loss must be differentiable and invariant to the permutation
of point clouds in both target S and ground-truth SG. The
Chamfer distance between S, SG ⊆ R3 proposed by Fan et

387

Figure 2: Model Architecture: Our point completion framework
comprises a 2-stage point cloud encoder, and a tree-structured de-
coder. The arrows of the decoder are multilayer perceptron net-
works (MLP). Similarly colored MLP share the same parameters.

where S2 is the complement of S1 in S1 i.e. S2 = S/S1.
This topology can be represented by G by assigning the
root node to represent S and the ﬁrst 2 children nodes of
G to represent S1 and S2, concluding our proof.

In summary, our propositions show that it is possible to
represent structure/topology on a discrete and ﬁnite point
set using a rooted tree in which every node of the tree repre-
sents an element of the topology and the edges of tree rep-
resent a subset relationship from child to parent. While this
representation of topology is not the only way to represent
a topology, we ﬁnd it to be suitable as a basis for designing
a point cloud decoder which is ﬂexible enough to represent
various structures and topology, but not too constrained as
to be forced to generate a speciﬁc or any topology. The de-
sign of such a decoder and our general framework for point
cloud generation and completion are presented next.

5. Structured Point Set Generation

Given that general structure and topology on discrete
and ﬁnite point sets can be represented as a rooted tree, we
present a novel decoder with a rooted tree structure which
takes as input an embedding representing a 3D shape and
generates the corresponding point cloud (Fig. 2). Our pro-
posed decoder generates point clouds according to a tree
structure where each node of the tree represents a subset of
the point cloud (Fig. 1). The decoder is trained using the
Chamfer distance as loss [9], within a framework that in-

MLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPMLPDecoderInput EmbeddingConcatenation3D PointMultilayer Perceptron (MLP)al. [9] meets these two requirements and is deﬁned as:

dCD(S, SG) = X

x∈S

min
y∈SG

kx − yk2 + X

y∈SG

kx − yk2

min
x∈S

(1)
For each point, the Chamfer distance ﬁnds the nearest
neighbor in the other set and computes their squared dis-
tances which are summed over both target and ground-truth
sets.

6. Experiments

We now evaluate our proposed decoder both quantita-
tively and qualitatively on the task of 3D point cloud shape
completion. A given partial input point cloud is processed
through an existing point cloud encoder and the resulting
embedding is processed by our decoder to generate a point
cloud. The encoder used in our ﬁnal model is the 2-stage
PointNet-based encoder from Yuan et al. [38].

Dataset: We evaluate our dataset on a subset of the
Shapenet dataset [4] derived from the dataset in Yuan et
al. [38] The ground-truth in [38] was generated by uni-
formly sampling 16384 points on the mesh surfaces and the
partial point clouds were generated by back-projecting 2.5D
depth images into 3D. In our experiments we use N = 2048
for both input and ground-truth point clouds which are ob-
tained by random subsampling/oversampling of the clouds
in [38]. We keep the same train/test split as [38].

Implementation Details: Our ﬁnal decoder has L = 6
levels and each MLP in the decoder tree generates a small
node feature embedding of size F = 8. When generat-
ing N = 2048 points, the root node has 4 children and all
other internal nodes in subsequent level generate 8 children,
yielding a total of N = 4 × (83) = 2048 points generated
by the decoder. Each MLP in the decoder is a has 3 stages
with 256, 64, and C channels respectively, where C = 8 for
inner nodes and C = 3 for leaf nodes.

Training Setup: We train all models for 300 epochs,
with a batch size of 32, a learning rate of 1e-2 or 5e-3
depending on stability, and Adagrad optimizer. The best
model is chosen based on the validation set.

Evaluation: We evaluate our model across 8 classes
from the Shapenet dataset, against state-of-the-art methods
on point cloud completion. For each class, the Chamfer
Distance is computed (averaged over the number of class
instances). Our ﬁnal metric is the mean Chamfer Distance
averaged across classes. In addition, we train a fully con-
nected decoder baseline with 4 layers of output dimensions
256, 512, 1024, and 3 × N . For N = 2048, the results of
the evaluation of our method against state-of-the-art meth-
ods are shown in Table 1, with qualitative results in Figure
4. Our method signiﬁcantly outperforms existing methods
across all classes, and shows a 33.9% relative improvement
over the next best method.

Figure 3: Number of Parameters: We analyze the performance
of our networks instantiations as a function of its number of pa-
rameters. Across all instantiations, our network outperforms pre-
vious works. A local minimum seems to emerge from this plot.
Note the Chamfer distance is reported multiplied by 104.

6.1. Encoder analysis

Methods proposed in previous works use a variety of en-
coders. The point cloud generated is dependent not only
on the decoder but also on the feature generated by the en-
coder. In this experiment we analyze the effect of encoder
choice on the performance of our decoder and that of pre-
vious decoder. The results of this analysis are tabulated in
Table 2. The ﬁrst encoder (A) used in previous works [13]
and for our fully connected baseline is a PointNet[13]. The
second encoder is proposed in [38] in which it demonstates
better performance than PointNet++ [24]. We show results
with these two encoders and compare against methods us-
ing each respectively. Regardless of the encoder used, our
method outperforms existing methods. There is a noticeable
gap in performance depending on the encoder chosen, but
comparing across methods using the same encoder, our net-
works still shows a signiﬁcant performance improvement.

6.2. Ablation studies

Design choices involved in our decoder include choosing
the number of features F generated for each node embed-
ding and the number of tree levels L. We analyze the effect
of these parameters for an output cloud size N = 2048 by
varying F in {8, 16, 32, 64}, and L in {2, 4, 6, 8}. This ab-
lations study was used to pick the model’s ﬁnal number of
layers and number of features. One important thing to note
is that since the number of output points is ﬁxed at 2048
in this experiment, increasing the number of levels requires
decreasing the number of children per level. This opera-
tion is therefore not similar to adding a new layer in con-
ventional networks and a deeper tree may not necessarily
improve performance.

In Figure 5a, we plot the Chamfer distance as a func-
tion of the number of levels L for different values of F. For

388

Method (cid:14) Eval.

Chamfer Distance (CD)

AtlasNet [13]

Plane Cabinet Car Chair Lamp Couch Table Watercraft Average
10.37
PointNetFCAE [base.] 10.31
11.18
8.09

23.40 13.41 24.16 20.24 20.82 17.52
19.07 11.83 24.69 20.31 20.09 17.57
20.15 13.25 21.48 18.19 19.09 17.80
10.53 19.33 18.52 16.44 16.34
18.32

11.62
10.50
10.69
10.21

17.69
16.80
16.48
14.72

Folding [37]

PCN [38]

Ours

5.50

12.02

8.90 12.56 9.54

12.20 9.57

7.51

9.72

Table 1: Point Cloud Completion Results on ShapeNet: Comparison of our approach against previous works for the resolution (N ∼=
2048). The Chamfer distance is reported multiplied by 104.

Method AtlasNet-A[13] PointNetFC-A[base.] Folding-B[37] PCN-B[38] Ours(A) Ours(B)

CD

17.69

16.80

16.48

14.72

12.97

9.72

Table 2: Encoder analysis: Comparison of our approach against previous works using different encoders. The Chamfer distance is
reported multiplied by 104. Encoder A is a 1-stage PointNet based encoder, and encoder B is a 2-stage PointNet encoder. Our method
outperforms all other methods with their respective encoders, with encoder B giving a better performance than encoder A.

F ∈ {8, 32, 64} the graphs exhibit different local minima
but for F = 6, the performance oscillates around an aver-
age value. In Figure 5b, we plot the Chamfer distance as
a function of the number of features F for different values
of L). The graphs for L ∈ {4, 6, 8} exhibit slightly simi-
lar trend though the pattern is non-convex. The graph for
L = 2 exhibits a very different pattern compare to the oth-
ers. In all experiments, regardless of the value for L and F
above, our method outperforms all previous method.

points, we keep the ground-truth at 2048 points and only in-
crease the output size: quantitative results are shown in Ta-
ble 3. We notice that our network’s performance increases
with the number of output points albeit the absolute im-
provement is less than that of PCN[38]. As a percentage,
the improvement is still signiﬁcant, ranging from 9.16% to
18.46% for our method while PCN’s improvement ranges
from -2.61% to 20.26%. In all resolutions, our method still
shows superior absolute performance.

6.3. Number of parameters

7. Discussion and limitations

The number of parameters used by each method can in-
ﬂuence performance so we analyze the performance of our
method as a function of the number of parameters for F = 8
and L ∈ {2, 4, 6, 8}. This analysis in shown in Figure 3
where the Chamfer distance is plotted as a function of the
number of network parameters. Our model varies in the
number of network parameters depending on design choice
with some instantiations of our method having fewer or
more parameters than previous methods. Regardless of the
number of parameters in experiments, our network shows
superior performance, which suggests that the number of
parameters is not the primary reason for our performance.

6.4. Analysis of point cloud resolution

Previous works have found that the shape of most objects
in Shapenet can be summarized by as few as N = 1024
points[23]. But for other applications such as graphics in
which the ability to get accurate normals is key, generat-
ing dense accurate point clouds can be useful. We therefore
analyze the performance of our network as the number of
output points scales upward. We compare performance at
N = 2048, 4096, 8192, 16384. Since the computation of
the Chamfer loss scales quadratically with the number of

The decoder proposed in our work aims at generating
structured point clouds by generating a point cloud as a col-
lection of its subsets. A constraint of our decoder is that
the tree needs to be sufﬁciently large: for instance, repre-
senting a topology of S whose elements are all subsets of S
requires our decoder to have at least 2|S| +1 nodes, which is
intractable. The capacity of the decoder therefore can limit
the possible topologies learned. The proposed architecture
is still a promising step towards incorporating structure in
decoders as unlike previous works, we generate a structured
point cloud without explicitly enforcing a speciﬁc structure
which allows the network to learn arbitrary topologies.
Visualizing learned structure: By design, each node in
our decoder generates a subset of S made of all its descen-
dant leaves. We can visualize learned structure by plotting
each node’s descendant leaves. In Fig. 1 and the supplemen-
tary material we visualize several decoder nodes. We notice
that several geometric clustering patterns emerge. Some
clusterings seem geometric while others appear random but
are consistent across.This can be seen as a consequence of
our adopting the more general deﬁnition of topology which
does not enforce the generated clustering to be smooth.
Usefulness & Redundancies: One interesting future study

389

Method

Resolution
(# points) AtlasNet [13] PointNetFCAE [base.] Folding [37] PCN [38] Ours
9.72
8.83
7.20
6.50

2048
4096
8192
16384

16.48
13.19
12.95
12.26

14.72
11.88
12.19
9.72

16.8
14.79
12.57
10.61

17.69
16.12
15.32
14.85

Table 3: Resolution analysis: Comparison of our approach against the best performing method (PCN) with varying point cloud resolution.
We train each method to generate N = 2048, 4096 points. The Chamfer distance is reported, multiplied by 104. Our method outperforms
PCN for all different resolutions, but the performance gap between the methods is higher for low-resolution point clouds.

Figure 4: Point Cloud Completion Results. A partial point cloud is given as input and our method generates a completed point cloud.

(a)

(b)

Figure 5: Ablation Experiments: We analyze the effect of varying different parameters in our network. We vary the number of node
features {8, 16, 32, 64}, and the number of tree levels {2, 4, 6, 8}, while keeping the number of outputs constants. For instance when the
number of levels L=2, the number of children per level is 32-64. When L=4, the number of children per level is 4, 4, 4, 8. All instantiations
of our method outperform previous works. The number of levels seem to suggest a local minimum, but the number of features does not
show a noticeable pattern. The Chamfer distance is reported multiplied by 104.

would be to explore if the learned structure embeddings
have value as representations in classiﬁcation. Another fu-
ture improvement of this work could be to propose a way to
generate structured point clouds without redundancies.
Multiple structures: While our proposed model can em-

bed arbitrary topologies, it is only able to embed a single
topology at test time. One avenue of exploration would be
to combine several such decoders and evaluate whether they
all converge to the same topology for S or whether each de-
coder learns to embed disjoint subsets of S.

390

References

[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas.
Learning representations and generative models for 3d point
clouds, 2018. 1

[2] M. Armstrong. Basic Topology. Undergraduate Texts in

Mathematics. Springer, 1990. 3, 4

[3] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. CoRR, abs/1608.04236, 2016. 1, 2

[4] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d
model repository. CoRR, abs/1512.03012, 2015. 6

[5] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In Proceedings of the European Conference
on Computer Vision (ECCV), 2016. 2

[6] A. Dai, C. R. Qi, and M. Niener. Shape completion us-
ing 3D-encoder-predictor CNNs and shape synthesis.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 1, 2

[7] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and
M. Nießner. Scancomplete: Large-scale scene completion
and semantic segmentation for 3d scans. In Proc. Computer
Vision and Pattern Recognition (CVPR), IEEE, 2018. 1

[8] R. F. de Figueiredo, P. Moreno, and A. Bernardino. Auto-
matic object shape completion from 3d point clouds for ob-
ject manipulation. In VISIGRAPP, 2017. 1

[9] H. Fan, H. Su, and L. Guibas. A Point Set Generation Net-
work for 3D Object Reconstruction from a Single Image.
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 1, 2, 3, 5, 6

[10] M. Gadelha, R. Wang, and S. Maji. Multiresolution tree net-
works for 3d point cloud processing. In The European Con-
ference on Computer Vision (ECCV), September 2018. 3

[11] R. Girdhar, D. Fouhey, M. Rodriguez, and A. Gupta. Learn-
ing a predictable and generative vector representation for ob-
jects. In ECCV, 2016. 2

[12] B. Graham, M. Engelcke, and L. van der Maaten. 3d se-
mantic segmentation with submanifold sparse convolutional
networks. CVPR, 2018. 2

[13] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. AtlasNet: A Papier-M\ˆach\’e Approach to
Learning 3D Surface Generation.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 1,
2, 3, 6, 7, 8

[14] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
resolution shape completion using deep neural networks for
global structure and local geometry inference. In IEEE In-
ternational Conference on Computer Vision (ICCV), October
2017. 1, 2

[15] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
Resolution Shape Completion Using Deep Neural Networks
for Global Structure and Local Geometry Inference. IEEE
International Conference on Computer Vision (ICCV), 2017.
2

[16] C. H¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-

diction for 3d object reconstruction. In 3DV, 2017. 1

[17] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-
networks for the recognition of 3d point cloud models. ICCV,
2017. 2

[18] T. Le and Y. Duan. Pointgrid: A deep network for 3d shape
IEEE Conference on Computer Vision and

understanding.
Pattern Recognition (CVPR), June 2018. 1, 2

[19] D. Li, T. Shao, H. Wu, and K. Zhou. Shape completion from
a single rgbd image. IEEE Transactions on Visualization and
Computer Graphics, 23(7):1809–1822, July 2017. 2

[20] J. Li, B. M. Chen, and G. H. Lee.

So-net: Self-
organizing network for point cloud analysis. arXiv preprint
arXiv:1803.04249, 2018. 1

[21] Y. Li, R. Bu, M. Sun, and B. Chen. Pointcnn. CoRR,

abs/1801.07791, 2018. 1, 2

[22] O. Litany, A. Bronstein, M. Bronstein, and A. Makadia. De-
formable shape completion with graph convolutional autoen-
coders.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 1

[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 1, 7

[24] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hi-
erarchical feature learning on point sets in a metric space. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 30, pages 5099–5108.
Curran Associates, Inc., 2017. 1, 6

[25] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. Oct-
netfusion: Learning depth fusion from data. In 3DV, pages
57–66. IEEE Computer Society, 2017. 2

[26] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover’s
distance as a metric for image retrieval. International journal
of computer vision, 40(2):99–121, 2000. 2

[27] A. Sharma, O. Grau, and M. Fritz. Vconv-dae: Deep vol-
umetric shape learning without object labels.
In G. Hua
and H. J´egou, editors, Computer Vision – ECCV 2016 Work-
shops, pages 236–250, Cham, 2016. Springer International
Publishing. 2

[28] A. Sinha, A. Unmesh, Q. Huang, and K. Ramani. Surfnet:
Generating 3d shape surfaces using deep residual networks.
In CVPR, pages 791–800. IEEE Computer Society, 2017. 1
[29] D. Stutz and A. Geiger. Learning 3D Shape Completion from
Laser Scan Data with Weak Supervision. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
2

[30] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H.
Yang, and J. Kautz. SPLATNet: Sparse lattice networks for
point cloud processing.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2530–2539, 2018. 2

[31] J. Varley, C. DeChant, A. Richardson, A. Nair, J. Ruales,
and P. Allen. Shape completion enabled robotic grasping.
In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ
International Conference on. IEEE, 2017. 1

391

[32] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong.
O-CNN: Octree-based Convolutional Neural Networks for
3D Shape Analysis. ACM Transactions on Graphics (SIG-
GRAPH), 36(4), 2017. 2

[33] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong. Adaptive O-
CNN: A Patch-based Deep Representation of 3D Shapes.
ACM Transactions on Graphics (SIGGRAPH Asia), 37(6),
2018. 2

[34] Z. Wang and F. Lu. Voxsegnet: Volumetric cnns for seman-
tic part segmentation of 3d shapes. CoRR, abs/1809.00226,
2018. 2

[35] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman, and
J. B. Tenenbaum. Learning shape priors for single-view 3d
completion and reconstruction. European Conference on
Computer Vision (ECCV), 2018. 1

[36] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1912–1920, June 2015.
2

[37] Y. Yang, C. Feng, Y. Shen, and D. Tian. FoldingNet: In-
terpretable Unsupervised Learning on 3D Point Clouds. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2, 3, 7, 8

[38] W. Yuan and D. Held. PCN : Point Completion Network.
International Conference on 3D Vision (3DV), 2018. 1, 2, 3,
6, 7, 8

392

