Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes

Yiran Zhong1

4

,

,

5, Pan Ji2, Jianyuan Wang1

,

3, Yuchao Dai3, Hongdong Li1

4

,

1Australian National University, 2NEC Labs America,

3Northwestern Polytechnical University, 4ACRV, 5Data61 CSIRO

{yiran.zhong, hongdong.li}@anu.edu.au, panji@nec-labs.com, daiyuchao@nwpu.edu.cn

Abstract

Unsupervised deep learning for optical ﬂow computa-
tion has achieved promising results. Most existing deep-net
based methods rely on image brightness consistency and lo-
cal smoothness constraint to train the networks. Their per-
formance degrades at regions where repetitive textures or
occlusions occur. In this paper, we propose Deep Epipo-
lar Flow, an unsupervised optical ﬂow method which in-
corporates global geometric constraints into network learn-
ing. In particular, we investigate multiple ways of enforc-
ing the epipolar constraint in ﬂow estimation. To allevi-
ate a “chicken-and-egg” type of problem encountered in
dynamic scenes where multiple motions may be present,
we propose a low-rank constraint as well as a union-of-
subspaces constraint for training. Experimental results
on various benchmarking datasets show that our method
achieves competitive performance compared with super-
vised methods and outperforms state-of-the-art unsuper-
vised deep-learning methods.

1. Introduction

Optical ﬂow estimation is a fundamental problem in
computer vision with many applications. Since Horn and
Schunck’s seminal work [14], various methods have been
developed using variational optimization [2, 43, 5], energy
minimization [19, 24, 32, 40], or deep learning [7, 8, 25,
33]. In this paper, we particularly tackle the problem of un-
supervised optical ﬂow learning using deep convolutional
neural networks (CNNs). Compared to its supervised coun-
terpart, unsupervised ﬂow learning does not require ground-
truth ﬂow, which is often hard to obtain, as supervision and
can thus be applied in broader domains.

Recent research has been focused on transforming tradi-
tional domain knowledge of optical ﬂow into deep learning,
in terms of either training loss formulation or network ar-
chitecture design. For example, in view of brightness con-
sistency between two consecutive images, a constraint that
has been commonly used in conventional optical ﬂow meth-

ods, researchers have formulated photometric loss [42, 31],
with the help of fully differentiable image warping [15], to
train deep neural networks. Other common techniques in-
cluding image pyramid [4] (to handle large ﬂow displace-
ments), total variation regularization [30, 37] and occlusion
handling [1] have also led to either new network structures
(e.g., pyramid networks [25, 33]) or losses (e.g., smoothness
loss and occlusion mask [35, 16]). In the unsupervised set-
ting, existing methods mainly rely on the photometric loss
and ﬂow smoothness loss to train deep CNNs. This, how-
ever, poses challenges for the neural networks to learn op-
tical ﬂow accurately in regions with repetitive textures and
occlusions. Although some methods [35, 16] jointly learn
occlusion masks, these masks do not mean to provide more
constraints but only to remove the outliers in the losses. In
light of the difﬁculties of learning accurate ﬂow in these re-
gions, we propose to incorporate global epipolar constraints
into ﬂow network training in this paper.

Leveraging epipolar geometry in ﬂow learning, however,
is not a trivial task. An inaccurate or wrong estimate of
fundamental matrices [13] would mislead the ﬂow network
training in a holistic way, and thus signiﬁcantly degrade the
model prediction accuracy. This is especially true when a
scene contains multiple independent moving objects as one
fundamental matrix can only describe the epipolar geom-
etry of one rigid motion. Instead of posing a hard epipo-
lar constraint, in this paper, we propose to use soft epipo-
lar constraints that are derived using low-rankness when the
scene is stationary, and union of subspaces structure when
the scene is motion agnostic. We thus formulate corre-
sponding losses to train our ﬂow networks unsupervisedly.

Our work makes an attempt towards incorporating epipo-
lar geometry into deep unsupervised optical ﬂow computa-
tion. Through extensive evaluations on standard datasets,
we show that our method achieves competitive performance
compared with supervised methods, and outperforms exist-
ing unsupervised methods by a clear margin. Speciﬁcally,
as of the date of paper submission, on KITTI and MPI Sin-
tel benchmarks, our method achieves the best performance
among published deep unsupervised optical ﬂow methods.

12095

2. Related work

Optical ﬂow estimation has been extensively studied for
decades. A signiﬁcant number of papers have been pub-
lished in this area. Below we only discuss a few geometry-
aware methods and recent deep-learning based methods that
we consider closely related to our method.

Supervised deep optical ﬂow. Recently, end-to-end learn-
ing based deep optical ﬂow approaches have shown their
superiority in learning optical ﬂow. Given a large amount
of training samples, optical ﬂow estimation is formulated
to learn the regression between image pair and correspond-
ing optical ﬂow. These approaches achieve comparable per-
formance to state-of-the-art conventional methods on sev-
eral benchmarks while being signiﬁcantly faster. FlowNet
[7] is a pioneer in this direction, which needs a large-size
synthetic dataset to supervise network learning. FlowNet2
[8] greatly extends FlowNet by stacking multiple encoder-
decoder networks one after the other, which could achieve
a comparable result to conventional methods on various
benchmarks. Recently, PWC-Net [33] combines sophisti-
cated conventional strategies such as pyramid, warping and
cost volume into network design and set the state-of-the-art
performance on KITTI [12, 23] and MPI Sintel [6]. These
supervised deep optical ﬂow methods are hampered by the
need for large-scale training data with ground truth optical
ﬂow, which also limits their generalization ability.

Unsupervised deep optical ﬂow. Instead of using ground
truth ﬂow as supervision, Yu et al. [42] and Ren et al. [28]
suggested that, similar to conventional methods, the image
warping loss can be used as supervision signals in learning
optical ﬂow. However, there is a signiﬁcant performance
gap between their work and the conventional ones. Then,
Simon et al. [31] analyzed the problem and introduced
bidirectional Census loss to handle illumination variation
between frames robustly. Concurrently, Yang et al. [35]
proposed an occlusion-aware warping loss to exclude oc-
cluded points in error computation. Very recently, Janai et
al. [16] extended two-view optical ﬂow to multi-view cases
with improved occlusion handling performance. Introduc-
ing sophisticated occlusion estimation and warping loss re-
duces the performance gap between conventional methods
and current unsupervised ones, nevertheless, the gap is still
huge. To address this issue, we propose a global epipolar
constraint in ﬂow estimation that largely narrows the gap.

Geometry-aware optical ﬂow. In the ﬁeld of cooperating
with geometry constrains, Valgaerts et al. [34] introduced
a variational model to simultaneously estimate the funda-
mental matrix and the optical ﬂow. Wedel et al. [36] uti-
lized fundamental matrix prior as a weak constraint in a
variational framework. Yamaguchi et al. [39] converted op-
tical ﬂow estimation task into a 1D search problem by us-
ing precomputed fundamental matrices and the small mo-

tion assumptions. These methods, however, assume that the
scene is mostly rigid (and thus a single fundamental matrix
is sufﬁcient to constrain two-view geometry), and treat the
dynamic parts as outliers [36]. Garg et al. [11] used the
subspace constraint on multi-frame optical ﬂow estimation
as a regularization term. However, this approach, assumes
an afﬁne camera model and works over entire sequences.
Wulff et al. [38] used semantic information to split the scene
into dynamic objects and static background and only ap-
plied strong geometric constraints on the static background.
Recently, inspired by multi-task learning, people started to
jointly estimate depth, camera poses and optical ﬂow in an
uniﬁed framework [26, 41, 44]. These work mainly lever-
age a consistency between ﬂows that estimated from a ﬂow
network and computed from poses and depth. This con-
straint only works for stationary scenes and their perfor-
mance is only comparable with unsupervised deep optical
ﬂow methods.

By contrast, our proposed method is able to handle both
stationary and dynamic scenarios without explicitly com-
puting fundamental matrices. This is achieved by introduc-
ing soft epipolar constraints derived from epipolar geome-
try, using low-rankness and union-of-subspaces properties.
Converting these constraints to proper losses, we can ex-
ert global geometric constraints in optical ﬂow learning and
obtain much better performance.

3. Epipolar Constraints in Optical Flow

Optical ﬂow aims at ﬁnding dense correspondences be-
tween two consecutive frames. Formally, let I t denote the
image at time t, and I t+1 the next image. For pixels xi in
I t, we would like to ﬁnd their correspondences x′
i in I t+1.
The displacement vectors v = [v1, ..., vN ] ∈ R2×N (with
N the total number of pixels in I t) are the optical ﬂow we
would like to estimate.

Recall that in two-view epipolar geometry [13], by us-
ing the homogeneous coordinates, a pair of point corre-
spondences in two frames x′
i, 1)T and xi =
(xi, yi, 1)T is related by a fundamental matrix F,

i = (x′

i, y′

x′T
i

Fxi = 0 .

(1)

In the following sections, we show how to enforce the
epipolar constraint as a global regularizer in ﬂow learning.

3.1. Two view Geometric Constraint

Given estimated optical ﬂow v, we can convert it to a
series of correspondences xi and x′
i in I t and I t+1 re-
spectively. Then these corresponding points can be used
to compute a fundamental matrix F by normalized 8 points
method [13]. Once the F is estimated, we can compute
its ﬁtting error. Directly optimizing Eq. (1) is not effec-
tive as it is only an algebraic error that does not reﬂect the

12096

real geometric distances. We can use the Gold Standard
method [13] to compute the geometric distances but it re-

quires reconstructing the 3D pointsbXi beforehand for every

point. Otherwise, we can use its ﬁrst-order approximation,
the Sampson distance LF to represent the geometric error,

ixT

hi = vec(x′
i ). Then the data matrix H = [h1, ..., hN ]
should be low-rank. This provides a possible way of reg-
ularizing optical ﬂow estimation via rank minimization in-
stead of explicitly computing F. Speciﬁcally, we can for-
mulate a loss as

.

(2)

Llowrank = rank(H) ,

(5)

LF =

NXi

(Fxi)2

1 + (Fxi)2

i)2

1 + (FT x′

i)2

2

(x′T
Fxi)2
i
2 + (FT x′

The difﬁculty of optimizing this equation comes from its
chicken-and-egg character: it consists of two mutually inter-
locked sub-problems, i.e., estimating a fundamental matrix
F from an estimated ﬂow and updating the ﬂow to comply
with the F. This alternating method, therefore, heavily re-
lies on proper initialization.

Up to now, we have only considered the static scene
scenario, where only ego-motion exists. In a multi-motion
scene, this method requires estimating F for each motion,
which again needs a motion segmentation step. It is still
feasible to address this problem via iteratively solving three
sub-tasks: (i) update ﬂow estimation; (ii) estimate Fm for
each rigid motion given current motion segmentation; (iii)
update motion segmentation based on the nearest Fm.

However, this method again has several inherent limita-
tions. First, the number of motions need to be known as a
priori which is almost impossible in general optical ﬂow es-
timation. Second, this method is still sensitive to the quality
of initial optical ﬂow estimation and motion labels. Incor-
rectly estimated ﬂow may generate wrong Fm, which will
in turn lead ﬂow estimation to the wrong solution, there-
fore making the estimation even worse. Third, the motion
segmentation step is non-differentiable, so with it, an end-
to-end learning becomes impossible.

To overcome these drawbacks, we formulate two soft
epipolar constraints using low-rankness and union-of-
subspaces properties. And we will show that these con-
straints can be easily included as extra losses to regularize
the network learning.

3.2. Low rank Constraint

In this section, we show that it is possible to enforce
a soft epipolar constraint without explicitly computing the
fundamental matrix in a static scene.

Note that we can rewrite the epipolar constraint in Eq. (1)

as

f T vec(x′

(3)
where f ∈ R9 is the vectorized fundamental matrix of F
and

i ) = 0 ,

ixT

vec(x′

ixT

i ) = (xix′

i, xiy′

i, xi, yix′

i, yiy′

i, yi, x′

i, y′

i, 1)T .

(4)
i ) lies on a subspace (of dimension
up to eight), called epipolar subspace [17]. Let us deﬁne

Observe that, vec(x′

ixT

which is unfortunately non-differentiable and is thus not
feasible to serve as a loss for ﬂow network training. For-
tunately, we can still use its convex surrogate, the nuclear
norm, to form a loss as

L∗
lowrank = kHk∗ ,

(6)

where the nuclear norm ||·||∗ can be computed by perform-
ing singular value decomposition (SVD) of H. Note that the
SVD operation is differentiable and has been implemented
in modern deep learning toolboxes such as Tensorﬂow and
Pytorch, so this nuclear norm loss can be easily incorpo-
rated into network training. We also note that though this
low-rank constraint is derived from epipolar geometry de-
scribed by a fundamental matrix, it still applies in degener-
ate cases where a fundamental matrix does not exist. For
example, when the motion is all zero or pure rotational, or
the scene is fully planar, H will have rank six; under certain
special motions, e.g., an object moving parallel to the image
plane, its H will have rank seven.

Comparing to the original epipolar constraint, one may
concern that this low-rank constraint is too loose to be effec-
tive, especially when the ambient space dimension is only
nine. Although a thorough theoretical analysis is out of the
scope of this paper (interested readers may refer to literature
such as [27]), we will show in our experiments that this loss
can improve the model performance by a signiﬁcant margin
when trained on data with mostly static scenes. However,
this loss becomes ineffective when a scene has more than
one motion, as the matrix H will then be full-rank.

3.3. Union of Subspaces Constraint

In this section, we introduce another soft epipolar con-
straint, namely union-of-subspaces constraint, which can be
applied in broader cases.

From Eq. (4), it’s not hard to observe that hi from one
rigid motion lies on a common epipolar subspace because
they all share the same fundamental matrix. When there
are multiple motions in a scene, hi will lie in a union of
subspaces. Note that this union-of-subspace structure has
been shown to be useful in motion segmentation from two
perspective images [20]. Here, we re-formulate it in opti-
cal ﬂow learning and come up with an effective loss using
closed-form solutions.

In particular, the union-of-subspaces structure can be
characterized by the self-expressiveness property [10], i.e.,

12097

Figure 1. Motion segmentation and afﬁnity matrix (constructed from C) visualization. The scene contains three motions annotated
by three different colors: the ego-motion and the two cars’ movements. On the right, we show a constructed afﬁnity matrix from C which
contains three diagonal blocks corresponding to these three motions. On the bottom left, we illustrate our estimated optical ﬂow and the
top left image shows that all these three motions are correctly segmented based on the C. The sparse dots on the image are the sampled
2000 points that has been used to compute C. It proves that our Union-of-Subspace constraint can work under multi-body scenarios.

a data point in one subspace can be represented by a lin-
ear combination of other points from the same subspace.
This has translated into a mathematical optimization prob-
lem [22, 18] as

min

C

1
2kCk2

F

s.t. H = HC .

(7)

where C is the subspace self-expression coefﬁcient and H
is a matrix function of estimated ﬂows. Note that, in sub-
space clustering literature, other norms on C have also been
used, e.g., nuclear norm in [21] and ℓ1 norm in [10]. We are
particularly interested in the Frobenius norm regularization
due to its simplicity and equivalence to nuclear norm opti-
mization [18], which is crucial for formulating an effective
loss for CNN training.

However, in real world scenarios, the ﬂow estimation in-
evitably contains noises. Therefore, we relax the constraints
in Eq.(7) by alternatively optimizing the function below

Lsubspace =

1
2kCk2

F +

λ
2kHC − Hk2
F ,

(8)

Instead of using a iterative solver, given an H, we can derive
a closed form solution for C, i.e.,

C∗ = (I + λHT H)−1λHT H.

(9)

Plugging the solution of C back to Eq.(8), we arrive at
our ﬁnal union-of-subspaces loss term that only depends on
the estimated ﬂow:

Lsubspace =
+

1
2k(I + λHT H)−1λHT Hk2
λ
2kH(I + λHT H)−1λHT H − Hk2
F .

F

(10)

Directly applying this loss to the whole image will
lead to GPU memory overﬂow due to the computation of

HT H ∈ RN ×N (with N the number of pixels in a image).

To avoid this, we employ a randomly sampling strategy to

sample 2000 ﬂow points in a ﬂow map and compute a loss
based on these samples. This strategy is valid because ran-
dom sampling will not change the intrinsic character of sets.
We remark that this subspace loss requires no prior
knowledge of the number of motions in a scene, so it can be
used to train a ﬂow network on a motion-agnostic dataset.
In a single-motion case, it works similarly to the low-rank
loss since the optimal loss is closely related to the rank of
H [18]. In a multi-motion case, as long as the epipolar sub-
spaces are disjoint and principle angles between them are
below certain thresholds [9], this loss can still serve as a
global regularizer. Even when the scene is highly non-rigid
or dynamic, unlike the hard epipolar constraint, this loss
won’t be detrimental to the network training because it will
have same values for both ground-truth ﬂows and wrong
ﬂows. In Fig. 1, we show the results of a typical image pair
from KITTI using this constraint, demonstrating the effec-
tiveness of our method.

4. Unsupervised Learning of Optical Flow

We formulate our unsupervised optical ﬂow estimation
approach as an optimization of image based losses and
epipolar constraint losses. In unsupervised optical ﬂow esti-
mation, only photometric loss Lphoto can provide data term.
Additionally, we use a smoothness term Lsmooth and our
epipolar constraint term LF|lowrank|subspace as our regulariza-
tion terms. Our overall loss L is a linear combination of
these three losses

L = Lphoto + µ1Lsmooth + µ2LF|lowrank|subspace,

(11)

where µ1, µ2 are the weights for each term. We empir-
ically set µ1 = 0.02 and µ2 = 0.02, 0.01, 0.001 for
LF,L∗
4.1. Image Warping Loss

lowrank,Lsubspace respectively.

Similarly to conventional methods, we leverage the most
i.e., I t, I t+1

popular brightness constancy assumption,

12098

should have similar pixel intensities, colors and gradients.
Our photometric error is then deﬁned by the difference
between the reference frame and the warped target frame
based on ﬂow estimation.

In [31], they target at the case which the illumination
may changes from frame to frame and propose a bidirec-
tional census transform C(·) to handle this situation. We
adopt this idea to our photometric error. Therefore, our pho-
tometric loss is a weighted summation of pixel intensities
(or color) loss Li, image gradient loss Lg and bidirectional
census loss Lc.

Lphoto = λ1Li + λ2Lc + λ3Lg,

(12)

where λ1 = 0.5, λ2 = 1, λ3 = 1 are the weights for each
term.

Inspired by [35], we only compute our photometric loss
on non-occluded areas O and normalize the loss by the
number of pixels of non-occluded regions. We determine
a pixel to be occluded or not by forward-backward consis-
tency check. If the sum of its forward and backward ﬂow is
above a threshold τ , we set the pixel as occluded. We use
τ = 3 in all experiments.

Our photometric loss is thus deﬁned as follows:

Oi

Li =" NXi=1
Lc =" NXi=1
Lg =" NXi=1

Oi · ϕ( ˆI t(xi) − I t(xi))# /
NXi
Oi · ϕ(bC t(xi) − C t(xi))# /
NXi
Oi · ϕ(∇ ˆI t(xi) − ∇I t(xi))# /
NXi

Oi

(13)

(14)

Oi

(15)

where ˆI t(xi) = I t+1(xi + vi) is computed through im-
age warping with the estimated ﬂow, and following [35], we
use a robust Charbonnier penalty ϕ(x) = √x2 + 0.0012 to

evaluate differences.

4.2. Smoothness Loss

Commonly, there are two kinds of smoothness prior in
conventional optical ﬂow estimation: One is piece-wise pla-
nar, and the other is piece-wise linear. The ﬁrst one can be
implemented by penalizing the ﬁrst order derivative of re-
covered optical ﬂow and the later one is by the second order
derivative. For most rigid scenes, piece-wise planar model
can provide a better interpolation. But for deformable cases,
piece-wise linear model suits better. Therefore, we use a
combination of these two models as our smoothness regu-
larization term. We further assumes that the edges in optical
ﬂows are edges in reference color images as well. Formally,
our image guided smoothness term can be deﬁned as:

Ls =X(cid:16)e−α1|∇I| |∇V | + e−α2|∇2I|(cid:12)(cid:12)∇2V(cid:12)(cid:12)(cid:17) /N,

(16)

where α1 = 0.5 and α2 = 0.5 and V ∈ RW ×H×2 is a

matrix form of v.

5. Experiments

We evaluate our methods on standard optical ﬂow bench-
marks, including KITTI [12, 23], MPI-Sintel [6], Flying
Chairs [7], and Middlebury [3]. We compare our results
with existing optical ﬂow estimation methods based on stan-
dard metrics, i.e., endpoint error (EPE) and percentage of
optical ﬂow outliers (Fl). We denote our method as EPI-
Flow.

5.1. Implementation details.

Architecture and Parameters. We implemented our
EPIFlow network in an end-to-end manner by adopting the
architecture of PWC-Net [33] as our base network due to its
state-of-the-art performance. The original PWC-Net takes
a structure of pyramid and learns on 5 different scales.
However, a warping error is ineffective on low resolutions.
Therefore, we pick the highest resolution output, upsam-
ple it to match the input resolution by bilinear interpola-
tion, and compute our self-supervised learning losses only
on that scale. The learning rate for initial training (from
scratch) is 10−4 and that for ﬁne-tuning is 10−5. Depend-
ing on the resolution of input images, the batch size is 4 or
8. We use the same data argumentation scheme as proposed
in FlowNet2 [8]. Our network’s typical speed varies from
0.07 to 0.25 seconds per frame during the training process,
depending on the input image size and the losses used, and
is around 0.04 seconds per frame in evaluation. The ex-
periments were tested on a regular computer equipped with
a Titan XP GPU. EPIFlow is signiﬁcantly faster compared
with conventional methods.

Pre-training. We pre-trained our network on the Flying
Chairs dataset using a weighted combination of the warp-
ing loss and smoothness loss. Flying Chairs is a syn-
thetic dataset consisting of rendered chairs superimposed
on real-world Flickr images. Training on such a large-
scale synthetic dataset allows the network to learn the gen-
eral concepts of optical ﬂow before handling complicated
real-world conditions, e.g., changeable light or motions.
To avoid trivial solutions, we disabled the occlusion-aware
term at the beginning of the training (i.e., the ﬁrst two
epochs). Otherwise, the network would generate all zero
occlusion masks which invalidate losses. The pre-training
roughly took forty hours and its returned model was used as
an initial model for other datasets.

5.2. Datasets

KITTI Visual Odometry (VO) Dataset. The KITTI VO
dataset contains 22 calibrated sequences with 87,060 con-
secutive pairs of real-world images. The ground truth poses

12099

KITTI 2012

KITTI 2015

Sintel Clean

Sintel Final

Method

EPE(all)

EPE(noc)

EPE(all) EPE(noc)

train

test

train

test

train

train

p EpicFlow [29]
MRFlow [38]

e
e
d
-
n
o
N

d SpyNet-ft [25]

e
s
i
v
r
e
p
u
S

d
e
s
i
v
r
e
p
u
s
n
U

FlowNet2-ft [8]
PWC-Net [33]
PWC-Net-ft [33]

UnsupFlownet [42]
DSTFlow-ft [28]
DF-Net-ft [44]
GeoNet [41]
UnFlow [31]
OAFlow-ft [35]
CCFlow [26]
Back2Future-ft [16]

Our-baseline
Our-gtF
Our-F
Our-low-rank
Our-sub
Our-sub-test-ft
Our-sub-train-ft

3.47

–

(4.13)
(1.28)
4.14
(1.45)

3.8
–

4.1
1.8
–
1.7

–
–

–
–
–
–

9.9

(11.30)
(4.30)
(10.43) 12.4 (3.29)
(3.54)

4.4
–
–
4.2
–
–

–
–
–
–
–

(3.2)
3.4

–
–

(1.26)

–
–
–

1.04
1.04
0.97
1.07
1.03
1.03
(0.99)

–

(3.29)
(3.55)

–
–

3.23
2.61
2.56
2.63
2.62
2.61
(2.51)

1.5
–

2.0
1.0
–
0.9

4.6
4.0
–
–
–
–
–
–

–
–
–
–
–

(1.1)
1.3

9.27

–

–

2.30
10.35
(2.16)

–

(16.79)
(8.98)
10.81
(8.10)
(8.88)
(5.66)
(6.59)

7.93
6.03
6.42
5.91
6.02
5.56
(5.55)

–
–

–
–
–
–

–

(6.96)

–

8.05

–
–
–

(3.22)

4.21
2.89
3.09
3.03
2.98
2.56
(2.46)

Fl−all
test

EPE(all)

EPE(all)

train

test

train

test

26.29% 2.27
12.19% (1.83)

35.07% (3.17)
10.41% (1.45)
2.55
9.60% (1.70)

–

–

–

4.11
2.53

6.64
4.16

–

3.86

–

3.56
(3.59)

(4.32)
(2.01)
3.93
(2.21)

–

6.29
5.38

8.36
5.74

–

5.17

–

39.00% (6.16) 10.41 (7.38) 11.28
25.70%

–
–

–
–

–
–

–
–

–
–
–

31.20% (4.03)
25.27%
22.94% (3.89)

–

–
–
–
–
–

6.72
6.15
6.21
6.39
6.15
(16.24%)
3.94
16.95% (3.54)

9.38
7.95

–

7.91
(5.95)

10.21
9.15

–

–

7.23

(5.52)

8.81

–
–
–
–
–

(6.84)
7.00

7.31
6.71
6.73
6.96
6.83
5.08
(4.99)

–
–
–
–
–

(8.33)
8.51

Table 1. Performance comparison on the KITTI and Sintel optical ﬂow benchmarks. The metric EPE(noc) indicates the average
endpoint error of non-occluded regions while the term EPE(all) is that for all pixels. The KITTI 2015 testing dataset evaluates results by
the percentage of ﬂow outliers (Fl). The baseline, gtF, F, low-rank, and sub models were trained on the KITTI VO dataset. The parentheses
indicate the corresponding models that were trained on the same data and the missing entries (-) indicate the results were not reported.
Note that the current STOA unsupervised method Back2Future Flow [16] uses three frames as input. Best results are marked by bold fonts.

of the ﬁrst 11 sequences are available. We ﬁne-tuned our
initial model on the KITTI VO dataset using various loss
combinations. We chose it for two reasons: (1) it provides
ground truth camera poses for every frame, which simpliﬁes
the problem of network performance analysis and (2) most
scenes in the KITTI VO dataset are stationary and thus can
be ﬁtted by an ego-motion. The relative poses (between a
pair of images) and camera calibration can be used to com-
pute fundamental matrices. To compare our various meth-
ods fairly, we use the ﬁrst 11 sequences as our training set.

KITTI Optical Flow Dataset. The KITTI optical ﬂow
dataset contains two subsets: KITTI 2012 and KITTI 2015,
where the ﬁrst one mostly contains stationary scenes and the
latter one includes more dynamic scenes. KITTI 2012 pro-
vides 194 annotated image pairs for training and 195 pairs
for testing while KITTI 2015 provides 200 pairs for train-
ing and 200 pairs for testing. Our training did not use the
KITTI datasets’ multiple-view extension images.

MPI Sintel Dataset. The MPI Sintel dataset provides nat-
uralistic frames which were captured from an open source

movie. It contains 1041 training image pairs with ground
truth optical ﬂows and pixel-wise occlusion masks, and also
provides 552 image pairs for benchmark testing. The scenes
of the MPI Sintel dataset were rendered under two different
complexity (Clean and Final). Unlike the KITTI datasets,
most scenes in the Sintel dataset are highly dynamic.

5.3. Quantitative and Qualitative Results

We use the sufﬁx “-baseline” to indicate our baseline
model that was trained using only photometric and smooth-
ness loss. “-F” represents the model that was trained us-
ing hard fundamental matrix constraint with estimated F. “-
gtF” means that we used the ground truth fundamental ma-
trix. “-low-rank” refers to the model applying the low rank
constraint, and “-sub” is the model using our subspace con-
straint. “-ft” denotes the model ﬁne-tuned on the datasets.

KITTI VO training results. We report our results that
were trained on the KITTI VO dataset in Table 1, where our
models are compared with various state-of-the-art methods.
Our methods outperform all previous learning-based unsu-

12100

Input

Ours

Back2Future [16]

Our Error

Back2Future Error [16]

Figure 2. Qualitative results on KITTI 2015 Test dataset. We compare our method with Back2Future Flow [16]. The second column
contains the ﬂows estimated by Our-sub-ft model while the third column contains the results of Back2Future Flow. The ﬂow error visual-
ization is also provided where correct estimates are depicted in blue and wrong ones in red. Consistent with the quantitative analysis, our
results are visually better on structural boundaries

Input

Ours

Back2Future [16]

Our Error

Back2Future Error [16]

Figure 3. Qualitative results on the MPI Sintel dataset. This ﬁgure shares the same layout with Fig. 2 except the top two rows are from
the Final set and the two bottom rows are from the Clean set. The errors are visualized in gray on the Sintel benchmark.

pervised optical ﬂow methods with a notable margin. Note
that most scenes in KITTI VO dataset are stationary, and
therefore the difference between our-gtF, our-F, our-low-
rank and our-sub is small across these benchmarks.

Benchmark Fine-tuning Results. We ﬁne-tuned our
models on each benchmark and report the results with a
sufﬁx ’-ft’ in Table 2. For example, simply following the
same hyper-parameters as before, we ﬁnetuned our models
on the KITTI 2015 testing data. After ﬁne-tuning, Our-sub
model shows great performance improvement and achieved
an EPE of 2.61 and 5.56 respectively on the KITTI 2012
and KITTI 2015 training datasets, which outperforms all the
deep unsupervised methods and many supervised methods.
Similarly, on the MPI Sintel trainings dataset, Our-sub-ft
model performs best among the unsupervised methods, with
an EPE of 3.94 on the Clean images and 5.08 on the Final
images. Furthermore, both on the KITTI and Sintel testing
benchmarks, our method outperformed the current state-of-

the-art unsupervised method Back2Future Flow by a mar-
gin. We improve the best unsupervised performance from
an Fl of 22.94% to 16.24% on KITTI 2015. The Our-sub-ft
model achieved an EPE of 6.84 on the Sintel Clean dataset
and 8.33 on the Final set, which are the results that unsuper-
vised methods have never touched before. Additionally, it
should be noted that the Back2Future Flow method is based
on a multi-frame formulation while our method only re-
quires two frames. Our model is also competitive compared
with some ﬁne-tuned supervised networks, such as SpyNet.

Qualitatively, as shown in Fig. 2 and Fig. 3, compared
with the results of Back2Future Flow, the shapes in our es-
timated ﬂows are more structured and have more explicit
boundaries which represent motion discontinuities. This
trend is also apparent in the ﬂow error images. For ex-
ample, on the KITTI 2015 dataset (Fig. 2), the results of
Back2Future Flow usually bring a larger region of error
with crimson colors around the objects.

It should be noted that ﬁne-tuning on the target datasets

12101

Input

Our-baseline

Our-F

Our-low-rank

Our-sub

Figure 4. Endpoint error performance of our various models on the KITTI 2015 training dataset. We compared Our-baseline, Our-F,
Our-low-rank, and Our-sub models on the KITTI 2015 dataset to analyze their performance when handling dynamic objects. The results
of the Our-sub model are much better.

(e.g., KITTI 2015) does not bring signiﬁcant improvement
because we have trained the models on a real-world dataset
KITTI VO. The models have learned the general concepts
of realistic optical ﬂows and ﬁne tuning just helps them fa-
miliar with the datasets’ characteristics. On the KITTI 2012
training set, the ﬁne-tuned model achieves very close results
with the Our-sub model, which are respectively 2.61 and
2.62 EPE. Fine-tuning on the Sintel Clean dataset improves
the result from 6.15 to 3.94 EPE, because the Sintel Clean
dataset renders the synthetic scenes under low complexity
and the images are quite different from the real world.

5.4. Ablation study

Figure 5. Endpoint error over epochs on the Sintel Final
dataset. We illustrate the endpoint errors over the training epochs
when using various combinations of constraints. For all the three
methods, the training started from the same pre-trained model
‘Our-baseline’. Combing the image warping and subspace con-
straints outperforms other two methods, which is consistent with
the ﬁnal ﬁne-tuned results reported in Table 2.

The Our-F, Our-low-rank and Our-sub models all work
well in stationary scenes and they have similar quantita-
tive performance. To further analyze their capabilities in
handling general dynamic scenarios, we ﬁne-tuned each
method on the KITTI 2015 and Sintel Final dataset. Both
of them involve multiple motions in an image while Sin-
tel scenes are more dynamic. As shown in Table 2, Our-
sub can handle dynamic scenarios best and achieves the
lowest EPE in both benchmarks. The hard fundamental
constraint shares a similar performance with our baseline

Method

KITTI 2015

Sintel Final

EPE(all) EPE(noc)

EPE(all)

Our-baseline-ft
Our-F-ft
Our-low-rank-ft
Our-sub-ft

6.16
6.19
5.72
5.56

2.85
2.85
2.62
2.56

5.87
NaN
5.59
5.08

Table 2. Fine-tuning results comparison on KITTI 2015 and
Sintel Final training sets. We ﬁne-tuned our models on the train-
ing sets of KITTI 2015 and Sintel Final dataset. The term NaN
indicates the model cannot converge.

model but cannot converge on the Sintal dataset, whose EPE
is reported as NaN. It is because a highly dynamic scene
does not have a global fundamental F. For the low-rank
constraint, its performance is not affected by dynamic ob-
jects while it cannot gain information by modeling multi-
ple movements as well. In Fig. 5, we provide the valida-
tion error curves over the training’s early stages on Sintal
ﬁnal dataset. The subspace loss helps the model converge
quicker and achieve lower cost than other methods.

6. Conclusion

In this paper, we have proposed effective methods to en-
force global epipolar geometry constraints for unsupervised
optical ﬂow learning. For a stationary scene, we applied the
low-rank constraint to regularize a globally rigid structure.
For general dynamic scenes (multi-body or deformable), we
proposed to use the union-of-subspaces constraint. Experi-
ments on various benchmarking datasets have proved the ef-
ﬁcacy and superiority of our methods compared with state-
of-the-art (unsupervised) deep ﬂow methods. In the future,
we plan to study the multi-frame extension, i.e., enforcing
geometric constraints across multiple frames.
Acknowledgement This research was supported in part by
Australia Centre for Robotic Vision, Data61 CSIRO, the
Natural Science Foundation of China grants (61871325,
61420106007) the Australian Research Council (ARC)
grants (LE190100080, CE140100016, DP190102261). The
authors are grateful to the GPUs donated by NVIDIA.

12102

References

[1] Luis Alvarez, Rachid Deriche, Th´eo Papadopoulo, and Javier
S´anchez. Symmetrical dense optical ﬂow estimation with oc-
clusions detection. Int. J. Comp. Vis., 75(3):371–385, 2007.
1

[2] Gilles Aubert, Rachid Deriche, and Pierre Kornprobst. Com-
puting optical ﬂow via variational techniques. SIAM Journal
on Applied Mathematics, 60(1):156–182, 1999. 1

[3] Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth,
Michael J. Black, and Richard Szeliski. A database and
evaluation methodology for optical ﬂow. Int. J. Comp. Vis.,
92(1):1–31, Mar 2011. 5

[4] Jean-Yves Bouguet. Pyramidal implementation of the afﬁne
lucas kanade feature tracker description of the algorithm. In-
tel Corporation, 5(1-10):4, 2001. 1

[5] Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim
Weickert. High accuracy optical ﬂow estimation based on a
theory for warping.
In Proc. Eur. Conf. Comp. Vis., pages
25–36. Springer, 2004. 1

[6] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
In Proc. Eur. Conf. Comp. Vis., pages 611–625, Oct. 2012.
2, 5

[7] Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der
Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learn-
ing optical ﬂow with convolutional networks. In Proc. IEEE
Int. Conf. Comp. Vis., pages 2758–2766, 2015. 1, 2, 5

[8] Ilg Eddy, Mayer Nikolaus, Saikia Tonmoy, Keuper Margret,
Dosovitskiy Alexey, and Brox Thomas. Flownet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., Jul 2017. 1, 2, 5, 6

[9] Ehsan Elhamifar and Ren´e Vidal. Clustering disjoint sub-
spaces via sparse representation. In IEEE International Con-
ference on Acoustics Speech and Signal Processing, pages
1926–1929. IEEE, 2010. 4

[10] Ehsan Elhamifar and Rene Vidal. Sparse subspace cluster-
ing: Algorithm, theory, and applications. IEEE Trans. Pat-
tern Anal. Mach. Intell., 35(11):2765–2781, 2013. 3, 4

[11] Ravi Garg, Luis Pizarro, Daniel Rueckert, and Lourdes
Agapito. Dense multi-frame optic ﬂow for non-rigid objects
using subspace constraints. In Proc. Asian Conf. Comp. Vis.,
pages 460–473, 2011. 2

[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2012.
2, 5

[13] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision. Cambridge university press,
2003. 1, 2, 3

[14] Berthold KP Horn and Brian G Schunck. Determining opti-

cal ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981. 1

In European Conference on
optical ﬂow with occlusions.
Computer Vision (ECCV), volume Lecture Notes in Com-
puter Science, vol 11220, pages 713–731. Springer, Cham,
Sept. 2018. 1, 2, 6, 7

[17] Pan Ji, Hongdong Li, Mathieu Salzmann, and Yiran Zhong.
Robust multi-body feature tracker: a segmentation-free ap-
proach. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages
3843–3851, 2016. 3

[18] Pan Ji, Mathieu Salzmann, and Hongdong Li. Efﬁcient dense
subspace clustering. In IEEE Winter Conference on Applica-
tions of Computer Vision, pages 461–468. IEEE, 2014. 4

[19] Vladimir Kolmogorov and Ramin Zabih. Computing visual
correspondence with occlusions via graph cuts. Technical
report, Ithaca, NY, USA, 2001. 1

[20] Zhuwen Li, Jiaming Guo, Loong-Fah Cheong, and Steven
Zhiying Zhou. Perspective motion segmentation via collab-
orative clustering. In Proc. IEEE Int. Conf. Comp. Vis., pages
1369–1376, 2013. 3

[21] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong
Yu, and Yi Ma. Robust recovery of subspace structures by
low-rank representation. IEEE Trans. Pattern Anal. Mach.
Intell., 35(1):171–184, 2013. 4

[22] Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang
Huang, and Shuicheng Yan. Robust and efﬁcient subspace
segmentation via least squares regression. In Proc. Eur. Conf.
Comp. Vis., pages 347–360. Springer, 2012. 4

[23] Moritz Menze and Andreas Geiger. Object scene ﬂow for
autonomous vehicles. In Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., 2015. 2, 5

[24] Moritz Menze, Christian Heipke, and Andreas Geiger. Dis-
crete optimization for optical ﬂow. In German Conference
on Pattern Recognition, pages 16–28. Springer, 2015. 1

[25] Anurag Ranjan and Michael J. Black. Optical ﬂow estima-
tion using a spatial pyramid network. In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., July 2017. 1, 6

[26] Anurag Ranjan, Varun Jampani, Kihwan Kim, Deqing Sun,
Jonas Wulff, and Michael J Black. Adversarial collabo-
ration: Joint unsupervised learning of depth, camera mo-
tion, optical ﬂow and motion segmentation. arXiv preprint
arXiv:1805.09806, 2018. 2, 6

[27] Benjamin Recht, Weiyu Xu, and Babak Hassibi. Neces-
sary and sufﬁcient conditions for success of the nuclear norm
heuristic for rank minimization. In IEEE Conference on De-
cision and Control, pages 3065–3070. IEEE, 2008. 3

[28] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,
and Hongyuan Zha. Unsupervised deep learning for optical
ﬂow estimation. In Proc. AAAI Conf. Artiﬁcial Intelligence,
volume 3, page 7, 2017. 2, 6

[29] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and
Cordelia Schmid. Epicﬂow: Edge-preserving interpolation
of correspondences for optical ﬂow.
In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., June 2015. 6

[15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Proc. Adv. Neural Inf. Pro-
cess. Syst., pages 2017–2025, 2015. 1

[30] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear
total variation based noise removal algorithms. Physica D:
nonlinear phenomena, 60(1-4):259–268, 1992. 1

[16] Joel Janai, Fatma G¨uney, Anurag Ranjan, Michael J. Black,
and Andreas Geiger. Unsupervised learning of multi-frame

[31] Meister Simon, Hur Junhwa, and Roth Stefan. Unﬂow:
Unsupervised learning of optical ﬂow with a bidirectional

12103

census loss.
AAAI’18, 2018. 1, 2, 5, 6

In Proc. AAAI Conf. Artiﬁcial Intelligence,

[32] Deqing Sun, Stefan Roth, and Michael J Black. Secrets of
optical ﬂow estimation and their principles. In Proc. IEEE
Conf. Comp. Vis. Patt. Recogn., pages 2432–2439. IEEE,
2010. 1

[33] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
PWC-Net: CNNs for optical ﬂow using pyramid, warping,
and cost volume.
In Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., 2018. 1, 2, 5, 6

[34] Levi Valgaerts, Andr´es Bruhn, and Joachim Weickert. A
variational model for the joint recovery of the fundamental
matrix and the optical ﬂow. In Proceedings of DAGM Sym-
posium on Pattern Recognition, pages 314–324, 2008. 2

[35] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng
Wang, and Wei Xu. Occlusion aware unsupervised learn-
ing of optical ﬂow.
In Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., June 2018. 1, 2, 5, 6

[36] A. Wedel, D. Cremers, T. Pock, and H. Bischof. Structure-
and motion-adaptive regularization for high accuracy optic
ﬂow. In Proc. IEEE Int. Conf. Comp. Vis., pages 1663–1668,
Sept 2009. 2

[37] Andreas Wedel, Thomas Pock, Christopher Zach, Horst
Bischof, and Daniel Cremers. An improved algorithm for
tv-l 1 optical ﬂow. In Statistical and geometrical approaches
to visual motion analysis, pages 23–45. Springer, 2009. 1

[38] Jonas Wulff, Laura Sevilla-Lara, and Michael J. Black. Op-
tical ﬂow in mostly rigid scenes. In Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., July 2017. 2, 6

[39] K. Yamaguchi, D. McAllester, and R. Urtasun. Robust
In Proc. IEEE Conf.

monocular epipolar ﬂow estimation.
Comp. Vis. Patt. Recogn., pages 1862–1869, June 2013. 2

[40] Jiaolong Yang and Hongdong Li. Dense, accurate optical
ﬂow estimation with piecewise parametric model. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., pages 1019–1027,
2015. 1

[41] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learn-
ing of dense depth, optical ﬂow and camera pose. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., volume 2, 2018. 2, 6

[42] Jason J. Yu, Adam W. Harley, and Konstantinos G. Derpanis.
Back to basics: Unsupervised learning of optical ﬂow via
brightness constancy and motion smoothness. In European
Conference on Computer Vision (ECCV) Workshops, pages
3–10, 2016. 1, 2, 6

[43] Christopher Zach, Thomas Pock, and Horst Bischof. A du-
ality based approach for realtime tv-l 1 optical ﬂow. In Joint
Pattern Recognition Symposium, pages 214–223. Springer,
2007. 1

[44] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Un-
supervised joint learning of depth and ﬂow using cross-task
consistency.
In Proc. Eur. Conf. Comp. Vis., pages 38–55,
2018. 2, 6

12104

