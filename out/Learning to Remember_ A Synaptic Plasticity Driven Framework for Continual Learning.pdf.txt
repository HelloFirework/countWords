A Synaptic Plasticity Driven Framework for Continual Learning

Learning to Remember:

Oleksiy Ostapenko§,*, Mihai Puscas¶,*, Tassilo Klein*, Patrick J¨ahnichen§, Moin Nabi*

§Humboldt-Universit¨at zu Berlin, ¶University of Trento, *SAP ML Research

Abstract

Models trained in the context of continual learning (CL)
should be able to learn from a stream of data over an un-
deﬁned period of time. The main challenges herein are:
1) maintaining old knowledge while simultaneously bene-
ﬁting from it when learning new tasks, and 2) guaranteeing
model scalability with a growing amount of data to learn
from.
In order to tackle these challenges, we introduce
Dynamic Generative Memory (DGM) - synaptic plasticity
driven framework for continual learning. DGM relies on
conditional generative adversarial networks with learnable
connection plasticity realized with neural masking. Speciﬁ-
cally, we evaluate two variants of neural masking: applied
to (i) layer activations and (ii) to connection weights di-
rectly. Furthermore, we propose a dynamic network ex-
pansion mechanism that ensures sufﬁcient model capac-
ity to accommodate for continually incoming tasks. The
amount of added capacity is determined dynamically from
the learned binary mask. We evaluate DGM in the continual
class-incremental setup on visual classiﬁcation tasks.

1. Introduction

Conventional Deep Neural Networks (DNN) fail to con-
tinually learn from a stream of data while maintaining
knowledge. Speciﬁcally, reusing old knowledge in new
contexts poses a severe challenge. Generally, there are sev-
eral fundamental obstacles on the way to a continually train-
able AI system:
the problem of forgetting when learning
from new data (catastrophic forgetting), lack of model scal-
ability, i.e. the inability to scale up the model’s size with a
continuously growing amount of training data, and ﬁnally
inability to transfer knowledge across tasks.

Several recent approaches [9, 34, 2, 1] try to mitigate
forgetting in ANNs while simulating synaptic plasticity di-
rectly in the task solving network. It is noteworthy that these
methods topically tackle the task-incremental scenario, i.e.
a separate classiﬁer is trained to make predictions about
each task. This further implies the availability of oracle
knowledge of the task label at inference time. Such eval-

uation is often referred to as multi-head evaluation in which
the task label is associated with a dedicated output head.
Alternatively, other approaches rely on single-head evalua-
tion [22, 2]. Here, the model is evaluated on all classes ob-
served during the training, no matter which task they belong
to. While single-head evaluation does not require oracle
knowledge of the task label, it also does not reduce the out-
put space of the model to the output space of the task. Thus
single-head evaluation represents a harder, yet more realis-
tic setup. Single-head evaluation is predominantly used in
class-incremental setup, in which every newly introduced
data batch contains examples of one to many new classes.

As opposed to the task-incremental situation, models
in class-incremental setup typically require the previously
learned information to be replayed when learning new cat-
egories [22, 2, 18]. The simplest way to accomplish this is
by retaining and replaying real samples of previously seen
categories to the task solver. However, retaining real sam-
ples has several intrinsic implications. First, it is very much
against the notion of bio-inspired design, as natural brains
do not feature the retrieval of information identical to orig-
inally exposed impressions [15]. Second, as pointed out by
[32, 22] storing raw samples of previous data can violate
data privacy and memory restrictions of real-world appli-
cations. Such restrictions are particularly relevant for the
vision domain with its continuously growing dataset sizes
and rigorous privacy constraints.

In this work, we address the “strict” class-incremental
setup. That is, we demand a classiﬁer to learn from a
stream of data with different classes occurring at different
times with no access to previously seen data, i.e. no storing
of real samples is allowed. Such a scenario is solely ad-
dressed by methods relying on generative memory - a gen-
erative network is used to memorize previously seen data
distributions, samples of which can be replayed to the clas-
siﬁer at any time. Several strategies exist to avoid catas-
trophic forgetting in generative networks. The most suc-
cessful approaches rely on deep generative replay (DGR)
[30] - repetitive retraining of the generator on a mix of syn-
thesized samples of previous categories and real samples of
new classes. In this work we propose Dynamic Generative

11321

Memory (DGM) with learnable connection plasticity rep-
resented by parameter level attention mechanism. As op-
posed to DGR, DGM features a single generator that is able
to incrementally learn about new tasks without the need to
replay previous knowledge.

Another important factor in the continual learning set-
ting is the ability to scale, i.e. to maintain sufﬁcient capac-
ity to accommodate for a continuously growing amount of
information. Given invariant resource constraints, it is in-
evitable that with a growing number of tasks to learn, the
model capacity is depleted at some point in time. This issue
is again exacerbated when simulating neural plasticity with
parameter level hard attention masking. In order to guaran-
tee sufﬁcient capacity and constant expressive power of the
underlying DNN, we keep the number of ”free” parameters
(i.e.
to which the gradient updates can be freely applied)
constant by expanding the network with exactly the number
of parameters that were blocked for the previous task.

Our contribution is twofold: (a) we introduce Deep Gen-
erative Memory (DGM) - an adversarially trainable genera-
tive network that features neural plasticity through efﬁcient
learning of a sparse attention masks for the network weights
(DGMw) or layer activations (DGMa); To the best of our
knowledge we are the ﬁrst to introduce weight level masks
that are learned simultaneously with the base network; Fur-
thermore, we conduct it in an adversarial context of a gen-
erative model; DGM is able to incrementally learn new in-
formation during adversarial training without the need to
replay previous knowledge to its generator. (b) We propose
an adaptive network expansion mechanism, facilitating re-
source efﬁcient continual learning. In this context, we com-
pare the proposed method to the state-of-the-art approaches
for continual learning. Finally, we demonstrate that DGMw
accommodates for higher efﬁciency, better parameter re-
usability and slower network growth than DGMa.

2. Related Work

Among the ﬁrst works dealing with catastrophic forget-
ting in the context of lifelong learning are [4, 16, 21], who
tackle this problem by employing shallow neural networks,
whereas our method makes use of modern deep architec-
tures. Lately, a wealth of works dealing with catastrophic
forgetting in context of DNNs have appeared in the litera-
ture, see e.g., [9, 34, 12, 29, 1, 24]. Thus, EWC [9] and
RWalk [2] rely on Fisher’s information to identify param-
eters that carry most of the information about previously
learned tasks, and apply structural regularization to “dis-
courage” change of these parameters. [34] and [1] identify
important parameter segments based on the sensitivity of
the loss or the learned prediction function to changes in the
parameter space.
Instead of relying on “soft” regulariza-
tion techniques, [29] and [26] propose to dedicate separate
parameter subspaces to separate tasks. Serr`a et al. [29] pro-

pose a hard attention to the task (HAT) mechanism. HAT
ﬁnds dedicated parameter subspaces for all tasks in a single
network while allowing them to mutually overlap. The op-
timal solution is then found in the corresponding parameter
subspace of each task. All of these methods have been pro-
posed for a “task-incremental learning” setup. In our work
we speciﬁcally propose a method to overcome catastrophic
forgetting within the “class-incremental” setup. Notably, a
method designed for class-incremental learning can be gen-
erally applied in a task-incremental setup.

Several continuous learning approaches [22, 18, 8], ad-
dress catastrophic forgetting in the class-incremental set-
ting, i.e. by storing raw samples of previously seen data
and making use of them during the training on subsequent
tasks. Thus, iCarl [22] proposes to ﬁnd m most representa-
tive samples of each class whose mean feature space most
closely approximates the entire feature space of the class.
The ﬁnal classiﬁcation task is done by the means of the
nearest mean-of-exemplars classiﬁer.

Recently, there has been a growing interest in employ-
ing deep generative models for memorizing previously seen
data distributions instead of storing old samples. [30, 31]
rely on the idea of generative replay, which requires retrain-
ing the generator at each time step on a mixture of syn-
thesized images of previous classes and real samples from
currently available data. However, apart from being inefﬁ-
cient in training, these approaches are severely prone to “se-
mantic drifting”. Namely, the quality of images generated
during every memory replay highly depends on the images
generated during previous replays, which can result in loss
of quality and forgetting over time. In contrast, we propose
to utilize a single generator that is able to incrementally
learn new information during the normal adversarial train-
ing without the need to replay previous knowledge. This is
achieved through efﬁciently learning a sparse mask for the
learnable units of the generator network.

Similar to our method, [28] proposed to avoid retrain-
ing the generator at every time-step on previous classes by
applying EWC [9] in the generative network. We pursue a
similar goal with the key difference of utilizing a hard atten-
tion mechanism similar to the one described by [29, 13, 14].
All three approaches make use of the techniques origi-
nally proposed in the context of binary-valued networks
[3]. Herein, binary weights are speciﬁcally learned from
a real-valued embedding matrix that is passed through a
binarization function. To this end, [13, 14] learn to mask
a pre-trained network without changing the weights of the
base networks, whereas [29] (HAT) features binary mask-
learning for the layer activations simultaneously to the train-
ing of the base network. While DGMa features HAT-
like layer activation masking, DGMw accomplishes binary
mask learning directly on the weights of the generator.
Other works propose to use non-binary ﬁlters to deﬁne a

11322

Not accessible

…

Real 1

Real 2

Real 𝑡 − 1

𝑍(

𝑍)

…

𝑍& ’(

𝑍&

G

Accessible

Synthesized 1	 … 𝑡

…

Fake 𝑡

Real 𝑡

D

𝐿"#$ .

𝐿&’(.

Figure 1: Dynamic Generative Memory: auxiliary output of D is trained on the real samples of the current task t and synthesized sample
of previously seen tasks 1...t − 1. Adversarial training is accomplished with real and fake samples of the current task. Connection plasticity
simulated with binary mask applied to the generators weights or activations is learned simultaneously to the adversarial training.

new task solving network in terms of a linear combination
of the parameters of a ﬁxed base network [24, 23].

Similarly to [33], we propose to expand the capacity of
the employed base network, in our case the samples gen-
erator. The expansion is performed dynamically with an
increasing amount of attained knowledge. However, [33]
propose to keep track of the semantic drift in every neuron,
and then expand the network by duplicating neurons that are
subject to sharp changes. In contrast, we compute weights
importance concurrently during the course of network train-
ing by modeling the neuron behavior using learnable binary
masks. As a result, our method explicitly does not require
any further network retraining after expansion.

Other approaches like [8, 7, 27] try to explicitly model
short and long term memory with separate networks.
In
contrast to these methods, our approach does not explicitly
keep two separate memory locations, but rather incorporates
it implicitly in a single memory network. Thus, the memory
transfer occurs during the binary mask learning from non-
binary (short term) to completely binary (long term) values.

3. Dynamic Generative Memory

i)}nt

i, yt

Adopting the notation of [2], let St = {(xt

i=1 de-
note a collection of data belonging to the task t ∈ T , where
xt
i ∈ yt are the ground truth la-
i ∈ X is the input data and yt
bels. While in the non-incremental setup the entire dataset
S = ∪|T |
t=0St is available at once, in an incremental setup
it becomes available to the model in chunks St speciﬁcally
only during the learning of task t. Thereby, St can be com-
posed of a collection of items from different classes, or even
from a single class only. Furthermore, at the test time the
output space covers all the labels observed so far featuring
the single head evaluation: Y t = ∪t

j=1yj .

We consider a continual learning setup, in which a task
solving model D has to learn its parameters θD from the
data St being available at the learning time of task t. Task

solver D should be able to maintain good performance on
all classes Y t seen so far during the training. A conventional
DNN, while being trained on St, would adapt its parameters
in a way that exhibits good performance solely on the labels
of the current task yt, the previous tasks would be forgotten.
To overcome this, we introduce a Generative Memory com-
ponent G, who’s task is to memorize previously seen data
distributions. As visualized in Fig. 1, samples of the previ-
ously seen classes are synthesized by G and replayed to the
task solver D at each step of continual learning to maintain
good performance on the entire Y t. We train a generative
adversarial network (GAN)[5] and a sparse mask for the
weights of its generator simultaneously. The learned masks
model connection plasticity of neurons, thus avoiding over-
writing of important units by restricting SGD updates to the
parameter segments of G that exhibit free capacity.

3.1. Learning Binary Masks

We consider a generator network GθG consisting of L
layers, and a discriminator network DθD . In our approach,
DθD serves as both: a discriminator for generated fake sam-
ples of the currently learned task (Ladv.) and as a classiﬁer
for the actual learning problem (Lcls.) following the AC-
GAN [19] architecture. The system has to continually learn
T tasks. During the SGD based training of task t, we learn
a set of binary masks M t = [mt
L] for the weights of
each layer. Output of a fully connected layer l is obtained
by combining the binary mask mt

l with the layer weights:

1, ..., mt

yt
l = σact[(mt

l ◦ Wl)⊤x], Wl ∈ Rn×p,

(1)

for σact being some activation function. Wl is the weight
matrix applied between layer l and l − 1, · ◦ · corresponds to
the Hadamard product. In DGMw mt
l is shaped identically
to Wl, whereas in case of DGMa the mask mt
l is shaped as
1×p and should be expanded to the size of Wl. Extension to
more complex models such as e.g. CNNs is straightforward.

11323

A single binary mask for a layer l and task t is given by:

mt

l = σ(s · et

l),

(2)

where et
l is a real-valued mask embeddings matrix, s is a
positive scaling parameter s ∈ R+, and σ a thresholding
function σ : R → [0, 1]. Similarly to [29] we use the sig-
moid function as a pseudo step-function to ensure gradient
ﬂow to the embeddings e. In training of DGMw, we an-
neal the scaling parameter s incrementally during epoch i
from 1/si
max is similarly
adjusted over the course of I epochs from 1/smax to smax
(global annealing with smax being a ﬁxed meta-parameter).
The annealing the scheme is largely adopted from [29]:

max (local annealing). si

max to si

si
max =

s =

1

smax

1

si
max

+ (smax −

+ (si

max −

1

smax

1

si
max

)

)

i − 1
I − 1
b − 1
B − 1

.

(3)

(4)

Here b ∈ {1, . . . , B} is the batch index and B the number
of batches in each epoch of SGD training. DGMa only fea-
tures global annealing of s, as it showed better performance.
In order to prevent the overwriting of the knowledge re-
lated to previous classes in the generator network, gradients
gl w.r.t.
the weights of each layer l are multiplied by the
reverse of the cumulated mask m≤t

:

l

l = [1 − m≤t
g′

l

]gl, m≤t

l = max(mt

l, mt−1

l

),

(5)

l corresponds to the new gradient matrix and m≤t

l

where g′
is the cumulated mask.

Analogously to [29], we promote sparsity of the binary
mask by adding a regularization term Rt to the loss function
of the AC-GAN[19] generator:

Rt(M t, M t−1) =

PL−1

l=1 PNl
PL−1

i=1 mt
l=1 PNl

i=1 1 − m<t

l,i

l,i(1 − m<t
l,i )

,

(6)

where Nl is the number of parameters of layer l. Here, pa-
rameters that were reserved previously are not penalized,
promoting reuse of units over reserving new ones.

3.2. Dynamic Network Expansion

As discussed by [33], signiﬁcant domain shift between
tasks leads to rapid network capacity exhaustion, manifest-
ing in decreasing expressive power of the underlying net-
work and ultimately in catastrophic forgetting. In case of
DGM this effect will be caused by decreasing number of
“free ” parameters over the course of training due to param-
eter reservation. To avoid this effect, we take measures to
ensure constant number of free parameters for each task.

DGMa. Consider a network layer l with an input vec-
tor of size n, an output vector of size p, and the mask

l ∈ [0, 1]1×p initialized with mask elements m1

m1
l of all
neurons of the layer set to 0.5 (real-valued embeddings e1
l
are initialized with 0). After the initial training cycle on task
1, the number of free output neurons in layer l will decrease
to p − δ1, where δt is the number of neurons reserved for a
generation task t, here t = 1. After the training cycle, the
number of output neurons p of the layer l will be expanded
by δ1. This guarantees that the free capacity of the layer is
kept constant at p neurons for each learning cycle.

′

′

′

DGMw. In case of DGMw, after the initial training cycle
the number of free weights will decrease to np − δ
1, with
δ
1 corresponding to the number of weights reserved for the
generation task 1. The number of output neurons p is ex-
panded by δ
1/n. The number of free weights of the layer is
kept constant, which can be veriﬁed by the following equa-
tion: (p + δt/n)n − δt = np. In practice we extend the
number of output neurons by ⌈δt/n⌉. The number of free
weight parameters in layer l is thus either np, if δt/n ∈ Z,
or np + p, otherwise.

3.3. Training of DGM

The proposed system combines the joint learning of
three tasks: a generative, a discriminative and ﬁnally, a clas-
siﬁcation task in the strictly class-incremental setup.

s − Lt

1, ..., X t

Using task labels as conditions, the generator network
must learn from a training set Xt = {X t
N } to gen-
erate images for task t. To this end, AC-GAN’s conditional
generator synthesizes images xt = GθG (t, z, Mt), where
θG represents the parameters of the generator network, z
denotes a random noise vector. The parameters correspond-
ing to each task are optimized in an alternating fashion. As
such, the generator optimization problem can be seen as
minimizing LG = Lt
c + λRU Rt, with Lc a cross en-
tropy classiﬁcation loss calculated on the the auxiliary out-
put, Ls a discriminative loss function used on the adversar-
ial output layer of the network (implemented to be compli-
ant with architectural requirements of WGAN[6]) , and Rt
the regularizer term expanded upon in equation 6. To pro-
mote efﬁcient parameter utilization, taking into considera-
tion the proportion of the network already in use, the regu-
larization weight λRU is multiplied by the ratio α = St
,
Sf ree
where St is the size of the network before training on task t,
and Sf ree is the number of free neurons. This ensures that
less parameters are reused during early stages of training,
and more during the later stages when the model already
has gained a certain level of maturity.

The discriminator is optimized similarly through mini-
mizing LD = Lt
gp represents
a gradient penalty term implemented as in [6] to ensure a
more stable training process.

gp, where Lt

s + λGP Lt

c + Lt

11324

Method

JT

c
i
d
o
s
i
p
E

y iCarl-S [22]
EWC-S[9]
RWalk-S[2]
PI-S [34]

r
o
m
e
m

.
t
a
r
e
n
e
G

y
r
o
m
e
m

EWC-M [28]
DGR [30]
MeRGAN [31]

DGMw (ours)
DGMa (ours)

MNIST (%)
A10
A5

SVHN(%)
A10
A5

CIFAR10(%)
A10
A5

ImageNet-50(%)
A30

A50

97.66

98.10

85.30

84.82

82.20

64.20

57.35

49.88

-
-
-
-

70.62
90.39
98.19

98.75
99.17

55.8
79.7
82.5
78.7

77.03
85.40
97.00

96.46
97.92

-
-
-
-

-
-
-
-

39.84
61.29
80.90

83.93
81.07

33.02
47.28
66.78

74.38
66.89

-
-
-
-

-
-
-

-
-
-
-

-
-
-

29.38

28.98

-
-
-

-
-
-

-
-
-

-
-
-

64.94
62.50

51.70
50.80

32.14
25.93

17.82
15.16

Table 1: Comparison to the benchmark presented by [2] (episodic memory with real samples) and [31] (generative memory) of approaches
evaluated in class-incremental setup. Joint training (JT) represents the upper bound. Both variants of our method are evaluated.

4. Experimental Results

We perform experiments measuring the classiﬁcation ac-
curacy of our system in a strictly class-incremental setup
on the following benchmark datasets: MNIST [11], SVHN
[17], CIFAR-10 [10], and ImageNet-50 [25]. Similarly to
[22, 31, 2] we report an average accuracy (At) over the held-
out test sets of classes 0...t seen so far during the training.

Datasets. The MNIST and SVHN datasets are com-
posed of 60000 and 99289 images respectively, containing
digits. The main difference is in the complexity and vari-
ance of the data used. SVHN’s images are cropped pho-
tos containing house numbers and as such present varying
viewpoints, illuminations, etc. CIFAR10 contains 60000 la-
beled images, split into 10 classes, roughly 6k images per
class. Finally, we use a subset of the iILSVRC-2012 dataset
containing 50 classes with 1300 images per category. All
images are further resized to 32 x 32 before use.

Implementation details. We make use of the same ar-
chitecture for the MNIST and SVHN experiments, a 3-layer
DCGAN [20], with the generator’s number of parameters
modiﬁed to be proportionally smaller than in [31] (10%
of MeRGAN’s size for DGMw, and 44% for DGMa on
MNIST and SVHN). The projection and reshape operation
is performed with a convolutional layer instead of a fully
connected one. For the CIFAR-10 experiments, we use the
ResNet architecture proposed by [20]. For the ImageNet-50
benchmark, the discriminator features a ResNet-18 archi-
tecture. All are modiﬁed to function as an AC-GAN[19].

All datasets are used to train a classiﬁcation network in
an incremental way. The performance of our method is eval-
uated quantitatively through comparison with benchmark
methods. Note that we compare our method mainly to the
approaches that rely on the idea of generative memory re-

Figure 2: Top-5 performance of DGMw together with upper and
lower performance bounds measured for ImageNet-50 benchmark.
DGM+real denotes variation with different ratios of real samples
added to the replay loop (25%-75% of samples being real)

play, e.g. replaying generator synthesized samples of previ-
ous classes to the task solver without storing real samples of
old data. For the sake of fairness, we only consider bench-
marks evaluated in class-incremental single-head evaluation
setup. Hereby, to best of our knowledge [31] represent the
state-of-the-art benchmark followed by [30] and [28]. Next,
we relax the strict incremental setup and allow partial stor-
age of real samples of previous classes. Here we compare
to the iCarl [22], which is the state-of-the-art method for
continual learning with storing real samples.

Results. A quantitative comparison of both variants
of the proposed approach with other methods is listed in
Tab. 1. We use joint training (JT) as an upper perfor-
mance bound, where the task solver D is trained in a non-
incremental fashion on all real samples without adversarial
training being involved. The ﬁrst set of methods evaluated
by [2] do not adhere to the strictly incremental setup, and
thus make use of stored samples, which is often referred to
as ”episodic memory”. The second set of methods we com-

11325

01245Task20406080100Accuracy (%)JTDGMwNo replayDGMw+real(a) Parameter re-usability ratios.

(b) Mask learning trajectories DGMa

Figure 3: Masks learning dynamics. Fig. (a) illustrates the ratio of newly blocked and reused neurons over the total number of used
neurons for a task t. Fig. (b) illustrates trajectories of mask value change for DGMa for a selected layer of G (bold line - layer occupation).

Method

JT
iCarl (K=1000)
iCarl (K=2000)
DGMw (K=1000)
DGMw (K=2000)
DGMw (r=0.75)
DGMw (r=0.5)
DGMw (r=0.25)
DGMw (r=0.1)
DGMw (r=0)

Top-1(%)

Top-5(%)

A30

A50

A30

A50

57.35
29.38
39.38
36.87
41.93
50.80
48.80
46.93
41.67
32.14

49.88
28.98
29.96
18.84
22.56
38.22
40.72
35.04
30.48
17.82

84.70
69.98
70.57
69.13
69.20
78.27
79.40
75.80
71.80
62.53

78.24
59.49
60.07
43.12
51.84
64.64
71.72
65.60
61.04
40.76

Table 2: Performance comparison of DGM and iCarl for different
values of r and memory size K.

pare with do not store any real data samples. Our method
outperforms the state of the art [28, 30] on the MNIST and
SVHN benchmarks through the integration of the memory
learning mechanism directly into the generator, and the ex-
pansion of said network as it saturates to accommodate new
information. We yield an increase in performance over [31],
a method that is based on a replay strategy for the gener-
ator and does not provide dynamic expansion mechanism
of the memory network, leading to increased training time
and sensitivity to semantic drift. As it can be observed for
both, our method and [31], the accuracy reported between
5 and 10-tasks of the MNIST benchmark has changed a lit-
tle, suggesting that for this dataset and evaluation method-
ology both approaches have largely curbed the effects of
catastrophic forgetting. Interestingly, DGM and MeRGAN
outperform JT on MNIST (A5) using the same architec-
ture. This suggests that the strictly incremental training
methodology forced the network to learn better generaliza-
tions compared to what it would learn given all the data.

Given the high accuracy reached on the MNIST dataset
largely gives rise to questions concerning saturation, we
opted to perform a further evaluation on the more visually

diverse SVHN dataset. In this context, increased data di-
versity translates to more difﬁcult generation and suscep-
tibility to catastrophic forgetting.
In fact, as can be seen
in Tab. 1, the difference between 5- and 10-task accuracies
is signiﬁcantly larger in all methods than what can be ob-
served in the MNIST experiments. DGM strongly outper-
forms all other methods on the SVHN benchmark. This can
be attributed primarily to the efﬁcient network expansion
that allows for more redundancy in reserving representative
neurons for each task, and a less destructive joint use of
neurons between tasks. Additionally, replay based methods
(like [30, 31]) can be potentially prone to generation of sam-
ples that represent class mixtures, especially for classes that
semantically interfere with each other. DGM is immune to
this problematic, since no generative replay is involved in
the generator’s training. Thus, DGM becomes more stable
in the face of catastrophic forgetting.

The quality of the generated images after 10 stages of in-
cremental training for MNIST and SVHN can be observed
in Fig. 5. The generative network is able to provide an in-
formative and diverse set of samples for all previously seen
classes without catastrophic forgetting.

Finally, in the ImageNet-50 benchmark, we incremen-
tally add 50 classes with 10 classes per step and evaluate the
classiﬁcation performance of DGM using single-head eval-
uation. The dynamics of the top-5 classiﬁcation accuracy of
our system is provided in Fig. 2. Looking at the qualitative
results shown in Fig. 5, it can be observed that generated
samples clearly feature class discriminative features which
are not forgotten after incremental training on 5 tasks of
the benchmark. Nevertheless, for each newly learned task
the discriminator network’s classiﬁcation layer is extended
with 10 new outputs, making the complexity of the classi-
ﬁcation problem to grow constantly (from 10-way classi-
ﬁcation to 50-way classiﬁcation). With the more complex
ImageNet samples also the generation task becomes much
harder than in datasets like MNIST and SVHN. These fac-
tors negatively impact the classiﬁcation performance of the

11326

012345678Task20406080100%Newly blocked ratio (NB)Reused ratio (R)010203040506070Epoch0.00.20.40.60.81.0Mask valuetask solver presented in Fig. 2, where DGMw performs sig-
niﬁcantly worse than the JT upper bound.

Figure 4: Network growth (numb. of neurons) in the incremental
MNIST setup. Comparison of best performing DGMw and com-
parable DGMa (DGMw A10: 96.46%, DGMa - 96.98%).

Next, we relax the strict incremental setup and allow the
DGM to partially store real samples of previous classes. We
compare the performance of DGM to the state-of-the-art
iCarl [22]1. Noteworthy, iCarl relies only on storing real
samples of previous classes introducing a smart sample se-
lection strategy. We deﬁne a ratio of stored real and total
replayed samples r = nr/N , where N is the total num-
ber of samples replayed per class and nr is the number of
randomly selected real samples stored per each previously
seen class. To keep the number of replayed samples bal-
anced with the number of real samples of the currently ob-
served classes, N is set to be equal to the average number of
samples per class in the currently observed data chunk St.
Furthermore, similarly to iCarl [22] we deﬁne K to be the
total number of real samples that can be stored by the algo-
rithm at any point of time. We compare DGMw with iCarl
for different values of K allowing the storage of K/|Y t|
samples per class.

From Tab. 2 we observe that DGM is outperformed by
iCarl when no real samples are replayed (i.e. r = 0) after
50 classes in top-1 and after 30 and 50 classes in top-5 accu-
racy. DGMw with r = 0 outperforms iCarl with K = 1000
in top-1 accuracy after 30 classes. Furthermore, we observe
that adding real samples to the replay loop boosts DGM’s
classiﬁcation accuracy beyond the iCarl’s one. Thus, al-
ready for r = 0.1 the performance of our system can be
improved signiﬁcantly. We now consider DGM and iCarl
with the same memory size K (we test for K = 1000 and
K = 2000). Here DGM outperforms iCarl in top-1 accu-
racy after 30 classes, and almost reaches it in Top-5 accu-
racy. This is largely attributed to the advantage of DGM us-
ing generated samples additionally to the stored real once.
Yet, a signiﬁcant performance drop is observed after learn-
ing 5 tasks (A50), where DGMw is outperformed by iCarl.
This can be attributed to (a) the fact that the number of sam-
ples replayed per class decreases over time due to ﬁxed K

1 We use classes of ImageNet-50 with 32 × 32 resolution and the iCarl

implementation under https://github.com/srebuffi/iCaRL

Dataset

MNIST

SVHN

Method
DGMw

Size init.
5.58e+4
MeRGAN 5.18e+5

Size ﬁnal A10(%)
3.83e+5
5.18e+5

96.46
97.00

DGMw

5.58e+4
MeRGAN 5.18e+5

3.99e+5
5.18e+5

74.38
66.78

Table 3: Comparison of generator’s base network’s size (number
of parameters) of DGMw and MeRGAN.

and increasing number of classes (e.g. for K = 2000, 66
samples are played per class after seeing 30 classes, and 40
samples after 50 classes), as well as (b) iCarl’s smart sam-
ples selection strategy that favors samples that better ap-
proximate the mean of all training samples per class. Such
samples selection strategy appears to works better in a situ-
ation where the number of real samples available per class
decreases over time. It is noteworthy that iCarl’s samples
selection strategy can also be applied to DGM.

Growth Pattern Analysis. One of the primary strengths
of DGM is an efﬁcient generator network expansion com-
ponent, removing which would lead to the inability of the
generator to accommodate for memorizing new task. Per-
formance of DGM is directly related to how the network
parameters are reserved during incremental learning,which
ultimately depends on the generator’s ability to general-
ize from previously learned tasks. Fig. 4 reports network
growth against the number of tasks learned. We ﬁnd that
learning masks directly for the layer weights (DGMw) sig-
niﬁcantly slows down the network growth. Furthermore,
one can observe the high efﬁciency of DGM’s sub-linear
growth pattern as compared to the worst-case linear growth
scenario. Interestingly, as shown in Tab.3, after incremen-
tally learning 10 classes the ﬁnal number of generator’s base
network’s parameters is lower than the one of the bench-
marked MeRGAN [31]. More speciﬁcally, we observe the
ﬁnal network’s size reduction of 26% on MNIST, and 23%
on SVHN as compared to MeRGAN’s ﬁxed generator. In
general, growth pattern of DGM depends on various fac-
tors: e.g. initialization size, similarity and order of classes
etc.. A rather low saturation tendency of DGM’s growth
pattern observed in Fig. 4 can be attributed to the fact that
with growing amount of information stored in the network,
selecting relevant knowledge becomes increasingly hard.

Plasticity Evolution Analysis. We analyze how learn-
ing is accomplished within a given task t, and how this
further affects the wider algorithm. For a given task t, its
binary mask Mt is initialized with the scaling parameter
s = 1. Fig. 3(b) shows the learning trajectories of the mask
values over the learning time of task t. Here, at task ini-
tialization of DGMa the mask is completely non-binary (all
mask values are 0.5). As training progresses, the scaling pa-
rameter s is annealed, the network is encouraged to search

11327

0123456789Task200400Number of neuronsLinear growthDGMaDGMwFigure 5: Images generated by DGM for MNIST(top), SVHN(middle) after training on 10 tasks, and ImageNet(bottom) after 5 tasks.

for the most efﬁcient parameter constellation (epoch 2-10).
But with most mask values near 0 (most of the units are not
used, high efﬁciency is reached), the network’s capacity to
learn is greatly curtailed. The optimization process pushes
the mask to become less sparse, the number of non-zero
mask values is steadily increasing until the optimal mask
constellation is found, a trend observed in the segment be-
tween the epoch 10 and 55. This behaviour can be seen as
a short-term memory formation - if learning was stopped at
e.g. epoch 40 only a relatively small fraction of learnable
units would be masked in a binary way, the units with non-
binary mask values would be still partially overwritten dur-
ing the subsequent learning resulting in forgetting. A tran-
sition from short to the long-term memory occurs largely
within the epochs 45-65. Here the most representative units
are selected and reserved by the network, parameters that
have not made this transition are essentially left as unused
for the learning task t. Finally, the optimal neuron constella-
tion is optimized for the given task from epoch 60 onwards.
For a given task t, masked units (neurons in DGMa, net-
work weights in DGMw) can be broadly divided into three
types: (i) units that are not used at all (U) [masked with 0]
, (ii) units that are newly blocked for the task (N Bt), (iii)
units that have been reused from previous tasks (Rt). Fig-
ure 3(a) presents the evolution of the ratio of the (N Bt) and
(Rt) types over the total number of units blocked for the
task t. Of particular importance is that the ratio of reused
units is increasing between tasks, while the ratio of newly
blocked units is decreasing. These trends can be justiﬁed by
the network learning to generalize better, leading to a more
efﬁcient capacity allocation for new tasks.

Memory Usage Analysis. We evaluate the viability
of generative memory usage from the perspective of re-
quired disc space. Storing the generator for the ImageNet-
50 benchmark (weights and masks) corresponds to the disc
space requirement of 228M B. Thereby storing the prepro-
cessed training samples of ImageNet-50 results in the re-
quired disc space of 315M B. In this particular case storing
the generator 27.5% more memory efﬁcient than storing the
training samples. Naturally, this effect will become more

pronounced for larger datasets.

As discussed in Sec. 4, DGMw features a more efﬁ-
cient network growth pattern as compared to DGMa. Yet,
DGMw’s attention masks are shaped identically to the
weight matrices and thus require more memory. Tab. 4
gives an overview of the required disc space for different
components of DGMa and DGMw (masks are stored in
a sparse form). Less total disc space is required to store
DGMw’s model as compared to DGMa, which suggests
that DGMw’s model growth efﬁciency compensates for the
higher memory required for storing its masks. During the
training, DGMw still exhibits a larger memory consump-
tion, as the real-valued mask embeddings for the currently
learned task must be kept in memory in a non-sparse form.

Size5(MB)

Size10(MB)

DGMa DGMw DGMa DGMw

Weights
Masks

Total

6.7
0.4

7.1

1.1
3.8

4.9

14.0
0.8

14.8

2.0
9.9

11.9

Table 4: Disc space required to store different components of
DGMw and DGMa in Megabytes (MB) after 5 and 10 tasks. Com-
pared models exhibit comparable performance on MNIST.

5. Conclusion

In this work we study the continual learning problem in a
single-head, strictly incremental context. We propose a Dy-
namic Generative Memory approach for class-incremental
continual learning. Our results suggest that DGM success-
fully overcomes catastrophic forgetting by making use of
a conditional generative adversarial model where the gen-
erator is used as a memory module endowed with neural
masking. We ﬁnd that neural masking works more efﬁcient
when applied directly to layers’ weights instead of activa-
tions. Future work will address the limitations of the DGM
including missing backward knowledge transfer and limited
saturation of the network growth pattern.

11328

References

[1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and
T. Tuytelaars. Memory aware synapses: Learning what (not)
to forget. CoRR, abs/1711.09601, 2017.

[2] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S. Torr.
Riemannian walk for incremental learning: Understanding
forgetting and intransigence. CoRR, abs/1801.10112, 2018.
[3] M. Courbariaux, Y. Bengio, and J.-P. B. David. Training
deep neural networks with binary weights during propaga-
tions. arxiv preprint. arXiv preprint arXiv:1511.00363, 2015.
[4] R. M. French. Catastrophic forgetting in connectionist net-

works. Trends in cognitive sciences, 3(4):128–135, 1999.

[5] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and
Y. Bengio. An empirical investigation of catastrophic for-
getting in gradient-based neural networks. arXiv preprint
arXiv:1312.6211, 2013.

[6] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. C. Courville. Improved training of wasserstein gans. In
Advances in Neural Information Processing Systems, pages
5767–5777, 2017.

[7] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual
arXiv preprint

memory network for continual learning.
arXiv:1710.10368, 2017.

[8] R. Kemker and C. Kanan. Fearnet: Brain-inspired model
for incremental learning. arXiv preprint arXiv:1711.10563,
2017.

[9] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness,
G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,
A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Ku-
maran, and R. Hadsell. Overcoming catastrophic forgetting
in neural networks. CoRR, abs/1612.00796, 2016.

[10] A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset.

online: http://www. cs. toronto. edu/kriz/cifar. html, 2014.

[11] Y. LeCun.

The mnist database of handwritten digits.

http://yann. lecun. com/exdb/mnist/, 1998.

[12] Z. Li and D. Hoiem. Learning without forgetting. CoRR,

abs/1606.09282, 2016.

[13] A. Mallya and S. Lazebnik. Piggyback: Adding multiple
tasks to a single, ﬁxed network by learning to mask. arXiv
preprint arXiv:1801.06519, 2018.

[14] M. Mancini, E. Ricci, B. Caputo, and S. R. Bul`o. Adding
new tasks to a single network with weight trasformations us-
ing binary masks. arXiv preprint arXiv:1805.11119, 2018.

[15] M. Mayford, S. A. Siegelbaum, and E. R. Kandel. Synapses
and memory storage. Cold Spring Harbor perspectives in
biology, page a005751, 2012.

[16] M. McCloskey and N. J. Cohen. Catastrophic interference
in connectionist networks: The sequential learning problem.
volume 24 of Psychology of Learning and Motivation, pages
109 – 165. Academic Press, 1989.

[17] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011.

[18] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational
continual learning. arXiv preprint arXiv:1710.10628, 2017.

[19] A. Odena, C. Olah, and J. Shlens. Conditional image
arXiv preprint

synthesis with auxiliary classiﬁer gans.
arXiv:1610.09585, 2016.

[20] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015.

[21] R. Ratcliff. Connectionist models of recognition memory:
Constraints imposed by learning and forgetting functions.
Psychological Review, pages 285–308, 1990.

[22] S. Rebufﬁ, A. Kolesnikov, and C. H. Lampert.

icarl: In-
cremental classiﬁer and representation learning. CoRR,
abs/1611.07725, 2016.

[23] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi.

parametrization of multi-domain deep neural network.
CVPR, 2018.

Efﬁcient
In

[24] A. Rosenfeld and J. K. Tsotsos. Incremental learning through

deep adaptation. TPAMI, 2018.

[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015.

[26] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,
J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell.
Progressive neural networks. CoRR, abs/1606.04671, 2016.
[27] J. Schwarz, J. Luketina, W. M. Czarnecki, A. Grabska-
Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell. Progress
& compress: A scalable framework for continual learning.
arXiv preprint arXiv:1805.06370, 2018.

[28] A. Seff, A. Beatson, D. Suo, and H. Liu.

learning in generative adversarial nets.
arXiv:1705.08395, 2017.

Continual
arXiv preprint

[29] J. Serr`a, D. Sur´ıs, M. Miron, and A. Karatzoglou. Overcom-
ing catastrophic forgetting with hard attention to the task.
CoRR, abs/1801.01423, 2018.

[30] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning
with deep generative replay. In Advances in Neural Informa-
tion Processing Systems, pages 2990–2999, 2017.

[31] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and
B. Raducanu. Memory Replay GANs: learning to generate
images from new categories without forgetting. In Advances
In Neural Information Processing Systems, 2018.

[32] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, Z. Zhang,
Incremental classiﬁer learning with generative

and Y. Fu.
adversarial networks. CoRR, abs/1802.00853, 2018.

[33] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning

with dynamically expandable networks. 2018.

[34] F. Zenke, B. Poole, and S. Ganguli.

titask learning through synaptic intelligence.
abs/1703.04200, 2017.

Improved mul-
CoRR,

11329

