SCOPS: Self-Supervised Co-Part Segmentation

Wei-Chih Hung1*, Varun Jampani2, Sifei Liu2, Pavlo Molchanov2, Ming-Hsuan Yang1, and Jan Kautz2

1UC Merced 2NVIDIA

Figure 1: Robustness to variations. Sample part segmentation obtained by SCOPS on different types of image collections:
(left) unaligned faces from CelebA [29], (middle) birds from CUB [44] and (right) horses from PASCAL VOC [11] dataset
images, showing that SCOPS can be robust to appearance, viewpoint and pose variations.

Abstract

Parts provide a good intermediate representation of ob-
jects that is robust with respect to the camera, pose and ap-
pearance variations. Existing works on part segmentation
is dominated by supervised approaches that rely on large
amounts of manual annotations and can not generalize to
unseen object categories. We propose a self-supervised
deep learning approach for part segmentation, where we
devise several loss functions that aids in predicting part seg-
ments that are geometrically concentrated, robust to object
variations and are also semantically consistent across dif-
ferent object instances. Extensive experiments on different
types of image collections demonstrate that our approach
can produce part segments that adhere to object bound-
aries and also more semantically consistent across object
instances compared to existing self-supervised techniques.

1. Introduction

Much of the computer vision involves analyzing objects
surrounding us, such as humans, cars, furniture, and so
on. A major challenge in analyzing objects is to develop
a model that is robust to the multitude of object transforma-
tions and deformations due to changes in camera pose, oc-

*This work is done when the author was doing internship at NVIDIA.

clusions, object appearance, and pose variations. Parts pro-
vide a good intermediate representation of objects that is ro-
bust with respect to these variations. As a result, part-based
representations are used in a wide range of object analysis
tasks such as 3D reconstruction [55], detection [12], ﬁne-
grained recognition [25], pose estimation [20], etc.

Several types of 2D part representations have been used
in the literature, with the three most common ones being
landmarks, bounding boxes, and part segmentations. A
common approach to the part analysis is to ﬁrst manually
annotate large amounts of data and then leverage fully-
supervised approaches to recognize parts [9, 29, 2, 4, 5].
However, these annotations, especially part segmentation,
are often quite costly. The annotations are also speciﬁc to a
single object category and usually do not generalize to other
object classes. Consequently, it is difﬁcult to scale the fully-
supervised models to unseen categories and there is a need
for weakly supervised techniques for part recognition that
only rely on very weak supervision or no supervision at all.

Part representations, once obtained, are robust to vari-
ations and help in high-level object understanding. How-
ever, obtaining part segmentations is challenging due to the
above mentioned intra-class variations. An image collec-
tion of a single object category, despite having the same
category objects, usually have high variability regarding
pose, object appearances, camera viewpoint, the presence

1869

of multiple objects, etc. Figure 1 shows some sample im-
ages from three different image collections. Notice the vari-
ability across different object instances. Any weakly or un-
supervised technique for part segmentation needs to reason
about correspondences between different images which is
challenging in such diverse image collections.

In this work, we propose a self-supervised deep learn-
ing framework for part segmentation. Given only an image
collection of the same object category, our model can learn
part segmentations that are semantically consistent across
different object instances. Our learning technique is class
agnostic, i.e., can be applied to any type of rigid or non-rigid
object categories. And, we only use very weak supervision
in the form of ImageNet pre-trained features [26, 39, 17],
which are readily available. Contrary to recent deep learn-
ing techniques [42, 41, 50], which learn landmarks (key-
points) in a weakly or un-supervised manner, our network
predicts part segmentation which provides much richer in-
termediate object representation compared to landmarks or
bounding boxes.

To train our segmentation network, we consider several
properties of a good part segmentation and encode that prior
knowledge into the loss functions. Speciﬁcally, we consider
four desirable characteristics of a part segmentation:

• Geometric concentration: Parts are concentrated geo-

metrically and form connected components.

• Robustness to variations: Part segments are robust
to object deformations due to pose

with respect
changes as well as camera and viewpoint changes.

• Semantic consistency: Part segments should be seman-
tically consistent across different object instances with
appearance and pose variations.

• Objects as union of parts: Parts appear on objects (not

background) and the union of parts forms an object.

We devise loss functions that favor part segmentations
that has above-mentioned qualities and use these loss func-
tions to train our part segmentation network. We discuss
these loss functions in detail in Section 3. We call our part
segmentation network “SCOPS” (Self-Supervised Co-Part
Segmentation). Figure 1 shows sample image collections
and the corresponding part segmentations that SCOPS pre-
dicts. These visual results indicate that SCOPS can estimate
part segmentations that are semantically consistent across
object instances despite large variability across object in-
stances.

When compared to recent unsupervised landmark de-
tection approaches [42, 41, 50], our approach is relatively
robust to appearance variations while also handling occlu-
sions. Moreover, our approach can handle multiple ob-
ject instances in an image which is not possible via land-
mark estimation with a ﬁxed number of landmarks. When
compared to the recent Deep Feature Factorization (DFF)

approach [10], ours can scale to larger datasets, can pro-
duce sharper part segments that adhere to object bound-
aries and also more semantically consistent across object
instances. We quantitatively evaluate our part segmenta-
tion results with an indirect measure of landmark estima-
tion accuracy on unaligned CelebA [29], AFLW [22] and
CUB [44] dataset images, and also with foreground seg-
mentation accuracy on the PASCAL VOC dataset [11]. Re-
sults indicate that SCOPS consistently performs favorably
against recent techniques. In summary, we propose a self-
supervised deep network that can predict part segmenta-
tions that are semantically consistent across object instances
while being relatively robust to object pose and appearance
variations, camera variations and occlusions.

2. Related Works

Object concept discovery CNNs have shown impressive
generalization capabilities across different computer vision
tasks [45, 35, 1]. As a result, several works try to in-
terpret and visualize the intermediate CNN representations
[49, 52, 3]. While some recent works [14, 3] demonstrate
the presence of object part information in pre-trained CNN
features, we aim to train a CNN that can predict consis-
tent part segmentations in a self-supervised manner. Some-
what similar to our objective, class activation maps (CAMs)
based methods [53, 34] propose to localize the dense re-
sponse on image with respect to a trained classiﬁer. How-
ever, without a learned part classiﬁer, CAMs cannot be di-
rectly applied to our problem setting. Recently, Collins et
al. [10] propose deep feature factorization (DFF) to estimate
the common part segments in images through Non-negative
Matrix Factorization (NMF) [27] on the ImageNet CNN
features. However, DFF requires joint optimization during
inference time, and it is costly to impose other constraints or
loss functions on part maps since there is no standalone in-
ference module. By posing as neural network inference, the
proposed SCOPS can readily leverage the wealth of neural
network loss functions developed in recent years. Any ad-
ditional constraints can be jointly optimized during training
time on large scale datasets, and the trained segmentation
network could be applied on a single image during infer-
ence.

Landmark detection Recently, several techniques have
been proposed to learn landmarks with weak or no super-
vision. Most of these works rely on geometric constrains
and landmarks equivariance to transformations. Thewlis
et al. [42] relied on geometric priors to learn landmarks
that are invariant
to afﬁne and spline transformations.
Zhang et al. [50] added reconstruction loss by reconstruct-
ing a given input image with predicted landmarks and local
features. Honari et al. [18] used a subset of labeled images
and sequential multitasking to improve ﬁnal landmark es-
timation. Simon et al. [38] used multi-view bootstrapping

870

to improve accuracy of hand landmark estimation. Suwa-
janakorn et al. [40] used multiple geometry aware losses
to discover 3D landmarks. In order to obtain unsupervised
landmarks, most of these works rely on simpliﬁed problem
settings such as using cropped images with only a single ob-
ject instance per image and allowing only minor occlusions.
We aim to predict part segments which provide richer rep-
resentation of objects compared to landmarks.

Dense image alignment Part segmentation is also related
to the task of dense alignment, where the objective is to
densely match pixels or landmarks from an object to an-
other object instance. While conventional approaches uti-
lize off-the-shelf feature descriptors matching to tackle the
problem, e.g., SIFT ﬂow based methods [28, 21, 6], recent
works [16, 46, 47, 48, 15] utilize annotated landmark pairs
and deep neural networks to learn a better feature descrip-
tor or matching function. To avoid the cost of dense anno-
tation, recent works propose to learn the dense alignment
under the weakly supervised setting where only image pairs
are required. Rocco et al. [30, 31] propose to jointly train
the feature descriptor and spatial transformation by maxi-
mizing the inlier count, while Shu et al. [37] propose De-
forming Autoencoder to align faces and disentangle expres-
sions. However, these weakly supervised methods assume
a certain family of spatial transforms, e.g., afﬁne or thin
plate spline grid, to align objects with similar poses. We ar-
gue that part segmentation is a more natural representation
for semantic correspondences since matching each pixel be-
tween different instances would be an ill-posed problem.
Part segmentations can also provide complex object defor-
mations without heavily parameterized spatial transforms.

Image co-segmentation Co-segmentation approaches pre-
dict the foreground pixels of the speciﬁc object given an
image collection. Most existing works [24, 32, 19, 43, 33]
jointly consider all images within the collection to gener-
ate the ﬁnal foreground segments via energy maximization,
and thus not suitable for testing on standalone images. In
contrast, we propose an end-to-end trainable network that
takes single image as input and outputs part segmentation
which is more challenging but provides more information
compared to foreground segmentation.

3. Self-Supervised Co-Part Segmentation

Given an image collection of the same object category,
we aim to learn a deep neural network that takes a single
image as input and outputs part segmentations. As out-
lined in Section 1, we focus on the important characteris-
tics of part segmentation and devise loss functions that en-
dorse these properties: geometric concentration, robustness
to variations, semantic consistency, and objects as the union
of parts. Here, we ﬁrst describe our overall framework fol-
lowed by the description of different loss functions and how
they encourage the above-mentioned properties. Along the

Input Image

Part Segmentation

Self-Supervised Constraints

Geometric
(Sec. 3.2)

Equivariance

(Sec. 3.3)

Semantic 
Consistency

(Sec. 3.4)

Part Segmentation Network

Figure 2: SCOPS framework. Our network takes single
image as input and predicts part segmentation. Geomet-
ric, Equivariance and Semantic Consistency constraints are
used to train the network in a self-supervised manner.

way, we also comment on how our loss functions are related
and different to existing loss functions in the literature.

3.1. Overall Framework

Figure 2 shows the overall framework of our proposed
method. Given an image collection {I} of the same object
category, we train a part segmentation network F param-
eterized by θf , which is a fully convolutional neural net-
work (FCN [36]) with a channel-wise softmax layer in the
end, to generate the part response maps R = F(I; θf ) ∈
[0, 1](K+1)×H×W , where K denotes the number of parts
and H × W is the image resolution. Our network pre-
dicts K + 1 channels with an additional channel indicat-
ing the background. To obtain the ﬁnal part segmenta-
tion results, we ﬁrst normalize each part map with it maxi-
mum response value in the spatial dimensions ˆR(k, i, j) =
R(k, i, j)/ maxu,v(R(k, u, v)), and we set the background
map as constant with value 0.1. The purpose of this nor-
malization is to enhance weak part responses. Then the
part segmentation is obtained with the arg max function
along the channel dimension. We use DeepLab-V2 [8] with
ResNet50 [17] as our part segmentation network.

Since we do not assume the availability of any ground
truth segmentation annotations, we formulate several con-
straints as differentiable loss functions to encourage the
above mentioned desired properties of a part segmenta-
tion, such as geometry concentration and semantic consis-
tency. The overall loss function for part segmentation net-
work is a weighted sum of different loss functions which
we describe next. Contrary to several co-segmentation ap-
proaches [24, 32, 19, 43, 33], which require multiple images
during test-time inference, our network only takes a single
image as input during the test time resulting in better porta-
bility of our trained model to unseen test images.

3.2. Geometric Concentration Loss

Pixels belonging to the same object part are usually
spatially concentrated within an image and form a con-
nected component unless there are occlusions or multiple
instances. To this end, we ﬁrst impose the geometric con-

871

Spatial Transform

𝑇"($)

shared

Spatial Transform

𝑇"($)

Color Jittering

𝑇&($)

s
h
a
r
e
d

Equivariance Loss

𝐿 ()* (Eqn. 3)

Figure 3: Equivariance loss. We transform a given image
with a random spatial transform and color jittering. We also
transform the part segmentation of the given image using
the same spatial transform to compare against the part seg-
mentation of the transformed image via equivariance loss.

centration on the part response maps to shape the part seg-
ments. Speciﬁcally, we utilize a loss term that encourages
all the pixels belonging to a part to be spatially close to the
part center. The part center for a part k along axis u is cal-
culated as

u = X
ck

u,v

u · R(k, u, v)/zk,

(1)

where zk = Pu,v
R(k, u, v) is the normalization term to
transform the part response map into a spatial probability
distribution function. Then, we formulate the geometric
concentration loss as

Lcon = X

X

k

u,v

||hu, vi − hck

u, ck

vi||2 · R(k, u, v)/zk, (2)

and it is differentiable with respect to ck
u, R(k, u, v), and zk.
This loss function encourages geometric concentration of
parts and tries to minimize the variance of spatial probabil-
ity distribution function R(k, u, v)/zk. This loss is closely
related to ones used in recent unsupervised landmark esti-
mation techniques [50, 42]. While Zhang et al. [50] ap-
proximate the landmark response maps with Gaussian dis-
tributions, we apply concentration loss mainly for penaliz-
ing part responses away from the part center.

Besides concentration loss, [50] and [42] propose a form
of separation (diversity) loss that maximizes the distance
between different landmarks. However, we do not employ
such constraint as this constraint would results in separated
part segments with background pixels in between.

3.3. Equivariance Loss

The second property that we want to advocate is that part
segmentation should be robust to the appearance and pose
variations. Figure 3 illustrates how we employ the equivari-
ance constraints to encourage the robustness to variations.
For each training image, we draw a random spatial trans-
form Ts(·) and appearance perturbation Ta(·) from a pre-
deﬁned parameter range. The detailed transform parame-
ters are present in the supplementary material. Then we
pass both the input image I and transformed image I ′ =
Ts(Ta(I)) through the segmentation network and obtain the
corresponding response maps R and R′. Given these part
response maps, we compute the part centers hck
vi and
hck′
u , ck′
v i using Eqn. 1. Then, the equivariance loss is de-
ﬁned as

u, ck

Leqv = λs

eqvDKL(R′||Ts(R))
eqv X

u , ck′

||hck′

+ λc

v i − Ts(hck

u, ck

vi)||2,

(3)

k

eqv, λc

where DKL(·) is the Kullback–Leibler divergence distance,
and λs
eqv are the loss balancing coefﬁcients. The ﬁrst
term corresponds to the part segmentation equivariance, and
the second term denotes the part center equivariance. We
use random similarity transformations (scale, rotation, and
shifting) for spatial transforms. We also experimented with
more complex transformations such as projective and thin-
plate-spline transformations, but did not observe any im-
provements in part segmentation.

Recent works on unsupervised landmark estimation [50,
42] use the above-mentioned equivariance loss on land-
marks (part centers). In this work, we extend the equivari-
ance loss to part segmentation, and our experiments indicate
that using only equivariance on part centers is not sufﬁcient
to obtain good part segmentation results.

3.4. Semantic Consistency Loss

Although equivariance loss favor part segmentations that
are robust to some object variations, the synthetically cre-
ated transformations would not be sufﬁcient to produce con-
sistency across different instances since the appearance and
pose variations between images are too high to be modeled
by any artiﬁcial transformations (See Figures 1 and 4 for
some sample instances). To encourage semantic consistency
across different object instances, we would need to explic-
itly leverage different instances in our loss function.

A key observation that we make use of is that the infor-
mation about objects and parts is embedded in intermediate
CNN features of classiﬁcation networks [3, 14, 10]. We
devise a novel semantic consistency loss function that taps
into this hidden part information of ImageNet trained fea-
tures [26, 39, 17], which are readily available these days.
Following the observation in [10], we assume that we can

872

Image Collection

Feature Extractor

CxHxW

Saliency

Constraint

𝑽

Semantic Consistency Loss

𝐿 )* (Eqn. 4)

Part Segmentation Network

𝑹

⨂

ReLU

Semantic Part Basis

{𝒘𝒌}

…

Orthonormal

Constraint
𝐿 ’( (Eqn. 5)

Figure 4: Semantic consistency loss. We enforce seman-
tic consistency of parts across instances by learning a se-
mantic part basis that is shared across all images. We use
orthonormal constraint to learn distinct part basis, and we
use saliency constraint to encourage parts to appear on fore-
ground objects.

ﬁnd representative feature clusters in the given classiﬁcation
features that are corresponding to different part segments.

Formally, given C-dimensional classiﬁcation features
V ∈ RC×H×W , we like to ﬁnd K representative part fea-
tures wk ∈ RC, k ∈ {1, 2, ..., K}. We simultaneously
learn part segmentation R and these representative part fea-
tures {wk} such that the classiﬁcation features V(u, v) of
an (u, v) pixel belonging to kth part is close to wk i.e.,
||V(u, v) − wk||2 → 0. Since the number of parts K
is usually smaller than feature dimensionality C, we can
see the representative part features {wk} as spanning a K-
dimensional subspace in a C-dimensional space. We call
these representative part features as part basis vectors.

Figure 4 illustrates the semantic consistency loss. Given
an image I, we obtain its part response map R. We also pass
I into a pre-trained classiﬁcation network and obtain feature
maps of an intermediate CNN layer. The feature map is bi-
linearly up-sampled to have the same spatial resolution of I
and R, resulting in V ∈ RC×H×W . We learn a set of part
basis vectors {wk} that are globally shared across differ-
ent object instances (training images) using the following
semantic consistency loss:

Lsc = X

||V(u, v) − X

R(k, u, v)wk||2,

(4)

u,v

k

where V(u, v) ∈ RC is the feature vector sampled at spa-
tial location (u, v). We learn both the part segmentation R
and the basis vectors {wk} at the same time using standard

back-propagation. To ensure that different part basis vec-
tors do not cancel each other out, we enforce non-negativity
on both features V and basis vectors {wk} by passing them
through a ReLU layer. The part segmentation R is naturally
non-negative as it is the output of a softmax function.

We view the semantic consistency loss as a linear sub-
space recovery problem with respect to the embedding
space provided by the feature extractor on the input im-
age collection. As the training progresses, the part bases
can gradually converge to the most representative direction
of each part in the embedding space provided by the pre-
trained deep features, and the recovered subspace can be
described as the span of the basis {wk}. Furthermore, the
non-negativity ensures that the weights R(k, u, v) could be
interpreted as part responses. With the proposed semantic
consistency loss, we explicitly enforce the cross-instance
semantic consistency through the learned part basis {wk}
since the same part response would have similar semantic
feature embedding in the pre-trained feature space.

Orthonormal Constraint When training with the seman-
tic consistency loss, it is possible for the different basis to
have similar feature embedding, especially when K is large
or the underlying rank of the subspace is smaller than K.
Having similar part basis, the part segmentation could be-
come noisy since the response from multiple channels could
all represent the same part segment. Therefore, we pro-
pose to impose an additional orthonormal constraint on the
part basis wk to push the part bases apart. Let ˆW denotes
the matrix with each row as a normalized part basis vector
ˆwk = wk/||wk||, and we formulate the orthonormal con-
straint as a loss function on W :

Lon = || ˆW ˆWT − IK||2
F ,

(5)

where ||·||2
F is Frobenius norm and IK is the identity matrix
of size K × K. The idea is to minimize the correlation
between different basis vectors, and thus we can obtain a
more concise basis set resulting in better part responses.

Saliency Constraint We observe that, when the input im-
age collection is small, or the number of parts K is large,
the proposed method tends to pick up some common back-
ground regions as object parts. To tackle this issue, we
utilize an unsupervised saliency detection method [54] to
suppress the background features in V so that the learned
part basis do not correspond to background regions. To this
end, for a given image and the unsupervised saliency map
D ∈ [0, 1]H×W , we soft-mask the feature map V as D ◦ V,
where ◦ is the Hadamard (entry-wise) product, before pass-
ing it into the semantic consistency loss function. Consider-
ing the non-salient pixels where D(u, v) = 0, the semantic
consistency loss (Eqn. 4) can be interpreted as solving the
following equation:

R(k, u, v)wk = 0,

(6)

873

which is essentially projecting the non-salient background
regions into the null space of the learned subspace spanned
by {wk}. This saliency constraint encapsulates our prior
knowledge that parts appear on objects (not background)
and the union of parts forms an object.
Several co-
segmentation techniques [32, 7, 13] also make use of
saliency maps to improve the segmentation result. How-
ever, to our best knowledge, we are the ﬁrst work to impose
the saliency constraint in the feature reconstruction loss.

Related to our semantic consistency loss, a recent
work [10] proposed a deep feature factorization (DFF) tech-
nique for part discovery. Instead of learning a part basis,
DFF proposes to directly factorize features V into response
maps R and basis matrix W using non-negative matrix fac-
torization (NMF); V → RW. Although DFF alleviates the
need for learning a part basis and also training segmentation
network, our learning strategy has several advantages com-
pared to DFF. First, we can make use of mini-batches and
standard gradient descent optimization techniques for learn-
ing part basis, whereas DFF performs NMF over the entire
image collection at once during inference time. This makes
our learning technique scalable to learning on large image
collections and can be applied on single test image. Sec-
ond, learning the part segmentation and basis using neural
networks enables easy incorporation of different constraints
on the part basis (e.g., orthonormal constraint) as well as the
incorporation of other loss functions such as concentration
and equivariance. Our experiments indicate that these loss
functions are essential to obtain good part segmentations
that are semantically consistent across images.

4. Experiments

Throughout the experiments, we refer to our technique
as “SCOPS” (Self-supervised Co-Part Segmentation). Since
SCOPS is self-supervised, the segmentation does not nec-
essarily correspond to the human annotated object parts.
Therefore, we quantitatively evaluate SCOPS with two dif-
ferent proxy measures on different object categories, in-
cluding CelebA [29], AFLW [22] (human faces), CUB [44]
(birds), and PASCAL [11] (common objects) datasets.

On CelebA, AFLW, and CUB datasets, we convert our
part segmentation into landmarks by taking part centers
(Eqn. 1) and evaluate against groundtruth annotations. Fol-
lowing recent works [50, 42], we ﬁt a linear regressor that
learns to map the detected landmarks to groundtruth land-
marks and evaluate the resulting model on test data. On
PASCAL, we aggregate the part segmentations and evalu-
ate them with the foreground segmentation IOU.
Implementation Details We implement SCOPS1 with Py-
Torch, and we train the networks with a single Nvidia GPU.
We use relu5 2 concatenated with relu5 4 from VGG-

1The code and models are available at https://varunjampani.

github.io/scops

Table 1: Landmark evaluation on unaligned CelebA.
Mean L2 distance comparing SCOPS to recent works (left)
and also ablation with different loss functions (right).

Method

Error (%)

SCOPS(K=8)

Error (%)

ULD (K=8) [50, 42]
DFF (K=8) [10]

SCOPS (K=4)
SCOPS (K=8)

40.82
31.30

21.76
15.01

sc

sc

only L
w/o L
w/o L
w/o L
w/o Saliency

con

eqv

23.53
28.49
21.85
18.60
22.11

Table 2: Landmark evaluation on unaligned AFLW.
Mean L2 distance comparing SCOPS to recent works.

Method

ULD (K=8) [50, 42]

DFF (K=8) [10]

SCOPS (K=8)

Error (%)

25.03

20.42

16.54

19 [39] as the pre-trained features V for the semantic con-
sistency loss.

4.1. Faces from Unaligned CelebA/AFLW

The CelebA dataset contains around 20k face images,
each annotated with a tight bounding box around face and 5
facial landmarks. One of the main advantages of SCOPS is
that it is relatively robust to pose and viewpoint variations
compared to recent landmark estimation works [50, 42]. To
demonstrate this, we experiment with unaligned CelebA
images where we choose images with face covering more
than 30% of the pixel area. Following the settings in [50],
we also exclude MAFL [51] (subset of CelebA) test images
from the train set resulting in a total of 45609 images. We
use the MAFL train set (5379 images) to ﬁt the linear re-
gression model and test on the MAFL test set (283 images).
In Table 1, we report the landmark regression errors in
terms of mean L2 distance normalized by inter-ocular dis-
tance. To compare with the existing unsupervised landmark
discovery works, we implement the loss functions, includ-
ing concentration, separation, landmark equivariance, and
reconstruction, as proposed in [50] and [42]. We train our
base network with these constraints and refer it as “ULD”.
To validate our implementation of ULD, we train it on the
align celebA images that yields 5.42% landmark estimation
error, which is comparable to the reported 5.83% in [42]
and 3.46% in [50]. However, when training and testing with
the unaligned images, we found that ULD has difﬁculty in
converging to semantically meaningful landmark locations,
resulting in high errors. We also compare with a recent self-
supervised part segmentation technique of DFF [10] by con-
sidering the part responses as landmark detections. We train
SCOPS to predict 4 and 8 parts with all the proposed con-
straints and show the comparison results in Table 1 (left).
The results show that SCOPS performs favorably against
other methods. The visual results of SCOPS (K = 8) in
Fig. 5 shows that SCOPS part segments are more semanti-

874

Table 3: Landmark evaluation on CUB. Normalized L2
distance comparing SCOPS to recent techniques (K=4).

Method

CUB-001

CUB-002

CUB-003

ULD [50, 42]
DFF [10]

SCOPS

30.12
22.42

18.50

29.36
21.62

18.82

28.19
21.98

21.07

cally consistent across different images compared to exist-
ing techniques. In addition, we train SCOPS on the AFLW
dataset [22], which contains 4198 face images (after ﬁlter-
ing) with 21 annotated landmarks. Following [50], we pre-
train the model on CelebA and ﬁnetune on AFLW. We show
the results in Table 2. Results indicate that SCOPS outper-
forms both ULD and DFF on this dataset images as well.
Even though the landmark prediction accuracy do not di-
rectly measure the learned part segmentation quality, these
results demonstrate that the learned part segmentations are
semantically consistent across instances under the challeng-
ing unaligned setting.

Ablation Study To validate the individual contribution of
the different constraints, we conduct a detailed ablation
study and show the results in Table 1 (right). The corre-
sponding visual results are shown in Figure 5. While re-
moving any of the constraints results in worse performance,
the semantic consistency loss Lsc is the most important con-
straint in the proposed framework, and removing it would
cause the most performance drop. Visual results in Fig-
ure 5 indicate that the learned parts would not have a seman-
tic meaning without Lsc. Results also indicate that train-
ing without geometric concentration loss Lcon would cause
some parts dominating large image areas, and no equivari-
ance loss Leqv makes the learned parts not consistent across
images. These results demonstrate that all our loss functions
are essential to learning good part segmentations.

4.2. Birds from CUB

We also evaluate the proposed method on a more chal-
lenging bird images from CUB-2011 dataset [44], which
consists of 11,788 images with 200 categories of birds and
15 landmark annotations. The dataset is challenging be-
cause of the various bird poses, e.g., standing, swimming,
or ﬂying, as well as the different camera viewpoints. We
train SCOPS with K = 4 on ﬁrst three bird categories and
compare to ULD and DFF. We show some qualitative re-
sults in Figure 6. With such level of object deformation,
we found that ULD has difﬁculty in localizing meaningful
parts. Compared to DFF, the part segments produced by
SCOPS has better boundary alignment within and outside
the object, and the learned part segmentation is also more
consistent across instances. Similar to previous Section 4.1,
we use the landmark detection as the proxy task through

e
g
a
m

i

]
2
4

,

0
5
[

D
L
U

]
0
1
[
F
F
D

S
P
O
C
S

c
s
L
o
/
w

c
s
L
y
l
n
o

n
o
c
L
o
/
w

v
q
e
L
o
/
w

y
c
n
e
i
l
a
s
o
/
w

Figure 5: Visual results on CelebA face images. SCOPS
produce consistent part segments compared to existing tech-
niques. Also shown is the effect of different loss constraints.

considering the part centers as detected landmarks. To ac-
count for the varying bird sizes across images, we normal-
ize the landmark estimation error by the width and height of
the provided ground truth bounding boxes. Table 3 shows
the quantitative results of different techniques. For all the
three bird categories, SCOPS performs favorably against

875

e
g
a
m

I

]
2
4

,

0
5
[

D
L
U

]
0
1
[
F
F
D

S
P
O
C
S

Figure 6: Visual results on CUB bird images. SCOPS
is robust to pose and camera variations while having better
boundary adherence compared to other techniques.

both the ULD [50, 42], and DFF [10] techniques. For the
CUB-2011 dataset, SCOPS as well as other techniques do
not distinguish between left-right symmetric parts. For in-
stance, the left-wing and right-wing are often predicted as
the same part. From a part segmentation perspective, such
behavior is reasonable. However, considering the landmark
regression task, the part center of the two fanned out wings
would be on the main body, resulting in less meaningful
landmarks. As a result, the landmark regression error may
not accurately reﬂect the co-part segmentation quality and
distinguishing the symmetric semantic parts remains a chal-
lenging problem on this dataset images.

4.3. Common Objects from PASCAL

We

also apply SCOPS on the PASCAL VOC
dataset [11], which contains images with common objects
with various deformations, viewing angles, and occlusions.
We extract the images that contain the speciﬁc object cate-
gory while the object bounding box occupies at least 20%
of the whole image. To remove signiﬁcant occlusions in the
images, we further exclude the images where only a small
portion of ground-truth parts are present in the PASCAL-
part dataset [9]. The models are trained separately for each
object category with K = 4. Although the PASCAL-
part dataset [9] provides the object parts annotation, a good
self-supervised part segmentation can produce semantically
consistent part segments that may not correspond to man-
ually annotated part segments. Therefore, we do not eval-
uate the results with the part-level Intersection over Union
(IoU) since it is not a good indicator. Instead, we evalu-
ate the results as co-segmentation by aggregating the part

e
g
a
m

I

S
P
O
C
S

Figure 7: Visual results on the PASCAL VOC datset [11].
SCOPS is robust to pose and appearance variations.

Table 4: Evaluation on the PASCAL VOC dataset.
Cosegmentation IoU comparing SCOPS to DFF on 7 ob-
ject classes of VOC (K=4).

class

horse

cow

sheep

aero

bus

car

motor

DFF [10]
DFF+CRF [10]

SCOPS
SCOPS+CRF

49.51
50.96

55.76
57.92

56.39
57.64

60.79
62.70

51.03
52.29

56.95
58.17

48.38
50.87

69.02
80.54

58.63
58.64

73.82
75.32

56.48
57.56

65.18
66.14

54.80
55.86

58.53
59.15

segments and computing the foreground object segmenta-
tion IoU. Since the co-segmentation metric only indicates
the overall object localization and not the part segmentation
consistency, this metric is only indicative of part segmenta-
tion quality. We show some visual results in Figure 7 and
the quantitative evaluations in Table 4.
In terms of IoU,
SCOPS outperform DFF by a considerable margin, both
with and without the CRF post-processing [23]. The visual
results show that SCOPS is robust to various appearance
and pose articulations. We show additional visual results in
the supplementary material.

5. Concluding Remarks

We propose SCOPS, a self-supervised technique for co-
part segmentation. Given an image collection of an object
category, SCOPS can learn to predict semantically consis-
tent part segmentations without using any ground-truth an-
notations. We devise several constraints, including geomet-
ric concentration, equivariance, as well as semantic consis-
tency, to train a deep neural network to discover seman-
tically consistent part segments while ensuring decent ge-
ometric conﬁgurations and cross instance correspondence.
Results on different types of image collections show that
SCOPS is robust to different object appearances, camera
viewpoints, as well as pose articulations. The qualitative
and quantitative results show that SCOPS performs favor-
ably against existing methods. We hope that the proposed
method could serve as a general framework for learning co-
part segmentation.
Acknowledgments. W.-C. Hung is supported in part by the
NSF CAREER Grant #1149783, gifts from Adobe, Verisk,
and NEC.

876

References

[1] Pulkit Agrawal, Ross Girshick, and Jitendra Malik. Analyz-
ing the performance of multilayer neural networks for object
recognition. In ECCV. Springer, 2014.

[2] Hossein Azizpour and Ivan Laptev. Object detection us-
ing strongly-supervised deformable part models. In ECCV.
Springer, 2012.

[3] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and
Antonio Torralba. Network dissection: Quantifying inter-
pretability of deep visual representations. In CVPR, 2017.

[4] Lubomir Bourdev and Jitendra Malik. Poselets: Body part
detectors trained using 3d human pose annotations. In ICCV.
IEEE, 2009.

[5] Steve Branson, Pietro Perona, and Serge Belongie. Strong
supervision from weak annotation: Interactive training of de-
formable part models. In ICCV. IEEE, 2011.

[6] Hilton Bristow, Jack Valmadre, and Simon Lucey. Dense
semantic correspondence where every pixel is a classiﬁer. In
ICCV, 2015.

[7] Kai-Yueh Chang, Tyng-Luh Liu, and Shang-Hong Lai. From
co-saliency to co-segmentation: An efﬁcient and fully unsu-
pervised energy minimization model. In CVPR, 2011.

[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. In TPAMI, 2017.

[9] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler,
Raquel Urtasun, and Alan Yuille. Detect what you can: De-
tecting and representing objects using holistic models and
body parts. In CVPR, 2014.

[10] Edo Collins, Radhakrishna Achanta, and Sabine S¨usstrunk.
Deep feature factorization for concept discovery. In ECCV,
2018.

[11] Mark Everingham, Luc J. Van Gool, Christopher K. I.
Williams, John M. Winn, and Andrew Zisserman. The pascal
visual object classes (voc) challenge. IJCV, 88(2):303–338,
2010.

[12] Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. A discriminatively trained, multiscale, deformable
part model. In CVPR, 2008.

[13] Huazhu Fu, Dong Xu, Stephen Lin, and Jiang Liu. Object-
based rgbd image co-segmentation with mutex constraint. In
CVPR, 2015.

[14] Abel Gonzalez-Garcia, Davide Modolo, and Vittorio Ferrari.
Do semantic parts emerge in convolutional neural networks?
IJCV, 2018.

[15] Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean

Ponce. Proposal ﬂow. In CVPR, 2016.

[16] Kai Han, Rafael S Rezende, Bumsub Ham, Kwan-Yee K
Wong, Minsu Cho, Cordelia Schmid, and Jean Ponce. Sc-
net: Learning semantic correspondence. In ICCV, 2017.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[18] Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vin-
cent, Christopher Pal, and Jan Kautz. Improving landmark
localization with semi-supervised learning. In CVPR, 2018.

[19] Armand Joulin, Francis Bach, and Jean Ponce. Multi-class

cosegmentation. In CVPR, 2012.

[20] Martin Kiefel and Peter Vincent Gehler. Human pose esti-

mation with ﬁelds of parts. In ECCV, 2014.

[21] Jaechul Kim, Ce Liu, Fei Sha, and Kristen Grauman. De-
formable spatial pyramid matching for fast dense correspon-
dences. In CVPR, 2013.

[22] Martin Koestinger, Paul Wohlhart, Peter M Roth, and Horst
Bischof. Annotated facial landmarks in the wild: A large-
scale, real-world database for facial landmark localization.
In ICCV workshops, 2011.

[23] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
in fully connected crfs with gaussian edge potentials. In Ad-
vances in neural information processing systems, pages 109–
117, 2011.

[24] Jonathan Krause, Hailin Jin, Jianchao Yang, and Li Fei-Fei.
Fine-grained recognition without part annotations. In CVPR,
2015.

[25] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for ﬁne-grained categorization. In
Proceedings of the IEEE International Conference on Com-
puter Vision Workshops, 2013.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

[27] Daniel D Lee and H Sebastian Seung. Learning the parts
of objects by non-negative matrix factorization. Nature,
401(6755):788, 1999.

[28] Ce Liu, Jenny Yuen, and Antonio Torralba. Sift ﬂow: Dense
correspondence across scenes and its applications.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2011.

[29] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.

Deep learning face attributes in the wild. In ICCV, 2015.

[30] Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convo-
lutional neural network architecture for geometric matching.
In CVPR, volume 2, 2017.

[31] Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. End-to-
end weakly-supervised semantic alignment. In CVPR, 2018.
[32] Michael Rubinstein, Armand Joulin, Johannes Kopf, and Ce
Liu. Unsupervised joint object discovery and segmentation
in internet images. In CVPR, 2013.

[33] Jose C Rubio, Joan Serrat, Antonio L´opez, and Nikos Para-
gios. Unsupervised co-segmentation through region match-
ing. In CVPR, 2012.

[34] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, Dhruv Batra, et al.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In ICCV, 2017.

[35] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan,
and Stefan Carlsson. Cnn features off-the-shelf: an astound-
ing baseline for recognition. In CVPR Workshop, 2014.

[36] Jonathan Shelhamer, Evan an Long and Trevor Darrell. Fully
convolutional networks for semantic segmentation. TPAMI,
2016.

[37] Zhixin Shu, Mihir Sahasrabudhe, Alp Guler, Dimitris Sama-
ras, Nikos Paragios, and Iasonas Kokkinos. Deforming au-

877

toencoders: Unsupervised disentangling of shape and ap-
pearance. ECCV, 2018.

[38] Tomas Simon, Hanbyul Joo, Iain A Matthews, and Yaser
Sheikh. Hand keypoint detection in single images using mul-
tiview bootstrapping. In CVPR, 2017.

[39] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015.

[40] Supasorn Suwajanakorn, Noah Snavely, Jonathan Tomp-
son, and Mohammad Norouzi. Discovery of latent 3d key-
points via end-to-end geometric reasoning. arXiv preprint
arXiv:1807.03146, 2018.

[41] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsuper-
vised learning of object frames by dense equivariant image
labelling. In NIPS, 2017.

[42] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsu-
pervised learning of object landmarks by factorized spatial
embeddings. In ICCV, 2017.

[43] Yi-Hsuan Tsai, Guangyu Zhong, and Ming-Hsuan Yang. Se-

mantic co-segmentation in videos. In ECCV, 2016.

[44] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011.

[45] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
NIPS, 2014.

[46] Sergey Zagoruyko and Nikos Komodakis. Learning to com-
In

pare image patches via convolutional neural networks.
CVPR, 2015.

[47] Jure Zbontar and Yann LeCun. Computing the stereo match-
In CVPR,

ing cost with a convolutional neural network.
2015.

[48] Jure Zbontar and Yann LeCun. Stereo matching by training
a convolutional neural network to compare image patches.
Journal of Machine Learning Research, 2016.

[49] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In ECCV. Springer, 2014.
[50] Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He,
and Honglak Lee. Unsupervised discovery of object land-
marks as structural representations. In CVPR, 2018.

[51] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou
Tang. Facial landmark detection by deep multi-task learning.
In ECCV. Springer, 2014.

[52] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Object detectors emerge in deep scene
cnns. In ICLR, 2015.

[53] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In CVPR, 2016.

[54] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun.
Saliency optimization from robust background detection. In
CVPR, 2014.

[55] Silvia Zufﬁ and Michael J Black. The stitched puppet: A
In CVPR,

graphical model of 3d human shape and pose.
2015.

878

