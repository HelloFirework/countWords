PDE Acceleration for Active Contours

Anthony Yezzi1∗, Ganesh Sundaramoorthi2, Minas Benyamin1

1 Georgia Institute of Technology

1 {ay10, mbenyamin3}@gatech.edu, 2 ganesh.sun@gmail.com

Abstract

Following the seminal work of Nesterov, accelerated
optimization methods have been used to powerfully boost
the performance of ﬁrst-order, gradient-based parameter
estimation in scenarios where second-order optimization
strategies are either inapplicable or impractical. Acceler-
ated gradient descent converges faster and performs a more
robust local search of the parameter space by initially over-
shooting then oscillating back into minimizers which have
a basis of attraction large enough to contain the overshoot.
Recent work has demonstrated how a broad class of accel-
erated schemes can be cast in a variational framework lead-
ing to continuum limit ODE’s. We extend their formulation
to the PDE framework, speciﬁcally for the inﬁnite dimen-
sional manifold of continuous curves, to introduce accel-
eration, and its added robustness, into the broad range of
PDE based active contours.

1. Introduction

Accelerated and stochastic gradient search methods have
been utilized extensively within the machine learning com-
munity [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Not only does accel-
erated gradient descent converge considerably faster than
traditional gradient descent, but it also performs a more ro-
bust local search of the parameter space by initially over-
shooting and then oscillating back as it settles into a ﬁnal
conﬁguration, thereby selecting only local minimizers with
a basis of attraction large enough to contain the initial over-
shoot. So far, however, accelerated optimization methods
have been restricted to searches over ﬁnite dimensional pa-
rameter spaces.

Recently, however, Wibisono, Wilson, and Jordan out-
lined a variational ODE framework in [12] (which we will
summarize brieﬂy in Section 2.4) formulated around the
Bregman divergence and which yields the continuum limit
of a broad class of accelerated optimization schemes, in-
cluding that of Nesterov’s accelerated gradient method [13]
whose continuum ODE limit was also demonstrated by Su,
Boyd, and Candes in [14]. We adapt this approach to the in-

ﬁnite dimensional PDE framework through the formulation
of a generalized time-explicit action which can be viewed
as a specialization of the Bregman Lagrangian presented
in [12]. While the extension we outline from the ODE
framework into the PDE framework is general enough to be
applied to a variety of inﬁnite-dimensional or distributed-
parameter optimization problems, the focus of this paper
will be on optimization using active contours.

Active contours and surfaces (e.g., [15, 16, 17, 18, 18])
have been widely used for the problem of segmentation and
3D dense reconstruction in computer vision, as they pro-
vide a powerful mechanism for modeling object shape and
geometry. Curves or surfaces are driven to segment images
typically by the optimization of an energy functional, which
in general are non-convex inﬁnite-dimensional problems.
Due to this non-convexity, traditional active contour mod-
els are sensitive to initialization and clutter in the image.
The past decade has attempted to reduce this sensitivity by
formulating active contour energies in terms of relaxed indi-
cator functions, which under some particular active contour
models, reduce to convex problems [19, 20, 21, 22] that can
be solved efﬁciently. While this has greatly advanced ac-
tive contours, such approaches do not extend to non-convex
problems. Thus, we reduce active contour sensitivity by
constructing a general method valid for any non-convex ac-
tive contour model. Furthermore, these methods remain rel-
evant in the era of deep learning, as there are a number of
problems where the large training set requirement of current
deep learning systems cannot be met, and common tricks
for small datasets (e.g., transfer learning, ﬁne tuning, etc)
are also not possible. Thus, there is a need for explicit mod-
els that reduce the training requirement. Active contours
offer such explicit models, and can be complementary to
deep learning (see [23, 24]).

Moving into the inﬁnite dimensional framework for ac-
celerated approaches introduces additional mathematical,
numerical, and computational challenges and technicalities
which do not arise in ﬁnite dimensions. For example, the
evolving parameter vector in ﬁnite dimensional optimiza-
tion can naturally be interpreted as a single moving particle
in Rn with a constant mass which, in accelerated optimiza-

12318

tion schemes, gains momentum during its evolution. Since
the mass is constant and ﬁxed to a single particle, there is
no need to explicitly model it. When evolving a continuous
curve, surface, region, or function, however, the notion of
accumulated momentum during the acceleration process is
much more ﬂexible, as the corresponding conceptual mass
can be locally distributed in several different ways through-
out the domain which will in turn signiﬁcantly affect the
evolution dynamics. In this ﬁrst paper, we develop the sim-
plest case of mass distributed along a curve with a constant
density per unit arclength. Therefore the total mass is not
constrained to be ﬁxed but evolves according to a contour’s
changing arclength.

The discrete implementation of accelerated PDE mod-
els will also differ greatly from existing momentum based
gradient descent schemes in ﬁnite dimensions. Spatial and
temporal steps sizes will be determined based on CFL sta-
bility conditions for ﬁnite difference approximations of the
PDE’s. Viscosity solutions will be required in the PDE
framework to propagate through shocks and rarefactions
that may occur during the evolution of a continuous front, a
phenomenon which manifests itself differently and is there-
fore handled differently in the ﬁnite dimensional case. As
such, these considerations will also impact the numerical
discretization of accelerated PDE models.
In part due to
these different discretization criteria and in part to avoid un-
necessary complexity in the manifold case, we will aban-
don the Bregman Lagrangian described in [12] and will
instead exploit a simpler time-explicit generalized action
which will allow us to work directly with the continuum ve-
locity of the evolving entity rather than ﬁnite displacements
with the Bregman divergence. Especially for the case of
curves and surfaces considered here, this avoids the compli-
cation of calculating geodesic distances on highly curved,
inﬁnite-dimensional manifolds, but lets us work more eas-
ily in the tangent space instead.

This work provides a solid theoretical framework for op-
timizing functionals deﬁned on contours (and surfaces1) via
accelerated optimization. We derive accelerated optimiza-
tion on contours, which requires signiﬁcant mathematical
effort (intricate calculations are in supplementary material).
As this work is primarily theoretical, we remain neutral on
the particular choice of active contour functional being min-
imized. An extremely large number of energy-based active
contour methodologies have been proposed over the past
three decades. Any of these models which are geometric in
nature (i.e. the energy to be minimized depends on the con-
tour geometry but not its particular parametric or implicit
representation) may be accelerated using the PDE scheme
presented here. While our illustrative results in Section 4

1While we do not explicitly treat the case of surfaces, the resulting
mathematical expressions are the same as the case of contours and require
no extra algorithmic effort besides that of the surface representation.

will be based on a narrowband level set implementation [25]
of the well-known Chan-Vese energy [26], which by now
can be solved well with convex approaches, similar robust-
ness improvements would be expected in applying this same
acceleration technique to minimize alternative contour en-
ergies, in particular non-convex ones that cannot be reduced
to a convex problem, as well. Recently we have also intro-
duced PDE acceleration into optimization problems for the
manifold of diffeomorphisms (image registration) [27, 28]
and linear function spaces (denoising and deblurring) [29].

2. Background and Prior Work

Geometric partial differential equations have played an
important role in image analysis and computer vision for
several decades now. Applications have ranged from
low-level processing operations such as denoising using
anisotropic diffusion, blind deconvolution, and contrast en-
hancement; to mid-level processing such as segmentation
using active contours and active surfaces, image registra-
tion, and motion estimation via optical ﬂow; to higher level
processing such as multiview stereo reconstruction, visual
tracking, SLAM, and shape analysis. See, for example,
[30, 31, 32] for introductions to PDE methods already estab-
lished within computer vision within the 1990’s, including
level set methods [33] already developed in the 1980’s for
shape propagation. Several such PDE methods have been
formulated, using the calculus of variations [34] as gradient
descent based optimization problems in functional spaces,
including geometric spaces of curves and surfaces.

2.1. PDE Based Active Contours

Coming to the speciﬁc focus of this paper, several ac-
tive contour models are formulated as gradient descent PDE
ﬂows of application-speciﬁc energy functionals E which re-
late the unknown contour C to given data measurements.
Such energy functionals are chosen to depend only upon
the geometric shape of the contour C, not its parameteriza-
tion. Under these assumptions the ﬁrst variation of E will
have the following form

δE = −ZC

f (δC · N ) ds

(1)

where f N represents a perturbation ﬁeld along the unit
normal N at each contour point and ds denotes the ar-
clength measure. Note that the ﬁrst variation depends only
upon the normal component of a permissible contour pertur-
bation δC. The form of f will depend upon the particular
choice of the energy. For example, in the popular Chan-
Vese active contour model [26] for image segmentation, f
would be expressed by (I − c1)2 − (I − c2)2 + ακ where
I denotes the image value at a given contour point, α an ar-
clength penalty weight, κ the curvature at a given contour
point, and c1 and c2 the means of the image inside and out-
side the contour respectively. As an alternative example, the
geodesic active contour model [35, 36] would correspond to

12319

f = φκN − (∇φ · N )N where φ > 0 represents a point
measurement designed to be small near a boundary of in-
terest and large otherwise. In all cases, though, the gradient
descent PDE will have the following explicit form.

∂C
∂t

= f N [explicit gradient ﬂow]

(2)

This class of contour ﬂows, evolving purely in the normal
direction, may be implemented implicitly in the level set
framework [33] by evolving a function ψ whose zero level
set represents the curve C as follows

∂ψ
∂t

= − ˆf k∇ψk [implicit level set ﬂow]

where ˆf (x, t) denotes a spatial extension of f (s, t) to
points away from the curve.

2.2. Sobolev Active Contours

The most notorious problem with active contour mod-
els is that the normal speed function f depends point-wise
upon noisy or textured data, resulting in ﬁne scale perturba-
tions to the evolving contour which cause it to attract to to
spurious local minimizers and be initialization dependent.
The traditional ﬁx is to add strong regularizing terms to the
energy which penalize ﬁne scale structure in the contour.

This energy regularization strategy has two drawbacks.
First, most regularizers often lead to higher order diffu-
sion terms in the gradient contour ﬂow, which can impose
smaller time step limitations on the numerical discretiza-
tion, slowing the evolution of the PDE. Second, regulariz-
ers, while they beneﬁcially force regularity on noise and
spurious structures also force regularity on the ﬁnal con-
verged contour. Thus they make it difﬁcult to capture fea-
tures such as sharp corners, or narrow protrusions/inlets.

Signiﬁcantly improved robustness, without additional
regularization, can be attained by using geometric Sobolev
gradients [37, 38, 39] in place of the standard L2-style gra-
dient employed by traditional active contours. We refer
to this class of active contours as Sobolev active contours,
whose evolution may be described by the following integral
PDE

= (f N ) ∗ K [Sobolev gradient ﬂow]

(3)

∂C
∂t

Here ∗ denotes convolution in the arclength measure with
a smoothing kernel K to invert the linear Sobolev gradient
operator. The numerical implementation is not carried out
this way, but the expression gives helpful insight into how
the Sobolev gradient ﬂow (3) relates to the usual gradient
ﬂow (2). Namely, the optimization process, not the energy
functional itself, is regularized by averaging point-wise gra-
dient forces f N through the kernel K to yield a smoother
contour evolution. This does not change the local minimiz-
ers of the energy functional, nor does it impose extra regu-
larity at convergence, but induces a dynamic coarse-to-ﬁne

evolution behavior [40], making the contour resistant to lo-
cal minima.

However, while the Sobolev gradient descent method is
successful in making an active contour or surface resistant
to a large class of unwanted local minimizers, it comes with
heavy computational cost. The linear operator inversion im-
poses a notable per-iteration cost, which we will instead dis-
tribute across iterations in the proceeding accelerated PDE
evolution schemes. Recent work [17] seeks to use Sobolev
gradients for surfaces using an approximation of the kernel
as a separable kernel, however this is only an approxima-
tion; our approach avoids convolution altogether.

2.3. Momentum and Nesterov Acceleration

If we step back to the ﬁnite dimensional case, an alterna-
tive and computationally cheaper method to regularize any
gradient descent based iteration scheme is to employ the use
of momentum. In such schemes each update is a weighted
combination of the previous update (the momentum term)
and the newly computed gradient at each step. This leads to
a temporal averaging of gradient information computed and
accumulated during the evolution process itself, rather than
a spatial averaging that occurs independently during each
time step. As such it adds insigniﬁcant per-iteration compu-
tation cost while signiﬁcantly boosting the robustness (and
often the convergence speed) of the optimization process.

Momentum methods, including stochastic variants [8, 7],
have become very popular in machine learning in recent
years [10, 9, 6, 5, 4, 2, 11, 3]. Strategic dynamically chang-
ing weights on the momentum term can further boost the
descent rate. Nesterov put forth a famous scheme in [13]
which attains an optimal rate of order 1
t2 in the case of a
smooth, convex energy function.

2.4. Variational Framework for Accelerated ODE’s

Recently Wibisono, Wilson and Jordan [12] presented a
variational generalization of Nesterov’s [13] and other mo-
mentum based gradient schemes in Rn based on the Breg-
man divergence of a convex distance generating function h

D(y, x) = h(y) − h(x) − h∇h(x), y − xi

(4)

and careful discretizations of the Euler-Lagrange equation
for the time integral (evolution time) of the following Breg-
man Lagrangian

L(X, V, t) = ea(t)+γ(t)hD(X + e−a(t)V, X) − eb(t)U(X)i
where the potential energy U represents the cost to be min-
imized. In the Euclidean case, where D(y, x) = 1
2 ky −xk2,
this simpliﬁes to

e−a(t) 1
2

kV k2

−ea(t)+b(t)U(X)

| {z }T




L = eγ(t)


12320

where T models the kinetic energy of a unit mass particle
in Rn. Nesterov’s methods [13, 41, 42, 43, 44, 45] belong
to a subfamily of Bregman Lagrangians with the following
choice of parameters (indexed by k > 0)

a = log k − log t,

b = k log t + log λ,

γ = k log t

which, in the Euclidean case, yields the following time-
explicit generalized action (compared to the time-implicit
standard action T − U in classical mechanics [46])

L =

tk+1

k (cid:16)T − λk2t

k−2U(cid:17)

(5)

In the case of k = 2, for example, the Euler-Lagrange equa-
tions for the integral of this time-explicit action yield the
continuum limit of Nesterov’s accelerated mirror descent
[45] derived in both [14, 4].

3. Acceleration in the PDE Framework

We now develop a general strategy, based on adaptation
of the Euclidean case of Wibisono, Wilson, and Jordan’s
formulation [12] reviewed in Section 2.4, for extending ac-
celerated optimization into the PDE framework. While our
approach will be motivated by the variational ODE frame-
work formulated around the Bregman divergence in [12],
several new considerations need to be addressed.

For example, the evolving parameter vector in ﬁnite di-
mensional optimization can naturally be interpreted as a sin-
gle moving particle in Rn with a constant mass which, in
accelerated optimization schemes, gains momentum during
its evolution. Since the mass is constant and ﬁxed to a sin-
gle particle, there is no need to explicitly model it. When
evolving a continuous curve, surface, region, or function,
however, the notion of accumulated momentum during the
acceleration process is much more ﬂexible, as the corre-
sponding conceptual mass can be locally distributed in sev-
eral different ways throughout the domain which will in turn
signiﬁcantly affect the evolution dynamics.
In this work,
we start with the simplest possible distributed mass model
by considering a constant mass density (per unit arclength)
along the active contour. This means, unlike the ﬁnite di-
mensional case, that total mass is not necessarily conserved
but evolves along with the contour as its arclength changes.
In all cases, though, the outcome of these formulations will
be a coupled system of ﬁrst-order PDE’s which govern the
simultaneous evolution of the continuous unknown (curves
in the case considered here), its velocity, as well as the sup-
plementary density function which describes the evolving
mass.

In addition, the numerical discretization of accelerated
PDE models will also differ greatly from existing momen-
tum based gradient descent schemes in ﬁnite dimensions.
Spatial and temporal steps sizes will be determined based
on CFL stability conditions for ﬁnite difference approxima-
tions of the PDE’s and viscosity solution schemes will be re-

quired to propagate through shocks and rarefactions that oc-
cur during the distributed continuous front evolution. This
is part of the reason we replace the more general Bregman-
Lagrangian in [12] with the simpler time-explicit general-
ized action (5), together with the additional beneﬁt that such
a choice allows us to work directly with the continuum ve-
locity of the evolving entity (or other generalizations that
are easily deﬁned within the tangent space of its relevant
manifold) rather than ﬁnite displacements utilized by the
Bregman divergence (4).

3.1. General Approach

Just as in [12], the energy functional E to be optimized
over the continuous inﬁnite dimensional unknown (whether
it be a function, a curve, a surface, or a diffeomorphic map-
ping) will represent the potential energy term U in the time-
explicit generalized action (5). Next, a customized kinetic
energy term T will be formulated to incorporate the dynam-
ics of the evolving estimate during the minimization pro-
cess. Note that just as the evolution time t would represent
an artiﬁcial time parameter for a continuous gradient de-
scent process, the kinetic energy term will be linked to arti-
ﬁcial dynamics incorporated into the accelerated optimiza-
tion process. As such, the accelerated optimization dynam-
ics can be designed completely independently of any po-
tential physical dynamics in cases where the distributed un-
known is be connected with the motion of real objects. Sev-
eral different strategies can be explored, depending upon the
geometry of the speciﬁc optimization problem, for deﬁning
kinetic energy terms, including various approaches for at-
tributing artiﬁcial mass (both its distribution and its ﬂow) to
the actual unknown of interest in order to boost the robust-
ness and speed of the optimization process.

Once the kinetic energy term has been formulated, the
accelerated evolution will obtained (prior to discretization)
using the Calculus of Variations[34] as the Euler-Lagrange
equation of the following time-explicit generalized action
integral

Z tk+1

k (cid:16)T − λk2t

k−2U(cid:17) dt

(6)

In the simple k = 2 case, the main difference between
the resulting evolution equations versus the classical Prin-
ciple of Least Action equations of motion (without the time
explicit terms in the Lagrangian) is an additional friction-
style term whose coefﬁcient of friction decreases inversely
proportional to time. This additional term, however, is cru-
cial to the accelerated minimization scheme. Without such
a frictional term, the Hamiltonian of the system (the total
energy T + U), would be conserved, and the associated
dynamical evolution would never converge to a stationary
point. Friction guarantees a monotonic dissipation of en-
ergy, allowing the evolution to converge to a state of zero
kinetic energy and locally minimal potential energy (the op-
timization objective).

12321

This yields a natural physical interpretation of acceler-
ated gradient optimization in terms of a mass rolling down
a potentially complicated terrain by the pull of gravity. In
gradient descent, its mass is irrelevant, and the ball always
rolls downward by gravity (the gradient). As such the gra-
dient directly regulates its velocity. In the accelerated case,
gravity regulates its acceleration. Friction can be used to
interpolate these behaviors, with gradient descent represent-
ing the inﬁnite frictional limit as pointed out in [12]. When
the friction is ﬁnite, the dynamics converge over time due
to a consistent monotonic decrease in total energy (kinetic
plus potential) rather than the potential energy alone as in
pure gradient descent.

Acceleration comes with two advantages. First, when-
ever the gradient is very shallow (the energy functional is
nearly ﬂat), acceleration allows the ball to accumulate ve-
locity as it moves so long as the gradient direction is self
reinforcing. As such, the ball approaches a minimum more
quickly. Second, the velocity cannot abruptly change near
a shallow minimum as in gradient descent. Its mass gives it
momentum, and even if the acceleration direction switches
in the vicinity of a shallow minimum, the accumulated mo-
mentum still moves it forward for a certain amount of time,
allowing the optimization process to look ahead for a po-
tentially deeper minimizer.

3.2. Accelerated Active Contours

We now illustrate the steps in the process for developing
PDE based accelerated optimization schemes for the spe-
ciﬁc case of geometric active contours. Not only does this
put us into the inﬁnite dimensional framework of PDE’s,
but it also puts us on a highly curved manifold, in which the
standard implementations of momentum using a weighted
combination of a previous update and a newly calculated
gradient no longer apply in such a straight forward manner.
The detailed derivations for all formulas in the proceeding
sections can be found in [1].

More speciﬁcally, in the case of an active contour, a gra-
dient (as well as any other “search direction”) is represented
by a vector ﬁeld on the evolving contour. As the contour
changes shape, any incorporation of old gradient informa-
tion from previous evolution steps, must be remapped onto
the current contour conﬁguration via an appropriate parallel
transport process on the manifold of curves. This will be
accomplished implicitly by the coupled PDE formulations
we derive in this section. Furthermore, the resulting cou-
pled PDE evolutions will retain the parameterization inde-
pendent property of gradient descent based active contours
models and will therefore remain amenable to implicit im-
plementation using Level Set Methods [33].

Geometric curve evolution framework We begin with
some differential contour evolution formulas that are

needed in order to formulate accelerated active contours. In
particular, we look at both the ﬁrst and second order evolu-
tion behavior of a contour in terms of the local geometric
frame given by its unit tangent and unit normal vectors.

Let C(p, t) denote an evolving curve where t represents
the evolution parameter and p ∈ [0, 1] denotes an indepen-
dent parameter along each ﬁxed curve. The unit tangent,
unit normal, and curvature will be denoted by T = ∂C
∂s , N ,
and κ respectively, with the sign convention for κ and the di-
rection convention for N chosen to respect the planar Frenet
equations ∂T
∂s = −κT , where s denotes the
time-dependent arclength parameter whose derivative with
(cid:13)
respect to p yields the parameterization speed ∂s
(cid:13)
(cid:13)
Letting α and β denote the tangential and normal speeds

∂s = κN and ∂N

∂p =

∂C
∂p

(cid:13)
(cid:13)
(cid:13)

.

of the curve,

∂C

∂t

= αT + βN

the frame itself can be shown to evolve as follows.

∂T

∂t

=(cid:18) ∂β

∂s

+ ακ(cid:19) N,

∂N

∂t

= −(cid:18) ∂β

∂s

+ ακ(cid:19) T

(7)

(8)

Differentiating the velocity decomposition (7) with respect
to t, followed by the frame evolution (8) substitution, yields
the acceleration of the contour

∂ 2C
∂t2

=(cid:18) ∂α

∂t

− β(cid:18) ∂β

∂s

+ ακ(cid:19)(cid:19) T +(cid:18) ∂β

∂t

+ α(cid:18) ∂β

∂s

+ ακ(cid:19)(cid:19) N (9)

which may be rewritten as the following two scalar evolu-
tion equations for the tangential and normal speeds respec-
tively.

∂α
∂t

=

∂β
∂t

=

∂2C

∂t2 · T + β(cid:18) ∂β
∂t2 · N − α(cid:18) ∂β

∂2C

∂s

∂s

+ ακ(cid:19) ,
+ ακ(cid:19)

(10)

Contour potential energy We start by taking the energy
or cost functional E for any desired novel or existing ge-
ometric active contour model, and we deﬁne it as the po-
tential energy U for the accelerated version of the chosen
model. So long as this original energy functional depends
only upon the shape of the contour C (not its parameteriza-
tion), the ﬁrst variation of the resulting potential energy will
have the following form, just as in (1) presented earlier in
Section 2.1, where f N denotes the backward local gradient
force at each contour point.

δU = −ZC

f (δC · N ) ds

Contour kinetic energy To formulate an accelerated evo-
lution model, we deﬁne a kinetic energy, which requires a
notion of mass coupled with velocity. The simplest starting
model would be one of constant mass density ρ (per unit
arclength along the contour) and an integral of the squared
norm of the point-wise contour evolution velocity2.

2The same kinetic energy model paired with the more classical action
T − U was used to develop dynamic geodesic snake models for visual
tracking in [47]

12322

T =

1
2

ρZC(cid:18) ∂C

∂t

·

∂C

∂t (cid:19) ds

(11)

Accelerated contour ﬂow Plugging this into the general-
ized action integral (6) and computing the Euler-Lagrange
equation leads to our accelerated model in the form of a
nonlinear wave equation.

∂ 2C
∂t2

=

λk2tk−2

ρ

·

∂C

∂s ! ∂C

∂t

acceleration

|{z}
−  ∂ 2C
|

∂s∂t

f N

−

∂

|{z}−gradient
2 (cid:13)(cid:13)(cid:13)(cid:13)
∂s (cid:18) 1
{z

−

k + 1

∂C

∂C

|
∂t (cid:13)(cid:13)(cid:13)(cid:13)

t

∂t

friction

2 ∂C

{z
}
∂s (cid:19)
}

wave propagation and advection terms (achieves parallel transport)

(12)

The ﬁrst term represents the same backward gradient
force (now with a time and mass dependent scaling fac-
tor) arising in the originally chosen gradient descent ac-
tive contour model. The second term represents a frictional
force which continually dissipates energy. This endows the
evolving system with a monotone decrease in total energy
(combined potential plus kinetic) over time, which is the ba-
sis for its convergence. Finally, the last two terms (bottom
line), accomplish the parallel transport of evolution forces
over time to the constantly changing contour shape, thereby
capturing and mapping the evolution history into the vector
ﬁeld along the updated active contour.

Coupled PDE system If we start with zero initial velocity
we can decompose this nonlinear second-order PDE into the
following coupled system of nonlinear ﬁrst order PDE’s

∂C
∂t

= βN,

∂β
∂t

=

λk2tk−2

ρ

f +

1
2

β2κ −

k + 1

t

β (13)

Since the contour evolution remains purely geometric (only
in the normal direction N ) we may also write down an im-
plicit level set coupled PDE system

∂ ˆβ
∂t

=

λk2t(k−2)

ρ

ˆf + ∇ ·(cid:18) 1

2

k∇ψk(cid:19) −
ˆβ2 ∇ψ

k + 1

t

ˆβ

(14)

∂ψ
∂t

= ˆβk∇ψk

where ˆf (x, t) and ˆβ(x, t) denote spatial extensions of f
and β respectively.
Numerical advantages A signiﬁcant advantage of the
coupled PDE system (14) is that narrow band level set meth-
ods may be used to simultaneously evolve the level set func-
tion ψ and the normal speed function β within a small sub-
set of a Cartesian grid representing a local neighborhood
around the curve C (represented implicitly as the zero level
set of ψ). Incorporating traditional momentum techniques
into fully global methods of discrete region evolution on
Cartesian grids (e.g. Chambolle-Pock) would not yield this
same computational advantage.

A second advantage of the accelerated active contour
scheme is the disappearance of diffusion terms that would
normally appear due to arclength regularization in gradient
descent. In such cases, the gradient f would include cur-
vature forces along the inward normal, giving rise to a ge-
ometric heat ﬂow ∂C
∂s2 . In the accelerated
case, ignoring the additional frictional and transport terms
∂t2 = ∂ 2C
in (12), we obtain the simple wave equation ∂ 2C
∂s2
instead (a more complicated wave equation with the addi-
tional terms).

∂t = κN = ∂ 2C

The fact

that regularizing diffusion terms turn into
wave terms offers yet another huge advantage numerically.
Namely, simple explicit forward-Euler discretizations of the
accelerated contour system (14) can be stably implemented
with time steps ∆t that are directly proportional to the grid
spacing ∆x, whereas the in the case of diffusion, stable
time steps are constrained by the square of the grid spac-
ing ∆x2, making explicit gradient descent PDE schemes
painfully slow on high resolution grids.

This signiﬁcant discrete time step improvement is a gen-
eral property of accelerated PDE’s which comes from Von
Neumann analysis of their explicit forward discretization.
We provide derivations and a detailed analysis of this phe-
nomenon in a companion work [1] for a variety of different
explicit Euler discretization schemes.

3.3. Options enabled for even greater robustness

Time integration of local gradient measurements f as the
curve evolves is the key mechanism by which the acceler-
ated active contour evolves with regularity despite the ab-
sence of the explicit diffusion style regularizing forces that
arise in their classic gradient descent counterparts. How-
ever, additional options for even further evolution regularity
are facilitated in the accelerated framework as well.

Sobolev-style gradient smoothing Additional averaging
of gradient measurements along the curve itself can be in-
corporated dynamically by heuristically adding a diffusion
term into the velocity evolution (not be be confused with a
diffusion term in the curve evolution) in (13) as follows

acceleration

z}|{∂β

∂t

=

λk2tk−2

ρ

gradient

z}|{f +

1
2

β2κ −

friction

k + 1

z }| {

t

β +

diffusion

z }| {

∂2β
∂s2

τ

(15)

where τ > 0 represents a tunable diffusion coefﬁcient.
Large values of τ would give preferential treatment to
coarse scale deformations of the evolving contour during
the early stages of evolution, with ﬁner scale deformations
gradually folding in more and more as the contour con-
verges toward a steady state conﬁguration.

Such a coarse-to-ﬁne behavior would be consistent with
that of a Sobolev active contour. In fact, diffusion over a ﬁ-
nite amount of time is similar to convolution with a smooth-

12323

ing kernel, which is indeed one way to relate the veloc-
ity ﬁeld of a Sobolev active contour with the simple gra-
dient ﬁeld f N . As such, the incorporation of a diffusion
term into the acceleration PDE is the closest and most di-
rect way to endow the accelerated active contour with addi-
tional coarse-to-ﬁne Sobolev active contour behaviors with-
out directly employing Sobolev norms in the deﬁnition of
the kinetic energy (which would require full linear operator
inversion at every time step during the accelerated ﬂow, just
as in actual Sobolev gradient ﬂows).

A key difference of such an added diffusion term, com-
pared to Sobolev active contours, is that this smoothing pro-
cess of the gradient ﬁeld along the contour is carried out
concurrently with the accelerated contour evolution itself,
rather than statically at each separate time step. As such, if
the diffusion coefﬁcient τ is small enough to allow stable
discretization of the PDE with the same time step dictated
by the other ﬁrst order terms, then no additional computa-
tional cost is incurred. As the diffusion coefﬁcient is in-
creased, however, the discrete CFL conditions arising from
the added second-order diffusion term will begin to dom-
inate in the numerical implementation of the PDE and re-
quire smaller and smaller time steps.

Stochastic acceleration terms The accelerated PDE
framework, unlike the gradient descent PDE framework, of-
fers a numerical opportunity to introduce random noise into
the evolution process without destroying the continuity of
the evolution process nor of the evolving object. For ex-
ample, we could replace the optional diffusion term with a
stochastic term as follows

acceleration

z}|{∂β

∂t

=

λk2tk−2

ρ

gradient

z}|{f +

friction

k + 1

z }| {

t

β +

β2κ −

1
2

noise

z}|{τ W (16)

where W represents random samples drawn from some
distribution and τ is a positive tunable coefﬁcient (similar to
the diffusion coefﬁcient in Section 3.3). Since the noise is
added to the acceleration, it gets twice integrated in the con-
struction of the updated contour (or surface) and therefore
does not immediately interfere with the continuity nor the
ﬁrst order differentiability of the evolving variable. As such,
both the speed β as well as the unit normal N of the contour
(and hence the velocity ∂C
∂t ), remain continuous during the
coupled PDE evolution. The contour therefore maintains
regularity (at least short term).

Adding random noise to a standard (non-accelerated)
gradient descent contour PDE, on the other hand, has never
been a viable option since noise added directly to the ve-
locity is integrated only once, which does not maintain con-
tinuity in the unit normal N of the evolving contour. As
such, the contour would immediately become irregular. As
such, accelerated PDE’s open up a whole new avenue for
the inclusion of stochastic terms (as often exploited in ﬁnite

Fig. 1. Different initial contours ﬂowing into local minima

dimensional problems) which offer increased resistance to
local minimizers. The potential beneﬁt of such a random
noise term would be to provide a second and independent
mechanism (beyond the acceleration) to perturb the opti-
mization ﬂow away from saddle points or shallow minimiz-
ers (e.g., see [48, 49] for PDE stochastic methods in the
context of deep learning).

4. Illustrative results

In this section we illustrate the performance boost of
reformulating an existing active contour model into the
accelerated framework and compare performance against
Chambolle-Pock. As the scope of this paper is not to in-
vent or put forward any particular active contour model,
but rather an accelerated framework that can apply to any
variational active model, we will keep the test images sim-
ple, such that the popular binary region based active contour
models (such as Chan-Vese) are well suited to the segmen-
tation task. We will, however, demonstrate that such mod-
els used without sufﬁcient regularity (in this case arc length
penalty), become prone to unwanted local minimizers when
implemented as standard gradient descent active contours.
While alternate global strategies have been developed in
recent years (e.g. Chambolle-Pock) to solve that problem
for this special class of binary region based active contours,
these strategies are not extendable with the same general-
ity as the PDE acceleration framework presented here for a
richer class of active contour models. We will see in these
couple illustrative examples, that simply applying the con-
tour acceleration is itself sufﬁcient to ﬁx the sensitivity to
local minimizers without the need to abandon the active
contour framework itself in favor of less general global con-
vex optimization methodologies.

In Figure 1 we see three different initial contour place-
ments (top, middle, bottom) evolving from left-to-right via
the gradient ﬂow PDE (2). Each gets trapped within a dif-
ferent local minimizer due to noise, all of which lie very
far away from the desired much deeper minimizer along the
rectangle boundary. Of course, stronger regularizing terms
could be added to the active contour energy functional to
impose smoothness on the contour, thereby making it resis-

12324

Fig. 2. Accelerated active contours ﬂowing to similar result

tant to noise. However, the point of this experiment was
to create an energy landscape littered with literally tens of
thousands (perhaps even hundreds of thousands) of local
minimizers in order to demonstrate the effects of acceler-
ation. Furthermore, stronger regularization would sacriﬁce
the ability to capture the sharp corners of the rectangle and
increase the computational cost due to smaller resulting step
size constraints in the PDE discretization.

We avoid both of these sacriﬁces by using the exact same
active contour force f within the accelerated PDE system
(13) instead.
In Figure 2, we see the effect of applying
accelerated contour evolution scheme with the same initial
contour placements and same energy functional (no addi-
tional regularizing terms). In all three cases, the accelerated
PDE system pushes the contour past the noise, driving it
toward a more robust minimum along the rectangle edge.

In Figure 3 we see this same dramatic difference on a
real seismographic image where we attempt to use an active
contour to pull out the rather noisy ”core” of the recorded
seismograph line. Along the left column we see four differ-
ent initial contour placements, where the ﬁrst three elliptical
initializations, which are far from the desired segmented re-
sult, pose a considerable challenge to a classical gradient
descent active contour. Minimal regularization is allowed
here given the spikey nature of the signal, at least in cases
where we wish to capture this ﬁne scale level of detail.

In the middle column, we see the converged active con-
tour results based on the standard gradient ﬂow version of
the evolution given by (2). Only in the last (bottom) case, is
the segmented result reasonable.

In the last column, we see the converged result of the
same active contour energy E and force f evolved using
the accelerated accelerated PDE system (13). While there
are very subtle differences in the ﬁnal results (as can be see
by the slight differences in the converged energy value), all
four are nonetheless reasonable now even from the ﬁst three
challenging initial contour placements.

In Table 1, we compare our method active contours (AC)
to global convex Chambolle/Pock (CP) [20], and ﬁnd com-
parable robustness to local minima/initialization as global
methods but with a signiﬁcant computational savings. We
choose the regularity such that standard active contours con-

Fig. 3. Non-accelerated (middle) vs. accelerated (right) ac-
tive contour results for same four initializations (left) on a
seismograph image. Cost functional values underneath.

Table 1:
[Left]: PDE Acceleration (AC) offers a com-
parable level of robustness to initialization as global con-
vex Chambolle/Pock (CP) in lower computational time.
[Right]: Visual comparison for the results with greatest en-
ergy difference in CP & AC shows that the energy differ-
ences are nearly in-perceptible.

verges to a local minima (not the global) over multiple dif-
ferent initializations, so that a better method is required to
optimize the energy. The regularity is also chosen with the
performance of CP in mind for the comparison, as CP also
requires a sufﬁciently high regularity, although lower than
standard active contours, to segment the region.

We run each AC and CP to convergence and measure the
computational time, and ﬁnal energy for 3 initializations (a
square inside and close to the desired segmentation - Near
square, a square far from the desired segmentation - Far
square, and a threshold of the image - Theshold Mask) and
4 different image resolutions. A scaled down noisy binary
square image of resolution 1120 x 1120 with the resulting
segmentation is also shown. Results are displayed in Table
1. This comparison shows that our method consistently ob-
tains comparable local optima over different initializations,
similar to CP, but with less computational time. Further-
more, our method applies more generally to non-convex
problems, where we would expect similar robustness in our
method, and where CP is not applicable.

References

[1] A. J. Yezzi and G. Sundaramoorthi, “Accelerated op-
timization in the PDE framework: Formulations for

12325

the active contour case,” CoRR, vol. abs/1711.09867,
2017.

[2] I. Mukherjee, K. Canini, R. Frongillo, and Y. Singer,
“Parallel boosting with momentum,” in Machine
Learning and Knowledge Discovery in Databases
(H. Blockeel, K. Kersting, S. Nijssen, and F. Zelezny,
eds.), pp. 17–32, Springer, Berlin, 2013.

[3] H. Li and Z. Lin, “Accelerated proximal gradient
methods for nonconvex programming,” in Advances in
Neural Information Processing Systems 28 (C. Cortes,
N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Gar-
nett, eds.), pp. 379–387, Curran Associates, Inc.,
2015.

[4] W. Krichene, A. Bayen, and P. L. Bartlett, “Acceler-
ated mirror descent in continuous and discrete time,”
in Advances in Neural Information Processing Sys-
tems 28 (C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, eds.), pp. 2845–2853,
Curran Associates, Inc., 2015.

[5] V. Jojic, S. Gould, and D. Koller, “Accelerated dual
decomposition for map inference,” in Proceedings of
the 27th International Conference on International
Conference on Machine Learning, ICML’10, pp. 503–
510, 2010.

[6] S. Ji and J. Ye, “An accelerated gradient method for
trace norm minimization,” in Proceedings of the 26th
Annual International Conference on Machine Learn-
ing, ICML ’09, pp. 457–464, 2009.

[7] C. Hu, W. Pan, and J. T. Kwok, “Accelerated gradient
methods for stochastic optimization and online learn-
ing,” in Advances in Neural Information Processing
Systems 22 (Y. Bengio, D. Schuurmans, J. D. Lafferty,
C. K. I. Williams, and A. Culotta, eds.), pp. 781–789,
Curran Associates, Inc., 2009.

[8] S. Ghadimi and G. Lan, “Accelerated gradient meth-
ods for nonconvex nonlinear and stochastic program-
ming,” Math. Program., vol. 156, no. 1-2, pp. 59–99,
2016.

[9] N. Flammarion and F. Bach, “From averaging to ac-
celeration, there is only a step-size,” in Proceedings
of Machine Learning Research, vol. 40, pp. 658–695,
2015.

[10] S. Bubeck, Y. T. Lee, and M. Singh, “A geometric al-
ternative to nesterov’s accelerated gradient descent,”
CoRR, vol. abs/1506.08187, 2015.

[11] B. O’Donoghue and E. Cand`es, “Adaptive restart for
accelerated gradient schemes,” Foundations of Com-
putational Mechanics, vol. 15, no. 3, pp. 715–732,
2015.

[12] A. Wibisono, A. C. Wilson, and M. I. Jordan, “A varia-
tional perspective on accelerated methods in optimiza-
tion,” Proceedings of the National Academy of Sci-
ences, p. 201614734, 2016.

[13] Y. Nesterov, “A method of solving a convex program-
ming problem with convergence rate o (1/k2),” in
Soviet Mathematics Doklady, vol. 27, pp. 372–376,
1983.

[14] W. Su, S. Boyd, and E. Cand`es, “A differential
equation for modeling nesterov’s accelerated gradient
method: Theory and insights,” in Advances in Neu-
ral Information Processing Systems, pp. 2510–2518,
2014.

[15] Y. Zhao, L. Rada, K. Chen, S. P. Harding, and
Y. Zheng, “Automated vessel segmentation using inﬁ-
nite perimeter active contour model with hybrid region
information with application to retinal images,” IEEE
Transactions on Medical Imaging, vol. 34, pp. 1797–
1807, Sept 2015.

[16] D. Bryner and A. Srivastava, “Bayesian active con-
tours with afﬁne-invariant, elastic shape prior,” in
2014 IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 312–319, June 2014.

[17] M. Slavcheva, M. Baust, and S. Ilic, “Sobolevfusion:
3d reconstruction of scenes undergoing free non-rigid
motion,” in The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[18] X. Sun, N.-M. Cheung, H. Yao, and Y. Guo, “Non-
rigid object tracking via deformable patches using
shape-preserved kcf and level sets,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5495–5503, 2017.

[19] T. F. Chan, S. Esedoglu, and M. Nikolova, “Algo-
rithms for ﬁnding global minimizers of image seg-
mentation and denoising models,” SIAM journal on
applied mathematics, vol. 66, no. 5, pp. 1632–1648,
2006.

[20] A. Chambolle and T. Pock, “A ﬁrst-order primal-dual
algorithm for convex problems with applications to
imaging,” Journal of mathematical imaging and vi-
sion, vol. 40, no. 1, pp. 120–145, 2011.

12326

[21] T. Goldstein, X. Bresson, and S. Osher, “Geometric
applications of the split bregman method: segmenta-
tion and surface reconstruction,” Journal of Scientiﬁc
Computing, vol. 45, no. 1-3, pp. 272–293, 2010.

[33] S. Osher and J. Sethian, “Fronts propagation with
curvature dependent speed: Algorithms based on
hamilton-jacobi formulations,” Journal of Computa-
tional Physics, vol. 79, pp. 12–49, 1988.

[22] T. Pock, A. Chambolle, D. Cremers, and H. Bischof,
“A convex relaxation approach for computing minimal
partitions,” in Computer Vision and Pattern Recogni-
tion, pp. 810–817, IEEE, 2009.

[23] N. Khan and G. Sundaramoorthi, “Learned shape-
tailored descriptors for segmentation,” in Proceedings
of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 666–674, 2018.

[24] W. Liu, Y. Song, D. Chen, Y. Yu, S. He, and R. W.
Lau, “Deformable object tracking with gated fusion,”
arXiv preprint arXiv:1809.10417, 2018.

[25] D. Adalsteinsson and J. Sethian, “A fast level set
method for propagating interfaces,” Journal Compu-
tational Physics, vol. 118, no. 2, pp. 269–277, 1995.

[34] J. L. Troutman, Variational Calclus and Optimal Con-

trol. Springer-Verlag, New York, 1996.

[35] V. Caselles, R. Kimmel, and G. Sapiro, “Geodesic ac-
tive contours,” International Journal on Comptuer Vi-
sion, vol. 22, no. 1, pp. 61–79, 1997.

[36] S. Kichenassamy, A. Kumar, P. Olver, A. Tannen-
baum, and A. Yezzi, “Conformal curvature ﬂows:
From phase transistions to active vision,” Archive for
Rational Mechanics and Analysis, vol. 134, pp. 275–
301, 1996.

[37] G. Charpiat, R. Keriven, J. Pons, and O. Faugeras,
“Designing spatially coherent minimizing ﬂows for
variational problems based on active contours,” in Int.
Conference Computer Vision, 2005.

[26] T. Chan and L. Vese, “Active contours without edges,”
IEEE Transactions on Image Processing, vol. 10,
no. 2, pp. 266–277, 2001.

[38] G. Sundaramoorthi, A. Yezzi, and A. Mennucci,
“Sobolev active contours,” Int. J. Computer Vision,
vol. 7, pp. 345–366, 2007.

[27] G. Sundaramoorthi and A. Yezzi, “Variational pdes
for acceleration on manifolds and application to
diffeomorphisms,” in Advances in Neural Informa-
tion Processing Systems 31 (S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, eds.), pp. 3793–3803, Curran Associates,
Inc., 2018.

[28] G. Sundaramoorthi and A.

“Acceler-
ated optimization in the pde framework: Formula-
tions for the manifold of diffeomorphisms,” arXiv,
vol. 1804.02307, 2018.

J. Yezzi,

[29] M. Benyamin, J. Calder, G. Sundaramoorthi, and A. J.
Yezzi, “Accelerated pde’s for efﬁcient solution of reg-
ularized inversion problems,” arXiv, vol. 1810.00410,
2018.

[39] G. Sundaramoorthi, A. Yezzi, and A. Mennucci,
“Coarse-to-ﬁne segmentation and tracking using
sobolev active contours,” IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, vol. 30, no. 5,
pp. 851–864, 2008.

[40] Y. Yang and G. Sundaramoorthi, “Shape tracking with
occlusions via coarse-to-ﬁne region based sobolev de-
scent,” Trans. Pattern Analysis and Machine Intelli-
gence, 2015.

[41] Y. Nesterov, Introductory Lectures on Convex Opti-
mization: A Basic Course. Springer Publishing Com-
pany, Incorporated, 1 ed., 2014.

[42] Y. Nesterov, “Gradient methods for minimizing com-
posite functions,” Math. Program., vol. 140, no. 1,
pp. 125–161, 2013.

[30] J. Sethian, Level Set Methods: Evolving Interfaces
in Geometry, Fluid Mechanics, Computer Vision, and
Material Science. Cambridge University Press, 1996.

[43] Y. Nesterov, “Accelerating the cubic regularization of
newton’s method on convex problems,” Math. Pro-
gram., vol. 112, no. 1, pp. 159–181, 2008.

[31] G. Sapiro, Geometric Partial Differential Equations
and Image Analysis. Cambridge Press, Cambridge,
England, 2000.

[44] Y. Nesterov and B. T. Polyak, “Cubic regularization
of newton method and its global performance,” Math.
Program., vol. 108, no. 1, pp. 177–205, 2006.

[32] S. Osher and N. Paragios, Geometric Level Set Meth-
ods in Imaging, Vision and Graphics. Springer, New
York, 2003.

[45] Y. Nesterov, “Smooth minimization of non-smooth
functions,” Math. Program., vol. 103, no. 1, pp. 127–
152, 2005.

12327

[46] H. Goldstein, C. P. Poole, and J. L. Safko, Classical

Mechanics. Addison Wesley, 2002.

[47] M. Niethamer and A. Tannenbaum,

“Dynamic
geodesic snakes for visual tracking,” IEEE Transac-
tions on Automatic Control, vol. 51, no. 4, pp. 562–
579, 2006.

[48] P. Chaudhari, A. Oberman, S. Osher, S. Soatto, and
G. Carlier, “Deep relaxation: partial differential equa-
tions for optimizing deep neural networks,” arXiv
preprint arXiv:1704.04932, 2017.

[49] P. Chaudhari and S. Soatto, “Stochastic gradient
descent performs variational
inference, converges
to limit cycles for deep networks,” arXiv preprint
arXiv:1710.11029, 2017.

12328

