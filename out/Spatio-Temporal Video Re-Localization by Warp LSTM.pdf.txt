Spatio-temporal Video Re-localization by Warp LSTM

Yang Feng♯∗

Lin Ma♮†

Wei Liu♮

Jiebo Luo♯

♮Tencent AI Lab

♯University of Rochester

{yfeng23,jluo}@cs.rochester.edu

forest.linma@gmail.com

wl2223@columbia.edu

Abstract

The need for efﬁciently ﬁnding the video content a
user wants is increasing because of the erupting of user-
generated videos on the Web. Existing keyword-based or
content-based video retrieval methods usually determine
what occurs in a video but not when and where. In this pa-
per, we make an answer to the question of when and where
by formulating a new task, namely spatio-temporal video
re-localization. Speciﬁcally, given a query video and a ref-
erence video, spatio-temporal video re-localization aims to
localize tubelets in the reference video such that the tubelets
semantically correspond to the query. To accurately local-
ize the desired tubelets in the reference video, we propose
a novel warp LSTM network, which propagates the spatio-
temporal information for a long period and thereby cap-
tures the corresponding long-term dependencies. Another
issue for spatio-temporal video re-localization is the lack
of properly labeled video datasets. Therefore, we reorga-
nize the videos in the AVA dataset to form a new dataset for
spatio-temporal video re-localization research. Extensive
experimental results show that the proposed model achieves
superior performances over the designed baselines on the
spatio-temporal video re-localization task.

1. Introduction

Video sharing websites or APPs are becoming more pop-
ular than ever before. Besides the traditional video-sharing
websites, including YouTube1 and Facebook2, the recently
emerged short video sharing APPs, such as SnapChat3 and
TikTok4, arouse the passion of ordinary users for creating
and sharing video contents. With more and more videos
generated every day, exploring so many videos becomes in-
creasingly challenging. It is necessary to build tools which

∗This work was done while Yang Feng was a Research Intern with Ten-

cent AI Lab.

†Corresponding author.
1https://www.youtube.com
2https://www.facebook.com
3https://www.snapchat.com
4https://www.tiktok.com

query

reference

Figure 1. The query is a video containing an action performed
by two characters. The reference video contains two boys per-
forming the same action. Given the query, spatio-temporal video
re-localization aims to localize the tubelets in the reference video
such that the tubelets express the same visual concept as the query.
The desired tubelet in the reference is marked by green. Best
viewed in color.

can help users ﬁnd the video contents they want efﬁciently.
Keyword-based video search is prevalent among users
when they want to ﬁnd some videos. Although it is a
powerful method, keyword-based video search results are
largely determined by the text information associated with
the videos. As such, content-based video retrieval (CBVR)
methods [2, 3, 5, 11, 23, 30, 44, 47, 51] are proposed for
tackling this problem. With the indexing and retrieval tech-
niques, a large list of videos is returned by the CBVR sys-
tem. Only the top results will be viewed by a user, as it is
time-consuming to browse the whole video from the begin-
ning to the end and thereby determine the relevance.

Two kinds of methods are designed to avoid browsing the
whole video. The ﬁrst kind is video summarization meth-
ods [32, 58], which generate a short synopsis for a long
video. The second kind of methods [7, 8, 13, 14, 19, 22,
31, 37, 41] try to trim the video segment of interest. Us-
ing natural language as a query, [14, 19] retrieve a speciﬁc
temporal segment in a video, which shares the same seman-
tic meaning as the query. By replacing the query sentence
with a sample video clip, video re-localization [13] aims
to temporally localize video segments, which semantically
correspond to the query video clip.

In this paper, we extend the temporal video re-
localization [13] to the spatio-temporal domain. Specif-

1288

ically, given a query video, spatio-temporal video re-
localization (STVR) aims to localize tubelets in a reference
video such that the tubelets are semantically coherent with
the query video. Figure 1 illustrates an example pair of
query and reference videos. There are several advantages
in localizing tubelets over temporal localization based on
whole frames. First, localizing tubelets can handle the cases
where multiple events are happening at the same time in the
reference video. When using whole video frames for recog-
nition or temporal detection tasks, it usually assumes that
only one event is undergoing. The assumption rarely holds
in unconstrained environments. Second, the recognition ac-
curacy will substantially increase because the inﬂuence of
background regions is reduced by only focusing on speciﬁc
regions. STVR is also a more challenging task than tempo-
ral video re-localization. First, the training videos should be
labeled with bounding boxes over a long period, which con-
sumes more human labors than temporal annotation only.
Second, detecting the bounding boxes at each frame is more
difﬁcult than only localizing the starting and ending bound-
ary points.

To address the STVR task, we propose a matching
framework consisting of three modules: query encoding,
reference encoding, and query-reference interaction. The
query encoding module encodes a query video of arbitrary
resolution into a series of ﬁxed size feature cubes. The ref-
erence encoding module encodes the given reference video
in a different manner. To keep the detailed spatio infor-
mation in the reference, the shape of the reference feature
cube is proportional to the resolution of the reference video.
In the query-reference interaction module, several bounding
box proposals are generated for the reference and then each
proposal is matched with the query to determine whether a
proposal and the query are semantically corresponding to
each other.

To accurately localize the tubelets in the reference, the
long-term spatio-temporal information needs to be mod-
eled. We propose a novel warp LSTM network for this
purpose. Warp LSTM is a variant of ConvLSTM [39]. In
ConvLSTM, the previous hidden state is concatenated with
the current input for further computation. Different from
ConvLSTM, the previous hidden state in warp LSTM is
warped before the concatenation to make the previous hid-
den state be aligned with the current input if any movement
in the video makes them unaligned. The warp of the hid-
den state at a previous time-step can compensate for small
movements in the video, which accurately aggregates the
spatio history information of moving objects.

In order to train the matching model for STVR, we create
a new dataset by reorganizing the videos in the AVA dataset
[16]. The AVA dataset is originally used for spatio-temporal
action localization. Each action tubelet is annotated with
one or several atomic action labels. We use one action

tubelet as the query and ﬁnd the tubelets with the same ac-
tion labels in the reference video. Two action tubelets are
semantically corresponding to each other if the action labels
of the two tubelets are exactly the same. The AVA dataset
provides a subset of videos for training and another subset
of videos for validation. We further split the action tubelets
into training, validation, and test subsets according to their
action categories. Such a splitting guarantees that the vali-
dation and testing categories do not overlap with the train-
ing categories.

In summary, our contributions are four-fold:
• We make the ﬁrst attempt to tackle the STVR task,
which aims to localize tubelets in the reference video
such that the tubelets semantically correspond to a
given query video.

• We propose a novel warp LSTM network to propa-
gate the spatio-temporal information between adjacent
frames for a long period and thereby capture the corre-
sponding long-term dependencies.

• We reorganize the videos in the AVA dataset [16] to

form a new dataset for the research on STVR.

• We conduct extensive experiments on the new dataset,
which shows that the warp LSTM performs better than
the competing methods.

2. Related Work

Video Representations. Convolutional Neural Net-
works (CNNs) have broken many records of computer vi-
sion tasks, such as image classiﬁcation [18, 36], object de-
tection [24], semantic segmentation [9], facial expression
recognition [55], and captioning [10, 28, 48, 49, 53]. Due
to the great success of CNNs on images, many researchers
tried to apply CNNs on videos.
[34, 42, 54] are mainly
based on 2D CNNs, in which the motion information is not
fully exploited. 3D CNNs are proposed in [27, 46, 56] to
capture more complex motion patterns. The recently pro-
posed I3D feature [4] has achieved state-of-the-art action
recognition results. Compared with 3D CNNs, the proposed
warp LSTM is able to model the long-term spatio-temporal
information of moving objects for classiﬁcation and lo-
calization tasks by explicitly modeling the movements in
videos.

Video Re-localization. Video Re-localization [13] aims
to ﬁnd segments in reference videos semantically corre-
sponding to a given query video. A more specialized task,
one-shot action localization [52], focuses on the tempo-
ral detection of actions in videos giving an example. The
STVR task to be solved in this paper is an extension of
temporal video re-localization. Besides predicting the start-
ing and ending points of a video segment, STVR also de-
tects the spatio localization of the video content that users
are interested in. Hoogs et al. [21] designed a system to
spatio-temporally retrieve people and vehicles in surveil-

1289

Figure 2. Comparison between ConvLSTM and warp LSTM. (a)
In ConvLSTM, the input xt and the hidden state at previous time-
step ht−1 are convolved with different ﬁlters and then added to
produce the new hidden state ht.
(b) In warp LSTM, ht−1 is
warped by a differentiable spline interpolation before the convo-
lution to compensate for the small motion between consecutive
video clips.

lance videos. STVR is different in that it is not specialized
in certain categories or types of videos.

Spatio-temporal Detection. Two related vision tasks
are video object detection and spatio-temporal action detec-
tion. All the three tasks need to model long-term spatio-
temporal dependencies to predict tubelets in videos. Al-
though the temporal information inside a clip is considered
in [22, 31], the bounding boxes are predicted independently
of the frames outside the short clip. To solve this problem,
both [33, 37] extract tubelet proposals from videos in the
ﬁrst stage and make the classiﬁcation in the second stage.
One assumption used in both [33, 37] is that the reception
ﬁeld of CNN features is large enough to handle the small
movements in a short time. With this assumption, the fea-
ture cropped at a previous anchor location is used to pre-
dict the bounding boxes at the current frame. Although
the reception ﬁeld is large enough to cover the objects with
small movements, the bounding box prediction will become
a more difﬁcult task on the feature map with offsets. Dif-
ferent from them, we align the previous feature maps with
the current feature map by warping. The proposed warp
LSTM can reduce the offset of the previous feature map
and thereby reduce the burden of the bounding box predic-
tion module.

3. Spatio-temporal Information Propagation

In this section, we present our proposed warp LSTM net-
work for modeling the long-term spatio-temporal informa-
tion in videos. Warp LSTM is a variant of ConvLSTM [39].
We ﬁrst give the background knowledge of ConvLSTM.

3.1. ConvLSTM

ConvLSTM extends the fully-connected LSTM [20] to
have convolutional structures in both the input-to-state and

t ¡ 1

t

Figure 3. An illustration of a moving object in two consecutive
time-steps. At time-step t − 1, the object is located at the bound-
ing box pt−1. In the following time-step, the object moves to the
location of bounding box pt. qt−1 is a bounding box at time-step
t − 1 having the same position as pt. Please note that the content
in qt−1 may be not semantically related to the object in pt.

state-to-state transitions:

it = σ(Wxi ∗ xt + Whi ∗ ht−1 + bi),
gt = σ(Wxg ∗ xt + Whg ∗ ht−1 + bg),
ft = σ(Wxf ∗ xt + Whf ∗ ht−1 + bf ),
ot = σ(Wxo ∗ xt + Who ∗ ht−1 + bo),
ct = ft ⊙ ct−1 + it ⊙ gt,
ht = ot ⊙ φ(ct),

(1)

where xt, ht, ct, it, ft, and ot are the ConvLSTM input,
hidden state, memory cell, input gate, forget gate, and out-
put gate at time-step t, respectively. All the W s and bs are
the parameters of the ConvLSTM layer. ∗ is the convolu-
tion operation and ⊙ is the element-wise product. σ and
φ are sigmoid non-linearity and hyperbolic tangent nonlin-
earity, respectively. In Eq. (1), xt and ht−1 are convolved
with different ﬁlters and then added for later computation,
as shown in Figure 2. This operation is equivalent to ﬁrst
concatenating xt and ht−1 along the channel dimension and
then computing the convolution.

3.2. Warp LSTM

If no movement happens in the video at the t-th time-
step, the concatenation of xt and ht−1 in ConvLSTM is per-
fectly ﬁne. However, when motion happens at time-step t,
the concatenation of xt and ht−1 may cause errors in spatio-
temporal localization tasks. Figure 3 shows a moving object
at two consecutive time-steps. At time-step t − 1, the object
is located at the bounding box pt−1. In the following time-
step, the object moves to the location of bounding box pt.
qt−1 is a bounding box at time-step t − 1 having the same
position as pt. The content in qt−1 may be depicting objects
other than the aforementioned object. As such, simply con-
catenating the features at the locations of qt−1 and pt may
introduce noises into the classiﬁcation and localization of
pt.

We propose warp LSTM to address this issue by warping
the hidden state at the previous time-step before concatenat-
ing it with the input. Figure 2 illustrates the proposed warp

1290

LSTM, where the warp is implemented by the differentiable
spline interpolation [12], as illustrated in Figure 4. Given
a set of 2-D control points {(x1, y1), . . . , (xn, yn)} on the
hidden state h, the warp operation tries to shift (xi, yi) to a
new position (xi + dxi, yi + dyi), where n is the number
of control points and (dxi, dyi) is the desired displacement
of the i-th control point. Let h′ denote the warped hidden
state, then we have:

h′[xi + dxi, yi + dyi] = h[xi, yi], ∀i ∈ {1, . . . , n}.

(2)

Besides shifting the control points, the warping is continu-
ous on the whole 2D space of h, resulting in a dense ﬂow
ﬁeld. The ﬂow ﬁeld is estimated by the polyharmonic inter-
polation [26]:

s(x, y) =

n

X

i=1

wiφk(k(x, y) − (xi, yi)k) + v1x + v2y + v3,

(3)
where φk is a set of radial basis functions. wi, v1, v2, and v3
are interpolation parameters. After optimization, the poly-
harmonic interpolation s will shift the control points exactly
to their desired locations. In addition, the warped h′ is a dif-
ferentiable function of h, (xi, yi), and (dxi, dyi).

In practice, the control points are ﬁxed in advance. We
evenly put horizontal lines and vertical lines in the 2D space
of h and put control points on the intersections of horizontal
and vertical lines. The displacement (dxi, dyi) is predicted
by an additional convolutional layer. We also add extra con-
trol points with zero displacements at the boundary. Two ra-
dial basis functions, i.e., φ1(r) = r and φ2(r) = r2 log(r),
are chosen for the interpolation. The proposed warp LSTM
is deﬁned as:

...

t1 ! t2

t2 ! t3

t3 ! t4

t8 ! t9

Figure 4. The illustration of the warp obtained by polyharmonic
interpolation. It can be observed that the spatio-temporal informa-
tion of a moving object is propagated for a long period.

at a previous time-step to the current time-step. However,
there are two major differences between the two methods.
The motivation of TrajGRU is to learn a dynamic connec-
tion structure, e.g., replacing the ﬁxed 3 × 3 convolution
with 5 learned dynamic links. Our motivation is to align the
previous feature map with the current feature map. Several
dense ﬂow ﬁelds are predicted by convolutional layers in
TrajGRU for warping, while the displacements of a set of
control points are predicted in the warp LSTM. The warp
computed by polyharmonic interpolation is continuous ev-
erywhere, while the dense ﬂows generated by convolutional
layers in TrajGRU may be not. TrajMF [29] is also designed
for explicit motion modeling. Compared with TrajMF, the
proposed warp LSTM is not handcrafted and is thus bene-
ﬁting from the feature learning ability of deep neural net-
works.

dt−1 = Wxd ∗ xt + Whd ∗ ht−1 + bd,
h′
t−1 = warp(ht−1, dt−1),
c′
t−1 = warp(ct−1, dt−1),

it = σ(Wxi ∗ xt + Whi ∗ h′
gt = σ(Wxg ∗ xt + Whg ∗ h′
ft = σ(Wxf ∗ xt + Whf ∗ h′
ot = σ(Wxo ∗ xt + Who ∗ h′
ct = ft ⊙ c′
ht = ot ⊙ φ(ct),

t−1 + it ⊙ gt,

t−1 + bi),
t−1 + bg),
t−1 + bf ),
t−1 + bo),

t−1, and c′

where dt−1, h′
t−1 are the displacement, warped
hidden state, and warped memory cell at time-step t − 1,
respectively. warp(·, ·) is the sparse image warping func-
tion5, which warps an image based on control point dis-
placements.

Discussion. The closest work to our proposed warp
LSTM is TrajGRU [40], which also warps the feature map

5https://www.tensorflow.org/api_docs/python/tf/

contrib/image/sparse_image_warp

4. Spatio-temporal Video Re-localization

(4)

Given a query video and a reference video, STVR aims
to localize tubelets in the reference video such that the
tubelets semantically correspond to the query video. To
achieve the goal, we design a novel model detecting bound-
ing boxes in the reference video based on the matching re-
sults between the query and reference videos. Our proposed
model is shown in Figure 5.

4.1. Video Feature Extraction

For the STVR task, both the temporal and spatio infor-
mation should be captured in the raw video feature. Hence,
we choose inﬂated 3D ConvNet (I3D) [4] as the feature ex-
tractor. The I3D model is originally trained on 64-frame
video snippets and tested on 250-frame video snippets. Us-
ing many frames together to extract video features is ﬁne for
video classiﬁcation task, but it may not be a good idea for
the spatio-temporal localization task because the regions we
want to locate may move over a long distance. Therefore,
we reduce the number of frames of the video snippets to 8.

1291

reference

3D 
CNN

3D 
CNN

3D 
CNN

.
.
.

3D 
CNN

warp 
LSTM

warp 
LSTM

warp 
LSTM

.
.
.

warp 
LSTM

proposals

query

RoI

Conv

FC

softmax

A

FC

bbox

regressor

RoI

3D 
CNN

3D 
CNN

.
.
.

3D 
CNN

Figure 5. The architecture of our proposed model for STVR. The inputs are a query and a reference video. Both query and reference are
split into clips and then fed into a 3D CNN to extract video features. Later, the long-term spatio-temporal information in the reference video
is aggregated by the warp LSTM to produce a new reference feature. Region proposal network [38] is applied on the new reference feature
to generate several proposals. For each proposal, we use an attention mechanism to select the most related query feature and concatenate
the proposal feature with the attention weighted query feature. The concatenated feature is used for the second stage prediction, which
outputs a reﬁned bounding box and a binary label indicating whether the query and the proposal are semantically corresponding to each
other. A(cid:13) denotes the attention mechanism, and the dashed rectangle means concatenating along the channel dimension.

We also re-sample all the videos at the FPS of 24 so that
each snippet is just 1
3 second long. We choose the activa-
tion values at the “Mixed 4c” layer in the I3D model as the
video feature, which has a spatio stride of 16 and a temporal
stride of 4.

Let ri ∈ R8×H×W ×3 denote the i-th reference clip,
where H and W are the height and width of the reference
video, respectively. The feature extraction for the reference
is given by:

i ∈ R2× H

ˆf r
i = I3D(ri),
where ˆf r
×512 is the extracted feature for the
i-th reference clip. The 4D feature is transformed to 3D
by ﬂattening along the temporal dimension and the channel
dimension:

× W
16

(5)

i = ﬂatten( ˆf r
f r
where f r
×1024 is the ﬂattened feature. For the
j-th clip in the query video qj , we apply 2D RoI pooling
after 3D convolution to generate a ﬁxed size feature:

i ∈ R

i ),

× W
16

(6)

H
16

16

f q
j = RoI (cid:0)ﬂatten(cid:0)I3D(qj)(cid:1)(cid:1) ,

(7)

where f q

j ∈ R7×7×1024 is the j-th query feature.

4.2. Reference Propagation

The extracted f r

i only contains the spatio-temporal in-
formation within the 8-frame clip. To propagate the spatio-
temporal information from previous clips of the reference
video to the i-th clip for better re-localization, we add a
warp LSTM layer to update the reference feature.

hi = warpLSTM(f r

i , hi−1),

(8)

×1024 is the hidden state of the warp
where hi ∈ R
LSTM, which also serves as a new reference representation.

H
16

× W
16

4.3. Proposal Generation

The proposal generation module aims to ﬁnd all the
bounding boxes containing the content of potential inter-
est in one clip. The generation of reference proposals is
designed following Faster RCNN [38]. hi is fed into the
region proposal network (RPN) to generate proposals:

pk = RPN(hi),
f p
k = RoI(hi, pk),

(9)

where pk is the predicted bounding box for the k-th pro-
posal and f p
k ∈ R7×7×1024 is the feature of the k-th pro-
posal obtained by RoI pooling.

4.4. Query and Reference Matching

We match every proposal in the reference clip with the
query video. The query video may be much longer than one
clip in the reference, which has only 8 frames. As such,
some parts in the query video may not well correspond to
a short proposal. Motivated by [13, 50], we design an at-
tention mechanism to select which part in the query video
should be matched with the proposal. For the k-th proposal,
the features of the query video are attentively summarized
as:

ek,j = tanh(W qavg(f q

j ) + W ravg(f p

k ) + bp),

αk,j =

exp(w⊤ek,j + bs)

Pi exp(w⊤ek,i + bs)

,

¯f q
k = X

αk,j f q
j ,

j

(10)

where ¯f q
k is the weighted query representation. W q, W r, w
are the weight parameters in the attention model with bp and

1292

l

s
e
p
m
a
S
 
f
o
 
r
e
b
m
u
N

 
f
o
 
m
h
t
i
r
a
g
o
L

10
9
8
7
6
5
4
3

0

50

100

train
val
test

300

350

150

200

Combinded Label

250

Figure 6. The distribution of the number of samples for each com-
bined label. The combined labels belonging to training, validation,
and testing sets are marked by green, blue, and red, respectively.
The combined labels with less than 32 tubelet samples in the train-
ing set are omitted for clarity.

bs denoting the bias terms. avg(·) means average pooling
along the spatio dimensions.

4.5. Label and Bounding Box Predictions

The proposal feature f p

k and the attentively weighted
query feature ¯f q
k are concatenated along the channel dimen-
sion for the ﬁnal label prediction and bounding box reﬁne-
ment. The ﬁnal label is binary, indicating whether the query
video and the k-th proposal are semantically corresponding
to each. The ground-truth label will be “true” if the query
video and the k-th proposal are indeed semantically corre-
sponding to each other. Otherwise, the ground-truth label
will be “false”. The bounding box regression layers are de-
signed following [38]. Please refer to [38] for more details.

5. The Reorganized Datasets

Existing video datasets are designed for other vision
tasks, such as classiﬁcation [35], temporal localization [1],
action recognition [43], captioning [6], and video sum-
marization [17]. None of them is suitable for the STVR
task, which requires pairs of query and reference videos.
The query should semantically correspond to some labeled
tubelets in the reference video. It will require a huge expen-
sive labor to collect and annotate such a video dataset.

As such, we propose to reorganize the AVA dataset for
the STVR task. The AVA dataset [16] is originally designed
for the spatio-temporal action localization task. There are
430 15-minute video clips with per second action bound-
ing box annotations. The annotated actions are 80 cate-
gories of atomic actions, including “stand”, “watch”, “lis-
ten”, etc. The actions are exhaustively annotated, which
results in 1.58 million action annotations with multiple la-
bels per person. The ﬁrst step of the reorganization is to
generate tubelets by linking the labeled bounding boxes at
each second. We will link two bounding boxes if they are
the consecutive bounding boxes of the same subject with
all the action labels being the same. After linking, the

tubelets with exactly the same action labels are regarded
as semantically corresponding to each other. For example,
a tubelet labeled with “stand + talk to” semantically corre-
sponds to other tubelets labeled with “stand + talk to” as
well. The tubelet does not correspond to the tubelets la-
beled with “stand” only, “talk to” only, or “sit + talk to”. It
can be understood as that the multiple atomic action labels
annotated with one bounding box are combined together.

Different from spatio-temporal action detection, STVR
aims to semantically match video tubelets beyond a prede-
ﬁned category list. Thus, we further split the video tubelets
according to their combined labels following [13], so that
the training categories have no overlap with the validation or
testing categories. We ﬁrst choose 54 combined labels hav-
ing over 100 tubelet samples from the 64 validation videos.
27 of them are used for validation and the other 27 are used
for testing. After ﬁxing the 54 combined labels, we remove
all the frames overlapping with the tubelets belonging to
the 54 combined labels in the 235 training videos. The
left tubelets in the 235 training videos are used to train our
STVR model. The numbers of tubelets belonging to differ-
ent combined labels are shown in Figure 6.

We describe how to create the query and reference pairs
in the following. The combined action labels having only
one tubelet sample are all discarded because no pair can be
formed for this combined label. For any query tubelet, we
randomly ﬁnd another tubelet having the same combined la-
bel as the target. Then we crop the whole segment contain-
ing the target tubelet as the reference video. Such cropping
will simplify the STVR task because the temporal bound-
ary is known. To avoid this, we crop a segment longer than
the target tubelet so that the reference video contains some
background before and after the target tubelet. One thing
to mention is that the cropped reference video may contain
more than one tubelet having the same label as the query.
All of the tubelets in the reference sharing the same label as
the query are regarded as target tubelets. During training,
the query tubelet and reference video are randomly paired,
while the pairs are ﬁxed for validation and testing.

Following the same intuition, we also reorganize the
videos in the UCF-101-24 dataset [43] for experiments.
Among the 24 action categories, 14, 5, and 5 classes are
used for training, validation, and testing, respectively.

6. Experiments

We conduct several experiments to verify the effective-
ness of warp LSTM in solving the STVR problem. First,
three baseline methods are designed and introduced. Then
we introduce our experimental settings including evaluation
criteria and implementation details. Finally, we report the
quantitative results and show the visualizations.

1293

frame

warp LSTM

TrajLSTM

Optical Flow

Figure 7. The visualization of the warped grids with different methods. Only one of the ﬁve links in TrajLSTM is shown here.

6.1. Baseline Methods

Existing spatio-temporal localization methods mainly
focus on localizing objects or actions in videos. As far as we
know, there is no method speciﬁcally designed for STVR.
So we design three baseline models for comparison.

Clip Independent Baseline. Clip independent baseline
is designed based on the spatio-temporal action localization
methods [22, 31]. The reference video is divided into a se-
ries of 8-frame clips and the bounding box prediction only
depends on the information within the current clip. The clip
independent baseline can be implemented by just removing
the warp LSTM layer in our proposed model in Figure 5.

Other ConvLSTM Variants.

The proposed warp
LSTM can be viewed as a variant to ConvLSTM [39]. So
we create a baseline to compare with the original ConvL-
STM by replacing warp LSTM with ConvLSTM. Similarly,
we also create a baseline for the comparison with Traj-
GRU [40]. We replace the polyharmonic interpolation with
the structure generating network in [40] and name this base-
line as TrajLSTM.

Optical Flow Baseline. Warping images by optical ﬂow
has been widely used in computer vision research. It is also
possible to warp the hidden state of ConvLSTM by the ac-
cumulated optical ﬂow. We create another baseline in which
the hidden state of ConvLSTM is warped according to opti-
cal ﬂow.

6.2. Experimental Settings

We resize all the videos to the resolution 320 × 320 be-
fore feeding them into the CNN models. The I3D model
we use is ﬁrst initialized by training on the Kinetics dataset
[4] and then ﬁne-tuned during the training of our model. To
form a batch during the training process, the length of the
reference video needs to be ﬁxed. The reference video is
ﬁxed to be 2 seconds long by randomly cropping or padding
zeros. During testing, the query and reference video in
full length are fed into the model without batching. For
warp LSTM, we put three horizontal and three vertical lines

on the 20 × 20 feature map, which leads to nine control
points: {(5, 5), (5, 10), (5, 15), (10, 5), (10, 10), (10, 15),
(15, 5), (15, 10), (15, 15)}. The displacements of the con-
trol points are predicted by one CNN layer with a kernel size
of 1 × 1. To reduce the number of model parameters, the
input-to-state and state-to-state convolutions in warp LSTM
are designed following the bottleneck block [18]. The 1024-
channel feature map is ﬁrst projected to 128-channel and a
skip connection is also added from the input to the output
of warp LSTM. The following region proposal layers and
bounding box regression layers are implemented by Ten-
sorﬂow Object Detection API [24]. The number of links for
TrajLSTM is set to be 5. FlowNet2 [25] is used for optical
ﬂow extraction in the optical ﬂow baseline. The optical ﬂow
is resized and rescaled by a factor of 1
16 to ﬁt the size of the
feature map.

All the models are trained using stochastic gradient de-
scent (SGD) with momentum value 0.9. The initial learning
rate is 0.03 and is divided by 10 after 10k iterations. The
batch size we use is set to be 8. It takes about ﬁve hours to
train one model on four Tesla P40 until the convergence.

6.3. Evaluation Metrics

The frame-mAP computed using a modiﬁed version of
the code6 released by ofﬁcial the AVA dataset website is
reported for evaluation. As described in Sec. 5, there are
27 combined labels for testing. Given a pair of query and
reference video, all the labeled bounding boxes in the refer-
ence belonging to the same combined label with the query
are regarded as ground-truth. The bounding boxes predicted
with positive labels are regarded as predictions. The aver-
age precision (AP) for one combined label is computed over
all the ground-truths and predictions belonging to that com-
bined label with IoU over 0.5. We report the mAP, which is
the average of the AP values over the 27 testing combined
labels.

6https://github.com/activitynet/ActivityNet/

blob/master/Evaluation/get_ava_performance.py

1294

groundtruth

warpLSTM

clip

Optical Flow

ConvLSTM

TrajLSTM

query

reference

query

reference

Figure 8. The visualization of the re-localization results. The bounding boxes with the largest conﬁdence of different methods are shown
in different colors.

Table 1. The frame-mAP computed with IoU threshold 0.5 of all
the methods.

Method

AVA UCF-101-24

Clip

ConvLSTM [39]

Optical Flow

TrajLSTM [40]

Warp LSTM

18.8
20.8
20.2
21.0
21.8

52.9
53.5
52.0
54.8
59.4

6.4. Quantitative Results

The quantitative results of all the methods are shown in
Table 1. Meanwhile, Figure 7 shows the warp visualization
of two videos in the test split. By comparing the mAP of
“Clip” and “’ConvLSTM’ baseline, we ﬁnd that propagat-
ing the spatio-temporal information at previous time-steps
to current time-step is better than doing the prediction in-
dependently for each clip. Using accumulated optical ﬂow
to warp the hidden state of the previous time-step leads to
worse results than ConvLSTM, which may be because the
error is too large in the accumulated optical ﬂow. It can be
seen in Figure 7 that the girds warped by optical ﬂow are
noisy. The mAP values of TrajLSTM and ConvLSTM are
very similar. The links learned by TrajLSTM on complex
action videos seem to be some ﬁxed offsets. The perfor-
mance of the warp LSTM is the best of all the methods. The
results show that propagating the long-term spatio-temporal
information by warp LSTM is helpful to STVR.

6.5. Qualitative Results

In the second row in Figure 7, it can be observed that
warp LSTM is able to detect the moving actor and warp
the previous feature maps to compensate for the movement.
The black in the third and fourth row means that these two
methods try to warp some regions outside the feature map
into the outputs. Figure 8 is the visualization of two STVR
results. The combined label of the ﬁrst and second queries
are “walk + talk to + watch” and “stand + answer phone”,
respectively. In the second clip of the ﬁrst reference video,
the person in the ground-truth bounding box is totally oc-

cluded. “Clip” and “Optical ﬂow” baseline fail to localize
correctly because of the occlusion. However, the other three
methods are able to handle the short occlusion because they
can use the spatio-temporal information in previous clips.
In the second example, the two men in the reference video
are both standing. The man on the left is labeled with “stand
+ answer phone” and the man on the right is labeled with
“stand + touch + listen to”. It is difﬁcult to distinguish the
combined label of these two men because their actions look
similar. “Clip”, “ConvLSTM”, “Optical Flow” and “TrajL-
STM” make at least one error among the six clips, while the
proposed warp LSTM correctly localizes the left man all the
time.

7. Conclusion

In this paper, we tackled the spatio-temporal video re-
localization problem for the ﬁrst time. Given a query video,
spatio-temporal video re-localization aims to ﬁnd tubelets
in a reference video such that the tubelets are semanti-
cally corresponding to the given query. Spatio-temporal
video re-localization is a natural extension of the tempo-
ral video-relocalization [13] which can be applied to video
retrieval and surveillance. To make spatio-temporal video
re-localization research possible, we created a new dataset
by reorganizing the videos in the AVA dataset [16]. Fur-
thermore, we proposed a matching model to capture the se-
mantic relationship between the query and reference videos.
The long-term spatio-temporal information is propagated
by a warp LSTM to generate better bounding box predic-
tions. The extensive experimental results show that our pro-
posed method is superior to baseline methods on the spatio-
temporal video re-localization task.

In the future, we plan to integrate the warp operation into

more sophisticated models such as [15, 45, 57].

Acknowledgement

This work is partially supported by NSF awards

1704309, 1722847, and 1813709.

1295

References

[1] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding.
In CVPR,
2015.

[2] Liujuan Cao, Rongrong Ji, Yue Gao, Wei Liu, and Qi Tian.
Mining spatiotemporal video patterns towards robust action
retrieval. Neurocomputing, 2013.

[3] Liujuan Cao, Xian-Ming Liu, Wei Liu, Rongrong Ji, and
Thomas Huang. Localizing web videos using social images.
Information Sciences, 2015.

[4] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In CVPR,
2017.

[5] Shih-Fu Chang, William Chen, Horace J Meng, Hari Sun-
daram, and Di Zhong. A fully automated content-based
video search engine supporting spatiotemporal queries.
CSVT, 1998.

[6] David L Chen and William B Dolan. Collecting highly par-

allel data for paraphrase evaluation. In ACL, 2011.

[7] Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and Tat-
Seng Chua. Temporally grounding natural sentence in video.
In EMNLP, 2018.

[8] Jingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, and Jiebo
Luo. Localizing natural language in videos. In AAAI, 2019.
[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV, 2018.

[10] Xinpeng Chen, Lin Ma, Wenhao Jiang, Jian Yao, and Wei
Liu. Regularizing rnns for caption generation by reconstruct-
ing the past with the present. In CVPR, 2018.

[11] Xin Chen, Chengcui Zhang, Shu-Ching Chen, and Stuart Ru-
bin. A human-centered multiple instance learning framework
for semantic video retrieval. IEEE Transactions on Systems,
Man, and Cybernetics, 2009.

[12] Forrester Cole, David Belanger, Dilip Krishnan, Aaron
Sarna, Inbar Mosseri, and William T Freeman. Synthesiz-
ing normalized faces from facial identity features. In CVPR,
2017.

[13] Yang Feng, Lin Ma, Wei Liu, Tong Zhang, and Jiebo Luo.

Video re-localization. In ECCV, 2018.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[19] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef
Sivic, Trevor Darrell, and Bryan Russell. Localizing mo-
ments in video with natural language. In ICCV, 2017.

[20] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural computation, 1997.

[21] Anthony Hoogs, AG Amitha Perera, Roderic Collins, Arslan
Basharat, Keith Fieldhouse, Chuck Atkins, Linus Sherrill,
Benjamin Boeckel, Russell Blue, Matthew Woehlke, et al.
An end-to-end system for content-based video retrieval us-
ing behavior, actions, and appearance with interactive query
reﬁnement. In AVSS, 2015.

[22] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolu-
tional neural network (t-cnn) for action detection in videos.
In ICCV, 2017.

[23] Jun-Wei Hsieh, Shang-Li Yu, and Yung-Sheng Chen.
Motion-based video retrieval by trajectory matching. CSVT,
2006.

[24] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,
Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-
jna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy
trade-offs for modern convolutional object detectors.
In
CVPR, 2017.

[25] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In CVPR,
2017.

[26] Armin Iske. Multiresolution methods in scattered data mod-

elling. Springer Science & Business Media, 2004.

[27] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. PAMI,
2013.

[28] Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and Tong
Zhang. Recurrent fusion network for image captioning. In
ECCV, 2018.

[29] Yu-Gang Jiang, Qi Dai, Wei Liu, Xiangyang Xue, and
Chong-Wah Ngo. Human action recognition in uncon-
strained videos by explicit motion modeling. TIP, 2015.

[30] Yu-Gang Jiang, Jiajun Wang, Qiang Wang, Wei Liu, and
Chong-Wah Ngo. Hierarchical visualization of video search
results for topic-based browsing. TMM, 2016.

[14] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia.
Tall: Temporal activity localization via language query. In
ICCV, 2017.

[31] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari,
and Cordelia Schmid. Action tubelet detector for spatio-
temporal action localization. In ICCV, 2017.

[15] Rohit Girdhar, Jo˜ao Carreira, Carl Doersch, and Andrew
arXiv preprint

Zisserman. A better baseline for ava.
arXiv:1807.10066, 2018.

[16] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-
oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
George Toderici, Susanna Ricco, Rahul Sukthankar, et al.
Ava: A video dataset of spatio-temporally localized atomic
visual actions. In CVPR, 2018.

[17] Michael Gygli, Helmut Grabner, Hayko Riemenschneider,
and Luc Van Gool. Creating summaries from user videos. In
ECCV, 2014.

[32] Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, and Tat-

suya Harada. aware video summarization. In CVPR, 2018.

[33] Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie
Yan, Xihui Liu, and Xiaogang Wang. Object detection in
videos with tubelet proposal networks. In CVPR, 2017.

[34] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
classiﬁcation with convolutional neural networks. In CVPR,
2014.

[35] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,

1296

Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950,
2017.

[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012.

[37] Dong Li, Zhaofan Qiu, Qi Dai, Ting Yao, and Tao Mei. Re-
current tubelet proposal and recognition networks for action
detection. In ECCV, 2018.

[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

[39] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm
network: A machine learning approach for precipitation
nowcasting. In NIPS, 2015.

[40] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-
Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Deep
learning for precipitation nowcasting: A benchmark and a
new model. In NIPS, 2017.

[53] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and
In

Image captioning with semantic attention.

Jiebo Luo.
CVPR, 2016.

[54] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vi-
jayanarasimhan, Oriol Vinyals, Rajat Monga, and George
Toderici. Beyond short snippets: Deep networks for video
classiﬁcation. In CVPR, 2015.

[55] Kaihao Zhang, Yongzhen Huang, Yong Du, and Liang Wang.
Facial expression recognition based on deep evolutional
spatial-temporal networks. TIP, 2017.

[56] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Wei Liu,
and Hongdong Li. Adversarial spatio-temporal learning for
video deblurring. TIP, 2019.

[57] Yue Zhang, Qi Liu, and Linfeng Song. Sentence-state lstm

for text representation. In ACL, 2018.

[58] Bin Zhao, Xuelong Li, and Xiaoqiang Lu. Hsa-rnn: Hier-
archical structure-adaptive rnn for video summarization. In
CVPR, 2018.

[41] Zheng Shou,

Junting Pan,

Jonathan Chan, Kazuyuki
Miyazawa, Hassan Mansour, Anthony Vetro, Xavier Giro-i
Nieto, and Shih-Fu Chang. Online detection of action start
in untrimmed, streaming videos. In ECCV, 2018.

[42] Karen Simonyan and Andrew Zisserman. Two-stream con-
In

volutional networks for action recognition in videos.
NIPS, 2014.

[43] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402, 2012.

[44] Chih-Wen Su, Hong-Yuan Mark Liao, Hsiao-Rong Tyan,
Chia-Wen Lin, Duan-Yu Chen, and Kuo-Chin Fan. Motion
ﬂow-based video retrieval. TMM, 2007.

[45] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Mur-
phy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric
relation network. In ECCV, 2018.

[46] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In ICCV, 2015.

[47] Rene Visser, Nicu Sebe, and Erwin Bakker. Object recog-
In International Conference on

nition for video retrieval.
Image and Video Retrieval, 2002.

[48] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruc-

tion network for video captioning. In CVPR, 2018.

[49] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong
Xu. Bidirectional attentive fusion with context gating for
dense video captioning. In CVPR, 2018.

[50] Shuohang Wang and Jing Jiang. Machine comprehen-
sion using match-lstm and answer pointer. arXiv preprint
arXiv:1608.07905, 2016.

[51] Rong Yan, Alexander G Hauptmann, and Rong Jin. Negative
pseudo-relevance feedback in content-based video retrieval.
In MM, 2003.

[52] Hongtao Yang, Xuming He, and Fatih Porikli. One-shot ac-
tion localization by learning sequence matching network. In
CVPR, 2018.

1297

