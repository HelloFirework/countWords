Learning Correspondence from the Cycle-consistency of Time

Xiaolong Wang*

Carnegie Mellon University

Allan Jabri*
UC Berkeley

Alexei A. Efros

UC Berkeley

xiaolonw@cs.cmu.edu

ajabri@eecs.berkeley.edu

efros@eecs.berkeley.edu

Input	Masks	(Instance-level)

(a)

Instance	Masks	Propagation

Input	Pose

(b)

Pose	Propagation

Input	Masks	(Semantic-level)

(c)

Semantic	Masks	Propagation

ùêº"

(d)

Flow	I" ‚Üí I#

ùêº#

Warping	I" to	I#

Input	Texture

(e)		Texture	Propagation

Figure 1: We propose to learn a representation for visual correspondence from raw video. Without any Ô¨Åne-tuning, the acquired represen-
tation generalizes to various tasks involving visual correspondence, allowing for propagation of: (a) Multiple Instance Masks; (b) Pose; (c)
Semantic Masks; (d) Long-Range Optical Flow; (e) Texture.

Abstract

We introduce a self-supervised method for learning vi-
sual correspondence from unlabeled video. The main idea
is to use cycle-consistency in time as free supervisory signal
for learning visual representations from scratch. At training
time, our model learns a feature map representation to be
useful for performing cycle-consistent tracking. At test time,
we use the acquired representation to Ô¨Ånd nearest neighbors
across space and time. We demonstrate the generalizability
of the representation ‚Äì without Ô¨Ånetuning ‚Äì across a range
of visual correspondence tasks, including video object seg-
mentation, keypoint tracking, and optical Ô¨Çow. Our ap-
proach outperforms previous self-supervised methods and
performs competitively with strongly supervised methods.1

1. Motivation

It is an oft-told story that when a young graduate stu-
dent asked Takeo Kanade what are the three most impor-
tant problems in computer vision, Kanade replied: ‚ÄúCor-
respondence, correspondence, correspondence!‚Äù
Indeed,

*Equal contribution.
1Project page: http://ajabri.github.io/timecycle

most fundamental vision problems, from optical Ô¨Çow and
tracking to action recognition and 3D reconstruction, re-
quire some notion of visual correspondence. Correspon-
dence is the glue that links disparate visual percepts into
persistent entities and underlies visual reasoning in space
and time.

Learning representations for visual correspondence,
from pixel-wise to object-level, has been widely explored,
primarily with supervised learning approaches requiring
large amounts of labelled data. For learning low-level corre-
spondence, such as optical Ô¨Çow, synthetic computer graph-
ics data is often used as supervision [10, 22, 50, 62], lim-
iting generalization to real scenes. On the other hand, ap-
proaches for learning higher-level semantic correspondence
rely on human annotations [71, 19, 65], which becomes pro-
hibitively expensive at large scale. In this work, our aim
is to learn representations that support reasoning at various
levels of visual correspondence (Figure 1) from scratch and
without human supervision.

A fertile source of free supervision is video. Because
the world does not change abruptly, there is inherent vi-
sual correspondence between observations adjacent in time.
The problem is how to Ô¨Ånd these correspondences and turn

12566

t	- 2

t	- 1

t

Start

Learning

Signal

End

Figure 2: A Cycle in Time. Given a video, tracking along the sequence formed by a cycle in time can be self-supervised: the target is
simply the beginning of the cycle. The yellow arrow between the start and end represents the differentiable learning signal.

them into a learning signal.
In a largely static world ob-
served by a stationary camera, such as a webcam trained on
the Eiffel Tower, correspondence is straightforward because
nothing moves and capturing visual invariance (to weather,
lighting) amounts to supervised metric learning. In the dy-
namic world, however, change in appearance is confounded
by movement in space. Finding correspondence becomes
more difÔ¨Åcult because capturing visual invariance now re-
quires learning to track, but tracking relies on a model of
visual invariance. This paper proposes to learn to do both
simultaneously, in a self-supervised manner.

The key idea is that we can obtain unlimited supervi-
sion for correspondence by tracking backward and then for-
ward (i.e. along a cycle in time) and using the inconsis-
tency between the start and end points as the loss function
(Figure 2). We perform tracking by template-matching in a
learned deep feature space. To minimize the loss ‚Äì i.e. to
be cycle-consistent ‚Äì the model must learn a feature repre-
sentation that supports identifying correspondences across
frames. As these features improve, the ability to track im-
proves, inching the model toward cycle-consistency. Learn-
ing to chain correspondences in such a feature space should
thus yield a visual similarity metric tolerant of local trans-
formations in time, which can then be used at test-time as a
stand-alone distance metric for correspondence.

While conceptually simple,

implementing objectives
based on cycle-consistency can be challenging. Without
additional constraints, learning can take shortcuts, making
correspondences cycle-consistent but wrong [88].
In our
case, a track that never moves is inherently cycle-consistent.
We avoid this by forcing the tracker to re-localize the
next patch in each successive frame. Furthermore, cycle-
consistency may not be achievable due to sudden changes in
object pose or occlusions; skip-cycles can allow for cycle-
consistency by skipping frames, as in Figure 3 (right). Fi-
nally, correspondence may be poor early in training, and
shorter cycles may ease learning, as in Figure 3 (left). Thus,
we simultaneously learn from many kinds of cycles to in-
duce a natural curriculum and provide better training data.

The proposed formulation can be used with any differ-
entiable tracking operation, providing a general framework
for learning representations for visual correspondence from
raw video. Because the method does not rely on human an-
notation, it can learn from the near inÔ¨Ånite video data avail-
able online. We demonstrate the usefulness of the learned
features for tasks at various levels of visual correspondence,
ranging from pose, keypoint, and segmentation propagation
(of objects and parts) to optical Ô¨Çow.

2. Related Work

Temporal Continuity in Visual Learning. Temporal
structure serves as a useful signal for learning because the
visual world is continuous and smoothly-varying. Spatio-
temporal stability is thought to play a crucial role in the
development of invariant representations in biological vi-
sion [83, 36, 77, 78]. For example, Wood [77] showed
that for newborn chicks raised in a visual world that was
not temporally smooth, object recognition abilities were
severely impaired. Computational approaches for unsuper-
vised learning have sought to leverage this continuity, such
as continuous transformation learning [13, 70], ‚Äúslow‚Äù fea-
ture learning [76, 91, 25] and information maximization be-
tween neighbouring patches in time [66]. Our work can be
seen as slow feature learning with Ô¨Åxation, learned end-to-
end without supervision.

Self-supervised Representation Learning from Video.
Learning representations from video using time as super-
vision has been extensively studied, both as future pre-
diction task [15, 60, 44, 42] as well as motion estima-
tion [2, 25, 63, 38, 40]. Our approach is most related to the
methods of Wang et al. [73, 74] and Pathak et al. [47], which
use off-the-shelf tools for tracking and optical Ô¨Çow respec-
tively, to provide supervisory signal for training. However,
representations learned in this way are inherently limited by
the power of these off-the-shelf tools as well as their failure
modes. We address this issue by learning the representation
and the tracker jointly, and Ô¨Ånd the two learning problems

2567

t ‚àík

t

t ‚àík

t

T (‚àí1) Back Track

T (1) Forward Track

Failed Track

Figure 3: Multiple Cycles and Skip Cycles. Cycle-consistency may not be achievable due to sudden changes in object pose or occlusions.
Our solution is to optimize multiple cycles of different lengths simultaneously. This allows learning from shorter cycles when the full cycle
is too difÔ¨Åcult (left). This also allows cycles that skip frames, which can deal with momentary occlusions (right).

to be complementary. Our work is also inspired by the inno-
vative approach of Vondrick et al [69] where video coloriza-
tion is used as a pretext self-supervised task for learning to
track. While the idea is very intriguing, in Section 4 we Ô¨Ånd
that colorization is a weaker source of supervision for cor-
respondence than cycle-consistency, potentially due to the
abundance of constant-color regions in natural scenes.

Tracking. Classic approaches to tracking treat it as a
matching problem, where the goal is to Ô¨Ånd a given ob-
ject/patch in the next frame (see [11] for overview), and the
key challenge is to track reliably over extended time pe-
riods [57, 79, 1, 27]. Starting with the seminal work of
Ramanan et al. [49], researchers largely turned to ‚Äútrack-
ing as repeated recognition‚Äù, where trained object detectors
are applied to each frame independently [3, 28, 80, 71, 19,
35, 65]. Our work harks back to the classic tracking-by-
matching methods in treating it as a correspondence prob-
lem, but uses learning to obtain a robust representation that
is able to model wide range of appearance changes.

Optical Flow. Correspondence at the pixel level ‚Äì map-
ping where each pixel goes in the next frame ‚Äì is the
optical Ô¨Çow estimation problem. Since the energy mini-
mization framework of Horn and Schunck [20] and coarse-
to-Ô¨Åne image warping by Lucas and Kanade [41], much
progress has been made in optical Ô¨Çow estimation [46, 6,
61, 10, 22, 50, 62]. However, these methods still struggle
to scale to long-range correspondence in dynamic scenes
with partial observability. These issues have driven re-
searchers to study methods for estimating long-range op-
tical Ô¨Çow [56, 5, 55, 51, 52, 34]. For example, Brox and
Malik [5] introduced a descriptor that matches region hier-
archies and provides dense and subpixel-level estimation of
Ô¨Çow. Our work can be viewed as enabling mid-level optical
Ô¨Çow estimation.

Mid-level Correspondence. Given our focus on Ô¨Ånding
correspondence at the patch level, our method is also re-
lated to the classic SIFT Flow [39] algorithm and other
methods for Ô¨Ånding mid-level correspondences between re-

gions across different scenes [30, 16, 87]. More recently,
researchers have studied modeling correspondence in deep
feature space [64, 33, 31, 17, 53, 54]. In particular, our work
draws from Rocco et al. [53, 54], who propose a differen-
tiable soft inlier score for evaluating quality of alignment
between spatial features and provides a loss for learning
semantic correspondences. Most of these methods rely on
learning from simulated or large-scale labeled datasets such
as ImageNet, or smaller custom human-annotated data with
narrow scope. We address the challenge of learning repre-
sentations of correspondence without human annotations.

Forward-Backward and Cycle Consistency. Our work
is inÔ¨Çuenced by the classic idea of forward-backward con-
sistency in tracking [57, 79, 1, 27], which has long been
used as an evaluation metric for tracking [27] as well as a
measure of uncertainty [1]. Recent work on optical Ô¨Çow es-
timation [43, 24, 68, 75, 45] also utilizes forward-backward
consistency as an optimization goal. For example, Meister
et al. [45] combines one-step forward and backward con-
sistency check with pixel reconstruction loss for learning
optical Ô¨Çows. Compared to pixel reconstruction, model-
ing correspondence in feature space allows us to follow and
learn from longer cycles. Forward-backward consistency is
a speciÔ¨Åc case of cycle-consistency, which has been widely
applied as a learning objective for 3D shape matching [21],
image alignment [87, 89, 88], depth estimation [86, 14, 84],
and image-to-image translation [90, 4]. For example Zhou
et al. [88] used 3D CAD models to render two synthetic
views for pairs of training images and construct a corre-
spondence Ô¨Çow 4-cycle. To the best of our knowledge, our
work is the Ô¨Årst to employ cycle-consistency across multi-
ple steps in time.

3. Approach

An overview of the training procedure is presented in
Figure 4a. The goal is to learn a feature space œÜ by tracking
a patch pt extracted from image It backwards and then for-
wards in time, while minimizing the cycle-consistency loss
lŒ∏ (yellow arrow). Learning œÜ relies on a simple tracking

2568

!

T
T

T
T

!

!

T
T

T
T

!

!

It‚àí2

It‚àí1

It
I

!
!

!

80 x 80

pt

240 x 240

It‚àí1

f (¬∑)

g(¬∑)

h(¬∑)

xp
t : c x 10 x 10

At-1,t

xp
t-1: c x 10 x 10

Œ∏t-1,t

Estimate
Transform

Bilinear
Sampler

reshape

900 x
10 x 10

transpose

xI

t-1: c x 30 x 30

T

(a) Training œÜ by End-to-end Cycle-consistent Tracking

(b) Differentiable Tracking Operation T

Figure 4: Method Overview. (a) During training, the model learns a feature space encoded by œÜ to perform tracking using tracker T .
By tracking backward and then forward, we can use cycle-consistency to supervise learning of œÜ. Note that only the initial patch pt is
explicitly encoded by œÜ; other patch features along the cycle are obtained by localizing image features. (b) We show one step of tracking
back in time from t to t ‚àí 1. Given input image features xI
t‚àí1. This
operation is performed iteratively to track along the cycle in (a).

t‚àí1 and query patch features xp

t , T localizes the patch xp

t‚àí1 in xI

operation T , which takes as inputs the features of a current
patch and a target image, and returns the image feature re-
gion with maximum similarity. Our implementation of T is
shown in Figure 4b: without information of where the patch
came from, T must match features encoded by œÜ to localize
the next patch. As shown in Figure 4a, T can be iteratively
applied backwards and then forwards through time to track
along an arbitrarily long cycle. The cycle-consistency loss
lŒ∏ is the euclidean distance between the spatial coordinates
of initial patch pt and the patch found at the end of the cycle
in It. In order to minimize lŒ∏, the model must learn a feature
space œÜ that allows for robustly measuring visual similarity
between patches along the cycle.

Note that T is only used in training and is deliberately
designed to be weak, so as to place the burden of repre-
sentation on œÜ. At test time, the learned œÜ is used directly
for computing correspondences. In the following, we Ô¨Årst
formalize cycle-consistent tracking loss functions and then
describe our architecture for mid-level correspondence.

3.1. Cycle Consistency Losses

We describe a formulation of cycle-consistent tracking
and use it to succinctly express loss functions based on tem-
poral cycle-consistency.

3.1.1 Recurrent Tracking Formulation

Consider as inputs a sequence of video frames It‚àík:t and a
patch pt taken from It. These pixel inputs are mapped to a
feature space by an encoder œÜ, such that xI
t‚àík:t = œÜ(It‚àík:t)
and xp

t = œÜ(pt).

Let T be a differentiable operation xI
s

s, where
s and t represent time steps. The role of T is to localize the
patch features xp
s that are most similar
to xp
t . We can apply T iteratively in a forward manner i
times from t ‚àí i to t ‚àí 1:

s in image features xI

t 7‚Üí xp

√ó xp

T (i)(xI

t‚àíi, xp) = T (xI

t‚àí1, T (xI

t‚àí2, ...T (xI

t‚àíi, xp)))

By convention, the tracker T can be applied backwards i
times from time t ‚àí 1 to t ‚àí i:

T (‚àíi)(xI

t‚àí1, xp) = T (xI

t‚àíi, T (xI

t‚àíi+1, ...T (xI

t‚àí1, xp)))

3.1.2 Learning Objectives

The following learning objectives rely on a measure of
agreement lŒ∏(xp
t ) between the initial patch and re-
localized patch (deÔ¨Åned in Section 3.2).

t , ÀÜxp

Tracking: The cycle-consistent loss Li

long is deÔ¨Åned as

Li

long = lŒ∏(xp

t , T (i)(xI

t‚àíi+1, T (‚àíi)(xI

t‚àí1, xp

t ))).

The tracker attempts to follow features backward and then
forward i steps in time to re-arrive to the initial query, as
depicted in Figure 4a.

Skip Cycle: In addition to cycles through consecutive
frames, we also allow skipping through time. We deÔ¨Åne the
loss on a two-step skip-cycle as Li

skip:

Li

skip = lŒ∏(xp

t , T (xI

t , T (xI

t‚àíi, xp

t ))).

This attempts longer-range matching by skipping to the
frame i steps away.

t and localized patch T (xI

Feature Similarity: We explicitly require the query
patch xp
t ) to be similar in
feature space. This loss amounts to the negative Frobenius
inner product between spatial feature tensors:

t‚àíi, xp

Li

sim = ‚àíhxp

t , T (xI

t‚àíi, xp

t )i

In principle, this loss can further be formulated as the inlier
loss from [54]. The overall learning objective sums over the
k possible cycles, with weight Œª = 0.1:

Li

sim + ŒªLi

skip + ŒªLi

long.

L =

k

X

i=1

2569

3.2. Architecture for Mid level Correspondence

The learning objective thus described can be used to train
arbitrary differentiable tracking models. In practice, the ar-
chitecture of the encoder determines the type of correspon-
dence captured by the acquired representation. In this work,
we are interested in a model for mid-level temporal corre-
spondence. Accordingly, we choose the representation to
be a mid-level deep feature map, coarser than pixel space
but with sufÔ¨Åcient spatial resolution to support tasks that
require localization. An overview is provided in Figure 4b.

3.2.1 Spatial Feature Encoder œÜ

We compute spatial features with a ResNet-50 architec-
ture [18] without res5 (the Ô¨Ånal 3 residual blocks). We re-
duce the spatial stride of res4 for larger spatial outputs. In-
put frames are 240 √ó 240 pixels, randomly cropped from
video frames re-scaled to have min(H, W ) = 256. The
size of the spatial feature of the frame is thus 30 √ó 30. Im-
age patches are 80 √ó 80, randomly cropped from the full
240 √ó 240 frame, so that the feature is 10 √ó 10. We per-
form l2 normalization on the channel dimension of spatial
features to facilitate computing cosine similarity.

3.2.2 Differentiable Tracker T

Given the representation from the encoder, we perform
tracking with T . As illustrated in Figure 4b, the differen-
tiable tracker is composed of three main components.

AfÔ¨Ånity function f
provides a measure of similarity be-
tween coordinates of spatial features xI and xp. We de-
note the afÔ¨Ånity function as f (xI , xp) := A, such that
f : Rc√ó30√ó30 √ó Rc√ó10√ó10 ‚àí‚Üí R900√ó100.

A generic choice for computing the afÔ¨Ånity is the dot
product between embeddings, referred to in recent litera-
ture as attention [67, 72] and more historically known as
normalized cross-correlation [10, 35]. With spatial grid j in
feature xI as xI (j) and the grid i in xp as xp(i),

A(j, i) =

exp (xI (j)‚ä∫xp(i))

Pj exp (xI (j)‚ä∫xp(i))

(1)

where the similarity A(j, i) is normalized by the softmax
over the spatial dimension of xI , for each xp(i). Note that
the afÔ¨Ånity function is deÔ¨Åned for any feature dimension.

Localizer g
takes afÔ¨Ånity matrix A as input and estimates
localization parameters Œ∏ corresponding to the patch in fea-
ture xI which best matches xp. g is composed of two convo-
lutional layers and one linear layer. We restrict g to output
3 parameters for the bilinear sampling grid (i.e. simpler
than [23]), corresponding to 2D translation and rotation:
g(A) := Œ∏, where g : R900√ó100 ‚àí‚Üí R3. The expressive-
ness of g is intentionally limited so as to place the burden
of representation on the encoder (see Appendix B).

Bilinear Sampler h uses the image feature xI and Œ∏ pre-
dicted by g to perform bilinear sampling to produce a new
patch feature h(xI , Œ∏) which is in the same size as xp, such
that h : Rc√ó30√ó30 √ó R3 ‚àí‚Üí Rc√ó10√ó10.

3.2.3 End-to-end Joint Training

The composition of encoder œÜ and T forms a differentiable
patch tracker, allowing for end-to-end training of œÜ and T :

xI , xp = œÜ(I), œÜ(p)

T (xI , xp) = h(xI , g(f (xI , xp)).

long and Li

Alignment Objective lŒ∏
is applied in the cycle-consistent
losses Li
skip, measuring the error in alignment
between two patches. We follow the formulation introduced
by [53]. Let M (Œ∏xp ) correspond to the bilinear sampling
grids used to form a patch feature xp from image feature
xI . Assuming M (Œ∏xp ) contains n sampling coordinates,
the alignment objective is deÔ¨Åned as:

lŒ∏(xp

‚àó, ÀÜxp

t ) =

1
n

n

X

i=1

||M (Œ∏xp

‚àó )i ‚àí M (Œ∏ÀÜxp

t

)i||2
2

4. Experiments

We report experimental results for a model trained on
training on other
the VLOG dataset [12] from scratch;
large video datasets such as Kinetics gives similar results
(see Appendix A.3). The trained representation is evalu-
ated without Ô¨Åne-tuning on several challenging video prop-
agation tasks: DAVIS-2017 [48], JHMDB [26] and Video
Instance-level Parsing (VIP) [85]. Through various experi-
ments, we show that the acquired representation generalizes
to a range of visual correspondence tasks (see Figure 5).

4.1. Common Setup and Baselines

Training. We train the model on the VLOG dataset [12]
without using any annotations or pre-training. The VLOG
dataset contains 114K videos and the total length of the
videos is 344 hours. During training, we set the number of
past frames as k = 4. We train on a 4-GPU machine with a
mini-batch size of 32 clips (8 clips per GPU), for 30 epochs.
The model is optimized with Adam [32] with a learning rate
of 0.0002 and momentum term Œ≤1 = 0.5, Œ≤2 = 0.999.

Inference. At test time, we use the trained encoder‚Äôs
representation to compute dense correspondences for video
propagation. Given initial labels of the Ô¨Årst frame, we prop-
agate the labels to the rest of the frames in the video. La-
bels are given by speciÔ¨Åed targets for the Ô¨Årst frame of
each task, with instance segmentation masks for DAVIS-
2017 [48], human pose keypoints JHMDB [26], and both
instance-level and semantic-level masks for VIP [85]. The
labels of each pixel are discretized to C classes. For seg-
mentation masks, C is the number of instance or semantic
labels. For keypoints, C is the number of keypoints. We

2570

Input

Outputs

Input

Outputs

(a)

Instance 

Masks

(b) 
Pose

(c) 

Semantic

Masks

(d) 

Texture

Figure 5: Visualizations of our propagation results. Given the labels as input in the Ô¨Årst frame, our feature can propagate them to the rest
of frames, without further Ô¨Åne-tuning. The labels include (a) instance masks in DAVIS-2017 [48], (b) pose keypoints in JHMDB [26], (c)
semantic masks in VIP [85] and even (d) texture map.

include a background class. We propagate the labels in the
feature space. The labels in the Ô¨Årst frame are one-hot vec-
tors, while propagated labels are soft distributions.

Propagation by k-NN. Given a frame It and a frame
It‚àí1 with labels, we compute their afÔ¨Ånity in feature space:
At‚àí1,t = f (œÜ(It‚àí1), œÜ(It)) (Eq. 1). We compute label yi
of pixel i in It as

yi = X

At‚àí1,t(j, i)yj,

j

(2)

where At‚àí1,t(j, i) is the afÔ¨Ånity between pixels i in It and j
in It‚àí1. We propagate from the top-5 pixels with the great-
est afÔ¨Ånity At‚àí1,t(j, i) for each pixel i. Labels are propa-
gated from It‚àí1:t‚àíK , as well as I1, and averaged. Finally,
we up-sample the label maps to image size. For segmenta-
tion, we use the argmax of the class distribution of each
pixel. For keypoints, we choose the pixel with the maxi-
mum score for each keypoint type.

Baselines. We compare with the following baselines:
‚Ä¢ Identity: Always copy the Ô¨Årst frame labels.
‚Ä¢ Optical Flow (FlowNet2 [22]): A state-of-the-art
method for predicting optical Ô¨Çow with neural networks
[22]. We adopt the open-source implementation which is
trained with synthetic data in a supervised manner. For a
target frame It, we compute the optical Ô¨Çow from frame
It‚àí1 to It and warp the labels in It‚àí1 to It.

‚Ä¢ SIFT Flow [39]: For a target frame It, we compute the
SIFT Flow between It and its previous frames. We prop-
agate the labels in K frames before It and the Ô¨Årst frame
via SIFT Flow warping. The propagation results are av-
eraged to compute the labels for It.

‚Ä¢ Transitive Invariance [74]: A self-supervised approach
that combines multiple objectives: (i) visual tracking on
raw video [73] and (ii) spatial context reasoning [9]. We
use the open-sourced pre-trained VGG-16 [58] model
and adopt our proposed inference procedure.

‚Ä¢ DeepCluster [8]: A self-supervised approach which uses
a K-means objective to iteratively update targets and
learn a mapping from images to targets. It is trained on
the ImageNet dataset without using annotations. We ap-
ply the trained model with VGG-16 and adopt the same
inference procedure as our method.

‚Ä¢ Video Colorization [69]: A self-supervised approach for
label propagation. Trained on the Kinetics [29] dataset, it
uses color propagation as self-supervision. The architec-
ture is based on 3D ResNet-18. We report their results.

‚Ä¢ ImageNet Pre-training [18]: The conventional setup for

supervised training of ResNet-50 on ImageNet.

‚Ä¢ Fully-Supervised Methods: We report fully-supervised
methods for reference, which not only use ImageNet pre-
training but also Ô¨Åne-tuning on the target dataset. Note
that these methods do not always follow the inference
procedure used with method, and labels of the Ô¨Årst frame
are not used for JHMDB and VIP at test time.

4.2. Instance Propagation on DAVIS 2017

We apply our model to video object segmentation on the
DAVIS-2017 validation set [48]. Given the initial masks of
the Ô¨Årst frame, we propagate the masks to the rest of the
frames. Note that there can be multiple instances in the Ô¨Årst
frame. We follow the standard metrics including the region

2571

model

Supervised

J (Mean) F (Mean)

model

Supervised

PCK@.1 PCK@.2

Identity
Random Weights (ResNet-50)
Optical Flow (FlowNet2) [22]
SIFT Flow [39]
Transitive Inv. [74]
DeepCluster [8]
Video Colorization [69]
Ours (ResNet-18)
Ours (ResNet-50)
ImageNet (ResNet-50) [18]
Fully Supervised [81, 7]

X

X

22.1
12.4
26.7
33.0
32.0
37.5
34.6
40.1
41.9
50.3
55.1

23.6
12.5
25.2
35.0
26.8
33.2
32.7
38.3
39.4
49.0
62.1

Table 1: Evaluation on instance mask propagation on DAVIS-
2017 [48]. We follow the standard metric on region similarity J
and contour-based accuracy F .

similarity J (IoU) and the contour-based accuracy F . We
set K = 7, the number of reference frames in the past.

We show comparisons in Table 1. Comparing to the re-
cent Video Colorization approach [69], our method is 7.3%
in J and 6.7% in F . Note that although we are only 4.4%
better than the DeepCluster baseline in J , we are better in
contour accuracy F by 6.2%. Thus, DeepCluster does not
capture dense correspondence on the boundary as well.

For fair comparisons, we also implemented our method
with a ResNet-18 encoder, which has less parameters com-
pared to the VGG-16 in [74, 8] and the 3D convolutional
ResNet-18 in [69]. We observe that results are only around
2% worse than our model with ResNet-50, which is still
better than the baselines.

While the ImageNet pre-trained network performs better
than our method on this task, we argue it is easy for the Ima-
geNet pre-trained network to recognize objects under large
variation as it beneÔ¨Åts from curated object-centric annota-
tion. Though our model is only trained on indoor scenes
without labels, it generalizes to outdoor scenes.

Although video segmentation is an important applica-
tion, it does not necessarily show that the representation
captures dense correspondence.

4.3. Pose Keypoint Propagation on JHMDB

To see whether our method is learning more spatially
precise correspondence, we apply our model on the task
of keypoint propagation on the split 1 validation set of JH-
MDB [26]. Given the Ô¨Årst frame with 15 labeled human
keypoints, we propagate them through time. We follow the
evaluation of the standard PCK metric [82], which measures
the percentage of keypoints close to the ground truth in dif-
ferent thresholds of distance. We set the number of refer-
ence frames same as experiments in DAVIS-2017.

As shown in Table 2, our method outperforms all self-
supervised baselines by a large margin. We observe
that SIFT Flow actually performs better than other self-
supervised learning methods in PCK@.1. Our method out-
performs SIFT Flow by 8.7% in PCK@.1 and 9.9% in

Identity
Optical Flow (FlowNet2) [22]
SIFT Flow [39]
Transitive Inv. [74]
DeepCluster [8]
Video Colorization [69]
Ours (ResNet-18)
Ours (ResNet-50)
ImageNet (ResNet-50) [18]
Fully Supervised [59]

X

X

43.1
45.2
49.0
43.9
43.2
45.2
57.3
57.7
58.4
68.7

64.5
62.9
68.6
67.0
66.9
69.6
78.1
78.5
78.4
92.1

Table 2: Evaluation on pose propagation on JHMDB [26]. We
report the PCK in different thresholds.

PCK@.2. Notably, our approach is only 0.7% worse than
ImageNet pre-trained features in PCK@.1 and performs
better in PCK@.2.

4.4. Semantic and Instance Propagation on VIP

We apply our approach on the Video Instance-level Pars-
ing (VIP) dataset [85], which is densely labeled with seman-
tic masks for different human parts (e.g., hair, right arm, left
arm, coat). It also has instance labels that differentiate hu-
mans. Most interestingly, the duration of a video ranges
from 10 seconds to 120 seconds in the dataset, which is
much longer than aforementioned datasets.

We test our method on the validation set of two tasks
in this dataset: (i) The Ô¨Årst task is to propagate the seman-
tic human part labels from the Ô¨Årst frame to the rest of the
video, and evaluate with the mean IoU metric; (ii) In the
second task, the labels in the Ô¨Årst frame are given with not
only the semantic labels but also the instance identity. Thus,
the model must differentiate the different arms of different
human instances. We use the standard instance-level human
parsing metric [37], mean Average Precision, for overlap
thresholds varying from 0.1 to 0.9. Since part segments are
relatively small (compared to objects in DAVIS-2017), we
increase the input image size to 560√ó560 for inference, and
use two reference frames, including the Ô¨Årst frame.

Semantic Propagation. As shown with the mIoU met-
ric in Table 3, our method again exceeds all self-supervised
baselines by a large margin (a [69] model is currently not
available). ImageNet pre-trained models have the advantage
of semantic annotation and thus do not necessarily have to
perform tracking. As shown in Figure 5(c), our method is
able to handle occlusions and multiple instances.

Part Instance Propagation. This task is more challeng-
ing. We show the results in mean AP r
vol in Table 3. Our
method performs close to the level of ImageNet pre-trained
features. We show different radial thresholds for average
precision (AP r
vol) in Table 4. ImageNet pre-trained features
performs better under smaller thresholds and worse under
larger thresholds, suggesting that it has an advantage in Ô¨Ånd-
ing coarse correspondence while our method is more capa-
ble of spatial precision.

2572

model

Supervised

mIoU

Identity
Optical Flow (FlowNet2) [22]
SIFT Flow [39]
Transitive Inv. [74]
DeepCluster [8]
Ours (ResNet-50)
ImageNet (ResNet-50) [18]
Fully Supervised [85]

X

X

13.6
16.1
21.3
19.4
21.8
28.9
34.7
37.9

AP r
vol

4.0
8.3
10.5
5.0
8.1
15.6
16.1
24.1

Table 3: Evaluation on propagating human part labels in Video
Instance-level Parsing (VIP) dataset [85]. We measure Semantic
Propagation with mIoU and Part Instance Propagation in AP r
vol.

model

Ours (ResNet-50)
ImageNet (ResNet-50) [18]

AP r
vol

15.6
16.1

IoU threshold

0.3

23.0
24.2

0.5

12.7
11.9

0.7

5.4
4.8

Table 4: A more detailed analysis of different thresholds for Part
Instance Propagation on the VIP dataset [85].

4.5. Texture Propagation

The acquired representation allows for propagation of
not only instance and semantic labels, but also textures. We
visualize texture propagation in Figure 5 (d); these videos
are samples from DAVIS-2017 [48]. We ‚Äúpaint‚Äù a texture of
6 colored stripes on an the object in the Ô¨Årst frame and prop-
agate it to the rest of the frames using our representation.
We observe that the structure of the texture is well preserved
in the following frames, demonstrating that the represen-
tation allows for Ô¨Ånding precise correspondence smoothly
though time. See the project page for video examples.

4.6. Video Frame Reconstructions

Though we do not optimize for pixel-level objectives at
training time, we can evaluate how well our method per-
forms on pixel-level reconstruction. SpeciÔ¨Åcally, given two
images Is and It distant in time in a video, we compute
coordinate-wise correspondences under the acquired repre-
sentation and generate a Ô¨Çow Ô¨Åeld for pixel movement be-
tween Is and It. We then upsample the Ô¨Çow Ô¨Åeld to the
same size as the image and warp it on Image Is to generate
a new image I ‚Ä≤
t (as shown in Figure 6). We compare the
L1 distance between I ‚Ä≤
t and It in RGB space and report the
reconstruction errors in Table 5.

For fair comparison, we perform this experiment on the
DAVIS-2017 validation set, which none of the reported
methods have seen. We experiment with two time gaps,
5 and 10 frames. For the smaller gap, FlowNet2 [22] per-
forms reasonably well, whereas reconstruction degrades for
larger gaps. In both cases, our method performs better than
FlowNet2 and the ImageNet pre-trained network. This is
encouraging: our method is not trained with pixel-level
losses, yet out-performs methods trained with pixel-level
tasks and human supervision.

!"

Flow from !" to !#

!#

Warping !" to !#

Figure 6: Given I1, I6 which have 5-frame gap, we compute the
long-range Ô¨Çows between them with our representation. This Ô¨Çow
can be used to warp I1 to generate image similar to I6.

model

Identity
Optical Flow (FlowNet2) [22]
ImageNet (ResNet-50) [18]
Ours (ResNet-50)

5-F

82.0
62.4
64.0
60.4

10-F

97.7
90.3
79.2
76.4

Table 5: We compute the long-range Ô¨Çow on two frames and warp
the Ô¨Årst one with the Ô¨Çow. We compare the warped frame with the
second frame in L1 distance. The gaps are 5 or 10 frames.

5. Limitations and Future Work

While in principle our method should keep improving
with more data, in practice, learning seems to plateau after
a moderate amount of training (i.e. 30 epochs). An impor-
tant next step is thus how to better scale to larger, noisier
data. A crucial component is improving robustness to oc-
clusions and partial observability, for instance, by using a
better search strategy for Ô¨Ånding cycles at training time. An-
other issue is deciding what to track at training time. Pick-
ing patches at random can result in issues such as station-
ary background patches and tracking ambiguity ‚Äì e.g. how
should one track a patch containing two objects that even-
tually diverge? Jointly learning what to track may also give
rise to unsupervised object detection. Finally, incorporat-
ing more context for tracking both at training and test time
may be important for learning more expressive models of
spatial-temporal correspondence.

We hope this work is a step toward learning from the
abundance of visual correspondence inherent in raw video
in a scalable and end-to-end manner. While our experiments
show promising results at certain levels of correspondence,
much work remains to cover the full spectrum.

Acknowledgements: We thank members of the BAIR
community for helpful discussions and feedback, and Sasha
Sax and Michael Janner for draft comments. AJ is sup-
ported by the P.D. Soros Fellowship. XW is supported by
the Facebook PhD Fellowship. This work was also sup-
ported, in part, by NSF grant IIS-1633310 and Berkeley
DeepDrive.

2573

References

[1] Recurrent Tracking using Multifold Consistency. CVPR

Workshop on PETS, 2007. 3

[2] P. Agrawal, J. Carreira, and J. Malik. Learning to see by

moving. In ICCV, 2015. 2

[3] M. Andriluka, S. Roth, and B. Schiele. People-tracking-by-
detection and people-detection-by-tracking. In CVPR, 2008.
3

[4] A. Bansal, S. Ma, D. Ramanan, and Y. Sheikh. Recycle-gan:

Unsupervised video retargeting. In ECCV, 2018. 3

[5] T. Brox, C. Bregler, and J. Malik. Large displacement optical

Ô¨Çow. In CVPR, 2009. 3

[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical Ô¨Çow estimation based on a theory for warping.
In ECCV, 2004. 3

[7] S. Caelles, K. Maninis, J. Pont-Tuset, L. Leal-Taix¬¥e, D. Cre-
mers, and L. Van Gool. One-shot video object segmentation.
In CVPR, 2017. 7

[8] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep
In

clustering for unsupervised learning of visual features.
ECCV, 2018. 6, 7, 8

[9] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In ICCV,
2015. 6

[10] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¬®ausser, C. Hazƒ±rbas¬∏,
V. Golkov, P. Van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical Ô¨Çow with convolutional networks.
arXiv, 2015. 1, 3, 5

[11] D. A. Forsyth and J. Ponce. Computer Vision - A Modern

Approach, Second Edition. Pitman, 2012. 3

[12] D. F. Fouhey, W. Kuo, A. A. Efros, and J. Malik. From

lifestyle vlogs to everyday interactions. In CVPR, 2018. 5

[13] P. Fldik.

Learning Invariance from Transformation Se-
quences. Neural Computation, 3(2):194‚Äì200, June 1991. 2
[14] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsuper-
vised monocular depth estimation with left-right consistency.
2017. 3

[15] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun.
Unsupervised learning of spatiotemporally coherent metrics.
ICCV, 2015. 2

[16] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal Ô¨Çow. In

CVPR, 2016. 3

[17] K. Han, R. S. Rezende, B. Ham, K.-Y. K. Wong, M. Cho,
C. Schmid, and J. Ponce. Scnet: Learning semantic corre-
spondence. ICCV, 2017. 3

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Computer Vision and Pattern
Recognition (CVPR), 2016. 5, 6, 7, 8

[19] D. Held, S. Thrun, and S. Savarese. Learning to track at 100

fps with deep regression networks. In ECCV, 2016. 1, 3

[20] B. K. Horn and B. G. Schunck. Determining optical Ô¨Çow.

ArtiÔ¨Åcial intelligence, 1981. 3

[21] Q.-X. Huang and L. Guibas.
Consistent shape maps
via semideÔ¨Ånite programming.
In Proceedings of
the
Eleventh Eurographics/ACMSIGGRAPH Symposium on Ge-
ometry Processing, 2013. 3

[22] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical Ô¨Çow estimation
with deep networks. In CVPR, 2017. 1, 3, 6, 7, 8

[23] M.

Jaderberg, K. Simonyan, A. Zisserman,

and
K. Kavukcuoglu. Spatial transformer networks. CoRR,
abs/1506.02025, 2015. 5

[24] J. Janai, F. G¬®uney, A. Ranjan, M. Black, and A. Geiger. Un-
supervised learning of multi-frame optical Ô¨Çow with occlu-
sions. In ECCV, 2018. 3

[25] D. Jayaraman and K. Grauman. Learning image representa-

tions tied to egomotion. In ICCV, 2015. 2

[26] H. Jhuang, J. Gall, S. ZufÔ¨Å, C. Schmid, and M. J. Black.
Towards understanding action recognition. In ICCV, 2013.
5, 6, 7

[27] Z. Kalal, K. Mikolajczyk, and J. Matas. Forward-backward
In ICPR,

error: Automatic detection of tracking failures.
2010. 3

[28] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-

detection. TPAMI, 2012. 3

[29] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vi-
jayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al.
The kinetics human action video dataset. arXiv:1705.06950,
2017. 6

[30] J. Kim, C. Liu, F. Sha, and K. Grauman. Deformable spatial
pyramid matching for fast dense correspondences. In CVPR,
2013. 3

[31] S. Kim, D. Min, B. Ham, S. Jeon, S. Lin, and K. Sohn. Fcss:
Fully convolutional self-similarity for dense semantic corre-
spondence. In CVPR, 2017. 3

[32] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv, 2014. 5

[33] D. Larlus and A. Vedaldi. Anchornet: A weakly supervised
network to learn geometry-sensitive features for semantic
matching. In CVPR, 2017. 3

[34] J. Lezama, K. Alahari, J. Sivic, and I. Laptev. Track to the
future: Spatio-temporal video segmentation with long-range
motion cues. In CVPR, 2011. 3

[35] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu. High performance
In

visual tracking with siamese region proposal network.
CVPR, 2018. 3, 5

[36] N. Li and J. J. DiCarlo. Unsupervised natural experience
rapidly alters invariant object representation in visual cortex.
Science (New York, N.Y.), 321(5895):1502‚Äì1507, Sept. 2008.
2

[37] Q. Li, A. Arnab, and P. H. Torr. Holistic, instance-level hu-

man parsing. arXiv, 2017. 7

[38] Y. Li, M. Paluri, J. M. Rehg, and P. Doll¬¥ar. Unsupervised

learning of edges. In CVPR, 2016. 2

[39] C. Liu, J. Yuen, and A. Torralba. Sift Ô¨Çow: Dense correspon-
dence across scenes and its applications. TPAMI, 2011. 3, 6,
7, 8

[40] S. Liu, G. Zhong, S. De Mello, J. Gu, M.-H. Yang, and
J. Kautz. Switchable temporal propagation network. ECCV,
2018. 2

[41] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. IJCAI, 1981.
3

2574

[42] Z. Luo, B. Peng, D.-A. Huang, A. Alahi, and L. Fei-Fei.
Unsupervised learning of long-term motion dynamics for
videos. 2017. 2

[62] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. Pwc-net: Cnns
for optical Ô¨Çow using pyramid, warping, and cost volume. In
CVPR, 2018. 1, 3

[43] D. Mahajan, F.-C. Huang, W. Matusik, R. Ramamoorthi,
and P. Belhumeur. Moving gradients: A path-based method
for plausible image interpolation.
In Proceedings of SIG-
GRAPH, ACM Transactions on Graphics, 2009. 3

[44] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale

video prediction beyond mean square error. arXiv, 2015. 2

[45] S. Meister, J. Hur, and S. Roth. UnÔ¨Çow: Unsupervised learn-
ing of optical Ô¨Çow with a bidirectional census loss. AAAI,
2018. 3

[46] E. M¬¥emin and P. P¬¥erez. Dense estimation and object-based
segmentation of the optical Ô¨Çow with robust techniques.
IEEE Transactions on Image Processing, 1998. 3

[47] D. Pathak, R. Girshick, P. Doll¬¥ar, T. Darrell, and B. Hariha-
ran. Learning features by watching objects move. In CVPR,
2017. 2

[48] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel¬¥aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 5, 6, 7,
8

[49] D. Ramanan, D. A. Forsyth, and A. Zisserman. Strike a pose:
Tracking people by Ô¨Ånding stylized poses. In CVPR, 2005.
3

[50] A. Ranjan and M. Black. Optical Ô¨Çow estimation using a

spatial pyramid network. In CVPR, 2017. 1, 3

[51] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicÔ¨Çow: Edge-preserving interpolation of correspondences
for optical Ô¨Çow. In CVPR, 2015. 3

[52] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
Deepmatching: Hierarchical deformable dense matching.
IJCV, 2016. 3

[53] I. Rocco, R. Arandjelovi¬¥c, and J. Sivic. Convolutional neu-
ral network architecture for geometric matching. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 3, 5

[54] I. Rocco, R. Arandjelovic, and J. Sivic. End-to-end weakly-

supervised semantic alignment. In CVPR, 2018. 3, 4

[55] M. Rubinstein, C. Liu, and W. T. Freeman. Towards longer

long-range motion trajectories. 2012. 3

[56] P. Sand and S. Teller. Particle video: Long-range motion

estimation using point trajectories. ICCV, 2008. 3

[57] I. K. Sethi and R. Jain. Finding Trajectories of Feature Points
in a Monocular Image Sequence. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, PAMI-9(1):56‚Äì73,
Jan. 1987. 3

[58] K. Simonyan and A. Zisserman. Very deep convolutional

networks for large-scale image recognition. arXiv, 2014. 6

[59] J. Song, L. Wang, L. Van Gool, and O. Hilliges. Thin-slicing
network: A deep structured model for pose estimation in
videos. In CVPR, 2017. 7

[60] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsuper-
vised learning of video representations using LSTMs. arXiv,
2015. 2

[63] H.-Y. Tung, H.-W. Tung, E. Yumer, and K. Fragkiadaki. Self-

supervised learning of motion capture. In NIPS, 2017. 2

[64] N. Ufer and B. Ommer. Deep semantic feature matching. In

CVPR, 2017. 3

[65] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and
P. H. Torr. End-to-end representation learning for correla-
tion Ô¨Ålter based tracking. In CVPR, 2017. 1, 3

[66] A. van den Oord, Y. Li, and O. Vinyals. Representa-
tion learning with contrastive predictive coding. CoRR,
abs/1807.03748, 2018. 2

[67] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all
you need. In Neural Information Processing Systems (NIPS),
2017. 5

[68] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar,
and K. Fragkiadaki. Sfm-net: Learning of structure and mo-
tion from video. arXiv, 2017. 3

[69] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and
K. Murphy. Tracking emerges by colorizing videos. 2017.
3, 6, 7

[70] G. Wallis. Spatio-temporal inÔ¨Çuences at the neural level of
object recognition. Network: Computation in Neural Sys-
tems, 9(2):265‚Äì278, Jan. 1998. 2

[71] N. Wang and D.-Y. Yeung. Learning a deep compact image

representation for visual tracking. In NIPS, 2013. 1, 3

[72] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural

networks. In CVPR, 2018. 5

[73] X. Wang and A. Gupta. Unsupervised learning of visual rep-

resentations using videos. In ICCV, 2015. 2, 6

[74] X. Wang, K. He, and A. Gupta. Transitive invariance for self-
supervised visual representation learning. In ICCV, 2017. 2,
6, 7, 8

[75] Y. Wang, Y. Yang, and W. Xu. Occlusion aware unsupervised

learning of optical Ô¨Çow. 2018. 3

[76] L. Wiskott and T. J. Sejnowski. Slow feature analysis: un-
supervised learning of invariances. Neural Computation,
14(4):715‚Äì770, Apr. 2002. 2

[77] J. N. Wood. A smoothness constraint on the development of

object recognition. Cognition, 153:140‚Äì145, 2016. 2

[78] J. N. Wood and S. M. W. Wood. The development of new-
born object recognition in fast and slow visual worlds. Pro-
ceedings. Biological Sciences, 283(1829), Apr. 2016. 2

[79] H. Wu, A. C. Sankaranarayanan, and R. Chellappa. In Situ
Evaluation of Tracking Algorithms Using Time Reversed
Chains. In 2007 IEEE Conference on Computer Vision and
Pattern Recognition, pages 1‚Äì8, June 2007. 3

[80] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A

benchmark. In CVPR, 2013. 3

[81] L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos.
EfÔ¨Åcient video object segmentation via network modulation.
2018. 7

[61] D. Sun, S. Roth, and M. J. Black. Secrets of optical Ô¨Çow

[82] Y. Yang and D. Ramanan. Articulated human detection with

estimation and their principles. In CVPR, 2010. 3

Ô¨Çexible mixtures of parts. TPAMI, 2013. 7

2575

[83] D.-J. Yi, N. B. Turk-Browne, J. I. Flombaum, M.-S. Kim,
B. J. Scholl, and M. M. Chun. Spatiotemporal object conti-
nuity in human ventral visual cortex. Proceedings of the Na-
tional Academy of Sciences, 105(26):8840‚Äì8845, July 2008.
2

[84] Z. Yin and J. Shi. Geonet: Unsupervised learning of dense

depth, optical Ô¨Çow and camera pose. In CVPR, 2018. 3

[85] Q. Zhou, X. Liang, K. Gong, and L. Lin. Adaptive temporal
encoding network for video instance-level human parsing. In
ACM MM, 2018. 5, 6, 7, 8

[86] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu-
In

pervised learning of depth and ego-motion from video.
CVPR, 2017. 3

[87] T. Zhou, Y. Jae Lee, S. X. Yu, and A. A. Efros. Flowweb:
Joint image set alignment by weaving consistent, pixel-wise
correspondences. In CVPR, 2015. 3

[88] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A.
Efros. Learning dense correspondence via 3d-guided cycle
consistency. In CVPR, 2016. 2, 3

[89] X. Zhou, M. Zhu, and K. Daniilidis. Multi-image matching

via fast alternating minimization. In ICCV, 2015. 3

[90] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. 2017. 3

[91] W. Zou, S. Zhu, K. Yu, and A. Y. Ng. Deep Learning
of Invariant Features via Simulated Fixations in Video.
In
F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,
editors, Advances in Neural Information Processing Systems
25, pages 3203‚Äì3211. Curran Associates, Inc., 2012. 2

2576

