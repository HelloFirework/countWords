Mixture Density Generative Adversarial Networks

Hamid Eghbal-zadeh1

Werner Zellinger2,3

Gerhard Widmer1

1 LIT AI Lab & Institute of Computational Perception, Johannes Kepler University Linz

2 Department of Knowledge-Based Mathematical Systems, Johannes Kepler University Linz

3 Software Compenetce Center Hagenberg GmbH

{hamid.eghbal-zadeh, werner.zellinger, gerhard.widmer}@jku.at

Abstract

Generative Adversarial Networks have a surprising abil-
ity to generate sharp and realistic images, but they are
known to suffer from the so-called mode collapse problem.
In this paper, we propose a new GAN variant called Mix-
ture Density GAN that overcomes this problem by encour-
aging the Discriminator to form clusters in its embedding
space, which in turn leads the Generator to exploit these
and discover different modes in the data. This is achieved
by positioning Gaussian density functions in the corners of
a simplex, using the resulting Gaussian mixture as a like-
lihood function over discriminator embeddings, and for-
mulating an objective function for GAN training that is
based on these likelihoods. We show how formation of
these clusters changes the probability landscape of the dis-
criminator and improves the mode discovery of the GAN.
We also show that the optimum of our training objective
is attained if and only if the generated and the real dis-
tribution match exactly. We support our theoretical re-
sults with empirical evaluations on three mode discovery
benchmark datasets (Stacked-MNIST, Ring of Gaussians
and Grid of Gaussians), and four image datasets (CIFAR-
10, CelebA, MNIST, and Fashion-MNIST). Furthermore, we
demonstrate (1) the ability to avoid mode collapse and dis-
cover all the modes and (2) superior quality of the gener-
ated images (as measured by the Fr´echet Inception Distance
(FID)), achieving the lowest FID compared to all baselines.

1. Introduction

Generative Adversarial Networks (GANs) [11] learn an
implicit estimate of the Probability Density Function (PDF)
underlying a set of training data, and can learn to generate
realistic new samples. One of the known issues in GANs is
the so-called mode collapse [1, 10, 22], where the generator

misses many modes when trained on a multi-modal dataset,
and achieves a low diversity in generating samples.

In this paper, we propose Mixture Density GAN (MD-
GAN), which is capable of generating high-quality sam-
ples, and in addition copes with the mode collapse prob-
lem and enables the GAN to generate samples with a high
variety. The central idea of MD-GAN is to enable the dis-
criminator to create several clusters in its output embedding
space for real images, and therefore provide better means
for distinguishing not only real and fake, but also between
different kinds of real images.

The discriminator in MD-GAN forms a number of clus-
ters1 over embeddings of real images which represent clus-
ters in the real data. To fool the discriminator, the generator
then has to generate images that the discriminator has to
embed close to the center of these clusters to increase the
likelihood of the generated images using the mixture den-
sity function. As there are multiple clusters, the generator
can discover various modes by generating images that end
up in various clusters in the discriminator embedding space.
MD-GAN’s Discriminator uses a d-dimensional embed-
ding space and is provided with an objective function that
pushes it towards forming clusters in this space that are ar-
ranged in the form of a Simplex2: each cluster center is lo-
cated in one of the vertices of this simplex.

In our experiments, we use seven benchmark datasets to
demonstrate the ability of MD-GAN to generate samples
with good quality and high variety, and to avoid mode col-
lapse. We show that MD-GAN outperforms the state of the
art, discovering the most number of modes in three bench-
mark datasets designed for evaluating mode collapse. Com-
paring our results to state-of-the-art methods in terms of the
Fr´echet Inception Distance (FID) [14] using only the basic
DCGAN [28] architecture, we will demonstrate that MD-
GAN also achieves state-of-the-art image quality.

1The number of clusters is a parameter that can be set.
2A simplex is a generalization of the notion of a tetrahedron with d
dimensions and d+1 vertices [25]. The cluster centers are thus equidistant.

5820

2. Related Work

GANs exhibit a surprising ability to generate sharp and
realistic images – in contrast to the blurry images gener-
ated via other techniques such as VAEs [15] –, but they
are known to be difﬁcult to train. Since the advent of the
ﬁrst GAN (‘vanilla’) [11], many variations have been pro-
posed to make training easier by optimizing an alternative
objective function. The Energy-Based GAN (EBGAN) [35]
and its improved variant Boundary-Equilibrium GAN (BE-
GAN) [3] use an auto-encoder as discriminator and recon-
struction loss as an energy function that assigns low ener-
gies to the regions near the data manifold and higher en-
ergies to others. The Wasserstein GAN (WGAN) [2] and
Wasserstein GAN with Gradient Penalty (WGAN-GP) [12]
minimize the Wasserstein distance between the activations
of real and fake images in a Lipschitz-constraint discrim-
inator. The McGAN [26] minimizes moment distances as
proposed in [33, 34]. SpectralNormGAN [24] controls the
Lipschitz constant of the discriminator using a novel weight
normalization technique and achieves training stability. In-
foGAN [5] disentangles the latent space of the generator by
maximising the mutual information between a subset of the
noise and the generated image, and stabilizes the training.
DeliGAN [13] increases the intra-class diversity in low-data
regimes by using a more expressive noise distribution drawn
from a Gaussian Mixture.

Some authors have investigated solutions to the mode
collapse problem. A Bayesian formulation is used [27]
to optimize a GAN using stochastic gradient Hamiltonian
Monte Carlo, which makes for more diverse generated sam-
ples. In [7] multiple discriminators are used to avoid mode
collapse while [9] laverages multiple generators to improve
the mode discovery. VEEGAN [29], uses a discriminator
that auto-encodes Gaussian noise and succeeds in discov-
ering the modes in the data, though it does not assess the
quality of generated images. Unrolled GAN [23] ”unrolls”
several gradient steps ahead when computing the gradients
for the generator to tell the generator how the discriminator
will behave in the next updates.

Our work is different from [7, 9] since it uses a sin-
gle discriminator and generator, in contrast to the multi-
discriminator and multi-agent GANs which uses an ensem-
ble of discriminators and generators. MD-GAN differs
from [27], which requires time-consuming MC sampling,
and from [5] that minimises mutual information. MD-GAN
uses equal updates in discriminator and generator, hence
different from [30, 23, 24], which require several additional
updates to compute the gradients for a single update of gen-
erator/discriminator. MD-GAN uses a simple uniform noise
and a vanilla generator, hence differs from [13]. Hence,
MD-GAN is computationally more efﬁcient and memory-
friendly. Also, MD-GAN differs from VEEGAN [29] as
it does not require an additional inference model for auto-

encoding: MG-GAN does not auto-encode. VEEGAN’s
objective is built upon a Gaussian distribution used in both
the discriminator and the generator, which has to match.
This limits VEEGAN since choosing the distribution of the
noise in the generator can indeed negatively affect the qual-
ity and diversity of a GAN as discussed in [31]. In contrast,
MD-GAN allows the noise distribution in the generator to
be selected independently from the embedding distribution
in the discriminator.

3. Background

3.1. Generative Adversarial Networks

A GAN usually consists of two neural networks compet-
ing with each other: (1) A generator network G that decodes
noise vectors into images, and (2) a discriminator network
D that encodes images into a notion of probability of an im-
age being real or fake. A real image is one from a dataset
of training images; an image generated by the generator is
considered a ‘fake’.

The generator tries to fool the discriminator by learning
to generate images that in the discriminator produce high
probabilities of being real and that thus cannot be distin-
guished from real images by the discriminator. For this
game to go on, the discriminator is trained to produce high
probabilities for images coming from the real image dataset,
and low probabilities for images coming from the genera-
tor. As training continues, the generator learns to generate
images that produce high probabilities in the discriminator,
which usually look also realistic to humans and are visually
similar to samples from the training dataset.

3.2. The Vanilla GAN Objectives

The loss of discriminator D and generator G in the two-
player minimax game of the Vanilla GAN was introduced
in [11] as

min

G

max

D

L(G, D) = min

G

max

D (cid:0)Ex∼pdata(cid:2) log D(x)(cid:3)
+ Ez∼pz(cid:2) log(1 − D(G(z)))(cid:3)(cid:1)

(1)

where for a given input image x, the discriminator D
outputs an estimated probability of the image coming from
the dataset of real images. pdata represents the distribu-
tion of real images, and pz the distribution of noise. z is
an observation from a random distribution pz; generator G
creates a fake image using this z. These two objectives
and their differences are discussed in detail in [10, 8]. In
the present paper, the GAN proposed in [11] will be called
vanilla GAN.

3.3. Mode Collapse Problem

As explained above, mode collapse happens when the
generator generates only samples from a number of modes

5821

in the data. To understand the reasons for mode collapse, we
ﬁrst have to keep in mind that the objective of the GAN is
a minimax objective, and two networks play against each
other, where the generator tries to fool the discriminator
by generating samples that are similar to the real samples,
while the discriminator tries to distinguish the fake samples
from the real ones. However, the fake generated samples
do not need to represent all the data modes; the discrimi-
nator can be fooled even if the generator generates samples
from only parts of the data space. The generator receives its
training signals directly from the discriminator, hence it is
important to know how the discriminator interacts with the
data space. As we discussed in Section 3.1, the discrimina-
tor can be basically seen as a binary classiﬁer that tries to
separate the real samples (with label 1) from fake samples
(label 0). Consequently, the discriminator has to assign high
probabilities to the areas of the data space where real exam-
ples are located, and low probabilities to the rest. These
probabilities can be also seen as the training signal sent to
the generator.

In Figure 1 we visualize the probability landscape of the
discriminator where we trained the GAN on data sampled
from a 2D grid of 25 Gaussians as real data. As can be seen
in Figure 1a, the vanilla GAN misses many of the clusters
and can only discover a small number of modes. Looking
at the probability landscape3 of the vanilla discriminator, it
can be seen that the discriminator produces very high prob-
abilities in the areas of the missing modes. This shows that
the generator already lost the game in those areas and can
no longer fool the discriminator in those areas.

We will tackle this issue by empowering the discrimina-
tor to better cover the data space and provide a more uni-
form probability landscape. In the following sections, we
show that by creating several clusters in the discriminator
embedding space and using these for the separation of real
from fake, MD-GAN can create a more uniform probability
landscape (Figure 1d) and provide better training signals to
the generator to recover all the modes (Figure 1c).

4. Mixture Density Generative Adversarial

Networks

4.1. Mixture Density GAN: The Intuition

As explained in the introduction, the basic idea in Mix-
ture Density GAN is to encourage the discriminator to form
a number of clusters over the embeddings of real images.
As also mentioned, these clusters will be positioned in an
equi-distant way, their center vectors forming a simplex.
Each cluster is represented by a Gaussian kernel. The whole

3The probability landscape is computed by feeding a mesh of data
points from the 2D space and computing the output probability for each
point. The probability is then shown in the position of that point, via a
color code.

(a) Generated samples
from vanilla GAN.

(b) Probability landscape
of the vanilla discrimina-
tor.

(c) Generated samples
from MD-GAN.

(d) Probability landscape
of
the MD-GAN dis-
criminator.

(e) Real data.

Figure 1: Comparison of probability landscape and gener-
ated data between vanilla and MD-GAN.

Figure 2: Block diagram of Mixture Density GAN. This
ﬁgure should be viewed in color.

collection thus makes up a Mixture of Gaussians, which
we will call a Simplex Gaussian Mixture Model (SGMM).
Each of the clusters draw embeddings of fake images to-
wards their center. This is achieved by using the SGMM as
a likelihood function. Each Gaussian kernel spreads its den-
sity over the embedding space: the closer an embedding to
the high probability areas (e.g, center of a cluster), the more
density it gets and, therefore, the more likelihood reward it
receives.

By deﬁning a likelihood function via the parameters of
a SGMM, in each update we train the discriminator to en-

5822

code real images to the centers of the clusters. The resulting
SGMM creates a mixture of clusters that draws the real em-
beddings towards the cluster centers (see Figure 2). Like-
wise, the generator will be rewarded if it generates samples
that end up in any of these clusters. Thus, if the fake embed-
dings are well spread around the cluster space – which they
are likely to be at the beginning of training, when they are
essentially just random projections –, it is likely that most
of the clusters will ‘catch’ some fake embeddings. There-
fore, the generator will tend to learn to generate samples
with more variety to cover all of the clusters, which ide-
ally results in discovering the modes present in the data. On
the other hand, it is reasonable to expect the discriminator
to create such clusters based on relevant similarities in the
data, since it is trained as a classiﬁer and therefore needs to
learn a meaningful distance in its embedding space.

To demonstrate this, we trained MD-GAN on a 2D
dataset of a grid of 25 Gaussians (Figure 1e) and visual-
ized some aspects of the resulting model in Figure 1 and
Figure 3. Figure 1 shows an example where MD-GAN
can discover more modes in the data, compared to vanilla
GAN. In particular, comparing Figure 1d and Figure 1b
shows the signiﬁcant differences in the respective probabil-
ity landscapes. In Figure 3, the ﬁrst row (a-c) shows the
samples generated in various epochs. As can be seen, the
data spreads nicely through data space and results in dis-
covering all the modes. The second row (d-e) shows the
percentages of real and fake embeddings assigned to each
Gaussian component in different epochs. Here, the x axis
represents the epochs and y the different components. Each
color represents a different Gaussian component; the width
of each color shows the percentage of embeddings assigned
to that component in the given epoch. As can be seen, the
embeddings of both real and fake are spread among all the
components, and the clusters we were aiming to form are
already created in the embedding space of MD-GAN dis-
criminator. The third row of Figure 3 shows a 2D PCA
projection of the embeddings from the 9D space of the dis-
criminator. Embeddings from the different Gaussian com-
ponents are shown in different color. As we expected, these
clusters reﬂect similarities/distances in input data space: as
the discriminator further differentiates the data into more
speciﬁc clusters (Fig. 3h), we see that data points from the
same Gaussian component (color) in the input data tend to
be embedded in coherent clusters in the embedding space
of the discriminator. In the next sections, we explain the
technical and theoretical details of MD-GAN.

4.2. Mixture Density GAN: The Model

As in the vanilla GAN, MD-GAN consists of a genera-
tor G and a discriminator D. MD-GAN uses a mixture of
Gaussians in its objective functions whose mean vectors are
placed in the vertices of a d-dimensional simplex, where d

(a) Ep 5

(b) Ep 100

(c) Final ep

(d) Cluster assignment for
real embeddings.

(e) Cluster assignment for
fake embeddings.

(f) Ep 5

(g) Ep 100

(h) Final ep

Figure 3:
a-c) Samples generated by Mixture Density
GAN’s generator in different epochs. d-e) Assignment per-
centages of embeddings to Gaussian components for real
and fake data. Each color represents a Gaussian cluster; the
width of the color in each column corresponds to the pro-
portion of embeddings that were assigned to that cluster af-
ter a whole training epoch. f-h) 2D PCA-projected embed-
dings of MD-GAN’s discriminator for real data in different
epochs. Embeddings of data points coming from the same
input Gaussian have the same color.

is a parameter.

Discriminator: The discriminator D in MD-GAN is a
neural network with d-dimensional output. For an input im-
age x, the discriminator creates an embedding e which is
simply the activation of the last layer of the network for in-
put x. The SGMM in MD-GAN is a Gaussian mixture with
the following properties:

1. The individual components are d-dimensional multi-
variate Gaussians (where d is the output/embedding
dimensionality of the discriminator network).

2. The model comprises d + 1 Gaussian components,
whose mean vectors are exactly the coordinates of the
vertices of a simplex.

3. The covariance matrices are diagonal and have equal
values on the main diagonal, in all the components.
Thus, all components are spherical Gaussians.

For an embedding e produced by the discriminator D,

we deﬁne the following likelihood function:

5823

lk (e) =

C

Xi=1

1

d + 1

· Φ(cid:16)e; µi, Σi(cid:17)

(2)

where Φ is the Gaussian PDF, µi is the mean vector, and Σi
is the covariance matrix for Gaussian component i, and C
is the number of Gaussian components in the mixture. Note
that each mixture weight equals

1

d+1 .

When a discrimination between real and fake images
is needed, the discriminator ﬁrst encodes the input image
x into the embedding e. Then, a likelihood lk (e) is
calculated for this embedding. lk (e) will be interpreted as
the probability of e being an embedding of a real image,
given the current model.

Generator: The generator G in MD-GAN is a regular
neural network decoder, decoding a random noise z from a
random distribution pz into an image.

4.3. The Mixture Density GAN Objectives

Denoting the encoding (output of the encoder, also re-
ferred to as the embedding) of an image x by discriminator
D as D(x), we propose MD-GAN’s objectives as follows:

min

G

max

L(G, D) =

D
min

G

max

D (cid:0)Ex∼pdata(cid:2) log(lk (D(x)))(cid:3)
+ Ez∼pz(cid:2) log(λ − lk (D(G(z))))(cid:3)(cid:1)

(3)

where the likelihood lk (e) for the given image embedding
e = D(x) is as deﬁned in Eq.(2).

We set λ to be the maximum value of the likelihood func-
tion lk (in Eq.(2)) in order to have only positive values in
the logarithm in Eq.(3) (see also Experimental Setup Sec-
tion). As discussed in [4], a Gaussian mixture can have
more high-probability peaks than its components 4. Hence,
we compute the maximum value of the likelihood function
using gradient descent. This is achieved by minimizing
− log(lk (D(x))) where D(x) is a feed-forward neural net-
work with only one dense layer and trained on a single and
ﬁxed data point. The likelihood value of the data point then
converges to the necessary maximum [4].

5. Theoretical Discussion

Recall that our goal is to allow multiple data clusters in
the discriminator’s embedding space while preserving the
discriminative power of the generative adversarial model
proposed in [11]. Let pgen represent the distribution of gen-
erated images, then the following holds:

4http://www.cs.toronto.edu/˜miguel/research/

GMmodes.html

and L(G, D)

Proposition 1 (Goodfellow et al. 2014) For
ﬁxed
as described in Eq.(1),
tor5D∗

G := arg maxD L(G, D) is given by

G
loss
the optimum discrimina-

discriminator

being

the

D∗

G(x) =

pdata(x)

pdata(x) + pgen(x)

Let λ be the maximum of the likelihood function. We then
deﬁne a new discriminator function ˜D by putting ˜D(x) :=
lk (D(x))/λ, which normalizes the likelihood function and
yields output values in the unit interval. By applying Propo-
sition 1 we obtain that

lk (D∗

G(x)) = λ ·

pdata(x)

pdata(x) + pgen(x)

(4)

for an optimum D∗
G := arg maxD L(G, D) of the MD-
GAN objective function in Eq.(3) with ﬁxed G. Eq.(4) char-
acterizes the multiple solutions of the discriminator objec-
tive. In the case of λ = 1 and lk being the identity func-
tion, the discriminator solution of [11] is obtained, where
the value of D∗
G(x) is one if and only if pgen(x) = 0. In
contrast, in our case, pgen(x) = 0 allows D∗
G(x) to be such
that the Gaussian likelihood is maximized. Thus, Eq.(4)
directly implements our idea of splitting up the optimal so-
lutions to create more meaningful clusters in the embedding
space.

As an example, let us consider the one-dimensional case

with the likelihood function deﬁned by

lk (e) :=

1
2

Φ(e; −1,

1
4

) +

1
2

Φ(e; 1,

1
4

)

Then, for a point x0 with pgen(x0) = 0, Eq.(4) reduces to
lk (D∗
G(x0)) = λ. This implies that the optimal discrimina-
tor D∗
G fulﬁlls either D∗
G(x0) ≃ −1, which,
for suitable covariance of the likelihood function, are near
the means of the component Gaussian densities. For x0
with pgen(x0) 6= 0 four different global optimal solutions
for D(x0) exist.

G(x0) ≃ 1 or D∗

The following Theorem 1 follows from [11, Theorem 1]
and shows that the optimum of our approach is achieved
if and only if the distribution of the real images and the
distribution of the generated images match exactly.

Theorem 1 Under the assumption of an optimal discrimi-
nator D∗
G as characterized by Eq.(4), the global optimum of
the training criterion minG L(G, D∗
G) in Eq.(3) is achieved
if and only if pdata = pgen.

Proof Under the assumption of an optimal discriminator
D∗
G, characterized by Eq.(4), our objective minG L(G, D∗
G)
can be reformulated as

5We keep the notation consistent with [11, Proposition 1]

5824

min

G

L(G, D∗

G)

= min

G

Ex∼pdata (cid:20)log(cid:18)λ ·

pdata(x)

pdata(x) + pgen(x)(cid:19)(cid:21)

+ Ex∼pgen (cid:20)log(cid:18)λ − λ ·

pdata(x)

pdata(x) + pgen(x)(cid:19)(cid:21)

= 2 log(λ)

+ min

G

Ex∼pdata (cid:20)log(cid:18)

pdata(x)

pdata(x) + pgen(x)(cid:19)(cid:21)

+ Ex∼pgen (cid:20)log(cid:18)

pgen(x)

pdata(x) + pgen(x)(cid:19)(cid:21) .

The optimality of our approach then follows from Theo-
rem 1 in [11]. (cid:4)

6. Empirical Results

6.1. Mode collapse evaluation

We use three different datasets and seven baselines
to compare with MD-GAN on all
the aforementioned
datasets: Vanilla GAN [11], Adversarially Learned Infer-
ence (ALI) [6], Unrolled GAN [23], VEEGAN [29], Deli-
GAN [13], InfoGAN [5] and SpectralNormGAN [24].

Stacked-MNIST: This dataset augments the MNIST
into a 1000 classes dataset. Gray-scale 28 × 28 images
from MNIST are randomly selected and ﬁlled into 3 chan-
nels of an RGB image. Since MNIST consists of 10 classes,
Stacked MNIST has 10 × 10 × 10 = 1000 classes. To eval-
uate Stack-MNIST experiments, we use the two measures
that were also used in [29, 23]. First, the number of classes
that a GAN can generate samples for: a CNN is trained on
1000 classes of Stacked MNIST and then is used to predict
the classes of generated samples. The maximum number of
classes that GAN can generate based on this classiﬁer after
25000 generated samples is reported. As a second measure,
the KL divergence between the label distribution of these
25000 samples and the real label distribution of Stacked
MNIST (uniform) is reported.

2D Grid of Gaussians: 25 Gaussian components with
a ﬁxed σ of 0.05 and mean vectors placed in a 5 × 5 grid
(Figure 1e) are used to sample training data from. For evalu-
ation, as proposed in [19, 29], we sample the generator for
2500 times and then report the number of Gaussian com-
ponents discovered. A Gaussian component is considered
“discovered” if a sample is generated within an L2 distance
of 3σ from the component’s mean. In addition to the num-
ber of discovered modes, we report the percentage of these
‘high quality’ samples with L2 ≤ 3σ.

2D Ring of Gaussians: 8 Gaussian components with
ﬁxed σ = 0.05 and means placed in a ring are used to sam-
ple training data from. The evaluation is analogous to the
2D grid of Gaussians.

6.2. Image quality evaluation

We use four standard datasets to compare MD-GAN to:
Vanilla GAN [11], Wasserstein GAN (WGAN) [2], Wasser-
stein with Gradient Penalty (WGAN-GP) [12], DRA-
GAN [16], and BEGAN [3]. For these experiments, the
Fr´echet Inception Distance (FID) [14] is used to evaluate
the quality of the generated images. The FID results are
computed using the provided code and trained inception
model from the github repository6 of the main author of the
FID paper.

Since we use results from [21] as our baselines on the
4 real image datasets, we also follow their FID computa-
tion procedure, which includes 10k sampling of real and
10k sampling of generated images.

MNIST [18] is a widely-used 28 × 28 image benchmark
dataset consisting of 60k images of hand-written digits in
10 classes.

Fashion-MNIST [32] comprises 28 × 28 gray-scale im-
ages of 70,000 fashion products from 10 categories, with
7,000 images per category.

CIFAR-10 [17] consists of 60,000 32 × 32 RGB images

in 10 classes, with 6,000 images per class.

CelebA [20] is a large-scale dataset with more than
200K images of celebrity faces, each with 40 attribute an-
notations. As done in [21], we use a 64 × 64 cropped and
centered version of CelebA.

6.3. Experimental Setup

6.3.1 Network architectures

To evaluate the merits of different GAN objectives, it is
important to keep the architecture the same. A standard
strategy in this context is to use the basic DCGAN archi-
tectures [28]. These architectures have a reasonable perfor-
mance, but are limited in terms of parameters and often used
for evaluating new objectives in GANs. Hence, to keep our
work comparable to the baselines, for the architectures in
our experiments on MNIST, Fashion-MNIST, CelebA and
CIFAR10, we use the same architectures as the other base-
lines used in [21].

To provide comparable results with our baselines for the
experiments on Stacked MNIST, we use three DCGAN ar-
chitectures: 1) the architecture used in [29] that has more
parameters (shown as B in Table 3), 2) the architecture used
in [23] with half the parameters in the discriminator (shown
as S 1
) and ﬁnally 3) the architecture used in [23] with a
quarter of parameters in the discriminator (shown as S 1
).

2

4

6https://github.com/bioinf-jku/TTUR

5825

Again, it is important to note that limiting the number of
parameters in a GAN makes mode discovery more difﬁcult
and demonstrates the ability of the objective in discovering
the modes. The architectures for 2D Ring and Grid of Gaus-
sians are the same as in [29, 23]. The last layer in the dis-
criminator of MD-GAN has the dimensionality of the cho-
sen simplex. All architectures used in our experiments are
detailed in the appendix.

6.3.2 Hyper-parameters and reproducibility

The vertices of our simplex have a distance of one from one
another. The variances in the diagonals of the covariance
matrices in all experiments with real images are set to 0.25,
and a 9D simplex (with 10 Gaussian components) is used.
The results of a grid search over the number of components
are provided in the appendix.

We use the noise distributions and dimensionality, batch
sizes and optimization methods reported in [21] for MNIST,
F-MNIST, CIFAR-10 and CelebA. The noise distribution
and dimensionality, as well as the architecture and optimiza-
tions for the 2D datasets and Stacked MNIST are the same
as in [29, 23]. The learning rates in mode-discovery exper-
iments for the MD-GAN as well as the implemented base-
lines are tuned for each method separately. These details
are provided in the appendix. To ensure reproducibility, the
source code of our method is available online.7

6.4. Results

6.4.1 Analysis of mode collapse behavior

The results of our 2D mode collapse experiments are pro-
vided in Tables 1 and 2. As can be seen, MD-GAN discov-
ers all the modes and at the same time, manages to generate
signiﬁcantly more high-quality data points compared to all
the baselines.

Looking at the results on Stacked MNIST in Table 3,
we see that again, MD-GAN outperforms all the baselines
using architectures with the same number of parameters.
Also MD-GAN achieves the lowest KL divergence of the
predicted labels, which shows that not only did MD-GAN
discover the most modes, but it also discovered them uni-
formly. These results are in line with the observations pre-
sented earlier in Figure 1d, which showed a more uniform
probability landscape in the discriminator that results in a
more uniform mode discovery.

6.4.2 Real image data and quality of generated images

The quantitative results of our experiments with the four
real image data sets are summarized in Table 4, where we
compare the FIDs achieved by MD-GAN with the results of

7https://github.com/eghbalz/mdgan

Table 1: Results of mode collapse experiments on 2D-Grid
of 25 Gaussians. †: results taken from [29]. ‡: results taken
from [29]. ∐: our implementation. All results are averages
over 5 runs.

method \measure

Vanilla [11]†

ALI [6]†

Unrolled GAN [23]‡

VEEGAN [29]†
DeliGAN [13]∐
InfoGAN [5]∐
SpecNorm [24]∐

MD-GAN

modes

(25)

3.3
15.8
23.6
24.6
21±2

17.2 ±4.95
23.8 ±1.59

25

% hq

(≤ 3 × std)

0.5
1.6
16
40

74.92±2.74
75.12 ±30.64
90.96 ±4.04
99.36±2.28

Table 2: Results of mode collapse experiments on 2D-Ring
of 8 Gaussians. †: results taken from [29]. ‡: results taken
from [29]. ∐: our implementation. All results are averages
over 5 runs.

method \ measure

modes

Vanilla [11]†

ALI [6]†

Unrolled GAN [23]‡

VEEGAN [29]†
DeliGAN [13]∐
InfoGAN [5]∐
SpecNorm [24]∐

MD-GAN

(8)

1
2.8
7.6
8

% hq

(≤ 3 × std)

99.3
0.13
35.6
52.9

6.4 ±1.85
3 ±1.54
6.8 ±1.16

8

98.28±0.4
98.88 ±1.51
86.64 ±9.76
89.03±3.69

other GAN variants reported in [21]. As can be seen, MD-
GAN achieved the lowest FID among all the baselines in all
datasets. Samples of the generated images are provided in
Figure 4.

6.5. Discussion

As can be seen in the mode collapse experiments above,
MD-GAN manages to discover all the modes in the data
and signiﬁcantly outperforms all the baselines. From the
results on real images, it can also be observed that MD-
GAN achieved the lowest FID in all datasets.

We showed in Figure 1 how the probability landscape
differs in MD-GAN and provides better training signals
for discovering the modes. Figure 3d and 3e showed how
the clusters in MD-GAN’s discriminator draw the embed-
dings of real and fake towards them and create a more
uniform distribution among components. Additionally, we
reported results on several mode discovery datasets and
demonstrated that using the mixture density function and

5826

Table 3: Results of mode collapse experiments on Stacked
MNIST. All results are averages over 5 runs.
†: results taken from [29]. ‡: results taken from [29].
∐: our implementation. B: Big DCGAN architecture, S 1
:
Small DCGAN architecture with half disc. parameters. S 1
:
Small DCGAN architecture with quarter disc. parameters.

2

4

method \ measure

arch.

Vanilla [11]†

ALI [6] †

VEEGAN [29] †
Unrl GAN [23] †

MD-GAN

Unrl GAN [23] ‡

Deli [13] ∐

InfoGAN [5] ∐
SpecNorm [24] ∐

MD-GAN

Unrl GAN [23] ‡

Deli [13] ∐

InfoGAN [5] ∐
SpecNorm [24] ∐

MD-GAN

2

2

B
B
B
B
B
S 1
S 1
S 1
S 1
S 1
S 1
S 1
S 1
S 1
S 1

2

2

2

4

4

4

4

4

(a) MNIST.

(b) FMNIST.

(c) CIFAR-10.

(d) CelebA.

modes
(1000)

99
16
150
48.7
1000

817.4 ±37.91

125.60 ±144.65
796.40 ±76.51
678.80±270.98

921.0±3.0
327.2 ±74.67
158.6 ±84.21

237.20 ±284.38
354.60 ±248.0
696.0±10.0

KL

(labels)

3.4
5.4
2.95
4.32

0.046±0.001

1.43 ±0.12
3.77 ±1.97
0.90±0.11
1.45 ±.81
0.80±0.06
4.66 ±0.46
3.22 ±0.96
2.87 ±0.94
2.44 ±1.10
1.32±0.015

Table 4: FIDs on different datasets from different methods.
†: Results taken from [21] which are best FIDs obtained in a
large-scale hyper-parameter search for each data set. Lower
FID values represent higher quality for generated images.

method \ db

MNT

FMNT

CFR

Clb

Real imagesdagger

Vanilla [11]†

Wasserstein [2]†

Wasserstein GP t[12]†

DRAGAN [16]†

BEGAN [3]†

MD-GAN

1.2
6.7
6.8
8.9
7.7
12.3
6.29

2.6
26.6
18.0
20.6
26.0
33.2
11.79

5.1
58.6
55.9
52.9
68.5
71.4
36.80

2.2
58.0
42.9
26.8
41.4
38.1
24.51

(e) SMNIST.

Figure 4: Randomly chosen samples from MD-GAN.

7. Conclusion

We have proposed the Mixture Density GAN, which suc-
ceeds in alleviating the mode collapse problem and gener-
ating high-quality images by allowing the Discriminator to
form separable clusters in its embedding space, which in
turn leads the Generator to generate data with more variety.
We analysed the optimum discriminator and showed that
it is achieved when the generated and the real distribution
match exactly. We demonstrated the ability of MD-GAN to
deal with mode collapse and generate realistic images us-
ing seven benchmark datasets. We demonstrated that MD-
GAN achieved the best results of all compared baselines on
all datasets, in terms of FID and the number of discovered
modes with high-quality generated data.

the clusters formed, MD-GAN outperformed several base-
lines.

8. Acknowledgement

To show that this mode discovery property does not
negatively effect the quality of the generated images, us-
ing standard architectures and four benchmark datasets, we
showed that MD-GAN achieved the lowest FID among all
the baselines, hence is capable of generating high quality
images.

We would like to thank Susanne Saminger-Platz and
Martin Heusel from Johannes Kepler University of Linz for
helpful discussions. This work has been partly funded by
the Austrian COMET Center SCCH. We also gratefully ac-
knowledge the support of NVIDIA Corporation with the do-
nation of a Titan X GPU used for this research.

5827

References

[1] Martin Arjovsky and L´eon Bottou. Towards principled meth-
ods for training generative adversarial networks.
In NIPS
2016 Workshop on Adversarial Training. In review for ICLR,
2017. 1

[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
2, 6, 8

[3] David Berthelot, Tom Schumm, and Luke Metz.

Be-
gan: Boundary equilibrium generative adversarial networks.
arXiv preprint arXiv:1703.10717, 2017. 2, 6, 8

[4] Miguel A. Carreira-Perpinan. Mode-ﬁnding for mixtures of
gaussian distributions. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 22(11):1318–1323, 2000. 5

[5] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable repre-
sentation learning by information maximizing generative ad-
versarial nets. In Advances in Neural Information Processing
Systems, 2016. 2, 6, 7, 8

[6] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex
Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron
Courville. Adversarially learned inference. arXiv preprint
arXiv:1606.00704, 2016. 6, 7, 8

[7] Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Gener-
ative multi-adversarial networks. International Conference
on Learning Representations, 2017. 2

[8] William Fedus, Mihaela Rosca, Balaji Lakshminarayanan,
Andrew M Dai, Shakir Mohamed, and Ian Goodfellow.
Many paths to equilibrium: GANs do not need to decrease
adivergence at every step. arXiv preprint arXiv:1710.08446,
2017. 2

[9] Arnab Ghosh, Viveka Kulharia, Vinay P Namboodiri,
Philip HS Torr, and Puneet K Dokania. Multi-agent di-
verse generative adversarial networks. In IEEE Conference
on Computer Vision and Pattern Recognition, 2018. 2

[10] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial

networks. arXiv preprint arXiv:1701.00160, 2016. 1, 2

[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems, 2014. 1, 2, 5, 6, 7, 8
[12] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron Courville.
Improved training of
wasserstein GANs. Advances in Neural Information Pro-
cessing Systems, 2017. 2, 6, 8

[13] Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and
R Venkatesh Babu. Deligan: Generative adversarial net-
works for diverse and limited data. In IEEE Conference on
Computer Vision and Pattern Recognition, 2017. 2, 6, 7, 8

[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium.
In Advances in Neural Information Processing Sys-
tems, 2017. 1, 6

[15] Diederik P Kingma and Max Welling. Auto-encoding vari-
ational bayes. International Conference on Learning Repre-
sentations, 2014. 2

[16] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt
Kira. On convergence and stability of gans. arXiv preprint
arXiv:1705.07215, 2017. 6, 8

[17] Alex Krizhevsky and Geoffrey Hinton. Learning multiple

layers of features from tiny images. 2009. 6

[18] Yann LeCun. The mnist database of handwritten digits.

http://yann. lecun. com/exdb/mnist/, 1998. 6

[19] Jae Hyun Lim and Jong Chul Ye. Geometric GAN. 2017. 6

[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
In International

Deep learning face attributes in the wild.
Conference on Computer Vision, 2015. 6

[21] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain
Gelly, and Olivier Bousquet. Are GANs created equal? a
large-scale study. Advances in Neural Information Process-
ing Systems, 2018. 6, 7, 8

[22] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
The numerics of gans. arXiv preprint arXiv:1705.10461,
2017. 1

[23] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
Dickstein. Unrolled generative adversarial networks. Inter-
national Conference on Learning Representations, 2017. 2,
6, 7, 8

[24] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks.
International Conference on Learning
Representations, 2018. 2, 6, 7, 8

[25] G. K. Monatsh. Mehrdimensionale geometrie. Monatshefte

f¨ur Mathematik und Physik, 1907. 1

[26] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan:
Mean and covariance feature matching gan. arXiv preprint
arXiv:1702.08398, 2017. 2

[27] Yunus Saatchi and Andrew Gordon Wilson. Bayesian GAN.
In Advances in Neural Information Processing Systems,
2017. 2

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in Neural Information Pro-
cessing Systems, 2016. 1, 6

[29] Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U
Gutmann, and Charles Sutton. VEEGAN: Reducing mode
collapse in gans using implicit variational learning. In Ad-
vances in Neural Information Processing Systems, 2017. 2,
6, 7, 8

[30] Thomas Unterthiner, Bernhard Nessler, G¨unter Klambauer,
Martin Heusel, Hubert Ramsauer, and Sepp Hochreiter.
Coulomb gans: Provably optimal nash equilibria via poten-
tial ﬁelds. International Conference on Learning Represen-
tations, 2018. 2

[31] Tom White. Sampling generative networks. arXiv preprint

arXiv:1609.04468, 2016. 2

[32] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
6

[33] Werner Zellinger, Thomas Grubinger, Edwin Lughofer,
Thomas Natschl¨ager, and Susanne Saminger-Platz. Central

5828

moment discrepancy (cmd) for domain-invariant representa-
tion learning. International Conference on Learning Repre-
sentations, 2017. 2

[34] Werner Zellinger, Bernhard A. Moser, Thomas Grub-
inger, Edwin Lughofer, Thomas Natschl¨ager, and Susanne
Saminger-Platz. Robust unsupervised domain adaptation for
neural networks via moment alignment.
Information Sci-
ences, 483:174–191, May 2019. 2

[35] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-
arXiv preprint

based generative adversarial network.
arXiv:1609.03126, 2016. 2

5829

