Deep Virtual Networks for Memory Efﬁcient Inference of Multiple Tasks

Eunwoo Kim1

Chanho Ahn2
1University of Oxford

Philip H.S. Torr1
2Seoul National University

Songhwai Oh2

{eunwoo.kim, philip.torr}@eng.ox.ac.uk

{mychahn, songhwai}@snu.ac.kr

Abstract

Deep networks consume a large amount of memory by
their nature. A natural question arises can we reduce that
memory requirement whilst maintaining performance.
In
particular, in this work we address the problem of memory
efﬁcient learning for multiple tasks. To this end, we propose
a novel network architecture producing multiple networks
of different conﬁgurations, termed deep virtual networks
(DVNs), for different tasks. Each DVN is specialized for a
single task and structured hierarchically. The hierarchical
structure, which contains multiple levels of hierarchy corre-
sponding to different numbers of parameters, enables mul-
tiple inference for different memory budgets. The building
block of a deep virtual network is based on a disjoint collec-
tion of parameters of a network, which we call a unit. The
lowest level of hierarchy in a deep virtual network is a unit,
and higher levels of hierarchy contain lower levels’ units
and other additional units. Given a budget on the number
of parameters, a different level of a deep virtual network can
be chosen to perform the task. A unit can be shared by dif-
ferent DVNs, allowing multiple DVNs in a single network.
In addition, shared units provide assistance to the target
task with additional knowledge learned from another tasks.
This cooperative conﬁguration of DVNs makes it possible
to handle different tasks in a memory-aware manner. Our
experiments show that the proposed method outperforms ex-
isting approaches for multiple tasks. Notably, ours is more
efﬁcient than others as it allows memory-aware inference
for all tasks.

1. Introduction

Recently, deep learning methods have made remark-
able progress in computer vision and machine learning
[13, 21, 30]. Although successful in many applications, it
is well-known that many deep neural networks have a high
memory footprint [10, 17]. This limits their practical ap-
plications, such as mobile phones, robots, and autonomous
vehicles of low capacity. The issue has been addressed by
research aimed at reducing the number of parameters of a

(Top left) Multi-task learning [2] for k tasks and (top
Figure 1.
right) memory efﬁcient learning [19] for a single task with nh dif-
ferent memory budgets, realized in a single network, respectively.
(Bottom) An overview of the proposed approach. The proposed ar-
chitecture contains multiple deep networks (deep virtual networks)
of different conﬁgurations for different tasks. Each deep virtual
network is specialized for a single task and allows multiple in-
ference for different memory budgets. Our approach incorporates
both multi-task and memory efﬁcient learning methods in a single
architecture, producing k × nh inference outputs, which signiﬁ-
cantly reduces the training efforts and network storage.

deep network to create a lightweight network [12, 14].

Unfortunately, developing such a compact network is ac-
companied by a tradeoff between accuracy and the num-
ber of parameters (referred as the memory1) at test time
[11, 16]. This requires efforts to ﬁnd a proper network that
gives competitive performance under a given memory bud-
get [9]. Besides, when a network model with a different
memory budget is required, we deﬁne and train a new net-
work, which incurs additional training cost.

Recently, several studies have been conducted on mul-
tiple inference under different memory budgets in a single

1We call the number of parameters as memory throughout the paper.

2710

trained architecture [19, 22], called memory efﬁcient infer-
ence. This problem can be achieved by designing a net-
work structure (e.g., nested [19] and fractal [22] structures)
which enables multiple inference corresponding to different
memory budgets. It allows ﬂexible accuracy-memory trade-
offs within a single network and thus can avoid introducing
multiple networks for different memory budget. Note that
memory budget may vary when tasks are performed simul-
taneously in a memory-limited device (e.g., an autonomous
vehicle with real-time visual and non-visual inference tasks
to process at once).

Obviously, memory efﬁcient inference can be an efﬁcient
strategy to provide different predictions in a network. How-
ever, prior works have applied the strategy to a single task
learning problem individually [19, 31], and addressing mul-
tiple tasks jointly (often called multi-task learning [2, 29])
with the strategy has been considered less. Learning mul-
tiple tasks2 simultaneously in a network can have a single
training stage and reduce the number of networks [2, 26].
This approach also has the potential to improve generaliza-
tion performance by sharing knowledge that represents as-
sociated tasks [5, 7, 39]. Despite its compelling beneﬁts, lit-
tle progress has been made so far in connection with mem-
ory efﬁcient inference. This is probably due to the difﬁculty
of constructing a single network that allows memory efﬁ-
cient inference for different tasks. The difﬁculty lies in the
structural limitation of a neural network to possess a differ-
ent structure for each task.

In this work, we aim to develop an efﬁcient deep learn-
ing approach that performs memory efﬁcient inference for
multiple tasks in a single network. To this end, we propose
a novel architecture containing multiple networks of dif-
ferent conﬁgurations termed deep virtual networks (DVNs).
Each DVN shares parameters of the architecture and per-
forms memory efﬁcient inference for its corresponding task.
A virtual network resembles a virtual machine [28] in a
computer system as multiple virtual machines can share re-
sources of a physical computer. Figure 1 gives an overview
of the proposed approach.

The proposed architecture is based on a backbone archi-
tecture, and we divide the network parameters into multi-
ple disjoint sets along with their corresponding structures
termed units. Speciﬁcally, units are collected by dividing
a set of feature maps in each layer into multiple subsets
throughout the layers in the architecture (see Figure 2). A
DVN is structured hierarchically which contains multiple
levels of hierarchy corresponding to different numbers of
units, and a lower level of hierarchy assigns fewer units and
a higher level of hierarchy contains more units. For exam-
ple, the lowest level of the hierarchy has a single unit. Each
level of the hierarchy in a DVN contains all preceding lower
levels’ units and one additional unit. Hence, different levels

2Multiple tasks refer to multiple datasets, unless stated otherwise.

Figure 2. A graphical illustration of the proposed approach which
is based on a backbone architecture (Physical Net) with k preas-
signed disjoint structures, called units. For a simple illustration,
we assume that the number of feature maps and their dimensions
are the same across all layers (here, we omit fully-connected lay-
ers). The proposed architecture produces k deep virtual networks
(Virtual Nets), sharing its units for k tasks. A deep virtual network
has a unique hierarchical structure with a different order of units
and is specialized for a designated task. The number of levels of
hierarchy in a deep virtual network is nh, which corresponds to the
number of different memory budgets. This allows k×nh inference
for k deep virtual networks. (Best viewed in color.)

of hierarchy in a DVN enables multiple inference according
to different memory budgets. In the proposed architecture,
a unit can be shared by different DVNs. This allows multi-
ple DVNs in a single deep network for multiple tasks. Each
deep virtual network has a unique conﬁguration (i.e., a hi-
erarchical structure with a different order of units), and is
specialized for a single task. The unique conﬁguration is de-
termined by the proposed rule discussed in Section 3.2. The
proposed approach can selectively provide an inference out-
put from its DVNs for a given task with the desired memory
budget. The approach is realized in a single training stage
based on a single backbone architecture (e.g., a residual net-
work [13]), which signiﬁcantly reduces training efforts and
network storage.

We apply our method to joint learning scenarios of mul-
tiple tasks using popular image classiﬁcation datasets. Our
results show that for all tasks DVNs are learned successfully
under different memory budgets. Even more, the results are
better than other approaches. We also measure the actual
processing time during inference to verify the practicality
of the proposal. In addition, we demonstrate our approach
on the task of sequential learning [24].

2711

The proposed approach introduces a new concept of vir-
tual networks in deep learning to perform multiple tasks in
a single architecture, making it highly efﬁcient.

3. Approach

3.1. Memory efﬁcient learning

2. Related Work

Multi-task learning. The aim of multi-task learning [2]
is to improve the performance of multiple tasks by jointly
learning them. Two popular approaches are learning a
single shared architecture with multiple output branches
[24, 25] and learning multiple different networks according
to tasks [27, 35]. We are particularly interested in multi-
task learning with a single shared network as it is memory
efﬁcient. Recently, a few approaches have been proposed
to perform multiple tasks in a single network by exploiting
unnecessary redundancy of the network [19, 26]. PackNet
[26] divides a set of network parameters into multiple dis-
joint subsets to perform multiple tasks by iteratively prun-
ing and packing the parameters. NestedNet [19] is a col-
lection of networks of different sizes which are constructed
in a network-in-network style manner. However, for a ﬁxed
budget the size of the assigned parameters of each network
will be reduced as the number of tasks increases, which may
cause a decrease in performance. Moreover, they can pro-
duce an inference output for each task. Whereas, our ap-
proach can overcome the issues by introducing deep virtual
networks sharing disjoint subsets of parameters in our archi-
tecture and their different conﬁgurations make it possible to
address multiple tasks (see Figure 2).

Multi-task learning can be extended to sequential learn-
ing [3, 24, 38], where tasks are learned sequentially without
accessing the datasets of old tasks. Following the popular
strategy in [24], we apply the proposed approach to sequen-
tial learning problems (see Section 3.3 and 4.5).

Memory efﬁcient learning. Memory efﬁcient learning is a
learning strategy to perform multiple inference according to
different budgets on the number of parameters (called mem-
ory) in a single network [19, 22, 37]. It enables ﬂexible in-
ference under varying memory budget, which is often called
the anytime prediction [40]. To realize the anytime predic-
tion, a self-similarity based fractal structure [22] was pro-
posed. A feedback system based on a recurrent neural net-
work [37] was proposed to perform different predictions ac-
cording to memory or time budgets. A nested network [19],
which consists of multiple networks of different scales, was
proposed to address different memory budget. However,
these approaches are conﬁned to performing an individual
task. In contrast, our method enables the anytime prediction
for multiple tasks using deep virtual networks.

To our knowledge, this work is the ﬁrst to introduce deep
virtual networks of different conﬁgurations from a single
deep network, which enables ﬂexible prediction under vary-
ing memory conditions for multiple tasks.

We discuss the problem of memory efﬁcient learning to
perform multiple inference with respect to different mem-
ory budgets for a single task. Assume that given a backbone
network we divide the network parameters into k disjoint
subsets, i.e., W = [W1, W2, ..., Wk]. We design the net-
work to be structured hierarchically by assigning the sub-
sets, in a way that the l-th level of hierarchy (l ≥ 2) con-
tains the subsets in the (l − 1)-th level and one additional
subset [19]. The lowest level of the hierarchy (l = 1) as-
signs a single subset and the highest level contains all sub-
sets (i.e., W). For example, when k = 3 we can assign
W1 to the lowest level in the hierarchy, [W1, W2] to the in-
termediate level, and [W1, W2, W3] to the highest level. A
hierarchical structure is determined by an order of subsets,
which is designed by a user before learning. In this work,
the number of levels of hierarchy, denoted as nh, is set to
the number of subsets, k. Each level of hierarchy deﬁnes a
network corresponding to the subsets and produces an out-
put. The hierarchical structure thus enables nh inference for
nh different numbers of subsets (memory budgets).

Given a dataset D consisting of image-label pairs and
nh levels of hierarchy , the set of parameters W can be op-
timized by the sum of nh loss functions

min
W

nh

Xl=1

L(cid:16)hl(W); D(cid:17),

(1)

where hl(W) is a set of parameters of W that are assigned
to the l-th level of hierarchy. There is a constraint on h
such that a higher level set includes a lower level set, i.e.,
hp(W) ⊆ hq(W), p ≤ q, ∀p, q ∈ [1, ..., nh], for a struc-
ture sharing parameters [19]. L(·) is a standard loss func-
tion (e.g., cross-entropy) of a network associated with D.
In addition, we enforce regularization on W (e.g., l2 decay)
for improved learning. By solving (1), a learned network
is collected and can perform nh inference corresponding to
nh memory budgets.

The function hl(W) can be designed by a pruning oper-
ation on W in element-wise [12] or group-wise (for feature
maps) [23]. Since our approach targets a practical time-
dependent inference, we follow the philosophy of group-
wise pruning approaches [14, 33] in this work. Note that
the problem (1) is applied to a single task (here, a dataset
D), rarely considering multiple tasks (or datasets). This is-
sue will be addressed in the following subsection with the
introduction of deep virtual networks.

3.2. Deep virtual network

Building block. Our network architecture is based on a
backbone architecture, and we divide the network parame-
ters into multiple disjoint subsets. Assume that there are k

2712

Figure 3. An example of constructing three different hierarchical structures in the r-th convolutional layer of a network, denoted as Mr,
which consists of three disjoint sets of feature maps or units (i.e., Mr = [M r
3 ] and the number indicates the unit index). The
number of tasks and the number of levels of hierarchy are three. Different orders of the units construct the hierarchical structures. Here,
hl,j(Mr) is a function that selects the sub-structure of Mr corresponding to the l-th level of hierarchy for the j-th task. S(i, j) returns
the level number at which the i-th unit M r

i is added to the hierarchy for the j-th task. (Best viewed in color.)

2 , M r

1 , M r

1 , W r

disjoint subsets in a network, which are collected by divid-
ing feature maps in each layer into k subsets across all lay-
ers.3 Formally, a set of network parameters is represented
as W = {W r}1≤r≤L, where L is the number of layers and
W r = [W r
o . The i-th sub-
I (i)×cr
o(i). Here,
set of W r is denoted as W r
wr and hr are the width and height of the convolution ker-
nel of the r-th layer, respectively. cr
o are the number
of input and output feature maps of the r-th layer, respec-
o(j) = cr
o.

I ×cr
i ∈ Rwr ×hr ×cr

k ] ∈ Rwr ×hr ×cr

2 , ..., W r

I (j) = cr

I and cr

j=1 cr

j=1 cr

The set of the i-th subsets over all layers is written as

tively, such that Pk

I and Pk

are collected sequentially, along with their task ID numbers,
and the datasets with adjacent task ID numbers are from
similar domains. The proposed rule is: (i) The unit i is as-
signed to the task i, and it becomes the lowest level in the
hierarchy for the task. (ii) The unit i is coupled with adja-
cent units that are not coupled. (iii) If there are two adjacent
units, the unit with a lower task ID number is coupled. For
example, assume that hl,j(W) is a function that selects the
subset of W of the l-th level of hierarchy for the task j.
When k = 3 and W = [W1, W2, W3], where Wi denotes
the parameters for the unit i, we construct the following hi-
erarchical structure4 from the rule for the task j

Wi = [W 1

i , W 2

i , ..., W L
i ].

(2)

h1,j(W) =Wj,

We call the corresponding network structure deﬁned by Wi
as unit i, which produces an inference output.

Hierarchical structure. The proposed approach produces
deep virtual networks (DVNs) of different network conﬁg-
urations (i.e., hierarchical structures) using shared units in
a network architecture, as illustrated in Figure 2. Each unit
is coupled with other units along the feature map direction
to form a hierarchical structure similar to the strategy de-
scribed in Section 3.1. The number of levels of hierarchy
is nh, where a level of the hierarchy includes all preced-
ing lower levels’ units and one additional unit. A different
hierarchical structure is constructed by a different order of
units. This introduces a unique DVN which is specialized
for a task. Thus, multiple DVNs of different network con-
ﬁgurations can be realized in a single network by sharing
units, for different tasks (see Figure 3). Whereas, the prob-
lem (1) is for a single task with a network conﬁguration,
which is equivalent to producing a single DVN.

[Wj, Wj−1],

h2,j(W) =( [W1, W2],
h3,j(W) =


[W1, W2, W3],
[Wj, Wj−1, Wj+1],
[Wj, Wj−1, Wj−2],

if 1 ≤ j ≤ k,

if j = 1,
if 1 < j ≤ k,

if j = 1,
if 1 < j < k,
if j = k.

(3)

The conﬁguration is different depending on the order of
units (see Figure 3 for an example).

Objective function. Given datasets for k tasks, T =
[D1, D2, ..., Dk], k deep virtual networks, the set of param-
eters W, and nh levels of hierarchy for each deep virtual
network, the proposed method can be optimized by solving
the sum of k × nh loss functions

min
W

Lk(W) ,

k

Xj=1  nh
Xl=1

L(cid:16)hl,j(W); Dj(cid:17)! ,

(4)

Rules for conﬁguring virtual networks. In order to deter-
mine different network conﬁgurations of deep virtual net-
works, we introduce a simple rule. We assume that datasets

where hl,j(W) is a function that selects the subset of W cor-
responding to the l-th level of hierarchy for the j-th task (or
deep virtual network j), such that hp,j(W) ⊆ hq,j(W), p ≤

3For simplicity, we omit a fully-connected layer. However, it is ap-

4When units are used together, additional parameters (interconnection

pended on top of the last convolutional layer to produce an output.

between units) are added to parameters of stand-alone units, Wi’s.

2713

q, ∀p, q ∈ [1, ..., nh], for all j ∈ [1, ..., k]. Note that in the
case when k = 1, the problem (4) reduces to the problem
(1) for a single task [19].

Learning. The unit i is learned based on the following gra-
dient with respect to Wi

D(l,j)(W) , Lj

where Lj
D(hl,j(W); Dk) and L(l,k)(W) ,
L(hl,k(W); Dk).
In the special case where nh = 1 and
hl,j(W) = W, ∀l, j, the problem (6) reduces to the learn-
ing without forgetting (LwF) problem [24], which has the
following gradient

∂Lk(W)

∂Wi

=

k

Xj=1

nh

Xl=S(i,j)




∂Ll,j(W)

∂Wi 
 ,

(5)

where Ll,j(W) , L(hl,j(W); Dj). S(i, j) returns the level
number at which the i-th unit is added to the hierarchy for
the j-th task (see Figure 3). The unit i is learned by aggre-
gating multiple gradients from the hierarchical structures of
deep virtual networks for all k tasks. Note that, for given
nh, the difference |nh − S(i, j)| inﬂuences on the amount
of the gradient (signiﬁcance) of the unit i for the task j as
the gradients from more levels accumulate. As the differ-
ence is larger, the signiﬁcance of the unit will be higher for
the task j. The proposed approach is trained in a way that
each unit is learned to have different signiﬁcance (different
S(i, j)) for all tasks. Note that the total amount of gradients
of a unit over all tasks is about same to those of other units
using the proposed conﬁguration rule. This prevents units
from having irregular scales of gradients.

3.3. Deep virtual network for sequential tasks

The proposed approach can also handle sequential tasks
[24]. Assume that the old tasks, from the ﬁrst to the (k −1)-
th task, have been learned beforehand. For the current (new)
task k, we construct an architecture with k units, where k−1
units correspond to the old tasks and the k-th unit represents
the current task. Based on the units, we construct k deep
virtual networks as described in Section 3.2.

Given a dataset for the task k, Dk, the set of parameters
W, k deep virtual networks, and nh levels of hierarchy, the
problem (k > 1) is formulated as

min
W

k−1

Xj=1  nh
Xl=1

Lj

D(cid:16)hl,j(W); Dk(cid:17)!+

nh

Xl=1

L(cid:16)hl,k(W); Dk(cid:17),

(6)
where Lj
D(hl,j(W); Dk) is a distillation loss between the
output of a network whose corresponding structure is deter-
mined by hl,j(W) and the output of the task j from the old
network when a new input Dk is given. The only excep-
tion from the problem (4) (which jointly learns k tasks) is
that we use a distillation loss function Lj
D(·) to preserve the
knowledge of the old tasks in the current sequence [24] (due
to the absence of the old datasets). For Lj
D(·), we adopt the
modiﬁed cross entropy function [15] following the practice
in [24]. The gradient of (6) with respect to Wi is

k−1

Xj=1

nh


Xl=S(i,j)


∂Lj

D(l,j)(W)
∂Wi


+

nh

Xl=S(i,k)

∂L(l,k)(W)

∂Wi

, (7)

∂Lj

D(W; D)
∂W

+

∂L(W; D)

∂W

.

(8)

k−1

Xj=1

Compared to our gradient in (7), LwF learns a single set
of parameters W, which reveals that the network has no
hierarchical structure and all tasks are performed without
memory efﬁcient inference.

4. Experiments

4.1. Experimental setup

We tested our approach on several supervised learning
problems using visual images. The proposed method was
applied to standard multi-task learning (joint learning) [2],
where we learn multiple tasks jointly, and sequential learn-
ing [24], where we focus on the k-th sequence with the
learned network for old tasks. We also applied the pro-
posed approach to hierarchical classiﬁcation [34], which is
the problem of classifying coarse-to-ﬁne class categories.
Our approach was performed based on four benchmark
datasets: CIFAR-10 and CIFAR-100 [20], STL-10 [4], and
Tiny-ImageNet5, based on two popular (backbone) models,
WRN-n-s [36] and ResNet-n [13], where n and s are the
number of layers and the scale factor over the number of
feature maps, respectively.

We ﬁrst organized three scenarios for joint learning of
multiple tasks. We performed a scenario (J1) consisting of
two tasks using the CIFAR-10 and CIFAR-100 datasets and
another scenario (J2) of four tasks whose datasets are col-
lected by dividing the number of classes of Tiny-ImageNet
into four subsets evenly. The third scenario (J3) consists
of three datasets, CIFAR-100, Tiny-ImageNet, and STL-10,
of different image scales (from 32×32 to 96×96). For hi-
erarchical classiﬁcation (H1), CIFAR-100 was used which
contains coarse classes (20 classes) and ﬁne classes (100
classes). For sequential learning, we considered two scenar-
ios, where a scenario (S1) has two tasks whose datasets are
collected by dividing the number of classes of CIFAR-10
into two subsets evenly, and another scenario (S2) consists
of two tasks using CIFAR-10 and CIFAR-100.

We compared the proposed approach with other recent
approaches handling multiple tasks: Feature Extraction
[6], LwF [24], DA-CNN [32], PackNet [26], and Nested-
Net [19]. We also compared with the backbone networks,
ResNet [13] and WRN [36], as baseline approaches per-
forming an individual task.

5https://tiny-imagenet.herokuapp.com/

2714

4.2. Implementation details

All the compared architectures were based on ResNet
[13] or WRN [36]. We followed the practice of construct-
ing the number of feature maps in residual blocks in [13] for
all applied scenarios. We constructed the building block of
a network for Tiny-ImageNet based on the practice of Im-
ageNet [13]. All the compared methods were learned from
scratch until the same epoch number and were initialized
using the Xavier method [8]. The proposed network was
trained by the SGD optimizer with Nesterov momentum of
0.9, where the mini-batch sizes were 128 for CIFAR and 64
for Tiny-ImageNet, respectively. We adopted batch normal-
ization [18] after each convolution operation.

We constructed units with respect to feature maps across
the convolution layers, except the ﬁrst input layer. Our deep
virtual networks have task-speciﬁc input layers for different
tasks or input scales, respectively, and the dimensionality
of their outputs are set to the same by varying the stride
size using convolution. When two units are used together,
the feature map size doubles and additional parameters (i.e.,
interconnection between the units) are needed to cover the
increased feature map size, in addition to parameters (intra-
connection) of stand-alone units. We also appended a fully
connected layer of a compatible size on top of each level of
hierarchy. All the proposed approaches were implemented
under the TensorFlow library [1], and their evaluations were
provided based on an NVIDIA TITAN Xp graphics card.

4.3. Joint learning

We conducted experiments for joint learning by compar-
ing with two approaches: PackNet+ (a grouped variant of
PackNet [26] to achieve actual inference speed-up by divid-
ing feature maps into multiple subsets similar to ours), and
NestedNet (with channel pruning) [19] which can perform
either multi-task learning or memory efﬁcient learning.

For the ﬁrst scenario (J1) using the two CIFAR datasets,
we split the number of parameters almost evenly along the
feature map dimension and assigned the ﬁrst half and all of
the parameters to the ﬁrst and second task, respectively, for
PackNet+ and NestedNet. Our architecture contains two
deep virtual networks (DVNs), and each DVN consists of
two units (and two levels of hierarchy) by splitting a set of
feature maps in every layer into two subsets evenly through-
out all associated layers. Here, each stand-alone unit has
25% of the parameter density, since inter-connected param-
eters between the two units are ignored (see Section 4.2).
For this scenario, WRN-32-4 [36] was used for all com-
pared approaches. Table 1 shows the results of the com-
pared approaches. Our approach gives four evaluations ac-
cording to tasks and memory budgets. Among them, the
evaluations using each stand-alone unit (top) do not com-
promise much on performance compared to those using all
units (bottom) on average. PackNet+ and NestedNet give

Table 1. Results of joint learning on CIFAR-10 (task 1) and
CIFAR-100 (task 2). NO and NT are the number of inference
outputs produced by a method and the total number of parame-
ters of a method, respectively. Baseline results are collected from
two independent networks. Ours provides two different inference
using a single unit (top) and all units (bottom) for each task.

Method

NO

Baseline [36]
PackNet+ [26]
NestedNet [19]

Ours

1
2
2

4

)

%

(
 
y
c
a
r
u
c
c
A

1

0.8

0.6

0.4

0.2

0

NT

Task 1

Task 2 Average
14.8M 94.8% 76.4% 85.6%
7.4M 94.5% 75.3% 84.9%
7.4M 94.7% 76.7% 85.7%
94.6% 75.0% 84.8%
95.1% 77.3% 86.2%

7.4M

Task 1 (Single unit)
Task 1 (All units)
Task 2 (Single unit)
Task 2 (All units)

20

40

60

80

100

120

140

160

180

200

Epoch

Figure 4. Performance curve of the proposed DVNs for the joint
learning on CIFAR-10 (task 1) and CIFAR-100 (task 2).

the comparable performance to our approach, but their max-
imum performance leveraging the whole network capacity
are poorer than ours. Baseline gives comparable perfor-
mance to the multi-task learning approaches, but it requires
2× larger number of parameters in this problem. The av-
erage inference times (and the numbers of parameters) of
our DVN using single and all associated units are 0.11ms
(1.9M) and 0.3ms (7.4M) for a single image, respectively.
We also provide the performance curve of the proposed ap-
proach on the test sets in Figure 4.

4 : 2

4 : 3

Figure 5(a) shows the results for the second scenario (J2)
using Tiny-ImageNet (four tasks). The ratios of parameters
for PackNet+ and NestedNet were 1
4 : 1 from task
1 to task 4, by dividing parameters into four subsets almost
evenly and assigning the ﬁrst j subsets to task j. Our ar-
chitecture contains four DVNs each of which has four units
and four levels of hierarchy. The ratios of parameters in
each hierarchy were 1
16 : 1 for each DVN. All
compared approaches were based on ResNet-42 [13]. As
shown in the ﬁgure, our approach outperforms the competi-
tors under similar memory budgets for all tasks. Moreover,
ours provides additional outputs for different memory bud-
gets, making it highly efﬁcient. Even though NestedNet
has the similar strategy of sharing parameters, it performs
poorer than ours. Unlike the previous example, the base-
line shows unsatisfying results and even requires 4× larger
network storage than ours to perform the same tasks.

16 : 4

16 : 9

In addition, we compared with NestedNet [19] on the
same scenario (J2) for memory efﬁcient inference. Since

2715

)

%

(
 
y
c
a
r
u
c
c
A

)

%

(
 
y
c
a
r
u
c
c
A

75

70

65

60

75

70

65

60

Task 1

Baseline
PackNet+
NestedNet
Ours

0

50

100

Density (%)

Task 3

0

50

100

Density (%)

75

70

)

%

(
 
y
c
a
r
u
c
c
A

65

0

70

65

)

%

(
 
y
c
a
r
u
c
c
A

60

0

Task 2

50

100

Density (%)

Task 4

50

100

Density (%)

75

70

65

)

%

(
 
y
c
a
r
u
c
c
A

60

0

75

70

65

)

%

(
 
y
c
a
r
u
c
c
A

60

0

Task 1

NestedNet (T1)
Ours

50

100

Density (%)

Task 3

NestedNet (T3)
Ours

50

100

Density (%)

75

70

65

)

%

(
 
y
c
a
r
u
c
c
A

60

0

70

65

)

%

(
 
y
c
a
r
u
c
c
A

60

0

Task 2

NestedNet (T2)
Ours

50

100

Density (%)

Task 4

NestedNet (T4)
Ours

50

100

Density (%)

(a) Multi-task learning

(b) Memory efﬁcient learning

Figure 5. Results of joint learning on the Tiny-ImageNet tasks with respect to parameter density ratios (budgets). (a) Multi-task learning:
Each deep virtual network in our approach provides four evaluations with respect to different parameter density ratios for each task, while
other methods produce an evaluation with a ﬁxed budget. Baseline requires four trained networks to achieve the results. (b) Memory
efﬁcient learning: Our deep virtual networks produce 4 × 4 inference outputs within a single trained network, while NestedNet requires
four different trained networks to perform memory efﬁcient inference for the same tasks, respectively. (·) denotes the task ID.

Table 2. Parameter density and speedup of our method with respect
to levels of hierarchy. l(i) denotes the level containing i units.
l(4)

l(3)

No. parameters

Density

Compression rate
Inference time (ms)
Practical speed-up

l(2)

l(1)
1.9M 7.5M 16.8M 29.8M
6.4% 25.2% 56.4% 100%
15.7×
0.18
5.8×

1.8×
0.67
1.6×

4.0×
0.38
2.8×

1×
1.05
1×

NestedNet performs memory efﬁcient inference for a task,
we trained it four times according to the number of tasks.
Whereas, our architecture was trained once and performed
memory efﬁcient inference for all the tasks from our DVNs.
Figure 5(b) shows that our method gained signiﬁcant perfor-
mance improvement over NestedNet for all the tasks. Table
2 summarizes the number of parameters and its associated
speed-ups of the proposed network.

16 : 9

For the third scenario (J3) on three different tasks, a set
of feature maps is divided into three subsets for the com-
pared methods. The ratios of parameters were 4
16 : 1
from task 1 (Tiny-ImageNet) to task 3 (STL-10). Each
DVN has the same density ratios in its hierarchical struc-
ture. ResNet-42 [13] was applied by carefully following
the network design and learning strategy designed for Ima-
geNet [13]. Figure 6 shows the results for the tasks. The
proposed method performs better than the compared ap-
proaches on average under similar parameter density ratios.
While PackNet+ and NestedNet give comparable perfor-
mance to ours for Tiny-ImageNet, they perform poorer than
ours for the other two tasks. Moreover, they produce a sin-
gle output for every task with a ﬁxed parameter density con-

)

%

(
 
y
c
a
r
u
c
c
A

50

40

30

Tiny-ImageNet (64x64)
60

Ours
NestedNet
PackNet+
Baseline
50

0

100
Parameter Density (%)

)

%

(
 
y
c
a
r
u
c
c
A

CIFAR-100 (32x32)

80

70

60

50

STL-10 (96x96)

)

%

(
 
y
c
a
r
u
c
c
A

80

75

70

0

50

100

Parameter Density (%)

0

100
Parameter Density (%)

50

Figure 6. Results on three different datasets (Tiny-ImageNet,
CIFAR-100, and STL-10) of different scales for joint learning.

Table 3. Results of the hierarchical classiﬁcation on CIFAR-100.
(·) denotes the number of classes. NO is the number of inference
outputs produced by a method. Baseline results are collected from
independent networks. NestedNet provides two different results
according to the number of tasks. Our approach performs four dif-
ferent inference, according to the number of parameters and tasks.

No. parameters

Baseline [36]
NestedNet [19]

Ours

NO
−

1
2
4

Task 1 (20)

Task 2 (100)

1.8M

7.4M

1.8M

7.4M

82.1% 84.9% 73.4% 75.7%
83.7%
76.6%
84.1% 86.1% 74.9% 76.9%

−

−

dition, while ours provides multiple outputs under different
density conditions for each dataset. The numbers of pa-
rameters and their inference times of our DVN are 0.65ms
(7.5M), 1.02ms (16.8M), and 1.51ms (29.8M), respectively,
for a single image from STL-10.

4.4. Hierarchical classiﬁcation

As another application of joint learning, we experi-
mented with the scenario (H1), hierarchical classiﬁcation
[34]. The aim is to model multiple levels of hierarchy of

2716

Table 4. Results of the sequential learning on the CIFAR-10 tasks. The proposed architecture contains two deep virtual networks each of
which provides two different evaluations using a single unit (right column) and all the units (left column) for each task.

Method

Feature Extraction [6] DA-CNN [32]

LwF [24] NestedNet [19]

Ours

Task 1
Task 2
Average

96.3%
85.7%
91.0%

96.3%
90.1%
93.2%

95.3%
97.1%
96.2%

93.9%
98.2%
96.05%

95.8%
95.4%
97.7%
98.1%
96.55% 96.95%

Table 5. Results of the sequential learning on the CIFAR-10 (task 1) and CIFAR-100 (task 2) datasets.

Method

Feature Extraction [6] DA-CNN [32]

LwF [24] NestedNet [19]

Ours

Task 1
Task 2
Average

94.9%
53.2%
74.05%

94.9%
57.4%
76.15%

93.4%
77.2%
85.3%

93.1%
77.9%
85.5%

93.4%
93.1%
78.0%
78.7%
85.55% 86.05%

class category for a dataset, and each level is considered as
a task. We evaluated on CIFAR-100 which has two-level
hierarchy of class category as described in Section 4.1. Our
architecture contains two deep virtual networks, and each
contains two units by dividing feature maps equally into
two sets. Thus, it produces four different inference outputs.
We compared with NestedNet [19] which can perform hi-
erarchical classiﬁcation in a single network. The backbone
network was WRN-32-4.

Table 3 shows the results of the applied methods. We
also provide the baseline results by learning an individual
network (WRN-32-2 or WRN-32-4) for the number of pa-
rameters and the number of classes. Overall, our approach
performs better than other compared methods for all cases.
Ours and NestedNet outperform the baseline probably due
to their property of sharing parameters between the tasks
as they are closely related to each other. The proposed ap-
proach produces a larger number of inference outputs than
NestedNet while keeping better performance.

4.5. Sequential learning

We conducted the scenario (S1) which consists of two
sequential tasks based on CIFAR-10, where the old (task
1) and new (task 2) tasks consist of the samples from the
ﬁrst and last ﬁve classes of the dataset, respectively. We
compared our approach with other methods that can per-
form sequential tasks: Feature Extraction [6], LwF [24],
DA-CNN [32] (with two additional fully-connected layers),
and NestedNet [19] (whose low- and high-level of hierarchy
in the network represent old and new tasks, respectively).

The proposed network consists of two units by divid-
ing feature maps into two subsets evenly (each stand-alone
unit has 25% parameter density ratio).
It constructs two
deep virtual networks providing four inference outputs. We
applied the WRN-32-4 architecture for all compared ap-
proaches. Table 4 shows the results of the compared meth-
ods. We observe that the proposed approach outperforms
other approaches. Notably, the results using stand-alone
units are better than others on average. Feature Extrac-
tion and DA-CNN nearly preserve the performance for the

ﬁrst task by maintaining the parameters of the ﬁrst task un-
changed, but their performances give the unsatisfactory re-
sults for the following task. Whereas, the results from LwF
and NestedNet are much better than those mentioned above
for the second task, but their results are worse than ours.

We also applied the proposal to another scenario (S2)
consisting of CIFAR-10 (old, task 1) and CIFAR-100 (new,
task 2). All the compared approaches were performed based
on WRN-32-8. Our DVNs were constructed and trained un-
der the same strategy to (S1). The results of the scenario
are summarized in Table 5. Our result using all units (right
column) gives the best performance on average among the
compared approaches. Moreover, our result using a stand-
alone unit (left column) also performs better than the best
competitors, LwF and NestedNet, which use the same dis-
tillation loss function [15].

5. Conclusion

In this work, we have presented a novel architecture pro-
ducing deep virtual networks (DVNs) to address multiple
objectives with respect to different tasks and memory bud-
gets. Each DVN has a unique hierarchical structure for a
task and enables multiple inference for different memory
budgets. Based on the proposed network, we can adap-
tively choose a DVN and one of its level of hierarchy for
a given task with the desired memory budget. The efﬁcacy
of the proposed method has been demonstrated under vari-
ous multi-task learning scenarios. To the best of our knowl-
edge, this is the ﬁrst work introducing the concept of virtual
networks in deep learning for multi-task learning.

Acknowledgements. This work was supported by the ERC
grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte
EP/M013774/1, EPSRC/MURI grant EP/N019474/1, Basic Sci-
ence Research Program through the National Research Founda-
tion of Korea (NRF) funded by the Ministry of Science and ICT
(NRF-2017R1A2B2006136), and AIR Lab (AI Research Lab)
of Hyundai Motor Company through HMC-SNU AI Consortium
Fund. We would also like to acknowledge the Royal Academy of
Engineering and FiveAI.

2717

References

[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, et al. TensorFlow:
Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467, 2016.

[2] Rich Caruana. Multitask learning. Machine learning,

28(1):41–75, 1997.

[3] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip H.S. Torr. Riemannian walk for incremental
learning: Understanding forgetting and intransigence. In Eu-
ropean Conference on Computer Vision. Springer, 2018.

[4] Adam Coates, Honglak Lee, and Andrew Y. Ng. An analysis
of single layer networks in unsupervised feature learning. In
AISTATS, 2011.

[5] Ronan Collobert and Jason Weston. A uniﬁed architecture
for natural language processing: Deep neural networks with
multitask learning. In International Conference on Machine
Learning, pages 160–167. ACM, 2008.

[6] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,
Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep
convolutional activation feature for generic visual recogni-
tion.
In International Conference on Machine Learning,
2014.

[7] Ross Girshick. Fast R-CNN.

In Proceedings of the IEEE
international conference on computer vision, pages 1440–
1448, 2015.

[8] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
culty of training deep feedforward neural networks. In Pro-
ceedings of the Thirteenth International Conference on Arti-
ﬁcial Intelligence and Statistics, 2010.

[9] Ariel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Hao Wu,
Tien-Ju Yang, and Edward Choi. Morphnet: Fast & simple
resource-constrained structure learning of deep networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2018.

[10] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pe-
dram, Mark A Horowitz, and William J Dally. EIE: efﬁ-
cient inference engine on compressed deep neural network.
In Computer Architecture, 2016 ACM/IEEE 43rd Annual In-
ternational Symposium on, pages 243–254. IEEE, 2016.

[11] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.
In International
Conference on Learning Representations, 2016.

[12] Song Han, Jeff Pool, John Tran, and William Dally. Learning
both weights and connections for efﬁcient neural network.
In Conference on Neural Information Processing Systems,
2015.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE Con-
ference on Computer Vision and Pattern Recognition, 2016.

[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015.

[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. MobileNets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[17] Forrest N Iandola, Song Han, Matthew W Moskewicz,
and Kurt Keutzer.
Khalid Ashraf, William J Dally,
SqueezeNet: AlexNet-level accuracy with 50x fewer pa-
rameters and <0.5 MB model size.
arXiv preprint
arXiv:1602.07360, 2016.

[18] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International Conference on Machine Learn-
ing, 2015.

[19] Eunwoo Kim, Chanho Ahn, and Songhwai Oh. NestedNet:
Learning nested sparse structures in deep neural networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2018.

[20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Uni-
versity of Toronto, 2009.

[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
ImageNet classiﬁcation with deep convolutional neural net-
works. In Conference on Neural Information Processing Sys-
tems, 2012.

[22] Gustav

Larsson, Michael Maire,

and Gregory
Shakhnarovich.
FractalNet: Ultra-deep neural networks
without residuals. In International Conference on Learning
Representations, 2017.

[23] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. In In-
ternational Conference on Learning Representations, 2017.
[24] Zhizhong Li and Derek Hoiem. Learning without forget-
ting. In European Conference on Computer Vision. Springer,
2016.

[25] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and S Yu
Philip. Learning multiple tasks with multilinear relationship
networks.
In Advances in Neural Information Processing
Systems, pages 1594–1603, 2017.

[26] Arun Mallya and Svetlana Lazebnik. PackNet: Adding mul-
tiple tasks to a single network by iterative pruning. In IEEE
Conference on Computer Vision and Pattern Recognition,
2018.

[27] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-
tial Hebert. Cross-stitch networks for multi-task learning.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3994–4003, 2016.

[28] Gerald J Popek and Robert P Goldberg. Formal requirements
for virtualizable third generation architectures. Communica-
tions of the ACM, 17(7):412–421, 1974.

[29] Sebastian Ruder. An overview of multi-task learning in deep

neural networks. arXiv preprint arXiv:1706.05098, 2017.

[14] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In International
Conference on Computer Vision, 2017.

[30] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Inter-
national Conference on Learning Representations, 2015.

2718

[31] Andreas Veit and Serge Belongie.

works with adaptive inference graphs.
arXiv:1711.11503, 2017.

Convolutional net-
arXiv preprint

[32] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Grow-
ing a brain: Fine-tuning by increasing model capacity.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017.

[33] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. Learning structured sparsity in deep neural networks.
In Conference on Neural Information Processing Systems,
2016.

[34] Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh
Jagadeesh, Dennis DeCoste, Wei Di, and Yizhou Yu. HD-
CNN: Hierarchical deep convolutional neural networks for
large scale visual recognition. In IEEE International Con-
ference on Computer Vision, 2015.

[35] Yongxin Yang and Timothy M Hospedales. Trace norm
arXiv preprint

regularised deep multi-task learning.
arXiv:1606.04038, 2016.

[36] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-

works. arXiv preprint arXiv:1605.07146, 2016.

[37] Amir R Zamir, Te-Lin Wu, Lin Sun, William B Shen,
Bertram E Shi, Jitendra Malik, and Silvio Savarese. Feed-
back networks. In IEEE Conference on Computer Vision and
Pattern Recognition, 2017.

[38] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning, 2017.

[39] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou
Tang. Facial landmark detection by deep multi-task learning.
In European Conference on Computer Vision, pages 94–108.
Springer, 2014.

[40] Shlomo Zilberstein. Using anytime algorithms in intelligent

systems. AI magazine, 17(3):73, 1996.

2719

