Detect-to-Retrieve: Efﬁcient Regional Aggregation for Image Search

Marvin Teichmann∗

Andr´e Araujo∗ Menglong Zhu Jack Sim

University of Cambridge, UK

Google AI, USA

mttt2@eng.cam.ac.uk

{andrearaujo, menglong, jacksim}@google.com

Abstract

Retrieving object instances among cluttered scenes efﬁ-
ciently requires compact yet comprehensive regional image
representations. Intuitively, object semantics can help build
the index that focuses on the most relevant regions. How-
ever, due to the lack of bounding-box datasets for objects of
interest among retrieval benchmarks, most recent work on
regional representations has focused on either uniform or
class-agnostic region selection. In this paper, we ﬁrst ﬁll the
void by providing a new dataset of landmark bounding boxes,
based on the Google Landmarks dataset, that includes 94k
images with manually curated boxes from 15k unique land-
marks. Then, we demonstrate how a trained landmark detec-
tor, using our new dataset, can be leveraged to index image
regions and improve retrieval accuracy while being much
more efﬁcient than existing regional methods. In addition, we
introduce a novel regional aggregated selective match kernel
(R-ASMK) to effectively combine information from detected
regions into an improved holistic image representation. R-
ASMK boosts image retrieval accuracy substantially with no
dimensionality increase, while even outperforming systems
that index image regions independently. Our complete image
retrieval system improves upon the previous state-of-the-art
by signiﬁcant margins on the Revisited Oxford and Paris
datasets. Code and data will be released.

1. Introduction

In this paper, we address the image retrieval problem:
given a query image, a system should efﬁciently retrieve
similar images from a database. Image retrieval systems are
usually composed of two main stages: (1) ﬁltering, where an
efﬁcient technique ranks database images according to their
similarity with respect to the query; (2) re-ranking, where a
small number of the most similar database images from the
ﬁrst stage are inspected in more detail and re-ranked.

Traditionally, hand-crafted local features [21, 6] were
coupled to Bag-of-Words-inspired techniques [35, 26, 27,
14, 15, 16, 37] to construct high-dimensional representations
used in the ﬁltering step. Local feature matching and geo-

∗Both authors contributed equally to this work.

R-ASMK

VLAD

R1

R2

R3

R3

R1

R2

n
o
i
t
a
g
e
r
g
g
A

 
l
a
n
o
g
e
R

i

 

 

n
o
i
t
c
a
r
t
x
E
e
r
u
t
a
e
F
 
l
a
c
o
L

n
o
i
t
c
e
t
e
D
n
o
g
e
R
 
+

i

 

Figure 1: Overview of our proposed regional aggregation method. Deep
local features (stars) and object regions (boxes) are extracted from an image.
Regional aggregation proceeds in two steps, using a large codebook of
visual words (red and yellow visual words are depicted): ﬁrst, per-region
VLAD description; second, sum pooling and per-visual word normalization.
Our ﬁnal regionally aggregated image representation can be combined to
selective match kernels and provide improved image similarity estimation:
we refer to this technique as regional aggregated selective match kernels
(R-ASMK). It leverages detected regions to improve image retrieval at no
dimensionality increase when compared to the original ASMK method [37].

metric veriﬁcation [26, 27, 3] (commonly using RANSAC
[8]) have been used as effective re-ranking strategies. Re-
cently, several deep learning techniques have been proposed
for these two stages. Global image representations based on
convolutional neural networks (CNN) can produce compact
embeddings to enable fast similarity computation in the ﬁl-
tering step [5, 4, 39, 1, 9, 30]. Local image representations
can also be extracted using CNNs, suitable to re-ranking via
spatial matching and geometric veriﬁcation [25, 24, 23].

Today’s image retrieval systems tend to fail when relevant
objects do not occupy a large enough fraction of database

15109

images, typically in cluttered scenes. Often, these objects
produce local features that can be used to ﬁnd local matches
against the query in the re-ranking stage. However, such
cluttered images usually fail to reach the re-ranking stage,
since their initial representation does not lead to high simi-
larity when compared to the query during the ﬁltering stage.
The most common solution to estimate an improved simi-
larity with respect to the query image is to extract and sepa-
rately store image representations for regions-of-interest in
the database, using a ﬁxed regional grid [2, 31] or a class-
agnostic detector [36, 17]. However, the existing region
selection techniques produce a large number of irrelevant
regions. In a recent large-scale experimental image retrieval
evaluation, Radenovic et al. [28] concluded that such re-
gional search approaches impose too high of a cost in terms
of memory and latency, with only small accuracy gains.

Contributions.
(1) Our ﬁrst contribution is aimed at im-
proving region selection: we introduce a dataset of manually
boxed landmark images, with 94k images from 15k unique
classes, and we show that detectors can be trained for ro-
bust landmark localization. (2) Our second contribution is
to leverage the trained detector and produce more efﬁcient
regional search systems, which improve accuracy for small
objects with only a modest increase to the database size –
much more efﬁciently than previously proposed techniques.
(3) In our third contribution, we propose regional aggregated
match kernels to leverage selected image regions and pro-
duce a discriminative image representation, illustrated in
Fig. 1. This new representation outperforms regional search
systems signiﬁcantly, while at the same time being more efﬁ-
cient: only one descriptor needs to be stored per image. Our
image retrieval system outperforms previously published
results by 9.3% absolute mean average precision on the
Revisited Oxford-Hard dataset, and 1.9% on the Revisited
Paris-Hard dataset [28]. Towards the goal of facilitating
further research, we will release both code and data.

2. Related Work
Datasets. To the best of our knowledge, no manually cu-
rated datasets of landmark bounding boxes exist. Gordo et
al. [9] use SIFT [21] matching to estimate boxes in landmark
images. Such boxes are biased towards the feature extrac-
tion and matching technique, and may contain localization
errors. Their dataset contains 49k boxed images, from 586
landmarks. In comparison, we use human raters to anno-
tate the regions of interest, and produce 94k boxed images
from 15k landmarks. The OpenImages dataset [19] contains
9M images, annotated with generic object bounding boxes.
Some of them may be considered landmarks, for example:
buildings, towers, skyscrapers, billboards. However, these
classes make for a small fraction of the entire dataset.

Regional search and aggregation. Region selection has
been explored in image retrieval systems. They have been

used with two different purposes: (i) regional search: se-
lected regions are encoded independently in the database,
allowing for retrieval of subimages; (ii) regional aggregation:
selected regions are used to improve image representations.
In the following, we review these two types of approaches.
Regional search. Many papers propose to describe re-
gions using VLAD [15] or Fisher Vectors [16]: Arandjelovic
and Zisserman [2] use a multi-scale grid to extract 14 re-
gions per image; Tao et al. [36] use Selective Search [40]
with thousands of regions per image; Kim et al. [17] use max-
imally stable extremal regions (MSER) [22]. Razavian et al.
[31] use a multi-scale grid with 30 regions per image, and
compute the similarity of two images by taking into account
the distances between all region pairs. Iscen et al. [13, 12]
leverage multi-scale grids in conjunction with CNN features
[29], to enable query expansion via diffusion. More recently,
Radenovic et al. [28] performed a comprehensive evaluation
of retrieval techniques and concluded that existing regional
search methods may improve recognition accuracy, how-
ever at signiﬁcantly larger memory and complexity costs. In
contrast, our Detect-to-Retrieve framework aims at efﬁcient
regional search via the use of a custom trained detector.

Regional aggregation. Tolias et al. [39] leverage the grid
structure from [31] to pool pretrained CNN features [18, 34]
into compact representations; approximately 20 regions are
selected per image. Radenovic et al. [29] build upon [39] by
re-training features on a dataset collected in an unsupervised
manner. Gordo et al. [9] train a region proposal network
[32] from semi-automatic bounding box annotations, to re-
place the grid from [39]. Hundreds of regions per image are
considered in this case. Our work departs from these papers
by using a small set of regions (fewer than 5 per image), and
by formulating regional aggregation as a new match kernel
(instead of regional sum-pooling as in [39, 9]).

3. Google Landmark Boxes Dataset

In this section, we introduce our newly collected Google
Landmark Boxes dataset, describing the manual annotation
process. Our work builds upon the recent Google Landmarks
dataset (GLD) [25], whose training set contains 1.2M im-
ages of 15k unique landmarks, with a wide variety of objects
including buildings, monuments, bridges, statues as well as
natural landmarks such as mountains, lakes and waterfalls.
Each image in this dataset is considered to only depict
one landmark. In some cases, a landmark may consist of a
set of buildings: for example, skylines, which are common
in this dataset, are considered as a single landmark. Since
GLD is collected in a semi-automatic manner considering
popular touristic locations, it is sometimes ambiguous what
the landmark of interest may be. When collecting bounding
box annotations, our goal is to capture the most prominent
landmark in the image, according to the fact that each image
is only assigned one landmark label. Each box should reﬂect

5110

Figure 2: Examples of annotated images from our Google Landmark Boxes dataset. A box is drawn around the most prominent landmark depicted in the
image. The dataset contains a wide variety of objects, ranging from man-made to natural landmarks.

the main object (or set of objects) which is showcased in
each dataset image. For this reason, we instructed human
operators to draw at most one box per image.

One of the main challenges in such a ﬁne-grained dataset
is the inherent long tail of number of image samples per
class. In GLD, some landmarks are associated to several
thousands of images, while for about half of the classes only
10 or fewer images are provided. Our goal is to represent
landmarks in a balanced manner in our new dataset, such
that trained detectors are able to localize a wide variety
of objects. For this reason, we ﬁrst separate part of the
1.2M training set into a validation set. We randomly select
four training and four validation images per landmark. In
total, this yields 58k and 36k boxed images for training and
validation, respectively. Note that this means that for about
40% of landmarks, all available images are annotated.

Examples of annotated images are shown in Fig. 2. In
some cases, it is not possible to identify a prominent land-
mark (see Fig. 3): the landmark of interest may be occluded,
or the image may actually show the surroundings of a land-
mark. We remove such corner cases from our dataset (this
applied to about 8% of images which were initially selected).
We will make all annotations public to stimulate progress in
the area of landmark recognition and image retrieval.

4. Regional Search and Aggregation

We present techniques that enhance image retrieval per-
formance by utilizing bounding boxes predicted by a trained
landmark detector. In particular, our approach builds on top
of deep local features (DELF) [25] and aggregated selective
match kernels (ASMK) [37], which were recently shown to
achieve state-of-the-art performance on a large-scale image
retrieval benchmark [28].

4.1. Background

We brieﬂy review the aggregated match kernel frame-
work by Tolias et al. [37]. An image X is described by a
set X = {x1, x2, . . . , xM } containing M local descriptors,
each of dimension D. A codebook C comprising C visual
words, learned using k-means, is used to quantize the de-
scriptors. Denote Xc = {x ∈ X : q(x) = c} as the subset
of descriptors from X which are assigned to visual word c
by the nearest neighbor quantizer q(x).

According to this framework, the similarity between two
images X and Y , represented by local descriptor sets X and
Y, can be computed as:

K(X, Y ) = γ(X )γ(Y)Xc∈C

σ(Φ(Xc)T Φ(Yc))

(1)

where Φ(X ) is an aggregated vector
representation,
σ(.) denotes a scalar selectivity function and γ(X ) =

(cid:0)Pc σ(Φ(Xc)T Φ(Xc))(cid:1)−1/2

is a normalization factor. This
formulation encompasses popular local feature aggregation
techniques, such as Bag-of-Words [35], VLAD [15] and
ASMK [37].

In particular, for VLAD, σ(u) = u and Φ(Xc) corre-
x − q(x).
For ASMK, σ(u) corresponds to a thresholded polynomial
selectivity function

sponds to an aggregated residual V (Xc) =Px∈Xc

σ(u) =(sign(u)|u|α,

0,

if u > τ

otherwise

(2)

where usually α = 3 and τ = 0; and Φ(Xc) corre-
sponds to a normalized aggregated residual ˆV (Xc) =
V (Xc)/ kV (Xc)k.

5111

Figure 3: Examples of Google Landmarks dataset images which do not depict a prominent landmark. In such cases (about 8% of images), no boxes were
drawn, and the images were not included in the Google Landmark Boxes dataset.

4.2. Regional Search

In this section, we consider image retrieval systems where
regional descriptors are stored independently in the database.
Denote the query image as X, and the database of N im-
ages as {Y (n)}, n = 1, 2, . . . , N . We are mainly interested
in the experimental conﬁguration where a query contains
a well-localized region-of-interest (i.e., the query in prac-
tice contains only one region), which is a common setting
in image retrieval. For the n-th database image, regions
rn = 1, . . . , Rn are predicted by a landmark detector, deﬁn-
ing the subimages {Y (n,rn)}. We denote Y (n,1) = Y (n)
as the subimage corresponding to the original image, and
always consider it as a valid region. To leverage uncluttered
representations, we store aggregated descriptors indepen-
n=1 Rn

dently for each subimage, which leads to a total ofPN

items in the database.

To compute the similarity between the query X and a
database image Y (n), we consider max-pooling or average-
pooling individual regional similarities, respectively:

simM AX (X, Y (n)) = maxr=1,...,Rn K(cid:16)X , Y (n,r)(cid:17) (3)

Rn

simAV G(X, Y (n)) =

1
Rn

Xr=1

K(cid:16)X , Y (n,r)(cid:17)

(4)

Max-pooling corresponds to assigning a database image’s
score considering only its highest-scoring subimage. Av-
erage pooling aggregates contributions from all subimages.
These two variants are compared in Sec. 5.

4.3. Regional Aggregated Match Kernels

Storing descriptors of each region independently in the
database incurs additional cost for both storage and search
computation. In this section, we consider utilizing the de-
tected bounding boxes to instead improve the aggregated rep-
resentations of database images – producing discriminative
descriptors at no additional cost. We extend the aggregated
match kernel framework of Tolias et al. [37] to regional
aggregated match kernels, as follows.

We start by noting that the average pooling similarity

Eq. (4) can be rewritten as:

simAV G(X, Y (n)) =

γ(X )Xc Xr

γ(Y (n,r))

Rn

σ(cid:16)Φ(Xc)T Φ(Y (n,r)

c

)(cid:17) (5)

Simple regional aggregation. For VLAD, this can be fur-
ther expanded as:

sim(R-VLAD)(X, Y (n))

γ(Y (n,r))

Rn

V (Xc)T V (Y (n,r)

c

)

γ(Y (n,r))

γ(X )V (Xc)T Xr

VR(Xc)T VR({Y (n,r)

c

Rn

}r)

V (Y (n,r)

c

)

(6)

(7)

= γ(X )Xc Xr
=Xc
=Xc

where we deﬁne

VR({Y (n,r)

c

}r) =

1

Rn Xr

γ(Y (n,r))V (Y (n,r)

c

)

(8)

Using this deﬁnition, note that VR(Xc) = γ(X )V (Xc).
This derivation indicates that average pooling of regional
VLAD similarities can be performed using aggregated re-
gional descriptors and does not require storage of each re-
gion’s representation separately1. We refer to this simple
regional aggregated kernel as R-VLAD.

A similar derivation can be obtained for ASMK in the
case where σ(.) is the identity function (i.e., no selectivity
is applied), by replacing V (Xc) by ˆV (Xc) in Eq. (6). A
straightforward matching kernel using this idea would apply
the selectivity function when comparing the query ASMK
representation against this aggregated representation. We
refer to this aggregation variant as Naive-R-ASMK.

Both the R-VLAD and Naive-R-ASMK kernels present
an important problem when using many detected regions
per image and large codebooks. For a given image region,

1Another way to see that this applies to VLAD kernels is to note
that VLAD similarity is computed via a simple inner product, and that
the average inner product with a set of vectors equals the inner product
with the set average; i.e., for vector x and set {yn}, 1
N Pn xT yn =
xT (cid:0) 1

N Pn yn(cid:1).

5112

most visual words will not be associated to any local feature,
leading to many all-zero residuals for the region. For visual
words that correspond to visual patterns observed in only
a small number of regions, this will lead to substantially
downweighted residuals. We propose to ﬁx this weakness
by developing the R-ASMK kernel as follows, inspired by
the changes introduced by the original ASMK with respect
to VLAD.

R-ASMK. We deﬁne the R-ASMK similarity between a
query and a database image as:

sim(R-ASMK)(X, Y (n)) =

Xc

c

σ(cid:16) ˆVR(Xc)T ˆVR({Y (n,r)
}r) = VR({Y (n,r)
VR({Y (n,r)

}r)

}r)

c

c

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

where ˆVR({Y (n,r)

c

}r)(cid:17)

(9)

is the normalized

regionally aggregated residual corresponding to visual word
c.

R-AMK. The kernels we presented in this section can be
regarded as different instantiations of a general regional
aggregated match kernel (R-AMK), deﬁned as follows:

KR(X, Y ) =Xc∈C

σ(cid:16)ΦR({X (r)

c }r)T ΦR({Y (r)

c }r)(cid:17) (10)

where {X (r)
c }r denotes the sets of local descriptors quan-
tized to visual word c, from each region of X. ΦR special-
izes to VR for R-VLAD, and to ˆVR for R-ASMK. Note that
this deﬁnition involves regional aggregation for both images,
while in this work we focus on the asymmetric case where
regional aggregation is applied to the database image only.
The asymmetric case is more relevant when the query im-
age is itself a well-localized region-of-interest, which is a
common setup in image retrieval benchmarks.

Binarization. For codebooks with a large number of vi-
sual words, the storage cost for such aggregated representa-
tions may be prohibitive. Binarization is an effective strat-
egy to allow scalable retrieval in these cases. We adopt
a similar binarization strategy as [37], where a binarized
version of ΦR can be obtained by the elementwise func-
tion b(x) = +1 if x > 0, −1 otherwise. We denote the
binarized version by a ⋆ superscript (e.g., R-ASMK⋆ is the
binarized version of R-ASMK).

5. Experiments

We present two types of experiments: ﬁrst, landmark
detection, to assess the quality of object detector models
trained on the new dataset. Second, we utilize the detected
landmarks to enhance image retrieval systems.

5.1. Landmark Detection

We train two types of detection models on the bounding
box data we have collected and described in Sec. 3: a sin-
gle shot Mobilenet-V2 [33] based SSD detector [20] and a
two stage Resnet-50 [10] based Faster-RCNN [32]. Stan-
dard object detection evaluation metric Average Precision
(AP) measured at 50% Intersection-over-Union ratio is used
during evaluation. Both models reach about 85% AP on
the validation set within 500k steps (85.61%, 84.37% re-
spectively). The models are trained with publicly available
Tensorﬂow Object Detection API [11]. The results indicate
that accurate landmark localization can be trained using our
dataset. The Mobilenet-V2-SSD variant runs at 27ms per
image, while the Resnet-50-Faster-RCNN runs at 89ms, both
numbers on a TitanX GPU.

5.2. Image Retrieval

We perform regional search and regional aggregation ex-
periments. The following describes the experimental setup.

Datasets. We use the Oxford [26] and Paris [27] datasets,
which have recently been revisited to correct annotation mis-
takes, add new query images and introduce new evaluation
protocols [28]; the datasets are referred to as ROxf and
RPar, respectively. There are 70 query images for each
dataset, with 4993 (6322) database images in the ROxf
(RPar) dataset. We report results on the Medium and Hard
setups; for ablations, we focus more speciﬁcally on the Hard
setup. Performance is measured using mean average preci-
sion (mAP) and mean precision at rank 10 (mP@10). We
also perform large-scale experiments using the R1M distrac-
tor set [28], which contains 1,001,001 images.
Image representation. We use the following setup in our
experiments, except where indicated otherwise. The released
DELF model [25] (pre-trained on the dataset from [9]) is
used, with the default conﬁguration (maximum of 1000 fea-
tures per region are extracted, with a required minimum
attention score of 100), except that the feature dimension-
ality is set to 128 as in previous work [28]. A 1024-sized
codebook is used when computing aggregated kernels; as
common practice, codebooks are trained on ROxf for re-
trieval experiments on RPar, and vice-versa. We focus on
improving the core image representations for retrieval, and
do not consider query expansion (QE) [7] techniques such
as Hamming QE [38], α QE [30] or diffusion [13, 12]; these
methods could be incorporated to our system to obtain even
stronger retrieval performance.

Region selection techniques. For our Detect-to-Retrieve
(D2R) framework, we adopt the trained Faster R-CNN de-
tector described in Sec. 5.1. We compare against previously
proposed region selection techniques for image retrieval: the
uniform grid from [31, 39] (denoted RMACB, for “RMAC
boxes”) and Selective Search (SS) [40, 36]. To vary the
number of regions per image, we do as follows: (i) for D2R,

5113

(a) Regional search evaluation.

(b) Regional aggregation evaluation.

Figure 4: Regional search and aggregation evaluations of different image representations, on ROxf-Hard. (a) Regional search: each regional representation
is stored independently in the database, leading to increased memory requirements. Our D2R-ASMK variants achieve signiﬁcant improvements over the
single-image baseline while requiring substantially fewer boxes compared to other region selection approaches. (b) Regional aggregation: each region
contributes to the aggregated representation for the entire image. The memory requirements are identical for the single-image baseline that does not use
regions. Our D2R-R-ASMK variants leverage the different landmark regions to compose a strong image representation, which is even more effective than
storing each regional representation separately.

Method

ASMK⋆

D2R-
ASMK⋆

D2R-R-
ASMK⋆

Det.

Thresh. mAP

ROxf-Hard RPar-Hard
Size

Size mAP

—

0.7
0.5
0.3
0.1

0.7
0.5
0.3
0.1

38.3

39.2
39.7
40.2
40.7

41.0
41.5
42.0
41.4

1

2.1
2.4
2.9
4.1

1
1
1
1

54.2

56.0
56.2
56.3
56.7

56.2
56.2
56.3
56.8

1

2.2
2.4
2.9
3.9

1
1
1
1

Table 1: Retrieval mAP and relative database size for the different region-
based techniques introduced in this work, on the ROxf-Hard and RPar-
Hard datasets, as a function of the landmark detector threshold used for
region selection. D2R-ASMK⋆ uses max-pooling similarity from Eq. (3).
The performances of both D2R-ASMK⋆ and D2R-R-ASMK⋆ tend to im-
prove as the detection threshold decreases (more regions are selected).
D2R-R-ASMK⋆ outperforms D2R-ASMK⋆ consistently, with a smaller
memory footprint.

we vary the landmark detector threshold; (ii) for RMACB,
we sweep the number of levels from 1 to 3; (iii) for SS, we
select the top {1, 2, 5, 10} boxes per image (as in this case
there are no conﬁdence scores associated to regions). For
all region selection techniques, we add the original image as
one of the selected regions.

Implementation details. We implemented the aggregated
kernel framework in Python. As a comparison against the
reference MATLAB implementation [37], our ASMK⋆ with
a 1024-sized codebook and DELF features obtains 37.91%
mAP in the ROxf-Hard dataset, while the reference imple-
mentation obtains 37.08%. Note that the reference binarized

implementation uses a similar conﬁguration as Hamming
Embedding (HE) [14], with a projection matrix before bi-
narizarion, residuals computed with respect to the median,
and IDF. We did not ﬁnd consistent improvements using
these, so we use the simpler version as described in Sec. 4.
Similarly, the reference implementation uses multiple visual
word assignments, but our preliminary experiments show
improved results using single assignment, making retrieval
faster and simpler – therefore we adopt single assignment in
our experiments. We extend the implementation to support
our regional search and aggregation techniques, and plan to
release code to foster reproducibility of results.

5.2.1 Regional Search

We compare aggregated match kernels, region selection tech-
niques and similarity computation methods on the ROxf-
Hard dataset. When performing regional search, multiple
regions are selected per image and stored independently in
the database, leading to increased memory cost. Fig. 4a
presents results for ASMK variants, where all techniques use
max-pooling similarity from Eq. (3), except for D2RAV G-
ASMK⋆, which uses average-pooling similarity from Eq. (4).
Combining our proposed D2R regions with ASMK enhances
mAP by 3.23% when using an average of 4.05 regions per
image.

We compare the different region selection approaches
using ASMK⋆. Our D2R-ASMK⋆ achieves 40.65% mAP
when using 4.05 regions per image, improvement of 2.31%
over the single-image ASMK⋆ baseline. Other region se-
lection approaches improve retrieval accuracy, but with sig-
niﬁcantly larger memory requirements. RMACB-ASMK⋆
requires 9.08 regions/image to achieve 40.43% mAP (this is

5114

5101520Regions per image38394041424344mAP (%)D2R-ASMKD2R-ASMKRMACB-ASMKSS-ASMKD2RAVG-ASMK5101520Regions per image38394041424344mAP (%)D2R-R-ASMKD2R-R-ASMKRMACB-R-ASMKSS-R-ASMKMethod

Medium

Hard

ROxf

mAP

mP@10

ROxf+R1M
mP@10

mAP

RPar

mAP

mP@10

RPar+R1M
mP@10
mAP

ROxf

mAP

mP@10

ROxf+R1M
mP@10

mAP

RPar

mAP

mP@10

RPar+R1M
mP@10
mAP

AlexNet-GeM [30]
VGG16-GeM [30]
ResNet101-GeM [30]
ResNet101-R-MAC [9]
HesAff-rSIFT-ASMK⋆ [37]
HesAff-rSIFT-ASMK⋆+SP [37]
HesAff-HardNet-ASMK⋆+SP [24]
DELF-ASMK⋆+SP [25, 28]

DELF-ASMK⋆ (reimpl.)
DELF-D2R-R-ASMK⋆ (ours)

— DELF-GLD (ours)

43.3
61.9
64.7
60.9
60.4
60.6
65.6
67.8

65.7
69.9
73.3

62.1
82.7
84.7
78.1
85.6
86.1
90.2
87.9

87.9
89.0
90.0

24.2
42.6
45.2
39.3
45.0
46.8
–
53.8

–
–
61.0

42.8
68.1
71.7
62.1
76.0
79.6
–
81.1

–
–
84.6

DELF-ASMK⋆+SP (reimpl.)
DELF-D2R-R-ASMK⋆+SP (ours)

— DELF-GLD (ours)

68.9
71.9
76.0

90.9
91.3
93.4

–
–
64.0

–
–
87.7

58.0
69.3
77.2
78.9
61.2
61.4
65.2
76.9

77.1
78.7
80.7

76.6
78.0
80.2

91.6
97.9
98.1
96.9
97.9
97.9
98.9
99.3

98.7
99.0
99.1

98.7
99.4
99.1

29.9
45.4
52.3
54.8
42.0
42.3
–
57.3

–
–
60.2

–
–
59.7

84.6
94.1
95.3
93.9
95.3
95.3
–
98.3

–
–
97.9

17.1
33.7
38.5
32.4
36.4
36.7
41.1
43.1

41.0
45.6
47.6

26.2
51.0
53.0
50.0
56.7
57.0
59.7
62.4

57.9
61.9
64.3

9.4
19.0
19.9
12.5
25.7
26.9
–
31.2

–
–
33.6

11.9
29.4
34.9
24.9
42.1
45.3
–
50.7

–
–
53.7

–
–
99.0

46.6
48.5
52.4

66.7
66.7
70.9

–
–
38.1

–
–
61.3

29.7
44.3
56.3
59.4
34.5
35.0
38.5
55.4

54.6
57.7
61.3

52.2
54.0
58.6

67.6
83.7
89.1
86.1
80.6
81.7
87.9
93.4

90.9
93.0
93.4

87.6
87.6
91.0

8.4
19.1
24.7
28.0
16.5
16.8
–
26.4

–
–
29.9

–
–
29.4

39.6
64.9
73.3
70.0
63.4
65.3
–
75.7

–
–
82.4

–
–
83.9

Table 2: Comparison of proposed techniques against state-of-the-art methods, on the ROxford (ROxf) and RParis (RPar) datasets (and their large-scale
extensions ROxf+R1M and RPar+R1M), with Medium and Hard evaluation protocols. Previously published results are presented in the ﬁrst block of rows.
The second and third block of rows present our experimental results, considering systems without and with spatial veriﬁcation (SP), respectively. In this
experiment, we use codebooks with 65k visual words, to make our results comparable to previous work [28]. DELF-GLD indicates a version of DELF which
we re-trained on the Google Landmarks dataset. Our methods achieve equal or improved performance for all evaluation protocols, datasets and metrics.

0.22% mAP below the previously mentioned D2R-ASMK⋆
operating point, despite requiring 2.24× more memory).
SS-ASMK⋆ beneﬁts from some regions, while performance
decreases when a large number of regions are selected, since
many of those regions are irrelevant.

Average pooling of individual regional similarities im-
proves upon the single-image baseline signiﬁcantly, at
low overhead memory requirements: D2RAV G-ASMK⋆
achieves 40.35% mAP with only 1.96× storage cost. Note
that in this case performance drops signiﬁcantly as more
regions are added, since irrelevant regional similarities are
added to the ﬁnal image similarity. We also experimented
with a D2R-VLAD representation: mAP improves from
30.17% (single-image) to 33.87% (2.87 regions/image).

Tab. 1 further presents D2R-ASMK⋆ results on the RPar-
Hard dataset. Regional search enables 2.5% mAP improve-
ment at 3.9 regions/image. Note that our D2R approach
is effective even if the landmarks in the Google Landmark
Boxes dataset present much larger variability than the land-
marks encountered in the ROxf/RPar datasets.

5.2.2 Regional Aggregated Match Kernels

In this section, we evaluate the proposed regional aggregated
match kernels. In this experiment, region selection is used
to produce an improved image representation, with no in-
crease in the aggregated descriptor dimensionality. Fig. 4b
compares different aggregation methods and region selection
approaches, on the ROxf-Hard dataset. Both our proposed
D2R-R-ASMK and D2R-R-ASMK⋆ variants achieve sub-
stantial improvements compared to their baselines which do
not use boxes for aggregation: 3.85% and 3.65% absolute
mAP improvements, respectively. We also compare our D2R
approach against other region selection methods. RMACB
and SS improve upon the baseline, however with limited

gain of at most 1.5% mAP.

More interestingly, our proposed kernels outperform even
the regional search conﬁguration where each region is in-
dexed separately in the database. Tab. 1 compiles experi-
mental results on ROxf-Hard and RPar-Hard. Our D2R-R-
ASMK⋆ method outperforms the best regional search variant
on both datasets, respectively by 1.3% and 0.1% absolute
mAP, with storage savings of 4.1× and 3.9×.

In another ablation experiment, we assess the perfor-
mance of simpler regional aggregation methods: R-VLAD
and Naive-R-ASMK. We use the trained detector to select re-
gions. For R-VLAD, mAP on ROxf improves from 30.17%
(single-image) to 30.91% when using 2.4 regions per image,
but degrades quickly as more regions are considered. In
particular, when setting a very low detection threshold (0.01)
to obtain 10.2 regions per image, performance degenerates
to 16.46% mAP – this agrees with the intuition that a large
number of regions is detrimental to R-VLAD. For Naive-R-
ASMK, no improvement is obtained when detected regions
are used: mAP drops from 39.72% to 31.42% when 1.96 re-
gions per image are used, and similarly degenerates to 9.2%
when using 10.2 regions per image. In comparison, using the
same detection threshold of 0.01, R-ASMK⋆ obtains 41.6%
mAP, i.e., performance is high even if using a large number
of regions, due to the improved aggregation technique.

5.2.3 Comparison Against State-of-the-Art

We compare our D2R-R-ASMK⋆ technique against state-
of-the-art image retrieval systems. To make our system
comparable with previously published results [28], for this
experiment we use a codebook with 65k visual words. We
also further experiment with re-training the DELF local fea-
ture on the Google Landmarks dataset (denoted as DELF-
GLD). Spatial veriﬁcation (SP) is used to re-rank the top 100

5115

Query

ASMK*

AP: 28.2% 

D2R-ASMK*

D2R-R-ASMK*

Query

ASMK*

D2R-ASMK*

D2R-R-ASMK*

AP: 29.2% 

AP: 35.4% 

AP: 51.4% 

AP: 38.7% 

AP: 56.7% 

AP: 9.9% 

AP: 9.1% 

AP: 22.8% 

AP: 14.8% 

AP: 23.2% 

AP: 16.3% 

Figure 5: Qualitative results for ASMK⋆ (baseline single-image method), D2R-ASMK⋆ (regional search) and D2R-R-ASMK⋆ (regional aggregation) on
ROxf-Hard. Four queries are presented, with their regions-of-interest highlighted. For each method, we show the ﬁrst ranked image where the methods
disagree. Red borders indicate incorrect results, and green borders indicate correct results. For D2R-ASMK⋆, we box the region used for the result (or leave
unboxed if the region corresponds to the entire image). For D2R-R-ASMK⋆, we box all regions used for aggregation. We also present average precision (AP)
for each method and query.

database images (we use RANSAC with an Afﬁne model).
Table 2 presents experimental results on ROxf and RPar,
using the Medium and Hard protocols, also including the
large-scale setup with R1M. Our proposed D2R-R-ASMK⋆
representation by itself, without spatial veriﬁcation, already
improves mAP when comparing against all previously pub-
lished results. SP further boosts performance by about 3%
mAP on ROxf; surprisingly, it slightly degrades perfor-
mance on the RPar dataset. Re-training DELF on GLD
improves performance by around 4%. Our best results im-
prove upon the previous state-of-the-art by 8.2% mAP on
ROxf-Medium, 1.8% mAP on RPar-Medium, 9.3% mAP
on ROxf-Hard and 1.9% in RPar-Hard (with similar gains
in the large-scale setup).
Memory. Our DELF-D2R-R-ASMK⋆ descriptors have the
exact same dimensionality as DELF-ASMK⋆. However,
DELF-ASMK⋆ is sparser and consumes less memory in
practice: 10.3GB, compared to 27.6GB for DELF-D2R-R-
ASMK⋆, in the large-scale ROxf+R1M dataset. This is still
much less than other local feature based approaches; e.g.
HesAff-rSIFT-ASMK⋆ requires 62GB [28], and HesAffNet-
HardNet++-ASMK⋆ [24] requires approximately 86.8GB.

5.2.4 Discussion

Our experiments demonstrate that selecting relevant image
regions can help boost image retrieval performance signif-
icantly. In our regional aggregation method, the detected
regions allow for effective re-weighting of local feature con-
tributions, emphasizing relevant visual patterns in the ﬁnal
image representation. Note, however, that it is crucial to
perform both region selection and regional aggregation in
a suitable manner. If the selected regions are not relevant
to the objects of interest, regional aggregation cannot be
very effective, as shown in Fig. 4b. Also, our experiments
with naive versions of regional aggregation indicate that the
aggregation needs to be performed in the right way: this is
related to the poor R-VLAD and Naive-R-ASMK results.

It may initially seem unintuitive that the regional search
method underperforms when compared to our regional ag-
gregation technique. However, this can be understood by
observing some retrieval result patterns, which are presented
in Fig. 5. The addition of separate regional representations
to the database may help retrieval of relevant small objects
in cluttered scenes, as illustrated with the successful bottom-
right D2R-ASMK⋆ retrieved image. However, it also in-
creases the chances of ﬁnding localized regions which are
similar but do not correspond to the same landmark, as illus-
trated with the top two cases.

Regional aggregation, on the other hand, can help re-
trieval by re-balancing the visual information presented in
an image. The top-right D2R-R-ASMK⋆ result shows a
database image where the detected boxes do not precisely
cover the query object; instead, several selected regions
cover it, and consequently its features are boosted. A simi-
lar case is illustrated in the bottom-left example, where the
main detected region in the database image does not cover
the object of interest entirely. The features inside the main
box are boosted but those outside are also used, generating a
more suitable representation for image retrieval.

6. Conclusions

In this paper, we present an efﬁcient regional aggregation
method for image retrieval. We ﬁrst introduce a dataset of
landmark bounding boxes, and show that landmark detectors
can be trained and leveraged for extracting regional repre-
sentations. Regional search using our detectors not only
provides superior retrieval performance but also much better
efﬁciency than existing regional methods. In addition, we
propose a novel regional aggregated match kernel framework
that further boosts the retrieval accuracy. Our full system
achieves state-of-the-art performance by a large margin on
two image retrieval datasets.

5116

References

[1] R. Arandjelovi´c, P. Gronat, A. Torii, T. Pajdla, and J. Sivic.
NetVLAD: CNN Architecture for Weakly Supervised Place
Recognition. In Proc. CVPR, 2016. 1

[2] R. Arandjelovic and A. Zisserman. All About VLAD. In

Proc. CVPR, 2013. 2

[3] Y. Avrithis and G. Tolias. Hough Pyramid Matching: Speeded-
up Geometry Re-ranking for Large Scale Image Retrieval.
IJCV, 2014. 1

[4] A. Babenko and V. Lempitsky. Aggregating Local Deep

Features for Image Retrieval. In Proc. ICCV, 2015. 1

[5] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky.

Neural Codes for Image Retrieval. In Proc. ECCV, 2014. 1
[6] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool. Speeded-Up

Robust Features (SURF). CVIU, 2008. 1

[7] O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman.
Total Recall: Automatic Query Expansion with a Generative
Feature Model for Object Retrieval. In Proc. ICCV, 2007. 5
[8] M. Fischler and R. Bolles. Random Sample Consensus: A
Paradigm for Model Fitting with Applications to Image Anal-
ysis and Automated Cartography. Communications of the
ACM, 1981. 1

[9] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep Image
Retrieval: Learning Global Representations for Image Search.
In Proc. ECCV, 2016. 1, 2, 5, 7

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning

for Image Recognition. In Proc. CVPR, 2016. 5

[11] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.
Speed/Accuracy Trade-offs for Modern Convolutional Object
Detectors. In Proc. CVPR, 2017. 5

[12] A. Iscen, Y. Avrithis, G. Tolias, T. Furon, and O. Chum. Fast
Spectral Ranking for Similarity Search. In Proc. CVPR, 2018.
2, 5

[13] A. Iscen, G. Tolias, Y. Avrithis, T. Furon, and O. Chum.
Efﬁcient Diffusion on Region Manifolds: Recovering Small
Objects with Compact CNN Representations. In Proc. CVPR,
2017. 2, 5

[14] H. J´egou, M. Douze, and C. Schmid. Hamming Embedding
and Weak Geometric Consistency for Large Scale Image
Search. In Proc. ECCV, 2008. 1, 6

[15] H. J´egou, M. Douze, C. Schmidt, and P. Perez. Aggregating
Local Descriptors into a Compact Image Representation. In
Proc. CVPR, 2010. 1, 2, 3

[16] H. J´egou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and
C. Schmid. Aggregating Local Image Descriptors into Com-
pact Codes.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2012. 1, 2

[17] H. J. Kim, E. Dunn, and J.-M. Frahm. Predicting Good Fea-
tures for Image Geo-Localization Using Per-Bundle VLAD.
In Proc. ICCV, 2015. 2

[18] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet Classi-
ﬁcation with Deep Convolutional Neural Networks. In Proc.
NIPS, 2012. 2

[19] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin,
J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig,

and V. Ferrari. The Open Images Dataset V4: Uniﬁed Im-
age Classiﬁcation, Object Detection, and Visual Relationship
Detection at Scale. arXiv:1811.00982, 2018. 2

[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. SSD: Single Shot Multibox Detector. In
Proc. ECCV, 2016. 5

[21] D. Lowe. Distinctive Image Features from Scale-Invariant

Keypoints. IJCV, 2004. 1, 2

[22] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust Wide-
Baseline Stereo from Maximally Stable Extremal Regions.
Image and Vision Computing, 2004. 2

[23] A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Work-
ing Hard to Know your Neighbor’s Margins: Local Descriptor
Learning Loss. In Proc. NIPS, 2017. 1

[24] D. Mishkin, F. Radenovic, and J. Matas. Repeatability Is Not
Enough: Learning Afﬁne Regions via Discriminability. In
Proc. ECCV, 2018. 1, 7, 8

[25] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han. Large-
Scale Image Retrieval with Attentive Deep Local Features. In
Proc. ICCV, 2017. 1, 2, 3, 5, 7

[26] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman.
Object Retrieval with Large Vocabularies and Fast Spatial
Matching. In Proc. CVPR, 2007. 1, 5

[27] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman.
Lost in Quantization: Improving Particular Object Retrieval
in Large Scale Image Databases. In Proc. CVPR, 2008. 1, 5

[28] F. Radenovi´c, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.
Revisiting Oxford and Paris: Large-Scale Image Retrieval
Benchmarking. In Proc. CVPR, 2018. 2, 3, 5, 7, 8

[29] F. Radenovi´c, G. Tolias, and O. Chum. CNN Image Retrieval
Learns from BoW: Unsupervised Fine-Tuning with Hard Ex-
amples. In Proc. ECCV, 2016. 2

[30] F. Radenovi´c, G. Tolias, and O. Chum. Fine-tuning CNN
Image Retrieval with No Human Annotation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2018. 1,
5, 7

[31] A. S. Razavian, J. Sullivan, S. Carlsson, and A. Maki. Visual
Instance Retrieval with Deep Convolutional Networks. ITE
Transactions on Media Technology and Applications, 2016.
2, 5

[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal
Networks. In Proc. NIPS, 2015. 2, 5

[33] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen. Inverted Residuals and Linear Bottlenecks: Mobile
Networks for Classiﬁcation, Detection and Segmentation. In
Proc. CVPR, 2018. 5

[34] K. Simonyan and A. Zisserman. Very Deep Convolutional
Networks for Large-Scale Image Recognition. In Proc. ICLR,
2015. 2

[35] J. Sivic and A. Zisserman. Video Google: A Text Retrieval
Approach to Object Matching in Videos. In Proc. ICCV, 2003.
1, 3

[36] R. Tao, E. Gavves, C. G. M. Snoek, and A. W. M. Smeulders.
Locality in Generic Instance Search from One Example. In
Proc. CVPR, 2014. 2, 5

5117

[37] G. Tolias, Y. Avrithis, and H. Jegou.

Image Search with
Selective Match Kernels: Aggregation Across Single and
Multiple Images. IJCV, 2015. 1, 3, 4, 5, 6, 7

[38] G. Tolias and H. Jegou. Visual Query Expansion with or
without Geometry: Reﬁning Local Descriptors by Feature
Aggregation. Pattern Recognition, 2014. 5

[39] G. Tolias, R. Sicre, and H. J´egou. Particular Object Retrieval
In Proc.

with Integral Max-Pooling of CNN Activations.
ICLR, 2015. 1, 2, 5

[40] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and
A. W. M. Smeulders. Selective Search for Object Recognition.
IJCV, 2013. 2, 5

5118

