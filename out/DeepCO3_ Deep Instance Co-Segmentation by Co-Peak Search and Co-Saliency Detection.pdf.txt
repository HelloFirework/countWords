DeepCO3: Deep Instance Co-segmentation by Co-peak Search and Co-saliency

Detection

Kuang-Jui Hsu1

2

,

Yen-Yu Lin1

Yung-Yu Chuang1

2

,

1Academia Sinica, Taiwan

2National Taiwan University, Taiwan

Abstract

In this paper, we address a new task called instance co-
segmentation. Given a set of images jointly covering object
instances of a speciﬁc category, instance co-segmentation
aims to identify all of these instances and segment each of
them, i.e. generating one mask for each instance. This task
is important since instance-level segmentation is preferable
for humans and many vision applications. It is also chal-
lenging because no pixel-wise annotated training data are
available and the number of instances in each image is un-
known. We solve this task by dividing it into two sub-tasks,
co-peak search and instance mask segmentation. In the for-
mer sub-task, we develop a CNN-based network to detect
the co-peaks as well as co-saliency maps for a pair of im-
ages. A co-peak has two endpoints, one in each image, that
are local maxima in the response maps and similar to each
other. Thereby, the two endpoints are potentially covered by
a pair of instances of the same category. In the latter sub-
task, we design a ranking function that takes the detected
co-peaks and co-saliency maps as inputs and can select the
object proposals to produce the ﬁnal results. Our method
for instance co-segmentation and its variant for object co-
localization are evaluated on four datasets, and achieve fa-
vorable performance against the state-of-the-art methods.
The source codes and the collected datasets are available
at https://github.com/KuangJuiHsu/DeepCO3/.

1. Introduction

Object co-segmentation aims to segment the common
objects repetitively appearing in a set of images. It is a fun-
damental and active research topic in computer vision. As
an important component of image content understanding, it
is essential to many vision applications, such as semantic
segmentation [48], image matching [4, 19, 25, 52, 60, 61],
object skeletonization [8, 27], and 3D reconstruction [42].

Object co-segmentation has recently gained signiﬁcant
progress owing to the fast development of convolutional
neural networks (CNNs). The CNN-based methods [21, 33,
62] learn the representation of common objects in an end-

Figure 1. Two examples of instance co-segmentation on categories
bird and sheep, respectively. An instance here refers to an object
appearing in an image. In each example, the top row gives the
input images while the bottom row shows the instances segmented
by our method. The instance-speciﬁc coloring indicates that our
method produces a segmentation mask for each instance.

to-end manner and can produce object-level results of high
quality. However, they do not explore instance-aware infor-
mation, i.e. one segmentation mask for each instance rather
than each class, which is more consistent with human per-
ception and offers better image understanding, such as the
locations and shapes of individual instances.

In this work, we present a new and challenging task
called instance-aware object co-segmentation (or instance
co-segmentation for short). Two examples of this task are
shown in Figure 1 for a quick start. Given a set of images of
a speciﬁc object category with each image covering at least
one instance of that category,
instance co-segmentation
aims to identify all of these instances and segment each of
them out, namely one mask for each instance. Note that un-
like semantic [18] or instance segmentation [65], no pixel-
wise data annotations are collected for learning. The object
category can be arbitrary and unknown, which means that
no training images of that category are available in advance.
Instance-level segments that can be obtained by solving this
task are valuable to many vision applications, such as au-
tonomous driving [2, 64], instance placement [31], image
and sentence matching [26] or amodal segmentation [23].

18846

Therefore, instance co-segmentation has a practical setting
in input collection and better accomplishing it potentially
advances the ﬁeld of computer vision.

In this paper, we develop a CNN-based method for in-
stance co-segmentation. Based on the problem setting, our
method has no access to annotated instance masks for learn-
ing and cannot involve any pre-training process. Inspired
by Zhou et al. [65]’s observation that object instances often
cover the peaks in a response map of a classier, we design a
novel co-peak loss to detect the common peaks (or co-peaks
for short) in two images. The co-peak loss is built upon a
4D tensor that is learned to encode the inter-image similar-
ity at every location. The co-peaks inferred from the learned
4D tensor correspond to two locations, one in each of the
two images, where discriminative and similar features are
present. Therefore, the two locations are potentially cov-
ered by two object instances. Using the co-peak loss alone
may lead to unfavorable false positives and negatives. Thus,
we develop the afﬁnity loss and the saliency loss to comple-
ment the co-peak loss. The former carries out discrimina-
tive feature learning for the 4D tensor construction by sep-
arating the foreground and background features. The lat-
ter estimates the co-saliency maps to localize the co-salient
objects in an image, and can make our model focus on co-
peak search in co-salient regions. The three loss functions
work jointly and can detect co-peaks of high quality. We
design a ranking function taking the detected co-peaks and
co-saliency maps as inputs and accomplish instance mask
segmentation by selecting object proposals.

We make the following contributions in this work. First,
we introduce a new and interesting task called instance co-
segmentation. Its input is a set of images containing object
instances of a speciﬁc category, and hence is easy to col-
lect. Its output is instance-aware segments, which are de-
sired in many vision applications. Thus, we believe instance
co-segmentation worth exploring. Second, a simple and ef-
fective method is developed for instance co-segmentation.
The proposed method learns a model based on the fully con-
volutional network (FCN) [40] by optimizing three losses,
including the co-peak, afﬁnity, and saliency losses. The
learned model can reliably detect co-peaks and co-saliency
maps for instance mask segmentation. Third, we collect
four datasets for evaluating instance co-segmentation. The
proposed method for instance co-segmentation and its vari-
ant for object co-localization [5,6,51,58,59] are extensively
evaluated on the four datasets. Our method performs favor-
ably against the state-of-the-art methods.

2. Related Work

Object co-segmentation. This task [13, 28, 45, 46, 54, 56,
57] aims to segment the common objects in images. Its ma-
jor difﬁculties lie in large intra-class variations and back-
ground clutter. Most methods rely on robust features, such

as handcrafted and deep learning based features, for ad-
dressing these difﬁculties. In addition, saliency evidence,
including single-image saliency [12, 20, 27, 28, 46, 53] or
multi-image co-saliency [3, 54, 57], has been explored to
localize the salient and common objects. Recently, CNN-
based methods [21, 33, 62] achieve better performance by
joint representation learning and co-segmentation.

Despite effectiveness, the aforementioned methods do
not provide instance-level results. In this work, we go be-
yond object co-segmentation and investigate instance co-
segmentation. Our method can determine the number, lo-
cations, and contours of common instances in each image,
and offers instance-aware image understanding.

Object co-localization. This task [5, 6, 51, 58, 59] discov-
ers the common instances in images. Different from object
co-segmentation, it is instance-aware. It detects and outputs
the bounding box of a single instance in each image even if
multiple instances are present in the image. Compared with
object co-localization, instance co-segmentation identiﬁes
all instances in an image in the form of instance segments.

Instance-aware segmentation.
Instance-aware segmen-
tation includes class-aware [1, 7, 15, 17, 65] and class-
agnostic [11, 24, 32] methods. Given training data of pre-
deﬁned categories, class-aware instance segmentation, aka
instance segmentation, learns a model to seek each object
instance belonging to one of these categories. A widely
used way for instance segmentation is to ﬁrst detect instance
bounding boxes and then segment the instances within the
bounding boxes [7, 15–17, 35, 38, 43]. Another way is to di-
rectly segment each instance without bounding box detec-
tion [1,30,36,39,65]. While most methods for instance seg-
mentation are supervised, Zhou et al. [65] present a weakly
supervised one. All these methods for instance segmenta-
tion rely on training data to learn the models. Despite the
effectiveness and efﬁciency in testing, their learned models
are not applicable to unseen object categories.

In practice, it is difﬁcult to enumerate all object cat-
egories of interest in advance and prepare class-speciﬁc
training data, which limits the applicability of class-aware
instance segmentation. Class-agnostic instance segmenta-
tion [11,24,32] aims at segmenting object instances of arbi-
trary categories, and has drawn recent attention. It is chal-
lenging because it involves both generic object detection
and segmentation. Instance co-segmentation is highly re-
lated to class-agnostic instance segmentation in the sense
that both of them can be applied to arbitrary and even un-
seen object categories. However, existing class-agnostic
methods require annotated training data in the form of ob-
ject contours. On the contrary, our method for instance
co-segmentation explores the mutual information regarding
the common instances in given images, and does not need
any pre-training procedure on additional data annotations.
Thus, our method has better generalization.

8847

(cid:38)(cid:82)(cid:16)(cid:83)(cid:72)(cid:68)(cid:78)(cid:3)(cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)

(cid:1835)(cid:3041)

(cid:1835)(cid:3040)

(cid:44)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:48)(cid:68)(cid:86)(cid:78)(cid:3)(cid:54)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:53)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87)

(cid:1709)
(cid:1709)

(cid:1832)(cid:3040)
(cid:1875) (cid:1860)(cid:1856)
(cid:883)(cid:3398)(cid:1845)(cid:3041)

(cid:1859)

(cid:1832)(cid:3041)
(cid:1875) (cid:1860)(cid:1856)
(cid:1845)(cid:3041)
(cid:53)(cid:68)(cid:81)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:1372)
(cid:485)

(cid:38)(cid:82)(cid:85)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:883)(cid:3400)(cid:883) (cid:70)(cid:82)(cid:81)(cid:89)(cid:17)

(cid:1846)(cid:3041)(cid:3040)
(cid:1845)(cid:4634)(cid:3041)
(cid:1845)(cid:4634)(cid:3040)

(cid:39)(cid:72)(cid:70)(cid:82)(cid:81)(cid:89)(cid:17)

(cid:48)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:38)(cid:82)(cid:16)(cid:83)(cid:72)(cid:68)(cid:78)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)

(cid:48)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:36)(cid:73)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:92)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)

(cid:1845)(cid:3041)

(cid:1845)(cid:4632)(cid:3041)

(cid:54)(cid:68)(cid:79)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)(cid:16)(cid:74)(cid:88)(cid:76)(cid:71)(cid:72)(cid:71)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)

(cid:4668)(cid:1841)(cid:3041)(cid:3036)(cid:4669)
(cid:1709)
(cid:1709)

Figure 2. Overview of our method, which contains two stages, co-peak search within the blue-shaded background and instance mask
segmentation within the red-shaded background. For searching co-peaks in a pair of images, our model extracts image features, estimates
their co-saliency maps, and performs feature correlation for co-peak localization. The model is optimized by three losses, including the
co-peak loss ℓt, the afﬁnity loss ℓa, and the saliency loss ℓs. For instance mask segmentation, we design a ranking function taking the
detected co-peaks, the co-saliency maps, and the object proposals as inputs, and select the top-ranked proposal for each detected instance.

3. Proposed Method

In this section, we give an overview of our method, de-
scribe its components, co-peak search and instance mask
segmentation, and provide the implementation details.

3.1. Overview

Suppose that a set of images D ✏ tIn✉N

n✏1 consisting
of object instances of a particular category is given, where
In P RW ✂H✂c is the nth image while W , H, and c are the
width, the height, and the number of channels of In, respec-
tively. The goal of instance co-segmentation is to identify
and segment each of all instances in D. Note that no train-
ing data with pixel-wise annotations are provided. In addi-
tion, both the object category and the number of instances
in each image are unknown.

In the proposed method, we decompose instance co-
segmentation into two stages, i.e. co-peak search and in-
stance mask segmentation. The overview of our method is
shown in Figure 2, where the two stages are highlighted
with the blue-shaded area and the red-shaded backgrounds,
respectively.

At the stage of co-peak search, we aim to seek co-peaks
in the response maps of two images, where a co-peak cor-
responds two discriminative and similar points, one in each
image, so that each point is potentially within an object in-
stance. We design a network model for co-peak detection.
The front part of our model is a fully convolutional network
(FCN) g, which extracts the feature maps of input images.
After feature extraction, our model is split into two streams.

One stream correlates the feature maps of two images for
co-peak localization. The other estimates the co-saliency
maps of input images, which in turn enforces FCN g to
generate more discriminative feature maps. Our model is
optimized by three novel losses, including the co-peak loss
ℓt, the afﬁnity loss ℓa, and the saliency loss ℓs. After op-
timization, co-peaks are detected and co-saliency maps are
estimated. At the stage of instance mask segmentation, we
design a ranking function that takes the detected co-peaks,
the estimated co-saliency maps, and the instance proposals
into account, and yield one mask for each detected instance.

3.2. Co-peak search

As shown in Figure 2, our model takes a pair of images,
In and Im, from D as input at a time. It ﬁrst extracts the
feature maps Fn P Rw✂h✂d for In, where w, h, and d are
the width, the height, and the number of channels, respec-
tively. Similarly, feature maps Fm P Rw✂h✂d are yielded
for Im. Our model is then divided into two streams. One
stream performs correlation between Fn and Fm, and yields
a 4D correlation tensor Tnm P Rw✂h✂w✂h. Each element
Tnm♣i, j, s, tq ✏ Tnm♣p, qq records the normalized inner
product between the feature vectors stored at two spatial lo-
cations, i.e. p ✏ ri, js in Fn and q ✏ rs, ts in Fm. The other
stream employs a 1 ✂ 1 convolutional layer to estimate the
co-saliency map ˜Sk P Rw✂h of Ik, and adopts deconvo-
lution layers to generate a high-resolution co-saliency map
Sk P RW ✂H , for k P tn, m✉. We design three loss func-
tions, including the co-peak loss ℓt, the afﬁnity loss ℓa, and
the saliency loss ℓs, to derive the network, leading to the

8848

following object function

3.2.2 Afﬁnity loss ℓa

L♣wq ✏ λt

  λa

N

➳

n✏1

N

➳

n✏1

➳

m✘n

➳

m✘n

ℓt♣In, Im; wq

ℓa♣In, Im; wq  

(1)

N

➳

n✏1

ℓs♣In; wq,

where w is the set of learnable parameters of the network.
Nonnegative weights λt and λa control the relative impor-
tance among the three losses. They are ﬁxed to 0.5 and
0.1 in this work, respectively. The co-peak loss ℓt stimu-
lates co-peak detection. The afﬁnity loss ℓa refers to the co-
saliency maps and enables discriminative feature learning.
The saliency loss ℓs working with the other two losses car-
ries out co-saliency detection and hence facilitates instance
co-segmentation. The three losses are elaborated in the fol-
lowing.

3.2.1 Co-peak loss ℓt

This loss aims to stimulate co-peak detection. A co-peak
consists of to two points, one in each of In and Im. Since
a co-peak covered by a pair of instances of the same object
category is desired, the two points of the co-peak must be
inside the object and similar to each other. Therefore, both
intra-image saliency and inter-image correlation are taken
into account in this loss.

As shown in Figure 2, our two-stream network produces
the intra-image saliency maps ˜Sn and ˜Sm in one stream and
inter-image correlation map Tnm in the other stream. To
jointly consider the two types of information, a saliency-
nm P Rw✂h✂w✂h is constructed
guided correlation tensor T s
with its elements deﬁned below

nm♣p, qq ✏ ˜Sn♣pq ˜Sm♣qqTnm♣p, qq,
T s

(2)

where p P P, q P P, and P is the set of all spatial coordi-
nates of the feature maps. In Eq. (2), ˜Sn♣pq is the saliency
value of ˜Sn at point p, and ˜Sm♣qq is similarly deﬁned.

To have more reliable keypoints to reveal object in-

stances, we deﬁne a co-peak as a local maximum in T s
nm
within a 4D local window of size 3 ✂ 3 ✂ 3 ✂ 3. Suppose
that ♣p, qq is a peak in T s
nm. Both point p in Fn and point
q in Fm are salient, and they are the most similar to each
other in a local region. The former property implies that the
two points probably reside in two salient object instances.
The latter one reveals that the two instances are likely of the
same class, since they have similar parts. Based on above
discussion, the co-peak loss used to stimulate reliable co-
peaks is deﬁned by

ℓt♣In, Imq ✏ ✁log ☎
✆

1

⑤Mnm⑤ ➳

♣p,qqPMnm

nm♣p, qq☞

T s

✌, (3)

where Mnm is the set of co-peaks.

The co-peak loss refers to the feature maps of the images,
so discriminative features that can separate instances from
background are preferable. Besides, the co-peak loss is ap-
plied to the locations of co-peaks, and features on other lo-
cations are ignored. The afﬁnity loss is introduced to ad-
dress the two issues.
It aims to derive the features with
which pixels in the salient regions are similar to each other
while being distinct from those in the background. For a
pair of images In and Im, a loss ˜ℓa♣In, Imq is deﬁned by
˜ℓa♣In, Imq ✏ ➳

˜Sn♣pq ˜Sn♣qq ♣1 ✁ Tnm♣p, qqq

➳

pPP

qPP

(4)

  α♣ ˜Sn♣pq ✁ ˜Sn♣qqq2Tnm♣p, qq,

where constant α is empirically set to 4.
In Eq. (4), the
ﬁrst term penalizes the case of low similarity between two
salient pixels, while the second term prevents high similar-
ity between a salient pixel and a non-salient pixel. The pro-
posed afﬁnity loss generalizes ˜ℓa in Eq. (4) to consider both
inter-image and intra-image afﬁnities and is deﬁned by

ℓa♣In, Imq ✏ ˜ℓa♣In, Imq   ˜ℓa♣In, Inq   ˜ℓa♣Im, Imq. (5)

3.2.3 Saliency loss ℓs

This term aims to identify the salient regions and can guide
the training of our model. Following the studies of ob-
ject co-segmentation [27, 28, 46, 53], we utilize an off-the-
shelf method for saliency detection. The resultant saliency
maps can serve as the object prior. In this work, we adopt
the unsupervised method, SVFSal [63], which produces the
saliency map ˆSn for image In. Note that the resolutions of
ˆSn and In are the same. Thus, the deconvolutional layers
are employed to increase the resolution. Following [22], the
saliency loss ℓs applied to image In is deﬁned by

ℓs♣Inq ✏ ➳

pPIn

ρn♣pq⑥Sn♣pq ✁ ˆSn♣pq⑥2
2,

(6)

where p indexes the pixels of In, ρn♣pq is a weight repre-
senting the importance of pixel p, and Sn is the predicted
saliency map for In by our model. The weight ρn♣pq deals
with the imbalance between the salient and non-salient ar-
eas. It is set to 1 ✁ ε if pixel p resides in the salient region,
and ε otherwise, where ε is the ratio of the salient area to the
whole image. The mean value of ˆSn is used as the thresh-
old to divide ˆSn into the salient and non-salient regions.
In this way, the salient and non-salient regions contribute
equally in Eq. (6). As shown in Figure 2, except for the de-
convolutional layers, our model used to produce maps tSn✉
is derived by the three losses jointly. Thus, tSn✉ derived
with both intra- and inter-image cues are called co-saliency
maps. This prior term is helpful as it compensates for the
lack of supervisory signals in instance co-segmentation.

8849

3.3. Instance mask segmentation

After optimizing Eq. (1), we simply use the detected
peaks on the estimated co-saliency maps as the ﬁnal co-
peaks, because detecting the co-peaks on all possible image
pairs is complicated. Thus, the peaks tpi
i✏1 of each im-
age In are collected, where M is the number of the peaks.
We adopt the method called peak back-propagation [65] to
infer an instance-aware heat map Oi
n. The
n is supposed to highlight the instance covering pi
map Oi
n.
An example is given in Figure 2.

n for each peak pi

n✉M

For instance mask generation, we utilize an unsuper-
vised method, called multi-scale combinatorial grouping
(MCG) [44], to produce a set of instance proposals for im-
age In. With the heat maps tOi
i✏1 and the co-saliency
map Sn, we extend the proposal ranking function in [65]
by further taking the co-saliency cues into account, and se-
lect the top-ranked proposal as the mask for each detected
peak. Speciﬁcally, given the maps Oi
n and Sn, the ranking
function R applied to an instance proposal P is deﬁned by

n✉M

R♣P q ✏ β♣Oi

n✝Snq✝P  ♣Oi

n✝Snq✝ ˆP ✁γ♣1✁Snq✝P, (7)
where ˆP is the contour of the proposal P and operator ✝
is the Frobenius inner product between two matrices. The
coefﬁcients β and γ are set to 0.8 and 10✁5, respectively.
In Eq. (7), three terms, i.e. the instance-aware, contour-
preserving, and object-irrelevant terms, are included. The
instance-aware term prefers the proposals that cover the re-
gions with high responses in Oi
n and high saliency in Sn.
The contour-preserving term focuses on the ﬁne-detailed
boundary information. The background map, 1 ✁ Sn, is
used in the object-irrelevant term to suppress background
regions. Compared with the ranking function in [65], ours
further exploits the properties of instance co-segmentation,
i.e. the high co-saliency values in object instances, and can
select more accurate proposals. Following a standard pro-
tocol of instance segmentation, we perform non-maximum
suppression (NMS) to remove the redundancies.

3.4. Implementation details

We implement the proposed method using MatCon-
vNet [55]. VGG-16 [49] is adopted as the feature extrac-
tor g. It is pre-trained on the ImageNet [47] dataset, and is
updated during optimizing Eq. (1). The same network ar-
chitecture is used in all experiments. Note that the objective
in Eq. (1) involves all image pairs. Direct optimization is
not feasible due to the limited memory size. Thereby, we
adopt the piecewise training scheme [50]. Namely, only a
subset of images is considered in each epoch, and the subset
size is set to 6 in this work. The learning rate, weight decay,
and momentum are set to 10✁6, 0.0005, and 0.9, respec-
tively. The optimization procedure stops after 40 epochs.
We choose ADAM [29] as the optimization solver. All im-
ages are resized to the resolution 448 ✂ 448 in advance. We

dataset

COCO-VOC

COCO-NONVOC

VOC12

SOC

(a)
12
32
18
5

(b)
1281
3130
891
522

(c)
3151
8303
2214
835

(d)

106.8
91.8
178.2
29.0

(e)
2.5
2.7
2.5
1.6

Table 1. Some statistics of the four collected datasets, including (a)
the number of classes, (b) the number of images, (c) the number
of instances, (d) the average number of images per class, and (e)
the average number of instances per image.

resize the instance co-segmentation results back to the orig-
inal image resolution for performance evaluation.

4. Experimental Results

In this section, our method for instance co-segmentation
and its variant for co-localization are evaluated. First,
the adopted datasets and evaluation metrics are described.
Then, the competing methods are introduced. Finally, the
comparison results are reported and analyzed.

4.1. Dataset collection

As instance co-segmentation is a new task, no public
benchmarks exist. Therefore, we establish four datasets
with pixel-wise instance annotations by collecting im-
ages from three public benchmarks,
including the MS
COCO [37], PASCAL VOC 2012 [9, 14], and SOC [10]
datasets. The following pre-processing is applied to each
dataset. First, we remove the images where objects of more
than one category are present. Second, we discard the cate-
gories that contain less than 10 images. The details of col-
lecting images from each dataset are described below.

MS COCO dataset. We collect images from the training
and validation sets of the MS COCO 2017 object detection
task. As MS CCCO is a large-scale dataset, we further re-
move the images that do not contain at least two instances.
Total 44 categories remain. Some competing methods are
pre-trained on PASCAL VOC 2012 dataset. For the ease
of comparison, we divide the 44 categories into two dis-
joint sets, COCO-VOC and COCO-NONVOC. The former
contains 12 categories covered by the PASCAL VOC 2012
dataset, while the latter contains the rest.

PASCAL VOC 2012 dataset. Because few pixel-wise
instance annotations are available in the PASCAL VOC
2012 dataset, we adopt the augmented VOC12 dataset [14],
which has 18 object categories after dataset preprocessing.

SOC dataset. SOC [10] is a newly collected dataset
for saliency detection. It provides image-level labels and
instance-aware annotations. After preprocessing, only ﬁve
object categories remain because many images contain ob-
ject instances of multiple categories and some categories
have less than 10 images.

8850

method

year

trained

CLRW [51] CVPR 2014 ✂
UODL [5] CVPR 2015 ✂
DDT [58]
IJCAI 2017 ✂
arXiv 2017
DDT  [59]
✂
ECCV 2018 ✂
NLDF [41] CVPR 2017 ❵
C2S-Net [34] ECCV 2018 ❵
CVPR 2018 ❵
✂

PRM [65]

DFF [6]

Ours

-

COCO-VOC
0.25 mAPr
13.7
2.2
10.1
10.6
11.6
18.2
13.4
14.6
21.1

mAPr
33.3
9.6
31.4
31.7
30.8
39.1
39.6
44.9
52.6

COCO-NONVOC

0.5 mAPr
24.6
8.5
25.7
26.0
22.6
23.9
25.1

0.25 mAPr
10.7
1.8
9.7
10.1
7.3
8.5
7.6

0.5 mAPr
29.2
9.4
30.7
33.6
27.7
34.3
30.1
45.3
45.6

VOC12
0.25 mAPr
10.5
2.0
8.8
9.4
13.7
12.7
10.7
14.8
16.7

-

35.3

-

12.3

SOC

0.5

0.5 mAPr
34.9
11.0
43.0
39.6
42.3
49.5
37.0

0.25 mAPr
15.6
2.7
25.7
22.4
17.0
21.6
12.5

-

54.2

-

26.0

Table 2. Performance of instance co-segmentation on the four collected datasets. The numbers in red and green show the best and the
second best results, respectively. The column “trained” indicates whether additional training data are used.

The statistics and the abbreviations of the four collected
datasets are given in Table 1. Note that our method can work
on images containing one or multiple instances of the com-
mon object category. The SOC dataset helps test this issue.
As shown in Table 1, the average number of instances in
SOC is 1.6, less than 2. It shows that there exist many im-
ages in this dataset with only one object instance. Please re-
fer to the supplementary material for more details and some
image samples of the four collected datasets.

4.2. Evaluation metrics

For instance co-segmentation, mean average precision
(mAP) [15] is adopted as the performance measure. Follow-
ing [65], we report mAP using the IoU thresholds at 0.25
and 0.5, denoted as mAPr

0.25 and mAPr

0.5, respectively.

For object co-localization,

the performance measure
CorLoc [5,6,51,58,59] is used as the evaluation metric. The
measure CorLoc is designed for evaluating the results in the
form of object bounding boxes. For comparing with meth-
ods whose output is object or instance segments, we extend
CorLoc to CorLocr to evaluate the results in the form of
object segments.

4.3. Competing methods

As instance co-segmentation is a new task, there are
no existing methods for performance comparison. We
adopt two strategies for comparing our method with exist-
ing ones. First, we consider competing methods of three
categories, including object co-localization, class-agnostic
saliency segmentation, and weakly supervised instance seg-
mentation. For methods of the three categories, we convert
their predictions into the results in the form of instance co-
segmentation, namely one segment mask for each detected
instance.
In this way, our method can be compared with
these methods on the task of instance co-segmentation.

Second, we compare our method with methods of all the
aforementioned three categories on the task of object co-
localization. To this end, we need to convert the output of
each compared method into the results in the form of object

co-localization, namely the object bounding box with the
highest conﬁdence in each image.

In the two strategies of method comparison, two types
of prediction conversion are required, including converting
a bounding box to an instance segment and its inverse di-
rection. Unless further speciﬁed, we adopt the following
way to convert a bounding box prediction to an instance
segment. Given a bounding box in an image, we apply
MCG [44] to that image to generate a set of instance pro-
posals, and retrieve the proposal with the highest IoU with
the bounding box to represent it. On the other hand, it is
easy to concert a given instance segment to a bounding box.
We simply use the bounding box of that instance segment to
represent it. In the following, the selected competing meth-
ods from each of the three categories are speciﬁed.

this category for comparison,

Object co-localization. We choose the state-of-the-art
methods of
including
CLRW [51], UODL [5], DDT [58], DDT  [59], and
DFF [6]. The ﬁrst two methods, CLRW and UODL, out-
put all bounding boxes with their scores, but cannot deter-
mine the number of instances in each image. Thus, we pick
the top-scored bounding boxes as many as the instances de-
tected by our method, and similarly apply NMS to remove
redundancies. The last three methods, DDT, DDT , and
DFF, ﬁrst produce the heat maps to highlight objects, then
convert the heat maps into the binary masks by using their
proposed mechanisms, and ﬁnally take the bounding boxes
of the connected components on the binary masks.

Class-agnostic instance segmentation (CAIS). We se-
lect two powerful methods, NLDF [41] and C2S-Net [34],
of this category as the competing methods. The algorithm
proposed in [32] is used to convert the saliency contours
generated by NLDF and C2S-Net into the results in the form
of instance co-segmentation.

Weakly supervised instance segmentation (WSIS). The
WSIS method, PRM [65], is trained on the PASCAL VOC
2012 dataset, and it cannot be applied to the images whose
categories are not covered by the PASCAL VOC 2012

8851

cow

sheep

horse

train

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 3. Results of instance co-segmentation on four object categories, i.e.cow, sheep, horse, and train, of the COCO-VOC dataset. (a)
Input images. (b) Ground truth. (c) ✒ (g) Results with instance-speciﬁc coloring generated by different methods including (c) our method,
(d) CLRW [51], (e) DFF [6], (f) NLDF [41], and (g) PRM [65], respectively.

(cid:954)(cid:3046)
(cid:954)(cid:3046)(cid:3397)(cid:954)(cid:3047)
(cid:954)(cid:3046)(cid:3397)(cid:954)(cid:3047)(cid:3397)(cid:954)(cid:3028)
(cid:954)(cid:3046)
(cid:954)(cid:3046)(cid:3397)(cid:954)(cid:3047)
(cid:954)(cid:3046)(cid:3397)(cid:954)(cid:3047)(cid:3397)(cid:954)(cid:3028)

Figure 4. Performance in mAPr
0.25 with different loss function
combinations on the COCO-VOC and COCO-NONVOC datasets.

dataset. Therefore, PRM is compared with our method only
on the COCO-VOC and VOC12 datasets.

4.4. Instance co-segmentation

For

the ease of performance analysis, we divide
i.e. trained
the evaluated methods into two groups,
and non-trained.
The group trained includes
NLDF [41], C2S-Net [34] and PRM [65]. Methods
of this group require additional training data other than
the input to instance co-segmentation. The other group
non-trained contains our method and the rest of the
competing methods. Methods of group non-trained
have access to only the input to instance co-segmentation.

Our method and all competing methods are evaluated
on the four collected datasets. Their performance is re-
ported in Table 2. The proposed method outperforms the
competing methods of group non-trained by large mar-
gins even though all of them access the same data. We at-

w/o co-saliency map
w co-saliency map

COCO-VOC
0.25 mAPr
12.4
21.1

mAPr
33.5
52.6

COCO-NONVOC

0.5 mAPr
25.3
35.3

0.25 mAPr
8.3
12.3

0.5

Table 3. Performance of our method working with the proposal
ranking function without or with the co-saliency information on
the COCO-VOC and COCO-NONVOC datasets.

tribute the performance gain yielded by our method to fea-
ture learning enabled CNNs. The competing methods of
group non-trained adopt pre-deﬁned features, and can-
not well deal with complex and diverse intra-class varia-
tions and background clutters. On the contrary, our method
leverages CNNs to carry out feature learning and instance
co-segmentation simultaneously,
leading to much better
performance. Although the methods of group trained
have access to additional training data, ours still reaches
more favorable results. The main reason is that our method
explores co-occurrent patterns via co-peak detection when
images for instance co-segmentation are available, while the
methods of group trained ﬁx their models after training
on additional data and cannot adapt themselves to newly
given images for instance co-segmentation.

To gain the insight into the quantitative results, Fig-
ure 3 visualizes the qualitative results generated by our
method, CLRW [51], DFF [6], NLDF [41], and PRM [65].
The major difﬁculties of instance segmentation lie in in-
stance mutual occlusions, intra-class variations, and clut-

8852

r
a
e
b

e
n
o
h
p
l
l
e
c

t
n
a
h
p
e
l
e

r
a
e
b
y
d
d
e
t

a
r
b
e
z

a
r
b
e
z

a
r
b
e
z

Figure 5. Seven examples, one in each row, of the co-localization
results by our method on the COCO-NONVOC dataset.

tered scene. As shown in Figure 3(c), our method still
works well when instance mutual occlusions occur on cate-
gories cow, sheep, and horse and large intra-class variations
and cluttered scene are present on category train. In Fig-
ure 3(d), CLRW yields some false alarms in the background
while has false negatives on category train. In Figure 3(e),
DFF cannot well address instance mutual occlusions due
to computing connected components for instance identiﬁ-
cation. In Figure 3(f) and Figure 3(g), NLDF and CRP per-
form favorably against other competing methods, but still
suffer from over-segmentation and misses, respectively.

Ablation studies. We analyze the proposed objective con-
sisting of three loss functions in Eq. (1) on the COCO-VOC
and COCO-NONVOC datasets, and report the results in
Figure 4. Except loss ℓs, the other two losses, ℓt and ℓa, are
added one by one. When ℓt is included, the performance
gains are signiﬁcant on both datasets. It implies that ℓt for
reliable co-peak search is important in our method. Once ℓa
is added, the performance is moderately enhanced, which
means that discriminative feature learning is helpful for in-
stance co-segmentation.
In addition to the objective, the
effect of referring to co-saliency maps in proposal ranking
is analyzed in Table 3. The results clearly point out that in-
formation from co-saliency detection is crucial to proposal
ranking.
It is not surprised. Since co-peaks identify the
keypoints within instances, we still need the evidence from
co-saliency maps to reveal the corresponding instances.

method

year

PR 2019

CLRW [51] CVPR 2014 ✂
UODL [5] CVPR 2015 ✂
DDT [58]
IJCAI 2017 ✂
DDT  [59]
✂
ECCV 2018 ✂
NLDF [41] CVPR 2017 ❵
C2S-Net [34] ECCV 2018 ❵
CVPR 2018 ❵
✂

PRM [65]

DFF [6]

Ours

-

trained COCO-VOC COCO-NONVOC VOC12 SOC
30.9
10.3
16.7
18.4
22.9
42.0
32.9

31.6
12.7
27.4
25.8
30.5
31.0
28.4

-

34.3

-

43.1

29.9
9.5
25.0
23.7
28.7
39.2
31.1
23.3
39.2

33.4
12.3
30.0
29.5
32.3
51.2
39.0
18.1
49.6

Table 4. Performance of object co-localization on the four datasets.
The numbers in red and green indicate the best and the second
best results, respectively. The column “trained” indicates whether
additional training data are used.

4.5. Object co-localization

We evaluate our method and the competing methods
for object co-localization in the four datasets we collected.
For our method, we pick the top-ranked proposal in each
image when evaluating the performance in CorLocr. Ta-
ble 4 reports the performance of all the compared methods.
Our method achieves the comparable or even better perfor-
mance, even though it is not originally designed for object
co-localization. Seven examples of object co-localization
by our method are shown in Figure 5, where accurate in-
stance masks and the corresponding bounding boxes are
discovered by our method.

5. Conclusions

In this paper, we present an interesting and challeng-
ing task called instance co-segmentation, and propose a
CNN-based method to effectively solve it without using
additional training data. We decompose this task into two
sub-tasks,
including co-peak search and instance mask
segmentation. In the former sub-task, we design three novel
losses, co-peak, afﬁnity, and saliency losses, for joint co-
peak and co-saliency map detection. In the latter sub-task,
we develop an effective proposal ranking algorithm, and
can retrieve high-quality proposals to accomplish instance
co-segmentation. Our method for instance co-segmentation
and its variant for object co-localization are extensively
evaluated on the four collected datasets. Both quantitative
and qualitative results show that our method and its variant
perform favorably against
In the
future, we plane to integrate the proposed method into
more high-level tasks, such as autonomous driving, visual
question answering, image and sentence matching where
instance-aware annotations are valuable.

the state-of-the-arts.

Acknowledgments. This work was supported in part by
Ministry of Science and Technology (MOST) under grants
107-2628-E-001-005-MY3 and 108-2634-F-007-009, and
MOST Joint Research Center for AI Technology and All
Vista Healthcare under grant 108-2634-F-002-004.

8853

References

[1] Min Bai and Raquel Urtasun. Deep watershed transform for

instance segmentation. In CVPR, 2017.

[2] Bert De Brabandere, Davy Neven, and Luc Van Gool. Se-
In

mantic instance segmentation for autonomous driving.
CVPR Workshop, 2017.

[3] Kai-Yueh Chang, Tyng-Luh Liu, and Shang-Hong Lai. From
co-saliency to co-segmentation: An efﬁcient and fully unsu-
pervised energy minimization model. In CVPR, 2011.

[4] Hsin-I Chen, Yen-Yu Lin, and Bing-Yu Chen.

Co-
segmentation guided hough transform for robust feature
matching. TPAMI, 2015.

[5] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce.
Unsupervised object discovery and localization in the wild:
Part-based matching with bottom-up region proposals.
In
CVPR, 2015.

[6] Edo Collins, Radhakrishna Achanta, and Sabine S¨usstrunk.
Deep feature factorization for concept discovery. In ECCV,
2018.

[7] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In

mantic segmentation via multi-task network cascades.
CVPR, 2016.

[8] Jifeng Dai, Ying Nian Wu, Jie Zhou, and Song-Chun Zhu.
Cosegmentation and cosketch by unsupervised learning. In
ICCV, 2013.

[9] Mark Everingham, Luc Van Gool, Christopher K.

I.
Williams, John Winn, and Andrew Zisserman. The pascal
visual object classes (VOC) challenge. IJCV, 2010.

[10] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-
Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clut-
ter: Bringing salient object detection to the foreground. In
ECCV, 2018.

[11] Ruochen Fan, Qibin Hou, Ming-Ming Cheng, Tai-Jiang Mu,
and Shi-Min Hu. S4Net: Single stage salient-instance seg-
mentation. In CVPR, 2019.

[12] H. Fu, D. Xu, B. Zhang, S. Lin, and R. Ward. Object-based
multiple foreground video co-segmentation via multi-state
selection graph. TIP, 2015.

[13] Junwei Han, Rong Quan, Dingwen Zhang, and Feiping Nie.
Robust object co-segmentation using background prior. TIP,
2018.

[14] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In ICCV, 2011.

[15] Bharath Hariharan, Pablo Arbelaez, Ross Girshick, and Ji-
tendra Malik. Simultaneous detection and segmentation. In
ECCV, 2014.

[16] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.

Boundary-aware instance segmentation. In CVPR, 2017.

[17] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-

shick. Mask R-CNN. In ICCV, 2017.

[18] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Aug-
mented multiple instance regression for inferring object con-
tours in bounding boxes. TIP, 2014.

[20] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Weakly
supervised saliency detection with a category-driven map
generator. In BMVC, 2017.

[21] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Co-
attention CNNs for unsupervised object co-segmentation. In
IJCAI, 2018.

[22] Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Xiaoning
Qian, and Yung-Yu Chuang. Unsupervised CNN-based co-
saliency detection with graphical optimization.
In ECCV,
2018.

[23] Yuan-Ting Hu, Hong-Shuo Chen, Kexin Hui, Jia-Bin Huang,
and Alexander Schwing. SAIL-VOS: Semantic amodal in-
stance level video object segmentation - a synthetic dataset
and baselines. In CVPR, 2019.

[24] Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing.
In

MaskRNN: Instance level video object segmentation.
NIPS, 2017.

[25] Yuan-Ting Hu and Yen-Yu Lin. Progressive feature match-
ing with alternate descriptor selection and correspondence
enrichment. In CVPR, 2016.

[26] Yan Huang, Wei Wang, and Liang Wang. Instance-aware im-
age and sentence matching with selective multimodal LSTM.
In CVPR, 2017.

[27] Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, and Jun-
song Yuan. Object co-skeletonization with co-segmentation.
In CVPR, 2017.

[28] Koteswar Rao Jerripothula, Jianfei Cai, and Junsong Yuan.
Image co-segmentation via saliency co-fusion. TMM, 2016.
[29] Diederik Kingma and Jimmy Ba. ADAM: A method for

stochastic optimization. In ICLR, 2014.

[30] Shu Kong and Charless Fowlkes. Recurrent pixel embedding

for instance grouping. In CVPR, 2018.

[31] Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-
Hsuan Yang, and Jan Kautz. Context-aware synthesis and
placement of object instances. In NIPS, 2018.

[32] Guanbin Li, Yuan Xie, Liang Lin, and Yizhou Yu. Instance-

level salient object segmentation. In CVPR, 2017.

[33] Weihao Li, Omid Hosseini Jafari, and Carsten Rother. Deep

object co-segmentation. In ACCV, 2018.

[34] Xin Li, Fan Yang, Hong Cheng, Wei Liu, and Dinggang
Shen. Contour knowledge transfer for salient object detec-
tion. In ECCV, 2018.

[35] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
In CVPR, 2017.

[36] Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang,
Liang Lin, and Shuicheng Yan. Proposal-free network for
instance-level object segmentation. TPAMI, 2018.

[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir
Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva
Ramanan, C. Lawrence Zitnick, and Piotr Doll´ar. Microsoft
COCO: Common objects in context. In ECCV, 2014.

[38] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
In

Path aggregation network for instance segmentation.
CVPR, 2018.

[19] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Ro-
bust image alignment with multiple feature descriptors and
matching-guided neighborhoods. In CVPR, 2015.

[39] Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu,
Houqiang Li, and Yan Lu. Afﬁnity derivation and graph
merge for instance segmentation. In ECCV, 2018.

8854

[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional models for semantic segmentation. In CVPR,
2015.

[59] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua Shen,
and Zhi-Hua Zhou. Unsupervised object discovery and co-
localization by deep descriptor transforming. PR, 2019.

[60] Tsun-Yi Yang, Jo-Han Hsu, Yen-Yu Lin, and Yung-Yu
Chuang. DeepCD: Learning deep complementary descrip-
tors for patch representations. In ICCV, 2017.

[61] Tsun-Yi Yang, Yen-Yu Lin, and Yung-Yu Chuang. Accumu-
lated stability voting: A robust descriptor from descriptors of
multiple scales. In CVPR, 2016.

[62] Zehuan Yuan, Tong Lu, and Yirui Wu. Deep-dense condi-
tional random ﬁelds for object co-segmentation. In IJCAI,
2017.

[63] Dingwen Zhang, Junwei Han, and Yu Zhang. Supervision by
fusion: Towards unsupervised learning of deep salient object
detector. In ICCV, 2017.

[64] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.

Instance-
level segmentation for autonomous driving with deep
densely connected mrfs. In CVPR, 2016.

[65] Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and Jianbin
Jiao. Weakly supervised instance segmentation using class
peak response. In CVPR, 2018.

[41] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin
Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep
features for salient object detection. In CVPR, 2017.

[42] Armin Mustafa and Adrian Hilton. Semantically coherent
In

co-segmentation and reconstruction of dynamic scenes.
CVPR, 2017.

[43] David Novotny, Samuel Albanie, Diane Larlus, and Andrea
Vedaldi. Semi-convolutional operators for instance segmen-
tation. In ECCV, 2018.

[44] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Fer-
ran Marques, and Jitendra Malik. Multiscale combinatorial
grouping for image segmentation and object proposal gener-
ation. TPAMI, 2017.

[45] Rong Quan, Junwei Han, Dingwen Zhang, and Feiping Nie.
Object co-segmentation via graph optimized-ﬂexible mani-
fold ranking. In CVPR, 2016.

[46] Michael Rubinstein, Armand Joulin, Johannes Kopf, and Ce
Liu. Unsupervised joint object discovery and segmentation
in internet images. In CVPR, 2013.

[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Fei-Fei Li.
ImageNet large scale visual recognition chal-
lenge. IJCV, 2015.

[48] Tong Shen, Guosheng Lin, Lingqiao Liu, Chunhua Shen, and
Ian Reid. Weakly supervised semantic segmentation based
on co-segmentation. In BMVC, 2017.

[49] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015.

[50] Charles Sutton and Andrew McCallum. Piecewise training

for structured prediction. ML, 2009.

[51] Kevin Tang, Armand Joulin, Li-Jia Li, and Fei-Fei Li. Co-

localization in real-world images. In CVPR, 2014.

[52] Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato. Joint re-
covery of dense correspondence and cosegmentation in two
images. In CVPR, 2016.

[53] Zhiqiang Tao, Hongfu Liu, Huazhu Fu, and Yun Fu.

Im-
age cosegmentation via saliency-guided constrained cluster-
ing with cosine similarity. In AAAI, 2017.

[54] Chung-Chi Tsai, Weizhi Li, Kuang-Jui Hsu, Xiaoning Qian,
and Yen-Yu Lin.
Image co-saliency detection and co-
segmentation via progressive joint optimization. TIP, 2018.
[55] Andrea Vedaldi and Karel Lenc. MatConvNet – Convolu-

tional neural networks for MATLAB. In ACMMM, 2015.

[56] Chuan Ping Wang, Hua Zhang, Liang Yang, Xiaochun Cao,
and Hongkai Xiong. Multiple semantic matching on aug-
mented n-partite graph for object co-segmentation. TIP,
2017.

[57] Wenguan Wang, Jianbing Shen, Hanqiu Sun, and Ling Shao.

Video co-saliency guided co-segmentation. TCSVT, 2018.

[58] Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie,
Jianxin Wu, Chunhua Shen, and Zhi-Hua Zhou. Deep de-
scriptor transforming for image co-localization.
In IJCAI,
2017.

8855

