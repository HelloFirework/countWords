Deep Single Image Camera Calibration with Radial Distortion

Manuel L´opez-Antequera*

Roger Mar´ı †
Javier Gonzalez-Jimenez‡

Pau Gargallo*
Gloria Haro§

Yubin Kuang*

Abstract

Single image calibration is the problem of predicting
the camera parameters from one image. This problem is
of importance when dealing with images collected in un-
controlled conditions by non-calibrated cameras, such as
crowd-sourced applications.
In this work we propose a
method to predict extrinsic (tilt and roll) and intrinsic (fo-
cal length and radial distortion) parameters from a single
image. We propose a parameterization for radial distor-
tion that is better suited for learning than directly predict-
ing the distortion parameters. Moreover, predicting addi-
tional heterogeneous variables exacerbates the problem of
loss balancing. We propose a new loss function based on
point projections to avoid having to balance heterogeneous
loss terms. Our method is, to our knowledge, the ﬁrst to
jointly estimate the tilt, roll, focal length, and radial distor-
tion parameters from a single image. We thoroughly analyze
the performance of the proposed method and the impact of
the improvements and compare with previous approaches
for single image radial distortion correction.

1. Introduction

Single image calibration deals with the prediction of
camera parameters from a single image. Camera calibration
is the ﬁrst step in many computer vision tasks e.g. Structure
from Motion, especially in applications where the captur-
ing conditions are not controlled is particularly challenging,
such as those relying on crowdsourced imagery.

The process of image formation is well understood and
has been studied extensively in computer vision [1], allow-
ing for very precise calibration of cameras when there are
enough geometric constraints to ﬁt the camera model. This
is a well established practice that is performed daily on an
industrial scale, but requires a set of images taken for the
purpose of calibration. Geometric based methods can also
be used with images taken outside of the lab, performing

*Mapillary, ﬁrstname@mapillary.com
†CMLA, ENS Cachan, mari@cmla.ens-cachan.fr
‡Universidad de M´alaga, javiergonzalez@uma.es
§Universitat Pompeu Fabra, gloria.haro@upf.edu

Figure 1. Our method is able to recover extrinsic (tilt, roll) and
intrinsic (focal length and radial distortion) parameters from sin-
gle images (top row). In the bottom row, we visualize the predicted
parameters by undistorting the input images and overlaying a hori-
zon line, which is a proxy for the tilt and roll angles.

best on images depicting man-made environments present-
ing strong cues such as vanishing points and straight lines
that can be used to recover the camera parameters [2, 3].
However, since geometric-based methods rely on detect-
ing and processing speciﬁc cues such as straight lines and
vanishing points, they lack robustness to images taken in
unstructured environments, with low quality equipment or
difﬁcult illumination conditions.

In this work we present a method to recover extrinsic
(tilt, roll) and intrinsic (focal length and radial distortion)
parameters given a single image. We train a convolutional
neural network to perform regression on alternative repre-
sentations of these parameters which are better suited for
prediction from a single image.

We advance with respect to the state of the art with three
main contributions: 1. a single parameter representation for
k1 and k2 based on a large database of real calibrated cam-
eras. 2. a representation of the radial distortion that is in-
dependent from the focal length and more easily learned by
the network. 3. a new loss function based on the projec-
tion of points to alleviate the problem of balancing hetero-

111817

geneous loss components.

To the best of our knowledge, this work is the ﬁrst
to jointly estimate the camera orientation and calibration
jointly while including radial distortion.

2. Related Work

Recent works have leveraged the success of convolu-
tional neural networks and proposed using learned methods
to estimate camera parameters. Through training, a CNN
can learn to detect the subtle but relevant cues for the task,
extending the range of scenarios where single image cali-
bration is feasible.

Different components of the problem of learned single
image calibration have been studied in the past: Workman
et al. [4] trained a CNN to perform regression of the ﬁeld
of view of a pinhole camera, later focusing on detecting the
horizon line on images [5], which is a proxy for the tilt and
roll angles of the camera if the focal length is known.

Rong et al. [6] use a classiﬁcation approach to calibrate
the single-parameter radial distortion model from Fitzgib-
bon [7]. Hold-Geoffroy et al. [8] ﬁrst combined extrin-
sic and intrinsic calibration in a single network, predicting
the tilt, roll and focal length of a pinhole camera through
a classiﬁcation approach. They relied on upright 360 de-
gree imagery to synthetically generate images of arbitrary
size, focal length and rotation, an approach that we borrow
to generate training data. Classic and learned methods can
be combined. In [9], learned methods are used to obtain a
prior distribution on the possible camera parameters, which
are then reﬁned using classic methods, accelerating the ex-
ecution time and robustness with respect to fully geomet-
ric methods. We do not follow such an approach in this
work. However, the prediction produced by our method can
be used as a prior in such pipelines.

When training a convolutional neural network for sin-
gle image calibration, the loss function is an aggregate of
several loss components, one for each parameter. This sce-
nario is usually known as multi-task learning [10]. Works
in multi-task learning deal with the challenges faced when
training a network to perform several tasks with separate
losses. Most of these approaches rely on a weighted sum
of the loss components, differing on the manner in which
the weights are set at training time: Kendall et al. [11] use
Gaussian and softmax likelihoods (for regression and clas-
siﬁcation, respectively) to weight the different loss compo-
nents according to a task-dependent uncertainty.
In con-
trast to these uncertainty based methods, Chen et al. [12]
determine the value of the weights by adjusting the gradient
magnitudes associated to each loss term.

When possible, domain knowledge can be used instead
of task-agnostic methods in order to balance loss compo-
nents: Yin et al. [13] perform single image calibration of an
8-parameter distortion model of ﬁsheye lenses. They note

the difﬁculty of balancing loss components of different na-
ture when attempting to directly minimize the parameter er-
rors and propose an alternative based on the photometric
error. In this work, we also explore the problem of balanc-
ing loss components for camera calibration and propose a
faster approach based on projecting points using the cam-
era model instead of deforming the image to calculate the
photometric error.

3. Method

We brieﬂy summarize our method and describe the de-

tails in subsequent sections.

We train a convolutional neural network to predict the
extrinsic and intrinsic camera parameters of a given image.
To achieve this, we use independent regressors that share a
common pretrained network architecture as the feature ex-
tractor, which we ﬁne-tune for the task. Instead of training
these regressors to predict the tilt θ, roll ψ, focal length f ,
and distortion parameters k1 and k2, we use proxy variables
that are directly visible in the image and independent from
each other. To obtain training data for the network, we rely
on a diverse panorama dataset from which we crop and dis-
tort panoramas to synthesize images taken using perspective
projection cameras with arbitrary parameters.

3.1. Camera Model

We consider a camera model with square pixels and cen-
tered principal point that is affected by radial distortion that
can be modeled by a two-parameter polynomial distortion.
The projection model is the following. World points
are transformed to local reference frame of the camera by
applying a rotation R and translation t. Let (X, Y, Z) be
the coordinates of a 3D point expressed in the local refer-
ence frame of the camera. The point is projected to the
plane Z = 1 to obtain the normalized image coordinates
(x, y) = (X/Z, Y /Z). Radial distortion scales the normal-
ized coordinates by a factor d, which is a function of the
radius r and the distortion coefﬁcients k1 and k2:

r =px2 + y2

d = 1 + k1r2 + k2r4

(xd, yd) = (d x, d y).

(1)

(2)

Finally, the focal length f scales the normalized and dis-
torted image coordinates to pixels: (ud, vd) = (f xd, f yd).
In this work, we do not attempt to recover the position
of the images nor the full rotation matrix, as that would re-
quire the network to memorize the appearance of the en-
vironment, turning our problem of single image calibration
into a different problem, so-called place recognition.

Instead, we rely on the horizon line as a reference frame,
the tilt θ and roll ψ angles
leaving two free parameters:
of the camera with respect to the horizon. This allows a

11818

ψ

ρ

τ

2
k

0.15

0.10

0.05

0.00

0.019k1 + 0.805k2

1

h

h = 2f tan( Fv
2 )

CC BY 2.0, photo by m01229

Figure 2. We use an alternative representation for the camera pa-
rameters that is based on image cues: The network is trained to
predict the distorted offset ρ and vertical ﬁeld of view Fv instead
of the tilt θ and focal length f . The undistorted offset τ is where
the horizon would be if there was no radial distortion.

network to be trained using images from a set of locations to
generalize well to other places, as long as there is sufﬁcient
visual diversity.

Thus, the parameters to be recovered by the network are
the tilt and roll angles (θ, ψ), the focal length f and distor-
tion parameters k1 and k2.

3.2. Parameterization

As revealed by previous work [4, 5, 8], an adequate pa-
rameterization of the variables to predict can greatly bene-
ﬁt convergence and ﬁnal performance of the network. For
the case of camera calibration, parameters such as the fo-
cal length or the tilt angles are difﬁcult to interpret from the
image content. Instead, they can be better represented by
proxy parameters that are directly observable in the image.
We begin by following already existing parameterizations
and propose new ones required to deal with the case of ra-
dially distorted images. We refer the reader to Figure 2 to
complement the text in this section.

We start by deﬁning the horizon line as done in [5]: “The
image location of the horizon line is deﬁned as the projec-
tion of the line at inﬁnity for any plane which is orthogonal
to the local gravity vector.”. This deﬁnition also holds true
for cameras with radial distortion, however, the projection
of the horizon line in the image will not necessarily remain
a straight line.1

The focal length f is related to the vertical and horizontal
ﬁelds of view through the image size of height h and width
w. The ﬁeld of view is directly related to the image content
and is thus more suitable for the task. We use the vertical

1If there is radial distortion, the horizon line (and any other straight
lines) will only be projected as a straight line in the image if it passes
through the center of the image.

−0.4

−0.3

−0.2

k1

−0.1

0.0

Figure 3. A distribution of k1 and k2 recovered from a large set
of SfM reconstructions reveals that, for many real cameras, these
parameters lie close to a one-dimensional manifold. We rely on
this to simplify our camera model such that k2 is a function of k1.

ﬁeld of view, deﬁned as

Fv = 2 arctan

h
2f

,

(3)

as a proxy for the focal length. During deployment of the
network, the image height h is known and the focal length
can be recovered from the predicted Fv.

The roll angle ψ of the camera is directly represented in
the image as the angle of the horizon line, not requiring any
alternative parameterization.

A good proxy for the tilt angle θ is the distance ρ from
the center of the image to the horizon line. Previous work
used such a parameterization for pinhole cameras with no
distortion [5], however, the presence of radial distortion
complicates this relationship slightly. We ﬁrst deﬁne the
undistorted offset τ as the distance from the image center to
the horizon line when there is no radial distortion. It can be
expressed as a function of the tilt angle and the focal length:

τ = f tan(θ).

(4)

The distorted offset ρ is related with τ by the radial distor-
tion scaling as expressed in Equation 2.

3.2.1 Distortion coefﬁcients in real cameras

We simplify the radial distortion model by expressing k2 as
a function of k1. This decision was initially motivated by a
practical consideration: independently sampling k1 and k2
often results in unrealistically distorted images. For images
from real lenses, the distortion coefﬁcients seem to lie in a
manifold. We conﬁrm this by studying the distribution of
k1 and k2 on a large collection of camera calibrations.

We use Structure from Motion (SfM) with self-
calibration to perform reconstructions on image sequences
taken with real cameras to estimate their parameters. We

11819

As with the focal length and the tilt, we propose to use
an alternative parameterization to express k1 in terms of a
visible magnitude, i.e. the distortion that is observed in the
image. We will then train the network to predict an apparent
distortion coefﬁcient that we denote as ˆk1.

As stated in Section 3.1, the camera model projects
points (X, Y, Z) in the camera reference frame to 2D nor-
malized camera coordinates (x, y) = (X/Z, Y /Z).

In the absence of radial distortion, pixels are obtained
from the undistorted normalized coordinates as (u, v) =
(f x, f y). When there is radial distortion, the radius of the
normalized coordinates is ﬁrst distorted before being con-
verted to pixels (ud, vd) = (f d x, f d y). In other words,
since the distortion is applied to the normalized image coor-
dinates, the visual effect not only depends on the distortion
parameters, but also also on the focal length.

Instead, we seek to represent the distortion effect as a
relationship between the distorted (ud, vd) and undistorted
pixels (u, v). Let us begin by expressing the radius of a
point r in normalized coordinates and its equivalent in pixel
units rpx :

r = rpx/f.

The same relationship holds when there is distortion:

r(d) = r(d)

px /f.

(6)

(7)

The undistorted and distorted points in normalized camera
coordinates are related by Eq. 2 and can be expressed as

r(d) = r(cid:0)1 + k1r2 + k2r4(cid:1) ,

(8)

in which we substitute r and r(d) from Eqs. 6 and 7 to obtain
the relationship between the radii in pixel units, obtaining
the apparent distortion coefﬁcients ˆk1and ˆk2:

ˆk1

ˆk2

r(d)

px = rpx 1 +

z}|{k1
f 2 r2

px +

px!
z}|{k2
f 4 r4

ˆk1 = k1/f 2

(9)

(10)

f = 0.8, k1 = −0.13

f = 3.2, k1 = −2.05

Figure 4. The apparent radial distortion ˆk1 represents the distortion
effect independently of the focal length f .
In these images we
ﬁx k2 = 0 and vary k1 and f while keeping a constant value of
ˆk1 = −0.2. Note that the curvature of the lines remains constant
after zooming in.

downloaded a collection of 1000 street-level imagery se-
quences of 100 geotagged images each from Mapillary.2
These sequences were captured by a diverse set of over 300
cameras, including most popular consumer-grade smart-
phones and action cameras that have been in the market
for the last 4 years. Sequences were selected such that the
SfM reconstructions would constrain the camera parame-
ters: they present loop closures or trajectories that are not a
straight line (as reported by the GPS geotag). The camera
parameters of each sequence are recovered as part of the re-
construction through bundle adjustment [14]. Since SfM is
sensitive to the initial calibration parameters, we repeat the
reconstructions initializing with the newly estimated cam-
era parameters until convergence.

The resulting set of radial distortion coefﬁcients is shown
in Figure 3, conﬁrming our initial observation. We obtain an
analytic expression as a model of this distribution by ﬁtting
a second degree polynomial

k2 = 0.019k1 + 0.805k2
1.

(5)

We observe two main groups of lenses: Fisheye lenses,
exhibiting strong radial distortion, with k1 < 0 and positive
k2 increasing in a quadratic manner with the magnitude of
k1, and conventional lenses, with both k1 and k2 close to 0.

3.2.2 Apparent distortion

Inferring the value of k1 from an image is not trivial. A
human observer would probably make a guess based on
the bending of straight lines. Nevertheless, both the focal
length and the radial distortion coefﬁcients determine such
bending. Radial distortion is more noticeable towards the
boundaries of the image but, as the focal length increases,
we gradually see a smaller crop of the center of the image.

2Mapillary is a crowdsourced street-level imagery platform.

Observe that for a ﬁxed value of k1, ˆk1 decreases as
f increases and vice-versa, representing the effect of ra-
dial distortion independently from f as shown in Figure 4.
Given a prediction of ˆk1 and f , both recoverable from the
network outputs, k1 and k2 can be retrieved through equa-
tions 5 and 10.

In summary, we represent a camera’s intrinsic and ex-
trinsic parameters with Ω = (ψ, ρ, Fv, ˆk1), where ψ is the
roll angle, ρ is the distorted offset, Fv is the vertical ﬁeld of
view and ˆk1 is the apparent radial distortion.

11820

f ′

pn

x1

xn

p′
n

p1
p′
1

f

Figure 5. An illustration of the projections used for the bearing
loss simpliﬁed by reducing it to two parameters: tilt θ (represented
by the orientation of the cameras) and focal length f . Two cam-
eras are used to project a regular grid of points x1 . . . xn onto
the unit sphere. The points p1 . . . pn, shown in green, are pro-
jected using the ground truth camera parameters Ω = (θ, f ).
The points p
n are projected using the predicted parameters
Ω′ = (θ′, f ′) and are shown in red. We obtain gradients for the
predicted camera parameters Ω′ through backpropagation of the
mean squared distance between points p

n and p1 . . . pn.

1 . . . p

1 . . . p

′

′

′

′

3.3. Bearing Loss

When a single architecture is trained to predict parame-
ters with different magnitudes, special care must be taken
to weigh the loss components such that the estimation of
certain parameters do not dominate the learning process.
We notice that for the case of camera calibration, instead
of optimizing the camera parameters separately, a single
metric based on the projection of points with the estimated
and ground truth camera parameters can be used. Let us
begin with the observation that a camera model is essen-
tially a simpliﬁed bidirectional mapping from pixel coor-
dinates in the image plane to bearings (direction vectors)
in 3D [15, 16]. The camera intrinsic and extrinsic param-
eters determine the direction of one such bearing for each
pixel in the image. The proposed loss measures errors on
these direction vectors instead of individual parameter er-
rors, achieving the goal of representing all the parameter
errors as a single metric.

Given an image taken with known camera parameters
Ω = (ψ, ρ, Fv, ˆk1) and a prediction of such parameters
given by the network Ω′ = (ψ′, ρ′, F ′
1), the bearing loss
is calculated as follows.

v, ˆk′

First, a regular grid of points x1 . . . xn is projected from
the image plane onto the unit sphere using the ground
truth parameters Ω obtaining the ground truth bearings
p1 . . . pn.3

3In order to project the points, the original parameter set ψ, θ, f, k1, k2
required by the camera model is recovered from the proxy parameters Ω
using equations 2, 3, 4, 5.

Then, the parameters Ω′ predicted by the network are
used to project the same grid points onto the unit sphere,
obtaining the set of predicted bearings p
n. We deﬁne
the bearing loss as the mean squared deviation between the
two sets of bearings:

1 . . . p

′

′

L(Ω′, Ω) =

1
n

n

Xi=1

(p

′

i − pi)2.

(11)

This process is illustrated in Figure 3.3.

To optimize this loss, the mapping from pixels to bear-
ings must be differentiable. This includes the radial undis-
tortion step, which does not have a closed-form solution.
Although there are several solutions for r in r(d) = r(1 +
k1r2+k2r4), the correct solution is that where r is closest to
r(d), which can be reliably found by performing ﬁxed point
iteration4 of the function rn+1 = r(d)/(1+k1r2
n) ini-
tialized at r0 = r(d). This process is differentiable and can
be used during training to backpropagate gradients through
the bearing loss.

n+k2r4

3.3.1 Disentangling sources of loss errors

The proposed loss solves the task balancing problem by ex-
pressing different errors in terms of a single measure. How-
ever, using several camera parameters to predict the bear-
ings introduces a new problem during learning: the devi-
ation of a point from its ideal projection can be attributed
to more than one parameter. In other words, an error from
one parameter can backpropagate through the bearing loss
to other parameters.

For example, picture a scenario where, for a training
sample, the network predicts all parameters perfectly ex-
cept for an excessively small ﬁeld of view: The predicted
bearings p
n are projected onto a smaller area on the
unit sphere than the ground truth bearings p1 . . . pn.

1 . . . p

′

′

In this case, there is more than one parameter that could
be modiﬁed to decrease this distance: both the focal length
and the radial distortion parameters can be changed to de-
crease the loss, but only the value of the focal length should
be modiﬁed, as the radial distortion has been perfectly
predicted in this example.
In other words, there will be
gradients propagating back through both parameters, even
though one of them is correct, causing the network to devi-
ate from the optimal solution. In practice, this slows down
learning and causes the accuracy to stagnate.

To avoid this problem, we disentangle the bearing loss,

4We implement this by repeatedly iterating and breaking on conver-
gence in PyTorch, but it rarely requires more than 4 steps to converge, so
it could be unrolled to a set number of iterations if using a framework that
relies on ﬁxed computational graphs.

11821

Parameter

Distribution

Values

Pan φ
Uniform
Distorted offset ρ Normal
Roll ψ
Cauchy
Aspect ratio w/h Varying

Focal length f
Distortion k1
Distortion k2

Uniform
Uniform

[0, 2π)
µ = 0.046, σ = 0.6
x0 = 0, γ ∈ {0.001, 0.1}
{1/1 9%, 5/4 1%, 4/3 66%,
3/2 20%, 16/9 4%}
[13, 38]
[−0.4, 0]
k2 = 0.019k1 + 0.805k2

1

Table 1. Distribution of the camera parameters used to generate our
training and validation sets. Units: f - mm, ψ- radians, ρ- fraction
of image height.

evaluating it individually for each parameter ψ, ρ, Fv, ˆk1:

v

Lψ = L((ψGT , ρGT , F GT
Lρ = L((ψGT , ρGT , F GT
LFv = L((ψGT , ρGT , F GT
= L((ψGT , ρGT , F GT

v

v

Lˆk1

v

1

1

, ˆkGT
, ˆkGT
, ˆkGT
, ˆkGT

1

1

), Ω)

), Ω)

), Ω)

), Ω)

L∗ =

Lψ + Lρ + LFv + Lˆk1

4

(12)

This modiﬁcation of the loss function greatly increases
convergence and ﬁnal accuracy, while maintaining the main
advantage of the bearing loss of expressing all parameter
errors in the same units.

3.4. Dataset

We use the SUN360 panorama dataset [17] to artiﬁcially
generate images taken with by cameras with arbitrary pan
φ, tilt θ, roll ψ, focal length f and distortion k1. High reso-
lution images of 9104 × 4452 pixels are used to render the
training and evaluation images as follows:

First, we divide the SUN360 dataset into training, evalu-
ation and test sets of 55681, 1298 and 165 images, respec-
tively. Separating the panorama dataset before generating
the perspective images ensures that no panoramas are used
to generate crops that end up in different datasets.

Then, from each panorama in the training and validation
sets, we generate seven perspective images by randomly
sampling the pan φ, offset ρ, roll ψ, aspect ratio, focal
length f and the distortion coefﬁcient k1 from the proba-
bility distributions found in Table 1, resulting in a dataset of
389, 767 training and 9, 086 validation images.

In a practical scenario, the distribution of the training set
should be designed to mimic that of the images that will
be used when deploying the network. For this paper we
have selected simple distributions that are consistent with
those found in large online image databases: we take the
same distributions as in previous work [8], except for the
inclusion of k1 for radial distortion. Additionally, we have

modiﬁed the distribution of f to be uniform in order to avoid
obtaining images with large focal lengths since the effect of
radial distortion in such images is negligible5.

For the test set we followed a different approach,
sampling from the 165 panoramas in the panorama test
set more extensively and evenly by taking 100 crops
from each panorama and using uniform distributions
also for the roll angle ψ ∼ U (−π/2, π/2), distorted
offset ρ ∼ U(−1.2, 1.2) and aspect ratios w/h ∼
U{1/1, 5/4, 4/3, 3/2, 16/9}. This results in 16, 500 im-
ages for our test set.

4. Experiments

We use a densenet-161 [18] pretrained on ImageNet [19]
as a feature extractor and replace the classiﬁer layer with
four regressors, each consisting of a ReLU-activated hidden
layer of 256 units followed by the output unit.

As explained before, images are generated with a vari-
ety of aspect ratios. We experimented with several ways
of feeding such images to the network: resizing, center-
cropping and letterboxing. Previous authors noticed bet-
ter results by square-cropping the images [5]. Like Hold-
Geoffroy et al. [8], we obtained best results by resizing the
images to a square. Even though there is deformation in the
image when its aspect ratio is changed, it appears to be that
keeping all of the image content by not cropping the image
is preferable to any negative effect the warping itself may
produce. All images are thus scaled to 224 × 224 pixels
before feeding them to the network.

We train the network by directly minimizing parameter
errors as well as using the proposed bearing loss. In the ﬁrst
case, we minimize a sum of weighted Huber losses:

LH = wψLH

ψ + wρLH

ρ + wFv LH

Fv + wˆk1

LH
ˆk1

(13)

For the bearing loss, the predicted and ground truth parame-
ters of each image are used to project bearings as described
in Section 3.3.

In both cases we minimize the losses using an Adam op-
timizer with learning rate 10−4 in batches of 42 images.
Through early stopping we ﬁnish training after 8-10 epochs.
We use a step learning rate decay such that the learning rate
is reduced by 30% at the end of each epoch.

4.1. Evaluation of the Loss Functions

We evaluate the bearing loss from Section 3.3 and com-
pare it to the weighted Huber loss (Eq. 13). The Huber loss
with unit weights performs better.

The results are comparable, except for the prediction of
ˆk1 which does not perform as well with the bearing loss.

5An additional problem with the choice of a long-tailed distribution to
sample the focal length is that it may produce values for f equivalent to
very low resolution crops.

11822

ψ

ρ

v
F

1
ˆk

Projection

Optimal Reg.

Regression

Figure 6. Optimal selection of the weights when combining differ-
ent loss components can greatly inﬂuence training. In gray, models
trained with one weight in {wψ, wρ, wFv , wˆk1 } set to 100 and the
rest to 1. In red, a model trained with all weights set to 1. In green,
a model trained with the bearing loss. These results indicate that
selecting appropriate weights is important for this task, but that
with the proposed parameterization, selecting unit weights yields
results that are better than the proposed bearing loss.

However, that may not always be the case, for example,
when using a different camera model, as reported by Yin
et al. [13], or when using a different parameterization than
the one we propose here. It just happens that this parameter-
ization is well suited to be trained with unit weights. To il-
lustrate the effect of selecting less optimal weights, we have
trained several networks using the weighted sum of Huber
losses (Eq. 13) with different sets of weights and compare
the resulting validation error curves in Figure 6.

For the rest of the paper, our approach or our network
refer to a network trained by minimizing the proposed pa-
rameters (Fv, ˆk1, ρ, ψ) directly using a sum of Huber losses
(Eq. 13) with wψ = wρ = wFv = wˆk1

= 1.

4.2. Effect of Distortion Parameterization

We compare the proposed parameterizations for the ra-
dial distortion coefﬁcient and the radially distorted offset
with a naive approach. For this purpose, we train a base-
line network to predict the distortion coefﬁcient and undis-
torted offset (k1, τ ), instead of the proposed apparent distor-
tion and distorted offset (ˆk1, ρ). The remaining parameters
(ψ, Fv) are as in our network. In both cases, we minimize
the sum of Huber losses from Eq. 13 with unit weights. The
remaining settings for the experiment are as described in
Section 4. After training, we compare the predictions of
both networks on the test set. Figure 7 shows scatter plots
comparing the predictions of ˆk1 and k1, as well as those
of the distorted offset ρ and undistorted offset τ , revealing
that the proposed parameterization is easier to learn (more
accurately predicted) than the baseline.

k1

ˆk1

τ

ρ

-0.3

-0.1

-2.0

-1.0

-0.5

0.6

-0.5

0.6

Figure 7. A comparison of the predictions of two networks: Our
approach (predicting the apparent distortion ˆk1 and the distorted
offset ρ) and a baseline predicting the distortion coefﬁcient k1 and
the undistorted offset τ . The horizontal and vertical axes in each
plot represent the ground truth and predicted values, respectively.
The diagonal line indicates a perfect prediction. Learning to pre-
dict ˆk1 is an easier task than directly predicting k1, as it is inde-
pendent of the focal length f . The distorted offset ρ is also easier
to predict than the undistorted offset τ , since it is directly visible
in the image and is independent of the distortion.

4.3. Error Distributions

There is a lack of consensus when evaluating single im-
age calibration networks: some previous works follow a
classiﬁcation approach and directly report accuracy values
[5]. Others establish a threshold on the regression errors and
also report accuracy values [8, 4]. Yin et al. [13] report peak
signal-to-noise ratio structural similarity errors. Rong et al.
[6] use a metric based on straight line segment lengths that
is only meaningful for radial distortion correction. Hold-
Geoffroy et al. [8] report error distributions grouped accord-
ing to the ground truth values in a box-percentile chart.

We follow the evaluation procedure from [8] of reporting
the error distributions of the predicted parameters. How-
ever, instead of reporting errors in terms of the alternative
parameterization used to ease learning (roll ψ, distorted off-
set ρ, ﬁeld of view Fv and apparent radial distortion ˆk1),
we report the errors in: roll ψ, tilt θ, focal length f and ra-
dial distortion coefﬁcient k1, since they are more commonly
used than the proposed parameterization and can be easily
compared with other approaches.

These error distributions are shown in Figure 8. The di-
agonal plots show the error distribution of the prediction
of each parameter with respect to its ground truth value.
We also study the error distributions of each parameter with
respect to the ground truth values of the other parameters.
This is shown in the off-diagonal plots, revealing some in-
teresting insights. For example, the plots from the ﬁrst col-
umn indicate the error distributions of all parameters with
respect with the ground truth value of the tilt angle θ. No-
tice that when θ is small (i.e. when the horizon is close to
the center of the image), the prediction errors for the tilt and
roll angles are small as well, while the errors for the focal
length f and the radial distortion coefﬁcient k1 are relatively
large. This is expected as many lines in the world are ver-
tical and parallel to the image plane when the tilt is zero,

11823

r
o
r
r
e

θ

r
o
r
r
e
ψ

50

35

20

5

70

40

10

r
o
r
r
e

f

9.0

5.0

1.5

r
o
r
r
e

1
k

.25

.15

.05

-60

-20

20

60

-40

-13

13

40

17

23

28

34

-0.33 -0.24 -0.16 -0.07

θ

ψ

f

k1

Figure 8. Errors on the test set of 16,500 images. The horizontal axis represents the ground truth values, while the vertical axis represents
the absolute error of the predictions. We show errors as a function of the ground truth value of the same parameter, as well as as a function
of other parameter’s ground truth values.

providing no information for predicting the focal length.

As stated in Section 3.4, the training set should be gen-
erated to replicate the distribution of images that will be
seen when deploying such a network. We expect the error
distributions to change according to the distribution of the
training data, since the span of these data directly relates to
the difﬁculty of the problem. For this reason, the absolute
errors seen in Figure 8 are not as relevant as the relation-
ships among them. These errors should be studied for the
speciﬁc application domain where a network like this is to
be deployed.

4.4. Comparison with Geometric based Undistor 

tion

Plumb-line methods are a well-known approach for sin-
gle image undistortion in the wild. These techniques predict
lens distortion based on the detected curvature of lines. We
compare our method to a state of the art plumb-line algo-
rithm by Santana-Cedr´es et al. [20]. Since they use a differ-
ent parameterization for the radial distortion, we compare
the photometric mean squared error on images from the test
set undistorted by both methods [21].

We obtain a lower MSE in 89% of the images in the
test set, but notice differences depending on the category
of the source panorama: for outdoor images with few or no
line segments (nature landscapes or open spaces with trees
and monuments), our method performs best in more than

90% of the images. The difference narrows down for indoor
and urban imagery, with our method outperforming [20] in
70-90% of the cases, depending on the category. This is ex-
pected as there are more line segments in images from these
classes that the plumb-line algorithm can rely on.

We attribute the higher accuracy of learned undistortion
to the ability of the model to interpret semantic cues. The
reader is referred to the supplementary material for a de-
tailed discussion.

5. Conclusions

We present a learning-based method that jointly predicts
the extrinsic and intrinsic camera parameters, including ra-
dial distortion. The proposed parameterization is disentan-
gled from the focal length and well suited for prediction.
We also introduce a new loss function to overcome the prob-
lem of loss balancing. Finally, we validate the superior per-
formance of the proposed method against geometric-based
undistortion methods.

In future work, we will explore distortion calibration
with single-parameter distortion models [7, 22]. More im-
portantly, we will apply single image camera calibration in
large-scale structure from motion on crowdsourced images
with diverse camera models, where we see the potential of
learning-based methods to enhance the robustness of the
system.

11824

References

[1] R. Hartley and A. Zisserman, Multiple view geometry in

computer vision. Cambridge university press, 2003. 1

[12] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabi-
novich, “GradNorm: Gradient normalization for adaptive
loss balancing in deep multitask networks,” arXiv preprint
arXiv:1711.02257, 2017. 2

[2] B. Caprile and V. Torre, “Using vanishing points for camera
calibration,” International Journal of Computer Vision,
vol. 4, no. 2, pp. 127–139, mar 1990. [Online]. Available:
https://doi.org/10.1007/BF00127813 1

[13] X. Yin, X. Wang, J. Yu, M. Zhang, P. Fua, and D. Tao,
“FishEyeRecNet: A Multi-Context Collaborative Deep
Network for Fisheye Image Rectiﬁcation,” pp. 1–16, 2018.
[Online]. Available: http://arxiv.org/abs/1804.04784 2, 7

[3] J. Deutscher, M. Isard, and J. MacCormick, “Automatic
Camera Calibration from a Single Manhattan Image,” in
Computer Vision — ECCV 2002, A. Heyden, G. Sparr,
M. Nielsen, and P. Johansen, Eds.
Berlin, Heidelberg:
Springer Berlin Heidelberg, 2002, pp. 175–188. 1

[4] S. Workman, C. Greenwell, M. Zhai, R. Baltenberger, and
N. Jacobs, “DEEPFOCAL: A method for direct focal length
estimation,” in 2015 IEEE International Conference on Im-
age Processing (ICIP), sep 2015, pp. 1369–1373. 2, 3, 7

[5] S. Workman, M. Zhai, and N. Jacobs, “Horizon Lines
[Online]. Available:

in the Wild,” pp. 1–12, 2016.
http://arxiv.org/abs/1604.02129 2, 3, 6, 7

[6] J. Rong, S. Huang, Z. Shang, and X. Ying, “Radial Lens
Distortion Correction Using Convolutional Neural Networks
Trained with Synthesized Images,” in Computer Vision –
ACCV 2016, S.-H. Lai, V. Lepetit, K. Nishino, and Y. Sato,
Eds. Cham: Springer International Publishing, 2017, pp.
35–49. 2, 7

[7] A. W. Fitzgibbon, “Simultaneous linear estimation of multi-
ple view geometry and lens distortion,” in Computer Vision
and Pattern Recognition, 2001. CVPR 2001. Proceedings
of the 2001 IEEE Computer Society Conference on, vol. 1.
IEEE, 2001, pp. I–I. 2, 8

[8] Y. Hold-Geoffroy, K. Sunkavalli, J. Eisenmann, M. Fisher,
E. Gambaretto, S. Hadap, and J.-F. Lalonde, “A Perceptual
Measure for Deep Single Image Camera Calibration,”
2018 IEEE Conference on Computer Vision and Pattern
http:
Recognition (CVPR), 2018.
//arxiv.org/abs/1712.01259 2, 3, 6, 7

[Online]. Available:

[9] M. Zhai, S. Workman, and N. Jacobs, “Detecting Vanishing
Points Using Global Image Context in a Non-Manhattan
World,” 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5657–5665, 2016.
[Online]. Available:
http://ieeexplore.ieee.org/document/
7780979/ 2

[10] R. Caruana, “Multitask learning,” Machine learning, vol. 28,

no. 1, pp. 41–75, 1997. 2

[11] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning us-
ing uncertainty to weigh losses for scene geometry and se-
mantics,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 2

[14] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgib-
bon, “Bundle adjustment—a modern synthesis,” in Interna-
tional workshop on vision algorithms. Springer, 1999, pp.
298–372. 4

[15] P. Sturm and S. Ramalingam, “A Generic Concept for
Camera Calibration,” in 8th European Conference on
Computer Vision (ECCV ’04),
ser. Lecture Notes in
Computer Science (LNCS), J. Matas and T. Pajdla, Eds.,
vol. 3022/2004.
Springer-
Verlag, May 2004, pp. 1–13. [Online]. Available: https:
//hal.inria.fr/inria-00524411 5

Prague, Czech Republic:

[16] S. Ramalingam, S. K. Lodha, and P. Sturm, “A generic
structure-from-motion framework,” Computer Vision and
Image Understanding, vol. 103, no. 3, pp. 218–228, 2006.
[Online]. Available: https://hal.inria.fr/inria-00384319 5

[17] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, “Recogniz-
ing scene viewpoint using panoramic place representation,”
in Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on.

IEEE, 2012, pp. 2695–2702. 6

[18] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-
berger, “Densely connected convolutional networks.” in
CVPR, vol. 1, no. 2, 2017, p. 3. 6

[19] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual
Recognition Challenge,” International Journal of Computer
Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015. 6

[20] D. Santana-Cedr´es, L. Gomez, M. Alem´an-Flores, A. Sal-
gado, J. Esclar´ın, L. Mazorra, and L. Alvarez, “An iterative
optimization algorithm for lens distortion correction using
two-parameter models,” Image Processing On Line, vol. 6,
pp. 326–364, 2016. 8

[21] R. Szeliski, “Prediction error as a quality metric for motion
and stereo,” in Proceedings of the Seventh IEEE Interna-
tional Conference on Computer Vision, vol. 2, Sep. 1999,
pp. 781–788 vol.2. 8

[22] C. Ishii, Y. Sudo, and H. Hashimoto, “An image conversion
algorithm from ﬁsh eye image to perspective image for hu-
man eyes,” in Advanced Intelligent Mechatronics, 2003. AIM
2003. Proceedings. 2003 IEEE/ASME International Confer-
ence on, vol. 2.

IEEE, 2003, pp. 1009–1014. 8

11825

