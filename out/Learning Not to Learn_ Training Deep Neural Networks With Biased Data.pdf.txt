Learning Not to Learn: Training Deep Neural Networks with Biased Data

Byungju Kim1

Hyunwoo Kim2
Sungjin Kim3
School of Electrical Engineering, KAIST, South Korea1

Kyungsu Kim3

Junmo Kim1

Beijing Institute of Technology2

Samsung Research3

{byungju.kim,junmo.kim}@kaist.ac.kr

{hwkim}@bit.edu.cn

{ks0326.kim,sj9373.kim}@samsung.com

Abstract

We propose a novel regularization algorithm to train
deep neural networks, in which data at training time is
severely biased. Since a neural network efﬁciently learns
data distribution, a network is likely to learn the bias in-
formation to categorize input data. It leads to poor perfor-
mance at test time, if the bias is, in fact, irrelevant to the
categorization. In this paper, we formulate a regularization
loss based on mutual information between feature embed-
ding and bias. Based on the idea of minimizing this mutual
information, we propose an iterative algorithm to unlearn
the bias information. We employ an additional network to
predict the bias distribution and train the network adversar-
ially against the feature embedding network. At the end of
learning, the bias prediction network is not able to predict
the bias not because it is poorly trained, but because the
feature embedding network successfully unlearns the bias
information. We also demonstrate quantitative and quali-
tative experimental results which show that our algorithm
effectively removes the bias information from feature em-
bedding.

1. Introduction

Machine learning algorithms and artiﬁcial intelligence
have been used in wide ranging ﬁelds. The growing vari-
ety of applications has resulted in great demand for robust
algorithms. The most ideal way to robustly train a neu-
ral network is to use suitable data free of bias. However
great effort is often required to collect well-distributed data.
Moreover, there is a lack of consensus as to what constitutes
well-distributed data.

Apart from the philosophical problem, the data distribu-
tion signiﬁcantly affects the characteristics of networks, as
current deep learning-based algorithms learn directly from
the input data. If biased data is provided during training,

Figure 1. Detrimental effect of biased data. The points colored
with high saturation indicate samples provided during training,
while the points with low saturation would appear in test scenario.
Although the classiﬁer is well-trained to categorize the training
data, it performs poorly with test samples because the classiﬁer
learns the latent bias in the training samples.

the machine perceives the biased distribution as meaningful
information. This perception is crucial because it weakens
the robustness of the algorithm and unjust discrimination
can be introduced.

A similar concept has been explored in the literature and
is referred to as unknowns [3]. The authors categorized
unknowns as follows: known unknowns and unknown un-
knowns. The key criterion differentiating these categories
is the conﬁdence of the predictions made by the trained
models. The unknown unknowns correspond to data points
that the model’s predictions are wrong with high conﬁ-
dence, e.g. high softmax score, whereas the known un-
knowns represent mispredicted data points with low conﬁ-
dence. Known unknowns have better chance to be detected
as the classiﬁer’s conﬁdence is low, whereas unknown un-
knowns are much difﬁcult to detect as the classiﬁer gener-
ates high conﬁdence score.

In this study, the data bias we consider has a similar ﬂa-
vor to the unknown unknowns in [3]. However, unlike the
unknown unknowns in [3], the bias does not represent data
points themselves. Instead, bias represents some attributes

9012

of data points, such as color, race, or gender.

Figure 1 conceptually shows how biased data can affect
an algorithm. The horizontal axis represents shape space
of the digits, while the vertical axis represents color space,
which is biased information for digit categorization.
In
practice, shape and color are independent features, so a data
point can appear anywhere in Figure 1. However, let us as-
sume that only the data points with high saturation are pro-
vided during training, but the points with low saturation are
present in the test scenario (yet are not accessible during
the training). If a machine learns to categorize the digits,
each solid line is a proper choice for the decision bound-
ary. Every decision boundary categorizes the training data
perfectly, but it performs poorly on the points with low sat-
uration. Without additional information, learning of the de-
cision boundary is an ill-posed problem, multiple decision
boundaries can be determined that perfectly categorize the
training data. Moreover, it is likely that a machine would
utilize the color feature because it is a simple feature to ex-
tract.

To ﬁt the decision boundary to the optimal classiﬁer in
Figure 1, we require simple prior information: Do not learn
from color distribution. To this end, we propose a novel
regularization loss, based on mutual information, to train
deep neural networks, which prevents learning of a given
bias. In other words, we regulate a network to minimize
the mutual information shared between the extracted fea-
ture and the bias we want to unlearn. Hereafter, the bias
that we intend to unlearn is referred to the target bias. For
example, the target bias is the color in Figure 1. Prior to the
unlearning of target bias, we assume that the existence of
data bias is known and that the relevant meta-data, such as
statistics or additional labels corresponding to the semantics
of the biases are accessible. Then, the problem can be for-
mulated in terms of an adversarial problem. In this scenario,
one network has been trained to predict the target bias. The
other network has been trained to predict the label, which
is the main objective of the network, while minimizing the
mutual information between the embedded feature and the
target bias. Through this adversarial training process, the
network can learn how to predict labels independent of the
target bias.

Our main contributions can be summarized as follows:
Firstly, we propose a novel regularization term, based on
mutual information, to unlearn target bias from the given
data. Secondly, we experimentally show that the proposed
regularization term minimizes the detrimental effects of
bias in the data. By removing information relating to the
target bias from feature embedding, the network was able
to learn more informative features for classiﬁcation.
In
all experiments, networks trained with the proposed regu-
larization loss showed performance improvements. More-
over, they achieved the best performance in the most experi-

ments. Lastly, we propose bias planting protocols for public
datasets. To evaluate bias removal problem, we intention-
ally planted bias to training set while maintaining test set
unbiased.

2. Related Works

The existence of unknown unknowns was experimen-
tally demonstrated by Attenberg et al. in [3]. The au-
thors separated the decisions rendered by predictive models
into four conceptual categories: known knowns, known un-
knowns, unknown knowns, and unknown unknowns. Sub-
sequently, the authors developed and participated in a “beat
the machine challenge”, which challenged the participants
to manually ﬁnd the unknown unknowns to fool the ma-
chine.

Several approaches for identifying unknown unknowns
have been also proposed [13, 4]. Lakkaraju et al. [13]
proposed an automatic algorithm using the explore-exploit
strategy. Bansal and Weld proposed a coverage-based utility
model that evaluates the coverage of discovered unknown
unknowns [4]. These approaches rely on an oracle for a sub-
set of test queries. Rather than relying on an oracle, Alvi et
al. [1] proposed joint learning and unlearning method to re-
move bias from neural network embedding. To unlearn the
bias, the authors applied confusion loss, which is computed
by calculating the cross-entropy between classiﬁer output
and a uniform distribution. Similar approaches, making net-
works to be confused, have been applied on various appli-
cations [8, 2, 6].

As mentioned by Alvi et al. in the paper [1], the unsu-
pervised domain adaptation (UDA) problem is closely re-
lated to the biased data problem. The UDA problem in-
volves generalizing the network embedding over different
domains [8, 23, 21]. The main difference between our prob-
lem and the UDA problem is that our problem does not as-
sume the access to the target images and instead, we are
aware of the description of the target bias.

Embracing the UDA problem, disentangling feature rep-
resentation has been widely researched in the literature. The
application of disentangled features has been explored in
detail [24, 17]. Using generative adversarial network [9],
more methods to learn disentangled representation [5, 16,
22] have been proposed. In particular, Chen et al. proposed
the InfoGAN [5] method, which learns and preserves se-
mantic context without supervision.

These studies highlighted the importance of feature dis-
entanglement, which is the ﬁrst step in understanding the
information contained within the feature. Inspired by var-
ious applications, we have attempted to remove certain in-
formation from the feature. In contrast to the InfoGan [5],
we minimize the mutual information in order not to learn.
However, removal of information is an antithetical concept
to learning and is also referred to as unlearning. Although

9013

x

r
e
v
e
r
s
e
 
g
r
a
d
i
e
n
t

f

C
l
a
s
s
i
f
i
c
a
t
i
o
n
 
L
a
b
e
l

g

K
n
o
w
n
 
B
i
a
s

h

Figure 2. Overall architecture of deep neural network. The net-
work g ◦ f is implemented with ResNet-18 [10] for real images
and plain network with four convolution layers for MNIST im-
ages.

the concept itself is the complete opposite of learning, it can
help learning algorithms. Herein, we describe an algorithm
for removing target information and present experimental
results and analysis to support the proposed algorithm.

3. Problem Statement

In this section, we formulate a novel regularization loss,
which minimizes the undesirable effects of biased data, and
describe the training procedure. The notations should be
deﬁned prior to introduction of the formulation. Unless
speciﬁcally mentioned, all notation refers to the following
terms hereafter. Assume we have an image x ∈ X and
corresponding label yx ∈ Y. We deﬁne a set of bias, B,
which contains every possible target bias that X can pos-
sess. In Figure 1, B is a set of possible colors, while Y rep-
resents a set of digit classes. We also deﬁne a latent function
b : X → B, where b(x) denotes the target bias of x. We de-
ﬁne random variables X and Y that have the value of x and
yx respectively.

The input image x is fed into the feature extraction net-
work f : X → RK , where K is the dimension of the
feature embedded by f . Subsequently, the extracted fea-
ture, f (x), is fed forward through both the label predic-
tion network g : RK → Y, and bias prediction network
h : RK → B. The parameters of each network are deﬁned
as θf , θg, and θh with the subscripts indicating their spe-
ciﬁc network. Figure 2 describes the overall architecture of
the neural networks. However, we do not explicitly desig-
nate a detailed architecture, since our regularization loss is
applicable to arbitrary network architectures.

3.1. Formulation

The objective of our work is to train a network that per-
forms robustly with unbiased data during test time, even
though the network is trained with biased data. The data

bias has following characteristic:

I(b(X train); Y ) ≫ I(b(X test); Y ) ≈ 0,

(1)

where X train and X test denote the random variable sam-
pled during the training and test procedure, respectively, and
I(·; ·) denotes the mutual information. Biased training data
results in the biased networks, so that the network relies
heavily on the bias of the data:

I(b(X); g(f (X))) ≫ 0.

(2)

To this end, we add the mutual information to the objective
function for training networks. We minimize the mutual
information over f (X), instead of g(f (X)). It is adequate
because the label prediction network, g, takes f (X) as its
input. From a standpoint of g, the training data is not biased
if the network f extracts no information of the target bias.
In other words, extracted feature f (x) should contain no
information of the target bias, b(x). Therefore, the training
procedure is to optimize the following problem:

min
θf ,θg

E˜x∼PX (·)[Lc(y˜x, g(f (˜x)))] + λI(b(X); f (X)), (3)

where Lc(·, ·) represents the cross-entropy loss, and λ is a
hyper-parameter to balance the terms.

The mutual information in Eq. (3) can be equivalently

expressed as follows:

I(b(X); f (X)) = H(b(X)) − H(b(X)|f (X)),

(4)

where H(·) and H(·|·) denote the marginal and conditional
entropy, respectively. Since the marginal entropy of bias is
constant that does not depend on θf and θg, H(b(X)) can be
omitted from the optimization problem, and we try to mini-
mize the negative entropy, −H(b(X)|f (X)). Eq. (4) is dif-
ﬁcult to directly minimize as it requires the posterior distri-
bution, P (b(X)|f (X)). Since it is not tractable in practice,
minimizing the Eq. (4) is reformulated using an auxiliary
distribution, Q, with an additional equality constraint:

E˜x∼PX (·)[E˜b∼Q(·|f (˜x))[log Q(˜b|f (˜x))]]

min
θf

s.t. Q(b(X)|f (X)) = P (b(X)|f (X)).

(5)

The beneﬁt of using the distribution Q is that we can di-
rectly calculate the objective function. Therefore, we can
train the feature extraction network, f , under the equality
constraint.

3.2. Training Procedure

As the equality constraint in Eq. (5) is difﬁcult to meet
(especially in the beginning of the training process), we
modify the equality constraint into minimizing KL diver-
gence between P and Q, so that Q gets closer to P as learn-
ing progresses. We relax the Eq. (5), so that the auxiliary

9014

distribution, Q, could be used to approximate the posterior
distribution. The relaxed regularization loss, LM I , is as fol-
lows:

LM I = E˜x∼PX (·)[E˜b∼Q(·|f (˜x))[log Q(˜b|f (˜x))]]

+ µDKL(P (b(X)|f (X))||Q(b(X)|f (X))),

(6)

where DKL denotes the KL-divergence and µ is hyper-
parameter which balances the two terms. Similar to the
method proposed by Chen et al. [5], we parametrize the
auxiliary distribution, Q, as the bias prediction network, h.
Note that we will train network h, so that the KL-divergence
is minimized. Provided that the distribution Q implemented
by network h converges to P (b(X)|f (X)), we only need to
train network f so that the ﬁrst term in Eq. (6) is minimized.
Although the posterior distribution, P (b(X)|f (X)), is
not tractable, the bias prediction network, h, is expected to
be trained to stochastically approximate P (b(X)|f (X)), if
we train the network with b(X) as the label with SGD opti-
mizer. Therefore, we relax the KL-divergence of Eq. (6)
with expectation of the cross-entropy loss between b(X)
and h(f (X)), and we train network h so that bias prediction
loss, LB, is minimized.

LB(θf , θh) = E˜x∼PX (·)[Lc(b(˜x), h(f (˜x)))].

(7)

Although training network h alone to minimize Eq. (7) is
enough to make Q closer to P , it will be additionally ben-
eﬁcial to train f to maximize Eq. (7) in an adversarial way,
i.e. to let the networks f and h play the minimax game. The
intuition is that the feature extracted by network f is mak-
ing the bias prediction difﬁcult. As f is trained to minimize
the ﬁrst term in Eq. (6), we can reformulate Eq. (6) using
LB instead of KL-divergence as follows:

min
θf

max

θh

E˜x∼PX (·)[E˜b∼Q(·|f (˜x))[log Q(˜b|f (˜x))]]

(8)

− µLB(θf , θh)

We train h to correctly predict the bias, b(X), from its
feature embedding, f (X). We train f to minimize the nega-
tive conditional entropy. The network h is ﬁxed while min-
imizing the negative conditional entropy. The network f
is also trained to maximize the cross-entropy to restrain h
from predicting b(X). Together with the primal classiﬁca-
tion problem, the minimax game is formulated as follows:

min
θf ,θg

max

θh

E˜x∼PX (·)[Lc(y˜x, g(f (˜x)))

+ λE˜b∼Q(·|f (˜x))[log Q(˜b|f (˜x))]]
− µLB(˜x, θf , θh)

(9)

In practice, the deep neural networks, f , g and h, are
trained with both adversarial strategy [9, 5] and gradient re-
versal technique [8]. Early in learning, g ◦ f are rapidly

trained to classify the label using the bias information. Then
h learns to predict the bias, and f begins to learn how to ex-
tract feature embedding independent of the bias. At the end
of the training, h regresses to the poor performing network
not because the bias prediction network, h, diverges, but be-
cause f unlearns the bias, so the feature embedding, f (X),
does not have enough information to predict the target bias.

4. Dataset

Most existing benchmarks are designed to evaluate a spe-
ciﬁc problem. The collectors often split the dataset into
train/test sets exquisitely. However, their efforts to main-
tain the train/test split to obtain an identical distribution ob-
scures our experiment. Thus, we intentionally planted bias
to well-balanced public benchmarks to determine whether
our algorithm could unlearn the bias.

4.1. Colored MNIST

We planted a color bias into the MNIST dataset [14].
To synthesize the color bias, we selected ten distinct col-
ors and assigned them to each digit category as their mean
color. Then, for each training image, we randomly sampled
a color from the normal distribution of the corresponding
mean color and provided variance, and colorized the digit.
Since the variance of the normal distribution is a parame-
ter that can be controlled, the amount of the color bias in
the data can be adjusted. For each test image, we randomly
choose a mean color among the ten pre-deﬁned colors and
followed the same colorization protocol as for the training
images. Each sub-datasets are denoted as follows:

• Train-σ2: Train images with colors sampled with σ2

• Test-σ2: Test images with colors sampled with σ2

Since the digits in the test sets are colored with random
mean colors, the Test-σ2 sets are unbiased. We varied σ2
from 0.02 to 0.05 with a 0.005 interval. Smaller values of
σ2 indicate more bias in the set. Thus, Train-0.02 is the
most biased set, whereas Train-0.05 is the least biased.

Figure 3 (a) shows samples from the colored MNIST,
where the images in the training set show that the color and
digit class are highly correlated. The color of the digit con-
tains sufﬁcient information to categorize the digits in the
training set, but it is insufﬁcient for the images in the test
set. Recognizing the color would rather disrupt the digit
categorization. Therefore, the color information must be
removed from the feature embedding.

4.2. Dogs and Cats

We evaluated our algorithm with the dogs and cats
database, developed by kaggle [12]. The original database
is a set of 25K images of dogs and cats for training and
12,500 images for testing. Similar to [13], we manually

9015

Mean

color

Baseline

Ours

Figure 5. Confusion matrices with test images colored by single mean color. Top row denotes the mean colors and their corresponding digit
classes in training data. The confusion matrices of baseline model show the network is biased owing to the biased data. On the contrary,
the networks trained by our algorithm are not biased to the color although they were trained with the same training data with the baseline.

As a result, EB1 and EB2 contain 36,004 and 16,800 facial
images respectively, and the test set contains 13129 images.
Figure 3 (c) shows that both EB1 and EB2 are biased with
respect to the age. Although it is not as clear as the color
bias in Figure 3 (a) and (b), EB1 consists of younger female
and older male celebrities, whereas EB2 consists of younger
male and older female celebrities. When gender is target
bias, B = {male, female}, and when age is target bias, B is
their age.

5. Experiments

5.1. Implementation

In the following experiments, we removed three types
of target bias: color, age, and gender. The age and gen-
der labels were provided in IMDB face dataset, therefore
LB(θf , θh) was optimized with supervision. On the other
hand, the color bias was removed via self-supervision. To
construct color labels, we ﬁrst sub-sampled the images by
factor of 4. In addition, the dynamic range of color, 0-255,
was quantized into eight even levels.

For the network architecture, we used ResNet-18 [10] for
real images and plain network with four convolution layers
for the colored MNIST experiments. The network archi-
tectures correspond to the parametrization of g ◦ f . In the
case we used ResNet-18, g was implemented as two resid-
ual blocks on the top, while f represents the rest. For plain
network for colored MNIST, both g and f consist of two
convolution layers. ResNet-18 was pretrained with Ima-
genet data [20] except for the last fully connected layer. We
implemented h with two convolution layers for color bias
and single fully connected layer for gender and age bias.
Every convolution layer is followed by batch normalization
[11] and ReLU activation layers. All the evaluation results
were averaged to be presented in this paper.

5.2. Results

We compare our training algorithm with other methods
that can be used for this task. The performance of the al-

gorithms mentioned in this section were re-implemented
based on the literature.
Colored MNIST. The amount of bias in the data was con-
trolled by adjusting the value of σ2. A network was trained
for each σ2 value from 0.02 to 0.05 and was evaluated with
the corresponding test set with the same σ2. Since a color
for each image was sampled with a given σ2, smaller σ2
implies severer color bias. Figure 4 shows the evaluation re-
sults of the colored MNIST. The baseline model represents
a network trained without additional regularization and the
baseline performance can roughly be used as an indication
of training data bias. The algorithm denoted as “Blind-
Eye” represents a network trained with confusion loss [1]
instead of our regularization. The other algorithm, denoted
as “Gray”, represents a network trained with grayscale im-
ages and it was also tested with grayscale images. For the
given color biased data, we converted the color digits into
grayscale. Conversion into grayscale is a trivial approach
that can be used to mitigate the color bias. We presume that
the conversion into grayscale does not reduce the informa-
tion signiﬁcantly since the MNIST dataset was originally
provided in grayscale.

The results of our proposed algorithm outperformed the
BlindEye [1] and baseline model with all values of σ2. No-
tably, we achieved similar performance as the model trained
and tested with grayscale images. Since we converted im-
ages in both training and test time, the network is much less
biased. In most experiments, our model performed slightly
better than the gray algorithm, suggesting that our regula-
tion algorithm can effectively remove the target bias and
encourage a network to extract more informative features.

To analyze the effect of the bias and proposed algorithm,
we re-colored the test images. We sampled with the same
protocol, but with ﬁxed mean color, which was assigned to
one of the ten digit classes of the biased training data. Fig-
ure 5 shows the confusion matrices drawn by the baseline
and our models with the re-colored test images. The digits
illustrated in the top row denotes the mean colors and their
corresponding digit class in training set. For example, the

9017

ﬁrst digit, red zero, signiﬁes the confusion matrices below
are drawn by test images colored reddish regardless of their
true label. It also stands for a fact that every digit of cate-
gory zero in training data is colored reddish.

In Figure 5, the matrices of the baseline show vertical
patterns, some of which are shared, such as digits 1 and 3.
The mean color for class 1 is teal; in RGB space it is (0,
128, 128). The mean color for class 3 is similar to that of
class 1. In RGB space, it is (0, 149, 182) and is called bondi
blue. This indicates that the baseline network is biased to
the color of digit. As observed from the ﬁgure, the confu-
sion matrices drawn by our algorithm (bottom row) show
that the color bias was removed.
Dogs and Cats. Table 1 presents the evaluation results,
where the baseline networks perform admirably, consider-
ing the complexity of the task due to the pretrained param-
eters. As mentioned in [18], neural networks prefer to cat-
egorize images based on shape rather than color. This en-
courages the baseline network to learn shapes, but the eval-
uation results presented in Table 1 imply that the networks
remain biased without regularization.

Similar to the experiment on the colored MNIST, sim-
plest approach for reducing the color bias is to convert the
images into grayscale. Unlike the MNIST dataset, conver-
sion would remove a signiﬁcant amount of information. Al-
though the networks for grayscale images performed better
than the baseline, Table 1 shows that the networks remain
biased to color. This is likely because of the criterion that
was used to implant the color bias. Since the original dataset
is categorized into bright and dark, the converted images
contain a bias in terms of brightness.

We used gradient reversal layer (GRL) [8] and adversar-
ial training strategy [5, 9] as components of our optimiza-
tion process. To analyze the effect of each component, we
ablated the GRL from our algorithm. We also trained net-
works with both confusion loss [1] and GRL, since they
can be used in conjunction with each other. Although the
GRL was originally proposed to solve unsupervised domain
adaptation problem [8], Table 1 shows that it is beneﬁcial
for bias removal. Together with either confusion loss or our
regularization, we obtained the performance improvements.
Furthermore, GRL alone notably improved the performance
suggesting that GRL itself is able to remove bias.

Figure 6 shows the qualitative effect of our proposed reg-
ularization. The prediction results of the baseline networks
do not change signiﬁcantly regardless of whether the query
image is cat or dog if the colors are similar. If a network is
trained with TB1, the network predicts a dark image to be
a cat and a bright image to be a dog. If another network is
trained with TB2, the network predicts a bright image to be
a cat and a dark image to be a dog. This implies that the
baseline networks are biased to color. On the other hand,
networks trained with our proposed algorithm successfully

Method

Baseline
Gray†
BlindEye [1]
GRL [8]
BlindEye+GRL
Ours-adv
Ours

Trained on TB1
Test

TB2

Trained on TB2
Test

TB1

.7498
.8366
.8525
.8356
.8937
.8853
.9029

.9254
.9483
.9517
.9462
.9582
.9594
.9638

.6645
.7192
.7812
.7813
.8610
.8630
.8726

.8524
.8687
.9038
.9012
.9291
.9298
.9376

Table 1. The evaluation results on dogs and cats dataset. All net-
works were evaluated with test set. Moreover, the networks trained
with TB1 were additionally evaluated with TB2, and vice versa. †
denotes that the network was tested with images converted into
grayscale. The Ours-adv denotes a model trained with Eq. (9)
without using gradient reversal layer. The best performing result
on each column is denoted as boldface and the second best result
is underlined.

Method

Trained on EB1
Test

EB2

Trained on EB2
Test

EB1

Learn Gender, Unlearn Age

Baseline
BlindEye [1]
Ours

.5986
.6374
.6800

.8442
.8556
.8666

.5784
.5733
.6418

.6975
.6990
.7450

Learn Age, Unlearn Gender

Baseline
BlindEye [1]
Ours

.5430
.6680
.6527

.7717
.7513
.7743

.4891
.6416
.6218

.6197
.6240
.6304

Table 2. Evaluation results on IMDB face dataset. All networks
were evaluated with test set and the other training set. The best
performing result on each column is denoted as boldface.

classiﬁed the query images independent of their colors. In
particular, Figure 6 (c) and (f) were identically predicted by
the baseline networks depending on their color. After re-
moving the color information from the feature embedding,
the images were correctly categorized according to their ap-
pearance.
IMDB face. For the IMDB face dataset, we conducted two
experiments; one to train the networks to classify age inde-
pendent of gender, and one to train the networks to classify
gender independent of age. Table 2 shows the evaluation
results from both experiments. The networks were trained
with either EB1 or EB2 and since they are extremely biased,
the baseline networks are also biased. By removing the tar-
get bias information from the feature embedding, overall
performances are improved. On the other hand, considering
that gender classiﬁcation is a two class problem, where ran-
dom guessing achieves 50% accuracy, the networks perform
poorly on gender classiﬁcation. Although Table 2 shows
that the performance improves after removing the target
bias from the feature embedding, the performance improve-
ment achieved using our algorithm is marginal compared to

9018

Trained with

Oracle
Baseline

Ours

Baseline

Ours

0.28

0.27

0.98

0.22

g
o
D

TB1

TB1

TB2

TB2

0.72

0.96

0.73

0.78

C
a
t

g
o
D

0.25

0.99

0.24

0.99

0.99

0.75

0.76

C
a
t

0.99

0.38

g
o
D

0.37

0.99

0.62

0.99

0.62

C
a
t

g
o
D

0.96

0.53

0.99

0.60

0.99

0.47

C
a
t

0.40

g
o
D

0.99

0.99

0.61

0.15

0.69

0.99

0.99

0.62

0.61

C
a
t

g
o
D

0.39

0.85

0.31

C
a
t

0.38

0.99

0.39

(a)

(b)

(c)

(d)

(e)

(f)

Figure 6. Qualitative results of dogs and cats dataset. The oracle model was trained with not only both TB1 and TB2, but also with images
we categorized as other color. For test images, prediction results of the oracle model were considered as their true labels. The stacked bar
charts below the ﬁgures visualize prediction results by each model. The baseline models tend to predict depending on the color, whereas
our model ignores the color information.

Trained with

True Label

Baseline

Ours

Baseline

Ours

EB1

EB1

EB2

EB2

e
l
a

M

1.00

0.96

0.98

0.94

0.96

F
e
m
a
l
e

e
l
a

M

1.00

0.47

0.92

0.99

0.95

0.53

F
e
m
a
l
e

e
l
a

M

1.00

0.99

0.96

0.61

F
e
m
a
l
e

e
l
a

M

0.91

0.99

0.39

1.00

0.99

0.99

0.97

0.67

0.49

e
l
a

M

F
e
m
a
l
e

1.00

0.33

0.51

F
e
m
a
l
e0.91

0.99

0.98

e
l
a

M

1.00

0.97

0.99

0.99

F
e
m
a
l
e

(a)

(b)

(c)

(d)

(e)

(f)

Figure 7. Qualitative results of gender classiﬁcation with IMDB face dataset. As in the Figure 6, the stacked bar charts represent the
prediction results. They show that the baseline models are biased to the age. On the other hand, the networks trained with proposed
algorithm predict the gender independent of their age.

previous experiments with other datasets. We presume that
this is because of the correlation between age and gender. In
the case of color bias, the bias itself is completely indepen-
dent of the categories. In other words, an effort to unlearn
the bias is purely beneﬁcial for digit categorization. Thus,
removing color bias from feature embedding improved the
performance signiﬁcantly because the network is able to fo-
cus on learning shape features. Unlike the color bias, age
and gender are not completely independent features. There-
fore, removing bias information from feature embedding
would not be completely beneﬁcial. This suggests that a
deep understanding of the speciﬁc data bias must precede
the removal of bias.

Figure 7 shows the qualitative effect of regularization on
the gender classiﬁcation task. Young, mid-age, and old in-
dividuals with both male and female are presented. Similar
to Figure 6, it implies that the baseline networks are biased
toward age. The baseline network trained with EB1 pre-
dicted both young male and young female images (Figure 7
(a) and (d)) as female with high conﬁdence. Meanwhile, the
network trained with EB2 predicted the same images as the
exact opposite gender with high conﬁdence. Upon removal
of age bias, the networks were trained to correctly predict
the gender.

6. Conclusion

In this paper, we propose a novel regularization term to
train deep neural networks when using biased data. The
core idea of using mutual information is inspired by Info-
Gan [5]. In constrast to the inspiring approach, we rather
minimize the mutual information in order not to learn. By
letting networks play minimax game, networks learn to cat-
egorize, while unlearning the bias. The experimental re-
sults showed that the networks trained with the proposed
regularization can extract bias-independent feature embed-
ding, achieving the best performance in the most of the ex-
periments. Furthermore, our model performed better than
“Gray” model which was trained with almost unbiased data,
indicating the feature embedding becomes even more infor-
mative. To conclude, we have demonstrated in this paper
that the proposed regularization improves the performance
of neural networks trained with biased data. We expect this
study to expand the usage of various data and to contribute
to the ﬁeld of feature disentanglement.

Acknowledgement

This research was supported by Samsung Research.

9019

learning. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 2

[17] Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris N. Metaxas,
and Manmohan Chandraker. Reconstruction for feature dis-
entanglement in pose-invariant face recognition. CoRR,
abs/1702.03041, 2017. 2

[18] Samuel Ritter, David GT Barrett, Adam Santoro, and Matt M
Botvinick. Cognitive psychology for deep neural networks:
A shape bias case study. arXiv preprint arXiv:1706.08606,
2017. 7

[19] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep
expectation of apparent age from a single image. In IEEE In-
ternational Conference on Computer Vision Workshops (IC-
CVW), December 2015. 5

[20] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge.
International Journal of Computer Vision (IJCV),
115(3):211–252, 2015. 6

[21] Ozan Sener, Hyun Oh Song, Ashutosh Saxena, and Silvio
Savarese. Learning transferrable representations for unsu-
pervised domain adaptation.
In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances
in Neural Information Processing Systems 29, pages 2110–
2118. Curran Associates, Inc., 2016. 2

[22] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre-
sentation learning gan for pose-invariant face recognition. In
In Proceeding of IEEE Computer Vision and Pattern Recog-
nition, Honolulu, HI, July 2017. 2

[23] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko.
Simultaneous deep transfer across domains and tasks. CoRR,
abs/1510.02192, 2015. 2

[24] Junho Yim, Heechul Jung, ByungIn Yoo, Changkyu Choi,
Dusik Park, and Junmo Kim. Rotating your face using multi-
task deep neural network. In 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 676–
684, June 2015. 2

References

[1] Mohsan Alvi, Andrew Zisserman, and Christoffer Nellaker.
Turning a blind eye: Explicit removal of biases and varia-
tion from deep neural network embeddings. arXiv preprint
arXiv:1809.02169, 2018. 2, 5, 6, 7

[2] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor
Darrell, and Anna Rohrbach. Women also snowboard: Over-
coming bias in captioning models. In The European Confer-
ence on Computer Vision (ECCV), September 2018. 2

[3] Joshua Attenberg, Panos Ipeirotis, and Foster Provost. Beat
the machine: Challenging humans to ﬁnd a predictive
model’s unknown unknowns. Journal of Data and Informa-
tion Quality (JDIQ), 6(1):1, 2015. 1, 2

[4] Gagan Bansal and Daniel S Weld. A coverage-based utility
model for identifying unknown unknowns. In Proc. of AAAI,
2018. 2

[5] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Infogan: Interpretable rep-
Sutskever, and Pieter Abbeel.
resentation learning by information maximizing generative
adversarial nets. CoRR, abs/1606.03657, 2016. 2, 4, 7, 8

[6] Abhimanyu Dubey, Otkrist Gupta, Pei Guo, Ramesh Raskar,
Ryan Farrell, and Nikhil Naik. Pairwise confusion for ﬁne-
grained visual classiﬁcation.
In The European Conference
on Computer Vision (ECCV), September 2018. 2

[7] E. Eidinger, R. Enbar, and T. Hassner. Age and gender es-
timation of unﬁltered faces. IEEE Transactions on Informa-
tion Forensics and Security, 9(12):2170–2179, Dec 2014. 5

[8] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. The Journal of Machine Learning
Research, 17(1):2096–2030, 2016. 2, 4, 7

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014. 2, 4, 7

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. CoRR,
abs/1512.03385, 2015. 3, 5, 6

[11] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. CoRR, abs/1502.03167, 2015. 6

[12] Kaggle. Dogs vs. cats, 2013. 4, 5

[13] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric
Horvitz. Discovering blind spots of predictive models: Rep-
resentations and policies for guided exploration. CoRR,
abs/1610.09064, 2016. 2, 4

[14] Yann LeCun and Corinna Cortes. MNIST handwritten digit

database. 2010. 4, 5

[15] G. Levi and T. Hassncer. Age and gender classiﬁcation
using convolutional neural networks.
In 2015 IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops (CVPRW), pages 34–42, June 2015. 5

[16] Yang Liu, Zhaowen Wang, Hailin Jin, and Ian Was-
sell. Multi-task adversarial network for disentangled feature

9020

