Efﬁcient Multi-Domain Learning by Covariance Normalization

Yunsheng Li
Nuno Vasconcelos
University of California San Diego

La Jolla, CA 92093

yul554@ucsd.edu, nvasconcelos@ucsd.edu

Abstract

The problem of multi-domain learning of deep networks
is considered. An adaptive layer is induced per target
domain and a novel procedure, denoted covariance nor-
malization (CovNorm), proposed to reduce its parameters.
CovNorm is a data driven method of fairly simple im-
plementation, requiring two principal component analyzes
(PCA) and ﬁne-tuning of a mini-adaptation layer. Neverthe-
less, it is shown, both theoretically and experimentally, to
have several advantages over previous approaches, such as
batch normalization or geometric matrix approximations.
Furthermore, CovNorm can be deployed both when target
datasets are available sequentially or simultaneously. Ex-
periments show that, in both cases, it has performance com-
parable to a fully ﬁne-tuned network, using as few as 0.13%
of the corresponding parameters per target domain.

1. Introduction

Convolutional nerual networks (CNNs) have enabled
transformational advances in classiﬁcation, object detection
and segmentation, among other tasks. However they have
non-trivial complexity. State of the art models contain mil-
lions of parameters and require implementation in expen-
sive GPUs. This creates problems for applications with
computational constraints, such as mobile devices or con-
sumer electronics. Figure 1 illustrates the problem in the
context of a smart home equipped with an ecology of de-
vices such as a camera that monitors package delivery and
theft, a fridge that keeps track of its content, a treadmill that
adjusts ﬁtness routines to the facial expression of the user,
or a baby monitor that keeps track of the state of a baby.
As devices are added to the ecology, the GPU server in the
house must switch between a larger number of classiﬁca-
tion, detection, and segmentation tasks. Similar problems
will be faced by mobile devices, robots, smart cars, etc.

Under the current deep learning paradigm,

this task
switching is difﬁcult to perform. The predominant strategy
is to use a different CNN to solve each task. Since only a

Figure 1: Multi-domain learning addresses the efﬁcient solution of sev-
eral tasks, deﬁned on different domains. Each task is solved by a different
network but all networks share a set of ﬁxed layers F, which contain the
majority of network parameters. These are complemented by small task-
speciﬁc adaptation layers A.

few models can be cached in the GPU, and moving mod-
els in and out of cache adds too much overhead to enable
real-time task switching, there is a need for very efﬁcient
parameter sharing across tasks. The individual networks
should share most of their parameters, which would always
reside on the GPU. A remaining small number of task spe-
ciﬁc parameters would be switched per task. This problem
is known as multi-domain learning (MDL) and has been
addressed with the architecture of Figure 1 [34, 38]. This
consists of set of ﬁxed layers (denoted as ’F’) shared by all
tasks and a set of task speciﬁc adaptation layers (denoted

5424

FAFAFASFAFAFASFAFAFASFAFAFASSmartCameraSmartFridgeSmartTreadmillBabyMonitorFsharedfixedweightsAtaskspecificlearnedweightsSsoftmaxlayermalization [2]. It is also quite simple, requiring two PCAs
and the ﬁnetuning of a very small mini-adaptation layer per
A layer and task. Experimental results show that it can out-
perform full network ﬁne-tuning while reducing A layers
to as little as 0.53% of the total parameters. When all tasks
can be learned together, A layers can be further reduced
to 0.51% of the full model size. This is achieved by com-
bining the individual PCAs into a global PCA model, of
parameters shared by all tasks, and only ﬁne-tunning mini-
adaptation layers in a task speciﬁc manner.

2. Related work

MDL is a transfer learning problem, namely the transfer
of a model trained on a source learning problem to an ecol-
ogy of target problems. This makes it related to different
types of transfer learning problems, which differ mostly in
terms of input, or domain, and range space, or task.

Task transfer: Task transfer addresses the use of a
model trained on a source task to the solution of a target
task. The two tasks can be deﬁned on the same or different
domains. Task transfer is prevalent in deep learning, where
a CNN pre-trained on a large source dataset, such as Ima-
geNet, is usually ﬁne-tunned [21] to a target task. While
extremely effective and popular, full network ﬁne-tunning
changes most network parameters, frequently all. MDL ad-
dresses this problem by considering multiple target tasks
and extensive parameter sharing between them.

Domain Adaptation: In domain adaptation, the source
and target tasks are the same, and a model trained on a
source domain is transfered to a target domain. Domain
adaptation can be supervised, in which case labeled data is
available for the target domain, or unsupervised, where it
is not. Various strategies have been used to address these
problems. Some methods seek the network parameters that
minimize some function of the distance between feature dis-
tributions in the two domains [24, 4, 43]. Others introduce
an adversarial loss that maximizes the confusion between
the two domains [8, 45]. A few methods have also proposed
to do the transfer at the image level, e.g. using GANs [11] to
map source images into (labeled) target images, then used to
learn a target classiﬁer [3, 41, 14]. All these methods exploit
the commonality of source and domain tasks to align source
and target domains. This is unlike MDL, where source and
target tasks are different. Nevertheless, some mechanisms
proposed for domain adaptation can be used for MDL. For
example, [5, 28] use a batch normalization layer to match
the statistics of source and target data, in terms of means
and standard deviation. This is similar to an early proposal
for MDL [2]. We show that these mechanisms underper-
form covariance normalization.

Multitask learning: Multi-task learning [6, 49] ad-
dresses the solution of multiple tasks by the same model. It
assumes that all tasks have the same visual domain. Popular

5425

Figure 2: Covariance normalization. Each adaptation layer A is approx-
imated by three transformations: ˜Wx, which implements a projection
onto the PCA space of the input x (principal component matrix Px and
eigenvalue matrix Ex), ˜Wy, which reconstructs the PCA space of the
output y (matrices Py and Ey), and a mini-adaptation layer Mxy.

as ’A’) ﬁne-tunned to each task. If the A layers are much
smaller than the F layers, many models can be cached si-
multaneously. Ideally, the F layers should be pre-trained,
e.g. on ImageNet, and used by all tasks without additional
training, enabling the use of special purpose chips to im-
plement the majority of the computations. While A layers
would still require a processing unit, the small amount of
computation could enable the use of a CPU, making it cost-
effective to implement each network on the device itself.

In summary, MDL aims to maximize the performance
of the network ecology while minimizing the ratio of task
speciﬁc (A) to total parameters (both types F and A) per
network. [34, 38] have shown that the architecture of Fig-
ure 1 can match the performance of fully ﬁne-tuning each
network in the ecology, even when A layers contain as few
as 10% of the total parameters. In this work, we show that
A layers can be substantially further shrunk, using a data-
driven low-rank approximation. As illustrated in Figure 2,
this is based on transformations that match the 2nd-order
statistics of the A layer inputs and outputs. Given prin-
cipal component analyses (PCAs) of both input and out-
put, the layer is approximated by a recoloring transfor-
mation: a projection into input PCA space, followed by
a reconstruction into the output PCA space. By control-
ling the intermediate PCA dimensions, the method enables
low-dimensional approximations of different input and out-
put dimensions. To correct the mismatch (between PCA
components) of two PCAs learned independently, a small
mini-adaptation layer is introduced between the two PCA
matrices, and ﬁne-tunned on the target target.

Since the overall transformation generalizes batch nor-
malization, the method is denoted covariance normaliza-
tion (CovNorm). CovNorm is shown to outperform, with
both theoretical and experimental arguments, purely geo-
metric methods for matrix approximation, such as the sin-
gular value decomposition (SVD) [35], ﬁne-tuning of the
original A layers [34, 38], or adaptation based on batch nor-

𝜇𝑥𝜇𝑦𝑷𝑥,𝑬𝑥𝑷𝑦,𝑬𝑦෪𝑾𝑥෩𝑪𝑦-𝜇𝑥+𝜇𝑦𝑴𝑥𝑦𝑨𝒙𝒚𝒙𝒚𝑭𝟏

𝝓(. )

a)

𝑭𝟐

𝑭′𝟏
𝐹′&

𝝓(. )

b)

𝑭′𝟐

𝑭𝟏

𝑨

𝝓(. )

𝑭𝟐

c)

Figure 3: a) original network, b) after ﬁne-tuning, and c) with adaptation layer A. In all cases, Wi is a weight layer and φ(.) a non-linearity.

examples include classiﬁcation and bounding box regres-
sion in object detection [9, 37], joint estimation of surface
normals and depth [7] or segmentation [29], joint represen-
tation in terms of attributes and facial landmarks [50, 33],
among others. Multitask learning is sometimes also used to
solve auxiliary tasks that strengthen performance of a task
of interest, e.g. by accounting for context [10], or represent-
ing objects in terms of classes and attributes [15, 29, 30, 25].
Recently, there have been attempts to learn models that
solve many problems jointly [18, 19, 48].

Most multitask learning approaches emphasize the learn-
ing of the interrelationships between tasks. This is fre-
quently accomplished by using a single network, combin-
ing domain agnostic lower-level network layers with task
speciﬁc network heads and loss functions [50, 7, 10, 15, 37,
19], or some more sophisticated forms of network branch-
ing [25]. The branching architecture is incompatible with
MDL, where each task has its own input, different from
those of all other tasks. Even when multi-task learning
is addressed with multiple tower networks, the emphasis
tends to be on inter-tower connections, e.g. through cross-
stitching [29, 17]. In MDL, such connections are not fea-
sible, because different networks can join the ecology of
Figure 1 asynchronously, as devices are turned on and off.

Lifelong learning: Lifelong learning aims to learn mul-
tiple tasks sequentially with a shared model. This can be
done by adapting the parameters of a network or adapting
the network architecture. Since training data is discarded
upon its use, constraints are needed to force the model to
remember what was previously learned. Methods that only
change parameters either use the model output on previous
tasks [23], previous parameters values [22], or previous net-
work activations [44] to regularize the learning of the target
task. They are very effective at parameter sharing, since
a single model solves all tasks. However, this model is
not optimal for any speciﬁc task, and can perform poorly
on all tasks, depending on the mismatch between source
and target domains [36]. We show that they can signiﬁ-
cantly underperform MDL with CovNorm. Methods that
adapt the network architecture usually add a tower per new
task [40, 1]. These methods have much larger complex-
ity than MDL, since several towers can be needed to solve
a single task [40], and there is no sharing of ﬁxed layers
across tasks.

Multi-domain learning: This work builds on previous
attempts at MDL, which have investigated different archi-
tectures for the adaptation layers of Figure 1. [2] used a BN
layer [16] of parameters tunned per task. While perform-
ing well on simple datasets, this does not have enough de-

grees of freedom to support transfer of large CNNs across
different domains. More powerful architectures were pro-
posed by [38], who used a 1 × 1 convolutional layer and
[34], who proposed a ResNet-style residual layer, known as
a residual adaptation (RA) module. These methods were
shown to perform surprisingly well in terms of recogni-
tion accuracy, equaling or surpassing the performance of
full network ﬁne tunning, but can still require a substan-
tial number of adaptation parameters, typically 10% of the
network size.
[35] addressed this problem by combining
adapters of multiple tasks into a large matrix, which is ap-
proximated with an SVD. This is then ﬁne-tuned on each
target dataset. Compressing adaptation layers in this way
was shown to reduce adaptive parameter counts to approxi-
mately half of [34]. However, all tasks have to be optimized
simultaneously. We show that CovNorm enables a further
ten-fold reduction in adaptation layer parameters, without
this limitation, although some additional gains are possible
with joint optimization.

3. MDL by covariance normalization

In this section, we introduce the CovNorm procedure for

MDL with deep networks.

3.1. Multi domain learning

Figure 3 a) motivates the use of A layers in MDL. The
ﬁgure depicts two ﬁxed weight layers, F1 and F2, and a
non-linear layer φ(.) in between. Since the ﬁxed layers are
pre-trained on a source dataset S, typically ImageNet, all
weights are optimized for the source statistics. For standard
losses, such as cross entropy, this is a maximum likelihood
(ML) procedure that matches F1 and F2 to the statistics
of activations x, y and u in S. However, when the CNN
is used on a different target domain, the statistics of these
variables change and F1, F2 are no longer an ML solution.
Hence, the network is sub-optimal and must be ﬁnetunned
on a target dataset T . This is denoted full network ﬁnetun-
ing and converts the network into an ML solution for T ,
with the outcome of Figure 3 b). In the target domain, the
intermediate random variables become x′, y′, and u′ and
the weights are changed accordingly, into F′

1 and F′

2.

While very effective, this procedure has two drawbacks,
which follow from updating all weights. First, it can be
computationally expensive, since modern CNNs have large
weight matrices. Second, because the weights F′
i are not
optimal for S, i.e. the CNN forgets the source task, there is a
need to store and implement two CNNs to solve both tasks.
This is expensive in terms of storage and computation and

5426

increases the complexity of managing the network ecology.
A device that solves both tasks must store two CNNs and
load them in and out of cache when it switches between
the tasks. These problems are addressed by the MDL ar-
chitecture of Figure 1, which is replicated in greater detail
It introduces an adaptation layer A and
on Figure 3 c).
ﬁne-tunes this layer only, leaving F1 and F2 unchanged. In
this case, the statistics of the input are still those of x′, but
the distributions along the network are now those of z′, y′′,
and u′′. Since F1 is ﬁxed, nothing can be done about z′.
However, the ﬁne-tuning of A encourages the statistics of
y′′ to match those of y′, i.e. y′′ = y′ and thus u′′ = u′.
Even if A cannot match statistics exactly, the mismatch is
reduced by repeating the procedure in subsequent layers,
e.g. introducing a second A layer after F2, and optimizing
adaptation matrices as a whole.

Figure 4: Top: covnorm approximates adaptation layer A by a sequence
of whitening ˜Wx, mini-adaptation Mx,y, and coloring ˜Cy operations.
Bottom: after covnorm, the mini adaptation layer can be absorbed into
˜Wx (shown in the ﬁgure) or ˜Cy.
goal is to replace A by a simpler matrix that maps z′ into
y′. For simplicity, we drop the primes and notation of Fig-
ure 3 in what follows, considering the problem of matching
statistics between input x and output y of a matrix A.

3.2. Adaptation layer size

3.3. Geometric approximations

Obviously, MDL has limited interest if A has size simi-
lar to F1. In this case, each domain has as many adaptation
parameters as the original network, all networks have twice
the size, task switching is complex, and training complexity
is equivalent to full ﬁne tunning of the original network. On
the other hand, if A is much smaller than F1, MDL is com-
putationally light and task-switching much more efﬁcient.
In summary, the goal is to introduce an adaptation layer A
as small as possible, but still powerful enough to match the
statistics of y′ and y′′. A simple solution is to make A a
batch normalization layer [16]. This was proposed in [2]
but, as discussed below, is not effective. To overcome this
problem, [38] proposed a linear transformation A and [34]
adopted the residual structure of [13], i.e. an adaptation
layer T = (I + A). To maximize parameter savings, A was
implemented with a 1× 1 convolutional layer in both cases.
This can, however, still require a non-trivial number of
parameters, especially in upper network layers. Let F1 con-
volve a bank of d ﬁlters of size k×k×l with l feature maps.
Then, F1 has size dk2l, y is d dimensional, and A a d × d
matrix. Since in upper network layers k is usually small and
d > l, A can be only marginally smaller than F1. [35] ex-
ploited redundancies across tasks to address this problem,
creating a matrix with the A layer parameters of multiple
tasks and computing a low-rank approximation of this ma-
trix with an SVD. The compression achieved with this ap-
proximation is limited, because the approximation is purely
geometric, not taking into account the statistics of z′ and
y′. In this work, we propose a more efﬁcient solution, mo-
tivated by the interpretation of A as converting the statistics
of z′ into those of y′. It is assumed that the ﬁne-tuning of A
produces an output variable y′′ whose statistics match those
of y′. This could leverage adaptation layers in other layers
of the network, but that is not important for the discussion
that follows. The only assumption is that y′′ = y′. The

One possibility is to use a purely geometric solution [35].
Geometrically, the closest low rank approximation of a ma-
trix A is given by the SVD, A = USVT . More pre-
cisely, the minimum Frobenius norm approximation ˜A =
arg min{B|rank(B)=r} ||A − B||2
F , where r < rank(A), is
˜A = U˜SVT where ˜S contains the r largest singular values
of A. This can be written as ˜A = CW, where C = Up˜S
and W = p˜SVT . If A ∈ Rd×d, these matrices have a
total of 2rd parameters. An even simpler solution is to de-
ﬁne C ∈ Rd×r and W ∈ Rr×d, replace A by their product

in Figure 3 c), and ﬁne-tune the two matrices instead of
A. We denote this as the ﬁne-tunned approximation (FTA).
These approaches are limited by their purely geometric na-
ture. Note that d is determined by the source model (output
dimension of F1) and ﬁxed. On the other hand, the dimen-
sion r should depend on the target dataset T . Intuitively,
if T is much smaller than S, or if the target task is much
simpler, it should be possible to use a smaller r than oth-
erwise. There is also no reason to believe that a single r,
or even a single ratio r/d, is suitable for all network lay-
ers. While r could be found by cross-validation, this be-
comes expensive when there are multiple adaptation layers
throughout the CNN. We next introduce an alternative, data
driven, procedure that bypasses these difﬁculties.

3.4. Covariance matching

Assume that, as illustrated in Figure 2, x and y are Gaus-
sian random variables of means µx, µy and covariances
Σx, Σy, respectively, related by y = Ax. Let the covari-
ances have eigendecomposition

Σx = PxExPT
x

Σy = PyEyPT
y

(1)

where Px, Py contain eigenvectors as columns and Ex, Ey
are diagonal eigenvalue matrices. We refer to the triplet

5427

Px = (Px, Ex, µx) as the PCA of x. Then, it is well known
that the statistics of x and y are related by

µy = Aµx

Σy = AΣxAT

(2)

and, combining (1) and (2), PyEyPT
y = APxExPT
This holds when Py√Ey = APx√Ex or, equivalently,

x AT .

A = PypEypE−1

x PT
x .

= CyWx

(3)

(4)

x PT

where Wx = pE−1
x is the “whitening matrix” of x and
Cy = PypEythe “coloring matrix” of y. It follows that
(2) holds if y = Ax is implemented with a sequence of
two operations. First, x is mapped into a variable w of zero
mean and identity covariance, by deﬁning

w = Wx(x − µx).

Second, w is mapped into y with

y = Cyw + µy.

(5)

(6)

In summary, for Gaussian x, the effect of A is simply the
combination of a whitening of x followed by a colorization
with the statistics of y.

3.5. Covariance normalization

The interpretation of the adaptation layer as a recoloring
operation (whitening + coloring) sheds light on the number
of parameters effectively needed for the adaptation, since
the PCAs Px,Py capture the effective dimensions of x and
y. Let kx (ky) be the number of eigenvalues signiﬁcantly
larger than zero in Ex (Ey). Then, the whitening and color-
ing matrices can be approximated by

˜Wx = q ˜E−1

x ˜PT

x

˜Cy = ˜Pyq ˜Ey

(7)

where ˜Ex ∈ Rkx×kx ( ˜Ey ∈ Rky×ky ) contains the non-zero
eigenvalues of Σx (Σy), and ˜Px ∈ Rd×kx ( ˜Py ∈ Rd×ky )
the corresponding eigenvectors. Hence, A is well approxi-
mated by a pair of matrices ( ˜Wx, ˜Cy) totaling d(kx + ky)
parameters.

On the other hand, the PCAs are only deﬁned up to
a permutation, which assigns an ordering to eigenval-
ues/eigenvectors. When the input and output PCAs are
computed independently, the principal components may not
be aligned. This can be ﬁxed by introducing a permutation
matrix between Cy and Wx in (4). The assumption that all
distributions are Gaussian also only holds approximately in
real networks. To account for all this, we augment the recol-
oring operation with a mini-adaptation layer Mx,y of size
kx × ky. This leads to the covariance normalization (Cov-
Norm) transform

˜y = ˜CyMx,y ˜Wx(x − µx) + µy,

(8)

Algorithm 1: Covariance Normalization
Data: source S and target T
1 Insert an adaptation layer A on a CNN trained on S
and ﬁne-tune A on T .
2 Store the layer input and output PCAs Px, Py,
select the kx, ky non-zero eigenvalues and
corresponding eigenvectors from each PCA, and
compute ˜Cy, ˜Wx with (7).

3 add mini-adaptation layer Mx,y and replace A by

(8). Note that, as usual, the constant
˜CyMx,y ˜Wxµx + µy can be implemented with a
vector of biases.

4 ﬁne-tune Mx,y with ˜Wx and ˜Cy on T and absorb

Mx,y into the larger of ˜Wx and ˜Cy.

where Mx,y is learned by ﬁne-tuning on the target dataset
T . Beyond improving recognition performance, this has
the advantage of further parameters savings. The direct
implementation of (8) increases the parameter count to
d(kx + ky) + kxky. However, after ﬁne-tuning, Mx,y can
be absorbed into one of the two other matrices , as shown in

Figure 4. When kx > ky, Mx,y ˜Wx has dimension ky × d
and replacing the two matrices by their product reduces the
total parameter count to 2dky.
In this case, we say that
Mx,y is absorbed into ˜Wx. Conversely, if kx < ky, Mx,y
can be absorbed into ˜Cy. Hence, the total parameter count
is 2d min(kx, ky). CovNorm is summarized in Algorithm 1.

3.6. The importance of covariance normalization

The beneﬁts of covariance matching can be seen by com-
parison to previously proposed MDL methods. Assume,
ﬁrst, that x and y consist of independent features. In this
case, Px, Py are identity matrices and (5)-(6) reduce to

yi = √ey,i

xi − µx,i
√ex,i

+ µy,i,

(9)

which is the batch normalization equation. Hence, Cov-
Norm is a generalized form of the latter. There are, how-
ever, important differences. First, there is no batch. The
normalizing distribution x is now the distribution of the fea-
ture responses of layer F1 on the target dataset T . Second,
the goal is not to facilitate the learning of F2, but produce
a feature vector y with statistics matched to F2. This turns
out to make a signiﬁcant difference. Since, in regular batch
normalization, F2 is allowed to change, it can absorb any
initial mismatch with the independence assumption. This is
not the case for MDL, where F2 is ﬁxed. Hence, (9) usually
fails, signiﬁcantly underperforming (5)-(6).

Next, consider the geometric solution. Since CovNorm
reduces to the product of two tall matrices, e.g. K =
˜CyMx,y and L = ˜Wx of size d × kx, it should be pos-

sible to replace it with the ﬁne-tuned approximation based
on two matrices of this size. Here, there are two difﬁculties.

5428

First, kx is not known in the absence of the PCA decompo-
sitions. Second, in our experience, even when kx is set to
the value used by PCA, the ﬁne-tuned approximation does
not work. As shown in the experimental section, when the
matrices are initialized with Gaussian weights, performance
can decrease signiﬁcantly. This is an interesting observa-
tion because A is itself initialized with Gaussian weights.
It appears that a good initialization is more critical for the
low-rank matrices.

x

Finally, CovNorm can be compared to the SVD, A =
USVT . From (3), this holds whenever V = Px, S =
pEypE−1
and U = Py. The problem is that the singu-
lar value matrix S conﬂates the variances of the input and
output PCAs. The fact that si = ey,i/ex,i has two impor-
tant consequences. First, it is impossible to recover the di-
mensions kx and ky by inspection of the singular values.
Second, the low-rank criteria of selecting the largest sin-
gular values is not equivalent to CovNorm. For example,
the principal components of x with largest eigenvalues ex,i
have the smallest singular values si. Hence, it is impos-
sible to tell if singular vectors vi of small singular values
are the most important (PCA components of large variance
for x) or the least important (noise). Conversely, the largest
singular values can simply signal the least important input
dimensions. CovNorm eliminates this problem by explicitly
selecting the important input and output dimensions.

3.7. Joint training

[35] considered a variant of MDL where the different
tasks of Figure 1 are all optimized simultaneously. This
is the same as assuming that a joint dataset T = ∪iTi is
available. For CovNorm, the only difference with respect to
the single dataset setting is that the PCAs Px,Py are now
those of the joint data T . These can be derived from the
PCAs Px,i,Py,i of the individual target datasets Ti with

µT =

ΣT = X

1
N X
i
Ni
N

i

Niµi

(PiEiPi

T + µiµi

T )) − µT µT

T (10)

where Ni is the cardinality of Ti. Hence, CovNorm can be
implemented by ﬁnetuning A to each Ti, storing the PCAs
Px,i,Py,i, using (10) to reconstruct the covariance of T ,
and computing the global PCA. When tasks are available
sequentially, this can be done recursively, combining the
PCA of all previous data with the PCA of the new data.
In summary, CovNorm can be extended to any number of
tasks, with constant storage requirements (a single PCA),
and no loss of optimality. This makes it possible to deﬁne
two CovNorm modes.

• independent: A layers of network i are adapted to
target dataset Ti. A PCA is computed for Ti and

the mini-adaptation ﬁne-tuned to Ti. This requires
2d min(kx, ky) task speciﬁc parameters (per layer) per
dataset.

• joint: a global PCA is learned from T and ˜Cy, ˜Wx
shared across tasks. Only a mini-adaptation layer is
ﬁne-tuned per Ti. This requires min(kx, ky) task-
speciﬁc parameters (per layer) per dataset. All Ti must
be available simultaneously.

The independent model is needed if, for example, the de-
vices of Figure 1 are produced by different manufacturers.

4. Experiments

In this section, we present results for both the indepen-

dent and joint CovNorm modes.

Dataset: [34] proposed the decathlon dataset for eval-
uation of MDL. However, this is a collection of relatively
small datasets. While sufﬁcient to train small networks, we
found it hard to use with larger CNNs. Instead, we used a
collection of seven popular vision datasets. SUN 397 [47]
contains 397 classes of scene images and more than a mil-
lion images. MITIndoor [46] is an indoor scene dataset
with 67 classes and 80 samples per class. FGVC-Aircraft
Benchmark [26] is a ﬁne-grained classiﬁcation dataset of
10, 000 images of 100 types of airplanes. Flowers102 [32]
is a ﬁne-grained dataset with 102 ﬂower categories and 40
to 258 images per class. CIFAR100 [20] contains 60, 000
tiny images, from 100 classes. Caltech256 [12] contains
30, 607 images of 256 object categories, with at least 80
samples per class. SVHN [31] is a digit recognition dataset
with 10 classes and more than 70, 000 samples. In all cases,
images are resized to 224× 224 and the training and testing
splits deﬁned by the dataset are used, if available. Other-
wise, 75% is used for training and 25% for testing.

Implementation:

In all experiments, ﬁxed F layers
were extracted from a source VGG16 [42] model trained
on ImageNet. This has convolution layers of dimensions
ranging from 64 to 4096.
In a set of preliminary experi-
ments, we compared the MDL performance of the architec-
ture of Figure 1 with these F layers and adaptation layers
implemented with 1) a convolutional layer A of kernel size
1 × 1 [38], 2) the residual adapters T = B2(I + AB1)
of [34], where B1 and B2 are batch normalization layers
and A as in 1), and 3) the parallel adapters of [35]. Since
residual adapters produced the best results, we adopted this
structure in all our experiments. However, CovNorm can be
used with any of the other structures, or any other matrix A.
Note that B1 could be absorbed into A after ﬁne-tuning but
we have not done so, for consistency with [34].

In all experiments, ﬁne-tuning used initial learning rate
of 0.001, reduced by 10 when the loss stops decreasing. Af-
ter ﬁne-tuning the residual layer, features were extracted at
the input and output of A and the PCAs Px,Py computed

5429

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Figure 5: Ratio of effective dimensions (η) for different network layers.
Left: MITIndoor. Right: CIFAR100.

k=1 ei
k=1 ei

and used in Algorithm 1. Principal components were se-
lected by the explained variance criterion. Once the eigen-
values ei were computed and sorted by decreasing magni-
tude, i.e. e1 ≥ e2 ≥ . . . ≥ ed, the variance explained by
the ﬁrst i eigenvalues is ri = Pi
. Given a threshold t,
Pd
the smallest index i∗ such that ri∗ > t was determined, and
only the i∗ ﬁrst eigenvalues/eigenvectors were kept. This
set the dimensions kx, ky (depending on whether the proce-
dure was used on Px or Py). Unless otherwise noted, we
used t = 0.99, i.e. 99% of the variance was retained.
Benﬁts of CovNorm: We start with some independent
MDL experiments that provide insight on the beneﬁts of
CovNorm over previous MDL procedures. While we only
report results for MITIndoor and CIFAR100, they are typi-
cal of all target datasets. Figure 5 shows the ratio η = ky/kx
of effective output to input dimensions, as a function of
adaptation layer. It shows that the input of A typically con-
tains more information than the output. Note that η is rarely
one, is almost always less than 0.6, frequently smaller than
0.3, and smallest for the top network layers.

We next compared CovNorm to batch normalization
(BN) [2], and geometric approximations based on the ﬁne-
tunned approximation (FTA) of Section 3.3. We also tested
a mix of the geometric approaches (SVD+FTA), where A
was ﬁrst approximated by the SVD and the matrices C, W
ﬁnetuned on T , and a mix of PCA and FTA (PCA+FTA),
where the mini-adaptation layer Mx,y of CovNorm was re-
moved and ˜Cy, ˜Wx ﬁne-tuned on T , to minimize the PCA
alignment problem. All geometric approximations were im-
plemented with low-rank parameter values r = d/2i, where
d is the dimension of x or y and i ∈ {2, . . . , 6}. For
CovNorm, the explained variance threshold was varied in
[0.8, 0.995]. Figure 6 shows recognition accuracies vs. the
% of parameters. Here, 100% parameters corresponds the
adaptation layers of [34]: a network with residual adapters
whose matrix A is ﬁne-tunned on T . This is denoted RA
and shown as an upper-bound. A second upper-bound is
shown for full network ﬁne tuning (FNFT). This requires
10× more parameters than RA. BN, which requires close to
zero parameters, is shown as a lower bound.
Several observations are possible. First, all geomet-
ric approximations underperform CovNorm. For compa-
rable sizes, the accuracy drop of the best geometric method

0.74

0.72

0.7

0.68

0.66

0.64

0.62

0.6

0.58

0.8

0.75

0.7

0.65

CovNorm

PCA+FTA

FTA

SVD+FTA

BN

FNFT

RA

CovNorm

PCA+FTA

FTA

SVD+FTA

BN

FNFT

RA

0.02

0.04

0.06

0.08

0.1

0.12

0.05

0.1

0.15

0.2

0.25

Figure 6: accuracy vs. % of parameters used for adaptation. Left: MITIn-
door. Right: CIFAR100.

(SVD+FTA) is as large as 2%. This is partly due to the use
of a constant low rank r throughout the network. This can-
not match the effective, data-dependent, dimensions, which
vary across layers (see Figure 5). CovNorm eliminates this
problem. We experimented with heuristics for choosing
variable ranks but, as discussed below (Figure 7), could
not achieve good performance. Among the geometric ap-
proaches, SVD+FTA outperforms FTA, which has perfor-
mance drops in most of datasets. It is interesting that, while
A is ﬁne-tuned with random initialization, the process is
not effective for the low-rank matrices of FTA. In several
datasets, FTA could not match SVD+FTA.

Even more surprising were the weaker results obtained
when the random initialization was replaced by the two
PCAs (PCA+FTA). Note the large difference between
PCA+FTA and CovNorm (up to 4%), which differ by the
mini-adaptation layer Mx,y. This is explained by the align-
ment problem of Section 3.5.
Interestingly, while mini-
adaptation layers are critical to overcome this problem, they
are as easy to ﬁne-tune as A. In fact, the addition of these
layers (CovNorm) often outperformed the full matrix A
(RA). In some datasets, like MITIndoor, with 4.8% of the
parameters, CovNorm matched the performance of RA, Fi-
nally, as previously reported by [34], FNFT frequently un-
derperformed RA. This is likely due to overﬁtting.

CovNorm vs SVD: Figure 7 provides empirical evi-
dence for the vastly different quality of the approximations
produced by CovNorm and the SVD. The ﬁgure shows a
plot of the variance explained by the eigenvalues of the in-
put and output distributions of an adaptation layer A and
the corresponding plot for its singular values. Note how the
PCA energy is packed into a much smaller number of coefﬁ-
cients than the singular value energy. This happens because
PCA only accounts for the subspaces populated by data,
restricting the low-rank approximation to these subspaces.
Conversely, the geometric approximation must approximate
the matrix behavior even outside of these subspaces. Note
that the SVD is not only less efﬁcient in identifying the im-
portant dimensions, but also makes it difﬁcult to determine
how many singular values to keep. This prevents the use of
a layer-dependent number of singular values.

Comparison to previous methods: Table 1 summarizes
the recognition accuracy and % of adaptation layer param-

5430

Table 1: Classiﬁcation accuracy and % of adaptation parameters (with respect to VGG size) per target
dataset.

FNFT

FGVC
85.73%

MITIndoor

71.77%

Flowers
95.67%

Caltech256

83.73%
100%

SVHN
96.41%

SUN397
57.29%

CIFAR100

80.45%

BN [2]

43.6%

57.6%

83.07%

73.66%

91.1%

47.04%

64.8%

0%

Independent learning

average
81.58%
100%

65.83%

0%

LwF[23]

66.25%

73.43%

89.12%

80.02%

44.13%

52.85%

72.94%

68.39%

RA [34]

88.92%

72.4%

96.43%

SVD+FTA

89.07%

71.66%

95.67%

0%

84.17%

10%

84.46%

5%

96.13%

57.38%

79.55%

96.04%

57.12%

78.28%

0%

82.16%

10%

81.75%

5%

FTA

87.31%

70.26%

95.43%

83.82%

95.96%

56.43%

78.23%

81.06%

CovNorm

88.98%
0.34%

72.51%
0.62%

96.76%
0.35%

5%

84.75%
0.46%

Joint learning

96.23% 57.97%
0.13%
0.71%

79.42%

1.1%

5%

82.37%
0.53%

1

0.8

0.6

0.4

0.2

0

1

0

0.8

0.6

0.4

0.2

0

0

svd
in_pca
out_pca

100

200

300

400

500

svd
in_pca
out_pca

100

200

300

400

500

SVD [35]

88.98%

71.7%

96.37%

83.63%

96%

56.58%

78.26%

81.65%

CovNorm

88.99%

73.0%

96.69%

5%

84.77%
0.51%

96.22%

58.2

79.22%

5%

82.44%
0.51%

Figure 7: Variance explained by eigenval-
ues of a layer input and output, and similar
plot for singular values. Left: MITIndoor.
Right: CIFAR100.

RA [34]
DAN [39]

Piggyback [27]

CovNorm

Airc

C100

SVHN
ImNet
81.67%
96.13%
59.67%
86.05% 89.67% 96.77%
57.74%
57.69%
79.09%
60.37% 69.37% 81.34% 98.75% 59.95% 99.14% 83.44%

avg acc
50.12% 76.89%
2621
2851
77.01%
49.38%
97.24% 47.48%
76.6%
2838
78.55% 3713
96.55%
48.92%

GTSR
97.57%
98.46%
97.27%

DPed
93.88%
91.3%
96.99%

57.13%
56.54%
57.45%

61.87%
64.12%
65.29%

81.20%
80.07%
79.87%

87.63%
87.69%

89.62%

DTD

OGlt

UCF

Flwr

S

#par

2

2.17
1.28
1.25

Table 2: Visual Decathlon results

eters vs. VGG model size (100% parameters), for various
methods. All abbreviations are as above. Beyond MDL,
we compare to learning without forgetting (LwF) [23] a
lifelong method to learn a model that shares all parame-
ters among datasets. The table is split into independent and
joint MDL. For joint learning, CovNorm is implemented
with (10) and compared to the SVD approach of [35].

Several observations can be made. First, CovNorm
adapts the number of parameters to the task, according to
its complexity and how different it is from the source (Ima-
geNet). For the simplest datasets, such as the 10-digit class
SVHN, adaptation can require as few as 0.13% task-speciﬁc
parameters. Datasets that are more diverse but ImageNet-
like, such as Caltech256, require around 0.46% parameters.
Finally, larger adaptation layers are required by datasets that
are either complex or quite different from ImageNet, e.g.
scene (MITIndoor, SUN397) recognition tasks. Even here,
adaptation requires less than 1% parameters. On average,
CovNorm requires 0.53% additional parameters per dataset.
Second, for independent learning, all methods based on
residual adapters signiﬁcantly outperform BN and LwF. As
shown by [34], RA outperforms FNFT. BN is uniformly
weak, LwF performs very well on MITIndoor and Cal-
tech256, but poorly on most other datasets. Third, Cov-
Norm outperforms even RA, achieving higher recognition
accuracy with 20× less parameters.
It also outperforms
SVD+FTA and FTA by ≈ 0.6% and ≈ 1.3%, respectively,
while reducing parameter sizes by a factor of ≈ 10. On a
per-dataset basis, CovNorm outperforms RA on all datasets
other than CIFAR100, and SVD+FTA and FTA on all of

them. In all datasets, the parameter savings are signiﬁcant.
Fourth, for joint training, CovNorm is substantially supe-
rior to the SVD [35], with higher recognition rates in all
datasets, gains of up to 1.62% (SUN397), and close to 10×
less parameters. Finally, comparing independent and joint
CovNorm, the latter has slightly higher recognition for a
slightly higher parameter count. Hence, the two approaches
are roughly equivalent.

Results on Visual Decathlon Table 2 presents results
on the Decathlon challenge [34], composed of ten differ-
ent datasets of small images (72 × 72). Models are trained
with a combination of training and validation set and results
obtained online. For fair comparison, we use the learning
protocol of [34]. CovNorm achieves state of the art perfor-
mance in terms of classiﬁcation accuracy, parameter size,
and decathlon score S.

5. Conclusion

CovNorm is an MDL technique of very simple imple-
mentation. When compared to previous methods, it dramat-
ically reduces the number of adaptation parameters without
loss of recognition performance. It was used to show that
large CNNs can be “recycled” across problems as diverse
as digit, object, scene, or ﬁne-grained classes, with no loss,
by simply tuning 0.5% of their parameters.

6. Acknowledgment

This work was partially funded by NSF awards IIS-
1546305 and IIS-1637941, a GRO grant from Samsung, and
NVIDIA GPU donations.

5431

References

[1] R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate:
Lifelong learning with a network of experts. In CVPR, pages
7120–7129, 2017.

[2] H. Bilen and A. Vedaldi. Universal representations: The
missing link between faces, text, planktons, and cat breeds.
arXiv preprint arXiv:1701.07275, 2017.

[3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-
ishnan. Unsupervised pixel-level domain adaptation with
generative adversarial networks.
In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 1, page 7, 2017.

[4] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In Advances in Neu-
ral Information Processing Systems, pages 343–351, 2016.

[5] F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bul`o.
In ICCV,

Autodial: Automatic domain alignment layers.
pages 5077–5085, 2017.

[6] R. Caruana. Multitask learning. In Learning to learn, pages

95–133. Springer, 1998.

[7] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2650–2658, 2015.

[8] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
International Conference in Machine

by backpropagation.
Learning, 2014.

[9] R. Girshick. Fast r-cnn. arXiv preprint arXiv:1504.08083,

2015.

[10] G. Gkioxari, R. Girshick, and J. Malik. Contextual action
recognition with r* cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1080–1088,
2015.

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[12] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-

egory dataset. 2007.

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[14] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adver-
sarial domain adaptation. arXiv preprint arXiv:1711.03213,
2017.

[15] J. Huang, R. S. Feris, Q. Chen, and S. Yan. Cross-domain
image retrieval with a dual attribute-aware ranking network.
In Proceedings of the IEEE international conference on com-
puter vision, pages 1062–1070, 2015.

[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.

[18] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using
uncertainty to weigh losses for scene geometry and seman-
tics. arXiv preprint arXiv:1705.07115, 3, 2017.

[19] I. Kokkinos. Ubernet: Training a universal convolutional
neural network for low-, mid-, and high-level vision using
diverse datasets and limited memory. In CVPR, volume 2,
page 8, 2017.

[20] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009.

[21] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,

521(7553):436, 2015.

[22] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang.
Overcoming catastrophic forgetting by incremental moment
matching.
In Advances in Neural Information Processing
Systems, pages 4655–4665, 2017.

[23] Z. Li and D. Hoiem. Learning without forgetting.

IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2017.

[24] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning
transferable features with deep adaptation networks. Inter-
national Conference in Machine Learning, 2015.

[25] Y. Lu, A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. S.
Feris. Fully-adaptive feature sharing in multi-task networks
with applications in person attribute classiﬁcation. In CVPR,
volume 1, page 6, 2017.

[26] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi.
Fine-grained visual classiﬁcation of aircraft. arXiv preprint
arXiv:1306.5151, 2013.

[27] A. Mallya, D. Davis, and S. Lazebnik. Piggyback: Adapt-
ing a single network to multiple tasks by learning to mask
weights.
In Proceedings of the European Conference on
Computer Vision (ECCV), pages 67–82, 2018.

[28] M. Mancini, L. Porzi, S. R. Bul`o, B. Caputo, and E. Ricci.
Boosting domain adaptation by discovering latent domains.
arXiv preprint arXiv:1805.01386, 2018.

[29] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-

stitch Networks for Multi-task Learning. In CVPR, 2016.

[30] P. Morgado and N. Vasconcelos. Semantically consistent
regularization for zero-shot recognition. In CVPR, volume 9,
page 10, 2017.

[31] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011.

[32] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth In-
dian Conference on, pages 722–729. IEEE, 2008.

[33] R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A deep
multi-task learning framework for face detection, landmark
localization, pose estimation, and gender recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2017.

[17] B. Jou and S.-F. Chang. Deep cross residual learning for mul-
titask visual recognition. In Proceedings of the 2016 ACM on
Multimedia Conference, pages 998–1007. ACM, 2016.

[34] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple
visual domains with residual adapters. In Advances in Neural
Information Processing Systems, pages 506–516, 2017.

5432

[35] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi.

Efﬁcient
parametrization of multi-domain deep neural networks.
arXiv preprint arXiv:1803.10082, 2018.

[36] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert.
icarl: Incremental classiﬁer and representation learning. In
Proc. CVPR, 2017.

[37] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: to-
wards real-time object detection with region proposal net-
works. IEEE transactions on pattern analysis and machine
intelligence, 39(6):1137–1149, 2017.

[38] A. Rosenfeld and J. K. Tsotsos. Incremental learning through

deep adaptation. arXiv preprint arXiv:1705.04228, 2017.

[39] A. Rosenfeld and J. K. Tsotsos. Incremental learning through
deep adaptation. IEEE transactions on pattern analysis and
machine intelligence, 2018.

[40] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,
J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Had-
sell.
arXiv preprint
arXiv:1606.04671, 2016.

Progressive neural networks.

[41] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training.
In CVPR, volume 2,
page 5, 2017.

[42] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[43] B. Sun and K. Saenko. Deep coral: Correlation alignment for
deep domain adaptation. In European Conference on Com-
puter Vision, pages 443–450. Springer, 2016.

[44] A. R. Triki, R. Aljundi, M. B. Blaschko, and T. Tuytelaars.
IEEE Conference Com-

Encoder based lifelong learning.
puter Vision and Pattern Recognition, 2017.

[45] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In Computer Vision and
Pattern Recognition (CVPR), volume 1, page 4, 2017.

[46] M. Valenti, B. Bethke, D. Dale, A. Frank, J. McGrew,
S. Ahrens, J. P. How, and J. Vian. The mit indoor multi-
vehicle ﬂight testbed.
In Robotics and Automation, 2007
IEEE International Conference on, pages 2758–2759. IEEE,
2007.

[47] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
Sun database: Large-scale scene recognition from abbey to
zoo.
In Computer vision and pattern recognition (CVPR),
2010 IEEE conference on, pages 3485–3492. IEEE, 2010.

[48] A. R. Zamir, A. Sax, W. Shen, L. Guibas, J. Malik, and
S. Savarese. Taskonomy: Disentangling task transfer learn-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3712–3722, 2018.

[49] Y. Zhang and Q. Yang. A survey on multi-task learning.

arXiv preprint arXiv:1707.08114, 2017.

[50] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Facial landmark
detection by deep multi-task learning. In European Confer-
ence on Computer Vision, pages 94–108. Springer, 2014.

5433

