Learning Channel-wise Interactions for Binary Convolutional Neural Networks

Ziwei Wang1,2,3, Jiwen Lu1,2,3, Chenxin Tao1, Jie Zhou1,2,3, Qi Tian4

1 Department of Automation, Tsinghua University, China

2 State Key Lab of Intelligent Technologies and Systems, China

3 Beijing National Research Center for Information Science and Technology, China

4 Huawei Noah’s Ark Lab, China

wang-zw18@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn; tcx16@mails.tsinghua.edu.cn;

jzhou@tsinghua.edu.cn; tian.qi@huawei.com

Abstract

In this paper, we propose a channel-wise interaction
based binary convolutional neural network learning method
(CI-BCNN) for efﬁcient inference. Conventional methods
apply xnor and bitcount operations in binary convolution
with notable quantization error, which usually obtains in-
consistent signs in binary feature maps compared with their
full-precision counterpart and leads to signiﬁcant informa-
tion loss. In contrast, our CI-BCNN mines the channel-wise
interactions, through which prior knowledge is provided to
alleviate inconsistency of signs in binary feature maps and
preserves the information of input samples during inference.
Speciﬁcally, we mine the channel-wise interactions by a re-
inforcement learning model, and impose channel-wise pri-
ors on the intermediate feature maps through the interacted
bitcount function. Extensive experiments on the CIFAR-10
and ImageNet datasets show that our method outperforms
the state-of-the-art binary convolutional neural networks
with less computational and storage cost.

1. Introduction

Deep convolutional neural networks have achieved state-
of-the-art performances in various vision applications such
as object detection [10, 33, 22], tracking [13, 28, 1], face
recognition [38, 29, 7] and many others. However, deploy-
ing deep convolutional neural networks in portable devices
for inference is still limited because of the huge compu-
tational and storage cost. Moreover, high degrees of re-
dundancy are exhibited in parameters of well-trained mod-
els [5]. Hence, it is desirable to design deep convolutional
neural networks with fewer parameters and lighter architec-
tures for efﬁcient inference.

Layer(cid:3)l

-0.9 -0.3 0.2

0.1

0.1

-0.8 -1.0 -0.8 0.1 -0.1

-0.5 0.1 -0.9 0.7

-0.1 -0.1 0.7

0.6

-0.3 -0.1 -0.8 0.3

0.9

0.8

0.8

(cid:1850)(cid:1870)(cid:28595)

Full-precision
Binary

Layer(cid:3)l

1

-1 -1 1
1
-1 -1 -1 1 -1
1
-1 1 -1 1
1
-1 -1 1
1
-1 -1 -1 1
1
Xnor-Net

(cid:1850)(cid:1854)(cid:28595)

(cid:1624)(cid:28595)

(cid:1624)(cid:28595)

-0.4 0.8

0.8

0.6

0.5

0.9

0.8 -0.1 0.9

(cid:26)(cid:1870)(cid:883)(cid:28595)

-0.6 -0.3 0.4

-0.5 -0.9 -0.5

0.9 -1.0 0.1

(cid:26)(cid:1870)(cid:884)(cid:28595)

-1 1 -1
1
1
1 -1 1

1

-1 -1 1
-1 -1 -1
1 -1 1

(cid:26)(cid:1854)(cid:883)(cid:28595)
(cid:26)(cid:1854)(cid:884)(cid:28595)

(cid:3404)(cid:28595)

-1.7 -0.1 -0.4

-0.8 0.2

-0.4 0.4

1.1

1.7

1.1

0.6

1.6 -1.1

0.2 -0.3

-0.7

0

-1.4

(cid:3404)(cid:28595)

-5 5 -1
-1 1
3
5
-1 1

5 -1
3
3
1 -1
-1 1 -3

Layer(cid:3)l+1

1
1

-5 -1 -1
-1
3
5
-1
-6
-1
-1
-3

3
3
-1

5
1
1

BatchNorm 

Sign

Channel-wise
   Interaction

CI-BCNN

-1 -1 -1
-1
1
1
-1

1
1

(cid:4)(cid:1314)(cid:1854)(cid:883)(cid:28595)

1
1
1

(cid:4)(cid:1314)(cid:1854)(cid:884)(cid:28595)

-1
-1
-1

1
1
-1

Layer(cid:3)l+1

-3.0 -0.1 -0.6

-1.5 0.4

-0.7 0.7

2.2

3.2

(cid:4)(cid:1870)(cid:883)(cid:28595)
(cid:1827)(cid:1870)(cid:883)(cid:28595)
(cid:4)(cid:1870)(cid:884)(cid:28595)
(cid:1827)(cid:1870)(cid:884)(cid:28595)

2.4 -1.2

0.6 -0.1

-0.6 0.4 -1.6

BatchNorm 

1.8

1.2

BatchNorm

Sign

Layer(cid:3)l+1

-1 1 -1
-1 1
1
1
-1 1

1 -1
1
1
1 -1
-1 1 -1

(cid:4)(cid:1854)(cid:883) (cid:28595)
(cid:4)(cid:1854)(cid:884) (cid:28595)

Input

Filters

Output

Inconsistent Signs

Corrected Signs

Figure 1. Convolution operations in real-valued neural networks
(top), Xnor-Net (the yellow box) and our CI-BCNN (the green
box). Because of the quantization error resulted from xnor
and bitcount operations, Xnor-Net usually outputs binary feature
maps which have the inconsistent signs compared with their full-
precision counterpart (the red circle). Our CI-BCNN provides pri-
ors according to channel-wise interactions to correct the inconsis-
tent signs (the blue circle), which preserves information for inter-
mediate feature maps (best viewed in color).

Recently, several neural network compression methods
have been proposed including pruning [9, 21, 12], quantiza-

tion [23, 17, 8], low-rank decomposition [6, 39, 43] and ef-
ﬁcient architecture design [18, 15, 26]. Among these meth-

4321568

ods, network quantization represents parameters of the neu-
ral networks in constrained bandwidth for faster processing
and less memory consumption. Neural networks with bi-
nary weights replace multiply-accumulate operations with
accumulation [3, 42, 14] to save storage cost and accel-
erate computation. However, real-valued calculation is
still computationally expensive. To address this, neural
networks with both binary weights and activations substi-
tute multiply-accumulation with xnor and bitcount opera-
tions [32, 23, 24]. However, applying xnor and bitcount
operations causes and accumulates notable quantization er-
ror, which usually results in inconsistent signs in binary fea-
ture maps compared with their full-precision counterpart.
The information loss in binary neural networks explains the
signiﬁcant performance degradation compared with real-
valued neural networks especially when evaluated in large-
scale datasets such as ImageNet [4].

In this paper, we present a CI-BCNN method to learn bi-
nary neural network with channel-wise interactions for ef-
ﬁcient inference. Unlike existing methods which directly
apply xnor and bitcount operations, our method learns in-
teracted bitcount according to the mined channel-wise inter-
actions. The inconsistent signs in binary feature maps are
corrected based on prior knowledge provided by channel-
wise interactions, so that information of input images is
preserved in the forward-propagation of binary neural net-
works. More speciﬁcally, we employ a reinforcement learn-
ing model to learn an directed acyclic graph for each convo-
lutional layer, which stands for implicit channel-wise inter-
actions. We obtain the interacted bitcount by adjusting the
output of the original bitcount in line with the effects ex-
erted by the graph. We train the binary convolutional neural
network and the structure of graph simultaneously. Figure 1
depicts the comparison between our CI-BCNN and the con-
ventional binary neural network, where inconsistent signs in
binary feature maps are corrected according to the channel-
wise interactions. Experiments on the CIFAR-10 [19] and
ImageNet datasets show that our CI-BCNN outperforms
most state-of-the-art binary neural networks by a large mar-
gin across various network architectures.

2. Related Work

Network Quantization: Network quantization has
aroused extensive interest in machine learning and com-
puter vision due to the reduction of the network complex-
ity for wide deployment. Existing methods can be di-
vided into two categories: neural networks with quanti-
zation on weights [3, 32, 42, 14] versus on both weights
and activations [32, 17, 23, 24]. Weight-only quantiza-
tion methods quantized weights in deep neural networks
to save storage cost and substitute the original multiply-
accumulation with accumulation for fast processing. Cour-
bariaux et al. binarized the real-valued weights via a rigid

sign function and obtained sufﬁciently high accuracy on
small datasets. Rastegari et al. approximated the real-
valued weights for binarization with a scaling factor to
improve the accuracy. Zhang et al. trained an adaptive
quantizer for weights according to their distribution, min-
imizing quantization error while staying compatible with
the bitwise operations. Hou et al. applied the Taylor Ex-
pansion to minimize the loss caused by quantization per-
turbation, and proposed a proximal Newton algorithm to
ﬁnd the optimal solution for quantization strategy. Em-
pirical studies showed that wider bandwidth for represent-
ing weights led to comparable performance with their full-
precision counterpart, ternary and other multi-bit quanti-
zation methods [44, 36, 25] were proposed to obtain bet-
ter performance. However, real-valued activations prevent
substantial acceleration due to the existed accumulation op-
erations.
In the latter regard, weights and activations are
both quantized so that multiply-accumulation is replaced
by xnor and bitcount operations, leading to much less com-
putational complexity. Rastegari et al. and Hubara et al.
proposed neural networks with both weights and activations
binarized, applying xnor and bitcount operations to substi-
tute multiply-accumulation to obtain appreciable speedup.
Lin et al. utilized more bases for weight and activation bi-
narization, enhancing the performance especially in large-
scale datasets. Liu et al. connected the real-valued activa-
tions of consecutive blocks with an identity shortcut before
binarization to strengthen the representational capability of
the network. They also used a new training algorithm to
accurately back-propagate the gradient. Nevertheless, ap-
plying xnor and bitcount operations causes and accumulates
the quantization error, leading to severe information loss be-
cause of inconsistent signs in binary feature maps compared
with their real-valued counterpart.

Deep Reinforcement Learning: Deep reinforcement
learning purposes to learn the policies for decision-making
problems, which obtains promising results in playing
games [27, 34], object detection [30, 31], visual track-
ing [16, 35, 40, 41] and many others. Recently, reinforce-
ment learning has been adopted to network compression.
Lin et al. applied a policy gradient model to judge the im-
portance of feature maps, and pruned the network adap-
tively based on the input images and current feature maps
to fully preserve the ability of the network. Ashok et al.
shrank a large teacher network to a small student network by
removing redundant layers and shrinking the size of the re-
maining layers, where a reinforcement learning model was
employed to learn the policy. He et al. efﬁciently sampled
the network architecture space by leveraging a reinforce-
ment learning model, so that the model is compressed auto-
matically without predeﬁned pipelines. In this paper, we ex-
tend the reinforcement learning model to mine the channel-
wise interactions in convolutional neural networks with bi-

4322569

nary weights and activations, through which the inconsis-
tent signs caused by xnor and bitcount operations are cor-
rected and information of input images is preserved in the
forward-propagation process.

3. Approach

In this section, we ﬁrst introduce neural networks with
binary weights and activations brieﬂy, which are efﬁcient
but suffer from inconsistent signs in intermediate feature
maps. Then we present the details of imposing channel-
wise interactions through the interacted bitcount. Finally,
we propose a policy gradient model to mine the channel-
wise interactions.

3.1. Binary Neural Networks

Al

f ×hl

r ∈ Rwl
a×hl

Let W l
r ∈ Rwl

f be the real-valued weights and
a be the full-precision activations of the lth
convolutional layer in a given L-layer CNN model, where
(wl
a) represents the width and height of
ﬁlters and feature maps in the lth layer. Al
r carries infor-
mation of input samples without binarization error:

f ) and (wl

f , hl

a, hl

Al

r = W l

r ⊛ Al−1

r

where ⊛ stands for the standard convolution and activation
layers are omitted for simplicity. In order to obtain neural
networks with less computational and storage cost, we uti-
lize binary weights and activations to replace the multiply-
accumulation with xnor and bitcount operations [32] in the
forward-propagation:

Al

b = sign(W l

b ⊙ Al−1

b

)

a

a×hl

f ×hl

f and Al

b ∈ {+1, −1}wl

b ∈ {+1, −1}wl

where W l
are binary weights and activations of the lth layer respec-
tively. ⊙ indicates element-wise binary product represent-
ing xnor and bitcount operations in binary neural networks,
where the bitcount is to count the number of ones in the
results of xnor operations in each convolution. sign means
the sign function which maps number larger than one to one
and to minus one otherwise.

The objective for binarizing convolutional neural net-
works is to minimize the distance between binary and real-
valued feature maps so that information loss is minimal,
which is written as follows:

min
b ,A

b

W l

l−1

||Al

r − Al

b||2

2

(1)

b = sign(Al

where the optimization is NP-hard and the equivalent equa-
tion is Al
r). Conventional methods obtain
the approximate solutions W l
b =
sign(Al−1

b = sign(W l

r) and Al−1

) by assuming:

r

sign(Al

r) ≈ sign(sign(W l
= sign(sign(W l

r) ⊛ sign(Al−1
r) ⊙ sign(Al−1

r

r

))

))

However, due to the quantization error occurred in xnor and

b compared with sign(Al

bitcount operations, the assumption does not always hold as
shown in Figure 1. The approximate solution has inconsis-
tent signs in Al
r), so that Equation
(1) is far from the optimal states. Moreover, the error is ac-
cumulated across layers and causes severe information loss
of input images in the forward-propagation. Our objective
is to minimize the difference between Al
r) in
each layer by correcting the inconsistent signs in Al
b.

b and sign(Al

3.2. Interacted Bitcount

Applying xnor operations brings signiﬁcant quantization
error compared with multiplication in full-precision. More-
over, original bitcount accumulate the error, which usually
outputs inconsistent signs in feature maps compared to their
real-valued counterpart. It is shown empirically that there is
implicit dependency among ﬁlters, through which reliable
priors are provided to offset the error resulted from xnor
and bitcount operations. The interacted bitcount modiﬁes
the original bitcount as follows:

(cid:2)pl
s,ij = pl

s,ij + (cid:3)

t

δl
ts(pl

t,ij)

(2)

s,ij and pl

where the upscript l represents the corresponding variable
in the lth convolutional layer. pl
t,ij are the integer
pixel values output by the original bitcount in the ith row
and jth column in the instructed (student) feature map F l
s
and directive (teacher) feature map F l
s,ij is
the corresponding pixel value output by the interacted bit-
ts represents the inﬂuence function imposing on F l
count. δl
s
from F l
t .

t respectively. (cid:2)pl

To prevent the network suffering from heavy computa-
tion overhead of interacted bitcount, we simply design δl
ts
as a discrete function. We partition the value range of pixels
in F l
ts| intervals with equal length when consider-
ing its interaction to F l
ts is an odd integer so that no
interaction exists if pl
t,ij stays near zero without sufﬁcient
information. Integer output of δl

ts is obtained as follows:

t into |K l

s. K l

δl
ts(pl

t,ij) = (

1 − |K l

ts|

2

+ k) ·

K l
ts
|K l
ts|

· [U0N0],

(3)

if

pl
t,ij ∈ (pk, pk+1],

k = 0, 1..., |K l

ts| − 1

where pk is the origin of the kth interval in value range par-
tition of the teacher feature map F l
t . N0 is the maximum in
value range of F l
t , which is identical for all feature maps in
the same layer. U0 means the ratio of unit pixel modiﬁcation
to N0, which is manually set to decide the importance of
the prior. [U0N0] stands for the minimal integer larger than
U0N0. Meanwhile, K l
ts can be a negative integer mean-
ing that the student and teacher feature maps are negatively
correlated. We have |K l
|K l

ts| choices from

[U0N0] for the output of δl
ts function, representing
s exerted by F l
t .

different effects on F l

[U0N0] to

ts|−1
2

1−|K l

ts|

2

4323570

Create

C1

Create and Reassign (cid:1837)(cid:1872)(cid:1871)(cid:1864) (cid:28595)

C1

C1

space S l

i across layer index l:
L(cid:4)

S =

e × S l
S l

i

C3

C4

C3

C4

C3

C4

l=1

C2

C5

C2

C5

C2

C5

e
t
a
e
r
C

C1

C1

C1

C3

C4

C3

C4

C3

C4

C2

C5

C2

C5

Create and Delete

C2

C5

Reassign (cid:1837)(cid:1872)(cid:1871)(cid:1864) (cid:28595)

0

0

-1

1

288

288

96

96

1

-1

(a)

-96

-96

-5

-4

-3

-2

-1

0

1

2

3

4

5

(b)

(cid:4667)(cid:28595)

-288
(cid:28595)

-288

(cid:577)(cid:1872)(cid:1871)(cid:1864)(cid:4666)(cid:1868)(cid:1872)(cid:481)(cid:1861)(cid:1862)(cid:1864)
(cid:1837)(cid:1872)(cid:1871)(cid:1864) (cid:3404)(cid:3398)(cid:885)(cid:28595)
(cid:1868)(cid:1872)(cid:481)(cid:1861)(cid:1862)(cid:1864)
(cid:1837)(cid:1872)(cid:1871)(cid:1864) (cid:3404)(cid:885)(cid:28595)
(cid:1837)(cid:1872)(cid:1871)(cid:1864) (cid:3404)(cid:883)(cid:883)(cid:28595)

Figure 2. Illustrations for state transitions and interacted bitcount
based on the mined graph.
(a) An example for graph mining.
We create edges, reassign K l
ts and delete edges between different
channels until ﬁnalizing the graph structure (best viewed in color).
(b) A fast way to calculate the impact of graph on interacted bit-
count according to pixel values in the teacher feature map via stair
functions, where N0 = 288 and U0 is set as 0.001 in the example.

es,ts in W l

e is deﬁned as the existence matrix W l

where S l
es ∈
{1, 0}cl×cl
and cl stands for the number of the channels in
the lth layer. For the element wl
es, it equals to one
if the directed interaction from tth to sth channel exists and
equals to zero otherwise. Zero matrix in CI-BCNN is equiv-
alent to conventional binary convolutional neural networks
without channel-wise interaction. The inﬂuence space S l
i is
modeled by the inﬂuence matrix W l
is with odd integers. In
this paper, we limit the space with ﬁnite discrete numbers
as W l
, where K0
is a hyperparameter representing the size of action space.
In our implementation, element wl
is is scaled to
K l
ts
|K l
pact of the corresponding interaction.

is ∈ {±3, ±5, ±7, ..., ±(2K0 + 1)}cl×cl

for regularization, which measures the im-

ts| · |K l

is,ts in W l

ts|−1
2K0

e and in inﬂuence Al

e consists of three compositional sets: Al

Action: The action set A is the direct product of action
space in existence Al
i across all lay-
ers. Al
e,c for edge
creation, Al
e,d for edge deletion and {unchange} for re-
maining the existence invariant. Al
i depicts all possible odd
integers in W l
is for existing edges. Moreover, we stop the
policy network when the graph converges or achieving the
maximal steps. The whole action set is described as:

3.3. Channel-wise Interaction Mining via Policy

A =

Gradient

The channel-wise interaction is deﬁned as edges in the
graph among channels, which is expressed as existence and
inﬂuence. Existence of an edge demonstrates the correla-
tion between the two connected nodes, represented by one
if the coherence is sufﬁciently signiﬁcant and zero other-
wise. An edge’s inﬂuence means the impact on the end
node imposed by the start node if the correlation exists.
Because partitioning the value range of the teacher fea-
ture map into more intervals stands for greater impact of
the channel-wise relationship, we depict the inﬂuence by
K l
ts. Mining the channel-wise interaction can be viewed
as a Marcov Decision Process (MDP), deﬁned as M =
{S, A, T (S, A), R(S, A)}. At each step, the agent takes
an action to create, delete or unchange edges to modify the
existence of edges in the graph together with assigning dif-
ferent values to K l
ts to represent various inﬂuences for all
existing edges. The agent iteratively revamps the structure
of the graph to maximize the gained reward until conver-
gence or achieving the upper limit of steps.

States: The state space S expresses the current structure
of the graph in all convolutional layers, which is represented
as the direct product of the existence space S l
e and inﬂuence

L(cid:4)

i=1

L(cid:4)

i=1

(Al

e × Al

i) ∪ {stop}

((Al

e,c ∪ Al

e,d ∪ {unchange}) × Al

i) ∪ {stop}

=

Figure 2 represents an example of stage transitions with

actions and a fast way to implement interacted bitcount.

′

Transition Function: T (S, A) → S

is the transition
function that shows the probability to convert the old state
into the new one . T is constructed after deﬁning state and
action space, which is the direct product of two transition
functions in all convolutional layers, T l
e for existence trans-
formation and T l
i for inﬂuence change:

T (S, A) =

L(cid:4)

i=1

T l
e (S l

e, Al

e) × T l

i (S l

i , Al

i, S l

e, Al
e)

, whose element wl

T l
e is represented by a existence transition matrix W l

et ∈
[0, 1]cl×cl
et,ij demonstrates the probabil-
ity of connecting the directed edge from the ith channel to
the jth one with the normalization (cid:5)i,j wl
et,ij = 1. We

select actions according to the following rules:

(1) Create: The density of the existence matrix ρ is deﬁned
as the ratio of ones in the existence matrix. When the

4324571

density of existence matrix is sparser than the hyper-
parameter ρmax, we create an edge directing to the jth
channel from the ith one if the sampling strategy based
on W l
et,ij and the edge has not
been connected.

et selects the element wl

(2) Delete:

The probability of deletion is
′l
et = N orm([− log wl

formu-
et,ij]cl×cl ), where
lated as W
N orm means the normalization operation that ensures
′l
et||1 = 1. The probabilities of creation and dele-
||W
tion are negatively related because low probabilities of
connection stand for the trend to disconnect the edges.
Meanwhile, differences of low probabilities in W l
et are
very small and can only be revealed by their power ex-
ponent, so we applied logarithm to represent the possi-
bility of deletion. We delete the existing edge between
the ith channel to the jth one if the sampling strategy
chooses the element w

′l
et,ij in W

′l
et.

(3) Unchange: We remain the existence of edges unvaried

if no creation or deletion happens.

As for the part of inﬂuence, we parameterize T l
ﬂuence matrix W l
deterministically in Al
tion:

i with a in-
and select odd numbers
i for Kts according to a stair func-

it ∈ [−1, 1]cl×cl

K l

ts =

wl
|wl

it,ts
it,ts|

· [2 ∗ [|K0wl

it,ts|] + 1]

(4)

Finally, we take the action stop to terminate the current
epoch of channel-wise interaction mining when the policy
network converges or achieves the maximal steps.

Reward Function: The reward function R(S, A) in

round τ is modeled as follows:

r(sτ , aτ ) = rc(sτ , aτ ) + rp(sτ , aτ )

= sgn(|C(sτ ) − C(sτ +1)| − h)

C(sτ ) − C(sτ +1)
|C(sτ ) − C(sτ +1)|

+

1
N

L

(cid:2)

(cid:2)

(cid:2)

l=1

t,s

i,j

|pl
||pl

t,ij(sτ +1)| − |pl
t,ij(sτ +1)| − |pl

s,ij(sτ +1)|
s,ij(sτ +1)||

where C(sτ ) represents the cross-entropy loss of the bi-
nary neural network for prediction under the graph mined
in round τ , and h is a positive threshold whose value is
assigned manually. pl
s,ij(sτ +1) means the
pixel values in the ith row and jth column of the student and
teacher feature maps, which are output by the interacted bit-
count with the graph mined in round t + 1. N stands for the
number of total pixels of feature maps in the binary neural

t,ij(sτ +1) and pl

networks, which equals to (cid:5)L

l=1 (cid:5)t,s (cid:5)i,j 1.

The physical meaning of the reward function is illus-
trated by two terms. rc encourages the graph imposed on
the binary neural network to decrease the cross-entropy loss
in classiﬁcation. The agent acquires the reward +1 or −1
if the reduced or increased cross-entropy loss is larger than

a set threshold h, while gains no reward when the loss does
not change apparently. rp aims to ensure that the teacher
feature map is more informative than the student one so that
reliable priors are provided. Because pixels carry more in-
formation are usually activated or deactivated signiﬁcantly,
we expect the mean absolute value in the teacher feature
map is higher than their counterpart in the student one.

We employ a Encoder-decoder RNN for the policy net-
es and W l
work, which takes the current state of graph W l
is
as input, while outputs the transition matrix W l
et and W l
it
for the binary convolutional layer. Figure 3 shows the over-
all framework for training our CI-BCNN with the policy
network. We utilize the REINFORCE algorithm [37] to op-
timize the policy network. The objective is maximizing the
expected return over the entire CI-BCNN learning process:

max

θ

Z(θ) = Eπ[

T(cid:3)

τ =1

γrτ (sτ , aτ )]

(5)

where θ means parameters in the policy network and π rep-
resents the selected policy. T stands for the time of sam-
pling for each training batch and γ is the discount factor.
According to the policy gradient method, we compute the
expected gradient of the objective as follows:

∇θZ = −Eπ[rτ (sτ , aτ )∇θ log p(aτ |sτ )]

(6)

We apply Monte-Carlo Sampling to obtain the approxi-
mated gradients due to the intractability of exhaustion for
all possible states. Meanwhile, p(aτ |sτ ) is entangled by ac-
tions for exploring edge existence and inﬂuence, and the
probability to choose inﬂuence is deterministic and non-
differentiable.
In order to back-propagate gradients, we
approximate the optimization problem as another differen-
tiable one (formulated in supplementary materials).

4. Experiments

In this section, we evaluated our method on two datasets
for image classiﬁcation: CIFAR-10 and ImageNet. We
ﬁrstly introduced the implementation details of our CI-
BCNN and illustrated the effectiveness and intuitive logic
of CI-BCNN by toy examples. Then we investigated the in-
ﬂuence of hyperparameters by ablation study and compared
the proposed CI-BCNN with state-of-the-art binarized neu-
ral networks regarding the accuracy. Finally, we analyzed
the storage and computation complexity during inference in
comparison with other methods.

4.1. Implementation Details

We trained our CI-BCNN with the VGG-small [42] and
ResNet20 architectures on the CIFAR-10 dataset. We em-
ployed ResNet18 and ResNet34 for the proposed CI-BCNN
in the experiments on the ImageNet dataset. We iteratively

4325572

Encoder

Decoder

(cid:4668)(cid:1845)(cid:1857)(cid:1864)(cid:481)(cid:1845)(cid:1861)(cid:1864)(cid:4669)(cid:28595)

(cid:4668)(cid:1845)(cid:1857)(cid:1864)(cid:3398)(cid:883)(cid:481)(cid:1845)(cid:1861)(cid:1864)(cid:3398)(cid:883)(cid:4669)(cid:28595)
(cid:1860)(cid:1864)(cid:3398)(cid:883)(cid:28595)

(cid:1860)(cid:1864)(cid:28595)

(cid:4668)(cid:1845)(cid:1857)(cid:1864)(cid:3397)(cid:883)(cid:481)(cid:1845)(cid:1861)(cid:1864)(cid:3397)(cid:883)(cid:4669)(cid:28595)
(cid:1860)(cid:1864)(cid:3397)(cid:883)(cid:28595)

(cid:1849)(cid:1857)(cid:1872)(cid:1864)(cid:3398)(cid:883)(cid:28595) (cid:1849)(cid:1861)(cid:1872)(cid:1864)(cid:3398)(cid:883)(cid:28595)
(cid:1834)(cid:1864)(cid:3398)(cid:883)(cid:28595)

(cid:1849)(cid:1857)(cid:1872)(cid:1864) (cid:28595) (cid:1849)(cid:1861)(cid:1872)(cid:1864) (cid:28595)
(cid:1834)(cid:1864)(cid:28595)

(cid:1849)(cid:1857)(cid:1872)(cid:1864)(cid:3397)(cid:883)(cid:28595)
(cid:1834)(cid:1864)(cid:3397)(cid:883)(cid:28595)

Graph Modification

C1

C4

C3

Transition

Layer l
C1

C4

C2

C5

11

C2

11

C5

C1

C2

C5

C4

C3

Transition

Layer l+1
C1

C4

11

C2

C5

C3

C3

(cid:1849)(cid:1861)(cid:1872)(cid:1864)(cid:3397)(cid:883)(cid:28595)

Figure 3. Overall framework for training the CI-BCNN. The left part is the policy network which consists of encoders and decoders. The
encoders take the state of each layer as input and decoders output the corresponding transition matrix according to the hidden variables.
The right part stands for the graphs in binary neural networks, where the lth convolutional layer modiﬁes its graph structure based on the
transition matrix W l

it (best viewed in color).

et and W l

trained the binary neural network and the agent for mining
channel-wise interactions in our CI-BCNN. In the training
of the binary neural networks, the weights were binarized
to the sign of real-valued weights multiplying the absolute
mean value of each kernel. We followed the suggestion in
XNOR-net [32] to keep the weights in the ﬁrst and last layer
real-valued. We used the Adam optimizer for all experi-
ments with the batchsize 128. For experiments on CIFAR-
10, we ran our algorithm for 100 epochs. The initial learn-
ing rate was set as 0.001 and decayed by multiplying 0.1
in the 50th and 90th epoch. In the training on ImageNet,
We set the initial learning rate as 0.001 with multiplying 0.1
in the 20th and 30th epochs out of the total 40 epochs for
ResNet18. The learning rate started from 0.005 with de-
cay by 10× in the 40th and 60th epochs out of the total
80 epochs for ResNet34. When ﬁnishing training, we froze
all convolutional layers with the constrained weights to −1
and +1, and retrained the BatchNorm layer for 1 epoch to
absorb the scaling factor.

When training the policy network, we applied two con-
volutional layers with a fully-connected layer for the en-
coder and used a fully-connected layer with two deconvo-
lutional layers for the decoder in each module of RNN. We
used cl
16 matrices of the size 16 × 16 to represent state and
transition matrices in the lth layer for memory saving and
computation acceleration. We set the hyperparameters U0,
ρmax, K0 and α as 0.01, 0.1, 2 and 0.001 respectively in
the comparison with state-of-the-art methods.

4.2. Toy Examples

The thought of the proposed CI-BCNN is to mine the
correlational graph among channels to correct inconsistent
signs in binary feature maps caused by the xnor and bitcount
operations. We conducted simple experiments a on the
MNIST dataset [20] to show the correctness of our thoughts
with intuition.

Effectiveness of the channel-wise interaction: We ar-

Figure 4. The square of correlation coefﬁcients among 10 channels
in the binary convolutional layer. Darker colors represent higher
correlations and the blue box demonstrates the connections mined
by our policy gradient networks.

gue there are implicit correlations among channels, pro-
viding priors for eliminating signiﬁcant quantization error
which causes inconsistent signs of pixel values in interme-
diate feature maps. Through our policy gradient network,
we can mine the relationships among channels. To vali-
date our thoughts, we designed the architecture with two
convolutional layers and one fully-connected layer for our
CI-BCNN. Figure 4 shows the square of correlation coefﬁ-
cients among different channels in the binary convolutional
layer, where darker colors mean higher correlations. The
blue box represents the channel-wise interactions learned by
our policy network. As can be seen, our model mined most
signiﬁcant correlations without irrelevant channels, which
provides reliable priors for the interacted bitcount.

Effectiveness of the interacted bitcount: Our inter-
acted bitcount utilized channel-wise interaction to provide
priors for recovering the original signs in the binary feature
map. We expect that more pixels in binary feature maps
have the same signs with their full-precision counterpart, so
that information of input images is preserved during infer-

4326573

Table 1. Comparison on the ratio of pixels with consistent signs in
different layers and corresponding accuracies of CI-BCNN.

Bitcout

Interacted Bitcount

Conv2
0.6238
0.6638

Conv3
0.6061
0.6244

Acc.(%)

99.01
99.10

A
c
c
u
r
a
c
y

A
c
c
u
r
a
c
y

ence. Table 1 shows the effect of the interacted bitcount
by the ratio of pixels with consistent signs in binary layers
and the classiﬁcation accuracy. The quantization error ac-
cumulates with the depth of layers, as the ratio is lower in
the Conv3 layer compared with the Conv2 layer. Our CI-
BCNN increases the ratio, which beneﬁts from the priors
provided by interacted bitcount.

4.3. Performance Analysis

In order to investigate the effect of channel-wise inter-
actions on intermediate feature maps, we conducted abla-
tion study with varying maximal densities of the existence
matrix ρ and ratios of unit pixel modiﬁcation U0. We re-
ported the classiﬁcation top-1 and top-5 accuracies on the
ImageNet dataset with the ResNet18 architecture.

Performances w.r.t. the maximal density of existence
matrix ρ: The density of existence matrix ρ is deﬁned as
the proportion of ones in the matrix, which is positively
correlated with the hyperparameter ρmax. Higher density
of existence matrix represents more channel-wise interac-
tions in the interacted bitcount. By changing the value of
ρmax in the training of the policy network, we can control
the ﬁnal density of existence matrix. The impact of ρmax on
the performance is illustrated in Figure 5(a). Medium den-
sity provides reliable priors for feature maps suffered from
inconsistent signs. High density assigns excess connections
in the graph with untrustworthy priors. Low density fails
to consider priors, which is unable to alleviate inconsistent
signs in binary feature maps caused by xnor and bitcount
operations.

Performances w.r.t. the ratio of unit pixel modiﬁca-
tion U0: Larger U0 in the interacted bitcount stands for
more signiﬁcant modiﬁcation, resulting in higher impor-
tance for the prior knowledge provided by channel-wise in-
teractions. The prior knowledge becomes more important
in classiﬁcation when compared with the posterior informa-
tion gained from input images. Figure 5(b) shows the per-
formance versus different U0. Medium U0 provides prior
knowledge for binarized neural networks, which is com-
bined with posteriors learned from the input image. Large
U0 enforces too strong priors on feature maps, ignoring the
knowledge obtained from the input sample. On the con-
trary, small U0 fails to impose affective priors on interme-
diate feature maps which suffer from inconsistent signs.

4.4. Comparison with State-of-the-art Methods

In this section, we compared the performance of our
CI-BCNN with existing methods including BNN [17],

(cid:1864)(cid:1867)(cid:1859)(cid:883)(cid:882)(cid:2025)(cid:1865)(cid:1853)(cid:1876) (cid:28595)

(cid:28595)

(cid:1864)(cid:1867)(cid:1859)(cid:883)(cid:882)(cid:1847)(cid:882)(cid:28595)

(cid:28595)

(a) Varying ρmax

(b) Varying U0

Figure 5. Top-1 and top-5 classiﬁcation accuracies on the Ima-
geNet dataset of the CI-BCNN in the architecture of ResNet18
with (a) varying ρmax and (b) varying U0. Variables are repre-
sented by their logarithm.

Table 2. Comparison of classiﬁcation accuracy (%) on CIFAR-10
with state-of-the-art methods in VGG-small and ResNet20.

Methods

Bitwidth (W/A) VGG-small ResNet20

Full-precision

BC
TTQ

HWGQ
LQ-Net

BNN

Xnor-Net
CI-BCNN

32/32

1/32
2/32

1/2
1/2

1/1
1/1
1/1

93.20

90.10

−

92.50
93.40

89.90
89.80
92.47

92.10

−

91.13

−

88.40

−
−

91.10

BC [3], BWN [32], Xnor-Net [32], Bi-Real-Net [24], ABC-
Net [23], LQ-Nets [42], SYQ [8] HWGQ [2] and TTQ [44]
through various architectures in image classiﬁcation tasks
on the CIFAR-10 and ImageNet datasets.

Comparison on CIFAR-10: The CIFAR-10 dataset
consists of 60,000 images of size 32×32, which are divided
into 10 categories. We applied 50,000 images as training set
and the rest 10,000 as the test set. We padded 4 pixels on
each side of the image and randomly cropped it into the size
of 32×32. Meanwhile, we scaled and biased all images into
the range [−1, 1]. We compared the accuracies of VGG-
small [42] and ResNet20 quantized by different methods.
Table 2 shows the results. The comparison clearly indicates
the proposed CI-BCNN outperforms the existed neural net-
works with one-bit weight and activations by a sizable mar-
gin. Our method is even comparable with HWGQ which
has activations in two bits and TTQ which has 2-bit weights
and real-valued activations in VGG-small and ResNet20 ar-
chitectures respectively.

Comparison on ImageNet:

ImageNet (ILSVRC12)
contains approximately 1.2 million training and 50K val-
idation images from 1,000 categories. ImageNet is much
more challenging because of its large scale and high diver-
sity. Followed by data augmentation of bias subtraction ap-
plied in CIFAR-10, a 224 × 224 region is randomly cropped
for training from the resized image whose shorter side is

4327574

Table 3. Comparison of classiﬁcation accuracy (%) on ImageNet
with state-of-the-art methods in ResNet18 and ResNet34.

4.5. Complexity Analysis

Methods

Bitwidth
(W/A)

ResNet18

ResNet34

top-1

top-5

top-1

top-5

Full-precision

32/32

69.30

89.20

73.30

91.30

BWN
TTQ

HWGQ
LQ-Net

SYQ

BNN

Xnor-Net
ABC-Net

Bi-Real-Net
CI-BCNN

CI-BCNN (add)

1/32
2/32

1/2
1/2
1/2

1/1
1/1
1/1
1/1
1/1
1/1

60.80
66.60

59.60
62.60
55.40

42.20
51.20
42.70
56.40
56.73

83.00
87.20

82.20
84.30
78.60

67.10
73.20
67.60
79.50
80.12

60.80

83.00

−

−

64.30
66.60

85.70
86.90

−

−

−

−

−

−

52.40
62.20
62.41

76.50
83.90
84.35

59.90

84.18

64.93

86.61

Table 4. Comparison of storage cost and FLOPs of different meth-
ods in ResNet18 and ResNet34.

ResNet18

ResNet34

Storage Cost

FLOPs

Full-precision

Xnor-Net

Bi-Real-Net
CI-BCNN

Full-precision

Xnor-Net

Bi-Real-Net
CI-BCNN

374.1Mbit
33.7Mbit
33.6Mbit
33.5Mbit

697.3Mbit
43.9Mbit
43.7Mbit
43.5Mbit

1.81 × 109
1.67 × 108
1.63 × 108
1.54 × 108

3.66 × 109
1.98 × 108
1.93 × 108
1.82 × 108

256. For inference, we employed the 224 × 224 center crop
from images. As demonstrated in [24], additional short-
cut in every consecutive convolutional layers enhance the
performance of binary neural networks, we employed ex-
tra shortcut for adjacent layers to further improve our CI-
BCNN. We compared our CI-BCNN with state-of-the-art
network quantization methods in ResNet18 and ResNet34
architectures and reported top-1 and top-5 accuracies in Ta-
ble 3, where CI-BCNN (add) means our binary neural net-
work with addtional shortcut applied in [24]. Bi-Real-Net
achieves the outstanding performances among neural net-
works with binary weights and activations by adding more
shortcuts and training with more accurate gradients. How-
ever, Bi-Real-Net fails to consider the quantization error
caused by xnor and bitcount operations, which leads to
inconsistent signs in binary feature maps with signiﬁcant
information loss. CI-BCNN preserves the information of
input samples during inference through the interacted bit-
count and the mined channel-wise interactions. Experi-
ments on the ImageNet dataset shows the superiority of
interacted bitcount directed by channel-wise interactions.
Moreover, CI-BCNN obtains higher accuracies compared
with HWGQ and BWN, which employs two-bit and ﬂoat
activations. In short, CI-BCNN is more competitive than
the state-of-the-art neural networks with binary weights and
activations.

We analyzed the computational and storage complexity
in comparison of Bi-Real Net, Xnor-Net and full-precision
networks to show the saving of memory and speedup during
inferences. The memory usage is represented by the storage
for parameters of networks, which is calculated as summa-
tion of 32 bits time real-valued parameters and 1 bit times
binary parameters. We use FLOPs to measure the computa-
tional complexity, following the calculation method in [11].
Because current generation of CPUs can implement 64 bi-
nary operations parallel in one block, the total FLOPs is
calculated as the number of ﬂoating point multiplications
plus 1
64 of the amount of binary multiplications. The results
are illustrated in Table 4 with our implementation settings.
The proposed CI-BCNN saves the storage cost by
11.17× and 16.03× in ResNet18 and ResNet34 respec-
tively, and speeds up the computation by 11.75× and
20.11× in the above architectures when compared with the
full-precision networks. In CI-BCNN, the storage overhead
is negligible because the additional parameters are only the
binary existence matrix and the discrete inﬂuence matrix
stored in low bits. Meanwhile, the extra computation cost
is resulted from interacted bitcount, which is insigniﬁcant
compared with standard binary convolutions. On the con-
trary, our CI-BCNN saves computation and storage cost
because scaling factors for weights and activations are re-
moved compared with Xnor-Net, and the real-valued accu-
mulation and batch normalization in extra shortcuts are not
used in comparison with Bi-Real-Net. Generally speaking,
CI-BCNN requires less memory usage and fewer FLOPs.

5. Conclusion

In this paper, we have proposed a binary convolutional
neural network method called CI-BCNN for efﬁcient infer-
ence. The proposed CI-BCNN mines the graph structure
among channels via policy gradient and imposes channel-
wise interactions by interacted bitcount, through which in-
consistent signs in binary feature maps are corrected and
information of input images is preserved during inference.
Extensive experimental results demonstrate effectiveness of
the proposed approach.

Acknowledgement

This work was supported in part in part by the Na-
tional Key Research and Development Program of China
under Grant 2016YFB1001001, in part by the National
Natural Science Foundation of China under 61822603,
Grant U1813218, Grant U1713214, Grant 61672306,
Grant 61572271, and in part by the Shenzhen Funda-
mental Research Fund (Subject Arrangement) under Grant
JCYJ20170412170602564.

4328575

References

[1] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
networks for object tracking.
In ECCV, pages 850–865,
2016.

[2] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconce-
los. Deep learning with low precision by half-wave gaussian
quantization. In CVPR, pages 5406–5414, 2017.

[3] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural networks with
binary weights during propagations. In NIPS, pages 3123–
3131, 2015.

[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248–255, 2009.

[5] Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Fre-
itas, et al. Predicting parameters in deep learning. In NIPS,
pages 2148–2156, 2013.

[6] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann Le-
Cun, and Rob Fergus. Exploiting linear structure within con-
volutional networks for efﬁcient evaluation. In NIPS, pages
1269–1277, 2014.

[7] Changxing Ding and Dacheng Tao. Trunk-branch ensemble
convolutional neural networks for video-based face recogni-
tion. TPAMI, 40(4):1002–1014, 2018.

[8] Julian Faraone, Nicholas Fraser, Michaela Blott, and
Philip HW Leong. Syq: Learning symmetric quantization
for efﬁcient deep neural networks. In CVPR, pages 4300–
4309, 2018.

[9] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.

[10] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask r-cnn. In ICCV, pages 2980–2988, 2017.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[12] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
In ICCV, vol-

for accelerating very deep neural networks.
ume 2, 2017.

[13] Seunghoon Hong, Tackgeun You, Suha Kwak, and Bohyung
Han. Online tracking by learning discriminative saliency
map with convolutional neural network.
In ICML, pages
597–606, 2015.

[14] Lu Hou, Quanming Yao, and James T Kwok.

Loss-
arXiv preprint

aware binarization of deep networks.
arXiv:1611.01600, 2016.

[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[16] Chen Huang, Simon Lucey, and Deva Ramanan. Learning
policies for adaptive tracking with deep feature cascades. In
ICCV, pages 105–114, 2017.

[17] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-
Yaniv, and Yoshua Bengio. Binarized neural networks. In
NIPS, pages 4107–4115, 2016.

[18] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally,
and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer pa-
rameters and¡ 0.5 mb model size.
arXiv preprint
arXiv:1602.07360, 2016.

[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple

layers of features from tiny images. 2009.

[20] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[21] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv
preprint arXiv:1608.08710, 2016.

[22] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
nam2016learningnetworks for object detection.
In CVPR,
pages 936–944, 2017.

[23] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate
binary convolutional neural network. In NIPS, pages 344–
352, 2017.

[24] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu,
and Kwang-Ting Cheng. Bi-real net: Enhancing the per-
formance of 1-bit cnns with improved representational ca-
pability and advanced training algorithm. arXiv preprint
arXiv:1808.00278, 2018.

[25] Christos Louizos, Karen Ullrich, and Max Welling. Bayesian
compression for deep learning. In NIPS, pages 3290–3300,
2017.

[26] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn architec-
ture design. arXiv preprint arXiv:1807.11164, 2018.

[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, An-
drei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
et al. Human-level control through deep reinforcement learn-
ing. Nature, 518(7540):529, 2015.

[28] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
pages 4293–4302, 2016.

[29] Aaron Nech and Ira Kemelmacher-Shlizerman. Level play-
ing ﬁeld for million scale face recognition. In CVPR, pages
3406–3415, 2017.

[30] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforce-
ment learning of region proposal networks for object detec-
tion. In CVPR, pages 6945–6954, 2018.

[31] Yongming Rao, Dahua Lin, Jiwen Lu, and Jie Zhou. Learn-
ing globally optimized object detector via policy gradient. In
CVPR, pages 6190–6198, 2018.

[32] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In ECCV, pages 525–
542, 2016.

[33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In CVPR, pages 779–788, 2016.

[34] David Silver, Aja Huang, Chris J Maddison, Arthur Guez,
Laurent Sifre, George Van Den Driessche, Julian Schrit-
twieser, Ioannis Antonoglou, Veda Panneershelvam, Marc
Lanctot, et al. Mastering the game of go with deep neural

4329576

networks and tree search. nature, 529(7587):484, 2016.

[35] James Steven Supancic III and Deva Ramanan. Tracking as
online decision-making: Learning a policy from streaming
videos with reinforcement learning.
In ICCV, pages 322–
331, 2017.

[36] Karen Ullrich, Edward Meeds, and Max Welling.

weight-sharing for neural network compression.
preprint arXiv:1702.04008, 2017.

Soft
arXiv

[37] Ronald J Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.

[38] Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen,
Fang Wen, Hongdong Li, and Gang Hua. Neural aggregation
network for video face recognition. In CVPR, pages 4362–
4371, 2017.

[39] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao.
On compressing deep models by low rank and sparse decom-
position. In CVPR, pages 7370–7379, 2017.

[40] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun,
and Jin Young Choi. Action-decision networks for visual
tracking with deep reinforcement learning. In CVPR, pages
1349–1358, 2017.

[41] Da Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang.
Deep reinforcement learning for visual object tracking in
videos. arXiv preprint arXiv:1701.08936, 2017.

[42] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and
Gang Hua. Lq-nets: Learned quantization for highly ac-
curate and compact deep neural networks. arXiv preprint
arXiv:1807.10029, 2018.

[43] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
Accelerating very deep convolutional networks for classiﬁ-
cation and detection. TPAMI, 38(10):1943–1955, 2016.

[44] Chenzhuo Zhu, Song Han, Huizi Mao, and William J
arXiv preprint

Trained ternary quantization.

Dally.
arXiv:1612.01064, 2016.

4330577

