Attribute-aware Face Aging with Wavelet-based Generative Adversarial

Networks

Yunfan Liu1 ∗

Qi Li1

2

,

3 ∗

,

Zhenan Sun1

2

4

,

,

1 Center for Research on Intelligent Perception and Computing, CASIA

2 National Laboratory of Pattern Recognition, CASIA

3 Artiﬁcial Intelligence Research, CAS, Jiaozhou, Qingdao, China

4 Center for Excellence in Brain Science and Intelligence Technology, CAS

yunfan.liu@cripac.ia.ac.cn, {qli, znsun}@nlpr.ia.ac.cn

Abstract

 Test Face       Output

 Test Face       Output

 Test Face       Output

Since it is difﬁcult to collect face images of the same sub-
ject over a long range of age span, most existing face aging
methods resort to unpaired datasets to learn age mappings.
However, the matching ambiguity between young and aged
face images inherent to unpaired training data may lead to
unnatural changes of facial attributes during the aging pro-
cess, which could not be solved by only enforcing identity
consistency like most existing studies do. In this paper, we
propose an attribute-aware face aging model with wavelet-
based Generative Adversarial Networks (GANs) to address
the above issues. To be speciﬁc, we embed facial attribute
vectors into both the generator and discriminator of the
model to encourage each synthesized elderly face image to
be faithful to the attribute of its corresponding input. In ad-
dition, a wavelet packet transform (WPT) module is incor-
porated to improve the visual ﬁdelity of generated images
by capturing age-related texture details at multiple scales
in the frequency space. Qualitative results demonstrate the
ability of our model in synthesizing visually plausible face
images, and extensive quantitative evaluation results show
that the proposed method achieves state-of-the-art perfor-
mance on existing datasets.

1. Introduction

Face aging, also known as age progression [16], aims at
rendering a given face image with aging effects while still
preserving personalized features. Applications of face ag-
ing techniques range from social security to digital enter-
tainment, such as predicting contemporary appearance of
missing children and cross-age identity veriﬁcation. Due
to the practical value of face aging, many approaches have

∗Authors contributed equally.

     Black            Asian
     Black            Asian

    White            Black
    White            Black

     Asian           White
     Asian           White

      Male          Female
      Male          Female

    Female          Male
    Female          Male

    Female          Male
    Female          Male

Race

Gender

Glasses

Bald

Figure 1. Examples of face aging with mismatched facial attributes
generated by face aging model without facial attribute embedding.
Four attributes (Race, Gender, Glasses, and Bald) are considered
and three sample results are presented for each. Labels of ‘Race’
and ‘Gender’ are all obtained via advanced publicly available APIs
of Face++ [13] and placed underneath each image.

been proposed to address this problem in the last two
decades [8, 20, 19, 21, 7]. With the rapid development of
deep learning, deep generative models are widely adopted
to synthesize aged face images [23, 3, 4]. However, the
most critical problem of these methods is that multiple face
images of the same person at different ages are required at
training stage, which is extremely expensive to collect in
practice and thus their applications are largely limited.

To deal with this problem, many recent studies resort to
unpaired face aging data to train the model [23, 28, 25, 9].
However, these approaches mainly focus on face aging it-
self while neglecting other critical conditional information
of the input (e.g., facial attributes), thus fail to regulate
the training process. Consequently, training face image
pairs with mismatched attributes would mislead the model

11877

into learning translations other than aging, causing serious
ghosting artifacts and even incorrect facial attributes in gen-
eration results. Fig. 1 shows some face aging results with
mismatched attributes.
In the rightmost face aging result
under ‘gender’, beard is mistakenly attached to the input
female face image. This is because the model learns that
growing a beard is a typical sign of aging but has no way
to know that this does not happen to a woman, as face im-
age pairs of young woman and old man could be treated
as positive training samples if no conditional information is
considered.

To suppress such undesired changes of semantic infor-
mation during the aging process, many recent face aging
studies attempt to supervise the output by enforcing iden-
tity consistency [28, 1, 25, 9]. However, as shown in Fig. 1,
personalized features are well preserved in the output for
all sample results, nevertheless, obvious unnatural changes
of facial attributes are still observed. In other words, well
maintained identity-related features do NOT imply reason-
able aging results when training with unpaired data. There-
fore, merely enforcing identity consistency is insufﬁcient to
eliminate matching ambiguities in unpaired training data,
thus fails to achieve satisfactory face aging performance.

To solve the above-mentioned issues, in this paper, we
propose a framework based on generative adversarial net-
works (GANs). Different from existing methods in the lit-
erature, we involve semantic conditional information of the
input by embedding facial attribute vectors in both the gen-
erator and discriminator, so that the model could be guided
to output elderly face images with attributes faithful to each
corresponding input. Furthermore, to enhance aging details,
based on the observation that signs of aging are mainly rep-
resented by wrinkles, laugh lines, and eye bags, which could
be treated as local textures, we employ wavelet packet trans-
form to extract features at multiple scales in the frequency
space efﬁciently.

To summarized, the main contributions are as follows:

• Facial attributes are incorporated as conditional infor-
mation into both the generator and discriminator for
face aging, since identity preservation is insufﬁcient
for generating reasonable results.

• Wavelet packet transform is adopted to extract features
of texture details at multiple scales in the frequency
domain for generating ﬁne-grained details of aging ef-
fects.

2. Related Work

In the last few decades, face aging has been a very
popular research topic and a great amount of algorithms
have been proposed to tackle this issue. In general, these
methods could be divided into three categories: physical
model-based methods, prototype-based methods, and deep
learning-based methods.

Physical model-based methods mechanically simulate
the changes of facial appearance w.r.t. time by modeling
the anatomical structure of human faces. Todd et al. [22]
modeled the translation of facial appearance by revised car-
dioidal strain transformation. Subsequent works investi-
gated the problem from various biological aspects includ-
ing muscles and overall facial structures [8, 20]. However,
physical model-based algorithms are computational expen-
sive and large amount of image sequences of the same sub-
ject are required to model aging effects.

Data-driven prototyping approaches [19, 21, 7] come
into view the next, where faces are divided into age groups
and each group is represented by an average face (proto-
type) computed from the training data. After that, transla-
tion patterns between prototypes are regarded as effects of
aging. The main problem of prototyping methods is that
personalized features are eliminated when calculating av-
erage faces, thus the identity information is not well pre-
served.

In recent years, deep generative models with temporal
architectures are adopted to synthesize images of elderly
faces [23, 3, 4]. However, in most of these works, face
image sequence over a long age span for each subject is
required thus their potential in practical use is limited. With
the success of GANs [5] in generating visually appealing
images, many efforts have been made to tackle the prob-
lem of face aging using GAN-based framework [28, 25, 9,
17, 24, 10]. Zhang et al. [28] proposed a conditional adver-
sarial autoencoder (CAAE) to achieve age progression and
regression by traversing in low-dimensional manifold. The
work most similar to ours is [25], in which a GAN-based
model with pyramid architecture is proposed, and identity
loss is adopted to achieve permanence. Besides preserving
identity information, we focus on alleviating the inﬂuence
of matching ambiguity of unpaired training samples and en-
suring attribute consistency by embedding facial attribute
vectors in the model.

3. Approach

• Extensive experiments have been conducted to demon-
strate the ability of the proposed method in rendering
accurate aging effects and preserving information of
both identity and facial attributes. Quantitative results
indicate that our method achieves state-of-the-art per-
formance.

In a unpaired face aging dataset, each young face im-
age might map to many elderly face candidates during the
training process, and image pairs with mismatched seman-
tic information may mislead the model into learning trans-
lations other than aging. To solve this problem, we present a
GAN-based face aging model that takes young face images

11878

Pixel Loss 
Identity Loss

residual connection

p
pp

Encoder

......

Decoder

Input face

Generator G

Fake elderly face
ace

Facial Attribute 

Vector

p

ace
Real young face

Gender: Female
Race: White
...

Real elderly face with
Real elderly face with

the same attribute
the same attribute

Wavelet Packet 

Transform

Discriminator D

p

p

p
pp

GAN 
Loss

Level 0 
LLLevel 0l 0l 0
Level 0 

Level 1 
LLLevel 1l 1l 1111
Level 1 
LLLLLL

Level 2
evelll 2l 2l 2
lll
Level 2
Level 2
Level 222
LeveL
…
…
Wavelet 
Wavelet 
Coefficients
Coefficients

Figure 2. An overview of the proposed face aging framework. An hourglass-shaped generator G learns the age mapping and outputs
lifelike elderly face images. A discriminator D is employed to distinguish synthesized face images from generic ones, based on multi-scale
wavelet coefﬁcients computed by the wavelet packet transform module. The p-dimensional attribute vector describing the input face image
is embedded to both the generator and discriminator to reduce matching ambiguity inherent to unpaired training data.

and their semantic information (i.e. facial attributes) as in-
put and outputs visually plausible aged faces accordingly.
The network consists of two parts: a facial attribute em-
bedded generator G and a wavelet-based discriminator D.
The generator network embeds facial attributes into young
face images and synthesizes aged faces. The discriminator
network is used to encourage the generation results to be in-
distinguishable from generic ones and to possess attributes
same as the corresponding input. An overview of the pro-
posed framework is presented in Fig. 2.

3.1. Facial Attribute Embedded Generator

Existing face aging studies [9, 25, 28] only take young
face images as inputs and then directly learn age map-
pings using GAN-based networks. Although constraints on
identity information and pixel values are usually imposed
to restrict modiﬁcations made to input images, facial at-
tributes may still undergo unnatural translations (as shown
in Fig. 1). Unlike previous works, we propose to incor-
porate both low-level image information (pixel values) and
high-level semantic information (facial attributes) into the
face aging model to regularize image translation patterns
and reduce the ambiguity of mappings between unpaired
young and aged faces. To be speciﬁc, the model takes
young face images and their corresponding attribute vectors
as input, and generates elderly face images with attributes
in agreement with the input ones.

Rather than supervising the attributes of generation re-
sults by simply adopting an additional loss term, we embed
the attribute vector in the generator so that semantic facial
information is well considered in the generation process and
encourages the model to produce face images with consis-
tent attributes more effectively. To be speciﬁc, we employ

an hourglass-shaped fully convolutional network as the gen-
erator, which has achieved success in previous image trans-
lation studies [6, 29]. It consists of an encoder network, a
decoder network, and four residual blocks in between as the
bottleneck. The input facial attribute vector is replicated and
concatenated to the output blob of the last residual block as
they both contain high-level semantic features. After the
combination, the decoder network transforms the concate-
nated feature blob back to the image space.

Since face aging could be considered as rendering aging
effects conditioned on the input young face image, we add
the input image to the output of the decoder to form a resid-
ual connection. Compared to synthesize the whole face im-
age, this structure automatically makes the generator focus
more on modeling the difference between input and output
face images, namely the representative signs of aging, and
be less likely to be distracted by visual content irrelevant
to aging, such as background. Finally, the numeric scale of
the resultant tensor is normalized by a hyperbolic tangent
(tanh) mapping and thus the generated elderly face image is
obtained.

3.2. Wavelet-based Discriminator

To force the generator to absorb the semantic informa-
tion of the input face image, a conditional discriminator
is employed. The discriminator has two main functions:
1) distinguish synthesized face images from generic ones;
2) check whether the attribute of each generation result is
faithful to that of the corresponding input.

To be speciﬁc, considering the fact that typical signs of
aging, such as wrinkles, laugh lines, and eye bags, could be
regarded as local image textures, we adopt wavelet packet
transform (WPT, see Fig. 3) to capture age-related textu-

11879

I(k)

hlow

2 ↓ 
Columns

hhigh

2 ↓ 
Columns

(a)

hlow

hhigh

hlow

hhigh

2 ↓ 

2 ↓ 
Rows
2 ↓ 

2 ↓ 
Rows

I(k+1)

IV(k+1)

IH(k+1)

ID(k+1)

Level 0

Level 1

(b)

Level 2

Figure 3. Demonstration of wavelet packet transform. (a) Low-
pass and high-pass decomposition ﬁlters (hlow and hhigh) are ap-
plied iteratively to the input on k-th level to compute wavelet coef-
ﬁcients on the next level; (b) a sample face image with its wavelet
coefﬁcients at different decomposing levels.

ral features. Speciﬁcally, multi-level WPT is performed to
provide a more comprehensive analysis of textures in the
given image, and wavelet coefﬁcients at each decomposing
level are fed into a convolutional pathway of the discrimi-
nator. Note that this is different from [9], since wavelet co-
efﬁcients are only used for discrimination in our work and
no prediction or reconstruction is involved.

To make the discriminator gain the ability of telling
whether attributes are preserved in generated images, the
input attribute vector is also replicated and concatenated to
the output of an intermediate convolutional block of each
pathway. At the end of the discriminator, same-sized out-
puts of all pathways are fused to form a single tensor, and
adversarial loss is then estimated against the label tensor.

Compared to extracting multi-scale features by a se-
quence of convolutional layers as in [25], the advantage of
using WPT is that the computational cost is signiﬁcantly
reduced since calculating wavelet coefﬁcients could be re-
garded as forwarding through a single convolutional layer.
Therefore, WPT greatly reduces the number of convolutions
performed in each forwarding process. Although this part
of the model has been simpliﬁed, it still takes the advantage
of multi-scale image texture analysis, which is helpful in
improving the visual ﬁdelity of generated images.

3.3. Overall Objective Functions

Training of GAN model simulates the process of opti-
mizing a minimax-max two-player game between the gen-
erator G and the discriminator D. Unlike regular GANs [5],
we adopt least square loss instead of negative log likeli-
hood loss for that margins between generated samples and
the decision boundary in the feature space are also mini-
mized, which further improves the quality of synthesized
images [12]. Practically, we pair up young face images
xi and their corresponding attribute vectors αi of dimen-

sion p, denoted as (xi, αi) ∼ Pyoung(x, α), and take them
as input to the model. Only generic aged faces with at-
tributes same as the input, i.e. (xi, αi) ∼ Pold(x, αi), are
considered as positive samples, and real young faces, i.e.
(xi, αi) ∼ Pyoung(x, α), are regarded as negative samples
to help D gain discriminating ability on aging effects.

Mathematically,

the objective function for G and D

could be written as follows,

LGAN (G) = E(xi,αi)∼Pyoung(x,α)[(D(G(xi, αi), αi)−1)2]
(1)

LGAN (D) = E(xi,αi)∼Pold(x,αi)[(D(xi, αi) − 1)2]+
E(xi,αi)∼Pyoung(x,α)D(G(xi, αi), αi)2+
E(xi,αi)∼Pyoung(x,α)D(xi, αi)2

(2)

where Pyoung and Pold denote the distribution of generic
face images of young and old subjects, respectively.

In addition, pixel loss and identity loss are adopted to
maintain consistency in both image-level and personalized
feature-level. To be speciﬁc, we utilize the VGG-Face de-
scriptor [14], denoted by φ, to extract the identity related
semantic representation of a face image. These two loss
terms could be formulated as,

Lpix = E(xi,αi)∼Pyoung(x,α)||G(xi, αi) − xi||2

F

(3)

Lid = E(xi,αi)∼Pyoung(x,α)||φ(G(xi, αi)) − φ(xi)||2

F (4)

In conclusion, overall objective functions of the pro-

posed model could be written as follows,

LG = LGAN (G) + λpixLpix + λidLid

LD = LGAN (D)

(5)

(6)

where λid and λpix are coefﬁcients balancing the impor-
tance of critics on identity and pixels, respectively. We op-
timize the model by minimizing LG and LD alternatively
until the optimality is reached.

4. Experiments

4.1. Dataset

MORPH [15] is a large aging dataset containing 55,000
face images of more than 13,000 subjects. Data samples in
MORPH are color images of near-frontal faces exhibiting
neutral expressions under uniform and moderate illumina-
tion with simple background. CACD [2] contains 163,446
face images of 2,000 celebrities captured in much less con-
trolled conditions. Besides large variations in pose, illumi-
nation, and expression (PIE variations), images in CACD
are collected via Google Image Search, making it a very
challenging dataset due to the mismatching between actual
face presented in each image and associated labels provided
(name and age).

11880

Test Face      31 – 40       41 – 50         51+

Test Face      31 – 40       41 – 50         51+

Test Face      31 – 40       41 – 50         51+

22 Years Old

27 Years Old

26 Years Old

27 Years Old

29 Years Old

21 Years Old

Figure 4. Sample results on Morph (ﬁrst row) and CACD (second row). The ﬁrst image in each result is the input test face image and
subsequent 3 images are synthesized elderly face images of the same subject in age group 31-40, 41-50 and 51+, respectively.

Test Face

Results of 
Prior Work

Results of 
Our Work

(translation to 51+)

18

48

22

42

21

28

27

29

28

26

[51-60]

[51-60]

51+

51+

50+

50+

Figure 5. Performance comparison with prior work on Morph (zoom in for a better view of the aging details). The second row shows the
results of prior work, where four methods are considered and two sample results are presented for each. These four methods are (from left
to right): CONGRE [18], HFA [26], GLCA-GAN [9], and PAG-GAN [25]. The last row shows the results of our method.

As for facial attributes, MORPH provides researchers
with labels including age, gender, and race for each im-
age. We choose ‘gender’ and ‘race’ to be the attributes
that are required to be preserved, since these two attributes
are guaranteed to remain unchanged during natural aging
process, and are relatively objective compared to attributes
such as ‘attractive’ or ‘chubby’ used in popular facial at-
tribute dataset CelebA [11]. For CACD, since face images
with race other than ‘white’ only takes a small portion of
the entire dataset, we only select ‘gender’ as the attribute
to preserve. To be speciﬁc, we go through the name list of
the celebrities and label the corresponding images accord-
ingly. This introduces noise in gender labels due to the mis-
matching between the annotated name and the actual face
presented in each image, which further increases the difﬁ-
culty for our method to achieve good performance on this
dataset. It is worthwhile to note that the proposed model
is highly expandable, as researchers may choose whatever
attributes to preserve simply by incorporating them in the
conditional facial attribute vector and arrange training im-
ages pairs accordingly.

4.2. Implementation Details

All face images are cropped and aligned according to the
ﬁve facial landmarks detected by MTCNN [27]. Following
the convention in [25, 9], we divide the face images into
four age groups, i.e., 30-, 31-40, 41-50, 51+, and only con-

sider translations from 30- to the other three age groups.
To evaluate the performance of the proposed method ob-
jectively, all metric measurements are conducted via stable
public APIs of Face++ [13]. Thresholds adopted in our
face veriﬁcation experiments (threshold=76.5, FAR=1e-5)
are the same as those used in [25]. Therefore, quantitative
results of our experiments are comparable to those reported
in [25].

We choose Adam to be the optimizer of both G and D
with learning rate and batch-size set to 1e−4 and 16, respec-
tively. Pixel-level critic is applied every 5 iterations, and D
is updated at every iteration. As for trade-off parameters,
λpix and λid are ﬁrstly set to make Lpix and Lid to be of the
same order of magnitude as LGAN (G), and then divided by
10 to emphasize the importance of the adversarial loss. All
experiments are conducted under 5-fold cross validation on
a Nvidia Titan Xp GPU.

4.3. Qualitative Results of Face Aging

Sample results on Morph and CACD are shown in Fig. 4.
It is clear that our method is able to simulate translations be-
tween age groups and synthesize elderly face images with
high visual ﬁdelity. In addition, our method is robust to vari-
ations in terms of race, gender, expression, and occlusion.

Performance comparison with prior work on Morph is
shown in Fig. 5. Traditional face aging methods, CON-
GRE [18] and HFA [26], only render subtle aging effects

11881

Table 1. Age estimation results on Morph and CACD (differences of mean ages are measured in absolute value).

Morph

CACD

Age group

31 - 40

41 - 50

51 +

Age group

31 - 40

41 - 50

51 +

Estimated Age Distributions

Estimated Age Distributions

Generic
Synthetic

38.60
38.47

47.74
47.55

57.25
56.57

Difference of mean ages

CAAE
GLCA-GAN
PAG-GAN
Ours

10.08
0.23
0.38
0.13

15.49
3.61
0.52
0.19

21.42
8.61
1.48
0.68

(a)

(c)

(b)

(d)

Figure 6. Distributions of the estimated ages. (a) synthetic faces on
Morph; (b) synthetic faces on CACD; (c) generic faces on Morph;
(d) generic faces on CACD.

within tight facial area, which fails to accurately simu-
late the aging process.
In contrast, GAN-based methods,
GLCA-GAN [9] and GAN with pyramid architecture pro-
posed in [25], referred to as PAG-GAN, have achieved
signiﬁcant improvement on the quality of generation re-
sults. However, our method further generates face images
of higher resolution (2×) with enhanced details compared
to GLCA-GAN, and reduces ghosting artifacts in the results
compared to PAG-GAN (e.g. ﬁner details of hair and beard).

4.4. Aging Accuracy and Identity Preservation

In this subsection, we report evaluation results on ag-
ing accuracy and identity preservation. The performance
of the proposed model is compared with previously state-
of-the-art methods CAAE [28], GLCA-GAN [9] and PAG-
GAN [25] to demonstrate the effectiveness.

Aging Accuracy: Age distributions of both generic and
synthetic faces in each age group are estimated, where less
discrepancy between real and fake images indicates more
accurate simulation of aging effects. On Morph and CACD,

Generic
Synthetic

38.51
38.88

46.54
47.42
Difference of mean ages

CAAE
GLCA-GAN
PAG-GAN
Ours

5.76
1.72
0.70
0.37

11.53
2.07
0.22
0.58

53.39
54.05

17.93
2.85
0.57
0.66

face images of age under or equal to 30 are considered as
testing samples, and their corresponding aged faces in the
other three age groups are synthesized. We estimated the
apparent age of both generation results and natural face im-
ages in the dataset using Face ++ APIs for fair comparison.
Age estimation results on Morph and CACD are shown
in Table 1 and Fig. 6. We compare our method with pre-
vious works in terms of differences between mean ages.
On Morph, it could be seen that estimated age distribu-
tions of synthetic elderly face images well match that of
natural images for all age groups. Our method consistently
outperforms other approached in all three aging processes,
demonstrating the effectiveness of our method. Signs of ag-
ing in results of CAAE are not obvious enough, leading to
large age estimation errors. On CACD, due to the existence
of mismatching between face images and associated labels,
slight performance drop could be observed. Still, the pro-
posed method achieves results comparable to previous state-
of-the-art. This shows that our method is relatively robust
to noise in attribute labels and thus lower the requirement
on the accuracy of the prior attribute detection process.

Identity Preservation: Face veriﬁcation experiments
are conducted to check whether the identity information has
been preserved during the face aging process. Similar to
previous literature, comparisons between synthetic elderly
face images from different age groups of the same subject
are also conducted to inspect if the identity information is
consistent among three separately trained age mappings.

Results of face veriﬁcation experiments are shown in Ta-
ble 2. On Morph, our method achieves the highest veriﬁ-
cation rate on all three translations and outperforms other
approaches by a clear margin, especially in the hardest case
(from 30- to 51+). This demonstrates that the proposed
method successfully achieves identity permanence during
face aging. On the more challenging dataset CACD contain-
ing mismatched labels, the performance of our method is
comparable to PAG-GAN with minor difference. Notably,
as the time interval between two face images of a single
subject increases, both veriﬁcation conﬁdence and accuracy

11882

Table 2. Face veriﬁcation results on Morph and CACD.

Morph

CACD

Age group

31 - 40

41 - 50

51 +

Age group

31 - 40

41 - 50

51 +

Veriﬁcation Conﬁdence

Veriﬁcation Conﬁdence

30 -
31 - 40
41 - 50

95.77
-
-

94.64
95.47
-
Veriﬁcation Rate (%)

CAAE
GLCA-GAN
PAG-GAN
Ours

15.07
97.66
100.00
100.00

12.02
96.67
98.91
100.00

87.53
89.53
90.50

8.22
91.85
93.09
98.26

30 -
31 - 40
41 - 50

93.67
-
-

91.54
91.74
-

Veriﬁcation Rate (%)

CAAE
GLCA-GAN
PAG-GAN
Ours

4.66
97.72
99.99
99.76

3.41
94.18
99.81
98.74

90.32
90.54
91.12

2.40
92.29
98.28
98.44

Table 3. Facial attributes preservation rates for ‘Gender’ and ‘Race’ on Morph and CACD.

Preservation Rate (%) of ‘Gender’

Preservation Rate (%) of ‘Race’

Morph

CACD

Morph

Age group

31 - 40

41 - 50

51 +

31 - 40

41 - 50

51 +

31 - 40

41 - 50

51 +

GLCA-GAN
PAG-GAN
Ours

96.30
95.96
97.37

95.43
93.77
97.21

95.77
92.47
96.07

87.27
83.97
90.71

86.79
81.28
87.63

85.89
70.05
87.19

91.79
95.83
95.86

89.52
88.51
94.10

89.34
87.98
93.22

decrease, which is reasonable as greater changes in facial
appearance may occur as more time elapsed.

4.5. Facial Attribute Consistency

We evaluate the performance of facial attribute preser-
vation by comparing facial attributes estimated before and
after age progression, and results are listed in Table 3. On
Morph, facial attributes of the majority of testing samples
(up to 97.37% for ‘gender’ and 95.86% for ‘race’) are well
preserved in the aging process. In addition, our method out-
performs both GLCA-GAN and PAG-GAN by clear mar-
gins on translations to all age groups. On CACD, due to
the inﬂuence of mistakenly labeled data samples, clear per-
formance drop could be observed compared to the results
on Morph. However, our method still gives better perfor-
mance on facial attributes preservation than other methods.
The advantage of our method in preserving the ‘gender’ at-
tribute becomes greater as the age gap increases, and ﬁnally
reaches 17.14% (87.19% over 70.05%) when translating to
the oldest age group 51+. From Table 3, we could conclude
that undesired changes of facial attributes are more likely to
happen as the age gap increases, and incorporating condi-
tional information is beneﬁcial for maintaining consistency
of target facial attributes in the aging process.

4.6. Ablation Study

In this part, experiments are conducted to fully ex-
plore the contribution of facial attribute embedding (FAE)

      Test Face            woFAE_woWPT       woFAE_wWPT        wFAE_woWPT            Proposed 

            19                              52                              50                            43                             53
    Black Female            Black Male                Black Male             Black Female           Black Female
                                           92.78                          91.08                       92.87                        89.94

(a)

            30                              65                             65                             52                             64
    White Female           White Male               White Male             White Female           White Female
                                           90.03                         76.85                       87.16                         81.66

(b)

Figure 7. Sample visual results of the ablation study. For each face,
the estimated age (ﬁrst row) and detected attributes (second row)
are listed underneath. Values in the last row are face veriﬁcation
conﬁdence between generation results and the test face.

and wavelet packet transform (WPT) in simulating accu-
rate age translations. We investigate the impact of includ-
ing/excluding attribute embedding (w/wo FAE) and wavelet
packet transform (w/wo WPT) on age distribution, face ver-
iﬁcation rate, and attribute preservation rate. All experi-
ments in this subsection are conducted only on Morph as
labels are noisy on CACD dataset.

Visual illustrations of face images generated by vari-
It is
ants of the proposed model are shown in Fig. 7.
clear that when both FAE and WPT are not
involved
(woFAE woWPT), generation results suffer from severe

11883

Table 4. Comparison of results on facial attribute preservation and aging accuracy between variants of the proposed model (differences of
mean ages are measured in absolute value).

Gender Preservation Rate (%)

Race Preservation Rate (%)

Deviation of Estimated Ages

Age group

31-40

41-50

51+

31-40

41-50

51+

31-40

41-50

woFAE / woWPT
woFAE / wWPT
wFAE / woWPT
Ours

95.72
96.15
97.21
97.37

94.21
94.90
96.91
97.21

93.60
93.61
95.85
96.07

95.04
93.89
95.22
95.86

93.55
88.63
94.35
94.10

90.83
90.21
91.43
93.22

0.44
0.68
0.82
0.13

1.72
0.41
0.52
0.19

51+

3.03
2.31
4.82
0.68

Table 5. Face veriﬁcation rates (%) of variants of the proposed
model on Morph

duces the intrinsic ambiguity in data mapping.

Age group

31-40

41-50

51+

woFAE / woWPT 100.00
100.00
woFAE / wWPT
100.00
wFAE / woWPT
Ours
100.00

100.00
99.88
100.00
100.00

99.92
98.06
98.86
98.26

ghosting artifacts. Due to the intrinsic matching ambiguity
of unpaired training data, the model without FAE mistak-
enly attaches moustache to the input female face image to
show the aging effect. Notably, growing a moustache does
not decrease the face veriﬁcation conﬁdence, as the gener-
ated face image still shares similar identity-related features
with the input. This again conﬁrms our observation that
enforcing identity consistency is insufﬁcient to obtain satis-
factory face aging results.

On the contrary, incorporating FAE suppresses the unde-
sired facial attribute drift by reducing the matching ambigu-
ity. To be speciﬁc, in Fig. 7, there is no more moustache in
generation results after adopting FAE thus facial attribute
consistency is achieved. Unfortunately, removing mous-
tache also wipes out aging-related textural details (wrinkles,
laugh lines, and eye bags), leading to relatively inaccurate
aging results (much younger than expected).

To solve this issue and generate more visually plausible
face images with vivid signs of aging, WPT is employed
as the initial layer of the discriminator. The contribution of
WPT could be easily seen by comparing the results obtained
under setting ‘woFAE / woWPT’ and ‘woFAE / wWPT’,
as well as ‘wFAE / woWPT’ and ‘Ours’. Although results
obtained under setting ‘woFAE / wWPT’ still suffer from
wrong facial attributes, ghosting artifacts are signiﬁcantly
alleviated and lifelike aging effects are clearly observed.

Quantitative results for ablation study are shown in Ta-
ble 4 and 5. According to results in Table 4, introducing
facial attribute embedding (wFAE) increases preservation
rates for both ‘gender’ and ‘race’ under all three age map-
pings, especially in the case of translating to 51+. This
proves the effectiveness of attribute embedding as it aligns
unpaired age data in terms of facial attributes and thus re-

In addition, it is clear that adopting WPT reduces the
discrepancies between age distributions of generic and syn-
thetic images in all cases. However, WPT provides little
help in maintaining facial attribute consistency. This is be-
cause WPT only captures feature based on low-level vi-
sual data and could not bridge the semantic gap, so that the
framework still suffers from mismatched data samples.

Combining results in Table 4 and 5, it could be seen
that while attribute preservation rates still have room for
improvement, veriﬁcation rates are about to reach perfec-
tion. This observation validates our statement that identity
preservation does not guarantee that facial attributes remain
stable during the aging process. Therefore, besides con-
straints on identity, supervision on facial attributes are also
helpful to reduce the intrinsic matching ambiguity of un-
paired data and achieve satisfactory face aging results.

5. Conclusion

In this paper, we propose a GAN-based framework to
synthesize aged face images. Due to the ineffectiveness of
identity constraints in reducing the matching ambiguity of
unpaired aging data, we propose to employ facial attributes
to tackle this issue. Speciﬁcally, we embed facial attribute
vectors to both the generator and discriminator to encour-
age generated images to be faithful to facial attributes of the
corresponding input image. To further improve the visual ﬁ-
delity of generated face images, wavelet packet transform is
introduced to extract textual features at multiple scales efﬁ-
ciently. Extensive experiments are conducted on Morph and
CACD, and qualitative results demonstrate that our method
could synthesize lifelike face images robust to both PIE
variations and noisy labels. Furthermore, quantitative re-
sults obtained via public APIs validate the effectiveness of
the proposed method in aging accuracy as well as identity
and attribute preservation.

Acknowledgements. This work is supported by the
National Natural Science Foundation of China (Grant No.
61702513, U1836217, 61427811).

11884

References

[1] Grigory Antipov, Moez Baccouche, and Jean-Luc Dugelay.
Face aging with conditional generative adversarial networks.
IEEE International Conference on Image Processing (ICIP),
pages 2089–2093, 2017.

[2] Bor-Chun Chen, Chu-Song Chen, and Winston H Hsu. Face
recognition and retrieval using cross-age reference coding
with cross-age celebrity dataset. IEEE Transactions on Mul-
timedia (TMM), 17(6):804–815, 2015.

[3] Chi Nhan Duong, Khoa Luu, Kha Gia Quach, and Tien D
Bui. Longitudinal face modeling via temporal deep restricted
boltzmann machines.
In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 5772–5780, 2016.

[4] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T Hoang Ngan
Le, and Marios Savvides. Temporal non-volume preserv-
ing approach to facial age-progression and age-invariant
face recognition. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pages 3755–3763,
2017.

[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems (NIPS), pages 2672–
2680, 2014.

[6] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
In European Conference on Computer Vision (ECCV), pages
694–711, 2016.

[7] Ira Kemelmacher-Shlizerman, Supasorn Suwajanakorn, and
Steven M Seitz. Illumination-aware age progression. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3334–3341, 2014.

[8] Andreas Lanitis, Christopher J. Taylor, and Timothy F
Cootes. Toward automatic simulation of aging effects on face
images. IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 24(4):442–455, 2002.

[9] Peipei Li, Yibo Hu, Qi Li, Ran He, and Zhenan Sun. Global
and local consistent age generative adversarial networks. In
International Conference on Pattern Recognition (ICPR),
pages 1073–1078, 2018.

[10] Qi Li, Yunfan Liu, and Zhenan Sun. Age progression and
regression with spatial attention modules. In arXiv preprint
arXiv:1903.02133, 2019.

[11] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild.
In Proceedings
of the IEEE International Conference on Computer Vision
(ICCV), 2015.

[12] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision (ICCV), pages 2813–
2821, 2017.

[13] Megvii Inc.

Face++ research toolkit. http://www.

faceplusplus.com.

[14] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al.
Deep face recognition. In British Machine Vision Conference
(BMVC), pages 41.1–41.12, 2015.

[15] Karl Ricanek and Tamirat Tesafaye. Morph: A longitudinal
image database of normal adult age-progression. In the Inter-
national Conference on Automatic Face and Gesture Recog-
nition (FG), pages 341–345, 2006.

[16] Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, and
Shuicheng Yan. Personalized age progression with aging
dictionary. In Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV), pages 3970–3978, 2015.

[17] Jingkuan Song, Jingqiu Zhang, Lianli Gao, Xianglong Liu,
and Heng Tao Shen. Dual conditional gans for face aging and
rejuvenation.
In Proceedings of the Twenty-Seventh Inter-
national Joint Conference on Artiﬁcial Intelligence (IJCAI),
pages 899–905, 2018.

[18] Jinli Suo, Xilin Chen, Shiguang Shan, Wen Gao, and Qiong-
hai Dai. A concatenational graph evolution aging model.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI), 34(11):2083–2096, 2012.

[19] Jinli Suo, Song-Chun Zhu, Shiguang Shan, and Xilin Chen.
A compositional and dynamic model for face aging. IEEE
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 32(3):385–401, 2010.

[20] Yusuke Tazoe, Hiroaki Gohara, Akinobu Maejima, and Shi-
geo Morishima. Facial aging simulator considering geome-
try and patch-tiled texture. In ACM SIGGRAPH, 2012.

[21] Bernard Tiddeman, Michael Burt, and David Perrett. Pro-
textures for perception
IEEE Computer graphics and applications,

totyping and transforming facial
research.
21(5):42–50, 2001.

[22] James T Todd, Leonard S Mark, Robert E Shaw, and John B
Pittenger. The perception of human growth. Scientiﬁc Amer-
ican, 242(2):132–145, 1980.

[23] Wei Wang, Zhen Cui, Yan Yan, Jiashi Feng, Shuicheng Yan,
Xiangbo Shu, and Nicu Sebe. Recurrent face aging. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2378–2386, 2016.

[24] Z. Wang, W. Luo X. Tang, and S. Gao. Face aging with
identity-preserved conditional generative adversarial net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018.

[25] Hongyu Yang, Di Huang, Yunhong Wang, and Anil K. Jain.
Learning face age progression: A pyramid architecture of
gans. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 31–39, 2018.
[26] Hongyu Yang, Di Huang, Yunhong Wang, Heng Wang, and
Yuanyan Tang. Face aging effect simulation using hidden
factor analysis joint sparse representation.
IEEE Transac-
tions on Image Processing (TIP), 25(6):2493–2507, 2016.

[27] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks.
IEEE Signal Processing Letters,
23(10):1499–1503, 2016.

[28] Zhifei Zhang, Yang Song, and Hairong Qi. Age progres-
sion/regression by conditional adversarial autoencoder.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 4352–4360, 2017.

11885

[29] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pages
2242–2251, 2017.

11886

