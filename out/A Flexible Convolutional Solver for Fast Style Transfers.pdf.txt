A Flexible Convolutional Solver for Fast Style Transfers

Gilles Puy
Technicolor

Patrick P´erez

Valeo.ai

975 Avenue des Champs Blancs

15 Rue de la Baume

F-35576 Cesson-S´evign´e

F-75008 Paris

gilles.puy@technicolor.com

patrick.perez@valeo.com

Abstract

We propose a new ﬂexible deep convolutional neural net-
work (convnet) to perform fast neural style transfers. Our
network is trained to solve approximately, but rapidly, the
artistic style transfer problem of [15] for arbritary styles.
While solutions already exist, our network is uniquely ﬂex-
ible by design: it can be manipulated at runtime to enforce
new constraints on the ﬁnal output. As examples, we show
that it can be modiﬁed to perform tasks such as fast photo
style transfer, or fast video style transfer with short term
consistency, with no retraining. This ﬂexibility stems from
the proposed architecture which is obtained by unrolling the
gradient descent algorithm used in [15]. Regularisations
added to [15] to solve a new task can be reported on-the-ﬂy
in our network, even after training.

1. Introduction

Style transfer is a longstanding problem [12, 23, 38, 13]
for which impressive results have recently been obtained
with deep convnets. In [15], a stylised image is obtained
by minimisation of a loss built from features provided by a
pre-trained convnet. The loss involves two terms: the ﬁrst
preserves the content of one image; the second transfers the
style of another image. The drawback of this method is its
speed as the stylised image is the result of a long optimi-
sation process. Nevertheless, this method is highly ﬂexi-
ble as the original style transfer loss can be manipulated to
guide the solution towards a desired result. This ﬂexibility
allows one to control perceptual factors during style trans-
fer [16]. It permits one to augment the original style loss
with regularisers favouring temporal consistency to perform
video style transfer [40, 41]. It also allows one to achieve
photo style transfer with the use of a regulariser encourag-
ing the transformation to be locally afﬁne [33]. This ﬂexi-
bility is crucial to construct a tool giving maximum freedom
to an artist. A methodological challenge is thus to make the
method of [15] faster while keeping most of its original ﬂex-

ibility, hence avoiding retraining for any new style transfer
feature one wishes to add.

All the methods solving [15] rapidly rely on the same
principle: a deep network taking as input a natural image
is trained to estimate a minimiser of the style transfer loss,
hence obtaining a stylised version of the input. The ﬁrst fast
methods [26, 48] crucially lacked ﬂexibility as one network
needed to be trained for each style. This shortcoming was
partly addressed by subsequent works showing that the style
can be encoded by a small subset of the network parameters
[6, 11]. Recent works show that it is possible to train a net-
work that takes as inputs any pair of painting and content
images and produces a stylised image, even for paintings
not viewed at training time [17, 25, 29]. Deep networks
trained for fast artistic style transfer have thus gained more
and more ﬂexibility. Yet, these fast style transfer methods
are not as ﬂexible as the method of [15] as one cannot plug
easily a new feature in them. For example, fast video style
transfer is achieved after adaptation of the network archi-
tecture and speciﬁc retraining [5, 20, 24, 41]. Similarly, the
authors of [30] had to adapt the network architecture of [29]
and retrain it to get it to work for fast photo style transfer.

Contributions. Specialising a network for each style
transfer task is a cumbersome work. Our main contribu-
tion is to propose instead a new ﬂexible network that is
(a) trained to solve rapidly the artistic style transfer prob-
lem of [15] for arbitrary styles via unsupervised learning
and (b) which can be modiﬁed at test time to take into ac-
count important modiﬁcations of the artistic style transfer
loss. These modiﬁcations do not need to be known at train-
ing time. No retraining is required when adding them in the
network. This high level of ﬂexibility stems from the pro-
posed network architecture that has the structure of the gra-
dient descent algorithm used in [15]: modiﬁcations brought
to the initial style transfer loss can be reported directly in
our trained network. Like existing methods, our network
yields fast controllable artistic style transfer results but, un-
like them, it can be used to perform several other tasks with-
out retraining. This ﬂexibility permits us to make this net-

18963

work perform photo style transfer faster than the state-of-
the-art methods, which constitutes our second contribution.
Our third contribution is to show that we can introduce, at
testing time, a regulariser to stylise videos without suffering
from ﬂickering artefacts. In comparison, existing state-of-
the-art methods which, like ours, are not speciﬁcally de-
signed for this task suffers from ﬂickering even when using
the same regulariser. As a last illustration of the network’s
ﬂexibility, we show that it can be used for user-guided1 tex-
ture super-resolution, again without retraining. Finally, let
us highlight that, unlike several works, we train our network
without using any painting as style but show that it gener-
alises to paintings.

2. Other related works

Related works about visual style transfer are discussed
in the introduction. We discuss here related works that in-
spired the design of our network architecture.

Algorithms unrolling. A wide range of classic itera-
tive solvers amount to repeated applications of linear and
component-wise non-linear maps. Unfolding such an algo-
rithm over a ﬁxed number of iterations allows one to see
it as several layers of a neural network with shared, pre-
deﬁned weights. This can be taken as the starting point
for an actual, trainable neural net. This idea was intro-
duced by [19] in the context of sparse coding:
the itera-
tive shrinkage thresholding algorithm (ISTA) [2] gives rise
of a learnable network (LISTA) for fast approximate sparse
coding. Each layer has independent trainable weights and
the whole network is trained under supervision. In the re-
lated context of linear inverse problems, including com-
pressed sensing [3, 36, 37, 34, 52] and image restoration
[39, 7, 32, 35, 49, 51], several works have followed this un-
rolling approach. In some cases, all the weights (tied or un-
tied) are trained for the speciﬁc task to solve, e.g., [49]. In
some other cases, only the weights deﬁning the image prior
are trained, the other weights being deﬁned by the data ﬁ-
delity term of the problem to solve, e.g., [7]. Note that, by
essence of such inverse problems, full supervision is pos-
sible: input-output samples of the transform to be inverted
are indeed readily obtained, e.g., by sensing or artiﬁcially
degrading natural images. By contrast, our approach is fully
unsupervised, as other fast style transfer alternatives.

Unsupervised neural solvers. Training without super-
vision a (fast) feed-forward network to solve approximately
a complex optimization problem is a challenging problem,
with important applications. Using the cost function of in-
terest as training loss seems straightforward, but the amount
of actual guidance provided by the loss must be sufﬁcient
for such an unsupervised training to succeed. Besides works

in fast style transfer, other recent works have demonstrated
successful instances of this paradigm. In [46], efﬁcient ﬂuid
simulation is addressed. Within each time step of a classic
solver of the incompressible Euler equation, the costly solv-
ing of the Poisson equation on the pressure ﬁeld is replaced
by a trained 3D convnet. In this case the loss amounts to the
divergence square norm. In a different context, [45] learn a
deep neural solver for a complex inverse rendering problem:
estimating the shape, expression and reﬂectance of a face
in a single image, by minimization of the pixel-wise dis-
crepancy between rendered and real faces. In both works,
trained solvers provide good approximation of the solutions
of the initial minimisation problem, but with massive accel-
eration compared to iterative solvers. In the present paper,
we also propose to learn an unsupervised neural solver for a
complex optimization problem, but following the unrolling
principle explained above. As a consequence, the acceler-
ation, already obtained by others, is not our main contri-
bution. More importantly, the unrolling approach gives the
possibility to adapt at runtime our network in order to ac-
commodate important changes to the original cost function.

3. Network architecture

3.1. Style transfer by gradient descent

Style transfer consists in transforming an image Xc ∈
Rn×3 – of n pixels and 3 color channels2 – to give it the
“style” of another image Xs ∈ Rn′×3 while preserving the
“content” in Xc. In [15], the content and style of an im-
age are deﬁned using deep features obtained from VGG-19
[44]. The style transfer method then consists in solving a
minimisation problem involving these deep features. The
minimisation is done over an image X ∈ Rn×3. We denote
by Fℓ, Cℓ ∈ Rnℓ×cℓ and Sℓ ∈ Rn′
ℓ×cℓ the features at the ℓth
layer of VGG-19 for the images X, Xc and Xs, respectively.
In [15], the features Cℓ encode the content of Xc while the
Gram matrices S⊺
Sℓ encode the style of Xs. The loss to
ℓ
minimise for style transfer thus takes the form

L(X) = Lc(X, Xc) + Ls(X, Xs),

(1)

the

where

content

c kFℓ − Cℓk2

=
c > 0, ensures
transfer of the content of Xc to the ﬁnal image (k·kF
stands for matrix Frobenius norm), while the style loss

F /(nℓ cℓ), with λℓ

Lc(X, Xc)

loss

s Lℓ

s(X, Xs) with

Pℓ∈Ic λℓ
Ls(X, Xs) =Pℓ∈Is λℓ

Lℓ

s(X, Xs) =

1
c2

ℓ (cid:13)(cid:13)(cid:13)(cid:13)

1
nℓ

F⊺
ℓ

Fℓ −

1
n′
ℓ

S⊺
ℓ

2

F

(2)

Sℓ(cid:13)(cid:13)(cid:13)(cid:13)

and λℓ
s > 0, ensures transfer of the style of Xs to the ﬁnal
image. We use Is = {conv1 1, conv2 1, conv3 1, conv4 1,

1We consider a setting where the user provides a texture example to

2Throughout, the two spatial dimensions of images and features maps

guide the result of the super-resolution algorithm.

are ﬂattened into a single vector dimension for notational convenience.

8964

(i) computing each feature Fℓ via a forward-pass in

VGG-19;

(ii) computing the partial derivative:

∂Lℓ
s
∂Fℓ

∝ Fℓ ·(cid:20) 1

nℓ

F⊺
ℓ

Fℓ −

1
n′
ℓ

S⊺
ℓ

Sℓ(cid:21) ;

(5)

(iii) back-propagating this partial derivative to the input to

Figure 1. Comparison of the architecture of VGG-19 and our for-
ward maps fℓ.

obtain ∇Lℓ
s;

conv5 1} for the style loss and Ic = {conv4 2} for the con-
tent loss, as, e.g., in [15].

A stylised image X∗ can be obtained by starting from an
image X(0) and by progressively updating it to minimise the
loss L (1) by gradient descent:

X(t+1) = X(t) − µ ∇L(X(t)),

(3)

where µ > 0 is the stepsize. While achieving impressive
results, this method is nevertheless slow but convnets have
been designed to minimise (1) approximately at a much
lower computational cost [17, 26, 48].

3.2. Learned gradient descent for style transfer

3.2.1 Global architecture

We propose a new convnet to achieve fast style transfer.
Its architecture follows the update rule of the gradient de-
scent algorithm (3) where the actual gradient is replaced by
a learned update gt:

X(t+1) = X(t) − gt(cid:16)X(t), Xs(cid:17) ,

t = 0, . . . , N − 1,

(4)

where N is the number of unrolled iterations. Note that the
proposed network can be viewed as a residual network [22].
If one follows exactly the idea of unrolling [19], the com-
putational architecture of gt should be identical to the ar-
chitecture of ∇L. To reduce the computational costs and
number of parameters to train, we allow ourselves to make
few simpliﬁcations. While the gradient ∇L is made of a
content term and a style term, we design gt by mimicking
the computational architecture of ∇Ls only; hence, the sole
dependence of gt on Xs in (4). However, as in [26], we ini-
tialise X(0) with the RGB image Xc, instead of the classic
random initialization of the gradient descent, and we use the
complete cost L in the training loss. This allows us to keep
the content of Xc in the ﬁnal image X(N ).

3.2.2 Architecture of gt.

The gradient ∇Ls, which gt should replace, is obtained in
4 steps:

s∇Lℓ
s.

(iv) compute the weighted sum ∇Ls =Pℓ∈Is λℓ

We construct gt by mimicking these 4 steps, but we re-
place the original VGG-19 by a new convnet whose ﬁlters
are trained to stylize any natural image in N learned updates
(4). To simplify notation, we drop the iteration index t be-
low. However, we highlight that all the ﬁlters are different
at each iteration t (untied weights) in our implementation.

[Transforming Step (i)] We replace the VGG-19 fea-
tures Fℓ by new features fℓ(X), ℓ = conv1 1, . . ., conv5 1
(see Fig. 1). We call each fℓ a forward map as it replaces
the forward pass in VGG-19. For each VGG-19 layer that is
involved in the style loss, we have a corresponding layer. In
both architectures, the spatial dimensions are successively
halved and the number of channels doubled (except at the
last layer). VGG-19 has at least an extra convolutional layer
before any pooling. The initial number of channels is 64 in
VGG-19 and L in our network.

[Transforming Step (ii)] The computation of this partial
derivative can be transformed by substituting fℓ(X) for Fℓ
in (5):

fℓ(X) ·(cid:18) 1

nℓ

fℓ(X)⊺ fℓ(X) −

1
n′
ℓ

G⊺
ℓ

S⊺
ℓ

Sℓ Hℓ(cid:19) .

(6)

However, we faced sudden explosions of the loss during
training when using (6). We suspect that this effect is due
to the matrix product fℓ(X)fℓ(X)⊺fℓ(X) as it can amplify
large values during inference and backpropagation. There-
fore, we substituted

fℓ(X) ·(cid:18) 1

n′
ℓ

G⊺
ℓ

S⊺
ℓ

Sℓ Hℓ(cid:19)

(7)

S⊺
ℓ

for (6). The learned matrices Gℓ, Hℓ reduce the dimension
of the VGG-19 style matrix S⊺
Sℓ to make it compatible
ℓ
with the channel dimension of our network. The matrix
G⊺
Sℓ Hℓ, called style ﬁlter in Fig. 2, governs the style
ℓ
that is applied to the input image. This ﬁlter corrects fℓ(X)
so that the style of X gets closer to the target style after
(mimicked) backpropagation. Any style can be applied at
runtime by computing the VGG-19 features Sℓ of the cho-
sen style image. Let us clarify that we use the original pre-
trained VGG-19 to compute the features Sℓ.

8965

Figure 2. Structure of one learned iteration gt for fast style transfer. The features Sℓ are the VGG-19 features of the style image, allowing
application of arbitrary styles at runtime. The matrices Gℓ, Hℓ, and all convolutional ﬁlters are learned. The parameter L governs the
number of channels at each layer and the number of ﬁlters in one learned iteration.

[Transforming (iii)] We mimic backpropagation using
backwards maps bℓ. The structure of each bℓ is symmetric
to the structure of the corresponding map fℓ. Each bℓ takes
as input the partial derivative (7) at layer ℓ and transforms it
to a learned update for this scale:

The complete structure of gt is presented in Fig. 2. All
convolutions are computed using reﬂection padding. Note
that the matrix multiplication denoted by “ × ” in (7) can
be implemented as a convolution with a ﬁlter of spatial size
1 × 1, so that the whole network is convolutional.

bℓ(cid:18) 1

n′
ℓ

fℓ(X) · G⊺
ℓ

S⊺
ℓ

Sℓ Hℓ(cid:19) .

(8)

3.2.3 Connections with existing methods

Note that during exact backpropagation to compute ∇Lℓ
s,
the non-linearities applied are not ReLUs but subgradients
of ReLU. Similarly, the non-linearities we use in bℓ are also
subgradients of ReLU. These non-linearities have the form
B · h(ReLU(A)) (see Fig. 2), where “ · ” denotes the point-
wise multiplication and h(·) is the heaviside step function:

h (ReLU(A))jc = h (A)jc =(cid:26) 0,

1,

if Ajc 6 0,
if Ajc > 0,

(9)

where j, c index the spatial position and the feature chan-
nel, respectively. This choice of non-linearities allowed us
to obtain better results than when using ReLU in both fℓ
and bℓ, as usually done in encoder-decoder architectures
(see Sec. 5). During backpropagation, the gradient satis-
ﬁes ∂B · h(A) + B · ∂h(A) = ∂B · h(A), as ∂h(A) = 0
(with a discontinuity at 0 treated below). Hence the gra-
dient continues to ﬂow wherever A > 0. Concerning the
discontinuity at 0, one should note that A is obtained after
convolution and before ReLU. There is low probability that
any value of A is exactly 0. If this event occurs, we can set
∂h(0) = 0.

[Transforming (iv)] The global learned update is ob-
tained by summing up the learned updates (8) for the 5
scales Is involved in the style loss:

g (X, Xs) = Xℓ∈Is

λℓ

s bℓ(cid:18) 1

n′
ℓ

fℓ(X) × G⊺
ℓ

S⊺
ℓ

Sℓ Hℓ(cid:19) .

(10)

Our convnet architecture is fundamentally different from
the one proposed by [26] and by follow-up works such as
[11, 17]. Beyond the residual architecture, the main dif-
ference resides in the control of the style: we control it by
ﬁltering the features fℓ independently at each scale, while
the style is controlled by instance normalisation parameters
in [11, 17]. Our convnet architecture shares more simi-
larities with [6, 29, 30]. While these architectures do not
mimick the iterative procedure of the gradient descent algo-
rithm, the style is also controlled by ﬁltering deep features.
We note however that [29, 30] require matrix inversions for
style transfer, whereas ours does not.

4. Runtime restructuring

In this section, we explain how our network can be
restructured at runtime to enforce new properties on the
stylised image. Results obtained via various such restruc-
turings are given in Section 5.

4.1. Runtime modiﬁcation of the style loss Ls(X, Xs)

In [16], the authors leverage the ﬂexibility of the method
of [15] to control perceptual factors during style transfer.
Such controls are done via modiﬁcations of the original
style loss Ls. As our network has the same structure as
∇Ls, these modiﬁcations brought to Ls can be transferred
in our network, at runtime, in a systematic way. It indeed

8966

sufﬁces to study the consequences of this change in ∇Ls
and report them in our network. We give 3 examples below.
In the method of [15], one can control the effect of each
style scale during stylisation by adapting the weights λℓ
s
in (1). Any change of these weights directly impacts the
weighted sum in Step (iv) of the computation of ∇Ls. We
can thus control the effect of each style scale in our network
by using the new value of λℓ

s in (10).

One can also wish to mix different style images Xi

s for
stylisation. Such a mixing is possible in [15, 16] by using

Proximal operators, which generalise projection operators,
are widely used in optimisation [8]. Note that if L and R
were convex, then the above algorithm would converge to
a solution of (13) [8]. In the non-convex case, the above
algorithm converges to a saddle point of (13) upon some
conditions on L and R [1].

We remark that the only difference between (3) and (14)
is the computation of PµR after each gradient update. By
copying this modiﬁcation in our network, the updates (4)
become

Ls(cid:0)X, {Xi
Fℓ ·  1

nℓ

style loss. The partial derivatives (ii) for this new loss sim-
plify to

s), withPi αi = 1, as new

s}(cid:1) =Pi αi Ls(X, Xi

F⊺
ℓ

Fℓ −

1
n′

ℓ Xi

αi S⊺

ℓ [Xi

s]Sℓ[Xi

s]! ,

(11)

s] denotes the features of the image Xi

where Sℓ[Xi
s at the ℓth
layer of VGG-19. By propagating this modiﬁcation, we can
mix different styles in our network by changing (7) to

1
n′
ℓ

fℓ(X) ·"G⊺

ℓ  Xi

αi S⊺

ℓ [Xi

s]Sℓ[Xi

s]! Hℓ# .

(12)

Finally, one can reﬁne the above mixture of styles with
spatial control, e.g., by associating each style Xi
s to a differ-
ent region in the content image. This is done by introducing
masks in the style loss [16]. Let Mi
ℓ denote the mask at
layer ℓ for the ith style and ith region. The masked style loss
satisﬁes

Fℓ)⊺(Mi
ℓ

Fℓ) −

1
n′
ℓ

ℓ [Xi
S⊺

s]Sℓ[Xi

Xℓ∈Is

ℓ

Pi n′

λℓ
s
c2

(Mi
ℓ

1
nℓ

ℓ Xi (cid:13)(cid:13)(cid:13)(cid:13)
ℓfℓ(X) ·(cid:0)G⊺

ℓ

−1 Mi

4.2. Adding new regularisers

ℓ [Xi
S⊺

s]Sℓ[Xi

s] Hℓ(cid:1).

After propagation of these changes in our network, spa-
tial control of the style is obtained by changing (7) to

2

F

.

s](cid:13)(cid:13)(cid:13)(cid:13)

Beyond the control of the style loss, one can easily im-
Indeed, let

pose additional constraints on the solution.
R(X) denote a regulariser and augment (1) with it:

min

X

L(X) + R(X).

(13)

X(t+1) = PRhX(t) − gt(cid:16)X(t), Xs(cid:17)i .

(16)

Hence, we are able to add the effect of any regulariser in our
network at runtime – without any retraining. Our network
inherits the ﬂexibility of optimisation algorithms.

4.2.1 Photo style transfer

Minimizing (1) yields good results when the style is a paint-
ing. Unfortunately, the results are less impressive when
the style is a photograph as the result is often not photo-
realistic. To solve this issue, [33] propose to favour trans-
formations from the content image to the ﬁnal image that
are locally afﬁne. This is done by adding a penalty term
to L in (1) which becomes L(X) + λL Tr(X⊺LX), where
L is the Matting Laplacian [28] of Xc. This regularisa-
tion can be integrated in our network easily via (16) with
R(X) = λL Tr(X⊺LX). Note that in this case, PR (Y) =
(I + 2λLL)−1 Y. We remark that [30] propose to post-
process the output of their network by multiplying it a ma-
trix similar to (I + 2λLL)−1. In our network, this process-
ing is done after each learned update, as done in the proxi-
mal gradient algorithm. Let us highlight another difference
with [30]: we never invert the matrix (I + 2λLL) to compute
PR (Y). Instead, we use a computationally efﬁcient method
from graph signal processing [43]. The method is detailed
in the supplementary material. In few words, it consists in
viewing the estimation of PR (Y) as ﬁltering Y on a graph
with Laplacian L. The corresponding ﬁlter can be approxi-
mated by a polynomial, which induces a fast graph ﬁltering
algorithm. This graph ﬁltering technique is used for, e.g.,
efﬁcient wavelet decompositions on graphs [21], fast spec-
tral clustering [47], or deep learning on graphs [9].

A strategy to solve this problem is to use the proximal gra-
dient descent algorithm whose iterations satisfy

4.2.2 Video style transfer

X(t+1) = PµRhX(t) − µ ∇L(cid:16)X(t)(cid:17)i ,

where PµR is the proximal operator associated to R:

(14)

PµR (Y) ∈ arg min

X

1
2

kX − Yk2

F + µR(X).

(15)

One of the challenges in video style transfer is ensuring
temporal consistency. The authors of [40] solve this prob-
lem by augmenting (1) with regularisers enforcing such a
consistency. These regularisers can easily be integrated in
our network. For simplicity, we consider the short term con-
sistency regulariser proposed in [40] and limit ourselves to
an online scenario: obtaining the (i + 1)th stylised image

8967

Xc - Xs

h - N :1 - L:64

h - N :2 - L:64

h - N :3 - L:64

Xc & Xs

Up to conv2 1 Up to conv3 1 Up to conv4 1 Up to conv5 1

LTV(X (N )) :
h - N :3 - L:16

0.33 ± 0.07

0.27 ± 0.06

0.26 ± 0.06

h - N :3 - L:32

h - N :4 - L:64

ReLU-N :3-L:64

Xc & Xs

×0.2

×0.6

×1.0

×1.4

Xc, X1

s & X2

s 0.2X1

s - 0.8X2

s 0.4X1

s - 0.6X2

s 0.6X1

s - 0.4X2

s 0.8X1

s - 0.2X2

s

0.29 ± 0.07
Xc - Xs

0.27 ± 0.06

ReLU

h

0.24 ± 0.05
Xc - Xs

0.27 ± 0.06

ReLU

h

Figure 3. Effect of the choice of N , L, and non-linearity (h(·) or
ReLU). The style appears in the bottom left corner of the con-
tent image. First and second rows: The mean achieved LTV (±
the standard deviation) on 1000 pairs of validation images appears
below each model. Third row: Additional results obtained with
h(·) or ReLU (N = 3, L = 64).

c

c onto X(i+1)

X(i+1) of a video sequence while X(i) was computed pre-
viously. Let wi(·) be the function that warp the original
image X(i)
, and Mi the binary mask indicat-
ing where this warping is valid (removing disocclusions and
motion boundaries as in [40]). The (i + 1)th stylised im-
age can be obtained as a solution of (13) using R(X) =
, where ⊙ is the pointwise
multiplication. A solution of this problem can be estimated
rapidly using our trained network via the updates (16).
The proximal operator PR is fast to compute and satisﬁes
,

λR (cid:13)(cid:13)
Mi ⊙ (wi(X(i)) − X)(cid:13)(cid:13)
PR (Y) = (cid:2)Y + 2λRM2

i(cid:3)−1
i ⊙ wi(X(i))(cid:3) ⊙ (cid:2)I + 2λRM2

where the inverse is computed elementwise.

2
F

4.2.3 User-guided texture super-resolution

We consider the problem of upscaling a low-resolution tex-
ture Xlow in a user-guided scenario. First, a user provides a
high-resolution image Xref of a texture that he judges sim-
ilar to Xlow. Then, we exploit this information to upscale
Xlow by using the fact that two similar textures have sim-
ilar VGG-19 Gram matrices [14]. This is done by min-
imising Ls(X, Xref ), where X is the image we are opti-
mising on for upscaling Xlow. We also seek to minimise
R(X) = λR kXlow − DXk2
F, where D models the down-
sampling operator, to ensure consistency of the solution
with Xlow. In total, we propose to solve minX Lc(X, Xbic)+
Ls(X, Xref ) + R(X). The image Xbic denotes the bicubic
upsampled version of Xlow, and the term Lc(X, Xbic) acts
as a second regulariser ensuring consistency between the

Figure 4. Choice of style scales (top row), global style intensity
(middle), and mixing of styles at runtime. The styles used appear
in the bottom corners of the content images (ﬁrst column).

observed low-resolution texture and the solution. A solu-
tion of this minimisation problem can be estimated rapidly
using our trained network via the updates (16), starting from
X(0) = Xbic. Note that the computation of PR can be esti-
mated by, e.g., gradient descent.

Let us acknowledge that this application was inspired by
[42] where the resolution of a skin texture is improved using
a database of such high-resolution textures.

5. Style transfer experiments

5.1. Training

tal Variation (TV) regularisation and usePN

As done in [26] for example, we augment L with the To-
t=1 LTV(X(t)),
with LTV(X) = L(X) + λTV TV(X), as training loss. The
TV regularisation favours visual “naturalness” of X(N ). The
pixel values of X(t) are clipped between 0 and 1 before eval-
uation of the loss. All ﬁlters are initialised using Xavier
initialisation [18], and trained using Adam [27] with a step-
size of 2·10−5 on the 2014 MS-COCO training dataset [31].
The training images are centrally cropped to the largest pos-
sible square and resized to 320 × 320. At each iteration,
two images are drawn at random: the ﬁrst is used as a con-
tent image, the second as a style image. Independent draws
of zero-mean uniform noise of amplitude randomly cho-
sen in [0, 0.1) is added to both images at each iteration.
Note that, unlike in several works on artistic style trans-
fer, we thus do not train our network on any painting. We
s = βℓ/α where
use λconv4 2
ℓ ) and βℓ are independent ran-
dom variables taking value 0 or 0.9−1 with probability 0.9
for the latter. The normalisation α favours visually similar
degree of stylization across styles. The variables βℓ permit
us to disentangle, via (10), the effect of each style scale ℓ in

α =Pℓ∈Is kS⊺

= 0.02, λTV = 0.3, and λℓ

F /(c2

ℓ n′2

Sℓk2

c

ℓ

8968

Xc & Xs

[15]

[25]

[29]

Ours

Xc & Xs

[15]

[25]

[29]

Ours

LTV(X):

0.107

0.235

0.254

0.210

LTV(X):

0.079

0.288

0.250

0.217

LTV(X):

0.131

0.208

0.216

0.225

LTV(X):

0.136

0.277

0.291

0.233

Xc & Xs

[33]

[30]

Ours

Xc & Xs

[33]

[30]

Ours

Figure 5. Top: Artistic style transfer results obtained with [15, 25, 29] and our method. The achieved loss LTV(X) is reported below each
image. Bottom: Photorealistic style transfer results obtained with [33, 30], and our method.

the ﬁnal result. New βℓ are drawn at each iteration.

5.2. Study of the architecture

We present in Fig. 3 artistic style transfer results for 7
different networks trained during 30 epochs: N = 1, 2, 3, 4
with L = 64 and L = 16, 32, 64 with N = 3. We remark
that the stylisation is visually improved when both N and L
increase. The mean loss achieved over 1000 pairs of content
and style images taken from the 2014 MS-COCO validation
set also improves when these parameters increase. Finally,
when using classical ReLU-decoders as backward maps, we
remark in Fig. 3 ellipsoidal structures and white artifacts
overlaid on the results. We hypothesise that the pointwise
multiplication with h(·) forces the network to better pre-
serve the contours and edges of the content image, avoiding
the presence of these parasite structures. These artefacts
appeared when training the network for arbitrary styles but
were absent when using few ﬁxed styles.

For all remaining experiments, we ﬁx N = 4, L = 64,

and train this network for 50 epochs.

5.3. Fast artistic style transfer

To illustrate that our network is, at least, as ﬂexible as ex-
isting state-of-the art solutions, we present in Fig. 4 results
where we control, at runtime, the number of style scales, the
global style intensity, and where we mix different styles.

We present in Fig. 5 artistic style transfer results ob-
tained with our network and compare them with those of
[15, 25, 29]. We use βℓ = 1 for all ℓ. For [15], we minimise
LTV(X) using the L-BFGS algorithm initialised with Xc,
as our network is trained to minimise this loss starting from

this image. Qualitatively, our results are similar to those of
state-of-the-art methods. Quantitatively, the method of [15]
reaches the lowest value for LTV(X) and all the fast meth-
ods obtain similar values. The second best value is, most of
the times, obtained by our network but this is expected as it
is trained to minimise LTV (while the others are not).

5.4. Fast photo style transfer.

Photo style transfer results obtained with [30, 33] and our
method are presented in Fig. 5. We use βℓ = 1 for all ℓ and
λL = 50. We achieve stylisations of similar visual quality
as the alternative methods. Nevertheless, no methods are
exempt of artifacts. The color in our results are sometimes
“ﬂattened”, as also noticeable in [30]. This is due to the
matting Laplacian ﬁltering: because of it, neighboring re-
gions that are similar in the content image and that get styl-
ized slightly differently, are ﬁnally averaged. All methods
would beneﬁt from a better ﬁlter/regularisation.

The mean computation times measured on an Nvidia
Tesla P100 Pascal GPU are reported below. Our method
method is nearly twice faster than the one of [30] at all
resolutions (columns “With Laplacian”). This advantage
is mainly due to the graph ﬁltering technique. After re-
moving any processing involving the matting Laplacian in
both methods (hence removing the photorealistic prior), our
method is still the fastest except at the highest resolution:

With Laplacian No Laplacian
Ours
[30]
0.04
0.79
0.11
3.12
15.23
0.37

Ours
0.44
1.86
8.11

[30]
0.20
0.23
0.34

Resolution
256 × 128
512 × 256
1024 × 512

8969

Xc & Xs

[29]

Ours

Xc & Xs

[30]

Ours

Ground truth & Xref

SRCNN [10]

Ours - Not guided

Ours - Guided

Ground truth & Xref

SRCNN [10]

Ours - Not guided

Ours - Guided

PSNR - SNR Gram 25.5 dB - 4.75 dB

28.7 dB - 6.26 dB

26.5 dB - 16.5 dB

PSNR - SNR Gram 28.1 dB - 6.45 dB

32.6 dB - 8.67 dB

32.1 dB - 10.9 dB

Figure 6. Top: Frames of two video sequences stylised with [29] or [30], and with our method using temporal regularisation. The style
appears in the bottom left corner of original frames. Bottom: Informed texture super-resolution (×3 in both directions) using a user
provided texture Xref (bottom left in ground truth image). The PSNR between the reconstructed and ground truth images, and the SNR
between the VGG-19 Gram matrices of the same images are provided below each result. Results are better viewed when zooming in.

5.5. Video style transfer.

We present in Fig. 6 video style transfer results. We com-
pare our method, which includes short-term consistency,
with those of [29] for artistic style transfer and [30] for
photo style transfer. We use a video from the MPI Sin-
tel dataset [4] for artistic style transfer. To enforce both
photorealism and short-term consistency, we compose the
corresponding proximal operators for simplicity. The op-
tical ﬂows between images are pre-computed using Deep-
Flow [50] but any fast ﬂow estimation method could be
used. We use λR = 0.37. The videos are provided in the
supplementary material.

We remark ﬂickering artefacts in the videos obtained
with [29, 30]. This ﬂickering is reduced with our method
thanks to the short-term consistency regularisation. One
can argue that it is also possible to apply the temporal con-
sistency proximal operator on the results of [29, 30]. This
indeed attenuates ﬂickering. Nevertheless, the videos pro-
vided in the supplementary material show that our results
present less ﬂickering than the post-processed results of
[29, 30]. In Fig. 6, the lack of temporal consistency is better
noticed on the hair of the main character in the video ob-
tained with [29], and in the sky in the video obtained with
[30]. Finally, we acknowledge that the stylisation in itself
is quite different for the methods of [29, 30] and ours (even
without using the temporal consistency proximal operator).

5.6. User guided texture super resolution

We present in Fig. 6 user-guided super-resolution results
of textures. The downsampling operator D consists of a
Gaussian ﬁlter of size 17 × 17 with σ = 3, followed by a

downsampling with stride 3. Note that we only process the
luminance. Colors are added back for visualisation only.
We present results obtained using SRCNN [10], with our
method using βℓ = 0 for all ℓ – hence not user-guided –
and with βℓ = 1 for ℓ = conv1 1, conv2 1, conv3 1, conv4 1,
βconv5 1 = 0 for the complete proposed method. We set
λR = 50. We measure the PSNR between the reconstructed
and ground truth images and the SNR between the VGG-19
Gram matrices of the same images (as these matrices are
similar for similar textures [14]).

Our method performs better than SRCNN. This is proba-
bly explained by the fact that SRCNN is speciﬁcally trained
for bicubic downsampling, whereas we use a strided Gaus-
sian ﬁltering. Concerning our results, similar PSNRs are
reached with and without user inputs but the SNRs between
the Gram matrices are improved with the user inputs. We
hence obtain a better reconstruction of the overall texture,
even though not pixel-accurate. Visually, our method with
user guidance permits to enhance high-frequencies.

6. Conclusion

We proposed a new deep, fully convolutional network for
fast artistic style transfer which has a key advantage: it can
be restructured at runtime to incorporate important modi-
ﬁcations of the artistic style transfer loss. Thanks to this
property, it is possible to perform photo style transfer faster
than state-of-the-art methods, video style transfer without
suffering from ﬂickering artefacts, and user-guided super-
resolution, all without retraining. We provide additional re-
sults and discuss some limitations of the technique in the
supplementary material.

8970

References

[1] Heddy Attouch, J´erˆome Bolte, and Benar F. Svaiter. Conver-
gence of descent methods for semi-algebraic and tame prob-
lems: proximal algorithms, forward–backward splitting, and
regularized Gauss–Seidel methods. Math. Program., 137(1-
2):91–129, 2013.

[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM J.
on Imaging Sci., 2(1):183–202, 2009.

[3] Mark Borgerding, Philip Schniter, and Sundeep Rangan.
AMP-inspired deep networks for sparse linear inverse prob-
lems. IEEE Trans. on Signal Processing, 65(16):4293–4308,
2017.

[4] Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and
Michael J. Black. A naturalistic open source movie for op-
tical ﬂow evaluation. In European Conference on Computer
Vision (ECCV), pages 611–625, 2012.

[5] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
Hua. Coherent online video style transfer. In IEEE Interna-
tional Conference on Computer Vision (ICCV), pages 1114–
1123, 2017.

[6] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
Hua. Stylebank: An explicit representation for neural image
style transfer. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2770–2779, 2017.

[7] Yunjin Chen and Thomas Pock. Trainable nonlinear reaction
diffusion: A ﬂexible framework for fast and effective image
restoration. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 39(6):1256–1272, 2017.

[8] Patrick L. Combettes and Jean-Christophe Pesquet. Fixed-
Point Algorithms for Inverse Problems in Science and Engi-
neering, chapter Proximal Splitting Methods in Signal Pro-
cessing, pages 185–212. Springer Optimization and Its Ap-
plications. Springer New York, 2011.

[9] Micha¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional neural networks on graphs with
fast localized spectral ﬁltering. In Advances in Neural Infor-
mation Processing Systems, pages 3844–3852, 2016.

[10] Chao Dong, Chen Change Loy, He Kaiming, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In European Conference on Computer Vi-
sion (ECCV), pages 184–199, 2014.

[11] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
A learned representation for artistic style. In International
Conference on Learning Representations, 2017.

[12] Alexei A. Efros and William T. Freeman. Image quilting for
texture synthesis and transfer. In Conference on Computer
graphics and interactive techniques (SIGGRAPH), pages
341–346, 2001.

[13] Oriel Frigo, Neus Sabater, Julie Delon, and Pierre Hellier.
Split and match: Example-based adaptive patch sampling for
unsupervised style transfer. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
553–561, 2016.

[14] Leon Gatys, Alexander S. Ecker, and Matthias Bethge. Tex-
ture synthesis using convolutional neural networks. In Ad-

vances in Neural Information Processing Systems (NIPS),
pages 262–270, 2015.

[15] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2414–2423, 2016.

[16] Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron
Hertzmann, and Eli Shechtman. Controlling perceptual fac-
tors in neural style transfer. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 3730–
3738, 2017.

[17] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent
Dumoulin, and Jonathon Shlens. Exploring the structure of
a real-time, arbitrary neural artistic stylization network. In
British Machine Vision Conference (BMVC), 2017.

[18] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
culty of training deep feedforward neural networks. In Inter-
national Conference on Artiﬁcial Intelligence and Statistics,
volume 9, pages 249–256, 2010.

[19] Karol Gregor and Yann LeCun. Learning fast approxima-
tions of sparse coding. In International Conference on In-
ternational Conference on Machine Learning (ICML), pages
399–406, 2010.

[20] Agrim Gupta, Justin Johnson, Alexandre Alahi, and Li Fei-
Fei. Characterizing and improving stability in neural style
transfer.
In 2017 IEEE International Conference on Com-
puter Vision (ICCV), pages 4087–4096, 2017.

[21] David K. Hammond, Pierre Vandergheynst, and R´emi Gri-
bonval. Wavelets on graphs via spectral graph theory. Appl.
Comput. Harmon. Anal., 30(2):129–150, 2011.

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, 2016.

[23] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian
Curless, and David H. Salesin. Image analogies. In Confer-
ence on Computer graphics and interactive techniques (SIG-
GRAPH), pages 327–340, 2001.

[24] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao
Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-time
neural style transfer for videos. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 7044–
7052, 2017.

[25] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In IEEE In-
ternational Conference on Computer Vision (ICCV), pages
1510–1519, 2017.

[26] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
In European Conference on Computer Vision (ECCV), pages
694–711, 2016.

[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. arXiv:1412.6980, 2014.

[28] Anat Levin, Dani Lischinski, and Yair Weiss. A closed-form
solution to natural image matting. IEEE Trans. Pattern Anal.
Mach. Intell., 30(2):228–242, 2008.

8971

[29] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,
and Ming-Hsuan Yang. Universal style transfer via feature
transforms. In Advances in Neural Information Processing
Systems (NIPS), pages 386–396, 2017.

[30] Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and
Jan Kautz. A closed-form solution to photorealistic im-
age stylization. European Conference on Computer Vision
(ECCV), pages 468–483, 2018.

[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
European Conference on Computer Vision (ECCV), 2014.

[32] Risheng Liu, Xin Fan, Shichao Cheng, Xiangyu Wang,
and Zhongxuan Luo. Proximal alternating direction net-
work: A globally converged deep unrolling framework.
arXiv:1711.07653, 2017.

[33] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala.
Deep photo style transfer. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 6997–7005,
2017.

[34] Morteza Mardani, Qingyun Sun, David Donoho, Vardan Pa-
pyan, Hatef Monajemi, Shreyas Vasanawala, and John Pauly.
Neural proximal gradient descent for compressive imaging.
arXiv:1806.03963, 2018.

[35] Tim Meinhardt, Michael Moller, Caner Hazirbas, and Daniel
Cremers. Learning proximal operators: Using denoising net-
works for regularizing inverse imaging problems. In IEEE
International Conference on Computer Vision (ICCV), pages
1799–1808, 2017.

[36] Chris Metzler, Ali Mousavi, and Richard Baraniuk. Learned
d-amp: Principled neural network based compressive image
recovery. In Advances in Neural Information Processing Sys-
tems (NIPS), pages 1772–1783, 2017.

[37] Ali Mousavi and Richard G. Baraniuk. Learning to invert:
Signal recovery via deep convolutional networks. In IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 2272–2276, 2017.

[38] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter
IEEE Computer

Shirley. Color transfer between images.
graphics and applications, 21(5):34–41, 2001.

[39] J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K.
Vijaya Kumar, and Aswin C. Sankaranarayanan. One net-
work to solve them all — solving linear inverse problems
using deep projection models. In IEEE International Confer-
ence on Computer Vision (ICCV), pages 5889–5898, 2017.

[40] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos. In German Conference on
Pattern Recognition, pages 26–36, 2016.

[41] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos and spherical images. Inter-

national Journal of Computer Vision, 126(11):1199–1219,
2018.

[42] Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and
Hao Li. Photorealistic facial texture inference using deep
neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2326–2335, 2017.

[43] David I. Shuman, Sunil K. Narang, Pascal Frossard, Anto-
nio Ortega, and Pierre Vandergheynst. The emerging ﬁeld
of signal processing on graphs: Extending high-dimensional
data analysis to networks and other irregular domains. IEEE
Signal Processing Magazine, 30(3):83–98, 2013.

[44] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. Interna-
tional Conference on Learning Representations, 2015.

[45] Ayush Tewari, Michael Zollh¨ofer, Hyeongwoo Kim, Pablo
Garrido, Florian Bernard, Patrick P´erez, and Christian
Theobalt. Mofa: Model-based deep convolutional face au-
toencoder for unsupervised monocular reconstruction.
In
IEEE International Conference on Computer Vision (ICCV),
pages 3735–3744, 2017.

[46] Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann,
and Ken Perlin. Accelerating Eulerian ﬂuid simulation with
convolutional networks.
International Conference on Ma-
chine Learning, 70:3424–3433, 2017.

[47] Nicolas Tremblay, Gilles Puy, R´emi Gribonval, and Pierre
Vandergheynst. Compressive spectral clustering. In Interna-
tional Conference on Machine Learning (ICML), volume 48,
pages 1002–1011, 2016.

[48] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor Lempitsky. Texture networks: Feed-forward synthesis
of textures and stylized images. In International Conference
on Machine Learning (ICML), volume 48, pages 1349–1357,
2016.

[49] Shenlong Wang, Sanja Fidler, and Raquel Urtasun. Proximal
deep structured models. In Advances in Neural Information
Processing Systems (NIPS), 2016.

[50] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and
Cordelia Schmid. Deepﬂow: Large displacement optical
ﬂow with deep matching. In IEEE International Conference
on Computer Vision (ICCV), pages 1385–1392, 2013.

[51] Dong Yang and Jian Sun. Proximal dehaze-net: A prior
learning-based deep network for single image dehazing. In
European Conference on Computer Vision (ECCV), pages
702–717, 2018.

[52] Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable
optimization-inspired deep network for image compressive
sensing. IEEE International Conference on Computer Vision
(CVPR), 2018.

8972

