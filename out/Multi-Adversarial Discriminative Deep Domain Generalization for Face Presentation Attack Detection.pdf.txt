Multi-adversarial Discriminative Deep Domain Generalization for Face

Presentation Attack Detection

Rui Shao
Pong C. Yuen
Department of Computer Science, Hong Kong Baptist University

Xiangyuan Lan

Jiawei Li

{ruishao, jwli, pcyuen}@comp.hkbu.edu.hk, xiangyuanlan@life.hkbu.edu.hk

Abstract

Face presentation attacks have become an increasing-
ly critical issue in the face recognition community. Many
face anti-spooﬁng methods have been proposed, but they
cannot generalize well on ”unseen” attacks. This work fo-
cuses on improving the generalization ability of face anti-
spooﬁng methods from the perspective of the domain gen-
eralization. We propose to learn a generalized feature s-
pace via a novel multi-adversarial discriminative deep do-
main generalization framework. In this framework, a multi-
adversarial deep domain generalization is performed under
a dual-force triplet-mining constraint. This ensures that the
learned feature space is discriminative and shared by mul-
tiple source domains, and thus is more generalized to new
face presentation attacks. An auxiliary face depth supervi-
sion is incorporated to further enhance the generalization
ability. Extensive experiments on four public datasets vali-
date the effectiveness of the proposed method.

1. Introduction

Face recognition technique has been successfully applied
in a variety of applications in the real life, such as automat-
ed teller machines (ATMs), mobile phones, and entrance
guard systems. The easy-access to the human face brings
the convenience of face recognition, but also the presenta-
tion attacks (PA). As simple as a printed photo paper (i.e.,
print attack) or a digital image/video (i.e., video replay at-
tack) could easily hack a face recognition system deployed
in a mobile phone or a laptop when those spoofs are visual-
ly close to the genuine faces. Thus, how to cope with these
presentation attacks prior to the step of face recognition has
become an increasingly critical concern in the face recogni-
tion community.

Various face anti-spooﬁng methods have been proposed.
Appearance-based methods aim to differentiate real and
fake faces based on various appearance cues, such as col-
or textures [5], image distortion cues [31] or deep fea-

Figure 1. This paper aims to learn a feature space that is discrimi-
native and shared by multiple source domains, and thus more gen-
eralized to new face presentation attacks.

tures [32]. Temporal-based methods are proposed to extract
various temporal cues, such as facial motions [23, 28, 26]
or rPPG [16, 18]. Although these methods obtain promis-
ing performance in intra-dataset experiments where train-
ing and testing data are from the same dataset, the perfor-
mance dramatically degrades in cross-dataset experiments
where training and testing data are from different datasets.
This is because existing face anti-spooﬁng methods capture
the differentiation cues that are dataset biased [1], and thus
cannot generalize well to testing data with different feature
distribution compared to training data (caused by different
materials of attacks or recording environments).

The straightforward way to solve this problem is to ex-
ploit the domain adaptation technique [27, 12, 25, 20, 34,
7, 24, 29, 6, 30, 33, 3] to align the feature distribution be-
tween training and testing data so that the trained model
with source data can be adapted on the target data. How-
ever, in the scenario of face anti-spooﬁng, we have no clue
on test data (target domain) when we train our model. It is
also difﬁcult or impossible to collect attacks with all possi-
ble materials and in all possible environments to train and

10023

OuluDomain 1CASIADomain 2IdiapDomain 3Generalized Feature SpaceMSUUnseen DomainTestingTrainingSeen Domainsadapt our model. To improve the generalization ability of
face anti-spooﬁng methods without using the target domain
information, this paper exploits the domain generalization
approach. Domain generalization assumes that there exist-
s a generalized feature space underlying the seen multiple
source domains and the unseen but related target domain,
on which a prediction model learned with training data from
the seen source domains can generalize well on the unseen
target domain.

The generalized feature space learned by the domain
generalization approach should be shared by multiple
source domains and discriminative [15, 22]. In this way, the
space can exploit the common differentiation cues for face
anti-spooﬁng across multiple source domains, which are
less likely to be domain biased and thus more generalized.
For example, instead of focusing on some domain-speciﬁc
differentiation cues such as the screen bezel of the attack
sample in CASIA dataset in Fig. 1, models learned in this
generalized feature space are able to extract more general-
ized cues shared by all source domains. For this purpose,
a multi-adversarial deep domain generalization method is
proposed to automatically and adaptively learn this gen-
eralized feature space shared by multiple source domain-
s. Speciﬁcally, under the adversarial learning scheme, the
generator which is trained for producing the domain-shared
features, competes with multiple domain discriminators si-
multaneously during the learning process, which gradually
guides the learned features to be indistinguishable for mul-
tiple domain discriminators. Therefore, the feature space
shared by all source domains can be automatically discov-
ered after the feature generator fools all domain discrimi-
nators successfully. To enhance the discriminability of the
learned generalized feature space during the adversarial do-
main generalization, we further impose a dual-force triplet-
mining constraint in the learning process, which ensures the
distance of each sample to its positive smaller than its neg-
ative in both intra and cross domains. Moreover, to further
strengthen the generalization ability of the learned features,
we incorporate face depth information as auxiliary super-
vision in the learning process. All of them consist of the
proposed framework.

Note that a similar deep domain generalization method
based on adversarial learning has been proposed in [15],
which learns the generalized feature space by aligning mul-
tiple source domains to an arbitrary prior distribution vi-
a adversarial feature learning. However, simply aligning
multiple source domains to a pre-deﬁned distribution may
be sub-optimal. The generalized feature space exists un-
derlying the seen multiple source domains and the unseen
target domain. This means that this generalized feature s-
pace could be learned based on the information provided by
multiple source domains. To this end, we exploit the shared
and discriminative information among multiple source do-

mains to automatically and adaptively search and learn this
generalized feature space without aligning any prior distri-
bution.

2. Related Work

Current

Face Anti-spooﬁng methods.

face anti-
spooﬁng methods can be roughly categorized into
appearance-based methods and temporal-based methods.
Appearance-based methods aim to detect attacks based on
various appearance cues. Multi-scale LBP [19] and color
textures [5] methods are proposed to extract various LBP
descriptors in grayscale, RGB, HSV or YCbCr color s-
paces to differentiate real/fake faces. Image distortion anal-
ysis [31] detects the surface distortions caused by the low-
er appearance quality of images or videos compared to real
face skin. Yang et al. [32] use CNN to extract different deep
features between real and fake faces. On the other hand,
temporal-based methods aim to differentiate real/fake via
extracting various temporal cues through multiple frames.
Dynamic textures are proposed in [23, 28, 26] to extrac-
t different facial motions. Liu et al. [17, 16] propose to
estimate rPPG signals from RGB face videos to detect at-
tacks. Moreover, the work proposed in [18] captures both
appearance and temporal cues, which learns a CNN-RNN
model to estimate the different face depth and rPPG signals
between real and fake faces. However, the performance of
both appearance and temporal-based methods are prone to
being degraded in cross-datasets test where test data comes
from different datasets (domains), and thus the feature dis-
tribution is different with that in train domain. This is due
to that the above methods are likely to extract some differ-
entiation cues that are biased to speciﬁc materials of attack-
s or recording environments in training datasests. There-
fore, from the perspective of the domain generalization, this
paper proposes to capture more generalized differentiation
cues to solve both print and video replay attacks.

Deep Domain Generalization methods. Several deep
domain generalization methods have been proposed. Moti-
ian et al. [21] propose to jointly minimize the semantic
alignment loss and the separation loss on deep learning
models. Li et al. [14] design a low-rank parameterized C-
NN model for end-to-end domain generalization learning.
The most related work is proposed in [15], which learns a
generalized feature space by aligning multiple source do-
mains to a pre-deﬁned distribution via adversarial learning.
However, it can not be guaranteed that the pre-deﬁned dis-
tribution is the optimal one for the feature space. Therefore,
simply aligning multiple source domains to a pre-deﬁned
distribution may be sub-optimal. Instead, in our proposed
deep domain generalization framework, the generalized fea-
ture space is automatically and adaptively learned based on
the knowledge provided by multiple source domains.

10024

Figure 2. Overview of the proposed method. The multi-adversarial deep domain generalization is ﬁrstly proposed to learn a generalized
feature space that is shared by multiple discriminative source domains. Moreover, the constraint of dual-force triplet mining is imposed on
the learning process, which improves the discriminability of the learned feature space. The auxiliary face depth is further incorporated to
learn more generalized differentiation cues in this feature space. The module with solid lines means it is being trained while the one with
dashed lines indicates that its parameters are ﬁxed.

3. Proposed Method

3.1. Overview

The focus of this paper is to learn a generalized feature
space to cope with various unseen face presentation attacks.
Although testing samples are from an unseen domain, they
still share some common properties with multiple source
domains in face presentation attacks. For example, the print
or video replay attacks from unseen domains may be pre-
sented in different materials or under different environments
compared to source domains, but they are all based on pa-
pers or video screens intrinsically. The common properties
can be exploited from some shared and discriminative infor-
mation across multiple source domains. That is, a feature
space that is discriminative and shared by multiple source
domains is more likely to be generalized well to unseen do-
mains. Based on this idea, as illustrated in Fig. 2, this pa-
per proposes a novel multi-adversarial discriminative deep
domain generalization framework to learn this generalized
feature space. Speciﬁcally, a feature generator is trained
to compete against multiple domain discriminators so as
to gradually learn the shared and discriminative feature s-
pace. Meanwhile, a dual-force triplet-mining constraint is
imposed to improve the discrimination ability of the feature

space during the adversarial learning process. Moreover, as
the guidance to learn more generalized differentiation cues
in the feature space, the auxiliary supervision of face depth
is further incorporated in the learning process.

3.2. Multi adversarial Deep Domain Generalization

Suppose that there are images with N source domain-
s, denoted as X = {X1, X2, ..., XN }, and corresponding
labels are denoted as Y = {Y1, Y2, ..., YN } with K cate-
gories (K = 2 in the face anti-spooﬁng task where Y = 0/1
is the label of attack/real). Given the labeled data in each
source domain, we may begin by exploiting the discrimina-
tive information in each source domain.
Pretrain Multiple Source Feature Extractors. For N
source domains, we pre-train multiple feature extractors
(M1, M2, ..., MN ) respectively based on K-way classiﬁca-
tion with a cross-entropy loss. We take the pretraining of the
feature extractor of source domain 1 as an example, which
is shown as follows:

Lcls(X1, Y1; M1, C1) =

− E(x1,y1)∼(X1,Y1)

K

X

k=1

1l[k = y1]logC1(M1(x1))

(1)

10025

Feature Extractor_1Feature Extractor_2Discriminators_1OuluDomain 1CASIADomain 2IdiapDomain N Multi-adversarial Deep Domain GeneralizationFeatureGeneratorDepthEstimationZFeature EmbedderF1F2FnTripletLossDepthLossAdvLossReal DepthFake Depth...Discriminators_2Discriminators_N......ClassifierClsLossFeature Extractor_N...Dual-force Triplet Miningabove multi-adversarial deep domain generalization as fol-
lows:

LDG(X, X1, X2, ..., XN ; G, D1, D2, ..., DN ) =
N

i=1

(cid:16)Ex∼X[log(Di(G(x)))]

X
+ Exi∼Xi [log(1 − Di(Mi(xi)))](cid:17)

(2)

where G denotes the feature generator, which tries to learn
the generalized feature space that is indistinguishable to
every discriminative source feature spaces simultaneous-
ly. Di denotes the i-th domain discriminator that tries to
distinguish the learned feature space with the discrimina-
tive feature space of source domain i. Through this multi-
adversarial learning process in the feature space, the gener-
alized feature space can be automatically learned and gen-
erated by the feature generator G.

3.3. Dual force Triplet mining Constraint

In print and video relay attacks, the intra-class distances
are prone to being larger than the inter-class distances. Fig.
4 shows the typical condition in video replay attacks illus-
trating this problem. In Fig. 4, for each real subject, the fake
face with the same identity has similar facial characteristics,
while the real face with the different identity has different
facial characteristics. This makes the negative more simi-
lar than the positive for each subject in each domain. Due
to different materials of attacks or recording environments
between different domains, this problem may also be severe
under the cross-domain scenario. Therefore, the discrimina-
tion ability of learned generalized feature space is prone to
being degraded. We thus aim to improve the discrimination
ability via mining the triplet relationship among samples.
Speciﬁcally, when learning the feature space, we force that:
1) the distance of each subject to its intra-domain positive
smaller than to its intra-domain negative, 2) and simultane-
ously the distance of each subject to its cross-domain posi-
tive smaller than to its cross-domain negative. We call this
as dual-force triplet-mining constraint. In this way, the dis-
crimination ability of generalized feature space can be im-
proved via this constraint during the domain generalization
process. Therefore, we can obtain:

LT rip(X, Y; G, E) =

[kE(G(xa

i )) − E(G(xp

j ))k2
2

X

∀ya=yp,ya6=yn,i=j
− kE(G(xa

+ γ

j ))k2

i )) − E(G(xn
X

[kE(G(xa

2 + α1]+
i )) − E(G(xp

k))k2
2

(3)

∀ya=yp,ya6=yn,i6=k

− kE(G(xa

i )) − E(G(xn

k ))k2

2 + α2]+

10026

Figure 3. The details of Multi-adversarial Deep Domain General-
ization. Suppose that we have three source domains for simplici-
ty. We train one feature generator to compete with three domain
discriminators simultaneously, and a shared feature space will be
adaptively learned after this feature generator successfully fools
all domain discriminators.

We thus obtain multiple discriminative feature spaces of
source domains encoded by multiple trained feature extrac-
tors (M1, M2, ..., MN ). However, these discriminative fea-
ture spaces contain a large portion of differentiation cues
that are biased to each source domain, which disables them
to be generalized well to unseen attacks.

Multi-adversarial Deep Domain Generalization. To
learn a more generalized feature space for face anti-
spooﬁng, we want to exploit the common discriminative in-
formation encoded by multiple feature extractors of source
domains. More generalized differentiation cues for face
anti-spooﬁng will thus be exploited from the common dis-
criminative information, which are less likely to be biased
to any source domain, and thus have better generalization
ability.

To this end, we introduce a multi-adversarial deep do-
main generalization method. Because the generalized fea-
ture space contains the common discriminative information,
this space can be discovered by ﬁnding the shared space of
multiple discriminative source feature spaces. This mean-
s this feature space is simultaneously as similar to every
discriminative feature space of source domains as possible.
Suppose that we have N source domains. Accordingly, we
have N discriminative feature spaces encoded by N pre-
trained feature extractors respectively. N domain discrim-
inators are introduced for N discriminative feature spaces
respectively, and we train one feature generator to compete
with all the N domain discriminators simultaneously. A
shared feature space will thus be automatically and adap-
tively learned after this feature generator successfully fools
all the N domain discriminators. Figure 3 shows the illus-
tration of this multi-adversarial domain generalization pro-
cess when we have three source domains. We formulate

GGeneralized Feature SpaceDiscriminative Feature Space 1Domain 1Domain 2Domain 3Generalized Feature SpaceDiscriminative Feature Space of domain 3M_1Discriminative Feature Space of domain 1Discriminative Feature Space of domain 2D_3M_2M_3D_2D_1Discriminative Feature Space 2Discriminative Feature Space 3LDep(X; Dep) = kDep(G(X)) − Ik2
2

(4)

where Dep is the depth estimator and I is the face depth
map for supervision.

3.5. Multi adversarial Discriminative Deep Domain

Generalization

As shown in Fig.

2, a classiﬁer C is incorporat-
ed to calculated the classiﬁcation loss LCls. We for-
mulate the objectives as mentioned above into a uniﬁed
multi-adversarial discriminative deep domain generaliza-
tion framework (MADDG) as follows:

min

G,E,C,Dep

max

D1,D2,...,DN

LM ADDG =

LDG + LT rip + LDep + LCls

(5)

Note that due to the limited training data in face anti-
spooﬁng datasets and the complex structure of the de-
signed network, we decompose the training process into two
phases for tractable optimization: 1) Training the G, E, C
and D1, D2, ..., DN together, with multi-adversarial do-
main generalization loss, dual-force triplet-mining loss and
classiﬁcation loss. 2) Training the G and Dep with the aux-
iliary face depth information loss. The above two phases
are iteratively repeated in the training process until conver-
gence. The overall objective is to enable the feature gener-
ator G to generate the generalized feature space.

4. Experiments

4.1. Datasets

Table 1. Comparison of four experimental datasets.
Display
devices

background

Extra
light

Complex

Attack

type

Dataset

C

I

M

O

No

Yes

No

Yes

Yes

Yes

Yes

No

Printed photo

Cut photo

Replayed video
Printed photo
Display photo
Replayed video
Printed photo
Replayed video
Printed photo
Display photo
Replayed video

iPad

iPhone 3GS

iPad

iPad Air
iPhone 5S

Dell 1905FP

Macbook Retina

We evaluate our work on four public face anti-spooﬁng
datasets, which contain both print and video replay attack-
s: Oulu-NPU [4] (O for short), CASIA-MFSD [35] (C for
short), Idiap Replay-Attack [8] (I for short), and MSU-
MFSD [31] (M for short). Table 1 shows the variations in
these four datasets. Some samples of the genuine faces and
attacks are shown in Fig. 5. From Table 1 and Fig. 5, we
can see that many kinds of variations, due to the differences
on materials, illumination, background, resolution and so

10027

Figure 4. The illustration of Dual-force Triplet-mining Constraint.
In print and video relay attacks, the negative is likely to be more
similar than the positive for each subject in both intra and cross
domains. This constraint attempts to solve this problem by mini-
mizing intra-class distance while maximizing inter-class distance
in both intra and cross domains.

where E denotes the feature embedder, and the superscript-
s a and p represent the same class, while a and n are d-
ifferent classes. The subscripts i and j represent the same
domain, while i and k are different domains. α1 and α2 rep-
resent pre-deﬁned intra-domain and cross-domain margins,
respectively.

3.4. Auxiliary Face Depth Information

To exploit more generalized differentiation cues in the
generalized feature space, we further incorporate the face
depth cues as the auxiliary information for training our fea-
ture generator. Through the comparison on the spatial in-
formation, it can be observed that live faces have face-like
depth, while faces of attacks presented in the ﬂat and planar
papers or video screens have no face depth. Therefore, the
face depth information can be exploited as more generalized
differentiation cues for face presentation attacks detection.
We utilize the state-of-the-art dense face alignment network
named PRNet [10] to estimate the depth map of real faces,
which serves as the supervision for the real faces. The depth
map of all zeros is set as the supervision for the fake faces.
The estimated face depth information may also be domain
biased. Therefore, different from the method in [18] which
uses the estimated face depth to directly do classiﬁcation,
we incorporate the face depth as the auxiliary information
into the training process of domain generalization. In this
way, the feature space is guided to exploit more generalized
differentiation cues related to the face depth in the learning
process. This auxiliary depth information is incorporated as
follows:

Subject 1CASIAPositiveNegativeSubject 2NegativeIdiapPositiveFigure 5. Sample frames from CASIA-MFSD [35], Idiap Replay-Attack [8], MSU-MFSD [31], and Oulu-NPU [4] datasets. The ﬁgures
with green border represent the real faces, while the ones with red border represent the video replay attacks. From these examples, it can be
seen that large cross-dataset variations due to the differences on materials, illumination, background, resolution and so on, cause signiﬁcant
domain shift among these datasets.

Table 2. The structure details of all components of the proposed network.

Feature Generator

Discriminator

Layer Chan./Stri. Out.Size

Layer Chan./Stri. Outp.Size

Feature Embedder & Classiﬁer
Layer Chan./Stri. Outp.Size

Depth Estimator

Layer Chan./Stri. Outp.Size

Input

pool1-3
128/2
256/2
512/2

1/1

conv2-1
conv2-2
conv2-3
conv2-4

16
8
4
3

Input

pool1-3

conv3-1
pool2-1
conv3-2
pool2-2
conv3-2

Average pooling

fc3-1
fc3-2

128/1

-/2

256/1

-/2

512/1

1/1
1/1

32
16
16
8
8

128

1

Input
image
64/1
128/1
196/1
128/1

-/2

128/1
196/1
128/1

-/2

128/1
196/1
128/1

-/2

conv1-1
conv1-2
conv1-3
conv1-4
pool1-1
conv1-5
conv1-6
conv1-7
pool1-2
conv1-8
conv1-9
conv1-10
pool1-3

256
256
256
256
128
128
128
128
64
64
64
64
32

Input

pool1-1+pool1-2+pool1-3
32
32
32

128/1
64/1
1/1

conv4-1
conv4-2
conv4-3

on, exist across these four datasets. Therefore, signiﬁcant
domain shift exists among these datasets.

4.2. Experimental Setting

We regard one dataset as one domain in our experiment.
For simplicity, three datasets in four are randomly select-
ed as source domains where we conduct the domain gener-
alization, and the remaining one is the unseen domain for
testing, which cannot be accessed in the training process.
Half Total Error Rate (HTER) [2] (half of the summation of
false acceptance rate and false rejection rate) and Area Un-
der Curve (AUC) are used as the evaluation metrics in our
experiments.

4.3. Implementation Details

Network Structure. Our deep network is implement-
ed on the platform of PyTorch. The detailed structure of
the proposed network is illustrated in Table 2. To be spe-
ciﬁc, each convolutional layer in the feature generator, fea-
ture embedder and depth estimator is followed by a batch
normalization layer and a rectiﬁed linear unit (ReLU) ac-
tivation function, and all convolutional kernel size is 3×3.
Following the standard setting in [14], each convolutional
layer in the discriminator is followed by a batch normaliza-
tion layer and a LeakyReLU activation function, and all k-
ernel size is 4×4. The size of input image is 256 × 256 × 6,
where we extract the RGB and HSV channels of each in-
put image. Inspired by the residual network [11], we use
a short-cut connection, which is concatenating the respons-
es of pool1-1, pool1-2 and pool1-3, and sending them to

conv4-1 for depth estimation. This operation helps to ease
the training procedure, and enables the auxiliary informa-
tion of face depth simultaneously to affect different layers
of the feature generator in the learning process.

Training Details. The Adam optimizer [13] is used for
the optimization. As described in section 3.5, we train the
whole network with two iterative phases. Due to differen-
t model complexity between the two training phases, we
use the learning rate 1e-5 in the ﬁrst phase, which trains
the G, E, C and D1, D2, ..., DN together. G and Dep are
trained in the second phase with the learning rate 1e-4. The
batch size is 20 per domain, and thus 60 for 3 training do-
mains totally. The hyperparameters γ, α1, and α2 are set to
0.1, 0.1, and 0.5, respectively.

Testing. For a new testing sample x, its classiﬁcation s-
core l is calculated for testing as follows: l = C(E(G(x)))
where G, E, C are the trained feature generator, feature em-
bedder, and classiﬁer, respectively.

4.4. Experimental Comparison

4.4.1 Baseline Methods

We compare several state-of-the-art face anti-spooﬁng
methods as follows: Multi-Scale LBP (MS LBP) [19] ;
Binary CNN [32]; Image Distortion Analysis (IDA) [31];
Color Texture (CT) [5]; LBPTOP [23]; and Auxil-
iary [18]: This method learns a CNN-RNN model to es-
timate the face depth from one frame and rPPG signals
through multiple frames. To fairly compare our method on-
ly using one frame information, we implement its face depth

10028

(a) CASIA(b) Idiap(c) MSU(d) OuluTable 3. Comparison to face anti-spooﬁng methods on four testing sets for domain generalization on face anti-spooﬁng.
I&C&M to O

O&M&I to C

O&C&I to M

O&C&M to I

Method

HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%)

MS LBP

Binary CNN

IDA

Color Texture

LBPTOP

Auxiliary(Depth Only)

Auxiliary(All)

Ours (MADDG)

29.76
29.25
66.67
28.09
36.90
22.72

–

17.69

78.50
82.87
27.86
78.47
70.80
85.88

–

88.06

54.28
34.88
55.17
30.58
42.60
33.52
28.4
24.5

44.98
71.94
39.05
76.89
61.05
73.15

–

84.51

50.30
34.47
28.35
40.40
49.45
29.14
27.6
22.19

51.64
65.88
78.25
62.78
49.54
71.69

–

84.99

50.29
29.61
54.20
63.59
53.15
30.17

–

27.98

49.31
77.54
44.59
32.71
44.09
77.61

–

80.02

Table 4. Comparison to adversarial domain generalization method on four testing sets for domain generalization on face anti-spooﬁng.

Method

O&C&I to M

O&M&I to C

O&C&M to I

I&C&M to O

HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%)

MMD-AAE

Ours (MADDG)

27.08
17.69

83.19
88.06

44.59
24.5

58.29
84.51

31.58
22.19

75.18
84.99

40.98
27.98

63.08
80.02

estimation component(denoted as Auxiliary(Depth Only)).
We also compare its reported results (denoted as Auxil-
iary(All)). Moreover, we also compare the related state-of-
the-art method in domain generalization for the face anti-
spooﬁng task: MMD-AAE [15].

4.4.2 Comparison Results

From the comparison results in Table 3 and Fig. 6, it can
be seen that the proposed method performs better than all
the state-of-the-art face anti-spooﬁng methods [19, 32, 31,
5, 18]. This is due to that all existing face anti-spooﬁng
methods focus on learning a feature space from multiple
source domains that only ﬁts to data in the source domain-
s. Comparatively, the proposed multi-adversarial discrim-
inative deep domain generalization explicitly exploits the
domain relationship of multiple source feature spaces, and
learns the shared and discriminative information between
them. This learns a generalized feature space that is more
likely to be shared between source domains and unseen tar-
get domain, and thus it is more able to extract more gener-
alized differentiation cues for face anti-spooﬁng.

Moreover, in Table 4 and Fig. 6, compared to the state-
of-the-art domain generalization method [15], we also out-
perform it for the face anti-spooﬁng task. This illustrates
that comparing to the feature space learned by aligning mul-
tiple source domains to a pre-deﬁned distribution, the fea-
ture space that is automatically and adaptively learned by
our proposed domain generalization framework is more fea-
sible for the task of face anti-spooﬁng.

4.5. Discussion

4.5.1 Ablation Study

The experimental results of ablation study for all testing set-
s are shown in Table 5. MADDG denotes the proposed

framework. MADDG wo/mgan denotes that the proposed
network without the multi-adversarial domain generaliza-
tion component.
In this setting, we remove the multiple
domain discriminators (D1, ..., DN ) in our network in the
training process. MADDG wo/trip denotes that the pro-
posed network without the dual-force triplet-mining con-
straint component.
In this setting, we do not calculate
and backpropagate the dual-force triplet-mining loss in the
training process. MADDG wo/dep denotes that the pro-
posed network without incorporating auxiliary face depth
information. In this setting, we remove the depth estimator
Dep in the training process.

Table 5 shows that the performance of the proposed net-
work degrade if any component is excluded. This veriﬁes
the contribution of each component to the whole network,
and shows that the proposed network optimizing all com-
ponents simultaneously in a uniﬁed framework can obtain
much better performance.

4.5.2 Fusion strategies comparison

Fusion strategies are usually utilized when we have multi-
ple domains data. Thus, we added two more baselines for
comparison, namely score-level fusion and feature-level fu-
sion in Table 6.
In score-level fusion, we train multiple
AlexNets for all source domains respectively, and use av-
erage fusion on the testing scores of all trained CNNs on
the target domain. In feature-level fusion, like [9], we train
multiple AlexNets and fuse the features from FC7 layers by
concatenation. One more fully connected layer is integrat-
ed to classify the fused feature. Table 6 shows our method
outperforms the above two kinds of fusion strategies. Sim-
ple fusion strategies cannot cope with various cross-domain
scenarios so that in some scenarios such as O&M&I to C,
the performance of both baseline methods drop signiﬁcant-
ly. Comparatively, our method is robust in all scenarios.

10029

1

0.8

0.6

0.4

0.2

e
t
a
R
 
e
k
a
F
 
e
s
a
F

l

O&C&I to M

O&M&I to C

Auxiliary(Depth)
MMD-AAE
Binary CNN
Color Texture
IDA
MS_LBP
LBPTOP
Ours

1

0.8

0.6

0.4

0.2

e
t
a
R
 
e
k
a
F
 
e
s
a
F

l

Auxiliary(Depth)
MMD-AAE
Binary CNN
Color Texture
IDA
MS_LBP
LBPTOP
Ours

0

0

0.2

0.4

0.6

0.8

1

0

0

0.2

0.4

0.6

0.8

1

e

t

 

a
R
e
k
a
F
e
s
a
F

 

l

O&C&M to I

I&C&M to O

1

0.8

0.6

0.4

0.2

0

0

Auxiliary(Depth)
MMD-AAE
Binary CNN
Color Texture
IDA
MS_LBP
LBPTOP
Ours

0.2

0.4

0.6

0.8

1

t

 

e
a
R
e
k
a
F
e
s
a
F

 

l

1

0.8

0.6

0.4

0.2

0

0

Auxiliary(Depth)
MMD-AAE
Binary CNN
Color Texture
IDA
MS_LBP
LBPTOP
Ours

0.2

0.4

0.6

0.8

1

False Living Rate

False Living Rate

False Living Rate

False Living Rate

Figure 6. ROC curves of four testing sets for domain generalization on face anti-spooﬁng.

Table 5. Evaluation of different components of the proposed framework on four testing sets for domain generalization on face
anti-spooﬁng.

Method

O&C&I to M

O&M&I to C

O&C&M to I

I&C&M to O

HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%)

MADDG wo/mgan
MADDG wo/trip
MADDG wo/dep
Ours(MADDG)

21.55
20.84
34.29
17.69

85.83
85.95
69.92
88.06

28.67
30.46
39.95
24.5

82.27
77.99
62.42
84.51

36.50
34.99
37.44
22.19

63.15
71.37
62.82
84.99

29.63
29.75
39.39
27.98

77.40
75.93
64.19
80.02

Table 6. Comparison to fusion strategies on four testing sets for domain generalization on face anti-spooﬁng.

Method

O&C&I to M

O&M&I to C

O&C&M to I

I&C&M to O

HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%) HTER(%) AUC(%)

Score Fusion
Feature Fusion
Ours (MADDG)

21.00
25.62
17.69

86.18
74.57
88.06

46.62
52.32
24.5

57.05
48.23
84.51

34.17
46.29
22.19

71.53
52.71
84.99

31.12
32.56
27.98

76.42
76.01
80.02

4.5.3 Limited source domains

We evaluate the domain generalization ability of the pro-
posed method when extremely limited source domain
datasets are available (i.e. only two source datasets). Since
signiﬁcant domain variation exists between MSU and Idiap
datasets, we choose these two datasets as source domains.
As such, the remaining ones (Oulu and CASIA) are cho-
sen for testing. The results in Table 7 show the proposed
method performs better than other methods. This veriﬁes
our method is more effective even in the challenging case.

Table 7. Comparison of domain generalization with limited source
domains for face anti-spooﬁng.
M&I to C

M&I to O

Method

HTER(%) AUC(%) HTER(%) AUC(%)

MS LBP

IDA
CT

LBPTOP

Ours

51.16
45.16
55.17
45.27
41.02

52.09
58.8
46.89
54.88
64.33

43.63
54.52
53.31
47.26
39.35

58.07
42.17
45.16
50.21
65.10

to exploit domain shared and discriminative properties to
learn more generalized cues when more source domains are
available, and thus the advantage of domain generalization
can be better exploited by our method.

5. Conclusion

To improve the generalization ability for face anti-
spooﬁng, this paper exploits the technique of domain gener-
alization to learn a generalized feature space without using
target domain data. Speciﬁcally, a novel multi-adversarial
deep domain generalization method is proposed to train one
feature generator to compete with multiple domain discrim-
inators simultaneously, so that the generalized feature space
can be automatically and adaptively learned. The discrim-
inability of the generalized feature space is improved by a
dual-force triplet-mining constraint in the feature learning
process. Meanwhile, the face depth supervision is incor-
porated to further enhance the generalization ability of this
feature space. Extensive experiments among four public
datasets validate the effectiveness of the proposed method.

Moreover, compared to the results in Table 7, when we
have more source domains such as the normal setting of do-
main generalization in Table 3, the other methods cannot get
much improvement and the proposed method outperforms
them in a larger gap. This means our method is more able

6. Acknowledgement

This project is partially supported by Hong Kong RGC
GRF HKBU12201215. The work of X. Lan is partially sup-
ported by HKBU Tier 1 Start-up Grant.

10030

References

[1] Torralba. Antonio and Alexei A. Efros. Unbiased look at

dataset bias. In CVPR, 2011. 1

[2] Samy Bengio and Johnny Mari´ethoz. A statistical signif-
In The Speaker and

icance test for person authentication.
Language Recognition Workshop, 2004. 6

[3] Bharath Bhushan Damodaran, Benjamin Kellenberger, Remi
Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep
joint distribution optimal transport for unsupervised domain
adaptation. In ECCV, 2018. 1

[4] Zinelabinde Boulkenafet and et al. Oulu-npu: A mobile face
In

presentation attack database with real-world variations.
FG, 2017. 5, 6

[5] Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour
Hadid. Face spooﬁng detection using colour texture analy-
sis. In IEEE Trans. Inf. Forens. Security, 11(8): 1818-1830,
2016. 1, 2, 6, 7

[6] Konstantinos Bousmalis, Nathan Silberman, David Dohan,
Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-
level domain adaptation with generative adversarial network-
s. In CVPR, 2017. 1

[7] Qingchao Chen, Yang Liu, Zhaowen Wang, Ian Wassell, and
Kevin Chetty. Re-weighted adversarial adaptation network
for unsupervised domain adaptation. In CVPR, 2018. 1

[8] Ivana Chingovska, Andr´e Anjos, and S´ebastien Marcel.
On the effectiveness of local binary patterns in face anti-
spooﬁng. In BIOSIG, 2012. 5, 6

[9] Litong Feng and et al. Integration of image quality and mo-
tion cues for face anti-spooﬁng: A neural network approach.
Journal of Visual Communication and Image Representation,
38:451–460, 2016. 7

[10] Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi
Zhou. Joint 3D face reconstruction and dense alignment with
position map regression network. In ECCV, 2018. 5

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 6

[12] Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen.
Duplex generative adversarial network for unsupervised do-
main adaptation. In CVPR, 2018. 1

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In arXiv preprint arXiv:1412.6980,
2014. 6

[14] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M.
Hospedales. Deeper, broader and artier domain generaliza-
tion. In ICCV, 2017. 2, 6

[15] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot.
Domain generalization with adversarial feature learning. In
CVPR, 2018. 2, 7

[16] Siqi Liu, Xiangyuan Lan, and Pong C. Yuen. Remote photo-
plethysmography correspondence feature for 3D mask face
presentation attack detection. In ECCV, 2018. 1, 2

[17] Siqi Liu, Pong C. Yuen, Shengping Zhang, and Guoying
Zhao. 3D mask face anti-spooﬁng with remote photoplethys-
mography. In ECCV, 2016. 2

[18] Yaojie Liu, Amin Jourabloo, and Xiaoming Liu. Learning
deep models for face anti-spooﬁng: Binary or auxiliary su-
pervision. In CVPR, 2018. 1, 2, 5, 6, 7

[19] Jukka M¨a¨att¨a, Abdenour Hadid, and Matti Pietik¨ainen. Face
spooﬁng detection from single images using micro-texture
analysis. In IJCB, 2011. 2, 6, 7

[20] Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bul,
Barbara Caputo, and Elisa Ricci. Boosting domain adap-
tation by discovering latent domains. In CVPR, 2018. 1

[21] Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gi-
anfranco Doretto. Uniﬁed deep supervised domain adapta-
tion and generalization. In ICCV, 2017. 2

[22] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-

ing. In TKDE, 2010. 2

[23] Tiago Freitas Pereira and et al. Face liveness detection using
dynamic texture. In EURASIP Journal on Image and Video
Processing, (1): 1-15, 2014. 1, 2, 6

[24] Pedro O. Pinheiro. Unsupervised domain adaptation with

similarity learning. In CVPR, 2018. 1

[25] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-
suya Harada. Maximum classiﬁer discrepancy for unsuper-
vised domain adaptation. In CVPR, 2018. 1

[26] Rui Shao, Xiangyuan Lan, and Pong C. Yuen. Deep con-
volutional dynamic texture learning with adaptive channel-
discriminability for 3D mask face anti-spooﬁng.
In IJCB,
2017. 1, 2

[27] Rui Shao, Xiangyuan Lan, and Pong C. Yuen. Feature
constrained by pixel: Hierarchical adversarial deep domain
adaptation. In ACM MM, 2018. 1

[28] Rui Shao, Xiangyuan Lan, and Pong C. Yuen. Joint discrim-
inative learning of deep dynamic textures for 3D mask face
anti-spooﬁng.
In IEEE Trans. Inf. Forens. Security, 14(4):
923-938, 2019. 1, 2

[29] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrel-
l. Adversarial discriminative domain adaptation. In CVPR,
2017. 1

[30] Riccardo Volpi, Pietro Morerio, Silvio Savarese, and Vittorio
Murino. Adversarial feature augmentation for unsupervised
domain adaptation. In CVPR, 2018. 1

[31] Di Wen, Hu Han, and Anil K. Jain. Face spoof detection
with image distortion analysis. In IEEE Trans. Inf. Forens.
Security, 10(4): 746-761, 2015. 1, 2, 5, 6, 7

[32] Jianwei Yang, Zhen Lei, and Stan Z. Li. Learn convolution-
al neural network for face anti-spooﬁng. In arXiv preprint
arXiv:1408.5601, 2014. 1, 2, 6, 7

[33] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogun-
bona. Importance weighted adversarial nets for partial do-
main adaptation. In CVPR, 2018. 1

[34] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Col-
laborative and adversarial network for unsupervised domain
adaptation. In CVPR, 2018. 1

[35] Zhiwei Zhang and et al. A face antispooﬁng database with

diverse attacks. In ICB, 2012. 5, 6

10031

