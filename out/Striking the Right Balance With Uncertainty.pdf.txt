Striking the Right Balance with Uncertainty

Salman Khan∗ Munawar Hayat∗

Syed Waqas Zamir

Jianbing Shen†

Ling Shao

Inception Institute of Artiﬁcial Intelligence, UAE

firstname.lastname@inceptioniai.org

Figure 1: Imbalance learning with Bayesian
uncertainty estimates. (a) We enforce class-
level margin penalty based on class uncer-
tainty. This pushes boundaries further away
from rare classes B and C. (b) We also con-
sider sample-level uncertainty that is mod-
eled as a Gaussian distribution. The learned
margins consider the conﬁdence level of
classiﬁer to re-adjust boundaries (i.e., pro-
vide more room to uncertain samples). This
improves the generalization ability of the
proposed model for imbalanced classes.

Abstract

1. Introduction

Learning unbiased models on imbalanced datasets is a
signiﬁcant challenge. Rare classes tend to get a concen-
trated representation in the classiﬁcation space which ham-
pers the generalization of learned boundaries to new test
examples. In this paper, we demonstrate that the Bayesian
uncertainty estimates directly correlate with the rarity of
classes and the difﬁculty level of individual samples. Sub-
sequently, we present a novel framework for uncertainty
based class imbalance learning that follows two key in-
sights: First, classiﬁcation boundaries should be extended
further away from a more uncertain (rare) class to avoid
over-ﬁtting and enhance its generalization. Second, each
sample should be modeled as a multi-variate Gaussian dis-
tribution with a mean vector and a covariance matrix de-
ﬁned by the sample’s uncertainty. The learned boundaries
should respect not only the individual samples but also their
distribution in the feature space. Our proposed approach
efﬁciently utilizes sample and class uncertainty informa-
tion to learn robust features and more generalizable classi-
ﬁers. We systematically study the class imbalance problem
and derive a novel loss formulation for max-margin learn-
ing based on Bayesian uncertainty measure. The proposed
method shows signiﬁcant performance improvements on six
benchmark datasets for face veriﬁcation, attribute predic-
tion, digit/object classiﬁcation and skin lesion detection.

∗Equal contribution, †Corresponding author

Objects, events, actions and visual concepts appear with
varying frequencies in real world imagery [38]. This of-
ten leads to highly skewed datasets where a few abundant
classes outnumber several rare classes in a typical long-
tail data distribution. The low amount of training data for
infrequent classes makes it challenging to learn optimal
classiﬁcation boundaries in the feature space. Existing ap-
proaches to tackle class imbalance either modify data distri-
bution [42, 7, 22] or introduce appropriate costs to re-weight
class errors [25, 1, 40]. The popular data-level approaches
are prone to over-ﬁtting while the cost-sensitive learning
requires careful choice of weights for successful training.
Despite an overwhelming success of deep neural networks
on computer vision problems, learning from highly imbal-
anced sets is still an open problem for deep learning [24].

This paper proposes a new direction towards learn-
ing balanced representations using deep neural networks
(Fig. 1). We use a principled approach to integrate Bayesian
uncertainty estimates for class imbalance learning at two
distinct levels, i.e., category-level and individual sample-
level. Our approach is based on the observation that rare
classes have higher uncertainty in the prediction space and
the associated classiﬁer conﬁdence levels are low. There-
fore, the uncertainty estimates can be used to expand deci-
sion regions for less frequent classes so that classiﬁer’s gen-
eralization to new test examples is improved. This concept
is illustrated in Fig. 1. Since all samples within a class do

1103

Class CClass BClass ABoundaries learnedusingsoft-max lossBoundaries learnedwith our cost functionSample distributionClass density distributionClass A SampleClass B SampleClass uncertaintynot have a uniform difﬁculty level, our approach also op-
timizes margins with respect to the uncertainty associated
with individual samples. The basic intuition for both cases
is the same: a classiﬁer should assign larger regions to more
uncertain (rare) samples/classes.
Related work: State-of-the-art deep imbalance learning
methods mainly propose novel objective functions [17].
Khan et al. [25] presented a cost-sensitive loss for CNNs
where class-speciﬁc weights were automatically learned.
Huang et al. [21] suggested a combination of triplet and
quintuplet losses to preserve local class structures. A mod-
iﬁed softmax was proposed in [31] to maximize the angu-
lar margin, thus avoiding class imbalance. Quite recently,
a meta-learning approach in [40] used selective instances
for training on imbalanced sets. These methods have their
respective limitations, e.g., [25] only considers class-level
costs, [21] is not differentiable and requires heavy pre-
processing for quintuplet creation, [31] can only maximize
margin on the hypersphere surface and [40] used an addi-
tional validation set to assign sample weights. Concurrent
to this work, [10] re-weights the loss by the inverse effective
number of samples to learn balanced representations.
Contributions: Our approach is distinct in two ways: (a)
this is the ﬁrst work to link class imbalance with Bayesian
uncertainty estimates [16], that have shown great promise
on other tasks [23, 15, 52], and (b) we incorporate both class
and sample-level conﬁdence estimates to appropriately re-
shape learned boundaries. The paper therefore introduces
(1) A principled margin-
the following major novelties.
enforcing formulation for softmax loss, underpinned by the
Bayesian uncertainty estimates. (2) Sample modeling using
multi-variate Gaussian distributions. The class boundaries
are optimized to respect second order moments which im-
proves generalization. (3) A fully differentiable loss formu-
lation that can be easily plugged into existing architectures
and used alongside other regularization techniques.

2. The Imbalance Problem

We begin with an in depth analysis of the imbalance
problem and draw several insights which lead to our pro-
posed framework. We base our analysis around softmax
loss, which is the most popular objective function for clas-
siﬁcation. For brevity, we consider a simplistic case of bi-
nary classiﬁcation with two classes A and B in the training
set denoted by image-label pairs: D = {xk, yk}K
k=1. The
goal is to learn an optimal set of ‘representative vectors’
(wA, wB for classes A and B, respectively) that lead to
minimal empirical loss on set D. The Class-Representative
Vectors (CRV) deﬁne a loss-minimizing hyper-plane ‘w’,
which is the boundary between two classes, i.e., given a
feature projection ‘f ’ corresponding to an input image x,
f ∈ w iff (wA − wB)f = 0 (ignoring unit biases). The
class imbalance problem exists when the class frequencies

Figure 2: Illustration for the class imbalance problem. True
class distributions are shown in green and red. Unbalanced
distributions lead to a skewed classiﬁcation boundary that is
biased towards the minority class.

τA, τB are greatly mismatched in the set D. As illustrated in
Fig. 2, in such cases, the hypothesis (w) learned on D using
a softmax loss can be biased towards the minority class and
signiﬁcantly different from the ideal separator (w∗). Next,
we breakdown the imbalance problem and explain underly-
ing reasons.

2.1. Bias due to Empirical Loss Minimization

We consider w∗ to be an optimal boundary obtained by
loss minimization with respect to the actual hidden distribu-
tions PA and PB of classes, i.e.:

w∗ = arg min
w

LP (w), where,

LP (w) =ZRw

B

PA(f )df +ZRw

A

PB(f )df ,

(1)

A , Rw

and Rw
B denote the classiﬁcation regions for classes A
and B, respectively. Given D, the empirical loss calculated
on the training set is:

LD(w) =#{xk|f k ∈ DA ∧ f k ∈ Rw
B }+
#{xk|f k ∈ DB ∧ f k ∈ Rw
A }

(2)

Further, assume that the normalized class frequencies τA
and τB are related as τA + τB = 1. Then, the expected
empirical loss for any hypothesis w is:

E[LD(w)] = τAZRw

B

PA(f )df + τBZRw

A

PB(f )df .

(3)

Note that τA 6= τB 6= 0.5 due to class imbalance and typ-
ically |τA − τB| > 0.5 in practical cases where a signiﬁ-
cant imbalance ratio exists. Next, we show that when large
imbalance exists, the learned classiﬁcation boundaries are
biased towards minority classes.

Theorem 1. For high imbalanced ratios, minimization of
empirical loss results in a hypothesis ˆw that is highly likely
to be biased towards the minority class ‘z’ such that Rw∗
z >
R ˆw
z . In other words, the classiﬁcation region induced by the
optimal separator is larger than the one induced by empiri-
cally learned boundary.

104

𝒘𝒘∗Ideal separatorBiased separator𝑤𝐴𝑤𝐵𝑤𝐴𝑤𝐵𝒘∗Ideal separatorBalanced Class Distributions in Training SetImbalanced Class Distributions in Training SetProof. According to Eq. 3, due to the imbalanced propor-
tion among classes, w∗ is more likely to incur higher empir-
ical error than an alternate hypothesis based on an empirical
loss, i.e., for any ˆw : Rz

ˆw, it is more likely that:

w∗ >Rz

LD(w∗) > LD( ˆw) because τAZRw
τBZRw

PB(f )df > τAZR ˆw

B

B

A

∗

PA(f )df + τBZR ˆw

A

PA(f )df +

∗

PB(f )df .

(4)

z > R ˆw

Then, for a signiﬁcant imbalance ratio such that τz << τrest,
it directly follows that Rw∗
z . Intuitively, this is a
natural implication of imbalanced class distribution which
forces the classiﬁer to shift ˆw closer to minority classes be-
cause it reduces empirical error. The likelihood of classiﬁer
bias is directly proportional to the imbalance rate.

A common strategy to tackle data imbalance is through
the introduction of cost-sensitive loss functions [25]. We
brieﬂy elaborate on the effect of these losses next and ex-
plain why this solution is sub-optimal.

2.2. Cost sensitive Loss

From Eq. 4, one simple solution seems to be the in-
troduction of costs to re-weight the minority class errors.
Existing cost-sensitive losses (particularly those based on
deep-networks [25]) adopt this idea and assign score-level
penalty to the minority class predictions. This means that
the classiﬁer is forced to correctly classify training samples
belonging to minority classes. This approach has certain
limitations. (1) Appropriately tuning class speciﬁc costs is a
challenging task as it requires domain-knowledge with costs
usually ﬁxed at the beginning and not dynamically changed
during the course of training. (2) A more stringent caveat is
that such costs do not affect the learned boundaries ˆw if the
training samples are separable [53, 25]. Further, when the
classes are non-separable, minority class representation in
the dataset is directly proportional to its mis-classiﬁcation
(3) Generally,
these costs are not applied at test time and therefore class-
boundaries are effectively unchanged. In summary, while
this practice enforces the classiﬁer to more accurately clas-
sify training samples belonging to minority classes, it does
not enhance generalization capability of the learned model.
This can be understood from the relation for generalization
error and Fig. 2. For an empirical distribution Q for classes
A and B, the generalization error (expected loss on the test
set T ) is:

probability, i.e.,: τz ∝ RR ˆw

Pz(f )df .

rest

E[LT (w)] = τ ′

AZRw

B

QA(f )df + τ ′

BZRw

A

QB(f )df ,

(5)

where τ ′
B are the normalized frequencies on the test set.
The paper aims to overcome these existing limitations by

A, τ ′

Figure 3: Top: One-dimensional Gaussian process regres-
sion using maximul-likelihood estimation. The lack of ob-
servations results in higher conﬁdence levels. Bottom: The
uncertainty estimates for imbalanced CIFAR-10 dataset.
The uncertainty is higher for classes with less representa-
tion.

proposing a new loss formulation that seeks to simultane-
ously extend minority class boundaries and enforce mar-
gin constraints on less represented classes to achieve bet-
ter generalization performance. We provide details of our
technique in the next section.

3. Bayesian Uncertainty Estimates

Bayesian models can provide uncertainty estimates
alongside output predictions. Given an input, the uncer-
tainty estimates correspond to the conﬁdence level for each
outcome predicted by the model. We hypothesize that the
conﬁdence-level of predictions is directly related to the
class representation in the training set. As illustrated in
Fig. 3, under-represented classes in the training set lead to
higher uncertainty and bigger conﬁdence intervals. In con-
trast, well-represented classes are associated with less un-
certainty and compact conﬁdence intervals.

We use deep CNNs with dropout to obtain Bayesian un-
certainty estimates. It has been proved that dropout-based
deep networks provide an approximation to Gaussian pro-
cess [16]. A Gaussian process is a Bayesian technique be-
cause it constructs a prior distribution over a family of func-
tions F [39]. This distribution is updated conditioned on
observations, i.e., all the functions that are consistent with
the labels are retained. At inference time, an output is ob-

105

0246810x10505101520f(x)Regression: Point Density and Uncertaintyf(x)=xcos(x)ObservationsPrediction95% confidence intervalC1C2C3C4C5C6C7C8C9C10Class Numbers0123ScoresClassification: Class Frequency and UncertaintyUncertaintyFrequencytained from each of the functions and expectation is com-
puted to generate the ﬁnal prediction. The variance of these
outputs gives an uncertainty estimate. In the following, we
ﬁrst provide an overview of dropout and then describe un-
certainty computation using dropout.

Dropout: Dropout was originally proposed as a regu-
larization measure for deep neural networks [46]. During
training, a sub-network is sampled from the full network
by randomly dropping a set of neurons.
In this manner,
each neuron is activated with a ﬁxed probability ‘p’. At
test time, full model is used for prediction and the output
activations are multiplied with the probability p to obtain
expectation. Suppose the network parameters are denoted
by Θ = {θ1, . . . , θL} for a total of L network layers. Then,
by applying masks m generated using i.i.d binary distribu-
tions, we can obtain N samples all corresponding to differ-
ent network conﬁgurations ˆΘ that form an ensemble M:

M ={ ˆΘi : i ∈ [1, N ]}, where, ˆΘi = Θ ◦ mi,
mi ={ml : l ∈ [1, L]}, s.t., ml ∼ Bernoulli(p)

(6)

Uncertainty: For each input xk, N model conﬁgura-
tions are applied to obtain a set of outputs {ˆy}. The ex-
pected output is calculated using Monte Carlo estimate for
i=1 ˆy(x; ˆΘi), where
ﬁrst moment (Eq(y|x)[y]): y ≈ 1
q denotes an output distribution that approximates the in-
tractable posterior distribution of deep Gaussian process.
The uncertainty is calculated using the second moment
(Vq(y|x)[y]) through Monte Carlo estimation:

N PN

u ≈ τ −1ID +

1
N

N

Xi=1

ˆyT ˆy − Eq(y|x)[y]T Eq(y|x)[y],

(7)

where τ is the model precision (a function of weight decay)
and IC ∈ RC×C is an identity matrix where C denotes the
number of classes.

4. Uncertainty based Max-margin Learning

The softmax loss can be computed for a given feature f

and its true class label y as follows:

Lsm = − log(cid:16) exp(wT
Pj exp(wT

y f )

j f )(cid:17),

(8)

where j ∈ [1, C] (C is the number of classes). In the above
loss formulation, we include the last fully connected layer
within softmax loss which will be useful for our analysis
later on. Further, for the sake of brevity, we do not mention
bias in Eq. 8. Note that the dot-product wT
y f can also be ex-
pressed as wT
y f = kwykkf k cos(αy). Therefore, if a class
z is rare in the training set, for an input feature belonging to
this class, the softmax loss enforces:

kwzkkf k cos(αz) > kwrestkkf k cos(αrest).

(9)

Intuitively, we would like to impose a large margin on more
uncertain classes. Our experiments show that the class un-
certainty is inversely proportional to its frequency in the
training set, i.e., rare classes are more uncertain (Fig. 3).
To improve generalization performance, we can impose a
more strict constraint for uncertain classes:

kwzkkf k cos(mzαz) > kwrestkkf k cos(mrestαrest),

where, m = max(1, ⌊0.5uy⌋), uz > urest, 0 ≤ αz ≤ π
uz
and uz ∈ R+. This implies that the classiﬁer will try to
separate rare classes by a more rigorous margin. The margin
maximizing softmax loss [32] is deﬁned as:

L′

sm = − log(cid:16) exp(kwykkf kψ(αy))
Pj exp(kwjkkf kψ(αj))(cid:17),

where ψ(·) is a continuous and monotonically decreasing
function in the range [0, π]:

(10)

ψ(αj) =((−1)r cos(mαj)−2r αj=y ∈h rπ

cos(αj)

j 6= y,

m i
m , (r+1)π

where r ∈ [0, m−1] is an integer. The gradient back-
propagation requires relations in terms of w and f , there-
fore we substitute cos(mαj) with its expansion in terms of
Chebyshev polynomials of the ﬁrst kind (Tm), i.e.,

cos(mαj) =

⌊m/2⌋

Xt=0 (cid:16) m

2t (cid:17)(cos2(αj) − 1)m cos(αj)m−2t.

Here, cos(αj) is substituted with
kwj kkf k . This gives us the
max-margin formulation in terms of differentiable relations.

wT

j f

5. Sample-level Uncertainty Modeling

Although uncertainty driven class-level margin enforce-
ment is important, not all samples in a class have equal
difﬁculty level. The samples that can potentially be mis-
classiﬁed have larger uncertainties. We therefore propose a
mechanism to incorporate sample-level uncertainty for im-
balanced learning. Existing classiﬁcation networks only use
the mean representation from a distribution of samples to
represent each training example. Inspired by [51], we pro-
pose to represent a single sample as a function of its ﬁrst and
second order moments. To this end, consider that the deep
feature representations of input media is randomly sampled
from a multi-variate Gaussian distribution: f ∼ N (µf , Σf ),
where µf and Σf , respectively denote mean and covariance
of the features. The softmax loss can be computed for a
given feature f and its true class label y using Eq. 8.

For an input feature to be correctly classiﬁed, its pro-
jection on the true class vector should give a maximum re-
sponse (Eq. 9). In contrast, an example is classiﬁed into a

106

wrong category k if the following condition holds true:

6. Experiments

∃j ∈ [1, C] s.t., wT

y f < wT

j f ,

j 6= y.

(11)

6.1. Datasets

We are interested in quantifying the probability of misclas-
siﬁcation taking into account the distribution of each sam-
ple. It can provide a measure of conﬁdence for loss esti-
mates computed on the training samples. Direct computa-
tion of the misclassiﬁcation probability using softmax loss
in Eq. 8 is intractable and can only be approximated. There-
fore, we introduce a simpler error function that models the
essential loss behavior.

E(f ) = wT

j f − wT

y f ,

j 6= y.

(12)

The formulation can be used to exactly compute the mis-
classiﬁcation probability as we will show next. Since the
error function is a linear transformation of input feature
f ∼ N (µf , Σf ), the resulting error distribution is also a
uni-variate Gaussian variable. The ﬁrst and second order
statistics of the error distribution can be given in terms of
µf , Σf as follows:

µE = E[E(f )] = (wj − wy)T µf
E = E[(E(f ) − µE )2] = (wj−wy)T Σf (wj−wy) (13)
σ2

Now, the misclassiﬁcation probability for a feature f can
be linked with error distribution because E(f ) > 0 denotes
a misclassiﬁcation. The complementary cumulative proba-
bility distribution function (CCDF) ˆFE is given as follows:

ˆFE (0) =P(E(f ) > 0) = 1 − P(E(f ) < 0)

=1 − P(cid:16) E(f ) − µE

σE

E(f ) − µE

< −

µE

σE(cid:17)

∼ N (0, 1)

if z =

σE

ˆFE (0) =1 − Fz(−

µE
σE

) =

1

2(cid:16)1 + erfh µE
p2σ2

Ei(cid:17),

(14)

where Fz denotes the cumulative probability distribution
function (CDF). The probability estimates are then used to
re-weight the loss values such that uncertainty is incorpo-
rated. The function ψ(·) is modiﬁed as follows to obtain an
improved loss function in Eq. 10:

ψ(αj) =(ˆFE (0)((−1)r cos(mαj) − 2r) αj=y ∈ h rπ

cos(αj)

j 6= y

m i
m , (r+1)π

(15)

The loss deﬁned above enforces a margin (m) between
output predictions weighted by the cumulative probability
(ˆFE (0)). A higher uncertainty means a stricter margin based
penalty for a class j. The modiﬁed loss function becomes
equal to the original softmax loss when the uncertainty is
zero and m = 1.

Face Recognition: Facial recognition datasets commonly
exhibit large-imbalance which poses a signiﬁcant challenge
for classiﬁer learning.
Following [12], we use VGG2
dataset [5] with 3,141,890 images of 8,631 subjects to
train our deep network. We evaluate the trained model on
four large-scale datasets namely Labeled Faces in the Wild
(LFW) [27] and YouTube Faces (YTF) [57], AgeDB [37]
and Celebrities in Frontal Proﬁle (CFP) [41]. LFW [27]
contains 13,233 web-collected images belonging to 5,749
different identities, with large variations in pose, expression
and illumination. We follow the standard protocol of ‘un-
restricted with labeled outside data’. YTF [57] has 3,425
sequences of 195 subjects. We follow the standard evalu-
ation protocol on 5,000 video pairs. AgeDB [37] dataset
has 12,240 images of 440 subjects. The test set is divided
into four groups with different year gaps (5, 10, 20 and 30
years). We only report the performance on the most chal-
lenging subset, AgeDB-30. CFP [41] has 500 subjects in to-
tal, each with 10 frontal and 4 proﬁle images. In this paper,
we only evaluate on the most challenging subset CFP-FP.
Skin Lesion Classiﬁcation: Edinburgh Dermoﬁt Image
Library (DIL) consists of 1,300 high quality skin lesion
images based on diagnosis from dermatologists and der-
matopathologists. There are 10 types of lesions identiﬁed
in this dataset including melanomas, seborrhoeic keratosis
and basal cell carcinomas. The number of images in each
category varies between 24 and 331 (mean 130, median 83).
Similar to [3], we report results with 3-fold cross validation.
Digit/Object Classiﬁcation: We evaluate on imbalanced
MNIST and CIFAR-10 datasets for generic digit and object
classiﬁcation. Standard MNIST consists of 70,000 images
of handwritten digits (0-9). Out of these, 60,000 images
are used for training (600/class) and the remaining 10,000
for testing (100/class). CIFAR-10 contains 60,000 images
belonging to 10 classes (6,000 images/class). The standard
train/test split for each class is ∼83.3%/16.7% images. We
evaluate our approach on the standard split as well as on
an artiﬁcially created imbalanced split. To imbalance the
training distribution, we randomly drop 90% of the samples
for half of the classes.
Attribute Prediction: We use the large-scale CelebA
dataset [33] for (multi-label) facial attribute prediction task.
This dataset consists of 202,599 images belonging to 10,177
human identities. Each image is annotated with a diverse
set of 40 binary attributes. There exists a signiﬁcant im-
balance in the training set with ratios up to 1:43. Following
the standard protocol [33], we use 152,770 images for train-
ing, 10,000 for validation, and remaining 19,867 for testing.
For evaluation, we report Balanced Classiﬁcation Accuracy
(BCA) deﬁned as: BCA = 0.5× tp
, where tp
Np

+ 0.5× tn
Nn

107

and tn, respectively denote true positives and true nega-
tives, and Np and Nn are total number of positive and neg-
ative samples. This evaluation metric is more suitable for
multi-label imbalanced learning tasks since it gives equal
weight to both majority and minority classes. Other evalu-
ation metrics used in the literature [33] which deﬁne accu-
racy as tp+tn
Np+Nn

can be biased towards majority classes.

6.1.1

Implementation Details

The uncertainty estimates are applied progressively during
training. We start with standard softmax (m = 1), fol-
lowed by class-level uncertainty based max-margin learn-
ing and ﬁnally sample-level uncertainty modeling during
the last 10 epochs. The compute intensive sample-level un-
certainty estimates are therefore only done for few epochs.
The proposed strategy can be related with curriculum learn-
ing, since it starts with a simple task by considering a bal-
anced class distribution, and gradually introduces harder
tasks by expanding or shrinking classiﬁcation boundaries of
different classes based upon their uncertainty estimates. For
attribute prediction on CelebA dataset, the training times
required for standard softmax and ours are ∼3.4 and 4.6
hours, respectively, on a Dell Precision 7920 machine with
TitanXp GPU. In our experiments, we ﬁxed m = 3 since
it gives relatively better results. Experiments on imbal-
anced CIFAR-10 for m = 2, 3, 4 achieve an accuracy of
80.2%, 80.6%, 80.5%, respectively. Values of N ≥ 5 give
stable uncertainty estimates. We ﬁxed N = 10 for the
optimal trade-off between reliable uncertainty and com-
pute efﬁciency. An ablation study on different values of
N = 5, 10, 20, 40 on imbalanced CIFAR-10 results in re-
spective accuracies of 80.4%, 80.6%, 80.6%, 80.7%.

For Skin Lesion detection, we deploy ResNet-18 back-
bone with two fully connected layers (with intermediate
rectiﬁed linear units non-linearities and dropout) inserted
after the global pooling layer. For face veriﬁcation tasks,
we train Squeeze and Excitation (SE) networks [20] with
ResNet-50 backbone. The face images are pre-processed to
112 × 112 using multi-task cascaded CNN [61]. After the
network is trained on VGG-2 dataset, we use features ex-
tracted after global pooling layer for face veriﬁcation evalu-
ations. On imbalanced MNIST dataset, we use the same set-
tings as in [25] to enable direct comparison with the recently
proposed imbalanced learning technique [25]. For experi-
ments on imbalanced CIFAR-10 dataset, we extract features
from VGG16 [44] pre-trained on ImageNet [11]. A simple
neural network with two hidden layers (512 neurons each)
with dropouts is trained on the extracted features to get un-
certainty estimates and perform classiﬁcation. Training a
network on VGG extracted features enables us to compare
against traditional imbalanced learning techniques. Specif-
ically, data level under-sampling & over-sampling methods

Methods using
non-public data

Novel Loss
Functions

Imbalance
Learning

Methods

LFW YTF

DeepFace [49]
FaceNet [43]
Web-scale [50]
DeepID2+ [48]
Baidu [30]
Center Face [56]
Marginal Loss [13]
Noisy Softmax [8]

Softmax+Contrastive [47]
Triplet Loss [43]
Large-Margin Softmax [32]
Center Loss [56]
SphereFace [31]
CosFace [55]

Range Loss [63]
Augmentation [36]
Center Inv. Loss [58]
Feature transfer [60]
LMLE [21]

97.35
99.63
98.37
99.47
99.13
99.28
99.48
99.18

98.78
98.70
99.10
99.05
99.42
99.33

99.52
98.06
99.12
99.37
99.51

91.4
95.4

-

93.2

-

94.9
96.0
94.9

93.5
93.4
94.0
94.4
95.0
96.1

93.7

-

93.9

-

95.8

This Paper

99.71

97.3

Table 1: Face Veriﬁcation Performance on LFW and YTF
datasets. We trained our model on VGG2 dataset. Most
methods in the ﬁrst cell use large-scale outside data that are
not publicly available. The second cell includes novel loss
functions. The state-of-the-art imbalanced learning meth-
ods are in the last group.

in Table 5 are used on VGG features, followed by the two
layered NN for classiﬁcation. For attribute prediction on
CelebA dataset, we train a model with ResNet-50 backbone
and two fully connected layers with dropout inserted after
the global pooling and the ﬁrst fully connected layers. The
model is trained to minimize sum of binary cross entropy
losses, using relatively smaller learning rates for the layers
before the global pooling layer and larger rates for the layers
inserted afterwards.

6.2. Results and Comparisons

Face Veriﬁcation: We compare our approach with 20
recent and top-performing methods on LFW and YTF
datasets (Table 1). We divide these methods into three cate-
gories: (a) methods that use large amounts of non-publicly
available data sources to train their models, (b) methods
that design novel loss functions for face veriﬁcation and (c)
methods that deal with data imbalance. We note that the per-
formances on both LFW and YTF are currently saturated
with many recent methods already surpassing human per-
formance. Our method achieves competitive performance
on both these datasets. Note that some of the compared
methods used as much as 200M images [43] and an ensem-
ble of 25 models [48] for training. Further evaluations on
additional datasets show that the proposed method achieves
veriﬁcation accuracies of 97.0% and 94.4% on CFP-FP and

108

Methods

Imbalanced

Performances

Split

Exp. 1 (5-classes) Exp. 2 (10-classes)

Hierarchical-KNN [3]
Hierarchical-Bayes [2]
Flat-KNN [3]

Baseline CNN [25]
CoSen CNN [25]

This paper

✓

✓

✓

✓

✓

✓

74.3 ± 2.5%
69.6 ± 0.4%
69.8 ± 1.6%

75.2 ± 2.7%
80.2 ± 2.5%
95.7 ± 1.2%

68.8 ± 2.0%
63.1 ± 0.6%
64.0 ± 1.3%

69.5 ± 2.3%
72.6 ± 1.6%
86.9 ± 0.7%

Table 2: Experimental results for Skin Lesion Classiﬁcation
on the DIL dataset.

Methods

Imbalanced Split Performances

Deeply Supervised Nets [29]
Generalized Pooling Func. [28]
Maxout NIN [6]

Baseline CNN [25]
CoSen CNN [25]
This paper

✗

✗

✗

✓

✓

✓

99.6%
99.7%
99.8%

97.1%
98.4%
98.7%

Table 3: Results for digit classiﬁcation on MNIST.

AgeDB-30 datasets, respectively.
Skin Lesion Detection: The results for skin lesion detec-
tion are listed in Table 2. We perform a 3-fold cross val-
idation to report the results. Compared to a recent cost-
sensitive CNN approach [25], we obtain an impressive ab-
solute performance gain of 15.5% and 14.3% on Experi-
ments 1 (5 classes) and 2 (10 classes), respectively.
Digit Classiﬁcation: For hand-written digit classiﬁcation,
we report our results on an imbalanced split of MNIST
dataset in Table 3. For the sake of comparison, we also re-
port some representative methods on the original balanced
split of MNIST. However, the two set of techniques are not
directly comparable since the bottom group uses ∼ 45%
less training data. Our technique outperforms other imbal-
ance learning approaches.
Attribute Prediction: We report the multi-label attribute
prediction results on CelebA dataset in Table 4. This dataset
is particularly challenging as it exhibits a high imbalance
with majority-to-minority ratio up to 1:43. We compare
with nine recent state-of-the-art methods. These methods
include techniques that speciﬁcally focus on class imbal-
ance learning (Table 4, right block). Our approach performs
signiﬁcantly better compared to both normal and class im-
balance learning methods. Speciﬁcally, we achieve top-
most accuracy in 23/40 attributes and second-best accuracy
in other 8/40 classes.
In particular, since our method fo-
cuses on assigning a larger classiﬁcation region to under-
represented classes, we achieve more pronounced boost for
the case of highly imbalanced classes. For example, out of
the top 50% most imbalanced classes (with fewer samples),
we achieve best performance in 16/20 (80%) cases. Quali-
tative examples for attribute prediction are shown in Fig. 4.
Object Classiﬁcation: We compare against a number of
popular imbalanced learning approaches including both

l
e
v
e
l

e
c
n
a
l
a
b
m

I

]
3
4
[

N
N
k
-
t
e
l
p
i
r
T

]
7
4
[
2
D
I
p
e
e
D

]
2
6
[

a
d
n
a
P

]
4
3
[

t
e
N
A

]
4
1
[
g
n
i
l
p
m
a
s
-
r
e
v
O

]
4
1
[
g
n
i
l
p
m
a
s
-
r
e
d
n
U

]
9
[

.
j
d
A
d
l
o
h
s
e
r
h
T

]
9
1
[

e
v
i
t
i
s
n
e
s
-
t
s
o
C

]
1
2
[
E
L
M
L

r
e
p
a
p
s
i

h
T

83 85
92
92
91
86 89
91

87 78
1
93 96 89
2
98 97 89
2
97 95 92
3
89 84
5
99 99 94
8
96 88
11 88 95
18 77 78 81 73
22 61 66 67 63
69 66
22 61 67
23 73 77 76 77
90 83
26 82 84
26 55 56 57 62
27 68 72 78 73
84 76
28 75 78
29 63 66 69 65
30 76 85 83 79
30 63 67 70 74
83 75
31 69 77
93 88
33 82 87
35 81 92 90 91
35 81 91 90 90
36 68 74 82 78
38 50 51 59 70
38 47 51 57 64
85
39 66 76 81
42 60 67 70 81
43 73 85 79 83
44 82 88 95 92
44 64 68 76 86
44 73 84 86 90
44 64 65 70 81
44 71 81 79 89
45 43 50 56 74
45 84 90 90 90
45 60 64 68 83
46 63 69 77 81
46 72 79 85 90
46 57 63 61 88
48 75 74 73

76 74 73

66 61 60

77 78 69 78
89 87 89 89
90 90 88 90
92 91 89 91
84 80 83 85
95 90 95 93
87 89 89 89
70 70 77 75
72 64
63 58
72 65
67 63
79 70 76 78
84 80 86 85
61 61
73 76 76 74
75 80 24 75
73 67
66 61
82 76 81 84
73 71
76 70 76 76
88 88 15 88
90 88 93 90
90 85 92 89
84 79 82
80 75
71 59
71 66 62
71 65 59
65 61
85 82 82 84 82
83 81 76
82 79
79 80 76 82
91 85 95 91
90 82 82
89 85 89 86
83 78 81 82 79
90 80 89
90 88
78 76 59
76 68
89 90 95 90
84 80 83
82 78
90 88
90 60 86 88 73
93 93 90

88 77
96 93
99 90
99 94
92 83
99 98
98 91
83 84
68 67
72 72
79 83
92 90
69
80 79
87 72
73 81
87 86
83
83 89
96 85
98 95
99 95
87
76
75
91
86
90 92
98 99
92 78 89
95 96
87
95
87
99 97
85
89
91 94
91
95

84 74
85 80 80
91 90

93 92 79

Attributes

Attractive
Mouth Open
Smiling
Wear Lipstick
High Cheekbones
Male
Heavy Makeup
Wavy Hair
Oval Face
Pointy Nose
Arched Eyebrows
Black Hair
Big Lips
Big Nose
Young
Straight Hair
Brown Hair
Bags Under Eyes
Wear Earrings
No Beard
Bangs
Blond Hair
Bushy Eyebrows
Wear Necklace
Narrow Eyes
5 o’clock Shadow
Receding Hairline
Wear Necktie
Eyeglasses
Rosy Cheeks
Goatee
Chubby
Sideburns
Blurry
Wear Hat
Double Chin
Pale Skin
Gray Hair
Mustache
Bald

Overall

-

72 77 80 81

82 78 79 82

84 87

Table 4: Multi-label attribute prediction results on CelebA
dataset. The compared methods are divided into two cat-
egories (a) left: methods without class imbalance learning
and (b) right: methods that focus on imbalance learning.

data-level (e.g., SMOTE NN [7], ADASYS NN [18]) and
algorithm-level imbalance removal techniques (e.g., cost-
sensitive SVM [4], NearMiss [35]), on imbalanced CIFAR-
10 dataset (by retaining only 10% of the samples for 50%
classes). Table 5 summarizes our results in terms of a di-

109

Methods

Accuracy

Precision

Performances
F1

Recall

G-Mean

IBA

Cost-sen SVM [4]
NearMiss [35]
SMOTE NN [7]
ADASYN NN [18]
Under-samp. Clustering [59]

Neighborhood Cleaning [26]
Instance Hardness [45]
This Paper

34.2
63.5
76.3
76.4
75.7

76.7
67.8
80.6

60.9±33.6
66.1±14.9
80.5±12.6
80.4±12.2
78.3±12.2
79.2±10.4
76.3±19.8
80.8±6.1

34.3±32.4
63.5±19.7
76.3± 22.1
76.4±21.1
75.7±10.3
76.7±18.6
67.8±18.7
80.6±9.3

30.5±23.0
62.8±14.2
75.0±12.3
75.2±10.6
75.9±6.9
75.9±8.6
67.6±10.8
80.4±6.3

69.7±29.4
79.2±9.5
88.3±6.4
88.2±6.0
87.0±6.8
85.7±10.2
79.9±11.1
88.7±5.2

55.5±30.6
61.8±14.8
77.1±11.9
76.9±11.4
74.8±12.2
73.2±18.1
63.4±17.4
77.6±9.6

deviation

Table 5: Performance on
CIFAR-10.
imbalanced
Standard
on
class-speciﬁc performance
is reported for each metric.
Our method performs bet-
ter compared to others on
a diverse set of evaluation
metrics.

Figure 4: Sample attribute predictions on the CelebA.

Method (↓)

CNN + Softmax loss

Accuracy

97.2

Dropout Rate (→)

0.3

0.5

0.7

CNN + UMM loss
CNN + UMM + SUM (Ours)

98.0
98.3

98.3
98.7

98.2
98.7

Table 6:
Ablation
study on
imbalanced
MNIST
dataset.

verse range of metrics (e.g., accuracy, F1, G-mean, IBA,
precision, recall). These metrics provide a comprehensive
view on the performances of ours and other imbalanced
techniques and are more suitable for imbalanced learning
scenarios. Our results show that the proposed uncertainty
based technique consistently performs better than the tra-
ditional imbalance removal methods. Particularly, for the
case of evaluation metrics that give equal importance to
rare classes (e.g., recall and F1 measure), our approach
achieves signiﬁcant performance boost of 3.9 and 4.5, re-
spectively. Furthermore, the deviation of individual class
scores is much lower compared to other techniques.
Ablation Study: We experiment with different variants of
our approach in Table 6. Speciﬁcally, we report the per-
formance of a simple baseline model with softmax loss and
compare it with the uncertainty-based margin (UMM) en-
forcement and sample-level uncertainty modeling (SUM).
We note a progression in performance from UMM to
UMM+SUM. This is because the SUM penalizes hard ex-
amples which further helps in improving generalizability.
We also study the effect of changing the dropout rate on our
uncertainty-based approach. Increasing the dropout rate be-
yond 0.5 does not help, while a smaller rate of 0.2 or 0.3
results in a performance drop.
Other margin enforcing losses with uncertainty: We also
experiment with recent variants of softmax loss that explic-
itly enforce margin constraints during classiﬁcation along

Loss Type

Settings Original with Uncer.
n

m

Softmax (Baseline)
Additive Margin [54]
Arc Margin [12]
Large Margin [32]
Sphere Product [31]

-
30
30
-
-

-

0.35
0.4
4
4

76.8
78.3
78.1
79.4
76.2

-

78.9
78.4
79.9
77.5

Table 7: The behavior of other recent margin-based loss
formulations with uncertainty. The accuracy values are re-
ported for imbalanced CIFAR-10 dataset. ‘n’, ‘m’ stands
for feature norm and margin, respectively.

with uncertainty estimates (Table 7). These methods in-
clude Additive Margin Softmax [54], Angular (Arc) Margin
Softmax [12], Large Margin Softmax [32] and SphereFace
[31]. As these loss functions have generally been proposed
for face veriﬁcation, we already compare with them in Ta-
ble 1. However, here our main goal is to analyze if the un-
certainty estimates help in learning better boundaries for the
rare classes. To this end, we use exactly the same features
for all techniques and use uncertainty estimates in place
of their original parameter m settings. Since our applica-
tion is different from face veriﬁcation, we note a relatively
lower performance from SphereFace and a higher perfor-
mance from Large Margin Softmax. Overall, the uncer-
tainty scores help in achieving discriminativeness between
difﬁcult classes and gives better performance in all cases.

7. Conclusion

We present a new approach to address class imbalance
problem, underpinned by the Bayesian uncertainty esti-
mates. We demonstrate that the classiﬁer conﬁdence levels
are directly associated with: (a) the difﬁculty-level of indi-
vidual samples and (b) the scarcity of the training data for
under-represented classes. Our proposed approach utilizes
uncertainty to enforce larger classiﬁcation regions for rare
classes and challenging training samples. This results in
better generalization of learned classiﬁer to new samples for
less frequent classes. We achieve signiﬁcant performance
gains on several datasets for face veriﬁcation, attribute pre-
diction, object/digit classiﬁcation and skin lesion detection.

110

Male Bags Under Eyes Mouth Open YoungBushy Eyebrows Black Hair Heavy MakeupAttractiveArched EyebrowsOval FaceEyeglassesBig Lips GoateeBig NoseReceding HairlineGray Hair High Cheekbones Oval FaceSmiling Wavy HairBig NoseChubby Double Chin EyeglassesNo BeardBangsNo BeardStraight Hair YoungBlack Hair References

[1] R. Akbani, S. Kwek, and N. Japkowicz. Applying support
vector machines to imbalanced datasets. In European con-
ference on machine learning, pages 39–50. Springer, 2004.

[2] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees. Non-
melanoma skin lesion classiﬁcation using colour image data
in a hierarchical k-nn classiﬁer.
In Biomedical Imaging
(ISBI), 2012 9th IEEE International Symposium on, pages
358–361. IEEE, 2012.

[3] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees. A color
and texture based hierarchical k-nn approach to the classi-
ﬁcation of non-melanoma skin lesions.
In Color Medical
Image Analysis, pages 63–86. Springer, 2013.

[4] P. Cao, D. Zhao, and O. Zaiane. An optimized cost-sensitive
svm for imbalanced data learning.
In Paciﬁc-Asia Confer-
ence on Knowledge Discovery and Data Mining, pages 280–
292. Springer, 2013.

[5] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman.
Vggface2: A dataset for recognising faces across pose and
age.
In International Conference on Automatic Face and
Gesture Recognition, 2018.

[6] J.-R. Chang and Y.-S. Chen. Batch-normalized maxout net-

work in network. arXiv preprint arXiv:1511.02583, 2015.

[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. Smote: synthetic minority over-sampling tech-
nique. Journal of artiﬁcial intelligence research, 16:321–
357, 2002.

[8] B. Chen, W. Deng, and J. Du. Noisy softmax: Improving
the generalization ability of dcnn via postponing the early
softmax saturation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5372–
5381, 2017.

[9] J. Chen, C.-A. Tsai, H. Moon, H. Ahn, J. Young, and C.-
H. Chen. Decision threshold adjustment in class prediction.
SAR and QSAR in Environmental Research, 17(3):337–352,
2006.

[10] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie. Class-
balanced loss based on effective number of samples. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2019.

[11] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei.
ImageNet: A large-scale hierarchical image database.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 248–255, 2009.

[12] J. Deng, J. Guo, and S. Zafeiriou. Arcface: Additive an-
gular margin loss for deep face recognition. arXiv preprint
arXiv:1801.07698, 2018.

[13] J. Deng, Y. Zhou, and S. Zafeiriou. Marginal loss for deep
face recognition.
In 2017 IEEE Conference on Computer
Vision and Pattern Recognition Workshops (CVPRW), pages
2006–2014. IEEE, 2017.

[14] C. Drummond and R. C. Holte. C4. 5, class imbalance, and
cost sensitivity: Why under-sampling beats over-sampling.
ICML Workshops, 2003.

[16] Y. Gal and Z. Ghahramani. Dropout as a bayesian approxi-
mation: Representing model uncertainty in deep learning. In
international conference on machine learning, pages 1050–
1059, 2016.

[17] M. Hayat, S. Khan, W. Zamir, J. Shen, and L. Shao. Max-
margin class imbalanced learning with gaussian afﬁnity.
arXiv preprint arXiv:1901.07711, 2019.

[18] H. He, Y. Bai, E. A. Garcia, and S. Li. Adasyn: Adaptive
synthetic sampling approach for imbalanced learning.
In
Neural Networks, 2008. IJCNN 2008.(IEEE World Congress
on Computational Intelligence). IEEE International Joint
Conference on, pages 1322–1328. IEEE, 2008.

[19] H. He and E. A. Garcia. Learning from imbalanced data.
IEEE Transactions on knowledge and data engineering,
21(9):1263–1284, 2009.

[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 7, 2017.

[21] C. Huang, Y. Li, C. Change Loy, and X. Tang. Learning
In Pro-
deep representation for imbalanced classiﬁcation.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5375–5384, 2016.

[22] P. Jeatrakul, K. W. Wong, and C. C. Fung. Classiﬁcation
of imbalanced data by combining the complementary neural
network and smote algorithm. In International Conference
on Neural Information Processing, pages 152–159. Springer,
2010.

[23] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning us-
ing uncertainty to weigh losses for scene geometry and se-
mantics. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.

[24] S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun. A
guide to convolutional neural networks for computer vision.
Synthesis Lectures on Computer Vision, 8(1):1–207, 2018.

[25] S. H. Khan, M. Hayat, M. Bennamoun, F. Sohel, and
R. Togneri. Cost sensitive learning of deep feature represen-
tations from imbalanced data. IEEE Transactions on Neural
Networks and Learning Systems, 2017.

[26] J. Laurikkala.

Improving identiﬁcation of difﬁcult small
classes by balancing class distribution.
In Conference on
Artiﬁcial Intelligence in Medicine in Europe, pages 63–66.
Springer, 2001.

[27] G. B. H. E. Learned-Miller. Labeled faces in the wild: Up-
dates and new reporting procedures. Technical Report UM-
CS-2014-003, University of Massachusetts, Amherst, May
2014.

[28] C.-Y. Lee, P. W. Gallagher, and Z. Tu. Generalizing pooling
functions in convolutional neural networks: Mixed, gated,
and tree. In Proceedings of the 19th International Confer-
ence on Artiﬁcial Intelligence and Statistics, pages 464–472,
2016.

[29] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. 2015.

[30] J. Liu, Y. Deng, T. Bai, Z. Wei, and C. Huang. Targeting ulti-
mate accuracy: Face recognition via deep embedding. arXiv
preprint arXiv:1506.07310, 2015.

[15] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner.
Detecting adversarial samples from artifacts. arXiv preprint
arXiv:1703.00410, 2017.

[31] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion.

111

[32] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin soft-
max loss for convolutional neural networks. In International
Conference on Machine Learning, pages 507–516, 2016.

[33] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face at-
tributes in the wild. In Proceedings of the IEEE International
Conference on Computer Vision, pages 3730–3738, 2015.

[34] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In Proceedings of International Con-
ference on Computer Vision (ICCV), 2015.

[35] I. Mani and I. Zhang. knn approach to unbalanced data
distributions: a case study involving information extraction.
In Proceedings of workshop on learning from imbalanced
datasets, volume 126, 2003.

[36] I. Masi, A. T. Trn, T. Hassner, J. T. Leksut, and G. Medioni.
Do we really need to collect millions of faces for effective
face recognition?
In European Conference on Computer
Vision, pages 579–596. Springer, 2016.

[37] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kot-
sia, and S. Zafeiriou. Agedb: the ﬁrst manually collected, in-
the-wild age database. In Proceedings of IEEE Intl Conf. on
Computer Vision and Pattern Recognition (CVPR-W 2017),
Honolulu, Hawaii, June 2017.

[38] S. Rahman, S. Khan, and F. Porikli. Zero-shot object de-
tection: Learning to simultaneously recognize and localize
novel concepts. arXiv preprint arXiv:1803.06049, 2018.

[39] C. E. Rasmussen. Gaussian processes in machine learn-
ing. In Advanced lectures on machine learning, pages 63–71.
Springer, 2004.

[40] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to
reweight examples for robust deep learning. In International
Conference on Machine Learning, 2018.

[41] C. C. V. P. R. C. D. J. S. Sengupta, J.C. Cheng. Frontal to
proﬁle face veriﬁcation in the wild. In IEEE Conference on
Applications of Computer Vision, February 2016.

[42] J. A. S´aez, J. Luengo, J. Stefanowski, and F. Herrera. Smote–
ipf: Addressing the noisy and borderline examples problem
in imbalanced classiﬁcation by a re-sampling method with
ﬁltering. Information Sciences, 291:184–203, 2015.

[43] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering.
In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 815–823, 2015.

[44] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations (ICLR), 2015.

[45] M. R. Smith, T. Martinez, and C. Giraud-Carrier. An in-
stance level analysis of data complexity. Machine learning,
95(2):225–256, 2014.

[46] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

[47] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
face representation by joint identiﬁcation-veriﬁcation.
In
Advances in neural information processing systems, pages
1988–1996, 2014.

[48] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 2892–2900, 2015.

[49] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁ-
cation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1701–1708, 2014.

[50] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Web-
scale training for face identiﬁcation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2746–2754, 2015.

[51] C. Tzelepis. Maximum Margin Learning Under Uncertainty.

PhD thesis, Queen Mary University of London, 2018.

[52] C. Tzelepis, V. Mezaris, and I. Patras. Linear maximum mar-
gin classiﬁer for learning from uncertain data. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 2017.
[53] B. C. Wallace, K. Small, C. E. Brodley, and T. A. Trikali-
In Data Mining (ICDM),
nos. Class imbalance, redux.
2011 IEEE 11th International Conference on, pages 754–
763. IEEE, 2011.

[54] F. Wang, J. Cheng, W. Liu, and H. Liu. Additive margin
softmax for face veriﬁcation. IEEE Signal Processing Let-
ters, 25(7):926–930, 2018.

[55] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li,
and W. Liu. Cosface: Large margin cosine loss for deep face
recognition.
In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[56] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina-
tive feature learning approach for deep face recognition. In
European Conference on Computer Vision, pages 499–515.
Springer, 2016.

[57] L. Wolf, T. Hassner, and I. Maoz. Face recognition in uncon-
strained videos with matched background similarity. In Com-
puter Vision and Pattern Recognition (CVPR), 2011 IEEE
Conference on, pages 529–534. IEEE, 2011.

[58] Y. Wu, H. Liu, J. Li, and Y. Fu. Deep face recognition with
center invariant loss.
In Proceedings of the on Thematic
Workshops of ACM Multimedia 2017, pages 408–414. ACM,
2017.

[59] S.-J. Yen and Y.-S. Lee. Cluster-based under-sampling ap-
proaches for imbalanced data distributions. Expert Systems
with Applications, 36(3):5718–5727, 2009.

[60] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker. Fea-
ture transfer learning for deep face recognition with long-tail
data. arXiv preprint arXiv:1803.09014, 2018.

[61] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection
and alignment using multitask cascaded convolutional net-
works. IEEE Signal Processing Letters, 23(10):1499–1503,
2016.

[62] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev.
Panda: Pose aligned networks for deep attribute modeling. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1637–1644, 2014.

[63] X. Zhang, Z. Fang, Y. Wen, Z. Li, and Y. Qiao. Range loss
for deep face recognition with long-tailed training data. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 5409–5418, 2017.

112

